CN116072130A - Automatic determination of timing window for voice subtitles in audio stream - Google Patents
Automatic determination of timing window for voice subtitles in audio stream Download PDFInfo
- Publication number
- CN116072130A CN116072130A CN202310074852.4A CN202310074852A CN116072130A CN 116072130 A CN116072130 A CN 116072130A CN 202310074852 A CN202310074852 A CN 202310074852A CN 116072130 A CN116072130 A CN 116072130A
- Authority
- CN
- China
- Prior art keywords
- audio data
- segments
- subtitle
- speech
- score
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/27—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the analysis technique
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/78—Detection of presence or absence of voice signals
- G10L25/87—Detection of discrete points within a voice signal
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L19/00—Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis
- G10L19/02—Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis using spectral analysis, e.g. transform vocoders or subband vocoders
- G10L19/022—Blocking, i.e. grouping of samples in time; Choice of analysis windows; Overlap factoring
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/03—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the type of extracted parameters
- G10L25/24—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the type of extracted parameters the extracted parameters being the cepstrum
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/48—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/93—Discriminating between voiced and unvoiced parts of speech signals
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/10—Indexing; Addressing; Timing or synchronising; Measuring tape travel
- G11B27/19—Indexing; Addressing; Timing or synchronising; Measuring tape travel by using information detectable on the record carrier
- G11B27/28—Indexing; Addressing; Timing or synchronising; Measuring tape travel by using information detectable on the record carrier by using information signals recorded by the same method as the main recording
Abstract
The invention relates to automatically determining timing windows for voice subtitles in an audio stream. The content system inputs segments of the audio stream into a speech classifier for classification, which generates an original score for the segments of the audio stream that indicates a likelihood that the corresponding segments of the audio stream include speech sounds. The content system generates binary scores for the audio stream based on the set of original scores, each binary score being generated based on an aggregation of the original scores from a consecutive series of segments of the audio stream. The content system generates one or more timing windows for the speech sounds in the audio stream based on the binary score, each timing window indicating an estimate of a start and end timestamp of the one or more speech sounds in the audio stream.
Description
Description of the division
The present application belongs to the divisional application of Chinese patent application No.201680081781.9 whose application date is 2016, 12 and 29.
Technical Field
The disclosed embodiments relate generally to computer-implemented methods for audio subtitles and, more particularly, to automatically determining timing windows for subtitles of voice sounds in an audio stream.
Background
Many media content items, such as video streams or audio streams, include both speech sounds and non-speech sounds. For speech sounds (e.g., spoken words, singed words), subtitles may be added to the content item so that the content may be consumed without hearing the audio stream of the content. A very large number (e.g., millions) of such content items may be uploaded to an online content system each day. However, not all of these content items are uploaded together with subtitles. Although these subtitles may later be added by an automatic speech sound recognition system, the accuracy of these subtitles is often very poor. Subtitles may also be added by other users (e.g., volunteers), but these volunteers may have to manually clock the start and end timestamps of each subtitle so that the subtitles match the start and end timestamps of the voice sounds in the content. This may be inconvenient for the user to perform and may prevent the volunteer from providing the subtitle.
Disclosure of Invention
In one embodiment, a computer-implemented method for automatically determining a timing window for a speech sound is described. The method further comprises the steps of: an audio stream is accessed and segments of the audio stream are input into a speech classifier for classification, which generates an original score for the segments of the audio stream that represents a likelihood that the corresponding segments of the audio stream include the occurrence of speech sounds. The method further comprises the steps of: generating binary scores for the audio stream based on the set of original scores, each binary score being generated based on an aggregation of the original scores from the continuous series of segments of the audio stream; and generating one or more timing windows for the speech sounds in the audio stream based on the binary score, each timing window indicating an estimate of a start and end timestamp of the one or more speech sounds in the audio stream.
The features and advantages described in this summary and the following detailed description are not all-inclusive. Many additional features and advantages will be apparent to one of ordinary skill in the art in view of the drawings, specification, and claims hereof.
Drawings
Fig. 1 is a high-level block diagram of an environment for automatically generating timing windows for voice subtitles according to an embodiment.
Fig. 2 is a high-level block diagram of a speech classifier according to an embodiment.
FIG. 3 illustrates an exemplary graph of the output of raw scores from a speech classifier according to an embodiment.
FIG. 4 is an exemplary graph illustrating a process by which a score smoother aggregates raw scores to generate binary scores, according to an embodiment.
FIG. 5 illustrates an exemplary graph with an exemplary output of a binary score from a score smoother and based on an aggregate value, according to an embodiment.
Fig. 6A is a high-level block diagram of a caption timing window modifier according to an embodiment.
Fig. 6B is a high-level block diagram of a subtitle module according to an embodiment.
Fig. 7 illustrates an exemplary User Interface (UI) with a subtitle box that has automatically generated a subtitle according to an embodiment.
Fig. 8 is a data flow diagram illustrating the actions of a content system for automatically generating non-voice subtitles from audio input according to an embodiment.
The figures depict various embodiments of the present invention for purposes of illustration only. Those skilled in the art will readily recognize from the following discussion that alternative embodiments of the structures and methods illustrated herein may be employed without departing from the principles of the invention described herein.
Detailed Description
I. Summary of the invention
The content system described herein provides the advantage of automatically generating timing windows for subtitles of speech sounds in audio. The content system analyzes an audio stream of the content item and divides the audio stream into a plurality of segments. For each segment, the content system extracts features and generates an original score that indicates the likelihood that the speech sound occurred in the segment. The original score is smoothed to generate a binary score, which in turn is used to generate a timing window for the subtitle box of the content item. The timing window indicates start and end time stamps of the speech sounds in the content item. Users, such as volunteers, may then provide subtitle text for these voice sound boxes. By automatically generating a timing window for the caption box instead of requesting the user to do so, the content system further motivates the user to provide more captions for content items that have been uploaded to the content system but have no associated captions. Hundreds of hours of content items may be uploaded to the content system every minute, and not all of these content items may have associated subtitles. Thus, for those people who have poor hearing or cannot enable the audio stream of the content item, providing subtitles to these users helps them enjoy the content item, and also provides an expanded audience for the content item.
II. System architecture
Fig. 1 is a high-level block diagram of an environment 100 for automatically generating timing windows for voice subtitles according to an embodiment. Fig. 1 shows a content system 105 having a content subsystem 140, a timing subsystem 110, a content store 130, and a subtitle subsystem 150. The content system 105 is connected to a network 190, the network 190 also being connected to the client device 170. Although certain elements are shown in fig. 1, in other embodiments, the environment may have different elements. Furthermore, the functionality between elements may be distributed differently in other embodiments to different or multiple modules.
Client device and network
Client device 170 is a computing device that may access content from content subsystem 140. A user of the client device 170 may access videos of the content subsystem 140 by browsing a content catalog using the client device 170, searching using keywords, viewing playlists from other users or system administrators (e.g., forming a collection of content for channels), or viewing content associated with a particular user group (e.g., community). Additionally, in some embodiments, the client device 170 may also be used to upload content to the content subsystem 140. Client device 170 may be a desktop, laptop, smart phone, tablet, wearable device, television, set-top box, and the like. Although fig. 1 shows only a single client device 120, it should be understood that many client devices (e.g., millions) may be in communication with the video hosting system 100 at any time.
In one embodiment, the client device 170 accesses content from the content subsystem 140 and presents the content to a user via the content presenter 175. In one embodiment, the content presenter is accessed through a web browser that includes a video player (e.g., a player conforming to the HTML5 standard). Further, the content presenter 175 may be capable of presenting content to a user concurrently with subtitles received from the subtitle subsystem 150. These subtitles may be used for speech sounds and/or non-speech sounds in the content. The content presenter 175 may be a web browser that allows a user to view web pages and content provided by the content subsystem 140.
In one embodiment, the client device 170 may upload the content item to the content system 105 (e.g., via the content presenter 175). Further, a user using the client device 170 may be able to contribute subtitle data to content items that have been uploaded to the content system 105. As described in further detail below, the content system 105 may provide the client device 170 with a timing window for the content item that indicates start and stop timestamps of voice sounds detected in the content item. The client device 170 may display these timing windows incorporated into the caption box to the user, allowing the user to input in the caption for the voice sounds associated with these timing windows.
Network system
The network 190 represents a communication path between the client device 170 and the content system 105. In one embodiment, the network 190 is the Internet, but can be any network including, but not limited to, LAN, MAN, WAN, mobile, wired or wireless networks, cloud computing networks, private networks, or virtual private networks, and any combination thereof. In addition, all or some of the links of network 190 may be encrypted using conventional encryption techniques, such as Secure Sockets Layer (SSL), secure HTTP, and/or Virtual Private Network (VPN). In another embodiment, entities may use custom and/or dedicated data communication techniques in place of or in addition to those described above.
Content system
The content system 105 provides content items to users of the client devices 170 and also receives content uploaded from the client devices 170. The content system 105 includes a content store 130, a content subsystem 140, a timing subsystem 110, and a caption subsystem 150. The content system 105 may also include additional elements, subsystems, and servers, such as a load balancing subsystem, a content distribution network, and the like. These and other additional elements are excluded from the illustration in fig. 1 in order to enhance ease of understanding. In addition, the functionality of each illustrated subsystem may be partitioned between more than one hardware device. For example, the functionality of content subsystem 140 as described herein may be distributed across multiple individual servers.
Content store
The content memory 130 of the content system 105 stores content items 135. The content items 135 may include video and/or audio content items of various durations, resolutions, etc. For example, the content item 135 may include a 4K video file, or a 360 degree video file, or a stereoscopic video file. Each content item may include an audio stream that is the audio portion of the content item (although the term "stream" is used herein, it does not necessarily mean that audio is streaming, i.e., is continuously received by the client device 170 while being transmitted to the client device by the content system 105). The content store 130 may also store associated metadata for the content item 135, including the voice subtitles 150 associated with the content item 135. Each content item 135 stored in the content store 130 may also be stored with and associated with metadata such as subtitles, descriptions, responsive reviews, and ratings.
The voice subtitle 150 includes transcribed text of a voice sound in a plurality of content items 135 in the content store 130. The speech sounds may include any type of audio that may be understood as a language. This may include spoken dialogue, rhythmic spoken and singing words, and the like. The voice sounds may also include voices in multiple languages (i.e., not just english). In one embodiment, voice subtitle 150 includes a plurality of entries, each entry including a timing window and text associated with the timing window. The timing window includes a start time stamp and an end time stamp, and the text includes a transcription of the voice sounds present in the content item between the start time stamp and the end time stamp. For example, the timing window may have a start timestamp of 00:54:12 seconds, an end timestamp of 00:54:17, and the associated text may be "one person may persuade anyone that he is actually another person, but not himself. Note that text may not accurately represent speech sounds in corresponding segments of a content item in all cases due to transcription, timing, or other errors. In one embodiment, the content item may also be associated with a non-voice subtitle. The non-voice subtitles may also include a timing window indicating when non-voice sounds (e.g., laughter) occur within the content item, and an indicator of what the non-voice sounds are (e.g., "music," or an identifier, such as a number for the non-voice sounds of the music).
Content subsystem
The content subsystem 140 provides access, viewing and listening to the content items 135 and allows uploading of the content items 135 (process not shown). The content subsystem 140 allows a user to access content items 135 in the content store 130 through a search and/or browsing interface. The content items 135 may originate from user uploads of content, searches or crawling of other websites or databases originating from content, and the like, or any combination thereof. For example, in one embodiment, content subsystem 140 may be configured to allow user uploading of content. The content subsystem 140 stores these uploaded content items in the content store 130. As another example, content subsystem 140 may retrieve content from other content databases over network 190.
In addition to content, the content subsystem 140 may also receive accompanying subtitle data. The subtitle data may indicate start and end times of subtitles corresponding to voices (and non-voice sounds) in the content. The content may also be associated with other information such as subtitles, descriptions, content types, authors and ratings, etc. The content subsystem 140 stores the content (and any associated metadata) in the content store 130.
Timing subsystem
The timing subsystem 110 generates timing windows for voice subtitles for the content items 135, at least for those content items that do not accompany the subtitle data when received by the content subsystem 140. As described above, these timing windows include start and end time stamps that indicate the start and end of speech sounds in the audio stream of the content item. To generate these timing windows, the timing subsystem 110 may include a speech classifier 115 and a score smoother 120. The speech classifier 115 generates an original score representing the numerical likelihood that a segment of the audio stream includes speech sounds. The score smoother 120 generates binary scores for the audio segments by smoothing the raw scores from the speech classifier 115. The binary score of a segment of the audio stream of the content item indicates a positive best estimate determination by the smoother 120 of whether the segment has speech sounds or no speech sounds, and may be used to generate a timing window of the audio stream that indicates the location in the audio stream where the speech sounds start and end.
The speech classifier 115 generates an original score indicating the likelihood that a segment of the audio stream (of the content item 135) includes speech sound (language-agnisic). To generate these raw scores, the speech classifier 115 segments or divides the audio stream into a plurality of segments (e.g., each segment is 250 milliseconds (ms) in length). Each segment may partially overlap with the preceding and following segments. The speech classifier 115 analyzes each segment using a trained model or classifier to determine the likelihood that the segment of the audio stream presents speech sounds. The likelihood may be represented using a numerical value, i.e., an original score. The model may be trained using features extracted from a corpus of data consisting of existing subtitles for speech sounds in an audio stream. While the model may be trained to indicate whether the sound is speech, for the purpose of generating a timing window, it may not be necessary to train the model to determine the actual phonemes, syllables, words, sentences, or other semantics or grammar of the speech sound, and may be sufficient to determine whether speech is present. Additional details regarding the speech classifier 115 are described below with reference to fig. 2.
The score smoother 120 generates a binary score from the raw score from the speech classifier 115 by smoothing the raw score. Since the raw score generated by the speech classifier 115 may fluctuate at a high frequency over a short period of time (e.g., 1 second), directly using the raw score may result in undesirable results with multiple timing windows for a single speech portion in the audio. From the perspective of someone attempting to add subtitles to time windows or read subtitles entered into those time windows, the amount of voice with subtitles entered into each window may be too small to consistently map to activity on the screen, and the viewer may have difficulty reading and operating on what is happening, as the time windows are typically too short to stay on the screen for any meaningful duration. Alternatively, the score smoother 120 produces a binary score that does not fluctuate significantly and is therefore "smoothed".
The score smoother 120 smoothes the original scores of particular segments of the audio stream by aggregating a series of original scores over successive segments of the audio stream to generate an aggregate value. For example, the score smoother 120 may aggregate the raw scores from segments of the audio stream for a total of 1000 milliseconds. Aggregation may be a mathematical or statistical operation, such as an average or median. If the aggregate value exceeds the threshold, the binary score for the series of consecutive segments may be set to 1 (i.e., on or high, indicating the presence of speech). Otherwise, the binary score for the series of consecutive segments may be set to 0 (i.e., off or low, indicating no speech). The score smoother 120 determines that successive segments of each series of binary scores may partially overlap successive segments of other series. After smoothing the original scores into binary scores, the score smoother 120 generates timing windows corresponding to these binary scores (e.g., each timing window associated with consecutive periods of binary score 1), and if the score smoother 120 determines that the timing windows introduce a large number of errors, the threshold may be adjusted. Additional details regarding score smoother 120 will be described below with reference to FIGS. 3-5.
Subtitle subsystem
The caption subsystem 150 provides the voice captions 140 to the client device 170 for presentation with the associated content items 135 and also provides an interface for the user to provide the voice captions 140 for content items 135 that may not have voice captions 140 associated therewith.
When a client device 170 requests and is presented with a content item 135, if the client device 170 also requests a subtitle for the content item 135, the subtitle subsystem 150 may provide the subtitle for the content item 135 to the client device 170 for presentation with the content item 135. The caption subsystem 150 may select the voice captions 140 associated with the content item 135 and send these captions to the client device 170 along with instructions for presenting text within certain ones of the voice captions at certain times during playback of the content item 135 such that the text in each entry is presented according to timing window information for the entry. For example, the instructions may cause the client device 170 to present text of the caption entry during playback between a start timestamp and an end timestamp of a timing window associated with the caption entry. Additionally, the instructions may also instruct the client device 170 how to visually format the text for presentation (e.g., font style, font type, font size, text location on screen, etc.). In one embodiment, in addition to the voice subtitles 140, the subtitle subsystem 150 retrieves a set of non-voice subtitles (e.g., laughter) describing non-voice sounds in the content item and sends these non-voice subtitles and instructions on how to render them to the client device 170.
For content items 135 that do not have subtitle information or have incomplete or inaccurate subtitle information (e.g., automatic subtitles using voice recognition may not produce accurate results), the subtitle subsystem 150 receives user-generated entries for the voice subtitles 140 from the client device 170. A user of the client device 170 may transcribe the voice sounds (e.g., spoken dialog) in the content item 135 into text and submit the transcribed text to the caption subsystem 150, which the caption subsystem 150 may store as part of the caption entry.
In other systems, when transcribing a particular series of voice sounds for a portion of voice, the user must specify the start and end time stamps for the portion of voice to the caption subsystem 150. In other words, the user must manually specify a timing window for each text portion transcribed by the user. The specification of the timing window is cumbersome for the user and thus may prevent the user from providing a transcription of the speech sounds.
To address this issue, the caption subsystem 150 includes a subtitle module 160 that uses the timing window generated by the timing subsystem 110 for the content item 135 to automatically provide a timing window for the speech portion in the content item 135. By providing an automatically determined timing window for the voice portion of the content item 135, the subtitle module 160 can greatly reduce the effort of the user when the user provides text transcribed by the user for the voice subtitle 140 of the content item 135. This, along with other features (e.g., a rating score for each user), increases the incentive for user assistance (e.g., crowd-sourced) to transcribe text from speech in the content item.
In addition, the caption subsystem 150 may also include a caption timing modifier 155 to create a final timing window for the caption box displayed to the user. The caption timing modifier 155 may modify the timing window generated by the timing subsystem 140 based on a set of configurations. The caption timing modifier 155 modifies the timing window to produce a set of caption boxes that are more visually pleasing to the viewer. For example, the caption timing modifier 155 may divide a timing window beyond a specific duration such that more than one caption box may be generated from the timing window. As another example, the subtitle timing modifier 155 may connect a plurality of timing windows shorter than a specific duration together, or may extend the timing window shorter than the specific duration. Additional details regarding the subtitle timing modifier 155 and the subtitle module 160 will be described below with reference to fig. 6-7.
III, voice subtitle classifier
Fig. 2 is a high-level block diagram of the voice subtitle classifier 115 according to an embodiment. The voice subtitle classifier of fig. 2 includes: an audio separator 210, a filter cascade model 220 (for determining features), and a classifier model 225 (for determining raw scores as described above). Although certain elements are shown in fig. 2, in other embodiments, the environment may have different elements. Furthermore, the functionality between elements may be distributed differently in other embodiments to different or multiple modules.
The audio separator 210 separates the audio stream of the content item 135 into different segments. The audio separator 210 divides the audio stream into small segments of a certain duration, each segment being offset by one interval from the start of the previous segment. In some cases, the duration of each segment is longer than the time offset interval between each segment, and thus the segments may partially overlap. For example, each segment may be separated by a 10 millisecond interval, and the duration of each segment may be 250 milliseconds. Thus, a 10 second audio clip will have 1,000 individual segments, a first segment starting at 0 milliseconds and ending at 250 milliseconds, and a second segment starting at 10 milliseconds and ending at 260 milliseconds, and so on. The last segment will begin at 9,750 milliseconds (i.e., 10,000-250 milliseconds) and end at 10,000 milliseconds. In particular, the number of segments within an audio clip of X duration will be: ((X-segment size)/interval size) +1.
Alternatively, the last segment may be shorter in duration and may not have a duration equal to the duration of the other segments. Although the duration is indicated as 250 milliseconds in this example, in other embodiments the duration is 1000 milliseconds (i.e., one second). In another embodiment, the last segment may be a regular segment length and, in case the segment exceeds the length of the audio clip, the segment is filled with a null or zero signal. For example, referring to the example above, for a segment starting at 9900 milliseconds in a 10 second clip, the remaining 150 milliseconds of the segment beyond the end of the audio clip may be filled with zeros.
The filter cascade model 220 receives segments of the audio stream of the content item from the audio separator 210 and extracts features of each corresponding segment. In one embodiment, the filter cascade model 220 is based on physiology of the human ear. The filter cascade model 220 may divide the input sound into a plurality of frequency channels and includes a cascade of a plurality of filters (with a gain control coupled to each filter). Each filter filters out a specific range of frequencies or sounds and the (numerical) output from these various filters is used as a basis for a feature that is used by classifier model 225 to classify the speech sounds in the segment. In one embodiment, the output of the filter may be processed to generate an auditory image that is used as a basis for the eigenvalues of the classifier model 225.
In one embodiment, the filter cascade model 220 is a cascade of asymmetric resonators with a fast acting compression (CARFAC) model. The CARFAC model in combination with a multi-time scale coupled Automatic Gain Control (AGC) network is based on an auditory filtered pole-zero filter cascade (PZFC) model. This mimics features of auditory physiology such as masking, compression of the traveling wave response, and stability of zero crossing times of signal levels. The output of the CARFAC model ("neural activity pattern") may be converted to capture the pitch, melody and other temporal and spectral features of the sound.
Although features are extracted using the filter cascade model 220 as described herein, in other embodiments, features may be extracted using another model, such as a spectrogram modified by the mel filter bank (mel filter bank). In other words, the speech classifier 115 utilizes mel-frequency cepstral coefficients (MFCCs) as the extraction features of the audio stream. These MFCCs represent audio power spectra based on pitch-aware scales, which are referred to as mel scales. Other methods of extracting features may also be used, such as using the original spectrogram of the audio segment itself as a feature.
After the values of the features are generated by the filter cascade model 220, the speech classifier 115 inputs the values of the features into the classifier model 225. The classifier model 225 may be a machine learning model, such as a deep neural network, bayesian network, support vector machine, or other machine learning model, that accepts as input the eigenvalues of an audio segment and generates an original score for the segment that indicates the likelihood that the segment includes speech sounds. The raw score may be amplified from 0 (0% likelihood) to 1 (100% likelihood). For example, classifier model 225 may indicate for an audio segment that the original score (i.e., likelihood) of the occurrence of speech sounds in the segment is 0.6 (60%).
To train the classifier model 225, features extracted from the training dataset (using a filter cascade model) are used. The training data includes an audio stream and corresponding subtitle labels indicating timing windows within the audio stream in which speech sounds are known to occur. The subtitle labels for the audio streams may also indicate that speech occurs at a certain point in time or in a segment of the audio stream or that speech occurs within a certain time range. The subtitle label may transcribe the actual speech in the audio stream or may simply indicate where the speech occurred. The training data may include speech from different languages and may include various forms of speech thereof, such as low-voice speech, singing speech, and the like. In some cases, voices in different languages and forms may be labeled differently. In one embodiment, the audio stream in the training data may be divided into intervals, e.g., two second intervals, each of which is to be used separately as a training sample.
The training data may be retrieved from the content store 110 and may be based on subtitles for content items that have been associated with subtitles provided by the user with a good trust score above a certain threshold. These trust scores are described in further detail below. When new training data is received, the training data may be updated and the model periodically retrained. Training data (e.g., from a corpus of broadcast subtitle data and audio) may also be retrieved or received from a third party. The classifier model 225 may train itself or training may be performed by an administrator or other user.
The training process may be iterative (e.g., by using back propagation), and for each iteration, the weights within the classifier model 225 may be updated to minimize errors between the output from the classifier model 225 and the ground truth data for all samples in the training dataset. For example, within a training set, occurrences of speech may be given a numerical score of "1" while non-occurrences of speech may be given a numerical score of "0". Classifier model 225 is trained such that it is closest to these values (e.g., "1" and "0") in the ground truth data, i.e., such that the delta between the output of score classifier model 225 and the ground truth values is minimized. Since the training data may not be entirely accurate, the values of certain portions of the training data may also be weighted differently depending on the source of the data. For example, the training data may be weighted according to the quality of the audio stream recording (e.g., microphone quality) of the source of the training data.
After the initial training process, features extracted from a set of test data separate from the training data and also associated with a set of subtitle labels may be fed into the classifier model 225 to verify that the output of the classifier model 225 is accurate. If the error in the test data display output exceeds a particular threshold, the classifier model 225 may be retrained using a larger data set or using a different initial set of weights.
Original score map of exemplary Speech classifier
FIG. 3 illustrates an exemplary plot 300 of the output of raw scores from the speech classifier 115, according to an embodiment. As shown in fig. 3, an original score is generated by the speech classifier 115 for the duration of the audio stream of each segmented content item. As shown in fig. 3, there are N segments, labeled segment number 310 on the horizontal axis of graph 300. Each segment has a particular duration and starts at a specified offset interval from the beginning of the previous segment, as described above. Since each segment may be offset from the previous portion only by a short offset interval, a single audio stream may have hundreds or thousands of segments or more.
The speech classifier 115 generates an original score for each of these segments. The raw score is represented on the graph 300 as a raw score plot line 330. As shown in fig. 3, the raw score generated by the speech classifier 115 may vary significantly over time, and the raw score may be distributed throughout the range of possible scores. Due to fluctuations in the original scores, they may not be readily used as an indicator of whether speech is occurring at a certain point in time in the audio stream. Alternatively, the raw scores are smoothed into a set of binary scores, as described below. Note that although the curves shown in fig. 3 may appear to be continuous for ease of illustration, the actual raw scores produced are discrete.
V. exemplary procedure for score smoothing
FIG. 4 is an exemplary graph 400 illustrating a process by which the score smoother 120 aggregates raw scores to generate binary scores, according to an embodiment. As previously described, the score smoother 120 "smoothes" the raw scores generated by the speech classifier 115 and generates a set of binary scores based on the raw scores. To generate these binary scores, the score smoother 120 generates or calculates an aggregate value for each of a series of consecutive segments in the audio stream based on the original scores (calculated by the speech classifier 115) for the segments in each series. The successive segments of each series for which the score smoother 120 is acting may include a certain number of segments (e.g., 100 overlapping segments equal to one second in time), and each series may be offset from the previous series by a certain number of segments (e.g., one segment) or by a certain time interval (e.g., 250 milliseconds).
The aggregate function may be calculated by the score smoother 120 using the aggregate function. An aggregation function may be any statistical or mathematical operation that generates a single value from multiple values of similar types, where a single value represents certain attributes, features, or other features that rely on multiple values. Examples of such aggregation functions may include median, mean, variance, standard deviation, geometric mean, and the like. In one embodiment, the score smoother 120 uses a predetermined aggregation function for the original scores in the series of segments to generate the aggregate value.
In another embodiment, the score smoother 120 selects between multiple aggregation functions to determine the best aggregation function to use. To determine the best aggregation function to use, the score smoother 120 may use the verification data set in which the caption tag is known and select an aggregation function that produces the aggregate value set that best matches the underlying facts of the verification data. For example, the score smoother 120 may first use the average as an aggregate function of the raw scores generated from the verification data (by the speech classifier 115) and compare the resulting aggregate value to subtitle labels of the verification data set to determine an error amount (delta) of the aggregate value from the value of the ground truth. The score smoother 120 may then use other aggregation functions to determine the error and select the aggregation function that yields the smallest error over all aggregate values.
As shown in FIG. 4, each of the exemplary series of consecutive segments 450A-D includes 100 segments. Since each segment is offset from the previous segment by 10 milliseconds, each segment 450 includes a segment of approximately one second. For each series of segments 450, the score smoother 120 calculates aggregate values 460A-D based on the raw scores of the segments in the respective series of segments 450. Thus, the raw scores of the segments in segment series 450A are used to calculate aggregate value 460A, and aggregate value 460B is calculated based on the raw scores in segment series 450B, and so on. For purposes of illustration, the aggregate value 460 shown in the graph 400 represents an average of the raw scores of the segments in the corresponding series of segments 450, however, in another embodiment, another aggregate function may be used to generate the aggregate value 460. Note that the average value in the illustration may be mathematically inaccurate and may be merely an approximation of the average value for illustration purposes.
Based on the aggregate values, the score smoother 120 generates binary scores, as described in further detail below with respect to FIG. 5.
FIG. 5 illustrates an exemplary plot 500 having an exemplary output of binary scores from the score smoother 120 and based on the aggregate value 460 as shown in FIG. 4, according to an embodiment. These binary scores are used to generate timing windows for subtitles of speech sounds in an audio stream. To generate a binary score, the score smoother 120 obtains each aggregate value 460 associated with each segment series and determines whether the aggregate value 460 is above or below a threshold. If the aggregate value 460 is above the threshold, the score smoother 120 sets the binary score of the portion of the audio stream corresponding to the segment series 450 that was used to generate the aggregate value 460 and that does not overlap any other segment series to "on" (examples are given below). Similarly, if aggregate value 460 is below the threshold, score smoother 120 sets the binary score to "off" for the corresponding portion.
In one embodiment, the threshold is a default pre-configured value. In another embodiment, the threshold value selected by the score smoother 120 varies based on an aggregation function used to generate the aggregate value. In another embodiment, the threshold is dynamically determined by the score smoother 120 based on the validation data set (ground truth). The score smoother 120 selects a threshold and generates a set of binary scores.
The score smoother 120 generates timing windows corresponding to the binary scores. For example, the score smoother 120 determines that the start time stamp corresponds to the time in the audio stream when the binary score switches from off to on, and the corresponding stop time stamp is the time when the binary score switches from on to off. After generating the timing window, the score smoother 120 compares the timing window to the verification data to determine an error value. The error value is equal to half of the sum of the actual start and end time stamps in the ground truth verification set and all deltas (differences) between the corresponding start and end time stamps calculated by the score smoother 120 using the binary scores. The score smoother 120 may iteratively adjust the threshold to achieve lower error values.
The above-described process is further illustrated in graph 500. As shown, the horizontal axis remains segment number 310, however, the vertical axis represents binary score 520 instead of the original score range, as shown in FIGS. 3 and 4. As shown in fig. 5, a binary score plot line 510 represents a plot of the generated binary score and moves between off and on (e.g., 0 and 1). The aggregate value 460 from fig. 4 is overlaid as a dashed line. When the aggregate value 460 is above the threshold 430 (represented by the dashed line), the binary score of the corresponding portion is set to "on" and when the aggregate value 460 is below the threshold 430, the binary score of the corresponding portion is set to "off. Each corresponding partial aggregate value is a portion of the audio stream from a start time stamp of a first segment used to calculate the aggregate value until a point in time at which a second series of segments is introduced. For example, in the illustrated graph 500, the portion corresponding to the aggregate value 460 starts at segment number 0 and ends at segment number 25 (the point at which the next series of segments and the next aggregate value start).
In the illustrated graph 500, the binary score plot line 510 remains in the "on" position until segment number 50, and then switches to the "off" position. The entire audio stream may comprise more segments. The binary score plots the entire length of the audio stream for line 510 until it reaches the end (segment N). Note that the jagged line in the binary score plot line 510 represents a portion of the binary score plot line 510 not shown here.
After generating the binary scores, the score smoother 120 also generates timing windows corresponding to the binary scores. The timing window corresponds to the time position at which the audio stream binary score switches between on and off. Specifically, the start time stamp of the timing window corresponds to the time when the binary score switches from "off" to "on" (or "0" to "1"), and the stop time stamp of the timing window corresponds to the time when the binary score switches from "on" to "off". The time period between the "start" and "stop" time stamps is the portion of the audio stream that the content system 105 determines that a voice sound is present. The raw score, binary score, and/or timing window may be stored by the timing subsystem 110 in the content memory 130, and in particular, may be stored as a voice caption 140 of the associated audio stream and content item 135.
By having the score smoother 120 perform this smoothing operation, the original score is converted to a set of binary scores that are less fluctuating and more stable, with longer periods between on/off transitions. The timing window generated by the score smoother 120 from these binary scores is more likely to increase the use of subtitles than a system that directly generates timing windows using the original scores. Note that the figures herein may not be drawn to scale and may be computationally/mathematically inaccurate, but are shown for illustrative purposes.
Exemplary subtitle timing modifier and subtitle Module
Caption timing modifier
Fig. 6A is a high-level block diagram of the caption timing modifier 155 according to an embodiment. The caption timing modifier 155 of fig. 6A includes a timing divider 610, a timing connector 615, and a timing extender 620. Although certain elements are shown in fig. 6A, in other embodiments, the environment may have different elements. Furthermore, functionality between elements may be distributed differently to different or multiple modules in other embodiments.
The timing divider 610 divides those timing windows of longer duration to generate more than one subtitle frame for such timing windows. As used herein, the duration of a timing window is the time difference between the start timestamp and the end or stop timestamp of the timing window. As described above, the start timestamp indicates the start of a voice sound in an audio stream, and the end or stop timestamp indicates the end of a voice sound in an audio stream. When the duration of the timing window is longer than a predefined maximum duration (e.g., 10 seconds), the timing divider 610 may divide or divide the timing window into a plurality of timing windows such that subtitle frames created from the plurality of timing windows have a shorter length and are more likely to push an increase in the use of subtitles by users of the content system 105. For example, if a timing window has a duration of 10 seconds (where speech sounds are present throughout the corresponding audio stream) without dividing the timing window into multiple timing windows, a subtitle box generated from the subtitle will have a duration of 10 seconds and may be very large when displayed on a screen. This may cause the user to disable the subtitle and reduce the adoption rate of the subtitle. Alternatively, the timing divider 610 divides these timing windows into smaller timing windows, which may generate subtitle frames for display for shorter periods of time.
The timing divider 610 may divide the timing window into smaller timing windows of a particular preset duration (e.g., 3 seconds). In some cases, by dividing the timing window into these smaller timing windows, the resulting timing window may include a remaining timing window (at the "end" of the original timing window) that has an undesirably short duration (e.g., 1 second) below the minimum threshold. In this case, the timing divider 610 may instead combine the timing window that extends the remaining timing window and the remaining timing window together and divide the combined timing window in half to generate two timing windows of the same duration and to generate an end time window that is not unduly short. The timing divider 610 may also simply divide the timing window into a certain number of partitions (e.g., two partitions) such that the resulting timing window is shorter than a predefined maximum duration, rather than dividing the timing window into multiple timing windows of a preset duration.
In one embodiment, the timing divider 610 receives an indication of a location in a timing window in which a short gap (e.g., a micro gap) occurs from the timing subsystem 110. These may be gaps of very short duration (e.g., 0.2 seconds) that are ignored by the smoothing process, but which may be used as the segmentation points. Although the speech sound is substantially continuous throughout the duration of the timing window, splitting the timing window at the gap allows for the generation of subtitle frames that naturally split at the gap points and thereby produce a more visually pleasing result.
The timing connector 615 combines timing windows below a predefined minimum duration to generate a subtitle frame that is presented for at least a predefined minimum period of time. When multiple timing windows in the audio stream have durations below a predefined minimum (e.g., 1 second) and are also within a certain interval (e.g., 0.5 seconds) relative to each other, the timing connector 615 may combine or connect the timing windows into a single timing window such that a caption frame based on the combined timing window is displayed for the duration of the combined timing window period, rather than displaying the duration of each original shorter timing window individually. This allows subtitle frames to be displayed that do not "flicker" where the subtitle frames are displayed and then removed from view in rapid succession. The timing connector 615 may connect these shorter duration timing windows together up to a predefined maximum duration, as previously described. If additional timing windows are to be connected, the timing connector 615 connects these additional timing windows into separate combined timing windows.
The timing extender 620 extends the timing window with a duration less than the predefined minimum such that the duration of the timing window is at least the duration of the predefined minimum. When the timing window is less than a predefined minimum duration but not adjacent in time to another timing window by a particular interval (e.g., 1 second) and is thus isolated from the other timing window, the timing extender 620 extends the duration of the timing window by shifting the start or end timestamp of the timing window such that the caption box generated by the timing window is presented for at least the predefined minimum duration. This creates a subtitle box that is displayed for a predefined minimum duration, allowing the user sufficient time to read the subtitle, rather than flashing the subtitle quickly during presentation.
Whether the timing extender 620 shifts the start time stamp back, advances the end time stamp, or both depends on the context in which the timing window is presented. If the start timestamp of the timing window is near or at the beginning of the audio stream (e.g., within 0.2 seconds), the timing extender 620 can move the end timestamp of the timing window forward until the duration of the timing window meets a predefined minimum. If the end timestamp of the timing window is near or at the end of the audio stream (e.g., within 0.5 seconds), the timing extender 620 moves the start timestamp of the timing window back until the duration of the timing window meets a predefined minimum. In one embodiment, the timing extender 620 may instead modify the audio stream to add silence portions in order to accommodate timing windows with shifted time stamps. For example, the timing extender 620 may extend the end timestamp of the timing window beyond the end of the audio stream and then insert the silence portion to the end of the audio stream that matches the duration of the timing window beyond the end of the original audio stream.
If the timing window is not near the end or beginning of the audio stream, the timing extender 620 may extend the end timestamp of the timing window until a predefined minimum is met. However, if extending the end timestamp results in the timing window being within a certain gap interval (e.g., 1 second) of another timing window of long duration, the timing extender 620 may instead shift both the start and end timestamps of the timing window so as to satisfy the predefined minimum duration and so that the timing window has at least a certain gap interval relative to the longer duration timing window.
Subtitle module
Fig. 6B is a high-level block diagram of the subtitle module 160 according to an embodiment. The subtitle module 160 of fig. 6B includes: a supplemental timing module 650, an automatic timing feedback module 655, and a user caption feedback module 660. Although certain elements are shown in fig. 6B, in other embodiments, the environment may have different elements. Furthermore, the functionality between elements may be distributed differently in other embodiments to different or multiple modules.
The supplemental timing module 650 provides a graphical interface with a caption box to the client device 170 for the user to provide caption text according to the automatically generated caption timing window. The automatically generated caption timing window may be received directly from the timing subsystem 110 or through the caption timing modifier 155. When the caption subsystem 150 receives a request from the client device 170 indicating that the user wishes to submit a caption of an audio stream associated with a content item, the auxiliary timing module 650 provides an interface to the user (e.g., by sending HTML or other code to the client device 170 presenting the interface) that allows the user to play back the audio stream (and any accompanying video) and to input caption text for voice sounds in the audio stream. The auxiliary timing module 650 also accesses timing windows of the audio stream (which may be dynamically generated or previously generated when uploading the content item) and provides the user with an interface with subtitle boxes and start and end timestamps for each subtitle box according to these timing windows, as well as the option to modify the start and end timestamps. This allows the user to view the duration of each caption, enter caption text associated with each caption, and modify the start and end timestamps for each caption if the automatically generated timing window for the caption is determined by the user to be inaccurate or somehow undesirable. An exemplary user interface presented by the auxiliary timing module 650 is shown in fig. 7 and described below.
In one embodiment, the auxiliary timing module 650 provides an interface to the user to allow the user to enter subtitles for the content item 135 in a long form. Thus, instead of entering subtitle text separately for each individual voice sound, the auxiliary timing module 650 may allow the user to enter the entire subtitle text for the content item in a single block (e.g., in a single text box). The auxiliary timing module 650 divides the input caption text into corresponding caption frames based on the timing window.
To this end, the auxiliary timing module 650 may determine an approximate speed or rate of speech (e.g., syllables per minute) in the audio stream of the content item 135 based on the duration of all automatically generated timing windows and the number of syllables in the input subtitle text. Using the rate information, the auxiliary timing module 650 also determines the location of gaps or breaks in the input text, which may be indicated by punctuation or other syntax (e.g., periods, new paragraphs, commas, etc.), which may correspond to gaps between automatically generated timing windows. For example, when it is determined from the speech rate that the sentence has the same duration as the timing window, the sentence in the input subtitle text may correspond to the end of the timing window in the automatically generated subtitle data.
After separating the input text into corresponding subtitle boxes, the auxiliary timing module 650 may prompt the user to verify whether the input text is correctly separated into correct subtitle boxes. The user may modify the text in each subtitle box if desired.
In one embodiment, the auxiliary timing module 650 may utilize a speech recognition engine to automatically transcribe speech in the content item into caption text, enter the caption text into the caption according to an automatically generated timing window, and present this completed caption set to the user for editing and verification. Since speech recognition of speech in the content item 135 may not be entirely accurate, the verification process allows the user to edit the transcribed text to improve accuracy. The auxiliary timing module 650 may also feed back the edited caption text to the speech recognition engine to improve its accuracy. By transcribing text first, and also automatically generating subtitles, the auxiliary timing module 650 may also save more time for the user when the user transcribes the voice sounds of the content item 135 into subtitle text.
The automatic timing feedback module 655 determines the quality of the automatically generated timing window based on feedback received from the user. In the content system 105, each grouping of content items, e.g., created by a single entity or under a single user name, etc., may be associated with a globally unique identifier. In some cases, the grouping of content items is identified at a channel level, where each channel contains a set of content items assigned to the group by a user associated with a username. The auto-timing feedback module 655 may collect feedback from the user on the quality of the auto-generated timing windows as they relate to the creation of the caption box for each channel by the auxiliary timing window module 650. The feedback is used to generate a score for the automatically generated timing window for the particular channel. Some passive feedback is considered negative and has a negative impact on the score of the automatically generated timing window of the channel. Examples of these include modifying the start and end timestamps of the time window of an automatically generated subtitle frame, deleting automatically added subtitle frames, and adding new subtitle frames for non-automatically detected voice sounds, etc. These modifications indicate that the automatically generated timing window is inaccurate. The lower number of these negative feedback events may alternatively cause the auto-timing feedback module 655 to increase the score of the automatically generated timing window for the channel. In some cases, feedback events from users with lower trust scores may be weighted to be less important in calculating the feedback scores.
If the channel's feedback score falls below a certain threshold (e.g., the threshold may be an average of the current feedback scores for all channels in the content system 105), the auto-timing feedback module 655 may send a message to the administrator, or may use the new subtitle from the user as new ground truth data for the channel to further optimize the classifier model used by the timing subsystem 110 to generate the binary score as described above. Such optimization and modification of parameters of the model may be specific to channels indicating feedback scores below a level, or may be universally applicable, or may weight certain channels more strongly than others.
The user subtitle feedback module 660 determines a trust score for the user contribution of the subtitle text. The user caption feedback module 660, upon receiving the user contribution of the caption text, performs one or more basic checks (e.g., a "soundness" check) on the caption text to verify its accuracy and quality. The check may relate to language, length and other characteristics of the received subtitle text.
For example, the user subtitle feedback module 660 may examine the subtitle text for inappropriate vocabulary, and in particular profanity or other strong language. Depending on the indicated rating of the content item, it may not be desirable to have a certain language appear to the audio stream, and thus the appearance of such language may cause the user caption feedback module 660 to decrease the trust score of the contributing user. As another example, the user subtitle feedback module 660 may check to see if the length or coverage of the provided subtitle text approximately matches the length of the content item. If not, user caption feedback module 660 may decrease the trust score of the contributing user. As a final example, the user subtitle feedback module 660 may check to see if the language of the received subtitle text matches the language indicated for the content item (e.g., "english"). If the languages do not match, user caption feedback module 660 may decrease the trust score of the contributing user.
If the user's trust score is low, user caption feedback module 660 may perform more checks on the user-provided contribution. In addition, the user caption feedback module 660 may request additional comments from other users as well as final comments by an entity (e.g., user) that is marked as the owner of the content item. If the user's trust score is high (e.g., above a threshold), the user caption feedback module 660 may perform less checking for the user's contribution and may require less comments from other users. When comments from other users are requested, the user caption feedback module 660 may indicate to the auxiliary timing module 650 that additional users are requested to verify the accuracy of the provided caption text. The number of changes made by other users to the provided subtitle text may be proportional to the decrease in trust score of the user that originally provided the subtitle text. In other words, the less changes made by other users during verification, the higher the user's trust score may be. If the user's trust score is below a certain level, the user caption feedback module 660 may prevent the user from providing caption text.
Exemplary subtitle user interface
Fig. 7 illustrates an exemplary User Interface (UI) of a subtitle box having automatically generated subtitles according to an embodiment. Although a set of UI elements is shown here, in other embodiments, the UI elements may be aesthetically different or may be significantly different with small changes. However, the functionality of the UIs in these other embodiments may be similar to that described herein.
The frame 710 displays the content items 135 for playback, and in particular, presents the portion of the content items 135 indicated by the search bar near the bottom of the frame 710. If the content item 135 includes video, the video is displayed in the frame 710. Otherwise, only audio can be presented.
The frame 720 displays a detected waveform of a sound signal of the audio stream over time. As shown in fig. 7, the frame 720 indicates a detected waveform that lasts for a segment time near the position of the indicated arrow.
The caption box input element 730 is a user interface element indicating that a caption box is displayed for a duration of an audio stream corresponding to a width of the caption box 730, and a start time stamp of the caption box corresponds to a left boundary of the caption box input element 730, and an end time stamp of the caption box corresponds to a right boundary of the caption box input element 730. The caption subsystem 150 (e.g., the subtitle module 160) may generate the caption box input element 730 using the timing window of the automatically generated caption as described above. For example, if the caption subsystem 150 determines that a voice sound occurs at 00:45 to 00:52, the caption subsystem 150 may generate a caption box input element 730 having a left boundary at 00:45 and a right boundary at 00:52. Note that when searching for content using a search bar and thus moving the displayed timestamps, the subtitle box input element 730 is also shifted in the user interface so that its boundaries always match the correct start and end timestamps.
The framework 740 displays the caption text and timing window of each caption text in a list interface instead of the timeline interface of the caption box input element 730. The user may interact with each subtitle box to edit its content or timing window in the frame 740 without looking for a corresponding point in the content item 135. This may be useful in the final inspection process. As previously described, the subtitle text may be automatically transcribed at the initial segmentation using a speech recognition engine. Further, as described above, the user may be allowed to first input the subtitle text as a text block, after which the subtitle subsystem 150 automatically determines where to split the input subtitle text to match the subtitle box with an automatically generated timing window. If the user is allowed to input text as large blocks, only a single subtitle text block is initially displayed, and a plurality of subtitle text blocks are displayed after the processing as described above.
IX. example flow for automatically generating a Caption timing Window
Fig. 8 is a data flow diagram illustrating the actions of the content system 105 for automatically generating non-voice subtitles from audio input according to an embodiment. In one embodiment, the operations in the flow diagram are attributed to the content system 105. Initially, the content system 105 accesses the audio stream at 805. This may be a separate audio stream (e.g., podcast) or an audio stream that is an audio segment of video.
The content system 105 inputs the audio stream segments into the speech classifier at 810 to generate the original score. The speech classifier generates, for each segment, an original score indicating the likelihood that the segment includes speech sound generation.
To this end, the content system 105 divides the audio stream into segments, each segment having a particular duration and being offset from the start of the previous segment by a particular duration. For example, the content system 105 divides the audio stream into 250 millisecond segments, each segment shifted 10 milliseconds from the previous segment.
The content system 105 inputs each segment into the filter cascade model to generate features from the segments. The content system 105 inputs features into the classifier model to determine the probability of speech sounds occurring in the segment. The classifier model may be a deep neural network.
The content system 105 generates a set of binary scores for the audio stream based on the original scores at 815. The binary score may be generated based on an aggregation of the original scores from a consecutive series of segments of the audio stream. For example, one binary score for a point in time in an audio stream may be generated based on an average of the original scores of consecutive segments that cover one second from the point in time. When the aggregate value exceeds the threshold, the content system 105 determines that the binary score for the corresponding point in time is "on". The threshold value may be adjusted based on an error value calculated using the validation data set.
The content system 105 generates 820 a timing window based on the binary score. Each timing window indicates a start time stamp and an end time stamp corresponding to the start and end of a voice sound in the audio stream. These timing windows may be used to generate subtitle boxes that correspond to the duration of the timing windows in order to allow a user to more easily provide (e.g., "crowd source") subtitles for content items in the content system 105.
Other considerations
Reference in the specification to "one embodiment" or "an embodiment" means that a particular feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment. The appearances of the phrase "in one embodiment" in various places in the specification are not necessarily all referring to the same embodiment.
It should be noted that the process steps and instructions are embodied in software, firmware or hardware and, when embodied in software, may be downloaded to reside on and be operated from different platforms used by a variety of operating systems.
The operations herein may also be performed by an apparatus. Furthermore, the computers referred to in the specification may include a single processor or may be architectures employing multiple processor designs for increased computing capability. It will be appreciated that a variety of programming languages may be used to implement the teachings of the invention as described herein, and any references below to specific languages are provided for disclosure of enablement and best mode of the invention.
While the invention has been particularly shown and described with reference to a preferred embodiment and several alternative embodiments, it will be understood by those skilled in the relevant art that various changes in form and detail may be made therein without departing from the spirit and scope of the invention. .
Finally, it should be noted that the language used in the specification has been principally selected for readability and instructional purposes, and may not have been selected to delineate or circumscribe the inventive subject matter. Accordingly, the disclosure of the present invention is intended to be illustrative, but not limiting, of the scope of the invention, which is set forth in the following claims.
Claims (20)
1. A method, comprising:
accessing audio data comprising a plurality of segments;
determining, by a processing device, that one or more of the plurality of segments comprises speech sounds;
recognizing a duration of the speech sound; and
a user interface element corresponding to a time duration of the voice sound is provided, wherein the user interface element indicates an estimate of a start and an end of the voice sound and is configured to receive subtitle text associated with the voice sound of the audio data.
2. The method of claim 1, further comprising:
Inputting the plurality of segments of the audio data into a speech classifier for classification, wherein the speech classifier generates a set of original scores representing a likelihood that the respective segment includes an occurrence of a speech sound;
generating binary scores for the audio data based on the set of original scores, wherein one of the binary scores is generated based on an aggregation of a continuous series of original scores from segments of the audio data; and
a timing window is generated for one or more of the speech sounds in the audio data based on the binary score, wherein the timing window indicates an estimate of a start time and an end time of the one or more speech sounds in the audio data.
3. The method of claim 2, wherein inputting the plurality of segments of the audio data into one or more speech classifiers for classification further comprises:
dividing the audio data into the plurality of segments, each segment having a particular duration and being offset from a start of the audio data;
filtering the audio signal of each of the plurality of segments using a filter model to produce an output having a plurality of frequency channels;
Identifying features based on the output of the filter model; and
the features are input into a machine learning model that is used to determine an original score that is indicative of a likelihood of the speech sound occurring in a respective segment of the audio data.
4. The method of claim 2, wherein generating a binary score for the audio data further comprises:
applying an aggregation function to the original scores of the continuous series of segments of the audio data;
generating a plurality of aggregated values based on an output of the aggregation function, each of the plurality of aggregated values being associated with one of the continuous series of segments; and is also provided with
The binary scores are generated based on the aggregate values, each binary score being generated based on whether the corresponding aggregate value exceeds a threshold.
5. The method of claim 4, wherein one or more of the plurality of segments is part of a verification data set, and the method further comprises:
calculating a total amount of error between the generated timing window and a ground truth timing window of the validation data set; and
the threshold is iteratively modified to minimize the amount of error.
6. The method of claim 1, wherein the user interface element comprises one of a plurality of subtitle boxes, and the method further comprises:
generating the plurality of caption frames for the audio data, each caption frame having a start timestamp and an end timestamp corresponding to the start and end timestamps of the generated timing window; and
the plurality of subtitle boxes are stored, wherein the subtitle boxes are configured to allow a user to input subtitle text in the subtitle boxes.
7. The method of claim 6, further comprising concatenating a set of timing windows temporally adjacent to each other into a single timing window, wherein each timing window in the set has a duration that is shorter than a predefined minimum.
8. The method of claim 6, further comprising dividing the timing window into a plurality of timing windows, the timing windows comprising a duration longer than a predefined maximum.
9. The method of claim 6, wherein the audio data comprises an audio stream and further comprising:
receiving a request from a client device to input subtitles for the audio stream;
transmitting the plurality of subtitle frames for presentation at the client device;
Receiving a plurality of caption text entries, each caption text entry associated with one of the plurality of caption boxes; and
the plurality of subtitle text entries are stored in association with respective subtitle boxes.
10. The method of claim 9, further comprising:
receiving a request from a client device to present subtitles associated with the audio stream; and
the plurality of subtitle boxes with associated subtitle text for the audio stream are sent to the client device for presentation on the client device.
11. The method of claim 2, wherein the start time corresponds to a transition of the binary score from a low value to a high value and the end time corresponds to a subsequent transition of the binary score from a high value to a low value.
12. A computer program product comprising a non-transitory computer-readable storage medium having instructions encoded thereon that, when executed by a processor, cause the processor to:
accessing audio data comprising a plurality of segments;
determining that one or more of the plurality of segments comprises a speech sound;
recognizing a duration of the speech sound; and
A user interface element corresponding to a time duration of the voice sound is provided, wherein the user interface element indicates an estimate of a start and an end of the voice sound and is configured to receive subtitle text associated with the voice sound of the audio data.
13. The computer program product of claim 12, wherein the non-transitory computer-readable storage medium further has instructions that cause the processor to:
inputting the plurality of segments of the audio data into a speech classifier for classification, wherein the speech classifier generates a set of original scores representing a likelihood that the respective segment includes an occurrence of a speech sound;
generating binary scores for the audio data based on the set of original scores, wherein one of the binary scores is generated based on an aggregation of a continuous series of original scores from segments of the audio data; and
a timing window is generated for one or more of the speech sounds in the audio data based on the binary score, wherein the timing window indicates an estimate of a start time and an end time of the one or more speech sounds in the audio data.
14. The computer program product of claim 13, wherein the non-transitory computer-readable storage medium further has instructions that cause the processor to:
dividing the audio data into the plurality of segments, each segment having a particular duration and being offset from a start of the audio data;
filtering the audio signal of each of the plurality of segments using a filter model to produce an output having a plurality of frequency channels;
identifying features based on the output of the filter model; and
the features are input into a machine learning model that is used to determine an original score that is indicative of a likelihood of the speech sound occurring in a respective segment of the audio data.
15. The computer program product of claim 13, wherein the non-transitory computer-readable storage medium further has instructions that cause the processor to:
applying an aggregation function to the original scores of the continuous series of segments of the audio data;
generating a plurality of aggregated values based on an output of the aggregation function, each of the plurality of aggregated values being associated with one of the continuous series of segments; and is also provided with
The binary scores are generated based on the aggregate values, each binary score being generated based on whether the corresponding aggregate value exceeds a threshold.
16. The computer program product of claim 13, wherein the user interface element comprises one of a plurality of subtitle boxes, and the non-transitory computer readable storage medium further has instructions that when executed by a processor cause the processor to:
generating the plurality of subtitle frames for the audio stream, each subtitle frame having start and end timestamps corresponding to start and end timestamps of the generated timing window; and
the plurality of subtitle boxes are stored, wherein the subtitle boxes are configured to allow a user to input subtitle text in the subtitle boxes.
17. A system, comprising:
a memory;
a processing device coupled to the memory, wherein the processing device is to:
accessing audio data comprising a plurality of segments;
determining that one or more of the plurality of segments comprises a speech sound;
recognizing a duration of the speech sound; and
a user interface element corresponding to a time duration of the voice sound is provided, wherein the user interface element indicates an estimate of a start and an end of the voice sound and is configured to receive subtitle text associated with the voice sound of the audio data.
18. The system of claim 17, wherein the processing device is further to:
inputting the plurality of segments of the audio data into a speech classifier for classification, wherein the speech classifier generates a set of original scores representing a likelihood that the respective segment includes an occurrence of a speech sound;
generating binary scores for the audio data based on the set of original scores, wherein one of the binary scores is generated based on an aggregation of a continuous series of original scores from segments of the audio data; and
a timing window is generated for one or more of the speech sounds in the audio data based on the binary score, wherein the timing window indicates an estimate of a start time and an end time of the one or more speech sounds in the audio data.
19. The system of claim 18, wherein the processing device is further to:
dividing the audio data into the plurality of segments, each segment having a particular duration and being offset from a start of the audio data;
filtering the audio signal of each of the plurality of segments using a filter model to produce an output having a plurality of frequency channels;
Identifying features based on the output of the filter model; and
the features are input into a machine learning model that is used to determine an original score that is indicative of a likelihood of the speech sound occurring in a respective segment of the audio data.
20. The system of claim 18, wherein the processing device is further configured to:
applying an aggregation function to the original scores of the continuous series of segments of the audio data;
generating a plurality of aggregated values based on an output of the aggregation function, each of the plurality of aggregated values being associated with one of the continuous series of segments; and is also provided with
The binary scores are generated based on the aggregate values, each binary score being generated based on whether the corresponding aggregate value exceeds a threshold.
Applications Claiming Priority (6)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201662330836P | 2016-05-02 | 2016-05-02 | |
US62/330,836 | 2016-05-02 | ||
US15/225,513 US10490209B2 (en) | 2016-05-02 | 2016-08-01 | Automatic determination of timing windows for speech captions in an audio stream |
US15/225,513 | 2016-08-01 | ||
PCT/US2016/069262 WO2017192181A1 (en) | 2016-05-02 | 2016-12-29 | Automatic determination of timing windows for speech captions in an audio stream |
CN201680081781.9A CN108604455B (en) | 2016-05-02 | 2016-12-29 | Automatic determination of timing window for speech captions in an audio stream |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201680081781.9A Division CN108604455B (en) | 2016-05-02 | 2016-12-29 | Automatic determination of timing window for speech captions in an audio stream |
Publications (1)
Publication Number | Publication Date |
---|---|
CN116072130A true CN116072130A (en) | 2023-05-05 |
Family
ID=60157569
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202310074852.4A Pending CN116072130A (en) | 2016-05-02 | 2016-12-29 | Automatic determination of timing window for voice subtitles in audio stream |
CN201680081781.9A Active CN108604455B (en) | 2016-05-02 | 2016-12-29 | Automatic determination of timing window for speech captions in an audio stream |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201680081781.9A Active CN108604455B (en) | 2016-05-02 | 2016-12-29 | Automatic determination of timing window for speech captions in an audio stream |
Country Status (4)
Country | Link |
---|---|
US (2) | US10490209B2 (en) |
EP (1) | EP3403261B1 (en) |
CN (2) | CN116072130A (en) |
WO (1) | WO2017192181A1 (en) |
Families Citing this family (16)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10014002B2 (en) * | 2016-02-16 | 2018-07-03 | Red Pill VR, Inc. | Real-time audio source separation using deep neural networks |
US9842609B2 (en) | 2016-02-16 | 2017-12-12 | Red Pill VR, Inc. | Real-time adaptive audio source separation |
US10490209B2 (en) * | 2016-05-02 | 2019-11-26 | Google Llc | Automatic determination of timing windows for speech captions in an audio stream |
US10582271B2 (en) * | 2017-07-18 | 2020-03-03 | VZP Digital | On-demand captioning and translation |
TWI651927B (en) * | 2018-02-14 | 2019-02-21 | National Central University | Signal source separation method and signal source separation device |
US10372991B1 (en) * | 2018-04-03 | 2019-08-06 | Google Llc | Systems and methods that leverage deep learning to selectively store audiovisual content |
JP7210938B2 (en) * | 2018-08-29 | 2023-01-24 | 富士通株式会社 | Text generation device, text generation program and text generation method |
CN113924620A (en) * | 2019-06-05 | 2022-01-11 | 哈曼国际工业有限公司 | Sound modification based on frequency composition |
US11735177B2 (en) * | 2019-12-12 | 2023-08-22 | Silicon Laboratories Inc. | Keyword spotting using machine learning |
CN111161711B (en) * | 2020-04-01 | 2020-07-03 | 支付宝(杭州)信息技术有限公司 | Method and device for sentence segmentation of flow type speech recognition text |
CN111986655B (en) * | 2020-08-18 | 2022-04-01 | 北京字节跳动网络技术有限公司 | Audio content identification method, device, equipment and computer readable medium |
CN113010698B (en) * | 2020-11-18 | 2023-03-10 | 北京字跳网络技术有限公司 | Multimedia interaction method, information interaction method, device, equipment and medium |
US11861800B2 (en) | 2020-12-30 | 2024-01-02 | Snap Inc. | Presenting available augmented reality content items in association with multi-video clip capture |
EP4272455A1 (en) * | 2020-12-30 | 2023-11-08 | Snap Inc. | Adding time-based captions to captured video |
US11924540B2 (en) | 2020-12-30 | 2024-03-05 | Snap Inc. | Trimming video in association with multi-video clip capture |
CN114783438B (en) * | 2022-06-17 | 2022-09-27 | 深圳市友杰智新科技有限公司 | Adaptive decoding method, apparatus, computer device and storage medium |
Family Cites Families (18)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
FR2842014B1 (en) | 2002-07-08 | 2006-05-05 | Lyon Ecole Centrale | METHOD AND APPARATUS FOR AFFECTING A SOUND CLASS TO A SOUND SIGNAL |
US7627567B2 (en) * | 2004-04-14 | 2009-12-01 | Microsoft Corporation | Segmentation of strings into structured records |
US20070011012A1 (en) | 2005-07-11 | 2007-01-11 | Steve Yurick | Method, system, and apparatus for facilitating captioning of multi-media content |
US8150065B2 (en) * | 2006-05-25 | 2012-04-03 | Audience, Inc. | System and method for processing an audio signal |
CN101548313B (en) * | 2006-11-16 | 2011-07-13 | 国际商业机器公司 | Voice activity detection system and method |
CN101540847A (en) * | 2008-03-21 | 2009-09-23 | 株式会社康巴思 | Caption producing system and caption producing method |
JP2012003326A (en) * | 2010-06-14 | 2012-01-05 | Sony Corp | Information processing device, information processing method, and program |
US8548803B2 (en) | 2011-08-08 | 2013-10-01 | The Intellisis Corporation | System and method of processing a sound signal including transforming the sound signal into a frequency-chirp domain |
GB2502944A (en) | 2012-03-30 | 2013-12-18 | Jpal Ltd | Segmentation and transcription of speech |
JP6150405B2 (en) | 2013-01-15 | 2017-06-21 | ヴィキ, インク.Viki, Inc. | System and method for captioning media |
US9324319B2 (en) | 2013-05-21 | 2016-04-26 | Speech Morphing Systems, Inc. | Method and apparatus for exemplary segment classification |
KR20150021258A (en) | 2013-08-20 | 2015-03-02 | 삼성전자주식회사 | Display apparatus and control method thereof |
US9652945B2 (en) * | 2013-09-06 | 2017-05-16 | Immersion Corporation | Method and system for providing haptic effects based on information complementary to multimedia content |
US9589564B2 (en) * | 2014-02-05 | 2017-03-07 | Google Inc. | Multiple speech locale-specific hotword classifiers for selection of a speech locale |
US10037313B2 (en) * | 2016-03-24 | 2018-07-31 | Google Llc | Automatic smoothed captioning of non-speech sounds from audio |
US10490209B2 (en) * | 2016-05-02 | 2019-11-26 | Google Llc | Automatic determination of timing windows for speech captions in an audio stream |
US10497382B2 (en) * | 2016-12-16 | 2019-12-03 | Google Llc | Associating faces with voices for speaker diarization within videos |
US10522186B2 (en) * | 2017-07-28 | 2019-12-31 | Adobe Inc. | Apparatus, systems, and methods for integrating digital media content |
-
2016
- 2016-08-01 US US15/225,513 patent/US10490209B2/en active Active
- 2016-12-29 CN CN202310074852.4A patent/CN116072130A/en active Pending
- 2016-12-29 EP EP16901159.0A patent/EP3403261B1/en active Active
- 2016-12-29 CN CN201680081781.9A patent/CN108604455B/en active Active
- 2016-12-29 WO PCT/US2016/069262 patent/WO2017192181A1/en active Application Filing
-
2019
- 2019-11-15 US US16/685,187 patent/US11011184B2/en active Active
Also Published As
Publication number | Publication date |
---|---|
EP3403261A1 (en) | 2018-11-21 |
EP3403261A4 (en) | 2019-08-28 |
WO2017192181A1 (en) | 2017-11-09 |
US11011184B2 (en) | 2021-05-18 |
CN108604455A (en) | 2018-09-28 |
US20200090678A1 (en) | 2020-03-19 |
EP3403261B1 (en) | 2021-02-03 |
US20170316792A1 (en) | 2017-11-02 |
CN108604455B (en) | 2023-02-10 |
US10490209B2 (en) | 2019-11-26 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN108604455B (en) | Automatic determination of timing window for speech captions in an audio stream | |
US11315546B2 (en) | Computerized system and method for formatted transcription of multimedia content | |
US10497382B2 (en) | Associating faces with voices for speaker diarization within videos | |
US10037313B2 (en) | Automatic smoothed captioning of non-speech sounds from audio | |
US9378423B2 (en) | Data recognition in content | |
US7983910B2 (en) | Communicating across voice and text channels with emotion preservation | |
US20200051582A1 (en) | Generating and/or Displaying Synchronized Captions | |
US9123330B1 (en) | Large-scale speaker identification | |
US11494434B2 (en) | Systems and methods for managing voice queries using pronunciation information | |
US20180047387A1 (en) | System and method for generating accurate speech transcription from natural speech audio signals | |
CN110781328A (en) | Video generation method, system, device and storage medium based on voice recognition | |
KR102246893B1 (en) | Interactive system, control method thereof, interactive server and control method thereof | |
Álvarez et al. | Automating live and batch subtitling of multimedia contents for several European languages | |
US20210034662A1 (en) | Systems and methods for managing voice queries using pronunciation information | |
CN114143479B (en) | Video abstract generation method, device, equipment and storage medium | |
CN109213974B (en) | Electronic document conversion method and device | |
US11176943B2 (en) | Voice recognition device, voice recognition method, and computer program product | |
US11410656B2 (en) | Systems and methods for managing voice queries using pronunciation information | |
JP6322125B2 (en) | Speech recognition apparatus, speech recognition method, and speech recognition program | |
Jitaru et al. | Lrro: a lip reading data set for the under-resourced romanian language | |
KR102314156B1 (en) | Method and system for providing service using media, and computer program for executing the method | |
US20230022515A1 (en) | Increasing user engagement through query suggestion | |
JP2022542415A (en) | Systems and methods for managing spoken queries using pronunciation information | |
JP2024015818A (en) | Speech recognition device, speech recognition method, and speech recognition program | |
CN117651943A (en) | Display apparatus |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |