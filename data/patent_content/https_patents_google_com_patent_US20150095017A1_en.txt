US20150095017A1 - System and method for learning word embeddings using neural language models - Google Patents
System and method for learning word embeddings using neural language models Download PDFInfo
- Publication number
- US20150095017A1 US20150095017A1 US14/075,166 US201314075166A US2015095017A1 US 20150095017 A1 US20150095017 A1 US 20150095017A1 US 201314075166 A US201314075166 A US 201314075166A US 2015095017 A1 US2015095017 A1 US 2015095017A1
- Authority
- US
- United States
- Prior art keywords
- word
- words
- data
- sample
- training
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Abandoned
Links
Images
Classifications
-
- G06F17/276—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/205—Parsing
- G06F40/216—Parsing using statistical methods
-
- G06F17/2735—
-
- G06F17/28—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/237—Lexical tools
- G06F40/242—Dictionaries
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/284—Lexical analysis, e.g. tokenisation or collocates
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/047—Probabilistic or stochastic networks
Definitions
- This invention relates to a natural language processing and information retrieval system, and more particularly to an improved system and method to enable efficient representation and retrieval of word embeddings based on a neural language model.
- Natural language processing and information retrieval systems based on neural language models are generally known, in which real-valued representations of words are learned by neural probabilistic language models (NPLMs) from large collections of unstructured text.
- NPLMs are trained to learn word embedding (similarity) information and associations between words in a phrase, typically to solve the classic task of predicting the next word in sequence given an input query phrase. Examples of such word representations and NPLMs are discussed in “A unified architecture for natural language processing: Deep neural networks with multitask learning”—Collobert and Weston (2008), “Parsing natural scenes and natural language with recursive neural networks”—Socher et al. (2011), “Word representations: A simple and general method for semi-supervised learning”—Turian et al. (2010).
- a system and computer-implemented method are provided of learning natural language word associations, embeddings, and/or similarities, using a neural network architecture, comprising storing data defining a word dictionary comprising words identified from training data consisting a plurality of sequences of associated words, selecting a predefined number of data samples from the training data, the selected data samples defining positive examples of word associations, generating a predefined number of negative samples for each selected data sample, the negative samples defining negative examples of word associations, wherein the number of negative samples generated for each data sample is a statistically small proportion of the number of words in the word dictionary, and training a neural probabilistic language model using the data samples and the generated negative samples.
- the negative samples for each selected data sample may be generated by replacing one or more words in the data sample with a respective one or more replacement words selected from the word dictionary.
- the one or more replacement words may be pseudo-randomly selected from the word dictionary based on frequency of occurrence of words in the training data.
- the number of negative samples generated for each data sample is between 1/10000 and 1/100000 of the number of words in the word dictionary.
- the neural probabilistic language model may output a word representation for an input word, representative of the association between the input word and other words in the word dictionary.
- a word association matrix may be generated, comprising a plurality of vectors, each vector defining a representation of a word in the word dictionary output by the trained neural language model.
- the word association matrix may be used to resolve a word association query. The query may be resolved without applying a word position-dependent weighting.
- training the neural language model does not apply a word position-dependent weighting.
- the training samples may each include a target word and a plurality of context words that are associated with the target word, and label data identifying the sample as a positive example of word association.
- the negative samples may each include a target word and a plurality of context words that are selected from the word dictionary, and label data identifying the sample as a negative example of word association.
- the neural language model may be configured to receive a representation of the target word and representations of the plurality of context words of an input sample, and to output a probability value indicative of the likelihood that the target word is associated with the context words.
- the neural language model may be configured to receive a representation of the target word and representations of at least one context word of an input sample, and to output a probability value indicative of the likelihood that at least one context word is associated with the target word.
- Training the neural language model may comprise adjusting parameters based on a calculated error value derived from the output probability value and the label associated with the sample.
- the word dictionary may be generated based on the training data, wherein the word dictionary includes calculated values of the frequency of occurrence of each word within the training data.
- the training data may be normalized.
- the training data comprises a plurality of sequences of associated words.
- the present invention provides a system and method of predicting a word association between words in a word dictionary, comprising processor implemented steps of storing data defining a word association matrix including a plurality of vectors, each vector defining a representation of a word derived from a trained neural probabilistic language model, receiving a plurality of query words, retrieving the associated representations of the query words from the word association matrix, calculating a candidate representation based on the retrieved representations, and determining at least one word in the word dictionary that matches the candidate representation, wherein the determination is made based on the word association matrix and without applying a word position-dependent weighting.
- the candidate representation may be calculated as the average representation of the retrieved representations.
- calculating the representation may comprise subtracting one or more retrieved representations from one or more other retrieved representations.
- One or more query words may be excluded from the word dictionary before calculating the candidate representation.
- Each word representation may be representative of the association or similarity between the input word and other words in the word dictionary.
- FIG. 1 is a block diagram showing the main components of a natural language processing system according to an embodiment of the invention.
- FIG. 2 is a block diagram showing the main components of a training engine of the natural language processing system in FIG. 1 , according to an embodiment of the invention.
- FIG. 3 is a block diagram showing the main components of a query engine of the natural language processing system in FIG. 1 , according to an embodiment of the invention.
- FIG. 4 is a flow diagram illustrating the main processing steps performed by the training engine of FIG. 2 according to an embodiment.
- FIG. 5 is a schematic illustration of an example neural language model being trained on an example input training sample.
- FIG. 6 is a flow diagram illustrating the main processing steps performed by the query engine of FIG. 3 according to an embodiment.
- FIG. 7 is a schematic illustration of an example analogy-based word similarity query being processed according to the present embodiment.
- FIG. 8 is a diagram of an example of a computer system on which one or more of the functions of the embodiment may be implemented.
- a natural language processing system 1 comprises a training engine 3 and a query engine 5 , each coupled to an input interface 7 for receiving user input via one or more input devices (not shown), such as a mouse, a keyboard, a touch screen, a microphone, etc.
- the training engine 3 and query engine 5 are also coupled to an output interface 9 for outputting data to one or more output devices (not shown), such as a display, a speaker, a printer, etc.
- the training engine 3 is configured to learn parameters defining a neural probabilistic language model 11 based on natural language training data 13 , such as a word corpus consisting of a very large sample of word sequences, typically natural language phrases and sentences.
- the trained neural language model 11 can be used to generate a word representation vector, representing the learned associations between an input word and all other words in the training data 13 .
- the trained neural language model 11 can also be used to determine a probability of association between an input target word and a plurality of context words.
- the context words may be the two words preceding the target word and the two words following the target word, in a sequence consisting five natural language words. Any number and arrangement of context words may be provided for a particular target word in a sequence.
- the training engine 3 may be configured to build a word dictionary 15 from the training data 13 , for example by parsing the training data 13 to generate and store a list of unique words with associated unique identifiers and calculated frequency of occurrence within the training data 13 .
- the training data 13 is pre-processed to normalize the sequences of natural language words that occur in the source word corpus, for example to remove punctuation, abbreviations, etc., while retaining the relative order of the normalized words in the training data 13 .
- the training engine 3 is also configured to generate and store a word representation matrix 17 comprising a plurality of vectors, each vector defining a representation of a word in the word dictionary 15 derived from the trained neural language model 11 .
- the training engine 3 is configured to apply a noise contrastive estimation technique to the process of training the neural language model 11 , whereby the model is trained using positive samples from the training data defining positive examples of word associations, as well as a predetermined number of generated negative samples (noise samples) defining negative examples of word associations.
- a predetermined number of negative samples are generated from each positive sample.
- each positive sample is modified to generate a plurality of negative samples, by replacing one or more words in the positive sample with a pseudo-randomly selected word from the word dictionary 15 .
- the replacement word may be pseudo-randomly selected, for example based on the stored associated frequencies of occurrences.
- the query engine 5 is configured to receive input of a plurality of query words, for example via the input interface 7 , and to resolve the query by determining one or more words that are determined to be associated with the query words.
- the query engine 5 identifies one or more associated words from the word dictionary 15 based on a calculated average of the representations of each query word retrieved from the word representation matrix 17 .
- the determination is made without applying a word position-dependent weighting to the scoring of the words or representations, as the inventors have realized that such additional computational overheads are not required to resolve queries for predicted words associations, as opposed to prediction of the next word in a sequence.
- word association query resolution by the query engine 5 of the present embodiment is computationally more efficient.
- the training engine 3 includes a dictionary generator module 21 for populating an indexed list of words in the word dictionary 15 based on identified words in the training data 13 .
- the unique index values may be of any form that can be presented in a binary representation, such as numerical, alphabetic, or alphanumeric symbols, etc.
- the dictionary generator module 21 is also configured to calculate and update the frequency of occurrence for each identified word, and to store the frequency data values in the word dictionary 15 .
- the dictionary generator module 21 may be configured to normalize the training data 13 as mentioned above.
- the training engine 3 also includes a neural language model training module 23 that receives positive data samples derived from the training data 13 by a positive sample generator module 25 , and negative data samples generated from each positive data sample by a negative sample generator module 27 .
- the negative sample generator module 27 receives each positive sample generated by the positive sample generator module 25 and generates a predetermined number of negative samples based on the received positive sample.
- the negative sample generator module 27 modifies each received positive sample to generate a plurality of negative samples by replacing a word in the positive sample with a pseudo-randomly selected word from the word dictionary 15 based on the stored associated frequencies of occurrences, such that words that appear more frequently in the training data 13 are selected more frequently for inclusion in the generated negative samples.
- the middle word in the sequence of words in the positive sample can be replaced by a pseudo-randomly selected word from the word dictionary 15 to derive a new negative sample.
- the base positive sample and the derived negative samples include the same predefined number of words and differ by one word.
- the training samples are associated with a positive label, indicative of a positive example of association between a target word and the surrounding context words in the sample.
- the negative samples are associated with a negative label, indicative of a negative example of word association because of the pseudo-random fabrication of the sample.
- the associations, embeddings and/or similarities between words are modeled by parameters (commonly referred to as weights) of the neural language model 11 .
- the neural language model training module 23 is configured to learn the parameters defining the neural language model based on the training samples and the negative samples, by recursively adjusting the parameters based on the calculated error or discrepancy between the predicted probability of word association of the input sample output by the model compared to the actual label of the sample.
- the training engine 3 includes a word representation matrix generator module 29 that determines and updates the word representation vector stored in the word representation matrix 17 for each word in the word dictionary 15 .
- the word representation vector values correspond to the respective values of the word representation that are output from a group of nodes in the hidden layer.
- the query engine 3 includes a query parser module 31 that receives an input query, for example from the input interface 7 .
- the input query includes two query words (womb, word 2 ), where the user is seeking a target word that is associated with both query words.
- a dictionary lookup module 33 communicatively coupled to the query parser module 31 , receives the query words and identifies the respective indices (w 2 , w 2 ) from a lookup of the index values stored in the word dictionary 15 .
- the identified indices for the query words are passed to a word representation lookup module 35 , coupled to the dictionary lookup module 33 , that retrieves the respective word representation vectors (v 1 , v 2 ) from the word representation matrix 17 .
- the retrieved word representation vectors are combined at a combining node 37 (or module), coupled to the word representation lookup module 35 , to derive an averaged word representation vector ( ⁇ circumflex over ( ⁇ ) ⁇ 3 ), that is representative of a candidate word associated with both query words.
- a word determiner module 39 coupled to the combining node 37 , receives the averaged word representation vector and determines one or more candidate matching words based on the word representation matrix 17 and the word dictionary 15 .
- the word determiner module 39 is configured to compute a ranked list of candidate matching word representations by performing a dot product of the average word representation vector and the word representation matrix. In this way, the processing does not involve application of any position-dependent weights to the word representations.
- the corresponding word for a matching vector can be retrieved from the word dictionary 15 based on the vector's index in the matrix 17 .
- the candidate word or words for the resolved query may be output by the word determiner module 39 , for example to the output interface 9 for output to the user.
- FIG. 4 schematically illustrating an exemplary neural language model being trained on an example input training sample.
- the process begins at step S 4 - 1 where the dictionary generator module 21 processes the natural language training data 13 to normalize the sequences of words in the training data 13 , for example to remove punctuation, abbreviations, formatting, XML headers, mapping all words to lowercase, replacing all numerical digits, etc.
- the dictionary generator module 21 identifies unique words of the normalized training data 13 , together with a count of the frequency of occurrence for each identified word in the list.
- an identified word may be classified as a unique word only if the word occurs at least a predefined number of times (e.g. five or ten times) in the training data.
- the identified words and respective frequency values are stored as an indexed list of unique words in the word dictionary 15 .
- the index is an integer value, from one to the number of unique words identified in the normalized training data 13 .
- two suitable freely-available datasets are the English Wikipedia data set with approximately 1.5 billion words, from which a word dictionary 15 of 800,000 unique normalized words can be determined, and the collection of Project Gutenberg texts with approximately 47 million words, from which a word dictionary 15 of 80,000 unique normalized words can be determined.
- the training sample generator module 25 generates a predetermined number of training samples by randomly selecting sequences of words from the normalized training data 13 .
- Each training sample is associated with a data label indicating that the training sample is a positive example of the associations between a target word and the surrounding context words in the training sample.
- Probabilistic neural language models specify the distribution for the target word w, given a sequence of words h, called the context.
- w is the next word in the sentence
- the context h is the sequence of words that precede w.
- the training process is interested in learning word representations as opposed to assigning probabilities to sentences, and therefore the models are not restricted to predicting the next word in sequence.
- the training process is configured in one embodiment to learn the parameters for a neural probabilistic language model by predicting the target word w from the words surrounding it.
- This model will be referred to as a vector log-bilinear language model (vLBL).
- the training process can be configured to predict the context word(s) from the target word, for an NPLM according to another embodiment.
- This alternative model will be referred to as an inverse vLBL (ivLBL).
- an example training sample 51 is the phrase “cat sat on the mat”, consisting of five words occurring in sequence in the normalized training data 13 .
- the target word w in this sample is “on” and the associated context consists the two words h 1 , h 2 preceding the target, and the two words h 3 , h 4 succeeding the target.
- the training samples may include any number of words.
- the context can consist of words preceding, following, or surrounding the word being predicted.
- the NPLM defines the distribution for the word to be predicted using the scoring function s ⁇ (w, h) that quantifies the compatibility between the context and the candidate target word.
- ⁇ are model parameters, which include the word embeddings.
- the scores are converted to probabilities by exponentiating and normalizing:
- the vLBL model has two sets of word representations: one for the target words (i.e. the words being predicted) and one for the context words.
- the target and the context representations for word w are denoted with q w and r w respectively.
- conventional models may compute the predicted representation for the target word by taking a linear combination of the context word feature vectors:
- c i is the weight vector for the context word in position i and ⁇ circle around (x) ⁇ denotes element-wise multiplication.
- the scoring function then computes the similarity between the predicted feature vector and one for word w:
- Equation 2 the conventional scoring function from Equations 2 and 3 is adapted to eliminate the position-dependent weights and computing the predicted feature vector ⁇ circumflex over (q) ⁇ (h) simply by averaging the context feature word vectors r w i :
- the ivLBL model is used to predict the context from the target word, based on an assumption that the words in different context positions are conditionally independent given the current word w:
- the context word distributions P i, ⁇ w (w i ) are simply vLBL models that condition on the current word w and are defined by the scoring function:
- the resulting model can be seen as a Na ⁇ ve Bayes classifier parameterized in terms of word embeddings.
- the scoring function in this alternative embodiment is thus adapted to compute the similarity between the predicted feature vector r w for a context word w, and the vector representation q for word w i , without position-dependent weights:
- b w i is the optional bias that captures the context-independent frequency of word w i .
- the present embodiments provide an efficient technique of training a neural probabilistic language model by learning to predict the context from the word, or learning to predict a target word from its context.
- These approaches are based on the principle that words with similar meanings often occur in the same contexts and thus the NPLM training process of the present embodiments efficiently look for word representations that capture their context distributions.
- the training process is further adapted to use noise-contrastive estimation (NCE) to train the neural probabilistic language model.
- NCE is based on the reduction of density estimation to probabilistic binary classification.
- a logistic regression classifier can be trained to discriminate between samples from the data distribution and samples from some “noise” distribution, based on the ratio of probabilities of the sample under the model and the noise distribution.
- the main advantage of NCE is that it allows the present technique to fit models that are not explicitly normalized making the training time effectively independent of the vocabulary size.
- the normalizing factor may be dropped from Equation 1 above, and exp(s ⁇ (w, h)) may simply be used in place of P ⁇ h (w) during training.
- the perplexity of NPLMs trained using this approach has been shown to be on par with those trained with maximum likelihood learning, but at a fraction of the computational cost.
- the negative sample generator module 27 receives each positive sample generated by the positive sample generator module 25 and generates a predetermined number of negative samples based on the received positive sample, by replacing a target word in the sequence of words in the positive sample with a pseudo-randomly selected word from the word dictionary 15 to derive a new negative sample.
- the number of negative samples that is generated for each positive sample is predetermined as a statistically small proportion of the total number of words in the word dictionary 15 .
- each positive sample is associated with a negative data label, indicative of a negative example of word association between the pseudo-randomly selected replacement target word and the surrounding context words in the negative sample.
- the positive and negative samples have fixed-length contexts.
- the NCE-based training technique can make use of any noise distribution that is easy to sample from and compute probabilities under, and that does not assign zero probability to any word.
- the (global) unigram distribution of the training data can be used as the noise distribution, a choice that is known to work well for training language models. Assuming that negative samples are k times more frequent than data samples, the probability that the given sample came from the data is
- this probability is obtained by using the trained model distribution in place of P d h :
- the scaling factor k in front of P n (w) accounts for the fact that negative samples are k times more frequent than data samples.
- Equation 7 the contribution of a word/context pair w; h to the gradient of Equation 7 can be estimated by generating k negative samples ⁇ x i ⁇ and computing:
- Equation 8 involves a sum over k negative samples instead of a sum over the entire vocabulary, making the NCE training time linear in the number of negative samples and independent of the vocabulary size. As the number of negative samples k is increased, this estimate approaches the likelihood gradient of the normalized model, allowing a trade off between computation cost and estimation accuracy.
- the neural language model training module 23 receives the generated training samples and the generated negative samples, and processes the samples in turn to train parameters defining the neural language model.
- a schematic illustration is provided for a vLBL NPLM according to an exemplary embodiment, being trained on one example training data sample.
- the neural language model in this example includes:
- Each connection between respective nodes in the model can be associated with a parameter (weight).
- the neural language model training module 23 recursively adjusts the parameters based on the calculated error or discrepancy between the predicted probability of word association of the input sample output by the model compared to the actual label of the sample. Such recursive training of model parameters of NPLMs is of a type that is known per se, and need not be described further.
- the word representation matrix generator module 29 determines the word representation vector for each word in the word dictionary 15 and stores the vectors as respective columns of data in a word representation matrix 17 , indexed according to the associated index value of the word in the word dictionary 15 .
- the word representation vector values correspond to the respective values of the word representation that are output from a group of nodes in the hidden layer.
- FIG. 6 schematically illustrating an example of an analogy-based word similarity query being processed according to the present embodiment.
- the process begins at step S 6 - 1 where the query parser module 31 receives an input query from the input interface 7 , identifying two or more query words, where the user is seeking a target word that is associated with all of the input query words.
- FIG. 7 illustrates an example query consisting of two input query words: “cat” (word 1 ) and “mat” (word 2 ).
- the dictionary lookup module 33 identifies the respective indices 351 for “cat” (w 1 ) and 1780 (w 2 ) for “mat”, from a lookup of the index values stored in the word dictionary 15 .
- the word representation lookup module 35 receives the identified indices (w 1 , w 2 ) for the query words and retrieves the respective word representation vectors r 351 for “cat” and r 1780 for “mat” (r w1 , r w2 ) from the word representation matrix 17 .
- the combining node 37 calculates the average word representation vector ⁇ circumflex over (q) ⁇ (h) of the retrieved word representation vectors (r w1 , r w2 ), representative of a candidate word associated with both query words.
- the present embodiment eliminates the use of position-dependent weights and computes the predicted feature vector simply by averaging the context word feature vectors, which ignores the order of context words.
- the word determiner module 39 receives the averaged word representation vector and determines one or more candidate matching words based on the word representation matrix 17 and the word dictionary 15 .
- the word determiner module 39 is configured to compute a ranked list of candidate matching word representations by performing a dot product of the average word representation vector ⁇ circumflex over (q) ⁇ (h) and the word representation matrix q w , without applying a word position-dependent weighting.
- the corresponding word or words for one or more best-matching vectors can be retrieved from the word dictionary 15 based on the vector's index in the matrix 17 .
- score vector index 5462 has the highest probability score of 0.25, corresponding to the word “sat” in the word dictionary 15 .
- the candidate word or words for the resolved query are output by the word determiner module 39 to the output interface 9 for output to the user.
- the above query resolution technique can be adapted and applied to other forms of analogy-based challenge sets, such as queries that consist of questions of the form “a is to b is as c is to —— ”, denoted as a:b ⁇ c:?.
- the task is to identify the held-out fourth word, with only exact word matches deemed correct.
- Word embeddings learned by neural language models have been shown to perform very well on these datasets when using the following vector-similarity-based protocol for answering the questions.
- ⁇ right arrow over (w) ⁇ is the representation vector for word w normalized to unit norm.
- the query a:b ⁇ c:? can be resolved by a modified embodiment, by finding the word d* with the representation closest to ⁇ right arrow over (b) ⁇ right arrow over (a) ⁇ + ⁇ right arrow over (c) ⁇ according to cosine similarity:
- Equation 11 can be rewritten as
- the entities described herein may be implemented by computer systems such as computer system 1000 as shown in FIG. 7 , shown by way of example.
- Embodiments of the present invention may be implemented as programmable code for execution by such computer systems 1000 . After reading this description, it will become apparent to a person skilled in the art how to implement the invention using other computer systems and/or computer architectures, including mobile systems and architectures, and the like.
- Computer system 1000 includes one or more processors, such as processor 1004 .
- Processor 1004 may be any type of processor, including but not limited to a special purpose or a general-purpose digital signal processor.
- Processor 1004 is connected to a communication infrastructure 1006 (for example, a bus or network).
- Computer system 1000 also includes a user input interface 1003 connected to one or more input device(s) 1005 and a display interface 1007 connected to one or more display(s) 1009 .
- Input devices 1005 may include, for example, a pointing device such as a mouse or touchpad, a keyboard, a touch screen such as a resistive or capacitive touch screen, etc.
- Computer system 1000 also includes a main memory 1008 , preferably random access memory (RAM), and may also include a secondary memory 610 .
- Secondary memory 1010 may include, for example, a hard disk drive 1012 and/or a removable storage drive 1014 , representing a floppy disk drive, a magnetic tape drive, an optical disk drive, etc.
- Removable storage drive 1014 reads from and/or writes to a removable storage unit 1018 in a well-known manner.
- Removable storage unit 1018 represents a floppy disk, magnetic tape, optical disk, etc., which is read by and written to by removable storage drive 1014 .
- removable storage unit 1018 includes a computer usable storage medium having stored therein computer software and/or data.
- secondary memory 1010 may include other similar means for allowing computer programs or other instructions to be loaded into computer system 1000 .
- Such means may include, for example, a removable storage unit 1022 and an interface 1020 .
- Examples of such means may include a program cartridge and cartridge interface (such as that previously found in video game devices), a removable memory chip (such as an EPROM, or PROM, or flash memory) and associated socket, and other removable storage units 1022 and interfaces 1020 which allow software and data to be transferred from removable storage unit 1022 to computer system 1000 .
- the program may be executed and/or the data accessed from the removable storage unit 1022 , using the processor 1004 of the computer system 1000 .
- Computer system 1000 may also include a communication interface 1024 .
- Communication interface 1024 allows software and data to be transferred between computer system 1000 and external devices. Examples of communication interface 1024 may include a modem, a network interface (such as an Ethernet card), a communication port, a Personal Computer Memory Card International Association (PCMCIA) slot and card, etc.
- Software and data transferred via communication interface 1024 are in the form of signals 1028 , which may be electronic, electromagnetic, optical, or other signals capable of being received by communication interface 1024 . These signals 1028 are provided to communication interface 1024 via a communication path 1026 .
- Communication path 1026 carries signals 1028 and may be implemented using wire or cable, fiber optics, a phone line, a wireless link, a cellular phone link, a radio frequency link, or any other suitable communication channel. For instance, communication path 1026 may be implemented using a combination of channels.
- computer program medium and “computer usable medium” are used generally to refer to media such as removable storage drive 1014 , a hard disk installed in hard disk drive 1012 , and signals 1028 . These computer program products are means for providing software to computer system 1000 . However, these terms may also include signals (such as electrical, optical or electromagnetic signals) that embody the computer program disclosed herein.
- Computer programs are stored in main memory 1008 and/or secondary memory 1010 . Computer programs may also be received via communication interface 1024 . Such computer programs, when executed, enable computer system 1000 to implement embodiments of the present invention as discussed herein. Accordingly, such computer programs represent controllers of computer system 1000 . Where the embodiment is implemented using software, the software may be stored in a computer program product 1030 and loaded into computer system 1000 using removable storage drive 1014 , hard disk drive 1012 , or communication interface 1024 , to provide some examples.
- the natural language processing system includes both a training engine and a query engine.
- the training engine and the query engine may instead be provided as separate systems, sharing access the respective data stores.
- the separate systems may be in networked communication with one another, and/or with the data stores.
- the mobile device stores a plurality of application modules (also referred to as computer programs or software) in memory, which when executed, enable the mobile device to implement embodiments of the present invention as discussed herein.
- application modules also referred to as computer programs or software
- the software may be stored in a computer program product and loaded into the mobile device using any known instrument, such as removable storage disk or drive, hard disk drive, or communication interface, to provide some examples.
Abstract
Description
- This application is based on, and claims priority to, U.S. Provisional Application No. 61/883,620, filed Sep. 27, 2013, the entire contents of which are fully incorporated herein by reference.
- This invention relates to a natural language processing and information retrieval system, and more particularly to an improved system and method to enable efficient representation and retrieval of word embeddings based on a neural language model.
- Natural language processing and information retrieval systems based on neural language models are generally known, in which real-valued representations of words are learned by neural probabilistic language models (NPLMs) from large collections of unstructured text. NPLMs are trained to learn word embedding (similarity) information and associations between words in a phrase, typically to solve the classic task of predicting the next word in sequence given an input query phrase. Examples of such word representations and NPLMs are discussed in “A unified architecture for natural language processing: Deep neural networks with multitask learning”—Collobert and Weston (2008), “Parsing natural scenes and natural language with recursive neural networks”—Socher et al. (2011), “Word representations: A simple and general method for semi-supervised learning”—Turian et al. (2010).
- When scaling up NLPMs to handle large vocabularies and solving the above classic task of predicting the next word in sequence, known techniques typically consider the relative word positions within the training phrases and the query phrases to provide accurate prediction query resolution. One approach is to learn conditional word embeddings using a hierarchical or tree-structured representation of the word space, as discussed for example in “Hierarchical probabilistic neural network language model”—Morin and Bengio (2005) and “A scalable hierarchical distributed language model”—Mnih and Hinton (2009). Another common approach is to compute normalized probabilities, applying word position-dependent weightings, as discussed for example in “A fast and simple algorithm for training neural probabilistic language models”—Mnih and The (2012), “Three new graphical models for statistical language modeling”—Mnih and Hinton (2009), and “Improving word representations via global context and multiple word prototypes”—Huang et al (2012). Consequently, training of known neural probabilistic language models is computationally demanding. Application of the trained NPLMs to predict a next word in sequence also requires significant processing resource.
- Natural language processing and information retrieval systems are also known from patent literature. WO2008/109665, U.S. Pat. No. 6,189,002 and U.S. Pat. No. 7,426,506 discuss examples of such systems for semantic extraction using neural network architecture.
- What is desired is a more robust neural probabilistic language model for representing word associations that can be trained and applied more efficiently, particularly to the problem of resolving analogy-based, unconditional, word similarity queries.
- Aspects of the present invention are set out in the accompanying claims.
- According to one aspect of the present invention, a system and computer-implemented method are provided of learning natural language word associations, embeddings, and/or similarities, using a neural network architecture, comprising storing data defining a word dictionary comprising words identified from training data consisting a plurality of sequences of associated words, selecting a predefined number of data samples from the training data, the selected data samples defining positive examples of word associations, generating a predefined number of negative samples for each selected data sample, the negative samples defining negative examples of word associations, wherein the number of negative samples generated for each data sample is a statistically small proportion of the number of words in the word dictionary, and training a neural probabilistic language model using the data samples and the generated negative samples.
- The negative samples for each selected data sample may be generated by replacing one or more words in the data sample with a respective one or more replacement words selected from the word dictionary. The one or more replacement words may be pseudo-randomly selected from the word dictionary based on frequency of occurrence of words in the training data.
- Preferably, the number of negative samples generated for each data sample is between 1/10000 and 1/100000 of the number of words in the word dictionary.
- The neural probabilistic language model may output a word representation for an input word, representative of the association between the input word and other words in the word dictionary. A word association matrix may be generated, comprising a plurality of vectors, each vector defining a representation of a word in the word dictionary output by the trained neural language model. The word association matrix may be used to resolve a word association query. The query may be resolved without applying a word position-dependent weighting.
- Preferably, training the neural language model does not apply a word position-dependent weighting. The training samples may each include a target word and a plurality of context words that are associated with the target word, and label data identifying the sample as a positive example of word association. The negative samples may each include a target word and a plurality of context words that are selected from the word dictionary, and label data identifying the sample as a negative example of word association.
- The neural language model may be configured to receive a representation of the target word and representations of the plurality of context words of an input sample, and to output a probability value indicative of the likelihood that the target word is associated with the context words. Alternatively, the neural language model may be configured to receive a representation of the target word and representations of at least one context word of an input sample, and to output a probability value indicative of the likelihood that at least one context word is associated with the target word. Training the neural language model may comprise adjusting parameters based on a calculated error value derived from the output probability value and the label associated with the sample.
- The word dictionary may be generated based on the training data, wherein the word dictionary includes calculated values of the frequency of occurrence of each word within the training data. The training data may be normalized. Preferably, the training data comprises a plurality of sequences of associated words.
- In another aspect, the present invention provides a system and method of predicting a word association between words in a word dictionary, comprising processor implemented steps of storing data defining a word association matrix including a plurality of vectors, each vector defining a representation of a word derived from a trained neural probabilistic language model, receiving a plurality of query words, retrieving the associated representations of the query words from the word association matrix, calculating a candidate representation based on the retrieved representations, and determining at least one word in the word dictionary that matches the candidate representation, wherein the determination is made based on the word association matrix and without applying a word position-dependent weighting.
- The candidate representation may be calculated as the average representation of the retrieved representations. Alternatively, calculating the representation may comprise subtracting one or more retrieved representations from one or more other retrieved representations.
- One or more query words may be excluded from the word dictionary before calculating the candidate representation. Each word representation may be representative of the association or similarity between the input word and other words in the word dictionary.
- In other aspects, there are provided computer programs arranged to carry out the above methods when executed by suitable programmable devices.
- There now follows, by way of example only, a detailed description of embodiments of the present invention, with references to the figures identified below.
-
FIG. 1 is a block diagram showing the main components of a natural language processing system according to an embodiment of the invention. -
FIG. 2 is a block diagram showing the main components of a training engine of the natural language processing system inFIG. 1 , according to an embodiment of the invention. -
FIG. 3 is a block diagram showing the main components of a query engine of the natural language processing system inFIG. 1 , according to an embodiment of the invention. -
FIG. 4 is a flow diagram illustrating the main processing steps performed by the training engine ofFIG. 2 according to an embodiment. -
FIG. 5 is a schematic illustration of an example neural language model being trained on an example input training sample. -
FIG. 6 is a flow diagram illustrating the main processing steps performed by the query engine ofFIG. 3 according to an embodiment. -
FIG. 7 is a schematic illustration of an example analogy-based word similarity query being processed according to the present embodiment. -
FIG. 8 is a diagram of an example of a computer system on which one or more of the functions of the embodiment may be implemented. - A specific embodiment of the invention will now be described for a process of training and utilizing a word embedding neural probabilistic language model. Referring to
FIG. 1 , a naturallanguage processing system 1 according to an embodiment comprises atraining engine 3 and aquery engine 5, each coupled to aninput interface 7 for receiving user input via one or more input devices (not shown), such as a mouse, a keyboard, a touch screen, a microphone, etc. Thetraining engine 3 andquery engine 5 are also coupled to anoutput interface 9 for outputting data to one or more output devices (not shown), such as a display, a speaker, a printer, etc. - The
training engine 3 is configured to learn parameters defining a neuralprobabilistic language model 11 based on naturallanguage training data 13, such as a word corpus consisting of a very large sample of word sequences, typically natural language phrases and sentences. The trainedneural language model 11 can be used to generate a word representation vector, representing the learned associations between an input word and all other words in thetraining data 13. The trainedneural language model 11 can also be used to determine a probability of association between an input target word and a plurality of context words. For example, the context words may be the two words preceding the target word and the two words following the target word, in a sequence consisting five natural language words. Any number and arrangement of context words may be provided for a particular target word in a sequence. - The
training engine 3 may be configured to build aword dictionary 15 from thetraining data 13, for example by parsing thetraining data 13 to generate and store a list of unique words with associated unique identifiers and calculated frequency of occurrence within thetraining data 13. Preferably, thetraining data 13 is pre-processed to normalize the sequences of natural language words that occur in the source word corpus, for example to remove punctuation, abbreviations, etc., while retaining the relative order of the normalized words in thetraining data 13. Thetraining engine 3 is also configured to generate and store aword representation matrix 17 comprising a plurality of vectors, each vector defining a representation of a word in theword dictionary 15 derived from the trainedneural language model 11. - As will be described in more detail below, the
training engine 3 is configured to apply a noise contrastive estimation technique to the process of training theneural language model 11, whereby the model is trained using positive samples from the training data defining positive examples of word associations, as well as a predetermined number of generated negative samples (noise samples) defining negative examples of word associations. A predetermined number of negative samples are generated from each positive sample. In one embodiment, each positive sample is modified to generate a plurality of negative samples, by replacing one or more words in the positive sample with a pseudo-randomly selected word from theword dictionary 15. The replacement word may be pseudo-randomly selected, for example based on the stored associated frequencies of occurrences. - The
query engine 5 is configured to receive input of a plurality of query words, for example via theinput interface 7, and to resolve the query by determining one or more words that are determined to be associated with the query words. Thequery engine 5 identifies one or more associated words from theword dictionary 15 based on a calculated average of the representations of each query word retrieved from theword representation matrix 17. In this embodiment, the determination is made without applying a word position-dependent weighting to the scoring of the words or representations, as the inventors have realized that such additional computational overheads are not required to resolve queries for predicted words associations, as opposed to prediction of the next word in a sequence. Advantageously, word association query resolution by thequery engine 5 of the present embodiment is computationally more efficient. - The
training engine 3 in the naturallanguage processing system 1 will now be described in more detail with reference toFIG. 2 . As shown, thetraining engine 3 includes adictionary generator module 21 for populating an indexed list of words in theword dictionary 15 based on identified words in thetraining data 13. The unique index values may be of any form that can be presented in a binary representation, such as numerical, alphabetic, or alphanumeric symbols, etc. Thedictionary generator module 21 is also configured to calculate and update the frequency of occurrence for each identified word, and to store the frequency data values in theword dictionary 15. Thedictionary generator module 21 may be configured to normalize thetraining data 13 as mentioned above. - The
training engine 3 also includes a neural languagemodel training module 23 that receives positive data samples derived from thetraining data 13 by a positivesample generator module 25, and negative data samples generated from each positive data sample by a negativesample generator module 27. The negativesample generator module 27 receives each positive sample generated by the positivesample generator module 25 and generates a predetermined number of negative samples based on the received positive sample. In this embodiment, the negativesample generator module 27 modifies each received positive sample to generate a plurality of negative samples by replacing a word in the positive sample with a pseudo-randomly selected word from theword dictionary 15 based on the stored associated frequencies of occurrences, such that words that appear more frequently in thetraining data 13 are selected more frequently for inclusion in the generated negative samples. For example, the middle word in the sequence of words in the positive sample can be replaced by a pseudo-randomly selected word from theword dictionary 15 to derive a new negative sample. In this way, the base positive sample and the derived negative samples include the same predefined number of words and differ by one word. - The training samples are associated with a positive label, indicative of a positive example of association between a target word and the surrounding context words in the sample. On the contrary, the negative samples are associated with a negative label, indicative of a negative example of word association because of the pseudo-random fabrication of the sample. As mentioned above, the associations, embeddings and/or similarities between words are modeled by parameters (commonly referred to as weights) of the
neural language model 11. The neural languagemodel training module 23 is configured to learn the parameters defining the neural language model based on the training samples and the negative samples, by recursively adjusting the parameters based on the calculated error or discrepancy between the predicted probability of word association of the input sample output by the model compared to the actual label of the sample. - The
training engine 3 includes a word representationmatrix generator module 29 that determines and updates the word representation vector stored in theword representation matrix 17 for each word in theword dictionary 15. The word representation vector values correspond to the respective values of the word representation that are output from a group of nodes in the hidden layer. - The
query engine 5 in the naturallanguage processing system 1 will now be described in more detail with reference toFIG. 3 . As shown, thequery engine 3 includes aquery parser module 31 that receives an input query, for example from theinput interface 7. In the example illustrated inFIG. 3 , the input query includes two query words (womb, word2), where the user is seeking a target word that is associated with both query words. - A
dictionary lookup module 33, communicatively coupled to thequery parser module 31, receives the query words and identifies the respective indices (w2, w2) from a lookup of the index values stored in theword dictionary 15. The identified indices for the query words are passed to a wordrepresentation lookup module 35, coupled to thedictionary lookup module 33, that retrieves the respective word representation vectors (v1, v2) from theword representation matrix 17. The retrieved word representation vectors are combined at a combining node 37 (or module), coupled to the wordrepresentation lookup module 35, to derive an averaged word representation vector ({circumflex over (ν)}3), that is representative of a candidate word associated with both query words. - A
word determiner module 39, coupled to the combiningnode 37, receives the averaged word representation vector and determines one or more candidate matching words based on theword representation matrix 17 and theword dictionary 15. In this embodiment, theword determiner module 39 is configured to compute a ranked list of candidate matching word representations by performing a dot product of the average word representation vector and the word representation matrix. In this way, the processing does not involve application of any position-dependent weights to the word representations. The corresponding word for a matching vector can be retrieved from theword dictionary 15 based on the vector's index in thematrix 17. The candidate word or words for the resolved query may be output by theword determiner module 39, for example to theoutput interface 9 for output to the user. - A brief description has been given above of the components forming part of the natural
language processing system 1 of the present embodiments. A more detailed description of the operation of these components will now be given with reference to the flow diagrams ofFIG. 4 , for an exemplary embodiment of the computer-implemented training process using thetraining engine 3. Reference is also made toFIG. 5 , schematically illustrating an exemplary neural language model being trained on an example input training sample. - As shown in
FIG. 4 , the process begins at step S4-1 where thedictionary generator module 21 processes the naturallanguage training data 13 to normalize the sequences of words in thetraining data 13, for example to remove punctuation, abbreviations, formatting, XML headers, mapping all words to lowercase, replacing all numerical digits, etc. At step S4-3, thedictionary generator module 21 identifies unique words of the normalizedtraining data 13, together with a count of the frequency of occurrence for each identified word in the list. Preferably, an identified word may be classified as a unique word only if the word occurs at least a predefined number of times (e.g. five or ten times) in the training data. - At step S4-5, the identified words and respective frequency values are stored as an indexed list of unique words in the
word dictionary 15. In this embodiment, the index is an integer value, from one to the number of unique words identified in the normalizedtraining data 13. For example, two suitable freely-available datasets are the English Wikipedia data set with approximately 1.5 billion words, from which aword dictionary 15 of 800,000 unique normalized words can be determined, and the collection of Project Gutenberg texts with approximately 47 million words, from which aword dictionary 15 of 80,000 unique normalized words can be determined. - At step S4-7, the training
sample generator module 25 generates a predetermined number of training samples by randomly selecting sequences of words from the normalizedtraining data 13. Each training sample is associated with a data label indicating that the training sample is a positive example of the associations between a target word and the surrounding context words in the training sample. - Probabilistic neural language models specify the distribution for the target word w, given a sequence of words h, called the context. Typically, in statistical language modeling, w is the next word in the sentence, while the context h is the sequence of words that precede w. In the present embodiment, the training process is interested in learning word representations as opposed to assigning probabilities to sentences, and therefore the models are not restricted to predicting the next word in sequence. Instead, the training process is configured in one embodiment to learn the parameters for a neural probabilistic language model by predicting the target word w from the words surrounding it. This model will be referred to as a vector log-bilinear language model (vLBL). Alternatively, the training process can be configured to predict the context word(s) from the target word, for an NPLM according to another embodiment. This alternative model will be referred to as an inverse vLBL (ivLBL).
- Referring to
FIG. 5 , anexample training sample 51 is the phrase “cat sat on the mat”, consisting of five words occurring in sequence in the normalizedtraining data 13. The target word w in this sample is “on” and the associated context consists the two words h1, h2 preceding the target, and the two words h3, h4 succeeding the target. It will be appreciated that the training samples may include any number of words. The context can consist of words preceding, following, or surrounding the word being predicted. Given the context h, the NPLM defines the distribution for the word to be predicted using the scoring function sθ(w, h) that quantifies the compatibility between the context and the candidate target word. Here θ are model parameters, which include the word embeddings. Generally, the scores are converted to probabilities by exponentiating and normalizing: -
- In one embodiment, the vLBL model has two sets of word representations: one for the target words (i.e. the words being predicted) and one for the context words. The target and the context representations for word w are denoted with qw and rw respectively. Given a sequence of context words h=w1; . . . ; wn, conventional models may compute the predicted representation for the target word by taking a linear combination of the context word feature vectors:
-
- where ci is the weight vector for the context word in position i and {circle around (x)} denotes element-wise multiplication.
- The scoring function then computes the similarity between the predicted feature vector and one for word w:
-
s θ(w,h)={circumflex over (q)}(h)T q wi +b wi (3) - where bw
i is an optional bias that captures the context-independent frequency of word w. In this embodiment, the conventional scoring function fromEquations 2 and 3 is adapted to eliminate the position-dependent weights and computing the predicted feature vector {circumflex over (q)}(h) simply by averaging the context feature word vectors rwi : -
- The result is something like a local topic model, which ignores the order of context words, potentially forcing it to capture more semantic information, possibly at the expense of syntax.
- In the alternative embodiment, the ivLBL model is used to predict the context from the target word, based on an assumption that the words in different context positions are conditionally independent given the current word w:
-
- The context word distributions Pi,θ w(wi) are simply vLBL models that condition on the current word w and are defined by the scoring function:
- The resulting model can be seen as a Naïve Bayes classifier parameterized in terms of word embeddings.
- The scoring function in this alternative embodiment is thus adapted to compute the similarity between the predicted feature vector rw for a context word w, and the vector representation q for word wi, without position-dependent weights:
-
s i,θ(w i ,w)=r w T q wi +b wi (7) - where bw
i is the optional bias that captures the context-independent frequency of word wi. - In this way, the present embodiments provide an efficient technique of training a neural probabilistic language model by learning to predict the context from the word, or learning to predict a target word from its context. These approaches are based on the principle that words with similar meanings often occur in the same contexts and thus the NPLM training process of the present embodiments efficiently look for word representations that capture their context distributions.
- In the present embodiments, the training process is further adapted to use noise-contrastive estimation (NCE) to train the neural probabilistic language model. NCE is based on the reduction of density estimation to probabilistic binary classification. Thus a logistic regression classifier can be trained to discriminate between samples from the data distribution and samples from some “noise” distribution, based on the ratio of probabilities of the sample under the model and the noise distribution. The main advantage of NCE is that it allows the present technique to fit models that are not explicitly normalized making the training time effectively independent of the vocabulary size. Thus, the normalizing factor may be dropped from
Equation 1 above, and exp(sθ(w, h)) may simply be used in place of Pθ h(w) during training. The perplexity of NPLMs trained using this approach has been shown to be on par with those trained with maximum likelihood learning, but at a fraction of the computational cost. - Accordingly, at step S4-9, the negative
sample generator module 27 receives each positive sample generated by the positivesample generator module 25 and generates a predetermined number of negative samples based on the received positive sample, by replacing a target word in the sequence of words in the positive sample with a pseudo-randomly selected word from theword dictionary 15 to derive a new negative sample. Advantageously, the number of negative samples that is generated for each positive sample is predetermined as a statistically small proportion of the total number of words in theword dictionary 15. For example, accurate results are achieved using a small, fixed number of noise samples generated from each positive sample, such as 5 or 10 negative samples per positive sample, which may be in the order of 1/10,000 to 1/100,000 of the number of unique normalized words in the word dictionary 15 (e.g. 80,000 or 800,000 as mentioned above). Each negative sample is associated with a negative data label, indicative of a negative example of word association between the pseudo-randomly selected replacement target word and the surrounding context words in the negative sample. Preferably, the positive and negative samples have fixed-length contexts. - The NCE-based training technique can make use of any noise distribution that is easy to sample from and compute probabilities under, and that does not assign zero probability to any word. For example, the (global) unigram distribution of the training data can be used as the noise distribution, a choice that is known to work well for training language models. Assuming that negative samples are k times more frequent than data samples, the probability that the given sample came from the data is
-
- In the present embodiment, this probability is obtained by using the trained model distribution in place of Pd h:
-
- where σ(x) is the logistic function and Δsθ(w,h)=sθ(w,h)−log(kPn(w)) is the difference in the scores of word w under the model and the (scaled) noise distribution. The scaling factor k in front of Pn(w) accounts for the fact that negative samples are k times more frequent than data samples.
- Note that in the above equation, sθ(w,h) is used in place of log Pθ h(w), ignoring the normalization term, because the technique uses an unnormalized model. This is possible because the NCE objective encourages the model to be approximately normalized and recovers a perfectly normalized model if the model class contains the data distribution. The model can be fitted by maximizing the log-posterior probability of the correct labels D averaged over the data and negative samples:
-
- In practice, the expectation over the noise distribution is approximated by sampling. Thus, the contribution of a word/context pair w; h to the gradient of
Equation 7 can be estimated by generating k negative samples {xi} and computing: -
- Note that the gradient in Equation 8 involves a sum over k negative samples instead of a sum over the entire vocabulary, making the NCE training time linear in the number of negative samples and independent of the vocabulary size. As the number of negative samples k is increased, this estimate approaches the likelihood gradient of the normalized model, allowing a trade off between computation cost and estimation accuracy.
- Returning to
FIG. 4 , at step S4-11, the neural languagemodel training module 23 receives the generated training samples and the generated negative samples, and processes the samples in turn to train parameters defining the neural language model. In the example illustrated inFIG. 5 , a schematic illustration is provided for a vLBL NPLM according to an exemplary embodiment, being trained on one example training data sample. The neural language model in this example includes: -
- an
input layer 53, comprising a plurality of groups 55 of input layer nodes, each group 55 of nodes receiving respective values of the representation of an input word (target word, w0 . . . wj, and context words, hn 0 . . . hn j of the sample, where j is the number of elements in the word vector representation); - a
hidden layer 57, also comprising a plurality of groups 55 of hidden layer nodes, each group 55 of nodes in the hidden layer being coupled to the nodes of the respective group of nodes in theinput layer 53, and outputting values of a word representation for the respective input word of the sample (target word representation, qw 0 . . . qw m, and context word representations, rwn 0 . . . rwn m, where m is a predefined number of nodes for the hidden layer); and - an
output node 59 coupled to the plurality of nodes of the hiddenlayer 57, and outputting a calculated probability value indicative of the likelihood that the input target word is associated with the input context words of the sample, for example based on the scoring function of Equation 4 above.
- an
- Each connection between respective nodes in the model can be associated with a parameter (weight). The neural language
model training module 23 recursively adjusts the parameters based on the calculated error or discrepancy between the predicted probability of word association of the input sample output by the model compared to the actual label of the sample. Such recursive training of model parameters of NPLMs is of a type that is known per se, and need not be described further. - At step S4-13, the word representation
matrix generator module 29 determines the word representation vector for each word in theword dictionary 15 and stores the vectors as respective columns of data in aword representation matrix 17, indexed according to the associated index value of the word in theword dictionary 15. The word representation vector values correspond to the respective values of the word representation that are output from a group of nodes in the hidden layer. - A brief description has been given above of the components forming part of the natural
language processing system 1 of the present embodiments. A more detailed description of the operation of these components will now be given with reference to the flow diagrams ofFIG. 6 , for an exemplary embodiment of the computer-implemented query resolution process using thequery engine 5. Reference is also made toFIG. 7 , schematically illustrating an example of an analogy-based word similarity query being processed according to the present embodiment. - As shown in
FIG. 6 , the process begins at step S6-1 where thequery parser module 31 receives an input query from theinput interface 7, identifying two or more query words, where the user is seeking a target word that is associated with all of the input query words. For example,FIG. 7 illustrates an example query consisting of two input query words: “cat” (word1) and “mat” (word2). At step S6-3, thedictionary lookup module 33 identifies therespective indices 351 for “cat” (w1) and 1780 (w2) for “mat”, from a lookup of the index values stored in theword dictionary 15. At step S6-5, the wordrepresentation lookup module 35 receives the identified indices (w1, w2) for the query words and retrieves the respective word representation vectors r351 for “cat” and r1780 for “mat” (rw1, rw2) from theword representation matrix 17. - At step S6-7, the combining
node 37 calculates the average word representation vector {circumflex over (q)}(h) of the retrieved word representation vectors (rw1, rw2), representative of a candidate word associated with both query words. As discussed above, the present embodiment eliminates the use of position-dependent weights and computes the predicted feature vector simply by averaging the context word feature vectors, which ignores the order of context words. - At step S6-9, the
word determiner module 39 receives the averaged word representation vector and determines one or more candidate matching words based on theword representation matrix 17 and theword dictionary 15. In this embodiment, theword determiner module 39 is configured to compute a ranked list of candidate matching word representations by performing a dot product of the average word representation vector {circumflex over (q)}(h) and the word representation matrix qw, without applying a word position-dependent weighting. - From the resulting vector of probability scores, the corresponding word or words for one or more best-matching vectors, e.g. the highest score, can be retrieved from the
word dictionary 15 based on the vector's index in thematrix 17. In the example illustrated inFIG. 7 , scorevector index 5462 has the highest probability score of 0.25, corresponding to the word “sat” in theword dictionary 15. At step S6-11, the candidate word or words for the resolved query are output by theword determiner module 39 to theoutput interface 9 for output to the user. - Those skilled in the art will appreciate that the above query resolution technique can be adapted and applied to other forms of analogy-based challenge sets, such as queries that consist of questions of the form “a is to b is as c is to ——”, denoted as a:b→c:?. In such an example, the task is to identify the held-out fourth word, with only exact word matches deemed correct. Word embeddings learned by neural language models have been shown to perform very well on these datasets when using the following vector-similarity-based protocol for answering the questions. Suppose {right arrow over (w)} is the representation vector for word w normalized to unit norm. Then, the query a:b→c:? can be resolved by a modified embodiment, by finding the word d* with the representation closest to {right arrow over (b)}−{right arrow over (a)}+{right arrow over (c)} according to cosine similarity:
-
- The inventors have realized that the present technique can be further adapted to exclude b and c from the vocabulary when looking for d* using
Equation 11, in order to achieve more accurate results. To see why this is necessary,Equation 11 can be rewritten as -
- where it can be seen that setting x to b or c maximizes the first or third term respectively (since the vectors are normalized), resulting in a high similarity score. This equation suggests the following interpretation of d*: it is simply the word with the representation most similar to {right arrow over (b)} and {right arrow over (c)} and dissimilar to {right arrow over (a)}, which makes it quite natural to exclude b and c themselves from consideration.
- The entities described herein, such as the natural
language processing system 1 or theindividual training engine 3 andquery engine 5, may be implemented by computer systems such ascomputer system 1000 as shown inFIG. 7 , shown by way of example. Embodiments of the present invention may be implemented as programmable code for execution bysuch computer systems 1000. After reading this description, it will become apparent to a person skilled in the art how to implement the invention using other computer systems and/or computer architectures, including mobile systems and architectures, and the like. -
Computer system 1000 includes one or more processors, such asprocessor 1004.Processor 1004 may be any type of processor, including but not limited to a special purpose or a general-purpose digital signal processor.Processor 1004 is connected to a communication infrastructure 1006 (for example, a bus or network). -
Computer system 1000 also includes a user input interface 1003 connected to one or more input device(s) 1005 and adisplay interface 1007 connected to one or more display(s) 1009.Input devices 1005 may include, for example, a pointing device such as a mouse or touchpad, a keyboard, a touch screen such as a resistive or capacitive touch screen, etc. -
Computer system 1000 also includes amain memory 1008, preferably random access memory (RAM), and may also include a secondary memory 610.Secondary memory 1010 may include, for example, ahard disk drive 1012 and/or aremovable storage drive 1014, representing a floppy disk drive, a magnetic tape drive, an optical disk drive, etc.Removable storage drive 1014 reads from and/or writes to aremovable storage unit 1018 in a well-known manner.Removable storage unit 1018 represents a floppy disk, magnetic tape, optical disk, etc., which is read by and written to byremovable storage drive 1014. As will be appreciated,removable storage unit 1018 includes a computer usable storage medium having stored therein computer software and/or data. - In alternative implementations,
secondary memory 1010 may include other similar means for allowing computer programs or other instructions to be loaded intocomputer system 1000. Such means may include, for example, aremovable storage unit 1022 and aninterface 1020. Examples of such means may include a program cartridge and cartridge interface (such as that previously found in video game devices), a removable memory chip (such as an EPROM, or PROM, or flash memory) and associated socket, and otherremovable storage units 1022 andinterfaces 1020 which allow software and data to be transferred fromremovable storage unit 1022 tocomputer system 1000. Alternatively, the program may be executed and/or the data accessed from theremovable storage unit 1022, using theprocessor 1004 of thecomputer system 1000. -
Computer system 1000 may also include acommunication interface 1024.Communication interface 1024 allows software and data to be transferred betweencomputer system 1000 and external devices. Examples ofcommunication interface 1024 may include a modem, a network interface (such as an Ethernet card), a communication port, a Personal Computer Memory Card International Association (PCMCIA) slot and card, etc. Software and data transferred viacommunication interface 1024 are in the form ofsignals 1028, which may be electronic, electromagnetic, optical, or other signals capable of being received bycommunication interface 1024. Thesesignals 1028 are provided tocommunication interface 1024 via a communication path 1026. Communication path 1026 carriessignals 1028 and may be implemented using wire or cable, fiber optics, a phone line, a wireless link, a cellular phone link, a radio frequency link, or any other suitable communication channel. For instance, communication path 1026 may be implemented using a combination of channels. - The terms “computer program medium” and “computer usable medium” are used generally to refer to media such as
removable storage drive 1014, a hard disk installed inhard disk drive 1012, and signals 1028. These computer program products are means for providing software tocomputer system 1000. However, these terms may also include signals (such as electrical, optical or electromagnetic signals) that embody the computer program disclosed herein. - Computer programs (also called computer control logic) are stored in
main memory 1008 and/orsecondary memory 1010. Computer programs may also be received viacommunication interface 1024. Such computer programs, when executed, enablecomputer system 1000 to implement embodiments of the present invention as discussed herein. Accordingly, such computer programs represent controllers ofcomputer system 1000. Where the embodiment is implemented using software, the software may be stored in acomputer program product 1030 and loaded intocomputer system 1000 usingremovable storage drive 1014,hard disk drive 1012, orcommunication interface 1024, to provide some examples. - Alternative embodiments may be implemented as control logic in hardware, firmware, or software or any combination thereof.
- It will be understood that embodiments of the present invention are described herein by way of example only, and that various changes and modifications may be made without departing from the scope of the invention.
- For example, in the embodiments described above, the natural language processing system includes both a training engine and a query engine. As the skilled person will appreciate, the training engine and the query engine may instead be provided as separate systems, sharing access the respective data stores. The separate systems may be in networked communication with one another, and/or with the data stores.
- In the embodiment described above, the mobile device stores a plurality of application modules (also referred to as computer programs or software) in memory, which when executed, enable the mobile device to implement embodiments of the present invention as discussed herein. As those skilled in the art will appreciate, the software may be stored in a computer program product and loaded into the mobile device using any known instrument, such as removable storage disk or drive, hard disk drive, or communication interface, to provide some examples.
- As a further alternative, those skilled in the art will appreciate that the hierarchical processing of words or representations themselves, as is known in the art, can be included in the query resolution process in order to further increase computational efficiency.
- Alternative embodiments may be envisaged, which nevertheless fall within the scope of the following claims.
Claims (41)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/075,166 US20150095017A1 (en) | 2013-09-27 | 2013-11-08 | System and method for learning word embeddings using neural language models |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201361883620P | 2013-09-27 | 2013-09-27 | |
US14/075,166 US20150095017A1 (en) | 2013-09-27 | 2013-11-08 | System and method for learning word embeddings using neural language models |
Publications (1)
Publication Number | Publication Date |
---|---|
US20150095017A1 true US20150095017A1 (en) | 2015-04-02 |
Family
ID=52740979
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/075,166 Abandoned US20150095017A1 (en) | 2013-09-27 | 2013-11-08 | System and method for learning word embeddings using neural language models |
Country Status (1)
Country | Link |
---|---|
US (1) | US20150095017A1 (en) |
Cited By (90)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN106022392A (en) * | 2016-06-02 | 2016-10-12 | 华南理工大学 | Deep neural network sample automatic accepting and rejecting training method |
US20160321244A1 (en) * | 2013-12-20 | 2016-11-03 | National Institute Of Information And Communications Technology | Phrase pair collecting apparatus and computer program therefor |
US20160357855A1 (en) * | 2015-06-02 | 2016-12-08 | International Business Machines Corporation | Utilizing Word Embeddings for Term Matching in Question Answering Systems |
CN106407333A (en) * | 2016-09-05 | 2017-02-15 | 北京百度网讯科技有限公司 | Artificial intelligence-based spoken language query identification method and apparatus |
US20170046625A1 (en) * | 2015-08-14 | 2017-02-16 | Fuji Xerox Co., Ltd. | Information processing apparatus and method and non-transitory computer readable medium |
WO2017057921A1 (en) * | 2015-10-02 | 2017-04-06 | 네이버 주식회사 | Method and system for automatically classifying data expressed by a plurality of factors with values of text word and symbol sequence by using deep learning |
WO2017143919A1 (en) * | 2016-02-26 | 2017-08-31 | 阿里巴巴集团控股有限公司 | Method and apparatus for establishing data identification model |
US20170286494A1 (en) * | 2016-03-29 | 2017-10-05 | Microsoft Technology Licensing, Llc | Computational-model operation using multiple subject representations |
KR20180008247A (en) * | 2016-07-14 | 2018-01-24 | 김경호 | Platform for providing task based on deep learning |
CN107785016A (en) * | 2016-08-31 | 2018-03-09 | 株式会社东芝 | Train the method and apparatus and audio recognition method and device of neural network aiding model |
CN108021544A (en) * | 2016-10-31 | 2018-05-11 | 富士通株式会社 | The method, apparatus and electronic equipment classified to the semantic relation of entity word |
US20180150753A1 (en) * | 2016-11-30 | 2018-05-31 | International Business Machines Corporation | Analyzing text documents |
US20180157989A1 (en) * | 2016-12-02 | 2018-06-07 | Facebook, Inc. | Systems and methods for online distributed embedding services |
JP2018156332A (en) * | 2017-03-16 | 2018-10-04 | ヤフー株式会社 | Generation device, generation method, and generation program |
US10095684B2 (en) * | 2016-11-22 | 2018-10-09 | Microsoft Technology Licensing, Llc | Trained data input system |
US20180293494A1 (en) * | 2017-04-10 | 2018-10-11 | International Business Machines Corporation | Local abbreviation expansion through context correlation |
US20180315430A1 (en) * | 2015-09-04 | 2018-11-01 | Google Llc | Neural Networks For Speaker Verification |
WO2018220566A1 (en) * | 2017-06-01 | 2018-12-06 | International Business Machines Corporation | Neural network classification |
CN109190126A (en) * | 2018-09-17 | 2019-01-11 | 北京神州泰岳软件股份有限公司 | The training method and device of word incorporation model |
CN109271636A (en) * | 2018-09-17 | 2019-01-25 | 北京神州泰岳软件股份有限公司 | The training method and device of word incorporation model |
CN109308353A (en) * | 2018-09-17 | 2019-02-05 | 北京神州泰岳软件股份有限公司 | The training method and device of word incorporation model |
KR20190018899A (en) * | 2017-08-16 | 2019-02-26 | 주식회사 인사이터 | Apparatus and method for analyzing sample words |
CN109543442A (en) * | 2018-10-12 | 2019-03-29 | 平安科技（深圳）有限公司 | Data safety processing method, device, computer equipment and storage medium |
US20190130221A1 (en) * | 2017-11-02 | 2019-05-02 | Royal Bank Of Canada | Method and device for generative adversarial network training |
CN109756494A (en) * | 2018-12-29 | 2019-05-14 | 中国银联股份有限公司 | A kind of negative sample transform method and device |
US20190188263A1 (en) * | 2016-06-15 | 2019-06-20 | University Of Ulsan Foundation For Industry Cooperation | Word semantic embedding apparatus and method using lexical semantic network and homograph disambiguating apparatus and method using lexical semantic network and word embedding |
US10354182B2 (en) | 2015-10-29 | 2019-07-16 | Microsoft Technology Licensing, Llc | Identifying relevant content items using a deep-structured neural network |
CN110134946A (en) * | 2019-04-15 | 2019-08-16 | 深圳智能思创科技有限公司 | A kind of machine reading understanding method for complex data |
CN110162770A (en) * | 2018-10-22 | 2019-08-23 | 腾讯科技（深圳）有限公司 | A kind of word extended method, device, equipment and medium |
CN110162766A (en) * | 2018-02-12 | 2019-08-23 | 深圳市腾讯计算机系统有限公司 | Term vector update method and device |
US10410624B2 (en) | 2016-03-17 | 2019-09-10 | Kabushiki Kaisha Toshiba | Training apparatus, training method, and computer program product |
CN110232393A (en) * | 2018-03-05 | 2019-09-13 | 腾讯科技（深圳）有限公司 | Processing method, device, storage medium and the electronic device of data |
CN110287494A (en) * | 2019-07-01 | 2019-09-27 | 济南浪潮高新科技投资发展有限公司 | A method of the short text Similarity matching based on deep learning BERT algorithm |
US10431210B1 (en) | 2018-04-16 | 2019-10-01 | International Business Machines Corporation | Implementing a whole sentence recurrent neural network language model for natural language processing |
US10430717B2 (en) | 2013-12-20 | 2019-10-01 | National Institute Of Information And Communications Technology | Complex predicate template collecting apparatus and computer program therefor |
US10437867B2 (en) | 2013-12-20 | 2019-10-08 | National Institute Of Information And Communications Technology | Scenario generating apparatus and computer program therefor |
US10460726B2 (en) | 2016-06-28 | 2019-10-29 | Samsung Electronics Co., Ltd. | Language processing method and apparatus |
CN110442759A (en) * | 2019-07-25 | 2019-11-12 | 深圳供电局有限公司 | Knowledge retrieval method and system, computer equipment and readable storage medium |
CN110516251A (en) * | 2019-08-29 | 2019-11-29 | 秒针信息技术有限公司 | A kind of construction method, construction device, equipment and the medium of electric business entity recognition model |
CN110708619A (en) * | 2019-09-29 | 2020-01-17 | 北京声智科技有限公司 | Word vector training method and device for intelligent equipment |
US10599977B2 (en) | 2016-08-23 | 2020-03-24 | International Business Machines Corporation | Cascaded neural networks using test ouput from the first neural network to train the second neural network |
CN111079410A (en) * | 2019-12-23 | 2020-04-28 | 五八有限公司 | Text recognition method and device, electronic equipment and storage medium |
CN111177367A (en) * | 2019-11-11 | 2020-05-19 | 腾讯科技（深圳）有限公司 | Case classification method, classification model training method and related products |
CN111191689A (en) * | 2019-12-16 | 2020-05-22 | 恩亿科(北京)数据科技有限公司 | Sample data processing method and device |
CN111414750A (en) * | 2020-03-18 | 2020-07-14 | 北京百度网讯科技有限公司 | Synonymy distinguishing method, device, equipment and storage medium for entries |
US10713783B2 (en) | 2017-06-01 | 2020-07-14 | International Business Machines Corporation | Neural network classification |
CN111488334A (en) * | 2019-01-29 | 2020-08-04 | 阿里巴巴集团控股有限公司 | Data processing method and electronic equipment |
US10740374B2 (en) * | 2016-06-30 | 2020-08-11 | International Business Machines Corporation | Log-aided automatic query expansion based on model mapping |
US10747427B2 (en) * | 2017-02-01 | 2020-08-18 | Google Llc | Keyboard automatic language identification and reconfiguration |
US20200279080A1 (en) * | 2018-02-05 | 2020-09-03 | Alibaba Group Holding Limited | Methods, apparatuses, and devices for generating word vectors |
US10789529B2 (en) * | 2016-11-29 | 2020-09-29 | Microsoft Technology Licensing, Llc | Neural network data entry system |
CN111931509A (en) * | 2020-08-28 | 2020-11-13 | 北京百度网讯科技有限公司 | Entity chain finger method, device, electronic equipment and storage medium |
CN111985235A (en) * | 2019-05-23 | 2020-11-24 | 北京地平线机器人技术研发有限公司 | Text processing method and device, computer readable storage medium and electronic equipment |
CN112101030A (en) * | 2020-08-24 | 2020-12-18 | 沈阳东软智能医疗科技研究院有限公司 | Method, device and equipment for establishing term mapping model and realizing standard word mapping |
CN112232065A (en) * | 2020-10-29 | 2021-01-15 | 腾讯科技（深圳）有限公司 | Method and device for mining synonyms |
WO2021053470A1 (en) * | 2019-09-20 | 2021-03-25 | International Business Machines Corporation | Selective deep parsing of natural language content |
CN112633007A (en) * | 2020-12-21 | 2021-04-09 | 科大讯飞股份有限公司 | Semantic understanding model construction method and device and semantic understanding method and device |
US10992763B2 (en) | 2018-08-21 | 2021-04-27 | Bank Of America Corporation | Dynamic interaction optimization and cross channel profile determination through online machine learning |
US11030402B2 (en) | 2019-05-03 | 2021-06-08 | International Business Machines Corporation | Dictionary expansion using neural language models |
US11032223B2 (en) | 2017-05-17 | 2021-06-08 | Rakuten Marketing Llc | Filtering electronic messages |
US20210174024A1 (en) * | 2018-12-07 | 2021-06-10 | Tencent Technology (Shenzhen) Company Limited | Method for training keyword extraction model, keyword extraction method, and computer device |
CN112966507A (en) * | 2021-03-29 | 2021-06-15 | 北京金山云网络技术有限公司 | Method, device, equipment and storage medium for constructing recognition model and identifying attack |
US20210200948A1 (en) * | 2019-12-27 | 2021-07-01 | Ubtech Robotics Corp Ltd | Corpus cleaning method and corpus entry system |
US11062198B2 (en) * | 2016-10-31 | 2021-07-13 | Microsoft Technology Licensing, Llc | Feature vector based recommender system |
US11075862B2 (en) | 2019-01-22 | 2021-07-27 | International Business Machines Corporation | Evaluating retraining recommendations for an automated conversational service |
US20210304056A1 (en) * | 2020-03-25 | 2021-09-30 | International Business Machines Corporation | Learning Parameter Sampling Configuration for Automated Machine Learning |
US11158118B2 (en) * | 2018-03-05 | 2021-10-26 | Vivacity Inc. | Language model, method and apparatus for interpreting zoning legal text |
WO2021217936A1 (en) * | 2020-04-29 | 2021-11-04 | 深圳壹账通智能科技有限公司 | Word combination processing-based new word discovery method and apparatus, and computer device |
US11182415B2 (en) | 2018-07-11 | 2021-11-23 | International Business Machines Corporation | Vectorization of documents |
US20210374361A1 (en) * | 2020-06-02 | 2021-12-02 | Oracle International Corporation | Removing undesirable signals from language models using negative data |
US11194968B2 (en) * | 2018-05-31 | 2021-12-07 | Siemens Aktiengesellschaft | Automatized text analysis |
US11205110B2 (en) * | 2016-10-24 | 2021-12-21 | Microsoft Technology Licensing, Llc | Device/server deployment of neural network data entry system |
US11222176B2 (en) | 2019-05-24 | 2022-01-11 | International Business Machines Corporation | Method and system for language and domain acceleration with embedding evaluation |
CN114297338A (en) * | 2021-12-02 | 2022-04-08 | 腾讯科技（深圳）有限公司 | Text matching method, apparatus, storage medium and program product |
US11341138B2 (en) * | 2017-12-06 | 2022-05-24 | International Business Machines Corporation | Method and system for query performance prediction |
US11341417B2 (en) | 2016-11-23 | 2022-05-24 | Fujitsu Limited | Method and apparatus for completing a knowledge graph |
CN114676227A (en) * | 2022-04-06 | 2022-06-28 | 北京百度网讯科技有限公司 | Sample generation method, model training method and search method |
WO2022134360A1 (en) * | 2020-12-25 | 2022-06-30 | 平安科技（深圳）有限公司 | Word embedding-based model training method, apparatus, electronic device, and storage medium |
US11386276B2 (en) | 2019-05-24 | 2022-07-12 | International Business Machines Corporation | Method and system for language and domain acceleration with embedding alignment |
CN114764444A (en) * | 2022-04-06 | 2022-07-19 | 云从科技集团股份有限公司 | Image generation and sample image expansion method, device and computer storage medium |
CN115114910A (en) * | 2022-04-01 | 2022-09-27 | 腾讯科技（深圳）有限公司 | Text processing method, device, equipment, storage medium and product |
US11481552B2 (en) * | 2020-06-01 | 2022-10-25 | Salesforce.Com, Inc. | Generative-discriminative language modeling for controllable text generation |
CN115344728A (en) * | 2022-10-17 | 2022-11-15 | 北京百度网讯科技有限公司 | Image retrieval model training method, image retrieval model using method, image retrieval model training device, image retrieval model using device, image retrieval model equipment and image retrieval model medium |
US11741392B2 (en) | 2017-11-20 | 2023-08-29 | Advanced New Technologies Co., Ltd. | Data sample label processing method and apparatus |
US11748248B1 (en) * | 2022-11-02 | 2023-09-05 | Wevo, Inc. | Scalable systems and methods for discovering and documenting user expectations |
US11797822B2 (en) | 2015-07-07 | 2023-10-24 | Microsoft Technology Licensing, Llc | Neural network having input and hidden layers of equal units |
CN116975301A (en) * | 2023-09-22 | 2023-10-31 | 腾讯科技（深圳）有限公司 | Text clustering method, text clustering device, electronic equipment and computer readable storage medium |
US11803883B2 (en) | 2018-01-29 | 2023-10-31 | Nielsen Consumer Llc | Quality assurance for labeled training data |
US11836591B1 (en) | 2022-10-11 | 2023-12-05 | Wevo, Inc. | Scalable systems and methods for curating user experience test results |
US11972344B2 (en) * | 2018-11-28 | 2024-04-30 | International Business Machines Corporation | Simple models using confidence profiles |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6178398B1 (en) * | 1997-11-18 | 2001-01-23 | Motorola, Inc. | Method, device and system for noise-tolerant language understanding |
US20010037324A1 (en) * | 1997-06-24 | 2001-11-01 | International Business Machines Corporation | Multilevel taxonomy based on features derived from training documents classification using fisher values as discrimination values |
US20060103674A1 (en) * | 2004-11-16 | 2006-05-18 | Microsoft Corporation | Methods for automated and semiautomated composition of visual sequences, flows, and flyovers based on content and context |
US20070174041A1 (en) * | 2003-05-01 | 2007-07-26 | Ryan Yeske | Method and system for concept generation and management |
US20120102033A1 (en) * | 2010-04-21 | 2012-04-26 | Haileo Inc. | Systems and methods for building a universal multimedia learner |
-
2013
- 2013-11-08 US US14/075,166 patent/US20150095017A1/en not_active Abandoned
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20010037324A1 (en) * | 1997-06-24 | 2001-11-01 | International Business Machines Corporation | Multilevel taxonomy based on features derived from training documents classification using fisher values as discrimination values |
US6178398B1 (en) * | 1997-11-18 | 2001-01-23 | Motorola, Inc. | Method, device and system for noise-tolerant language understanding |
US20070174041A1 (en) * | 2003-05-01 | 2007-07-26 | Ryan Yeske | Method and system for concept generation and management |
US20060103674A1 (en) * | 2004-11-16 | 2006-05-18 | Microsoft Corporation | Methods for automated and semiautomated composition of visual sequences, flows, and flyovers based on content and context |
US20120102033A1 (en) * | 2010-04-21 | 2012-04-26 | Haileo Inc. | Systems and methods for building a universal multimedia learner |
Non-Patent Citations (2)
Title |
---|
A Discriminative Language Model with Pseudo-Negative Samples by Daisuke Okanohara and Junichi Tsujii as appearing in the proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 73?80, Prague, Czech Republic, June 2007 * |
A Discriminative Language Model with Pseudo-Negative Samples by Daisuke Okanohara and Junichi Tsujii as appearing in the proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 73â80, Prague, Czech Republic, June 2007 * |
Cited By (120)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20160321244A1 (en) * | 2013-12-20 | 2016-11-03 | National Institute Of Information And Communications Technology | Phrase pair collecting apparatus and computer program therefor |
US10430717B2 (en) | 2013-12-20 | 2019-10-01 | National Institute Of Information And Communications Technology | Complex predicate template collecting apparatus and computer program therefor |
US10095685B2 (en) * | 2013-12-20 | 2018-10-09 | National Institute Of Information And Communications Technology | Phrase pair collecting apparatus and computer program therefor |
US10437867B2 (en) | 2013-12-20 | 2019-10-08 | National Institute Of Information And Communications Technology | Scenario generating apparatus and computer program therefor |
US10467268B2 (en) * | 2015-06-02 | 2019-11-05 | International Business Machines Corporation | Utilizing word embeddings for term matching in question answering systems |
US20160357855A1 (en) * | 2015-06-02 | 2016-12-08 | International Business Machines Corporation | Utilizing Word Embeddings for Term Matching in Question Answering Systems |
US20160358094A1 (en) * | 2015-06-02 | 2016-12-08 | International Business Machines Corporation | Utilizing Word Embeddings for Term Matching in Question Answering Systems |
US10467270B2 (en) * | 2015-06-02 | 2019-11-05 | International Business Machines Corporation | Utilizing word embeddings for term matching in question answering systems |
US11288295B2 (en) * | 2015-06-02 | 2022-03-29 | Green Market Square Limited | Utilizing word embeddings for term matching in question answering systems |
US11797822B2 (en) | 2015-07-07 | 2023-10-24 | Microsoft Technology Licensing, Llc | Neural network having input and hidden layers of equal units |
US20170046625A1 (en) * | 2015-08-14 | 2017-02-16 | Fuji Xerox Co., Ltd. | Information processing apparatus and method and non-transitory computer readable medium |
US10860948B2 (en) * | 2015-08-14 | 2020-12-08 | Fuji Xerox Co., Ltd. | Extending question training data using word replacement |
US20180315430A1 (en) * | 2015-09-04 | 2018-11-01 | Google Llc | Neural Networks For Speaker Verification |
US10586542B2 (en) * | 2015-09-04 | 2020-03-10 | Google Llc | Neural networks for speaker verification |
US11961525B2 (en) | 2015-09-04 | 2024-04-16 | Google Llc | Neural networks for speaker verification |
US11107478B2 (en) | 2015-09-04 | 2021-08-31 | Google Llc | Neural networks for speaker verification |
US10643109B2 (en) | 2015-10-02 | 2020-05-05 | Naver Corporation | Method and system for automatically classifying data expressed by a plurality of factors with values of text word and symbol sequence by using deep learning |
WO2017057921A1 (en) * | 2015-10-02 | 2017-04-06 | 네이버 주식회사 | Method and system for automatically classifying data expressed by a plurality of factors with values of text word and symbol sequence by using deep learning |
US10354182B2 (en) | 2015-10-29 | 2019-07-16 | Microsoft Technology Licensing, Llc | Identifying relevant content items using a deep-structured neural network |
WO2017143919A1 (en) * | 2016-02-26 | 2017-08-31 | 阿里巴巴集团控股有限公司 | Method and apparatus for establishing data identification model |
US11551036B2 (en) | 2016-02-26 | 2023-01-10 | Alibaba Group Holding Limited | Methods and apparatuses for building data identification models |
US10410624B2 (en) | 2016-03-17 | 2019-09-10 | Kabushiki Kaisha Toshiba | Training apparatus, training method, and computer program product |
US20170286494A1 (en) * | 2016-03-29 | 2017-10-05 | Microsoft Technology Licensing, Llc | Computational-model operation using multiple subject representations |
US10592519B2 (en) * | 2016-03-29 | 2020-03-17 | Microsoft Technology Licensing, Llc | Computational-model operation using multiple subject representations |
CN106022392A (en) * | 2016-06-02 | 2016-10-12 | 华南理工大学 | Deep neural network sample automatic accepting and rejecting training method |
US10984318B2 (en) * | 2016-06-15 | 2021-04-20 | University Of Ulsan Foundation For Industry Cooperation | Word semantic embedding apparatus and method using lexical semantic network and homograph disambiguating apparatus and method using lexical semantic network and word embedding |
US20190188263A1 (en) * | 2016-06-15 | 2019-06-20 | University Of Ulsan Foundation For Industry Cooperation | Word semantic embedding apparatus and method using lexical semantic network and homograph disambiguating apparatus and method using lexical semantic network and word embedding |
US10460726B2 (en) | 2016-06-28 | 2019-10-29 | Samsung Electronics Co., Ltd. | Language processing method and apparatus |
US10740374B2 (en) * | 2016-06-30 | 2020-08-11 | International Business Machines Corporation | Log-aided automatic query expansion based on model mapping |
KR20180008247A (en) * | 2016-07-14 | 2018-01-24 | 김경호 | Platform for providing task based on deep learning |
US10599977B2 (en) | 2016-08-23 | 2020-03-24 | International Business Machines Corporation | Cascaded neural networks using test ouput from the first neural network to train the second neural network |
CN107785016A (en) * | 2016-08-31 | 2018-03-09 | 株式会社东芝 | Train the method and apparatus and audio recognition method and device of neural network aiding model |
CN106407333A (en) * | 2016-09-05 | 2017-02-15 | 北京百度网讯科技有限公司 | Artificial intelligence-based spoken language query identification method and apparatus |
US11205110B2 (en) * | 2016-10-24 | 2021-12-21 | Microsoft Technology Licensing, Llc | Device/server deployment of neural network data entry system |
US11062198B2 (en) * | 2016-10-31 | 2021-07-13 | Microsoft Technology Licensing, Llc | Feature vector based recommender system |
CN108021544A (en) * | 2016-10-31 | 2018-05-11 | 富士通株式会社 | The method, apparatus and electronic equipment classified to the semantic relation of entity word |
US10095684B2 (en) * | 2016-11-22 | 2018-10-09 | Microsoft Technology Licensing, Llc | Trained data input system |
US11341417B2 (en) | 2016-11-23 | 2022-05-24 | Fujitsu Limited | Method and apparatus for completing a knowledge graph |
US10789529B2 (en) * | 2016-11-29 | 2020-09-29 | Microsoft Technology Licensing, Llc | Neural network data entry system |
US20180150753A1 (en) * | 2016-11-30 | 2018-05-31 | International Business Machines Corporation | Analyzing text documents |
US10839298B2 (en) * | 2016-11-30 | 2020-11-17 | International Business Machines Corporation | Analyzing text documents |
US20180157989A1 (en) * | 2016-12-02 | 2018-06-07 | Facebook, Inc. | Systems and methods for online distributed embedding services |
US10832165B2 (en) * | 2016-12-02 | 2020-11-10 | Facebook, Inc. | Systems and methods for online distributed embedding services |
US11327652B2 (en) | 2017-02-01 | 2022-05-10 | Google Llc | Keyboard automatic language identification and reconfiguration |
US10747427B2 (en) * | 2017-02-01 | 2020-08-18 | Google Llc | Keyboard automatic language identification and reconfiguration |
JP2018156332A (en) * | 2017-03-16 | 2018-10-04 | ヤフー株式会社 | Generation device, generation method, and generation program |
US20180293494A1 (en) * | 2017-04-10 | 2018-10-11 | International Business Machines Corporation | Local abbreviation expansion through context correlation |
US10839285B2 (en) * | 2017-04-10 | 2020-11-17 | International Business Machines Corporation | Local abbreviation expansion through context correlation |
US11032223B2 (en) | 2017-05-17 | 2021-06-08 | Rakuten Marketing Llc | Filtering electronic messages |
US11138724B2 (en) | 2017-06-01 | 2021-10-05 | International Business Machines Corporation | Neural network classification |
WO2018220566A1 (en) * | 2017-06-01 | 2018-12-06 | International Business Machines Corporation | Neural network classification |
US11935233B2 (en) | 2017-06-01 | 2024-03-19 | International Business Machines Corporation | Neural network classification |
GB2577017A (en) * | 2017-06-01 | 2020-03-11 | Ibm | Neural network classification |
US10713783B2 (en) | 2017-06-01 | 2020-07-14 | International Business Machines Corporation | Neural network classification |
KR101990586B1 (en) | 2017-08-16 | 2019-06-18 | 주식회사 인사이터 | Apparatus and method for analyzing sample words |
KR20190018899A (en) * | 2017-08-16 | 2019-02-26 | 주식회사 인사이터 | Apparatus and method for analyzing sample words |
US11062179B2 (en) * | 2017-11-02 | 2021-07-13 | Royal Bank Of Canada | Method and device for generative adversarial network training |
US20190130221A1 (en) * | 2017-11-02 | 2019-05-02 | Royal Bank Of Canada | Method and device for generative adversarial network training |
US11741392B2 (en) | 2017-11-20 | 2023-08-29 | Advanced New Technologies Co., Ltd. | Data sample label processing method and apparatus |
US11341138B2 (en) * | 2017-12-06 | 2022-05-24 | International Business Machines Corporation | Method and system for query performance prediction |
US11803883B2 (en) | 2018-01-29 | 2023-10-31 | Nielsen Consumer Llc | Quality assurance for labeled training data |
US20200279080A1 (en) * | 2018-02-05 | 2020-09-03 | Alibaba Group Holding Limited | Methods, apparatuses, and devices for generating word vectors |
US10824819B2 (en) * | 2018-02-05 | 2020-11-03 | Alibaba Group Holding Limited | Generating word vectors by recurrent neural networks based on n-ary characters |
CN110162766A (en) * | 2018-02-12 | 2019-08-23 | 深圳市腾讯计算机系统有限公司 | Term vector update method and device |
CN110232393A (en) * | 2018-03-05 | 2019-09-13 | 腾讯科技（深圳）有限公司 | Processing method, device, storage medium and the electronic device of data |
US11158118B2 (en) * | 2018-03-05 | 2021-10-26 | Vivacity Inc. | Language model, method and apparatus for interpreting zoning legal text |
US10692488B2 (en) | 2018-04-16 | 2020-06-23 | International Business Machines Corporation | Implementing a whole sentence recurrent neural network language model for natural language processing |
US10431210B1 (en) | 2018-04-16 | 2019-10-01 | International Business Machines Corporation | Implementing a whole sentence recurrent neural network language model for natural language processing |
US11194968B2 (en) * | 2018-05-31 | 2021-12-07 | Siemens Aktiengesellschaft | Automatized text analysis |
US11182415B2 (en) | 2018-07-11 | 2021-11-23 | International Business Machines Corporation | Vectorization of documents |
US10992763B2 (en) | 2018-08-21 | 2021-04-27 | Bank Of America Corporation | Dynamic interaction optimization and cross channel profile determination through online machine learning |
CN109271636A (en) * | 2018-09-17 | 2019-01-25 | 北京神州泰岳软件股份有限公司 | The training method and device of word incorporation model |
CN109190126A (en) * | 2018-09-17 | 2019-01-11 | 北京神州泰岳软件股份有限公司 | The training method and device of word incorporation model |
CN109308353A (en) * | 2018-09-17 | 2019-02-05 | 北京神州泰岳软件股份有限公司 | The training method and device of word incorporation model |
CN109543442A (en) * | 2018-10-12 | 2019-03-29 | 平安科技（深圳）有限公司 | Data safety processing method, device, computer equipment and storage medium |
CN110162770A (en) * | 2018-10-22 | 2019-08-23 | 腾讯科技（深圳）有限公司 | A kind of word extended method, device, equipment and medium |
US11972344B2 (en) * | 2018-11-28 | 2024-04-30 | International Business Machines Corporation | Simple models using confidence profiles |
US20210174024A1 (en) * | 2018-12-07 | 2021-06-10 | Tencent Technology (Shenzhen) Company Limited | Method for training keyword extraction model, keyword extraction method, and computer device |
US11947911B2 (en) * | 2018-12-07 | 2024-04-02 | Tencent Technology (Shenzhen) Company Limited | Method for training keyword extraction model, keyword extraction method, and computer device |
CN109756494A (en) * | 2018-12-29 | 2019-05-14 | 中国银联股份有限公司 | A kind of negative sample transform method and device |
US11075862B2 (en) | 2019-01-22 | 2021-07-27 | International Business Machines Corporation | Evaluating retraining recommendations for an automated conversational service |
CN111488334A (en) * | 2019-01-29 | 2020-08-04 | 阿里巴巴集团控股有限公司 | Data processing method and electronic equipment |
CN110134946A (en) * | 2019-04-15 | 2019-08-16 | 深圳智能思创科技有限公司 | A kind of machine reading understanding method for complex data |
US11030402B2 (en) | 2019-05-03 | 2021-06-08 | International Business Machines Corporation | Dictionary expansion using neural language models |
CN111985235A (en) * | 2019-05-23 | 2020-11-24 | 北京地平线机器人技术研发有限公司 | Text processing method and device, computer readable storage medium and electronic equipment |
US11386276B2 (en) | 2019-05-24 | 2022-07-12 | International Business Machines Corporation | Method and system for language and domain acceleration with embedding alignment |
US11222176B2 (en) | 2019-05-24 | 2022-01-11 | International Business Machines Corporation | Method and system for language and domain acceleration with embedding evaluation |
CN110287494A (en) * | 2019-07-01 | 2019-09-27 | 济南浪潮高新科技投资发展有限公司 | A method of the short text Similarity matching based on deep learning BERT algorithm |
CN110442759A (en) * | 2019-07-25 | 2019-11-12 | 深圳供电局有限公司 | Knowledge retrieval method and system, computer equipment and readable storage medium |
CN110516251A (en) * | 2019-08-29 | 2019-11-29 | 秒针信息技术有限公司 | A kind of construction method, construction device, equipment and the medium of electric business entity recognition model |
US11449675B2 (en) | 2019-09-20 | 2022-09-20 | International Business Machines Corporation | Selective deep parsing of natural language content |
US11748562B2 (en) | 2019-09-20 | 2023-09-05 | Merative Us L.P. | Selective deep parsing of natural language content |
GB2602602A (en) * | 2019-09-20 | 2022-07-06 | Ibm | Selective deep parsing of natural language content |
US11120216B2 (en) | 2019-09-20 | 2021-09-14 | International Business Machines Corporation | Selective deep parsing of natural language content |
WO2021053470A1 (en) * | 2019-09-20 | 2021-03-25 | International Business Machines Corporation | Selective deep parsing of natural language content |
CN110708619A (en) * | 2019-09-29 | 2020-01-17 | 北京声智科技有限公司 | Word vector training method and device for intelligent equipment |
CN111177367A (en) * | 2019-11-11 | 2020-05-19 | 腾讯科技（深圳）有限公司 | Case classification method, classification model training method and related products |
CN111191689A (en) * | 2019-12-16 | 2020-05-22 | 恩亿科(北京)数据科技有限公司 | Sample data processing method and device |
CN111079410A (en) * | 2019-12-23 | 2020-04-28 | 五八有限公司 | Text recognition method and device, electronic equipment and storage medium |
US20210200948A1 (en) * | 2019-12-27 | 2021-07-01 | Ubtech Robotics Corp Ltd | Corpus cleaning method and corpus entry system |
US11580299B2 (en) * | 2019-12-27 | 2023-02-14 | Ubtech Robotics Corp Ltd | Corpus cleaning method and corpus entry system |
CN111414750A (en) * | 2020-03-18 | 2020-07-14 | 北京百度网讯科技有限公司 | Synonymy distinguishing method, device, equipment and storage medium for entries |
US20210304056A1 (en) * | 2020-03-25 | 2021-09-30 | International Business Machines Corporation | Learning Parameter Sampling Configuration for Automated Machine Learning |
WO2021217936A1 (en) * | 2020-04-29 | 2021-11-04 | 深圳壹账通智能科技有限公司 | Word combination processing-based new word discovery method and apparatus, and computer device |
US11481552B2 (en) * | 2020-06-01 | 2022-10-25 | Salesforce.Com, Inc. | Generative-discriminative language modeling for controllable text generation |
US20210374361A1 (en) * | 2020-06-02 | 2021-12-02 | Oracle International Corporation | Removing undesirable signals from language models using negative data |
CN112101030A (en) * | 2020-08-24 | 2020-12-18 | 沈阳东软智能医疗科技研究院有限公司 | Method, device and equipment for establishing term mapping model and realizing standard word mapping |
CN111931509A (en) * | 2020-08-28 | 2020-11-13 | 北京百度网讯科技有限公司 | Entity chain finger method, device, electronic equipment and storage medium |
CN112232065A (en) * | 2020-10-29 | 2021-01-15 | 腾讯科技（深圳）有限公司 | Method and device for mining synonyms |
CN112633007A (en) * | 2020-12-21 | 2021-04-09 | 科大讯飞股份有限公司 | Semantic understanding model construction method and device and semantic understanding method and device |
WO2022134360A1 (en) * | 2020-12-25 | 2022-06-30 | 平安科技（深圳）有限公司 | Word embedding-based model training method, apparatus, electronic device, and storage medium |
CN112966507A (en) * | 2021-03-29 | 2021-06-15 | 北京金山云网络技术有限公司 | Method, device, equipment and storage medium for constructing recognition model and identifying attack |
CN114297338A (en) * | 2021-12-02 | 2022-04-08 | 腾讯科技（深圳）有限公司 | Text matching method, apparatus, storage medium and program product |
CN115114910A (en) * | 2022-04-01 | 2022-09-27 | 腾讯科技（深圳）有限公司 | Text processing method, device, equipment, storage medium and product |
CN114764444A (en) * | 2022-04-06 | 2022-07-19 | 云从科技集团股份有限公司 | Image generation and sample image expansion method, device and computer storage medium |
CN114676227A (en) * | 2022-04-06 | 2022-06-28 | 北京百度网讯科技有限公司 | Sample generation method, model training method and search method |
US11836591B1 (en) | 2022-10-11 | 2023-12-05 | Wevo, Inc. | Scalable systems and methods for curating user experience test results |
CN115344728A (en) * | 2022-10-17 | 2022-11-15 | 北京百度网讯科技有限公司 | Image retrieval model training method, image retrieval model using method, image retrieval model training device, image retrieval model using device, image retrieval model equipment and image retrieval model medium |
US11748248B1 (en) * | 2022-11-02 | 2023-09-05 | Wevo, Inc. | Scalable systems and methods for discovering and documenting user expectations |
CN116975301A (en) * | 2023-09-22 | 2023-10-31 | 腾讯科技（深圳）有限公司 | Text clustering method, text clustering device, electronic equipment and computer readable storage medium |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20150095017A1 (en) | System and method for learning word embeddings using neural language models | |
US11741109B2 (en) | Dialogue system, a method of obtaining a response from a dialogue system, and a method of training a dialogue system | |
Jung | Semantic vector learning for natural language understanding | |
US11210306B2 (en) | Dialogue system, a method of obtaining a response from a dialogue system, and a method of training a dialogue system | |
CN107066464B (en) | Semantic natural language vector space | |
Kim et al. | Two-stage multi-intent detection for spoken language understanding | |
US11797822B2 (en) | Neural network having input and hidden layers of equal units | |
US20190130249A1 (en) | Sequence-to-sequence prediction using a neural network model | |
CN107480143B (en) | Method and system for segmenting conversation topics based on context correlation | |
WO2019153737A1 (en) | Comment assessing method, device, equipment and storage medium | |
US11379668B2 (en) | Topic models with sentiment priors based on distributed representations | |
CN107729313B (en) | Deep neural network-based polyphone pronunciation distinguishing method and device | |
CN107180084B (en) | Word bank updating method and device | |
Atia et al. | Increasing the accuracy of opinion mining in Arabic | |
CN114580382A (en) | Text error correction method and device | |
CN110134777B (en) | Question duplication eliminating method and device, electronic equipment and computer readable storage medium | |
He et al. | A two-stage biomedical event trigger detection method integrating feature selection and word embeddings | |
WO2014073206A1 (en) | Information-processing device and information-processing method | |
US20240111956A1 (en) | Nested named entity recognition method based on part-of-speech awareness, device and storage medium therefor | |
Patra et al. | A kernel-based approach for biomedical named entity recognition | |
CN114003682A (en) | Text classification method, device, equipment and storage medium | |
CN110837730B (en) | Method and device for determining unknown entity vocabulary | |
Celikyilmaz et al. | An empirical investigation of word class-based features for natural language understanding | |
CN107729509A (en) | The chapter similarity decision method represented based on recessive higher-dimension distributed nature | |
Devkota et al. | Knowledge of the ancestors: Intelligent ontology-aware annotation of biological literature using semantic similarity |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: DEEPMIND TECHNOLOGIES LIMITED, UNITED KINGDOMFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:MNIH, ANDRIY;KAVUKCUOGLU, KORAY;REEL/FRAME:032098/0499Effective date: 20140116 |
|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:DEEPMIND TECHNOLOGIES LIMITED;REEL/FRAME:032746/0855Effective date: 20140422 |
|
STCB | Information on status: application discontinuation |
Free format text: ABANDONED -- FAILURE TO RESPOND TO AN OFFICE ACTION |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044144/0001Effective date: 20170929 |
|
AS | Assignment |
Owner name: DEEPMIND TECHNOLOGIES LIMITED, UNITED KINGDOMFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044242/0116Effective date: 20170921 |
|
AS | Assignment |
Owner name: DEEPMIND TECHNOLOGIES LIMITED, UNITED KINGDOMFree format text: CORRECTIVE ASSIGNMENT TO CORRECT THE DECLARATION PREVIOUSLY RECORDED AT REEL: 044144 FRAME: 0001. ASSIGNOR(S) HEREBY CONFIRMS THE DECLARATION;ASSIGNOR:DEEPMIND TECHNOLOGIES LIMITED;REEL/FRAME:058722/0008Effective date: 20220111 |