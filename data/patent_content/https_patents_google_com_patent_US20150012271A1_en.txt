US20150012271A1 - Speech recognition using domain knowledge - Google Patents
Speech recognition using domain knowledge Download PDFInfo
- Publication number
- US20150012271A1 US20150012271A1 US14/048,199 US201314048199A US2015012271A1 US 20150012271 A1 US20150012271 A1 US 20150012271A1 US 201314048199 A US201314048199 A US 201314048199A US 2015012271 A1 US2015012271 A1 US 2015012271A1
- Authority
- US
- United States
- Prior art keywords
- score
- search results
- search
- candidate transcription
- transcription
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/1815—Semantic context, e.g. disambiguation of the recognition hypotheses based on word meaning
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
Definitions
- This specification describes technologies related to speech recognition.
- Automatic speech recognition is an important technology that is used in mobile devices and other devices. In general, automatic speech recognition attempts to provide accurate transcriptions of what a person has said.
- a speech recognition system can use information about one or more domains to re-score candidate transcriptions for an utterance. Re-scoring using domain information may indicate a correct candidate transcription better than output of a general speech recognizer.
- a speech recognition system can re-score candidate transcriptions using data related to use of the candidate transcriptions as queries. For example, the speech recognition system can receive content, such as search results, that a search service for a particular domain identifies as relevant to a candidate transcription. As another example, the speech recognition system can receive data indicating a frequency that the candidate transcription, or a portion of the candidate transcription, has been submitted as a query.
- the speech recognition system may provide a score based on the received data to a classifier that is trained to evaluate candidate transcriptions, to obtain an output from the classifier for the candidate transcription. In this manner, the speech recognition system may obtain classifier outputs for each of multiple candidate transcriptions for an utterance. The speech recognition system may then select from among the multiple candidate transcriptions based on the classifier outputs. For example, the candidate transcriptions may be ranked according to their classifier outputs and a highest-ranking candidate transcription may be selected.
- a method performed by data processing apparatus includes receiving data that indicates multiple candidate transcriptions for an utterance.
- the method includes, for each of the candidate transcriptions: receiving data relating to use of the candidate transcription as a search query; providing, to a trained classifier, a score that is based on the received data relating to use of the candidate transcription as a search query; and receiving, from the trained classifier, a classifier output that indicates a likelihood that the candidate transcription is a correct transcription.
- the method includes selecting from among the multiple candidate transcriptions based on the classifier outputs.
- implementations of this and other aspects include corresponding systems, apparatus, and computer programs, configured to perform the actions of the methods, encoded on computer storage devices.
- a system of one or more computers can be so configured by virtue of software, firmware, hardware, or a combination of them installed on the system that in operation cause the system to perform the actions.
- One or more computer programs can be so configured by virtue of having instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
- Implementations may include one or more of the following features. For instance, receiving the data relating to use of the candidate transcription as a search query includes receiving data indicating a frequency of submission of the candidate transcription as a search query; and providing, to the trained classifier, the score that is based on the received data relating to use of the candidate transcription as a search query includes providing, as input to the trained classifier, a score that is indicative of the frequency of submission of the candidate transcription as a search query.
- Implementations may include one or more of the following features. For instance, receiving the data relating to use of the candidate transcription as a search query includes receiving data indicating content that a search service identifies as relevant to the candidate transcription. The method may include determining a feature score based on the content that the search service identifies as relevant to the candidate transcription. Providing, to the trained classifier, the score that is based on the received data relating to use of the candidate transcription as a search query includes providing the feature score to a classifier that is trained to indicate a likelihood that a candidate transcription is correct based at least partially on a feature score for content identified by the search service.
- Implementations may include one or more of the following features. For instance, receiving data indicating content that a search service identifies as relevant to the candidate transcription includes receiving data indicating first content that a search service for a first domain identifies as relevant to the candidate transcription. Determining the feature score based on the content includes determining a first feature score for a first feature based on the first content. The method may include, for each of the candidate transcriptions: receiving data indicating second content that a search service for a second domain identifies as relevant to the candidate transcription, the second content being different from the first content; determining a second feature score for a second feature based on the second content, the second feature being different from the first feature; and providing the second feature score to the trained classifier. Receiving, from the trained classifier, a classifier output for the candidate transcription includes receiving a classifier output based on the first feature score and the second feature score.
- Implementations may include one or more of the following features. For instance, determining the feature score based on the content includes determining a feature score that indicates a degree that the content matches the candidate transcription. Determining the feature score that indicates the degree that the content matches the candidate transcription includes determining a feature score indicating a degree that a highest-ranked search result indicated by the content matches the candidate transcription. Determining the feature score that indicates the degree that the content matches the candidate transcription includes determining a feature score indicating a degree that a second-ranked search result indicated by the content matches the candidate transcription. Determining the feature score based on the content includes accessing a score that is generated by the search service. The method may include providing one or more of the multiple candidate transcriptions to a search service as a query.
- Selecting from among the multiple candidate transcriptions includes: ranking the multiple candidate transcriptions based on the classifier outputs; and selecting the highest-ranked candidate transcription.
- Providing the score to the trained classifier includes providing the score to a maximum entropy classifier.
- the trained classifier has been trained using (i) a first set of scores corresponding to first features relevant to a first domain and (ii) a second set of feature scores for second features relevant to a second domain, the second features being different from the first features.
- FIG. 1 is a block diagram that illustrates an example of a system for speech recognition using domain knowledge.
- FIGS. 2A and 2B are diagrams that illustrate examples of search results.
- FIGS. 3A and 3B are diagrams that illustrate examples of word lattices.
- FIG. 4 is a flow diagram that illustrates an example of a process for speech recognition using domain knowledge.
- FIG. 5 is a flow diagram that illustrates an example of a process for training a classifier.
- FIG. 1 is a block diagram that illustrates an example of a system 100 for speech recognition using domain knowledge.
- the system 100 includes a client device 110 , a computing system 120 , and a network 118 .
- the computing system 120 receives audio data 112 from the client device 110 , and the computing system 120 uses domain-specific information to provide a transcription 175 for the audio data 112 .
- the figure shows stages (A) to (H) which illustrate a flow of data.
- the client device 110 can be, for example, a desktop computer, a laptop computer, a cellular phone, a smart phone, a tablet computer, a music player, an e-book reader, or a navigation system.
- the functions performed by the computing system 120 can be performed by individual computer systems or can be distributed across multiple computer systems.
- the network 118 can be wired or wireless or a combination of both, and may include private networks and/or public networks, such as the Internet.
- domains represent different uses for speech recognition output. For example, searching the World Wide Web (e.g., using a transcription as a query) may be considered to be one domain, and searching a media library may be considered a second domain. As additional examples, searching an application catalog may be considered a third domain, and providing a voice action (e.g., a voice command) may be considered a fourth domain.
- a domain may represent a one or more subject matter areas, such as music, movies, television, sports, and so on.
- a domain may represent a particular type or collection of data to be searched, such as web pages for a web search, or an application catalog for a search of applications.
- a single, general speech recognizer may be used for speech recognition across multiple different domains.
- some speech recognizers may not be able to take advantage of information about the particular domains or contexts in which a transcription will be used.
- the correct transcription for an utterance may be included in a set of candidate transcriptions that the speech recognizer provides, but the correct transcription may not be indicated as most likely to be correct. For example, rather than being indicated as the most likely transcription, the correct transcription may be assigned the second or third position in a ranking of candidate transcriptions provided by the speech recognizer.
- street names may occur infrequently in training data for a speech recognizer, causing the speech recognizer to consider street names as unlikely to be the best transcription in many instances.
- street names may be common in GPS navigation directions.
- knowing that a particular transcription represents a street name in a navigation domain may suggest that the particular transcription is more likely to be correct than the speech recognizer system has indicated.
- information about the domains or contexts in which recognized speech may be used can indicate whether a transcription is appropriate. For example, information about different domains may indicate whether a word or phrase is a plausible query for one or more of the different domains.
- information about one or more domains can be used to re-score and re-rank candidate transcriptions using domain information. For example, after a set of candidate transcriptions are identified for an utterance, further processing may be done for a set of the n-best candidate transcriptions, where n is an integer (e.g., the 3, 5, or 10 most likely transcriptions). Rather than accepting the transcription that the speech recognizer indicates is most likely, the set of n-best candidate transcriptions is re-ranked using domain information that was not accessible to the speech recognizer system.
- a language model that is broad enough to model naturally spoken queries may not be able to disambiguate between acoustically confusable sentences such as “I say” and “Ice Age.”
- the input is a query meant for an entertainment search service for finding information about TV shows, music, and movies, it appears more likely that “Ice Age,” the title of a movie, is what the user meant.
- Each of the n-best candidate transcriptions can be provided as a query to a search service (or to multiple different search services). Search results for each of the n-best candidate transcriptions are analyzed, for example, to determine how well each set of search results matches the corresponding query that produced the search results. Information about the quality of search results can indicate a degree that each candidate transcription is a plausible search query, which may indicate a likelihood that the candidate transcription is correct. If a candidate transcription does not produce meaningful search results, the candidate transcription is unlikely to be a correct transcription. On the other hand, if a candidate transcription produces high-quality search results (e.g., results that match the candidate transcription well), the candidate transcription is more likely to be correct. The information about the quality of the search results may be used to re-rank the n-best candidate transcriptions and select the transcription for an utterance.
- information about how often a candidate transcription is submitted as a query can indicate how plausible it is that a candidate transcription is a search query. For example, if the text of a candidate transcription is frequently submitted as a query in a particular domain (e.g., in a media search environment), the candidate transcription is more likely to be a correct transcription for a spoken query in that domain than other candidate transcriptions that are not submitted as queries as frequently in the domain. Information about query submission frequencies can also be used to re-rank the n-best candidate transcriptions, in addition to or instead of information about search results.
- information about search results from one or more domains can be provided to a trained classifier, along with feature information from the speech recognizer.
- the classifier which has been trained using domain information, can indicate a likelihood score for each candidate transcription.
- the highest-ranking candidate transcriptions may be ranked according to the classifier outputs rather than likelihood scores provided by the speech recognizer that identified the candidate transcriptions. Because the classifier receives domain information that is not available to the speech recognizer system, the outputs of the classifier may indicate likelihoods that are more accurate than those provided by the speech recognizer.
- a user 102 speaks, and the client device 110 records the utterance of the user 102 .
- the user 102 may speak words that represent a query for searching, for example, the World Wide Web, a media library, an application store catalog, or another collection of data.
- the client device 110 transmits audio data 112 that includes the user's utterance to the computing system 120 over the network 118 .
- the computing system 120 receives the audio data 112 and obtains a set of candidate transcriptions.
- the audio data 112 may be provided to a speech recognizer module 130 that may produces a word lattice indicating multiple different possible transcriptions for the utterance in the audio data 112 . Examples of word lattices are shown in FIGS. 3A and 3B .
- the speech recognizer module 130 may use an acoustic model and language model to generate a word lattice or otherwise identify candidate transcriptions.
- the speech recognizer module 130 may also indicate which of the candidate transcriptions the speech recognizer module 130 considers most likely to be correct, for example, by providing likelihood scores and/or ranking for the candidate transcriptions.
- the computing system 120 identifies a set of highest-ranking candidate transcriptions 135 from within the set of candidate transcriptions received. For example, using likelihood scores or ranking information from the speech recognizer module 130 , the computing system 120 may select n candidate transcriptions with the highest likelihoods, where n is an integer. In the illustrated example, the top five candidate transcriptions (e.g., the five that are indicated as most likely to be correct) are selected as the set of highest-ranking candidate transcriptions 135 .
- the computing system 120 provides each of the candidate transcriptions 135 to at least one search service 140 a , 140 b .
- the candidate transcriptions may be provided to search services for multiple domains.
- each of the candidate transcriptions 135 is provided as a query to a first search service 140 a for a first domain.
- Each of the candidate transcriptions 135 is also provided as a query to a second search service 140 b for a second domain.
- the candidate transcriptions 135 are each provided as independent queries.
- the transcription “write” is provided as a query to the search service 140 a , and is also provided to the search service 140 b , without information about the other candidate transcriptions 135 .
- the transcription “right” is provided individually to the search service 140 a and the search service 140 b , and so on.
- Each of the search services 140 a , 140 b provides a set of search results for each of the candidate transcriptions 135 .
- the first search service 140 a provides search results 145 a , which includes five different sets of search results, one set for each of the candidate transcriptions 135 .
- the second search service 140 b provides search results 145 b , which also includes five different sets of search results, one set for each of the candidate transcriptions 135 . Because the search services 140 a , 140 b correspond to different domains, the search results 145 a from the first search service 140 a may be different from the search results 145 b from the second search service 140 b .
- a search service for a news domain and a search service for a shopping domain may both provide search results that identify web pages. However, when the same query is provided to the search services, each search service may indicate different sets of web pages in the search results.
- the search services 140 a , 140 b may be implemented in any appropriate hardware, firmware, software, or combinations thereof.
- the search services 140 a , 140 b may include one or more server systems separate from the computing system 120 .
- the computing system 120 and the search services 140 a , 140 b may be implemented together, for example, in a datacenter or a single computer system.
- the search services 140 a , 140 b are implemented as applications or services on the computing system 120 or another computer system.
- search results 145 a , 145 b from different search service 140 a , 140 b may identify different types of documents or different types of information.
- the search results from one domain may indicate web pages or other resources
- the search results from another domain may indicate artists, songs, and albums from a music library.
- Other domains may provide search results that are news headlines, shopping results, voice actions, or other types of results.
- FIGS. 2A and 2B are diagrams that illustrate examples of search results.
- FIG. 2A illustrates a set of search results 200 received from the first search service 140 a in response to the query “right.”
- the first search service 140 a is associated with a web search domain, so each of the results identifies a web page identified as relevant to the query.
- FIG. 2B illustrates a set of search results 250 received from the second search service 140 b in response to the query “right.”
- the second search service 140 b is associated with a music library search domain, so each of the results indicates a song identified as relevant to the query.
- the search services 140 a , 140 b also provide sets of search results for each of the other candidate transcriptions 135 .
- a search service for a domain may provide search results for each of multiple categories of information.
- a media search domain may search within artist data (e.g., a list or database of artists known within the domain) and provide a first set of search results that are artist names.
- the media search domain may also search within album data and provide a second set of search results that are album names.
- the media search domain may search within song data, movie data, television data, and application data to provide additional sets of search results that are, respectively, song titles, movie titles, television titles, and application names.
- a domain may provide distinct sets of search results that identify the best matches to a query in each of multiple different categories of information.
- the computing system 120 uses a feature analysis module 150 to determine feature scores 155 for one or more features of the search results 145 a , 145 b .
- the features may indicate different aspects or attributes of search results, or may indicate characteristics of different categories of search results.
- the feature analysis module 150 may also determine feature scores for speech recognition features and semantic features.
- the feature analysis module 150 may determine other scores, such as query submission scores based on a frequency that a candidate transcription has been submitted as a query.
- the feature scores 155 represent scores for search result features.
- One or more of the feature scores 155 may indicate a level of quality of search results 145 a , 145 b .
- a feature score 155 may indicate a confidence score, or quality score, a relevance score, an information retrieval score, or other measure that a search service provides.
- Feature scores may indicate how well the search results for a given candidate transcription 135 match the candidate transcription 135 .
- the degree that a search result matches a candidate transcription may be based on, for example, whether the search result completely or partially matches the candidate transcription, whether words in the candidate transcription occur in the same sequence that the words occur in the candidate transcription, and/or a number of extra words in the search result or a number of words omitted from the search result relative to the candidate transcription.
- feature scores may indicate the number of search results returned for a query, and/or whether or not the top-ranked search result (e.g., the title of a document, media item, etc.) includes a complete string match for the candidate transcription.
- the feature analysis module 150 provides feature scores 155 for each of three different features labeled F1, F2, and F3, but feature scores may be determined more or fewer features.
- a feature score 155 is determined for each of the features F1, F2, and F3 for each candidate transcription 135 .
- the feature scores 155 for each candidate transcription 135 are determined using the sets of search results corresponding to that candidate transcription. For example, the sets of search results received in response to the query “write” are analyzed to determine the feature scores 155 for the candidate transcription “write”; the sets of search results received in response to the query “right” are analyzed to determine the feature scores 155 for the candidate transcription “right”; and so on.
- a feature score 155 may be determined based on an individual search result, such as the highest-ranked search result in a set. In some implementations, a feature score 155 may be based on multiple search results, for example, a feature score may indicate a total number of search results provided for a particular domain for a particular candidate transcription.
- the features may include domain features corresponding to different domains. Because each domain may provide different types of search results, the characteristics of search results that relevant may be different for different domains.
- the feature scores for features associated with a particular domain are determined based on the search results received from the particular domain. For example, the features F1 and F2 may be features relevant to a first domain, and feature scores 155 for these features may be determined using the search results 145 a from the first search service 140 a . For the candidate transcription “right,” domain scores for domain features relating to a web search domain may be determined based on the web search results 200 of FIG. 2A .
- the feature F3 may be a feature relevant to a second domain, and feature scores 155 for this feature may be determined using the search results 145 b from the second search service 140 b .
- domain scores for domain features relating to a music search domain may be determined based on the music search results 250 of FIG. 2B .
- the features can be selected to indicate likelihoods that a candidate transcription represents a navigational query.
- a navigational query may be a query that represents a request for a single, specific result, such as a particular web page or a particular song or movie title.
- Features that represent characteristics of individual search results e.g., the top-ranked search result, or the second-ranked search result, etc. may be indicators of the degree that a candidate transcription corresponds to a specific result.
- search results from a media catalog domain may indicate lists of which artists, albums, songs, movies, TV shows, and applications are considered most relevant to a candidate transcription.
- the domain features for the media catalog domain may represent characteristics of the highest-ranked search result within different media categories.
- the domain features may include: (i) how well the best artist name search result matches the candidate transcription; (ii) how well the best album name search result matches the candidate transcription; (iii) how well the best song name search result matches the candidate transcription; (iv) how well the best movie title search result matches the candidate transcription; (v) how well the best TV show title search result matches the candidate transcription; and (vi) how well the best application name search result matches the candidate transcription.
- Additional domain features for the media catalog domain may represent the degree that the second-ranked search result (e.g., the second-highest ranked result) for each media category matches the corresponding candidate transcription.
- a media viewing domain may indicate search results that indicate lists of movies, TV shows, or video clips.
- the domain features for the media catalog domain may represent the degree that the top-ranked search result for each of the media categories of movies, TV shows, or video clips matches the corresponding candidate transcription.
- the domain features may additionally or alternatively include features that represent the degree that the second-ranked search result for each of the media categories matches the corresponding candidate transcription.
- the highest-ranked movie result has a feature score indicating a high degree of match with the candidate transcription.
- the second-ranked movie result would tend to have a score indicating a lower degree of match, since the title is different from the title spoken by the user.
- feature scores indicate that the highest-ranked and second-ranked results are both poor matches for the candidate transcription, the candidate transcription may be unlikely to be correct.
- feature scores 155 may be determined for other features or aspects of search results.
- a category score can be determined. For a certain search service, it may be known, for example, that 50% of search results provided are movies, 30% of search results provided are TV shows, and 20% of search results are web videos. In this example, the likelihood that a search result is a movie could be estimated as 50%.
- the category score can indicate a likelihood that a search result provided by the search service is expected to be an item in the same category as the highest-ranked search result. For example, it may be determined that the highest-ranked search result is in the “movie” category, and the category score can be indicative of the likelihood that a movie, rather than another form of media, will be presented as a search result.
- a next category score (e.g., a category score for the next search result) can also be determined.
- the next category probability score can indicate a likelihood that an item in the same category as the second-highest-ranked search result will be provided as a search result.
- the category score and the next category score can indicate how different the highest-ranked result and the second-highest-ranked result are. The larger the difference between the likelihoods indicated by the two category scores, the more likely the candidate transcription represents a navigational query.
- the feature analysis module 150 may determine scores for speech recognition features and semantic features.
- Speech recognition features can provide information from the speech recognizer module 130 .
- one speech recognition feature score may be a language model score, such as a score from an n-gram language model of the speech recognizer model 130 .
- Another example is a rank position of a candidate transcription 135 in the first pass ranking from the speech recognizer N-best list.
- the highest-ranked candidate transcription 135 e.g., at “position 0,” may be considered to be the best hypothesis from the speech recognizer module 130 , and may be used as a baseline.
- Other examples of speech recognition feature scores include the number of tokens (e.g., words) in a candidate transcription 135 , and a confidence score for a candidate transcription 135 from the speech recognizer module 130 .
- Semantic feature scores may be generated based on pattern matching. Many voice queries are commands like “show me movies by Jim Carey”, or “open App”. If a candidate transcription 135 matches a popular voice action pattern, it has a higher likelihood of being a correct recognition than a candidate transcription that does not match a known pattern. Each of the candidate transcriptions 135 may be parsed with a finite-state-machine-based pattern matching system, and semantic parsing feature scores may be created for popular voice actions, such as an “Open app” action, or a “Play media” action (e.g., play movies, music, TV show). Semantic features may be expressed in binary values. For example, if a candidate transcription matches a predetermined pattern, or any of multiple predetermined patterns, a semantic feature score of “1” may be provided, and otherwise a semantic feature score of “0” may be provided.
- the computing system 120 provides a feature vector to a classifier module 160 and receives a classifier output 165 from the classifier module 160 .
- the feature vector can include feature scores 155 derived from the search results, as well as speech recognition feature scores and semantic feature scores.
- the classifier module 160 may be a classifier that has been trained to indicate a likelihood that a candidate transcription is correct based on, for example, the feature scores for search results corresponding to the candidate transcription.
- the classifier module 160 may be trained with, and may receive as input, sets of input data that include feature scores for the domain features for one or more domains. Through training, the classifier module 160 may learn to discriminate among different combinations of feature scores to be able to indicate which feature score values and combinations of feature score values suggest a transcription is likely to be correct and which feature score values and combinations do not.
- the feature vector for a particular candidate transcription 135 can include the feature scores 155 determined from the search results 145 a , 145 b for the particular candidate transcription 135 .
- the feature vector may include the feature scores 155 of “0.8,” “0.7” and “0.8,” respectively, for the three domain features F1, F2, and F3 illustrated, which represent characteristics of the search results 200 , 250 .
- the feature vector may include feature scores 155 for each of multiple domains.
- the feature vector may include feature scores for a media catalog domain, feature scores for a voice action domain, and feature scores for a web search domain.
- the classifier module 160 may be able to use information from multiple domains to provide better estimates of the likelihood of a transcription being correct.
- a classifier module 160 that receives feature scores for multiple domains may provide more accurate results than, for example, a domain-specific classifier that only receives feature scores for a single domain.
- the feature vector may also include speech feature scores, which may indicate characteristics of the candidate transcription, word lattice, or other output of the speech recognizer module 130 .
- speech feature scores included in the feature vector may indicate one or more of, for example: (i) the position of the candidate transcription within the ranking provided by the speech recognizer module; (ii) a number of tokens (e.g., words) included in the candidate transcription; (iii) a language model score produced by a language model; and (iv) a speech confidence score, such as an overall confidence score generated by the speech recognizer module 130 .
- Other information about a candidate transcription may additionally or alternatively be provided in a feature vector.
- the arrow from the candidate transcriptions 135 to the feature analysis module 150 indicates that information in the feature vector may include or be derived from output of the speech recognizer module 130 .
- the classifier module 160 can be a maximum entropy classifier.
- the classifier module 160 may use linear models to assess the feature vectors it receives.
- the classifier module 160 may use decision trees or other models or machine learning techniques.
- scores for candidate transcriptions are determined and provided to the classifier module 160 without requesting or obtaining search results for the candidate transcriptions.
- the scores can include speech recognition feature scores based on output of the speech recognizer module 130 , and/or the scores can include semantic feature scores based on the text of the candidate transcription.
- the scores may include query submission scores that are indicative of how often a candidate transcription, or a portion of the candidate transcription, has been submitted as a search query.
- query submission data can be used to determine how often users have submitted entered the text of the candidate transcription in a search query field and submitted the text as a search query.
- Query submission scores can be domain-specific, for example, with query submission scores for each of multiple different domains indicating how often the candidate transcription is submitted as a query in the different domains.
- the computing system 120 receives classifier outputs 165 for the candidate transcriptions 135 and re-ranks the set of highest-ranking candidate transcriptions 135 according to the classifier outputs 165 .
- the classifier module 160 may provide a classifier output 165 for each candidate transcription 135 (e.g., for each feature vector received) that indicates a likelihood that the candidate transcription 135 is correct.
- the computing system 120 produces a new ranking 170 that is different from original ranking indicated by the speech recognizer module 130 .
- the positions of the transcriptions “write” and “right” have been switched to indicate that the transcription “right” is predicted as most likely to be correct.
- the computing system 120 selects the highest-ranked candidate transcription 175 from the new ranking 170 as the transcription for the audio data 112 .
- the computing system 120 provides the selected candidate transcription 175 to the client device 110 over the network 118 .
- the client device 110 may then use the transcription in whatever manner the user 102 instructs, for example, by submitting the transcription 175 as search query, interpreting the transcription 175 as a voice action, or using the transcription in another manner.
- the feature vector provided to the classifier module 160 includes feature scores from one or more three different feature categories: speech recognition features, semantic features, and search result features.
- Speech recognition features may include information produced by a language model for a given candidate transcription, such as a language model probability, a position within a ranking, a number of tokens, or a confidence score from the speech recognizer.
- Semantic features may indicate information about of pattern matching analysis, for example, matching a candidate transcription to a grammar. For example, if a candidate transcription matches a popular voice action pattern, it has a better chance to be correct recognition. Many voice queries are commands, such as “show me movies by Jim Carey” or “open website.” A candidate transcription may be parsed with a finite-state-machine-based pattern matching system to determine an extent that the candidate transcription matches different grammars for, for example, opening an application, playing a movie, requesting navigation directions, and so on. The feature score for each semantic feature may indicate a degree of match between the candidate transcription and a different grammar.
- Search result features represent characteristics of search results.
- feature scores for a particular candidate transcription represent characteristics of the search results obtained when the particular candidate transcription is used as a query.
- the search result features may indicate a degree to which a candidate transcription is a navigational query in a particular category of search results.
- various categories of search results from a domain may include TV shows, video clips, movie titles, application names, album names, and song titles.
- the various search result feature scores for this domain may respectively indicate, for example, how likely a candidate transcription is to be a particular TV show, how likely the candidate transcription is to be a movie title, and so on. This likelihood can be determined as a degree to which the candidate transcription matches the search result. For example, if the candidate transcription matches the top-ranked movie title in a set of search results, it is very likely that the candidate transcription is a navigational query specifying the movie title.
- Feature scores may be determined for other search result features as well. For example, a feature score may indicate a number of search results returned for a given query. In general, a query that returns a large number of results is more likely to be a correct transcription than a query that returns fewer results. Another feature score may indicate whether the top search result has a complete string match to the candidate transcription/query that produced the result. Another feature score may indicate a degree to which a query is navigational, for example, as determined based on a distribution of user interactions with the search result. For example, if a large percentage of users click the top result in response to a query, the query may represent a navigational request.
- the candidate transcriptions 135 illustrated in FIG. 1 are individual words, in some implementations, one or more of the transcriptions may be phrases including multiple words.
- the audio data 112 may represent an entire query spoken by the user, and each of the transcriptions may be a candidate transcription for the audio data 112 as a whole.
- FIG. 1 shows processing using search results from two different domains and using the features of the two different domains.
- Candidate transcriptions may be evaluated using features of search results from more domains or fewer domains than illustrated.
- results are obtained for only a single domain, and the classifier receives only feature scores for a single domain.
- the classifier may be a domain-specific classifier.
- search services 140 a , 140 b are illustrated as separate modules, in some implementations, the same search service may provide search results for different domains.
- a single search infrastructure may perform searches for different domains by, for example, searching using different parameters, searching different within data collections, or using other variations.
- FIG. 3A is an example of a word lattice 300 that may be provided by a speech recognizer system.
- the word lattice 300 represents multiple possible combinations of words that may form different candidate transcriptions for an utterance.
- the word lattice 300 includes one or more nodes 302 a - g that correspond to the possible boundaries between words.
- the word lattice 300 includes multiple edges 304 a - 1 for the possible words in the transcription hypotheses that result from the word lattice 300 .
- each of the edges 304 a - 1 can have one or more weights or probabilities of that edge being the correct edge from the corresponding node.
- the weights are determined by the speech recognizer module system and can be based on, for example, a confidence in the match between the speech data and the word for that edge and how well the word fits grammatically and/or lexically with other words in the word lattice 300 .
- the most probable path through the word lattice 300 may include the edges 304 c , 304 e , 304 i , and 304 k , which have the text “we're coming about 11:30.”
- a second best path may include the edges 304 d , 304 h , 304 j , and 304 l , which have the text “deer hunting scouts 7:30.”
- Each pair of nodes may have one or more paths corresponding to the alternate words in the various candidate transcriptions.
- the initial most probable path between the node pair beginning at the node 302 a and ending at the node 302 c is the edge 304 c “we're”.
- This path has alternate paths that include the edges 304 a - b “we are” and the edge 304 d “deer”.
- FIG. 3B is an example of a hierarchical word lattice 350 that may be provided by a speech recognizer system.
- the word lattice 350 includes nodes 352 a - 1 that represent the words that make up the various candidate transcriptions for an utterance.
- the edges between the nodes 352 a - 1 show that the possible candidate transcriptions include: (1) the nodes 352 c , 352 e , 352 i , and 352 k “we're coming about 11:30”; (2) the nodes 352 a , 352 b , 352 e , 352 i , and 352 k “we are coming about 11:30”; (3) the nodes 352 a , 352 b , 352 f , 352 g , 352 i , and 352 k “we are come at about 11:30”; (4) the nodes 352 d , 352 f , 352 g , 352 i , and 352 k “deer come at about 11:30”; (5) the nodes 352 d , 352 h , 352 j , and 352 k “deer hunting scouts 11:30”; and (6) the nodes 352 d , 352 h , 35
- edges between the nodes 352 a - 1 may have associated weights or probabilities based on the confidence in the speech recognition and the grammatical/lexical analysis of the resulting text.
- “we're coming about 11:30” may currently be the best hypothesis and “deer hunting scouts 7:30” may be the next best hypothesis.
- One or more divisions 354 a - d can be made in the word lattice 350 that group a word and its alternates together.
- the division 354 a includes the word “we're” and the alternates “we are” and “deer”.
- the division 354 b includes the word “coming” and the alternates “come at” and “hunting”.
- the division 354 c includes the word “about” and the alternate “scouts” and the division 354 d includes the word “11:30” and the alternate “7:30”.
- FIG. 4 is a flow diagram that illustrates an example of a process 400 for speech recognition using domain knowledge.
- the operations of the process 400 may be performed by one or more computing systems, such as the computing system 120 of FIG. 1 .
- Data that indicates multiple candidate transcriptions for an utterance is received ( 402 ).
- the data can include a word lattice that indicates multiple alternative translations for the utterance.
- the data can indicate likelihoods that the various candidate transcriptions are correct.
- a predefined number of candidate transcriptions, such as the n-best candidate transcriptions as indicated by the speech recognizer, may be identified and may be evaluated.
- Data relating to use of a particular candidate transcription as a query is received ( 404 ).
- the data indicates content that a search service identifies as relevant to the candidate transcription.
- the particular candidate transcription can be provided to a search service as a query, and search results for the particular candidate transcription may be received.
- the particular candidate transcription may be provided to search services for multiple different domains, and a set of search results may be received for each of the different domains.
- the data relating to use of a particular candidate transcription as a query indicates a frequency that the particular candidate transcription, or a portion of the particular candidate transcription, has been submitted as a query.
- the frequency can indicate how often a query exactly matching the text of the candidate transcription is submitted.
- the frequency can indicate how often the candidate transcription occurs within submitted queries, even if the query includes additional text.
- the data can indicate how often the candidate transcription was submitted with respect to a specific domain.
- the data indicates frequencies that the candidate transcription was submitted to search services for each of multiple different domains.
- the data can indicate a first frequency of submission for a first domain, such as a web search domain, and the data can indicate a second frequency of submission for a second domain, such as a media search domain.
- the frequency can be a measure of query submissions that reflects submissions in multiple domains.
- a frequency can be expressed in a variety of different forms, including, for example, a number of submissions or other measure of query submissions, an measure of submissions within a particular time period (e.g., within the previous week, month, or year), and a relative rate of submission or measure of popularity relative to other queries.
- a score based on the data relating to use of a particular candidate transcription as a query is provided to a classifier ( 406 ).
- the classifier may be a classifier that has been trained to indicate a likelihood that a candidate transcription is correct based at least partially on received scores.
- the classifier is a maximum entropy classifier.
- the classifier is trained to indicate a likelihood that a candidate transcription is correct based on scores for semantic features, speech recognition features, search result features, and/or query submission frequencies.
- the classifier may be a classifier that has been trained using (i) a first set of scores corresponding to first features relevant to a first domain and (ii) a second set of scores for second features relevant to a second domain, where the second features are different from the first features.
- a score is determined based on the content.
- the content may include a set of search results, and the score may be a value that indicates a characteristic of one or more of the search results.
- the score may a relevance score or other score that is generated by the search service.
- the score may indicate a degree that the content matches the candidate transcription. For example, the score may indicate a degree that a highest-ranked search result in a set of search results matches the candidate transcription. As another example, the score may indicate a degree that a second-highest-ranked search result in a set of search results matches the candidate transcription.
- different features represent different types or categories of search results within the domain. For example, one feature may represent a characteristic of search results that are artist names, another feature may represent a characteristic of search results that are song titles, and so on.
- the score may be provided to the classifier in a feature vector that includes scores for domain-specific features of different domains.
- a first domain may be associated with a first set of features, and the feature scores for the first set of features may be determined based on search results from the first domain.
- a second domain may be associated with a second, different set of features, and the scores for the second set of features may be determined based on search results from the second domain.
- the feature vector may also include scores for speech features, for example: a position of the candidate transcription within the ranking provided by the speech recognizer module; a number of tokens (e.g., words) included in the candidate transcription; a language model score produced by a language model; and a speech recognizer confidence score.
- the feature vector may also include scores for semantic features.
- one or more query submission scores can be determined based on query submission data. For example, a score indicative of the frequency that the candidate transcription has been submitted as a query in a particular domain can be determined and input to the classifier. In some implementations, query submission scores indicative of submission frequencies in different domains can be determined and input to the classifier.
- a classifier output for the candidate transcription is received from the classifier ( 408 ).
- the classifier output may be a score that indicates a likelihood that the candidate transcription is a correct transcription, in view of the scores associated with the candidate transcription and use of the candidate transcription as a query.
- the operations of ( 404 )-( 408 ) may be repeated for each of multiple candidate transcriptions for the utterance. For example, the operations may be repeated for each of the n-best candidate transcriptions to obtain a classifier output for each of the n-best candidate transcriptions
- one or more candidate transcriptions are selected based on the classifier outputs ( 410 ). For example, the candidate transcription having the classifier output that indicates the highest likelihood of correctness may be selected.
- the multiple candidate transcriptions may be ranked based on the classifier outputs, and the highest-ranked candidate transcription may be selected.
- the selected candidate transcription may be provided to a client device or other system as a transcription for the utterance.
- search results are received from search services for different domains, and scores may be determined for features of different domains. For example, for a music search domain, a score may indicate a degree that a top-ranked search result from a set of song titles matches the candidate transcription. For a web search domain, a score may indicate a degree that a top-ranked search result from among a set of web pages matches the candidate transcription.
- the scores for multiple domains may be provided in the same feature vector so the classifier can evaluate the candidate transcription based on different aspects of search results from different domains.
- the score is a first feature score determined for a first set of content identified by the search service for a first domain.
- Data indicating second content that a search service for a second domain identifies as relevant to the candidate transcription is also received, and the second content is different from the first content.
- a second feature score for a second feature of the second content is received.
- the second feature may be different from the first feature.
- the first feature score and the second feature score are both provided to the classifier, for example, as part of a single feature vector.
- the classifier output that is received may be based on the first feature score and the second feature score.
- FIG. 5 is a flow diagram that illustrates an example of a process 500 for training a classifier.
- the operations of the process 500 may be performed by one or more computing systems, such as the computing system 120 of FIG. 1 .
- Training data is received ( 502 ).
- the training data can indicate a set of multiple candidate transcriptions for each of multiple training utterances.
- the training data can indicate a predefined number of candidate transcriptions for each utterance, such as the n-best candidate transcriptions (e.g., top 3, 5, or 10 transcriptions) as indicated by a speech recognizer.
- the training data may also indicate, for each of the candidate transcriptions, a likelihood score output by the speech recognizer.
- the candidate transcriptions are each labeled ( 504 ). For example, the correct transcription for each training utterance is labeled as being correct. The other candidate transcriptions for each utterance are labeled as incorrect. In this manner, the evaluating candidate transcriptions may be done within a binary classification framework, e.g., each candidate transcription is either correct or incorrect.
- a score related to use of the candidate transcription as a query is determined ( 506 ).
- the score can be indicative of a frequency that the candidate transcription has been submitted as a query.
- the score can be determined based on search results for the candidate transcription.
- a particular candidate transcription can be provided to a search service as a query, and search results for the particular candidate transcription may be received.
- the particular candidate transcription may be provided to search services for multiple different domains, and a set of search results may be received for each of the different domains.
- the score may be a relevance score or other score that is generated by the search service.
- the score may indicate a degree that the content matches the candidate transcription.
- the score may indicate a degree that a highest-ranked search result in a set of search results matches the candidate transcription.
- the score may indicate a degree that a second-highest-ranked search result in a set of search results matches the candidate transcription.
- different features represent characteristics of different types or categories of search results in the domain. For example, one feature may represent a characteristic of search results that are artist names, another feature may represent a characteristic of search results that are song titles, and so on.
- a classifier is trained using the score ( 508 ).
- the classifier may be provided the label for the candidate transcription and a feature vector that includes the determined score.
- the feature vector may include scores for multiple features associated with a domain.
- the feature vector may include scores for multiple different domains, where the scores for each domain are determined based on a set of search results from that domain.
- the feature vector may include any of the scores discussed above, including, for example, scores for speech recognition features, semantic features, search result features, and query submission frequencies.
- Training the classifier using information about multiple different domains can provide a unified framework that is more robust than, for example, a domain-specific classifier that takes into account characteristics of only a single domain.
- weights or other parameters of the classifier may be adjusted to improve the classifier's ability to estimate the probability that a feature vector represents a correct transcription.
- multiple scores representing different characteristics may be determined and used to train the classifier. For example, one or multiple scores for semantic features, search result features, and/or speech recognition features may be determined and used to train the classifier. In addition, or as an alternative, one or more scores indicative of query submission frequencies may be determined and used to train the classifier. After training is complete, the same types of scores provided to the classifier during training may be input to the trained classifier to evaluate candidate transcriptions.
- the classifier may be trained using any of various machine learning techniques.
- the classifier is a maximum entropy classifier.
- the operations of ( 506 )-( 508 ) may be repeated using scores for each of the different candidate transcriptions for each of the training utterances.
- the classifier may learn to indicate a likelihood that a candidate transcription is correct based on the scores it receives.
- the output of the trained classifier can indicate, for a given feature vector or other set of input data, a probability that the candidate transcription corresponding to the feature vector is correct.
- the classifier uses conditional maximum entropy (ME) modeling.
- ME modeling may involve identifying a distribution that has the maximum entropy subject to known constraints expressed as features.
- Using a maximum entropy model allows arbitrary constraints to be used as features. This allows language modeling to capture short term dependency such as n-grams, long sentence dependency such as syntax, and semantics.
- the ME model may combine arbitrary context features from search results, syntax features from parsing, and speech features.
- a feature, f i (c, h) can be a real-valued function of the candidate transcription or hypothesis h and a target class c.
- Maximum entropy allows the model distribution to be restricted to have the same expected value for this feature as its empirical value from training data, H.
- h) must have the property:
- ⁇ h ⁇ H ⁇ f i ⁇ ( c , h ) ⁇ h ⁇ H ⁇ ⁇ c ⁇ P ⁇ ( c
- the maximum entropy distribution may be expressed as:
- the candidate transcription or hypothesis, h can be a context that is represented by the feature vector for a particular candidate transcription.
- the feature functions, f i (c, h), which are typically binary, define constraints for the model.
- optimal weights, ⁇ i, corresponding to features f i (c, h) are learned so that the log-likelihood of the training data, H, is maximized.
- the weights, i are learned via improved iterative scaling algorithm, or other algorithms such as a conjugate gradient descent algorithm or a limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm.
- Embodiments of the invention and all of the functional operations described in this specification may be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the invention may be implemented as one or more computer program products, i.e., one or more modules of computer program instructions encoded on a computer-readable storage device for execution by, or to control the operation of, data processing apparatus.
- the computer-readable storage device may be, for example, a machine-readable storage substrate, a memory device, a composition of matter producing a machine-readable propagated signal, or a combination of one or more of them.
- the computer-readable storage device may be a non-transitory machine-readable medium.
- data processing apparatus encompasses all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus may include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a propagated signal is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.
- a computer program (also known as a program, software, software application, script, or code) may be written in any form of programming language, including compiled or interpreted languages, and it may be deployed in any form, including as a standalone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program does not necessarily correspond to a file in a file system.
- a program may be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code).
- a computer program may be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification may be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows may also be performed by, and apparatus may also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- FPGA field programmable gate array
- ASIC application specific integrated circuit
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- a computer need not have such devices.
- a computer may be embedded in another device, e.g., a tablet computer, a mobile telephone, a personal digital assistant (PDA), a mobile audio player, a Global Positioning System (GPS) receiver, to name just a few.
- Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- the processor and the memory may be supplemented by, or incorporated in, special purpose logic circuitry.
- embodiments of the invention may be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user may provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices may be used to provide for interaction with a user as well; for example, feedback provided to the user may be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input.
- Embodiments of the invention may be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user may interact with an implementation of the invention, or any combination of one or more such back end, middleware, or front end components.
- the components of the system may be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system may include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Abstract
Description
- This application claims priority to U.S. Provisional Application No. 61/842,550, filed on Jul. 3, 2013, and U.S. Provisional Application No. 61/843,710, filed on Jul. 8, 2013. The entire contents of U.S. Provisional Patent Application Ser. No. 61/842,550 and U.S. Provisional Application No. 61/843,710 are incorporated herein by reference.
- This specification describes technologies related to speech recognition.
- Automatic speech recognition is an important technology that is used in mobile devices and other devices. In general, automatic speech recognition attempts to provide accurate transcriptions of what a person has said.
- A speech recognition system can use information about one or more domains to re-score candidate transcriptions for an utterance. Re-scoring using domain information may indicate a correct candidate transcription better than output of a general speech recognizer. In some implementations, a speech recognition system can re-score candidate transcriptions using data related to use of the candidate transcriptions as queries. For example, the speech recognition system can receive content, such as search results, that a search service for a particular domain identifies as relevant to a candidate transcription. As another example, the speech recognition system can receive data indicating a frequency that the candidate transcription, or a portion of the candidate transcription, has been submitted as a query. The speech recognition system may provide a score based on the received data to a classifier that is trained to evaluate candidate transcriptions, to obtain an output from the classifier for the candidate transcription. In this manner, the speech recognition system may obtain classifier outputs for each of multiple candidate transcriptions for an utterance. The speech recognition system may then select from among the multiple candidate transcriptions based on the classifier outputs. For example, the candidate transcriptions may be ranked according to their classifier outputs and a highest-ranking candidate transcription may be selected.
- In a general aspect, a method performed by data processing apparatus includes receiving data that indicates multiple candidate transcriptions for an utterance. The method includes, for each of the candidate transcriptions: receiving data relating to use of the candidate transcription as a search query; providing, to a trained classifier, a score that is based on the received data relating to use of the candidate transcription as a search query; and receiving, from the trained classifier, a classifier output that indicates a likelihood that the candidate transcription is a correct transcription. The method includes selecting from among the multiple candidate transcriptions based on the classifier outputs.
- Other implementations of this and other aspects include corresponding systems, apparatus, and computer programs, configured to perform the actions of the methods, encoded on computer storage devices. A system of one or more computers can be so configured by virtue of software, firmware, hardware, or a combination of them installed on the system that in operation cause the system to perform the actions. One or more computer programs can be so configured by virtue of having instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
- Implementations may include one or more of the following features. For instance, receiving the data relating to use of the candidate transcription as a search query includes receiving data indicating a frequency of submission of the candidate transcription as a search query; and providing, to the trained classifier, the score that is based on the received data relating to use of the candidate transcription as a search query includes providing, as input to the trained classifier, a score that is indicative of the frequency of submission of the candidate transcription as a search query.
- Implementations may include one or more of the following features. For instance, receiving the data relating to use of the candidate transcription as a search query includes receiving data indicating content that a search service identifies as relevant to the candidate transcription. The method may include determining a feature score based on the content that the search service identifies as relevant to the candidate transcription. Providing, to the trained classifier, the score that is based on the received data relating to use of the candidate transcription as a search query includes providing the feature score to a classifier that is trained to indicate a likelihood that a candidate transcription is correct based at least partially on a feature score for content identified by the search service.
- Implementations may include one or more of the following features. For instance, receiving data indicating content that a search service identifies as relevant to the candidate transcription includes receiving data indicating first content that a search service for a first domain identifies as relevant to the candidate transcription. Determining the feature score based on the content includes determining a first feature score for a first feature based on the first content. The method may include, for each of the candidate transcriptions: receiving data indicating second content that a search service for a second domain identifies as relevant to the candidate transcription, the second content being different from the first content; determining a second feature score for a second feature based on the second content, the second feature being different from the first feature; and providing the second feature score to the trained classifier. Receiving, from the trained classifier, a classifier output for the candidate transcription includes receiving a classifier output based on the first feature score and the second feature score.
- Implementations may include one or more of the following features. For instance, determining the feature score based on the content includes determining a feature score that indicates a degree that the content matches the candidate transcription. Determining the feature score that indicates the degree that the content matches the candidate transcription includes determining a feature score indicating a degree that a highest-ranked search result indicated by the content matches the candidate transcription. Determining the feature score that indicates the degree that the content matches the candidate transcription includes determining a feature score indicating a degree that a second-ranked search result indicated by the content matches the candidate transcription. Determining the feature score based on the content includes accessing a score that is generated by the search service. The method may include providing one or more of the multiple candidate transcriptions to a search service as a query. Selecting from among the multiple candidate transcriptions includes: ranking the multiple candidate transcriptions based on the classifier outputs; and selecting the highest-ranked candidate transcription. Providing the score to the trained classifier includes providing the score to a maximum entropy classifier. The trained classifier has been trained using (i) a first set of scores corresponding to first features relevant to a first domain and (ii) a second set of feature scores for second features relevant to a second domain, the second features being different from the first features.
- The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other potential features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
-
FIG. 1 is a block diagram that illustrates an example of a system for speech recognition using domain knowledge. -
FIGS. 2A and 2B are diagrams that illustrate examples of search results. -
FIGS. 3A and 3B are diagrams that illustrate examples of word lattices. -
FIG. 4 is a flow diagram that illustrates an example of a process for speech recognition using domain knowledge. -
FIG. 5 is a flow diagram that illustrates an example of a process for training a classifier. - Like reference numbers and designations in the various drawings indicate like elements.
-
FIG. 1 is a block diagram that illustrates an example of asystem 100 for speech recognition using domain knowledge. Thesystem 100 includes aclient device 110, acomputing system 120, and anetwork 118. In the example, thecomputing system 120 receivesaudio data 112 from theclient device 110, and thecomputing system 120 uses domain-specific information to provide atranscription 175 for theaudio data 112. The figure shows stages (A) to (H) which illustrate a flow of data. - The
client device 110 can be, for example, a desktop computer, a laptop computer, a cellular phone, a smart phone, a tablet computer, a music player, an e-book reader, or a navigation system. The functions performed by thecomputing system 120 can be performed by individual computer systems or can be distributed across multiple computer systems. Thenetwork 118 can be wired or wireless or a combination of both, and may include private networks and/or public networks, such as the Internet. - In some implementations, domains represent different uses for speech recognition output. For example, searching the World Wide Web (e.g., using a transcription as a query) may be considered to be one domain, and searching a media library may be considered a second domain. As additional examples, searching an application catalog may be considered a third domain, and providing a voice action (e.g., a voice command) may be considered a fourth domain. A domain may represent a one or more subject matter areas, such as music, movies, television, sports, and so on. In addition, or as an alternative, a domain may represent a particular type or collection of data to be searched, such as web pages for a web search, or an application catalog for a search of applications.
- In some instances, a single, general speech recognizer may be used for speech recognition across multiple different domains. However, some speech recognizers may not be able to take advantage of information about the particular domains or contexts in which a transcription will be used. In some instances, the correct transcription for an utterance may be included in a set of candidate transcriptions that the speech recognizer provides, but the correct transcription may not be indicated as most likely to be correct. For example, rather than being indicated as the most likely transcription, the correct transcription may be assigned the second or third position in a ranking of candidate transcriptions provided by the speech recognizer.
- For example, street names may occur infrequently in training data for a speech recognizer, causing the speech recognizer to consider street names as unlikely to be the best transcription in many instances. However, street names may be common in GPS navigation directions. When recognizing an utterance, knowing that a particular transcription represents a street name in a navigation domain may suggest that the particular transcription is more likely to be correct than the speech recognizer system has indicated. Accordingly, information about the domains or contexts in which recognized speech may be used can indicate whether a transcription is appropriate. For example, information about different domains may indicate whether a word or phrase is a plausible query for one or more of the different domains.
- To improve speech recognition accuracy, information about one or more domains can be used to re-score and re-rank candidate transcriptions using domain information. For example, after a set of candidate transcriptions are identified for an utterance, further processing may be done for a set of the n-best candidate transcriptions, where n is an integer (e.g., the 3, 5, or 10 most likely transcriptions). Rather than accepting the transcription that the speech recognizer indicates is most likely, the set of n-best candidate transcriptions is re-ranked using domain information that was not accessible to the speech recognizer system.
- For example, a language model that is broad enough to model naturally spoken queries may not be able to disambiguate between acoustically confusable sentences such as “I say” and “Ice Age.” However, if it is known that the input is a query meant for an entertainment search service for finding information about TV shows, music, and movies, it appears more likely that “Ice Age,” the title of a movie, is what the user meant.
- Each of the n-best candidate transcriptions can be provided as a query to a search service (or to multiple different search services). Search results for each of the n-best candidate transcriptions are analyzed, for example, to determine how well each set of search results matches the corresponding query that produced the search results. Information about the quality of search results can indicate a degree that each candidate transcription is a plausible search query, which may indicate a likelihood that the candidate transcription is correct. If a candidate transcription does not produce meaningful search results, the candidate transcription is unlikely to be a correct transcription. On the other hand, if a candidate transcription produces high-quality search results (e.g., results that match the candidate transcription well), the candidate transcription is more likely to be correct. The information about the quality of the search results may be used to re-rank the n-best candidate transcriptions and select the transcription for an utterance.
- In a similar manner, information about how often a candidate transcription is submitted as a query can indicate how plausible it is that a candidate transcription is a search query. For example, if the text of a candidate transcription is frequently submitted as a query in a particular domain (e.g., in a media search environment), the candidate transcription is more likely to be a correct transcription for a spoken query in that domain than other candidate transcriptions that are not submitted as queries as frequently in the domain. Information about query submission frequencies can also be used to re-rank the n-best candidate transcriptions, in addition to or instead of information about search results.
- As discussed further below, information about search results from one or more domains can be provided to a trained classifier, along with feature information from the speech recognizer. The classifier, which has been trained using domain information, can indicate a likelihood score for each candidate transcription. The highest-ranking candidate transcriptions may be ranked according to the classifier outputs rather than likelihood scores provided by the speech recognizer that identified the candidate transcriptions. Because the classifier receives domain information that is not available to the speech recognizer system, the outputs of the classifier may indicate likelihoods that are more accurate than those provided by the speech recognizer.
- In the example of
FIG. 1 , during stage (A), auser 102 speaks, and theclient device 110 records the utterance of theuser 102. For example, theuser 102 may speak words that represent a query for searching, for example, the World Wide Web, a media library, an application store catalog, or another collection of data. Theclient device 110 transmitsaudio data 112 that includes the user's utterance to thecomputing system 120 over thenetwork 118. - During stage (B), the
computing system 120 receives theaudio data 112 and obtains a set of candidate transcriptions. For example, theaudio data 112 may be provided to aspeech recognizer module 130 that may produces a word lattice indicating multiple different possible transcriptions for the utterance in theaudio data 112. Examples of word lattices are shown inFIGS. 3A and 3B . Thespeech recognizer module 130 may use an acoustic model and language model to generate a word lattice or otherwise identify candidate transcriptions. Thespeech recognizer module 130 may also indicate which of the candidate transcriptions thespeech recognizer module 130 considers most likely to be correct, for example, by providing likelihood scores and/or ranking for the candidate transcriptions. - During stage (C), the
computing system 120 identifies a set of highest-rankingcandidate transcriptions 135 from within the set of candidate transcriptions received. For example, using likelihood scores or ranking information from thespeech recognizer module 130, thecomputing system 120 may select n candidate transcriptions with the highest likelihoods, where n is an integer. In the illustrated example, the top five candidate transcriptions (e.g., the five that are indicated as most likely to be correct) are selected as the set of highest-rankingcandidate transcriptions 135. - During stage (D), the
computing system 120 provides each of thecandidate transcriptions 135 to at least onesearch service first search service 140 a for a first domain. Each of the candidate transcriptions 135 is also provided as a query to asecond search service 140 b for a second domain. The candidate transcriptions 135 are each provided as independent queries. For example, the transcription “write” is provided as a query to thesearch service 140 a, and is also provided to thesearch service 140 b, without information about theother candidate transcriptions 135. Similarly, the transcription “right” is provided individually to thesearch service 140 a and thesearch service 140 b, and so on. - Each of the
search services candidate transcriptions 135. Thefirst search service 140 a providessearch results 145 a, which includes five different sets of search results, one set for each of thecandidate transcriptions 135. Thesecond search service 140 b providessearch results 145 b, which also includes five different sets of search results, one set for each of thecandidate transcriptions 135. Because thesearch services first search service 140 a may be different from the search results 145 b from thesecond search service 140 b. For example, a search service for a news domain and a search service for a shopping domain may both provide search results that identify web pages. However, when the same query is provided to the search services, each search service may indicate different sets of web pages in the search results. - The
search services search services computing system 120. In other implementations, thecomputing system 120 and thesearch services search services computing system 120 or another computer system. - In some implementations, search results 145 a, 145 b from
different search service -
FIGS. 2A and 2B are diagrams that illustrate examples of search results.FIG. 2A illustrates a set of search results 200 received from thefirst search service 140 a in response to the query “right.” In the example, thefirst search service 140 a is associated with a web search domain, so each of the results identifies a web page identified as relevant to the query.FIG. 2B illustrates a set ofsearch results 250 received from thesecond search service 140 b in response to the query “right.” In the example, thesecond search service 140 b is associated with a music library search domain, so each of the results indicates a song identified as relevant to the query. In the same manner that thesearch services search results 200, 250 for the candidate transcription “right,” thesearch services other candidate transcriptions 135. - In some implementations, a search service for a domain may provide search results for each of multiple categories of information. For example, a media search domain may search within artist data (e.g., a list or database of artists known within the domain) and provide a first set of search results that are artist names. The media search domain may also search within album data and provide a second set of search results that are album names. Similarly, the media search domain may search within song data, movie data, television data, and application data to provide additional sets of search results that are, respectively, song titles, movie titles, television titles, and application names. In this manner, a domain may provide distinct sets of search results that identify the best matches to a query in each of multiple different categories of information.
- Referring again to
FIG. 1 , during stage (E), thecomputing system 120 uses afeature analysis module 150 to determinefeature scores 155 for one or more features of the search results 145 a, 145 b. The features may indicate different aspects or attributes of search results, or may indicate characteristics of different categories of search results. In addition to determining search result feature scores for the search results 145 a, 145 b, thefeature analysis module 150 may also determine feature scores for speech recognition features and semantic features. In addition, or as an alternative, thefeature analysis module 150 may determine other scores, such as query submission scores based on a frequency that a candidate transcription has been submitted as a query. - The feature scores 155 represent scores for search result features. One or more of the feature scores 155 may indicate a level of quality of
search results feature score 155 may indicate a confidence score, or quality score, a relevance score, an information retrieval score, or other measure that a search service provides. Feature scores may indicate how well the search results for a givencandidate transcription 135 match thecandidate transcription 135. The degree that a search result matches a candidate transcription may be based on, for example, whether the search result completely or partially matches the candidate transcription, whether words in the candidate transcription occur in the same sequence that the words occur in the candidate transcription, and/or a number of extra words in the search result or a number of words omitted from the search result relative to the candidate transcription. In some implementations, feature scores may indicate the number of search results returned for a query, and/or whether or not the top-ranked search result (e.g., the title of a document, media item, etc.) includes a complete string match for the candidate transcription. - In the example, the
feature analysis module 150 providesfeature scores 155 for each of three different features labeled F1, F2, and F3, but feature scores may be determined more or fewer features. Afeature score 155 is determined for each of the features F1, F2, and F3 for eachcandidate transcription 135. The feature scores 155 for eachcandidate transcription 135 are determined using the sets of search results corresponding to that candidate transcription. For example, the sets of search results received in response to the query “write” are analyzed to determine the feature scores 155 for the candidate transcription “write”; the sets of search results received in response to the query “right” are analyzed to determine the feature scores 155 for the candidate transcription “right”; and so on. Afeature score 155 may be determined based on an individual search result, such as the highest-ranked search result in a set. In some implementations, afeature score 155 may be based on multiple search results, for example, a feature score may indicate a total number of search results provided for a particular domain for a particular candidate transcription. - The features may include domain features corresponding to different domains. Because each domain may provide different types of search results, the characteristics of search results that relevant may be different for different domains. The feature scores for features associated with a particular domain are determined based on the search results received from the particular domain. For example, the features F1 and F2 may be features relevant to a first domain, and
feature scores 155 for these features may be determined using the search results 145 a from thefirst search service 140 a. For the candidate transcription “right,” domain scores for domain features relating to a web search domain may be determined based on the web search results 200 ofFIG. 2A . The feature F3 may be a feature relevant to a second domain, andfeature scores 155 for this feature may be determined using the search results 145 b from thesecond search service 140 b. For the same candidate transcription “right,” domain scores for domain features relating to a music search domain may be determined based on themusic search results 250 ofFIG. 2B . - The features can be selected to indicate likelihoods that a candidate transcription represents a navigational query. A navigational query may be a query that represents a request for a single, specific result, such as a particular web page or a particular song or movie title. Features that represent characteristics of individual search results (e.g., the top-ranked search result, or the second-ranked search result, etc.) may be indicators of the degree that a candidate transcription corresponds to a specific result.
- As an example, search results from a media catalog domain may indicate lists of which artists, albums, songs, movies, TV shows, and applications are considered most relevant to a candidate transcription. The domain features for the media catalog domain may represent characteristics of the highest-ranked search result within different media categories. For example, the domain features may include: (i) how well the best artist name search result matches the candidate transcription; (ii) how well the best album name search result matches the candidate transcription; (iii) how well the best song name search result matches the candidate transcription; (iv) how well the best movie title search result matches the candidate transcription; (v) how well the best TV show title search result matches the candidate transcription; and (vi) how well the best application name search result matches the candidate transcription. Additional domain features for the media catalog domain may represent the degree that the second-ranked search result (e.g., the second-highest ranked result) for each media category matches the corresponding candidate transcription.
- As another example, a media viewing domain may indicate search results that indicate lists of movies, TV shows, or video clips. The domain features for the media catalog domain may represent the degree that the top-ranked search result for each of the media categories of movies, TV shows, or video clips matches the corresponding candidate transcription. Similarly, the domain features may additionally or alternatively include features that represent the degree that the second-ranked search result for each of the media categories matches the corresponding candidate transcription.
- Typically, if a user has spoken, for example, a title of a movie, and the candidate transcription is correct, the highest-ranked movie result has a feature score indicating a high degree of match with the candidate transcription. The second-ranked movie result would tend to have a score indicating a lower degree of match, since the title is different from the title spoken by the user. By contrast, when feature scores indicate that the highest-ranked and second-ranked results are both poor matches for the candidate transcription, the candidate transcription may be unlikely to be correct.
- In some implementations, feature scores 155 may be determined for other features or aspects of search results. For example, a category score can be determined. For a certain search service, it may be known, for example, that 50% of search results provided are movies, 30% of search results provided are TV shows, and 20% of search results are web videos. In this example, the likelihood that a search result is a movie could be estimated as 50%. The category score can indicate a likelihood that a search result provided by the search service is expected to be an item in the same category as the highest-ranked search result. For example, it may be determined that the highest-ranked search result is in the “movie” category, and the category score can be indicative of the likelihood that a movie, rather than another form of media, will be presented as a search result. As another example, a next category score (e.g., a category score for the next search result) can also be determined. The next category probability score can indicate a likelihood that an item in the same category as the second-highest-ranked search result will be provided as a search result. The category score and the next category score can indicate how different the highest-ranked result and the second-highest-ranked result are. The larger the difference between the likelihoods indicated by the two category scores, the more likely the candidate transcription represents a navigational query.
- In addition to, or instead of determining feature scores for search results, the
feature analysis module 150 may determine scores for speech recognition features and semantic features. - Speech recognition features can provide information from the
speech recognizer module 130. For example, one speech recognition feature score may be a language model score, such as a score from an n-gram language model of thespeech recognizer model 130. Another example is a rank position of acandidate transcription 135 in the first pass ranking from the speech recognizer N-best list. The highest-rankedcandidate transcription 135, e.g., at “position 0,” may be considered to be the best hypothesis from thespeech recognizer module 130, and may be used as a baseline. Other examples of speech recognition feature scores include the number of tokens (e.g., words) in acandidate transcription 135, and a confidence score for acandidate transcription 135 from thespeech recognizer module 130. - Semantic feature scores may be generated based on pattern matching. Many voice queries are commands like “show me movies by Jim Carey”, or “open App”. If a
candidate transcription 135 matches a popular voice action pattern, it has a higher likelihood of being a correct recognition than a candidate transcription that does not match a known pattern. Each of thecandidate transcriptions 135 may be parsed with a finite-state-machine-based pattern matching system, and semantic parsing feature scores may be created for popular voice actions, such as an “Open app” action, or a “Play media” action (e.g., play movies, music, TV show). Semantic features may be expressed in binary values. For example, if a candidate transcription matches a predetermined pattern, or any of multiple predetermined patterns, a semantic feature score of “1” may be provided, and otherwise a semantic feature score of “0” may be provided. - During stage (F), for each of the
candidate transcriptions 135, thecomputing system 120 provides a feature vector to aclassifier module 160 and receives aclassifier output 165 from theclassifier module 160. The feature vector can includefeature scores 155 derived from the search results, as well as speech recognition feature scores and semantic feature scores. Theclassifier module 160 may be a classifier that has been trained to indicate a likelihood that a candidate transcription is correct based on, for example, the feature scores for search results corresponding to the candidate transcription. Theclassifier module 160 may be trained with, and may receive as input, sets of input data that include feature scores for the domain features for one or more domains. Through training, theclassifier module 160 may learn to discriminate among different combinations of feature scores to be able to indicate which feature score values and combinations of feature score values suggest a transcription is likely to be correct and which feature score values and combinations do not. - The feature vector for a
particular candidate transcription 135 can include the feature scores 155 determined from the search results 145 a, 145 b for theparticular candidate transcription 135. For example, for the transcription “right,” the feature vector may include the feature scores 155 of “0.8,” “0.7” and “0.8,” respectively, for the three domain features F1, F2, and F3 illustrated, which represent characteristics of the search results 200, 250. The feature vector may includefeature scores 155 for each of multiple domains. For example, the feature vector may include feature scores for a media catalog domain, feature scores for a voice action domain, and feature scores for a web search domain. In this manner theclassifier module 160 may be able to use information from multiple domains to provide better estimates of the likelihood of a transcription being correct. As a result, aclassifier module 160 that receives feature scores for multiple domains may provide more accurate results than, for example, a domain-specific classifier that only receives feature scores for a single domain. - The feature vector may also include speech feature scores, which may indicate characteristics of the candidate transcription, word lattice, or other output of the
speech recognizer module 130. For example, speech feature scores included in the feature vector may indicate one or more of, for example: (i) the position of the candidate transcription within the ranking provided by the speech recognizer module; (ii) a number of tokens (e.g., words) included in the candidate transcription; (iii) a language model score produced by a language model; and (iv) a speech confidence score, such as an overall confidence score generated by thespeech recognizer module 130. Other information about a candidate transcription may additionally or alternatively be provided in a feature vector. InFIG. 1 , the arrow from thecandidate transcriptions 135 to thefeature analysis module 150 indicates that information in the feature vector may include or be derived from output of thespeech recognizer module 130. - The
classifier module 160 can be a maximum entropy classifier. Theclassifier module 160 may use linear models to assess the feature vectors it receives. In addition, or as an alternative, theclassifier module 160 may use decision trees or other models or machine learning techniques. - In some implementations, scores for candidate transcriptions are determined and provided to the
classifier module 160 without requesting or obtaining search results for the candidate transcriptions. For example, the scores can include speech recognition feature scores based on output of thespeech recognizer module 130, and/or the scores can include semantic feature scores based on the text of the candidate transcription. In addition, or as an alternative, the scores may include query submission scores that are indicative of how often a candidate transcription, or a portion of the candidate transcription, has been submitted as a search query. For example, query submission data can be used to determine how often users have submitted entered the text of the candidate transcription in a search query field and submitted the text as a search query. Query submission scores can be domain-specific, for example, with query submission scores for each of multiple different domains indicating how often the candidate transcription is submitted as a query in the different domains. - During stage (G), the
computing system 120 receivesclassifier outputs 165 for thecandidate transcriptions 135 and re-ranks the set of highest-rankingcandidate transcriptions 135 according to the classifier outputs 165. Theclassifier module 160 may provide aclassifier output 165 for each candidate transcription 135 (e.g., for each feature vector received) that indicates a likelihood that thecandidate transcription 135 is correct. - In the illustrated example, the
computing system 120 produces anew ranking 170 that is different from original ranking indicated by thespeech recognizer module 130. In thenew ranking 170, the positions of the transcriptions “write” and “right” have been switched to indicate that the transcription “right” is predicted as most likely to be correct. Thecomputing system 120 selects the highest-rankedcandidate transcription 175 from thenew ranking 170 as the transcription for theaudio data 112. - During stage (H), the
computing system 120 provides the selectedcandidate transcription 175 to theclient device 110 over thenetwork 118. Theclient device 110 may then use the transcription in whatever manner theuser 102 instructs, for example, by submitting thetranscription 175 as search query, interpreting thetranscription 175 as a voice action, or using the transcription in another manner. - In some implementations, the feature vector provided to the
classifier module 160 includes feature scores from one or more three different feature categories: speech recognition features, semantic features, and search result features. Speech recognition features may include information produced by a language model for a given candidate transcription, such as a language model probability, a position within a ranking, a number of tokens, or a confidence score from the speech recognizer. - Semantic features may indicate information about of pattern matching analysis, for example, matching a candidate transcription to a grammar. For example, if a candidate transcription matches a popular voice action pattern, it has a better chance to be correct recognition. Many voice queries are commands, such as “show me movies by Jim Carey” or “open website.” A candidate transcription may be parsed with a finite-state-machine-based pattern matching system to determine an extent that the candidate transcription matches different grammars for, for example, opening an application, playing a movie, requesting navigation directions, and so on. The feature score for each semantic feature may indicate a degree of match between the candidate transcription and a different grammar.
- Search result features represent characteristics of search results. In particular, feature scores for a particular candidate transcription represent characteristics of the search results obtained when the particular candidate transcription is used as a query. The search result features may indicate a degree to which a candidate transcription is a navigational query in a particular category of search results. For example, various categories of search results from a domain may include TV shows, video clips, movie titles, application names, album names, and song titles. The various search result feature scores for this domain may respectively indicate, for example, how likely a candidate transcription is to be a particular TV show, how likely the candidate transcription is to be a movie title, and so on. This likelihood can be determined as a degree to which the candidate transcription matches the search result. For example, if the candidate transcription matches the top-ranked movie title in a set of search results, it is very likely that the candidate transcription is a navigational query specifying the movie title.
- Feature scores may be determined for other search result features as well. For example, a feature score may indicate a number of search results returned for a given query. In general, a query that returns a large number of results is more likely to be a correct transcription than a query that returns fewer results. Another feature score may indicate whether the top search result has a complete string match to the candidate transcription/query that produced the result. Another feature score may indicate a degree to which a query is navigational, for example, as determined based on a distribution of user interactions with the search result. For example, if a large percentage of users click the top result in response to a query, the query may represent a navigational request.
- Although the
candidate transcriptions 135 illustrated inFIG. 1 are individual words, in some implementations, one or more of the transcriptions may be phrases including multiple words. For example, theaudio data 112 may represent an entire query spoken by the user, and each of the transcriptions may be a candidate transcription for theaudio data 112 as a whole. - The example of
FIG. 1 shows processing using search results from two different domains and using the features of the two different domains. Candidate transcriptions may be evaluated using features of search results from more domains or fewer domains than illustrated. In some implementations, results are obtained for only a single domain, and the classifier receives only feature scores for a single domain. For example, the classifier may be a domain-specific classifier. - Although the
search services -
FIG. 3A is an example of aword lattice 300 that may be provided by a speech recognizer system. Theword lattice 300 represents multiple possible combinations of words that may form different candidate transcriptions for an utterance. - The
word lattice 300 includes one or more nodes 302 a-g that correspond to the possible boundaries between words. Theword lattice 300 includes multiple edges 304 a-1 for the possible words in the transcription hypotheses that result from theword lattice 300. In addition, each of the edges 304 a-1 can have one or more weights or probabilities of that edge being the correct edge from the corresponding node. The weights are determined by the speech recognizer module system and can be based on, for example, a confidence in the match between the speech data and the word for that edge and how well the word fits grammatically and/or lexically with other words in theword lattice 300. - For example, initially, the most probable path through the
word lattice 300 may include theedges edges - Each pair of nodes may have one or more paths corresponding to the alternate words in the various candidate transcriptions. For example, the initial most probable path between the node pair beginning at the
node 302 a and ending at thenode 302 c is theedge 304 c “we're”. This path has alternate paths that include the edges 304 a-b “we are” and theedge 304 d “deer”. -
FIG. 3B is an example of ahierarchical word lattice 350 that may be provided by a speech recognizer system. Theword lattice 350 includes nodes 352 a-1 that represent the words that make up the various candidate transcriptions for an utterance. The edges between the nodes 352 a-1 show that the possible candidate transcriptions include: (1) thenodes nodes nodes nodes nodes nodes - Again, the edges between the nodes 352 a-1 may have associated weights or probabilities based on the confidence in the speech recognition and the grammatical/lexical analysis of the resulting text. In this example, “we're coming about 11:30” may currently be the best hypothesis and “deer hunting scouts 7:30” may be the next best hypothesis. One or more divisions 354 a-d can be made in the
word lattice 350 that group a word and its alternates together. For example, thedivision 354 a includes the word “we're” and the alternates “we are” and “deer”. Thedivision 354 b includes the word “coming” and the alternates “come at” and “hunting”. Thedivision 354 c includes the word “about” and the alternate “scouts” and thedivision 354 d includes the word “11:30” and the alternate “7:30”. -
FIG. 4 is a flow diagram that illustrates an example of aprocess 400 for speech recognition using domain knowledge. The operations of theprocess 400 may be performed by one or more computing systems, such as thecomputing system 120 ofFIG. 1 . - Data that indicates multiple candidate transcriptions for an utterance is received (402). For example, the data can include a word lattice that indicates multiple alternative translations for the utterance. The data can indicate likelihoods that the various candidate transcriptions are correct. A predefined number of candidate transcriptions, such as the n-best candidate transcriptions as indicated by the speech recognizer, may be identified and may be evaluated.
- Data relating to use of a particular candidate transcription as a query is received (404). In some implementations, the data indicates content that a search service identifies as relevant to the candidate transcription. For example, the particular candidate transcription can be provided to a search service as a query, and search results for the particular candidate transcription may be received. The particular candidate transcription may be provided to search services for multiple different domains, and a set of search results may be received for each of the different domains.
- In some implementations, the data relating to use of a particular candidate transcription as a query indicates a frequency that the particular candidate transcription, or a portion of the particular candidate transcription, has been submitted as a query. As an example, the frequency can indicate how often a query exactly matching the text of the candidate transcription is submitted. As another example, the frequency can indicate how often the candidate transcription occurs within submitted queries, even if the query includes additional text.
- The data can indicate how often the candidate transcription was submitted with respect to a specific domain. In some implementations, the data indicates frequencies that the candidate transcription was submitted to search services for each of multiple different domains. For example, the data can indicate a first frequency of submission for a first domain, such as a web search domain, and the data can indicate a second frequency of submission for a second domain, such as a media search domain. Alternatively, the frequency can be a measure of query submissions that reflects submissions in multiple domains. A frequency can be expressed in a variety of different forms, including, for example, a number of submissions or other measure of query submissions, an measure of submissions within a particular time period (e.g., within the previous week, month, or year), and a relative rate of submission or measure of popularity relative to other queries.
- A score based on the data relating to use of a particular candidate transcription as a query is provided to a classifier (406). The classifier may be a classifier that has been trained to indicate a likelihood that a candidate transcription is correct based at least partially on received scores. In some implementations, the classifier is a maximum entropy classifier.
- In some implementations, the classifier is trained to indicate a likelihood that a candidate transcription is correct based on scores for semantic features, speech recognition features, search result features, and/or query submission frequencies. The classifier may be a classifier that has been trained using (i) a first set of scores corresponding to first features relevant to a first domain and (ii) a second set of scores for second features relevant to a second domain, where the second features are different from the first features.
- In some implementations, when a search service identifies content relevant to the particular candidate transcription, a score is determined based on the content. The content may include a set of search results, and the score may be a value that indicates a characteristic of one or more of the search results. In some implementations, the score may a relevance score or other score that is generated by the search service. The score may indicate a degree that the content matches the candidate transcription. For example, the score may indicate a degree that a highest-ranked search result in a set of search results matches the candidate transcription. As another example, the score may indicate a degree that a second-highest-ranked search result in a set of search results matches the candidate transcription. In some domains, different features represent different types or categories of search results within the domain. For example, one feature may represent a characteristic of search results that are artist names, another feature may represent a characteristic of search results that are song titles, and so on.
- The score may be provided to the classifier in a feature vector that includes scores for domain-specific features of different domains. For example, a first domain may be associated with a first set of features, and the feature scores for the first set of features may be determined based on search results from the first domain. A second domain may be associated with a second, different set of features, and the scores for the second set of features may be determined based on search results from the second domain. The feature vector may also include scores for speech features, for example: a position of the candidate transcription within the ranking provided by the speech recognizer module; a number of tokens (e.g., words) included in the candidate transcription; a language model score produced by a language model; and a speech recognizer confidence score. The feature vector may also include scores for semantic features.
- In addition or as an alternative to scores based on search results, one or more query submission scores can be determined based on query submission data. For example, a score indicative of the frequency that the candidate transcription has been submitted as a query in a particular domain can be determined and input to the classifier. In some implementations, query submission scores indicative of submission frequencies in different domains can be determined and input to the classifier.
- A classifier output for the candidate transcription is received from the classifier (408). The classifier output may be a score that indicates a likelihood that the candidate transcription is a correct transcription, in view of the scores associated with the candidate transcription and use of the candidate transcription as a query. The operations of (404)-(408) may be repeated for each of multiple candidate transcriptions for the utterance. For example, the operations may be repeated for each of the n-best candidate transcriptions to obtain a classifier output for each of the n-best candidate transcriptions
- From among the multiple candidate transcriptions, one or more candidate transcriptions are selected based on the classifier outputs (410). For example, the candidate transcription having the classifier output that indicates the highest likelihood of correctness may be selected. The multiple candidate transcriptions may be ranked based on the classifier outputs, and the highest-ranked candidate transcription may be selected. The selected candidate transcription may be provided to a client device or other system as a transcription for the utterance.
- In some implementations, search results are received from search services for different domains, and scores may be determined for features of different domains. For example, for a music search domain, a score may indicate a degree that a top-ranked search result from a set of song titles matches the candidate transcription. For a web search domain, a score may indicate a degree that a top-ranked search result from among a set of web pages matches the candidate transcription. The scores for multiple domains may be provided in the same feature vector so the classifier can evaluate the candidate transcription based on different aspects of search results from different domains.
- In some implementations, the score is a first feature score determined for a first set of content identified by the search service for a first domain. Data indicating second content that a search service for a second domain identifies as relevant to the candidate transcription is also received, and the second content is different from the first content. A second feature score for a second feature of the second content is received. The second feature may be different from the first feature. The first feature score and the second feature score are both provided to the classifier, for example, as part of a single feature vector. The classifier output that is received may be based on the first feature score and the second feature score.
-
FIG. 5 is a flow diagram that illustrates an example of a process 500 for training a classifier. The operations of the process 500 may be performed by one or more computing systems, such as thecomputing system 120 ofFIG. 1 . - Training data is received (502). The training data can indicate a set of multiple candidate transcriptions for each of multiple training utterances. For example, the training data can indicate a predefined number of candidate transcriptions for each utterance, such as the n-best candidate transcriptions (e.g., top 3, 5, or 10 transcriptions) as indicated by a speech recognizer. The training data may also indicate, for each of the candidate transcriptions, a likelihood score output by the speech recognizer.
- The candidate transcriptions are each labeled (504). For example, the correct transcription for each training utterance is labeled as being correct. The other candidate transcriptions for each utterance are labeled as incorrect. In this manner, the evaluating candidate transcriptions may be done within a binary classification framework, e.g., each candidate transcription is either correct or incorrect.
- A score related to use of the candidate transcription as a query is determined (506). For example, the score can be indicative of a frequency that the candidate transcription has been submitted as a query. In some implementations, the score can be determined based on search results for the candidate transcription. A particular candidate transcription can be provided to a search service as a query, and search results for the particular candidate transcription may be received. The particular candidate transcription may be provided to search services for multiple different domains, and a set of search results may be received for each of the different domains.
- In some implementations, the score may be a relevance score or other score that is generated by the search service. The score may indicate a degree that the content matches the candidate transcription. For example, the score may indicate a degree that a highest-ranked search result in a set of search results matches the candidate transcription. As another example, the score may indicate a degree that a second-highest-ranked search result in a set of search results matches the candidate transcription. For some domains, different features represent characteristics of different types or categories of search results in the domain. For example, one feature may represent a characteristic of search results that are artist names, another feature may represent a characteristic of search results that are song titles, and so on.
- A classifier is trained using the score (508). For example, the classifier may be provided the label for the candidate transcription and a feature vector that includes the determined score. The feature vector may include scores for multiple features associated with a domain. In addition, the feature vector may include scores for multiple different domains, where the scores for each domain are determined based on a set of search results from that domain. The feature vector may include any of the scores discussed above, including, for example, scores for speech recognition features, semantic features, search result features, and query submission frequencies.
- Training the classifier using information about multiple different domains can provide a unified framework that is more robust than, for example, a domain-specific classifier that takes into account characteristics of only a single domain. To train the classifier, weights or other parameters of the classifier may be adjusted to improve the classifier's ability to estimate the probability that a feature vector represents a correct transcription.
- For each candidate transcription, multiple scores representing different characteristics may be determined and used to train the classifier. For example, one or multiple scores for semantic features, search result features, and/or speech recognition features may be determined and used to train the classifier. In addition, or as an alternative, one or more scores indicative of query submission frequencies may be determined and used to train the classifier. After training is complete, the same types of scores provided to the classifier during training may be input to the trained classifier to evaluate candidate transcriptions.
- The classifier may be trained using any of various machine learning techniques. In some implementations, the classifier is a maximum entropy classifier. The operations of (506)-(508) may be repeated using scores for each of the different candidate transcriptions for each of the training utterances. As a result, the classifier may learn to indicate a likelihood that a candidate transcription is correct based on the scores it receives. The output of the trained classifier can indicate, for a given feature vector or other set of input data, a probability that the candidate transcription corresponding to the feature vector is correct.
- In some implementations, the classifier uses conditional maximum entropy (ME) modeling. ME modeling may involve identifying a distribution that has the maximum entropy subject to known constraints expressed as features. Using a maximum entropy model allows arbitrary constraints to be used as features. This allows language modeling to capture short term dependency such as n-grams, long sentence dependency such as syntax, and semantics. The ME model may combine arbitrary context features from search results, syntax features from parsing, and speech features.
- For the equations below, a feature, fi(c, h), can be a real-valued function of the candidate transcription or hypothesis h and a target class c. Maximum entropy allows the model distribution to be restricted to have the same expected value for this feature as its empirical value from training data, H. Thus, the learned conditional distribution P(c|h) must have the property:
-
- The maximum entropy distribution may be expressed as:
-
- where Z(h) is a normalization factor:
-
- In the above equations, the candidate transcription or hypothesis, h, can be a context that is represented by the feature vector for a particular candidate transcription. The feature functions, fi(c, h), which are typically binary, define constraints for the model. During ME training, optimal weights, λi, corresponding to features fi(c, h) are learned so that the log-likelihood of the training data, H, is maximized. The weights, i, are learned via improved iterative scaling algorithm, or other algorithms such as a conjugate gradient descent algorithm or a limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm.
- Embodiments of the invention and all of the functional operations described in this specification may be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the invention may be implemented as one or more computer program products, i.e., one or more modules of computer program instructions encoded on a computer-readable storage device for execution by, or to control the operation of, data processing apparatus. The computer-readable storage device may be, for example, a machine-readable storage substrate, a memory device, a composition of matter producing a machine-readable propagated signal, or a combination of one or more of them. The computer-readable storage device may be a non-transitory machine-readable medium. The term “data processing apparatus” encompasses all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus may include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them. A propagated signal is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.
- A computer program (also known as a program, software, software application, script, or code) may be written in any form of programming language, including compiled or interpreted languages, and it may be deployed in any form, including as a standalone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program may be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program may be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- The processes and logic flows described in this specification may be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows may also be performed by, and apparatus may also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer may be embedded in another device, e.g., a tablet computer, a mobile telephone, a personal digital assistant (PDA), a mobile audio player, a Global Positioning System (GPS) receiver, to name just a few. Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks. The processor and the memory may be supplemented by, or incorporated in, special purpose logic circuitry.
- To provide for interaction with a user, embodiments of the invention may be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user may provide input to the computer. Other kinds of devices may be used to provide for interaction with a user as well; for example, feedback provided to the user may be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input.
- Embodiments of the invention may be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user may interact with an implementation of the invention, or any combination of one or more such back end, middleware, or front end components. The components of the system may be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), e.g., the Internet.
- The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- While this specification contains many specifics, these should not be construed as limitations on the scope of the invention or of what may be claimed, but rather as descriptions of features specific to particular embodiments of the invention. Certain features that are described in this specification in the context of separate embodiments may also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment may also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination may in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
- Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems may generally be integrated together in a single software product or packaged into multiple software products.
- Thus, particular embodiments of the invention have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims may be performed in a different order and still achieve desirable results.
Claims (24)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/048,199 US9646606B2 (en) | 2013-07-03 | 2013-10-08 | Speech recognition using domain knowledge |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201361842550P | 2013-07-03 | 2013-07-03 | |
US201361843710P | 2013-07-08 | 2013-07-08 | |
US14/048,199 US9646606B2 (en) | 2013-07-03 | 2013-10-08 | Speech recognition using domain knowledge |
Publications (2)
Publication Number | Publication Date |
---|---|
US20150012271A1 true US20150012271A1 (en) | 2015-01-08 |
US9646606B2 US9646606B2 (en) | 2017-05-09 |
Family
ID=52133399
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/048,199 Active 2033-11-24 US9646606B2 (en) | 2013-07-03 | 2013-10-08 | Speech recognition using domain knowledge |
Country Status (1)
Country | Link |
---|---|
US (1) | US9646606B2 (en) |
Cited By (173)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20150026179A1 (en) * | 2013-07-22 | 2015-01-22 | Kabushiki Kaisha Toshiba | Electronic device and method for processing clips of documents |
US20150161985A1 (en) * | 2013-12-09 | 2015-06-11 | Google Inc. | Pronunciation verification |
US20150335807A1 (en) * | 2013-01-18 | 2015-11-26 | University Of Pittsburgh - Of The Commonwealth System Of Higher Education | Removal of carbon dioxide via dialysis |
US20150348551A1 (en) * | 2014-05-30 | 2015-12-03 | Apple Inc. | Multi-command single utterance input method |
CN105206267A (en) * | 2015-09-09 | 2015-12-30 | 中国科学院计算技术研究所 | Voice recognition error correction method with integration of uncertain feedback and system thereof |
US20160358606A1 (en) * | 2015-06-06 | 2016-12-08 | Apple Inc. | Multi-Microphone Speech Recognition Systems and Related Techniques |
US20170018271A1 (en) * | 2015-07-13 | 2017-01-19 | Microsoft Technology Licensing, Llc | Delayed binding in response selection during input understanding processing |
US9668024B2 (en) | 2014-06-30 | 2017-05-30 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US9865265B2 (en) | 2015-06-06 | 2018-01-09 | Apple Inc. | Multi-microphone speech recognition systems and related techniques |
US9865248B2 (en) | 2008-04-05 | 2018-01-09 | Apple Inc. | Intelligent text-to-speech conversion |
WO2018057427A1 (en) * | 2016-09-21 | 2018-03-29 | Intel Corporation | Syntactic re-ranking of potential transcriptions during automatic speech recognition |
US9934775B2 (en) | 2016-05-26 | 2018-04-03 | Apple Inc. | Unit-selection text-to-speech synthesis based on predicted concatenation parameters |
US9966060B2 (en) | 2013-06-07 | 2018-05-08 | Apple Inc. | System and method for user-specified pronunciation of words for speech synthesis and recognition |
US9972304B2 (en) | 2016-06-03 | 2018-05-15 | Apple Inc. | Privacy preserving distributed evaluation framework for embedded personalized systems |
US9971774B2 (en) | 2012-09-19 | 2018-05-15 | Apple Inc. | Voice-based media searching |
US9986419B2 (en) | 2014-09-30 | 2018-05-29 | Apple Inc. | Social reminders |
US20180211652A1 (en) * | 2017-01-26 | 2018-07-26 | Samsung Electronics Co., Ltd. | Speech recognition method and apparatus |
US10043516B2 (en) | 2016-09-23 | 2018-08-07 | Apple Inc. | Intelligent automated assistant |
US10049668B2 (en) | 2015-12-02 | 2018-08-14 | Apple Inc. | Applying neural network language models to weighted finite state transducers for automatic speech recognition |
US10049675B2 (en) | 2010-02-25 | 2018-08-14 | Apple Inc. | User profiling for voice input processing |
US10049663B2 (en) | 2016-06-08 | 2018-08-14 | Apple, Inc. | Intelligent automated assistant for media exploration |
US10067938B2 (en) | 2016-06-10 | 2018-09-04 | Apple Inc. | Multilingual word prediction |
US10079014B2 (en) | 2012-06-08 | 2018-09-18 | Apple Inc. | Name recognition system |
US10083690B2 (en) | 2014-05-30 | 2018-09-25 | Apple Inc. | Better resolution when referencing to concepts |
US10089072B2 (en) | 2016-06-11 | 2018-10-02 | Apple Inc. | Intelligent device arbitration and control |
US10108612B2 (en) | 2008-07-31 | 2018-10-23 | Apple Inc. | Mobile device having human language translation capability with positional feedback |
US10169329B2 (en) | 2014-05-30 | 2019-01-01 | Apple Inc. | Exemplar-based natural language processing |
US10192552B2 (en) | 2016-06-10 | 2019-01-29 | Apple Inc. | Digital assistant providing whispered speech |
US10223066B2 (en) | 2015-12-23 | 2019-03-05 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10249300B2 (en) | 2016-06-06 | 2019-04-02 | Apple Inc. | Intelligent list reading |
US10248452B2 (en) * | 2016-05-20 | 2019-04-02 | Microsoft Technology Licensing, Llc | Interaction framework for executing user instructions with online services |
US10269345B2 (en) | 2016-06-11 | 2019-04-23 | Apple Inc. | Intelligent task discovery |
US10283110B2 (en) | 2009-07-02 | 2019-05-07 | Apple Inc. | Methods and apparatuses for automatic speech recognition |
US10297253B2 (en) | 2016-06-11 | 2019-05-21 | Apple Inc. | Application integration with a digital assistant |
US10303715B2 (en) | 2017-05-16 | 2019-05-28 | Apple Inc. | Intelligent automated assistant for media exploration |
US10311144B2 (en) | 2017-05-16 | 2019-06-04 | Apple Inc. | Emoji word sense disambiguation |
US10311871B2 (en) | 2015-03-08 | 2019-06-04 | Apple Inc. | Competing devices responding to voice triggers |
US10318871B2 (en) | 2005-09-08 | 2019-06-11 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
US10332518B2 (en) | 2017-05-09 | 2019-06-25 | Apple Inc. | User interface for correcting recognition errors |
US20190214016A1 (en) * | 2018-01-05 | 2019-07-11 | Nuance Communications, Inc. | Routing system and method |
US10354011B2 (en) | 2016-06-09 | 2019-07-16 | Apple Inc. | Intelligent automated assistant in a home environment |
US10356243B2 (en) | 2015-06-05 | 2019-07-16 | Apple Inc. | Virtual assistant aided communication with 3rd party service in a communication session |
US10366158B2 (en) | 2015-09-29 | 2019-07-30 | Apple Inc. | Efficient word encoding for recurrent neural network language models |
US10381016B2 (en) | 2008-01-03 | 2019-08-13 | Apple Inc. | Methods and apparatus for altering audio output signals |
US10395654B2 (en) | 2017-05-11 | 2019-08-27 | Apple Inc. | Text normalization based on a data-driven learning network |
US10403283B1 (en) | 2018-06-01 | 2019-09-03 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US10403278B2 (en) | 2017-05-16 | 2019-09-03 | Apple Inc. | Methods and systems for phonetic matching in digital assistant services |
US10410637B2 (en) | 2017-05-12 | 2019-09-10 | Apple Inc. | User-specific acoustic models |
US10417266B2 (en) | 2017-05-09 | 2019-09-17 | Apple Inc. | Context-aware ranking of intelligent response suggestions |
US10417405B2 (en) | 2011-03-21 | 2019-09-17 | Apple Inc. | Device access using voice authentication |
US10431204B2 (en) | 2014-09-11 | 2019-10-01 | Apple Inc. | Method and apparatus for discovering trending terms in speech requests |
US10429817B2 (en) * | 2016-12-19 | 2019-10-01 | Honeywell International Inc. | Voice control of components of a facility |
US10438595B2 (en) | 2014-09-30 | 2019-10-08 | Apple Inc. | Speaker identification and unsupervised speaker adaptation techniques |
US10445429B2 (en) | 2017-09-21 | 2019-10-15 | Apple Inc. | Natural language understanding using vocabularies with compressed serialized tries |
US10446137B2 (en) | 2016-09-07 | 2019-10-15 | Microsoft Technology Licensing, Llc | Ambiguity resolving conversational understanding system |
US10446143B2 (en) | 2016-03-14 | 2019-10-15 | Apple Inc. | Identification of voice inputs providing credentials |
US10453443B2 (en) | 2014-09-30 | 2019-10-22 | Apple Inc. | Providing an indication of the suitability of speech recognition |
US10474753B2 (en) | 2016-09-07 | 2019-11-12 | Apple Inc. | Language identification using recurrent neural networks |
US10482874B2 (en) | 2017-05-15 | 2019-11-19 | Apple Inc. | Hierarchical belief states for digital assistants |
US10490187B2 (en) | 2016-06-10 | 2019-11-26 | Apple Inc. | Digital assistant providing automated status report |
US10496705B1 (en) | 2018-06-03 | 2019-12-03 | Apple Inc. | Accelerated task performance |
US10509862B2 (en) | 2016-06-10 | 2019-12-17 | Apple Inc. | Dynamic phrase expansion of language input |
US10521466B2 (en) | 2016-06-11 | 2019-12-31 | Apple Inc. | Data driven natural language event detection and classification |
US10529332B2 (en) | 2015-03-08 | 2020-01-07 | Apple Inc. | Virtual assistant activation |
US10567477B2 (en) | 2015-03-08 | 2020-02-18 | Apple Inc. | Virtual assistant continuity |
US10572810B2 (en) | 2015-01-07 | 2020-02-25 | Microsoft Technology Licensing, Llc | Managing user interaction for input understanding determinations |
US10592604B2 (en) | 2018-03-12 | 2020-03-17 | Apple Inc. | Inverse text normalization for automatic speech recognition |
US10593346B2 (en) | 2016-12-22 | 2020-03-17 | Apple Inc. | Rank-reduced token representation for automatic speech recognition |
US10636424B2 (en) | 2017-11-30 | 2020-04-28 | Apple Inc. | Multi-turn canned dialog |
US10643611B2 (en) | 2008-10-02 | 2020-05-05 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US10657328B2 (en) | 2017-06-02 | 2020-05-19 | Apple Inc. | Multi-task recurrent neural network architecture for efficient morphology handling in neural language modeling |
US10657961B2 (en) | 2013-06-08 | 2020-05-19 | Apple Inc. | Interpreting and acting upon commands that involve sharing information with remote devices |
US10671428B2 (en) | 2015-09-08 | 2020-06-02 | Apple Inc. | Distributed personal assistant |
US10684703B2 (en) | 2018-06-01 | 2020-06-16 | Apple Inc. | Attention aware virtual assistant dismissal |
US10691473B2 (en) | 2015-11-06 | 2020-06-23 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US10699717B2 (en) | 2014-05-30 | 2020-06-30 | Apple Inc. | Intelligent assistant for home automation |
US10706841B2 (en) | 2010-01-18 | 2020-07-07 | Apple Inc. | Task flow identification based on user intent |
US10714117B2 (en) | 2013-02-07 | 2020-07-14 | Apple Inc. | Voice trigger for a digital assistant |
US10726832B2 (en) | 2017-05-11 | 2020-07-28 | Apple Inc. | Maintaining privacy of personal information |
US10733993B2 (en) | 2016-06-10 | 2020-08-04 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US10733982B2 (en) | 2018-01-08 | 2020-08-04 | Apple Inc. | Multi-directional dialog |
US10733375B2 (en) | 2018-01-31 | 2020-08-04 | Apple Inc. | Knowledge-based framework for improving natural language understanding |
US10741185B2 (en) | 2010-01-18 | 2020-08-11 | Apple Inc. | Intelligent automated assistant |
US10747498B2 (en) | 2015-09-08 | 2020-08-18 | Apple Inc. | Zero latency digital assistant |
US10748546B2 (en) | 2017-05-16 | 2020-08-18 | Apple Inc. | Digital assistant services based on device capabilities |
US10755051B2 (en) | 2017-09-29 | 2020-08-25 | Apple Inc. | Rule-based natural language processing |
US10755703B2 (en) | 2017-05-11 | 2020-08-25 | Apple Inc. | Offline personal assistant |
CN111583907A (en) * | 2020-04-15 | 2020-08-25 | 北京小米松果电子有限公司 | Information processing method, device and storage medium |
US10769385B2 (en) | 2013-06-09 | 2020-09-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
US10777203B1 (en) * | 2018-03-23 | 2020-09-15 | Amazon Technologies, Inc. | Speech interface device with caching component |
US10789959B2 (en) | 2018-03-02 | 2020-09-29 | Apple Inc. | Training speaker recognition models for digital assistants |
US10789945B2 (en) | 2017-05-12 | 2020-09-29 | Apple Inc. | Low-latency intelligent automated assistant |
US10791176B2 (en) | 2017-05-12 | 2020-09-29 | Apple Inc. | Synchronization and task delegation of a digital assistant |
US10795541B2 (en) | 2009-06-05 | 2020-10-06 | Apple Inc. | Intelligent organization of tasks items |
US10810274B2 (en) | 2017-05-15 | 2020-10-20 | Apple Inc. | Optimizing dialogue policy decisions for digital assistants using implicit feedback |
US10818288B2 (en) | 2018-03-26 | 2020-10-27 | Apple Inc. | Natural assistant interaction |
US10839159B2 (en) | 2018-09-28 | 2020-11-17 | Apple Inc. | Named entity normalization in a spoken dialog system |
US10861436B1 (en) * | 2016-08-24 | 2020-12-08 | Gridspace Inc. | Audio call classification and survey system |
US10892996B2 (en) | 2018-06-01 | 2021-01-12 | Apple Inc. | Variable latency device coordination |
US10909331B2 (en) | 2018-03-30 | 2021-02-02 | Apple Inc. | Implicit identification of translation payload with neural machine translation |
US10928918B2 (en) | 2018-05-07 | 2021-02-23 | Apple Inc. | Raise to speak |
US10984780B2 (en) | 2018-05-21 | 2021-04-20 | Apple Inc. | Global semantic word embeddings using bi-directional recurrent neural networks |
US11010561B2 (en) | 2018-09-27 | 2021-05-18 | Apple Inc. | Sentiment prediction from textual data |
US11010550B2 (en) | 2015-09-29 | 2021-05-18 | Apple Inc. | Unified language modeling framework for word prediction, auto-completion and auto-correction |
US11010127B2 (en) | 2015-06-29 | 2021-05-18 | Apple Inc. | Virtual assistant for media playback |
US11023513B2 (en) | 2007-12-20 | 2021-06-01 | Apple Inc. | Method and apparatus for searching using an active ontology |
US11025565B2 (en) | 2015-06-07 | 2021-06-01 | Apple Inc. | Personalized prediction of responses for instant messaging |
US20210174795A1 (en) * | 2019-12-10 | 2021-06-10 | Rovi Guides, Inc. | Systems and methods for providing voice command recommendations |
US11048473B2 (en) | 2013-06-09 | 2021-06-29 | Apple Inc. | Device, method, and graphical user interface for enabling conversation persistence across two or more instances of a digital assistant |
US11069336B2 (en) | 2012-03-02 | 2021-07-20 | Apple Inc. | Systems and methods for name pronunciation |
US11070949B2 (en) | 2015-05-27 | 2021-07-20 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on an electronic device with a touch-sensitive display |
US11068656B2 (en) * | 2019-04-10 | 2021-07-20 | International Business Machines Corporation | Displaying text classification anomalies predicted by a text classification model |
US11080012B2 (en) | 2009-06-05 | 2021-08-03 | Apple Inc. | Interface for a virtual digital assistant |
US11100408B2 (en) * | 2015-12-06 | 2021-08-24 | Xeeva, Inc. | System and/or method for generating clean records from imperfect data using model stack(s) including classification model(s) and confidence model(s) |
US11107475B2 (en) * | 2019-05-09 | 2021-08-31 | Rovi Guides, Inc. | Word correction using automatic speech recognition (ASR) incremental response |
US11120372B2 (en) | 2011-06-03 | 2021-09-14 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US11127397B2 (en) | 2015-05-27 | 2021-09-21 | Apple Inc. | Device voice control |
US11133008B2 (en) | 2014-05-30 | 2021-09-28 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US11140099B2 (en) | 2019-05-21 | 2021-10-05 | Apple Inc. | Providing message response suggestions |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11170166B2 (en) | 2018-09-28 | 2021-11-09 | Apple Inc. | Neural typographical error modeling via generative adversarial networks |
US11204787B2 (en) | 2017-01-09 | 2021-12-21 | Apple Inc. | Application integration with a digital assistant |
US11217251B2 (en) | 2019-05-06 | 2022-01-04 | Apple Inc. | Spoken notifications |
US11227589B2 (en) | 2016-06-06 | 2022-01-18 | Apple Inc. | Intelligent list reading |
US11231904B2 (en) | 2015-03-06 | 2022-01-25 | Apple Inc. | Reducing response latency of intelligent automated assistants |
US11237797B2 (en) | 2019-05-31 | 2022-02-01 | Apple Inc. | User activity shortcut suggestions |
US11262978B1 (en) * | 2019-06-19 | 2022-03-01 | Amazon Technologies, Inc. | Voice-adapted reformulation of web-based answers |
US11269678B2 (en) | 2012-05-15 | 2022-03-08 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US20220083900A1 (en) * | 2020-09-11 | 2022-03-17 | Fortinet, Inc. | Intelligent vector selection by identifying high machine-learning model skepticism |
US11281993B2 (en) | 2016-12-05 | 2022-03-22 | Apple Inc. | Model and ensemble compression for metric learning |
US11289073B2 (en) | 2019-05-31 | 2022-03-29 | Apple Inc. | Device text to speech |
US11289086B2 (en) * | 2019-11-01 | 2022-03-29 | Microsoft Technology Licensing, Llc | Selective response rendering for virtual assistants |
US11301477B2 (en) | 2017-05-12 | 2022-04-12 | Apple Inc. | Feedback analysis of a digital assistant |
US11307752B2 (en) | 2019-05-06 | 2022-04-19 | Apple Inc. | User configurable task triggers |
US11314370B2 (en) | 2013-12-06 | 2022-04-26 | Apple Inc. | Method for extracting salient dialog usage from live data |
US11350253B2 (en) | 2011-06-03 | 2022-05-31 | Apple Inc. | Active transport based notifications |
US11344656B2 (en) | 2017-05-22 | 2022-05-31 | Advitos Gmbh | Methods and systems for removing carbon dioxide |
US11348573B2 (en) | 2019-03-18 | 2022-05-31 | Apple Inc. | Multimodality in digital assistant systems |
US11351291B2 (en) | 2016-03-14 | 2022-06-07 | Advitos Gmbh | Systems or apparatuses and methods for performing dialysis |
US11360641B2 (en) | 2019-06-01 | 2022-06-14 | Apple Inc. | Increasing the relevance of new available information |
US11386266B2 (en) | 2018-06-01 | 2022-07-12 | Apple Inc. | Text correction |
US11388291B2 (en) | 2013-03-14 | 2022-07-12 | Apple Inc. | System and method for processing voicemail |
US11386889B2 (en) * | 2018-12-03 | 2022-07-12 | Google Llc | Contextual tagging and biasing of grammars inside word lattices |
US11423908B2 (en) | 2019-05-06 | 2022-08-23 | Apple Inc. | Interpreting spoken requests |
US11462215B2 (en) | 2018-09-28 | 2022-10-04 | Apple Inc. | Multi-modal inputs for voice commands |
US11467802B2 (en) | 2017-05-11 | 2022-10-11 | Apple Inc. | Maintaining privacy of personal information |
US11468282B2 (en) | 2015-05-15 | 2022-10-11 | Apple Inc. | Virtual assistant in a communication session |
US11475898B2 (en) | 2018-10-26 | 2022-10-18 | Apple Inc. | Low-latency multi-speaker speech recognition |
US11475884B2 (en) | 2019-05-06 | 2022-10-18 | Apple Inc. | Reducing digital assistant latency when a language is incorrectly determined |
US11488406B2 (en) | 2019-09-25 | 2022-11-01 | Apple Inc. | Text detection using global geometry estimators |
US11496600B2 (en) | 2019-05-31 | 2022-11-08 | Apple Inc. | Remote execution of machine-learned models |
US11495218B2 (en) | 2018-06-01 | 2022-11-08 | Apple Inc. | Virtual assistant operation in multi-device environments |
US11518581B2 (en) | 2011-09-21 | 2022-12-06 | Hydros Bottle, Llc | Water bottle |
US11532306B2 (en) | 2017-05-16 | 2022-12-20 | Apple Inc. | Detecting a trigger of a digital assistant |
US11537821B2 (en) | 2019-04-10 | 2022-12-27 | International Business Machines Corporation | Evaluating text classification anomalies predicted by a text classification model |
US11601552B2 (en) | 2016-08-24 | 2023-03-07 | Gridspace Inc. | Hierarchical interface for adaptive closed loop communication system |
US11638059B2 (en) | 2019-01-04 | 2023-04-25 | Apple Inc. | Content playback on multiple devices |
US11657813B2 (en) | 2019-05-31 | 2023-05-23 | Apple Inc. | Voice identification in digital assistant systems |
US11671920B2 (en) | 2007-04-03 | 2023-06-06 | Apple Inc. | Method and system for operating a multifunction portable electronic device using voice-activation |
US11696060B2 (en) | 2020-07-21 | 2023-07-04 | Apple Inc. | User identification using headphones |
US11715459B2 (en) | 2016-08-24 | 2023-08-01 | Gridspace Inc. | Alert generator for adaptive closed loop communication system |
US11721356B2 (en) | 2016-08-24 | 2023-08-08 | Gridspace Inc. | Adaptive closed loop communication system |
US11765209B2 (en) | 2020-05-11 | 2023-09-19 | Apple Inc. | Digital assistant hardware abstraction |
US11790914B2 (en) | 2019-06-01 | 2023-10-17 | Apple Inc. | Methods and user interfaces for voice-based control of electronic devices |
US11798547B2 (en) | 2013-03-15 | 2023-10-24 | Apple Inc. | Voice activated device for use with a voice-based digital assistant |
US11809483B2 (en) | 2015-09-08 | 2023-11-07 | Apple Inc. | Intelligent automated assistant for media search and playback |
US11810578B2 (en) | 2020-05-11 | 2023-11-07 | Apple Inc. | Device arbitration for digital assistant-based intercom systems |
US11838734B2 (en) | 2020-07-20 | 2023-12-05 | Apple Inc. | Multi-device audio adjustment coordination |
US11853536B2 (en) | 2015-09-08 | 2023-12-26 | Apple Inc. | Intelligent automated assistant in a media environment |
US11875789B2 (en) * | 2016-08-19 | 2024-01-16 | Google Llc | Language models using domain-specific model components |
US20240022445A1 (en) * | 2022-07-14 | 2024-01-18 | Lenovo (Singapore) Pte. Ltd. | Methods, systems, and program products for identifying location of problems of delivered audio |
US11886805B2 (en) | 2015-11-09 | 2024-01-30 | Apple Inc. | Unconventional virtual assistant interactions |
US11914848B2 (en) | 2020-05-11 | 2024-02-27 | Apple Inc. | Providing relevant data items based on context |
Families Citing this family (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP6375521B2 (en) * | 2014-03-28 | 2018-08-22 | パナソニックＩｐマネジメント株式会社 | Voice search device, voice search method, and display device |
US10832161B2 (en) * | 2016-08-05 | 2020-11-10 | Conduent Business Services, Llc | Method and system of processing data for training a target domain classifier |
CN107221328B (en) * | 2017-05-25 | 2021-02-19 | 百度在线网络技术（北京）有限公司 | Method and device for positioning modification source, computer equipment and readable medium |
US10867609B2 (en) * | 2018-05-18 | 2020-12-15 | Sorenson Ip Holdings, Llc | Transcription generation technique selection |
Citations (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6615172B1 (en) * | 1999-11-12 | 2003-09-02 | Phoenix Solutions, Inc. | Intelligent query engine for processing voice based queries |
US6879956B1 (en) * | 1999-09-30 | 2005-04-12 | Sony Corporation | Speech recognition with feedback from natural language processing for adaptation of acoustic models |
US20050182628A1 (en) * | 2004-02-18 | 2005-08-18 | Samsung Electronics Co., Ltd. | Domain-based dialog speech recognition method and apparatus |
US20080183699A1 (en) * | 2007-01-24 | 2008-07-31 | Google Inc. | Blending mobile search results |
US20100169244A1 (en) * | 2008-12-31 | 2010-07-01 | Ilija Zeljkovic | Method and apparatus for using a discriminative classifier for processing a query |
US20120271842A1 (en) * | 2006-01-30 | 2012-10-25 | Gordon Sun | Learning retrieval functions incorporating query differentiation for information retrieval |
US20130024195A1 (en) * | 2008-03-19 | 2013-01-24 | Marc White | Corrective feedback loop for automated speech recognition |
US8694303B2 (en) * | 2011-06-15 | 2014-04-08 | Language Weaver, Inc. | Systems and methods for tuning parameters in statistical machine translation |
US20140136197A1 (en) * | 2011-07-31 | 2014-05-15 | Jonathan Mamou | Accuracy improvement of spoken queries transcription using co-occurrence information |
Family Cites Families (102)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5027406A (en) | 1988-12-06 | 1991-06-25 | Dragon Systems, Inc. | Method for interactive speech recognition and training |
JPH03163623A (en) | 1989-06-23 | 1991-07-15 | Articulate Syst Inc | Voice control computor interface |
US6965864B1 (en) | 1995-04-10 | 2005-11-15 | Texas Instruments Incorporated | Voice activated hypermedia systems using grammatical metadata |
US6292767B1 (en) | 1995-07-18 | 2001-09-18 | Nuance Communications | Method and system for building and running natural language understanding systems |
US5712957A (en) | 1995-09-08 | 1998-01-27 | Carnegie Mellon University | Locating and correcting erroneously recognized portions of utterances by rescoring based on two n-best lists |
US6064959A (en) | 1997-03-28 | 2000-05-16 | Dragon Systems, Inc. | Error correction in speech recognition |
US6961700B2 (en) | 1996-09-24 | 2005-11-01 | Allvoice Computing Plc | Method and apparatus for processing the output of a speech recognition engine |
US5797123A (en) | 1996-10-01 | 1998-08-18 | Lucent Technologies Inc. | Method of key-phase detection and verification for flexible speech understanding |
US5895466A (en) | 1997-08-19 | 1999-04-20 | At&T Corp | Automated natural language understanding customer service system |
US6021384A (en) | 1997-10-29 | 2000-02-01 | At&T Corp. | Automatic generation of superwords |
US6633235B1 (en) | 1998-06-15 | 2003-10-14 | Winbond Electronics Corp. | Method and apparatus for allowing a personal computer to control one or more devices |
JP2000076040A (en) | 1998-09-03 | 2000-03-14 | Matsushita Electric Ind Co Ltd | Voice input network terminal equipment |
US6453292B2 (en) | 1998-10-28 | 2002-09-17 | International Business Machines Corporation | Command boundary identifier for conversational natural language |
US6839669B1 (en) | 1998-11-05 | 2005-01-04 | Scansoft, Inc. | Performing actions identified in recognized speech |
US7881936B2 (en) | 1998-12-04 | 2011-02-01 | Tegic Communications, Inc. | Multimodal disambiguation of speech recognition |
US8275617B1 (en) | 1998-12-17 | 2012-09-25 | Nuance Communications, Inc. | Speech command input recognition system for interactive computer display with interpretation of ancillary relevant speech query terms into commands |
US6556970B1 (en) | 1999-01-28 | 2003-04-29 | Denso Corporation | Apparatus for determining appropriate series of words carrying information to be recognized |
US6643620B1 (en) | 1999-03-15 | 2003-11-04 | Matsushita Electric Industrial Co., Ltd. | Voice activated controller for recording and retrieving audio/video programs |
US20020032564A1 (en) | 2000-04-19 | 2002-03-14 | Farzad Ehsani | Phrase-based dialogue modeling with particular application to creating a recognition grammar for a voice-controlled user interface |
US6513006B2 (en) | 1999-08-26 | 2003-01-28 | Matsushita Electronic Industrial Co., Ltd. | Automatic control of household activity using speech recognition and natural language |
US7725307B2 (en) | 1999-11-12 | 2010-05-25 | Phoenix Solutions, Inc. | Query engine for processing voice based queries including semantic decoding |
US6785671B1 (en) | 1999-12-08 | 2004-08-31 | Amazon.Com, Inc. | System and method for locating web-based product offerings |
WO2001084535A2 (en) | 2000-05-02 | 2001-11-08 | Dragon Systems, Inc. | Error correction in speech recognition |
US6587824B1 (en) | 2000-05-04 | 2003-07-01 | Visteon Global Technologies, Inc. | Selective speaker adaptation for an in-vehicle speech recognition system |
US20060143007A1 (en) | 2000-07-24 | 2006-06-29 | Koh V E | User interaction with voice information services |
US6922670B2 (en) | 2000-10-24 | 2005-07-26 | Sanyo Electric Co., Ltd. | User support apparatus and system using agents |
US6795808B1 (en) | 2000-10-30 | 2004-09-21 | Koninklijke Philips Electronics N.V. | User interface/entertainment device that simulates personal interaction and charges external database with relevant data |
JP2004516516A (en) | 2000-12-18 | 2004-06-03 | コーニンクレッカ フィリップス エレクトロニクス エヌ ヴィ | How to save utterance and select vocabulary to recognize words |
JP3523213B2 (en) | 2001-03-28 | 2004-04-26 | 株式会社ジャストシステム | Command processing device, command processing method, and command processing program |
US20020198714A1 (en) | 2001-06-26 | 2002-12-26 | Guojun Zhou | Statistical spoken dialog system |
US20030093419A1 (en) | 2001-08-17 | 2003-05-15 | Srinivas Bangalore | System and method for querying information using a flexible multi-modal interface |
JP3997459B2 (en) | 2001-10-02 | 2007-10-24 | 株式会社日立製作所 | Voice input system, voice portal server, and voice input terminal |
US20030149566A1 (en) | 2002-01-02 | 2003-08-07 | Esther Levin | System and method for a spoken language interface to a large database of changing records |
US7203907B2 (en) | 2002-02-07 | 2007-04-10 | Sap Aktiengesellschaft | Multi-modal synchronization |
US7693720B2 (en) | 2002-07-15 | 2010-04-06 | Voicebox Technologies, Inc. | Mobile systems and methods for responding to natural language speech utterance |
US20050038781A1 (en) | 2002-12-12 | 2005-02-17 | Endeca Technologies, Inc. | Method and system for interpreting multiple-term queries |
US6993482B2 (en) | 2002-12-18 | 2006-01-31 | Motorola, Inc. | Method and apparatus for displaying speech recognition results |
US20050004799A1 (en) | 2002-12-31 | 2005-01-06 | Yevgenly Lyudovyk | System and method for a spoken language interface to a large database of changing records |
US20050021826A1 (en) | 2003-04-21 | 2005-01-27 | Sunil Kumar | Gateway controller for a multimodal system that provides inter-communication among different data and voice servers through various mobile devices, and interface for that controller |
US7890526B1 (en) | 2003-12-30 | 2011-02-15 | Microsoft Corporation | Incremental query refinement |
US7836044B2 (en) | 2004-06-22 | 2010-11-16 | Google Inc. | Anticipated query generation and processing in a search engine |
US7580363B2 (en) | 2004-08-16 | 2009-08-25 | Nokia Corporation | Apparatus and method for facilitating contact selection in communication devices |
US7599838B2 (en) | 2004-09-01 | 2009-10-06 | Sap Aktiengesellschaft | Speech animation with behavioral contexts for application scenarios |
US7925506B2 (en) | 2004-10-05 | 2011-04-12 | Inago Corporation | Speech recognition accuracy via concept to keyword mapping |
US7499940B1 (en) | 2004-11-11 | 2009-03-03 | Google Inc. | Method and system for URL autocompletion using ranked results |
US8942985B2 (en) | 2004-11-16 | 2015-01-27 | Microsoft Corporation | Centralized method and system for clarifying voice commands |
US7457751B2 (en) | 2004-11-30 | 2008-11-25 | Vocera Communications, Inc. | System and method for improving recognition accuracy in speech recognition applications |
US7751551B2 (en) | 2005-01-10 | 2010-07-06 | At&T Intellectual Property I, L.P. | System and method for speech-enabled call routing |
US7450698B2 (en) | 2005-01-14 | 2008-11-11 | At&T Intellectual Property 1, L.P. | System and method of utilizing a hybrid semantic model for speech recognition |
US7437297B2 (en) | 2005-01-27 | 2008-10-14 | International Business Machines Corporation | Systems and methods for predicting consequences of misinterpretation of user commands in automated systems |
US7788248B2 (en) | 2005-03-08 | 2010-08-31 | Apple Inc. | Immediate search feedback |
US7720684B2 (en) | 2005-04-29 | 2010-05-18 | Nuance Communications, Inc. | Method, apparatus, and computer program product for one-step correction of voice interaction |
US7424431B2 (en) | 2005-07-11 | 2008-09-09 | Stragent, Llc | System, method and computer program product for adding voice activation and voice control to a media player |
US7640160B2 (en) | 2005-08-05 | 2009-12-29 | Voicebox Technologies, Inc. | Systems and methods for responding to natural language speech utterance |
US20080065617A1 (en) | 2005-08-18 | 2008-03-13 | Yahoo! Inc. | Search entry system with query log autocomplete |
US7949529B2 (en) | 2005-08-29 | 2011-05-24 | Voicebox Technologies, Inc. | Mobile systems and methods of supporting natural language human-machine interactions |
KR20070024771A (en) | 2005-08-30 | 2007-03-08 | 엔에이치엔(주) | System and method for providing automatically completed query using automatic query transform |
US8265939B2 (en) | 2005-08-31 | 2012-09-11 | Nuance Communications, Inc. | Hierarchical methods and apparatus for extracting user intent from spoken utterances |
US7603360B2 (en) | 2005-09-14 | 2009-10-13 | Jumptap, Inc. | Location influenced search results |
JP2007142840A (en) | 2005-11-18 | 2007-06-07 | Canon Inc | Information processing apparatus and information processing method |
EP1969456A4 (en) | 2005-12-12 | 2015-11-04 | Tegic Comm Llc | Mobile device retrieval and navigation |
US8271107B2 (en) | 2006-01-13 | 2012-09-18 | International Business Machines Corporation | Controlling audio operation for data management and data rendering |
US20070208567A1 (en) | 2006-03-01 | 2007-09-06 | At&T Corp. | Error Correction In Automatic Speech Recognition Transcripts |
US7813926B2 (en) | 2006-03-16 | 2010-10-12 | Microsoft Corporation | Training system for a speech recognition application |
JP4551961B2 (en) | 2006-03-31 | 2010-09-29 | パイオニア株式会社 | VOICE INPUT SUPPORT DEVICE, ITS METHOD, ITS PROGRAM, RECORDING MEDIUM RECORDING THE PROGRAM, AND NAVIGATION DEVICE |
US9318108B2 (en) | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
US7856351B2 (en) | 2007-01-19 | 2010-12-21 | Microsoft Corporation | Integrated speech recognition and semantic classification |
US8886540B2 (en) | 2007-03-07 | 2014-11-11 | Vlingo Corporation | Using speech recognition results based on an unstructured language model in a mobile communication facility application |
US8886545B2 (en) | 2007-03-07 | 2014-11-11 | Vlingo Corporation | Dealing with switch latency in speech recognition |
US8838457B2 (en) | 2007-03-07 | 2014-09-16 | Vlingo Corporation | Using results of unstructured language model based speech recognition to control a system-level function of a mobile communications facility |
JP4412504B2 (en) | 2007-04-17 | 2010-02-10 | 本田技研工業株式会社 | Speech recognition apparatus, speech recognition method, and speech recognition program |
US8260809B2 (en) | 2007-06-28 | 2012-09-04 | Microsoft Corporation | Voice-based search processing |
US8165877B2 (en) | 2007-08-03 | 2012-04-24 | Microsoft Corporation | Confidence measure generation for speech related searching |
WO2009025356A1 (en) | 2007-08-22 | 2009-02-26 | Nec Corporation | Voice recognition device and voice recognition method |
US8438024B2 (en) | 2007-10-05 | 2013-05-07 | International Business Machines Corporation | Indexing method for quick search of voice recognition results |
US8594996B2 (en) | 2007-10-17 | 2013-11-26 | Evri Inc. | NLP-based entity recognition and disambiguation |
US8478578B2 (en) | 2008-01-09 | 2013-07-02 | Fluential, Llc | Mobile speech-to-speech interpretation system |
US8099289B2 (en) | 2008-02-13 | 2012-01-17 | Sensory, Inc. | Voice interface and search for electronic devices including bluetooth headsets and remote systems |
US7917368B2 (en) | 2008-02-25 | 2011-03-29 | Mitsubishi Electric Research Laboratories, Inc. | Method for interacting with users of speech recognition systems |
US8676577B2 (en) | 2008-03-31 | 2014-03-18 | Canyon IP Holdings, LLC | Use of metadata to post process speech recognition output |
US8126839B2 (en) | 2008-06-19 | 2012-02-28 | Yahoo! Inc. | Methods and apparatuses for adapting a ranking function of a search engine for use with a specific domain |
US8364481B2 (en) | 2008-07-02 | 2013-01-29 | Google Inc. | Speech recognition with parallel recognition tasks |
US8762153B2 (en) | 2008-08-18 | 2014-06-24 | At&T Intellectual Property I, L.P. | System and method for improving name dialer performance |
US8676904B2 (en) | 2008-10-02 | 2014-03-18 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
JP5199391B2 (en) | 2008-11-25 | 2013-05-15 | 旭化成株式会社 | Weight coefficient generation apparatus, speech recognition apparatus, navigation apparatus, vehicle, weight coefficient generation method, and weight coefficient generation program |
JP5334178B2 (en) | 2009-01-21 | 2013-11-06 | クラリオン株式会社 | Speech recognition apparatus and data update method |
CN102483753A (en) | 2009-05-27 | 2012-05-30 | 谷歌公司 | Computer Application Data In Search Results |
EP2339576B1 (en) | 2009-12-23 | 2019-08-07 | Google LLC | Multi-modal input on an electronic device |
US8494852B2 (en) | 2010-01-05 | 2013-07-23 | Google Inc. | Word-level correction of speech input |
US8433570B2 (en) | 2010-01-06 | 2013-04-30 | General Motors Llc | Method of recognizing speech |
US8626511B2 (en) | 2010-01-22 | 2014-01-07 | Google Inc. | Multi-dimensional disambiguation of voice commands |
US20110184740A1 (en) | 2010-01-26 | 2011-07-28 | Google Inc. | Integration of Embedded and Network Speech Recognizers |
US20110246944A1 (en) | 2010-04-06 | 2011-10-06 | Google Inc. | Application-independent text entry |
US8392411B2 (en) | 2010-05-20 | 2013-03-05 | Google Inc. | Automatic routing of search results |
US8738377B2 (en) | 2010-06-07 | 2014-05-27 | Google Inc. | Predicting and learning carrier phrases for speech input |
US8224654B1 (en) | 2010-08-06 | 2012-07-17 | Google Inc. | Editing voice input |
US8473289B2 (en) | 2010-08-06 | 2013-06-25 | Google Inc. | Disambiguating input based on context |
US8359020B2 (en) | 2010-08-06 | 2013-01-22 | Google Inc. | Automatically monitoring for voice input based on context |
US8731939B1 (en) | 2010-08-06 | 2014-05-20 | Google Inc. | Routing queries based on carrier phrase registration |
US8239366B2 (en) | 2010-09-08 | 2012-08-07 | Nuance Communications, Inc. | Method and apparatus for processing spoken search queries |
US9679561B2 (en) | 2011-03-28 | 2017-06-13 | Nuance Communications, Inc. | System and method for rapid customization of speech recognition models |
US8719192B2 (en) | 2011-04-06 | 2014-05-06 | Microsoft Corporation | Transfer of learning for query classification |
-
2013
- 2013-10-08 US US14/048,199 patent/US9646606B2/en active Active
Patent Citations (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6879956B1 (en) * | 1999-09-30 | 2005-04-12 | Sony Corporation | Speech recognition with feedback from natural language processing for adaptation of acoustic models |
US6615172B1 (en) * | 1999-11-12 | 2003-09-02 | Phoenix Solutions, Inc. | Intelligent query engine for processing voice based queries |
US20050182628A1 (en) * | 2004-02-18 | 2005-08-18 | Samsung Electronics Co., Ltd. | Domain-based dialog speech recognition method and apparatus |
US20120271842A1 (en) * | 2006-01-30 | 2012-10-25 | Gordon Sun | Learning retrieval functions incorporating query differentiation for information retrieval |
US20080183699A1 (en) * | 2007-01-24 | 2008-07-31 | Google Inc. | Blending mobile search results |
US20130024195A1 (en) * | 2008-03-19 | 2013-01-24 | Marc White | Corrective feedback loop for automated speech recognition |
US20100169244A1 (en) * | 2008-12-31 | 2010-07-01 | Ilija Zeljkovic | Method and apparatus for using a discriminative classifier for processing a query |
US8694303B2 (en) * | 2011-06-15 | 2014-04-08 | Language Weaver, Inc. | Systems and methods for tuning parameters in statistical machine translation |
US20140136197A1 (en) * | 2011-07-31 | 2014-05-15 | Jonathan Mamou | Accuracy improvement of spoken queries transcription using co-occurrence information |
Cited By (276)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11928604B2 (en) | 2005-09-08 | 2024-03-12 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
US10318871B2 (en) | 2005-09-08 | 2019-06-11 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
US11671920B2 (en) | 2007-04-03 | 2023-06-06 | Apple Inc. | Method and system for operating a multifunction portable electronic device using voice-activation |
US11979836B2 (en) | 2007-04-03 | 2024-05-07 | Apple Inc. | Method and system for operating a multi-function portable electronic device using voice-activation |
US11023513B2 (en) | 2007-12-20 | 2021-06-01 | Apple Inc. | Method and apparatus for searching using an active ontology |
US10381016B2 (en) | 2008-01-03 | 2019-08-13 | Apple Inc. | Methods and apparatus for altering audio output signals |
US9865248B2 (en) | 2008-04-05 | 2018-01-09 | Apple Inc. | Intelligent text-to-speech conversion |
US10108612B2 (en) | 2008-07-31 | 2018-10-23 | Apple Inc. | Mobile device having human language translation capability with positional feedback |
US10643611B2 (en) | 2008-10-02 | 2020-05-05 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US11900936B2 (en) | 2008-10-02 | 2024-02-13 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US11348582B2 (en) | 2008-10-02 | 2022-05-31 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US10795541B2 (en) | 2009-06-05 | 2020-10-06 | Apple Inc. | Intelligent organization of tasks items |
US11080012B2 (en) | 2009-06-05 | 2021-08-03 | Apple Inc. | Interface for a virtual digital assistant |
US10283110B2 (en) | 2009-07-02 | 2019-05-07 | Apple Inc. | Methods and apparatuses for automatic speech recognition |
US11423886B2 (en) | 2010-01-18 | 2022-08-23 | Apple Inc. | Task flow identification based on user intent |
US10741185B2 (en) | 2010-01-18 | 2020-08-11 | Apple Inc. | Intelligent automated assistant |
US10706841B2 (en) | 2010-01-18 | 2020-07-07 | Apple Inc. | Task flow identification based on user intent |
US10692504B2 (en) | 2010-02-25 | 2020-06-23 | Apple Inc. | User profiling for voice input processing |
US10049675B2 (en) | 2010-02-25 | 2018-08-14 | Apple Inc. | User profiling for voice input processing |
US10417405B2 (en) | 2011-03-21 | 2019-09-17 | Apple Inc. | Device access using voice authentication |
US11120372B2 (en) | 2011-06-03 | 2021-09-14 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US11350253B2 (en) | 2011-06-03 | 2022-05-31 | Apple Inc. | Active transport based notifications |
US11518581B2 (en) | 2011-09-21 | 2022-12-06 | Hydros Bottle, Llc | Water bottle |
US11069336B2 (en) | 2012-03-02 | 2021-07-20 | Apple Inc. | Systems and methods for name pronunciation |
US11321116B2 (en) | 2012-05-15 | 2022-05-03 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US11269678B2 (en) | 2012-05-15 | 2022-03-08 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US10079014B2 (en) | 2012-06-08 | 2018-09-18 | Apple Inc. | Name recognition system |
US9971774B2 (en) | 2012-09-19 | 2018-05-15 | Apple Inc. | Voice-based media searching |
US20150335807A1 (en) * | 2013-01-18 | 2015-11-26 | University Of Pittsburgh - Of The Commonwealth System Of Higher Education | Removal of carbon dioxide via dialysis |
US11862186B2 (en) | 2013-02-07 | 2024-01-02 | Apple Inc. | Voice trigger for a digital assistant |
US10714117B2 (en) | 2013-02-07 | 2020-07-14 | Apple Inc. | Voice trigger for a digital assistant |
US11557310B2 (en) | 2013-02-07 | 2023-01-17 | Apple Inc. | Voice trigger for a digital assistant |
US10978090B2 (en) | 2013-02-07 | 2021-04-13 | Apple Inc. | Voice trigger for a digital assistant |
US11636869B2 (en) | 2013-02-07 | 2023-04-25 | Apple Inc. | Voice trigger for a digital assistant |
US11388291B2 (en) | 2013-03-14 | 2022-07-12 | Apple Inc. | System and method for processing voicemail |
US11798547B2 (en) | 2013-03-15 | 2023-10-24 | Apple Inc. | Voice activated device for use with a voice-based digital assistant |
US9966060B2 (en) | 2013-06-07 | 2018-05-08 | Apple Inc. | System and method for user-specified pronunciation of words for speech synthesis and recognition |
US10657961B2 (en) | 2013-06-08 | 2020-05-19 | Apple Inc. | Interpreting and acting upon commands that involve sharing information with remote devices |
US11727219B2 (en) | 2013-06-09 | 2023-08-15 | Apple Inc. | System and method for inferring user intent from speech inputs |
US10769385B2 (en) | 2013-06-09 | 2020-09-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
US11048473B2 (en) | 2013-06-09 | 2021-06-29 | Apple Inc. | Device, method, and graphical user interface for enabling conversation persistence across two or more instances of a digital assistant |
US20150026179A1 (en) * | 2013-07-22 | 2015-01-22 | Kabushiki Kaisha Toshiba | Electronic device and method for processing clips of documents |
US9607080B2 (en) * | 2013-07-22 | 2017-03-28 | Kabushiki Kaisha Toshiba | Electronic device and method for processing clips of documents |
US11314370B2 (en) | 2013-12-06 | 2022-04-26 | Apple Inc. | Method for extracting salient dialog usage from live data |
US20150161985A1 (en) * | 2013-12-09 | 2015-06-11 | Google Inc. | Pronunciation verification |
US9837070B2 (en) * | 2013-12-09 | 2017-12-05 | Google Inc. | Verification of mappings between phoneme sequences and words |
US10657966B2 (en) | 2014-05-30 | 2020-05-19 | Apple Inc. | Better resolution when referencing to concepts |
US11133008B2 (en) | 2014-05-30 | 2021-09-28 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US20150348551A1 (en) * | 2014-05-30 | 2015-12-03 | Apple Inc. | Multi-command single utterance input method |
US10083690B2 (en) | 2014-05-30 | 2018-09-25 | Apple Inc. | Better resolution when referencing to concepts |
US11699448B2 (en) | 2014-05-30 | 2023-07-11 | Apple Inc. | Intelligent assistant for home automation |
US10169329B2 (en) | 2014-05-30 | 2019-01-01 | Apple Inc. | Exemplar-based natural language processing |
US9966065B2 (en) * | 2014-05-30 | 2018-05-08 | Apple Inc. | Multi-command single utterance input method |
US10699717B2 (en) | 2014-05-30 | 2020-06-30 | Apple Inc. | Intelligent assistant for home automation |
US10714095B2 (en) | 2014-05-30 | 2020-07-14 | Apple Inc. | Intelligent assistant for home automation |
US10497365B2 (en) | 2014-05-30 | 2019-12-03 | Apple Inc. | Multi-command single utterance input method |
US11257504B2 (en) | 2014-05-30 | 2022-02-22 | Apple Inc. | Intelligent assistant for home automation |
US11810562B2 (en) | 2014-05-30 | 2023-11-07 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US10878809B2 (en) | 2014-05-30 | 2020-12-29 | Apple Inc. | Multi-command single utterance input method |
US10417344B2 (en) | 2014-05-30 | 2019-09-17 | Apple Inc. | Exemplar-based natural language processing |
US11670289B2 (en) | 2014-05-30 | 2023-06-06 | Apple Inc. | Multi-command single utterance input method |
US11838579B2 (en) | 2014-06-30 | 2023-12-05 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US9668024B2 (en) | 2014-06-30 | 2017-05-30 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US10904611B2 (en) | 2014-06-30 | 2021-01-26 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US11516537B2 (en) | 2014-06-30 | 2022-11-29 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US10431204B2 (en) | 2014-09-11 | 2019-10-01 | Apple Inc. | Method and apparatus for discovering trending terms in speech requests |
US10438595B2 (en) | 2014-09-30 | 2019-10-08 | Apple Inc. | Speaker identification and unsupervised speaker adaptation techniques |
US10453443B2 (en) | 2014-09-30 | 2019-10-22 | Apple Inc. | Providing an indication of the suitability of speech recognition |
US10390213B2 (en) | 2014-09-30 | 2019-08-20 | Apple Inc. | Social reminders |
US9986419B2 (en) | 2014-09-30 | 2018-05-29 | Apple Inc. | Social reminders |
US10572810B2 (en) | 2015-01-07 | 2020-02-25 | Microsoft Technology Licensing, Llc | Managing user interaction for input understanding determinations |
US11231904B2 (en) | 2015-03-06 | 2022-01-25 | Apple Inc. | Reducing response latency of intelligent automated assistants |
US10529332B2 (en) | 2015-03-08 | 2020-01-07 | Apple Inc. | Virtual assistant activation |
US10930282B2 (en) | 2015-03-08 | 2021-02-23 | Apple Inc. | Competing devices responding to voice triggers |
US11842734B2 (en) | 2015-03-08 | 2023-12-12 | Apple Inc. | Virtual assistant activation |
US10567477B2 (en) | 2015-03-08 | 2020-02-18 | Apple Inc. | Virtual assistant continuity |
US11087759B2 (en) | 2015-03-08 | 2021-08-10 | Apple Inc. | Virtual assistant activation |
US10311871B2 (en) | 2015-03-08 | 2019-06-04 | Apple Inc. | Competing devices responding to voice triggers |
US11468282B2 (en) | 2015-05-15 | 2022-10-11 | Apple Inc. | Virtual assistant in a communication session |
US11070949B2 (en) | 2015-05-27 | 2021-07-20 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on an electronic device with a touch-sensitive display |
US11127397B2 (en) | 2015-05-27 | 2021-09-21 | Apple Inc. | Device voice control |
US10356243B2 (en) | 2015-06-05 | 2019-07-16 | Apple Inc. | Virtual assistant aided communication with 3rd party service in a communication session |
US10681212B2 (en) | 2015-06-05 | 2020-06-09 | Apple Inc. | Virtual assistant aided communication with 3rd party service in a communication session |
US10614812B2 (en) | 2015-06-06 | 2020-04-07 | Apple Inc. | Multi-microphone speech recognition systems and related techniques |
US10304462B2 (en) | 2015-06-06 | 2019-05-28 | Apple Inc. | Multi-microphone speech recognition systems and related techniques |
US9865265B2 (en) | 2015-06-06 | 2018-01-09 | Apple Inc. | Multi-microphone speech recognition systems and related techniques |
US10013981B2 (en) * | 2015-06-06 | 2018-07-03 | Apple Inc. | Multi-microphone speech recognition systems and related techniques |
US20160358606A1 (en) * | 2015-06-06 | 2016-12-08 | Apple Inc. | Multi-Microphone Speech Recognition Systems and Related Techniques |
US11025565B2 (en) | 2015-06-07 | 2021-06-01 | Apple Inc. | Personalized prediction of responses for instant messaging |
US11010127B2 (en) | 2015-06-29 | 2021-05-18 | Apple Inc. | Virtual assistant for media playback |
US11947873B2 (en) | 2015-06-29 | 2024-04-02 | Apple Inc. | Virtual assistant for media playback |
US20170018271A1 (en) * | 2015-07-13 | 2017-01-19 | Microsoft Technology Licensing, Llc | Delayed binding in response selection during input understanding processing |
US10249297B2 (en) * | 2015-07-13 | 2019-04-02 | Microsoft Technology Licensing, Llc | Propagating conversational alternatives using delayed hypothesis binding |
US11500672B2 (en) | 2015-09-08 | 2022-11-15 | Apple Inc. | Distributed personal assistant |
US11853536B2 (en) | 2015-09-08 | 2023-12-26 | Apple Inc. | Intelligent automated assistant in a media environment |
US11809483B2 (en) | 2015-09-08 | 2023-11-07 | Apple Inc. | Intelligent automated assistant for media search and playback |
US10671428B2 (en) | 2015-09-08 | 2020-06-02 | Apple Inc. | Distributed personal assistant |
US11954405B2 (en) | 2015-09-08 | 2024-04-09 | Apple Inc. | Zero latency digital assistant |
US11550542B2 (en) | 2015-09-08 | 2023-01-10 | Apple Inc. | Zero latency digital assistant |
US11126400B2 (en) | 2015-09-08 | 2021-09-21 | Apple Inc. | Zero latency digital assistant |
US10747498B2 (en) | 2015-09-08 | 2020-08-18 | Apple Inc. | Zero latency digital assistant |
CN105206267B (en) * | 2015-09-09 | 2019-04-02 | 中国科学院计算技术研究所 | A kind of the speech recognition errors modification method and system of fusion uncertainty feedback |
CN105206267A (en) * | 2015-09-09 | 2015-12-30 | 中国科学院计算技术研究所 | Voice recognition error correction method with integration of uncertain feedback and system thereof |
US11010550B2 (en) | 2015-09-29 | 2021-05-18 | Apple Inc. | Unified language modeling framework for word prediction, auto-completion and auto-correction |
US10366158B2 (en) | 2015-09-29 | 2019-07-30 | Apple Inc. | Efficient word encoding for recurrent neural network language models |
US11526368B2 (en) | 2015-11-06 | 2022-12-13 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US10691473B2 (en) | 2015-11-06 | 2020-06-23 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US11809886B2 (en) | 2015-11-06 | 2023-11-07 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US11886805B2 (en) | 2015-11-09 | 2024-01-30 | Apple Inc. | Unconventional virtual assistant interactions |
US10354652B2 (en) | 2015-12-02 | 2019-07-16 | Apple Inc. | Applying neural network language models to weighted finite state transducers for automatic speech recognition |
US10049668B2 (en) | 2015-12-02 | 2018-08-14 | Apple Inc. | Applying neural network language models to weighted finite state transducers for automatic speech recognition |
US11669750B2 (en) | 2015-12-06 | 2023-06-06 | Xeeva, Inc. | System and/or method for generating clean records from imperfect data using model stack(s) including classification model(s) and confidence model(s) |
US11100408B2 (en) * | 2015-12-06 | 2021-08-24 | Xeeva, Inc. | System and/or method for generating clean records from imperfect data using model stack(s) including classification model(s) and confidence model(s) |
US10223066B2 (en) | 2015-12-23 | 2019-03-05 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10942703B2 (en) | 2015-12-23 | 2021-03-09 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US11853647B2 (en) | 2015-12-23 | 2023-12-26 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US11351291B2 (en) | 2016-03-14 | 2022-06-07 | Advitos Gmbh | Systems or apparatuses and methods for performing dialysis |
US10446143B2 (en) | 2016-03-14 | 2019-10-15 | Apple Inc. | Identification of voice inputs providing credentials |
US10248452B2 (en) * | 2016-05-20 | 2019-04-02 | Microsoft Technology Licensing, Llc | Interaction framework for executing user instructions with online services |
US9934775B2 (en) | 2016-05-26 | 2018-04-03 | Apple Inc. | Unit-selection text-to-speech synthesis based on predicted concatenation parameters |
US9972304B2 (en) | 2016-06-03 | 2018-05-15 | Apple Inc. | Privacy preserving distributed evaluation framework for embedded personalized systems |
US11227589B2 (en) | 2016-06-06 | 2022-01-18 | Apple Inc. | Intelligent list reading |
US10249300B2 (en) | 2016-06-06 | 2019-04-02 | Apple Inc. | Intelligent list reading |
US10049663B2 (en) | 2016-06-08 | 2018-08-14 | Apple, Inc. | Intelligent automated assistant for media exploration |
US11069347B2 (en) | 2016-06-08 | 2021-07-20 | Apple Inc. | Intelligent automated assistant for media exploration |
US10354011B2 (en) | 2016-06-09 | 2019-07-16 | Apple Inc. | Intelligent automated assistant in a home environment |
US10067938B2 (en) | 2016-06-10 | 2018-09-04 | Apple Inc. | Multilingual word prediction |
US11657820B2 (en) | 2016-06-10 | 2023-05-23 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US11037565B2 (en) | 2016-06-10 | 2021-06-15 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US10490187B2 (en) | 2016-06-10 | 2019-11-26 | Apple Inc. | Digital assistant providing automated status report |
US10509862B2 (en) | 2016-06-10 | 2019-12-17 | Apple Inc. | Dynamic phrase expansion of language input |
US10733993B2 (en) | 2016-06-10 | 2020-08-04 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US10192552B2 (en) | 2016-06-10 | 2019-01-29 | Apple Inc. | Digital assistant providing whispered speech |
US11809783B2 (en) | 2016-06-11 | 2023-11-07 | Apple Inc. | Intelligent device arbitration and control |
US10297253B2 (en) | 2016-06-11 | 2019-05-21 | Apple Inc. | Application integration with a digital assistant |
US10089072B2 (en) | 2016-06-11 | 2018-10-02 | Apple Inc. | Intelligent device arbitration and control |
US10942702B2 (en) | 2016-06-11 | 2021-03-09 | Apple Inc. | Intelligent device arbitration and control |
US11749275B2 (en) | 2016-06-11 | 2023-09-05 | Apple Inc. | Application integration with a digital assistant |
US10521466B2 (en) | 2016-06-11 | 2019-12-31 | Apple Inc. | Data driven natural language event detection and classification |
US10580409B2 (en) | 2016-06-11 | 2020-03-03 | Apple Inc. | Application integration with a digital assistant |
US11152002B2 (en) | 2016-06-11 | 2021-10-19 | Apple Inc. | Application integration with a digital assistant |
US10269345B2 (en) | 2016-06-11 | 2019-04-23 | Apple Inc. | Intelligent task discovery |
US11875789B2 (en) * | 2016-08-19 | 2024-01-16 | Google Llc | Language models using domain-specific model components |
US11715459B2 (en) | 2016-08-24 | 2023-08-01 | Gridspace Inc. | Alert generator for adaptive closed loop communication system |
US11721356B2 (en) | 2016-08-24 | 2023-08-08 | Gridspace Inc. | Adaptive closed loop communication system |
US11601552B2 (en) | 2016-08-24 | 2023-03-07 | Gridspace Inc. | Hierarchical interface for adaptive closed loop communication system |
US10861436B1 (en) * | 2016-08-24 | 2020-12-08 | Gridspace Inc. | Audio call classification and survey system |
US10474753B2 (en) | 2016-09-07 | 2019-11-12 | Apple Inc. | Language identification using recurrent neural networks |
US10446137B2 (en) | 2016-09-07 | 2019-10-15 | Microsoft Technology Licensing, Llc | Ambiguity resolving conversational understanding system |
US10242670B2 (en) * | 2016-09-21 | 2019-03-26 | Intel Corporation | Syntactic re-ranking of potential transcriptions during automatic speech recognition |
WO2018057427A1 (en) * | 2016-09-21 | 2018-03-29 | Intel Corporation | Syntactic re-ranking of potential transcriptions during automatic speech recognition |
US10553215B2 (en) | 2016-09-23 | 2020-02-04 | Apple Inc. | Intelligent automated assistant |
US10043516B2 (en) | 2016-09-23 | 2018-08-07 | Apple Inc. | Intelligent automated assistant |
US11281993B2 (en) | 2016-12-05 | 2022-03-22 | Apple Inc. | Model and ensemble compression for metric learning |
US11003163B2 (en) | 2016-12-19 | 2021-05-11 | Honeywell International Inc. | Voice control of components of a facility |
US10429817B2 (en) * | 2016-12-19 | 2019-10-01 | Honeywell International Inc. | Voice control of components of a facility |
US10593346B2 (en) | 2016-12-22 | 2020-03-17 | Apple Inc. | Rank-reduced token representation for automatic speech recognition |
US11656884B2 (en) | 2017-01-09 | 2023-05-23 | Apple Inc. | Application integration with a digital assistant |
US11204787B2 (en) | 2017-01-09 | 2021-12-21 | Apple Inc. | Application integration with a digital assistant |
US20180211652A1 (en) * | 2017-01-26 | 2018-07-26 | Samsung Electronics Co., Ltd. | Speech recognition method and apparatus |
US10332518B2 (en) | 2017-05-09 | 2019-06-25 | Apple Inc. | User interface for correcting recognition errors |
US10741181B2 (en) | 2017-05-09 | 2020-08-11 | Apple Inc. | User interface for correcting recognition errors |
US10417266B2 (en) | 2017-05-09 | 2019-09-17 | Apple Inc. | Context-aware ranking of intelligent response suggestions |
US11467802B2 (en) | 2017-05-11 | 2022-10-11 | Apple Inc. | Maintaining privacy of personal information |
US10847142B2 (en) | 2017-05-11 | 2020-11-24 | Apple Inc. | Maintaining privacy of personal information |
US10726832B2 (en) | 2017-05-11 | 2020-07-28 | Apple Inc. | Maintaining privacy of personal information |
US11599331B2 (en) | 2017-05-11 | 2023-03-07 | Apple Inc. | Maintaining privacy of personal information |
US10755703B2 (en) | 2017-05-11 | 2020-08-25 | Apple Inc. | Offline personal assistant |
US10395654B2 (en) | 2017-05-11 | 2019-08-27 | Apple Inc. | Text normalization based on a data-driven learning network |
US11837237B2 (en) | 2017-05-12 | 2023-12-05 | Apple Inc. | User-specific acoustic models |
US10789945B2 (en) | 2017-05-12 | 2020-09-29 | Apple Inc. | Low-latency intelligent automated assistant |
US11538469B2 (en) | 2017-05-12 | 2022-12-27 | Apple Inc. | Low-latency intelligent automated assistant |
US11580990B2 (en) | 2017-05-12 | 2023-02-14 | Apple Inc. | User-specific acoustic models |
US11405466B2 (en) | 2017-05-12 | 2022-08-02 | Apple Inc. | Synchronization and task delegation of a digital assistant |
US11380310B2 (en) | 2017-05-12 | 2022-07-05 | Apple Inc. | Low-latency intelligent automated assistant |
US10791176B2 (en) | 2017-05-12 | 2020-09-29 | Apple Inc. | Synchronization and task delegation of a digital assistant |
US10410637B2 (en) | 2017-05-12 | 2019-09-10 | Apple Inc. | User-specific acoustic models |
US11301477B2 (en) | 2017-05-12 | 2022-04-12 | Apple Inc. | Feedback analysis of a digital assistant |
US11862151B2 (en) | 2017-05-12 | 2024-01-02 | Apple Inc. | Low-latency intelligent automated assistant |
US10810274B2 (en) | 2017-05-15 | 2020-10-20 | Apple Inc. | Optimizing dialogue policy decisions for digital assistants using implicit feedback |
US10482874B2 (en) | 2017-05-15 | 2019-11-19 | Apple Inc. | Hierarchical belief states for digital assistants |
US11217255B2 (en) | 2017-05-16 | 2022-01-04 | Apple Inc. | Far-field extension for digital assistant services |
US10403278B2 (en) | 2017-05-16 | 2019-09-03 | Apple Inc. | Methods and systems for phonetic matching in digital assistant services |
US10748546B2 (en) | 2017-05-16 | 2020-08-18 | Apple Inc. | Digital assistant services based on device capabilities |
US11532306B2 (en) | 2017-05-16 | 2022-12-20 | Apple Inc. | Detecting a trigger of a digital assistant |
US10311144B2 (en) | 2017-05-16 | 2019-06-04 | Apple Inc. | Emoji word sense disambiguation |
US10303715B2 (en) | 2017-05-16 | 2019-05-28 | Apple Inc. | Intelligent automated assistant for media exploration |
US11675829B2 (en) | 2017-05-16 | 2023-06-13 | Apple Inc. | Intelligent automated assistant for media exploration |
US10909171B2 (en) | 2017-05-16 | 2021-02-02 | Apple Inc. | Intelligent automated assistant for media exploration |
US11344656B2 (en) | 2017-05-22 | 2022-05-31 | Advitos Gmbh | Methods and systems for removing carbon dioxide |
US10657328B2 (en) | 2017-06-02 | 2020-05-19 | Apple Inc. | Multi-task recurrent neural network architecture for efficient morphology handling in neural language modeling |
US10445429B2 (en) | 2017-09-21 | 2019-10-15 | Apple Inc. | Natural language understanding using vocabularies with compressed serialized tries |
US10755051B2 (en) | 2017-09-29 | 2020-08-25 | Apple Inc. | Rule-based natural language processing |
US10636424B2 (en) | 2017-11-30 | 2020-04-28 | Apple Inc. | Multi-turn canned dialog |
US10885919B2 (en) * | 2018-01-05 | 2021-01-05 | Nuance Communications, Inc. | Routing system and method |
US20190214016A1 (en) * | 2018-01-05 | 2019-07-11 | Nuance Communications, Inc. | Routing system and method |
US10733982B2 (en) | 2018-01-08 | 2020-08-04 | Apple Inc. | Multi-directional dialog |
US10733375B2 (en) | 2018-01-31 | 2020-08-04 | Apple Inc. | Knowledge-based framework for improving natural language understanding |
US10789959B2 (en) | 2018-03-02 | 2020-09-29 | Apple Inc. | Training speaker recognition models for digital assistants |
US10592604B2 (en) | 2018-03-12 | 2020-03-17 | Apple Inc. | Inverse text normalization for automatic speech recognition |
US10777203B1 (en) * | 2018-03-23 | 2020-09-15 | Amazon Technologies, Inc. | Speech interface device with caching component |
US11437041B1 (en) * | 2018-03-23 | 2022-09-06 | Amazon Technologies, Inc. | Speech interface device with caching component |
US11887604B1 (en) * | 2018-03-23 | 2024-01-30 | Amazon Technologies, Inc. | Speech interface device with caching component |
US11710482B2 (en) | 2018-03-26 | 2023-07-25 | Apple Inc. | Natural assistant interaction |
US10818288B2 (en) | 2018-03-26 | 2020-10-27 | Apple Inc. | Natural assistant interaction |
US10909331B2 (en) | 2018-03-30 | 2021-02-02 | Apple Inc. | Implicit identification of translation payload with neural machine translation |
US11854539B2 (en) | 2018-05-07 | 2023-12-26 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11487364B2 (en) | 2018-05-07 | 2022-11-01 | Apple Inc. | Raise to speak |
US11169616B2 (en) | 2018-05-07 | 2021-11-09 | Apple Inc. | Raise to speak |
US11900923B2 (en) | 2018-05-07 | 2024-02-13 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11907436B2 (en) | 2018-05-07 | 2024-02-20 | Apple Inc. | Raise to speak |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US10928918B2 (en) | 2018-05-07 | 2021-02-23 | Apple Inc. | Raise to speak |
US10984780B2 (en) | 2018-05-21 | 2021-04-20 | Apple Inc. | Global semantic word embeddings using bi-directional recurrent neural networks |
US11386266B2 (en) | 2018-06-01 | 2022-07-12 | Apple Inc. | Text correction |
US11360577B2 (en) | 2018-06-01 | 2022-06-14 | Apple Inc. | Attention aware virtual assistant dismissal |
US11495218B2 (en) | 2018-06-01 | 2022-11-08 | Apple Inc. | Virtual assistant operation in multi-device environments |
US10892996B2 (en) | 2018-06-01 | 2021-01-12 | Apple Inc. | Variable latency device coordination |
US11009970B2 (en) | 2018-06-01 | 2021-05-18 | Apple Inc. | Attention aware virtual assistant dismissal |
US11630525B2 (en) | 2018-06-01 | 2023-04-18 | Apple Inc. | Attention aware virtual assistant dismissal |
US10720160B2 (en) | 2018-06-01 | 2020-07-21 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US10403283B1 (en) | 2018-06-01 | 2019-09-03 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US10984798B2 (en) | 2018-06-01 | 2021-04-20 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US10684703B2 (en) | 2018-06-01 | 2020-06-16 | Apple Inc. | Attention aware virtual assistant dismissal |
US11431642B2 (en) | 2018-06-01 | 2022-08-30 | Apple Inc. | Variable latency device coordination |
US10496705B1 (en) | 2018-06-03 | 2019-12-03 | Apple Inc. | Accelerated task performance |
US10944859B2 (en) | 2018-06-03 | 2021-03-09 | Apple Inc. | Accelerated task performance |
US10504518B1 (en) | 2018-06-03 | 2019-12-10 | Apple Inc. | Accelerated task performance |
US11010561B2 (en) | 2018-09-27 | 2021-05-18 | Apple Inc. | Sentiment prediction from textual data |
US11893992B2 (en) | 2018-09-28 | 2024-02-06 | Apple Inc. | Multi-modal inputs for voice commands |
US10839159B2 (en) | 2018-09-28 | 2020-11-17 | Apple Inc. | Named entity normalization in a spoken dialog system |
US11462215B2 (en) | 2018-09-28 | 2022-10-04 | Apple Inc. | Multi-modal inputs for voice commands |
US11170166B2 (en) | 2018-09-28 | 2021-11-09 | Apple Inc. | Neural typographical error modeling via generative adversarial networks |
US11475898B2 (en) | 2018-10-26 | 2022-10-18 | Apple Inc. | Low-latency multi-speaker speech recognition |
US11386889B2 (en) * | 2018-12-03 | 2022-07-12 | Google Llc | Contextual tagging and biasing of grammars inside word lattices |
US11638059B2 (en) | 2019-01-04 | 2023-04-25 | Apple Inc. | Content playback on multiple devices |
US11783815B2 (en) | 2019-03-18 | 2023-10-10 | Apple Inc. | Multimodality in digital assistant systems |
US11348573B2 (en) | 2019-03-18 | 2022-05-31 | Apple Inc. | Multimodality in digital assistant systems |
US11537821B2 (en) | 2019-04-10 | 2022-12-27 | International Business Machines Corporation | Evaluating text classification anomalies predicted by a text classification model |
US11068656B2 (en) * | 2019-04-10 | 2021-07-20 | International Business Machines Corporation | Displaying text classification anomalies predicted by a text classification model |
US11074414B2 (en) * | 2019-04-10 | 2021-07-27 | International Business Machines Corporation | Displaying text classification anomalies predicted by a text classification model |
US11675491B2 (en) | 2019-05-06 | 2023-06-13 | Apple Inc. | User configurable task triggers |
US11705130B2 (en) | 2019-05-06 | 2023-07-18 | Apple Inc. | Spoken notifications |
US11307752B2 (en) | 2019-05-06 | 2022-04-19 | Apple Inc. | User configurable task triggers |
US11475884B2 (en) | 2019-05-06 | 2022-10-18 | Apple Inc. | Reducing digital assistant latency when a language is incorrectly determined |
US11217251B2 (en) | 2019-05-06 | 2022-01-04 | Apple Inc. | Spoken notifications |
US11423908B2 (en) | 2019-05-06 | 2022-08-23 | Apple Inc. | Interpreting spoken requests |
US20230252997A1 (en) * | 2019-05-09 | 2023-08-10 | Rovi Guides, Inc. | Word correction using automatic speech recognition (asr) incremental response |
US11107475B2 (en) * | 2019-05-09 | 2021-08-31 | Rovi Guides, Inc. | Word correction using automatic speech recognition (ASR) incremental response |
US20210350807A1 (en) * | 2019-05-09 | 2021-11-11 | Rovi Guides, Inc. | Word correction using automatic speech recognition (asr) incremental response |
US11651775B2 (en) * | 2019-05-09 | 2023-05-16 | Rovi Guides, Inc. | Word correction using automatic speech recognition (ASR) incremental response |
US11888791B2 (en) | 2019-05-21 | 2024-01-30 | Apple Inc. | Providing message response suggestions |
US11140099B2 (en) | 2019-05-21 | 2021-10-05 | Apple Inc. | Providing message response suggestions |
US11360739B2 (en) | 2019-05-31 | 2022-06-14 | Apple Inc. | User activity shortcut suggestions |
US11237797B2 (en) | 2019-05-31 | 2022-02-01 | Apple Inc. | User activity shortcut suggestions |
US11289073B2 (en) | 2019-05-31 | 2022-03-29 | Apple Inc. | Device text to speech |
US11657813B2 (en) | 2019-05-31 | 2023-05-23 | Apple Inc. | Voice identification in digital assistant systems |
US11496600B2 (en) | 2019-05-31 | 2022-11-08 | Apple Inc. | Remote execution of machine-learned models |
US11790914B2 (en) | 2019-06-01 | 2023-10-17 | Apple Inc. | Methods and user interfaces for voice-based control of electronic devices |
US11360641B2 (en) | 2019-06-01 | 2022-06-14 | Apple Inc. | Increasing the relevance of new available information |
US11262978B1 (en) * | 2019-06-19 | 2022-03-01 | Amazon Technologies, Inc. | Voice-adapted reformulation of web-based answers |
US11488406B2 (en) | 2019-09-25 | 2022-11-01 | Apple Inc. | Text detection using global geometry estimators |
US11289086B2 (en) * | 2019-11-01 | 2022-03-29 | Microsoft Technology Licensing, Llc | Selective response rendering for virtual assistants |
US11676586B2 (en) * | 2019-12-10 | 2023-06-13 | Rovi Guides, Inc. | Systems and methods for providing voice command recommendations |
US20210174795A1 (en) * | 2019-12-10 | 2021-06-10 | Rovi Guides, Inc. | Systems and methods for providing voice command recommendations |
CN111583907A (en) * | 2020-04-15 | 2020-08-25 | 北京小米松果电子有限公司 | Information processing method, device and storage medium |
US11810578B2 (en) | 2020-05-11 | 2023-11-07 | Apple Inc. | Device arbitration for digital assistant-based intercom systems |
US11765209B2 (en) | 2020-05-11 | 2023-09-19 | Apple Inc. | Digital assistant hardware abstraction |
US11914848B2 (en) | 2020-05-11 | 2024-02-27 | Apple Inc. | Providing relevant data items based on context |
US11924254B2 (en) | 2020-05-11 | 2024-03-05 | Apple Inc. | Digital assistant hardware abstraction |
US11838734B2 (en) | 2020-07-20 | 2023-12-05 | Apple Inc. | Multi-device audio adjustment coordination |
US11750962B2 (en) | 2020-07-21 | 2023-09-05 | Apple Inc. | User identification using headphones |
US11696060B2 (en) | 2020-07-21 | 2023-07-04 | Apple Inc. | User identification using headphones |
US20220083900A1 (en) * | 2020-09-11 | 2022-03-17 | Fortinet, Inc. | Intelligent vector selection by identifying high machine-learning model skepticism |
US11973611B2 (en) * | 2022-07-14 | 2024-04-30 | Lenovo (Singapore) Pte. Ltd. | Methods, systems, and program products for identifying location of problems of delivered audio |
US20240022445A1 (en) * | 2022-07-14 | 2024-01-18 | Lenovo (Singapore) Pte. Ltd. | Methods, systems, and program products for identifying location of problems of delivered audio |
Also Published As
Publication number | Publication date |
---|---|
US9646606B2 (en) | 2017-05-09 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9646606B2 (en) | Speech recognition using domain knowledge | |
US9336298B2 (en) | Dialog-enhanced contextual search query analysis | |
US10860639B2 (en) | Query response using media consumption history | |
US9846836B2 (en) | Modeling interestingness with deep neural networks | |
US9330661B2 (en) | Accuracy improvement of spoken queries transcription using co-occurrence information | |
US9842592B2 (en) | Language models using non-linguistic context | |
US9286892B2 (en) | Language modeling in speech recognition | |
US9454957B1 (en) | Named entity resolution in spoken language processing | |
US10838746B2 (en) | Identifying parameter values and determining features for boosting rankings of relevant distributable digital assistant operations | |
US20130060769A1 (en) | System and method for identifying social media interactions | |
US11016968B1 (en) | Mutation architecture for contextual data aggregator | |
US9589563B2 (en) | Speech recognition of partial proper names by natural language processing | |
US8731930B2 (en) | Contextual voice query dilation to improve spoken web searching | |
US10482876B2 (en) | Hierarchical speech recognition decoder | |
WO2008016102A1 (en) | Similarity calculation device and information search device | |
US10872601B1 (en) | Natural language processing | |
CN114661872B (en) | Beginner-oriented API self-adaptive recommendation method and system | |
Moyal et al. | Phonetic search methods for large speech databases | |
Bourlard et al. | Processing and linking audio events in large multimedia archives: The eu inevent project | |
US20230153534A1 (en) | Generating commonsense context for text using knowledge graphs | |
US20220415312A1 (en) | Multi-tier speech processing and content operations | |
Chen et al. | Topic segmentation on spoken documents using self-validated acoustic cuts | |
KR102648990B1 (en) | Peer learning recommendation method and device | |
Staš et al. | Language model speaker adaptation for transcription of Slovak parliament proceedings | |
US11935533B1 (en) | Content-related actions based on context |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:PENG, FUCHUN;SHAHSHAHANI, BEN;ROY, HOWARD SCOTT;SIGNING DATES FROM 20130925 TO 20131007;REEL/FRAME:031502/0599 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044097/0658Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |