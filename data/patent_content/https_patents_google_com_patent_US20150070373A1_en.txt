US20150070373A1 - Clarification of Zoomed Text Embedded in Images - Google Patents
Clarification of Zoomed Text Embedded in Images Download PDFInfo
- Publication number
- US20150070373A1 US20150070373A1 US13/593,300 US201213593300A US2015070373A1 US 20150070373 A1 US20150070373 A1 US 20150070373A1 US 201213593300 A US201213593300 A US 201213593300A US 2015070373 A1 US2015070373 A1 US 2015070373A1
- Authority
- US
- United States
- Prior art keywords
- font
- image
- text
- word
- regions
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Abandoned
Links
- 238000005352 clarification Methods 0.000 title claims abstract description 20
- 238000009877 rendering Methods 0.000 claims abstract description 15
- 238000003909 pattern recognition Methods 0.000 claims abstract description 14
- 238000000034 method Methods 0.000 claims description 43
- 238000012015 optical character recognition Methods 0.000 claims description 29
- 230000007246 mechanism Effects 0.000 claims description 14
- 239000003086 colorant Substances 0.000 claims description 8
- 239000007787 solid Substances 0.000 claims description 7
- 238000001914 filtration Methods 0.000 claims description 6
- 239000002131 composite material Substances 0.000 claims description 2
- 238000005516 engineering process Methods 0.000 abstract description 29
- 230000008569 process Effects 0.000 description 21
- 238000004590 computer program Methods 0.000 description 13
- 238000004891 communication Methods 0.000 description 5
- 238000003860 storage Methods 0.000 description 5
- 230000003190 augmentative effect Effects 0.000 description 4
- 238000010586 diagram Methods 0.000 description 4
- 238000013500 data storage Methods 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 238000013459 approach Methods 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 238000013461 design Methods 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 239000000463 material Substances 0.000 description 2
- 238000013341 scale-up Methods 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- RYGMFSIKBFXOCR-UHFFFAOYSA-N Copper Chemical compound [Cu] RYGMFSIKBFXOCR-UHFFFAOYSA-N 0.000 description 1
- 230000009471 action Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 238000004422 calculation algorithm Methods 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 238000009264 composting Methods 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 230000009977 dual effect Effects 0.000 description 1
- 230000002708 enhancing effect Effects 0.000 description 1
- 239000000835 fiber Substances 0.000 description 1
- 230000014509 gene expression Effects 0.000 description 1
- 238000003384 imaging method Methods 0.000 description 1
- 230000001771 impaired effect Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 238000012545 processing Methods 0.000 description 1
- 238000005070 sampling Methods 0.000 description 1
- 230000035945 sensitivity Effects 0.000 description 1
- 238000013179 statistical model Methods 0.000 description 1
- 210000003813 thumb Anatomy 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T11/00—2D [Two Dimensional] image generation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/10—Text processing
- G06F40/103—Formatting, i.e. changing of presentation of documents
- G06F40/109—Font handling; Temporal or kinetic typography
-
- G06T5/77—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V30/00—Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition
- G06V30/10—Character recognition
- G06V30/24—Character recognition characterised by the processing or recognition method
- G06V30/242—Division of the character sequences into groups prior to recognition; Selection of dictionaries
- G06V30/244—Division of the character sequences into groups prior to recognition; Selection of dictionaries using graphical properties, e.g. alphabet type or font
- G06V30/245—Font recognition
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/232—Orthographic correction, e.g. spell checking or vowelisation
Definitions
- a clarification tool includes a raster, edge-finding filter configured to identify text embedded in images, a pattern recognition tool configured to match the identified text against a database of font-families and patterns, a text grouping tool configured to group text into a word region, a font-family selector tool configured to select the proper font-family to use to clarify the embedded text for the word region and a rendering engine configured to render the clarified text in a text display tool.
- FIG. 1 is a diagrammatic representation illustrating an example process for clarification of zoomed text embedded in an image in accordance with an implementation of the present technology.
- FIG. 2 is a block diagram illustrating an example device implementing clarification of zoomed text embedded in an image in accordance with an implementation of the present technology.
- FIG. 3 is a flow chart illustrating an example process for implementing clarification of zoomed text embedded in an image in accordance with an implementation of the present technology.
- FIG. 4 is a block diagram illustrating an example of a system for clarification of zoomed text embedded an image in accordance with an implementation of the present technology.
- a glyph is an element of writing including single characters that are each self-contained units of written text or punctuation—a vector, spline-based representation of an individual character in a language.
- That image i.e., raster graphics image, bitmap
- raster graphics image bitmap
- dot matrix data structure representing a generally rectangular grid of pixels viewable by a monitor, paper, or other display medium.
- Raster graphics are resolution dependent. They cannot scale up to various resolutions without loss of apparent quality resulting in pixilation or a blurry image appearance. However, as mentioned above vector graphics can easily scale up to the quality of the device rendering them.
- vector graphics may be described as the use of geometric characteristics such as points, lines, curves, and shapes or polygons, which are all based on mathematical expressions, to represent images in computer graphics.
- clarity is maintained in a web page when a browser increases the font size of symbol-encoded text of a web page because it redraws the text that is already encoded as vector graphics.
- clarity is lost in textual-appearing content in a raster graphics image during enlargement because the bits making up the “text” are not encoded and are not differentiated from any other bits in the image.
- An implementation in accordance with the technologies described herein draws from four major technological foundations together to improve the clarity of text in images: image filtering, image pattern recognition, a database of commonly used font-families and their patterns, and browser-based, client-side canvas compositing to produce zoomed images with clear text. These technologies makes it easier for a user to read zoomed text embedded over images in a web page or electronic document.
- the technology may be applied to any web browser rendering engine, such as those shared among tablet computers and/or smartphone mobile devices.
- the technology may be applied further to the rendering of web pages or E-books (which often contain poor quality scans of original published graphics, maps and technical diagrams).
- FIG. 1 illustrates a diagrammatic representation showing an example process 100 of clarification of zoomed in text embedded in an image.
- Example process 100 shows in general a view of an original banner image 102 , an image without zoom on a web page 104 , and the text clarification procedure on zoomed image 106 .
- the browser zooms in on image 110 .
- new texts are drawn on image 112 and finally the composited result 114 is presented.
- FIG. 1 depicts an area of the image that may be referred to as a word region 116 (e.g., “Welcome”).
- the word region 116 is clarified, zoomed into word region 118 .
- the clarified word region 118 is overlaid and composited onto the original image 102 as new image 120 .
- FIG. 2 shows an example zoomed-text clarification device 200 implementing clarification of zoomed text embedded in images in accordance with an implementation of the present technology.
- device 200 includes a raster image filter 202 , optical character recognition (OCR) engine 206 , and a text display mechanism 220 .
- OCR optical character recognition
- the raster image filter 202 is configured to perform a level of image filtering with regard to contrasts or inverse imaging. For instance, image filter 202 may scan image pixels and lock in the image by performing multiple passes. For example, a pass may locate any text embedded in the image as 100% black or 80% gray on a white background. Further, the image filter 202 may include a text region identifier 204 configured to distinguish what is text and what is an image and subsequent passes through filter 202 may lock in the edges of the text and recognize the font of interest. Examples of approaches that may be employed to identify regions that are likely to contain text are edge-finding approaches or a discreet cosine transform amongst others. This occurs before any OCR and font recognition function has been performed.
- the OCR engine 206 includes image pattern/font-family database 208 , a font-family mechanism 212 , and a word-region analyzer 216 .
- the OCR engine is used to capture the text and font-family of the image(s) scanned.
- OCR 206 may include an image pattern/font-family database 208 configured to match image patterns and font-families to textual content found by text region identifier 204 .
- an out-of-the-box OCR software package may be augmented/linked with database 208 and used.
- the proposed database would contain the “signatures” for each glyph of the most popular fonts. After the normal OCR process were completed, the second pass would look among all the ‘a’s, for example, to find the glyph with the signature that most closely approximates the one detected.
- OCRopusTM is an open source OCR engine, that allows pluggable back-ends for character recognition; within the OCRopusTM project, IRecognizeLine is the name of the interface for just such an extension and implementing the additional font-family weighting as a module for that interface is an example of one way by which adapting/augmenting an off-the-shelf OCR implementation could be achieved.
- the IRecognizeLine interface is for text line recognizers. In its most common form, it is used to transform images of text lines into recognition lattices. An implementation may process through each letter of every word and match that letter against all known signatures for that letter in the augmented glyph database. At the end of a word, if, for example, the characters most closely approximated ‘Times New Roman’ and two characters most closely approximated ‘Georgia’, the engine would return ‘Times New Roman’ for the whole word based on the assumption that artists do not typically change fonts in the middle of a word.
- a given text or glyph may be defined as a single character in a font-family.
- the text or glyph is an element, which can be defined further as a self-contained unit in a language. Some texts are based on proximity to each other (i.e., dual characters). East Asian languages have a concept of a text so they can still be described as a self-contained unit represented by a vector to render the texts as well. Thus, the term text is applicable to any language.
- the text display mechanism 220 includes text grouping mechanism 210 , a text rendering engine, and an image overlay 222 . With the text grouping mechanism 210 , the device 200 groups the identified textual content into word regions as depicted in FIG. 1 at word region 116 .
- a word region is, for example, a group of identified texts that form a word (e.g., “Welcome”) and a font-family used in embedded text in an image is comprised of a library of texts.
- the OCR engine 206 discussed above is configured to bias an entire word region toward only one font-family. This biasing is performed toward a particular font-family, which occurs most often during the OCR process.
- device 200 uses the font-family selector mechanism 212 to select a font found in database 208 that matches the word region.
- the font-family selector mechanism 212 may include a majority rule 214 which specifies which font occurred within any given word region the most or the majority of the time. The majority rule 214 will then select the font that occurred the most as the font for the entire word region.
- Word region analyzer 216 configured to analyze the word region in a client-side device for a user.
- Word region analyzer 216 may include a spell check 218 to insure that the word region has no misspellings or the like.
- Spell check 218 may be configured as any conventional spell check but without a grammar check feature.
- OCR engines identify word regions as a normal part of their process. Once the augmented OCR engine returns the words detected and their coordinates and font families, the process would look for opportunities to combine adjacent words in to sentences that occur on the same line. Again, on the assumption that artists do not change fonts in the middle of sentences, the device 200 may choose to render an entire line with a single font despite the OCR engine determining that slightly different fonts were detected in adjacent words.
- the text-rendering engine 219 is configured to render text that corresponds to the matched pattern and font-family determined by the font-family selector mechanism 212 and majority rule 214 as described above.
- device 200 uses a text display mechanism 220 configured to display the rendered text over the original image via an image overlay 222 as depicted in FIG. 1 as clarified word region 118 .
- Image overlay 222 is configured to composite and detect the location of the original image using the colors and coordinates of the originally detected contiguous solid-color regions of the image. The color might be determined by sampling color underneath the vector region that would be covered by the rendering of text that is about to take place.
- the rendered text is scaled using the same scaling as in use for the plain text in the given web page and the coordinates are adjusted so that the rendered text substantially covers the original image text.
- FIG. 3 depicts a flow chart showing an example process 300 that implements the techniques described herein for clarification of zoomed text embedded in an image.
- the process 300 may be performed, at least in part, by a browser-client device (not shown) showing web page content.
- the process 300 begins with the browser client device obtaining an image with textual content and then, at 304 , identifying any images embedded with textual content by applying a raster, edge-finding filter to all images embedded in the web page.
- the filter finds contiguous regions of solid colors. Photographs do not typically contain such regions (though line art does). This is a first-pass attempt to avoid images that have no text.
- a banner image 102 might be processed that contains the word “Welcome” in black text on a photographic background wherein the filter identifies each letter as a region of contiguous black color.
- the identified text regions are passed to additional pattern recognition. This may be done as part of the initial OCR scan or after initial OCR scan is complete. For example, a less intense OCR scan may be performed.
- the browser client matches (if possible) the identified textual content against a database of defined text patterns and their font-families (such as database 108 in FIG. 1 ). For instance in FIG. 1 , the act might find that the “W” in “Welcome” most closely matches the upper-case “W” from the font-family “Georgia Bold” while all other characters most closely match the lower-case variants of their counterparts from the font-family “Times New Roman.”
- the browser client groups texts that are spatially located close to each other into a word region and a majority-rule 114 technique is performed to make a font-family selection and to clarify the embedded text for that particular word region.
- a ranking may be applied such as 1 point is assigned to “Georgia Bold” and 6 points is assigned to “Times New Roman” making “Times New Roman” the winning or majority font-family for that particular word region.
- the second-pass pattern recognition is done on each text region at a lower sensitivity but only selecting from a pool of texts in a majority-rule font-family for a particular word region.
- the text selected in the second-pass is assumed to be the correct text. For example in FIG. 2 , “W” “e” “1” “c” “o” “m” “e” might be the result (notice that the number 1 might be detected in place of a lower-case “l”).
- OCR engines detect gaps of space between glyph regions and represent these spatially separated areas as distinct words. This has conventionally been used to augment the OCR engine by way of performing spell check. However, with the new techniques described herein, the same internal data structures are used to weight words in to font-family weighting groups.
- a spell-check 118 is applied to correct miss-identified word region sub-texts.
- the most likely candidate's correct spelling is then chosen. For instance in FIG. 1 , “Welcome” becomes “Welcome”.
- the browser client analyzes word regions and determines which the font-family and pattern that corresponds thereto.
- the browser client renders with the same text overlaying the original image in order to stand out over the blurry text zoomed in underneath.
- the client-side device or browser produces coordinates, words and font data. Based upon that data, the browser client, at 316 , renders the text over the original zoomed image. At 318 , the rendered text is displayed on the screen.
- the rendered text may be a slightly larger and bolder to overlay the rendered text over the original zoomed, unclarified image and displayed 318 .
- Modern OCR engines store the coordinates of the detected, corrected and converted glyph and word regions. For example, many PDF documents contain both original scans of paper documents as well as the OCR results to facilitate copy-and-paste from the document.
- the new techniques described herein reuse this locality data from the OCR engine to inform the text rendering process: the text is painted on the image at the coordinates for the words conveyed from the OCR engines internal representation.
- each chosen replacement text is rendered and composited over the top of the originally detected text location in the original image, respectively, using the coordinates of the originally detected contiguous solid-color regions.
- the rendered text is then scaled using the same scaling as in use for the zoomed image in the web page and the coordinates are adjusted so that the rendered text substantially covers the original image text with each respective replacement text.
- This process is illustrated as a collection of blocks in a logical flow graph, which represents a sequence of operations that can be implemented in mechanics alone or a combination with hardware, software, and/or firmware.
- the blocks represent instructions stored on one or more computer-readable storage media that, when executed by one or more processors, perform the recited operations.
- the technology described herein may omit the use of an initial edge-finding filter 102 and go directly to the first pass at pattern recognition by enhancing the efficiency of the pattern recognition algorithm inside the OCR engine 106 .
- the technology described herein may perform all computation and composting operations on a remote server (e.g. in the cloud) to off-load the task from the end-user device or a user may skip the spell checking 118 or use a statistical model rather than dictionary-based matching, to bias toward complete words.
- the technology may redefine “word” to mean entire regions of text that statistically and commonly occur next to each other in a given language.
- the technology may avoid human languages in which the concept of individual words comprised of contiguous texts does not apply (e.g. many Eastern Asian languages).
- the technology might avoid rendering the original image entirely (e.g. in a “high accessibility, high contrast” mode) and render the matched text as plain web page text, directly.
- the technologies described herein may be used in ebooks by applying the techniques either to ebook documents before loading the documents into a client-device thus bypassing the computational process (i.e., document scan-side of ebooks) or by applying them in the ebooks (i.e., client-side). Either way, the result would render clear embedded text within the ebook document.
- a tangential variation on the technology described herein may be used by in-vehicle camera (i.e., computer vision) and heads up display (HUD) systems to enhance the visibility of text of on the road signs by rendering the detected text onto the windshield via HUD in the line-of-sight between the driver's eyes and the road sign.
- in-vehicle camera i.e., computer vision
- HUD heads up display
- a browser program module is computer program that is designed to be executed by a computer or other computing system.
- a mobile browser program module is a similar computer program module that designed to be executed on a mobile computing device, such as a so-called smartphone.
- any suitable type of technology can be utilized to implement the technologies and techniques described herein.
- suitable, known technologies include (by way of example and not limitation): any mobile device (e.g., smartphones, tablets, ebooks, etc.) and any touchscreen device (e.g., all-in-one desktops, etc.).
- the technologies described herein may include either client side or server side uses depending where it is desired to do the computation.
- FIG. 4 depicts a high-level block diagram illustrating an example computer system 400 suitable for implementing the text clarification device 200 of FIG. 2 .
- the computer system 400 may be implemented using hardware or a combination of software and hardware.
- the illustrated computer system 400 includes a processor 402 , a memory 404 , and data storage 406 coupled to a bus 408 or other communication mechanism for communicating information.
- An input/output (I/O) module 410 is also coupled to the bus 408 .
- a communications module 412 , a device 414 , and a device 416 are coupled to the I/O module 410 .
- the processor 402 may be a general-purpose microprocessor, a microcontroller, a Digital Signal Processor (DSP), an Application Specific Integrated Circuit (ASIC), a Field Programmable Gate Array (FPGA), a Programmable Logic Device (PLD), a controller, a state machine, gated logic, discrete hardware components, or any other suitable entity that can perform calculations or other manipulations of information.
- DSP Digital Signal Processor
- ASIC Application Specific Integrated Circuit
- FPGA Field Programmable Gate Array
- PLD Programmable Logic Device
- controller a state machine, gated logic, discrete hardware components, or any other suitable entity that can perform calculations or other manipulations of information.
- the processor 402 may be used for processing information.
- the processor 402 can be supplemented by, or incorporated in, special purpose logic circuitry.
- the memory 404 may be Random Access Memory (RAM), a flash memory, a Read Only Memory (ROM), a Programmable Read-Only Memory (PROM), an Erasable PROM (EPROM), registers, a hard disk, a removable disk, a CD-ROM, a DVD, or any other suitable storage device used for storing information, a computer program, and/or instructions to be executed by the processor 402 .
- RAM Random Access Memory
- ROM Read Only Memory
- PROM Programmable Read-Only Memory
- EPROM Erasable PROM
- registers a hard disk, a removable disk, a CD-ROM, a DVD, or any other suitable storage device used for storing information, a computer program, and/or instructions to be executed by the processor 402 .
- RAM Random Access Memory
- ROM Read Only Memory
- PROM Programmable Read-Only Memory
- EPROM Erasable PROM
- registers a hard disk, a removable disk, a CD-ROM, a DVD, or any other suitable
- a computer program as discussed herein does not necessarily correspond to a file in a file system.
- a computer program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, subprograms, or portions of code).
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- a module refers to a component that is hardware, firmware, and/or a combination thereof with software (e.g., a computer program.)
- a computer program as discussed herein does not necessarily correspond to a file in a file system.
- a computer program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, subprograms, or portions of code).
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the instructions may be implemented in one or more computer program products, i.e., one or more modules of computer program instructions encoded on one or more computer readable media for execution by, or to control the operation of, the computer system 400 , and according to any method well known to those of skill in the art.
- computer-readable media includes computer-storage media.
- computer-storage media may include, but are not limited to, magnetic storage devices (e.g., hard disk, floppy disk, and magnetic strips), optical disks (e.g., compact disk (CD) and digital versatile disk (DVD)), smart cards, flash memory devices (e.g., thumb drive, stick, key drive, and SD cards), and volatile and non-volatile memory (e.g., random access memory (RAM), read-only memory (ROM)).
- magnetic storage devices e.g., hard disk, floppy disk, and magnetic strips
- optical disks e.g., compact disk (CD) and digital versatile disk (DVD)
- smart cards e.g., compact disk (CD) and digital versatile disk (DVD)
- flash memory devices e.g., thumb drive, stick, key drive, and SD cards
- volatile and non-volatile memory e.g., random access memory (RAM), read-only memory (ROM)
- the data storage 406 may be a magnetic disk or optical disk, for example.
- the data storage 406 may function to store information and instructions to be used by the processor 402 and other components in the computer system 400 .
- the bus 408 may be any suitable mechanism that allows information to be exchanged between components coupled to the bus 508 .
- the bus 408 may be transmission media such as coaxial cables, copper wire, and fiber optics, optical signals, and the like.
- the I/O module 410 can be any input/output module.
- Example input/output modules 410 include data ports such as Universal Serial Bus (USB) ports.
- USB Universal Serial Bus
- the communications module 412 may include networking interface cards, such as Ethernet cards and modems.
- the device 414 may be an input device.
- Example devices 414 include a keyboard, a pointing device, a mouse, or a trackball, by which a user can provide input to the computer system 400 .
- the device 416 may be an output device.
- Example devices 416 include displays such as cathode ray tubes (CRT) or liquid crystal display (LCD) monitors that display information, such as web pages, for example, to the user.
- CTR cathode ray tubes
- LCD liquid crystal display
- the term “or” is intended to mean an inclusive “or” rather than an exclusive “or.” That is, unless specified otherwise or clear from context, “X employs A or B” is intended to mean any of the natural inclusive permutations. That is, if X employs A; X employs B; or X employs both A and B, then “X employs A or B” is satisfied under any of the foregoing instances.
- the articles “a” and “an” as used in this application and the appended claims should generally be construed to mean “one or more,” unless specified otherwise or clear from context to be directed to a singular form.
- any aspect or design described herein as “example” is not necessarily to be construed as preferred or advantageous over other aspects or designs. Rather, use of the word example is intended to present concepts and techniques in a concrete fashion.
- the term “techniques,” for instance, may refer to one or more devices, apparatuses, systems, methods, articles of manufacture, and/or computer-readable instructions as indicated by the context described herein.
Abstract
Described herein are technologies related to clarification of zoomed text embedded in images. This Abstract is submitted with the understanding that it will not be used to interpret or limit the scope or meaning of the claims. A clarification tool includes a raster, edge-finding filter configured to identify text embedded in images, a pattern recognition tool configured to match the identified text against a database of font-families and patterns, a text grouping tool configured to group text into a word region, a font-family selector tool configured to select the proper font-family to use to clarify the embedded text for the word region and a rendering engine configured to render the clarified text in a text display tool.
Description
- Because of their limited screen size, users of mobile devices often zoom-in on a displayed image. Sometimes, these images include text embedded therein. When zoomed or enlarged, the embedded text may appear blurry or pixelated. While conventional mobile devices make use of texture filtering to reduce the appearance of pixilation in an image, this texture filtering does not improve the clarity of image content.
- Thus, when a device displays apparent text that is embedded in an image, that text is often difficult to read or completely unreadable when zoomed or enlarged.
- The technologies described herein are related to text clarification in embedded images. In accordance with one or more implementation described herein, a clarification tool includes a raster, edge-finding filter configured to identify text embedded in images, a pattern recognition tool configured to match the identified text against a database of font-families and patterns, a text grouping tool configured to group text into a word region, a font-family selector tool configured to select the proper font-family to use to clarify the embedded text for the word region and a rendering engine configured to render the clarified text in a text display tool.
- This Summary is submitted with the understanding that it will not be used to interpret or limit the scope or meaning of the claims. This Summary is not intended to identify key features or essential features of the claimed subject matter, nor is it intended to be used as an aid in determining the scope of the claimed subject matter.
-
FIG. 1 is a diagrammatic representation illustrating an example process for clarification of zoomed text embedded in an image in accordance with an implementation of the present technology. -
FIG. 2 is a block diagram illustrating an example device implementing clarification of zoomed text embedded in an image in accordance with an implementation of the present technology. -
FIG. 3 is a flow chart illustrating an example process for implementing clarification of zoomed text embedded in an image in accordance with an implementation of the present technology. -
FIG. 4 is a block diagram illustrating an example of a system for clarification of zoomed text embedded an image in accordance with an implementation of the present technology. - The Detailed Description references the accompanying figures. In the figures, the left-most digit(s) of a reference number identifies the figure in which the reference number first appears. The same numbers are used throughout the drawings to reference like features and components.
- Disclosed herein are technologies for clarification of zoomed text that are embedded in images. These technologies will differentiate the textual-appearing content embedded in images on a web page or other electronic document, identify this content, find/match the font codes for this content, and then generate/render appropriately sized text representing this content in the matched font much like what is seen on the web page text.
- Modern web browsers on computers and mobile devices make use of zooming to improve the accessibility of content for people with low or impaired vision and to make content readable on small screens. Unfortunately, web sites often embed text inside of images: in banners, buttons or other visual elements. When browsers zoom-in on a web page, they succeed at improving the clarity of any plain text in the web page by increasing the size of the rendered, vector graphics or characters (i.e., texts or glyphs). However, they fail to improve the clarity of text embedded in any images on the web page. A glyph is an element of writing including single characters that are each self-contained units of written text or punctuation—a vector, spline-based representation of an individual character in a language.
- The text found in or otherwise embedded in an image is not a glyph. Rather, it is part of the image itself. In computer graphics, that image (i.e., raster graphics image, bitmap) is a dot matrix data structure representing a generally rectangular grid of pixels viewable by a monitor, paper, or other display medium.
- Raster graphics are resolution dependent. They cannot scale up to various resolutions without loss of apparent quality resulting in pixilation or a blurry image appearance. However, as mentioned above vector graphics can easily scale up to the quality of the device rendering them.
- More specifically, vector graphics may be described as the use of geometric characteristics such as points, lines, curves, and shapes or polygons, which are all based on mathematical expressions, to represent images in computer graphics.
- In the technologies described herein, clarity is maintained in a web page when a browser increases the font size of symbol-encoded text of a web page because it redraws the text that is already encoded as vector graphics. However, clarity is lost in textual-appearing content in a raster graphics image during enlargement because the bits making up the “text” are not encoded and are not differentiated from any other bits in the image.
- An implementation in accordance with the technologies described herein draws from four major technological foundations together to improve the clarity of text in images: image filtering, image pattern recognition, a database of commonly used font-families and their patterns, and browser-based, client-side canvas compositing to produce zoomed images with clear text. These technologies makes it easier for a user to read zoomed text embedded over images in a web page or electronic document.
- The technology may be applied to any web browser rendering engine, such as those shared among tablet computers and/or smartphone mobile devices. The technology may be applied further to the rendering of web pages or E-books (which often contain poor quality scans of original published graphics, maps and technical diagrams).
-
FIG. 1 illustrates a diagrammatic representation showing anexample process 100 of clarification of zoomed in text embedded in an image.Example process 100 shows in general a view of anoriginal banner image 102, an image without zoom on aweb page 104, and the text clarification procedure on zoomedimage 106. In more detail, after the image exports toimage format 108, the browser zooms in onimage 110. Next, new texts are drawn onimage 112 and finally thecomposited result 114 is presented. Additionally,FIG. 1 depicts an area of the image that may be referred to as a word region 116 (e.g., “Welcome”). As discussed inFIG. 2 , theword region 116 is clarified, zoomed intoword region 118. Theclarified word region 118 is overlaid and composited onto theoriginal image 102 asnew image 120. -
FIG. 2 shows an example zoomed-text clarification device 200 implementing clarification of zoomed text embedded in images in accordance with an implementation of the present technology. As depicted,device 200 includes araster image filter 202, optical character recognition (OCR)engine 206, and atext display mechanism 220. - The
raster image filter 202 is configured to perform a level of image filtering with regard to contrasts or inverse imaging. For instance,image filter 202 may scan image pixels and lock in the image by performing multiple passes. For example, a pass may locate any text embedded in the image as 100% black or 80% gray on a white background. Further, theimage filter 202 may include a text region identifier 204 configured to distinguish what is text and what is an image and subsequent passes throughfilter 202 may lock in the edges of the text and recognize the font of interest. Examples of approaches that may be employed to identify regions that are likely to contain text are edge-finding approaches or a discreet cosine transform amongst others. This occurs before any OCR and font recognition function has been performed. - The
OCR engine 206 includes image pattern/font-family database 208, a font-family mechanism 212, and a word-region analyzer 216. The OCR engine is used to capture the text and font-family of the image(s) scanned. OCR 206 may include an image pattern/font-family database 208 configured to match image patterns and font-families to textual content found by text region identifier 204. For example, an out-of-the-box OCR software package may be augmented/linked withdatabase 208 and used. The proposed database would contain the “signatures” for each glyph of the most popular fonts. After the normal OCR process were completed, the second pass would look among all the ‘a’s, for example, to find the glyph with the signature that most closely approximates the one detected. - By way of example, OCRopus™ is an open source OCR engine, that allows pluggable back-ends for character recognition; within the OCRopus™ project, IRecognizeLine is the name of the interface for just such an extension and implementing the additional font-family weighting as a module for that interface is an example of one way by which adapting/augmenting an off-the-shelf OCR implementation could be achieved.
- The IRecognizeLine interface is for text line recognizers. In its most common form, it is used to transform images of text lines into recognition lattices. An implementation may process through each letter of every word and match that letter against all known signatures for that letter in the augmented glyph database. At the end of a word, if, for example, the characters most closely approximated ‘Times New Roman’ and two characters most closely approximated ‘Georgia’, the engine would return ‘Times New Roman’ for the whole word based on the assumption that artists do not typically change fonts in the middle of a word.
- A given text or glyph may be defined as a single character in a font-family. The text or glyph is an element, which can be defined further as a self-contained unit in a language. Some texts are based on proximity to each other (i.e., dual characters). East Asian languages have a concept of a text so they can still be described as a self-contained unit represented by a vector to render the texts as well. Thus, the term text is applicable to any language. The
text display mechanism 220 includestext grouping mechanism 210, a text rendering engine, and animage overlay 222. With thetext grouping mechanism 210, thedevice 200 groups the identified textual content into word regions as depicted inFIG. 1 atword region 116. A word region is, for example, a group of identified texts that form a word (e.g., “Welcome”) and a font-family used in embedded text in an image is comprised of a library of texts. - Since textual content is usually rendered by the web page authors using a single font, the
OCR engine 206 discussed above is configured to bias an entire word region toward only one font-family. This biasing is performed toward a particular font-family, which occurs most often during the OCR process. - Moreover,
device 200 uses the font-family selector mechanism 212 to select a font found indatabase 208 that matches the word region. The font-family selector mechanism 212 may include amajority rule 214 which specifies which font occurred within any given word region the most or the majority of the time. Themajority rule 214 will then select the font that occurred the most as the font for the entire word region. - Now
device 200 uses theword region analyzer 216 configured to analyze the word region in a client-side device for a user.Word region analyzer 216 may include aspell check 218 to insure that the word region has no misspellings or the like.Spell check 218 may be configured as any conventional spell check but without a grammar check feature. - OCR engines identify word regions as a normal part of their process. Once the augmented OCR engine returns the words detected and their coordinates and font families, the process would look for opportunities to combine adjacent words in to sentences that occur on the same line. Again, on the assumption that artists do not change fonts in the middle of sentences, the
device 200 may choose to render an entire line with a single font despite the OCR engine determining that slightly different fonts were detected in adjacent words. - The text-
rendering engine 219 is configured to render text that corresponds to the matched pattern and font-family determined by the font-family selector mechanism 212 andmajority rule 214 as described above. - Finally,
device 200 uses atext display mechanism 220 configured to display the rendered text over the original image via animage overlay 222 as depicted inFIG. 1 as clarifiedword region 118.Image overlay 222 is configured to composite and detect the location of the original image using the colors and coordinates of the originally detected contiguous solid-color regions of the image. The color might be determined by sampling color underneath the vector region that would be covered by the rendering of text that is about to take place. The rendered text is scaled using the same scaling as in use for the plain text in the given web page and the coordinates are adjusted so that the rendered text substantially covers the original image text. -
FIG. 3 depicts a flow chart showing anexample process 300 that implements the techniques described herein for clarification of zoomed text embedded in an image. Theprocess 300 may be performed, at least in part, by a browser-client device (not shown) showing web page content. - At 302, the
process 300 begins with the browser client device obtaining an image with textual content and then, at 304, identifying any images embedded with textual content by applying a raster, edge-finding filter to all images embedded in the web page. The filter finds contiguous regions of solid colors. Photographs do not typically contain such regions (though line art does). This is a first-pass attempt to avoid images that have no text. Referring again toFIG. 1 , abanner image 102 might be processed that contains the word “Welcome” in black text on a photographic background wherein the filter identifies each letter as a region of contiguous black color. - At 306, the identified text regions are passed to additional pattern recognition. This may be done as part of the initial OCR scan or after initial OCR scan is complete. For example, a less intense OCR scan may be performed. At 308, the browser client matches (if possible) the identified textual content against a database of defined text patterns and their font-families (such as
database 108 inFIG. 1 ). For instance inFIG. 1 , the act might find that the “W” in “Welcome” most closely matches the upper-case “W” from the font-family “Georgia Bold” while all other characters most closely match the lower-case variants of their counterparts from the font-family “Times New Roman.” - Next, at 310, once a candidate match for each text is identified, the browser client groups texts that are spatially located close to each other into a word region and a majority-
rule 114 technique is performed to make a font-family selection and to clarify the embedded text for that particular word region. By way of the above example, a ranking may be applied such as 1 point is assigned to “Georgia Bold” and 6 points is assigned to “Times New Roman” making “Times New Roman” the winning or majority font-family for that particular word region. - Optionally, during a second-pass pattern recognition inside the
same OCR engine 106, for each word region, the second-pass pattern recognition is done on each text region at a lower sensitivity but only selecting from a pool of texts in a majority-rule font-family for a particular word region. The text selected in the second-pass is assumed to be the correct text. For example inFIG. 2 , “W” “e” “1” “c” “o” “m” “e” might be the result (notice that the number 1 might be detected in place of a lower-case “l”). - OCR engines detect gaps of space between glyph regions and represent these spatially separated areas as distinct words. This has conventionally been used to augment the OCR engine by way of performing spell check. However, with the new techniques described herein, the same internal data structures are used to weight words in to font-family weighting groups.
- Optionally, a spell-
check 118 is applied to correct miss-identified word region sub-texts. The most likely candidate's correct spelling is then chosen. For instance inFIG. 1 , “Welcome” becomes “Welcome”. - At 312, the browser client analyzes word regions and determines which the font-family and pattern that corresponds thereto. At 314, the browser client renders with the same text overlaying the original image in order to stand out over the blurry text zoomed in underneath.
- During the OCR action of 306, the client-side device or browser produces coordinates, words and font data. Based upon that data, the browser client, at 316, renders the text over the original zoomed image. At 318, the rendered text is displayed on the screen. The rendered text may be a slightly larger and bolder to overlay the rendered text over the original zoomed, unclarified image and displayed 318.
- Modern OCR engines store the coordinates of the detected, corrected and converted glyph and word regions. For example, many PDF documents contain both original scans of paper documents as well as the OCR results to facilitate copy-and-paste from the document. However, the new techniques described herein reuse this locality data from the OCR engine to inform the text rendering process: the text is painted on the image at the coordinates for the words conveyed from the OCR engines internal representation.
- In the browser rendering engine, each chosen replacement text is rendered and composited over the top of the originally detected text location in the original image, respectively, using the coordinates of the originally detected contiguous solid-color regions. The rendered text is then scaled using the same scaling as in use for the zoomed image in the web page and the coordinates are adjusted so that the rendered text substantially covers the original image text with each respective replacement text.
- This process is illustrated as a collection of blocks in a logical flow graph, which represents a sequence of operations that can be implemented in mechanics alone or a combination with hardware, software, and/or firmware. In the context of software/firmware, the blocks represent instructions stored on one or more computer-readable storage media that, when executed by one or more processors, perform the recited operations.
- Note that the order in which the process are described is not intended to be construed as a limitation, and any number of the described process blocks can be combined in any order to implement the process or an alternate process. Additionally, individual blocks may be deleted from the process without departing from the spirit and scope of the subject matter described herein.
- Alternatively and in further implementations, the technology described herein may omit the use of an initial edge-finding
filter 102 and go directly to the first pass at pattern recognition by enhancing the efficiency of the pattern recognition algorithm inside theOCR engine 106. Additionally, the technology described herein may perform all computation and composting operations on a remote server (e.g. in the cloud) to off-load the task from the end-user device or a user may skip the spell checking 118 or use a statistical model rather than dictionary-based matching, to bias toward complete words. Moreover, the technology may redefine “word” to mean entire regions of text that statistically and commonly occur next to each other in a given language. Furthermore, the technology may avoid human languages in which the concept of individual words comprised of contiguous texts does not apply (e.g. many Eastern Asian languages). In addition, the technology might avoid rendering the original image entirely (e.g. in a “high accessibility, high contrast” mode) and render the matched text as plain web page text, directly. - Additionally, the technologies described herein may be used in ebooks by applying the techniques either to ebook documents before loading the documents into a client-device thus bypassing the computational process (i.e., document scan-side of ebooks) or by applying them in the ebooks (i.e., client-side). Either way, the result would render clear embedded text within the ebook document.
- Alternatively, a tangential variation on the technology described herein may be used by in-vehicle camera (i.e., computer vision) and heads up display (HUD) systems to enhance the visibility of text of on the road signs by rendering the detected text onto the windshield via HUD in the line-of-sight between the driver's eyes and the road sign.
- As used herein, a browser program module is computer program that is designed to be executed by a computer or other computing system. A mobile browser program module is a similar computer program module that designed to be executed on a mobile computing device, such as a so-called smartphone.
- Any suitable type of technology can be utilized to implement the technologies and techniques described herein. Examples of suitable, known technologies include (by way of example and not limitation): any mobile device (e.g., smartphones, tablets, ebooks, etc.) and any touchscreen device (e.g., all-in-one desktops, etc.). Again, the technologies described herein may include either client side or server side uses depending where it is desired to do the computation.
-
FIG. 4 depicts a high-level block diagram illustrating anexample computer system 400 suitable for implementing thetext clarification device 200 ofFIG. 2 . In certain aspects, thecomputer system 400 may be implemented using hardware or a combination of software and hardware. - The illustrated
computer system 400 includes aprocessor 402, amemory 404, anddata storage 406 coupled to a bus 408 or other communication mechanism for communicating information. An input/output (I/O)module 410 is also coupled to the bus 408. Acommunications module 412, adevice 414, and adevice 416 are coupled to the I/O module 410. - The
processor 402 may be a general-purpose microprocessor, a microcontroller, a Digital Signal Processor (DSP), an Application Specific Integrated Circuit (ASIC), a Field Programmable Gate Array (FPGA), a Programmable Logic Device (PLD), a controller, a state machine, gated logic, discrete hardware components, or any other suitable entity that can perform calculations or other manipulations of information. Theprocessor 402 may be used for processing information. Theprocessor 402 can be supplemented by, or incorporated in, special purpose logic circuitry. - The
memory 404 may be Random Access Memory (RAM), a flash memory, a Read Only Memory (ROM), a Programmable Read-Only Memory (PROM), an Erasable PROM (EPROM), registers, a hard disk, a removable disk, a CD-ROM, a DVD, or any other suitable storage device used for storing information, a computer program, and/or instructions to be executed by theprocessor 402. Theymemory 404 may store code that creates an execution environment for one or more computer programs used to implement technology described herein. - A computer program as discussed herein does not necessarily correspond to a file in a file system. A computer program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, subprograms, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- Unless indicated otherwise by the context, a module refers to a component that is hardware, firmware, and/or a combination thereof with software (e.g., a computer program.) A computer program as discussed herein does not necessarily correspond to a file in a file system. A computer program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, subprograms, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- The instructions may be implemented in one or more computer program products, i.e., one or more modules of computer program instructions encoded on one or more computer readable media for execution by, or to control the operation of, the
computer system 400, and according to any method well known to those of skill in the art. The term “computer-readable media” includes computer-storage media. For example, computer-storage media may include, but are not limited to, magnetic storage devices (e.g., hard disk, floppy disk, and magnetic strips), optical disks (e.g., compact disk (CD) and digital versatile disk (DVD)), smart cards, flash memory devices (e.g., thumb drive, stick, key drive, and SD cards), and volatile and non-volatile memory (e.g., random access memory (RAM), read-only memory (ROM)). - The
data storage 406 may be a magnetic disk or optical disk, for example. Thedata storage 406 may function to store information and instructions to be used by theprocessor 402 and other components in thecomputer system 400. - The bus 408 may be any suitable mechanism that allows information to be exchanged between components coupled to the bus 508. For example, the bus 408 may be transmission media such as coaxial cables, copper wire, and fiber optics, optical signals, and the like.
- The I/
O module 410 can be any input/output module. Example input/output modules 410 include data ports such as Universal Serial Bus (USB) ports. - The
communications module 412 may include networking interface cards, such as Ethernet cards and modems. - The
device 414 may be an input device.Example devices 414 include a keyboard, a pointing device, a mouse, or a trackball, by which a user can provide input to thecomputer system 400. - The
device 416 may be an output device.Example devices 416 include displays such as cathode ray tubes (CRT) or liquid crystal display (LCD) monitors that display information, such as web pages, for example, to the user. - One or more implementations are described herein with reference to illustrations for particular applications. It should be understood that the implementations are not intended to be limiting. Those skilled in the art with access to the teachings provided herein will recognize additional modifications, applications, and implementations within the scope thereof and additional fields in which the technology would be of significant utility. In the above description of example implementations, for purposes of explanation, specific numbers, materials, configurations, and other details are set forth in order to better explain implementations as claimed. However, it will be apparent to one skilled in the art that the claims may be practiced using details different from the examples described herein. In other instances, well-known features are omitted or simplified to clarify the description of the example implementations.
- As used in this application, the term “or” is intended to mean an inclusive “or” rather than an exclusive “or.” That is, unless specified otherwise or clear from context, “X employs A or B” is intended to mean any of the natural inclusive permutations. That is, if X employs A; X employs B; or X employs both A and B, then “X employs A or B” is satisfied under any of the foregoing instances. In addition, the articles “a” and “an” as used in this application and the appended claims should generally be construed to mean “one or more,” unless specified otherwise or clear from context to be directed to a singular form.
- The inventors intend the described example implementations to be primarily examples. The inventors do not intend these example implementations to limit the scope of the appended claims. Rather, the inventors have contemplated that the claimed technology might also be embodied and implemented in other ways, in conjunction with other present or future technologies.
- Moreover, any aspect or design described herein as “example” is not necessarily to be construed as preferred or advantageous over other aspects or designs. Rather, use of the word example is intended to present concepts and techniques in a concrete fashion. The term “techniques,” for instance, may refer to one or more devices, apparatuses, systems, methods, articles of manufacture, and/or computer-readable instructions as indicated by the context described herein.
- In the claims appended herein, the inventor invokes 35 U.S.C. §112, paragraph 6 only when the words “means for” or “steps for” are used in the claim. If such words are not used in a claim, then the inventor does not intend for the claim to be construed to cover the corresponding structure, material, or acts described herein (and equivalents thereof) in accordance with 35 U.S.C. §112, paragraph 6.
Claims (22)
1. A method that facilitates text clarification, the method comprising:
obtaining an image that has textual content;
identifying the textual content by filtering the image via a raster, edge-finding filter to locate the textual content within the image by finding contiguous regions of solid colors in the image;
passing the identified textual content to a high-sensitivity pattern recognition scan;
matching the identified textual content to a database of defined image patterns and font-families;
grouping the identified textual content into word regions by associating any identified textual content that is spaced proximal to one another into the word regions;
analyzing the word regions to correspond to the matched patterns and font-families;
rendering the word regions as text; and
displaying the rendered text overlaid with the word regions in the image.
2. A method as recited in claim 1 wherein the analyzing includes:
determining a scale of the rendered text for the matched patterns and font-families; and
adjusting the scale of the rendered text to match the scale of the identified textual content.
3. A method as recited in claim 1 wherein the analyzing includes applying a spell check to the word regions.
4. A method as recited in claim 1 wherein the analyzing includes determining a font-family for the word regions.
5. A method as recited in claim 4 wherein the determining includes:
applying a majority rule to select a font for the word regions, wherein the majority of the matched patterns and font-families with respect to the identified textual content is the chosen patterns and font-families for the word regions.
6. A method as recited in claim 1 wherein the overlaying includes:
compositing the rendered word regions over the top of an image location by using coordinates of the contiguous regions of solid colors in the image.
7. A method as recited in claim 6 wherein the overlaying includes:
scaling the rendered word regions to represent an enlarged image of the obtained image.
8. A method as recited in claim 1 wherein the analyzing, further includes:
applying a low-sensitivity pattern recognition scan to the word regions, wherein the pattern recognition scan is configured to only select the matched patterns and font-families for the word regions by the majority rule.
9. A computing system comprising a web browser program module that includes one or more computer-readable media having stored thereon instructions that, when executed on one or more processors, direct the one or more processors to perform the method as recited in claim 1 .
10. A mobile computing system comprising a mobile web browser program module that includes one or more computer-readable media having stored thereon instructions that, when executed on one or more processors, direct the one or more processors to perform the method as recited in claim 1 .
11. A system that facilitates text clarification comprising:
an image filter configured to apply a raster, edge-finding filter to an image to identify any textual content within the image by finding contiguous regions of solid colors in the image;
an optical character recognition (OCR) engine configured to apply a high-sensitivity optical character recognition scan to the identified textual content, wherein the OCR engine includes an image pattern and font-family database;
a text grouping mechanism configured to group the identified text content into word regions by associating any identified textual content that is spaced proximal to one another into the word regions;
a font-family selector mechanism configured to select and match a font-family from the image pattern and font-family database to the word regions;
a word region analyzer configured to analyze the word regions to render text that corresponds to the matched pattern and font-family;
a text-rendering engine configured to render the text to correspond to the matched pattern and font-family; and
a display configured to display and overlay the rendered text.
12. A system as recited in claim 11 wherein the word region analyzer is configured to:
determine a scale of the rendered text for the matched pattern and font-family of the word region; and
adjust the scale of the rendered text to match the scale of the word region in the image.
13. A system as recited in claim 11 wherein the display is configured to:
composite the rendered word regions over the top of an image location by using coordinates of the contiguous regions of solid colors in the image.
14. A system as recited in claim 11 wherein the image pattern and font-family database is configured to hold defined image patterns and font-families and to match the patterns and font-families to the corresponding identified textual content.
15. A system as recited in claim 11 wherein the font-family selector mechanism is configured to apply a majority rule to select a font for the word regions, wherein the majority of the matched pattern and font-family with respect to the identified textual content is the chosen pattern and font-family for the word regions.
16. One or more computer-readable media having stored thereon instructions that, when executed on one or more processors, direct the one or more processors to perform operations for text clarification, the operations comprising:
obtaining an image that has textual content;
identifying the textual content by filtering the image via a raster, edge-finding filter to locate the textual content within the image by finding contiguous regions of solid colors in the image;
passing the identified textual content to a high-sensitivity pattern recognition scan;
matching the identified textual content to a database of defined image patterns and font-families;
grouping the identified textual content into word regions by associating any identified textual content that is spaced proximal to one another into the word regions;
analyzing the word regions to correspond to the matched patterns and font-families;
rendering the word regions as text; and
displaying the rendered text overlaid with the word regions in the image.
17. One or more computer-readable media as recited in claim 16 wherein the analyzing includes:
determining a scale of the rendered text for the matched pattern and font-family of the word regions; and
adjusting the scale of the rendered text to match the scale of the word regions in the image.
18. One or more computer-readable media as recited in claim 16 wherein the analyzing includes:
applying a spell check to the word regions.
19. One or more computer-readable media as recited in claim 16 , wherein the matching includes:
determining a font-family for the word region by applying a majority rule to select a font for the word regions, wherein the majority of the matched pattern and font-family with respect to the identified textual content is the chosen pattern and font-family for the word regions.
20. One or more computer-readable media as recited in claim 16 , wherein the overlaying includes:
compositing the rendered word regions over the top of an image location by using coordinates of the contiguous regions of solid colors in the image.
21. One or more computer-readable media as recited in claim 20 , wherein the overlaying includes:
scaling the rendered word regions to represent an enlarged image of the obtained image.
22. One or more computer-readable media as recited in claim 16 , wherein the analyzing further includes:
applying a low-sensitivity pattern recognition scan to the word regions, wherein the pattern recognition scan is configured to only select the matched pattern and font-family for the word regions by the majority rule.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/593,300 US20150070373A1 (en) | 2012-08-23 | 2012-08-23 | Clarification of Zoomed Text Embedded in Images |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/593,300 US20150070373A1 (en) | 2012-08-23 | 2012-08-23 | Clarification of Zoomed Text Embedded in Images |
Publications (1)
Publication Number | Publication Date |
---|---|
US20150070373A1 true US20150070373A1 (en) | 2015-03-12 |
Family
ID=52625156
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/593,300 Abandoned US20150070373A1 (en) | 2012-08-23 | 2012-08-23 | Clarification of Zoomed Text Embedded in Images |
Country Status (1)
Country | Link |
---|---|
US (1) | US20150070373A1 (en) |
Cited By (14)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20140372870A1 (en) * | 2013-06-17 | 2014-12-18 | Tencent Technology (Shenzhen) Company Limited | Method, device and system for zooming font in web page file, and storage medium |
US20160241865A1 (en) * | 2015-02-16 | 2016-08-18 | Disney Enterprises, Inc. | Systems and Methods for Embedding Metadata into Video Contents |
CN105956147A (en) * | 2016-05-13 | 2016-09-21 | 广西骋天信息科技有限公司 | Method and system for generating budget sheet according to drawing |
CN106127118A (en) * | 2016-06-15 | 2016-11-16 | 珠海迈科智能科技股份有限公司 | A kind of English word recognition methods and device |
US20180035006A1 (en) * | 2016-07-29 | 2018-02-01 | Kyocera Document Solutions Inc. | Print transformation effects |
US20180285681A1 (en) * | 2017-03-29 | 2018-10-04 | Konica Minolta, Inc. | Image processing apparatus, control method therefor, and program |
US10437918B1 (en) * | 2015-10-07 | 2019-10-08 | Google Llc | Progressive image rendering using pan and zoom |
WO2023078281A1 (en) * | 2021-11-05 | 2023-05-11 | 北京字节跳动网络技术有限公司 | Picture processing method and apparatus, device, storage medium and program product |
EP4180769A1 (en) * | 2021-11-12 | 2023-05-17 | Rockwell Collins, Inc. | System and method for providing more readable font characters in size adjusting avionics charts |
US11842429B2 (en) | 2021-11-12 | 2023-12-12 | Rockwell Collins, Inc. | System and method for machine code subroutine creation and execution with indeterminate addresses |
US11854110B2 (en) | 2021-11-12 | 2023-12-26 | Rockwell Collins, Inc. | System and method for determining geographic information of airport terminal chart and converting graphical image file to hardware directives for display unit |
US11887222B2 (en) | 2021-11-12 | 2024-01-30 | Rockwell Collins, Inc. | Conversion of filled areas to run length encoded vectors |
US11915389B2 (en) | 2021-11-12 | 2024-02-27 | Rockwell Collins, Inc. | System and method for recreating image with repeating patterns of graphical image file to reduce storage space |
US11954770B2 (en) | 2021-11-12 | 2024-04-09 | Rockwell Collins, Inc. | System and method for recreating graphical image using character recognition to reduce storage space |
Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6701023B1 (en) * | 1999-09-28 | 2004-03-02 | Adobe Systems Incorporated | Reducing appearance differences between coded and noncoded units of text |
US20080089594A1 (en) * | 2006-10-13 | 2008-04-17 | Bayer Theodore F | Method and system for converting image text documents in bit-mapped formats to searchable text and for searching the searchable text |
US20090016647A1 (en) * | 2007-07-10 | 2009-01-15 | Canon Kabushiki Kaisha | Image processing apparatus and control method thereof |
US20100329555A1 (en) * | 2009-06-23 | 2010-12-30 | K-Nfb Reading Technology, Inc. | Systems and methods for displaying scanned images with overlaid text |
-
2012
- 2012-08-23 US US13/593,300 patent/US20150070373A1/en not_active Abandoned
Patent Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6701023B1 (en) * | 1999-09-28 | 2004-03-02 | Adobe Systems Incorporated | Reducing appearance differences between coded and noncoded units of text |
US20080089594A1 (en) * | 2006-10-13 | 2008-04-17 | Bayer Theodore F | Method and system for converting image text documents in bit-mapped formats to searchable text and for searching the searchable text |
US20090016647A1 (en) * | 2007-07-10 | 2009-01-15 | Canon Kabushiki Kaisha | Image processing apparatus and control method thereof |
US20100329555A1 (en) * | 2009-06-23 | 2010-12-30 | K-Nfb Reading Technology, Inc. | Systems and methods for displaying scanned images with overlaid text |
Cited By (18)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20140372870A1 (en) * | 2013-06-17 | 2014-12-18 | Tencent Technology (Shenzhen) Company Limited | Method, device and system for zooming font in web page file, and storage medium |
US9916287B2 (en) * | 2013-06-17 | 2018-03-13 | Tencent Technology (Shenzhen) Company Limited | Method, device and system for zooming font in web page file, and storage medium |
US10154275B2 (en) * | 2015-02-16 | 2018-12-11 | Disney Enterprises, Inc. | Systems and methods for embedding metadata into video contents |
US20160241865A1 (en) * | 2015-02-16 | 2016-08-18 | Disney Enterprises, Inc. | Systems and Methods for Embedding Metadata into Video Contents |
US10437918B1 (en) * | 2015-10-07 | 2019-10-08 | Google Llc | Progressive image rendering using pan and zoom |
CN105956147A (en) * | 2016-05-13 | 2016-09-21 | 广西骋天信息科技有限公司 | Method and system for generating budget sheet according to drawing |
CN106127118A (en) * | 2016-06-15 | 2016-11-16 | 珠海迈科智能科技股份有限公司 | A kind of English word recognition methods and device |
US20180035006A1 (en) * | 2016-07-29 | 2018-02-01 | Kyocera Document Solutions Inc. | Print transformation effects |
US20180285681A1 (en) * | 2017-03-29 | 2018-10-04 | Konica Minolta, Inc. | Image processing apparatus, control method therefor, and program |
US10607103B2 (en) * | 2017-03-29 | 2020-03-31 | Konica Minolta, Inc. | Image processing apparatus, control method therefor, and program |
WO2023078281A1 (en) * | 2021-11-05 | 2023-05-11 | 北京字节跳动网络技术有限公司 | Picture processing method and apparatus, device, storage medium and program product |
EP4180769A1 (en) * | 2021-11-12 | 2023-05-17 | Rockwell Collins, Inc. | System and method for providing more readable font characters in size adjusting avionics charts |
US11748923B2 (en) | 2021-11-12 | 2023-09-05 | Rockwell Collins, Inc. | System and method for providing more readable font characters in size adjusting avionics charts |
US11842429B2 (en) | 2021-11-12 | 2023-12-12 | Rockwell Collins, Inc. | System and method for machine code subroutine creation and execution with indeterminate addresses |
US11854110B2 (en) | 2021-11-12 | 2023-12-26 | Rockwell Collins, Inc. | System and method for determining geographic information of airport terminal chart and converting graphical image file to hardware directives for display unit |
US11887222B2 (en) | 2021-11-12 | 2024-01-30 | Rockwell Collins, Inc. | Conversion of filled areas to run length encoded vectors |
US11915389B2 (en) | 2021-11-12 | 2024-02-27 | Rockwell Collins, Inc. | System and method for recreating image with repeating patterns of graphical image file to reduce storage space |
US11954770B2 (en) | 2021-11-12 | 2024-04-09 | Rockwell Collins, Inc. | System and method for recreating graphical image using character recognition to reduce storage space |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20150070373A1 (en) | Clarification of Zoomed Text Embedded in Images | |
US8379027B2 (en) | Rendering engine test system | |
US8761511B2 (en) | Preprocessing of grayscale images for optical character recognition | |
EP1361544B1 (en) | System and method for editing electronic images | |
US9898548B1 (en) | Image conversion of text-based images | |
US7912322B2 (en) | Method and apparatus for magnifying computer screen display | |
US20040075699A1 (en) | Method and apparatus for highlighting graphical objects | |
CN109948549B (en) | OCR data generation method and device, computer equipment and storage medium | |
US9218680B2 (en) | Systems and methods for rendering graphical content and glyphs | |
US20040202352A1 (en) | Enhanced readability with flowed bitmaps | |
CN110569839B (en) | Bank card number identification method based on CTPN and CRNN | |
KR20200020305A (en) | Method and Apparatus for character recognition | |
US20210217180A1 (en) | Method and apparatus for generating background-free image, device, and medium | |
WO2022002002A1 (en) | Image processing method, image processing apparatus, electronic device, and storage medium | |
US8824806B1 (en) | Sequential digital image panning | |
CN113762235A (en) | Method and device for detecting page overlapping area | |
Sandnes | Lost in OCR-translation: pixel-based text reflow to the rescue: magnification of archival raster image documents in the browser without horizontal scrolling | |
KR20140116777A (en) | Display apparatus and Method for outputting text thereof | |
US20100017708A1 (en) | Information output apparatus, information output method, and recording medium | |
Rathi et al. | Recognition and conversion of handwritten MODI characters | |
JP3171626B2 (en) | Character recognition processing area / processing condition specification method | |
EP2466548A1 (en) | Method of processing an object-based image file with content type dependent image processing algorithms | |
JP2013003990A (en) | Image processing device, image processing method, and program | |
JP5277750B2 (en) | Image processing program, image processing apparatus, and image processing system | |
Nakamura et al. | Scene text magnifier |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:CLINTON, JASON, MR.;REEL/FRAME:028861/0226Effective date: 20120821 |
|
STCB | Information on status: application discontinuation |
Free format text: ABANDONED -- FAILURE TO RESPOND TO AN OFFICE ACTION |