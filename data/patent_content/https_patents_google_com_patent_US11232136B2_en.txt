US11232136B2 - Contextual voice search suggestions - Google Patents
Contextual voice search suggestions Download PDFInfo
- Publication number
- US11232136B2 US11232136B2 US15/194,153 US201615194153A US11232136B2 US 11232136 B2 US11232136 B2 US 11232136B2 US 201615194153 A US201615194153 A US 201615194153A US 11232136 B2 US11232136 B2 US 11232136B2
- Authority
- US
- United States
- Prior art keywords
- entity
- terms
- digital assistant
- computing devices
- suggested
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/332—Query formulation
- G06F16/3322—Query formulation using system suggestions
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/31—Indexing; Data structures therefor; Storage structures
- G06F16/313—Selection or weighting of terms for indexing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/3331—Query processing
- G06F16/334—Query execution
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/35—Clustering; Classification
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/451—Execution arrangements for user interfaces
- G06F9/453—Help systems
-
- G06K9/18—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V30/00—Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition
- G06V30/10—Character recognition
- G06V30/22—Character recognition characterised by the type of writing
- G06V30/224—Character recognition characterised by the type of writing of printed characters having additional code marks or containing code marks
Definitions
- This specification relates to search engines.
- a user can request information by inputting a query to a search engine.
- the search engine can process the query and can provide information for output to the user in response to the query.
- This specification describes systems and methods for providing suggested queries to a user device in response to receiving user input invoking digital assistant functionality, e.g., a virtual digital assistant or conversational agent, at the user device.
- digital assistant functionality e.g., a virtual digital assistant or conversational agent
- the system analyzes screen data indicating content displayed on the user device to identify one or more entities referenced by the content.
- the system also constructs suggested requests, e.g., queries, that refer to the identified entities.
- the suggested requests may be presented to the user on the user device.
- inventions of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- a system of one or more computers can be configured to perform particular operations or actions by virtue of having software, firmware, hardware, or a combination thereof installed on the system that in operation causes or cause the system to perform the actions.
- One or more computer programs can be configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
- determining the classification comprises selecting an entity type for the entity from among a plurality of entity types.
- determining the suggested request comprises: determining a pronoun for the entity; and generating a suggested query that includes the pronoun to refer to the entity referenced in the content indicated by the screen data.
- determining the pronoun comprises: accessing mapping data that indicates pronouns for a plurality of entities; and selecting a pronoun that the mapping data indicates corresponding to the entity.
- determining the suggested request comprises: identifying one or more query types associated with the classification; selecting a particular query type from the one or more query types; and determining a suggested query of the particular query type.
- the method further comprises determining one or more attributes associated with the entity type, and determining a query that asks about one of the attributes.
- determining the suggested request comprises determining a suggested request to initiate communication with a person or to navigate to a location.
- the method further comprises recognizing one or more terms displayed on the screen of the computing device using optical character recognition on the screen data; and wherein determining the classification comprises determining a semantic classification for the recognized one or more terms.
- recognizing the one or more terms comprises recognizing multiple different terms; wherein the method further comprises ranking the recognized multiple different terms; and wherein determining the suggested request comprises determining a suggested request that refers to the entity corresponding to the highest-ranking term.
- the method further comprises determining text formatting characteristics of the multiple different terms indicated by the screen data; wherein ranking the recognized multiple different terms comprises ranking the multiple different terms based at least in part on the determined formatting characteristics.
- ranking the recognized multiple different terms comprises ranking the multiple different terms based at least in part on (i) classifications assigned to the multiple different terms, (ii) sizes at which the multiple different terms are displayed, (iii) frequencies that the multiple different terms are displayed on the screen, (iv) quantities of actions that can be generated for the multiple different terms, or (v) measures of previous suggestions relating to the multiple different terms that were provided and acted upon by a user.
- the method further comprises determining that the screen references only a single entity or only a single entity of a particular entity type; and wherein providing the suggested request is performed in response to determining only a single entity or only a single entity of a particular entity type.
- a system implementing contextual voice search suggestions processes user device screen capture information locally to generate suggested voice queries directly on the user device. This avoids the need of communicating with external servers over one or more networks and enables suggested voice queries to be provided efficiently with lower latency than other systems that provide voice search suggestions. For example, network round-trip latency may result in a delay of several seconds for many networks. By contrast, using analysis of screen data to identify entities referenced when performed locally, a client device can often provide a suggested query to a user in less than a second. This reduction in delay can significantly enhance a user's experience when interacting with a digital assistant.
- the local analysis of contextual data and assessment of entities referenced allows the system to provide suggested requests when bandwidth is limited or when a network connection is unavailable.
- contextual voice search suggestions may be generated and provided when the system has unreliable or no network connectivity.
- network results relating to the generated contextual voice search suggestions e.g., search results pages, may be appended to the locally-generated contextual voice search suggestions. Since the network results do not overlap with the contextual voice search suggestions generated by the system, no additional computations are required to de-duplicate on-device contextual voice search suggestions with network actions.
- a system that generates contextual voice search suggestions may be activated when digital assistant functionality of a user device is invoked from any interface.
- the system may read data from any third party application running on the user device in order to generate contextual voice search suggestions. This ability allows the digital assistant functionality the flexibility to provide contextually relevant suggested queries for any user interface of any application.
- a system that generates contextual voice search suggestions is able to train a user of a user device to talk about what is displayed on the screen of a user device when interacting with the digital assistant.
- Many users are not aware that the digital assistant understands the context of what is shown on the screen, or that the digital assistant can properly interpret pronouns and other references to the content.
- the contextually appropriate suggestions demonstrate the digital assistant's ability to read and understand content shown on the user interface and to determine context of a user request.
- a system that generates contextual voice search suggestions may incorporate past actions a user has taken or places a user has visited when generating contextual voice search suggestions.
- FIG. 1 is a diagram that illustrates an example system for providing a suggested search request in response to receiving user input invoking a digital assistant.
- FIGS. 2A-2C are diagrams that illustrate example user interfaces.
- FIG. 3 depicts a flowchart of an example process for providing a search request in response to receiving user input invoking a digital assistant.
- This specification describes a system for using contextual information from a user device, e.g., screen-capture information, to generate suggested requests, e.g., suggested voice queries or voice commands.
- the suggested requests may be used to introduce a user to additional functionalities of a personal digital assistant available on a user device, such as the possibility of using pronouns that refer to content displayed on the user device when communicating with the personal digital assistant.
- FIG. 1 is a diagram that illustrates an example of a system 100 for providing a suggested search request in response to receiving user input invoking a digital assistant.
- the system 100 includes a user device 104 .
- the user device 104 includes an analysis module 110 , a term identifier module 120 , a semantic model 130 , a ranking module 140 and a search query generator module 150 .
- the user device 104 may communicate with one or more servers 106 over a network 108 .
- the diagram shows steps (A) to (G), which illustrate a flow of data, and which may occur in the sequence as illustrated or in a different sequence.
- the user device 104 can be a computing device, e.g., a mobile phone, smart phone, personal digital assistant (PDA), music player, e-book reader, tablet computer, a wearable computing device, laptop computer, desktop computer, or other portable or stationary computing device.
- the user device 104 can feature a microphone, keyboard, touchscreen, or other interface that enables the user 102 to provide inputs to the user device 104 .
- the user device 104 includes a digital assistant functionality, e.g., a computer program that works as a personal assistant and performs tasks or services for a user 102 of the user device 104 .
- the digital assistant functionality may perform tasks or services, e.g., answering user 102 questions or making recommendations, based on received user input, location awareness, and information accessed from a variety of online sources, e.g., weather or traffic conditions, news, user schedules, or retail prices.
- the digital assistant functionality may access a large networked database of information, e.g., hypertext information, and use software agents to assist searching for information.
- Digital assistants are often configured to accept various forms of input from a user, such as voice inputs and typed inputs.
- digital assistants may provide various forms of output, such as a graphical user interface and synthesized speech or text-to-speech output.
- the user device 104 displays a user interface (UI) 112 .
- the user device 104 may display an application installed or otherwise running on the user device 104 .
- the user device 104 displays a UI 112 that includes a browser showing content relating to a news article.
- the user device 104 is configured to provide digital assistant functionality, as discussed below.
- the digital assistant functionality may be part of the operating system of the user device 104 , an application of the user device 104 , or other software.
- the digital assistant functionality is separate from the application shown on the user device 104 , and the digital assistant functionality may be used with any third-party application or interface on the user device 104 .
- a user 102 of the user device 104 provides input that invokes a digital assistant functionality of the user device 104 .
- the provided input that invokes the digital assistant functionality of the user device 104 may include pressing a home button of the user device 104 for an extended period of time, pressing an on-screen button of the user device 104 , selecting or highlighting a term visible on the screen, invoking the digital assistant functionality using an application installed on the user device 104 , speaking a hotword to activate the digital assistant functionality, or providing other user input.
- the user 102 of the user device 104 invokes the digital assistant functionality of the user device 104 by pressing an on-screen button displayed on the UI 112 of the user device 104 .
- the analysis module 110 receives data encoding the UI and analyzes the UI to capture context information for the UI.
- the analysis module 110 may capture context information for the UI in response to the user 102 providing input to the user device 104 that invokes the digital assistant functionality of the user device 104 .
- the analysis module 110 may capture context information for the UI using a system-supported method of obtaining screen information, e.g., requesting data from a framebuffer of the user device 104 .
- Captured context information for the UI may include visible on-screen information, e.g., a screenshot or bitmap of what is shown on the screen at the time the digital assistant was invoked.
- the captured context information for the UI 112 may include a bitmap of the browser showing content relating to a news article, as shown in FIG. 1 .
- Other types of screen data may also be obtained.
- the analysis module 110 may submit a request to the operating system of the user device 104 for text that is shown on the screen. In addition to receiving data indicating the visible text, the analysis module 110 may receive data indicating the location, placement, and formatting of the text.
- Captured context information for the UI may further include off-screen information, e.g., information in hidden “drawers” of the UI, e.g., content that is not currently visible, but would be shown in a left/right/up/down swiping or scrolling view of the screen.
- the captured context information for the UI may include both on-screen and off-screen information available in a web page displayed in a browser on the user device 104 .
- captured context information for the UI may include information relating to multiple open conversations in an instant messaging service running on the user device 104 , even if one or more of the open conversations have been minimized.
- the analysis module 110 may further capture context information for the UI relating to previous user interactions with the user device 104 , e.g., from a log, from a data cache, or by communicating with the one or more servers 106 .
- the analysis module 110 may access information relating to previous voice search queries input to the user device 104 by the user 102 .
- the analysis module may analyze the previous voice search queries to capture context information for the UI, e.g., information relating to the type of voice search query a user is likely to submit, when a user last submitted a voice search query of a particular type or structure, or whether submitted voice search queries were successful or not.
- a query may be considered successful if a user selects one of the results of the query, while query where processing is cancelled by the user or the user does not select any results may be considered unsuccessful.
- context information for the UI may further include contextual information relating to other device factors such as a current location of the user device 104 , connectivity of the user device 104 (e.g., whether a network connection is present, what type of connection is present, or what speed of connection is available), device language, applications in which a user is currently logged into, or whether bandwidth of the user device is metered or not.
- device factors such as a current location of the user device 104 , connectivity of the user device 104 (e.g., whether a network connection is present, what type of connection is present, or what speed of connection is available), device language, applications in which a user is currently logged into, or whether bandwidth of the user device is metered or not.
- the term identifier module 120 receives data representing the captured context information from the analysis module 110 and analyzes the data to identify one or more terms or entities displayed in the UI. For example, the term identifier module 120 may perform optical character recognition (OCR) on a captured on-screen image, e.g., screen shot or framebuffer data, to determine text displayed on the UI. The term identifier module 120 may further use OCR to identify a language of the UI, or to translate portions of determined text into other languages. In some implementations, this OCR processing is performed locally by the user device 104 , as illustrated.
- OCR optical character recognition
- the term identifier module 120 can process the received text data to determine the various words and phrases visible on the UI or in hidden elements of the UI.
- the term identifier module 120 may communicate with the one or more servers 106 and use server-based OCR to determine text displayed on the UI or to identify a language of the text.
- image recognition may be used to determine text descriptions or labels for images in the UI.
- the user device 104 may send some or all of a screenshot to a server for image processing to determine text describing elements of photographs or other graphics.
- the term identifier module 120 may perform OCR on the news article shown in UI 112 , or receive text data from a rendering subsystem of the user device 104 , and identify one or more terms including “Usain Bolt,” “World Championships” or “Justin Gatlin.”
- the term identifier module 120 may perform OCR on a captured on-screen image including contact details of a contact saved in a phonebook on the user device 104 and identify a name of the contact, e.g., “Lisa Smith.”
- the term identifier module 120 may perform image recognition on a captured on-screen image to identify one or more terms displayed in the UI.
- the term identifier module 120 may perform photo recognition on the image included in the news article shown in UI 112 and identify one or more terms displayed in the UI based on recognizing one or more objects or entities in the image.
- the term identifier module 120 may further identify styles and associated characteristics of the identified one or more terms. For example, the term identifier module 120 may determine style or formatting characteristics associated with each identified one or more terms, e.g., whether an identified term was displayed as text in a title on the UI or in a body of accompanying text, whether an identified term was displayed as text in a smaller or larger font size compared to other portions of text in the UI, whether the identified term was displayed in a central or large image included in the UI, or whether the identified term was displayed as a central or large feature within an image included in the UI. In some implementations, the term identifier module 120 may determine a bounding rectangle for a term to determine the size or area on the UI that the term occupies. As discussed further below, the style and formatting information may be used to help determine which of the identified terms is most important or of most interest to the user.
- style or formatting characteristics may be used to help determine which of the identified terms is most important or of most interest to the user.
- the semantic model 130 receives data representing the identified one or more terms from the term identifier module 120 and assigns classifications to the identified one or more terms. For example, for one or more identified terms the semantic model 130 may assign a corresponding entity type from multiple entity types, e.g., physical location, human name, restaurant, famous landmark, celebrity, or food. In this manner, the semantic model 130 can determine the types of item that each term represents.
- entity types e.g., physical location, human name, restaurant, famous landmark, celebrity, or food.
- the semantic model 130 may assign specific classifications to well-known terms or entities, e.g., celebrity names or famous landmarks, using a knowledge graph.
- the sematic model 130 may assign general classifications, e.g., categories, to ambiguous or lesser-known terms or entities. For example, the term “Lisa Smith” may not be known or recognized by the semantic model 130 , but that the pattern of two successive capitalized words indicates the term is a human name. While identifying a specific entity type or category may be preferred, simply identifying that a term represents an entity may be sufficient in some instances.
- the semantic model 130 may be stored and run entirely on the user device 104 , and so may be developed to achieve an appropriate tradeoff of size, processing time, and quality of output.
- the semantic model 130 may include information that indicates specific frequently-used entities and corresponding entity categories and/or general patterns to identify entities in text.
- the semantic model 130 may select multiple term or entity types for an identified term. For example, the semantic model 130 may receive data representing the identified term “Usain Bolt” and classify the term “Usain Bolt” as both a celebrity and a human name. In other cases the semantic model 130 may not assign a classification to an identified term, e.g., if the identified term is not recognized as belonging to a term type or class.
- the semantic model 130 may determine attributes associated with each identified one or more terms, e.g., a height, length, age, speed or cost.
- the term “Usain Bolt” may be classified as a celebrity or a runner, and certain attributes may be associated with members of those classes.
- the semantic model 130 may determine attributes associated with “Usain Bolt” including age, height, and average running speed.
- the semantic model 130 may classify a term “The Golden Gate Bridge” as a structure, and may determine that “The Golden Gate Bridge” has attributes including length and age, like other structures.
- the ranking module 140 receives data representing the identified one or more terms, their assigned classifications, and their identified styles and associated attributes.
- the ranking module 140 ranks the identified one or more terms.
- the ranking can be used to determine the terms that are most important or most likely represent to topic of the UI the on the user device 104 . Additionally, the ranking may indicate which terms most likely represent entities that can be clearly understood by the user and would facilitate generation of suggested queries.
- the ranking module 140 may rank the identified one or more terms based at least in part on the classifications assigned to the identified terms. For example, the ranking module 140 may discard or give lower rankings to identified terms that have not been assigned a classification.
- terms that have classifications assigned, or which have attributes associated with them may be ranked higher. Some classifications or attributes may be more highly weighted than others. For example, terms ranked as representing people may be ranked more highly than terms representing locations.
- the ranking module 140 may rank the identified one or more terms based at least in part on determined styles or formatting characteristics of the identified one or more terms. For example, the ranking module 140 may rank terms that are displayed as text in a title on the UI higher than terms that are displayed in a body of accompanying text, e.g., the ranking module 140 may rank the term “Usain Bolt” in the UI 112 higher than the term “Justin Gatlin” since the term “Usain Bolt” appears in the title of the news article shown in the UI 112 . As another example, the ranking module 140 may rank terms that are displayed in bold font on the UI higher than terms that are not displayed in bold font.
- the ranking module 140 may rank the identified one or more terms based at least in part on sizes at which the identified terms are displayed in the UI. For example, the ranking module 140 may rank an identified term that is displayed as text in a larger font higher than identified terms that are displayed in smaller fonts, or may rank identified terms that are displayed as central or large features within an image higher than terms that are displayed as smaller features within an image, e.g., the ranking module 140 may rank the term “Usain Bolt” in the UI 112 higher than the term “Justin Gatlin” since the term “Usain Bolt” appears in a larger font than the term “Justin Gatlin.”
- the ranking module 140 may rank the identified one or more terms based at least in part on frequencies that the identified terms are displayed on the screen. For example, the ranking module 140 may rank an identified term that is displayed multiple times on the UI higher than an identified term that is displayed once on the UI, e.g., the ranking module 140 may rank the term “Usain Bolt” in the UI 112 higher than the term “World Championships” since the term “Usain Bolt” is displayed multiple times in the UI 112 , whereas the term “World Championships” is displayed only once.
- the ranking module 140 may rank the identified one or more terms based at least in part on the quantity or quality of actions that can be generated for each respective term. For example, the ranking module 140 may rank an identified term that is associated with a higher number of actions higher than an identified term that is associated with a lower number of actions. For example, a term that indicates a person in the user's contact list could have an associated action of sending a text message, initiating a phone call, sending an email. A term that represents a celebrity may have associated actions of submitting queries about different attributes of the celebrity.
- the ranking module 140 may rank the identified one or more terms based at least in part on measures of previous suggestions relating to the identified terms that were provided and acted upon by a user. For example, the ranking module 140 may rank an identified term that was included in a suggested search request that was previously presented to the user and acted upon higher than an identified term that was not previously presented to a user or was presented to a user but not acted upon.
- the query generator module 150 receives data representing the ranked one or more identified terms and their classifications from the ranking module 140 .
- the query generator module 150 determines a suggested query or request using the identified one or more terms and based on their classifications. For example, the query generator module 150 may determine a suggested query based on a highest-ranked identified term and the classification of the highest ranked identified term, e.g., a highest-ranked identified term may include the term “The Gherkin” that was classified as a building and determine a suggested query such as “Where is The Gherkin?” or “How tall is The Gherkin?”
- the query generator module 150 determines a pronoun for an identified term and generates a suggested query that includes the pronoun to refer to the term. For example, the query generator module 150 may access mapping data that indicates pronouns for a plurality of entities, e.g., locally on the user device 104 or using one or more servers 106 , and select a pronoun that the mapping data indicates as corresponding to the entity.
- the query generator module 150 may determine the pronoun “he” or “his” for the identified term “Usain Bolt,” or may determine the pronoun “it” for the identified term “World Championships.” The query generator module 150 may then generate a suggested query that includes the pronoun to refer to the term “Usain Bolt,” e.g., “What is his average running speed?” “How tall is he?” or “How old is he?”
- the query generator module 150 may determine a suggested query or request based on additional decisions or factors. For example, the query generator module 150 may determine which of multiple questions for an identified term type best fits a context of other words or images displayed on the UI. As another example, the query generator module 150 may determine which suggested query best fits a user's search or browsing history. In some implementations, certain query formats or query types may be associated with certain term classifications. For example, one set of queries may be used refer to people, another set of queries may be used to refer to locations, and so on.
- the query generator module 150 may determine whether the user has used search queries or requests referring to the on-screen context before. As another example the query generator module 150 may determine whether there is a single or multiple identified terms and determine a suggested query that does not ambiguously refer to multiple identified terms, e.g., does not include a pronoun that may refer to multiple identified terms. As another example, although a name of a popular chain of restaurants may be an identified term, the name may be considered ambiguous if a particular location of the restaurant is not specified in the UI, or if multiple different locations of the chain are indicated.
- the query generator module 150 may determine a suggested query or request by identifying one or more query types associated with a classification of an identified term, e.g., pre-set queries, queries exhibiting certain patterns or grammars. For example, a term that has been classified as a famous landmark may be associated with query types including “Directions to ⁇ term>” or “ ⁇ term> opening times,” or a term that has been classified as a human name may be associated with query types including “Call ⁇ term>” or “How old is ⁇ term>?” The query generator module 150 may then select a particular query type from the one or more query types, e.g., based on obtained context information, and determine a suggested query of the particular query type.
- a term that has been classified as a famous landmark may be associated with query types including “Directions to ⁇ term>” or “ ⁇ term> opening times,” or a term that has been classified as a human name may be associated with query types including “Call ⁇ term>” or “How old is ⁇ term>?”
- the determined suggested query may include a suggested request to initiate communication with a person, e.g., “call Lisa” or “call her,” or to navigate to a location, e.g., “Directions to Berlin” or “How do I get there?”
- the determined suggested query may include questions about attributes associated with an identified term, such as “How tall is ⁇ term>?” or “How old is ⁇ term>?”
- the suggested query may include a pronoun to refer to a corresponding identified term.
- a suggested request, and the term or entity that the request refers to are selected to reduce or avoid potential confusion. For example, if the terms shown on the UI include two people, and both are determined to be male, a query such as “tell me more about him” would be ambiguous. Accordingly, the query generator module 150 may assess the various terms that have been identified to determine whether multiple terms would use the same pronouns. If there is a conflict, e.g., multiple entities could be referred to by the same pronoun, then the query generator module 150 may select a different term that uses a different pronoun, even if it has a lower ranking. As an alternative, the query generator module 150 may use the highest-ranked term but use the actual term rather than a pronoun.
- the query generator module 150 could suggest (i) “what is Usain Bolt's top speed” or (ii) “when did it happen,” e.g., referring to the “World Championships” rather than the two runners mentioned.
- the query generator module may generate a suggested request by incorporating past actions a user has taken or places a user has visited. For example, the query generator module may determine that the user has performed a navigation action but not a directions action, and that they have an image of a burger on the screen, then generate the suggested request “directions to ⁇ burger restaurant>.”
- the suggested request is provided to or displayed in the UI of the user device 104 , e.g., in the form of a card 114 .
- the suggested request may be displayed in the UI in response to the user input invoking the digital assistant, as described above with reference to stage (B).
- multiple suggested requests may be provided to the UI of the user device 104 for display.
- the suggested request is shown over only a portion of the UI, leaving areas of the UI visible for the user to see the context, e.g., content of the application, that is referred to in the suggested request.
- a search results page associated with one or more of the suggested requests may be provided to the UI of the user device.
- the suggested request includes a suggested search query, such as “How old is she?” or “What is his average running speed?”
- the user device 104 may display a search results page associated with a first suggested search query.
- Example user interfaces are illustrated below with reference to FIGS. 2A-2C .
- the suggested request provided to or displayed in the UI of the user device 104 may be used to train the user 102 to take full advantage of the functionality of the digital assistant installed on the user device 104 .
- the query generator module 150 may determine a suggested request that is not of initial benefit to the user but illustrates the possibility of using pronouns when communicating with the digital assistant.
- the user device 104 may determine that the user has previously acted upon suggested requests relating to calling a contact whose contact information is displayed when the user invokes the digital functionality of the user device, e.g., suggested requests such as “call Lisa” or “call Dave.” To illustrate the possibility of using pronouns when communicating with the digital assistant the user device 104 may therefore generate the suggested requests “call her” or “call him.” In this manner, the system may suggest a variety of voice commands or other requests, and not simply queries that request information.
- FIG. 2A is a diagram that illustrates an example user interface 200 of a user device 204 .
- the example user interface 200 is a representative user interface of a user device 204 , e.g., a smart phone, displaying an application running on the user device 204 .
- the application running on the user device 204 includes a browser showing content relating to a news article.
- the browser includes several features, such as a title 206 of the current article being viewed by a user 202 of the user device 204 , an image 208 relating to the article and a body of text 212 .
- the features of the user interface 200 exhibit formatting characteristics and styles. For example, the title 206 of the news article is in large, boldface font. The body of text is in smaller, standard font.
- the image 208 is displayed centrally within the user interface 200 and includes at least two visible objects, namely two runners.
- the example user interface 200 further includes an on-screen button 216 for invoking a digital assistant functionality of the user device 204 .
- the on-screen button is a representative method for invoking the digital assistant functionality, and other methods may be used to invoke the digital assistant functionality as described above with reference to FIG. 1 .
- the user 202 may activate the digital assistant functionality by pressing 214 or selecting the on-screen button 216 .
- FIG. 2B is a diagram that illustrates an example user interface 210 .
- the example user interface 210 may be displayed in response to activating the digital assistant functionality of the user device 204 .
- the example user interface 210 includes a suggestion card 218 that has been overlaid the user interface 200 as described above with reference to FIG. 2A .
- the suggestion card 218 is a representative visually appealing suggestion card displayed as pixels on the user device 204 .
- the suggestion card 218 includes the suggested query “what is his average running speed?” In some implementations the user device 204 may also or instead provide an audible suggested query to the user 202 .
- the suggested query is a suggested request that has been generated by the user device based on the content displayed in the user interface at the time the digital assistant functionality was invoked, e.g., based on content displayed in user interface 200 .
- the user device may have obtained screen data indicating content displayed on the screen of the user device 204 in response to receiving input 214 invoking the digital assistant functionality of the user device, e.g., screen data indicating content relating to the news article “Usain Bolt beats rival in World Championships 200 m final.”
- the user device 204 may have determined that the entity “Usain Bolt” is referenced in the obtained screen data, and may have classified the entity “Usain Bolt” as a human name, celebrity or sportsman.
- the user device 204 may have determined a suggested request that refers to the entity “Usain Bolt” based on the determined classifications, as well as other factors, and provided the determined suggested request for display in suggestion card 218 . Generating suggested requests in response to receiving user input invoking a digital assistant is described in more detail below with reference to FIG. 3 .
- the suggested request includes the pronoun “his.”
- the pronoun “his” refers to the entity “Usain Bolt,” as referenced in user interfaces 200 and 210 .
- the user device 204 may have accessed mapping data that indicated the pronoun “his” corresponds to the entity “Usain Bolt.” Determining pronouns for identified entities is described in more detail below with reference to FIG. 3 .
- FIG. 2C is a diagram that illustrates an example user interface 220 .
- the example user interface 220 may be displayed in response to receiving voice query 228 .
- the voice query 228 may be provided by a user 202 in response to reading the suggested query displayed in suggestion card 218 .
- the user 202 may have invoked the digital assistant functionality of the user device 204 and noticed the suggestion card 218 with the suggested query “What is his average speed?” It is likely that the user 202 was either reading the news article displayed in user interface 200 at the time of invoking the digital assistant functionality, or had been recently reading the news article prior to invoking the digital assistant functionality. Therefore, the user may determine that the suggested query “What is his average speed” is referring to Usain Bolt's average running speed. As shown in FIG. 2C , the user 202 has found this suggested query interesting and appealing, and has used the digital assistant to activate a search based on the suggested query.
- the user device In response to receiving the search request 228 , the user device has submitted the search query “What is Usain Bolt's average running speed?” to a search engine and displayed corresponding search results 224 and 226 .
- the user device has also displayed alternative suggested queries relating to the entity “Usain Bolt,” such as “Usain Bolt age” and “Usain Bolt height.”
- FIG. 3 presents an example process 300 for providing a search request in response to receiving user input invoking a digital assistant.
- the process can be performed by one or more computing devices.
- the one or more computing devices can include a client device, a server system, or a combination of both a client device and a server system.
- the actions of the process 300 may be performed exclusively by a client device or by a server system.
- a computing device may be a mobile phone, smart phone, personal digital assistant (PDA), music player, e-book reader, tablet computer, wearable computing device such as a watch, glasses, or necklace, laptop computer, desktop computer, or other portable or stationary computing device.
- PDA personal digital assistant
- a computing device may be a central processing unit (CPU) of a device, a particular processing core within a CPU, and so on.
- the process 300 can be performed by the system user device 104 , or by the user device 104 in cooperation with the server system 106 , in response to receiving user input invoking a digital assistant by a user 102 at user device 104 .
- the system receives user input that invokes digital assistant functionality of a computing device (step 302 ).
- the system may receive user input including pressing a home button of the computing device for an extended period of time, pressing an alternative on-screen button of the computing device, selecting or highlighting a term visible on a screen of the computing device, invoking the digital assistant functionality using an application installed on the computing device, speaking a hotword to activate the digital assistant functionality or other user-directed input.
- the system obtains screen data indicating content displayed on a screen of the computing device (step 304 ).
- the obtained screen data may include data indicating content displayed in an application open on the computing device, e.g., data indicating contact information for the contact “Lisa Smith.”
- the system determines a classification for an entity referenced in the content indicated by the screen data (step 306 ).
- the system determines a classification for an entity referenced in the content indicated by the screen data by selecting an entity type for the entity from among a plurality of entity types. For example, the system may classify an entity “Lisa Smith” as a human name, or classify an entity “The Gherkin” as a building.
- the system may recognize one or more terms or entities displayed on the screen of the computing device using optical character recognition on the screen data. In such cases the system may determine a classification for a recognized entity or term by determining a semantic classification for the recognized entity or terms. In some cases the system may recognize multiple different terms, and may rank the recognized multiple different terms.
- the system may rank the recognized multiple different terms by ranking the multiple different terms based at least in part on (i) classifications assigned to the multiple different terms, (ii) sizes at which the multiple different terms are displayed, (iii) frequencies that the multiple different terms are displayed on the screen, (iv) quantities of actions that can be generated for the multiple different terms, or (v) measures of previous suggestions relating to the multiple different terms that were provided and acted upon by a user.
- the system may further determine text formatting characteristics of the multiple different terms indicated by the screen data, such as a size of the font used for each of the multiple different terms, a position of each of the multiple different terms within the screen or whether the multiple terms appear in bold or italic font.
- the system may rank the recognized multiple different terms based at least in part on the determined formatting characteristics. For example, the system may rank terms that are positioned in a more prominent position within the screen or are displayed with a larger or boldface font higher than terms that are not in a prominent position or are displayed in standard size font.
- the system determines a suggested request that refers to the entity based on the determined classification (step 308 ). For example, in some implementations the system may determine a suggested request that refers to an entity corresponding to a highest-ranking term, as described above with reference to step 306 .
- the system may determine a suggested request by determining a pronoun for the entity and generating a suggested query that includes the pronoun to refer to the entity referenced in the content indicated by the screen data.
- the system may determine a pronoun for the entity by accessing mapping data that indicates pronouns for a plurality of entities and selecting a pronoun that the mapping data indicates corresponding to the entity. For example, the system may recognize the term “Lisa Smith” as a female human name, and may access mapping data that indicates the pronouns “she” or “her” for a female human name.
- the system may recognize the entity “The Gherkin” as a famous building, and may access mapping data that indicates the pronouns “it” or “that” for a building. The system may then use the determined pronouns to generate respective suggested requests such as “Do you want to call her?” “Send her an SMS,” “How tall is it?” or “How do I get to it?”
- the system may determine a suggested request by identifying one or more query types associated with the determined classification, e.g., query types with particular grammars or patterns such as “directions to ⁇ term>” or “how old is ⁇ term>,” where ⁇ term> may include the term itself or a determined pronoun that refers to the term.
- the query types may include preset queries, such as “tell me more about it” or “where is it?”
- the system may select a particular query type from the one or more query types, and determine a suggested query of the particular query type. For example, the system may select the query type “direct me to ⁇ term>” and determine the suggested query “direct me to Berlin” or “direct me there” for the entity “Berlin.”
- the system further determines one or more attributes associated with the entity type and determines a query that asks about one of the attributes. For example, the system may determine that the attributes height and age may be associated with a famous building, and may determine a query that asks about one of these attributes such as “how tall is The Gherkin?” or “How old is it?”
- the system may determine a suggested request that initiates communication with a person or to navigate to a location.
- the determined suggested request may include a request that initiates a call with a person, e.g., a contact in a phonebook on a user device or a restaurant employee, or that initiates the sending of an email or SMS.
- the determined suggested request may include a request that navigates a user to a location, e.g., to an entity referenced in the content displayed on the screen of the computing device.
- the system may generate a suggested request by incorporating past actions a user has taken or places a user has visited. For example, the system may determine that the user has performed a navigation action but not a directions action, and that they have an image of a burger on the screen, then generate the suggested request “directions to ⁇ burger restaurant>.”
- the system provides the suggested request in response to receiving the user input that invokes the digital assistant functionality (step 310 ).
- the system may provide the suggested request in the form of a visible card that is displayed on the screen of the computing device, as shown in FIG. 2B .
- a search results page associated with one or more of the suggested requests may be displayed on the screen of the computing device.
- the computing device may display a search results page associated with the suggested search query, as shown in FIG. 2C .
- the system may alternatively or in addition provide the suggested request in the form of an audible voice request.
- the system may determine that the screen references only a single entity or only a single entity of a particular entity type and may provide the suggested request in response to determining only a single entity or only a single entity of a particular entity type.
- Embodiments and all of the functional operations described in this specification may be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments may be implemented as one or more computer program products, i.e., one or more modules of computer program instructions encoded on a computer readable medium for execution by, or to control the operation of, data processing apparatus.
- the computer readable medium may be a machine-readable storage device, a machine-readable storage substrate, a memory device, a composition of matter effecting a machine-readable propagated signal, or a combination of one or more of them.
- data processing apparatus encompasses all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus may include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a propagated signal is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.
- a computer program (also known as a program, software, software application, script, or code) may be written in any form of programming language, including compiled or interpreted languages, and it may be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program does not necessarily correspond to a file in a file system.
- a program may be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code).
- a computer program may be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification may be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows may also be performed by, and apparatus may also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- FPGA field programmable gate array
- ASIC application specific integrated circuit
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- a computer need not have such devices.
- a computer may be embedded in another device, e.g., a tablet computer, a mobile telephone, a personal digital assistant (PDA), a mobile audio player, a Global Positioning System (GPS) receiver, to name just a few.
- PDA personal digital assistant
- GPS Global Positioning System
- Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto optical disks e.g., CD ROM and DVD-ROM disks.
- the processor and the memory may be supplemented by, or incorporated in, special purpose logic circuitry.
- embodiments may be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user may provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices may be used to provide for interaction with a user as well; for example, feedback provided to the user may be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input.
- Embodiments may be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user may interact with an implementation, or any combination of one or more such back end, middleware, or front end components.
- the components of the system may be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system may include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- HTML file In each instance where an HTML file is mentioned, other file types or formats may be substituted. For instance, an HTML file may be replaced by an XML, JSON, plain text, or other types of files. Moreover, where a table or hash table is mentioned, other data structures (such as spreadsheets, relational databases, or structured files) may be used.
Abstract
Description
Claims (15)
Priority Applications (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/194,153 US11232136B2 (en) | 2016-06-27 | 2016-06-27 | Contextual voice search suggestions |
PCT/US2016/065609 WO2018004729A1 (en) | 2016-06-27 | 2016-12-08 | Generating contextual search suggestions |
CN201680088538.XA CN109791550A (en) | 2016-06-27 | 2016-12-08 | Scene search is generated to suggest |
EP16820081.4A EP3472724A1 (en) | 2016-06-27 | 2016-12-08 | Generating contextual search suggestions |
US17/582,822 US20220222282A1 (en) | 2016-06-27 | 2022-01-24 | Contextual voice search suggestions |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/194,153 US11232136B2 (en) | 2016-06-27 | 2016-06-27 | Contextual voice search suggestions |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/582,822 Continuation US20220222282A1 (en) | 2016-06-27 | 2022-01-24 | Contextual voice search suggestions |
Publications (2)
Publication Number | Publication Date |
---|---|
US20170371885A1 US20170371885A1 (en) | 2017-12-28 |
US11232136B2 true US11232136B2 (en) | 2022-01-25 |
Family
ID=57681760
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/194,153 Active 2037-05-09 US11232136B2 (en) | 2016-06-27 | 2016-06-27 | Contextual voice search suggestions |
US17/582,822 Pending US20220222282A1 (en) | 2016-06-27 | 2022-01-24 | Contextual voice search suggestions |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/582,822 Pending US20220222282A1 (en) | 2016-06-27 | 2022-01-24 | Contextual voice search suggestions |
Country Status (4)
Country | Link |
---|---|
US (2) | US11232136B2 (en) |
EP (1) | EP3472724A1 (en) |
CN (1) | CN109791550A (en) |
WO (1) | WO2018004729A1 (en) |
Families Citing this family (99)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8677377B2 (en) | 2005-09-08 | 2014-03-18 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
US9318108B2 (en) | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
US8977255B2 (en) | 2007-04-03 | 2015-03-10 | Apple Inc. | Method and system for operating a multi-function portable electronic device using voice-activation |
US8676904B2 (en) | 2008-10-02 | 2014-03-18 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US10706373B2 (en) | 2011-06-03 | 2020-07-07 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US10276170B2 (en) | 2010-01-18 | 2019-04-30 | Apple Inc. | Intelligent automated assistant |
US8682667B2 (en) | 2010-02-25 | 2014-03-25 | Apple Inc. | User profiling for selecting user specific voice input processing information |
US9634855B2 (en) | 2010-05-13 | 2017-04-25 | Alexander Poltorak | Electronic personal interactive device that determines topics of interest using a conversational agent |
US10417037B2 (en) | 2012-05-15 | 2019-09-17 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
CN104969289B (en) | 2013-02-07 | 2021-05-28 | 苹果公司 | Voice trigger of digital assistant |
US10652394B2 (en) | 2013-03-14 | 2020-05-12 | Apple Inc. | System and method for processing voicemail |
US10748529B1 (en) | 2013-03-15 | 2020-08-18 | Apple Inc. | Voice activated device for use with a voice-based digital assistant |
KR101959188B1 (en) | 2013-06-09 | 2019-07-02 | 애플 인크. | Device, method, and graphical user interface for enabling conversation persistence across two or more instances of a digital assistant |
US10176167B2 (en) | 2013-06-09 | 2019-01-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
US10296160B2 (en) | 2013-12-06 | 2019-05-21 | Apple Inc. | Method for extracting salient dialog usage from live data |
US10170123B2 (en) | 2014-05-30 | 2019-01-01 | Apple Inc. | Intelligent assistant for home automation |
US9430463B2 (en) | 2014-05-30 | 2016-08-30 | Apple Inc. | Exemplar-based natural language processing |
US9966065B2 (en) | 2014-05-30 | 2018-05-08 | Apple Inc. | Multi-command single utterance input method |
US9715875B2 (en) | 2014-05-30 | 2017-07-25 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US9338493B2 (en) | 2014-06-30 | 2016-05-10 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US10127911B2 (en) | 2014-09-30 | 2018-11-13 | Apple Inc. | Speaker identification and unsupervised speaker adaptation techniques |
US9668121B2 (en) | 2014-09-30 | 2017-05-30 | Apple Inc. | Social reminders |
US10152299B2 (en) | 2015-03-06 | 2018-12-11 | Apple Inc. | Reducing response latency of intelligent automated assistants |
US9886953B2 (en) | 2015-03-08 | 2018-02-06 | Apple Inc. | Virtual assistant activation |
US9721566B2 (en) | 2015-03-08 | 2017-08-01 | Apple Inc. | Competing devices responding to voice triggers |
US10460227B2 (en) | 2015-05-15 | 2019-10-29 | Apple Inc. | Virtual assistant in a communication session |
US10200824B2 (en) | 2015-05-27 | 2019-02-05 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on a touch-sensitive device |
US10083688B2 (en) | 2015-05-27 | 2018-09-25 | Apple Inc. | Device voice control for selecting a displayed affordance |
US9578173B2 (en) | 2015-06-05 | 2017-02-21 | Apple Inc. | Virtual assistant aided communication with 3rd party service in a communication session |
US20160378747A1 (en) | 2015-06-29 | 2016-12-29 | Apple Inc. | Virtual assistant for media playback |
US10331312B2 (en) | 2015-09-08 | 2019-06-25 | Apple Inc. | Intelligent automated assistant in a media environment |
US10740384B2 (en) | 2015-09-08 | 2020-08-11 | Apple Inc. | Intelligent automated assistant for media search and playback |
US10671428B2 (en) | 2015-09-08 | 2020-06-02 | Apple Inc. | Distributed personal assistant |
US10747498B2 (en) | 2015-09-08 | 2020-08-18 | Apple Inc. | Zero latency digital assistant |
US10691473B2 (en) | 2015-11-06 | 2020-06-23 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US10956666B2 (en) | 2015-11-09 | 2021-03-23 | Apple Inc. | Unconventional virtual assistant interactions |
US10223066B2 (en) | 2015-12-23 | 2019-03-05 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US11227589B2 (en) | 2016-06-06 | 2022-01-18 | Apple Inc. | Intelligent list reading |
US10586535B2 (en) | 2016-06-10 | 2020-03-10 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
DK201670540A1 (en) | 2016-06-11 | 2018-01-08 | Apple Inc | Application integration with a digital assistant |
DK179415B1 (en) | 2016-06-11 | 2018-06-14 | Apple Inc | Intelligent device arbitration and control |
US10474753B2 (en) | 2016-09-07 | 2019-11-12 | Apple Inc. | Language identification using recurrent neural networks |
US11204787B2 (en) | 2017-01-09 | 2021-12-21 | Apple Inc. | Application integration with a digital assistant |
DK201770383A1 (en) | 2017-05-09 | 2018-12-14 | Apple Inc. | User interface for correcting recognition errors |
DK180048B1 (en) | 2017-05-11 | 2020-02-04 | Apple Inc. | MAINTAINING THE DATA PROTECTION OF PERSONAL INFORMATION |
US10395654B2 (en) | 2017-05-11 | 2019-08-27 | Apple Inc. | Text normalization based on a data-driven learning network |
US10726832B2 (en) | 2017-05-11 | 2020-07-28 | Apple Inc. | Maintaining privacy of personal information |
DK201770427A1 (en) | 2017-05-12 | 2018-12-20 | Apple Inc. | Low-latency intelligent automated assistant |
US11301477B2 (en) | 2017-05-12 | 2022-04-12 | Apple Inc. | Feedback analysis of a digital assistant |
DK179745B1 (en) | 2017-05-12 | 2019-05-01 | Apple Inc. | SYNCHRONIZATION AND TASK DELEGATION OF A DIGITAL ASSISTANT |
DK179496B1 (en) | 2017-05-12 | 2019-01-15 | Apple Inc. | USER-SPECIFIC Acoustic Models |
US10311144B2 (en) | 2017-05-16 | 2019-06-04 | Apple Inc. | Emoji word sense disambiguation |
US10303715B2 (en) | 2017-05-16 | 2019-05-28 | Apple Inc. | Intelligent automated assistant for media exploration |
US20180336892A1 (en) | 2017-05-16 | 2018-11-22 | Apple Inc. | Detecting a trigger of a digital assistant |
DK179560B1 (en) | 2017-05-16 | 2019-02-18 | Apple Inc. | Far-field extension for digital assistant services |
US10467640B2 (en) * | 2017-11-29 | 2019-11-05 | Qualtrics, Llc | Collecting and analyzing electronic survey responses including user-composed text |
US10592604B2 (en) | 2018-03-12 | 2020-03-17 | Apple Inc. | Inverse text normalization for automatic speech recognition |
US10818288B2 (en) | 2018-03-26 | 2020-10-27 | Apple Inc. | Natural assistant interaction |
US10928918B2 (en) | 2018-05-07 | 2021-02-23 | Apple Inc. | Raise to speak |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
CN110544473B (en) * | 2018-05-28 | 2022-11-08 | 百度在线网络技术（北京）有限公司 | Voice interaction method and device |
DK179822B1 (en) | 2018-06-01 | 2019-07-12 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US10892996B2 (en) | 2018-06-01 | 2021-01-12 | Apple Inc. | Variable latency device coordination |
DK180639B1 (en) | 2018-06-01 | 2021-11-04 | Apple Inc | DISABILITY OF ATTENTION-ATTENTIVE VIRTUAL ASSISTANT |
US11010561B2 (en) | 2018-09-27 | 2021-05-18 | Apple Inc. | Sentiment prediction from textual data |
US11462215B2 (en) | 2018-09-28 | 2022-10-04 | Apple Inc. | Multi-modal inputs for voice commands |
US11170166B2 (en) | 2018-09-28 | 2021-11-09 | Apple Inc. | Neural typographical error modeling via generative adversarial networks |
US10839159B2 (en) | 2018-09-28 | 2020-11-17 | Apple Inc. | Named entity normalization in a spoken dialog system |
US11475898B2 (en) | 2018-10-26 | 2022-10-18 | Apple Inc. | Low-latency multi-speaker speech recognition |
US10984791B2 (en) | 2018-11-29 | 2021-04-20 | Hughes Network Systems, Llc | Spoken language interface for network management |
US11638059B2 (en) | 2019-01-04 | 2023-04-25 | Apple Inc. | Content playback on multiple devices |
US11132174B2 (en) * | 2019-03-15 | 2021-09-28 | Adobe Inc. | Facilitating discovery of verbal commands using multimodal interfaces |
US11348573B2 (en) | 2019-03-18 | 2022-05-31 | Apple Inc. | Multimodality in digital assistant systems |
CN111858860B (en) * | 2019-04-19 | 2023-08-29 | 百度在线网络技术（北京）有限公司 | Search information processing method and system, server and computer readable medium |
US11307752B2 (en) | 2019-05-06 | 2022-04-19 | Apple Inc. | User configurable task triggers |
US11475884B2 (en) | 2019-05-06 | 2022-10-18 | Apple Inc. | Reducing digital assistant latency when a language is incorrectly determined |
US11423908B2 (en) | 2019-05-06 | 2022-08-23 | Apple Inc. | Interpreting spoken requests |
DK201970509A1 (en) | 2019-05-06 | 2021-01-15 | Apple Inc | Spoken notifications |
US11140099B2 (en) | 2019-05-21 | 2021-10-05 | Apple Inc. | Providing message response suggestions |
US11496600B2 (en) | 2019-05-31 | 2022-11-08 | Apple Inc. | Remote execution of machine-learned models |
US11289073B2 (en) | 2019-05-31 | 2022-03-29 | Apple Inc. | Device text to speech |
DK180129B1 (en) | 2019-05-31 | 2020-06-02 | Apple Inc. | User activity shortcut suggestions |
DK201970510A1 (en) | 2019-05-31 | 2021-02-11 | Apple Inc | Voice identification in digital assistant systems |
US11227599B2 (en) | 2019-06-01 | 2022-01-18 | Apple Inc. | Methods and user interfaces for voice-based control of electronic devices |
US11360641B2 (en) | 2019-06-01 | 2022-06-14 | Apple Inc. | Increasing the relevance of new available information |
US11438452B1 (en) | 2019-08-09 | 2022-09-06 | Apple Inc. | Propagating context information in a privacy preserving manner |
US11675996B2 (en) * | 2019-09-13 | 2023-06-13 | Microsoft Technology Licensing, Llc | Artificial intelligence assisted wearable |
US11488406B2 (en) | 2019-09-25 | 2022-11-01 | Apple Inc. | Text detection using global geometry estimators |
US11810578B2 (en) | 2020-05-11 | 2023-11-07 | Apple Inc. | Device arbitration for digital assistant-based intercom systems |
US11061543B1 (en) | 2020-05-11 | 2021-07-13 | Apple Inc. | Providing relevant data items based on context |
US11043220B1 (en) | 2020-05-11 | 2021-06-22 | Apple Inc. | Digital assistant hardware abstraction |
US11755276B2 (en) | 2020-05-12 | 2023-09-12 | Apple Inc. | Reducing description length based on confidence |
DK202070658A1 (en) | 2020-06-01 | 2022-01-13 | Apple Inc | Suggesting executable actions in response to detecting events |
US11490204B2 (en) | 2020-07-20 | 2022-11-01 | Apple Inc. | Multi-device audio adjustment coordination |
US11438683B2 (en) | 2020-07-21 | 2022-09-06 | Apple Inc. | User identification using headphones |
CN112507128A (en) * | 2020-12-07 | 2021-03-16 | 云南电网有限责任公司普洱供电局 | Content filling prompting method for power distribution network operation file and related equipment |
US11947783B2 (en) * | 2021-01-25 | 2024-04-02 | Google Llc | Undoing application operation(s) via user interaction(s) with an automated assistant |
US11741957B2 (en) | 2021-06-17 | 2023-08-29 | International Business Machines Corporation | Chatbot with augmented reality based voice command navigation |
CN113656634A (en) * | 2021-08-03 | 2021-11-16 | 深圳邦企邦信息技术有限公司 | Intelligent query method and device for non-call type recording file |
Citations (34)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5218537A (en) * | 1989-12-21 | 1993-06-08 | Texas Instruments Incorporated | System and method for using a computer to generate and teach grammar lessons |
US20040024739A1 (en) * | 1999-06-15 | 2004-02-05 | Kanisa Inc. | System and method for implementing a knowledge management system |
US20060123220A1 (en) * | 2004-12-02 | 2006-06-08 | International Business Machines Corporation | Speech recognition in BIOS |
US20060206220A1 (en) * | 2003-12-02 | 2006-09-14 | Honeywell International Inc. | Natural language installer setup for controller |
US20070174043A1 (en) * | 2004-02-25 | 2007-07-26 | Mikko Makela | Method and an apparatus for requesting a service in a network |
US20080104052A1 (en) * | 2006-10-31 | 2008-05-01 | Microsoft Corporation | Implicit, specialized search of business objects using unstructured text |
US7478089B2 (en) * | 2003-10-29 | 2009-01-13 | Kontera Technologies, Inc. | System and method for real-time web page context analysis for the real-time insertion of textual markup objects and dynamic content |
US20090204598A1 (en) * | 2008-02-08 | 2009-08-13 | Microsoft Corporation | Ad retrieval for user search on social network sites |
US20100094835A1 (en) * | 2008-10-15 | 2010-04-15 | Yumao Lu | Automatic query concepts identification and drifting for web search |
US20100217657A1 (en) * | 1999-06-10 | 2010-08-26 | Gazdzinski Robert F | Adaptive information presentation apparatus and methods |
US20110047514A1 (en) * | 2009-08-24 | 2011-02-24 | Emma Butin | Recording display-independent computerized guidance |
US20110246880A1 (en) * | 2010-04-06 | 2011-10-06 | Microsoft Corporation | Interactive application assistance, such as for web applications |
US20120016678A1 (en) | 2010-01-18 | 2012-01-19 | Apple Inc. | Intelligent Automated Assistant |
US20120034904A1 (en) * | 2010-08-06 | 2012-02-09 | Google Inc. | Automatically Monitoring for Voice Input Based on Context |
US20120078936A1 (en) * | 2010-09-24 | 2012-03-29 | Microsoft Corporation | Visual-cue refinement of user query results |
US8326833B2 (en) * | 2007-10-04 | 2012-12-04 | International Business Machines Corporation | Implementing metadata extraction of artifacts from associated collaborative discussions |
US20130173604A1 (en) | 2011-12-30 | 2013-07-04 | Microsoft Corporation | Knowledge-based entity detection and disambiguation |
US20130185336A1 (en) * | 2011-11-02 | 2013-07-18 | Sri International | System and method for supporting natural language queries and requests against a user's personal data cloud |
US20130275875A1 (en) | 2010-01-18 | 2013-10-17 | Apple Inc. | Automatically Adapting User Interfaces for Hands-Free Interaction |
US20130275164A1 (en) | 2010-01-18 | 2013-10-17 | Apple Inc. | Intelligent Automated Assistant |
US20130297317A1 (en) * | 2012-04-16 | 2013-11-07 | Htc Corporation | Method for offering suggestion during conversation, electronic device using the same, and non-transitory storage medium |
US20130311508A1 (en) * | 2012-05-17 | 2013-11-21 | Grit Denker | Method, apparatus, and system for facilitating cross-application searching and retrieval of content using a contextual user model |
US20130326353A1 (en) * | 2012-06-02 | 2013-12-05 | Tara Chand Singhal | System and method for context driven voice interface in handheld wireles mobile devices |
US20140032529A1 (en) | 2006-02-28 | 2014-01-30 | Adobe Systems Incorporated | Information resource identification system |
US20140040748A1 (en) * | 2011-09-30 | 2014-02-06 | Apple Inc. | Interface for a Virtual Digital Assistant |
US20140108424A1 (en) * | 2012-10-11 | 2014-04-17 | Nuance Communications, Inc. | Data store organizing data using semantic classification |
US20140222436A1 (en) * | 2013-02-07 | 2014-08-07 | Apple Inc. | Voice trigger for a digital assistant |
US20150179186A1 (en) | 2013-12-20 | 2015-06-25 | Dell Products, L.P. | Visual Audio Quality Cues and Context Awareness in a Virtual Collaboration Session |
US20150227522A1 (en) * | 2014-02-07 | 2015-08-13 | Chacha Search, Inc | Method and system for selection of a media file based on a response |
US20150279348A1 (en) * | 2014-03-25 | 2015-10-01 | Microsoft Corporation | Generating natural language outputs |
US20150350118A1 (en) * | 2014-05-30 | 2015-12-03 | Apple Inc. | Canned answers in messages |
US20160078115A1 (en) * | 2014-09-16 | 2016-03-17 | Breach Intelligence LLC | Interactive System and Method for Processing On-Screen Items of Textual Interest |
US9542099B1 (en) * | 2016-02-16 | 2017-01-10 | William Linden | Touch-sensitive rectangular panel and control method thereof |
US20170031931A1 (en) * | 2015-07-28 | 2017-02-02 | Expedia, Inc. | Disambiguating search queries |
Family Cites Families (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6244873B1 (en) * | 1998-10-16 | 2001-06-12 | At&T Corp. | Wireless myoelectric control apparatus and methods |
US8150872B2 (en) * | 2005-01-24 | 2012-04-03 | The Intellection Group, Inc. | Multimodal natural language query system for processing and analyzing voice and proximity-based queries |
US9098568B2 (en) * | 2009-08-04 | 2015-08-04 | Google Inc. | Query suggestions from documents |
US9015043B2 (en) * | 2010-10-01 | 2015-04-21 | Google Inc. | Choosing recognized text from a background environment |
US9652556B2 (en) * | 2011-10-05 | 2017-05-16 | Google Inc. | Search suggestions based on viewport content |
US9338493B2 (en) * | 2014-06-30 | 2016-05-10 | Apple Inc. | Intelligent automated assistant for TV user interactions |
-
2016
- 2016-06-27 US US15/194,153 patent/US11232136B2/en active Active
- 2016-12-08 WO PCT/US2016/065609 patent/WO2018004729A1/en active Search and Examination
- 2016-12-08 CN CN201680088538.XA patent/CN109791550A/en active Pending
- 2016-12-08 EP EP16820081.4A patent/EP3472724A1/en not_active Withdrawn
-
2022
- 2022-01-24 US US17/582,822 patent/US20220222282A1/en active Pending
Patent Citations (35)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5218537A (en) * | 1989-12-21 | 1993-06-08 | Texas Instruments Incorporated | System and method for using a computer to generate and teach grammar lessons |
US20100217657A1 (en) * | 1999-06-10 | 2010-08-26 | Gazdzinski Robert F | Adaptive information presentation apparatus and methods |
US20040024739A1 (en) * | 1999-06-15 | 2004-02-05 | Kanisa Inc. | System and method for implementing a knowledge management system |
US7478089B2 (en) * | 2003-10-29 | 2009-01-13 | Kontera Technologies, Inc. | System and method for real-time web page context analysis for the real-time insertion of textual markup objects and dynamic content |
US20060206220A1 (en) * | 2003-12-02 | 2006-09-14 | Honeywell International Inc. | Natural language installer setup for controller |
US20070174043A1 (en) * | 2004-02-25 | 2007-07-26 | Mikko Makela | Method and an apparatus for requesting a service in a network |
US20060123220A1 (en) * | 2004-12-02 | 2006-06-08 | International Business Machines Corporation | Speech recognition in BIOS |
US20140032529A1 (en) | 2006-02-28 | 2014-01-30 | Adobe Systems Incorporated | Information resource identification system |
US20080104052A1 (en) * | 2006-10-31 | 2008-05-01 | Microsoft Corporation | Implicit, specialized search of business objects using unstructured text |
US8326833B2 (en) * | 2007-10-04 | 2012-12-04 | International Business Machines Corporation | Implementing metadata extraction of artifacts from associated collaborative discussions |
US20090204598A1 (en) * | 2008-02-08 | 2009-08-13 | Microsoft Corporation | Ad retrieval for user search on social network sites |
US20100094835A1 (en) * | 2008-10-15 | 2010-04-15 | Yumao Lu | Automatic query concepts identification and drifting for web search |
US20110047514A1 (en) * | 2009-08-24 | 2011-02-24 | Emma Butin | Recording display-independent computerized guidance |
US20130185081A1 (en) | 2010-01-18 | 2013-07-18 | Apple Inc. | Maintaining Context Information Between User Interactions with a Voice Assistant |
US20130275875A1 (en) | 2010-01-18 | 2013-10-17 | Apple Inc. | Automatically Adapting User Interfaces for Hands-Free Interaction |
US20130275164A1 (en) | 2010-01-18 | 2013-10-17 | Apple Inc. | Intelligent Automated Assistant |
US20120016678A1 (en) | 2010-01-18 | 2012-01-19 | Apple Inc. | Intelligent Automated Assistant |
US20110246880A1 (en) * | 2010-04-06 | 2011-10-06 | Microsoft Corporation | Interactive application assistance, such as for web applications |
US20120034904A1 (en) * | 2010-08-06 | 2012-02-09 | Google Inc. | Automatically Monitoring for Voice Input Based on Context |
US20120078936A1 (en) * | 2010-09-24 | 2012-03-29 | Microsoft Corporation | Visual-cue refinement of user query results |
US20140040748A1 (en) * | 2011-09-30 | 2014-02-06 | Apple Inc. | Interface for a Virtual Digital Assistant |
US20130185336A1 (en) * | 2011-11-02 | 2013-07-18 | Sri International | System and method for supporting natural language queries and requests against a user's personal data cloud |
US20130173604A1 (en) | 2011-12-30 | 2013-07-04 | Microsoft Corporation | Knowledge-based entity detection and disambiguation |
US20130297317A1 (en) * | 2012-04-16 | 2013-11-07 | Htc Corporation | Method for offering suggestion during conversation, electronic device using the same, and non-transitory storage medium |
US20130311508A1 (en) * | 2012-05-17 | 2013-11-21 | Grit Denker | Method, apparatus, and system for facilitating cross-application searching and retrieval of content using a contextual user model |
US20130326353A1 (en) * | 2012-06-02 | 2013-12-05 | Tara Chand Singhal | System and method for context driven voice interface in handheld wireles mobile devices |
US20140108424A1 (en) * | 2012-10-11 | 2014-04-17 | Nuance Communications, Inc. | Data store organizing data using semantic classification |
US20140222436A1 (en) * | 2013-02-07 | 2014-08-07 | Apple Inc. | Voice trigger for a digital assistant |
US20150179186A1 (en) | 2013-12-20 | 2015-06-25 | Dell Products, L.P. | Visual Audio Quality Cues and Context Awareness in a Virtual Collaboration Session |
US20150227522A1 (en) * | 2014-02-07 | 2015-08-13 | Chacha Search, Inc | Method and system for selection of a media file based on a response |
US20150279348A1 (en) * | 2014-03-25 | 2015-10-01 | Microsoft Corporation | Generating natural language outputs |
US20150350118A1 (en) * | 2014-05-30 | 2015-12-03 | Apple Inc. | Canned answers in messages |
US20160078115A1 (en) * | 2014-09-16 | 2016-03-17 | Breach Intelligence LLC | Interactive System and Method for Processing On-Screen Items of Textual Interest |
US20170031931A1 (en) * | 2015-07-28 | 2017-02-02 | Expedia, Inc. | Disambiguating search queries |
US9542099B1 (en) * | 2016-02-16 | 2017-01-10 | William Linden | Touch-sensitive rectangular panel and control method thereof |
Non-Patent Citations (9)
Title |
---|
‘www.blog.evernote.com’ [online] "How Evernote's Image Recognition Works, Evernote Tech Blog," Jul. 18, 2013, [retrieved on Mar. 23, 2015] Retrieved from Internet URL<http://blog.evernote.com/tech/2013/07/18/how-evernotes-image-recognition-works/> 4 pages. |
‘www.boxoft.com’ [online] "Boxoft Screen OCR—capture screen and convert screen text into editable electronic text," Jun. 26, 2012 [retrieved on May 9, 2018] Retrieved from Internet: URL<https://web/archive/org/web/20120626172711/http://www.boxoft.com/screen-ocr/> 2 pages. |
‘www.mydigitallife.onfo’ [online] "Copy Screenshot Image as Text with Screen Capture OCR My Digital Life," Nov. 19, 2015, [retrieved on May 29, 2018] Retrieved from Internet: URL<https://web.archive.org/web/20151119131504/http://www.mydigitallife.info/copy-screenshot-image0as0text-with-screen-capture-ocr/> 1 page. |
European Patent Office; Summons to attend oral proceedings pursuant to Rule 115(1) EPC issued in Application No. 16820081.4; 9 pages; dated Jun. 18, 2021. |
Goode, "Google Now on Tap might just be the killer feature of Android Marshmallow," Oct. 5, 2015 [retrieved on Jun. 27, 2016], Retrieved from the Internet: URL<http://www.theyerge.com/2015/10/5/9444039/google-now-on-tap-feature-android-m-hands-on-video>, 4 pages. |
International Preliminary Report on Patentability issued in International Application No. PCT/US2016/065609, dated Sep. 12, 2018, 26 pages. |
International Search Report and Written Opinion issued in International Application No. PCT/US2016/065609, dated Feb. 13, 2017, 13 pages. |
Okonkwo, "Now on Tap update: Text Select and Image Search," Jun. 1, 2016 [retrieved on Jun. 27, 2016], Retrieved from the Internet: URL<https//search.googleblog.com/2016/06/now-on-tap-update-test-select-and-image.html>, 9 pages. |
Written Opinion issued in International Application No. PCT/US2016/065609, dated Jun. 11, 2018, 7 pages. |
Also Published As
Publication number | Publication date |
---|---|
US20220222282A1 (en) | 2022-07-14 |
EP3472724A1 (en) | 2019-04-24 |
CN109791550A (en) | 2019-05-21 |
WO2018004729A1 (en) | 2018-01-04 |
US20170371885A1 (en) | 2017-12-28 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20220222282A1 (en) | Contextual voice search suggestions | |
JP7478869B2 (en) | Application Integration with Digital Assistants | |
JP6749440B2 (en) | Intelligent task discovery | |
US11809886B2 (en) | Intelligent automated assistant in a messaging environment | |
US11599331B2 (en) | Maintaining privacy of personal information | |
KR102194757B1 (en) | Multi-turn canned conversation | |
US10417266B2 (en) | Context-aware ranking of intelligent response suggestions | |
US10909331B2 (en) | Implicit identification of translation payload with neural machine translation | |
US10445429B2 (en) | Natural language understanding using vocabularies with compressed serialized tries | |
US11715464B2 (en) | Using augmentation to create natural language models | |
US20180349472A1 (en) | Methods and systems for providing query suggestions | |
EP3699907A1 (en) | Maintaining privacy of personal information | |
WO2018222776A1 (en) | Methods and systems for customizing suggestions using user-specific information | |
US20220374597A1 (en) | Word prediction with multiple overlapping contexts | |
US10152521B2 (en) | Resource recommendations for a displayed resource | |
KR20190104197A (en) | Contextual speech-driven deep bookmarking | |
US11003667B1 (en) | Contextual information for a displayed resource | |
US11120083B1 (en) | Query recommendations for a displayed resource | |
KR102120605B1 (en) | Client server processing with natural language input to maintain privacy of personal information | |
CN117940879A (en) | Digital assistant for providing visualization of clip information | |
KR20190134794A (en) | Hierarchical Belief States for Digital Assistants | |
CN110574023A (en) | offline personal assistant |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:AGGARWAL, VIKRAM;RAMCHANDRAN, APARNA RAJAN;REEL/FRAME:039292/0426Effective date: 20160729 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044567/0001Effective date: 20170929 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: FINAL REJECTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: FINAL REJECTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: AWAITING TC RESP, ISSUE FEE PAYMENT VERIFIED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |