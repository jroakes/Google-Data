CN110024002A - For using contribution region to carry out the system and method for reconstruction of optical wave field - Google Patents
For using contribution region to carry out the system and method for reconstruction of optical wave field Download PDFInfo
- Publication number
- CN110024002A CN110024002A CN201780074450.7A CN201780074450A CN110024002A CN 110024002 A CN110024002 A CN 110024002A CN 201780074450 A CN201780074450 A CN 201780074450A CN 110024002 A CN110024002 A CN 110024002A
- Authority
- CN
- China
- Prior art keywords
- camera
- view
- light
- contribution
- plane
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T15/00—3D [Three Dimensional] image rendering
- G06T15/50—Lighting effects
- G06T15/503—Blending, e.g. for anti-aliasing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T15/00—3D [Three Dimensional] image rendering
- G06T15/06—Ray-tracing
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T15/00—3D [Three Dimensional] image rendering
- G06T15/50—Lighting effects
- G06T15/506—Illumination models
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2200/00—Indexing scheme for image data processing or generation, in general
- G06T2200/21—Indexing scheme for image data processing or generation, in general involving computational photography
Abstract
One kind is for including identifying with a part of associated light (122,124) of view and selecting camera view set from the multiple camera views for indicating light field based on light and the intersection point (106) of plane (108) according to the method for light field (100,1212,1214) render view.Each camera view has the associated contribution region (302,304,602,604) of setting in the plane.Associated contribution region is Chong Die with contribution region associated by other camera views in the camera view set of point of intersection.The method also includes that the feature of light is determined based on the contribution factor for each of camera view set camera view.The contribution factor is determined based on the relative position of the intersection point in associated contribution region.
Description
Background technique
Virtual reality (VR) and augmented reality (AR) application, which are generally sought, to be allowed users in scene (virtual or real generation
Boundary) in move everywhere and allow users to from the helmet (HMD) of user or other VR/AR viewing equipment current appearance
The scene is watched at state.It proposes using light field as light content format with can be according to the shadow of multiple and different view rendering scenes
Picture.However, the light field suitable for reality description can be used a large amount of process resource, including central processing unit (CPU) and
The a large amount of processing cycle of graphics processing unit (GPU).For mobile consumer device, have since mobile consumer device is opposite
Storage, transmission and the process resource of limit are usually particularly problematic using stock number required for the light field for VR and AR application.
Detailed description of the invention
The disclosure may be better understood by reference to attached drawing and make multiple feature and advantage of the disclosure for ability
The technical staff in domain is apparent.
Fig. 1 and Fig. 2 is the figure indicated according to the example light field of some embodiments.
Fig. 3 is the figure according to the example camera plane of some embodiments.
Fig. 4 is the block diagram for the exemplary method of render view in light field illustrated according to some embodiments.
Fig. 5 is the figure according to the example camera level of some embodiments.
Fig. 6 and Fig. 7 is the figure according to the example selection to the camera for realizing camera level of some embodiments.
Fig. 8 is the block diagram for the exemplary method of render view in light field illustrated according to some embodiments.
Fig. 9 and Figure 10 is the exemplary figure for illustrating the focus that view is changed in light field according to some embodiments.
Figure 11 is the frame for illustrating the exemplary method of the focus for changing view in light field according to some embodiments
Figure.
Figure 12 is the block diagram according to the display system for being able to use light field of some embodiments.
Figure 13 is the example implementation of the lightfield compression equipment of the display system in the Figure 12 illustrated according to some embodiments
The figure of mode.
Figure 14 is that light field decompression/rendering apparatus of the display system in the Figure 12 illustrated according to some embodiments shows
The figure of example implementation.
The use of identical appended drawing reference in different attached drawings indicates similar or identical item.
Specific embodiment
It is described below and is intended to be related in light field multiple specific embodiments of render view and details by providing and convey
Thorough understanding of the present disclosure.It is, however, to be understood that the present disclosure is not limited to these specific embodiments and details, it is described
Specific embodiment and details are only used as example and therefore the scope of the present disclosure is intended to only by appended claims and its coordinate
Limitation.It is to be further understood that those skilled in the art will be appreciated that according to known system and method, depend on
Specific design and other demands, using the disclosure to be used for expected purpose and benefit in any amount of alternate embodiment
Place.
It can be by light field (lightfield) (also referred to as lumen figure (lumigraph) or light transmission field (photic field))
It is conceptualized as the amount of the light flowed in each direction in the space of restriction or environment by each point, wherein by five dimensions
Plenoptic function specifies the direction of every light in light field and specifies the amplitude of each light by corresponding radiance.Ginseng
Common methods of the numberization for the light field of computer graphical implementation are parameterized via biplane, in the biplane parameter
Light field is expressed as to the fluoroscopy images set of st plane (commonly referred to as " focal plane "), each of described set image in change
Indicate the perspective of the virtual camera of corresponding position in the uv plane (commonly referred to as " camera plane ") parallel with st plane
Figure.
As shown in Figure 1, can be used and the battle array of surface (such as plane surface, such as focal plane 110) associated image
It arranges to render the view at the visual angle from observer 126.Observer can be extended to according to from surface (for example, focal plane 110)
126 light (for example, light 122 or light 124) constructs the view of observer 126.Although by surface (for example, following institute
The focal plane 110 stated or camera plane 108) it is illustrated as plane, surface can have the form of other non-planar manifolds.
In the embodiment of diagram in Fig. 1, according in two parallel planes (for example, camera plane 108 and focal plane
110) intersection point of glazed thread indicates the direction of light.In one example, using the phase with specified coordinate " u " and " v "
Machine plane 108 and focal plane 110 with specified coordinate " s " and " t " pass through camera plane 108 in focal plane 110 to specify
The direction of the light extended between observer 126.
The view of observer 126 is shown in display equipment 120.It is set based on the display by camera plane 108 and modeling
The features of standby 120 light (for example, light 122 or light 124) extended between focal plane 110 and observer 126 renders
The part for the view to be shown in display equipment 120.Although illustrating single display device and observer, can be used more than one
A display equipment.It is, for example, possible to use two displays, one is used for right-eye view for left-eye view and one, and
It can be to each eyes render view or image of observer.A part of view can be pixel or pixel set.One
In a example, feature includes radiance (radiance).In another example, feature may include color value or transparency
(alpha)。
Light characteristics can be determined based on the corresponding part of camera view.It can be based on light and camera plane 108
Intersect the selection to camera or camera position subset is associated with light.It is selected based on the intersection of light and focal plane 110
Select camera view associated with camera or camera position subset.It in one example, can be by the two-dimensional array of camera position
It is associated with camera plane 108.It can be by the two-dimensional array of camera view with the camera position from plane 108 and in focal plane
Camera view position on 110 is associated.For example, the two-dimensional array of the camera view in focal plane 110 uniquely with camera
Each of array of position camera position is associated, and the array of the camera position is associated with camera plane 108.
Although focal plane 110 and camera plane 108 are illustrated as plane, other non-planar manifolds can be used.For example,
Camera in camera plane 108 can be arranged on sphere outward with camera.Observer can be in any position of ball interior
It sets, and any view can be reconstructed from ball interior.In such an example, exist to indicate to project light field figure on it
The various ways of the focal plane of picture: can be by camera view image projection to the sphere bigger than camera sphere, it can be by phase
Machine view image projects on the single plane before observer, or can be by camera view image projection to each camera
Or in the plane before camera position etc..Method as described herein allows to be distributed camera array or projection camera view on it
Image, offer different user viewing experience various non-planar manifolds.
Fig. 2 is illustrated to be indicated according to the biplane of the typical light field of some embodiments.Example light field 100 is by image (example
Such as, image 103,104,105) the composition of two dimension (2D) array 102, described image are also referred to as camera view herein, wherein battle array
Each of column 102 image indicates to reach camera plane 108 from whole points on focal plane 110 (with dimension " s " and " t ")
The light of corresponding points (for example, point 106) on (with dimension " u " and " v ").In such implementation, in array 102
Image (camera view) indicates off-axis or shearing, perspective the view of scene or environment.It therefore, can be by 2D array 102
It is expressed as the array of image, wherein each image i has by coordinate (si,ti) position in the array that defines.In addition, each
A image is indicated by the 2D array of pixel.The array of the pixel of image can be contemplated that the array of image tiles, wherein each
A image tiles indicate corresponding pixel region.Reference picture 105 illustrates, each image by such as 4x4 tile (for example,
Tile 114,115) array are constituted.Although tile 114 and 115 is illustrated as quadrangle tile, tile can have multilateral shape,
For example, triangle.Each tile (for example, tile 114,115) and other tiles can be sent separately central processing
Unit and graphics processing unit are to render.
Therefore, as shown in the light field 100 in Fig. 2, light field can be indicated by the image of scene or the array of camera view,
Wherein each image indicates the perspective view of the scene of the slight plane displacement of the perspective view relative to other images in light field.It is logical
Often, light field includes this kind of image of relatively large amount, and can be rendered or be captured these images with relatively high resolution ratio.
When rendering view from light field, based on extending to observer's 126 from camera plane 108 and focal plane 110
The feature of light comes the part of render view.The feature of each light is determined based on the subset of image or camera view.Phase
Machine view is associated with camera position and based on contribution region associated by the camera position on light and camera plane 108
Intersect and camera view is selected based on the intersection of light and focal plane 110.
It in one embodiment, can be based on tribute associated by each of light and camera plane 108 camera position
The position of the intersection in region is offered to select the subset of camera position and associated camera view.In an alternative embodiment,
Contribute region associated with each of focal plane 110 camera view.It can be based on the contribution of the weighting of camera view
It determines the feature (for example, radiance, color value or transparency) of light, the camera view and has and camera plane
Each of the subset of camera position in contribution region of intersection overlapping of light in 108 camera position is associated.For example,
Light can be determined based on the summation of the feature of the weighting obtained from camera view associated with selected camera position
Feature.Weighted value associated with camera view, also referred to as contribution factor, can be based on the ray intersection in contribution region
Relative position determine.Optionally, the summation of the contribution factor of selected camera view can be used for example to normalize
Weighted value (contribution factor) associated with contribution.
For example, as shown in Figure 3, the camera position in the array of camera position associated with camera plane 108
It can have associated contribution region 302 or 304.Although contribution region 302 or 304 is illustrated as disk, contribution region can
With other shapes.For example, contribution can be round, ellipse, quadrangle or polygon disk.
As shown, light intersects with camera plane in a certain position (for example, position 306 or position 308).Selection has
The camera position in the contribution region Chong Die with intersection location is to provide the subset of camera position.It is selected using the subset of camera position
Select the camera view for determining the feature of light.For example, based on the contribution region Chong Die with the position 306 of the intersection of light,
The light for intersecting at position 306 can lead to selection to camera position C0, C1, C2 and C4.In another example, light with
When camera plane intersects at position 308, camera position C0, C4, C6 and C7 are selected.
Camera view can be selected based on the projection of the subset of camera position and light on focal plane.It can be based on tribute
The relative position of ray intersection in region is offered to be weighted to the contribution of camera view.For example, the weighted value or contribution of contribution
The factor can change with the radial distance away from the camera position in contribution region.In one example, contribution factor can be with
Distance increase away from camera position changes to 0 from 1 in a smooth manner.For example, contribution can increase with the distance away from camera position
Linearly reduce.In another example, contribution can be reduced with the radial distance away from camera position with Gaussian dependence.
In order to further illustrate using selected camera view to carry out render view, method 400 shown in Fig. 4 includes
The viewpoint of determination observer as indicated at block 402.Position and orientation based on observer are come using the light extended in light field
Render the view to show to observer and for example using setting in two parallel planes (such as, camera plane and focal plane)
On four dimensional coordinate system light can be generated.
Display image or view are rendered can choose light as shown in frame 404.Light passes through camera plane and wears
The display plane for indicating display equipment is crossed, extends to observer from focal plane.One of light and view to be shown or image
Split-phase association.The part of view can be pixel or can be pixel set.The feature of light can be used to reconstruct display
A part, the feature of such as specified light that will be shown by pixel.
As shown in block 406, the camera position set in the array of camera position can be identified.Selected light with
Camera plane intersection.It can choose the phase seat in the plane with the contribution region Chong Die with the point of the intersection of the light in camera plane
It sets.Contribution region can be used to determine which camera position associated with light and determination and the associated contribution of camera view
The factor, the camera view are associated with selected camera position.
As shown in block 408, the intersection based on selected camera position and light and focal plane can choose camera
View set.Can be used with and the part of the associated camera view in the part of view of ray intersection generate the spy of light
Sign, the feature of the light are used to determine the part of view or shown image.
The feature of light is determined camera view set as shown in 410, can be used.In one example, feature
It may include radiance, color value or transparency.Particularly, from the part for corresponding to view associated with light or image
Each camera view in select a part, and in the feature for determining light, use the part in camera view
Feature.It is, for example, possible to use the summations of the normalized weighting of the feature of the part of selected camera view to specify light
Feature.
It can be based on the relative distance of the intersection point of camera position associated with camera view to the light in contribution region
To determine the weighted value or contribution factor of the feature from each camera view.Alternatively, light and focal plane phase can be based on
It hands over the relative distance of the position of camera view in focal plane and determines contribution factor.It can be returned based on the summation of contribution factor
One changes each of contribution factor.For example, each camera view is and camera view phase for the contribution of the feature of light
The product of the value of the feature of the part of associated contribution factor and camera view.By the contribution of each selected camera view
The summation of the contribution factor of selected camera view is added and is optionally based on to normalize each selected camera
The contribution of view.
In other words, the feature of light can be the summation of the contribution of each camera view.By the way that contribution factor is multiplied
Contribution is normalized divided by the summation of each contribution factor with the characteristic value of camera view and by product to determine each phase
The contribution of machine view.Alternatively, by the product addition of the value of each contribution factor and camera view and can divide the sum by
The summation of each contribution factor is to normalize.
As shown in block 412, the part of display image can be rendered based on the feature of light.For example, can will be with
The associated radiance value of the feature of light is applied to the part of display image.In a specific example, light can be based on
Feature be arranged pixel value.
The process of selection light can be repeated for each of display image part.As shown in frame 414, view
Or the part of image rendered may be displayed in display equipment, and can indicate the viewpoint of observer.
This method provides technical advantage, including using less processor resource and accelerating to show to observer
The rendering of view or image.For example, visibility processing and rendering tinter it is simpler than conventional method and it is usual faster.Reconstruct
Filter is flexible, and this method allows through the size in change contribution region to double and depth of field aperture mould with edge
It trades off between paste.The size and shape in contribution region can be the size and shape of scattered scape.Therefore, camera light is imitated in contribution region
Circle.
In addition, providing the level of detail of improved adaptability light field using the method in contribution region.For example, this method permits
Perhaps the level level of details rendering.For example, camera position or associated contribution region can be by hierarchical organizations.Example hierarchical packet
Include kd tree, Octree or enclosure body level (BVH) etc..In at least one embodiment, the two-dimensional image array of light field will be indicated
In image organizational be binary tree structure.
When adjusting level of detail, the level in the contribution region of the feature to identify camera view and determining light
Using providing further advantage.For example, the view or image of more details are gradually rendered there are budget in process resource
When for display, level of detail can be gradually increased.In another example, as observer position is closer to scene, Ke Yiji
Calculate increasingly detailed view.In another example, the level of detail gradually increased can be determined until meeting standard.
In concept, can by increase camera density and camera view density or the degree of approach, or by using more
Carry out the camera view of higher resolution ratio, or both, to provide gradually more detailed details.For example, as shown in Figure 5, it can be with
Camera density is selectively changed by selecting the subset of camera position.In initial level of detail, phase seat in the plane can choose
502 are set, and the camera view from camera position 502 can be used to determine the feature of one or more light.In order to mention
High level of detail can add additional camera or camera position.It is, for example, possible to use the phases from camera position 502 and 504
Machine view come generate middle rank level of detail.Optionally, at the level of detail of middle rank, intermediate image in different resolution can be used for
Each camera view generated from camera position 502 or 504.It can be improved using the camera or camera position of greater density
Further details level, for example, being selected using camera position 502,504 and 506 useful in the feature for generating light
Camera view.Optionally, when selecting the camera or camera position of larger number or density, it is also an option that higher resolution
Camera view.
As described above, increased camera density can be further increased using the resolution ratio of the camera view gradually increased.
For example, low-resolution cameras view also can be used when generating the feature of light using camera position 502.Using increase
Camera density when, such as when using camera position 502 and 504, middle class resolution ratio can be used for camera view.Another
In example, when using the arranged in high density for utilizing camera position 502,504 and 506, the image of higher resolution can be used
In camera view.In one example, the side of each image of high-resolution can be lower resolution image above-mentioned
Size approximately twice as.
This method can be advantageously used for providing improved picture quality, Huo Zheke when processor is allowed using budget
With the image for generating low resolution at sighting distance and when observer is obviously very close to, high-resolution is generated
Image.
Further illustrate the use of camera level, Fig. 6 illustrates camera array 600, wherein based on selected by level
Level select camera position 602.It can be selected based on light with the intersection in region (for example, contribution region 604) is contributed
The contribution factor of camera position 602 and camera view associated with camera position 602 and associated camera view can be with
It is obtained from the intersection point and the relative position in contribution region 604 of light and camera plane.
As shown, contributing region when selecting subset or the lower density arrangement of camera view or camera position 602
604 can relative to the earth be not that other camera positions 606 of a part of selected level in level are Chong Die in array.
Optionally, in the light characteristics of low-density level for determining level, low-resolution cameras view can be with 602 phase of camera position
Association.
Use the increased density and optionally associated with camera position higher resolution of camera position
Higher level of detail may be implemented in camera view.For example, as shown in Figure 7, can choose has than institute in camera array 600
The camera array 700 of the camera position 702 of the higher camera density of camera position 602 of selection.It can reduce contribution region 704
Size to adapt to the more high density for the selected camera position of array 700.It is alternatively possible to by the figure of higher resolution
As being used as camera view to further enhance the level of detail to be rendered.As shown, contribution region 704 is less than contribution region
604。
For example, the method 800 illustrated in Fig. 8 may include selection level of detail as shown in frame 802.Such as frame 804
Shown in, it is based on selected level of detail, can choose the subarray of the camera position in camera array.Subarray can wrap
Include the camera position set compared with low-density or may include all or less than camera position.
As shown in frame 806, it can be arranged based on the density of subarray related to each camera position on subarray
The size in the contribution region of connection.For example, there is larger sized contribution region compared with the subarray of the camera position of low-density.
It, can be with render view or display image as shown in frame 808.For example, can be by following come render view or aobvious
Diagram picture: determining and shows the associated light of each section of image, based on the contribution region with selected ray intersection come
Select the contribution of camera or camera position and the weighting based on camera view associated with selected camera position come really
Determine the feature of light.It can use light characteristics to carry out render view or show each section of image.Optionally, such as 812 place of frame
Show, view can be shown on the display device.
Alternatively, iterative process can be used to determine whether to meet criterion or whether exhaust pre- as shown in frame 810
It calculates and (such as, handles budget).For example, if rendering low resolution view, system can determine that additional processing budget whether may be used
With and can be used additional processing budget and improve the level of detail of view or image.For example, as shown in frame 802, it can
To select more detailed level, as shown in frame 804, the camera subarray of greater density can choose, as shown in frame 806,
Smaller contribution region can be set for the subarray of greater density, and can increased density based on camera subarray and
Associated camera view shows image or view to render in more detail.
Other standards can be with, it may for example comprise relative distance of the observer away from camera plane or camera view plane, image
Clarity, or illustrate the function of the movement of observer.As shown in frame 812, there is the image of desired level of detail in rendering
Or when view, the view or image can be shown on the display device.
The above method further advantageously allows the more effective generation of the view for the variation for having in focus.In example
In, the focus of view can be changed by changing the relative position of focal plane 110 and camera plane 108.For example, such as Fig. 9 and
Shown in Figure 10, distance can be increased relative to the position of camera plane 108 by mobile focal plane 110 or reduce distance come more
Change the focal position of view.For example, if focal plane 110 is moved to position B, the phase of light and camera plane 108 from position A
Friendship does not change.However, the position change that light intersects with focal plane 110.For example, focal plane 110 is moved to position from position A
B (Fig. 9) makes the intersection point in focal plane 110 be converted to point B (see Figure 10) from point A, this leads to the camera view to contribution light characteristics
The different selections of figure.When contributing region to be present in camera plane, the relative distance for adjusting focal plane is kept to phase seat in the plane
Set the selection with contribution factor.Reselect camera view.Alternatively, in contribution factor based on the contribution region in focal plane 110
Intersection position when, the change of the relative position of focal plane 110 can cause from camera view it is associated it is different contribution because
Son, and selected camera position remains unchanged.
For example, illustrated method 1100 out in Figure 11, as shown in frame 1102, including relocates focal plane 110.Such as
Shown in frame 1104, the variation of the position of the intersection based on light selects different camera view subsets.In contribution region and phase
When machine plane is associated, selected camera position and associated contribution factor are remained unchanged, and are reduced for changing focus
Processing load.
As shown in frame 1106, modified camera view subset can be used to determine light characteristics.Particularly, can make
Light characteristics are determined with the weighted array of the feature of each section of modified camera view subset.It is, for example, possible to use cameras
The summation of the weighting of the feature of each section of view determines light characteristics.It can be based on the position of the intersection of light and contribution region
It sets to determine contribution factor.In this example, contribution factor is normalized based on the summation of contribution factor.In a specific example,
Contribution factor is determined based on the position of the intersection in the contribution region in light and camera plane.Alternatively, being based on light and Jiao Ping
The position of the intersection in the contribution region in face determines contribution factor.
As shown in frame 1108, the mistake that camera view subset is selected based on the position of the change of ray intersection can be repeated
Journey shows each section of image or view to generate.As shown in frame 1112, the display image or view rendered by process can
To show on the display device.
In another example, the visual field and scattered scape can be influenced by the size and shape in contribution region.Particularly, tribute
It offers region and imitates camera aperture.
The above method can be realized in one or more systems, optionally use network.Figure 12 illustrates example and shows
System 1200.Display system 1200 decompresses/rendering portion including light field generating device 1202, lightfield compression component 1206, light field
Part 1208 and display equipment 1210.
In this example, light field generating device 1202 generates or obtains the light field 1212 for indicating scene 1213.In order to illustrate light
Field generating device 1202 may include light-field camera, for example, the light-field camera is configured as capture light field 1212, such as via quilt
Be configured in camera plane multiple camera apparatus of the multiple images of capturing scenes 1213 with different view, obtained in it is more
A image is arranged with 2D image array format, and the 2D pattern matrix 102 in such as Fig. 2 is illustrated.Alternatively, light field generating device
1202 may include graphics rendering device, (such as, such as the graphics rendering device is configured as by using light field Rendering
The contribution regional development and technology) image in pattern matrix to render light field 1212 is real come the VR or AR for generating scene 1213
The light field 1212 of existing mode.No matter the light field 1212 is generated by image capture or figure rendering, by light field
1212 caching or be otherwise stored in storage equipment 1204 in (for example, random access memory, disk drive etc.) so as to
It is further processed.
Lightfield compression component 1206, which can be operated, to be generated with squeezed light field 1212 by than the less data institute of light field 1212
The light field 1214 of the compression of expression, and therefore more suitable for effectively storing, transmitting and handling.
In one example, the light field of compression 1214 is supplied to light field decompression/rendering component 1208 so as to further
Processing, as a result, by by the light field 1214 of compression be stored in can by component 1206,1208 access storage equipment 1216 in, lead to
Crossing will compress via one or more networks 1218 or other data communications interconnection (for example, data cable) or combinations thereof
The expression of light field 1214 be transmitted to light field decompression/rendering component 1208 from lightfield compression component 1206, to provide compression
Light field 1214.
In order to illustrate component 1206,1208 being embodied as the component of larger equipment, and storing equipment 1216 can be with
Hard disk including system storage or larger equipment.As another example, component 1206,1208 can away from each other, and pass through
The light field 1214 of compression is supplied to component 1208 from component 1206 by the server being located on network 1218.
In another example, light field decompression/rendering component 208 can be operated to identify and to show at display equipment 1210
The view for the scene 1213 shown, and from which image in the light field 1214 that the view of the identification can identify compression which
Tile will be used to render the image (that is, which image tiles is " visible " in the view identified) for indicating view.Light
Field decompression/rendering component 1208 can access identified image tiles from the light field 1214 of compression and decompress tile with life
At the image tiles of decompression.According to the image tiles of decompression, light field decompression/rendering component 1208 can render one or
Multiple display images (rendering image 1220), one or more of display images are provided to display equipment 1210 for showing
Show.
In order to illustrate in some embodiments, display equipment 1210 includes head-mounted display (HMD) equipment, and wants wash with watercolours
Current pose of the view of the scene 1213 of dye based on HMD device relative to the reference frame of scene 1213.Identifying this
In the case where posture, light field decompression/identification of rendering component 1208 indicates the compression of visible image from given posture
The tile of light field 1214 decompresses identified tile, renders left-eye image and eye image according to the tile of decompression, and
It is used for while showing to HMD device offer left-eye image and eye image to provide the 3D view of scene 1213 to the user of HMD
Figure.
In some implementations, light field decompression/rendering component 1208 is with relatively limited processing, storage or biography
It is realized in the mobile device or other equipment of defeated resource.Therefore, in order to convenient for generating the compression of the image 1220 of rendering
Light field 1214 is effectively treated, and lightfield compression component 1206 uses one or more lightfield compression technologies, and such as parallax prediction is replaced
(DPR) compress technique 1222 is changed to reduce the light field 1214 for indicating to be provided to the compression of light field decompression/rendering component 1208
Required data volume.
In addition to DPR compress technique 1222, other one or more compress techniques are can be used in lightfield compression component 1206, and
And other than DPR compress technique 1222, the decompression of one or more compensation is can be used in light field decompression/rendering component 1208
Contracting technology.For example, lightfield compression component 1206 can be decompressed using disparity compensation prediction (DCP) compression process 1226 and light field
Contracting/rendering component 1208 can use corresponding DCP decompression process 1228.
Figure 13 illustrates the exemplary hardware implementation of the lightfield compression component 1206 according at least some of embodiment.Institute
In the example of description, lightfield compression component 1206 includes application processor 1302, and the application processor 1302, which has, to be coupled to
The interface of non-transitory computer-readable storage media 1304, is coupled to the interface of storage equipment 1204, and is coupled to network
1218 and storage equipment 1216 in the interface of one or two.Application processor 1302 may include one or more centres
Manage unit (CPU), graphics processing unit (GPU) or combinations thereof.Computer readable storage medium 1304 includes such as one or more
A random access memory (RAM), one or more read-only memory (ROM), one or more hard disk drives, one or more
A CD drive, one or more flash drives etc..
Computer readable storage medium 1304 stores software in the form of executable instruction, and the executable instruction is configured
It is manipulation application processor 1302 to execute one or more of procedures described herein.It can be in order to illustrate, the software
Including for example, the DCP module 1310 for executing DCP cataloged procedure, the tree behaviour for executing binary tree generation and ergodic process
Make module 1312, searched to calculate for the movement of each DDV with reference to tile for motion search process based on one or more
Rope module 1314 and the tile replacement module 1316 that process is eliminated for executing selective tile.
Figure 14 illustrates light field decompression/rendering component 1208 exemplary hardware realization side according at least some of embodiment
Formula.In the described example, light field decompression/render component 1208 includes CPU 1402, and the CPU 1402 has coupling
To non-transitory computer-readable storage media 1404 interface and be coupled to the interface of Inertial Measurement Unit (IMU) 1406, with
And it is coupled to network 1218 and stores the interface of one or two in equipment 1216 (being not shown in Figure 14).Light field decompression/
Rendering component 1208 further comprises GPU 1408, the GPU 1408 have be coupled to CPU 1402 interface, be coupled to it is non-
The interface of temporary computer readable storage medium 1410 is coupled to the interface of frame buffer 1412 and is coupled to display control
The interface of device 1414.Then, display controller is coupled to one or more display panels, the left eye display surface of such as HMD device
Plate 1416 and right-eye display panel 1418.
In at least one embodiment, the workload of light field decompression process and render process is in CPU 1402 and GPU
It is distributed between 1408.Store software in the form of executable instruction in order to illustrate, computer readable storage medium 1404, it is described can
It executes instruction and is configured as operation CPU 1402 identification and DCP decoding (on-demand) light field tile to render the specific view for indicating scene
The image of figure, simultaneous computer readable storage medium storing program for executing 1410 store software, the executable instruction in the form of executable instruction
Operation GPU 1408 is configured as with any eliminated tile needed for reconstruction render image, and then using acquired
Image is rendered with the tile of reconstruct.In order to illustrate the end CPU software may include: view determination module 1420, and the view is true
Cover half block 1420 is used to determine the posture of HMD device 1210 (Figure 12) via IMU 1406 and determines scene 1213 according to posture
In active view；Visibility analysis module 1422, the visibility analysis module 1422 for identification " can from active view
See " or the image watt as described above for according to the light field 1214 of the compression of the images of active view render scenes 1213
Piece set；And tile decoder module 1424, the tile decoder module 1424 are used to access the light field 1214 in compression
Tile those of in the set of middle presentation carries out DCP decoding to tile, and provides current tile (for example, tile to GPU 1408
1426)。
In the sample implementation of contribution region method, light field tile can be sent to GPU, and can be several by disk
What structure is sent as tinter variable for determining the weighted value or contribution factor of each segment.It can be carried out by CPU pair
Render the selection of which light field tile.More specifically, can by using tile quadrangle as geometry pass to GPU come
Complete effective GPU implementation.The location and shape of disk are transmitted as tinter parameter.Then using projective textures come to watt
Piece is textured.Using the light of the colored spots on from eyes to tile and disk or contributes the intercept in region and determine the sample
Weighted value.
Example
By the method for using contribution zone rendering image and use with barycentric interpolation (barycentric
Interpolation the method for triangular mesh (triangular mesh)) is compared.For test purpose, realizing
The identical light field including camera position and camera view is used when two methods.
Table 1 illustrates every frame render time of every kind of method on distinct device.
1 render time of table
* adjustment contribution zone radius, which provides, will keep the better performance being completely covered in contribution region.
As shown in table 1, it when using region method is contributed, when especially realizing on the mobile apparatus, obtains quite big
Performance gain.
In some embodiments, some aspects of above-mentioned technology can be by one or more in the processing system of execution software
A processor is realized.Software includes storage or is otherwise tangibly embodied in non-transitory computer-readable storage media
One or more executable instruction set.Software may include instruction and certain data, described instruction and certain data by
When one or more processors execute, one or more processors are operated to execute the one or more aspects of above-mentioned technology.It is non-
Temporary computer readable storage medium may include for example, disk or optical disc memory apparatus, solid storage device (such as flash memory),
Cache, random access memory (RAM) or other non-volatile memory devices or device etc..It is stored in non-transitory calculating
Executable instruction on machine readable storage medium storing program for executing can be source code, assembler language code, object code or by one or more
Other processor interpretation or otherwise executable instruction formats.
Computer readable storage medium may include during use can be by any storage medium of computer system accesses
Or the combination of storage medium is to provide instruction and/or data to computer system.Such storage medium may include but unlimited
In optical medium (for example, CD (CD), digital versatile disc (DVD), Blu-ray Disc), magnetic medium (for example, floppy disk, tape
Or magnetic hard drive), volatile memory (for example, random access memory (RAM) or cache memory), it is non-easily
The property lost memory (for example, read-only memory (ROM) or flash memory) or the storage medium for being based on MEMS (MEMS).Computer
Readable storage medium storing program for executing can be embedded in computing system (for example, system RAM or ROM), be fixedly attached to computing system (example
Such as, magnetic hard drive), computing system is removably attached to (for example, CD or based on universal serial bus (USB)
Flash memory) or via wired or wireless network (for example, network-accessible storage equipment (NAS)) it is coupled to computer system.
It is noted that not needing the above whole show described in general description or example, it may not be necessary to specific
Movable a part, and other one or more activities can be executed other than those described activities.In addition, movable
The sequence listed is not necessarily the sequence of its execution.
In specification in front, these concepts are described by reference to specific embodiment.However, ordinary skill
Personnel understand, without prejudice to the scope of the present invention proposed in the following claims, can carry out each
Kind modifications and changes.Therefore, the description and the appended drawings should be considered as illustrative and not restrictive, and all such modifications purport
It is being included within the scope of the invention.
As it is used herein, term " includes ", " containing ", "comprising", "comprising", " having ", " having " or its is any
Other variations are intended to cover non-exclusive inclusion.E.g., including the process, method of the list of feature, article or device are different
Surely be only limitted to the feature, but may include not expressly listed or this process, method, article or device intrinsic its
His feature.In addition, unless expressly stated to the contrary, "or" refer to inclusive or and not refer to it is exclusive or.For example,
In following any one, condition A or B are satisfied: A is true (or presence) and B is false (or being not present), A be it is false (or not
In the presence of) and B be that true (or presence) and A and B are true (or presence).
Moreover, describing element and component as described herein using "a" or "an".This just for the sake of convenient and
Provide the general significance to the scope of the invention.The description is construed as including one or at least one, and odd number also wraps
Plural number is included, unless significantly referring else.
Benefit, other advantages and solution to the problem are described relative to specific embodiment above.However, beneficial
Place, advantage, solution to the problem and any benefit, advantage or solution may be made to occur or become more significant
Crucial, the required or basic feature that any feature has not been explained as any or whole claims.
After specification is read, it will be appreciated by those skilled in the art that in order to clear and herein individual
Certain features described in the context of embodiment can also combine offer in a single embodiment.On the contrary, in order to succinct
And the various features described in the context of single embodiment can also be provided separately or be provided with any sub-portfolio.This
Outside, to value described in range reference include this within the scope of each of and each value.
Claims (29)
1. one kind is for the method according to light field (100,1212,1214) render view, which comprises
It identifies and a part of associated light (122,124) of the view；
Based on the intersection point (106) of the light (122,124) and plane (108) come from indicate the light field (100,1212,
1214) camera view set is selected in multiple camera views, each of described camera view set camera view has
The associated contribution region (302,304,602,604) being arranged on the plane (108), the associated contribution region
(302,304,602,604) with associated by other camera views in the camera view set at the intersection point (106)
Contribution region (302,304,602,604) be overlapped (302,304,602,604) s (302,304,602,604)；And
Determined based on the contribution factor for each of the camera view set camera view light (122,
124) feature, the contribution factor are based on the friendship in associated contribution region (302,304,602,604)
The relative position (306,308) of point (106) determines.
2. according to the method described in claim 1, wherein the plane (108) be with camera position (502,504,506,602,
702) the associated camera plane (108) of array (102,600,700), the associated phase seat in the plane of each camera view
(306,308) are set, there is the associated contribution region (302,304,602,604).
3. according to the method described in claim 2, wherein the associated camera position (306,308) is arranged on the tribute
Offer the center of region (302,304,602,604).
4. according to claim 2 or method as claimed in claim 3, wherein the multiple camera view and focal plane (108)
(110) it is associated.
5. according to the method described in claim 4, further comprising selecting the camera plane (108) (108) and the coke flat
Relative distance between face (108) (110).
6. according to method described in any one in preceding claims, wherein the part of the view is pixel or picture
Element set.
7. according to method described in any one in preceding claims, wherein described be characterized in color value.
8. according to method described in any one in preceding claims, wherein the contribution region (302,304,602,
It 604) is round or oval disk shape in the plane (108).
9. according to claim 1 to method described in any one in 7, wherein the contribution region (302,304,602,
604) there is polygonal shape in the plane (108).
10. according to method described in any one in preceding claims, wherein the contribution factor is with the intersection point (106)
The radial distance at the center away from contribution region (302,304,602,604) is reduced.
11. according to the method described in claim 10, wherein the contribution factor is linearly reduced with the distance away from the center.
12. according to the method described in claim 10, wherein the contribution factor with the distance away from the center in a gaussian manner
It reduces.
13. according to method described in any one in preceding claims, wherein the feature is based on the camera view collection
The summation of the contribution of conjunction determines that the contribution of each of described camera view set camera view is regarded based on each camera
The product of the feature of a part of figure and the contribution factor determines.
14. according to the method for claim 13, wherein the contribution factor using the camera view set contribution because
The summation of son normalizes.
15. further comprising according to method described in any one in preceding claims:
To each part of the view, the light (122,124) are identified, select the camera view set, and really
The feature of the fixed light (122,124)；
The view is rendered based on the feature of the light；And
The view is shown in display equipment (120,1210).
16. method of the one kind for rendering the view to show in display equipment (120,1210), which comprises
Selection extends through the light of a part of the view, camera plane (108) and focal plane (110) from observer (126)
Line (122,124), the camera plane (108) and camera position (502,504,506,602,702) array (102,600,
700) be associated, the arrays of the focal plane (110) and camera position (502,504,506,602,702) (102,600,
700) array (102,600,700) of camera view associated by the camera position (502,504,506,602,702) in is related
Connection, the light (122,124) and the camera plane (108) the first intersection location (306,308) intersect and with it is described
Focal plane (110) is intersected in the second intersection location (306,308), each of described camera plane (108) camera position
(306,308) associated contribution region (302,304,602,604)；
Identify camera position (502,504,506,602,702) the array (102,600,700) in camera position (502,
504,506,602,702) gather, each of described camera position (502,504,506,602,702) set camera position
The associated contribution region (302,304,602,604) of (306,308) and first intersection location (306,308) weight
It is folded；
Based on the camera position (502,504,506,602,702) set and the second intersection point in the focal plane (110)
(106) the camera view set in the array (102,600,700) to identify camera view；
The spy of the light (122,124) is determined based on a part of each of camera view set camera view
Sign, described a part of each camera view is associated with described a part of the view, the light (122,124)
The feature determined based on the feature of the part of the camera view with the summation of the product of associated contribution factor,
Contribution factor associated with each camera view based on camera position associated by each described camera view (306,
308) relative position (306,308) of the first intersection point (106) in associated contribution region (302,304,602,604) is come true
It is fixed；And
The part of the display is generated based on the feature of the light (122,124).
17. further comprising according to the method for claim 16, selecting the camera plane (108) (108) and the coke
Relative distance between plane (110).
18. according to claim 16 or claim 17 described in method, wherein the part of the view is pixel or picture
Element set.
19. method described in any one in 6 to 18 according to claim 1, wherein described be characterized in color value.
20. according to claim 16 or claim 19 in any one described in method, wherein the contribution region (302,
It 304,602,604) in the plane (108) is round or oval disk shape.
21. method described in any one in 6 to 19 according to claim 1, wherein the contribution region (302,304,602,
604) there is polygonal shape in the plane (108).
22. method described in any one in 6 to 21 according to claim 1, wherein the contribution factor is with the intersection point
(106) radial distance at the center away from contribution region (302,304,602,604) is reduced.
23. according to the method for claim 22, wherein the contribution factor is linearly reduced with the distance away from the center.
24. according to the method for claim 22, wherein the contribution factor with the distance away from the center in a gaussian manner
It reduces.
25. method described in any one in 6 to 24 according to claim 1, wherein the contribution factor uses the camera
The summation of the contribution factor of view set normalizes.
26. method described in any one in 6 to 25 according to claim 1, further comprises, to each portion of the view
Point, the light (122,124) are selected, identifies that the camera position (502,504,506,602,702) gather, identifies the phase
Machine view set and the feature for determining the light (122,124)；The view is rendered based on the feature of the light
Figure；And the view is shown on the display equipment (120,1210).
27. one kind is for the method according to light field (100,1212,1214) render view, which comprises
The first light (122,124) characteristic set according to the determination of first camera view set for a plurality of light, described first
Camera view set is selected based on first camera density and the first contribution region (302,304,602,604) size；
The second light (the 122,124) characteristic set for being used for a plurality of light is determined according to second camera view set, it is described
Second camera view set is selected based on second camera density and the second contribution region (302,304,602,604) size,
Described in first camera density be less than the second camera density, wherein it is described first contribution region (302,304,602,604)
Size is greater than described second contribution region (302,304,602,604) size；And
Carry out render view using second light (122,124) characteristic set；And
The view is shown in display equipment (120,1210).
28. according to the method for claim 27, further comprising determining when processor budget is available and being used for third camera
Third light (122,124) characteristic set of a plurality of light in view set, the third camera view set are based on
Third camera density and third contribute region (302,304,602,604) size to select, wherein the second camera density is small
In the third camera density, wherein second contribution region (302,304, the 602,604) size is contributed greater than the third
Region (302,304,602,604) size.
29. one kind is used for the system according to light field (100,1212,1214) render view, the system comprises:
It shows equipment (120,1210)；
It stores equipment (1204,1216), storage equipment (the 1204,1216) storage is by described in represented by multiple camera views
Light field (100,1212,1214)；And
Counting circuit, the counting circuit and the display equipment (120,1210) and the storage equipment (1204,1216) are logical
Letter, the counting circuit require method described in any one in 1 to 28 to perform claim.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/395,278 US10460505B2 (en) | 2016-12-30 | 2016-12-30 | Systems and methods for lightfield reconstruction utilizing contribution regions |
US15/395,278 | 2016-12-30 | ||
PCT/US2017/059058 WO2018125374A1 (en) | 2016-12-30 | 2017-10-30 | Systems and methods for lightfield reconstruction utilizing contribution regions |
Publications (2)
Publication Number | Publication Date |
---|---|
CN110024002A true CN110024002A (en) | 2019-07-16 |
CN110024002B CN110024002B (en) | 2021-05-04 |
Family
ID=61028159
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201780074450.7A Active CN110024002B (en) | 2016-12-30 | 2017-10-30 | Method and system for rendering views from light field |
Country Status (4)
Country | Link |
---|---|
US (1) | US10460505B2 (en) |
EP (1) | EP3563351A1 (en) |
CN (1) | CN110024002B (en) |
WO (1) | WO2018125374A1 (en) |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN111696190A (en) * | 2019-07-17 | 2020-09-22 | 谷歌有限责任公司 | Lighting effects from luminous inserted content |
CN114567767A (en) * | 2022-02-23 | 2022-05-31 | 京东方科技集团股份有限公司 | Display device, light field acquisition method, image data transmission method and related equipment |
CN117422809A (en) * | 2023-12-19 | 2024-01-19 | 浙江优众新材料科技有限公司 | Data processing method for rendering light field image |
Families Citing this family (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10325414B2 (en) * | 2017-05-08 | 2019-06-18 | Microsoft Technology Licensing, Llc | Application of edge effects to 3D virtual objects |
US10360704B2 (en) * | 2017-08-11 | 2019-07-23 | Microsoft Technology Licensing, Llc | Techniques for providing dynamic multi-layer rendering in graphics processing |
US11360470B2 (en) | 2018-10-05 | 2022-06-14 | Thomas A. Youmans | Apparatus for detecting tilt, lean, movement, rotation, of a user, rider, payload |
US11308682B2 (en) * | 2019-10-28 | 2022-04-19 | Apical Limited | Dynamic stereoscopic rendering method and processor |
US11893668B2 (en) | 2021-03-31 | 2024-02-06 | Leica Camera Ag | Imaging system and method for generating a final digital image via applying a profile to image information |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1328673A (en) * | 1998-09-29 | 2001-12-26 | 伦敦大学学院 | Energy propagation modelling apparatus |
JP2004258775A (en) * | 2003-02-24 | 2004-09-16 | Canon Inc | Light beam space data processing method and space data processor |
US20120229679A1 (en) * | 2008-01-23 | 2012-09-13 | Georgiev Todor G | Methods and Apparatus for Full-Resolution Light-Field Capture and Rendering |
CN102959945A (en) * | 2010-06-29 | 2013-03-06 | 皇家飞利浦电子股份有限公司 | Method and system for producing a virtual output image from data obtained by an array of image capturing devices |
CN104156916A (en) * | 2014-07-31 | 2014-11-19 | 北京航空航天大学 | Light field projection method used for scene illumination recovery |
Family Cites Families (15)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6009188A (en) | 1996-02-16 | 1999-12-28 | Microsoft Corporation | Method and system for digital plenoptic imaging |
US6097394A (en) * | 1997-04-28 | 2000-08-01 | Board Of Trustees, Leland Stanford, Jr. University | Method and system for light field rendering |
US6476805B1 (en) | 1999-12-23 | 2002-11-05 | Microsoft Corporation | Techniques for spatial displacement estimation and multi-resolution operations on light fields |
US8125508B2 (en) * | 2006-01-24 | 2012-02-28 | Lifesize Communications, Inc. | Sharing participant information in a videoconference |
EP2070328B1 (en) | 2006-10-05 | 2011-02-16 | Vestel Elektronik Sanayi ve Ticaret A.S. | Watermark detection method for broadcasting |
EP2163099A2 (en) * | 2007-05-30 | 2010-03-17 | Nxp B.V. | Method of determining an image distribution for a light field data structure |
US7962033B2 (en) * | 2008-01-23 | 2011-06-14 | Adobe Systems Incorporated | Methods and apparatus for full-resolution light-field capture and rendering |
US8155456B2 (en) * | 2008-04-29 | 2012-04-10 | Adobe Systems Incorporated | Method and apparatus for block-based compression of light-field images |
EP4336447A1 (en) * | 2008-05-20 | 2024-03-13 | FotoNation Limited | Capturing and processing of images using monolithic camera array with heterogeneous imagers |
CN104081414B (en) * | 2011-09-28 | 2017-08-01 | Fotonation开曼有限公司 | System and method for coding and decoding light field image file |
EP2817955B1 (en) * | 2012-02-21 | 2018-04-11 | FotoNation Cayman Limited | Systems and methods for the manipulation of captured light field image data |
US9462164B2 (en) * | 2013-02-21 | 2016-10-04 | Pelican Imaging Corporation | Systems and methods for generating compressed light field representation data using captured light fields, array geometry, and parallax information |
US9519972B2 (en) * | 2013-03-13 | 2016-12-13 | Kip Peli P1 Lp | Systems and methods for synthesizing images from image data captured by an array camera using restricted depth of field depth maps in which depth estimation precision varies |
US9854176B2 (en) * | 2014-01-24 | 2017-12-26 | Lucasfilm Entertainment Company Ltd. | Dynamic lighting capture and reconstruction |
US9936187B2 (en) * | 2016-05-18 | 2018-04-03 | Siemens Healthcare Gmbh | Multi-resolution lightfield rendering using image pyramids |
-
2016
- 2016-12-30 US US15/395,278 patent/US10460505B2/en not_active Expired - Fee Related
-
2017
- 2017-10-30 CN CN201780074450.7A patent/CN110024002B/en active Active
- 2017-10-30 EP EP17835736.4A patent/EP3563351A1/en active Pending
- 2017-10-30 WO PCT/US2017/059058 patent/WO2018125374A1/en active Search and Examination
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1328673A (en) * | 1998-09-29 | 2001-12-26 | 伦敦大学学院 | Energy propagation modelling apparatus |
JP2004258775A (en) * | 2003-02-24 | 2004-09-16 | Canon Inc | Light beam space data processing method and space data processor |
US20120229679A1 (en) * | 2008-01-23 | 2012-09-13 | Georgiev Todor G | Methods and Apparatus for Full-Resolution Light-Field Capture and Rendering |
CN102959945A (en) * | 2010-06-29 | 2013-03-06 | 皇家飞利浦电子股份有限公司 | Method and system for producing a virtual output image from data obtained by an array of image capturing devices |
CN104156916A (en) * | 2014-07-31 | 2014-11-19 | 北京航空航天大学 | Light field projection method used for scene illumination recovery |
Cited By (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN111696190A (en) * | 2019-07-17 | 2020-09-22 | 谷歌有限责任公司 | Lighting effects from luminous inserted content |
CN114567767A (en) * | 2022-02-23 | 2022-05-31 | 京东方科技集团股份有限公司 | Display device, light field acquisition method, image data transmission method and related equipment |
CN117422809A (en) * | 2023-12-19 | 2024-01-19 | 浙江优众新材料科技有限公司 | Data processing method for rendering light field image |
CN117422809B (en) * | 2023-12-19 | 2024-03-19 | 浙江优众新材料科技有限公司 | Data processing method for rendering light field image |
Also Published As
Publication number | Publication date |
---|---|
EP3563351A1 (en) | 2019-11-06 |
US10460505B2 (en) | 2019-10-29 |
US20180190006A1 (en) | 2018-07-05 |
CN110024002B (en) | 2021-05-04 |
WO2018125374A1 (en) | 2018-07-05 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN110024002A (en) | For using contribution region to carry out the system and method for reconstruction of optical wave field | |
US10839591B2 (en) | Stereoscopic rendering using raymarching and a virtual view broadcaster for such rendering | |
US10127722B2 (en) | Mobile capture visualization incorporating three-dimensional and two-dimensional imagery | |
KR102502794B1 (en) | Methods and systems for customizing virtual reality data | |
Lin et al. | Deep multi depth panoramas for view synthesis | |
CN108693970B (en) | Method and apparatus for adapting video images of a wearable device | |
JP7191079B2 (en) | Apparatus and method for generating a tiled three-dimensional image representation of a scene | |
US10957063B2 (en) | Dynamically modifying virtual and augmented reality content to reduce depth conflict between user interface elements and video content | |
Bonatto et al. | Real-time depth video-based rendering for 6-DoF HMD navigation and light field displays | |
CN113170213A (en) | Image synthesis | |
US20230283759A1 (en) | System and method for presenting three-dimensional content | |
US11706395B2 (en) | Apparatus and method for selecting camera providing input images to synthesize virtual view images | |
US11710256B2 (en) | Free-viewpoint method and system | |
JP7479386B2 (en) | An image signal representing a scene | |
KR20200131817A (en) | Methods and devices to facilitate 3D object visualization and manipulation for multiple devices | |
WO2022046240A1 (en) | Freeview video coding | |
EP3564905A1 (en) | Conversion of a volumetric object in a 3d scene into a simpler representation model | |
JP7462668B2 (en) | An image signal representing a scene | |
RU2817803C2 (en) | Image signal representing scene | |
RU2771957C2 (en) | Device and method for generating mosaic representation of three-dimensional scene image | |
Thatte | Cinematic virtual reality with head-motion parallax | |
CN117635454A (en) | Multi-source light field fusion rendering method, device and storage medium | |
Artois et al. | OpenDIBR: Open Real-Time Depth-Image-Based renderer of light field videos for VR | |
CN116918321A (en) | Parallax occlusion rendering with reduced movement latency | |
CN109685882A (en) | Using light field as better background in rendering |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |