US20120316862A1 - Augmenting statistical machine translation with linguistic knowledge - Google Patents
Augmenting statistical machine translation with linguistic knowledge Download PDFInfo
- Publication number
- US20120316862A1 US20120316862A1 US13/493,475 US201213493475A US2012316862A1 US 20120316862 A1 US20120316862 A1 US 20120316862A1 US 201213493475 A US201213493475 A US 201213493475A US 2012316862 A1 US2012316862 A1 US 2012316862A1
- Authority
- US
- United States
- Prior art keywords
- phrase
- language
- features
- phrases
- word
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Abandoned
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/42—Data-driven translation
- G06F40/44—Statistical methods, e.g. probability models
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/268—Morphological analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/53—Processing of non-Latin text
Definitions
- Statistical machine translation generally utilizes statistical models to provide a translation from a source language to a target language.
- One type of SMT is phrase-based statistical machine translation.
- Phrase-based SMT can map sets of words (phrases) from a source language to a target language.
- Phrase-based SMT may rely on lexical information, e.g., the surface form of the words.
- the source language and the target language may have significant lexical differences, such as when one of the languages is morphologically-rich.
- a computer-implemented technique can include receiving, at a computing system including one or more processors, a translation model including a plurality of pairs of phrases, each of the plurality of pairs of phrases including a first phrase of one or more words in a first language and a second phrase of one or more words in a second language, wherein a specific first phrase is aligned with a specific second phrase for a specific pair of phrases.
- the technique can include determining, at the computing system, one or more features for each of the plurality of pairs of phrases based on linguistic differences between the first and second languages to obtain a plurality of sets of features.
- the technique can include associating, at the computing system, the plurality of sets of features with the plurality of pairs of phrases, respectively, to obtain a modified translation model.
- the technique can also include performing, at the computing system, statistical machine translation from the first language to the second language using the modified translation model.
- the modified translation model has lexical and inflectional agreement for each of the plurality of pairs of phrases.
- one of the first and second languages is a morphologically-rich language.
- the morphologically-rich language is characterized by morphological processes that produce a large number of word forms for a given root word.
- the morphologically-rich language is a synthetic language.
- one of the first and second languages is a non-morphologically-rich language.
- the non-morphologically-rich language is an isolating language or an analytic language.
- the one or more features include at least one of parts of speech features and dependency features.
- the parts of speech features include at least one of personal pronouns, possessive pronouns, determiners, particles, and wh-nouns.
- the dependency features include syntactic divergences including at least one of word order, verb-less phrases, and possessiveness.
- the dependency features include lexical divergences including at least one of idiomatic expressions, prepositions, named entities, ambiguity, and alignment difficulties.
- performing the statistical machine translation using the modified translation model further includes: receiving, at the computing system, one or more words in the first language; generating, at the computing system, one or more potential translations of the one or more words using the modified translation model, the one or more potential translations having one or more probability scores, respectively; selecting, at the computing system, one of the one or more potential translations based on the one or more probability scores to obtain a selected translation; and outputting, at the computing system, the selected translation.
- the technique can include receiving, at a computing system including one or more processors, a translation model configured for translation between a first language and a second language.
- the technique can include receiving, at the computing system, a plurality of pairs of phrases, each of the plurality of pairs of phrases including a first phrase of one or more words in the first language and a second phrase of one or more words in the second language, wherein a specific first phrase is aligned with a specific second phrase for a specific pair of phrases.
- the technique can include receiving, at the computing system, a source phrase for translation from the first language to the second language.
- the technique can include determining, at the computing system, a translated phrase based on the source phrase using the translation model.
- the technique can include determining, at the computing system, a selected second phrase from the plurality of pairs of phrases based on the translated phrase.
- the technique can include predicting, at the computing system, one or more features for each word of the translated phrase based on the selected second phrase, a selected first phrase associated with the selected second phrase, and linguistic differences between the first and second languages.
- the technique can include modifying, at the computing system, the translated phrase based on the one or more features to obtain a modified translated phrase.
- the technique can also include outputting, from the computing system, the modified translated phrase.
- the translated phrase has lexical and inflectional agreement with the source phrase.
- the one or more features include at least one of lemma, parts of speech features, morphological features, and dependency features.
- predicting the one or more features for each word in the translated phrase further includes determining, at the computing system, at least one of the lemma, the part of speech features, and the morphological features for each word in the selected first phrase and for each word in the selected second phrase.
- predicting the one or more features for each word in the translated phrase further includes projecting, at the computing system, dependency relations from the selected first phrase to the selected second phrase based on an alignment between the selected first phrase and the selected second phrase and the part of speech features of both the selected first phrase and the selected second phrase.
- the parts of speech features include at least one of personal pronouns, possessive pronouns, determiners, particles, and wh-nouns.
- the dependency features include syntactic divergences including at least one of word order, verb-less phrases, and possessiveness.
- the dependency features include lexical divergences including at least one of idiomatic expressions, prepositions, named entities, ambiguity, and alignment difficulties.
- one of the first and second languages is a morphologically-rich language.
- the morphologically-rich language is characterized by morphological processes that produce a large number of word forms for a given root word.
- the morphologically-rich language is a synthetic language.
- one of the first and second languages is a non-morphologically-rich language.
- the non-morphologically-rich language is an isolating language or an analytic language.
- a system is also presented.
- the system can include one or more computing devices configured to perform operations.
- the operations can include receiving a translation model including a plurality of pairs of phrases, each of the plurality of pairs of phrases including a first phrase of one or more words in a first language and a second phrase of one or more words in a second language, wherein a specific first phrase is aligned with a specific second phrase for a specific pair of phrases.
- the operations can include determining one or more features for each of the plurality of pairs of phrases based on linguistic differences between the first and second languages to obtain a plurality of sets of features.
- the operations can include associating the plurality of sets of features with the plurality of pairs of phrases, respectively, to obtain a modified translation model.
- the operations can also include performing statistical machine translation from the first language to the second language using the modified translation model.
- the modified translation model has lexical and inflectional agreement for each of the plurality of pairs of phrases.
- one of the first and second languages is a morphologically-rich language.
- the morphologically-rich language is characterized by morphological processes that produce a large number of word forms for a given root word.
- the morphologically-rich language is a synthetic language.
- one of the first and second languages is a non-morphologically-rich language.
- the non-morphologically-rich language is an isolating language or an analytic language.
- the one or more features include at least one of parts of speech features and dependency features.
- the parts of speech features include at least one of personal pronouns, possessive pronouns, determiners, particles, and wh-nouns.
- the dependency features include syntactic divergences including at least one of word order, verb-less phrases, and possessiveness.
- the dependency features include lexical divergences including at least one of idiomatic expressions, prepositions, named entities, ambiguity, and alignment difficulties.
- the operation of performing the statistical machine translation using the modified translation model further includes: receiving one or more words in the first language; generating one or more potential translations of the one or more words using the modified translation model, the one or more potential translations having one or more probability scores, respectively; selecting one of the one or more potential translations based on the one or more probability scores to obtain a selected translation; and outputting the selected translation.
- the system can include one or more computing devices configured to perform operations.
- the operations can include receiving a translation model configured for translation between a first language and a second language.
- the operations can include receiving a plurality of pairs of phrases, each of the plurality of pairs of phrases including a first phrase of one or more words in the first language and a second phrase of one or more words in the second language, wherein a specific first phrase is aligned with a specific second phrase for a specific pair of phrases.
- the operations can include receiving a source phrase for translation from the first language to the second language.
- the operations can include determining a translated phrase based on the source phrase using the translation model.
- the operations can include determining a selected second phrase from the plurality of pairs of phrases based on the translated phrase.
- the operations can include predicting one or more features for each word of the translated phrase based on the selected second phrase, a selected first phrase associated with the selected second phrase, and linguistic differences between the first and second languages.
- the operations can include modifying the translated phrase based on the one or more features to obtain a modified translated phrase.
- the operations can also include outputting the modified translated phrase.
- the translated phrase has lexical and inflectional agreement with the source phrase.
- the one or more features include at least one of lemma, parts of speech features, morphological features, and dependency features.
- the operation of predicting the one or more features for each word in the translated phrase further includes determining at least one of the lemma, the part of speech features, and the morphological features for each word in the selected first phrase and for each word in the selected second phrase.
- the operation of predicting the one or more features for each word in the translated phrase further includes projecting dependency relations from the selected first phrase to the selected second phrase based on an alignment between the selected first phrase and the selected second phrase and the part of speech features of both the selected first phrase and the selected second phrase.
- the parts of speech features include at least one of personal pronouns, possessive pronouns, determiners, particles, and wh-nouns.
- the dependency features include syntactic divergences including at least one of word order, verb-less phrases, and possessiveness.
- the dependency features include lexical divergences including at least one of idiomatic expressions, prepositions, named entities, ambiguity, and alignment difficulties.
- one of the first and second languages is a morphologically-rich language.
- the morphologically-rich language is characterized by morphological processes that produce a large number of word forms for a given root word.
- the morphologically-rich language is a synthetic language.
- one of the first and second languages is a non-morphologically-rich language.
- the non-morphologically-rich language is an isolating language or an analytic language.
- FIG. 1 is a tree illustrating example relationships between various Arabic pronouns
- FIG. 2A illustrates an example of correct and incorrect translations of an attached Arabic preposition
- FIG. 2B illustrates an example of correct and incorrect translations of separate Arabic prepositions
- FIG. 3 illustrates an example system for executing techniques according to some implementations of the present disclosure
- FIG. 4 illustrates an example dependency tree projection according to some implementations of the present disclosure
- FIG. 5A illustrates an example of extracting an Arabic predicate-subject relation from an English syntactic dependency parse tree according to some implementations of the present disclosure
- FIG. 5B illustrates an example of extracting an Arabic relative words relation from an English syntactic dependency parse tree according to some implementations of the present disclosure
- FIG. 6 is a flow diagram of an example technique for generating a modified language model according to some implementations of the present disclosure.
- FIG. 7 is a flow diagram of an example technique for post-processing of a translated phrase according to some implementations of the present disclosure.
- SMT Statistical Machine Translation
- a morphologically-rich language can be characterized by morphological processes that produce a large number of word forms for a given root word.
- a morphologically-rich language may be a synthetic language.
- a non-morphologically-rich language may be an isolating language or an analytic language.
- the greater the lexical and syntactic divergences between the two languages the more the need for incorporating linguistic information in the translation process increases.
- Arabic is a polysynthetic language where every word has many attachments
- segmentation of Arabic words is expected to improve translation from or to English. which is an isolating language (i.e., each unit of meaning is represented by a separate word). Segmentation also helps the sparsity problem of morphologically rich languages. Segmentation has considered Arabic-English translation because this direction does not require post-processing to connect the Arabic segments into whole words. Additionally, segmentation of Arabic can help achieve better translation quality.
- Tokenization and normalization of Arabic data may be utilized to improve SMT.
- Post processing may be needed to reconnect the morphemes into valid Arabic words, e.g., to de-tokenize and de-normalize the previously tokenized and normalized surface forms.
- Training SMT models on Arabic lemmas instead of surface forms may increase translation quality.
- reordering of the source sentence as preprocessing can be used. Because word order in Arabic is different from English, reordering may help alignment as well as the order of words in the target sentence.
- Another reordering approach focuses on verb-subject constructions for Arabic-to-English SMT. This approach may differentiate between main clauses and subordinate clauses and applied different rules for each case. Reordering based on automatic learning can have an advantage of being language independent. Some techniques try to decrease the divergences between the two languages through preprocessing and post-processing to make the two languages more similar. Other techniques incorporate linguistic information in the translation and language models. One technique uses lexical, shallow syntactic, syntactic and positional context features. Adding context features may help identify disambiguation as well as other specification problems such as choosing whether a noun should be accusative, dative or genitive in German. The features can be added to a log-linear translation model, Minimum Error Rate Training (MERT) can then be performed to estimate the mixture coefficients.
- MTT Minimum Error Rate Training
- Source-side contextual features have been considered in which grammatical dependency relations are incorporated in Phrase-Based SMT (PB-SMT) as a number of features.
- Other efforts toward improving target phrase selection can include applying source-similarity features in PB-SMT.
- the source sentences are maintained along with the phrase pairs in the phrase table. In translating a source sentence, similarity between the source sentence and the sentences from which the phrase pair were extracted was considered as a feature in the log-linear translation model.
- JMLLM Joint morphological-lexical language model
- Predicting the correct inflection of words in morphologically rich languages has been performed on Russian and Arabic SMT outputs including applying a Maximum Entropy Markov Model (k-MEMM).
- k-MEMM Maximum Entropy Markov Model
- a Maximum Entropy Markov Model is a structured prediction model where the current prediction is conditioned on the previous k predictions.
- Integrating other sources of linguistic information, such as morphological or syntactic information, in the translation process can provide improvements especially for translation between language pairs with large topological divergences.
- English-Arabic is an example of these language pairs.
- Arabic-English translation is also difficult, it does not require the generation of rich morphology at the target side. Translation from English to Arabic is one focus of this specification; however, the used techniques are also useful for translation into other morphologically rich languages.
- One goal of the techniques described in this specification is to solve some of the problems resulting from the large gap between English and Arabic.
- This specification introduces two techniques for improving English-Arabic translation through incorporating morphology and syntax in SMT.
- the first part applies changes to the statistical translation model while the second part is post-processing.
- adding syntactic features to the phrase table is described.
- the syntactic features are based on the English syntactic information and include part-of-speech (POS) and dependency features.
- POS features are suggested to penalize phrases that include English words that do not correspond to Arabic words. These phrases are sources of error because they usually translate to Arabic words with different meaning or even with different POS tags.
- An example is a phrase containing the English word “the” which should map in Arabic to the noun prefix “Al” and never appear as a separate word.
- the choice of POS features can depend on the used Arabic segmentation.
- Dependency features are features that rely on the syntactic dependency parse tree of the sentences from which a certain phrase was extracted. These features are suggested because they can solve a number of error categories, the main two of which are lexical agreement and inflectional morphological agreement.
- An example of lexical agreement is phrasal verbs where a verb takes a specific preposition to convey a specific meaning. When the verb and the preposition are in separate phrases, they are less likely to translate correctly. However, selecting a phrase containing both words in the same phrase may increase the likelihood of their lexical agreement.
- Inflectional agreement is a syntactic-morphological feature of the Arabic language. Some words should have morphological agreement with other words in the sentence, e.g., an adjective should morphologically agree with the noun it modifies in gender, number, etc. Morphological agreement also applies to other related words such as verbs and their subjects, words connected by conjunction and others. To increase the likelihood of correct inflectional agreement of two syntactically related words, a phrase containing both words should be selected by the decoder. This increases the likelihood of their agreement since phrases are extracted from morphologically correct training sentences. The weights of the added features are then evaluated automatically using the Minimum Error Rate Training (MERT) algorithm. The results show an improvement in the automatic evaluation score BLEU over the baseline system.
- MTT Minimum Error Rate Training
- the second part of the specification introduces a post-processing framework for fixing inflectional agreement in MT output.
- the present specification focuses on specific constructions, e.g., morphological agreement between syntactically dependent words.
- the framework is also a probabilistic framework which models each syntactically extracted morphological agreement relation separately.
- the framework predicts each feature such as gender, number, etc. separately instead of predicting the surface forms, which decreases the complexity of the system and allows training with smaller corpora.
- the predicted features along with the lemmas are then passed to the morphology generation module which generates the correct inflections.
- the second part introduces a probabilistic framework for morphological generation incorporating syntactic, morphological and lexical information sources through post-processing.
- dependency features also aim at solving inflectional agreement, it may have limitations that can be overcome by post-processing.
- First, dependency features are added for words which are at small distances in the sentence. This is because phrase based SMT systems may limit the length of phrases. Related words that have distances more than the maximum phrase length are not helped.
- phrases that contain related words could be absent from the phrase table because they were not in the training data or were filtered because they were not frequent enough.
- other features that have more weight than dependency features could lead to selecting other phrases.
- the component that can motivate selecting the correctly inflected words is the N-gram language model.
- N-gram language model For example, 3- or 4-gram language models may be used, which means agreement between close words can be captured.
- the language model can fix the agreement issues where:
- One embodiment of the described subject matter improves the inflectional agreement of the Arabic translation output as proven by the automatic and human evaluations.
- a log linear statistical machine translation system can be used.
- the log linear approach to SMT uses the maximum-entropy framework to search for the best translation into a target language of a source language text given the following decision rule:
- Training the translation model starts by aligning the words of the sentence pairs in the training data using, for example, one of the IBM translation models.
- a phrase extraction algorithm can be used.
- feature functions could be defined at the phrase level.
- the Viterbi alignment is the alignment a 1 J such that:
- a 1 J arg ⁇ ⁇ max a 1 J ⁇ ⁇ Pr ⁇ ( f 1 J , a 1 J
- a j is the index of the word in e 1 l to which f j is aligned.
- Word alignments can be calculated using GIZA++, which uses the IBM models 1 to 5 and the Hidden Markov Model (HMM) alignment model, all of which do not permit a source word to align to multiple target words.
- GIZA++ allows many-to-many alignments by combining the Viterbi alignments of both directions: source-to-target and target-to-source using some heuristics.
- phrase table Given an aligned phrase pair: source-language phrase f and a corresponding target-language phrase ⁇ , the most common phrase features are:
- the log linear model is very easy to extend by adding new features. After adding new features, feature weights need to be calculated using MERT which is described below.
- Equation 5 shows a model using the feature functions discussed above in addition to a feature function representing the language model probability.
- e ⁇ 1 I arg ⁇ ⁇ max e 1 I ⁇ ⁇ ⁇ 1 ⁇ p ⁇ ( e 1 I
- weights ⁇ 1 M can be calculated using, for example, gradient descent to maximize the likelihood of the data according to the following equation:
- MERT Minimum Error Rate Training
- E(r, e) is the result of computing a score based on an automatic evaluation metric, e.g., BLEU and ê(f s ; ⁇ 1 M ) is the best output translation according to equation 1.
- Arabic morphology is complex if compared to English morphology. Similar to English, Arabic morphology has two functions: derivation and inflection, both of which are discussed above. On the other hand, there are two different types of morphology, i.e., two different ways of applying changes to the stem or the base word. These two types are the templatic morphology and the affixational morphology. The functions and types of morphology are discussed in this section. As will be shown, Arabic affixational morphology is the most complex and constitutes the majority of English-Arabic translation problems.
- Derivational Morphology is about creating words from other words (root or stem) while the core meaning is changed.
- An example from English is creating “writer” from “write”.
- generating kAtb “writer” from ktb “to write” is an example of derivational morphology in Arabic.
- Inflectional Morphology is about creating words from other words (root or stem) while the core meaning remains unchanged.
- An example from English is inflecting the verb “write” to “writes” in the case of third person singular.
- Another example is generating the plural of a noun, e.g., “writers” from “writer”.
- An example in Arabic is generating AlbnAt “the girls” from Albnt “the girl”.
- Arabic inflectional morphology is much more complex than English inflectional morphology.
- English nouns are inflected for number (singular/plural) and verbs are inflected for number and tense (singular/plural, present/past/past-participle).
- English Adjectives are not inflected.
- Arabic nouns and adjectives are inflected for gender (feminine/masculine), number (singular/dual/plural), state (definite/indefinite).
- Arabic verbs are inflected also for gender and number besides tense (command/imperfective/perfective), voice (active/passive) and person (1/2/3).
- the root morpheme consists of three, four, or five consonants. Every root has an abstract meaning that's shared by all its derivatives. According to known templates, the root is modified by adding additional vowels and consonants in a specific order to certain positions to generate different words. For example, the word kAtb “writer” is derived from the root ktb “to write” by adding alef “A” between the first and second letters of the root.
- English affixational morphology is simpler because there is no clitics attached to the words.
- Affixational morphology in English is used for both inflection and derivation. Examples include:
- English words do not have attachable clitics like Arabic. Examples I and II illustrate how one Arabic word can correspond to five and four English words respectively.
- Arabic affixational and inflectional morphology are very complex especially compared to English.
- the complex morphology is the main reason behind the problems of sparsity, agreement, and lexical divergences as will be explained in more detail in this chapter.
- Verbs should agree morphologically with their subjects.
- Verbs that follow the subject agree with the subject in number and gender (see example III).
- verbs that precede the subject agree with the subject in gender while having the singular 3rd person inflection (see example IV).
- Example V shows how the adjective “polite” follows the noun “sisters” in being definite, feminine and plural. This is an example where the noun refers to humans and is in the regular feminine plural form.
- Example VI shows how the adjective polite follows the noun in being definite, masculine and plural.
- the noun is in the masculine broken plural form.
- the adjective follows the noun in definiteness; however, the adjective is feminine and singular, while the noun is masculine and plural. This is because the noun is a broken plural representing more than one object (e.g., books).
- a noun is modified by a number, the noun is inflected differently according to the value of the number. For example, if the number is 1, the noun will be singular. If the number is 2, the noun should have dual inflection. For numbers (3-10), the noun should be plural. For numbers (>11), the noun should be singular.
- Sparsity is a result of Arabic complex inflectional morphology and the various attachable clitics.
- the number of Arabic words in a parallel corpus is 20% less than English words
- the number of unique Arabic words can be over 50% more than the number of unique English words.
- VSO Verb-Subject-Object
- SVO Subject-Verb-Object
- the order SVO also occurs in Arabic but is less frequent. Therefore subjects can be pre-verbal or post-verbal. Additionally, subjects can be pro-dropped, e.g., subject pronouns do not need to be expressed because they are inferable from the verb conjugation.
- Example VIII shows a case of a pro-dropped subject. The subject is a masculine third-person pronoun that is dropped because it can be inferred from the verb inflection.
- verb-less sentences are nominal sentences which have no verbs. They usually exhibit the zero copula phenomenon, e.g., the noun and the predicate are joined without overt marking. These sentences are usually mapped to affirmative English sentences containing the copular verb to be in the present form.
- Example X shows an example of a nominal sentence.
- the Arabic equivalent of possessiveness between nouns and of the of-relationship is called ldafa.
- the idafa construct is achieved by having the word indicating the possessed entity precede a definite form of the possessing entity. Refer to example XI for an illustration of this construct.
- Lexical divergences are the differences between the two languages at the lexical level. They result in translation problems, some of which are discussed in this section.
- Expressions are usually incorrectly translated as they are translated as separate words. Mapping each word or a few words to their corresponding meaning in Arabic usually results in a meaningless translation or at least a translation with a meaning that does not correctly match the English expression. Examples XII and XIII illustrate the problem.
- Verbs that take prepositions cause problems in translation. Translating the verb alone to an Arabic verb and the preposition to a corresponding Arabic preposition usually results in errors. The same applies to prepositions needed by nouns. In example XIV, although “meeting” is translated correctly to its corresponding Arabic word, the direct translation of the preposition leads to a wrong phrase.
- Named entities cause a problem in translation. Translating named entities word-by-word results in wrong Arabic output.
- Manual error analysis can be performed, in one example, on a small sample of 30 sentences which were translated using a state-of-the-art phrase-based translation system. Despite the small sample size, most errors described appeared in the output sentences. Morphological, syntactic and lexical divergences contributed to the errors. These divergences make the alignment of sentences from both languages very difficult and consequently result in problems in phrase extraction and mapping. Therefore, errors in the phrase table were very common.
- Phrase table errors could directly lead to errors in the final translation output. They can result, for example, in missing or additional clitics in Arabic words and sometimes extra Arabic words. Besides, it is very common that English verbs map to Arabic nouns in the phrase table, which results in problems in the final grammatical structure of the output sentence. Ambiguity is also a phrase table problem. This is because the phrase table is based on the surface forms not taking context into consideration. Seventeen sentences out of the thirty had errors because of these phrase table problems.
- Morphological agreement is a major problem in the Arabic output.
- the main problems are the agreement of the adjective with the noun and the agreement of the verb with the subject.
- Nine sentences had problems with adjective-noun agreement, while two had problems with verb-subject agreement.
- POS features are added to penalize the incorrectly mapped phrase pairs.
- the English part of these phrase pairs usually does not have a corresponding Arabic translation (see examples above). Therefore, it is usually paired with incorrect Arabic phrases.
- the POS features can be added to discourage these phrase pairs from being selected by the decoder. These features mark phrases that consist of one or more of personal and possessive pronouns, prepositions, determiners, particles and wh-words.
- Example POS features are summarized in Table A. After adding the features, MERT is used to calculate their weights.
- FIG. 1 shows a diagram of the Arabic pronouns. It shows that pronouns are divided into separate and attached. Separate pronouns are the subject pronouns. On the other hand, pronouns could be attached to verbs, prepositions or nouns. Pronouns attached to verbs are either subject or object pronouns. Pronouns can attach also to prepositions or nouns. All tree leaves represent personal pronouns except for the pronouns attached to nouns which are possessive pronouns. The leaf corresponding to possessive pronouns is highlighted. There are two reasons why personal pronouns should be penalized as separate phrases:
- FIG. 1 a tree 100 illustrating example relationships between various Arabic pronouns. As shown, a possessive pronoun 104 is always attached to the end of a noun.
- FIG. 1 there are attached and separate prepositions in Arabic. Prepositions were discussed above as an example of lexical divergences. Translating prepositions separately can be harmful because sometimes they should be attached to Arabic words and sometimes context is needed in order to select the correct preposition.
- FIG. 2A illustrates an example of an attached preposition, which was translated incorrectly.
- FIG. 2B also illustrates an example of separate prepositions, which were also translated incorrectly because they were translated as separate phrases.
- the determiner class (DT) in English includes, in addition to other words, the definite and indefinite articles “the” and “a” or an”, respectively.
- the definite article corresponds to an “Al” attached as a prefix to the noun.
- Table C shows their entries in the phrase table. As shown, they correspond to prepositions, which is very harmful to the adequacy and fluency of the output sentence.
- Wh-nouns include wh-determiners, wh-pronouns, and possessive wh-pronouns having the POS tags WDT, WP, and WRB, respectively.
- POS tags can be added to discourage selecting separate phrases which are limited to wh-nouns. The motivation for this is mainly gender and number agreement. When they are attached in one phrase with the word they refer to, they would probably be translated in the correct form.
- interesting morphological agreement relations include for example, noun-adjective and verb-subject relations.
- Lexical agreement relations include, for example, relations between phrasal verbs and their prepositions. For example, “talk about” is correct while “talk on” is not. Selecting a phrase where “talk” and its preposition “about” are attached guarantees their agreement.
- nsubj is also specifically useful if the subject is a pronoun, in which case it will be most of the time omitted in the Arabic and help in generating the correct verb inflection.
- Adding det is beneficial in two ways. First, to discourage selecting a phrase with a separate “the” which would result in a wrong Arabic translation as shown in Table C. Second, attaching the determiner to its noun causes the Arabic word to have the correct form whether to have “Al” or not if the determiner in English is “the” or “a”, respectively.
- the post-processor is based on a learning framework. Given a training corpus of aligned sentence pairs, the system is trained to predict inflections of words in MT output sentences.
- the system uses a number of multi-class classifiers, one for each feature. For example, there is a separate classifier for gender and a separate classifier for number.
- FIG. 3 illustrates an example of the system pipeline 300 .
- the system 300 and the techniques of the present disclosure can be implemented by one or more computing devices, each including one or more processors.
- the computing device(s) can operate in a parallel or distributed architecture, and the processor(s) of a specific computing device can also operate in a parallel or distributed architecture.
- a morphology analyzer 312 analyzes the Arabic sentences. It specifies the lemma, the part-of-speech (POS) tag and the morphological features (e.g., gender, number, person) for every word in the sentence.
- a syntax projector 316 projects the dependency relations from the English sentence to the Arabic sentence using the alignments and the POS tags of both sentences. Subsequently, it extracts the agreement relations using the projected syntax.
- a feature vector extractor 320 is responsible for extracting the feature vectors out of the available lexical, morphological and syntactic information. The feature vectors as well as the correct labels which are extracted from the reference data are then used to train the classifiers by a classifier trainer 324 .
- the MT translation output as well as the source sentence and the alignments are required. This can be referred to as a classification phase 328 .
- Data from a machine translation aligned input/output datastore 332 goes through the same steps as in the training phase 304 .
- the extracted feature vectors are then used to make predictions for each feature separately using a classifier 336 .
- a morphology generator 340 After prediction/classification by the classifier 336 , the correct features are then used along with the lemmas of the words to generate the correct inflected word by a morphology generator 340 . Output of the morphology generator 340 can be stored in a first post-processed output datastore 344 . Finally, an LM filter 348 uses an N-gram language model to add some robustness to the system 300 against the errors of alignment, morphology analysis and generation and classification. Output of the LM filter 348 can be stored in a second post-processed output datastore 352 . If the generated sentence has a lower LM score than the baseline sentence, the baseline sentence is not updated.
- a tree-based structured probabilistic model such as k-MEMM or CRF that use the dependency tree is theoretically very effective.
- dependency trees for Arabic sentences may be of poor quality and would result in a very noisy model that might degrade the MT output quality.
- Predicting the inflection of each word according to its agreement relation separately can be very effective.
- the relations are independent, for example, fixing the inflection of the adjective in an adjective-noun agreement relation is independent from fixing the inflection of the verb in a verb-subject agreement. Therefore, separating prediction adds robustness to the system and allows training with smaller corpora.
- MADA Morphological Analysis and Disambiguation for Arabic
- the system is built on the Buckwalter analyzer which generates multiple analyses for every word.
- MADA uses another analyzer and generator tool ALMORGEANA to convert the output of Buckwalter from a stem-affix format to a lexeme-and-feature format.
- the analysis output includes the diacriticized form (diac), the lexeme/lemma (lex), the Buckwalter tag .(bw) and the English gloss (gloss).
- the lexeme, POS tag and all other known features from Table E are input to the MORGEANA tool.
- the system searches in the lexicon for the word which has the most similar analysis.
- the analysis and generation tools can be used to change the declension of a word. For example, to change a word w from the feminine to the masculine form, the following steps are taken:
- Arabic dependency tree parsers exist, for example the Berkeley and Stanford parsers, they can have poor quality. Parallel aligned data can instead be used to project the syntax tree from English to Arabic.
- the English parse tree is a labeled dependency tree following the grammatical representations described above. Two approaches to projection can be considered.
- a dependency relation is a function h s such that for any s i , h s (i) is the head of s i and l s (i) is the label of the relation between s i and h s (i).
- A is a set of pairs (s i , t j ) such that s i is aligned to t j .
- h t (j) is the head of t j
- l t (j) is the label of the relation between t j and h l (j). Similar to the unlabeled tree projection, projection can be performed according to the following rule:
- FIG. 4 an example of the direct projection approach is illustrated.
- the alignment between source and target sentences is one-to-one, therefore, there is no ambiguity problem.
- An English parse tree is illustrated at 400 for the following example sentence: “Swan in Fife, Scotland dies with H5N1 bird flu virus infection.”
- errors by the English parser 400 were projected to an Arabic parse tree 450 .
- the resulting translation after projection to the Arabic parse tree 450 was the following incorrect translation: “H5N1 birds flu virus infection from dies Scotland, Fife in swans.”
- an Arabic agreement relation is extracted if the English adjective aligns to an Arabic adjective, while the English noun aligns to an Arabic noun.
- selecting the noun for the amod relation is based on the heuristic that the first noun after a preposition is marked as the noun of the relation.
- the motivation behind this rule is illustrated by example XV. If the first word of the multiple word alignment was selected as the noun of the relation, a link amod(AlfwtwgrAfy, mwad) would be extracted although amod(AlfwtogrAfy, IltSwyr) is the correct link. Therefore, linguistic analysis of erroneous agreement links lead to the mentioned rule.
- Example XVI illustrates the ambiguity problem in a case of one-to-many alignment from English to Arabic.
- the word airline aligns to two Arabic words.
- the first word can be selected as the word being described by the adjective. However, in some cases, this rule introduces error.
- FIG. 5A illustrates how the predicate and the subject of a verb-less Arabic sentence 500 can be extracted from an English syntactic dependency parse tree 520 indirectly.
- the predicate ITyf has an agreement relation with the subject of the sentence Alwld.
- the agreement link of interest is the link from the verb to the subject, which is the reverse of the link in the English dependency parse tree.
- FIG. 5B illustrates an example projection of the ref relation from an English dependency parse tree 550 to an Arabic sentence 570 .
- the feature vector extractor 320 as shown in the framework diagram in FIG. 3 follows the morphology analysis and syntax projection.
- the selected features are from many sources of information: lexical, morphological and syntactic.
- Table F summarizes the features used in the feature vectors used by the classifiers.
- morphological features include the features returned by the morphology analyzer 312 of FIG. 3 .
- an analysis of classification errors revealed regular errors caused by the morphology analyzer 312 , for which two extra features were added: feminine ending and number of English gloss.
- the morphology analyzer 312 does not make use of these endings since it does not analyze the surface forms of the words; however, it uses prefix, suffix and stem lexicons to generate the features. Incorrect or missing labels to these words in the used lexicons would result in these errors.
- the number of the English gloss is also added to overcome the persistent error of the analyzer to analyze broken plurals as singular. The reason is because it actually identifies a plural by whether the stem is attached to a clitic for plural marking. However, in the case of broken plurals, there is no affix added to the stem, however, it is derived from the singular form, a case of derivational morphology. As a solution to this problem, a feature is added to indicate whether any of the English glosses of the word is plural or not.
- Syntactic features for Arabic include part of speech tags of the current and head words.
- Lexical features include the stem of the head word and the English gloss.
- English features include the part of speech tags and the surface forms of the aligned and head words.
- general features include the dependency relation type and whether the head comes before or after the current word in the sentence. The latter feature is useful for example in the case of verbs where the verb inflection rules are different for the SVO order versus the VSO order.
- the extracted feature vectors as well as the correct labels are needed.
- the data used for training is a set of parallel aligned sentences.
- the agreement relations are extracted as described above. For every relation pair, prediction is done for one word given the inflection of the parent and other bilingual and lexical information that are encoded in the feature vectors.
- the result of the morphological analyzer 312 for a specific feature is then used as the label for this feature classifier. Erroneous labels result in noisy training data and in imprecise classification accuracy evaluation.
- an automatic tool can be used for selecting the best classification model for each feature and also for selecting the best parameters for this model using cross-validation.
- the reported accuracy is actually the mean accuracy of the folds of the cross validation.
- N-Gram Language model is a probabilistic model specifically a model which predicts the next word in a sentence given the previous N words based on the Markov assumption.
- the N-Gram language model probability of a sentence is approximated as a multiplication of N-Gram probabilities as shown. in the second part of equation 9.
- the language model probability can be used as an indicator to the correctness and fluency of a modification.
- a comparison can be made between P(output sentence) and P(post-processed sentence). If the post-processed sentence has much less probability (e.g., less by a difference more than a certain threshold) than the output translation, changes to the sentence are canceled.
- Change filtering using a language model is expected to provide some robustness against all sources of errors in the system.
- the language model is not very reliable. A simple example would be that the generated inflected word is out of vocabulary (OOV) for the language model, although it is morphologically the correct one.
- OOV out of vocabulary
- BLEU score is used. The BLEU score of the output is compared to BLEU score of the baseline MT system output. Because of the BLEU score limitations in evaluating morphological agreement, human evaluation is also used.
- the human raters are provided with the source sentence and two Arabic sentence outputs, one is the output of baseline system and the other is the post-processed sentence.
- the sentences are shuffles; therefore, the raters score the sentences without knowing their sources. They give a rating between 0 and 6 according to meaning and grammar. 6 is the best rating for perfect meaning and grammar, while 0 is the lowest rating for cases when there is no meaning preserved and thus the grammar is irrelevant. Ratings from 5 to 3 are for sentences whose meaning is preserved but with increasing grammar mistakes. Ratings below 3 are for sentences that have no meaning, in which case the grammar becomes irrelevant and has minimal effect on the quality score.
- the human evaluation results are not expected to directly reflect whether the inflectional agreement, which is a grammatical feature, is fixed or not in the sentences.
- having the correct inflectional agreement should correspond to in-creasing the sentence score.
- sentences with no preserved meanings are not expected to receive higher scores for correct morphological agreement.
- the technique 600 can receive, e.g., at a computing system including one or more processors, a translation model including a plurality of pairs of phrases.
- Each of the plurality of pairs of phrases can include a first phrase of one or more words in a first language and a second phrase of one or more words in a second language.
- a specific first phrase can be aligned with a specific second phrase for a specific pair of phrases.
- the technique 600 can determine, e.g., at the computing system, one or more features for each of the plurality of pairs of phrases based on linguistic differences between the first and second languages to obtain a plurality of sets of features.
- the technique 600 can associate, e.g., at the computing system, the plurality of sets of features with the plurality of pairs of phrases, respectively, to obtain a modified translation model.
- the technique 600 can, e.g., at the computing system, perform statistical machine translation from the first language to the second language using the modified translation model. The technique 600 can then end or return to 604 for one or more additional cycles.
- the technique 700 can, e.g., at a computing system including one or more processors, receive a translation model configured for translation between a first language and a second language.
- the technique 700 can, e.g., at the computing system, receive a plurality of pairs of phrases. Each of the plurality of pairs of phrases can include a first phrase of one or more words in the first language and a second phrase of one or more words in the second language. A specific first phrase can be aligned with a specific second phrase for a specific pair of phrases.
- the technique 700 can, e.g., at the computing system, receive a source phrase for translation from the first language to the second language.
- the technique 700 can, e.g., at the computing system, determine a translated phrase based on the source phrase using the translation model.
- the technique 700 can, e.g., at the computing system, determine a selected second phrase from the plurality of pairs of phrases based on the translated phrase.
- the technique 700 can, e.g., at the computing system, predict one or more features for each word of the translated phrase based on the selected second phrase, a selected first phrase associated with the selected second phrase, and linguistic differences between the first and second languages.
- the technique 700 can, e.g., at the computing system, modify the translated phrase based on the one or more features to obtain a modified translated phrase.
- the technique 700 can, e.g., at the computing system, output the modified translated phrase. The technique 700 can then end or return to 704 for one or more additional cycles.
- Embodiments of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on computer storage medium for execution by, or to control the operation of, data processing apparatus.
- the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- a computer storage medium can be, or be included in, a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them.
- a computer storage medium is not a propagated signal, a computer storage medium can be a source or destination of computer program instructions encoded in an artificially generated propagated signal.
- the computer storage medium can also be, or be included in, one or more separate physical components or media (e.g., multiple CDs, disks, or other storage devices).
- the operations described in this specification can be implemented as operations performed by a data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
- the term “data processing apparatus” encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or multiple ones, or combinations, of the foregoing
- the apparatus can include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- the apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them.
- the apparatus, and execution environment can realize various different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures.
- a computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a standalone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment.
- a computer program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code).
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output.
- the processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing actions in accordance with instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a universal serial bus (USB) flash drive), to name just a few.
- Devices suitable for storing computer program instructions and data include all forms of nonvolatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) Monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) Monitor
- a keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network.
- Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), an inter-network (e.g., the Internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
- LAN local area network
- WAN wide area network
- inter-network e.g., the Internet
- peer-to-peer networks e.g., ad hoc peer-to-peer networks.
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- a server transmits data (e.g., an HTML page) to a client device (e.g., for purposes of displaying data to and receiving user input from a user interacting with the client device).
- client device e.g., for purposes of displaying data to and receiving user input from a user interacting with the client device.
- Data generated at the client device e.g., a result of the user interaction
Abstract
A computer-implemented technique can include receiving, at a computing system including one or more processors, a translation model including a plurality of aligned pairs of phrases in first and second languages. The technique can include determining, at the computing system, one or more features for each of the plurality of pairs of phrases based on linguistic differences between the first and second languages to obtain a plurality of sets of features. The technique can include associating, at the computing system, the plurality of sets of features with the plurality of pairs of phrases, respectively, to obtain a modified translation model. The technique can also include performing, at the computing system, statistical machine translation from the first language to the second language using the modified translation model.
Description
- This application claims the benefit of U.S. Provisional Application No. 61/495,928, filed on Jun. 10, 2011. The entire disclosure of the above application is incorporated herein by reference.
- This section provides background information related to the present disclosure which is not necessarily prior art.
- Statistical machine translation (SMT) generally utilizes statistical models to provide a translation from a source language to a target language. One type of SMT is phrase-based statistical machine translation. Phrase-based SMT can map sets of words (phrases) from a source language to a target language. Phrase-based SMT may rely on lexical information, e.g., the surface form of the words. The source language and the target language, however, may have significant lexical differences, such as when one of the languages is morphologically-rich.
- A computer-implemented technique is presented. The technique can include receiving, at a computing system including one or more processors, a translation model including a plurality of pairs of phrases, each of the plurality of pairs of phrases including a first phrase of one or more words in a first language and a second phrase of one or more words in a second language, wherein a specific first phrase is aligned with a specific second phrase for a specific pair of phrases. The technique can include determining, at the computing system, one or more features for each of the plurality of pairs of phrases based on linguistic differences between the first and second languages to obtain a plurality of sets of features. The technique can include associating, at the computing system, the plurality of sets of features with the plurality of pairs of phrases, respectively, to obtain a modified translation model. The technique can also include performing, at the computing system, statistical machine translation from the first language to the second language using the modified translation model.
- In some embodiments, the modified translation model has lexical and inflectional agreement for each of the plurality of pairs of phrases.
- In other embodiments, one of the first and second languages is a morphologically-rich language.
- In some embodiments, the morphologically-rich language is characterized by morphological processes that produce a large number of word forms for a given root word.
- In other embodiments, the morphologically-rich language is a synthetic language.
- In some embodiments, one of the first and second languages is a non-morphologically-rich language.
- In other embodiments, the non-morphologically-rich language is an isolating language or an analytic language.
- In some embodiments, the one or more features include at least one of parts of speech features and dependency features.
- In other embodiments, the parts of speech features include at least one of personal pronouns, possessive pronouns, determiners, particles, and wh-nouns.
- In some embodiments, the dependency features include syntactic divergences including at least one of word order, verb-less phrases, and possessiveness.
- In other embodiments, the dependency features include lexical divergences including at least one of idiomatic expressions, prepositions, named entities, ambiguity, and alignment difficulties.
- In some embodiments, performing the statistical machine translation using the modified translation model further includes: receiving, at the computing system, one or more words in the first language; generating, at the computing system, one or more potential translations of the one or more words using the modified translation model, the one or more potential translations having one or more probability scores, respectively; selecting, at the computing system, one of the one or more potential translations based on the one or more probability scores to obtain a selected translation; and outputting, at the computing system, the selected translation.
- Another computer-implemented technique is also presented. The technique can include receiving, at a computing system including one or more processors, a translation model configured for translation between a first language and a second language. The technique can include receiving, at the computing system, a plurality of pairs of phrases, each of the plurality of pairs of phrases including a first phrase of one or more words in the first language and a second phrase of one or more words in the second language, wherein a specific first phrase is aligned with a specific second phrase for a specific pair of phrases. The technique can include receiving, at the computing system, a source phrase for translation from the first language to the second language. The technique can include determining, at the computing system, a translated phrase based on the source phrase using the translation model. The technique can include determining, at the computing system, a selected second phrase from the plurality of pairs of phrases based on the translated phrase. The technique can include predicting, at the computing system, one or more features for each word of the translated phrase based on the selected second phrase, a selected first phrase associated with the selected second phrase, and linguistic differences between the first and second languages. The technique can include modifying, at the computing system, the translated phrase based on the one or more features to obtain a modified translated phrase. The technique can also include outputting, from the computing system, the modified translated phrase.
- In some embodiments, the translated phrase has lexical and inflectional agreement with the source phrase.
- In other embodiments, the one or more features include at least one of lemma, parts of speech features, morphological features, and dependency features.
- In some embodiments, predicting the one or more features for each word in the translated phrase further includes determining, at the computing system, at least one of the lemma, the part of speech features, and the morphological features for each word in the selected first phrase and for each word in the selected second phrase.
- In other embodiments, predicting the one or more features for each word in the translated phrase further includes projecting, at the computing system, dependency relations from the selected first phrase to the selected second phrase based on an alignment between the selected first phrase and the selected second phrase and the part of speech features of both the selected first phrase and the selected second phrase.
- In some embodiments, the parts of speech features include at least one of personal pronouns, possessive pronouns, determiners, particles, and wh-nouns.
- In other embodiments, the dependency features include syntactic divergences including at least one of word order, verb-less phrases, and possessiveness.
- In some embodiments, the dependency features include lexical divergences including at least one of idiomatic expressions, prepositions, named entities, ambiguity, and alignment difficulties.
- In other embodiments, one of the first and second languages is a morphologically-rich language.
- In some embodiments, the morphologically-rich language is characterized by morphological processes that produce a large number of word forms for a given root word.
- In other embodiments, the morphologically-rich language is a synthetic language.
- In some embodiments, one of the first and second languages is a non-morphologically-rich language.
- In other embodiments, the non-morphologically-rich language is an isolating language or an analytic language.
- A system is also presented. The system can include one or more computing devices configured to perform operations. The operations can include receiving a translation model including a plurality of pairs of phrases, each of the plurality of pairs of phrases including a first phrase of one or more words in a first language and a second phrase of one or more words in a second language, wherein a specific first phrase is aligned with a specific second phrase for a specific pair of phrases. The operations can include determining one or more features for each of the plurality of pairs of phrases based on linguistic differences between the first and second languages to obtain a plurality of sets of features. The operations can include associating the plurality of sets of features with the plurality of pairs of phrases, respectively, to obtain a modified translation model. The operations can also include performing statistical machine translation from the first language to the second language using the modified translation model.
- In some embodiments, the modified translation model has lexical and inflectional agreement for each of the plurality of pairs of phrases.
- In other embodiments, one of the first and second languages is a morphologically-rich language.
- In some embodiments, the morphologically-rich language is characterized by morphological processes that produce a large number of word forms for a given root word.
- In other embodiments, the morphologically-rich language is a synthetic language.
- In some embodiments, one of the first and second languages is a non-morphologically-rich language.
- In other embodiments, the non-morphologically-rich language is an isolating language or an analytic language.
- In some embodiments, the one or more features include at least one of parts of speech features and dependency features.
- In other embodiments, the parts of speech features include at least one of personal pronouns, possessive pronouns, determiners, particles, and wh-nouns.
- In some embodiments, the dependency features include syntactic divergences including at least one of word order, verb-less phrases, and possessiveness.
- In other embodiments, the dependency features include lexical divergences including at least one of idiomatic expressions, prepositions, named entities, ambiguity, and alignment difficulties.
- In some embodiments, the operation of performing the statistical machine translation using the modified translation model further includes: receiving one or more words in the first language; generating one or more potential translations of the one or more words using the modified translation model, the one or more potential translations having one or more probability scores, respectively; selecting one of the one or more potential translations based on the one or more probability scores to obtain a selected translation; and outputting the selected translation.
- Another system is also presented. The system can include one or more computing devices configured to perform operations. The operations can include receiving a translation model configured for translation between a first language and a second language. The operations can include receiving a plurality of pairs of phrases, each of the plurality of pairs of phrases including a first phrase of one or more words in the first language and a second phrase of one or more words in the second language, wherein a specific first phrase is aligned with a specific second phrase for a specific pair of phrases. The operations can include receiving a source phrase for translation from the first language to the second language. The operations can include determining a translated phrase based on the source phrase using the translation model. The operations can include determining a selected second phrase from the plurality of pairs of phrases based on the translated phrase. The operations can include predicting one or more features for each word of the translated phrase based on the selected second phrase, a selected first phrase associated with the selected second phrase, and linguistic differences between the first and second languages. The operations can include modifying the translated phrase based on the one or more features to obtain a modified translated phrase. The operations can also include outputting the modified translated phrase.
- In some embodiments, the translated phrase has lexical and inflectional agreement with the source phrase.
- In other embodiments, the one or more features include at least one of lemma, parts of speech features, morphological features, and dependency features.
- In some embodiments, the operation of predicting the one or more features for each word in the translated phrase further includes determining at least one of the lemma, the part of speech features, and the morphological features for each word in the selected first phrase and for each word in the selected second phrase.
- In other embodiments, the operation of predicting the one or more features for each word in the translated phrase further includes projecting dependency relations from the selected first phrase to the selected second phrase based on an alignment between the selected first phrase and the selected second phrase and the part of speech features of both the selected first phrase and the selected second phrase.
- In some embodiments, the parts of speech features include at least one of personal pronouns, possessive pronouns, determiners, particles, and wh-nouns.
- In other embodiments, the dependency features include syntactic divergences including at least one of word order, verb-less phrases, and possessiveness.
- In some embodiments, the dependency features include lexical divergences including at least one of idiomatic expressions, prepositions, named entities, ambiguity, and alignment difficulties.
- In other embodiments, one of the first and second languages is a morphologically-rich language.
- In some, embodiments, the morphologically-rich language is characterized by morphological processes that produce a large number of word forms for a given root word.
- In other embodiments, the morphologically-rich language is a synthetic language.
- In some embodiments, one of the first and second languages is a non-morphologically-rich language.
- In other embodiments, the non-morphologically-rich language is an isolating language or an analytic language.
- The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
- The drawings described herein are for illustrative purposes only of selected embodiments and not all possible implementations, and are not intended to limit the scope of the present disclosure.
-
FIG. 1 is a tree illustrating example relationships between various Arabic pronouns; -
FIG. 2A illustrates an example of correct and incorrect translations of an attached Arabic preposition; -
FIG. 2B illustrates an example of correct and incorrect translations of separate Arabic prepositions; -
FIG. 3 illustrates an example system for executing techniques according to some implementations of the present disclosure; -
FIG. 4 illustrates an example dependency tree projection according to some implementations of the present disclosure; -
FIG. 5A illustrates an example of extracting an Arabic predicate-subject relation from an English syntactic dependency parse tree according to some implementations of the present disclosure; -
FIG. 5B illustrates an example of extracting an Arabic relative words relation from an English syntactic dependency parse tree according to some implementations of the present disclosure; -
FIG. 6 is a flow diagram of an example technique for generating a modified language model according to some implementations of the present disclosure; and -
FIG. 7 is a flow diagram of an example technique for post-processing of a translated phrase according to some implementations of the present disclosure. - Corresponding reference numerals indicate corresponding parts throughout the several views of the drawings.
- Statistical Machine Translation (SMT) is a method for translating text from a source language to a target language. SMT, however, can provide inaccurate or imprecise translations when compared to high quality human translation, especially when one of the two languages is morphologically rich. A morphologically-rich language can be characterized by morphological processes that produce a large number of word forms for a given root word. For example, a morphologically-rich language may be a synthetic language. Alternatively, for example, a non-morphologically-rich language may be an isolating language or an analytic language. Also, the greater the lexical and syntactic divergences between the two languages, the more the need for incorporating linguistic information in the translation process increases.
- Because Arabic is a polysynthetic language where every word has many attachments, segmentation of Arabic words is expected to improve translation from or to English. which is an isolating language (i.e., each unit of meaning is represented by a separate word). Segmentation also helps the sparsity problem of morphologically rich languages. Segmentation has considered Arabic-English translation because this direction does not require post-processing to connect the Arabic segments into whole words. Additionally, segmentation of Arabic can help achieve better translation quality.
- Tokenization and normalization of Arabic data may be utilized to improve SMT. Post processing may be needed to reconnect the morphemes into valid Arabic words, e.g., to de-tokenize and de-normalize the previously tokenized and normalized surface forms. Training SMT models on Arabic lemmas instead of surface forms may increase translation quality.
- To help with syntactic divergences, specifically word order, reordering of the source sentence as preprocessing can be used. Because word order in Arabic is different from English, reordering may help alignment as well as the order of words in the target sentence.
- Another reordering approach focuses on verb-subject constructions for Arabic-to-English SMT. This approach may differentiate between main clauses and subordinate clauses and applied different rules for each case. Reordering based on automatic learning can have an advantage of being language independent. Some techniques try to decrease the divergences between the two languages through preprocessing and post-processing to make the two languages more similar. Other techniques incorporate linguistic information in the translation and language models. One technique uses lexical, shallow syntactic, syntactic and positional context features. Adding context features may help identify disambiguation as well as other specification problems such as choosing whether a noun should be accusative, dative or genitive in German. The features can be added to a log-linear translation model, Minimum Error Rate Training (MERT) can then be performed to estimate the mixture coefficients.
- Source-side contextual features have been considered in which grammatical dependency relations are incorporated in Phrase-Based SMT (PB-SMT) as a number of features. Other efforts toward improving target phrase selection can include applying source-similarity features in PB-SMT. In some techniques, the source sentences are maintained along with the phrase pairs in the phrase table. In translating a source sentence, similarity between the source sentence and the sentences from which the phrase pair were extracted was considered as a feature in the log-linear translation model.
- Language models that rely on morphological features in addition to lexical features were developed to overcome sparsity as well as inflectional agreement errors. The sparsity problem impacts SMT both in the bilingual translation model and in the used language model. Because Arabic is morphologically rich, e.g., most base words are inflected by adding affixes that indicate gender, case, tense, number, and other features, its vocabulary is very large. This can lead to incorrect language model probability estimation because of the sparsity and the high Out-of-Vocabulary (OOV) rate. A Joint morphological-lexical language model (JMLLM) can combine the lexical information with the information extracted from a morphological analyzer. Predicting the correct inflection of words in morphologically rich languages has been performed on Russian and Arabic SMT outputs including applying a Maximum Entropy Markov Model (k-MEMM). A Maximum Entropy Markov Model is a structured prediction model where the current prediction is conditioned on the previous k predictions.
- Integrating other sources of linguistic information, such as morphological or syntactic information, in the translation process can provide improvements especially for translation between language pairs with large topological divergences. English-Arabic is an example of these language pairs. While Arabic-English translation is also difficult, it does not require the generation of rich morphology at the target side. Translation from English to Arabic is one focus of this specification; however, the used techniques are also useful for translation into other morphologically rich languages.
- One goal of the techniques described in this specification is to solve some of the problems resulting from the large gap between English and Arabic. This specification introduces two techniques for improving English-Arabic translation through incorporating morphology and syntax in SMT. The first part applies changes to the statistical translation model while the second part is post-processing. In the first part, adding syntactic features to the phrase table is described. The syntactic features are based on the English syntactic information and include part-of-speech (POS) and dependency features. POS features are suggested to penalize phrases that include English words that do not correspond to Arabic words. These phrases are sources of error because they usually translate to Arabic words with different meaning or even with different POS tags. An example is a phrase containing the English word “the” which should map in Arabic to the noun prefix “Al” and never appear as a separate word. For example, in scenarios in which the used Arabic segmentation does not separate the “A1” from nouns, the choice of POS features can depend on the used Arabic segmentation.
- The techniques described in this specification are motivated at least in part by the structural and morphological divergences between the two languages. Two reasons behind adding these syntactic features are the complex affixation to Arabic words as well as the lexical and inflectional agreement.
- Dependency features are features that rely on the syntactic dependency parse tree of the sentences from which a certain phrase was extracted. These features are suggested because they can solve a number of error categories, the main two of which are lexical agreement and inflectional morphological agreement. An example of lexical agreement is phrasal verbs where a verb takes a specific preposition to convey a specific meaning. When the verb and the preposition are in separate phrases, they are less likely to translate correctly. However, selecting a phrase containing both words in the same phrase may increase the likelihood of their lexical agreement.
- Inflectional agreement is a syntactic-morphological feature of the Arabic language. Some words should have morphological agreement with other words in the sentence, e.g., an adjective should morphologically agree with the noun it modifies in gender, number, etc. Morphological agreement also applies to other related words such as verbs and their subjects, words connected by conjunction and others. To increase the likelihood of correct inflectional agreement of two syntactically related words, a phrase containing both words should be selected by the decoder. This increases the likelihood of their agreement since phrases are extracted from morphologically correct training sentences. The weights of the added features are then evaluated automatically using the Minimum Error Rate Training (MERT) algorithm. The results show an improvement in the automatic evaluation score BLEU over the baseline system.
- The second part of the specification introduces a post-processing framework for fixing inflectional agreement in MT output. In particular, the present specification focuses on specific constructions, e.g., morphological agreement between syntactically dependent words. The framework is also a probabilistic framework which models each syntactically extracted morphological agreement relation separately. Also, the framework predicts each feature such as gender, number, etc. separately instead of predicting the surface forms, which decreases the complexity of the system and allows training with smaller corpora. The predicted features along with the lemmas are then passed to the morphology generation module which generates the correct inflections.
- In contrast to the first part of the specification which aims at improving morphology by adding features and thus modifying the main pipeline of SMT, the second part introduces a probabilistic framework for morphological generation incorporating syntactic, morphological and lexical information sources through post-processing. While dependency features also aim at solving inflectional agreement, it may have limitations that can be overcome by post-processing. First, dependency features are added for words which are at small distances in the sentence. This is because phrase based SMT systems may limit the length of phrases. Related words that have distances more than the maximum phrase length are not helped. Second, phrases that contain related words could be absent from the phrase table because they were not in the training data or were filtered because they were not frequent enough. Finally, other features that have more weight than dependency features could lead to selecting other phrases.
- Using the decoder of the baseline system, the component that can motivate selecting the correctly inflected words is the N-gram language model. For example, 3- or 4-gram language models may be used, which means agreement between close words can be captured. The language model can fix the agreement issues where:
-
- The correct inflected word form is present in the phrase table.
- Inflected phrases having the same semantics are clustered and all other translation feature values are normalized.
- If both conditions apply, the correct inflected form of a word can be generated if the agreement relation is with a close word. However, the above two conditions may be difficult to apply, for example, because of the following reasons:
-
- Sparsity: The correct inflection of a word that agrees with the rest of the sentence might be absent from the phrase table because it was not in the training data or appeared very few times and subsequently got filtered.
- The lack of robust Arabic analysis and disambiguation tools leads to erroneous clustering of words. Because the units in the phrase table are actually phrases and not words, clustering becomes more difficult and more ambiguous. Clustering errors may hurt the semantic quality of the SMT system and should be avoided.
- Therefore, a different approach to solving agreement issues in SMT through post processing is described. The approach can avoid the above problems because it:
-
- relies on syntactic dependencies to identify potential agreements and therefore can handle agreement between largely distant words.
- generates inflected word forms that were never seen in the parallel training data, which helps in solving the sparsity problem.
- works with the output of any machine translation system.
- is language independent.
- One embodiment of the described subject matter improves the inflectional agreement of the Arabic translation output as proven by the automatic and human evaluations.
- A log linear statistical machine translation system can be used. The log linear approach to SMT uses the maximum-entropy framework to search for the best translation into a target language of a source language text given the following decision rule:
-
- where e1 l=e1, e2, e3 . . . el is the best translation for the input foreign language text string, e.g., sentence, f1 J=f1, f2, f3 . . . fJ, hm(e1 l,f1 J) are the used feature functions including, for example, translation and language model probabilities. For example, the translated English output target language text string for an input source language test string in Arabic. The unknown parameters λ1 M are the weights of the feature functions and are evaluated using development data as will be discussed below.
- Training the translation model starts by aligning the words of the sentence pairs in the training data using, for example, one of the IBM translation models. To move from word-level translation systems to phrase-based systems which can capture context more powerfully, a phrase extraction algorithm can be used. Subsequently, feature functions could be defined at the phrase level.
- Using a translation model which translates a source-language sentence f into a target-language sentence e through maximizing a linear combination of features and weights allows easily extending it by defining new feature functions.
- For every sentence pair (e1 l,f1 J), the Viterbi alignment is the alignment a1 J such that:
-
- where aj is the index of the word in e1 l to which fj is aligned.
- Word alignments can be calculated using GIZA++, which uses the IBM models 1 to 5 and the Hidden Markov Model (HMM) alignment model, all of which do not permit a source word to align to multiple target words. GIZA++ allows many-to-many alignments by combining the Viterbi alignments of both directions: source-to-target and target-to-source using some heuristics.
- After alignment and phrase extraction, the phrases and associated features are stored in a phrase table. Given an aligned phrase pair: source-language phrase
f and a corresponding target-language phrase ē, the most common phrase features are: -
- The phrase translation probability:
-
-
- The inverse phrase translation probability:
-
-
- Additional features can include syntactic information, context information, etc.
- The log linear model is very easy to extend by adding new features. After adding new features, feature weights need to be calculated using MERT which is described below.
- For log linear SMT translation systems, the output translation is governed by equation 1. In such systems, the best translation is the one that maximizes the linear combination of weighted features. Equation 5 shows a model using the feature functions discussed above in addition to a feature function representing the language model probability.
-
- where the translation and inverse translation probabilities are calculated as the multiplication of the separate phrase probabilities shown in equations 3 and 4, respectively. The third feature function is the language model probability of the output sentence.
- These weights λ1 M can be calculated using, for example, gradient descent to maximize the likelihood of the data according to the following equation:
-
- using a parallel training corpus consisting of S sentence pairs. This method corresponds to maximizing the likelihood of the training data, but it does not maximize translation quality for unseen data. Therefore, Minimum Error Rate Training (MERT) is used instead. A different objective function which takes into account translation quality by using automatic evaluation metrics such as BLEU score is used. MERT aims at optimizing the following equation, instead:
-
- where E(r, e) is the result of computing a score based on an automatic evaluation metric, e.g., BLEU and ê(fs;λ1 M) is the best output translation according to equation 1.
- Arabic morphology is complex if compared to English morphology. Similar to English, Arabic morphology has two functions: derivation and inflection, both of which are discussed above. On the other hand, there are two different types of morphology, i.e., two different ways of applying changes to the stem or the base word. These two types are the templatic morphology and the affixational morphology. The functions and types of morphology are discussed in this section. As will be shown, Arabic affixational morphology is the most complex and constitutes the majority of English-Arabic translation problems.
-
- Inflectional Morphology is about creating words from other words (root or stem) while the core meaning remains unchanged. An example from English is inflecting the verb “write” to “writes” in the case of third person singular. Another example is generating the plural of a noun, e.g., “writers” from “writer”. An example in Arabic is generating
- Templatic Morphology
- In Arabic, the root morpheme consists of three, four, or five consonants. Every root has an abstract meaning that's shared by all its derivatives. According to known templates, the root is modified by adding additional vowels and consonants in a specific order to certain positions to generate different words. For example, the word
- Affixational Morphology
- This morphological type is common in most languages. It is about creating a new word from other words (roots or stems) by adding affixes: prefixes and suffixes. Affixes added to Arabic base-words are either inflectional markings or attachable clitics. Assuming inflectional markings are included in the BASE WORD, attachable clitics in Arabic follows a strict order as in: [cnj+[prt+[art+BASE WORD+pro]]]
- Prefixes include:
-
- cnj: conjunctions such as w,f meaning and, then, respectively.
- prt: some prepositions and particles such as b,l,k meaning by/with, to, as, respectively.
- art: definite article Al meaning the.
- inflectional markings for tense, gender, number, person, etc.
Suffixes include: - pro personal pronouns for verbs and possessive pronouns for nouns.
- By contrast, English affixational morphology is simpler because there is no clitics attached to the words. Affixational morphology in English is used for both inflection and derivation. Examples include:
-
- Inflectional markings such as adding ed to the end of the verb to indicate past or past participle tense. Also, adding s to the end of the present form of a verb indicates it is singular. On the other hand, adding s to the end of a noun indicates that it's plural.
- Derivational morphology, for example, adding er to read generates a different word which is reader. Examples of prefixes include “mis,” “in” and “un” for negation as in “misrepresent,” “incorrect” and “undeniable,” respectively.
- English words do not have attachable clitics like Arabic. Examples I and II illustrate how one Arabic word can correspond to five and four English words respectively.
- As shown above, Arabic affixational and inflectional morphology are very complex especially compared to English. The complex morphology is the main reason behind the problems of sparsity, agreement, and lexical divergences as will be explained in more detail in this chapter.
- In Arabic, there are rules that govern the inflection of words according to their relations with other words in the sentence. These rules are referred to throughout the specification as “agreement rules.” Agreement rules can involve information such as the grammatical type of the words, e.g., Part-of-Speech (POS) tags, the relationship type between the words and other specific variables. In this section, a few undetailed agreement rules are explained as examples.
- Verbs should agree morphologically with their subjects. Verbs that follow the subject (in SVO order) agree with the subject in number and gender (see example III). On the other hand, verbs that precede the subject (VSO) agree with the subject in gender while having the singular 3rd person inflection (see example IV).
- Adjectives always follow their noun in determinism, gender and number. There are many other factors that add more rules, for example if the adjective is describing a human or an object, if the noun is plural but not in the regular plural form (e.g., broken plural), etc. Example V shows how the adjective “polite” follows the noun “sisters” in being definite, feminine and plural. This is an example where the noun refers to humans and is in the regular feminine plural form.
- Example VI shows how the adjective polite follows the noun in being definite, masculine and plural. In this example, the noun is in the masculine broken plural form.
- In example VII, the adjective follows the noun in definiteness; however, the adjective is feminine and singular, while the noun is masculine and plural. This is because the noun is a broken plural representing more than one object (e.g., books).
- If a noun is modified by a number, the noun is inflected differently according to the value of the number. For example, if the number is 1, the noun will be singular. If the number is 2, the noun should have dual inflection. For numbers (3-10), the noun should be plural. For numbers (>11), the noun should be singular.
- Words that have conjunction relations always agree with each other.
- There are other cases of agreement. For example, demonstrative and relative pronouns should agree with the nouns that they co-refer to.
- Sparsity is a result of Arabic complex inflectional morphology and the various attachable clitics. In some implementations, while the number of Arabic words in a parallel corpus is 20% less than English words, the number of unique Arabic words can be over 50% more than the number of unique English words.
- Sparsity causes many errors in SMT output in a number of ways:
-
- Absence of word forms: the correct inflection of a word that agrees with the rest of the sentence could be absent from the phrase table because it was not in the training data or was infrequent and therefore was filtered.
- Poor Translation Probability Estimation: In SMT, translation probabilities are estimated through counting (refer to equation 3). Sparsity implies that words appear less frequently in the training data, which implies poor estimation of probabilities.
- Poor Language Model Estimation: Sparsity also causes poor estimation of language model probabilities.
- Word Order
- Subjects
- The main sentence structure in Arabic is Verb-Subject-Object (VSO), while English sentence structure is Subject-Verb-Object (SVO). The order SVO also occurs in Arabic but is less frequent. Therefore subjects can be pre-verbal or post-verbal. Additionally, subjects can be pro-dropped, e.g., subject pronouns do not need to be expressed because they are inferable from the verb conjugation. Example VIII shows a case of a pro-dropped subject. The subject is a masculine third-person pronoun that is dropped because it can be inferred from the verb inflection.
- Adjectives
- In Arabic, adjectives follow the nouns that they modify as opposed to English where the adjectives precede the nouns. Example IX shows the order of nouns and adjectives.
- Verb-Less Sentences
- In Arabic, verb-less sentences are nominal sentences which have no verbs. They usually exhibit the zero copula phenomenon, e.g., the noun and the predicate are joined without overt marking. These sentences are usually mapped to affirmative English sentences containing the copular verb to be in the present form. Example X shows an example of a nominal sentence.
- One possible problem that can result from this syntactic divergence is when none of the three phrases “The weather is wonderful”, “The weather is”, or “is wonderful” exists in the phrase table, in which case “is” would be translated separately to an incorrect word. This results from the bad alignment of the word “is” to other Arabic words during training.
- Possessiveness
- The Arabic equivalent of possessiveness between nouns and of the of-relationship is called ldafa. The idafa construct is achieved by having the word indicating the possessed entity precede a definite form of the possessing entity. Refer to example XI for an illustration of this construct.
- Lexical Divergences
- Lexical divergences are the differences between the two languages at the lexical level. They result in translation problems, some of which are discussed in this section.
- Idiomatic Expressions
- Expressions are usually incorrectly translated as they are translated as separate words. Mapping each word or a few words to their corresponding meaning in Arabic usually results in a meaningless translation or at least a translation with a meaning that does not correctly match the English expression. Examples XII and XIII illustrate the problem.
- Prepositions
- Verbs that take prepositions cause problems in translation. Translating the verb alone to an Arabic verb and the preposition to a corresponding Arabic preposition usually results in errors. The same applies to prepositions needed by nouns. In example XIV, although “meeting” is translated correctly to its corresponding Arabic word, the direct translation of the preposition leads to a wrong phrase.
- Named Entities
- Named entities cause a problem in translation. Translating named entities word-by-word results in wrong Arabic output.
- Ambiguity
-
- Alignment Difficulties
- Direct mapping of English words to Arabic words is not possible because of the lexical, morphological and grammatical differences. During alignment, this problem generates errors that are transferred to the phrase table. Some examples include:
-
- Auxiliaries: In English, auxiliaries can be added, for example, to express certain tenses or to express the passive voice. This is not the case in Arabic where different tenses are represented by inflecting the verbs or by different diacritizations. This problem results in erroneous word mappings resulting from aligning an English sentence containing auxiliaries to an Arabic sentence. For example, sometimes “was” translates to “
- Verb to be: Sentences with a present verb to be such as “The girl is nice” translates in Arabic to a nominal sentence (see above). If “is” is selected in a separate phrase, an extra incorrect word will be added to the sentence.
- Particles also usually result in extra Arabic prepositions, nouns or verbs breaking the semantic and grammatical structure of the Arabic sentence.
- Error Analysis Summary
- Manual error analysis can be performed, in one example, on a small sample of 30 sentences which were translated using a state-of-the-art phrase-based translation system. Despite the small sample size, most errors described appeared in the output sentences. Morphological, syntactic and lexical divergences contributed to the errors. These divergences make the alignment of sentences from both languages very difficult and consequently result in problems in phrase extraction and mapping. Therefore, errors in the phrase table were very common.
- Phrase table errors could directly lead to errors in the final translation output. They can result, for example, in missing or additional clitics in Arabic words and sometimes extra Arabic words. Besides, it is very common that English verbs map to Arabic nouns in the phrase table, which results in problems in the final grammatical structure of the output sentence. Ambiguity is also a phrase table problem. This is because the phrase table is based on the surface forms not taking context into consideration. Seventeen sentences out of the thirty had errors because of these phrase table problems.
- Morphological agreement is a major problem in the Arabic output. The main problems are the agreement of the adjective with the noun and the agreement of the verb with the subject. Nine sentences had problems with adjective-noun agreement, while two had problems with verb-subject agreement.
- Named Entities and acronyms which were translated directly resulted in errors in nine sentences.
- POS Features
- In general, most POS features are added to penalize the incorrectly mapped phrase pairs. The English part of these phrase pairs usually does not have a corresponding Arabic translation (see examples above). Therefore, it is usually paired with incorrect Arabic phrases. The POS features can be added to discourage these phrase pairs from being selected by the decoder. These features mark phrases that consist of one or more of personal and possessive pronouns, prepositions, determiners, particles and wh-words. Example POS features are summarized in Table A. After adding the features, MERT is used to calculate their weights.
-
TABLE A Word classes for connectable phrases POS Explanation PRP Personal Pronouns: subject pronouns and object pronouns PRP$ Possessive Pronouns DT Determiners: a, the, this, etc. IN Prepositions RP Particles WDT Wh-determiner: what, which WP Wh-pronoun: who, whether, which (head of a wh- noun phrase) WRB Possessive Wh-pronoun: whose - Personal Pronouns
- Personal pronouns in Arabic can be separate or attached. Similar to English, there are subject pronouns and object pronouns. In addition to the singular and plural pronouns, Arabic has also dual pronouns. Personal pronouns can attach to the end of verbs (subject or object) and to the end of prepositions.
FIG. 1 shows a diagram of the Arabic pronouns. It shows that pronouns are divided into separate and attached. Separate pronouns are the subject pronouns. On the other hand, pronouns could be attached to verbs, prepositions or nouns. Pronouns attached to verbs are either subject or object pronouns. Pronouns can attach also to prepositions or nouns. All tree leaves represent personal pronouns except for the pronouns attached to nouns which are possessive pronouns. The leaf corresponding to possessive pronouns is highlighted. There are two reasons why personal pronouns should be penalized as separate phrases: -
- Subject pronouns are separate pronouns. They are uncommon because pronominal subjects can also be attached or dropped.
- If a separate pronominal subject is generated in the target sentence, selecting a phrase containing the pronoun and verb together guarantee that they agree in gender, number, etc.
- Possessive Pronouns
- Referring now to
FIG. 1 , atree 100 illustrating example relationships between various Arabic pronouns. As shown, apossessive pronoun 104 is always attached to the end of a noun. Example: - Because possessive pronouns in English are separate words, there are entries for them in the phrase table. These entries usually map to Arabic words with different meanings. Table B shows some phrase table entries to show what are they mapped to in Arabic. Sometimes, these phrases are selected by the decoder, which usually results in erroneous translations. Therefore, penalizing those phrases should prevent them from being selected.
- Prepositions and Particles
- As shown in
FIG. 1 , there are attached and separate prepositions in Arabic. Prepositions were discussed above as an example of lexical divergences. Translating prepositions separately can be harmful because sometimes they should be attached to Arabic words and sometimes context is needed in order to select the correct preposition.FIG. 2A illustrates an example of an attached preposition, which was translated incorrectly.FIG. 2B also illustrates an example of separate prepositions, which were also translated incorrectly because they were translated as separate phrases. - Therefore, selecting phrases containing just prepositions should be avoided. By adding a feature to mark these phrases, the feature is expected to get a negative weight and therefore penalized compared to other available phrases.
- Particles when translated separately usually result in additional Arabic words because phrasal verbs including the verb and preposition can map to an Arabic verb.
- Determiners
- The determiner class (DT) in English includes, in addition to other words, the definite and indefinite articles “the” and “a” or an”, respectively. In Arabic, the definite article corresponds to an “Al” attached as a prefix to the noun. There is no indefinite article in Arabic. Having them in separate phrases introduces noise. Table C shows their entries in the phrase table. As shown, they correspond to prepositions, which is very harmful to the adequacy and fluency of the output sentence.
- Wh-Nouns
- Wh-nouns include wh-determiners, wh-pronouns, and possessive wh-pronouns having the POS tags WDT, WP, and WRB, respectively. Features for these POS tags can be added to discourage selecting separate phrases which are limited to wh-nouns. The motivation for this is mainly gender and number agreement. When they are attached in one phrase with the word they refer to, they would probably be translated in the correct form.
- Dependency Features
- These features are based on the syntactic dependency parse tree of the English source sentence. They mark the phrases which have the two words of a specific set of relations in the phrase. For example, a feature amod (adjective modifier) is added to a phrase which contains both the adjective and the noun. These features are expected to get positive weights when trained by MERT and thus make the decoder favor these phrases over others. The suggested dependency features are summarized in Table D. The relations' names follow the Stanford typed dependencies.
-
TABLE D Dependency Relations Used as Features Relation Explanation Example acomp Adjectival Complement She is beautiful r(is, beautiful) amod Adjectival Modifier Same eats red meat:, r(meat, red) aux Auxiliary Sam has died: r(died, has) conj. Conjunct Same is nice and honest: r(nice, honest) det Determiner The wall is high: r(wall, the) nsubj Nominal Subject Same left: r(left, Sam) num Numeric Modifier I ate 3 apples: r(apples, 3) ref Referent I saw the book which you bought: r(book, which) - The motivation behind these dependency features is mainly agreement: morphological or lexical. Assume that a1 and a2 are Arabic words that should have morphological agreement. Because phrases are extracted from the training data which are assumed to be morphologically correct, using a phrase that contains a1 and a2 assures that they agree.
- As explained above, interesting morphological agreement relations include for example, noun-adjective and verb-subject relations. Lexical agreement relations include, for example, relations between phrasal verbs and their prepositions. For example, “talk about” is correct while “talk on” is not. Selecting a phrase where “talk” and its preposition “about” are attached guarantees their agreement.
- Some of the dependency features are also motivated by the alignment problems discussed above. These problems arise from trying to align English sentences containing words that have no corresponding separate Arabic words in the Arabic sentences. For example, the acomp relation should favor selecting the phrase “is beautiful” over selecting the two separate phrase “is” and “beautiful”. This is because the phrase “is” would translate to an incorrect Arabic word. Also, aux is motivated by the same reason, because most auxiliaries have no corresponding words in Arabic.
- Relations amod, nsubj, num, ref and conj are all motivated by inflectional agreement. The relation nsubj is also specifically useful if the subject is a pronoun, in which case it will be most of the time omitted in the Arabic and help in generating the correct verb inflection.
- Adding det is beneficial in two ways. First, to discourage selecting a phrase with a separate “the” which would result in a wrong Arabic translation as shown in Table C. Second, attaching the determiner to its noun causes the Arabic word to have the correct form whether to have “Al” or not if the determiner in English is “the” or “a”, respectively.
- Fixing Inflectional Agreement through Post-Processing
- In this portion of the specification, a post-processing framework is described. The goal of this system is to fix inflectional agreement between syntactically related words in machine translation output.
- The post-processor is based on a learning framework. Given a training corpus of aligned sentence pairs, the system is trained to predict inflections of words in MT output sentences. The system uses a number of multi-class classifiers, one for each feature. For example, there is a separate classifier for gender and a separate classifier for number.
FIG. 3 illustrates an example of thesystem pipeline 300. Thesystem 300 and the techniques of the present disclosure can be implemented by one or more computing devices, each including one or more processors. The computing device(s) can operate in a parallel or distributed architecture, and the processor(s) of a specific computing device can also operate in a parallel or distributed architecture. - Referring now to
FIG. 3 , in thetraining phase 304, a reference alignedparallel corpus 308 is used. Amorphology analyzer 312 analyzes the Arabic sentences. It specifies the lemma, the part-of-speech (POS) tag and the morphological features (e.g., gender, number, person) for every word in the sentence. Asyntax projector 316 projects the dependency relations from the English sentence to the Arabic sentence using the alignments and the POS tags of both sentences. Subsequently, it extracts the agreement relations using the projected syntax. Afeature vector extractor 320 is responsible for extracting the feature vectors out of the available lexical, morphological and syntactic information. The feature vectors as well as the correct labels which are extracted from the reference data are then used to train the classifiers by aclassifier trainer 324. - For the prediction of the correct features, the MT translation output as well as the source sentence and the alignments are required. This can be referred to as a
classification phase 328. Data from a machine translation aligned input/output datastore 332 goes through the same steps as in thetraining phase 304. The extracted feature vectors are then used to make predictions for each feature separately using aclassifier 336. - After prediction/classification by the
classifier 336, the correct features are then used along with the lemmas of the words to generate the correct inflected word by amorphology generator 340. Output of themorphology generator 340 can be stored in a firstpost-processed output datastore 344. Finally, anLM filter 348 uses an N-gram language model to add some robustness to thesystem 300 against the errors of alignment, morphology analysis and generation and classification. Output of theLM filter 348 can be stored in a secondpost-processed output datastore 352. If the generated sentence has a lower LM score than the baseline sentence, the baseline sentence is not updated. - As mentioned above, a system that can predict the correct inflections of specific words, e.g., words whose inflection is governed by agreement relations is described. A number of separate multi-class classifiers are trained, one for each morphological feature.
- One way certain parts of a sentence should be inflected in correspondence to the inflection of other parts, e.g., the inflection of a verb based on its subject inflection or the inflection of an adjective based on the noun can be encoded in a finite set of rules. However, such rules can be very difficult to enumerate. The rules could differ from a “part of speech” to another and from one language to another. The difficulty of writing manual rules also arises from the existence of exceptional cases to all rules. Therefore, taking this approach both requires writing all set of POS and language dependent rules and also requires handling all the special cases.
- For example, consider the inflection of an adjective in agreement with the modified noun.
- The general rule: an adjective should follow the noun in gender, number, case and state.
-
-
- If the noun is a broken plural representing objects (no persons), the adjective should be feminine and singular no matter what the gender of the noun is.
- If the noun is a broken plural representing persons (masculine), the adjective could be in a broken plural or a regular plural form.
- If the noun is a feminine plural representing objects, the adjective can be and is preferred to be singular.
Therefore, a learning approach which could be easily extended to different agreement relations and different languages is preferable.
- If all the dimensions affecting the correct word inflection could be encoded in a feature vector, many state-of-the-art probabilistic approaches can be used to predict the correct inflections. For example, a structured probabilistic model based on sentence order decomposition can be used. This system can have limitations in modeling agreement because their probabilistic model does not use the dependencies effectively. Although the prediction of a word inflection strongly depend on the inflection of the parent of the agreement relation, the feature vector in their system is composed of the stem of the parent.
- A tree-based structured probabilistic model such as k-MEMM or CRF that use the dependency tree is theoretically very effective. However, dependency trees for Arabic sentences may be of poor quality and would result in a very noisy model that might degrade the MT output quality.
- Predicting the inflection of each word according to its agreement relation separately can be very effective. As will be explained below, the relations are independent, for example, fixing the inflection of the adjective in an adjective-noun agreement relation is independent from fixing the inflection of the verb in a verb-subject agreement. Therefore, separating prediction adds robustness to the system and allows training with smaller corpora.
- For Arabic analysis, the Morphological Analysis and Disambiguation for Arabic (MADA) system can be used. The system is built on the Buckwalter analyzer which generates multiple analyses for every word. MADA uses another analyzer and generator tool ALMORGEANA to convert the output of Buckwalter from a stem-affix format to a lexeme-and-feature format.
- Afterwards, it uses an implementation of support vector machines which includes Viterbi encoding to disambiguate the results of AL-MORGEANA analyses. The result is a list of morphological features of every word taking the context (neighboring words in the sentence) into consideration. The morphological features that are evaluated by MADA are illustrated in Table E. The last four rows of the table represent the attachable clitics whose positions in the word are governed by [prc3 [prc2 [prc1 [pro0 BASEWORD enc0]]]]. For more details about those clitics and their functions, the user is referred to the MADA+TOKAN Manual. In addition to the features listed in the table, the analysis output includes the diacriticized form (diac), the lexeme/lemma (lex), the Buckwalter tag .(bw) and the English gloss (gloss).
- For generation, the lexeme, POS tag and all other known features from Table E are input to the MORGEANA tool. The system searches in the lexicon for the word which has the most similar analysis.
- The analysis and generation tools can be used to change the declension of a word. For example, to change a word w from the feminine to the masculine form, the following steps are taken:
-
- Input to MADA the surface form of the word w. MADA will output the best lexeme l and list of features f for this word.
- Change the gender feature in f from f(eminine) to m(asculine): f[gen]=m. The modified feature list can be referred to as
f . - Input to the MORGEANA generator the lexeme l and the modified feature list f. MORGEANA will generate the word
w which is the masculine surface form of w.
-
TABLE E Morphological Features resulting from MADA analysis Label Name Possible Values pos Part of verb, noun, (adj)ective, prep(osition), part(icle) and speech others, (total: 34) asp Aspect c(ommand), i(mperfective), p(erfective), na(not applicable) cas Case n(ominative), a(ccusative), g(entitive), u(ndefined), na gen Gender f(eminine), m(asculine), na mod Mood i(ndicative), p(lural), d(ual), u(ndefined), na num Number s(ingular), p(lural), (d(ual), u(ndefined), na per Person 1, 2, 3, na stt State i(ndefinite), d(efinite), c(construct/possessive/idafa), u(ndefined), na vox Voice A(ctive), p(assive), u(ndefined), na prc0 Proclitic 0, na, >a_ques (interrogative particle >a) level 0 prc1 Proclitic 0, na, fa, wa level 1 prc2 Proclitic bi ka, la, li, sa, ta, wa, fi, lA, mA, yA, wA, hA level 2 enc0 Enclitic 0, na, pronouns, possessive pronouns and other particles - To extract the morphologically dependent pairs (agreement pairs), syntax relations are needed. Although Arabic dependency tree parsers exist, for example the Berkeley and Stanford parsers, they can have poor quality. Parallel aligned data can instead be used to project the syntax tree from English to Arabic. The English parse tree is a labeled dependency tree following the grammatical representations described above. Two approaches to projection can be considered.
- Given a source sentence consisting of a set of tokens si . . . sn, a dependency relation is a function hs such that for any si, hs(i) is the head of si and ls(i) is the label of the relation between si and hs(i).
- Given an aligned target sentence tj . . . tm, A is a set of pairs (si, tj) such that si is aligned to tj. Similarly, ht(j) is the head of tj and lt(j) is the label of the relation between tj and hl(j). Similar to the unlabeled tree projection, projection can be performed according to the following rule:
- Labels can be also projected using:
- Although this approach is helpful for identifying some Arabic dependency relations, it has a number of limitations.
-
- Errors in the English parse tree are also projected to the Arabic parse tree.
- Many-to-many alignments introduce ambiguities that are difficult to resolve.
- The algorithm projects a dependency link as long as two pairs of words are aligned. Therefore, alignment errors result in projection errors. Also, the algorithm does not take into consideration the difference in structure between the two languages. For example, an Arabic noun might align to an English verb. In this case, the Arabic sentence can has a relation of “nsubj” with a noun head.
- Referring now to
FIG. 4 , an example of the direct projection approach is illustrated. In this example, the alignment between source and target sentences is one-to-one, therefore, there is no ambiguity problem. An English parse tree is illustrated at 400 for the following example sentence: “Swan in Fife, Scotland dies with H5N1 bird flu virus infection.” However, as can be seen, errors by theEnglish parser 400 were projected to an Arabic parsetree 450. More specifically, the resulting translation after projection to the Arabic parsetree 450 was the following incorrect translation: “H5N1 birds flu virus infection from dies Scotland, Fife in swans.” - Because of the above limitations, a different approach to partial tree projection can be used. This approach makes use of the Arabic analysis for robustness. It also takes syntactic divergences between the two languages into account.
- One goal of the dependency tree projection is the extraction of dependencies between pairs of words that should have morphological agreement, e.g., agreement links. Therefore, there is no need to first project the English tree to an Arabic tree from which agreement links can be extracted, which could introduce more errors. Therefore, extract agreement links can be extracted directly using the lexical and syntactic information of both the English and Arabic sentences taking into consideration the typological differences between the two languages. The projection of some of the interesting relations is explained below.
- Adjective Relation (amod)
- For an amod relation, an Arabic agreement relation is extracted if the English adjective aligns to an Arabic adjective, while the English noun aligns to an Arabic noun.
- In the case when the English word aligns to multiple Arabic words, selecting the noun for the amod relation is based on the heuristic that the first noun after a preposition is marked as the noun of the relation. The motivation behind this rule is illustrated by example XV. If the first word of the multiple word alignment was selected as the noun of the relation, a link amod(AlfwtwgrAfy, mwad) would be extracted although amod(AlfwtogrAfy, IltSwyr) is the correct link. Therefore, linguistic analysis of erroneous agreement links lead to the mentioned rule.
- Example XVI illustrates the ambiguity problem in a case of one-to-many alignment from English to Arabic. The word airline aligns to two Arabic words. The first word can be selected as the word being described by the adjective. However, in some cases, this rule introduces error.
- In an Arabic verb-less sentence, the predicate follows the subject of the sentence.
FIG. 5A illustrates how the predicate and the subject of a verb-lessArabic sentence 500 can be extracted from an English syntactic dependency parsetree 520 indirectly. Specifically, the predicate ITyf has an agreement relation with the subject of the sentence Alwld. - The agreement link of interest is the link from the verb to the subject, which is the reverse of the link in the English dependency parse tree.
- Relative Words (ref)
- One way to extract the noun to which a relative word refers is through the English dependency parse tree.
FIG. 5B illustrates an example projection of the ref relation from an English dependency parsetree 550 to anArabic sentence 570. - The
feature vector extractor 320 as shown in the framework diagram inFIG. 3 follows the morphology analysis and syntax projection. The selected features are from many sources of information: lexical, morphological and syntactic. Table F summarizes the features used in the feature vectors used by the classifiers. -
TABLE F The features used by the classifiers Arabic Morphological asp, cas, gen, mod, num, per, stt, vox, Features prc0 prc1, prc2, enc0 (refer to table 6.1) Feminine Ending: yes, no Gloss Number: singular or plural Plural Type: regular, irregular (broken plurals) Syntactic Part-of-speech (aligned, head) Lexical Stem(head), English gloss(head) English Syntactic part-of-speech (aligned, head) Features Lexical surface form (aligned, head) General Relation Type amod, verb, acomp, etc. Head position before, after - For Arabic Features, morphological features include the features returned by the
morphology analyzer 312 ofFIG. 3 . In some implementations, an analysis of classification errors revealed regular errors caused by themorphology analyzer 312, for which two extra features were added: feminine ending and number of English gloss. Although all nouns and adjectives in Arabic that have particular endings, namely, “morphology analyzer 312 confuses them frequently. Themorphology analyzer 312 does not make use of these endings since it does not analyze the surface forms of the words; however, it uses prefix, suffix and stem lexicons to generate the features. Incorrect or missing labels to these words in the used lexicons would result in these errors. - The number of the English gloss is also added to overcome the persistent error of the analyzer to analyze broken plurals as singular. The reason is because it actually identifies a plural by whether the stem is attached to a clitic for plural marking. However, in the case of broken plurals, there is no affix added to the stem, however, it is derived from the singular form, a case of derivational morphology. As a solution to this problem, a feature is added to indicate whether any of the English glosses of the word is plural or not.
- The feature “Plural Type” is added because it, significantly affects the decision about the correct inflection. For example, a regular masculine plural noun has its modifying adjective in masculine plural form, while an irregular plural noun usually has its modifying adjective in feminine singular form.
- Syntactic features for Arabic include part of speech tags of the current and head words. Lexical features include the stem of the head word and the English gloss.
- English features include the part of speech tags and the surface forms of the aligned and head words. On the other hand, general features include the dependency relation type and whether the head comes before or after the current word in the sentence. The latter feature is useful for example in the case of verbs where the verb inflection rules are different for the SVO order versus the VSO order.
- In order to perform classifier training (see
classifier trainer 324 ofFIG. 3 ), the extracted feature vectors as well as the correct labels are needed. The data used for training is a set of parallel aligned sentences. The agreement relations are extracted as described above. For every relation pair, prediction is done for one word given the inflection of the parent and other bilingual and lexical information that are encoded in the feature vectors. The result of themorphological analyzer 312 for a specific feature is then used as the label for this feature classifier. Erroneous labels result in noisy training data and in imprecise classification accuracy evaluation. - For training the classifiers, an automatic tool can be used for selecting the best classification model for each feature and also for selecting the best parameters for this model using cross-validation. The reported accuracy is actually the mean accuracy of the folds of the cross validation.
- In classification, after agreement relations and then feature vectors are extracted, prediction is done separately for each feature using the corresponding classifier.
- An N-Gram Language model is a probabilistic model specifically a model which predicts the next word in a sentence given the previous N words based on the Markov assumption. The N-Gram language model probability of a sentence is approximated as a multiplication of N-Gram probabilities as shown. in the second part of equation 9.
-
- The language model probability can be used as an indicator to the correctness and fluency of a modification. A comparison can be made between P(output sentence) and P(post-processed sentence). If the post-processed sentence has much less probability (e.g., less by a difference more than a certain threshold) than the output translation, changes to the sentence are canceled. Change filtering using a language model is expected to provide some robustness against all sources of errors in the system. However, the language model is not very reliable. A simple example would be that the generated inflected word is out of vocabulary (OOV) for the language model, although it is morphologically the correct one.
- To evaluate the system performance, accuracy of the classifiers is evaluated and compared to two other prediction algorithms. Prediction accuracy does not, however, measure the performance of the whole system. To evaluate the final output of the system, BLEU score is used. The BLEU score of the output is compared to BLEU score of the baseline MT system output. Because of the BLEU score limitations in evaluating morphological agreement, human evaluation is also used.
- BLEU proved to be unreliable for evaluating morphological agreement because of the following:
-
- In the evaluation data, every sentence has one reference human translation.
- Because BLEU is based on merely counting which words (inflected surface forms) exist in the reference translations, two problems arise:
- There are cases where the updated words do not exist at all in the reference translation. The example in Table G shows a sentence whose agreement problem was fixed but this did not result in any change in BLEU score. Although the gender was corrected to be masculine in both words, this resulted in zero difference in BLEU score because the reference translation contained a synonym “AlmdAry” of the corrected word and not the word itself “Al<stwA′y”.
- There are cases where the original word inflection scored higher in BLEU score because the original word simply existed in the reference translation, although the agreement was wrong. The example in Table H shows how the SMT output sentence received a higher score than the post-processed one although the agreement was corrected and the whole sentence is grammatically and morphologically correct. The words in bold local governments can be noticed to disagree in definiteness in the translation output because local is definite and governments is indefinite. Although they were corrected to be both indefinite in the post-processed sentence, the post-processed sentence received a lower BLEU score. The reason is that the reference translation contains the definite form of the word governments and the word local. It is known that BLEU score is based on counting the words in the candidate translation whose surface forms exist in the reference translations. After correcting the agreement in post-processing, both the indefinite words local and governments were considered to be absent in the reference translation and thus BLEU score decreased.
-
TABLE H Example about invalidity of BLEU Source Farms which are governed by local governments Output Al.mHlyp HkwmAt tHkmhA alty AlmzArE. Local.def governments.indef govern it which farms. Post Processed mHlyp HkwmAt tHkmhA alty AlmzArE. Local.indef governments.indef govern it which farms. Reference Al.mHlyp Al.HkwmAt 1>dArp AlxADEp AlmzArE. Local.def governments.def the administration that are under farms ‘Farms that are under the administration of the local governments’ - Side by side human evaluation is used for the evaluation of the techniques. The goal is to rate the translation quality. The human raters are provided with the source sentence and two Arabic sentence outputs, one is the output of baseline system and the other is the post-processed sentence. The sentences are shuffles; therefore, the raters score the sentences without knowing their sources. They give a rating between 0 and 6 according to meaning and grammar. 6 is the best rating for perfect meaning and grammar, while 0 is the lowest rating for cases when there is no meaning preserved and thus the grammar is irrelevant. Ratings from 5 to 3 are for sentences whose meaning is preserved but with increasing grammar mistakes. Ratings below 3 are for sentences that have no meaning, in which case the grammar becomes irrelevant and has minimal effect on the quality score.
- Therefore, the human evaluation results are not expected to directly reflect whether the inflectional agreement, which is a grammatical feature, is fixed or not in the sentences. For sentences with high quality meaning, having the correct inflectional agreement should correspond to in-creasing the sentence score. However, sentences with no preserved meanings are not expected to receive higher scores for correct morphological agreement.
- Referring now to
FIG. 6 , anexample technique 600 for generating a modified translation model is illustrated. At 604, thetechnique 600 can receive, e.g., at a computing system including one or more processors, a translation model including a plurality of pairs of phrases. Each of the plurality of pairs of phrases can include a first phrase of one or more words in a first language and a second phrase of one or more words in a second language. A specific first phrase can be aligned with a specific second phrase for a specific pair of phrases. At 608, thetechnique 600 can determine, e.g., at the computing system, one or more features for each of the plurality of pairs of phrases based on linguistic differences between the first and second languages to obtain a plurality of sets of features. At 612, thetechnique 600 can associate, e.g., at the computing system, the plurality of sets of features with the plurality of pairs of phrases, respectively, to obtain a modified translation model. At 616, thetechnique 600 can, e.g., at the computing system, perform statistical machine translation from the first language to the second language using the modified translation model. Thetechnique 600 can then end or return to 604 for one or more additional cycles. - Referring now to
FIG. 7 , anexample technique 700 for post-processing of a translated phrase is illustrated. At 704, thetechnique 700 can, e.g., at a computing system including one or more processors, receive a translation model configured for translation between a first language and a second language. At 704, thetechnique 700 can, e.g., at the computing system, receive a plurality of pairs of phrases. Each of the plurality of pairs of phrases can include a first phrase of one or more words in the first language and a second phrase of one or more words in the second language. A specific first phrase can be aligned with a specific second phrase for a specific pair of phrases. At 712, thetechnique 700 can, e.g., at the computing system, receive a source phrase for translation from the first language to the second language. At 716, thetechnique 700 can, e.g., at the computing system, determine a translated phrase based on the source phrase using the translation model. At 720, thetechnique 700 can, e.g., at the computing system, determine a selected second phrase from the plurality of pairs of phrases based on the translated phrase. At 724, thetechnique 700 can, e.g., at the computing system, predict one or more features for each word of the translated phrase based on the selected second phrase, a selected first phrase associated with the selected second phrase, and linguistic differences between the first and second languages. At 728, thetechnique 700 can, e.g., at the computing system, modify the translated phrase based on the one or more features to obtain a modified translated phrase. At 732, thetechnique 700 can, e.g., at the computing system, output the modified translated phrase. Thetechnique 700 can then end or return to 704 for one or more additional cycles. - Embodiments of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on computer storage medium for execution by, or to control the operation of, data processing apparatus. Alternatively or in addition, the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus. A computer storage medium can be, or be included in, a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. Moreover, while a computer storage medium is not a propagated signal, a computer storage medium can be a source or destination of computer program instructions encoded in an artificially generated propagated signal. The computer storage medium can also be, or be included in, one or more separate physical components or media (e.g., multiple CDs, disks, or other storage devices). The operations described in this specification can be implemented as operations performed by a data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
- The term “data processing apparatus” encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or multiple ones, or combinations, of the foregoing The apparatus can include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). The apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus, and execution environment can realize various different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures.
- A computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a standalone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. A computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing actions in accordance with instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a universal serial bus (USB) flash drive), to name just a few. Devices suitable for storing computer program instructions and data include all forms of nonvolatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) Monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's client device in response to requests received from the web browser.
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), an inter-network (e.g., the Internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
- The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, a server transmits data (e.g., an HTML page) to a client device (e.g., for purposes of displaying data to and receiving user input from a user interacting with the client device). Data generated at the client device (e.g., a result of the user interaction) can be received from the client device at the server.
- While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any inventions or of what may be claimed, but rather as descriptions of features specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
- Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
- Thus, particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. In some cases, the actions recited in the claims can be performed in a different order and still achieve desirable results. In addition, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In certain implementations, multitasking and parallel processing may be advantageous.
Claims (50)
1. A computer-implemented method comprising:
receiving, at a computing system including one or more processors, a translation model including a plurality of pairs of phrases, each of the plurality of pairs of phrases including a first phrase of one or more words in a first language and a second phrase of one or more words in a second language, wherein a specific first phrase is aligned with a specific second phrase for a specific pair of phrases;
determining, at the computing system, one or more features for each of the plurality of pairs of phrases based on linguistic differences between the first and second languages to obtain a plurality of sets of features;
associating, at the computing system, the plurality of sets of features with the plurality of pairs of phrases, respectively, to obtain a modified translation model; and
performing, at the computing system, statistical machine translation from the first language to the second language using the modified translation model.
2. The computer-implemented method of claim 1 , wherein the modified translation model has lexical and inflectional agreement for each of the plurality of pairs of phrases.
3. The computer-implemented method of claim 1 , wherein one of the first and second languages is a morphologically-rich language.
4. The computer-implemented method of claim 3 , wherein the morphologically-rich language is characterized by morphological processes that produce a large number of word forms for a given root word.
5. The computer-implemented method of claim 3 , wherein the morphologically-rich language is a synthetic language.
6. The computer-implemented method of claim 3 , wherein one of the first and second languages is a non-morphologically-rich language.
7. The computer-implemented method of claim 6 , wherein the non-morphologically-rich language is an isolating language or an analytic language.
8. The computer-implemented method of claim 1 , wherein the one or more features include at least one of parts of speech features and dependency features.
9. The computer-implemented method of claim 8 , wherein the parts of speech features include at least one of personal pronouns, possessive pronouns, determiners, particles, and wh-nouns.
10. The computer-implemented method of claim 8 , wherein the dependency features include syntactic divergences including at least one of word order, verb-less phrases, and possessiveness.
11. The computer-implemented method of claim 8 , wherein the dependency features include lexical divergences including at least one of idiomatic expressions, prepositions, named entities, ambiguity, and alignment difficulties.
12. The computer-implemented method of claim 1 , wherein performing the statistical machine translation using the modified translation model further includes:
receiving, at the computing system, one or more words in the first language;
generating, at the computing system, one or more potential translations of the one or more words using the modified translation model, the one or more potential translations having one or more probability scores, respectively;
selecting, at the computing system, one of the one or more potential translations based on the one or more probability scores to obtain a selected translation; and
outputting, at the computing system, the selected translation.
13. A computer-implemented method comprising:
receiving, at a computing system including one or more processors, a translation model configured for translation between a first language and a second language;
receiving, at the computing system, a plurality of pairs of phrases, each of the plurality of pairs of phrases including a first phrase of one or more words in the first language and a second phrase of one or more words in the second language, wherein a specific first phrase is aligned with a specific second phrase for a specific pair of phrases;
receiving, at the computing system, a source phrase for translation from the first language to the second language;
determining, at the computing system, a translated phrase based on the source phrase using the translation model;
determining, at the computing system, a selected second phrase from the plurality of pairs of phrases based on the translated phrase;
predicting, at the computing system, one or more features for each word of the translated phrase based on the selected second phrase, a selected first phrase associated with the selected second phrase, and linguistic differences between the first and second languages;
modifying, at the computing system, the translated phrase based on the one or more features to obtain a modified translated phrase; and
outputting, from the computing system, the modified translated phrase.
14. The computer-implemented method of claim 13 , wherein the translated phrase has lexical and inflectional agreement with the source phrase.
15. The computer-implemented method of claim 13 , wherein the one or more features include at least one of lemma, parts of speech features, morphological features, and dependency features.
16. The computer-implemented method of claim 15 , wherein predicting the one or more features for each word in the translated phrase further includes determining, at the computing system, at least one of the lemma, the part of speech features, and the morphological features for each word in the selected first phrase and for each word in the selected second phrase.
17. The computer-implemented method of claim 16 , wherein predicting the one or more features for each word in the translated phrase further includes projecting, at the computing system, dependency relations from the selected first phrase to the selected second phrase based on an alignment between the selected first phrase and the selected second phrase and the part of speech features of both the selected first phrase and the selected second phrase.
18. The computer-implemented method of claim 15 , wherein the parts of speech features include at least one of personal pronouns, possessive pronouns, determiners, particles, and wh-nouns.
19. The computer-implemented method of claim 15 , wherein the dependency features include syntactic divergences including at least one of word order, verb-less phrases, and possessiveness.
20. The computer-implemented method of claim 15 , wherein the dependency features include lexical divergences including at least one of idiomatic expressions, prepositions, named entities, ambiguity, and alignment difficulties.
21. The computer-implemented method of claim 13 , wherein one of the first and second languages is a morphologically-rich language.
22. The computer-implemented method of claim 21 wherein the morphologically-rich language is characterized by morphological processes that produce a large number of word forms for a given root word.
23. The computer-implemented method of claim 21 , wherein the morphologically-rich language is a synthetic language.
24. The computer-implemented method of claim 21 , wherein one of the first and second languages is a non-morphologically-rich language.
25. The computer-implemented method of claim 24 , wherein the non-morphologically-rich language is an isolating language or an analytic language.
26. A system comprising:
one or more computing devices configured to perform operations including:
receiving a translation model including a plurality of pairs of phrases, each of the plurality of pairs of phrases including a first phrase of one or more words in a first language and a second phrase of one or more words in a second language, wherein a specific first phrase is aligned with a specific second phrase for a specific pair of phrases;
determining one or more features for each of the plurality of pairs of phrases based on linguistic differences between the first and second languages to obtain a plurality of sets of features;
associating the plurality of sets of features with the plurality of pairs of phrases, respectively, to obtain a modified translation model; and
performing statistical machine translation from the first language to the second language using the modified translation model.
27. The system of claim 26 , wherein the modified translation model has lexical and inflectional agreement for each of the plurality of pairs of phrases.
28. The system of claim 26 , wherein one of the first and second languages is a morphologically-rich language.
29. The system of claim 28 , wherein the morphologically-rich language is characterized by morphological processes that produce a large number of word forms for a given root word.
30. The system of claim 28 , wherein the morphologically-rich language is a synthetic language.
31. The system of claim 28 , wherein one of the first and second languages is a non-morphologically-rich language.
32. The system of claim 31 , wherein the non-morphologically-rich language is an isolating language or an analytic language.
33. The system of claim 26 , wherein the one or more features include at least one of parts of speech features and dependency features.
34. The system of claim 33 , wherein the parts of speech features include at least one of personal pronouns, possessive pronouns, determiners, particles, and wh-nouns.
35. The system of claim 33 , wherein the dependency features include syntactic divergences including at least one of word order, verb-less phrases, and possessiveness.
36. The system of claim 33 , wherein the dependency features include lexical divergences including at least one of idiomatic expressions, prepositions, named entities, ambiguity, and alignment difficulties.
37. The system of claim 26 , wherein the operation of performing the statistical machine translation using the modified translation model further includes:
receiving one or more words in the first language;
generating one or more potential translations of the one or more words using the modified translation model, the one or more potential translations having one or more probability scores, respectively;
selecting one of the one or more potential translations based on the one or more probability scores to obtain a selected translation; and
outputting the selected translation.
38. A system comprising:
one or more computing devices configured to perform operations including:
receiving a translation model configured for translation between a first language and a second language;
receiving a plurality of pairs of phrases, each of the plurality of pairs of phrases including a first phrase of one or more words in the first language and a second phrase of one or more words in the second language, wherein a specific first phrase is aligned with a specific second phrase for a specific pair of phrases;
receiving a source phrase for translation from the first language to the second language;
determining a translated phrase based on the source phrase using the translation model;
determining a selected second phrase from the plurality of pairs of phrases based on the translated phrase;
predicting one or more features for each word of the translated phrase based on the selected second phrase, a selected first phrase associated with the selected second phrase, and linguistic differences between the first and second languages;
modifying the translated phrase based on the one or more features to obtain a modified translated phrase; and
outputting the modified translated phrase.
39. The system of claim 38 , wherein the translated phrase has lexical and inflectional agreement with the source phrase.
40. The system of claim 38 , wherein the one or more features include at least one of lemma, parts of speech features, morphological features, and dependency features.
41. The system of claim 40 , wherein the operation of predicting the one or more features for each word in the translated phrase further includes determining at least one of the lemma, the part of speech features, and the morphological features for each word in the selected first phrase and for each word in the selected second phrase.
42. The system of claim 41 , wherein the operation of predicting the one or more features for each word in the translated phrase further includes projecting dependency relations from the selected first phrase to the selected second phrase based on an alignment between the selected first phrase and the selected second phrase and the part of speech features of both the selected first phrase and the selected second phrase.
43. The system of claim 40 , wherein the parts of speech features include at least one of personal pronouns, possessive pronouns, determiners, particles, and wh-nouns.
44. The system of claim 40 , wherein the dependency features include syntactic divergences including at least one of word order, verb-less phrases, and possessiveness.
45. The system of claim 40 , wherein the dependency features include lexical divergences including at least one of idiomatic expressions, prepositions, named entities, ambiguity, and alignment difficulties.
46. The system of claim 38 , wherein one of the first and second languages is a morphologically-rich language.
47. The system of claim 46 wherein the morphologically-rich language is characterized by morphological processes that produce a large number of word forms for a given root word.
48. The system of claim 46 , wherein the morphologically-rich language is a synthetic language.
49. The system of claim 46 , wherein one of the first and second languages is a non-morphologically-rich language.
50. The system of claim 49 , wherein the non-morphologically-rich language is an isolating language or an analytic language.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/493,475 US20120316862A1 (en) | 2011-06-10 | 2012-06-11 | Augmenting statistical machine translation with linguistic knowledge |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201161495928P | 2011-06-10 | 2011-06-10 | |
US13/493,475 US20120316862A1 (en) | 2011-06-10 | 2012-06-11 | Augmenting statistical machine translation with linguistic knowledge |
Publications (1)
Publication Number | Publication Date |
---|---|
US20120316862A1 true US20120316862A1 (en) | 2012-12-13 |
Family
ID=46276066
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/493,475 Abandoned US20120316862A1 (en) | 2011-06-10 | 2012-06-11 | Augmenting statistical machine translation with linguistic knowledge |
Country Status (2)
Country | Link |
---|---|
US (1) | US20120316862A1 (en) |
WO (1) | WO2012170817A1 (en) |
Cited By (163)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20120116751A1 (en) * | 2010-11-09 | 2012-05-10 | International Business Machines Corporation | Providing message text translations |
US20120158398A1 (en) * | 2010-12-17 | 2012-06-21 | John Denero | Combining Model-Based Aligner Using Dual Decomposition |
US20120290290A1 (en) * | 2011-05-12 | 2012-11-15 | Microsoft Corporation | Sentence Simplification for Spoken Language Understanding |
CN103116578A (en) * | 2013-02-07 | 2013-05-22 | 北京赛迪翻译技术有限公司 | Translation method integrating syntactic tree and statistical machine translation technology and translation device |
US20130185049A1 (en) * | 2012-01-12 | 2013-07-18 | International Business Machines Corporation | Predicting Pronouns for Pro-Drop Style Languages for Natural Language Translation |
US20130197896A1 (en) * | 2012-01-31 | 2013-08-01 | Microsoft Corporation | Resolving out-of-vocabulary words during machine translation |
US20140067394A1 (en) * | 2012-08-28 | 2014-03-06 | King Abdulaziz City For Science And Technology | System and method for decoding speech |
US20140309986A1 (en) * | 2013-04-11 | 2014-10-16 | Microsoft Corporation | Word breaker from cross-lingual phrase table |
US20150149176A1 (en) * | 2013-11-27 | 2015-05-28 | At&T Intellectual Property I, L.P. | System and method for training a classifier for natural language understanding |
US20150154184A1 (en) * | 2013-12-04 | 2015-06-04 | International Business Machines Corporation | Morphology analysis for machine translation |
US9064006B2 (en) | 2012-08-23 | 2015-06-23 | Microsoft Technology Licensing, Llc | Translating natural language utterances to keyword search queries |
US20150293910A1 (en) * | 2014-04-14 | 2015-10-15 | Xerox Corporation | Retrieval of domain relevant phrase tables |
US9244984B2 (en) | 2011-03-31 | 2016-01-26 | Microsoft Technology Licensing, Llc | Location based conversational understanding |
US9298287B2 (en) | 2011-03-31 | 2016-03-29 | Microsoft Technology Licensing, Llc | Combined activation for natural user interface systems |
US9412365B2 (en) | 2014-03-24 | 2016-08-09 | Google Inc. | Enhanced maximum entropy models |
US20160307566A1 (en) * | 2015-04-16 | 2016-10-20 | Apple Inc. | Parsimonious continuous-space phrase representations for natural language processing |
US9519643B1 (en) | 2015-06-15 | 2016-12-13 | Microsoft Technology Licensing, Llc | Machine map label translation |
JP2017004179A (en) * | 2015-06-08 | 2017-01-05 | 日本電信電話株式会社 | Information processing method, device, and program |
US9558182B1 (en) | 2016-01-08 | 2017-01-31 | International Business Machines Corporation | Smart terminology marker system for a language translation system |
EP3079075A4 (en) * | 2013-12-04 | 2017-08-02 | National Institute Of Information And Communications Technology | Learning device, translation device, learning method, and translation method |
US9760566B2 (en) | 2011-03-31 | 2017-09-12 | Microsoft Technology Licensing, Llc | Augmented conversational understanding agent to identify conversation context between two humans and taking an agent action thereof |
US20170308526A1 (en) * | 2016-04-21 | 2017-10-26 | National Institute Of Information And Communications Technology | Compcuter Implemented machine translation apparatus and machine translation method |
US9842592B2 (en) | 2014-02-12 | 2017-12-12 | Google Inc. | Language models using non-linguistic context |
US9842168B2 (en) | 2011-03-31 | 2017-12-12 | Microsoft Technology Licensing, Llc | Task driven user intents |
US9858343B2 (en) | 2011-03-31 | 2018-01-02 | Microsoft Technology Licensing Llc | Personalization of queries, conversations, and searches |
US9865248B2 (en) | 2008-04-05 | 2018-01-09 | Apple Inc. | Intelligent text-to-speech conversion |
US20180011833A1 (en) * | 2015-02-02 | 2018-01-11 | National Institute Of Information And Communications Technology | Syntax analyzing device, learning device, machine translation device and storage medium |
US9898460B2 (en) | 2016-01-26 | 2018-02-20 | International Business Machines Corporation | Generation of a natural language resource using a parallel corpus |
US9959271B1 (en) * | 2015-09-28 | 2018-05-01 | Amazon Technologies, Inc. | Optimized statistical machine translation system with rapid adaptation capability |
US9966060B2 (en) | 2013-06-07 | 2018-05-08 | Apple Inc. | System and method for user-specified pronunciation of words for speech synthesis and recognition |
US9986419B2 (en) | 2014-09-30 | 2018-05-29 | Apple Inc. | Social reminders |
US10043516B2 (en) | 2016-09-23 | 2018-08-07 | Apple Inc. | Intelligent automated assistant |
US10049675B2 (en) | 2010-02-25 | 2018-08-14 | Apple Inc. | User profiling for voice input processing |
US10079014B2 (en) | 2012-06-08 | 2018-09-18 | Apple Inc. | Name recognition system |
US10083690B2 (en) | 2014-05-30 | 2018-09-25 | Apple Inc. | Better resolution when referencing to concepts |
US10108612B2 (en) | 2008-07-31 | 2018-10-23 | Apple Inc. | Mobile device having human language translation capability with positional feedback |
US10134394B2 (en) | 2015-03-20 | 2018-11-20 | Google Llc | Speech recognition using log-linear model |
US10185713B1 (en) | 2015-09-28 | 2019-01-22 | Amazon Technologies, Inc. | Optimized statistical machine translation system with rapid adaptation capability |
US20190065485A1 (en) * | 2016-04-04 | 2019-02-28 | Wovn Technologies, Inc. | Translation system |
US10268684B1 (en) * | 2015-09-28 | 2019-04-23 | Amazon Technologies, Inc. | Optimized statistical machine translation system with rapid adaptation capability |
US10303715B2 (en) | 2017-05-16 | 2019-05-28 | Apple Inc. | Intelligent automated assistant for media exploration |
US10311871B2 (en) | 2015-03-08 | 2019-06-04 | Apple Inc. | Competing devices responding to voice triggers |
US10311144B2 (en) | 2017-05-16 | 2019-06-04 | Apple Inc. | Emoji word sense disambiguation |
US10332518B2 (en) | 2017-05-09 | 2019-06-25 | Apple Inc. | User interface for correcting recognition errors |
US10354652B2 (en) | 2015-12-02 | 2019-07-16 | Apple Inc. | Applying neural network language models to weighted finite state transducers for automatic speech recognition |
US10356243B2 (en) | 2015-06-05 | 2019-07-16 | Apple Inc. | Virtual assistant aided communication with 3rd party service in a communication session |
US10381016B2 (en) | 2008-01-03 | 2019-08-13 | Apple Inc. | Methods and apparatus for altering audio output signals |
US10395654B2 (en) | 2017-05-11 | 2019-08-27 | Apple Inc. | Text normalization based on a data-driven learning network |
US10403278B2 (en) | 2017-05-16 | 2019-09-03 | Apple Inc. | Methods and systems for phonetic matching in digital assistant services |
US10403283B1 (en) | 2018-06-01 | 2019-09-03 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US10410637B2 (en) | 2017-05-12 | 2019-09-10 | Apple Inc. | User-specific acoustic models |
US10417266B2 (en) | 2017-05-09 | 2019-09-17 | Apple Inc. | Context-aware ranking of intelligent response suggestions |
US10417344B2 (en) | 2014-05-30 | 2019-09-17 | Apple Inc. | Exemplar-based natural language processing |
US10417405B2 (en) | 2011-03-21 | 2019-09-17 | Apple Inc. | Device access using voice authentication |
US10431204B2 (en) | 2014-09-11 | 2019-10-01 | Apple Inc. | Method and apparatus for discovering trending terms in speech requests |
US10438595B2 (en) | 2014-09-30 | 2019-10-08 | Apple Inc. | Speaker identification and unsupervised speaker adaptation techniques |
US10445429B2 (en) | 2017-09-21 | 2019-10-15 | Apple Inc. | Natural language understanding using vocabularies with compressed serialized tries |
US10453443B2 (en) | 2014-09-30 | 2019-10-22 | Apple Inc. | Providing an indication of the suitability of speech recognition |
US10474753B2 (en) | 2016-09-07 | 2019-11-12 | Apple Inc. | Language identification using recurrent neural networks |
US10482874B2 (en) | 2017-05-15 | 2019-11-19 | Apple Inc. | Hierarchical belief states for digital assistants |
US10496705B1 (en) | 2018-06-03 | 2019-12-03 | Apple Inc. | Accelerated task performance |
US10497365B2 (en) | 2014-05-30 | 2019-12-03 | Apple Inc. | Multi-command single utterance input method |
US10529332B2 (en) | 2015-03-08 | 2020-01-07 | Apple Inc. | Virtual assistant activation |
US10567477B2 (en) | 2015-03-08 | 2020-02-18 | Apple Inc. | Virtual assistant continuity |
US10580409B2 (en) | 2016-06-11 | 2020-03-03 | Apple Inc. | Application integration with a digital assistant |
US10593346B2 (en) | 2016-12-22 | 2020-03-17 | Apple Inc. | Rank-reduced token representation for automatic speech recognition |
US10592604B2 (en) | 2018-03-12 | 2020-03-17 | Apple Inc. | Inverse text normalization for automatic speech recognition |
US10636424B2 (en) | 2017-11-30 | 2020-04-28 | Apple Inc. | Multi-turn canned dialog |
US10643611B2 (en) | 2008-10-02 | 2020-05-05 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US10642934B2 (en) | 2011-03-31 | 2020-05-05 | Microsoft Technology Licensing, Llc | Augmented conversational understanding architecture |
US10657328B2 (en) | 2017-06-02 | 2020-05-19 | Apple Inc. | Multi-task recurrent neural network architecture for efficient morphology handling in neural language modeling |
US10657961B2 (en) | 2013-06-08 | 2020-05-19 | Apple Inc. | Interpreting and acting upon commands that involve sharing information with remote devices |
US10684703B2 (en) | 2018-06-01 | 2020-06-16 | Apple Inc. | Attention aware virtual assistant dismissal |
US10699717B2 (en) | 2014-05-30 | 2020-06-30 | Apple Inc. | Intelligent assistant for home automation |
US10714117B2 (en) | 2013-02-07 | 2020-07-14 | Apple Inc. | Voice trigger for a digital assistant |
US10726832B2 (en) | 2017-05-11 | 2020-07-28 | Apple Inc. | Maintaining privacy of personal information |
US10733982B2 (en) | 2018-01-08 | 2020-08-04 | Apple Inc. | Multi-directional dialog |
US10733993B2 (en) | 2016-06-10 | 2020-08-04 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US10733375B2 (en) | 2018-01-31 | 2020-08-04 | Apple Inc. | Knowledge-based framework for improving natural language understanding |
US10741185B2 (en) | 2010-01-18 | 2020-08-11 | Apple Inc. | Intelligent automated assistant |
US10748546B2 (en) | 2017-05-16 | 2020-08-18 | Apple Inc. | Digital assistant services based on device capabilities |
US10755051B2 (en) | 2017-09-29 | 2020-08-25 | Apple Inc. | Rule-based natural language processing |
US10755703B2 (en) | 2017-05-11 | 2020-08-25 | Apple Inc. | Offline personal assistant |
US10769386B2 (en) | 2017-12-05 | 2020-09-08 | Sap Se | Terminology proposal engine for determining target language equivalents |
US10769385B2 (en) | 2013-06-09 | 2020-09-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
US10789959B2 (en) | 2018-03-02 | 2020-09-29 | Apple Inc. | Training speaker recognition models for digital assistants |
US10789945B2 (en) | 2017-05-12 | 2020-09-29 | Apple Inc. | Low-latency intelligent automated assistant |
US10791176B2 (en) | 2017-05-12 | 2020-09-29 | Apple Inc. | Synchronization and task delegation of a digital assistant |
US10810274B2 (en) | 2017-05-15 | 2020-10-20 | Apple Inc. | Optimizing dialogue policy decisions for digital assistants using implicit feedback |
US10818288B2 (en) | 2018-03-26 | 2020-10-27 | Apple Inc. | Natural assistant interaction |
US10832664B2 (en) | 2016-08-19 | 2020-11-10 | Google Llc | Automated speech recognition using language models that selectively use domain-specific model components |
US10839159B2 (en) | 2018-09-28 | 2020-11-17 | Apple Inc. | Named entity normalization in a spoken dialog system |
US10885279B2 (en) * | 2018-11-08 | 2021-01-05 | Microsoft Technology Licensing, Llc | Determining states of content characteristics of electronic communications |
US10892996B2 (en) | 2018-06-01 | 2021-01-12 | Apple Inc. | Variable latency device coordination |
US20210019479A1 (en) * | 2018-09-05 | 2021-01-21 | Tencent Technology (Shenzhen) Company Limited | Text translation method and apparatus, storage medium, and computer device |
US10904611B2 (en) | 2014-06-30 | 2021-01-26 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US10909331B2 (en) | 2018-03-30 | 2021-02-02 | Apple Inc. | Implicit identification of translation payload with neural machine translation |
US10928918B2 (en) | 2018-05-07 | 2021-02-23 | Apple Inc. | Raise to speak |
US10942703B2 (en) | 2015-12-23 | 2021-03-09 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10942702B2 (en) | 2016-06-11 | 2021-03-09 | Apple Inc. | Intelligent device arbitration and control |
US10956666B2 (en) | 2015-11-09 | 2021-03-23 | Apple Inc. | Unconventional virtual assistant interactions |
US10984780B2 (en) | 2018-05-21 | 2021-04-20 | Apple Inc. | Global semantic word embeddings using bi-directional recurrent neural networks |
US11010561B2 (en) | 2018-09-27 | 2021-05-18 | Apple Inc. | Sentiment prediction from textual data |
US11010127B2 (en) | 2015-06-29 | 2021-05-18 | Apple Inc. | Virtual assistant for media playback |
US20210150148A1 (en) * | 2019-11-20 | 2021-05-20 | Academia Sinica | Natural language processing method and computing apparatus thereof |
US11023513B2 (en) | 2007-12-20 | 2021-06-01 | Apple Inc. | Method and apparatus for searching using an active ontology |
US11025565B2 (en) | 2015-06-07 | 2021-06-01 | Apple Inc. | Personalized prediction of responses for instant messaging |
US11048473B2 (en) | 2013-06-09 | 2021-06-29 | Apple Inc. | Device, method, and graphical user interface for enabling conversation persistence across two or more instances of a digital assistant |
US11070949B2 (en) | 2015-05-27 | 2021-07-20 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on an electronic device with a touch-sensitive display |
US11069336B2 (en) | 2012-03-02 | 2021-07-20 | Apple Inc. | Systems and methods for name pronunciation |
US11069347B2 (en) | 2016-06-08 | 2021-07-20 | Apple Inc. | Intelligent automated assistant for media exploration |
US11120372B2 (en) | 2011-06-03 | 2021-09-14 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US11126400B2 (en) | 2015-09-08 | 2021-09-21 | Apple Inc. | Zero latency digital assistant |
US11127397B2 (en) | 2015-05-27 | 2021-09-21 | Apple Inc. | Device voice control |
US11133008B2 (en) | 2014-05-30 | 2021-09-28 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US11140099B2 (en) | 2019-05-21 | 2021-10-05 | Apple Inc. | Providing message response suggestions |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11170166B2 (en) | 2018-09-28 | 2021-11-09 | Apple Inc. | Neural typographical error modeling via generative adversarial networks |
US11204787B2 (en) | 2017-01-09 | 2021-12-21 | Apple Inc. | Application integration with a digital assistant |
US11217251B2 (en) | 2019-05-06 | 2022-01-04 | Apple Inc. | Spoken notifications |
US11227589B2 (en) | 2016-06-06 | 2022-01-18 | Apple Inc. | Intelligent list reading |
US11231904B2 (en) | 2015-03-06 | 2022-01-25 | Apple Inc. | Reducing response latency of intelligent automated assistants |
US11237797B2 (en) | 2019-05-31 | 2022-02-01 | Apple Inc. | User activity shortcut suggestions |
US11269678B2 (en) | 2012-05-15 | 2022-03-08 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US11281993B2 (en) | 2016-12-05 | 2022-03-22 | Apple Inc. | Model and ensemble compression for metric learning |
US11289073B2 (en) | 2019-05-31 | 2022-03-29 | Apple Inc. | Device text to speech |
US11301477B2 (en) | 2017-05-12 | 2022-04-12 | Apple Inc. | Feedback analysis of a digital assistant |
US11307752B2 (en) | 2019-05-06 | 2022-04-19 | Apple Inc. | User configurable task triggers |
US11314370B2 (en) | 2013-12-06 | 2022-04-26 | Apple Inc. | Method for extracting salient dialog usage from live data |
US11348573B2 (en) | 2019-03-18 | 2022-05-31 | Apple Inc. | Multimodality in digital assistant systems |
US11350253B2 (en) | 2011-06-03 | 2022-05-31 | Apple Inc. | Active transport based notifications |
US11360641B2 (en) | 2019-06-01 | 2022-06-14 | Apple Inc. | Increasing the relevance of new available information |
US11388291B2 (en) | 2013-03-14 | 2022-07-12 | Apple Inc. | System and method for processing voicemail |
US11386266B2 (en) | 2018-06-01 | 2022-07-12 | Apple Inc. | Text correction |
US11423886B2 (en) | 2010-01-18 | 2022-08-23 | Apple Inc. | Task flow identification based on user intent |
US11423908B2 (en) | 2019-05-06 | 2022-08-23 | Apple Inc. | Interpreting spoken requests |
US11462215B2 (en) | 2018-09-28 | 2022-10-04 | Apple Inc. | Multi-modal inputs for voice commands |
US11467802B2 (en) | 2017-05-11 | 2022-10-11 | Apple Inc. | Maintaining privacy of personal information |
US11468282B2 (en) | 2015-05-15 | 2022-10-11 | Apple Inc. | Virtual assistant in a communication session |
US11475898B2 (en) | 2018-10-26 | 2022-10-18 | Apple Inc. | Low-latency multi-speaker speech recognition |
US11475884B2 (en) | 2019-05-06 | 2022-10-18 | Apple Inc. | Reducing digital assistant latency when a language is incorrectly determined |
US11488406B2 (en) | 2019-09-25 | 2022-11-01 | Apple Inc. | Text detection using global geometry estimators |
US11495218B2 (en) | 2018-06-01 | 2022-11-08 | Apple Inc. | Virtual assistant operation in multi-device environments |
US11496600B2 (en) | 2019-05-31 | 2022-11-08 | Apple Inc. | Remote execution of machine-learned models |
US11500672B2 (en) | 2015-09-08 | 2022-11-15 | Apple Inc. | Distributed personal assistant |
US11526368B2 (en) | 2015-11-06 | 2022-12-13 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US11532306B2 (en) | 2017-05-16 | 2022-12-20 | Apple Inc. | Detecting a trigger of a digital assistant |
US11538576B2 (en) | 2019-10-15 | 2022-12-27 | International Business Machines Corporation | Illustrative medical imaging for functional prognosis estimation |
US11638059B2 (en) | 2019-01-04 | 2023-04-25 | Apple Inc. | Content playback on multiple devices |
US11657813B2 (en) | 2019-05-31 | 2023-05-23 | Apple Inc. | Voice identification in digital assistant systems |
US11671920B2 (en) | 2007-04-03 | 2023-06-06 | Apple Inc. | Method and system for operating a multifunction portable electronic device using voice-activation |
US11696060B2 (en) | 2020-07-21 | 2023-07-04 | Apple Inc. | User identification using headphones |
US11710003B2 (en) * | 2018-02-26 | 2023-07-25 | Tencent Technology (Shenzhen) Company Limited | Information conversion method and apparatus, storage medium, and electronic device |
US11755276B2 (en) | 2020-05-12 | 2023-09-12 | Apple Inc. | Reducing description length based on confidence |
US11765209B2 (en) | 2020-05-11 | 2023-09-19 | Apple Inc. | Digital assistant hardware abstraction |
US11790914B2 (en) | 2019-06-01 | 2023-10-17 | Apple Inc. | Methods and user interfaces for voice-based control of electronic devices |
US11797781B2 (en) | 2020-08-06 | 2023-10-24 | International Business Machines Corporation | Syntax-based multi-layer language translation |
US11798547B2 (en) | 2013-03-15 | 2023-10-24 | Apple Inc. | Voice activated device for use with a voice-based digital assistant |
US11809483B2 (en) | 2015-09-08 | 2023-11-07 | Apple Inc. | Intelligent automated assistant for media search and playback |
US11838734B2 (en) | 2020-07-20 | 2023-12-05 | Apple Inc. | Multi-device audio adjustment coordination |
US11853536B2 (en) | 2015-09-08 | 2023-12-26 | Apple Inc. | Intelligent automated assistant in a media environment |
US11914848B2 (en) | 2020-05-11 | 2024-02-27 | Apple Inc. | Providing relevant data items based on context |
US11928604B2 (en) | 2005-09-08 | 2024-03-12 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9928236B2 (en) * | 2015-09-18 | 2018-03-27 | Mcafee, Llc | Systems and methods for multi-path language translation |
Citations (12)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US4873634A (en) * | 1987-03-27 | 1989-10-10 | International Business Machines Corporation | Spelling assistance method for compound words |
US20040102201A1 (en) * | 2002-11-22 | 2004-05-27 | Levin Robert E. | System and method for language translation via remote devices |
US6937974B1 (en) * | 1998-03-03 | 2005-08-30 | D'agostini Giovanni | Translation system and a multifunction computer, particularly for treating texts and translation on paper |
US7228270B2 (en) * | 2001-07-23 | 2007-06-05 | Canon Kabushiki Kaisha | Dictionary management apparatus for speech conversion |
US20070282590A1 (en) * | 2006-06-02 | 2007-12-06 | Microsoft Corporation | Grammatical element generation in machine translation |
US20080319736A1 (en) * | 2007-06-21 | 2008-12-25 | Microsoft Corporation | Discriminative Syntactic Word Order Model for Machine Translation |
US20090228263A1 (en) * | 2008-03-07 | 2009-09-10 | Kabushiki Kaisha Toshiba | Machine translating apparatus, method, and computer program product |
US7624005B2 (en) * | 2002-03-28 | 2009-11-24 | University Of Southern California | Statistical machine translation |
US7827028B2 (en) * | 2006-04-07 | 2010-11-02 | Basis Technology Corporation | Method and system of machine translation |
US20110144992A1 (en) * | 2009-12-15 | 2011-06-16 | Microsoft Corporation | Unsupervised learning using global features, including for log-linear model word segmentation |
US20130006954A1 (en) * | 2011-06-30 | 2013-01-03 | Xerox Corporation | Translation system adapted for query translation via a reranking framework |
US20140309986A1 (en) * | 2013-04-11 | 2014-10-16 | Microsoft Corporation | Word breaker from cross-lingual phrase table |
-
2012
- 2012-06-08 WO PCT/US2012/041554 patent/WO2012170817A1/en active Application Filing
- 2012-06-11 US US13/493,475 patent/US20120316862A1/en not_active Abandoned
Patent Citations (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US4873634A (en) * | 1987-03-27 | 1989-10-10 | International Business Machines Corporation | Spelling assistance method for compound words |
US6937974B1 (en) * | 1998-03-03 | 2005-08-30 | D'agostini Giovanni | Translation system and a multifunction computer, particularly for treating texts and translation on paper |
US7228270B2 (en) * | 2001-07-23 | 2007-06-05 | Canon Kabushiki Kaisha | Dictionary management apparatus for speech conversion |
US7624005B2 (en) * | 2002-03-28 | 2009-11-24 | University Of Southern California | Statistical machine translation |
US20040102201A1 (en) * | 2002-11-22 | 2004-05-27 | Levin Robert E. | System and method for language translation via remote devices |
US7827028B2 (en) * | 2006-04-07 | 2010-11-02 | Basis Technology Corporation | Method and system of machine translation |
US8209163B2 (en) * | 2006-06-02 | 2012-06-26 | Microsoft Corporation | Grammatical element generation in machine translation |
US20070282590A1 (en) * | 2006-06-02 | 2007-12-06 | Microsoft Corporation | Grammatical element generation in machine translation |
US20080319736A1 (en) * | 2007-06-21 | 2008-12-25 | Microsoft Corporation | Discriminative Syntactic Word Order Model for Machine Translation |
US20090228263A1 (en) * | 2008-03-07 | 2009-09-10 | Kabushiki Kaisha Toshiba | Machine translating apparatus, method, and computer program product |
US20110144992A1 (en) * | 2009-12-15 | 2011-06-16 | Microsoft Corporation | Unsupervised learning using global features, including for log-linear model word segmentation |
US20130006954A1 (en) * | 2011-06-30 | 2013-01-03 | Xerox Corporation | Translation system adapted for query translation via a reranking framework |
US20140309986A1 (en) * | 2013-04-11 | 2014-10-16 | Microsoft Corporation | Word breaker from cross-lingual phrase table |
Non-Patent Citations (8)
Title |
---|
Kistina Toutanova et al., Applying Morphology Generation Models to Machine Translation, ACL_08: HLT, page 514-522, June 2008 * |
Lee, Morphological Analysis for Statistical Machine Translation, Proceedings of HLT-NAACL, pages 57-60, 2004 * |
Minkov et al., Generating Complex Morphology for Machine Translation, Association for Computational Linguistics, pages 128-135, June 2007 * |
Soha Sultam, Applying Morphology to English-Arabic Statistical Machine Translation, Master's Thesis Nr. 11, ETH Zurich, May. 2011 * |
Soha Sultan, Applying Morphology to English-Arabic Statistical Machine Translation, Master's Thesis Nr. 11, ETH Zurich, May. 2011 * |
Toutanova et al., Applying Morphology Generation Models to Machine Translation, ACL_08: HLT, pages 514-522, June 2008 * |
Ueffing et al., Using POS Information for Statistical Machine Translation into Morphologically Rich Languages, Proceedings of the tenth conference on European, 2003 * |
Yeniterzi et al., Syntax-to-Morphology Mapping in Factored Phrase-Based Statistical Machine Translation from English to Turkish, Association for Computational Linguistics, pages 454-464, July 2010 * |
Cited By (250)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11928604B2 (en) | 2005-09-08 | 2024-03-12 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
US11671920B2 (en) | 2007-04-03 | 2023-06-06 | Apple Inc. | Method and system for operating a multifunction portable electronic device using voice-activation |
US11023513B2 (en) | 2007-12-20 | 2021-06-01 | Apple Inc. | Method and apparatus for searching using an active ontology |
US10381016B2 (en) | 2008-01-03 | 2019-08-13 | Apple Inc. | Methods and apparatus for altering audio output signals |
US9865248B2 (en) | 2008-04-05 | 2018-01-09 | Apple Inc. | Intelligent text-to-speech conversion |
US10108612B2 (en) | 2008-07-31 | 2018-10-23 | Apple Inc. | Mobile device having human language translation capability with positional feedback |
US10643611B2 (en) | 2008-10-02 | 2020-05-05 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US11900936B2 (en) | 2008-10-02 | 2024-02-13 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US11348582B2 (en) | 2008-10-02 | 2022-05-31 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US10741185B2 (en) | 2010-01-18 | 2020-08-11 | Apple Inc. | Intelligent automated assistant |
US11423886B2 (en) | 2010-01-18 | 2022-08-23 | Apple Inc. | Task flow identification based on user intent |
US10692504B2 (en) | 2010-02-25 | 2020-06-23 | Apple Inc. | User profiling for voice input processing |
US10049675B2 (en) | 2010-02-25 | 2018-08-14 | Apple Inc. | User profiling for voice input processing |
US20120116751A1 (en) * | 2010-11-09 | 2012-05-10 | International Business Machines Corporation | Providing message text translations |
US20120158398A1 (en) * | 2010-12-17 | 2012-06-21 | John Denero | Combining Model-Based Aligner Using Dual Decomposition |
US10417405B2 (en) | 2011-03-21 | 2019-09-17 | Apple Inc. | Device access using voice authentication |
US10642934B2 (en) | 2011-03-31 | 2020-05-05 | Microsoft Technology Licensing, Llc | Augmented conversational understanding architecture |
US10049667B2 (en) | 2011-03-31 | 2018-08-14 | Microsoft Technology Licensing, Llc | Location-based conversational understanding |
US9244984B2 (en) | 2011-03-31 | 2016-01-26 | Microsoft Technology Licensing, Llc | Location based conversational understanding |
US9298287B2 (en) | 2011-03-31 | 2016-03-29 | Microsoft Technology Licensing, Llc | Combined activation for natural user interface systems |
US10296587B2 (en) | 2011-03-31 | 2019-05-21 | Microsoft Technology Licensing, Llc | Augmented conversational understanding agent to identify conversation context between two humans and taking an agent action thereof |
US10585957B2 (en) | 2011-03-31 | 2020-03-10 | Microsoft Technology Licensing, Llc | Task driven user intents |
US9858343B2 (en) | 2011-03-31 | 2018-01-02 | Microsoft Technology Licensing Llc | Personalization of queries, conversations, and searches |
US9842168B2 (en) | 2011-03-31 | 2017-12-12 | Microsoft Technology Licensing, Llc | Task driven user intents |
US9760566B2 (en) | 2011-03-31 | 2017-09-12 | Microsoft Technology Licensing, Llc | Augmented conversational understanding agent to identify conversation context between two humans and taking an agent action thereof |
US10061843B2 (en) | 2011-05-12 | 2018-08-28 | Microsoft Technology Licensing, Llc | Translating natural language utterances to keyword search queries |
US9454962B2 (en) * | 2011-05-12 | 2016-09-27 | Microsoft Technology Licensing, Llc | Sentence simplification for spoken language understanding |
US20170011025A1 (en) * | 2011-05-12 | 2017-01-12 | Microsoft Technology Licensing, Llc | Sentence simplification for spoken language understanding |
US20120290290A1 (en) * | 2011-05-12 | 2012-11-15 | Microsoft Corporation | Sentence Simplification for Spoken Language Understanding |
US11120372B2 (en) | 2011-06-03 | 2021-09-14 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US11350253B2 (en) | 2011-06-03 | 2022-05-31 | Apple Inc. | Active transport based notifications |
US20130185049A1 (en) * | 2012-01-12 | 2013-07-18 | International Business Machines Corporation | Predicting Pronouns for Pro-Drop Style Languages for Natural Language Translation |
US8903707B2 (en) * | 2012-01-12 | 2014-12-02 | International Business Machines Corporation | Predicting pronouns of dropped pronoun style languages for natural language translation |
US8990066B2 (en) * | 2012-01-31 | 2015-03-24 | Microsoft Corporation | Resolving out-of-vocabulary words during machine translation |
US20130197896A1 (en) * | 2012-01-31 | 2013-08-01 | Microsoft Corporation | Resolving out-of-vocabulary words during machine translation |
US11069336B2 (en) | 2012-03-02 | 2021-07-20 | Apple Inc. | Systems and methods for name pronunciation |
US11321116B2 (en) | 2012-05-15 | 2022-05-03 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US11269678B2 (en) | 2012-05-15 | 2022-03-08 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US10079014B2 (en) | 2012-06-08 | 2018-09-18 | Apple Inc. | Name recognition system |
US9064006B2 (en) | 2012-08-23 | 2015-06-23 | Microsoft Technology Licensing, Llc | Translating natural language utterances to keyword search queries |
US20140067394A1 (en) * | 2012-08-28 | 2014-03-06 | King Abdulaziz City For Science And Technology | System and method for decoding speech |
US10978090B2 (en) | 2013-02-07 | 2021-04-13 | Apple Inc. | Voice trigger for a digital assistant |
US11557310B2 (en) | 2013-02-07 | 2023-01-17 | Apple Inc. | Voice trigger for a digital assistant |
US11636869B2 (en) | 2013-02-07 | 2023-04-25 | Apple Inc. | Voice trigger for a digital assistant |
US11862186B2 (en) | 2013-02-07 | 2024-01-02 | Apple Inc. | Voice trigger for a digital assistant |
US10714117B2 (en) | 2013-02-07 | 2020-07-14 | Apple Inc. | Voice trigger for a digital assistant |
CN103116578A (en) * | 2013-02-07 | 2013-05-22 | 北京赛迪翻译技术有限公司 | Translation method integrating syntactic tree and statistical machine translation technology and translation device |
US11388291B2 (en) | 2013-03-14 | 2022-07-12 | Apple Inc. | System and method for processing voicemail |
US11798547B2 (en) | 2013-03-15 | 2023-10-24 | Apple Inc. | Voice activated device for use with a voice-based digital assistant |
US20140309986A1 (en) * | 2013-04-11 | 2014-10-16 | Microsoft Corporation | Word breaker from cross-lingual phrase table |
US9330087B2 (en) * | 2013-04-11 | 2016-05-03 | Microsoft Technology Licensing, Llc | Word breaker from cross-lingual phrase table |
US9966060B2 (en) | 2013-06-07 | 2018-05-08 | Apple Inc. | System and method for user-specified pronunciation of words for speech synthesis and recognition |
US10657961B2 (en) | 2013-06-08 | 2020-05-19 | Apple Inc. | Interpreting and acting upon commands that involve sharing information with remote devices |
US10769385B2 (en) | 2013-06-09 | 2020-09-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
US11727219B2 (en) | 2013-06-09 | 2023-08-15 | Apple Inc. | System and method for inferring user intent from speech inputs |
US11048473B2 (en) | 2013-06-09 | 2021-06-29 | Apple Inc. | Device, method, and graphical user interface for enabling conversation persistence across two or more instances of a digital assistant |
US20150149176A1 (en) * | 2013-11-27 | 2015-05-28 | At&T Intellectual Property I, L.P. | System and method for training a classifier for natural language understanding |
US9678939B2 (en) * | 2013-12-04 | 2017-06-13 | International Business Machines Corporation | Morphology analysis for machine translation |
EP3079075A4 (en) * | 2013-12-04 | 2017-08-02 | National Institute Of Information And Communications Technology | Learning device, translation device, learning method, and translation method |
US9779086B2 (en) | 2013-12-04 | 2017-10-03 | National Institute Of Information And Communications Technology | Learning apparatus, translation apparatus, learning method, and translation method |
US20150154184A1 (en) * | 2013-12-04 | 2015-06-04 | International Business Machines Corporation | Morphology analysis for machine translation |
US11314370B2 (en) | 2013-12-06 | 2022-04-26 | Apple Inc. | Method for extracting salient dialog usage from live data |
US9842592B2 (en) | 2014-02-12 | 2017-12-12 | Google Inc. | Language models using non-linguistic context |
US9412365B2 (en) | 2014-03-24 | 2016-08-09 | Google Inc. | Enhanced maximum entropy models |
US9582499B2 (en) * | 2014-04-14 | 2017-02-28 | Xerox Corporation | Retrieval of domain relevant phrase tables |
US20150293910A1 (en) * | 2014-04-14 | 2015-10-15 | Xerox Corporation | Retrieval of domain relevant phrase tables |
US10417344B2 (en) | 2014-05-30 | 2019-09-17 | Apple Inc. | Exemplar-based natural language processing |
US11670289B2 (en) | 2014-05-30 | 2023-06-06 | Apple Inc. | Multi-command single utterance input method |
US10878809B2 (en) | 2014-05-30 | 2020-12-29 | Apple Inc. | Multi-command single utterance input method |
US11699448B2 (en) | 2014-05-30 | 2023-07-11 | Apple Inc. | Intelligent assistant for home automation |
US10083690B2 (en) | 2014-05-30 | 2018-09-25 | Apple Inc. | Better resolution when referencing to concepts |
US11810562B2 (en) | 2014-05-30 | 2023-11-07 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US10657966B2 (en) | 2014-05-30 | 2020-05-19 | Apple Inc. | Better resolution when referencing to concepts |
US11133008B2 (en) | 2014-05-30 | 2021-09-28 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US10497365B2 (en) | 2014-05-30 | 2019-12-03 | Apple Inc. | Multi-command single utterance input method |
US10714095B2 (en) | 2014-05-30 | 2020-07-14 | Apple Inc. | Intelligent assistant for home automation |
US11257504B2 (en) | 2014-05-30 | 2022-02-22 | Apple Inc. | Intelligent assistant for home automation |
US10699717B2 (en) | 2014-05-30 | 2020-06-30 | Apple Inc. | Intelligent assistant for home automation |
US10904611B2 (en) | 2014-06-30 | 2021-01-26 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US11516537B2 (en) | 2014-06-30 | 2022-11-29 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US11838579B2 (en) | 2014-06-30 | 2023-12-05 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US10431204B2 (en) | 2014-09-11 | 2019-10-01 | Apple Inc. | Method and apparatus for discovering trending terms in speech requests |
US10390213B2 (en) | 2014-09-30 | 2019-08-20 | Apple Inc. | Social reminders |
US10438595B2 (en) | 2014-09-30 | 2019-10-08 | Apple Inc. | Speaker identification and unsupervised speaker adaptation techniques |
US9986419B2 (en) | 2014-09-30 | 2018-05-29 | Apple Inc. | Social reminders |
US10453443B2 (en) | 2014-09-30 | 2019-10-22 | Apple Inc. | Providing an indication of the suitability of speech recognition |
US20180011833A1 (en) * | 2015-02-02 | 2018-01-11 | National Institute Of Information And Communications Technology | Syntax analyzing device, learning device, machine translation device and storage medium |
US11231904B2 (en) | 2015-03-06 | 2022-01-25 | Apple Inc. | Reducing response latency of intelligent automated assistants |
US10529332B2 (en) | 2015-03-08 | 2020-01-07 | Apple Inc. | Virtual assistant activation |
US10930282B2 (en) | 2015-03-08 | 2021-02-23 | Apple Inc. | Competing devices responding to voice triggers |
US11087759B2 (en) | 2015-03-08 | 2021-08-10 | Apple Inc. | Virtual assistant activation |
US10567477B2 (en) | 2015-03-08 | 2020-02-18 | Apple Inc. | Virtual assistant continuity |
US10311871B2 (en) | 2015-03-08 | 2019-06-04 | Apple Inc. | Competing devices responding to voice triggers |
US11842734B2 (en) | 2015-03-08 | 2023-12-12 | Apple Inc. | Virtual assistant activation |
US10134394B2 (en) | 2015-03-20 | 2018-11-20 | Google Llc | Speech recognition using log-linear model |
US9842105B2 (en) * | 2015-04-16 | 2017-12-12 | Apple Inc. | Parsimonious continuous-space phrase representations for natural language processing |
US20160307566A1 (en) * | 2015-04-16 | 2016-10-20 | Apple Inc. | Parsimonious continuous-space phrase representations for natural language processing |
US11468282B2 (en) | 2015-05-15 | 2022-10-11 | Apple Inc. | Virtual assistant in a communication session |
US11070949B2 (en) | 2015-05-27 | 2021-07-20 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on an electronic device with a touch-sensitive display |
US11127397B2 (en) | 2015-05-27 | 2021-09-21 | Apple Inc. | Device voice control |
US10681212B2 (en) | 2015-06-05 | 2020-06-09 | Apple Inc. | Virtual assistant aided communication with 3rd party service in a communication session |
US10356243B2 (en) | 2015-06-05 | 2019-07-16 | Apple Inc. | Virtual assistant aided communication with 3rd party service in a communication session |
US11025565B2 (en) | 2015-06-07 | 2021-06-01 | Apple Inc. | Personalized prediction of responses for instant messaging |
JP2017004179A (en) * | 2015-06-08 | 2017-01-05 | 日本電信電話株式会社 | Information processing method, device, and program |
US9519643B1 (en) | 2015-06-15 | 2016-12-13 | Microsoft Technology Licensing, Llc | Machine map label translation |
US11947873B2 (en) | 2015-06-29 | 2024-04-02 | Apple Inc. | Virtual assistant for media playback |
US11010127B2 (en) | 2015-06-29 | 2021-05-18 | Apple Inc. | Virtual assistant for media playback |
US11126400B2 (en) | 2015-09-08 | 2021-09-21 | Apple Inc. | Zero latency digital assistant |
US11550542B2 (en) | 2015-09-08 | 2023-01-10 | Apple Inc. | Zero latency digital assistant |
US11853536B2 (en) | 2015-09-08 | 2023-12-26 | Apple Inc. | Intelligent automated assistant in a media environment |
US11500672B2 (en) | 2015-09-08 | 2022-11-15 | Apple Inc. | Distributed personal assistant |
US11809483B2 (en) | 2015-09-08 | 2023-11-07 | Apple Inc. | Intelligent automated assistant for media search and playback |
US11954405B2 (en) | 2015-09-08 | 2024-04-09 | Apple Inc. | Zero latency digital assistant |
US10268684B1 (en) * | 2015-09-28 | 2019-04-23 | Amazon Technologies, Inc. | Optimized statistical machine translation system with rapid adaptation capability |
US9959271B1 (en) * | 2015-09-28 | 2018-05-01 | Amazon Technologies, Inc. | Optimized statistical machine translation system with rapid adaptation capability |
US10185713B1 (en) | 2015-09-28 | 2019-01-22 | Amazon Technologies, Inc. | Optimized statistical machine translation system with rapid adaptation capability |
US11526368B2 (en) | 2015-11-06 | 2022-12-13 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US11809886B2 (en) | 2015-11-06 | 2023-11-07 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US11886805B2 (en) | 2015-11-09 | 2024-01-30 | Apple Inc. | Unconventional virtual assistant interactions |
US10956666B2 (en) | 2015-11-09 | 2021-03-23 | Apple Inc. | Unconventional virtual assistant interactions |
US10354652B2 (en) | 2015-12-02 | 2019-07-16 | Apple Inc. | Applying neural network language models to weighted finite state transducers for automatic speech recognition |
US10942703B2 (en) | 2015-12-23 | 2021-03-09 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US11853647B2 (en) | 2015-12-23 | 2023-12-26 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US9558182B1 (en) | 2016-01-08 | 2017-01-31 | International Business Machines Corporation | Smart terminology marker system for a language translation system |
US9898460B2 (en) | 2016-01-26 | 2018-02-20 | International Business Machines Corporation | Generation of a natural language resource using a parallel corpus |
US20190065485A1 (en) * | 2016-04-04 | 2019-02-28 | Wovn Technologies, Inc. | Translation system |
US10878203B2 (en) * | 2016-04-04 | 2020-12-29 | Wovn Technologies, Inc. | Translation system |
US20170308526A1 (en) * | 2016-04-21 | 2017-10-26 | National Institute Of Information And Communications Technology | Compcuter Implemented machine translation apparatus and machine translation method |
US11227589B2 (en) | 2016-06-06 | 2022-01-18 | Apple Inc. | Intelligent list reading |
US11069347B2 (en) | 2016-06-08 | 2021-07-20 | Apple Inc. | Intelligent automated assistant for media exploration |
US11657820B2 (en) | 2016-06-10 | 2023-05-23 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US10733993B2 (en) | 2016-06-10 | 2020-08-04 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US11037565B2 (en) | 2016-06-10 | 2021-06-15 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US11749275B2 (en) | 2016-06-11 | 2023-09-05 | Apple Inc. | Application integration with a digital assistant |
US10942702B2 (en) | 2016-06-11 | 2021-03-09 | Apple Inc. | Intelligent device arbitration and control |
US10580409B2 (en) | 2016-06-11 | 2020-03-03 | Apple Inc. | Application integration with a digital assistant |
US11809783B2 (en) | 2016-06-11 | 2023-11-07 | Apple Inc. | Intelligent device arbitration and control |
US11152002B2 (en) | 2016-06-11 | 2021-10-19 | Apple Inc. | Application integration with a digital assistant |
US11875789B2 (en) | 2016-08-19 | 2024-01-16 | Google Llc | Language models using domain-specific model components |
US11557289B2 (en) | 2016-08-19 | 2023-01-17 | Google Llc | Language models using domain-specific model components |
US10832664B2 (en) | 2016-08-19 | 2020-11-10 | Google Llc | Automated speech recognition using language models that selectively use domain-specific model components |
US10474753B2 (en) | 2016-09-07 | 2019-11-12 | Apple Inc. | Language identification using recurrent neural networks |
US10553215B2 (en) | 2016-09-23 | 2020-02-04 | Apple Inc. | Intelligent automated assistant |
US10043516B2 (en) | 2016-09-23 | 2018-08-07 | Apple Inc. | Intelligent automated assistant |
US11281993B2 (en) | 2016-12-05 | 2022-03-22 | Apple Inc. | Model and ensemble compression for metric learning |
US10593346B2 (en) | 2016-12-22 | 2020-03-17 | Apple Inc. | Rank-reduced token representation for automatic speech recognition |
US11656884B2 (en) | 2017-01-09 | 2023-05-23 | Apple Inc. | Application integration with a digital assistant |
US11204787B2 (en) | 2017-01-09 | 2021-12-21 | Apple Inc. | Application integration with a digital assistant |
US10417266B2 (en) | 2017-05-09 | 2019-09-17 | Apple Inc. | Context-aware ranking of intelligent response suggestions |
US10741181B2 (en) | 2017-05-09 | 2020-08-11 | Apple Inc. | User interface for correcting recognition errors |
US10332518B2 (en) | 2017-05-09 | 2019-06-25 | Apple Inc. | User interface for correcting recognition errors |
US10847142B2 (en) | 2017-05-11 | 2020-11-24 | Apple Inc. | Maintaining privacy of personal information |
US10726832B2 (en) | 2017-05-11 | 2020-07-28 | Apple Inc. | Maintaining privacy of personal information |
US11599331B2 (en) | 2017-05-11 | 2023-03-07 | Apple Inc. | Maintaining privacy of personal information |
US11467802B2 (en) | 2017-05-11 | 2022-10-11 | Apple Inc. | Maintaining privacy of personal information |
US10755703B2 (en) | 2017-05-11 | 2020-08-25 | Apple Inc. | Offline personal assistant |
US10395654B2 (en) | 2017-05-11 | 2019-08-27 | Apple Inc. | Text normalization based on a data-driven learning network |
US11580990B2 (en) | 2017-05-12 | 2023-02-14 | Apple Inc. | User-specific acoustic models |
US11405466B2 (en) | 2017-05-12 | 2022-08-02 | Apple Inc. | Synchronization and task delegation of a digital assistant |
US10410637B2 (en) | 2017-05-12 | 2019-09-10 | Apple Inc. | User-specific acoustic models |
US11538469B2 (en) | 2017-05-12 | 2022-12-27 | Apple Inc. | Low-latency intelligent automated assistant |
US11301477B2 (en) | 2017-05-12 | 2022-04-12 | Apple Inc. | Feedback analysis of a digital assistant |
US11380310B2 (en) | 2017-05-12 | 2022-07-05 | Apple Inc. | Low-latency intelligent automated assistant |
US10791176B2 (en) | 2017-05-12 | 2020-09-29 | Apple Inc. | Synchronization and task delegation of a digital assistant |
US10789945B2 (en) | 2017-05-12 | 2020-09-29 | Apple Inc. | Low-latency intelligent automated assistant |
US11862151B2 (en) | 2017-05-12 | 2024-01-02 | Apple Inc. | Low-latency intelligent automated assistant |
US10482874B2 (en) | 2017-05-15 | 2019-11-19 | Apple Inc. | Hierarchical belief states for digital assistants |
US10810274B2 (en) | 2017-05-15 | 2020-10-20 | Apple Inc. | Optimizing dialogue policy decisions for digital assistants using implicit feedback |
US10403278B2 (en) | 2017-05-16 | 2019-09-03 | Apple Inc. | Methods and systems for phonetic matching in digital assistant services |
US10303715B2 (en) | 2017-05-16 | 2019-05-28 | Apple Inc. | Intelligent automated assistant for media exploration |
US10311144B2 (en) | 2017-05-16 | 2019-06-04 | Apple Inc. | Emoji word sense disambiguation |
US10909171B2 (en) | 2017-05-16 | 2021-02-02 | Apple Inc. | Intelligent automated assistant for media exploration |
US11532306B2 (en) | 2017-05-16 | 2022-12-20 | Apple Inc. | Detecting a trigger of a digital assistant |
US11675829B2 (en) | 2017-05-16 | 2023-06-13 | Apple Inc. | Intelligent automated assistant for media exploration |
US11217255B2 (en) | 2017-05-16 | 2022-01-04 | Apple Inc. | Far-field extension for digital assistant services |
US10748546B2 (en) | 2017-05-16 | 2020-08-18 | Apple Inc. | Digital assistant services based on device capabilities |
US10657328B2 (en) | 2017-06-02 | 2020-05-19 | Apple Inc. | Multi-task recurrent neural network architecture for efficient morphology handling in neural language modeling |
US10445429B2 (en) | 2017-09-21 | 2019-10-15 | Apple Inc. | Natural language understanding using vocabularies with compressed serialized tries |
US10755051B2 (en) | 2017-09-29 | 2020-08-25 | Apple Inc. | Rule-based natural language processing |
US10636424B2 (en) | 2017-11-30 | 2020-04-28 | Apple Inc. | Multi-turn canned dialog |
US10769386B2 (en) | 2017-12-05 | 2020-09-08 | Sap Se | Terminology proposal engine for determining target language equivalents |
US10733982B2 (en) | 2018-01-08 | 2020-08-04 | Apple Inc. | Multi-directional dialog |
US10733375B2 (en) | 2018-01-31 | 2020-08-04 | Apple Inc. | Knowledge-based framework for improving natural language understanding |
US11710003B2 (en) * | 2018-02-26 | 2023-07-25 | Tencent Technology (Shenzhen) Company Limited | Information conversion method and apparatus, storage medium, and electronic device |
US10789959B2 (en) | 2018-03-02 | 2020-09-29 | Apple Inc. | Training speaker recognition models for digital assistants |
US10592604B2 (en) | 2018-03-12 | 2020-03-17 | Apple Inc. | Inverse text normalization for automatic speech recognition |
US10818288B2 (en) | 2018-03-26 | 2020-10-27 | Apple Inc. | Natural assistant interaction |
US11710482B2 (en) | 2018-03-26 | 2023-07-25 | Apple Inc. | Natural assistant interaction |
US10909331B2 (en) | 2018-03-30 | 2021-02-02 | Apple Inc. | Implicit identification of translation payload with neural machine translation |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11487364B2 (en) | 2018-05-07 | 2022-11-01 | Apple Inc. | Raise to speak |
US11854539B2 (en) | 2018-05-07 | 2023-12-26 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11169616B2 (en) | 2018-05-07 | 2021-11-09 | Apple Inc. | Raise to speak |
US11900923B2 (en) | 2018-05-07 | 2024-02-13 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11907436B2 (en) | 2018-05-07 | 2024-02-20 | Apple Inc. | Raise to speak |
US10928918B2 (en) | 2018-05-07 | 2021-02-23 | Apple Inc. | Raise to speak |
US10984780B2 (en) | 2018-05-21 | 2021-04-20 | Apple Inc. | Global semantic word embeddings using bi-directional recurrent neural networks |
US10684703B2 (en) | 2018-06-01 | 2020-06-16 | Apple Inc. | Attention aware virtual assistant dismissal |
US10984798B2 (en) | 2018-06-01 | 2021-04-20 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US10720160B2 (en) | 2018-06-01 | 2020-07-21 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US11431642B2 (en) | 2018-06-01 | 2022-08-30 | Apple Inc. | Variable latency device coordination |
US11630525B2 (en) | 2018-06-01 | 2023-04-18 | Apple Inc. | Attention aware virtual assistant dismissal |
US11009970B2 (en) | 2018-06-01 | 2021-05-18 | Apple Inc. | Attention aware virtual assistant dismissal |
US11386266B2 (en) | 2018-06-01 | 2022-07-12 | Apple Inc. | Text correction |
US11360577B2 (en) | 2018-06-01 | 2022-06-14 | Apple Inc. | Attention aware virtual assistant dismissal |
US11495218B2 (en) | 2018-06-01 | 2022-11-08 | Apple Inc. | Virtual assistant operation in multi-device environments |
US10403283B1 (en) | 2018-06-01 | 2019-09-03 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US10892996B2 (en) | 2018-06-01 | 2021-01-12 | Apple Inc. | Variable latency device coordination |
US10504518B1 (en) | 2018-06-03 | 2019-12-10 | Apple Inc. | Accelerated task performance |
US10944859B2 (en) | 2018-06-03 | 2021-03-09 | Apple Inc. | Accelerated task performance |
US10496705B1 (en) | 2018-06-03 | 2019-12-03 | Apple Inc. | Accelerated task performance |
US20210019479A1 (en) * | 2018-09-05 | 2021-01-21 | Tencent Technology (Shenzhen) Company Limited | Text translation method and apparatus, storage medium, and computer device |
US11853709B2 (en) * | 2018-09-05 | 2023-12-26 | Tencent Technology (Shenzhen) Company Limited | Text translation method and apparatus, storage medium, and computer device |
US11010561B2 (en) | 2018-09-27 | 2021-05-18 | Apple Inc. | Sentiment prediction from textual data |
US10839159B2 (en) | 2018-09-28 | 2020-11-17 | Apple Inc. | Named entity normalization in a spoken dialog system |
US11462215B2 (en) | 2018-09-28 | 2022-10-04 | Apple Inc. | Multi-modal inputs for voice commands |
US11170166B2 (en) | 2018-09-28 | 2021-11-09 | Apple Inc. | Neural typographical error modeling via generative adversarial networks |
US11893992B2 (en) | 2018-09-28 | 2024-02-06 | Apple Inc. | Multi-modal inputs for voice commands |
US11475898B2 (en) | 2018-10-26 | 2022-10-18 | Apple Inc. | Low-latency multi-speaker speech recognition |
US10885279B2 (en) * | 2018-11-08 | 2021-01-05 | Microsoft Technology Licensing, Llc | Determining states of content characteristics of electronic communications |
US11638059B2 (en) | 2019-01-04 | 2023-04-25 | Apple Inc. | Content playback on multiple devices |
US11348573B2 (en) | 2019-03-18 | 2022-05-31 | Apple Inc. | Multimodality in digital assistant systems |
US11783815B2 (en) | 2019-03-18 | 2023-10-10 | Apple Inc. | Multimodality in digital assistant systems |
US11675491B2 (en) | 2019-05-06 | 2023-06-13 | Apple Inc. | User configurable task triggers |
US11307752B2 (en) | 2019-05-06 | 2022-04-19 | Apple Inc. | User configurable task triggers |
US11475884B2 (en) | 2019-05-06 | 2022-10-18 | Apple Inc. | Reducing digital assistant latency when a language is incorrectly determined |
US11423908B2 (en) | 2019-05-06 | 2022-08-23 | Apple Inc. | Interpreting spoken requests |
US11217251B2 (en) | 2019-05-06 | 2022-01-04 | Apple Inc. | Spoken notifications |
US11705130B2 (en) | 2019-05-06 | 2023-07-18 | Apple Inc. | Spoken notifications |
US11888791B2 (en) | 2019-05-21 | 2024-01-30 | Apple Inc. | Providing message response suggestions |
US11140099B2 (en) | 2019-05-21 | 2021-10-05 | Apple Inc. | Providing message response suggestions |
US11496600B2 (en) | 2019-05-31 | 2022-11-08 | Apple Inc. | Remote execution of machine-learned models |
US11360739B2 (en) | 2019-05-31 | 2022-06-14 | Apple Inc. | User activity shortcut suggestions |
US11289073B2 (en) | 2019-05-31 | 2022-03-29 | Apple Inc. | Device text to speech |
US11237797B2 (en) | 2019-05-31 | 2022-02-01 | Apple Inc. | User activity shortcut suggestions |
US11657813B2 (en) | 2019-05-31 | 2023-05-23 | Apple Inc. | Voice identification in digital assistant systems |
US11360641B2 (en) | 2019-06-01 | 2022-06-14 | Apple Inc. | Increasing the relevance of new available information |
US11790914B2 (en) | 2019-06-01 | 2023-10-17 | Apple Inc. | Methods and user interfaces for voice-based control of electronic devices |
US11488406B2 (en) | 2019-09-25 | 2022-11-01 | Apple Inc. | Text detection using global geometry estimators |
US11538576B2 (en) | 2019-10-15 | 2022-12-27 | International Business Machines Corporation | Illustrative medical imaging for functional prognosis estimation |
US20210150148A1 (en) * | 2019-11-20 | 2021-05-20 | Academia Sinica | Natural language processing method and computing apparatus thereof |
US11568151B2 (en) * | 2019-11-20 | 2023-01-31 | Academia Sinica | Natural language processing method and computing apparatus thereof |
US11914848B2 (en) | 2020-05-11 | 2024-02-27 | Apple Inc. | Providing relevant data items based on context |
US11924254B2 (en) | 2020-05-11 | 2024-03-05 | Apple Inc. | Digital assistant hardware abstraction |
US11765209B2 (en) | 2020-05-11 | 2023-09-19 | Apple Inc. | Digital assistant hardware abstraction |
US11755276B2 (en) | 2020-05-12 | 2023-09-12 | Apple Inc. | Reducing description length based on confidence |
US11838734B2 (en) | 2020-07-20 | 2023-12-05 | Apple Inc. | Multi-device audio adjustment coordination |
US11696060B2 (en) | 2020-07-21 | 2023-07-04 | Apple Inc. | User identification using headphones |
US11750962B2 (en) | 2020-07-21 | 2023-09-05 | Apple Inc. | User identification using headphones |
US11797781B2 (en) | 2020-08-06 | 2023-10-24 | International Business Machines Corporation | Syntax-based multi-layer language translation |
Also Published As
Publication number | Publication date |
---|---|
WO2012170817A1 (en) | 2012-12-13 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20120316862A1 (en) | Augmenting statistical machine translation with linguistic knowledge | |
Màrquez et al. | Semantic role labeling: an introduction to the special issue | |
Tiedemann | Recycling translations: Extraction of lexical data from parallel corpora and their application in natural language processing | |
Green et al. | A class-based agreement model for generating accurately inflected translations | |
Costa-Jussá et al. | Statistical machine translation enhancements through linguistic levels: A survey | |
Seddah et al. | Hard time parsing questions: Building a questionbank for french | |
Maučec et al. | Slavic languages in phrase-based statistical machine translation: a survey | |
Hlaing et al. | Improving neural machine translation with POS-tag features for low-resource language pairs | |
Farrús et al. | Study and correlation analysis of linguistic, perceptual and automatic machine translation evaluations | |
Ojha | English-bhojpuri smt system: Insights from the karaka model | |
US11216617B2 (en) | Methods, computer readable media, and systems for machine translation between Arabic and Arabic sign language | |
Tyers et al. | Developing prototypes for machine translation between two Sámi languages | |
Rosa | Automatic post-editing of phrase-based machine translation outputs | |
Bjerva | Multi-class animacy classification with semantic features | |
Hwang et al. | Improving statistical machine translation using shallow linguistic knowledge | |
Felice | Linguistic indicators for quality estimation of machine translations | |
Weller et al. | Munich-Edinburgh-Stuttgart submissions at WMT13: Morphological and syntactic processing for SMT | |
Gwinnup et al. | The AFRL-MITLL WMT16 News-Translation Task Systems | |
Farreús et al. | Study and correlation analysis of linguistic, perceptual, and automatic machine translation evaluations | |
Pilevar et al. | Persiansmt: A first attempt to english-persian statistical machine translation | |
Diab et al. | Semantic processing of semitic languages | |
Shilon | Transfer-based Machine Translation between morphologically-rich and resource-poor languages: The case of Hebrew and Arabic | |
Mareček | Merged bilingual trees based on Universal Dependencies in Machine Translation | |
Delmonte | Getting Past the Language Gap: Innovations in Machine Translation | |
Herrmann et al. | Linguistic structure in statistical machine translation |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:SULTAN, SOHA MOHSEN HASSAN;HALL, KEITH;SIGNING DATES FROM 20120608 TO 20120611;REEL/FRAME:028354/0110 |
|
STCB | Information on status: application discontinuation |
Free format text: ABANDONED -- FAILURE TO RESPOND TO AN OFFICE ACTION |