US8825563B1 - Semi-supervised and unsupervised generation of hash functions - Google Patents
Semi-supervised and unsupervised generation of hash functions Download PDFInfo
- Publication number
- US8825563B1 US8825563B1 US13/184,013 US201113184013A US8825563B1 US 8825563 B1 US8825563 B1 US 8825563B1 US 201113184013 A US201113184013 A US 201113184013A US 8825563 B1 US8825563 B1 US 8825563B1
- Authority
- US
- United States
- Prior art keywords
- training examples
- matrix
- constrained
- weight vector
- hash function
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G06N99/005—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L9/00—Cryptographic mechanisms or cryptographic arrangements for secret or secure communications; Network security protocols
- H04L9/32—Cryptographic mechanisms or cryptographic arrangements for secret or secure communications; Network security protocols including means for verifying the identity or authority of a user of the system or for message authentication, e.g. authorization, entity authentication, data integrity or data verification, non-repudiation, key authentication or verification of credentials
- H04L9/3236—Cryptographic mechanisms or cryptographic arrangements for secret or secure communications; Network security protocols including means for verifying the identity or authority of a user of the system or for message authentication, e.g. authorization, entity authentication, data integrity or data verification, non-repudiation, key authentication or verification of credentials using cryptographic hash functions
Definitions
- This specification relates to generating hash functions for use in nearest neighbor search.
- Nearest neighbor search identifies a group of items that are most similar to a given item in a feature space. Each of the items is represented by a feature vector in the feature space.
- One method for performing nearest neighbor search uses binary hashing to map the features of an item to a Hamming code in a Hamming space, i.e., a sequence of ones and zeros, and then compares the resulting Hamming codes.
- Good binary hashing functions map items that are similar in the original space to similar Hamming codes and map items that are dissimilar in the original space to dissimilar Hamming codes.
- a K-bit Hamming code is generated for an item using a sequence of K hash functions, each of which specifies the value of one bit in the Hamming code.
- K hash functions each of which specifies the value of one bit in the Hamming code.
- Various methods for example, Locality Sensitive Hashing, Shift-Invariant Kernel-based Hashing, and Spectral Hash have previously been used to determine appropriate hash functions.
- one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of storing a plurality of training examples and a plurality of constraints for the training examples, wherein, for one or more constrained training examples in the plurality of training examples, there is at least one constraint in the plurality of constraints, and the constraint either identifies a particular other training example in the plurality of training examples as a neighbor of the training example or identifies a particular other training example in the plurality of training examples as a non-neighbor of the training example; initializing a current constraint weight for each of the constraints to a respective initial value; generating an ordered sequence of hash functions, wherein each hash function defines a mapping of features of an item to a bit in a corresponding ordered sequence of bits, wherein the hash function for a particular bit determines the value of the particular bit according to a respective weight vector for the hash function, wherein generating the sequence of hash functions comprises sequentially determining the weight vector for each hash function, and
- a system of one or more computers can be configured to perform particular operations or actions by virtue of having software, firmware, hardware, or a combination of them installed on the system that in operation causes or cause the system to perform the actions.
- One or more computer programs can be configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.
- the accuracy measure is derived from a sum of the current constraint weights not satisfied by the weight vector subtracted from a sum of the current constraint weights satisfied by the weight vector.
- Determining the weight vector for the current hash function comprises determining an eigenvector for a matrix of the form X l S k X l T , wherein X l is a matrix of feature vectors of the constrained training examples and S k is a matrix of the current constraint weights.
- Determining the eigenvector comprises extracting a first eigenvector.
- Determining the eigenvector comprises determining an approximate eigenvector.
- the determined weight vector for the current hash function further maximizes a variance in projections resulting from applying the determined weight vector to the plurality of training examples.
- the plurality of training examples includes one or more unconstrained training examples for which there is no constraint in the plurality of constraints.
- Determining the weight vector for the current hash function further comprises, after the weight vector is determined, updating the plurality of training examples to be a residual calculated using the determined weight vector.
- the residual is calculated by removing a contribution of a subspace of the training examples spanned by the determined weight vector.
- the residual satisfies the equation X ⁇ w k w k T X, wherein X is a matrix of feature vectors of the plurality of training examples and w k is the determined weight vector for the current hash function.
- Determining the weight vector for the current hash function comprises determining an eigenvector for a matrix of the form X l S k X l T + ⁇ XX T , wherein X l is a matrix of feature vectors of the constrained training examples, S k is a matrix of the current constraint weights, and X is a matrix of feature vectors of the plurality of training examples.
- Determining the eigenvector comprises extracting a first eigenvector.
- Determining the eigenvector comprises determining an approximate eigenvector.
- Updating the current constraint weights comprises updating the current constraint weights to be S k ⁇ T( ⁇ tilde over (S) ⁇ k ,S k ), where S k is a matrix of the current constraint weights, ⁇ tilde over (S) ⁇ k measures a signed magnitude of pairwise relationships of projections of the one or more constrained training examples using the determined weight vector, and the function T is a truncated gradient of ⁇ tilde over (S) ⁇ k .
- ⁇ tilde over (S) ⁇ k X l T w k w k T X l , wherein X l is a matrix of feature vectors of the constrained training examples and w k is the determined weight vector for the current hash function.
- the actions further include performing a nearest neighbor search using the weight vectors stored in the data store.
- another innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of storing a plurality of constrained training examples, a plurality of first training examples, and a plurality of constraints for the constrained training examples, wherein, for each constrained training example, there is at least one constraint in the plurality of constraints, and the constraint either identifies a particular other constrained training example as a neighbor of the constrained training example or identifies a particular other constrained training example as a non-neighbor of the constrained training example, wherein the plurality of first training examples is initialized to include the constrained training examples and one or more unconstrained training examples for which there is no constraint in the plurality of constraints; generating an ordered sequence of hash functions, wherein each hash function defines a mapping of features of an item to a corresponding bit in an ordered sequence of bits, wherein the hash function for a particular bit determines the value of the particular bit according to a respective weight vector for the hash function, wherein generating the
- Determining the weight vector for the current hash function comprises determining an eigenvector for a matrix of the form X l S k X l T + ⁇ XX T , wherein X l is a matrix of feature vectors of the constrained training examples, S k is a matrix of the current constraint weights, and X is a matrix of feature vectors of the plurality of training examples.
- Determining the eigenvector comprises extracting a first eigenvector.
- Determining the eigenvector comprises determining an approximate eigenvector.
- the accuracy measure is further derived from a current constraint weight for each constraint, and determining the weight vector for the current hash function further comprises, after the weight vector is determined, updating the current constraint weights to increase a magnitude of the current constraint weight of any constraint not satisfied by the determined weight vector for the current hash function.
- the accuracy measure is derived from a sum of the current constraint weights not satisfied by the weight vector subtracted from a sum of the current constraint weights satisfied by the weight vector.
- Updating the current constraint weights comprises updating the current constraint weights to be S k ⁇ T( ⁇ tilde over (S) ⁇ k ,S k ), where S k is a matrix of the current constraint weights, ⁇ tilde over (S) ⁇ k measures a signed magnitude of pairwise relationships of projections of the one or more constrained training examples using the determined weight vector, and the function T is a truncated gradient of ⁇ tilde over (S) ⁇ k .
- ⁇ tilde over (S) ⁇ k X l T w k w k T X l , wherein X l is a matrix of feature vectors of the constrained training examples and w k is the determined weight vector for the current hash function.
- the actions further include performing a nearest neighbor search using the weight vectors stored in the data store.
- another innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of storing a plurality of constrained training examples, a plurality of first training examples, and a plurality of constraints for the constrained training examples, wherein, for each constrained training example, there is at least one constraint in the plurality of constraints, and the constraint either identifies a particular other constrained training example as a neighbor of the constrained training example or identifies a particular other constrained training example as a non-neighbor of the constrained training example, wherein the plurality of first training examples is initialized to include the constrained training examples and one or more non-constrained training examples for which there is no constraint in the plurality of constraints; generating an adjusted covariance matrix from the plurality of constrained training examples, the plurality of first training examples, and the plurality of constraints, wherein the adjusted covariance matrix is generated by adding an accuracy term and a regularizer term, wherein the accuracy term is derived from the constrained training examples and the constraints, and
- the accuracy term corresponds to an accuracy measure derived from a number of constraints satisfied by the weight vector, a number of constraints not satisfied by the weight vector, and a constraint weight for each constraint.
- the accuracy term is X l S k X l T
- the regularizer term is ⁇ XX T
- the adjusted covariance matrix is derived by adding the accuracy term and the regularizer term, wherein, wherein X l is a matrix of feature vectors of the constrained training examples, S k is a matrix of the constraint weights, and X is a matrix of feature vectors of the plurality of training examples.
- Generating an ordered sequence of hash functions from the adjusted covariance matrix comprises sequentially determining a weight vector for each hash function in the sequence of hash functions.
- Generating an ordered sequence of hash functions from the adjusted covariance matrix comprises selecting a weight vector for each hash function so that a matrix of the selected weight vectors W optimizes the equation
- the actions further include performing a nearest neighbor search using the ordered sequence of hash functions.
- each hash function defines a mapping of features of an item to a bit in a corresponding ordered sequence of bits, wherein the hash function for a particular bit determines the value of the particular bit according to a respective weight vector, wherein generating the hash function comprises sequentially determining the weight vectors for each hash function, and wherein determining the weight vector for a current hash function in the sequence after a weight vector for at least one previously determined hash function has been determined comprises: determining the weight vector for the current hash function, wherein the determined weight vector maximizes an accuracy measure derived from a respective set of constraints generated for all previously determined hash functions; selecting a plurality of pseudo-neighbor pairs of training examples and a plurality of pseudo-non-neighbor pair of training examples for use in determining the weight vector for a next hash function in the sequence,
- Determining the weight vector for the current hash function comprises determining an eigenvector for a matrix of the form
- Determining the eigenvector comprises determining an approximate eigenvector.
- the determined weight vector for the current hash function further maximizes a variance in projections resulting from applying the determined weight vector to the plurality of training examples.
- Determining the weight vector for the current hash function comprises determining an eigenvector for a matrix of the form
- S MC i is a matrix corresponding to the constraints generated from the weights for the hash function i and X MC i is a pseudo-constrained training example matrix that includes all of the training examples that are in either in the plurality of pseudo-neighbor pairs of training examples or the plurality of pseudo-non-neighbor pairs of training examples identified for the hash function i, and wherein X is a matrix of feature vectors for all of the training examples in the plurality of training examples.
- Determining the eigenvector comprises extracting a first eigenvector. Determining the eigenvector comprises determining an approximate eigenvector. Determining the weight vector for the current hash function further comprises, after the weight vector is determined, updating the plurality of training examples to a residual value, given the determined weight vector. The residual is calculated by removing a contribution of a subspace of the training examples spanned by the determined weight vector. The residual satisfies the equation X ⁇ w k w k T X, wherein X is a matrix of feature vectors of the plurality of training examples and w k is the determined weight vector for the current hash function.
- the actions further include performing a nearest neighbor search using the weight vectors stored in the data store.
- a sequence of hash functions can be generated using data dependent projections. This results in good discrimination from even short Hamming codes.
- Short Hamming codes can be used to represent items. This leads to fast retrieval and low storage requirements. The meaning of short is generally relative to the dimensions of the feature vectors of the training data. For example, if the each item in the training data is represented by a vector of 128 features, a Hamming code of 64 bits is short. However, short is defined differently for other dimensionalities.
- Hash functions can be generated in supervised, semi-supervised, and unsupervised training environments. Overfitting of hash functions can be avoided.
- FIG. 1 is a block diagram of an example system, including a semi-supervised training sub-system and a nearest neighbor search sub-system.
- FIG. 2 is a flow chart of an example method for generating a sequence of hash functions in a semi-supervised training environment.
- FIG. 3 is a flow chart of an example method for determining a weight vector for each hash function in an ordered sequence of hash functions.
- FIG. 4 is a flow chart of an example method for sequentially determining the weight vector for each hash function in an ordered sequence of hash functions.
- FIG. 5 is a block diagram of an example system, including an unsupervised training sub-system and a nearest neighbor search sub-system.
- FIG. 6 illustrates an example of a one-dimensional axis onto which the features of items are projected using a weight vector that was determined for a hash function.
- FIG. 7 is a flow chart of an example method for generating a sequence of hash functions in an unsupervised training environment.
- FIG. 1 is a block diagram of an example system 100 , including a semi-supervised training sub-system 102 and a nearest neighbor search sub-system 104 .
- the system 100 is implemented in one or more computers.
- the semi-supervised training system 102 generates an ordered sequence of hash functions 106 .
- the ordered sequence of hash functions 106 specifies a mapping from features of an item to a corresponding ordered sequence of bits, i.e., a sequence of ones and zeros.
- Each hash function in the sequence of hash functions 106 is represented by a weight vector that defines a mapping of features of an item to a single bit in the sequence of bits.
- the signum function sgn(•) returns ⁇ 1 if the argument is negative and 1 if the argument is positive.
- each hash function h k is defined by its weight vector w k .
- the above formulations of the hash function give values that are either 1 or ⁇ 1.
- the corresponding bit y k (x) for the hash function can be calculated from h k (x) as follows:
- the hash function can have more than two output values.
- the hash function is a continuous function, and the bit value is determined by comparing the output of the hash function to a predetermined threshold.
- the nearest neighbor search sub-system 104 uses the hash functions 106 to generate a hash values from the features of the items and then generates appropriate bits corresponding to the hash values.
- This binary hashing maps a feature vector of an item to a particular location in a Hamming space, so that close neighbors in the original space have similar codes in the Hamming space, e.g., similar strings of bits.
- the nearest neighbors, or approximate nearest neighbors can be efficiently determined by comparing the strings of bits to identify similar strings. For example, conventional methods for determining nearest neighbors or approximate nearest neighbors can be used.
- the nearest neighbor search performed by the nearest neighbor search sub-system can be used in various applications.
- the nearest neighbor search sub-system can identify a group of images that are estimated to have the most similar visual features to the features of a particular image.
- the nearest neighbor search can be used to identify a group of images estimated to be the most similar to an image input as an image query by a user.
- the items are images and the features are visual features, for example colors, shapes, lines, and edges.
- the nearest neighbor search sub-system 104 maps the features of each image to a Hamming space using the stored hash functions 106 and compares the representations of the image features in the Hamming space.
- the nearest neighbor search sub-system uses similarity of the images in the Hamming space as a surrogate for actual similarity.
- the nearest neighbor search sub-system can identify a group of videos that are estimated to have the most similar features to the features of a particular video. For example, if a user is currently viewing a particular video, the nearest neighbor search sub-system can be used to identify videos that have similar features, and these identified videos can be presented to the user.
- the items are videos and the features are video features, for example, visual features, motion features, and audio features.
- the nearest neighbor search sub-system 104 maps the features of each video to a Hamming space using the stored hash functions 106 and compares the representations of the video features in the Hamming space. Similarity of the features in the Hamming space is used as a surrogate for actual similarity.
- the nearest neighbor search sub-system can identify a group of documents that are estimated to have the most similar features to the features of a particular document. For example, if a user is currently viewing a particular document, the nearest neighbor search sub-system can be used to identify documents that have similar features, and these identified documents can be presented to the user. As another example, the nearest neighbor search sub-system can compare the features of documents to identify documents that have very similar features, and then only include one document from each group of documents with similar features in an index of documents. In these implementations, the items are documents and the features are document features, for example, textual features.
- the nearest neighbor search sub-system 104 maps the features of each document to a Hamming space using the stored hash functions 106 and compares the representations of the document features in the Hamming space. Similarity of the features in the Hamming space is used as a surrogate for actual similarity.
- the semi-supervised training sub-system 102 generates the ordered sequence of hash functions 106 from stored training data 108 and stored constraints on the training data 110 .
- the training data 108 is made up of training examples. Each training example corresponds to an item of a particular type. For example, each training example can be a feature vector corresponding to features of an item.
- all of the training examples correspond to items of the same type.
- the training examples can all correspond to images, all correspond to videos, all correspond to documents, all correspond to users, or all correspond to another type of data.
- the constraints 110 specify one or more constraints on the training data.
- Each constraint identifies a pair of training examples in the training data 108 as neighbors, or identifies a pair of training examples in the training data 108 as non-neighbors.
- Two training examples are neighbors when they are similar enough to each other to be considered related. For example, two training examples can be neighbors when they have both been assigned the same class label. For example, two images are neighbors if they are both labeled as being pictures of dogs.
- two training examples are neighbors when they are neighbors in a feature space, e.g., when they are less than a threshold distance away from each other in the feature space according to a distance measure, e.g., a metric or non-metric distance measure.
- the constraints can be derived from pairs of training examples known to be neighbors or non-neighbors, or from pairs of training examples having the same label or different labels.
- the constraints 110 do not necessarily include a constraint for every training example in the training data 108 .
- One or more of the training examples in the training data 108 can have no corresponding constraints in the constraints 110 .
- each training example for which there is at least one constraint in the constraints 110 will be referred to as a “constrained training example,” and each training example for which there is no constraint in the constraints 110 will be referred to as an “unconstrained training example.”
- the semi-supervised training sub-system 102 processes the training data 108 and the constraints 110 to generate the hash functions 106 using a hash function generator 112 .
- the hash function generator 112 generates each hash function in the sequence of hash functions as will be described in more detail below with reference to FIGS. 2-4 .
- FIG. 2 is a flow chart of an example method 200 for generating a sequence of hash functions in a semi-supervised training environment.
- the example method 200 is described in reference to a system that performs the method 200 .
- the system can be, for example, the semi-supervised training sub-system 102 , described above with reference to FIG. 1 .
- the system stores a plurality of training examples and constraints for the training examples ( 202 ), for example, as described above with reference to FIG. 1 .
- the system represents the training examples as a matrix X, where each column of X corresponds to the features of a particular training example.
- the constraints will be described as two groups of pairs of training example: M and C, where M is all pairs of training examples (X a , X b ) that are neighbors, and C is all pairs of training examples (X a , X b ) that are non-neighbors.
- the system generates an ordered sequence of hash functions by determining a weight vector for each hash function ( 204 ).
- Each hash function defines a mapping of features of an item to a bit in a corresponding sequence of bits, as described above with reference to FIG. 1 .
- Two example methods for determining the weight vectors are described in more detail below with reference to FIGS. 3 and 4 .
- the system stores the determined weight vectors for each hash function in a data store ( 206 ).
- FIG. 3 is a flow chart of an example method 300 for determining a weight vector for each hash function in an ordered sequence of hash functions.
- the example method 300 is described in reference to a system that performs the method 300 .
- the system can be, for example, the semi-supervised training sub-system 102 , described above with reference to FIG. 1 .
- the system generates an adjusted covariance matrix ( 302 ) for use in determining the weight vectors.
- S ij ⁇ 1 : ( x i , x j ) ⁇ M - 1 : ( x i , x j ) ⁇ C 0 : otherwise ,
- S ij is the ith row and the jth column of the matrix S
- X i and X j are the ith and jth training examples in the constrained training example matrix X l
- M is the set of pairs of training examples that are neighbors
- C is the set of pairs of training examples that are non-neighbors.
- the formulation of the adjusted covariance matrix given above is one in which X has been normalized to have a mean of zero.
- the system replaces the X term with (X ⁇ ), where ⁇ is the mean of the data represented by X.
- the adjusted covariance matrix has two components, an accuracy term (X l SX l T ) and a regularizer term ( ⁇ XX T ).
- the accuracy term reflects the empirical accuracy of the hash function over the constrained training examples.
- the accuracy term is derived from the constrained training examples and the constraints.
- the accuracy term is derived from a measure of the empirical accuracy of a hash function as follows.
- the empirical accuracy of a sequence of hash functions H is derived from the total number of correctly classified pairs minus the total number of wrongly classified pairs, summed over each bit of the hash function.
- the empirical accuracy can be calculated according to the following formula:
- J ⁇ ( H ) ⁇ k ⁇ ⁇ ⁇ ( x i , x j ) ⁇ M ⁇ h k ⁇ ( x i ) ⁇ h k ⁇ ( x j ) - ⁇ ( x i , x j ) ⁇ C ⁇ h k ⁇ ( x i ) ⁇ h k ⁇ ( x j ) ⁇
- M is the set of pairs of constrained training examples that are neighbors
- C is the set of pairs of constrained training examples that are non-neighbors
- h k (x) is given above.
- the objective function J(H) can be modified to replace the sgn( ) function with the signed magnitude, resulting in the following formula:
- J ⁇ ( W ) ⁇ k ⁇ ⁇ ⁇ ( x i , x j ) ⁇ M ⁇ w k T ⁇ x i ⁇ x j T ⁇ w k - ⁇ ( x i , x j ) ⁇ C ⁇ w k T ⁇ x i ⁇ x j T ⁇ w k ⁇ , where w k is the weight vector for the kth hash function.
- This modified form takes into account a desire that if two items x i and x j are similar, the product of their projections according to the weight vector w k should have a large value, e.g., greater than zero, and that if two items x i and x j are dissimilar, the product of their projections according to the weight vector w k should have a small value, e.g., less than zero.
- J(W) can be further rewritten as follows:
- X l is a matrix of the constrained training examples
- X is a matrix of all of the training examples
- S is a matrix of the constraints, defined as described above.
- This final formulation of J(W) is used to generate the accuracy term of the adjusted covariance matrix, described above.
- the regularizer term avoids overfitting of the hash function by giving preference to hash directions that maximize the variance of the projected data, e.g., maximize the variance in the projected values resulting from applying the weight vector to the training examples.
- the regularizer term is derived from both the constrained training examples and unconstrained training examples.
- the system derives the regularizer term from an estimate of the variance of the projections resulting when the weight vector is applied to the training examples, e.g.:
- E is an expected value function
- X is a matrix of all of the training examples
- n is the number of training examples in matrix X
- ⁇ is a positive constant chosen to determine the weight given to R(W) relative to the J(W).
- R ⁇ ( W ) ⁇ ⁇ ⁇ k ⁇ w k T ⁇ XX T ⁇ w k .
- the system generates a weight vector for each hash function in the sequence of hash functions from the adjusted covariance matrix ( 304 ).
- the weight vectors are chosen to maximize the accuracy over the constrained training examples and to maximize the variance of the hash function values over both the constrained and the unconstrained training examples.
- the system imposes the orthogonality constraints on the matrix of weight vectors and uses iterative methods to compute approximate eigenvectors for the matrix, rather than extracting the top K eigenvectors themselves.
- the system does not impose orthogonality constraints, and uses standard numerical optimization methods, for example, gradient descent, to optimize the combined equation Q(W) and determine the matrix of weight vectors W.
- standard numerical optimization methods for example, gradient descent
- the system sequentially determines the weight vectors, as described in more detail below with reference to FIG. 4 .
- FIG. 4 is a flow chart of an example method 400 for sequentially determining the weight vector for each hash function in an ordered sequence of hash functions.
- the example method 400 is described in reference to a system that performs the method 400 .
- the system can be, for example, the semi-supervised training sub-system 102 , described above with reference to FIG. 1 .
- the system initializes a current constraint weight for each constraint in a group of stored constraints to a respective initial value ( 402 ).
- the initial values are selected so as to distinguish between constraints that specify two training examples are neighbors and constraints that specify that two training examples are not neighbors. For example, each constraint that specifies that two training examples are neighbors can be initialized to a positive value, e.g., 1, and each constraint that specifies that two training examples are not neighbors can be initialized to a negative value, e.g., ⁇ 1.
- the system represents the current constraint weights as a current constraint weight matrix S.
- Each row of S corresponds to a constrained training example in a matrix of constrained training examples X l
- each column of S also corresponds to a constrained training example in the matrix X l .
- the current constraint weights in S are initialized according to the following formula:
- S ij ⁇ 1 : ( x i , x j ) ⁇ M - 1 : ( x i , x j ) ⁇ C 0 : otherwise , where S ij is the ith row and the jth column of the matrix S, and X i and X j are the ith and jth training examples in the constrained training example matrix X l . In some implementations S ii is always assigned a value of 0.
- each weight vector can be selected to minimize errors in classification given by the previously selected weight vectors.
- the system does this by adjusting the constraint weights to account for pairs of constrained training examples that are incorrectly classified by the previously determined weight vector, as described in more detail below.
- the system For each current hash function in the sequence of hash functions, the system does the following. First, the system determines a weight vector for the current hash function from the current constraint weights ( 404 ).
- the determined weight vector maximizes both an accuracy measure and a variance measure.
- the accuracy measure is derived from a number of constraints satisfied by the weight vector, the number of constraints violated by the weight vector, and the current constraint weight of the constraints.
- the variance is a variance of the values generated by the hash function when the hash function is applied to the constrained and unconstrained training examples.
- the system determines the weight vector for the current hash function by generating an adjusted covariance matrix using the current constraint weights, and then extracting the first eigenvector from the generated adjusted covariance matrix.
- the extracted eigenvector maximizes both the accuracy measure and the variance as a result of the structure of the adjusted covariance matrix.
- the system implicitly imposes orthogonality constraints on the weight matrix W.
- the system rather than extracting the first eigenvector from the adjusted covariance matrix, the system approximates the first eigenvector using an iterative technique. In implementations where the system does not impose orthogonality constraints, the system can alternatively use standard numerical optimization methods to solve for the weight vector.
- the system updates the current constraint weights for use in calculating a weight vector of the next hash function in the sequence of hash functions ( 406 ).
- the current constraint weights are updated to increase a magnitude of the current constraint weight of any constraint not satisfied by the determined weight vector for the current hash function.
- a constraint is not satisfied if the results of applying the hash function for the weight vector to the two constrained training examples lead to a different classification than the classification specified by the constraint.
- a constraint is not satisfied if the results of the current hash function applied to two training examples are different, and the constraint specifies that the pair of training examples are neighbors.
- a constraint is not satisfied if the results of the current hash function applied to two training examples is the same, and the constraint specifies that the pair of training examples are non-neighbors.
- T( ⁇ tilde over (S) ⁇ i,j k ,S i,j ) is derived as follows:
- T ⁇ ( S i , j k ⁇ , S i , j ) ⁇ ( S i , j k ⁇ : sgn ⁇ ( S i , j k ⁇ ⁇ S ij ) ⁇ 0 0 : sgn ⁇ ( S i , j k ⁇ ⁇ S ij ) ⁇ 0.
- the condition ⁇ tilde over (S) ⁇ i,j k ⁇ S ij ⁇ 0 for a pair (x i ,x j ) indicates that the weight vector for the current hash function does not satisfy a constraint on training examples x i and x j .
- the system computes the residual of the constrained and unconstrained training examples, using the determined weight vector for the current hash function ( 408 ).
- This updating of the constrained and unconstrained training examples in the matrix X removes the contribution of the subspace of constrained and unconstrained training examples spanned by the projection direction defined by w k .
- the system does not update both the residual and the current constraint weights.
- the system does not update the current constraint weights S.
- the system when the system is processing the last hash function in the sequence of hash functions, the system does not update the constraint weights or calculate the residual.
- FIG. 5 is a block diagram of an example system 500 , including an unsupervised training sub-system 502 and a nearest neighbor search sub-system 104 .
- the system 500 is implemented in one or more computers.
- the unsupervised training sub-system 502 includes a hash function generator 504 that generates hash functions 506 from training data 508 .
- the nearest neighbor search sub-system 104 uses the generated hash functions, for example, as described above with reference to FIG. 1 .
- the unsupervised training sub-system 502 stores only training data 508 and does not store any static constraints on the training data. Instead, the unsupervised hash function generator 504 generates constraints for use in generating a particular hash function from the previously generated hash functions. The unsupervised hash function generator 504 generates these constraints to reduce thresholding errors.
- FIG. 6 illustrates an example of a one-dimensional axis 602 onto which the features of items are projected using a weight vector that was determined for a hash function.
- the regions marked as r ⁇ and r+ are located very close to the boundary 604 , and regions marked as R ⁇ and R+ are located far away from the boundary 604 .
- the selection of the boundary 604 leads to possible thresholding errors, where items whose projections are close together are assigned different bits, or items whose projections are far apart are assigned the same bits. For example, points in r ⁇ and points in r+ are assigned different hash bits, even though their projections are quite close. Also, points in r ⁇ and R ⁇ and points in r+ and R+ are assigned the same hash bits, even though their projections are far apart.
- the unsupervised hash function generator determines a group of pseudo-neighbor pairs (M) and pseudo-non-neighbor pairs (C) resulting from the projection.
- the pseudo-neighbor pairs M are pairs of items that have different hash function values even though the projections for the items in the pair are close together.
- the pseudo-non-neighbor pairs C are pairs of items that have the same hash function value even though the projections for the items in the pair are far apart.
- the unsupervised hash function generator 504 generates the groups of pseudo-neighbor pairs M and the group of pseudo-non-neighbor pairs C
- the unsupervised hash function generator 504 generates constraints corresponding to the requirement that pairs in the group of pseudo-neighbor pairs M be neighbors and pairs in the group of pseudo-non-neighbor pairs C be non-neighbors.
- the unsupervised hash function generator 504 represents the constraints generated from hash function k as a constraint matrix S MC k , where the element s of S MC k at column i and row j is defined as follows:
- the unsupervised hash function generator 504 uses the constraints generated as described above to generate the hash functions 506 . This is described in more detail below with reference to FIG. 7 .
- FIG. 7 is a flow chart of an example method 700 for generating a sequence of hash functions in an unsupervised training environment.
- the example method 700 is described in reference to a system that performs the method 700 .
- the system can be, for example, the unsupervised training sub-system 502 , described above with reference to FIG. 5 .
- the system stores a plurality of training examples ( 702 ), for example, as described above with reference to FIGS. 1 and 5 .
- the system represents the training examples as a matrix X, where each column of X corresponds to the features of a particular training example.
- the system then sequentially determines the weight vector for each function in the sequence of hash functions. As part of this process, the system also generates a group of constraints for each hash function from pairs of pseudo-neighbor training examples and pseudo-non-neighbor training examples. The constraints from previously determined hash functions are used to determine the weight vectors for subsequent hash functions. This means that each weight vector is selected to minimize potential thresholding errors arising from the previously selected weight vectors.
- the system For each current hash function in the sequence of hash functions, the system does the following. First, the system determines a weight vector for the current hash function ( 704 ). The weight vector maximizes an accuracy measure derived from a group of constraints generated for all previously determined hash functions.
- the system determines the weight vector by generating an adjusted covariance matrix from the training examples and the previously determined constraints, and then extracting the first eigenvector, or generating an approximate eigenvector, of the adjusted covariance matrix.
- the adjusted covariance matrix can be generated according to the following formula:
- X MC i is a matrix corresponding to the constraints generated from the weights for the hash function i, for example, as described above with reference to FIG. 6 .
- X MC i is a pseudo-constrained training example matrix that includes all of the training examples that are in either in the group of pseudo-neighbor pairs of training examples M or the group of pseudo-non-neighbor pairs of training examples C used to generate the constraints for the hash function i.
- X is a matrix that includes all of the training examples, for example, as described above.
- ⁇ is a decay factor that is used to exponentially decay the contribution of old constraint matrices. ⁇ has a value that is less than 1.
- the system determines a weight vector that both maximizes an accuracy measure derived from a group of constraints generated for all previously determined hash functions and maximizes a variance measure derived from a variance in projections resulting from applying the determined weight vector to the training examples.
- the system generates an adjusted covariance matrix from the training examples and the previously determined constraints and all of the training examples, and then determines the weight vector by extracting the first eigenvector, or generating an approximate eigenvector, of the adjusted covariance matrix.
- the adjusted covariance matrix can be generated according to the following formula:
- the system can use a default group of constraints, for example, a set of no constraints, to determine the weight vector for the first hash function, for example, using the adjusted covariance matrix given above.
- the system can select the weight vector for the first hash function at random, for example, by randomly selecting a value for each weight in the weight vector.
- the system selects pairs of pseudo-neighbor training examples and pairs of pseudo-non-neighbor training examples according to the determined weight vector for the current hash function ( 706 ), for example, as described above with reference to FIG. 6 .
- the system generates a group of constraints for the current hash function to reflect the selected pairs of pseudo-neighbor training examples and the selected pairs of pseudo-non-neighbor training examples, for example, as described above with reference to FIG. 6 .
- the system does not explicitly store the individual constraint matrices and pseudo-constrained training matrices, i.e., the matrices having subscripts or superscripts 0 through k ⁇ 1 that are used to calculate the hash function and matrices for iteration k. In these implementations, the system instead does incremental updates at the end of each iteration.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a non-transitory computer storage medium for execution by, or to control the operation of, data processing apparatus.
- the program instructions can be encoded on a propagated signal that is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- the computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.
- data processing apparatus encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- the apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code).
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read-only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing or executing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a universal serial bus (USB) flash drive), to name just a few.
- PDA personal digital assistant
- GPS Global Positioning System
- USB universal serial bus
- Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto-optical disks e.g., CD-ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Abstract
Description
where Xl is a matrix of feature vectors of the constrained training examples, S is a matrix of constraint weights, and X is a matrix of feature vectors of the plurality of training examples.
wherein for each value of i, SMC i is a matrix corresponding to the constraints generated from the weights for the hash function i and XMC i is a pseudo-constrained training example matrix that includes all of the training examples that are in either in the plurality of pseudo-neighbor pairs of training examples or the plurality of pseudo-non-neighbor pairs of training examples identified for the hash function i. Determining the eigenvector comprises extracting a first eigenvector. Determining the eigenvector comprises determining an approximate eigenvector. The determined weight vector for the current hash function further maximizes a variance in projections resulting from applying the determined weight vector to the plurality of training examples. Determining the weight vector for the current hash function comprises determining an eigenvector for a matrix of the form
wherein for each value of i, SMC i is a matrix corresponding to the constraints generated from the weights for the hash function i and XMC i is a pseudo-constrained training example matrix that includes all of the training examples that are in either in the plurality of pseudo-neighbor pairs of training examples or the plurality of pseudo-non-neighbor pairs of training examples identified for the hash function i, and wherein X is a matrix of feature vectors for all of the training examples in the plurality of training examples. Determining the eigenvector comprises extracting a first eigenvector. Determining the eigenvector comprises determining an approximate eigenvector. Determining the weight vector for the current hash function further comprises, after the weight vector is determined, updating the plurality of training examples to a residual value, given the determined weight vector. The residual is calculated by removing a contribution of a subspace of the training examples spanned by the determined weight vector. The residual satisfies the equation X−wkwk TX, wherein X is a matrix of feature vectors of the plurality of training examples and wk is the determined weight vector for the current hash function. The actions further include performing a nearest neighbor search using the weight vectors stored in the data store.
M=X l SX l T +ηXX T,
where Xl is a matrix of the constrained training examples, X is a matrix of all of the training examples, η is a scaling factor, described below, and S is a matrix of the constraints, defined as follows:
where Sij is the ith row and the jth column of the matrix S, and Xi and Xj are the ith and jth training examples in the constrained training example matrix Xl, M is the set of pairs of training examples that are neighbors, and C is the set of pairs of training examples that are non-neighbors.
where M is the set of pairs of constrained training examples that are neighbors, C is the set of pairs of constrained training examples that are non-neighbors, and hk(x) is given above.
where wk is the weight vector for the kth hash function. This modified form takes into account a desire that if two items xi and xj are similar, the product of their projections according to the weight vector wk should have a large value, e.g., greater than zero, and that if two items xi and xj are dissimilar, the product of their projections according to the weight vector wk should have a small value, e.g., less than zero.
where Xl is a matrix of the constrained training examples, X is a matrix of all of the training examples, and S is a matrix of the constraints, defined as described above. This final formulation of J(W) is used to generate the accuracy term of the adjusted covariance matrix, described above.
where E is an expected value function, X is a matrix of all of the training examples, n is the number of training examples in matrix X, and β is a positive constant chosen to determine the weight given to R(W) relative to the J(W).
M=x l SX l T +ηXX T,
can be derived from the combined equation.
where Sij is the ith row and the jth column of the matrix S, and Xi and Xj are the ith and jth training examples in the constrained training example matrix Xl. In some implementations Sii is always assigned a value of 0.
M k =X l S k X l T +ηXX T,
the extracted eigenvector maximizes both the accuracy measure and the variance as a result of the structure of the adjusted covariance matrix. In these implementations, the system implicitly imposes orthogonality constraints on the weight matrix W.
S k+1 =S k −αT({tilde over (S)} k ,S k),
where Sk+1 are the updated constraint weights, Sk are the current constraint weights, {tilde over (S)}k measures the signed magnitude of pairwise relationships of the projections of Xl using the weight vector wk, and the function T is the truncated gradient of {tilde over (S)}k.
S i,j =S i,j −α{tilde over (S)} i,j k,
where α is a step size. In some implementations, α is chosen so that a is less than one divided by β, where β=maxi∥xi∥2. This ensures that α{tilde over (S)}i,j k<1, leading to numerically stable updates.
X=X−w k w k T X.
M={(x i ,x j)}: h(x i)·h(x j)=−1,|w T(x i −x j)|≦ε
C={(x i ,x j)}: h(x i)·h(x j)=1,|w T(x i −x j)|≧ζ,
where ε and ζ are constants that define the regions described above.
where SMC i is a matrix corresponding to the constraints generated from the weights for the hash function i, for example, as described above with reference to
where SMC i, XMC i, and λ have the values described above, and η has the value described above with reference to
X=X−w k w k T X,
as described in more detail above with reference to
Claims (33)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/184,013 US8825563B1 (en) | 2010-05-07 | 2011-07-15 | Semi-supervised and unsupervised generation of hash functions |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US33260310P | 2010-05-07 | 2010-05-07 | |
US13/103,992 US8510236B1 (en) | 2010-05-07 | 2011-05-09 | Semi-supervised and unsupervised generation of hash functions |
US13/184,013 US8825563B1 (en) | 2010-05-07 | 2011-07-15 | Semi-supervised and unsupervised generation of hash functions |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/103,992 Continuation US8510236B1 (en) | 2010-05-07 | 2011-05-09 | Semi-supervised and unsupervised generation of hash functions |
Publications (1)
Publication Number | Publication Date |
---|---|
US8825563B1 true US8825563B1 (en) | 2014-09-02 |
Family
ID=48916746
Family Applications (4)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/103,992 Active 2032-02-13 US8510236B1 (en) | 2010-05-07 | 2011-05-09 | Semi-supervised and unsupervised generation of hash functions |
US13/184,014 Active 2032-02-01 US8583567B1 (en) | 2010-05-07 | 2011-07-15 | Semi-supervised and unsupervised generation of hash functions |
US13/183,939 Active 2033-04-04 US8924339B1 (en) | 2010-05-07 | 2011-07-15 | Semi-supervised and unsupervised generation of hash functions |
US13/184,013 Active 2032-09-21 US8825563B1 (en) | 2010-05-07 | 2011-07-15 | Semi-supervised and unsupervised generation of hash functions |
Family Applications Before (3)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/103,992 Active 2032-02-13 US8510236B1 (en) | 2010-05-07 | 2011-05-09 | Semi-supervised and unsupervised generation of hash functions |
US13/184,014 Active 2032-02-01 US8583567B1 (en) | 2010-05-07 | 2011-07-15 | Semi-supervised and unsupervised generation of hash functions |
US13/183,939 Active 2033-04-04 US8924339B1 (en) | 2010-05-07 | 2011-07-15 | Semi-supervised and unsupervised generation of hash functions |
Country Status (1)
Country | Link |
---|---|
US (4) | US8510236B1 (en) |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10127567B2 (en) | 2015-09-25 | 2018-11-13 | The Nielsen Company (Us), Llc | Methods and apparatus to apply household-level weights to household-member level audience measurement data |
US20210334647A1 (en) * | 2020-04-26 | 2021-10-28 | EMC IP Holding Company LLC | Method, electronic device, and computer program product for determining output of neural network |
Families Citing this family (15)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6210406B1 (en) | 1998-12-03 | 2001-04-03 | Cordis Webster, Inc. | Split tip electrode catheter and signal processing RF ablation system |
US8510236B1 (en) | 2010-05-07 | 2013-08-13 | Google Inc. | Semi-supervised and unsupervised generation of hash functions |
US9146987B2 (en) * | 2013-06-04 | 2015-09-29 | International Business Machines Corporation | Clustering based question set generation for training and testing of a question and answer system |
US9230009B2 (en) | 2013-06-04 | 2016-01-05 | International Business Machines Corporation | Routing of questions to appropriately trained question and answer system pipelines using clustering |
WO2015065435A1 (en) | 2013-10-31 | 2015-05-07 | Hewlett-Packard Development Company, L.P. | Storing time series data for a search query |
US9348900B2 (en) | 2013-12-11 | 2016-05-24 | International Business Machines Corporation | Generating an answer from multiple pipelines using clustering |
CN104978729A (en) * | 2014-04-08 | 2015-10-14 | 华中科技大学 | Image hashing method based on data sensing |
WO2015167562A1 (en) | 2014-04-30 | 2015-11-05 | Hewlett-Packard Development Company, L.P. | Using local memory nodes of a multicore machine to process a search query |
RU2632133C2 (en) | 2015-09-29 | 2017-10-02 | Общество С Ограниченной Ответственностью "Яндекс" | Method (versions) and system (versions) for creating prediction model and determining prediction model accuracy |
CN106095811B (en) * | 2016-05-31 | 2018-11-27 | 天津中科智能识别产业技术研究院有限公司 | A kind of image search method of the discrete Hash of supervision based on optimum code |
CN105868743B (en) * | 2016-05-31 | 2018-11-27 | 天津中科智能识别产业技术研究院有限公司 | It is a kind of based on the face retrieval method for quickly supervising discrete Hash |
US9594741B1 (en) * | 2016-06-12 | 2017-03-14 | Apple Inc. | Learning new words |
RU2693324C2 (en) | 2017-11-24 | 2019-07-02 | Общество С Ограниченной Ответственностью "Яндекс" | Method and a server for converting a categorical factor value into its numerical representation |
CN109639739B (en) * | 2019-01-30 | 2020-05-19 | 大连理工大学 | Abnormal flow detection method based on automatic encoder network |
CN113407661B (en) * | 2021-08-18 | 2021-11-26 | 鲁东大学 | Discrete hash retrieval method based on robust matrix decomposition |
Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20080037877A1 (en) * | 2006-08-14 | 2008-02-14 | Microsoft Corporation | Automatic classification of objects within images |
US20080133496A1 (en) * | 2006-12-01 | 2008-06-05 | International Business Machines Corporation | Method, computer program product, and device for conducting a multi-criteria similarity search |
US7617231B2 (en) * | 2005-12-07 | 2009-11-10 | Electronics And Telecommunications Research Institute | Data hashing method, data processing method, and data processing system using similarity-based hashing algorithm |
US20100332812A1 (en) * | 2009-06-24 | 2010-12-30 | Doug Burger | Method, system and computer-accessible medium for low-power branch prediction |
US20120143853A1 (en) * | 2010-12-03 | 2012-06-07 | Xerox Corporation | Large-scale asymmetric comparison computation for binary embeddings |
US8510236B1 (en) | 2010-05-07 | 2013-08-13 | Google Inc. | Semi-supervised and unsupervised generation of hash functions |
-
2011
- 2011-05-09 US US13/103,992 patent/US8510236B1/en active Active
- 2011-07-15 US US13/184,014 patent/US8583567B1/en active Active
- 2011-07-15 US US13/183,939 patent/US8924339B1/en active Active
- 2011-07-15 US US13/184,013 patent/US8825563B1/en active Active
Patent Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7617231B2 (en) * | 2005-12-07 | 2009-11-10 | Electronics And Telecommunications Research Institute | Data hashing method, data processing method, and data processing system using similarity-based hashing algorithm |
US20080037877A1 (en) * | 2006-08-14 | 2008-02-14 | Microsoft Corporation | Automatic classification of objects within images |
US20080133496A1 (en) * | 2006-12-01 | 2008-06-05 | International Business Machines Corporation | Method, computer program product, and device for conducting a multi-criteria similarity search |
US20100332812A1 (en) * | 2009-06-24 | 2010-12-30 | Doug Burger | Method, system and computer-accessible medium for low-power branch prediction |
US8510236B1 (en) | 2010-05-07 | 2013-08-13 | Google Inc. | Semi-supervised and unsupervised generation of hash functions |
US8583567B1 (en) | 2010-05-07 | 2013-11-12 | Google Inc. | Semi-supervised and unsupervised generation of hash functions |
US20120143853A1 (en) * | 2010-12-03 | 2012-06-07 | Xerox Corporation | Large-scale asymmetric comparison computation for binary embeddings |
Non-Patent Citations (5)
Title |
---|
Fergus et al., "Semi-supervised Learning in Gigantic Image Collections," Advances in Neural Information Processing Systems (NIPS), 2006, 1-9. |
Gionis et al., Similarity search in high dimensions via hashing, In Proc. of 25th VLDB, 1999. * |
Pinto et al., "Feature Selection on the Web People Search task ," CONACYT #106625, 2009, 9 pages. |
Torralba et al., Small codes and large image databases for recognition, In Proc. of CVPR, pp. 1-8, 2008. * |
Wang et al., "Sequential Projection Learning for Hashing with Compact Codes", Jun. 2010, International Conference of Machine Learning, pp. 1-8. * |
Cited By (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10127567B2 (en) | 2015-09-25 | 2018-11-13 | The Nielsen Company (Us), Llc | Methods and apparatus to apply household-level weights to household-member level audience measurement data |
US10853824B2 (en) | 2015-09-25 | 2020-12-01 | The Nielsen Company (Us), Llc | Methods and apparatus to apply household-level weights to household-member level audience measurement data |
US11687953B2 (en) | 2015-09-25 | 2023-06-27 | The Nielsen Company (Us), Llc | Methods and apparatus to apply household-level weights to household-member level audience measurement data |
US20210334647A1 (en) * | 2020-04-26 | 2021-10-28 | EMC IP Holding Company LLC | Method, electronic device, and computer program product for determining output of neural network |
Also Published As
Publication number | Publication date |
---|---|
US8510236B1 (en) | 2013-08-13 |
US8583567B1 (en) | 2013-11-12 |
US8924339B1 (en) | 2014-12-30 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8825563B1 (en) | Semi-supervised and unsupervised generation of hash functions | |
US11829880B2 (en) | Generating trained neural networks with increased robustness against adversarial attacks | |
US20180260414A1 (en) | Query expansion learning with recurrent networks | |
CN111279362B (en) | Capsule neural network | |
US8831358B1 (en) | Evaluating image similarity | |
CN108496189B (en) | Method, system, and storage medium for regularizing machine learning models | |
US20220076136A1 (en) | Method and system for training a neural network model using knowledge distillation | |
US10489688B2 (en) | Personalized digital image aesthetics in a digital medium environment | |
US10762283B2 (en) | Multimedia document summarization | |
US20190377987A1 (en) | Discriminative Caption Generation | |
US20160140425A1 (en) | Method and apparatus for image classification with joint feature adaptation and classifier learning | |
US20160253596A1 (en) | Geometry-directed active question selection for question answering systems | |
US20170249340A1 (en) | Image clustering system, image clustering method, non-transitory storage medium storing thereon computer-readable image clustering program, and community structure detection system | |
CN110659740A (en) | Ordering and updating machine learning models based on data input at edge nodes | |
US20130204905A1 (en) | Remapping locality-sensitive hash vectors to compact bit vectors | |
US8386490B2 (en) | Adaptive multimedia semantic concept classifier | |
US11475059B2 (en) | Automated image retrieval with graph neural network | |
JP6848091B2 (en) | Information processing equipment, information processing methods, and programs | |
CN108334640A (en) | A kind of video recommendation method and device | |
US20200293873A1 (en) | Generating vector representations of documents | |
US20190347359A1 (en) | Search system for providing web crawling query prioritization based on classification operation performance | |
US20230376764A1 (en) | System and method for increasing efficiency of gradient descent while training machine-learning models | |
US20220374498A1 (en) | Data processing method of detecting and recovering missing values, outliers and patterns in tensor stream data | |
US10438131B1 (en) | Spherical random features for polynomial kernels | |
US9870199B2 (en) | Generating compact representations of high-dimensional data |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:KUMAR, SANJIV;WANG, JUN;REEL/FRAME:026675/0864Effective date: 20110524 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044277/0001Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551)Year of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |