CN112187659A - Method for mitigating hash correlation in a multi-path network - Google Patents
Method for mitigating hash correlation in a multi-path network Download PDFInfo
- Publication number
- CN112187659A CN112187659A CN202011064670.1A CN202011064670A CN112187659A CN 112187659 A CN112187659 A CN 112187659A CN 202011064670 A CN202011064670 A CN 202011064670A CN 112187659 A CN112187659 A CN 112187659A
- Authority
- CN
- China
- Prior art keywords
- group
- switch
- mapping
- new
- hash
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000034 method Methods 0.000 title claims abstract description 47
- 230000000116 mitigating effect Effects 0.000 title abstract description 9
- 238000013507 mapping Methods 0.000 claims abstract description 62
- 230000015654 memory Effects 0.000 claims description 63
- 230000006870 function Effects 0.000 claims description 57
- 230000008569 process Effects 0.000 description 14
- 238000010586 diagram Methods 0.000 description 11
- 230000008859 change Effects 0.000 description 3
- 235000008694 Humulus lupulus Nutrition 0.000 description 2
- 238000004891 communication Methods 0.000 description 2
- 238000012545 processing Methods 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000009827 uniform distribution Methods 0.000 description 2
- 241001522296 Erithacus rubecula Species 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 238000004422 calculation algorithm Methods 0.000 description 1
- 230000001186 cumulative effect Effects 0.000 description 1
- 125000004122 cyclic group Chemical group 0.000 description 1
- 238000005315 distribution function Methods 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000003362 replicative effect Effects 0.000 description 1
- 238000005096 rolling process Methods 0.000 description 1
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L47/00—Traffic control in data switching networks
- H04L47/10—Flow control; Congestion control
- H04L47/12—Avoiding congestion; Recovering from congestion
- H04L47/125—Avoiding congestion; Recovering from congestion by balancing the load, e.g. traffic engineering
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L45/00—Routing or path finding of packets in data switching networks
- H04L45/74—Address processing for routing
- H04L45/745—Address table lookup; Address filtering
- H04L45/7453—Address table lookup; Address filtering using hashing
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L45/00—Routing or path finding of packets in data switching networks
- H04L45/12—Shortest path evaluation
- H04L45/125—Shortest path evaluation based on throughput or bandwidth
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L45/00—Routing or path finding of packets in data switching networks
- H04L45/24—Multipath
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L45/00—Routing or path finding of packets in data switching networks
- H04L45/54—Organization of routing tables
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L45/00—Routing or path finding of packets in data switching networks
- H04L45/70—Routing based on monitoring results
Abstract
Methods for mitigating hash correlation in a multipath network are provided. In this regard, a hash correlation may be found between a first switch and a second switch in a network. In this network, a first egress port will be selected at a first switch from among a first set of egress ports for forwarding packets, and a second egress port will be selected at a second switch from among a second set of egress ports for forwarding packets, where the first set has a first set size and the second set has a second set size. Upon finding the hash correlation, a new second group size may be selected that is coprime to the first group size, and the second group of egress ports may be mapped to a mapping group having the new second group size. The second switch may be configured to route the packet according to the mapping group.
Description
Technical Field
The present disclosure relates to a method for mitigating hash correlation in a multipath network.
Background
Load balancing is critical in network operation and management. For example, with equal cost multi-path (ECMP) routing, packets forwarded to a single destination may occur over multiple lowest cost paths based on a hash of the packet's header fields, which allows bandwidth over multiple paths to be used. In such a system, packets may be forwarded in multiple flows, and load balancing may be achieved by hashing one or more header fields so that packets in each flow follow the same path to avoid packet reordering. Likewise, in addition to being able to assign different weights to multiple paths, weighted cost multi-path (WCMP) routing also allows packets to be forwarded to a single destination on multiple paths, making some paths more likely to be selected than others.
One challenging problem with hash-based load balancing is that reusing the same or related hash functions in different routers can cause load imbalances. For example, when different switches repeatedly use the same hashing algorithm, a single link may be selected for all traffic to one destination prefix, while other links are underutilized or unused. One way to avoid hash correlation is to use a different hash function for each switch in the network. However, the switch chip supports a limited number of hash functions. In addition, a computation using a large number of hash functions is impractical because it may create a bottleneck at high packet rates. Another way to mitigate hash correlation is to manipulate several hash functions on a per-particular switch basis to obtain more variation. However, this operation may not be applicable to an odd number of equal cost paths due to the rolling nature of Cyclic Redundancy Checks (CRCs), or when randomness is reserved for other network management. Providing different seeds to the switch chip hash function may alleviate but not solve the hash correlation problem. Yet another way to mitigate hash correlation is to select a hash function based on the value of the time-to-live (TTL) in the packet header. However, this approach requires changes to the packet processing pipeline in the data plane and thus may require hardware changes and vendor cooperation.
Disclosure of Invention
The present disclosure provides: determining, by one or more processors, that a hash correlation exists between a first switch in a network and a second switch in the network, wherein a first egress port is to be selected at the first switch from among a first set of egress ports for forwarding packets, a second egress port is to be selected at the second switch from among a second set of egress ports for forwarding packets, and the first set has a first set size and the second set has a second set size; upon determining that hash correlations exist, selecting, by the one or more processors, a new second set of sizes that are coprime to the first set of sizes; mapping, by the one or more processors, a second set of egress ports to a mapping set having a new second set size; and configuring, by the one or more processors, the second switch to route the packet according to the mapping group.
Determining that a hash correlation exists may be based on determining that a first hash function applied at the first switch is the same as a second hash function applied at the second group, and that the second group is reachable by the first group.
The second group may be an ECMP group and the new second group size may be selected further based on a covariance of the ECMP group satisfying a predetermined threshold.
The second group may be a WCMP group and the new second group size may be selected further based on K-S statistics of the WCMP group satisfying a predetermined threshold.
The new second set of sizes may be selected further based on memory capacity to satisfy routing tables in the network.
The method may further comprise: determining, by the one or more processors, that a first memory usage by the first switch for the first group is greater than a second memory usage by the second switch for the second group, wherein selecting the new second group size is based on the first memory usage being greater than the second memory usage.
The method may further comprise: determining, by the one or more processors, that a first memory usage by the first switch for the first group is less than a second memory usage by the second switch for the second group, wherein instead of selecting and mapping to a mapping group a new second group size is selected for mapping to the first group.
The second group may be a WCMP group and mapping the second group of egress ports to a mapping group having the new second group size may be further based on routing weights assigned to the second group of egress ports. The method may further comprise: receiving, by the one or more processors, telemetry data for a network; updating, by the one or more processors, routing weights to a second set of egress ports based on the received telemetry data; and remapping, by the one or more processors, the second group of egress ports based on the updated routing weights.
The present disclosure further provides a system comprising one or more processors. The one or more processors are configured to: determining that a hash correlation exists between a first switch in the network and a second switch in the network, wherein a first egress port is to be selected at the first switch from among a first set of egress ports for forwarding packets, a second egress port is to be selected at the second switch from among a second set of egress ports for forwarding packets, and the first set has a first set size and the second set has a second set size; upon determining that hash correlation exists, selecting a new second set of sizes that are coprime to the first set of sizes; mapping the second set of egress ports to a mapping set having a new second set size; and configuring the second switch to route the packet according to the mapping group.
Determining that a hash correlation exists may be based on determining that a first hash function applied at the first switch is the same as a second hash function applied at the second group, and that the second group is reachable by the first group.
The second group may be an ECMP group and the new second group size may be selected further based on the covariance of the ECMP group satisfying a predetermined threshold.
The second group may be a WCMP group, and a new second group size may be selected further based on K-S statistics of the WCMP group satisfying a predetermined threshold.
The new second set of sizes may be selected further based on memory capacity that satisfies routing tables in the network.
The one or more processors may be further configured to determine that a first memory usage by the first switch for the first group is greater than a second memory usage by the second switch for the second group, wherein selecting the new second group size is based on the first memory usage being greater than the second memory usage.
The one or more processors may be further configured to determine that a first memory usage rate by the first switch for the first group is less than a second memory usage rate by the second switch for the second group, wherein instead of selecting and mapping to a mapping group a new second group size is selected for mapping the first group to the first mapping group.
The second group may be a WCMP group and mapping the second group of egress ports to a mapping group having the new second group size may be further based on routing weights assigned to the second group of egress ports. The one or more processors may be further configured to: receiving telemetry data of a network; updating routing weights to a second set of egress ports based on the received telemetry data; and remapping the second set of egress ports based on the updated routing weights.
The present disclosure still further provides a non-transitory computer-readable storage medium storing instructions executable by one or more processors to perform a method. The method comprises the following steps: determining that a hash correlation exists between a first switch in the network and a second switch in the network, wherein a first egress port is to be selected at the first switch from among a first set of egress ports for forwarding packets, a second egress port is to be selected at the second switch from among a second set of egress ports for forwarding packets, and the first set has a first set size and the second set has a second set size; upon determining that hash correlation exists, selecting a new second set of sizes that are coprime to the first set of sizes; mapping a second set of egress ports to a mapping set having the new second set of sizes; and configuring the second switch to route the packet according to the mapping group.
Determining that a hash correlation exists may be based on determining that a first hash function applied at the first switch is the same as a second hash function applied at the second group, and that the second group is reachable by the first group.
Drawings
Fig. 1A and 1B illustrate routing through an example multipath network, according to aspects of the present disclosure.
Fig. 2 illustrates an example system configured to mitigate hash correlation in accordance with aspects of the present disclosure.
FIG. 3 is a pictorial diagram illustrating an example process for mitigating hash correlation, in accordance with aspects of the present disclosure.
Fig. 4 illustrates an example diagram for determining hash correlations in accordance with aspects of the present disclosure.
Fig. 5A and 5B show exemplary diagrams of parameters for co-prime selection of WCMP networks, according to aspects of the present disclosure.
FIG. 6 illustrates an example coprime selection for reducing memory usage in accordance with aspects of the present disclosure.
Fig. 7A and 7B illustrate example route mappings for WCMP networks, in accordance with aspects of the present disclosure.
FIG. 8 is a pictorial diagram illustrating an example alternative process for mitigating hash correlation, in accordance with aspects of the present disclosure.
Fig. 9 is a flow diagram of an example method in accordance with aspects of the present disclosure.
Detailed Description
Techniques generally relate to mitigating hash correlation in a multipath network. For example, to route packets through a multipath network, a first egress port may be selected among a first group of egress ports at a first switch and a second egress port may be selected among a second group of egress ports at a second switch, where the first group has a first group size and the second group has a second group size. Each such selection may be performed by applying a hash function and then a modulo function based on the corresponding group size. In this regard, the presence of hash correlations at two switches may result in the same path being selected by both switches, leaving other paths between the two switches unused.
To address the above-described problem, a network controller may be configured to determine whether hash correlations exist between switches in a network. For example, if a first hash function applied at a first switch and a second hash function applied at a second switch are the same, and if the second group is reachable by the first group, hash correlation may exist. Upon finding the hash correlation, the network controller may select a new group size for the first group and/or the second group and remap the groups accordingly. For example, the network controller may select a new second set of sizes that is coprime to the first set of sizes and map the second set of egress ports to a mapped set having the new second set of sizes. The network controller may then configure the second switch to route the packet according to the mapping group.
The techniques are advantageous in a number of ways. For example, by avoiding adding new hash functions to the switch, the system can mitigate hash dependencies without requiring the switch vendor to support the new hash functions or create computational bottlenecks. By avoiding manipulation of existing hash functions, computation of a CRC is not required and randomness can be preserved for other network functions. Additionally, techniques can be adapted to mitigate hash correlation in both ECMP and WCMP networks, such as dynamically adapting the characteristics of the mapping of WCMP egress ports according to routing weight changes. Features of the technique also provide for reducing the overall memory usage by switches in the network and, at the same time, improve traffic consistency by using various error boundaries for co-prime group size selection at different switches.
Fig. 1A and 1B are block diagrams of a multipath network 100 in which hash-based load balancing may be used. Network 100 may include multiple computing devices, such as computing devices 110, 120, 130, 140 as shown. The multiple computing devices may be geographically distributed and/or organized in clusters. For example, computing devices 110 and 130 may be located in one location or cluster, while computing devices 120 and 140 may be located in another location or cluster. To route packets from a computing device of one location or cluster to a computing device in another location or cluster, one or more switches may be provided in network 100, such as the illustrated switches S1, S2, S3, S4, S5, S6, S7, and S8. In some cases, switches in a multipath network may be arranged in a hierarchy such that routes from a source to a destination need to be routed through switches on each level of the hierarchy before reaching the destination. The example network 100 has a hierarchy with three tiers of switches, a first tier including switch S1, a second tier including switches S2, S3, and a third tier including switches S4, S5, S6, S7. Although only a few computing devices and a few switches are shown, in a practical example, a system may include any of a number of computing devices and any of a number of switches.
Packets may be routed from a source computing device to a destination device over multiple possible paths. In the example shown in fig. 1A, the packet may be forwarded from the source computing device 110 to the destination computing device 120 over four possible paths as indicated by the dashed arrows. To determine which of the multiple paths to use for forwarding the packet, one or more hash functions may be applied to one or more header fields of the packet. For example, a hash function may be applied at each layer that requires selection of an egress port among multiple possibilities. In the example shown, a first hash function H1 may be applied at switch S1 to select between switches S2 and S3 for the next hop, a second hash function H2 may be applied at switch S2 to select between switches S4 and S5 for the next hop, and a second hash function H2 may also be applied at switch S3 to select between switches S6 and S7 for the next hop.
The set of available egress ports for selection as the next hop may be referred to as a group. Thus in multi-path routing, multiple groups are available at one or more hops between a source and a destination. For example, the egress ports of switch S1 to switches S2 and S3 are in the group of size n-2, the egress ports of switch S2 to switches S4 and S5 are in the group of n-2, and the egress ports of switch S3 to switches S6 and S7 are also in the group of n-2. For ECMP routing, each egress port in the ECMP group may be assigned an equal weight such that there is an equal chance that any of the egress ports can be selected for routing the packet flow. For WCMP routing, egress ports in a WCMP group may be assigned different weights such that some of the egress ports are more likely to be selected for routing packet flows than other egress ports.
In this regard, the next hop may be mapped by applying a hash function and then applying a modulo operation to the result of the hash function. As an example, the egress port of the switch may be selected for the next hop based on applying a hash function to 5 tuples of fields such as a packet header including a source prefix (e.g., IP address), a source port, a destination prefix (e.g., IP address), a destination port, and a protocol, with a resulting value of h. A modulo operation may then be applied to the resulting value h, e.g., by the group size n at the switch, which may be expressed as h% n. Thus, the egress port for forwarding the packet at switch S1 may be selected by applying H1 to the header field of the packet, and then modulo the hash result by 2. Likewise, the egress port for forwarding the packet at switch S2 may be selected by applying H2 to the header field of the packet, and then modulo the hash result by 2.
As illustrated by fig. 1A, when the hash functions for hops from different layers are not identical or relevant, each egress port at each layer has a certain chance of being selected as the next hop for the packet, thereby providing load balancing. In contrast, fig. 1B illustrates load imbalance caused by using the same or related hash functions. As shown, the same hash function H1 is applied at both switch S1 and switches S2 and S3. For example, at switch S1, applying H1 and then% 2 to a packet with a particular set of header fields x may result in a value of 0, based on which the egress port to switch S2 may be selected. Then when the packet arrives at switch S2, H1 and then% 2 are applied again to the header field x, which again yields the value 0, based on which the egress port to switch S4 is selected. Thus, while the egress ports to switches S4 and S5 may both be used as the next hop from switch S2, the path through switch S4 is always selected, leaving the alternate path through switch S5 unused and thereby causing a load imbalance between switches S4 and S5. Likewise, the same hash correlation at switch S3 may cause switch S7 to be selected over switch S6 at all times, causing a load imbalance between switches S6 and S7. Although the same hash function H1 is used as an illustration in this example, in other examples, the hash function at each hop may be related rather than the same.
To mitigate hash correlation in a network, the hash function and/or modulo operation must be changed at each level to select the next hop. FIG. 2 is a block diagram illustrating an example system 200 configured to address the issues illustrated by FIG. 1B. System 200 includes a network controller 210 and a data plane 250 that includes a plurality of switches for routing packets between computing devices in a network. Although only a few switches 251, 252, 253, 254 are shown, in a practical example, a system may include any of a number of computing devices and any of a number of switches.
As mentioned above, to mitigate hash correlation, it may be necessary to change the hash function and/or modulo operation at one or more layers at which an egress port will be selected among a plurality of possibilities. Also as described above, generating a new hash function may require hardware changes and/or increased computation time. Thus, the system 200 is configured to mitigate hash correlation by ensuring that modulo arithmetic decorrelates the hash function for each hop. To do so, the network controller 210 may include a co-prime selector 230 that calculates one or more new group sizes for the modulo operations described with reference to fig. 3, 4, 5A-B, and 6. As described with reference to fig. 3, 7A-B, the network controller 210 may also include a routing mapper 240 configured to map, for any group having a new group size, egress ports in the group to mapped groups according to the new group size. As also described with reference to fig. 3, 7A-B, the network controller 210 may further generate forwarding rules based on the mapped egress ports and any groups that have not been changed, and also based on routing weights 220 in the case of WCMP. Network controller 210 may then configure the switches in data plane 250 to route packets according to the forwarding rules.
As also shown, network controller 210 may monitor and control the routing of switches in data plane 250. For example, network controller 210 may assign routing weights 220 to different routes provided by switches in data plane 250. For an ECMP network, all routes between a source and a destination are assigned equal weights. For WCMP networks, routes between a source and a destination may be assigned different routing weights 220, which may be based on network dynamics such as telemetry data. For example, telemetry data may be received by network controller 210 from a switch in the network. Thus, network controller 210 may update routing weights based on updates in the telemetry data and remap egress ports based on the updated routing weights.
The network controller 210 may be hardware-based or may be software-defined, such as a Software Defined Network (SDN) controller. For example, network controller 210 may be software running on one or more computing devices that include one or more processors, memory, and other components typically found in general purpose computing devices. The one or more processors may be application specific components, such as an application specific integrated circuit ("ASIC") that may be custom made or off the shelf, or may be any other conventional processor, such as a commercially available microprocessor, CPU, or the like. Although not required, one or more of the processors may include dedicated hardware components for performing specific computing processes.
The memory of the computing device may be of any non-transitory type capable of storing information accessible by the processor, such as a hard drive, memory card, ROM, RAM, DVD, CD-ROM, writable memory, and read-only memory. The memory of the computing device is capable of storing information, including data and instructions, that is accessible by the processor. For example, a memory of a computing device can store instructions that can be executed by a processor. The memory of the computing device can also include data that can be retrieved, manipulated and/or stored by the processor.
The instructions may be any set of instructions to be directly executed by one or more processors, such as machine code, or any set of instructions to be indirectly executed by one or more processors, such as scripts. In this regard, the terms "instructions," "applications," "steps," and "programs" can be used interchangeably herein. The instructions can be stored in object code format for direct processing by the processor, or in any other computing device language including a collection of script or independent source code modules that are interpreted or compiled in advance as needed.
The data can have any of a number of structures. For example, data can be stored in computer registers, in relational databases as tables with many different fields and records, or in XML documents. The data can also be formatted in any computing device readable format such as, but not limited to, binary values, ASCII, or Unicode. Further, the data can include any information sufficient to identify the relevant information, such as numbers, descriptive text, proprietary codes, pointers, references to data stored in other memories, such as at other network locations, or information used by the function to compute the relevant data.
Although not shown, the computing device may optionally include other components typically found in a general purpose computer device. For example, the computing device may include output devices such as a display, speakers, a touch pad, and so forth. The computing device may also include user input devices such as a mouse, keyboard, touch screen, microphone, sensors, and the like.
Each of the switches 251, 252, 253, 254 may contain one or more processors, memory, and other components typically found in general purpose computing devices, as described above. The computing devices running network controller 210 and switches 251, 252, 253, 254 may be at different nodes of a network, such as the network shown in fig. 1A, and may be capable of direct and indirect communication with other nodes of the network. The networks and intermediate nodes described herein can be interconnected using various protocols and systems such that the networks can be part of the internet, world wide web, specific intranets, wide area networks, or local networks. The network can utilize standard communication protocols such as ethernet, WiFi, and HTTP, protocols proprietary to one or more companies, and various combinations of the foregoing. Although certain advantages are obtained when information is sent or received as described above, other aspects of the subject matter described herein are not limited to any particular manner of transmission of information.
Although not shown, the system may further include one or more storage systems, which may be of any type capable of storing computerized storage of information accessible by one or more of the computing devices running the network controller 210 and/or the switches 251, 252, 253, 254, such as hard drives, memory cards, ROM, RAM, DVD, CD-ROM, writable memory, and read-only memory. Further, the storage system may comprise a distributed storage system in which data is stored on a plurality of different storage devices that may be physically located at the same or different geographic locations. The storage system may be connected to various computing devices via a network and/or may be directly connected to any of the computing devices running network controllers 210 and/or 251, 252, 253, 254.
The computing devices and their respective processors, memories, and other elements running network controller 210 and/or switches 251, 252, 253, 254 may each include multiple processors, computers, computing devices, or memories that may or may not be stored within the same physical enclosure. For example, the memory of the computing device may be a hard drive or other storage medium located in a different enclosure than the enclosure of the computing device. Thus, references to a processor, computer, computing device, or memory are to be understood as including references to a collection of processors, computers, computing devices, or memories that may or may not operate in parallel. For example, the computing devices may include server computing devices operating as a load balancing server farm, a distributed system, and the like. Still further, while some of the functions described below are indicated as being performed on a single computing device having a single processor, various aspects of the subject matter described herein can be implemented by multiple computing devices, e.g., communicating information over a network.
Fig. 3 is a pictorial diagram illustrating an example process for mitigating hash correlation in a multipath network, which may be performed by network controller 210. Fig. 3 illustrates this process with reference to the example network 100 of fig. 1B. For example, since H1% 2 is applied at switch S1 and also at switch S2, the network controller 210 may determine that a hash correlation exists between the two layers.
In further aspects, the network controller 210 may be configured to find network-wide hash correlations, for example, by constructing a directed graph. Fig. 4 illustrates an example directed graph 400. The figure shows the group v ═ s, d, where s is the switch and d is the destination prefix. Thus, graph 400 represents group v1 ═ (S1,10.1.1.1), where the edge (v1, v2) represents group v1 of switch S1 is the first pre-hop of group v2 of switch S2 along the routing path of target prefix 10.1.1.1. If the group vj is reachable from group vi, their hash functions are the same, and both groups are even in size, the network controller 210 may determine that the edges (vi, vj) are relevant. Thus, since group v2 of switch S2 is reachable from group v1 of switch S1, and the hash function H1 applied at both switches is the same, groups v1 and v2 are related.
To mitigate hash correlation, instead of changing the hash function, the modulo operation may be changed by using a new group size for v1 and/or v 2. Thus, instead of changing the H1 applied at switch S2, a new group size q may be calculated such that H1% q is different from H1% 2. In this regard, according to the co-prime theorem, the network controller 210 may use the co-prime selector 230 to calculate a new group size q such that q is co-prime with n. For example, since n is 2, q may be any odd number greater than 1 in order to be coprime with n. In the example shown, an integer 5 is chosen as the new group size q.
For example, the co-prime selector 230 may select an integer by reducing the error (v, q) to satisfy the threshold e, where the error (v, q) represents the error that applies q to the group v. For ECMP groups with a uniform distribution, the error (v, q) can be quantified by the variance coefficients for the ECMP group. A larger coefficient of variance indicates higher non-uniformity and is therefore generally less desirable for ECMP load balancing. The coefficient of variance can be calculated as the standard deviation from the set U ═ Ui|i∈[1,L]} of the mean value of u, where uiIndicating the number of repetitions, u, of the ith egress port in an ECMP groupiIs equal to
In contrast, for a set of WCMPs that do not have a uniform distribution, the error (v, q) can be quantified by the K-S statistic instead of the variance coefficients. Therefore, the co-prime selector 230 may select integers by reducing the error (v, q) based on the K-S statistic being below a predetermined threshold T. Fig. 5A and 5B show example graphs 510 and 520 of K-S statistics for a WCMP group with a weight ratio of 3: 1. The K-S statistic of graph 510 indicates the upper bound distance between two Cumulative Distribution Functions (CDFs). The p-value of graph 520 indicates the probability that two CDFs are identical. Thus, in this example, when the K-S statistic is below 0.07, the p value satisfies a predetermined threshold T of 0.01.
Note that the co-prime selector 230 in this example selects the new group size q-5, which is one of many possibilities co-prime with 2. In this regard, the selection of a new group size may be further based on a number of factors other than coprime with the size of another group. For example, the smaller the relatively prime integer selected as the new group size, the greater the chance that hash correlation may not be effectively mitigated. On the other hand, larger integers generate more entries that occupy more memory space, which can result in the routing table exhausting space. In addition, for multi-path networks with multiple layers of switches, multiple co-prime numbers may need to be selected for new group sizes to mitigate hash correlations between different layers, which may exacerbate these problems. Therefore, a coprime selection may be based on balancing these factors.
FIG. 6 is a block diagram illustrating an example co-prime selection. The multipath network 600 is shown with three tiers of switches, a first tier including switch S1, a second tier including switches S2, S3, S4, S5, and a third tier including switches S6, S7, S8, S9. Also as shown, switch S1 has a group with 4 egress ports, whereas switches S2, S3, S4, and S5 each have a group with 2 egress ports. To reduce the total memory usage in the network, new group sizes (if any) may be assigned in descending order based on the memory usage of the switches on which the groups reside. For example, groups may be aggregated by their destination prefixes and sorted according to M [ S [ v ] ], where v represents a group, S [ v ] is a switch that includes v, and M [ S [ v ] ] is the memory usage of switch S [ v ]. The coprime selector 230 may assign L as the group size of the group with the largest memory usage and then find the coprime size for the other groups. In other words, the bank with the largest memory usage does not change its bank size. Thus, in the example shown, if it is determined that switch S1 has greater memory usage than switches S2, S3, S4, S5, then the group of switch S1 is assigned a group size L of 4. A relatively prime group size may then be determined for each of the switches S2, S3, S4, S5 in the next layer. For example, a relatively prime number may be selected for each group based on reducing the error as described above with reference to fig. 3. In the example shown, an integer 5 is selected for each group at switches S2, S3, S4, and S5 that is coprime with 4.
Additionally or alternatively, the system may be further configured to minimize the error for a given memory size. For example, the error threshold e for ECMP or the error threshold T for ECMP described above may be assigned an initial small value and the error threshold is iteratively increased to determine the group size until the memory usage of the network fits within the memory capacity C. Various errors may be used to determine the co-prime group size for different switches within the network. For example, a bottleneck switch Sm can be identified, which can be the switch in the network that uses the largest memory among all switches. The margin of error E Sm for the group on the switch Sm may be given an initial small value and then incremented at a predefined rate σ E until the memory usage of the network fits within the memory capacity C.
Returning to fig. 3, to generate a new mapped group based on the new group size, the route mapper 240 may then map entries representing the actual egress ports L of the group v into q buckets. For example, the mapping may be performed by replicating one or more of the egress ports. As an example, the front of group v may beindex 0 as indicated in the LPM table 310.
Fig. 3 further illustrates that when a packet with a destination prefix 10.1.2.1 arrives at switch S2, the destination prefix can be matched with a prefix in the Longest Prefix Match (LPM) table 310. LPM table 310 shows that the first group at switch S2 has prefixes 10.1.1.0/24 starting from base index 0, and the second group at switch S2 has prefixes 10.1.2.0/24 starting from base index 2. From LPM table 310, it may be determined that the packet' S destination prefix 10.1.2.1 has the longest match with prefix 10.1.2.0/24, and therefore a second group of egress ports at switch S2 will be selected. That is, a selection will be made from the mapping group of switch S2, the mapping group of switch S2 including 5 entries with indices 2-6. Thus, H1 may be applied to the hash field of the packet, which may result in a value of 9, for example. A modulo of 5 may then be applied to the value 9, resulting in a value of 4. This value of 4 may be added to base index 2 of the mapping group, which then generates index 6 corresponding to egress port 1 of switch S2. Egress port 1 may then be used to forward the packet, for example, as shown in fig. 1B, egress port 1 may forward the packet to switch S4.
Fig. 3 thus illustrates resizing and mapping groups of egress ports for switch S2 to change LPM 310 and multipath table 320 for switch S2. Since the resizing has already been done for switch S2, switch S1 may not need to be resized and mapped, for example, if there is no other hash correlation other than H1 and H2 as described above. The LPM 310 and multipath table 320 for switch S1 may remain unchanged. Thus, the hash results at switch S1 and switch S2 are made uncorrelated by making changes at switch S2.
For WCMP networks, the route mapper 240 may be further configured to map entries according to the routing weights 220. For example, fig. 7A and 7B illustrate example mappings for a WCMP group. Both fig. 7A and 7B illustrate the mapping of WCMP groups for two actual outlet ports, outlet port 1 with weight 3 and outlet port 2 with weight 1, to a new group size of q 7.
In the example of fig. 7A, mapping is performed on the WCMP groups as if there are W ECMP egress ports, where W is the sum of the weights of the WCMP groups, and the indices are mapped in a round robin fashion until a predetermined new group size is met. Thus, as shown, a group of outlet ports is considered to be mapped to a group with 7 indices if there are W-3 + 1-4 ports (3 of outlet ports 1 and 1 of outlet ports 2). Front sideEgress port 1 is mapped 6 times, whereas egress port 2 is mapped 1 time, so that the observed weight ratio for the new mapping group is 6:1, which is twice the original expected ratio of 3: 1. This is further indicated by a K-S statistic of 0.11 and a p value of 1.6 x 10-5.
Fig. 7B illustrates an alternative way to map the set of WCMPs to ensure that the observed weight ratio is close to the desired weight ratio. In this regard, the quotient and remainder of q/W may be handled differently. For a quotient, each WCMP egress port in the WCMP group may be duplicated WCMP egress port 1 and 2 is duplicatedoutlet port 1 is therefore replicated 3 x 1 times, and outlet port 2 is replicated 1 x 1 times. For 3 entries with a remainder r-7% 4, the first r' -3% 2-1 outlet ports are duplicated more than the other outlet ports, so outlet port 1 is duplicated more than twice, whereas outlet port 1 is duplicated more than once. Thus, the result is that the first r' ═ 7%, 4%, 2 ═ 1 outlet ports are replicatedegress port 1, however the remaining egress ports are duplicatedegress port 2 are thus generated. The observed weight ratio for the new mapping group is 5:2, which is closer to the original expected weight ratio of 3:1 compared to fig. 7A. This is further indicated by a smaller K-S statistic of 0.035 and a larger p-value of 0.56, as compared to the example of FIG. 7A.
Fig. 8 is a block diagram illustrating another example process for mitigating hash correlation in a multipath network as an alternative to the process of fig. 3. Fig. 8 illustrates this alternative process with reference to the example network 100 of fig. 1B, and shows some similar features as fig. 3, including an LPM 810, a multipath table 820, and a co-prime selector 830. However, instead of having a route mapper 240 as shown in fig. 3, the alternative process of fig. 8 changes the data pipeline by adding another modulo operation 840. For example, as shown, when a packet with the same header as the example of fig. 3 arrives at switch S2 for forwarding, a hash function H1 is applied to the header to produce a value of 9, and then a modulo operation% 2 is applied to produce a value of 4. An additional modulo operation 840 of% 5 is then applied by switch S2, resulting in a value of 0. By adding this value of 0 to the base index 2 of the group, the egress port with index 2 corresponding to the actual egress port 2 on switch S2, which is routed to switch S5, is selected. In this regard, since the alternative process illustrated by fig. 8 does not use the route mapper 240 in the network controller 210, changes need to be made to the data plane 250. For example, a switch in the network may be configured to apply additional modulo operations to the packet header. Thus, to implement this alternative process, the involvement of the vendor that manufactured the switch chip may be required.
Fig. 9 is a flow diagram 900 illustrating an example method in accordance with aspects of the present disclosure. The method may be performed using any of the above-described systems, modifications thereof, or various systems having different configurations. It should be understood that the operations involved in the following methods need not be performed in the exact order described. Rather, various operations may be processed in a different order or concurrently, and operations may be added or omitted. Although fig. 9 illustrates one example method, variations of the method may be performed, for example, as described above with reference to fig. 2-8. Flowchart 900 may be performed by one or more processors, such as one or more processors running network controller 210. The one or more processors may receive data and make a determination based on the data as described above with reference to fig. 2-8.
Referring to fig. 9, at block 910, it is determined that a hash correlation exists between a first switch in a network and a second switch in the network. An example of such a network is shown in fig. 1B, where a first egress port is to be selected among a first group of egress ports at a first switch for forwarding packets, a second egress port is to be selected among a second group of egress ports at a second switch for forwarding packets, and the first group has a first group size and the second group has a second group size. Further as illustrated by the example of fig. 1B, hash correlations may be found by determining that a first hash function applied at a first switch and a second hash function applied at a second switch are the same and that a second group is reachable by the first group. The hash correlation may be determined using a directed graph as shown in fig. 4.
At block 920, upon determining that hash correlations exist, a new second set of sizes may be selected that are relatively prime to the first set of sizes. For an ECMP group, a new second group size may be selected based on the covariance of the ECMP group satisfying a predetermined threshold. For WCMP groups, such as shown in fig. 5A and 5B, a new second group size may be selected based on the K-S statistics of the WCMP group satisfying a predetermined threshold. In some cases, a new second set of sizes may be further selected to meet the memory capacity of routing tables in the network. In addition, whether to assign a new group size to the first group or the second group may be determined based on comparing the memory usage of the two groups.
At block 930, the second group of egress ports is mapped to a mapping group having a new second group size. For example, mapping may be performed by copying one or more egress ports in the multipath routing table into multiple entries, such as shown in fig. 3. Also as shown in fig. 7A and 7B, routing weights may be considered when mapping according to the new group size for the WCMP group. In this regard, where the routing weights may be updated based on updated telemetry data, an update or remapping according to the updated routing weights may be performed.
At block 940, the second switch is configured to route the packet according to the mapping group. For example, as illustrated by a comparison of the examples shown in fig. 1B and fig. 3, although the same hash function may still be applied at the first switch and the second switch, because the selection of the egress port at the second switch is from a new mapping group having a new group size, the result is that all egress ports of the mapping group have the opportunity to be selected. Thus, hash correlation is mitigated and load balancing can be achieved.
The techniques are advantageous in a number of ways. For example, by avoiding adding new hash functions to the switch, the system can mitigate hash dependencies without requiring the switch vendor to support the new hash functions or create computational bottlenecks. By avoiding manipulation of existing hash functions, computation of a CRC is not required and randomness can be preserved for other network functions. Additionally, techniques can be adapted to mitigate hash correlation in both ECMP and WCMP networks, such as dynamically adapting the characteristics of the mapping of WCMP egress ports according to routing weight changes. Features of the technique also provide for reducing the overall memory usage by switches in the network and, at the same time, improve traffic consistency by using various error boundaries for co-prime group size selection at different switches.
Unless otherwise stated, the foregoing alternative examples are not mutually exclusive, but may be implemented in various combinations to achieve unique advantages. As these and other variations and combinations of the features discussed above can be utilized without departing from the subject matter defined by the claims, the foregoing description of the embodiments should be taken by way of illustration rather than by way of the subject matter defined by the claims. Furthermore, the provision of examples and phrases such as "and" including "and the like described herein should not be construed as limiting the claimed subject matter to the particular example; rather, the examples are not intended to be limited to only one of many possible embodiments. In addition, the same reference numbers in different drawings can identify the same or similar elements.
Claims (20)
1. A method, comprising:
determining, by one or more processors, that a hash correlation exists between a first switch in a network and a second switch in the network, wherein a first egress port is to be selected at the first switch from among a first set of egress ports for forwarding packets and a second egress port is to be selected at the second switch from among a second set of egress ports for forwarding packets, and the first set has a first set size and the second set has a second set size;
upon determining that the hash correlation exists, selecting, by the one or more processors, a new second set of sizes that are coprime to the first set of sizes;
mapping, by the one or more processors, the second set of egress ports to a mapping set having the new second set of sizes; and
configuring, by the one or more processors, the second switch to route packets according to the mapping group.
2. The method of claim 1, wherein determining that the hash correlation exists is based on determining that a first hash function applied at the first switch is the same as a second hash function applied at the second switch, and the second group is reachable by the first group.
3. The method of claim 1, wherein the second group is an ECMP group and the new second group size is selected further based on a covariance of the ECMP group satisfying a predetermined threshold.
4. The method of claim 1, wherein the second group is a WCMP group and the new second group size is selected further based on K-S statistics of the WCMP group satisfying a predetermined threshold.
5. The method of claim 1, wherein the new second set of sizes is selected further based on satisfying memory capacity of a routing table in the network.
6. The method of claim 1, further comprising:
determining, by the one or more processors, that a first memory usage by the first switch for the first group is greater than a second memory usage by the second switch for the second group, wherein selecting the new second group size is based on the first memory usage being greater than the second memory usage.
7. The method of claim 1, further comprising:
determining, by the one or more processors, that a first memory usage by the first switch for the first group is less than a second memory usage by the second switch for the second group, wherein instead of selecting and mapping to the mapping group, a new first group size is selected for mapping the first group.
8. The method of claim 1, wherein the second group is a WCMP group, and mapping the second group of egress ports to the mapped group having the new second group size is further based on routing weights assigned to the second group of egress ports.
9. The method of claim 8, further comprising:
receiving, by the one or more processors, telemetry data for the network;
updating, by the one or more processors, routing weights to the second set of egress ports based on the received telemetry data;
remapping, by the one or more processors, the second set of egress ports based on the updated routing weights.
10. A system, comprising:
one or more processors configured to:
determining that a hash correlation exists between a first switch in a network and a second switch in the network, wherein a first egress port is to be selected at the first switch from among a first set of egress ports for forwarding packets, a second egress port is to be selected at the second switch from among a second set of egress ports for forwarding packets, and the first set has a first set size and the second set has a second set size;
upon determining that the hash correlation exists, selecting a new second set of sizes that are coprime to the first set of sizes;
mapping the second set of egress ports to a mapping set having the new second set of sizes; and
configuring the second switch to route packets according to the mapping group.
11. The system of claim 10, wherein determining that the hash correlation exists is based on determining that a first hash function applied at the first switch is the same as a second hash function applied at the second switch, and the second group is reachable by the first group.
12. The system of claim 10, wherein the second group is an ECMP group and the new second group size is selected further based on a covariance of the ECMP group meeting a predetermined threshold.
13. The system of claim 10, wherein the second group is a WCMP group and the new second group size is selected further based on K-S statistics of the WCMP group satisfying a predetermined threshold.
14. The system of claim 10, wherein the new second set of sizes is selected further based on satisfying memory capacity of a routing table in the network.
15. The system of claim 10, wherein the one or more processors are further configured to determine that a first memory usage by the first switch for the first group is greater than a second memory usage by the second switch for the second group, wherein selecting the new second group size is based on the first memory usage being greater than the second memory usage.
16. The system of claim 10, wherein the one or more processors are further configured to determine that a first memory usage rate by the first switch for the first group is less than a second memory usage rate by the second switch for the second group, wherein instead of selecting and mapping to the mapping group the new second group size is selected for mapping the first group to the first mapping group.
17. The system of claim 10, wherein the second group is a WCMP group and mapping the second group of egress ports to the mapping group having the new second group size is further based on routing weights assigned to the second group of egress ports.
18. The system of claim 17, wherein the one or more processors are further configured to:
receiving telemetry data for the network;
updating routing weights to the second set of egress ports based on the received telemetry data;
remapping the second set of egress ports based on the updated routing weights.
19. A non-transitory computer-readable storage medium storing instructions executable by one or more processors to perform a method, the method comprising:
determining that a hash correlation exists between a first switch in a network and a second switch in the network, wherein a first egress port is to be selected at the first switch from among a first set of egress ports for forwarding packets, a second egress port is to be selected at the second switch from among a second set of egress ports for forwarding packets, and the first set has a first set size and the second set has a second set size;
upon determining that the hash correlation exists, selecting a new second set of sizes that are coprime to the first set of sizes;
mapping the second set of egress ports to a mapping set having the new second set of sizes; and
configuring the second switch to route packets according to the mapping group.
20. The non-transitory computer-readable storage medium of claim 19, wherein determining that the hash correlation exists is based on determining that a first hash function applied at the first switch is the same as a second hash function applied at the second switch, and the second group is reachable by the first group.
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/857,862 | 2020-04-24 | ||
US16/857,862 US11223561B2 (en) | 2020-04-24 | 2020-04-24 | Method to mitigate hash correlation in multi-path networks |
Publications (1)
Publication Number | Publication Date |
---|---|
CN112187659A true CN112187659A (en) | 2021-01-05 |
Family
ID=73698607
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202011064670.1A Pending CN112187659A (en) | 2020-04-24 | 2020-09-30 | Method for mitigating hash correlation in a multi-path network |
Country Status (3)
Country | Link |
---|---|
US (2) | US11223561B2 (en) |
EP (1) | EP3902212B1 (en) |
CN (1) | CN112187659A (en) |
Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7206861B1 (en) * | 2002-07-29 | 2007-04-17 | Juniper Networks, Inc. | Network traffic distribution across parallel paths |
US20110013638A1 (en) * | 2009-07-14 | 2011-01-20 | Broadcom Corporation | Node based path selection randomization |
CN104137494A (en) * | 2012-02-29 | 2014-11-05 | 瑞典爱立信有限公司 | Compound masking and entropy for data packet classification using tree-based binary pattern matching |
CN105049359A (en) * | 2014-04-29 | 2015-11-11 | 英特尔公司 | Technologies for distributed routing table lookup |
EP3057270A1 (en) * | 2015-02-12 | 2016-08-17 | Intel Corporation | Technologies for modular forwarding table scalability |
CN107094114A (en) * | 2016-01-13 | 2017-08-25 | 英特尔公司 | Technology for modularization forward table scalability |
Family Cites Families (12)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP4822997B2 (en) * | 2006-09-20 | 2011-11-24 | 富士通株式会社 | Communication apparatus and communication method |
US8014278B1 (en) | 2007-12-17 | 2011-09-06 | Force 10 Networks, Inc | Adaptive load balancing between ECMP or LAG port group members |
US8244909B1 (en) | 2009-06-18 | 2012-08-14 | Google Inc. | Method, apparatus and networking equipment for performing flow hashing using quasi cryptographic hash functions |
US8503456B2 (en) * | 2009-07-14 | 2013-08-06 | Broadcom Corporation | Flow based path selection randomization |
US8705551B2 (en) | 2011-07-27 | 2014-04-22 | Fujitsu Limited | Method and system for management of flood traffic over multiple 0:N link aggregation groups |
US9331946B2 (en) * | 2013-01-08 | 2016-05-03 | Hitachi, Ltd. | Method and apparatus to distribute data center network traffic |
US20150078375A1 (en) | 2013-09-13 | 2015-03-19 | Broadcom Corporation | Mutable Hash for Network Hash Polarization |
US10725990B2 (en) | 2015-12-01 | 2020-07-28 | Facebook, Inc. | Co-prime hashing |
JP6690093B2 (en) * | 2016-08-10 | 2020-04-28 | 富士通株式会社 | Judgment program, communication device, and judgment method |
US11128541B2 (en) * | 2019-07-22 | 2021-09-21 | Cisco Technology, Inc. | Evaluating the impact of transceiver temperature on interface utilization |
US20220368625A1 (en) * | 2019-10-09 | 2022-11-17 | Curated Networks | Multipath routing in communication networks |
US20210119930A1 (en) * | 2019-10-31 | 2021-04-22 | Intel Corporation | Reliable transport architecture |
-
2020
- 2020-04-24 US US16/857,862 patent/US11223561B2/en active Active
- 2020-09-30 CN CN202011064670.1A patent/CN112187659A/en active Pending
- 2020-12-03 EP EP20211500.2A patent/EP3902212B1/en active Active
-
2022
- 2022-01-05 US US17/569,096 patent/US20220131800A1/en active Pending
Patent Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7206861B1 (en) * | 2002-07-29 | 2007-04-17 | Juniper Networks, Inc. | Network traffic distribution across parallel paths |
US20110013638A1 (en) * | 2009-07-14 | 2011-01-20 | Broadcom Corporation | Node based path selection randomization |
CN104137494A (en) * | 2012-02-29 | 2014-11-05 | 瑞典爱立信有限公司 | Compound masking and entropy for data packet classification using tree-based binary pattern matching |
CN105049359A (en) * | 2014-04-29 | 2015-11-11 | 英特尔公司 | Technologies for distributed routing table lookup |
EP3057270A1 (en) * | 2015-02-12 | 2016-08-17 | Intel Corporation | Technologies for modular forwarding table scalability |
CN107094114A (en) * | 2016-01-13 | 2017-08-25 | 英特尔公司 | Technology for modularization forward table scalability |
Non-Patent Citations (2)
Title |
---|
ZUOWEI CAO, XIAO CHEN: "Catching the Flow with Locality Sensitive Hashing in Programmable Data Planes", 2018 IEEE 9TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING AND SERVICE SCIENCE * |
李海龙: "网络处理器分组转换引擎PTE的研究与设计", 中国优秀硕士学位论文数据库 * |
Also Published As
Publication number | Publication date |
---|---|
EP3902212B1 (en) | 2024-04-24 |
US20220131800A1 (en) | 2022-04-28 |
US20210336884A1 (en) | 2021-10-28 |
EP3902212A1 (en) | 2021-10-27 |
US11223561B2 (en) | 2022-01-11 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11075986B2 (en) | Weighted load balancing using scaled parallel hashing | |
US9571400B1 (en) | Weighted load balancing in a multistage network using hierarchical ECMP | |
US7821925B2 (en) | Traffic distribution techniques utilizing initial and scrambled hash values | |
DK3143753T3 (en) | METHOD AND LOAD BALANCE METHOD OF ANYCAST DATA TRAFFIC | |
US8503456B2 (en) | Flow based path selection randomization | |
US9608913B1 (en) | Weighted load balancing in a multistage network | |
EP2276207B1 (en) | Node based path selection randomization | |
US9716592B1 (en) | Traffic distribution over multiple paths in a network while maintaining flow affinity | |
US9329914B2 (en) | All-to-all message exchange in parallel computing systems | |
EP3087721B1 (en) | Traffic engineering for large scale data center networks | |
US10397097B2 (en) | Weighted next hop selection at a router using an equal cost multipath process | |
CN103873367B (en) | Route data grouping, method and device for determining route and fat tree network | |
US11962485B2 (en) | Selecting and deduplicating forwarding equivalence classes | |
CN112187659A (en) | Method for mitigating hash correlation in a multi-path network | |
US11695428B2 (en) | Effective seeding of CRC functions for flows' path polarization prevention in networks | |
US11765072B2 (en) | Weighted bandwidth allocation for adaptive routing | |
US20180212881A1 (en) | Load-based compression of forwarding tables in network devices | |
Jiang et al. | A fine-grained rule partition algorithm in cloud data centers | |
US11770338B2 (en) | Increasing multi-path size using hierarchical forwarding equivalent classes | |
CN113206783B (en) | Method and apparatus for weighted cost multipath packet processing | |
Iqbal | Evaluation of Current Load Balancing Techniques in a Software Defined Network Aimed at Improving Quality of Service | |
Lutz | Maximizing Virtual Machine Pair Placement in Data Center Networks _ |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |