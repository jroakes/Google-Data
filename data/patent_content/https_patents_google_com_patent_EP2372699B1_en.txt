EP2372699B1 - Coding of audio or video samples using multiple quantizers - Google Patents
Coding of audio or video samples using multiple quantizers Download PDFInfo
- Publication number
- EP2372699B1 EP2372699B1 EP10155236A EP10155236A EP2372699B1 EP 2372699 B1 EP2372699 B1 EP 2372699B1 EP 10155236 A EP10155236 A EP 10155236A EP 10155236 A EP10155236 A EP 10155236A EP 2372699 B1 EP2372699 B1 EP 2372699B1
- Authority
- EP
- European Patent Office
- Prior art keywords
- quantizers
- quantizer
- quantization
- signal
- ensemble
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000013139 quantization Methods 0.000 claims abstract description 122
- 238000005192 partition Methods 0.000 claims abstract description 30
- 238000000034 method Methods 0.000 claims description 72
- 238000009826 distribution Methods 0.000 claims description 30
- 230000006870 function Effects 0.000 claims description 23
- 238000006073 displacement reaction Methods 0.000 claims description 19
- 238000012549 training Methods 0.000 claims description 16
- 238000005457 optimization Methods 0.000 claims description 13
- 238000004590 computer program Methods 0.000 claims description 3
- 238000004519 manufacturing process Methods 0.000 abstract description 3
- 238000013461 design Methods 0.000 description 12
- 239000013598 vector Substances 0.000 description 11
- 238000013459 approach Methods 0.000 description 6
- 230000010354 integration Effects 0.000 description 6
- 230000005540 biological transmission Effects 0.000 description 5
- 230000003247 decreasing effect Effects 0.000 description 5
- 238000009877 rendering Methods 0.000 description 5
- 238000003860 storage Methods 0.000 description 5
- 230000003044 adaptive effect Effects 0.000 description 4
- 230000008901 benefit Effects 0.000 description 4
- 230000001419 dependent effect Effects 0.000 description 4
- 239000011159 matrix material Substances 0.000 description 4
- 230000008569 process Effects 0.000 description 4
- 230000002829 reductive effect Effects 0.000 description 4
- 238000007796 conventional method Methods 0.000 description 3
- 230000000694 effects Effects 0.000 description 3
- 238000001914 filtration Methods 0.000 description 3
- 238000002372 labelling Methods 0.000 description 3
- 230000000670 limiting effect Effects 0.000 description 3
- 238000013507 mapping Methods 0.000 description 3
- 238000012986 modification Methods 0.000 description 3
- 230000004048 modification Effects 0.000 description 3
- 238000012545 processing Methods 0.000 description 3
- 238000004891 communication Methods 0.000 description 2
- 230000007423 decrease Effects 0.000 description 2
- 238000010586 diagram Methods 0.000 description 2
- 230000009467 reduction Effects 0.000 description 2
- 238000005070 sampling Methods 0.000 description 2
- 230000005236 sound signal Effects 0.000 description 2
- 230000001360 synchronised effect Effects 0.000 description 2
- 238000012935 Averaging Methods 0.000 description 1
- 230000006978 adaptation Effects 0.000 description 1
- 238000004458 analytical method Methods 0.000 description 1
- 230000006399 behavior Effects 0.000 description 1
- 230000009286 beneficial effect Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 238000006243 chemical reaction Methods 0.000 description 1
- 230000002860 competitive effect Effects 0.000 description 1
- 230000007812 deficiency Effects 0.000 description 1
- 238000011161 development Methods 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000014509 gene expression Effects 0.000 description 1
- 230000007274 generation of a signal involved in cell-cell signaling Effects 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000008447 perception Effects 0.000 description 1
- 230000000737 periodic effect Effects 0.000 description 1
- 238000012805 post-processing Methods 0.000 description 1
- 238000007781 pre-processing Methods 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 230000002441 reversible effect Effects 0.000 description 1
- 238000002922 simulated annealing Methods 0.000 description 1
- 238000013179 statistical model Methods 0.000 description 1
- 230000007704 transition Effects 0.000 description 1
- 238000009827 uniform distribution Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L19/00—Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/124—Quantisation
- H04N19/126—Details of normalisation or weighting functions, e.g. normalisation matrices or variable uniform quantisers
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
- H04N19/146—Data rate or code amount at the encoder output
- H04N19/147—Data rate or code amount at the encoder output according to rate distortion criteria
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/60—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using transform coding
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/90—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using coding techniques not provided for in groups H04N19/10-H04N19/85, e.g. fractals
- H04N19/94—Vector quantisation
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L19/00—Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis
- G10L19/02—Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis using spectral analysis, e.g. transform vocoders or subband vocoders
- G10L19/032—Quantisation or dequantisation of spectral components
- G10L19/035—Scalar quantisation
Definitions
- the invention disclosed herein generally relates to devices and methods for processing signals, and particularly to devices and methods for quantizing signals.
- Typical applications may include a quantization device for audio or video signals or a digital audio encoder.
- Quantization is the process of approximating a continuous or quasi-continuous (digital but relatively high-resolving) range of values by a discrete set of values.
- Simple examples of quantization include rounding of a real number to an integer, bit-depth transition and analogue-to-digital conversion. In the latter case, an analogue signal is expressed in terms of digital reference levels. Integer quantization indices may be used for labelling the reference levels, whose total number determines the bit rate needed for transmitting the digital signal at a certain time resolution. As used herein, quantization does not necessarily include changing the time resolution of the signal, such as by sampling or downsampling it with respect to time.
- Analogue signals are commonly quantized to enable transmission over a digital network.
- the reconstruction step at the receiving end may consist of a straightforward replacement of the quantization indices by analogue segments corresponding to the digital reference levels.
- perceptible quantization noise may occur in the reconstructed signal.
- the signal power will decrease with decreasing rate.
- transform-based quantization of audio signals where the source signal is decomposed into frequency components, which are then subjected to scalar quantization, annoying artefacts appear at low rate. At such rates, many of the frequency components are intermittently quantized to zero.
- a time-frequency plot of the signal power the non-zero episodes may occupy isolated areas, reminiscent of islands.
- quantization is generally applied to reduce the bit rate for the purpose of saving bandwidth and storage cost.
- Dithering may improve the audible or visual impression of the reconstructed signal.
- So-called subtractive dither where the dither is added at the encoder before quantization and subtracted at the decoder after decoding of the quantizer, is particularly effective since it allows coding to remain asymptotically optimal (with increasing rate) from the rate-distortion perspective. It has been established that dithering can mitigate artefacts that are associated with an unintended statistical correlation between the quantization error and the source signal value. However, at low rates, where the quantization artefacts are most problematic, the established dithering techniques result in a reconstructed signal that has a probability density that differs significantly from that of the original signal. Such differences are generally observable and perceived as annoying.
- parametric coders At low rates, signals are commonly described with so-called parametric coders. In contrast to the above-described coders that quantize the signal waveform, parametric coders do not attempt to preserve the waveform, but attempt to preserve only a set of salient features of the signal. To this purpose, parametric coders transmit only the time-varying parameters of a generative signal model. Usually, the models can be interpreted as a statistical model of the signal. At the decoder, a new signal is generated using the signal model. In general such signal generation provides the best perceived performance at very low rate, but, because of shortcomings of the signal model, does not provide competitive performance at high rate.
- the present invention seeks to mitigate, alleviate or eliminate one or more of the above-mentioned deficiencies and drawbacks singly or in combination. It would also be desirable to provide a quantization method and quantization device that introduces a limited amount of perceptible artefacts when applied to audio coding or video coding, down to moderate bit rates.
- a signal may be a function of time or a time series that is received in real time or retrieved from a storage or communication entity, e.g., a file or bit stream containing previously recorded data to be processed.
- a sample is a value of a one-dimensional or multidimensional signal, and can thus have any number of components.
- a multidimensional signal can be a sequence of blocks of samples of a one-dimensional signal.
- the entropy can be calculated analytically or numerically.
- quantization is understood as the associated processes of encoding and decoding a signal or a sequence of signal samples, wherein the encoded signal is expressed in terms of a discrete set of values.
- a quantizer is an apparatus for encoding and/or decoding. The properties of a quantizer depends on its quantization cells and their corresponding reconstruction values. Thus, a quantizer may also be an abstract representation of an apparatus or an algorithm in terms of its quantization cells and corresponding reconstruction values.
- an ensemble of quantizers is a collection of quantizers labelled by values of a parameter. An ensemble of quantizers may be finite or infinite, and countable or uncountable, the latter case arising, e.g., if a continuous-valued parameter is used.
- the invention provides an encoding method characterized by the fact that several quantizers are applied, one at a time, in an alternating fashion.
- the method may comprise the following steps:
- the invention provides a decoding method which is characterized by an alternation between several quantizers used for assigning a reconstruction point to a quantization index in the sequence to be reconstructed.
- the decoding method may comprise the following steps:
- the encoding method and the decoding method are both applicable in a multi-dimensional (vector) case, with appropriate modifications.
- the partition into quantization cells in higher dimensions may be more conveniently defined in terms of Voronoi regions of a point set, rather than in terms of cell boundaries.
- the boundaries of the Voronoi regions can be regarded as implied boundaries of a partition.
- Each of the methods may also be applied to a transform of a signal, such as time-variable components corresponding to frequency components.
- the encoding and decoding may be performed by entities that are separated in time or space. For instance, encoding may be carried out in a transmitter unit and decoding in a receiving unit, which is connected to the transmitter unit by a digital communications network. Further, encoded analogue data may be stored in quantized form in a volatile or non-volatile memory; decoding may then take place when the quantized data have been loaded from the memory and the encoded data are to be reconstructed in analogue form. Moreover, quantized data may be transmitted or stored together with the quantization parameters (e.g., parameters defining the partition into quantization cells or parameters used for characterizing the reconstruction points). The quantization parameters may then be used for carrying out decoding of the quantized data.
- quantization parameters e.g., parameters defining the partition into quantization cells or parameters used for characterizing the reconstruction points.
- a computer program product storing computer-readable instructions for executing one of the above methods.
- apparatus for encoding, decoding or quantization there are further provided apparatus for encoding, decoding or quantization.
- a method of manufacturing an encoder, decoder or quantizer according to the invention which method includes designing an ensemble of quantizers.
- An embodiment of the invention as an apparatus can in general be readily adapted to a method and vice versa.
- apparatus and methods according to the invention generally share the same advantages.
- the invention solves the problem of introducing only a limited amount of artefacts into the subsequently reconstructed signal by virtue of the alternation between quantizers. Hence, no statistical correlation between quantization errors and signal values - that could otherwise be connected with undesirable artefacts which may be audible in the reconstructed signal - can arise.
- the quantizers are advantageously selected such that the variance of the signal remains similar to that of the original signal, even when the rate is reduced, further decreasing the observable distortion.
- the repeated selection of quantizers follows a predetermined sequence.
- Such predetermined sequence may be pseudo-random.
- the sequence may be periodic with period of such duration that audible correlation-related artefacts arise to a limited extent or not at all.
- a decoding method or decoding apparatus has access to the predetermined sequence which the selection of quantizers follows; this is how the method obtains an indication of a particular quantizer to which a quantization index refers.
- the quantization indices generated by the sequence of quantizers can be decoded using the appropriate quantizers.
- the repeated selection of quantizers used for encoding is made randomly, such as by means of a random-number generator.
- random selection is preferably accompanied by outputting an indication of the quantizers which have been selected and for what samples.
- this embodiment may further include assigning a selection probability to each quantizer; the selection probability governs the random selection, so that some quantizers can be given greater weight than others. It is also possible to select the quantizers with a uniform probability.
- Each of the preceding embodiments are preferably combined with a decoder that obtains information of the outcomes of the random selection, so as to enable decoding of the quantization indices by appropriate decoders.
- the selection probabilities of the quantizers of the ensemble are chosen so that the statistical distribution of the reconstruction values is close to (approximates) the distribution of the original signal values.
- the approximation accuracy can be measured by a Kullback-Leibler divergence.
- the selection probabilities may vary in time with estimated probabilities of the values in the original signal.
- the frequency at which a quantizer is selected or reselected - whether randomly or deterministically - is constant.
- the frequency may also be variable over time or be related to the sample frequency.
- the ensemble of quantizers used by an encoding method or a decoding method further contains quantizers that are all defined on the basis of optimizations of quantization cell locations and corresponding reconstruction points. All such optimizations are performed with respect to a fixed objective function (criterion) which is a weighted sum of at least:
- the bit rate may be measured in bits per second.
- the signal distortion is a conventional distortion measure. In particular, it may be a perceptually weighted squared-error criterion.
- a measure of the fidelity of the probability distributions may be a relative entropy - or Kullback-Leibler divergence - between a source signal and a reconstructed signal. The Kullback-Leibler divergence vanishes when the probability distributions of the original and reconstructed signal are identical.
- a method of simultaneous design of an ensemble of quantizers By fixing suitable weights, a pre-defined balance between the mean performance of an ensemble of the quantizers can be achieved.
- all quantizers in the obtained ensemble are obtained by optimization with respect to the same objective function (criterion), which provides for homogeneous audio quality of the subsequently reconstructed signal.
- criterion the objective function
- the inclusion of a relative entropy conveys a further advantage, namely that the optimization does not yield degenerated solutions as long as the initial values (initial quantizers) provided to the optimization are distinct. This ensures that the quantization operations achieved by distinct quantizers indeed has sufficient independence that only a limited amount of audible correlation-related artefacts can arise.
- the inclusion of relative entropy ensures that the signal variance does not converge to zero when the rate decreases to zero.
- a method for designing an ensemble of optimal quantizers comprises the following steps:
- the above method may form part of an encoding method or a decoding method. It may also be performed in a standalone fashion for the manufacture of quantization apparatus according to the invention.
- the ensemble of quantizers thus obtained may be stored on a memory and included in a hardware product for decoding or encoding, respectively.
- the proportionality constant C is at most 0.01, and preferably 0.003.
- the training sequence is representative of the signal (music, speech, video, etc.) to be coded by the quantizers.
- the size of the quantization cells of an initial quantizer is an exponential function of the entropy (see definition above) of the training signal and the bit rate. This will ensure a fast divergence to an optimal quantizer.
- a method of adaptively coding a sequence of source signal samples is a further development of the encoding methods disclosed above, insofar as it further comprises defining at least two ensembles of quantizers using different objective functions .
- a decision to define a new ensemble of quantizers may be taken responsive to an observed variation of properties of the source signal samples to be encoded.
- the objective function (criterion) used for defining the new ensemble of quantizers may be adapted in accordance with the observed variation of the properties.
- the adaptation may take place by reweighting a sum of a bit rate, a signal distortion and a fidelity of the probability distributions of a source signal and a reconstructed signal.
- the design procedure is equivalent to optimizing any one of the three measures subject to constraints on the other two measures.
- the weighted criterion then forms the extended criterion resulting from the well-known Lagrange-multiplier approach. In this case the two relevant weights are determined by the constraints.
- One aspect of the invention is the usage of the designed ensemble of optimized quantizers for encoding a sequence of source signal values. For this purpose, a time ordering of the quantizers is selected. The time ordering must result in an overall selection rate of each quantizer that corresponds to the design probability of that quantizer, thus resulting in a fidelity of the distribution of the reconstructed points consistent with the design goal Encoder and decoder are knowledgeable about the selection-order of the quantizers. The time ordering is generally a pseudo-random sequence.
- the optimal solution is that all quantizers of the ensemble are identical to a conventional rate-distortion optimal quantizer. In general such coders are referred to as waveform coders. If the fidelity of the probability is weighted relatively high, the reconstructed signal will have a probability density that resembles that of the original signal, but it will, not in general lead to an optimal trade-off between rate and distortion. For the case where the distortion weight approaches zero, the distortion obviously becomes high, the rate for coding the signal will approach zero, and the decoded signal will resemble that obtained with a conventional parametric coding setup.
- each vector component may encode one audio channel, or it may describe a subsequent signal segment.
- the first two sections discuss and define quantizers and performance measures in general.
- the next two sections use these definitions to define the embodiment of the generalized quantizer according to the invention.
- a fifth section discusses adaptive quantization according to the invention.
- the source signal, the quantization index and the reconstructed signal will be modelled as random variables X, I and X ⁇ .
- the variables X and X ⁇ can be scalars or vectors, and the variable I is a scalar.
- Realizations of X and X, denoted as x and x ⁇ take values in real space of one or more dimensions, whereas realizations of I take values in a countable set, such as the natural numbers.
- variable X is taken to be the variable that forms the input to the quantizer.
- X ⁇ is then subjected to an inverse transform or filtering operation before rendering an approximation of the original signal.
- the distortion criterion used in the quantization may depend on the transform or filtering operation.
- the conventional goal of quantizer design is to minimize a distortion measure (which is a function of quantization error) between the source signal and the quantized signal, subject to a bit rate budget. That is, it optimizes the operational rate-distortion trade-off of the quantizer.
- the optimal reconstructed signal will depend on the choice of the measure.
- the penalty assigned by a distortion measure increases when the quantization error is increased by a factor greater than one. This means that with increasing rate the reconstructed signal more accurately matches the signal waveform of the source.
- waveform coding is sometimes referred to as "waveform coding".
- the mapping from X to I is a space partition, with I representing an index to a particular cell. As for each quantization cell only one number, the index, is transmitted, minimization of the distortion leads to a unique reconstruction value, or centroid, for each cell. This corresponds to optimization of the decoder for the quantizer. Important in the current context is that the optimization procedure generally results in a one-to-one mapping from I to X ⁇ . (An exception to this is a decoder with dithering that is independent of the encoder.) As an aside it is noted that iterative optimization of the decoder and the partition (which corresponds to the encoder) is a conventional method for optimization of the quantizer, commonly referred to as the Lloyd algorithm, the Lloyd/Max algorithm, or the generalized Lloyd algorithm.
- mapping from I to X is one-to-one, X ⁇ is discrete. At low rates this leads to a situation where very few reconstruction points are used.
- the number of available points is restricted to 2 l , where l is the selected codeword length.
- the quantization step size increases with decreasing rate. At low rates only very few reconstruction points have a probability that differs significantly from zero.
- the random variable X generally has a smaller variance than the original variable.
- Shannon lower bound in this case the optimal quantizer results in a reconstruction X that is statistically independent from the quantization error X - X ⁇ and that means the variances of X and X - X ⁇ must add to the variance of X.
- the limiting case of zero rate also illustrates this variance-reducing behavior: assuming a squared-error criterion the best reconstruction is the variable mean.
- the basic properties of a signal are commonly characterized by the statistical distribution of its samples, which can be estimated by studying the signal over time.
- a description of the joint probability distribution of the samples constitutes a model of the signal and facilitates the generation of data.
- Many so-called parametric coding algorithms can be interpreted as characterizing a joint probability distribution of signal samples that is appropriate for a particular sound, coding the parameters and generating a new signal according to this probability distribution. Such coding algorithms do not reconstruct the signal waveform and generally perform poorly compared to waveform coding at high rate.
- the limited number of reconstruction points of a conventional quantizer even when optimized for a different criterion, fundamentally limits its ability to create a reconstructed signal with a probability distribution that resembles the original signal. This means that a generalization of conventional quantizer is needed if the probability distribution of the reconstructed signal must be able to approximate that of the original signal.
- a relevant modification of a conventional quantizer is the dithered quantizer.
- This modification is not a generalization of a conventional quantizer because it does not include the conventional quantizer as a special case; there is no natural adjustment of the system that leads to a continuum between conventional and dithered quantization that is optimal in a meaningful sense.
- Dither removes the discrete nature of the distribution of the reconstructed signal.
- Subtractive dither is particularly attractive as it facilitates asymptotic rate-distortion optimality and can be used as an illustrative example of the methodology.
- Subtractive dither relies on a single quantizer. For each quantization operation, the signal is randomly displaced relative to the quantizer. The methodology requires that the quantizer is uniform and generally the dither is chosen to be uniform in the unit quantization interval. This implies that the dithered quantizer is not optimal at low rate because it is uniform and because the quantizer is displaced in a random manner.
- the present invention reduces to a selectable degree the problem that, at low rate, a conventional quantizer uses only a small set of reconstruction points frequently.
- a central aspect of the invention is the usage of an ensemble of quantizers.
- the ensemble is denoted by J , with j ⁇ j indexing the quantizers.
- Each quantizer of the ensemble is characterized by its own partition and its own set of reconstruction points.
- the individual quantizers of the ensemble are each selected for use with a particular relative frequency. As the reconstruction points of the quantizers do not have to be identical, the number of commonly used reconstruction points can be increased arbitrarily.
- the present invention also reduces to a selectable degree the problem that, with decreasing rate, a conventional quantizer progressively reduces the signal variance. This can be seen from considering what happens at zero rate.
- a conventional quantizer progressively reduces the signal variance. This can be seen from considering what happens at zero rate.
- only one reconstruction point is used for each quantizer of the ensemble.
- the ensemble-based approach facilitates optimization for criteria that relate to reconstruction point density.
- each of three distortion measures must be specified.
- specifications are provided for the measures that facilitate a particular embodiment that can form a part of a system for the encoding of audio signals.
- the matrix F can be dependent on the signal X, or, for example, put emphasis on the low frequencies in a transform coder.
- Such more general implementations do not affect the principle of the method and for the present embodiment F in eq. (1) is set equal to the identity matrix.
- the distortion measure of eq. (1) must be averaged over the probability density for the individual quantization cells.
- Eq. (2) can be applied to each quantizer of the ensemble separately.
- the average distortion for the ensemble is then the average of the distortion for all of the quantizers.
- the computation of the performance measure (2) is straightforward.
- the distortion D ( j ) of eq. (2) can be computed by numerical or analytic integration of the probability density of X over all quantization cells (each indexed by i). If the computation is based on a database - in the sense of a set of stored values - then the integral for each cell V i is approximated by a sum of the distortions for each data point within a quantization cell, divided by the total number of points in the entire database.
- the distortion for the ensemble of quantizers is obtained simply by averaging over the quantizers.
- the next measure of performance is the rate, which is denoted by R and refers to the average rate.
- the rate can be specified in bits per sample (or in bits per second) required for encoding the quantization index I.
- constrained-entropy (variable-rate) coding is used. Extension of the embodiment to constrained-resolution coding is straightforward.
- the total rate is then the average of the rates of all quantizers:
- the computation of the performance measure (4) is conventional.
- the third measure is the fidelity of the probability distribution of X with respect to the probability distribution of X.
- a complication in this respect is that, at least in principle, X is described by a probability density, denoted by f x (.), and X is a discrete random variable with a probability mass function p x ⁇ (.) ⁇
- f x ⁇ a probability density
- f x ⁇ a probability density
- the Kullback-Leibler divergence is nonnegative and it vanishes when the densities are equal.
- the weighting of the "local penalty" log f X x f X ⁇ x is the probability density of the source signal X.
- the reconstructed variable, f x ⁇ ( x ) has been generated by a plurality of quantizers in the ensemble, and thus, its probability density depends on all the quantizers involved. Therefore, unlike conventional methods, the computation of the density f x ⁇ necessarily takes into account all the quantizers in the ensemble. Thus, it cannot be computed with conventional techniques, and an approximate computation method will now be discussed.
- the first term, ⁇ R k f x ( x ) log (f x (x)) dx, is the negative differential entropy, which is a property of the signal, and does not depend on the quantizers. The reason to compute it is to find the actual Kullback-Leibler divergence. If the probability density is known, then it can be obtained by conventional integration (either numerical or analytical). If only a database is given, then various techniques are possible as described in M. Nilsson and W. B. Kleijn, "On the estimation of differential entropy from data located on embedded manifolds," IEEE Trans. Information Theory, vol. 53, no. 7, pp. 2330-2341, 2007 .
- the number of cells in the partition W equals the total number of reconstruction points in all quantizers of the ensemble together.
- boundary points of the ensemble partition can be selected arbitrarily to belong to one of the nearest cells; this does not affect the algorithm.
- the ensemble partition W is used only as a basis for the computation of the Kullback-Leibler divergence during the design stage of the quantizer.
- the second term of eq. (7) can now be computed with a resolution that is equal to the ensemble partition. That is, the second term of eq. (7) is computed as a sum ⁇ R k ⁇ f X x ⁇ log f g x d x ⁇ ⁇ i ⁇ A p l i ⁇ log q l i W i , where p I is the probability-mass function resulting from the original signal when partitioned by W, where q I is the probability-mass function of the reconstructed signal when partitioned by W, and where W i denotes the volume of ensemble Voronoi region W i .
- p I ( i ) ⁇ wi f x ( x ) dx.
- Eq. (9) can then be rewritten as ⁇ R k ⁇ f X x ⁇ log f R x d x ⁇ ⁇ i ⁇ A ⁇ W i ⁇ f X x ⁇ dx log ⁇ V i ⁇ f X x ⁇ dx J ⁇ W i .
- the Kullback-Leibler divergence can be approximated by integration of the probability density of X over the quantization cells V i and W i . If the computation is based on a database, then the probabilities ⁇ wi f x (x) dx and ⁇ v i fX(x)dx can be estimated by counting the number of data points in the cells and dividing by the total number of points in the database.
- the quantizer consists of an ensemble of quantizers that have been optimized to operate as a set.
- the quantizers can be computed for each use, or when the environment triggers a recomputation, or stored.
- Encoder and decoder have an identical listing that specifies the quantizer used for each data point (which can be a scalar or a vector). For the illustrative embodiment all quantizers occur equally often in the list.
- the quantizers can be implemented in many different ways. For example they may be based on a stored codebook of reconstruction points. Alternatively they may be based on rounding operations that remove precision from an integer variable. This may be combined with a non-uniform warping of the scale of the input signal. Yet another alternative is that reconstruction points are created based on real-time estimation of the distribution of the input signal as described further below. In this case the estimated distribution is also transmitted to the decoder.
- each quantizer consists of a codebook of reconstruction points. Each reconstruction point is associated with an index that is stored or transmitted.
- the acquisition and rendering devices may include pre-processing and post-processing, respectively.
- an input signal may be subjected to transforms that are presented as an input vector to the encoder according to the invention.
- the first step of the rendering device would be to perform an inverse transform.
- the lossless encoding of the transmission index can be based on conventional established methods such as Huffman coding or arithmetic coding.
- Figure 1 shows an exemplary embodiment of an encoder and a decoder according to the invention.
- a source signal X to be encoded is connected to one at a time of a succession of individual quantizers (E) belonging to a ensemble of quantizers.
- the encoders are connected according to a predetermined order, where two quantizers carrying the indices 71 and 9 have been connected in the past, a quantizer indexed by 47 is currently connected, and quantizers 14 and 22 are to be connected next.
- the currently connected quantizer returns a quantization index in response to the source signal sample. More precisely, the index indicates what quantization cell the source signal sample belongs to.
- the set of quantization indices returned by the quantizers forms a sequence I of quantization indices.
- the lower half of figure 1 shows an exemplary embodiment of a decoder, which may cooperate with the decoder described above.
- the decoding is performed by a succession of quantizers (D) respectively labelled by indices 71, 9, 47, 14 and 22.
- the succession of quantizers for decoding is synchronized with the succession of quantizers used for encoding, so that corresponding quantizers used for both encoding and decoding. More precisely, a quantization index that has been generated by comparison with the quantization cells of a particular quantizer will be reconstructed by a reconstruction point optimized to reconstruct these values in the same quantization cell.
- the set of reconstructed samples forms a sequence X of reconstructed signal values.
- Figure 2 shows an encoder in accordance with an embodiment of the invention.
- the encoder comprises a memory 201 storing a plurality of quantizers (E) for encoding, each comprising a partition of the signal space into quantization cells associated with respective reconstruction points. While the encoder shown in figure 1 applies a predetermined sequence of quantizers, the encoder of figure 2 may involve random selection of the quantizers by means of a quantizer selector 202. The selection operation is shown symbolically as the quantizer memory 201 being displaceable sideways by the quantizer selector 202.
- An encoding device 203 encodes each source signal sample by a quantization index determined by the currently selected quantizer.
- the past selections of quantizers are stored, transmitted, or generated by identical copies (with identical initialization) of the quantizer selector 202, so as to facilitate subsequent (possibly immediate) decoding of the quantization indices by the appropriate quantizers.
- a particular quantizer with index j is selected with identical probability. Normally this selection probability equals the selection probability used during the design stage (as stated before eq. (10), it is generally convenient to make the selection probability for all quantizers equal). If in the implementation the quantizer selection probabilities are different from the design quantizer selection probabilities this will, in general, increase the Kullback-Leibler distance between the (continuous approximation of the) density of the reconstructed signal and the density of the original signal. Thus, usage of the selection probability employed during the design generally corresponds to using the selection probabilities that optimize a measure of fidelity of the reconstruction distribution.
- Each of the three terms of eq. (12) depends on the density of the original data, f x (.), the properties of the ensemble of quantizers, and the particular approximation made to compute the smoothed density f x ⁇ (.).
- the properties of the ensemble of quantizers are determined by the partitions and sets of reconstruction points ⁇ V i , c i ⁇ i ⁇ A , where it is remembered that A indexes the partitions and reconstruction points of all quantizers in the ensemble of quantizers J simultaneously.
- Scalar quantization suffers particularly strongly from the discretization effect, as the specific reconstruction points repeat for each successive sample of a process.
- the discretization effect is reduced for vector quantization, increasingly so with increasing dimensionality.
- vector quantization does reduce the signal variance with decreasing rate, as this reduction in signal variance is optimal from a rate-distortion viewpoint.
- Each quantizer in the ensemble is limited to having the same number of quantization cells.
- :n 1,2, ...,
- ⁇ , where A j A J , labels the reconstruction value of quantizer j. More-over the interlacing of the indices is associated with an ordering of the reconstruction points: c i ⁇ c i+1 ⁇ i ⁇ ⁇ 1, ...,
- + 1 cell boundaries, where end boundaries for quantizer j are defined by b j - ⁇ and b
- l j l +j ⁇ . No specific constraint on physical ordering of the cell bounds needs to be imposed. The lower bound b i and that upper bound b i +
- the definitions of the performance measures simplify for the scalar case.
- the expressions will be provided explicitly. They can be evaluated by analytic integration, by numerical integration, or by counting data from a database, over the Voronoi regions of the partition V and and the partition W .
- D X ⁇ ⁇ 1 J ⁇ i ⁇ A ⁇ b i b i + J ⁇ x - c i 2 ⁇ f X x ⁇ dx
- D X ⁇ ⁇ 1 J ⁇ i ⁇ A ⁇ b i - c i 0 ⁇ x 2 ⁇ f X ⁇ x + c i ⁇ dx + ⁇ 0 b i + J - c i ⁇ x 2 ⁇ f X ⁇ x + c i ⁇ dx , c i ⁇ dx ,
- the Kullback-Leibler divergence can then be written as K X ⁇ ⁇ ⁇ ⁇ i ⁇ A ⁇ ⁇ i ⁇ i + 1 f X x ⁇ dx log ⁇ J ⁇ ⁇ ⁇ i ⁇ i + 1 ⁇ f X x ⁇ dx ⁇ b i b i + J ⁇ f X x ⁇ dx ,
- the iterative training algorithm for the ensemble of quantizers of the illustrative embodiment can now be formulated.
- the initial ensemble of quantizers is based on knowledge from high-rate theory and uses zero weight for the Kullback-Leibler divergence (at high rate the Kullback-Leibler divergence approaches zero).
- the quantizers are uniform.
- the number of reconstruction points can be selected to be sufficient to cover a range 8 ⁇ / ⁇ where ⁇ 2 is the signal variance (square of standard deviation). Initially the uniform quantizers are interlaced to obtain a uniform distribution of the reconstruction points from all quantizers of the ensemble.
- the algorithm iteratively searches for displacements of the boundaries and the reconstruction values from a set of displacements.
- the global displacement set can be selected advantageously as ⁇ -0.003 ⁇ , 0, 0.003 ⁇ .
- a local displacement set is computed with the purpose of preserving the interlacing. The algorithm of the illustrative embodiment is then described as:
- the described methodology has a fixed initial configuration. It is noted that the algorithm is not guaranteed to converge to a global minimum. Thus, it may be advantageous to start from a set of initial configurations and select the best performing end solution.
- the training method described above leads to a particular solution for each combination or ⁇ and ⁇ . Recomputation is necessary in situations where the allowed rate or the signal changes with time. In such scenarios the quantizer ensemble must be adapted to the particular situation at hand and that requires a fast computation of the quantizer.
- An alternative embodiment exploits that, in general, the quantizer ensemble solutions vary gradually as a function of variation or ⁇ and ⁇ . This can be exploited for fast computation of a set of quantizers in the following training and quantization method.
- the training method is:
- An embodiment of an adaptive quantization method is:
- a computer program may be stored or distributed on a suitable medium, such as an optical storage medium or a solid-state medium supplied together with or as part of other hardware, but may also be distributed in other forms, such as via the Internet or other wired or wireless telecommunication systems. Any reference signs in the claims should not be construed as limiting the scope.
Abstract
Description
- The invention disclosed herein generally relates to devices and methods for processing signals, and particularly to devices and methods for quantizing signals. Typical applications may include a quantization device for audio or video signals or a digital audio encoder.
- Quantization is the process of approximating a continuous or quasi-continuous (digital but relatively high-resolving) range of values by a discrete set of values. Simple examples of quantization include rounding of a real number to an integer, bit-depth transition and analogue-to-digital conversion. In the latter case, an analogue signal is expressed in terms of digital reference levels. Integer quantization indices may be used for labelling the reference levels, whose total number determines the bit rate needed for transmitting the digital signal at a certain time resolution. As used herein, quantization does not necessarily include changing the time resolution of the signal, such as by sampling or downsampling it with respect to time.
- Analogue signals are commonly quantized to enable transmission over a digital network. The reconstruction step at the receiving end may consist of a straightforward replacement of the quantization indices by analogue segments corresponding to the digital reference levels. However, at least if a moderate number of reference levels are applied, perceptible quantization noise may occur in the reconstructed signal. More-over, for the common case where a conventional distortion criterion is used and the signal is zero mean, the signal power will decrease with decreasing rate. In transform-based quantization of audio signals, where the source signal is decomposed into frequency components, which are then subjected to scalar quantization, annoying artefacts appear at low rate. At such rates, many of the frequency components are intermittently quantized to zero. In a spectrogram, a time-frequency plot of the signal power, the non-zero episodes may occupy isolated areas, reminiscent of islands.
- The above problem - and possibly other drawbacks associated with quantization - may be mitigated by increasing the bit rate. However, quantization is generally applied to reduce the bit rate for the purpose of saving bandwidth and storage cost.
- The problem can also be addressed by attempting to optimize the quantization procedure itself. However, well-known theoretical bounds exist on the rate-distortion performance of a quantizer. Low-rate quantizers that are trained with, for example, the well-known generalized Lloyd algorithm perform close to the theoretical bound but still suffer from the mentioned artefacts.
- Dithering may improve the audible or visual impression of the reconstructed signal. So-called subtractive dither, where the dither is added at the encoder before quantization and subtracted at the decoder after decoding of the quantizer, is particularly effective since it allows coding to remain asymptotically optimal (with increasing rate) from the rate-distortion perspective. It has been established that dithering can mitigate artefacts that are associated with an unintended statistical correlation between the quantization error and the source signal value. However, at low rates, where the quantization artefacts are most problematic, the established dithering techniques result in a reconstructed signal that has a probability density that differs significantly from that of the original signal. Such differences are generally observable and perceived as annoying.
- In addition to these attempts to improve the quantization itself, the field of audio technology offers several techniques for reducing the low-rate artefacts a posteriori: band limitation (see M. Erne, "Perceptual audio coders 'what to listen for"', 111th Convention of the Audio Engineering Society, Sept. 2001), a regularization method for tonal-like signals (see L. Daudet and M. Sandler, "MDCT analysis of sinusoids: exact results and applications to coding artifacts reduction", IEEE Transactions on Speech and Audio Processing, vol. 12, no. 3, May 2004) and noise fill (see S. A. Ramprashad, "High quality embedded wideband speech coding using an inherently layered coding paradigm," in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing ICASSP '00, vol. 2, June 2000).
- At low rates, signals are commonly described with so-called parametric coders. In contrast to the above-described coders that quantize the signal waveform, parametric coders do not attempt to preserve the waveform, but attempt to preserve only a set of salient features of the signal. To this purpose, parametric coders transmit only the time-varying parameters of a generative signal model. Usually, the models can be interpreted as a statistical model of the signal. At the decoder, a new signal is generated using the signal model. In general such signal generation provides the best perceived performance at very low rate, but, because of shortcomings of the signal model, does not provide competitive performance at high rate.
- It is with respect to the above considerations and others that the present invention has been made. The present invention seeks to mitigate, alleviate or eliminate one or more of the above-mentioned deficiencies and drawbacks singly or in combination. It would also be desirable to provide a quantization method and quantization device that introduces a limited amount of perceptible artefacts when applied to audio coding or video coding, down to moderate bit rates.
- As used herein, a signal may be a function of time or a time series that is received in real time or retrieved from a storage or communication entity, e.g., a file or bit stream containing previously recorded data to be processed. A sample is a value of a one-dimensional or multidimensional signal, and can thus have any number of components. A multidimensional signal can be a sequence of blocks of samples of a one-dimensional signal. The entropy of a signal X mav be defined as follows:
where fx is a probability density (which may be discrete) of the signal X. The entropy can be calculated analytically or numerically. Further, quantization is understood as the associated processes of encoding and decoding a signal or a sequence of signal samples, wherein the encoded signal is expressed in terms of a discrete set of values. A quantizer is an apparatus for encoding and/or decoding. The properties of a quantizer depends on its quantization cells and their corresponding reconstruction values. Thus, a quantizer may also be an abstract representation of an apparatus or an algorithm in terms of its quantization cells and corresponding reconstruction values. Finally, an ensemble of quantizers is a collection of quantizers labelled by values of a parameter. An ensemble of quantizers may be finite or infinite, and countable or uncountable, the latter case arising, e.g., if a continuous-valued parameter is used. - To better address one or more of the above concerns, quantization methods and devices as defined in the independent claims are provided. Embodiments of the invention are defined in the dependent claims.
- In a first aspect, the invention provides an encoding method characterized by the fact that several quantizers are applied, one at a time, in an alternating fashion. In an illustrative one-dimensional (scalar) case, the method may comprise the following steps:
- defining an ensemble of quantizers (φ (j),j = 1,2, ...), each of which comprises a partition into quantization cells
- encoding each source signal sample (xk ) by a quantization index indicating a quantization cell of a quantizer (φ (j(k)) ) selected either randomly or following a predetermined sequence, from said ensemble of quantizers, wherein the quantization cell contains the source signal sample
- repeatedly selecting, from said ensemble of quantizers, a quantizer (φ (j(k'))) to be used for subsequent encoding and if the selection is random, further outputting an indication of past and/or present selections of quantizers so as to facilitate subsequent decoding.
- In a second aspect, the invention provides a decoding method which is characterized by an alternation between several quantizers used for assigning a reconstruction point to a quantization index in the sequence to be reconstructed. In a one-dimensional case, the decoding method may comprise the following steps:
- obtaining an ensemble of quantizers (φ (j),j = 1, 2, ...), each of which comprises a plurality of reconstruction points (ci (j),i = 1, 2, ...);
- for each quantization index (ik ):
- o obtaining an indication of a particular quantizer (φ (j(k))) to which said quantization index (ik ) refers by accessing either stored past random selections or a predetermined sequence, on which the selection of quantizers is based during encoding; and
- o decoding said quantization index by a corresponding reconstruction point (cik).
- The encoding method and the decoding method are both applicable in a multi-dimensional (vector) case, with appropriate modifications. E.g., the partition into quantization cells in higher dimensions may be more conveniently defined in terms of Voronoi regions of a point set, rather than in terms of cell boundaries. The boundaries of the Voronoi regions can be regarded as implied boundaries of a partition. Each of the methods may also be applied to a transform of a signal, such as time-variable components corresponding to frequency components.
- The encoding and decoding may be performed by entities that are separated in time or space. For instance, encoding may be carried out in a transmitter unit and decoding in a receiving unit, which is connected to the transmitter unit by a digital communications network. Further, encoded analogue data may be stored in quantized form in a volatile or non-volatile memory; decoding may then take place when the quantized data have been loaded from the memory and the encoded data are to be reconstructed in analogue form. Moreover, quantized data may be transmitted or stored together with the quantization parameters (e.g., parameters defining the partition into quantization cells or parameters used for characterizing the reconstruction points). The quantization parameters may then be used for carrying out decoding of the quantized data.
- In further aspects of the invention, there are provided a computer program product storing computer-readable instructions for executing one of the above methods. There are further provided apparatus for encoding, decoding or quantization. Additionally, there is provided a method of manufacturing an encoder, decoder or quantizer according to the invention, which method includes designing an ensemble of quantizers. An embodiment of the invention as an apparatus can in general be readily adapted to a method and vice versa. Furthermore, apparatus and methods according to the invention generally share the same advantages.
- The invention solves the problem of introducing only a limited amount of artefacts into the subsequently reconstructed signal by virtue of the alternation between quantizers. Hence, no statistical correlation between quantization errors and signal values - that could otherwise be connected with undesirable artefacts which may be audible in the reconstructed signal - can arise. The quantizers are advantageously selected such that the variance of the signal remains similar to that of the original signal, even when the rate is reduced, further decreasing the observable distortion.
- In one embodiment of the encoding method or encoding apparatus, the repeated selection of quantizers follows a predetermined sequence. Such predetermined sequence may be pseudo-random. The sequence may be periodic with period of such duration that audible correlation-related artefacts arise to a limited extent or not at all.
- As a counterpart to the preceding embodiment, a decoding method or decoding apparatus has access to the predetermined sequence which the selection of quantizers follows; this is how the method obtains an indication of a particular quantizer to which a quantization index refers. Thus, the quantization indices generated by the sequence of quantizers can be decoded using the appropriate quantizers.
- In one embodiment, the repeated selection of quantizers used for encoding is made randomly, such as by means of a random-number generator. To facilitate subsequent decoding, such random selection is preferably accompanied by outputting an indication of the quantizers which have been selected and for what samples. Preferably, this embodiment may further include assigning a selection probability to each quantizer; the selection probability governs the random selection, so that some quantizers can be given greater weight than others. It is also possible to select the quantizers with a uniform probability.
- Each of the preceding embodiments are preferably combined with a decoder that obtains information of the outcomes of the random selection, so as to enable decoding of the quantization indices by appropriate decoders.
- In an embodiment, the selection probabilities of the quantizers of the ensemble are chosen so that the statistical distribution of the reconstruction values is close to (approximates) the distribution of the original signal values. Advantageously, the approximation accuracy can be measured by a Kullback-Leibler divergence. The selection probabilities may vary in time with estimated probabilities of the values in the original signal.
- In one embodiment, the frequency at which a quantizer is selected or reselected - whether randomly or deterministically - is constant. The frequency may also be variable over time or be related to the sample frequency.
- In an embodiment, the ensemble of quantizers used by an encoding method or a decoding method further contains quantizers that are all defined on the basis of optimizations of quantization cell locations and corresponding reconstruction points. All such optimizations are performed with respect to a fixed objective function (criterion) which is a weighted sum of at least:
- a bit rate,
- a signal distortion, and
- a fidelity of the probability distributions of a source signal and a reconstructed signal.
- The bit rate may be measured in bits per second. The signal distortion is a conventional distortion measure. In particular, it may be a perceptually weighted squared-error criterion. A measure of the fidelity of the probability distributions may be a relative entropy - or Kullback-Leibler divergence - between a source signal and a reconstructed signal. The Kullback-Leibler divergence vanishes when the probability distributions of the original and reconstructed signal are identical.
- In addition to the preceding embodiment, there is provided a method of simultaneous design of an ensemble of quantizers. By fixing suitable weights, a pre-defined balance between the mean performance of an ensemble of the quantizers can be achieved. Advantageously, all quantizers in the obtained ensemble are obtained by optimization with respect to the same objective function (criterion), which provides for homogeneous audio quality of the subsequently reconstructed signal. However, the inclusion of a relative entropy conveys a further advantage, namely that the optimization does not yield degenerated solutions as long as the initial values (initial quantizers) provided to the optimization are distinct. This ensures that the quantization operations achieved by distinct quantizers indeed has sufficient independence that only a limited amount of audible correlation-related artefacts can arise. Moreover, the inclusion of relative entropy ensures that the signal variance does not converge to zero when the rate decreases to zero.
- In an embodiment, a method for designing an ensemble of optimal quantizers (being optimal in the above sense) comprises the following steps:
- receiving a sequence (XT) of training signal samples;
- defining said fixed objective function (η);
- defining a displacement step (Cσ) proportional to the standard deviation (σ) of the training signal samples;
- for each quantizer to be generated:
- o defining an initial quantizer (φ̂(j))) comprising quantization cells (together forming a partition) and corresponding reconstruction points;
- o performing one or more iterations, each consisting of the following steps:
- ■ locally optimizing the partition, such that real or implied cell boundaries remain within the displacement step (Cσ), with respect to the objective function (η) evaluated for the sequence (XT) of training samples, thereby obtaining an optimized partition;
- ■ locally optimizing each of said initial reconstruction points within at most the displacement step with respect to the objective function (η) evaluated for the sequence (XT ) of training samples, thereby obtaining an optimized reconstruction point (c i (j) and
- o storing a quantizer (φ(j)) comprising the optimized quantization cells and reconstruction points;
- outputting an ensemble of quantizers (φ (j),j = 1,2, ...) containing the stored quantizers.
- The above method may form part of an encoding method or a decoding method. It may also be performed in a standalone fashion for the manufacture of quantization apparatus according to the invention. In particular, the ensemble of quantizers thus obtained may be stored on a memory and included in a hardware product for decoding or encoding, respectively.
- In an embodiment related to the above (sub)method for designing an ensemble of quantizers, the proportionality constant C is at most 0.01, and preferably 0.003. Preferably, the training sequence is representative of the signal (music, speech, video, etc.) to be coded by the quantizers.
- In a further embodiment related to the (sub)method for designing an ensemble of quantizers, the size of the quantization cells of an initial quantizer is an exponential function of the entropy (see definition above) of the training signal and the bit rate. This will ensure a fast divergence to an optimal quantizer. In particular, the size may be set to Δ = 2h(X)-R, where h(X) is the entropy and R is the rate.
- In an embodiment of the invention, there is provided a method of adaptively coding a sequence of source signal samples. The method is a further development of the encoding methods disclosed above, insofar as it further comprises defining at least two ensembles of quantizers using different objective functions . A decision to define a new ensemble of quantizers may be taken responsive to an observed variation of properties of the source signal samples to be encoded. The objective function (criterion) used for defining the new ensemble of quantizers may be adapted in accordance with the observed variation of the properties. The adaptation may take place by reweighting a sum of a bit rate, a signal distortion and a fidelity of the probability distributions of a source signal and a reconstructed signal.
- The design procedure is equivalent to optimizing any one of the three measures subject to constraints on the other two measures. The weighted criterion then forms the extended criterion resulting from the well-known Lagrange-multiplier approach. In this case the two relevant weights are determined by the constraints.
- One aspect of the invention is the usage of the designed ensemble of optimized quantizers for encoding a sequence of source signal values. For this purpose, a time ordering of the quantizers is selected. The time ordering must result in an overall selection rate of each quantizer that corresponds to the design probability of that quantizer, thus resulting in a fidelity of the distribution of the reconstructed points consistent with the design goal Encoder and decoder are knowledgeable about the selection-order of the quantizers. The time ordering is generally a pseudo-random sequence.
- By weighting the three components of the design criterion, a trade-off between the fidelity of the probability distribution, the rate, and the conventional distortion can be achieved. If the fidelity of the probability is weighted by a zero weight, then the optimal solution is that all quantizers of the ensemble are identical to a conventional rate-distortion optimal quantizer. In general such coders are referred to as waveform coders. If the fidelity of the probability is weighted relatively high, the reconstructed signal will have a probability density that resembles that of the original signal, but it will, not in general lead to an optimal trade-off between rate and distortion. For the case where the distortion weight approaches zero, the distortion obviously becomes high, the rate for coding the signal will approach zero, and the decoded signal will resemble that obtained with a conventional parametric coding setup.
- Any of the above embodiments can be generalized into a multi-dimensional quantization process, wherein the source signal, the quantization index and the reconstructed signal are vector-valued. In the context of audio coding, each vector component may encode one audio channel, or it may describe a subsequent signal segment.
- Embodiments of the present invention will now be disclosed in more detail with reference to the accompanying drawings, on which:
-
figure 1 is a signal diagram of an encoder and a decoder applying different quantizers in an alternating fashion, in accordance with an embodiment of the invention; and -
figure 2 is a block diagram showing the components of an encoder in accordance with an embodiment of the invention. - In this description of embodiments of the invention, the first two sections discuss and define quantizers and performance measures in general. The next two sections use these definitions to define the embodiment of the generalized quantizer according to the invention. A fifth section discusses adaptive quantization according to the invention.
- In the following description of methods and apparatus according to the invention, the source signal, the quantization index and the reconstructed signal will be modelled as random variables X, I and X̂. The variables X and X̂ can be scalars or vectors, and the variable I is a scalar. Realizations of X and X, denoted as x and x̂, take values in real space of one or more dimensions, whereas realizations of I take values in a countable set, such as the natural numbers.
- It is common to apply quantization to a transform or filtering of an original signal segment or of an original signal. Herein, the variable X is taken to be the variable that forms the input to the quantizer. X̂ is then subjected to an inverse transform or filtering operation before rendering an approximation of the original signal. The distortion criterion used in the quantization may depend on the transform or filtering operation.
- The conventional goal of quantizer design is to minimize a distortion measure (which is a function of quantization error) between the source signal and the quantized signal, subject to a bit rate budget. That is, it optimizes the operational rate-distortion trade-off of the quantizer. As the quantizer attempts to minimize a distortion measure, the optimal reconstructed signal will depend on the choice of the measure. However, in general the penalty assigned by a distortion measure increases when the quantization error is increased by a factor greater than one. This means that with increasing rate the reconstructed signal more accurately matches the signal waveform of the source. Thus, coding a source with a quantizer is sometimes referred to as "waveform coding".
- The mapping from X to I is a space partition, with I representing an index to a particular cell. As for each quantization cell only one number, the index, is transmitted, minimization of the distortion leads to a unique reconstruction value, or centroid, for each cell. This corresponds to optimization of the decoder for the quantizer. Important in the current context is that the optimization procedure generally results in a one-to-one mapping from I to X̂. (An exception to this is a decoder with dithering that is independent of the encoder.) As an aside it is noted that iterative optimization of the decoder and the partition (which corresponds to the encoder) is a conventional method for optimization of the quantizer, commonly referred to as the Lloyd algorithm, the Lloyd/Max algorithm, or the generalized Lloyd algorithm.
- Since the mapping from I to X is one-to-one, X̂ is discrete. At low rates this leads to a situation where very few reconstruction points are used. In the case of constrained-resolution coding, the number of available points is restricted to 2 l , where l is the selected codeword length. For the case of constrained-entropy coding, the quantization step size increases with decreasing rate. At low rates only very few reconstruction points have a probability that differs significantly from zero.
- In addition to having a discrete nature, the random variable X generally has a smaller variance than the original variable. This is clear for the case that the so-called Shannon lower bound can be reached: in this case the optimal quantizer results in a reconstruction X that is statistically independent from the quantization error X - X̂ and that means the variances of X and X - X̂ must add to the variance of X. The limiting case of zero rate also illustrates this variance-reducing behavior: assuming a squared-error criterion the best reconstruction is the variable mean.
- The discrete nature of the quantized variable X̂, as well as its reduced variance, lead to artefacts in the reconstructed signal. These artefacts are associated with the fact that the reconstructed signal no longer has the properties that human perception assumes that it should have. In effect, a quantizer does not use all information available to reconstruct the signal. This motivates the introduction of a constraint on certain basic properties of the signal. For example, in speech coding it is natural to constrain the reconstructed speech signal to be speech-like.
- The basic properties of a signal are commonly characterized by the statistical distribution of its samples, which can be estimated by studying the signal over time. A description of the joint probability distribution of the samples constitutes a model of the signal and facilitates the generation of data. Many so-called parametric coding algorithms can be interpreted as characterizing a joint probability distribution of signal samples that is appropriate for a particular sound, coding the parameters and generating a new signal according to this probability distribution. Such coding algorithms do not reconstruct the signal waveform and generally perform poorly compared to waveform coding at high rate.
- Particularly at low rates, the limited number of reconstruction points of a conventional quantizer, even when optimized for a different criterion, fundamentally limits its ability to create a reconstructed signal with a probability distribution that resembles the original signal. This means that a generalization of conventional quantizer is needed if the probability distribution of the reconstructed signal must be able to approximate that of the original signal.
- A relevant modification of a conventional quantizer is the dithered quantizer. (This modification is not a generalization of a conventional quantizer because it does not include the conventional quantizer as a special case; there is no natural adjustment of the system that leads to a continuum between conventional and dithered quantization that is optimal in a meaningful sense.) Dither removes the discrete nature of the distribution of the reconstructed signal. Subtractive dither is particularly attractive as it facilitates asymptotic rate-distortion optimality and can be used as an illustrative example of the methodology. Subtractive dither relies on a single quantizer. For each quantization operation, the signal is randomly displaced relative to the quantizer. The methodology requires that the quantizer is uniform and generally the dither is chosen to be uniform in the unit quantization interval. This implies that the dithered quantizer is not optimal at low rate because it is uniform and because the quantizer is displaced in a random manner.
- The present invention reduces to a selectable degree the problem that, at low rate, a conventional quantizer uses only a small set of reconstruction points frequently. A central aspect of the invention is the usage of an ensemble of quantizers. The ensemble is denoted by J, with j ∈ j indexing the quantizers. Each quantizer of the ensemble is characterized by its own partition and its own set of reconstruction points. The individual quantizers of the ensemble are each selected for use with a particular relative frequency. As the reconstruction points of the quantizers do not have to be identical, the number of commonly used reconstruction points can be increased arbitrarily.
- The present invention also reduces to a selectable degree the problem that, with decreasing rate, a conventional quantizer progressively reduces the signal variance. This can be seen from considering what happens at zero rate. In this case only one reconstruction point is used for each quantizer of the ensemble. For each quantizer that is the reconstruction point nearest to the signal mean. By suitably setting the values of these reconstruction points, and the frequency of the selection of the corresponding quantizer, a distribution of the reconstruction that equals the distribution of the original signal, thus preserving signal variance, is made possible.
- In general, as the partition of quantization cells and the set of corresponding reconstruction points of each quantizer of the ensemble can be optimized individually, and as the discrete nature of the reconstruction can be reduced, the ensemble-based approach facilitates optimization for criteria that relate to reconstruction point density.
- As was mentioned above, conventional quantization aims to optimize the operational rate-distortion trade-off. One interpretation of the present invention is that it optimizes the rate-distortion trade-off subject to a constraint on the fidelity of the probability distribution of X̂. Another interpretation is that it trades distortion versus rate versus fidelity of the probability density.
- For an embodiment of the invention each of three distortion measures must be specified. In the following, specifications are provided for the measures that facilitate a particular embodiment that can form a part of a system for the encoding of audio signals.
- Conventional distortion measures are generally based on the squared error criterion:
- For a particular quantizer indexed by j , the distortion measure of eq. (1) must be averaged over the probability density for the individual quantization cells. Thus, the average distortion for a quantizer j is written as
where φ (j) describes the parameters of quantizer j, that determine the quantization operation Q (j)(.) that maps x to the corresponding x̂, fx (.) is the probability density of the random variable X, k is the dimensionality of X, Vi is the quantization cell which will be reconstructed by ci , so that Q ( j) (x) = ci whenever i ∈ A(j) and x ∈ Vi . Eq. (2) can be applied to each quantizer of the ensemble separately. The average distortion for the ensemble is then the average of the distortion for all of the quantizers. As the invention provides use of one quantizer at a time, the computation of the performance measure (2) is straightforward. When f x (x) is known, then the distortion D(j) of eq. (2) can be computed by numerical or analytic integration of the probability density of X over all quantization cells (each indexed by i). If the computation is based on a database - in the sense of a set of stored values - then the integral for each cell Vi is approximated by a sum of the distortions for each data point within a quantization cell, divided by the total number of points in the entire database. -
- The next measure of performance is the rate, which is denoted by R and refers to the average rate. The rate can be specified in bits per sample (or in bits per second) required for encoding the quantization index I. In the present embodiment, constrained-entropy (variable-rate) coding is used. Extension of the embodiment to constrained-resolution coding is straightforward. Using the well-known technique of arithmetic coding, the average rate can be made arbitrarily close to the entropy rate of the index I, the limiting case being:
Eq. (4) must be applied to each quantizer of the ensemble separately. The total rate is then the average of the rates of all quantizers: - Thus, the computation of the performance measure (4) is conventional. When the probability density function of the variable X, fx(.), is known, then the rate R of eq. (4) can be computed by numerical or analytic integration of the probability density of X over every quantization cell to render the cell probabilities: pI (i) =∫ vi fx (x) dx. If the computation is based on a database, then the pI ( i ) can be estimated by counting the number of data in the cell and dividing by the total number of points in the database.
- The third measure is the fidelity of the probability distribution of X with respect to the probability distribution of X. A complication in this respect is that, at least in principle, X is described by a probability density, denoted by fx (.), and X is a discrete random variable with a probability mass function px̂ (.)̂ With an increasing number of quantizers present in the ensemble, the set of reconstructed values may progressively lose their discrete character and the probability distribution of X̂ can be characterized by a probability density fx̂ (.) A natural measure of fidelity of the density fx̂ (.) is the Kullback-Leibler divergence with respect to fx̂ (.), which is defined as
- The Kullback-Leibler divergence is nonnegative and it vanishes when the densities are equal. Note that the weighting of the "local penalty" log
-
- The first term,∫R k fx (x)log (fx(x)) dx, is the negative differential entropy, which is a property of the signal, and does not depend on the quantizers. The reason to compute it is to find the actual Kullback-Leibler divergence. If the probability density is known, then it can be obtained by conventional integration (either numerical or analytical). If only a database is given, then various techniques are possible as described in M. Nilsson and W. B. Kleijn, "On the estimation of differential entropy from data located on embedded manifolds," IEEE Trans. Information Theory, vol. 53, no. 7, pp. 2330-2341, 2007.
- The approximation of the second term in eq. (7), ∫R k fx (x)log(fx̂(x))dx requires additional definitions. It is convenient to define a single index set A = {1,2, ..., |A|} for all cells of all quantizers in the ensemble. The cell-index sets A (j) are then distinct subsets that make up the index set A, i.e., A = UjEJ A(j). For each quantizer j, a partition V(j) with Voronoi regions that partition Rk can be defined. Define Vi (j) to be the Voronoi region of the cell with index i in quantizer j. Note that, as defined here, the cell indices of a particular quantizer are not necessarily consecutive but, for example, interlaced with those of other quantizers (strictly considered this makes the superscript in V(j) redundant).
- To approximate eq. (7) in a practical method, it is beneficial to exploit the high density of reconstruction points in the ensemble of quantizers (as compared to the density of the individual quantizers). (This advantage occurs if the weight of the Kullback-Leibler divergence is sufficiently high.) For the purpose of making this approximation, an ensemble partition W with ensemble Voronoi regions Wi can be defined:
- Thus, the number of cells in the partition W equals the total number of reconstruction points in all quantizers of the ensemble together. As is usual for quantizers, boundary points of the ensemble partition can be selected arbitrarily to belong to one of the nearest cells; this does not affect the algorithm. It is again emphasized that the ensemble partition W is used only as a basis for the computation of the Kullback-Leibler divergence during the design stage of the quantizer.
- The second term of eq. (7) can now be computed with a resolution that is equal to the ensemble partition. That is, the second term of eq. (7) is computed as a sum
- Clearly, pI (i) =∫ wi fx (x)dx. The probability qI (i) is equal to the product of the probability P(j = J) of using quantizer j and the probability of quantization cell i given that the signal is quantized with the particular quantizer j. The probability of quantization cell i given that quantizer j is applied is ∫ vi fx(x)dx. This means that qI (i) = P(j = j) ∫vi fx(x)dx. The probability P(j = J) is set by the designer.
-
- In practice, the direct approximation of eq. (7) is no more difficult than the computation of eq. (10) and is more meaningful:
Where the arguments of the Kullback-Leibler divergence show that in this case the Kullback-Leibler divergence depends on the random input variable X and on the quantizer parameters of the entire ensemble of quantizers, φ = {φ(j):j ∈ J}. When the probability density function of the variable X, fx (.), is known, then the Kullback-Leibler divergence can be approximated by integration of the probability density of X over the quantization cells Vi and Wi . If the computation is based on a database, then the probabilities ∫ wi fx (x) dx and ∫vi fX(x)dx can be estimated by counting the number of data points in the cells and dividing by the total number of points in the database. - The quantizer consists of an ensemble of quantizers that have been optimized to operate as a set. The quantizers can be computed for each use, or when the environment triggers a recomputation, or stored. Encoder and decoder have an identical listing that specifies the quantizer used for each data point (which can be a scalar or a vector). For the illustrative embodiment all quantizers occur equally often in the list.
- The quantizers can be implemented in many different ways. For example they may be based on a stored codebook of reconstruction points. Alternatively they may be based on rounding operations that remove precision from an integer variable. This may be combined with a non-uniform warping of the scale of the input signal. Yet another alternative is that reconstruction points are created based on real-time estimation of the distribution of the input signal as described further below. In this case the estimated distribution is also transmitted to the decoder. In the following illustrative embodiment, each quantizer consists of a codebook of reconstruction points. Each reconstruction point is associated with an index that is stored or transmitted.
- For the illustrative embodiment the encoder each coding operation consists of the following steps:
- (1) Acquisition of an input signal (scalar or vector) from the acquisition device;
- (2) Selection of the quantizer index j from a time-sequential list of indices;
- (3) Determination of the quantization index by searching for the nearest neighbour of the input signal in the codebook j;
- (4) Lossless encoding of the transmission index; and
- (5) Transmission or storage of codeword.
- The decoder of the illustrative embodiment consists of the corresponding decoding steps in reverse order:
- (1) Receipt of codeword from transmission channel or retrieval of codeword from storage;
- (2) Decoding of the codeword, rendering the quantization index;
- (3) Selection of the quantizer index j from a time-sequential list that is synchronized in time with that of the encoder;
- (4) Look up of the reconstruction point in the codebook corresponding to the quantization index; and
- (5) Output of the decoded signal to the rendering device.
- For clarity it is noted that the acquisition and rendering devices may include pre-processing and post-processing, respectively. For example, an input signal may be subjected to transforms that are presented as an input vector to the encoder according to the invention. In such a situation the first step of the rendering device would be to perform an inverse transform. It is furthermore noted that the lossless encoding of the transmission index can be based on conventional established methods such as Huffman coding or arithmetic coding.
-
Figure 1 shows an exemplary embodiment of an encoder and a decoder according to the invention. In the upper part of the figure, a source signal X to be encoded is connected to one at a time of a succession of individual quantizers (E) belonging to a ensemble of quantizers. As is symbolically indicated, the encoders are connected according to a predetermined order, where two quantizers carrying theindices - The lower half of
figure 1 shows an exemplary embodiment of a decoder, which may cooperate with the decoder described above. Similarly, the decoding is performed by a succession of quantizers (D) respectively labelled byindices -
Figure 2 shows an encoder in accordance with an embodiment of the invention. The encoder comprises amemory 201 storing a plurality of quantizers (E) for encoding, each comprising a partition of the signal space into quantization cells associated with respective reconstruction points. While the encoder shown infigure 1 applies a predetermined sequence of quantizers, the encoder offigure 2 may involve random selection of the quantizers by means of aquantizer selector 202. The selection operation is shown symbolically as thequantizer memory 201 being displaceable sideways by thequantizer selector 202. Anencoding device 203 encodes each source signal sample by a quantization index determined by the currently selected quantizer. Suitably, the past selections of quantizers are stored, transmitted, or generated by identical copies (with identical initialization) of thequantizer selector 202, so as to facilitate subsequent (possibly immediate) decoding of the quantization indices by the appropriate quantizers. - In encoder and decoder a particular quantizer with index j is selected with identical probability. Normally this selection probability equals the selection probability used during the design stage (as stated before eq. (10), it is generally convenient to make the selection probability for all quantizers equal). If in the implementation the quantizer selection probabilities are different from the design quantizer selection probabilities this will, in general, increase the Kullback-Leibler distance between the (continuous approximation of the) density of the reconstructed signal and the density of the original signal. Thus, usage of the selection probability employed during the design generally corresponds to using the selection probabilities that optimize a measure of fidelity of the reconstruction distribution.
- The quantization design method is based on the optimization of a weighted average of the performance measures discussed in subsection II. Without loss of generality, the weight for the rate can be set equal to unity. If this procedure is interpreted as resulting from the well-known Lagrange-multiplier approach, then the weight values are then determined by the constraints. Using the notation defined above, the overall distortion measure is:
where the rate and distortion are averaged over the ensemble of quantizers and are, therefore, dependent on the parameters of all quantizers. - Each of the three terms of eq. (12) depends on the density of the original data, fx (.), the properties of the ensemble of quantizers, and the particular approximation made to compute the smoothed density fx̂ (.). The properties of the ensemble of quantizers are determined by the partitions and sets of reconstruction points {Vi , ci}i∈A, where it is remembered that A indexes the partitions and reconstruction points of all quantizers in the ensemble of quantizers J simultaneously.
- The illustrative embodiment provided here is for a scalar quantizer. Scalar quantization suffers particularly strongly from the discretization effect, as the specific reconstruction points repeat for each successive sample of a process. The discretization effect is reduced for vector quantization, increasingly so with increasing dimensionality. However, it is noted that vector quantization does reduce the signal variance with decreasing rate, as this reduction in signal variance is optimal from a rate-distortion viewpoint.
- A scalar quantizer j can be described by its reconstruction points {Ci}i∈A (j) and its cell boundaries {b i}i∈B (j)where A =U j∈J A (j) is the set of indices labelling the cells of quantizer j, and B (j) is the set of indices labelling the cell boundaries of quantizer j. That is, the parameter set determining the quantizer is φ (j) = {{bi } i∈B(j), {ci} i∈A (j)}.he set B is defined as the union of all B (j), that is B =U j∈ J B(j). Each quantizer in the ensemble is limited to having the same number of quantization cells.
- Without loss of generality the quantization indices are defined as being interlaced. That is, the set of indices A (j) = {j + n|J|:n = 1,2, ..., |A (j)|}, where
- The definitions of the performance measures simplify for the scalar case. In the following, the expressions will be provided explicitly. They can be evaluated by analytic integration, by numerical integration, or by counting data from a database, over the Voronoi regions of the partition V and and the partition W.
-
-
-
- The iterative training algorithm for the ensemble of quantizers of the illustrative embodiment can now be formulated. The initial ensemble of quantizers is based on knowledge from high-rate theory and uses zero weight for the Kullback-Leibler divergence (at high rate the Kullback-Leibler divergence approaches zero). The quantizers are uniform. The quantizer step size is set to Δ = 2h(x)-R, where h(X) = - ∫R fx (x) log(fx (x)) dx can be computed analytically or numerically. Advantageously, the number of reconstruction points can be selected to be sufficient to cover a range 8σ/Δ where σ2 is the signal variance (square of standard deviation). Initially the uniform quantizers are interlaced to obtain a uniform distribution of the reconstruction points from all quantizers of the ensemble.
- The algorithm iteratively searches for displacements of the boundaries and the reconstruction values from a set of displacements. The global displacement set can be selected advantageously as {-0.003σ, 0, 0.003σ}. For optimizing the reconstruction points, a local displacement set is computed with the purpose of preserving the interlacing. The algorithm of the illustrative embodiment is then described as:
- Initialization
- o Set µ and λ
- o Define the displacement set
- o Define a uniform point set {ci } i∈A ; centered at the origin and with spacing Δ/|J|
- o Define a uniform set {bi} i∈B , using b 1 = c 1 - Δ/2
- End initialization
- Loop over iterations
- o Loop over quantizers j = 1, ..., |J|
- ■ Loop over boundaries {bi } i∈B (j)
- Select from displacement set the boundary displacement that minimizes η and update the boundary accordingly
- ■ End loop over boundaries
- ■ Loop over boundaries {bi } i∈B (j)
- o End loop over quantizers
- o Loop over quantizers j = 1, ..., |J|
- ■ Loop over reconstruction points {ci } i∈A (j)
- Compute the local displacement set that is the global displacement, except that it is limited so that at most it halves the distance to the nearest reconstruction point
- Select from local displacement set the reconstruction-point displacement that minimizes η and update the reconstruction point accordingly
- ■ End loop over boundaries
- ■ Loop over reconstruction points {ci } i∈A (j)
- o End loop over quantizers
- o Loop over quantizers j = 1, ..., |J|
- End loop over iterations
- The described methodology has a fixed initial configuration. It is noted that the algorithm is not guaranteed to converge to a global minimum. Thus, it may be advantageous to start from a set of initial configurations and select the best performing end solution.
- In an alternative embodiment and in a further effort to obtain a good solution for various situations, it may be advantageous to start with a low weight µ and gradually increase this weight with the iteration number in a method resembling simulated annealing.
- It is noted that the training algorithm is significantly faster if not the full performance measures are evaluated for each displacement, but only the terms of eq. (14), (15), and (17) that change with the displacement.
- The training method described above leads to a particular solution for each combination or µ and λ. Recomputation is necessary in situations where the allowed rate or the signal changes with time. In such scenarios the quantizer ensemble must be adapted to the particular situation at hand and that requires a fast computation of the quantizer. An alternative embodiment exploits that, in general, the quantizer ensemble solutions vary gradually as a function of variation or µ and λ. This can be exploited for fast computation of a set of quantizers in the following training and quantization method.
- The training method is:
- 1. Define a set of values for or µ and λ that cover the space of values for µ and λ that are expected to be used in a particular time-varying scenario.
- 2. Run the training algorithm (see subsection IV above) for each member of the set and obtain a quantizer ensemble for each member of the set.
- 3. Store the set of quantizer ensembles, each labelled with an index p and the vector v p = [µp ,λp ] in a table.
- It is now possible to define an adaptive quantization method. An embodiment of an adaptive quantization method is:
- 1. Acquire the desired weights v = [µ,λ] for the scenario at hand.
- 2. Based on the vector v = [µ,λ] find the |P| nearest neighbours {vp } p∈P in the table such that v = [µ,λ] is inside their convex hull. The neighbours can be found using a simple squared error or weighted squared error criterion: (v - vp )C(v - vp )t, where C is a weighting matrix and a checking of the hull condition. |P| is advantageously selected to be 3 but can be larger than 3.
- 3. Find a suitable positive weighting vector s |P|, with s lPl ts lP | = 1. For the illustrative embodiment where |P|=3, find s |P| such that v = s 1 v 1 + s2v2 + s 3 v 3 (always possible because of the convexhull condition).
- 4. Compute the quantizer ensemble or the quantizer needed. To compute the boundaries, compute the s lP|-weighted sum of corresponding boundaries of the neighbouring quantizer ensembles or quantizers. To compute the reconstruction points, compute the s |P| weighted sum of corresponding reconstruction points of the |P| neighbouring quantizer ensembles or quantizers.
- While the invention has been illustrated and described in detail in the drawings and foregoing description, such illustration and description are to be considered illustrative or exemplary and not restrictive; the invention is not limited to the disclosed embodiments. Alternative embodiments of the present invention may differ as regards, at least, the source signal distribution estimation, the fineness of the quantization cells, the distortion measure, the choice of reconstruction distribution and the algorithm for sampling the reconstruction distribution.
- Other variations to the disclosed embodiments can be understood and effectuated by those skilled in the art in practicing the claimed invention, from a study of the drawings, the disclosure, and the appended claims. In the claims, the word 'comprising' does not exclude other elements or steps, and the indefinite article 'a' or 'an' does not exclude a plurality. A single processor or other unit may fulfil the functions of several items received in the claims. The mere fact that certain measures are recited in mutually different dependent claims does not indicate that a combination of these measured cannot be used to advantage. A computer program may be stored or distributed on a suitable medium, such as an optical storage medium or a solid-state medium supplied together with or as part of other hardware, but may also be distributed in other forms, such as via the Internet or other wired or wireless telecommunication systems. Any reference signs in the claims should not be construed as limiting the scope.
Claims (11)
- A method for coding a sequence (X) of samples of a one-dimensional or multidimensional audio or video source signal as a sequence (I) of quantization indices, the method including:defining an ensemble of quantizers (φ(j),j ∈ J), each of which comprises a partition into quantization cells and a reconstruction point associated with each quantization cell; characterized byselecting, either randomly or following a predetermined sequence, for each source signal sample, a quantizer from said ensemble of quantizers which is to be used for encoding the sample; andencoding the source signal sample by a quantization index indicating a quantization cell of the selected quantizer, wherein the quantization cell contains the source signal sample,wherein, if the selection of quantizers is random, the method further includes:outputting an indication of past and/or present selections of quantizers, so as to facilitate subsequent decoding.
- A method according to claim 1, further including:assigning, to each quantizer in said ensemble of quantizers, a selection probability governing the random selection of quantizers.
- A method according to claim 2, wherein the selection probabilities are determined so as to optimize a measure of fidelity for the probability distribution of the reconstructed signal, such as a relative entropy between a source signal and a reconstructed signal.
- A method for decoding a one-dimensional or multidimensional audio or video source signal encoded as a sequence (I) of quantization indices, the method including:obtaining an ensemble of quantizers (φ(j), j ∈ J), each of which comprises a plurality of reconstruction points;and characterized by:for each quantization index:(i) obtaining an indication of a particular quantizer to which said quantization index refers by accessing either stored past random selections or a predetermined sequence, on which the selection of quantizers is based during encoding; and(ii) decoding said quantization index by a corresponding reconstruction point.
- A method according to any one of the preceding claims, wherein each quantizer (φ(j)) in said ensemble of quantizers is defined on the basis of optimizations of quantization cell locations and reconstruction points with respect to a fixed objective function which is a weighted sum of at least:a bit rate,a signal distortion, anda fidelity of the probability distribution of a source signal and a reconstructed signal, such as a relative entropy between a source signal and a reconstructed signal.
- A method according to claim 5 referring to any of claims 1 to 3, further comprising designing an ensemble of optimal quantizers (φ(j), j ∈ J) by the following steps:receiving a sequence (XT ) of training signal samples;defining said fixed objective function (η);defining a displacement step (Cσ) proportional to the standard deviation (σ) of the training signal samples;for each quantizer to be generated:defining an initial quantizer (φ̂(j)) comprising a partition into quantization cells and corresponding reconstruction points;performing at least one iteration of the following steps:(i) locally optimizing, within the displacement step, the partition with respect to the objective function (η) evaluated for the sequence (XT ) of training signal samples, thereby obtaining an optimized partition;(ii) locally optimizing each of said initial reconstruction points within at most the displacement step (Cσ) with respect to the objective function (η) evaluated for the sequence (XT ) of training signal samples, thereby obtaining an optimized reconstruction point (ci (j)); andstoring a quantizer (φ(j)) comprising the optimized quantization cells and reconstruction points; andoutputting an ensemble of quantizers (φ(j), j ∈ J) containing the stored quantizers.
- A method for adaptively encoding a sequence of samples of a one-dimensional or multidimensional audio or video source signal, the method including the steps of any one of claims 5 or 6 referring to any of claims 1 to 3 and further including:defining, responsive to an observed variation of properties of the source signal samples, a further ensemble of quantizers on the basis of optimizations with respect to a further fixed objective function, which is a weighted sum of at least:a bit rate,a signal distortion, anda fidelity of the probability distributions of a source signal and a reconstructed signal, such as a relative entropy between a source signal and a reconstructed signal,wherein the weighting of the further objective function is determined in accordance with the properties of the source signal samples.
- A computer-program product comprising computer-readable instructions which when executed perform any one of the methods set forth in the preceding claims.
- An encoder for coding a sequence (X) of samples of a one-dimensional or multidimensional audio or video source signal as a sequence (I) of quantization indices, the encoder including:a quantizer memory (201) for storing an ensemble of quantizers (φ(j), j ∈ J), each of which comprises a partition into quantization cells and a reconstruction point in each quantization cell; characterized by further including :a quantizer selector (202) for selecting, either randomly or based on a predetermined sequence, for each source signal sample, a quantizer from said stored ensemble of quantizers which is to be used for encoding the sample; andan encoding device (203) for encoding each source signal sample by a quantization index indicating a quantization cell of the quantizer selected for the source signal sample that contains the source signal sample, wherein the quantizer selector comprises:either a sequence memory for storing a predefined sequence governing the selection of quantizers,or a random selector adapted to select a quantizer from said ensemble of quantizers and for storing a sequence indicative of at least the latest selected quantizers, so as to facilitate subsequent decoding.
- An encoder according to claim 9, wherein the quantizer memory further is adapted to store a selection probability associated with each quantizer, based on which the random selector is adapted to operate.
- A decoder for decoding a one-dimensional or multidimensional audio or video source signal encoded as a sequence (I) of quantization indices, the encoder including:a quantizer memory for storing an ensemble of quantizers (φ(j), j E J), each comprising a plurality of reconstruction points;characterized by:a quantizer indicator for storing, for each quantization index, an indication of a particular quantizer to which it refers, said indications being either stored past random selections or a predetermined sequence on which the selection of quantizers is based during encoding; anda decoding device for decoding said quantization index by a corresponding reconstruction point.
Priority Applications (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
EP10155236A EP2372699B1 (en) | 2010-03-02 | 2010-03-02 | Coding of audio or video samples using multiple quantizers |
PCT/EP2011/052936 WO2011107434A1 (en) | 2010-03-02 | 2011-02-28 | Distribution-constrained quantization |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
EP10155236A EP2372699B1 (en) | 2010-03-02 | 2010-03-02 | Coding of audio or video samples using multiple quantizers |
Publications (2)
Publication Number | Publication Date |
---|---|
EP2372699A1 EP2372699A1 (en) | 2011-10-05 |
EP2372699B1 true EP2372699B1 (en) | 2012-12-19 |
Family
ID=42289611
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP10155236A Active EP2372699B1 (en) | 2010-03-02 | 2010-03-02 | Coding of audio or video samples using multiple quantizers |
Country Status (2)
Country | Link |
---|---|
EP (1) | EP2372699B1 (en) |
WO (1) | WO2011107434A1 (en) |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
BR112015025009B1 (en) | 2013-04-05 | 2021-12-21 | Dolby International Ab | QUANTIZATION AND REVERSE QUANTIZATION UNITS, ENCODER AND DECODER, METHODS FOR QUANTIZING AND DEQUANTIZING |
US10645386B1 (en) | 2019-01-03 | 2020-05-05 | Sony Corporation | Embedded codec circuitry for multiple reconstruction points based quantization |
Family Cites Families (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JPS62222783A (en) * | 1986-03-24 | 1987-09-30 | Kokusai Denshin Denwa Co Ltd <Kdd> | Highly efficient encoding system for animation picture |
JPH05500445A (en) * | 1990-05-14 | 1993-01-28 | イーストマン・コダック・カンパニー | Block adaptive linear predictive coding with multidimensional adaptive gain and bias |
JP2003018593A (en) * | 2001-06-29 | 2003-01-17 | Matsushita Electric Ind Co Ltd | Method and apparatus for encoding video, method and apparatus for allocating video coding quantity, and recording medium |
KR100446630B1 (en) * | 2002-05-08 | 2004-09-04 | 삼성전자주식회사 | Vector quantization and inverse vector quantization apparatus for the speech signal and method thereof |
-
2010
- 2010-03-02 EP EP10155236A patent/EP2372699B1/en active Active
-
2011
- 2011-02-28 WO PCT/EP2011/052936 patent/WO2011107434A1/en active Application Filing
Also Published As
Publication number | Publication date |
---|---|
EP2372699A1 (en) | 2011-10-05 |
WO2011107434A1 (en) | 2011-09-09 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
KR101343267B1 (en) | Method and apparatus for audio coding and decoding using frequency segmentation | |
CN101223582B (en) | Audio frequency coding method, audio frequency decoding method and audio frequency encoder | |
RU2696292C2 (en) | Audio encoder and decoder | |
JP4963498B2 (en) | Quantization of speech and audio coding parameters using partial information about atypical subsequences | |
KR101278805B1 (en) | Selectively using multiple entropy models in adaptive coding and decoding | |
US10404984B2 (en) | Method and apparatus for pyramid vector quantization indexing and de-indexing of audio/video sample vectors | |
EP2490215A2 (en) | Method and apparatus to extract important spectral component from audio signal and low bit-rate audio signal coding and/or decoding method and apparatus using the same | |
US6128346A (en) | Method and apparatus for quantizing a signal in a digital system | |
JP2007506986A (en) | Multi-resolution vector quantization audio CODEC method and apparatus | |
RU2640722C2 (en) | Improved quantizer | |
KR20190040063A (en) | Quantizer with index coding and bit scheduling | |
US8285544B2 (en) | Restrained vector quantisation | |
CN110491398B (en) | Encoding method, encoding device, and recording medium | |
EP2372699B1 (en) | Coding of audio or video samples using multiple quantizers | |
JP2010500819A (en) | A method for quantizing speech and audio by efficient perceptual related retrieval of multiple quantization patterns | |
KR20240022588A (en) | Compress audio waveforms using neural networks and vector quantizers | |
EP3248190B1 (en) | Method of encoding, method of decoding, encoder, and decoder of an audio signal | |
EP4229632A1 (en) | Signal coding using a generative model and latent domain quantization | |
WO2022130477A1 (en) | Encoding device, decoding device, encoding method, decoding method, and program | |
WO2022190195A1 (en) | Information processing system, encoding device, decoding device, model learning device, information processing method, encoding method, decoding method, model learning method, and program storage medium | |
CN117616498A (en) | Compression of audio waveforms using neural networks and vector quantizers | |
RU2022107245A (en) | MULTI-LAY FORMAT FOR AUDIO CODING | |
CN116708844A (en) | Encoding method, decoding method, device, medium and computing equipment | |
Iordache et al. | Index assignment for transmitting vector quantized LSF parameters over binary Markov channels | |
You | Linear Prediction |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
AK | Designated contracting states |
Kind code of ref document: A1Designated state(s): AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO SE SI SK SM TR |
|
AX | Request for extension of the european patent |
Extension state: AL BA ME RS |
|
17P | Request for examination filed |
Effective date: 20120309 |
|
17Q | First examination report despatched |
Effective date: 20120420 |
|
RAP1 | Party data changed (applicant data changed or rights of an application transferred) |
Owner name: GOOGLE INC. |
|
GRAP | Despatch of communication of intention to grant a patent |
Free format text: ORIGINAL CODE: EPIDOSNIGR1 |
|
GRAS | Grant fee paid |
Free format text: ORIGINAL CODE: EPIDOSNIGR3 |
|
GRAA | (expected) grant |
Free format text: ORIGINAL CODE: 0009210 |
|
AK | Designated contracting states |
Kind code of ref document: B1Designated state(s): AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO SE SI SK SM TR |
|
REG | Reference to a national code |
Ref country code: GBRef legal event code: FG4D |
|
RIN1 | Information on inventor provided before grant (corrected) |
Inventor name: LI, MINYUEInventor name: KLEIJN, WILLEM BASTIAAN |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: EP |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: REFRef document number: 589745Country of ref document: ATKind code of ref document: TEffective date: 20130115 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R096Ref document number: 602010004134Country of ref document: DEEffective date: 20130221 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ESFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20130330Ref country code: FIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20121219Ref country code: NOFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20130319Ref country code: SEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20121219Ref country code: LTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20121219 |
|
REG | Reference to a national code |
Ref country code: NLRef legal event code: VDEPEffective date: 20121219Ref country code: ATRef legal event code: MK05Ref document number: 589745Country of ref document: ATKind code of ref document: TEffective date: 20121219 |
|
REG | Reference to a national code |
Ref country code: LTRef legal event code: MG4D |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LVFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20121219Ref country code: GRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20130320Ref country code: SIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20121219 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ISFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20130419Ref country code: SKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20121219Ref country code: EEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20121219Ref country code: CZFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20121219Ref country code: BEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20121219Ref country code: ATFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20121219Ref country code: BGFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20130319 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: NLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20121219Ref country code: PLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20121219Ref country code: PTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20130419Ref country code: ROFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20121219 |
|
PLBE | No opposition filed within time limit |
Free format text: ORIGINAL CODE: 0009261 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: NO OPPOSITION FILED WITHIN TIME LIMIT |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MCFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20130331Ref country code: DKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20121219 |
|
26N | No opposition filed |
Effective date: 20130920 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: CYFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20121219Ref country code: HRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20121219 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ITFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20121219 |
|
REG | Reference to a national code |
Ref country code: IERef legal event code: MM4A |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R097Ref document number: 602010004134Country of ref document: DEEffective date: 20130920 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: IEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20130302 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20121219 |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: PL |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LIFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20140331Ref country code: CHFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20140331 |
|
REG | Reference to a national code |
Ref country code: FRRef legal event code: PLFPYear of fee payment: 6 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SMFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20121219 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: FRPayment date: 20150317Year of fee payment: 6 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: TRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20121219 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20121219Ref country code: LUFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20130302Ref country code: HUFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMIT; INVALID AB INITIOEffective date: 20100302 |
|
REG | Reference to a national code |
Ref country code: FRRef legal event code: STEffective date: 20161130 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: FRFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20160331 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R081Ref document number: 602010004134Country of ref document: DEOwner name: GOOGLE LLC (N.D.GES.D. STAATES DELAWARE), MOUN, USFree format text: FORMER OWNER: GOOGLE, INC., MOUNTAIN VIEW, CALIF., US |
|
P01 | Opt-out of the competence of the unified patent court (upc) registered |
Effective date: 20230505 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: DEPayment date: 20240327Year of fee payment: 15Ref country code: GBPayment date: 20240327Year of fee payment: 15 |