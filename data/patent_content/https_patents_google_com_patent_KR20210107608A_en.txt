KR20210107608A - Systems and methods for extracting temporal information from animated media content items using machine learning - Google Patents
Systems and methods for extracting temporal information from animated media content items using machine learning Download PDFInfo
- Publication number
- KR20210107608A KR20210107608A KR1020217001267A KR20217001267A KR20210107608A KR 20210107608 A KR20210107608 A KR 20210107608A KR 1020217001267 A KR1020217001267 A KR 1020217001267A KR 20217001267 A KR20217001267 A KR 20217001267A KR 20210107608 A KR20210107608 A KR 20210107608A
- Authority
- KR
- South Korea
- Prior art keywords
- media content
- content item
- text string
- temporal analysis
- computing system
- Prior art date
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/40—Extraction of image or video features
- G06V10/62—Extraction of image or video features relating to a temporal dimension, e.g. time-based feature extraction; Pattern tracking
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/80—Generation or processing of content or additional data by content creator independently of the distribution process; Content per se
- H04N21/83—Generation or processing of protective or descriptive data associated with content; Content structuring
- H04N21/845—Structuring of content, e.g. decomposing content into time segments
- H04N21/8456—Structuring of content, e.g. decomposing content into time segments by decomposing the content in the time domain, e.g. in time segments
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/02—Knowledge representation; Symbolic representation
- G06N5/022—Knowledge engineering; Knowledge acquisition
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/40—Information retrieval; Database structures therefor; File system structures therefor of multimedia data, e.g. slideshows comprising image and additional audio data
- G06F16/43—Querying
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/40—Information retrieval; Database structures therefor; File system structures therefor of multimedia data, e.g. slideshows comprising image and additional audio data
- G06F16/43—Querying
- G06F16/435—Filtering based on additional data, e.g. user or group profiles
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/40—Information retrieval; Database structures therefor; File system structures therefor of multimedia data, e.g. slideshows comprising image and additional audio data
- G06F16/48—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/489—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using time information
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/78—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/77—Processing image or video features in feature spaces; using data integration or data reduction, e.g. principal component analysis [PCA] or independent component analysis [ICA] or self-organising maps [SOM]; Blind source separation
- G06V10/776—Validation; Performance evaluation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/41—Higher-level, semantic clustering, classification or understanding of video scenes, e.g. detection, labelling or Markovian modelling of sport events or news items
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/60—Type of objects
- G06V20/62—Text, e.g. of license plates, overlay texts or captions on TV images
- G06V20/635—Overlay text, e.g. embedded captions in a TV program
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/70—Labelling scene content, e.g. deriving syntactic or semantic representations
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/45—Management operations performed by the client for facilitating the reception of or the interaction with the content or administrating data related to the end-user or to the client device itself, e.g. learning user preferences for recommending movies, resolving scheduling conflicts
- H04N21/466—Learning process for intelligent management, e.g. learning user preferences for recommending movies
- H04N21/4662—Learning process for intelligent management, e.g. learning user preferences for recommending movies characterized by learning algorithms
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/80—Generation or processing of content or additional data by content creator independently of the distribution process; Content per se
- H04N21/81—Monomedia components thereof
- H04N21/812—Monomedia components thereof involving advertisement data
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/80—Generation or processing of content or additional data by content creator independently of the distribution process; Content per se
- H04N21/81—Monomedia components thereof
- H04N21/8146—Monomedia components thereof involving graphical data, e.g. 3D object, 2D graphics
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/082—Learning methods modifying the architecture, e.g. adding, deleting or silencing nodes or connections
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N7/00—Computing arrangements based on specific mathematical models
- G06N7/01—Probabilistic graphical models, e.g. probabilistic networks
Abstract
컴퓨터로 구현되는 방법은 하나 이상의 컴퓨팅 디바이스를 포함하는 컴퓨팅 시스템에 의해 순차 디스플레이를 위한 복수의 이미지 프레임을 포함하는 미디어 컨텐츠 아이템을 기술하는 데이터를 수신하는 단계를 포함할 수 있다. 방법은 컴퓨팅 시스템에 의해 미디어 컨텐츠 아이템을 기술하는 데이터를 미디어 컨텐츠 아이템을 기술하는 데이터를 수신하도록 구성된 기계 학습 시간 분석 모델에 입력하는 단계, 및 미디어 컨텐츠 아이템을 기술하는 데이터를 수신하는 것에 응답하여, 미디어 컨텐츠 아이템의 복수의 이미지 프레임을 순차적으로 시청하는 것과 관련된 시간 정보를 기술하는 시간 분석 데이터를 출력하는 단계를 포함한다. 방법은 컴퓨팅 시스템에 의해 그리고 기계 학습된 시간 분석 모델의 출력으로서 시간 분석 데이터를 수신하는 것을 포함할 수 있다.The computer-implemented method can include receiving, by a computing system including one or more computing devices, data describing an item of media content that includes a plurality of image frames for sequential display. The method includes inputting, by the computing system, data describing the media content item into a machine learning temporal analysis model configured to receive data describing the media content item, and in response to receiving the data describing the media content item, and outputting temporal analysis data describing temporal information related to sequentially viewing a plurality of image frames of the media content item. The method may include receiving temporal analysis data by the computing system and as an output of the machine learned temporal analysis model.
Description
본 출원은 2020년 2월 21일에 출원된 미국 가출원 번호 62/979,624의 계속 출원으로, 그 개시 내용은 모든 목적을 위해 전체가 본 원에 참고로 포함된다.This application is a continuation-on of U.S. Provisional Application No. 62/979,624, filed on February 21, 2020, the disclosure of which is incorporated herein by reference in its entirety for all purposes.
본 개시는 일반적으로 컴퓨터 비전에 관한 것이다. 보다 구체적으로, 본 개시는 기계 학습을 사용하여 애니메이션 미디어 컨텐츠 아이템에서 시간 정보를 추출하기 위한 시스템 및 방법에 관한 것이다.BACKGROUND This disclosure relates generally to computer vision. More particularly, this disclosure relates to systems and methods for extracting temporal information from animated media content items using machine learning.
모바일 디바이스의 사용자 간의 통신을 위한 기존의 접근 방식은 단순히 SMS, 소셜 네트워킹 애플리케이션을 통한 메시징 또는 "문자 메시지"에 의존할 수 있다. 인터넷 또는 모바일 디바이스 사용자들은 이러한 다양한 매체를 통해 메시지를 교환할 수 있다. 그러나, 종종 사용자는 GIF(Graphics Interchange Format)와 같은 미디어 컨텐츠를 통해 또는 정적 또는 애니메이션(된) 이미지 세트를 포함하는 이미지 파일을 통해 통신하기를 원할 수 있다. 사용자들은 인터넷에서 GIF를 검색하고, 운영 체제의 기본 웹 브라우저를 통해 이것을 복사하고, 다양한 메시징 애플리케이션에 이 GIF를 붙여 넣을 수 있다. 이러한 기존 시스템은 자원을 낭비하거나 수동 개입 없이 동적 인터페이스 내에서 분류된 컨텐츠를 제공하는데 적합하지 않다. 더욱이 이러한 종래의 시스템은 미디어 컨텐츠 아이템의 뷰어(viewer)에게 전달되는 애니메이션 미디어 컨텐츠 아이템에 관한 유용한 시간 정보를 추출하지 못한다.Existing approaches for communication between users of mobile devices may simply rely on SMS, messaging via social networking applications, or “text messaging”. Internet or mobile device users may exchange messages through these various media. However, often users may wish to communicate via media content, such as in Graphics Interchange Format (GIF), or via image files containing static or animated (animated) image sets. Users can search for GIFs on the Internet, copy them through the operating system's default web browser, and paste these GIFs into various messaging applications. These existing systems are not suitable for providing classified content within a dynamic interface without wasting resources or manual intervention. Moreover, these conventional systems fail to extract useful temporal information about the animated media content item that is delivered to the viewer of the media content item.
본 개시의 실시예의 양태 및 장점은 다음의 설명에서 부분적으로 설명되거나, 그 설명으로부터 학습될 수 있거나, 그 실시예의 실행을 통해 학습될 수 있다.Aspects and advantages of embodiments of the present disclosure are set forth in part in the description that follows, may be learned from the description, or may be learned through practice of the embodiments.
본 개시의 일 양태에 따르면, 컴퓨터 구현 방법은 하나 이상의 컴퓨팅 디바이스를 포함하는 컴퓨팅 시스템에 의해, 순차 디스플레이를 위한 복수의 이미지 프레임을 포함하는 미디어 컨텐츠 아이템을 기술하는 데이터를 수신하는 단계와; 컴퓨팅 시스템에 의해, 미디어 컨텐츠 아이템을 기술하는 데이터를 수신하고, 상기 미디어 컨텐츠 아이템을 기술하는 데이터를 수신하는 것에 응답하여 미디어 컨텐츠 아이템의 복수의 이미지 프레임을 순차적으로 시청하는 것과 관련된 시간 정보를 기술하는 시간 분석 데이터 출력하도록 구성된 기계 학습된 시간 분석 모델에 상기 미디어 컨텐츠 아이템을 기술하는 데이터를 입력하는 단계와; 그리고 컴퓨팅 시스템에 의해 기계 학습된 시간 분석 모델의 출력으로서, 시간 분석 데이터를 수신하는 단계를 포함할 수 있다. According to one aspect of the present disclosure, a computer implemented method includes: receiving, by a computing system including one or more computing devices, data describing a media content item comprising a plurality of image frames for sequential display; receiving, by the computing system, data describing a media content item and describing temporal information associated with sequentially viewing a plurality of image frames of the media content item in response to receiving the data describing the media content item inputting data describing the media content item into a machine learned temporal analysis model configured to output temporal analysis data; and receiving the temporal analysis data as an output of the temporal analysis model machine learned by the computing system.
본 개시의 다른 양태에 따르면, 컴퓨팅 시스템은 하나 이상의 프로세서와; 미디어 컨텐츠 아이템을 기술하는 데이터를 수신하고, 미디어 컨텐츠 아이템을 기술하는 데이터를 수신하는 것에 응답하여 미디어 컨텐츠 아이템의 복수의 이미지 프레임을 순차적으로 시청하는 것과 관련된 시간 정보를 기술하는 시간 분석 데이터를 출력하도록 구성된 기계 학습된 시간 분석 모델을 포함할 수 있다. 시간 정보는 복수의 이미지 프레임의 개별 이미지 프레임에 의해 기술되지 않는다. 컴퓨팅 시스템은 하나 이상의 프로세서에 의해 실행될 때 컴퓨팅 시스템으로 하여금 동작들을 수행하게 하는 명령들을 집합적으로 저장하는 하나 이상의 비-일시적 컴퓨터 판독 가능 매체를 포함할 수 있다. 상기 동작들은 순차 디스플레이를 위한 복수의 이미지 프레임을 포함하는 미디어 컨텐츠 아이템을 기술하는 데이터를 수신하는 단계; 미디어 컨텐츠 아이템을 기술하는 데이터를 기계 학습된 시간 분석 모델에 입력하는 단계; 및 기계 학습된 시간 분석 모델의 출력으로서, 시간 분석 데이터를 수신하는 단계를 포함할 수 있다.According to another aspect of the present disclosure, a computing system includes one or more processors; receive data describing the media content item, and in response to receiving the data describing the media content item, output temporal analysis data describing temporal information associated with sequentially viewing a plurality of image frames of the media content item; It may include a constructed machine-learned temporal analysis model. The temporal information is not described by individual image frames of a plurality of image frames. The computing system may include one or more non-transitory computer-readable media that collectively store instructions that, when executed by one or more processors, cause the computing system to perform operations. The operations may include receiving data describing a media content item comprising a plurality of image frames for sequential display; inputting data describing media content items into a machine-learned temporal analysis model; and receiving, as an output of the machine-learned temporal analysis model, temporal analysis data.
본 개시의 다른 양태에 따르면, 기계 학습된 시간 분석 모델을 트레이닝하기 위한 컴퓨터 구현 방법은 하나 이상의 컴퓨팅 디바이스를 포함하는 컴퓨팅 시스템에 의해, 순차 디스플레이를 위한 복수의 이미지 프레임을 포함하는 미디어 컨텐츠 아이템을 기술하는 데이터를 수신하는 단계와; 컴퓨팅 시스템에 의해, 미디어 컨텐츠 아이템을 기술하는 데이터를 수신하고, 그 미디어 컨텐츠 아이템을 기술하는 데이터를 수신하는 것에 응답하여 미디어 컨텐츠 아이템의 복수의 이미지 프레임을 순차적으로 시청하는 것과 관련된 시간 정보를 기술하는 시간 분석 데이터를 출력하도록 구성된 기계 학습된 시간 분석 모델에 상기 미디어 컨텐츠 아이템을 기술하는 데이터를 입력하는 단계를 포함할 수 있다. 시간 정보는 복수의 이미지 프레임의 개별 이미지 프레임에 의해 기술되지 않는다. 이 방법은 컴퓨팅 시스템에 의해 기계 학습된 시간 분석 모델의 출력으로서, 시간 분석 데이터를 수신하는 단계와; 그리고 컴퓨팅 시스템에 의해, 시간 분석 데이터와 지상 실측 시간 분석 데이터의 비교에 기초하여 상기 기계 학습된 시간 분석 모델의 하나 이상의 파라미터를 조정하는 단계를 포함할 수 있다. According to another aspect of the present disclosure, a computer implemented method for training a machine learned temporal analysis model describes, by a computing system including one or more computing devices, a media content item comprising a plurality of image frames for sequential display. receiving data to receiving, by the computing system, data describing a media content item, and in response to receiving data describing the media content item, describing temporal information associated with sequentially viewing a plurality of image frames of the media content item and inputting data describing the media content item into a machine learned temporal analysis model configured to output temporal analysis data. The temporal information is not described by individual image frames of a plurality of image frames. The method includes receiving temporal analysis data as an output of a temporal analysis model machine learned by a computing system; and adjusting, by the computing system, one or more parameters of the machine-learned temporal analysis model based on the comparison of the temporal analysis data and the ground-ground temporal analysis data.
본 개시의 다른 양태들은 다양한 시스템, 장치, 비-일시적 컴퓨터 판독 가능 매체, 사용자 인터페이스 및 전자 디바이스에 관한 것이다.Other aspects of the disclosure relate to various systems, apparatus, non-transitory computer-readable media, user interfaces, and electronic devices.
본 개시의 다양한 실시예의 이들 및 다른 특징, 양태 및 이점은 다음의 설명 및 첨부된 청구 범위를 참조하여 더 잘 이해될 것이다. 본 명세서에 포함되어 그 일부를 구성하는 첨부 도면은 본 개시의 예시적인 실시예를 예시하고, 상세한 설명과 함께 관련 원리를 기술하는 역할을 한다.These and other features, aspects and advantages of various embodiments of the present disclosure will be better understood with reference to the following description and appended claims. BRIEF DESCRIPTION OF THE DRAWINGS The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate exemplary embodiments of the present disclosure and, together with the description, serve to describe the relevant principles.
당업자들을 향한 실시예들의 상세한 논의는 첨부된 도면들을 참조하는 명세서에 기재되어 있다.
도 1a는 본 개시의 예시적인 실시예들에 따른 하나 이상의 기계 학습 모델을 사용하여 애니메이션 미디어 컨텐츠 아이템들로부터 시간 정보를 추출하기 위한 예시적인 컴퓨팅 시스템의 블록도를 도시한다.
도 1b는 본 개시의 예시적인 실시예들에 따른 하나 이상의 기계 학습 모델을 사용하여 애니메이션 미디어 컨텐츠 아이템들로부터 시간 정보를 추출하기 위한 예시적인 컴퓨팅의 블록도를 도시한다.
도 1c는 본 개시의 예시적인 실시예들에 따른 하나 이상의 기계 학습된 모델을 사용하여 애니메이션 미디어 컨텐츠 아이템들로부터 시간 정보를 추출하기 위한 예시적인 컴퓨팅 디바이스의 블록도를 도시한다.
도 2는 본 개시의 예시적인 실시예들에 따른 예시적인 기계 학습 시간 분석 모델의 블록도를 도시한다.
도 3은 본 개시의 예시적인 실시예들에 따른 하나 이상의 기계 학습 모델을 사용하여 애니메이션 미디어 컨텐츠 아이템들로부터 시간 정보를 추출하기 위한 예시적인 방법의 흐름도를 도시한다.
도 4는 본 개시의 예시적인 실시예에 따른 일련의 텍스트 문자열을 디스플레이하는 예시적인 애니메이션 미디어 컨텐츠 아이템의 개략도이다.
도 5a는 본 개시의 양태에 따른 미디어 컨텐츠 관리 시스템을 도시하는 고레벨 블록도이다.
도 5b는 본 개시의 양태에 따른 동적 인터페이스에서 애니메이션 입력들을 구현하기 위해 검색을 수행하는 프로세스를 도시하는 고레벨 블록도이다.
도 6a는 본 발명의 양태에 따른 일 실시예에 따라 미디어 컨텐츠 관리 시스템의 블록도를 도시하는 미디어 컨텐츠 관리 시스템의 표현적 미디어 컨텐츠를 조달, 구성 및 검색하기 위한 시스템의 네트워크 다이어그램이다.
도 6b는 본 개시의 양태에 따른 일 실시예에 따라 미디어 컨텐츠 관리 시스템에서 검색을 수행하기 위해 조달된 컨텐츠를 분류하기 위한 시스템의 상위 레벨 블록도이다.
도 6c는 본 개시의 양태에 따른 미디어 컨텐츠 관리 시스템에서 복합 컨텐츠 아이템을 구성하기 위한 시스템의 상위 레벨 블록도이다.
도 7a는 본 개시의 양태에 따른 미디어 컨텐츠 관리 시스템에서 조달된 컨텐츠를 분류하기 위한 시스템의 상위 레벨 블록도이다.
도 7b는 본 개시의 양태에 따른 동적 인터페이스에서 애니메이션 입력들을 구현하기 위해 검색을 수행하기 위한 시스템의 고레벨 블록도이다.
도 8a-c는 본 개시의 양태에 따른 미디어 컨텐츠 관리 시스템에서 컨텐츠와 상호 작용하기 위해 제공되는 동적 키보드 인터페이스의 예시적인 스크린 샷이다. 과
도 9는 본 개시의 양태에 따라 표현적 미디어 컨텐츠를 조달, 구성 및/또는 검색하도록 구성된 디바이스에 배치된 예시적인 컴퓨팅 플랫폼을 도시한다.
복수의 도면에 걸쳐 반복되는 참조 번호는 다양한 구현에서 동일한 특징을 식별하기 위한 것이다.A detailed discussion of embodiments directed to those skilled in the art is set forth in the specification with reference to the accompanying drawings.
1A shows a block diagram of an example computing system for extracting temporal information from animated media content items using one or more machine learning models in accordance with example embodiments of the present disclosure.
1B illustrates a block diagram of example computing for extracting temporal information from animated media content items using one or more machine learning models in accordance with example embodiments of the present disclosure.
1C illustrates a block diagram of an example computing device for extracting temporal information from animated media content items using one or more machine learned models in accordance with example embodiments of the present disclosure.
2 depicts a block diagram of an exemplary machine learning temporal analysis model in accordance with example embodiments of the present disclosure.
3 depicts a flow diagram of an example method for extracting temporal information from animated media content items using one or more machine learning models in accordance with example embodiments of the present disclosure.
4 is a schematic diagram of an exemplary animated media content item displaying a series of text strings in accordance with an exemplary embodiment of the present disclosure;
5A is a high-level block diagram illustrating a media content management system in accordance with aspects of the present disclosure.
5B is a high-level block diagram illustrating a process for performing a search to implement animation inputs in a dynamic interface in accordance with an aspect of the present disclosure.
6A is a network diagram of a system for procuring, organizing, and retrieving expressive media content in a media content management system, illustrating a block diagram of a media content management system in accordance with an embodiment of the present invention.
6B is a high-level block diagram of a system for categorizing procured content for performing a search in a media content management system in accordance with an embodiment in accordance with aspects of the present disclosure.
6C is a high-level block diagram of a system for organizing complex content items in a media content management system in accordance with aspects of the present disclosure.
7A is a high-level block diagram of a system for categorizing procured content in a media content management system in accordance with aspects of the present disclosure.
7B is a high-level block diagram of a system for performing a search to implement animation inputs in a dynamic interface in accordance with an aspect of the present disclosure.
8A-C are example screen shots of a dynamic keyboard interface provided for interacting with content in a media content management system in accordance with aspects of the present disclosure. class
9 illustrates an example computing platform deployed in a device configured to procure, organize, and/or retrieve expressive media content in accordance with aspects of the present disclosure.
Reference numbers that are repeated throughout the drawings are intended to identify the same feature in various implementations.
개요summary
일반적으로, 본 개시는 기계 학습을 사용하여 애니메이션(된) 미디어 컨텐츠 아이템으로부터 시간 정보를 추출하기 위한 시스템 및 방법에 관한 것이다. 특정 애니메이션 미디어 컨텐츠 아이템들이 애니메이션으로 보여질 때, 이들은 이 애니메이션 미디어 컨텐츠 아이템들의 임의의 단일 프레임이 분석될 때 전달되지 않을 수 있는 정보를 뷰어(시청자)에게 전달할 수 있다. 이러한 시간 정보를 추출하도록 시간 분석 기계 학습 모델이 트레이닝될 수 있다. 이 정보에는 다양한 유용한 애플리케이션이 있다. 예를 들어, 이 시간 정보는 프레임별로 시청되거나 분석될 때 안전하지 않은(unsafe) 것처럼 보일 수 있는 "안전하지 않은"(예를 들어, 저속하거나 공격적인 등) 미디어 컨텐츠 아이템을 식별하는데 사용될 수 있다. 본 명세서에 설명된 바와 같이 시간 정보를 추출하는 것은 이러한 아이템들의 플래그 지정 및/또는 검색 결과로부터 이러한 아이템들의 제거를 용이하게 할 수 있다. 더욱이, 이러한 시간 정보를 추출하는 것은 미디어 컨텐츠 아이템들이 사용자가 그들을 인식할 때 더 잘 "이해"되기 때문에 그 아이템들의 분류를 개선할 수 있다. 결과적으로 더 나은 검색 결과 및 제안들이 사용자에게 제공될 수 있다.Generally, this disclosure relates to systems and methods for extracting temporal information from animated (animated) media content items using machine learning. When certain animated media content items are shown as animations, they can convey information to the viewer (viewer) that may not be delivered when any single frame of these animated media content items is parsed. A temporal analysis machine learning model can be trained to extract such temporal information. This information has many useful applications. For example, this temporal information can be used to identify "unsafe" (eg, vulgar or offensive, etc.) media content items that may appear unsafe when viewed or analyzed frame by frame. Extracting temporal information as described herein may facilitate flagging such items and/or removal of such items from search results. Moreover, extracting such temporal information can improve the classification of media content items as they are better "understood" when the user recognizes them. As a result, better search results and suggestions can be provided to the user.
보다 구체적으로, 본 개시의 양태에 따르면, 시간 정보를 추출하기 위한 컴퓨터 구현 방법은 GIF(Graphics Interchange Format) 파일과 같은 순차 디스플레이를 위한 복수의 이미지 프레임을 포함하는 미디어 컨텐츠 아이템을 기술(설명)하는 데이터를 수신하는 것을 포함할 수 있다. 예를 들어, 미디어 컨텐츠 아이템을 기술하는 데이터는 각 이미지 프레임의 이미지 컴포넌트(예를 들어, 에지 또는 픽셀 속성)를 정의할 수 있으며, 예를 들어 GIF 파일 자체일 수 있다. 방법은 미디어 컨텐츠 아이템을 기술하는 데이터를 기계 학습(된) 시간 분석 모델에 입력하는 것을 포함할 수 있다. 기계 학습 시간 분석 모델은 미디어 컨텐츠 아이템을 기술하는 데이터를 수신하고, 이에 응답하여 미디어 컨텐츠 아이템의 복수의 이미지 프레임을 순차적으로 시청하는 것과 관련된 시간 정보를 기술하는 시간 분석 데이터를 출력하도록 구성될 수 있다. 전술한 바와 같이, 복수의 이미지 프레임의 개별 이미지 프레임은 이 시간 정보를 전달하거나 기술하지 못할 수 있다. 따라서, 조합된 이미지 프레임 세트에 대한 분석 또는 분류는 각 프레임에 개별적으로 적용되는 기술에 대한 개선을 제공할 수 있다. More specifically, in accordance with aspects of the present disclosure, a computer implemented method for extracting temporal information describes (describes) a media content item comprising a plurality of image frames for sequential display, such as a Graphics Interchange Format (GIF) file. It may include receiving data. For example, the data describing the media content item may define the image component (eg, edge or pixel properties) of each image frame, eg the GIF file itself. The method may include inputting data describing the media content item into a machine learning (old) temporal analysis model. The machine learning temporal analysis model may be configured to receive data describing a media content item and, in response, output temporal analysis data describing temporal information associated with sequentially viewing a plurality of image frames of the media content item. . As mentioned above, individual image frames of a plurality of image frames may not convey or describe this temporal information. Thus, analysis or classification of a set of combined image frames can provide improvements to techniques applied individually to each frame.
예를 들어, 일부 미디어 컨텐츠 아이템은 동적 캡션(자막)을 포함할 수 있으며, 동적 캡션의 단어 또는 문자는 미디어 컨텐츠 아이템이 애니메이션으로 렌더링될 때 순차적으로 디스플레이된다. 애니메이션 미디어 컨텐츠 아이템은 제1 텍스트 문자열(예를 들어, 제1 문구, 단어, 문자 등)을 포함하는 제1 이미지 프레임 및 그 제1 이미지 프레임 이후에 순차적으로 디스플레이하기 위한 제2 텍스트 문자열(예를 들어, 제1 문구, 단어, 문자 등)을 포함하는 제2 이미지 프레임을 포함할 수 있다. 문장이나 문구의 각 단어는 미디어 컨텐츠 아이템이 애니메이션으로 렌더링될 때 순차적으로 디스플레이(예를 들어, 깜박임)될 수 있다. 기계 학습 시간 분석 모델은 애니메이션 미디어 컨텐츠 아이템의 뷰어가 인식하는 완전한 동적 캡션의 시맨틱(의미론적) 의미를 기술하는 시간 분석 데이터를 출력할 수 있다. 이 예에서 시간 분석 데이터는 제2 텍스트 문자열 이전에 순차적으로 판독되는 제1 텍스트 문자열의 시맨틱 의미를 기술할 수 있다. 이 시맨틱 의미는 결합될 때 텍스트 문자열의 의미를 평가하지 않고 제1 텍스트 문자열 및/또는 제2 텍스트 문자열을 개별적으로 판독하는 경우에 부족할 수 있다. 따라서, 시간 분석 모델에 의해 출력되는 시간 정보는 뷰어에 의해 판독될 때 전체 동적 캡션 또는 미디어 컨텐츠 아이템의 시맨틱 의미를 기술할 수 있다.For example, some media content items may include dynamic captions (captions), in which words or characters in the dynamic caption are displayed sequentially as the media content item is rendered into an animation. An animated media content item includes a first image frame including a first text string (eg, a first phrase, word, character, etc.) and a second text string (eg, for sequential display after the first image frame) For example, it may include a second image frame including the first phrase, word, character, etc.). Each word in a sentence or phrase may be displayed sequentially (eg, blinking) as the media content item is rendered into an animation. The machine learning temporal analysis model may output temporal analysis data describing the semantic (semantic) meaning of the full dynamic caption recognized by the viewer of the animated media content item. In this example, the temporal analysis data may describe the semantic meaning of the first text string sequentially read before the second text string. This semantic meaning may be lacking in the case of separately reading the first text string and/or the second text string without evaluating the meaning of the text string when combined. Thus, the temporal information output by the temporal analysis model can describe the semantic meaning of the entire dynamic caption or media content item when read by a viewer.
이러한 동적 캡션의 일 예로서, 제1 텍스트 문자열은 "굿(Good)"을 포함할 수 있고, 제2 텍스트 문자열은 "그리프(Grief)"를 포함할 수 있다. 개별적으로 읽거나 분석되면 "굿"은 행복이나 기쁨을 전달할 수 있고 "그리프"는 슬픔을 전달할 수 있다. 각 텍스트 문자열 분석의 단순 조합은 뷰어가 읽은 대로 동적 캡션의 의미를 포착하지 못한다. 그러나, 기계 학습 시간 분석 모델을 이용하여 분석하는 경우, "굿 그리프"라는 문구를 기술하는 시간 정보를 추출하여 그 문구의 의미를 기술할 수 있으며, 이는 가벼운 좌절감과 같은 완전히 다른 감정을 전달할 수 있다. 따라서, 기계 학습 시간 분석 모델은 애니메이션 미디어 컨텐츠 아이템에 나타나는 텍스트 문자열의 개별 분석보다 동적 캡션으로부터 의미를 더 잘 추출할 수 있다.As an example of such a dynamic caption, the first text string may include “Good” and the second text string may include “Grief”. When read or analyzed individually, "good" can convey happiness or joy, and "griff" can convey sadness. A simple combination of parsing each text string doesn't capture the meaning of the dynamic caption as the viewer reads it. However, when analyzing using a machine learning temporal analysis model, it is possible to extract temporal information describing the phrase "good griff" and describe the meaning of the phrase, which can convey a completely different emotion, such as mild frustration. . Thus, the machine learning temporal analysis model can better extract meaning from dynamic captions than individual analysis of text strings appearing in animated media content items.
또한, 동적 캡션의 일부는 때때로 외관이 변화될 수 있는데, 이는 동적 캡션의 의미에 기인한다. 예를 들어 동적 캡션이 애니메이션으로 렌더링될 때 동적 캡션의 단어 및/또는 문자의 외관(예를 들어, 위치, 색상, 크기, 포ㅌ트 등)이 다를 수 있다. 이러한 외모의 변화는 강조 및 강조 해제와 같은 의미를 전달할 수 있다. 예를 들어, 동적 캡션의 특정 단어는 동적 캡션의 다른 단어에 비해 해당 단어를 더 크게, 더 굵게, 더 생생한 색상 등으로 만듬으로써 강조될 수 있다. 일반적으로 이해되는 바와 같이, 하나의 문장은 강조가 문장에서 어디에 위치하는냐에 따라 다양한 의미를 가질 수 있다. "나는 그녀가 내 돈을 훔쳤다고 결코 말하지 않았다"라는 문장은 강조의 변경이 문장의 의미를 어떻게 변화시킬 수 있는지를 보여주는 예로 사용되었다. 이 문장은 강조되는 단어에 따라 7가지 의미가 있다. " 나는 그녀가 내 돈을 훔쳤다고 결코 말하지 않았다"는 것은 다른 누군가가 돈을 훔쳤다는 것을 의미한다. "나는 그녀가 내 돈을 훔쳤다고 결코 말하지 않았다"는 것은 누군가가 "그녀가 내 돈을 훔쳤다"고 말하는 화자(사람)를 비난했음을 의미한다. 시간 분석 모델은 동적 캡션의 다양한 텍스트 문자열의 상이한 외관에 기초하여 동적 캡션을 의미하는 시맨틱을 기술하는 시간 정보를 추출하도록 트레이닝될 수 있다.Also, some of the dynamic captions may sometimes change in appearance, which is due to the meaning of the dynamic caption. For example, when a dynamic caption is rendered as an animation, the appearance (eg, position, color, size, port, etc.) of words and/or characters in the dynamic caption may be different. These changes in appearance can convey meanings such as highlighting and deemphasizing. For example, certain words in the dynamic caption may be emphasized by making the word larger, bolder, more vivid color, etc. compared to other words in the dynamic caption. As is generally understood, a sentence can have various meanings depending on where the emphasis is placed in the sentence. The sentence "I never said she stole my money" was used as an example to show how a change in emphasis could change the meaning of the sentence. This sentence has seven meanings depending on the word being emphasized. " I never said she stole my money" means someone else stole it. "I never said she stole my money" means that someone blamed the speaker for saying "She stole my money". The temporal analysis model may be trained to extract temporal information that describes the semantics that imply a dynamic caption based on the different appearances of various text strings in the dynamic caption.
유사하게, 시간 분석 모델(들)은 객체, 사람 등의 변화하는 장면(예를 들어, 텍스트가 있거나 없는)에 포함된 시간 정보를 추출할 수 있다. 복수의 이미지 프레임은 제1 장면을 기술하거나 묘사하는 제1 이미지 프레임 및 제1 이미지 프레임 이후에 순차적으로 디스플레이하기 위한 제2 장면을 기술하는 제2 이미지 프레임을 포함할 수 있다. 시간 정보는 제1 장면 또는 제2 장면을 개별적으로 보거나 분석하는 것에 의해 기술되거나 전달되지 않은 제2 장면 이전에 순차적으로 시청되는 제1 장면의 시맨틱 의미를 기술할 수 있다. 또한, 추출된 시간 정보는 특정 장면 이후에 디스플레이되는 특정 캡션의 조합을 기술할 수 있거나 그 반대의 경우도 가능함을 이해해야 한다.Similarly, the temporal analysis model(s) may extract temporal information contained in a changing scene (eg, with or without text) of objects, people, etc. The plurality of image frames may include a first image frame describing or depicting a first scene and a second image frame describing a second scene for sequentially displaying after the first image frame. The temporal information may describe the semantic meaning of a first scene viewed sequentially before a second scene that is not described or delivered by individually viewing or analyzing the first scene or the second scene. It should also be understood that the extracted temporal information may describe a specific combination of captions displayed after a specific scene, or vice versa.
일부 실시예에서, 시간 분석 데이터는 미디어 컨텐츠 아이템의 감정 컨텐츠를 기술할 수 있다. 상이한 감정 컨텐츠의 예로는 불안, 두려움, 분노, 행복, 슬픔, 시기심, 욕망, 관심, 충격, 지루함, 놀라움, 안도감, 혐오감, 수치심, 동정심 및 임의의 다른 인간 감정이 포함될 수 있다. 특정 장면 시퀀스는 프레임 단위 또는 장면 단위로 보거나 분석될 때 각 개별 장면이 특정 감정을 전달하지 못하더라도 그 특정 감정을 전달할 수 있다. 추출된 시간 분석 데이터는 특정 감정을 기술할 수 있다.In some embodiments, the temporal analysis data may describe the emotional content of the media content item. Examples of different emotional content may include anxiety, fear, anger, happiness, sadness, envy, desire, interest, shock, boredom, surprise, relief, disgust, shame, sympathy, and any other human emotion. When viewed or analyzed on a frame-by-frame or scene-by-scene basis, a specific scene sequence can convey a specific emotion even if each individual scene fails to convey that specific emotion. The extracted temporal analysis data may describe a specific emotion.
일부 실시예에서, 시간 분석 모델(예를 들어, 그의 분할 모델)은 (예를 들어, 그것의 일부에 대응하는) 미디어 컨텐츠 아이템으로부터 하나 이상의 미디어 컨텐츠 아이템 세그먼트를 생성하도록 구성될 수 있다. 시간 분석 데이터는 미디어 컨텐츠 아이템의 세그먼트들과 각각 연관된 시간 정보를 기술하는 중간 시간 분석 데이터를 생성하도록 구성될 수 있다. 시간 분석 모델은 중간 시간 분석 데이터를 처리하여 시간 분석 데이터를 생성하도록 구성될 수 있다.In some embodiments, a temporal analysis model (eg, a segmentation model thereof) may be configured to generate one or more media content item segments from a media content item (eg, corresponding to a portion thereof). The temporal analysis data may be configured to generate intermediate temporal analysis data that describes temporal information associated with each segment of the media content item. The temporal analysis model may be configured to process the intermediate temporal analysis data to generate temporal analysis data.
일부 실시예에서, 분할 모델은 표시(appearing), 변경, 이동되는 캡션에 기초하여 및/또는 미디어 컨텐츠 아이템에서 변경되는 장면에 기초하여 미디어 컨텐츠 아이템을 자동으로 분할하도록 구성될 수 있다.In some embodiments, the segmentation model may be configured to automatically segment a media content item based on an appearing, changing, moving caption and/or based on a scene changing in the media content item.
또한, 일부 실시예에서, 시스템(예를 들어, 시간 분석 모델)은 다른 미디어 컨텐츠 아이템의 일부 또는 전부에 대응하는 것으로 미디어 컨텐츠 아이템의 하나 이상의 세그먼트를 식별하도록 구성될 수 있다. 시스템은 다른 미디어 컨텐츠 아이템(예를 들어, 시간 분석 데이터 및/또는 그 메타 데이터)으로부터 세그먼트(들)에 대한 메타 데이터 및/또는 시간 분석 데이터를 검색할 수 있다. 검색된 데이터(예를 들어, 시간 분석 데이터 및/또는 메타 데이터)는 미디어 컨텐츠 아이템에 대한 시간 분석 데이터를 생성하는데 사용될 수 있다. 예를 들어, 검색된 데이터는 미디어 컨텐츠 아이템 자체의 시간 컨텐츠를 기술하는 복합 데이터를 생성하기 위해 분석, 가중, 결합, 보간 등이 될 수 있다. 예를 들어, 미디어 컨텐츠 아이템의 제1 세그먼트의 컨텐츠를 기술하는 제1 메타 데이터가 검색될 수 있고, 미디어 컨텐츠 아이템(88)의 제1 세그먼트 이후에 디스플레이되는 미디어 컨텐츠 아이템의 제2 세그먼트의 컨텐츠를 기술한 제2 메타 데이터가 검색될 수 있다. 미디어 컨텐츠 아이템에 대한 시간 분석 데이터는 제2 메타 데이터에 의해 기술된 컨텐츠 이전에 시청되는 제1 메타 데이터에 의해 기술된 컨텐츠의 효과 분석에 기초하여 생성될 수 있다. 따라서, 미디어 컨텐츠 아이템의 세그먼트에 대한 세그먼트 시간 분석 데이터 및/또는 세그먼트 메타 데이터는 미디어 컨텐츠 아이템에 대한 시간 분석 데이터를 생성하는데 사용될 수 있다.Further, in some embodiments, the system (eg, temporal analysis model) may be configured to identify one or more segments of media content items as corresponding to some or all of the other media content items. The system may retrieve metadata and/or temporal analysis data for the segment(s) from other media content items (eg, temporal analysis data and/or metadata thereof). The retrieved data (eg, temporal analysis data and/or metadata) may be used to generate temporal analysis data for the media content item. For example, the retrieved data may be analyzed, weighted, combined, interpolated, etc. to produce composite data describing the temporal content of the media content item itself. For example, first meta data describing the content of a first segment of media content item may be retrieved and the content of a second segment of media content item displayed after the first segment of
일부 실시예에서, 미디어 컨텐츠 아이템은 광고 및/또는 스폰서(후원) 컨텐츠를 포함할 수 있다. 이러한 미디어 컨텐츠 아이템들은 (예를 들어, 동적 키보드 인터페이스 내에서) 검색 질의에 응답하여 제공될 수 있고 및/또는 사용자에게 제안으로(예를 들어, 메시징 애플리케이션을 사용하여 메시지를 작성하기 위한 자동 완성 기능의 일부로) 제공될 수 있다. 본 명세서에 설명된 시간 정보를 추출하는 것은 스폰서 미디어 컨텐츠에 특히 유용할 수 있다. 향상된 범주화 (categorization) 및/또는 검색 질의에 향상된 검색 결과를 제공하면 사용자 참여를 유도하여 광고의 효과를 높일 수 있다. 예를 들어, 광고는 동적 캡션의 시맨틱 의미 및/또는 감정 컨텐츠에 의해 더 잘 범주화될 수 있다. 예를 들어, 이와 관련된 검색 질의에 대한 응답으로 놀라움을 묘사하는 광고가 (프레임별 분석을 사용하여 쉽게 결정되지 않는 방식으로) 제공될 수 있다. 그 결과, 검색 질의에 응답하여 더 관련성이 높은 스폰서 미디어 컨텐츠 아이템이 제공될 수 있으며, 이에 따라 스폰서 미디어 컨텐츠 아이템에 대한 사용자 참여가 증가한다.In some embodiments, media content items may include advertisements and/or sponsored (sponsored) content. These media content items may be provided in response to a search query (eg, within a dynamic keyboard interface) and/or as a suggestion to the user (eg, an autocomplete function for composing a message using a messaging application) as part of) may be provided. Extracting temporal information described herein may be particularly useful for sponsored media content. By providing improved categorization and/or improved search results to a search query, user participation can be encouraged to increase the effectiveness of advertisements. For example, advertisements may be better categorized by the semantic meaning of dynamic captions and/or emotional content. For example, an advertisement depicting surprise (in a manner not readily determined using frame-by-frame analysis) may be presented in response to a related search query. As a result, more relevant sponsored media content items may be presented in response to a search query, thereby increasing user engagement with the sponsored media content items.
본 개시의 시스템 및 방법은 많은 기술적 효과 및 이점을 제공할 수 있다. 예를 들어, 주어진 검색 질의에 응답하여 더 적고 보다 정확한 검색 결과가 사용자 컴퓨팅 디바이스로 전송될 수 있다. 결과적으로, 검색 결과를 전송 및/또는 저장하는데 더 적은 컴퓨팅 소스(예를 들어, 사용자 컴퓨팅 디바이스 및/또는 서버 컴퓨팅 디바이스의 저장소 크기, 데이터 전송 대역폭 등)가 소비되거나 필요할 수 있다. 부가적으로, 사용자는 적절한 또는 특정 미디어 컨텐츠 아이템을보다 쉽게 찾을 수 있으므로 사용자가 수행해야 하는 검색 횟수를 줄일 수 있다. 또한, 미디어 컨텐츠 아이템의 분석 또는 분류는 미디어 컨텐츠 아이템의 컨텐츠에 대한 보다 정확한 설명을 제공할 수 있다. 예를 들어, 기계 학습 시간 분석 모델에 입력되는 미디어 컨텐츠 아이템을 기술하는 데이터는 이미지 프레임 세트의 각 이미지 프레임(예를 들어, 에지 또는 픽셀 속성)의 이미지 컴포넌트를 기술하거나 정의할 수 있으며, 예를 들어 , GIF 파일 자체일 수 있다. 기계 학습 시간 분석 모델은 시간 분석 데이터 출력을 생성하기 위해 이 기본(underlying) 이미지 프레임 데이터를 분석하도록 트레이닝될 수 있다. 기계 학습 시간 분석 모델에 의해 출력된 시간 분석 데이터는 이미지 프레임 세트의 프레임별 분석에 의해 생성된 데이터보다 더 정확한 방식으로 (애니메이션으로 표시될 때) 이미지 프레임 세트에 디스플레이된 주제를 기술할 수 있다. 따라서, 본 출원은 기술적 문제(예를 들어, GIF의 짧은 비디오와 같은 미디어의 이해, 분류 및/또는 범주화)에 대한 기술적 솔루션(예를 들어, 향상된 이미지 및 비디오 처리 및 분석)을 제공한다.The systems and methods of the present disclosure can provide many technical effects and advantages. For example, fewer and more accurate search results may be sent to the user computing device in response to a given search query. As a result, fewer computing sources (eg, storage size of user computing devices and/or server computing devices, data transfer bandwidth, etc.) may be consumed or required to transmit and/or store search results. Additionally, the user may more easily find a suitable or specific media content item, thereby reducing the number of searches the user has to perform. Additionally, analysis or classification of media content items may provide a more accurate description of the content of the media content items. For example, data describing media content items input to a machine learning temporal analysis model may describe or define an image component of each image frame (e.g., edge or pixel properties) in a set of image frames, e.g. For example, it could be the GIF file itself. A machine learning temporal analysis model may be trained to analyze this underlying image frame data to produce a temporal analysis data output. The temporal analysis data output by the machine learning temporal analysis model can describe the subjects displayed in the image frame set (when represented as an animation) in a more accurate manner than data generated by frame-by-frame analysis of the image frame set. Accordingly, the present application provides technical solutions (eg, improved image and video processing and analysis) to technical problems (eg, understanding, classification and/or categorization of media such as short videos in GIFs).
예를 들어, 시간 정보가 설명, 식별 및/또는 사용되어, 프레임 단위로 분석될 때 나타나지 않을 수 있는 미디어 컨텐츠 아이템들에서 사전 결정된 패턴을 식별 및/또는 검출할 수 있다. 본 명세서에 설명된 바와같이 시간 정보를 추출하는 것은 미디어 컨텐츠가 프레임 단위로 분석될 때 일반적으로 검출할 수 없는 그러한 사전 결정된 패턴을 식별하거나 탐지하는 것을 용이하게 할 수 있고, 상기 사전 결정된 패턴과 관련되고 및/또는 이를 포함하는 미디어 컨텐츠 아이템들을 검색 결과로부터 제거하는 것을 용이하게 할 수 있다. 결과적으로, 검색 결과를 보다 정확하고 효율적으로 제공할 수 있어 계산 자원과 전송 대역폭을 절약할 수 있다.For example, temporal information may be described, identified and/or used to identify and/or detect predetermined patterns in media content items that may not appear when analyzed frame by frame. Extracting temporal information as described herein may facilitate identifying or detecting such predetermined patterns that are not normally detectable when media content is analyzed frame by frame, and associated with the predetermined pattern and/or may facilitate removal of media content items containing the same from search results. As a result, search results can be provided more accurately and efficiently, saving computational resources and transmission bandwidth.
보다 구체적으로, 본 개시의 양태에 따르면, 시간 정보를 추출하기 위한 컴퓨터 구현 방법은 GIF(Graphics Interchange Format) 파일과 같은 순차 디스플레이를 위한 복수의 이미지 프레임을 포함하는 미디어 컨텐츠 아이템을 기술하는 데이터를 수신하는 것을 포함할 수 있다. 방법은 미디어 컨텐츠 아이템을 기술하는 데이터를 기계 학습 시간 분석 모델에 입력하는 것을 포함할 수 있다. 기계 학습 시간 분석 모델은 미디어 컨텐츠 아이템을 기술하는 데이터를 수신하고, 이에 응답하여 미디어 컨텐츠 아이템의 복수의 이미지 프레임을 순차적으로 시청하는 것과 관련된 시간 정보를 기술하는 시간 분석 데이터를 출력하도록 구성될 수 있다.More specifically, in accordance with aspects of the present disclosure, a computer implemented method for extracting temporal information receives data describing an item of media content including a plurality of image frames for sequential display, such as a Graphics Interchange Format (GIF) file. may include doing The method may include inputting data describing the media content item into a machine learning temporal analysis model. The machine learning temporal analysis model may be configured to receive data describing a media content item and, in response, output temporal analysis data describing temporal information associated with sequentially viewing a plurality of image frames of the media content item. .
또한, 복수의 이미지 프레임은 제1 디지털 이미지를 포함하는 제1 이미지 프레임 및 제1 이미지 프레임 이후에 순차적으로 디스플레이하기 위한 제2 디지털 이미지를 포함할 수 있고, 제2 이미지 프레임은 제2 디지털 이미지를 포함할 수 있다. 시간 분석 데이터에 의해 기술된 시간 정보는 제1 디지털 이미지 또는 제2 디지털 이미지를 개별적으로 검출함으로써 기술(설명)되지 않은 제2 디지털 이미지 이전에 순차적으로 검출되는 제1 디지털 이미지가 기술한 패턴을 기술할 수 있다.In addition, the plurality of image frames may include a first image frame including the first digital image and a second digital image for sequentially displaying after the first image frame, wherein the second image frame includes the second digital image. may include The temporal information described by the temporal analysis data describes a pattern described by the first digital image that is sequentially detected before the second digital image that is not described (described) by individually detecting the first digital image or the second digital image. can do.
또한, 제1 이미지 프레임의 제1 디지털 이미지는 색상, 굵기, 위치 또는 모양 중 적어도 하나에 의해 제2 이미지 프레임의 제2 디지털 이미지의 외관과 상이한 외관을 가질 수 있다. 또한, 시간 분석 데이터는 제2 디지털 이미지의 외관과 상이한 외관을 갖는 제1 디지털 이미지와 관련된 패턴을 기술할 수 있다. 제2 디지털 이미지의 외관과 상이한 외관을 갖는 제1 디지털 이미지와 관련된 상기 패턴은 사전 결정된 패턴과 비교될 수 있고, 패턴이 사전 결정된 패턴과 일치하는 경우, 사전 결정된 패턴과 연관되거나 이를 포함하는 미디어 컨텐츠 항목은 검색 결과에서 제거될 수 있다.Also, the first digital image of the first image frame may have an appearance different from that of the second digital image of the second image frame by at least one of color, thickness, position, or shape. Further, the temporal analysis data may describe a pattern associated with the first digital image having an appearance different from that of the second digital image. The pattern associated with the first digital image having an appearance different from that of the second digital image may be compared to a predetermined pattern, and if the pattern matches the predetermined pattern, media content associated with or including the predetermined pattern Items can be removed from search results.
일 예로서, 본 개시 내용의 시스템 및 방법은 애플리케이션, 브라우저 플러그인의 컨텍스트(맥럭) 또는 다른 컨텍스트에서 포함되거나 그렇지 않으면 사용될 수 있다. 따라서, 일부 구현에서, 본 개시 내용의 모델들은 랩탑, 태블릿 또는 스마트 폰과 같은 사용자 컴퓨팅 디바이스에 포함되거나 저장되고 이들에 의해 구현될 수 있다. 또 다른 예로서, 모델들은 클라이언트-서버 관계에 따라 사용자 컴퓨팅 디바이스와 통신하는 서버 컴퓨팅 디바이스에 포함되거나 저장되고 이들에 의해 구현될 수 있다. 예를 들어, 모델들은 웹 서비스(예를 들어, 웹 이메일 서비스)의 일부로서 서버 컴퓨팅 디바이스에 의해 구현될 수 있다.As an example, the systems and methods of the present disclosure may be included or otherwise used in the context of an application, browser plug-in, or other context. Accordingly, in some implementations, models of the present disclosure may be included in, stored on, and implemented by a user computing device, such as a laptop, tablet, or smart phone. As another example, models may be included in, stored on, and implemented by a server computing device that communicates with a user computing device according to a client-server relationship. For example, the models may be implemented by a server computing device as part of a web service (eg, web email service).
이제 도면을 참조하여, 본 개시의 예시적인 실시예들이 더 상세히 논의될 것이다.With reference now to the drawings, exemplary embodiments of the present disclosure will be discussed in greater detail.
예시적인 디바이스 및 시스템Exemplary devices and systems
도 1a는 본 개시의 예시적인 실시예들에 따른 기계 학습된 모델들을 사용하여 애니메이션 미디어 컨텐츠 아이템들로부터 시간 정보를 추출하기 위한 예시적인 컴퓨팅 시스템(10)의 블록도를 도시한다. 시스템(10)은 네트워크(80)를 통해 통신 적으로 결합된 사용자 컴퓨팅 디바이스(11), 서버 컴퓨팅 시스템(40) 및 트레이닝 컴퓨팅 시스템(60)을 포함할 수 있다.1A shows a block diagram of an
사용자 컴퓨팅 디바이스(11)는 예를 들어 개인용 컴퓨팅 디바이스(예를 들어, 랩탑 또는 데스크탑), 모바일 컴퓨팅 디바이스(예를 들어, 스마트 폰 또는 태블릿), 게임 콘솔 또는 컨트롤러, 웨어러블 컴퓨팅 디바이스, 임베디드 컴퓨팅 디바이스 또는 임의의 다른 유형 컴퓨팅 디바이스와 같은 임의의 유형의 컴퓨팅 디바이스일 수 있다. The
사용자 컴퓨팅 디바이스(11)는 하나 이상의 프로세서(12) 및 메모리(14)를 포함한다. 하나 이상의 프로세서(12)는 임의의 적절한 처리 디바이스(예를 들어, 프로세서 코어, 마이크로 프로세서, ASIC, FPGA, 컨트롤러, 마이크로 컨트롤러 등) 일 수 있으며, 하나의 프로세서 또는 작동 가능하게 연결된 복수의 프로세서일 수 있다. 메모리(14)는 RAM, ROM, EEPROM, EPROM, 플래시 메모리 디바이스, 자기 디스크 등 및 이들의 조합과 같은 하나 이상의 비-일시적 컴퓨터 판독 가능 저장 매체를 포함할 수 있다. 메모리(14)는 사용자 컴퓨팅 디바이스(11)로 하여금 동작들을 수행하게 하기 위해 프로세서(12)에 의해 실행되는 데이터(16) 및 명령들(18)을 저장할 수 있다.
사용자 컴퓨팅 디바이스(11)는 하나 이상의 시간 분석 모델(20)을 저장하거나 포함할 수 있다. 예를 들어, 시간 분석 모델(들)(20)은 신경망(예를 들어, 심층 신경망) 또는 다른 다층 비선형 모델과 같은 다양한 기계 학습 모델이거나 이를 포함할 수 있다. 신경망에는 번복 신경망(예를 들어, 장단기 메로리 반복 신경망), 피드 포워드 신경망 또는 기타 형태의 신경망이 포함될 수 있다. 예시적인 시간 분석 모델(들)(20)은 도 2를 참조하여 논의된다.The
일부 구현에서, 하나 이상의 시간 분석 모델(들)(20)은 네트워크(80)를 통해 서버 컴퓨팅 시스템(40)으로부터 수신될 수 있고, 사용자 컴퓨팅 디바이스 메모리(14)에 저장되고, 하나 이상의 프로세서(12)에 의해 사용되거나 구현될 수 있다. 일부 구현에서, 사용자 컴퓨팅 디바이스(11)는 (예를 들어, 모델(20)의 다수의 인스턴스에 걸쳐 병렬 분석을 수행하기 위해) 단일 시간 분석 모델(들)(20)의 다수의 병렬 인스턴스를 구현할 수 있다.In some implementations, one or more temporal analysis model(s) 20 may be received from
추가적으로 또는 대안적으로, 하나 이상의 시간 분석 모델(들)(50)이 클라이언트-서버 관계에 따라 사용자 컴퓨팅 디바이스(11)와 통신하는 서버 컴퓨팅 시스템(40)에 포함되거나 저장hl고 및 그에 의해 구현될 수 있다. 예를 들어, 시간 분석 모델(들)(50)은 웹 서비스의 일부로서 서버 컴퓨팅 시스템(40)에 의해 구현될 수 있다. 따라서, 하나 이상의 모델(20)이 사용자 컴퓨팅 디바이스(11)에 저장 및 구현될 수 있고 및/또는 하나 이상의 모델(50)이 서버 컴퓨팅 시스템(40)에 저장 및 구현될 수 있다.Additionally or alternatively, one or more temporal analysis model(s) 50 may be included in or stored in and implemented by a
사용자 컴퓨팅 디바이스(11)는 또한 사용자 입력을 수신하는 하나 이상의 사용자 입력 컴포넌트(22)를 포함할 수 있다. 예를 들어, 사용자 입력 컴포넌트(22)는 사용자 입력 객체(예를 들어, 손가락 또는 스타일러스)의 터치에 민감한 터치 감지 컴포넌트(예를 들어, 터치 감지 디스플레이 스크린 또는 터치 패드)일 수 있다. 터치 감지 컴포넌트는 가상 키보드를 구현하는 역할을 할 수 있다. 다른 예시적인 사용자 입력 컴포넌트는 마이크로폰, 기존 키보드 또는 사용자가 통신을 입력할 수 있는 기타 수단이 포함된다.The
서버 컴퓨팅 시스템(40)은 하나 이상의 프로세서(42) 및 메모리(44)를 포함한다. 하나 이상의 프로세서(42)는 임의의 적절한 처리 디바이스(예를 들어, 프로세서 코어, 마이크로 프로세서, ASIC, FPGA, 컨트롤러, 마이크로 컨트롤러 등)일 수 있고, 하나의 프로세서 또는 작동 가능하게 연결된 복수의 프로세서일 수 있다. 메모리(44)는 RAM, ROM, EEPROM, EPROM, 플래시 메모리 디바이스, 자기 디스크 등 및 이들의 조합과 같은 하나 이상의 비-일시적 컴퓨터 판독 가능 저장 매체를 포함할 수 있다. 메모리(44)는 서버 컴퓨팅 시스템(40)으로 하여금 동작들을 수행하게 하기 위해 프로세서(42)에 의해 실행되는 데이터(46) 및 명령들(48)을 저장할 수 있다.
일부 구현에서, 서버 컴퓨팅 시스템(40)은 하나 이상의 서버 컴퓨팅 디바이스를 포함하거나 그에 의해 구현된다. 서버 컴퓨팅 시스템(40)이 복수의 서버 컴퓨팅 디바이스를 포함하는 경우, 이러한 서버 컴퓨팅 디바이스는 순차 컴퓨팅 아키텍처, 병렬 컴퓨팅 아키텍처 또는 이들의 일부 조합에 따라 동작할 수 있다.In some implementations,
전술한 바와 같이, 서버 컴퓨팅 시스템(40)은 하나 이상의 기계 학습 시간 분석 모델(50)을 저장하거나 이를 포함할 수 있다. 예를 들어, 모델들(50)은 신경망(예를 들어, 심층 반복 신경망) 또는 다른 다층 비선형 모델과 같은 다양한 기계 학습 모델일 수 있거나 이를 포함할 수 있다. 예시적인 모델들(50)은 도 2를 참조하여 논의된다.As noted above, the
서버 컴퓨팅 시스템(40)은 네트워크(80)를 통해 통신적으로 결합된 트레이닝 컴퓨팅 시스템(60)과의 상호 작용을 통해 모델(20, 50)을 트레이닝할 수 있다. 트레이닝 컴퓨팅 시스템(60)은 서버 컴퓨팅 시스템(40)과 분리될 수 있거나 서버 컴퓨팅 시스템(40)의 일부일 수 있다.
트레이닝 컴퓨팅 시스템(60)은 하나 이상의 프로세서(62) 및 메모리(64)를 포함할 수 있다. 하나 이상의 프로세서(62)는 임의의 적절한 처리 디바이스(예를 들어, 프로세서 코어, 마이크로 프로세서, ASIC, FPGA, 컨트롤러, 마이크로 컨트롤러 등)일 수 있으며, 하나의 프로세서 또는 작동 가능하게 연결된 복수의 프로세서일 수 있다. 메모리(64)는 RAM, ROM, EEPROM, EPROM, 플래시 메모리 디바이스, 자기 디스크 등 및 이들의 조합과 같은 하나 이상의 비-일시적 컴퓨터 판독 가능 저장 매체를 포함할 수 있다. 메모리(64)는 트레이닝 컴퓨팅 시스템(160)으로 하여금 동작들을 수행하게 하기 위해 프로세서(62)에 의해 실행되는 데이터(66) 및 명령들(68)을 저장할 수 있다. 일부 구현에서, 트레이닝 컴퓨팅 시스템(60)은 하나 이상의 서버 컴퓨팅 디바이스를 포함하거나 그에 의해 구현된다.
트레이닝 컴퓨팅 시스템(60)은 예를 들어, 오류의 역방향 전파와 같은 다양한 트레이닝 또는 학습 기술을 사용하여 서버 컴퓨팅 시스템(40)에 저장된 기계 학습 모델(50) 및/또는 사용자 컴퓨팅 디바이스(11)에 저장된 모델(20)을 트레이닝하는 모델 트레이너(70)를 포함할 수 있다. 일부 구현에서, 오류의 역방향 전파를 수행하는 것은 시간에 따라 잘린 역전파(truncated backpropagation)를 수행하는 것을 포함할 수 있다. 모델 트레이너(160)는 트레이닝되는 모델들의 일반화 기능을 개선하기 위해 다수의 일반화 기술(예를 들어, 가중치 감소, 탈락(dropouts) 등)을 수행할 수 있다.The
일부 구현에서, 사용자가 동의를 제공한 경우, 트레이닝 예들은 (예를 들어, 사용자 컴퓨팅 디바이스(11)의 사용자에 의해 이전에 제공된 통신에 기초하여) 사용자 컴퓨팅 디바이스(11)에 의해 제공될 수 있다. 따라서, 이러한 구현에서, 사용자 컴퓨팅 디바이스(11)로 제공된 모델(20)은 사용자 컴퓨팅 디바이스(11)로부터 수신된 사용자-특정 통신 데이터에 대해 트레이닝 컴퓨팅 시스템(60)에 의해 트레이닝될 수 있다. 일부 인스턴스(경우)에서, 이 프로세스는 모델을 개인화하는 것으로 지칭될 수 있다.In some implementations, if the user has provided consent, training examples may be provided by the user computing device 11 (eg, based on a communication previously provided by the user of the user computing device 11 ). . Thus, in this implementation, the
모델 트레이너(70)는 원하는 기능을 제공하기 위해 사용되는 컴퓨터 로직을 포함한다. 모델 트레이너(70)는 범용 프로세서를 제어하는 하드웨어, 펌웨어 및/또는 소프트웨어로 구현될 수 있다. 예를 들어, 일부 구현에서, 모델 트레이너(70)는 저장 디바이스에 저장되고, 메모리에 로드되고, 하나 이상의 프로세서에 의해 실행되는 프로그램 파일들을 포함한다. 다른 구현에서, 모델 트레이너(70)는 RAM 하드 디스크 또는 광학 또는 자기 매체와 같은 유형의 컴퓨터 판독 가능 저장 매체에 저장되는 하나 이상의 컴퓨터 실행 가능 명령 세트를 포함한다. 모델 트레이너(70)는 트레이닝 데이터(72)에 기초하여 모델(들)(20, 50)을 트레이닝할 수 있다.The
네트워크(80)는 근거리 통신망(예를 들어, 인트라넷), 광역 통신망(예를 들어, 인터넷), 또는 이들의 일부 조합과 같은 임의의 유형의 통신 네트워크일 수 있으며, 임의의 수의 유선 또는 무선 링크를 포함할 수 있다. 일반적으로, 네트워크(180)를 통한 통신은 다양한 통신 프로토콜(예를 들어, TCP/IP, HTTP, SMTP, FTP), 인코딩 또는 포멧(예를 들어, HTML, XML) 및/또는 보호 체계(예를 들어, VPN, 보안 HTTP, SSL) 사용하여 임의의 유형의 유선 및/또는 무선 연결을 통해 수행될 수 있다.
도 1a는 본 개시를 구현하는데 사용될 수 있는 하나의 예시적인 컴퓨팅 시스템을 도시한다. 다른 컴퓨팅 시스템이 또한 사용될 수 있다. 예를 들어, 일부 구현에서, 사용자 컴퓨팅 디바이스(11)는 모델 트레이너(70) 및 트레이닝 데이터 세트(72)를 포함할 수 있다. 이러한 구현에서, 모델들(120)은 사용자 컴퓨팅 디바이스(102)에서 로컬로 트레이닝되고 사용될 수 있다. 이러한 구현의 일부에서, 사용자 컴퓨팅 디바이스(102)는 사용자-특정 데이터에 기초하여 모델들(120)을 개인화하기 위해 모델 트레이너(160)를 구현할 수 있다.1A illustrates one example computing system that may be used to implement the present disclosure. Other computing systems may also be used. For example, in some implementations, the
도 1b는 본 개시의 예시적인 실시예들에 따라 수행하는 예시적인 컴퓨팅 디바이스(82)의 블록도를 도시한다. 컴퓨팅 디바이스(82)는 사용자 컴퓨팅 디바이스 또는 서버 컴퓨팅 디바이스일 수 있다. 1B shows a block diagram of an
컴퓨팅 디바이스(82)는 다수의 애플리케이션(예를 들어, 애플리케이션 1~N)을 포함한다. 각 애플리케이션에는 자체 기계 학습 라이브러리와 기계 학습 모델(들)이 포함되어 있다. 예를 들어, 각 애플리케이션에는 기계 학습 모델이 포함될 수 있다. 예시적인 애플리케이션에는 텍스트 메시징 애플리케이션, 이메일 애플리케이션, 받아쓰기 애플리케이션, 가상 키보드 애플리케이션, 브라우저 애플리케이션 등이 있다.
도 1b에 도시된 바와 같이, 각각의 애플리케이션은 예를 들어 하나 이상의 센서, 컨텍스트 관리자, 디바이스 상태 컴포넌트 및/또는 추가 컴포넌트와 같은 컴퓨팅 디바이스의 다수의 다른 컴포넌트와 통신할 수 있다. 일부 구현에서, 각 애플리케이션은 API(예를 들어, 공개 API)를 사용하여 각 디바이스 컴포넌트와 통신할 수 있다. 일부 구현에서 각 애플리케이션에서 사용하는 API는 해당 애플리케이션에 특정적이다.As shown in FIG. 1B , each application may communicate with a number of other components of the computing device, such as, for example, one or more sensors, context managers, device state components, and/or additional components. In some implementations, each application may communicate with each device component using an API (eg, a public API). In some implementations, the APIs used by each application are specific to that application.
도 1c는 본 개시의 예시적인 실시예들에 따라 수행하는 예시적인 컴퓨팅 디바이스(84)의 블록도를 도시한다. 컴퓨팅 디바이스(84)는 사용자 컴퓨팅 디바이스 또는 서버 컴퓨팅 디바이스일 수 있다.1C shows a block diagram of an
컴퓨팅 디바이스(84)는 다수의 애플리케이션(예를 들어, 애플리케이션 1~N)을 포함한다. 각 애플리케이션은 중앙 지능 계층과 통신한다. 예시적인 애플리케이션에는 텍스트 메시징 애플리케이션, 이메일 애플리케이션, 받아쓰기 애플리케이션, 가상 키보드 애플리케이션, 브라우저 애플리케이션 등이 있다. 일부 구현에서, 각 애플리케이션은 API(예를 들어, 모든 애플리케이션에 걸친 공통 API)를 사용하여 중앙 지능 계층(및 그 안에 저장된 모델(들))과 통신할 수 있다.
중앙 지능 계층은 다수의 기계 학습 모델을 포함한다. 예를 들어, 도 1c에 도시된 바와 같이. 개별 기계 학습 모델(예를 들어, 모델)은 각 애플리케이션에 대해 제공되고 중앙 지능 계층에 의해 관리될 수 있다. 다른 구현에서, 둘 이상의 애플리케이션이 단일 기계 학습 모델을 공유할 수 있다. 예를 들어, 일부 구현에서, 중앙 지능 계층은 모든 애플리케이션에 대해 단일 모델(예를 들어, 단일 모델)을 제공할 수 있다. 일부 구현에서, 중앙 지능 계층은 컴퓨팅 디바이스(84)의 운영 체제내에 포함되거나 이에 의해 구현된다.The central intelligence layer contains a number of machine learning models. For example, as shown in Figure 1c. A separate machine learning model (eg, model) may be provided for each application and managed by a central intelligence layer. In other implementations, two or more applications may share a single machine learning model. For example, in some implementations, a central intelligence layer may provide a single model (eg, a single model) for all applications. In some implementations, the central intelligence layer is included within or implemented by the operating system of
중앙 지능 계층은 중앙 디바이스 데이터 계층과 통신할 수 있다. 중앙 디바이스 데이터 계층은 컴퓨팅 디바이스(84)를 위한 데이터의 중앙 저장소일 수 있다. 도 1c에 도시된 바와 같이, 중앙 디바이스 데이터 계층은 예를 들어 하나 이상의 센서, 컨텍스트 관리자, 디바이스 상태 컴포넌트 및/또는 추가 컴포넌트와 같은 컴퓨팅 디바이스의 많은 다른 컴포넌트와 통신할 수 있다. 일부 구현에서, 중앙 디바이스 데이터 계층은 API(예를 들어, 개인 API)를 사용하여 각 디바이스 컴포넌트와 통신할 수 있다.The central intelligence layer may communicate with the central device data layer. The central device data layer may be a central repository of data for the
예시적인 모델 배열Example model arrangement
도 2는 본 개시의 예시적인 실시예들에 따른 예시적인 시간 분석 모델(86)의 블록도를 도시한다. 일부 구현에서, 시간 분석 모델(86)은 순차적인 디스플레이를 위한 복수의 이미지 프레임(예를 들어, GIF 파일)을 포함하는 미디어 컨텐츠 아이템(88)을 기술하는 데이터를 수신하도록 트레이닝된다. 기계 학습 시간 분석 모델(86)은 미디어 컨텐츠 아이템(88)을 기술하는 데이터를 수신하고, 이에 응답하여 미디어 컨텐츠 아이템(88)의 복수의 이미지 프레임을 순차적으로 시청하는 것과 관련된 시간 정보를 기술하는 시간 분석 데이터(90)를 출력하도록 구성될 수 있다. 미디어 컨텐츠 아이템(88)의 개별 이미지 프레임은 시간 분석 데이터(90)에 의해 기술된 시간 정보를 전달하거나 기술하지 못할 수 있다. 시간 분석 모델(86)은 또한 미디어 컨텐츠 아이템(88)의 하나 이상의 이미지 프레임에서 인식된 하나 이상의 텍스트 문자열을 기술하는 텍스트 인식 데이터(89)를 수신하도록 구성될 수 있다. 텍스트 인식 데이터(89)는 예를 들어,도 6a 및 7b를 참조하여 후술되는 바와 같이 자연어 파서(NLP)로부터 수신될 수 있다.2 shows a block diagram of an exemplary
일부 실시예에서, 시간 분석 모델(86)(예를 들어, 그의 분할 모델)은 미디어 컨텐츠 아이템(88)으로부터 하나 이상의 미디어 컨텐츠 아이템 세그먼트를 생성하도록 구성될 수 있다. 시간 분석 데이터(90)는 미디어 컨텐츠 아이템(88)의 세그먼트들과 각각 연관된 시간 정보를 기술하는 중간 시간 분석 데이터를 생성하도록 구성될 수 있다. 시간 분석 모델(86)은 중간 시간 분석 데이터를 처리하여 시간 분석 데이터(90)를 생성하도록 구성될 수 있다.In some embodiments, temporal analysis model 86 (eg, a segmentation model thereof) may be configured to generate one or more media content item segments from
일부 실시예에서, 분할 모델은 표시, 변경, 이동되는 캡션에 기초하여 및/또는 미디어 컨텐츠 아이템(88)에서 변경되는 장면에 기초하여 미디어 컨텐츠 아이템(88)을 자동으로 분할하도록 구성될 수 있다.In some embodiments, the segmentation model may be configured to automatically segment the
또한, 일부 실시예에서, 시스템(예를 들어, 시간 분석 모델(86))은 다른 미디어 컨텐츠 아이템의 일부 또는 전부에 대응하는 것으로 미디어 컨텐츠 아이템(88)의 하나 이상의 세그먼트를 식별하도록 구성될 수 있다. 시스템은 다른 미디어 컨텐츠 아이템(예를 들어, 시간 분석 데이터 및/또는 그의 메타 데이터)으로부터 세그먼트(들)에 대한 메타 데이터 및/또는 시간 분석 데이터를 검색할 수 있다. 검색된 데이터(예를 들어, 시간 분석 데이터 및/또는 메타 데이터)는 미디어 컨텐츠 아이템(88)에 대한 시간 분석 데이터를 생성하는데 사용될 수 있다. 예를 들어, 검색된 데이터는 분석, 가중, 결합, 보간되어, 미디어 컨텐츠 아이템(88) 자체의 시간 컨텐츠를 기술하는 복합 데이터를 생성할 수 있다. 예를 들어, 미디어 컨텐츠 아이템(88)의 제1 세그먼트의 컨텐츠를 기술하는 제1 메타 데이터가 검색될 수 있으며, 미디어 컨텐츠 아이템(88)의 제1 세그먼트 이후에 디스플레이되는 미디어 컨텐츠 아이템(88)의 제2 세그먼트의 컨텐츠를 기술하는 제2 메타 데이터가 검색될 수 있다. 미디어 컨텐츠 아이템(88)에 대한 시간 분석 데이터는 제2 메타 데이터에 의해 기술된 컨텐츠 이전에 시청되는 제1 메타 데이터에 의해 기술된 컨텐츠의 효과 분석에 기초하여 생성될 수 있다. 따라서, 미디어 컨텐츠 아이템(88)의 세그먼트에 대한 세그먼트 시간 분석 데이터 및/또는 세그먼트 메타 데이터를 사용하여 미디어 컨텐츠 아이템(88)에 대한 시간 분석 데이터를 생성할 수 있다.Further, in some embodiments, the system (eg, temporal analysis model 86 ) may be configured to identify one or more segments of
본 개시의 양태는 시간 분석 모델(86)을 트레이닝하는 것에 관한 것이다. 시간 분석 모델(86)의 하나 이상의 파라미터는 시간 분석 모델(86)에 의해 출력된 시간 분석 데이터(90)와 지상 실측 시간 분석 데이터(92)의 비교에 기초하여 조정될 수 있다. 예를 들어, 시간 분석 데이터(90)와 지상 실측 시간 분석 데이터(92) 사이의 비교를 기술하는 시간 분석 손실(94)이 계산될 수 있다. 시간 분석 모델(86)의 파라미터들은 예를 들어 시간 분석 손실(94)에 기초하여, 오류의 역전파를 사용하여 반복적으로 조정될 수 있다.Aspects of the present disclosure relate to training a
시간 분석 데이터(90) 및/또는 지상 실측 시간 분석 데이터(92)는 다양한 정보를 포함하거나 설명할 수 있다. 예를 들어, 시간 분석 데이터(90) 및/또는 지상 실측 시간 분석 데이터(92)는 하나 이상의 컨텐츠 연관(예를 들어, 도 5a 내지 5c를 참조하여 아래에 설명된 바와 같이), 애니메이션 미디어 컨텐츠 아이템의 감정 컨텐츠, 동적 캡션의 의미(예를 들어, 임의의 강조를 포함함)등을 포함하거나 기술할 수 있다. The
예시적인 방법Exemplary method
도 3은 본 개시의 예시적인 실시예들에 따라 수행할 예시적인 방법의 흐름도를 도시한다. 도 3은 예시 및 논의를 위해 특정 순서로 수행되는 단계들을 도시하지만, 본 개시의 방법은 특별히 예시된 순서 또는 배열로 제한되지 않는다. 방법(300)의 다양한 단계는 본 발명의 범위를 벗어나지 않고 다양한 방식으로 생략, 재배열, 결합 및/또는 적응될 수 있다.3 shows a flowchart of an exemplary method to be performed in accordance with exemplary embodiments of the present disclosure; 3 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the specifically illustrated order or arrangement. Various steps of
단계(302)에서, 컴퓨팅 시스템은 그래픽 교환 포맷(GIF) 파일과 같은 순차 디스플레이를 위한 복수의 이미지 프레임을 포함하는 미디어 컨텐츠 아이템을 기술하는 데이터를 수신할 수 있다. 예를 들어, 컴퓨팅 시스템은도 5a 내지 7b를 참조하여 설명된 바와 같이 미디어 컨텐츠 저장소(106), 미디어 컨텐츠 소스(124), 제3 자 애플리케이션(202) 및/또는 사용자 디바이스(102)로부터 미디어 컨텐츠 아이템(들)을 수신할 수 있다.At 302 , the computing system may receive data describing an item of media content including a plurality of image frames for sequential display, such as a graphic interchange format (GIF) file. For example, the computing system may provide media content from a
단계(304)에서, 컴퓨팅 시스템은 미디어 컨텐츠 아이템을 기술하는 데이터를, 예를 들어 도 1a 내지 2를 참조하여 위에서 설명된 바와 같이 기계 학습 시간 분석 모델에 입력할 수 있다. 기계 학습 시간 분석 모델은 미디어 컨텐츠 아이템을 기술하는 데이터를 수신하고, 이에 응답하여 미디어 컨텐츠 아이템의 복수의 이미지 프레임을 순차적으로 시청하는 것과 관련된 시간 정보를 기술하는 시간 분석 데이터를 출력하도록 구성될 수 있다. 복수의 이미지 프레임의 개별 이미지 프레임은 시간 정보를 전달하거나 기술하지 못할 수 있다. 시간 분석 데이터는 미디어 컨텐츠 아이템의 시맨틱 의미, 톤 및/또는 감정 컨텐츠 및/또는 그 안에 포함된 동적 캡션을 기술할 수 있다.At
단계(306)에서, 컴퓨팅 시스템은 기계 학습 시간 분석 모델의 출력으로서 시간 분석 데이터를 수신할 수 있다. 시간 분석 데이터는 프레임별로 보여지거나 분석할 때 안전하지 않은 것처럼 보일 수 있는 "안전하지 않은"(예를 들어, 저속하거나 공격적인 등) 미디어 컨텐츠 아이템을 식별하고 및/또는 사용자가 인식할 때 더 잘 "이해"될 수 있으므로 미디어 컨텐츠 아이템의 향상된 범주화를 촉진하는데 사용될 수 있다. 따라서, 시간 분석 데이터는 사용자를 위한 미디어 컨텐츠 아이템을 포함하는 검색 결과 및/또는 제안의 관련성을 향상시키기 위해 사용될 수 있다.At 306 , the computing system may receive temporal analysis data as an output of the machine learning temporal analysis model. Temporal analysis data identifies "unsafe" (e.g., vulgar or offensive, etc.) media content items that may appear unsafe when viewed frame-by-frame or when analyzed and/or better "perceived" by the user. can be "understood" and thus can be used to facilitate improved categorization of media content items. Accordingly, temporal analysis data may be used to improve the relevance of search results and/or suggestions including media content items for users.
도 4를 참조하면, 일부 구현에서, 미디어 컨텐츠 아이템은 동적 캡션을 포함할 수 있으며, 여기서 동적 캡션의 단어 또는 문자는 미디어 컨텐츠 아이템이 애니메이션으로 렌더링될 때 순차적으로 디스플레이된다. 애니메이션 미디어 컨텐츠 아이템은 제1 텍스트 문자열(408)(예를 들어, 제1 문구, 단어, 문자 등)을 포함하는 제1 이미지 프레임(402) 및 제1 이미지 프레임(402) 이후에 순차적으로 디스플레이하기 위한 제2 텍스트 문자열(410)(예를 들어, 제1 문구, 단어, 문자 등)을 포함하는 제2 이미지 프레임(404)을 포함할 수 있다. 애니메이션 미디어 컨텐츠 아이템은 또한 선택적으로 제3 텍스트 문자열(412)을 포함하는 제3 이미지 프레임(406)을 포함할 수 있다.4 , in some implementations, the media content item may include a dynamic caption, wherein words or characters in the dynamic caption are displayed sequentially as the media content item is rendered into an animation. Displaying the animated media content item sequentially after the first image frame 402 and the first image frame 402 including the first text string 408 (eg, a first phrase, word, character, etc.) A
이미지 프레임(402, 404, 406)을 연결하는 동적 캡션이 (예를 들어, 화살표(414, 416)로 도시된 바와같이) 순차적으로 디스플레이될 수 있다. 미디어 컨텐츠 아이템이 애니메이션으로 렌더링될 때 문장이 순차적으로 디스플레이(예를 들어 깜박암)될 수 있다. 시간 분석 데이터는 애니메이션 미디어 컨텐츠 아이템의 뷰어에 의해 인식되는 대로 완전한 동적 캡션의 시맨틱 의미를 기술할 수 있다. 보다 구체적으로, 시간 분석 데이터는 제2 텍스트 문자열(410)(및 존재한다면 제3 텍스트 문자열(412 등)) 이전에 순차적으로 판독되는 제1 텍스트 문자열(408)의 시맨틱 의미를 기술할 수 있다. 이러한 시맨틱 의미는 제1 텍스트 문자열(408) 또는 제2 텍스트 문자열(410)의 개별 판독에 부족할 수 있다. 따라서, 시간 분석 모델에 의해 출력되는 시간 정보는 뷰어 또는 미디어 컨텐츠 아이템에 의해 판독될 때 전체 동적 캡션의 시맨틱 의미를 기술할 수 있다. Dynamic captions connecting image frames 402 , 404 , 406 may be displayed sequentially (eg, as shown by arrows 414 , 416 ). Sentences may be sequentially displayed (eg, blinked) as the media content item is rendered as an animation. The temporal analysis data may describe the semantic meaning of the full dynamic caption as perceived by the viewer of the animated media content item. More specifically, the temporal analysis data may describe the semantic meaning of the
이러한 동적 캡션의 일 예로서, 제1 텍스트 문자열은 "굿(Good)"을 포함할 수 있고, 제2 텍스트 문자열은 "그리프(Grief)"를 포함할 수 있다. 개별적으로 읽거나 분석하면, "굿"은 행복이나 기쁨을 전달할 수 있고 "그리프"는 슬픔을 전달할 수 있다. 그러나, 동적 캡션으로서 분석될 때, "굿 그리프(Good Grief)"라는 문구는 가벼운 좌절감과 같은 완전히 다른 감정을 전달한다.As an example of such a dynamic caption, the first text string may include “Good” and the second text string may include “Grief”. When read or analyzed individually, "good" can convey happiness or joy, and "griff" can convey sadness. However, when analyzed as a dynamic caption, the phrase "Good Grief" conveys an entirely different emotion, such as mild frustration.
다시 도 4를 참조하면, 동적 캡션의 일부는 때때로 외관을 변경할 수 있다. 예를 들어, 동적 캡션이 애니메이션에서 렌더링될 때 동적 캡션의 단어 및/또는 문자의 모양(예를 들어, 위치, 색상, 크기, 폰트 등)이 다를 수 있다. 이러한 외관의 변화는 강조와 같은 의미를 전달할 수 있다. 동적 캡션의 특정 단어는 동적 캡션의 다른 단어에 비해 단어를 더 크게, 더 굵게, 더 생생한 색상 등으로 함으로써 강조할 수 있다. 이러한 강조는 동적 캡션의 의미에 영향을 줄 수 있다. 일반적으로 이해되는 바와 같이, 동일한 문장은 그 문장에서 강조되는 단어(들)에 따라 다양한 상이한 의미를 가질 수 있다. 예를 들어, "나는 그녀가 내 돈을 훔쳤다고 결코 말하지 않았다"라는 문장은 강조가 문장의 의미를 어떻게 바꿀 수 있는지에 대한 예로 사용되었다. 이 문장은 강조되는 단어에 따라 7가지의 상이한 의미를 갖는다. " 나는 그녀가 내 돈을 훔쳤다고 결코 말하지 않았다"는 것은 다른 누군가가 그랬다는 것을 암시한다. "나는 그녀가 내 돈을 훔쳤다고 결코 말하지 않았다"는 것은 누군가가 그녀가 그의 돈을 훔쳤다고 말하는 화자(사람)를 비난했음을 의미한다. 시간 분석 모델은 동적 캡션의 텍스트 문자열의 상이한 외관에 기초하여 이러한 상이한 의미의 의미를 기술하는 시간 정보를 추출하도록 트레이닝될 수 있다.Referring again to FIG. 4 , some of the dynamic captions can sometimes change appearance. For example, when a dynamic caption is rendered in an animation, the shape (eg, position, color, size, font, etc.) of words and/or characters in the dynamic caption may be different. This change in appearance can convey the same meaning as emphasis. Certain words in dynamic captions can be emphasized by making them larger, bolder, more vivid colors, etc. compared to other words in dynamic captions. This emphasis can affect the meaning of dynamic captions. As is generally understood, the same sentence can have a variety of different meanings depending on the word(s) being emphasized in the sentence. For example, the sentence "I never said she stole my money" was used as an example of how emphasis can change the meaning of the sentence. This sentence has 7 different meanings depending on the word being emphasized. " I never said she stole my money" suggests someone else did. "I never said she stole my money" means that someone blamed the speaker for saying she stole his money. The temporal analysis model can be trained to extract temporal information describing the meaning of these different meanings based on the different appearances of the text strings of dynamic captions.
일 예로서, 애니메이션 미디어 컨텐츠 아이템은 3개의 이미지 프레임(402, 404, 406)을 순차적으로 디스플레이할 수 있다. 제1 이미지 프레임(402)은 "나는 결코 말하지 않았다"라는 제1 텍스트 문자열(408)을 포함할 수 있다. 제2 이미지 프레임(404)은 제2 텍스트 문자열(410 "그녀")을 포함할 수 있고, 제3 이미지 프레임(406)은 제3 텍스트 문자열(412)인 "나의 돈을 훔쳤다"를 포함할 수 있다. 하나 이상의 텍스트 문자열(예를 들어, 이 예에서 제2 텍스트 문자열(410))은 동적 캡션의 다른 단어에 비해 더 크게, 더 굵게, 더 생생한 색상 등으로 디스플레이함으로써 강조될 수 있다.As an example, an animated media content item may sequentially display three image frames 402 , 404 , 406 . The first image frame 402 may include a
시간 분석 데이터는 순차적으로 읽을 때 동적 자막의 의미를 설명할 수 있다. 도 4를 참조하여 위에서 설명한 예에서, 시간 분석 데이터는 다른 텍스트 문자열(408, 412)의 외관과 상이한 제2 텍스트 문자열(410)의 강조에 의해 부여된 시맨틱 의미를 포함할 수 있다. 보다 구체적으로, 이 예에서, 시간 분석 데이터는 화자가 누군가가 자신의 돈을 훔쳤다고 말했음을 의미하는 시맨틱 의미를 기술할 수 있다. 따라서, 시간 분석 데이터는 동적 캡션의 텍스트 문자열(408, 410, 412)의 상이한 시각적 특성으로부터의 강조를 포함하여, 독자가 순차적으로 볼 때 동적 캡션의 시맨틱 의미를 기술할 수 있다.Temporal analysis data can explain the meaning of dynamic subtitles when read sequentially. In the example described above with reference to FIG. 4 , the temporal analysis data may include a semantic meaning imparted by the emphasis of the second text string 410 different from the appearance of the other text strings 408 , 412 . More specifically, in this example, the temporal analysis data may describe a semantic meaning meaning that the speaker said that someone stole his money. Thus, temporal analysis data can describe the semantic meaning of the dynamic caption when viewed sequentially by the reader, including emphasis from different visual characteristics of the text strings 408 , 410 , 412 of the dynamic caption.
유사하게, 시간 분석 모델(들)은 객체, 사람 등의 변화하는 장면(예를 들어, 텍스트 포함 또는 제외)에 포함된 시간 정보를 추출할 수 있다. 복수의 이미지 프레임은 제1 장면을 기술하거나 묘사하는 제1 이미지 프레임 및 제1 이미지 프레임 이후 순차적으로 디스플레이하기 위한 제2 장면을 기술하는 제2 이미지 프레임을 포함할 수 있다. 시간 정보는 제1 장면 또는 제2 장면을 개별적으로 보거나 분석하는 것에 의해 기술 또는 전달되지 않은 제2 장면 이전에 순차적으로 시청되는 제1 장면의 시맨틱 의미를 설명할 수 있다.Similarly, the temporal analysis model(s) may extract temporal information contained in a changing scene (eg, with or without text) of an object, person, etc. The plurality of image frames may include a first image frame describing or depicting a first scene and a second image frame describing a second scene for sequentially displaying after the first image frame. The temporal information may describe the semantic meaning of a first scene viewed sequentially prior to a second scene not described or conveyed by individually viewing or analyzing the first scene or the second scene.
일부 실시예에서, 시간 분석 데이터는 미디어 컨텐츠 아이템의 감정 컨텐츠를 기술할 수 있다. 상이한 감정 컨텐츠의 예로는 불안, 두려움, 분노, 행복, 슬픔, 시기심, 욕망, 관심, 충격, 지루함, 놀라움, 안도감, 혐오감, 수치심, 동정심 및 기타 모든 인간 감정이 포함될 수 있다. 특정 장면 시퀀스는 프레임 단위 또는 장면 단위로 보거나 분석될 때 개별 장면이 특정 감정을 전달하지 않더라도 해당 특정 감정을 전달할 수 있다.In some embodiments, the temporal analysis data may describe the emotional content of the media content item. Examples of different emotional content may include anxiety, fear, anger, happiness, sadness, envy, desire, interest, shock, boredom, surprise, relief, disgust, shame, sympathy and all other human emotions. When viewed or analyzed on a frame-by-frame or scene-by-scene basis, a specific scene sequence can convey that specific emotion even if the individual scenes do not convey that specific emotion.
일부 실시예에서, 미디어 컨텐츠 아이템은 광고 및/또는 스폰서 컨텐츠를 포함할 수 있다. 이러한 미디어 컨텐츠 아이템들은 검색 질의에 대한 응답으로 (예를 들어, 동적 키보드 인터페이스 내에서) 제공되거나 사용자에 대한 제안으로(예를 들어, 메시징 애플리케이션을 사용하여 메시지를 작성하기 위한 자동 완성 기능의 일부로) 제공될 수 있다. 본 명세서에 설명된 시간 정보를 추출하는 것은 스폰서 미디어 컨텐츠에 특히 유용할 수 있다. 향상된 범주화 및/또는 향상된 검색 결과를 검색 질의에 제공하면 사용자 참여를 유도하여 광고의 효과를 높일 수 있다. 예를 들어, 광고는 동적 캡션의 시맨틱 의미 및/또는 감정 컨텐츠에 의해 더 잘 범주화될 수 있다. 예를 들어, 이와 관련된 검색 질의에 대한 응답으로 놀라움을 묘사하는 광고가 제공될 수 있다. 결과적으로 사용자는 스폰서 미디어 컨텐츠 아이템에 참여할 가능성이 더 높다.In some embodiments, media content items may include advertisements and/or sponsored content. These media content items may be provided in response to a search query (eg, within a dynamic keyboard interface) or as a suggestion to a user (eg, as part of an autocomplete feature for composing a message using a messaging application). may be provided. Extracting temporal information described herein may be particularly useful for sponsored media content. Providing improved categorization and/or improved search results to search queries can increase the effectiveness of advertising by encouraging user engagement. For example, advertisements may be better categorized by the semantic meaning of dynamic captions and/or emotional content. For example, an advertisement depicting surprise may be presented in response to a search query related thereto. As a result, users are more likely to engage with sponsored media content items.
예시적인 구성Exemplary configuration
도 5a는 일부 실시예에 따른 미디어 컨텐츠 관리 시스템(100)을 나타내는 고레벨 블록도이다. 미디어 컨텐츠 관리 시스템(100)은 미디어 컨텐츠 저장소(106)에 저장된 미디어 컨텐츠 소스(124)로부터 미디어 컨텐츠 아이템(104)을 수신할 수 있다. 도 5a 및 다른 도면은 동일한 요소를 식별하기 위해 동일한 참조 번호를 사용한다. "102a"와 같이 참조 번호 뒤의 문자는 텍스트가 해당 특정 참조 번호를 갖는 요소를 구체적으로 지칭함을 나타낸다. "102"와 같이 다음 문자가 없는 텍스트의 참조 번호는 해당 참조 번호를 갖는 도면의 요소 중 일부 또는 전부를 지칭한다(예를 들어, 텍스트에서 "102"는 도면에서 참조 번호 "102a" 및/또는 "102b"를 나타낸다). 설명을 단순화하고 명확하게 하기 위해 단지 2개의 사용자 디바이스(102)가 도 5a에 도시되어 있다. 관리자는 일 실시예에서, 별도의 로그인 프로세스를 통해 사용자 디바이스(102)(예를 들어, 사용자 디바이스(102a 및 102b))를 통해 미디어 컨텐츠 관리 시스템(100)에 액세스할 수 있다.5A is a high-level block diagram illustrating a media
위에서 언급한 바와 같이, 미디어 컨텐츠 아이템(104)은 애니메이션 GIF(일련의 이미지), 정적 이미지, 시청각 컨텐츠 아이템/비디오와 같은 다양한 유형의 컨텐츠뿐만 아니라 다수의 애니메이션 GIF 및/또는 이미지 컨텐츠와 같은 복합 컨텐츠 아이템을 포함할 수 있다. 미디어 컨텐츠 아이템(104)은 미디어 컨텐츠 관리 시스템(100)에 수신되고 미디어 컨텐츠 저장소(106)에 저장된다. 미디어 컨텐츠 아이템(104)은 컨텐츠 소스, 치수, 컨텐츠 브랜딩(예를 들어, 파라마운트 영화사, NBC 유니버셜 등), 컨텐츠에 포함된 문자, 컨텐츠에 포함된 텍스트 문자열 등과 같은 하나 이상의 속성을 가질 수 있다. 속성들은 일 실시예에서 메타 데이터 속성을 포함할 수 있다.As noted above, the
미디어 컨텐츠 저장소(106)에서, 미디어 컨텐츠 아이템(104)은 컬렉션 또는 미디어 컨텐츠 아이템(104)의 그룹과 관련하여 저장될 수 있다. 일 실시예에서, 컬렉션은 미디어 컨텐츠 관리 시스템(100)의 관리자에 의해 생성될 수 있다. 일 실시예에서, 컬렉션은 그 컬렉션 내의 미디어 컨텐츠 아이템들(104)에 의해 공유되는 하나 이상의 속성에 기초하여 자동으로 생성될 수 있다. 일 실시예에서, 컨텐츠 연관 또는 고유 식별자는 미디어 컨텐츠 관리 시스템(100)에서 컬렉션을 나타내기 위해 사용될 수 있다. 예를 들어, 미디어 컨텐츠 아이템(104)은 미디어 컨텐츠 관리 시스템(100)에서 "#happy"컬렉션의 일부로서 "연관 컨텐츠"일 수 있다. 일 실시예에서, 사용자 또는 관리자는 "#happy"컬렉션의 일부로서 미디어 컨텐츠 아이템(104)을 컨텐츠 연관시킬 수 있다. 다른 실시예에서, 미디어 컨텐츠 아이템(104)은 컨텐츠 연결 저장소(118)에 저장된 컨텐츠 연관들을 사용하여 컨텐츠 연관 모듈(108)에 의해 미디어 컨텐츠 아이템(104)과 자동으로 연관될 수 있다. 이러한 방식으로, 컨텐츠는 미디어 컨텐츠 관리 시스템(100)에서 "#happy"와 같은 컨텐츠 연관을 사용하여 조달되고 번주화될 수 있다. 개별 컬렉션 또는 파일 세트는 각각 미디어 컨텐츠 관리 시스템(100)에서 컨텐츠 연관으로 라벨링될 수 있다. 일 실시예에서, 특정 파일은 하나 이상의 컨텐츠 연관과 연관될 수 있다.In the
컨텐츠 연관 모듈(108)은 예를 들어 도 2의 시간 분석 모델(86)을 참조하여 전술한 바와 같이 하나 이상의 시간 분석 모델(109)을 포함할 수 있다. 컨텐츠 연관 모듈(108)은 미디어 컨텐츠 아이템(104)을 분석하고, 미디어 컨텐츠 아이템으로부터 추출된 시간 분석 데이터에 기초하여 미디어 컨텐츠 아이템들(104)에 대한 컨텐츠 연관을 생성하도록 구성될 수 있다.The
[0092][0092]
일 실시예에서, 미디어 컨텐츠 관리 시스템(100)의 사용자는 사용자 디바이스(102a)를 통해 미디어 컨텐츠 관리 시스템(100)에 컨텐츠를 추가할 수 있다. 예를 들어, 사용자는 사용자 디바이스(102a)상의 브라우저(110)를 사용하여 웹 페이지(112)를 브라우징하여 찾은 컨텐츠 아이템(114)을 "저장"할 수 있도록 사용자 디바이스(102a)에 애플리케이션 확장(116)을 설치했을 수 있다. 애플리케이션 확장(116)을 사용하여 컨텐츠 아이템(114)을 저장함으로써, URL(Uniform Resource Locator)은 일 실시예에서 컨텐츠 아이템의 속성으로서 컨텐츠 아이템(114)과 관련하여 저장될 수 있다. 애플리케이션 확장(116)은 일 실시예에서, 사용자가 웹 페이지를 탐색하고 웹 페이지에 제시된 미디어 컨텐츠 아이템을 수집할 수 있게 하는 다운로드 가능한 애플리케이션을 포함할 수 있다. 예를 들어, 블로그용 웹 페이지는 미디어 컨텐츠 관리 시스템(100)에서 이용 가능하거나 이용 가능하지 않을 수 있는 특히 흥미로운 컨텐츠 아이템을 게시(posting)할 수 있다. 애플리케이션 확장(116)을 사용하면, 사용자는 웹 페이지(112)를 탐색하고, 브라우저(110)를 통해 메뉴에 액세스하고, 웹 페이지(112)에 제시된 하나 이상의 컨텐츠 아이템(114)을 저장하기 위한 옵션을 선택할 수 있다. 일 실시예에서, 애플리케이션 확장(116)은 모바일 브라우저들(110)이 이 기능을 수행할 수 있게 하는 모바일 애플리케이션이다. 다른 실시예에서, 애플리케이션 확장(116)은 모바일 디바이스 또는 데스크탑 컴퓨터에서 브라우저(110)를 통해 다운로드될 수 있는 브라우저 확장 애플리케이션 또는 애플릿(applet)일 수 있다. 추가 실시예에서, 애플리케이션 확장(116)은 사용자가 미디어 컨텐츠 관리 시스템(100)의 미디어 컨텐츠 저장소(106)에 컨텐츠 아이템들(114)을 직접 업로드하는 것을 가능하게 할 수 있다.In one embodiment, a user of the media
다른 실시예에서, 컨텐츠 아이템(114)의 사본은 위에서 설명된 애플리케이션 확장(116)을 조작하는 사용자의 일부로서 미디어 컨텐츠 저장소(106)에 저장된다. 추가 실시예에서, 컨텐츠 아이템(114)의 링크 또는 URL은 미디어 컨텐츠 저장소(106)에 저장된다. 또 다른 실시예에서, 컨텐츠 아이템(114)의 사본은 "저장된"컬렉션의 일부로서 사용자 디바이스(102a)에 저장되거나 사용자 디바이스(102a)상의 사용자 생성 컬렉션에 저장된다. 사용자는 "저장된" 컬렉션과 같은 사용자-생성 컬렉션을 포함하여, 컬렉션들이 사용자 디바이스들(102) 사이에서 동기화될 수 있도록 다양한 사용자 디바이스(102)에서 자신의 계정에 로그인할 수 있다.In another embodiment, a copy of the content item 114 is stored in the
웹 페이지(112) 상에 제시되거나 웹 서버를 통해 액세스 가능한 컨텐츠 아이템(114)은 일 실시예에서 다른 방식으로 미디어 컨텐츠 관리 시스템(100)의 관리자에 의해 조달될 수 있다. 예를 들어, 영화 스튜디오, 텔레비전 스튜디오, 브랜드 소유자 및 기타 컨텐츠 생성자와 같은 컨텐츠 소유자는 라이센스 컨텐츠가 미디어 컨텐츠 저장소(106)에 전달되고 저장될 수 있도록 미디어 컨텐츠 관리 시스템(100)의 관리자와 협력할 수 있다. 이러한 조달 프로세스에서, 컨텐츠 소유자들은 전술한 바와 같이 미리 채워진 속성들을 갖는 미디어 컨텐츠 아이템들(104)을 제공할 수 있다. 컨텐츠 소유자와 같은 미디어 컨텐츠 소스(124)는 예를 들어 제3 자 소스 또는 웹 사이트에 의해 유지되고 운영되는 서버상의 컨텐츠 저장소 또는 데이터베이스를 포함할 수 있다. 조달 프로세스의 일부로서, 컨텐츠 아이템들(104)은 컨텐츠 연관 저장소(118)로부터의 하나 이상의 컨텐츠 연관과 연관하여 그들을 저장함으로써 하나 이상의 컬렉션으로 범주화될 수 있다. 일 실시예에서, 컨텐츠 연관들은 컨텐츠 아이템들(104)의 속성에 기초하여 컨텐츠 연관 모듈(108)에 의해 자동으로 생성될 수 있다. 다른 실시예에서, 컨텐츠 연관들은 하나 이상의 사용자 인터페이스 또는 API(application programming interface)를 통해 선택될 수 있다. 추가 실시예에서, 미디어 컨텐츠 아이템들(104)은 사용자 디바이스(102)상의 하나 이상의 사용자 인터페이스를 통해 미디어 컨텐츠 저장소(106)에 저장된 후 미디어 컨텐츠 관리 시스템(100)의 사용자들에 의해 연관된 컨텐츠일 수 있다.A content item 114 presented on a
도 5a에 추가로 도시된 바와 같이, 동적 키보드 인터페이스(122)는 예를 들어 사용자 디바이스(102b) 상에 제공될 수 있다. 동적 키보드 인터페이스(122)는 미디어 컨텐츠 아이템(104)뿐만 아니라 미디어 컨텐츠 아이템(104)의 컬렉션을 포함할 수 있다. 예를 들어, 동적 키보드 인터페이스(122)는 "#FOMO(#포모)"로 연관된 미디어 컨텐츠 아이템(104) 컨텐츠의 컬렉션을 포함할 수 있다. "#FOMO"는 인터넷 속어의 표현으로 "고립 공포감"을 의미한다. 따라서, "#FOMO" 컬렉션에 포함된 미디어 컨텐츠 아이템들(104)은 특정 표현인 "고립 공포감"에 관한 표현적 진술에 관한 것일 수 있거나 이를 포함할 수 있다. 하나 이상의 표현적 진술은 일 실시예에서 미디어 컨텐츠 아이템(104)으로부터 추출 및/또는 해석될 수 있다. 예를 들어, 큐레이팅 사용자는 이미지에서 "FOMO"라는 깜박이는 텍스트, 그 이미지 속 캐릭터가 외롭거나 친구가 없거나 멋진 이벤트를 놓칠까봐 두려워한다는 것을 나타내는 영화 또는 TV 쇼의 캡션 대화와 같이 "고립 공포감"이라는 표현과 관련된 미디어 컨텐츠 아이템(104)의 이미지들에 기초하여 미디어 컨텐츠 아이템(104)을 "#FOMO"로 컨텐츠 연관시킬 수 있다. 조달 프로세스를 통해, 표현적 진술은 미디어 컨텐츠 관리 시스템(100)의 컨텐츠 연관에 매핑될 수 있다. 이러한 표현적 진술은 일 실시예에서 동적 인터페이스에서 애니메이션 입력을 통해 검색을 수행할 때 사용자의 검색 의도와 관련될 수 있다.As further shown in FIG. 5A , a
도 5a에 도시된 바와 같이. 동적 키보드 인터페이스(122)는 또한 다른 애니메이션된 키들 또는 애니메이션된 입력을 구현하는 동적 키보드의 영역들을 포함할 수 있다. 악수하는 두 손, 우는 아기, 안경, "#happy" 컨텐츠 연관 및 "#LOL"컨텐츠 연관의 애니메이션 키는 샴페인 병을 더 포함하는 "#FOMO" 애니메이션 키에 추가하여 에시적인 애니메이션 키 예시로서 도시되었다. 도시되지는 않았지만, 애니메이션 키들은 동적 키보드 인터페이스(122)에서 애니메이션으로 렌더링되는 미디어 컨텐츠 아이템들(104)을 포함할 수 있으며, 이는 컨텐츠가 키 내에서 일정한 루프로 이동할 수 있음을 의미한다. 미디어 컨텐츠 아이템(104)은 일 실시예에서 동적 인터페이스에서 애니메이션 입력을 가능하게 하기 위해 전처리될 수 있다.As shown in Figure 5a.
동적 키보드 인터페이스(122)에서 애니메이션 키 중 하나를 선택하면, 사용자 디바이스(102b)는 검색 인터페이스 모듈(120)을 통해 미디어 컨텐츠 관리 시스템(100)과 통신할 수 있다. 일 실시예에서, 사용자의 검색 이력 및/또는 사용자의 공유 이력은 동적 키보드 인터페이스(122)의 각 사용자에 대한 개인화 저장소(150)에 개인화된 정보로서 저장될 수 있다. (GPS 및/또는 IP 주소를 통한) 위치, 설치된 언어 키보드, 디폴트 언어 선택, 전화 정보, 연락처 정보, 설치된 메시징 애플리케이션 등과 같은 다른 개인화된 정보가 사용자 디바이스(102)에 대해 캡처될 수 있다. 개인화 저장소(150)에 포함된 데이터는 예를 들어 사용자의 검색 의도를 결정할 때 검색 인터페이스 모듈(120)에 의해 하나 이상의 요인(factor)으로서 사용될 수 있다. 도 5b에 더 도시된 바와 같이, 동적 키보드 인터페이스(122)는 사용자 디바이스(102b)에 설치된 동적 키보드 애플리케이션(130)을 통해 사용자 디바이스(102b)에서 렌더링될 수 있다. 동적 키보드 애플리케이션(130)은 동적 키보드 인터페이스(122)가 제3자 키보드로서 사용자 디바이스(102b)를 통해 액세스될 수 있게 하는 동적 키보드 사용자 인터페이스(132)를 설치할 수 있다. 이러한 방식으로, 메시징 애플리케이션(140)을 사용하는 메시징 사용자는 메시징 애플리케이션(140) 내에서 동적 키보드 인터페이스(122)에 액세스할 수 있다.Upon selecting one of the animated keys in the
도 5b는 일 실시예에서, 동적 인터페이스에서 애니메이션 입력을 구현하기 위해 검색을 수행하는 프로세스를 도시하는 고레벨 블록도이다. 도 5b에 더 도시된 바와 같이, 미디어 컨텐츠 아이템(104)은 검색 인터페이스 모듈(120)과 통신하는 동적 키보드 사용자 인터페이스(132)를 통해 동적 키보드 인터페이스(122)에서 렌더링된다. 일 실시예에서, 일련의 컬렉션이 동적 키보드 인터페이스(122)에 디스플레이되도록 선택될 수 있다. 도 1b에 도시된 바와 같이, 동적 키보드 인터페이스(122)는 "#PLEASE", "#HAPPY", "#RUDE"및 "#FACEPALM" 컬렉션을 포함한다. 여기에 포함된 예들에서는 해시 태그 심볼(#')가 사용되었지만, 컨텐츠 연관들이 반드시 해시 태그로 시작될 필요는 없다. 동적 키보드 인터페이스(122)에서 애니메이션 키를 선택함으로써, 미디어 컨텐츠 아이템(104)의 컬렉션은 검색 인터페이스 모듈(120)에 의해 미디어 컨텐츠 저장소(106)로부터 검색된 다음 동적 키보드 인터페이스(122)에서 동적 키보드 사용자 인터페이스(132)에 의해 렌더링될 수 있다. 이러한 방식으로, 검색하는 사용자는 "#HAPPY"와 같은 선택된 컨텐츠 연관을 이용함으로써 미디어 컨텐츠 관리 시스템(100)을 검색하고 있다. 검색된 미디어 컨텐츠 아이템(104)의 컬렉션은 동적 키보드 인터페이스(122)내에서 렌더링될 수 있다. "#HAPPY" 컬렉션은 실시간으로 업데이트되고 추가될 수 있기 때문에, 검색하는 사용자에게는 새로운 아이템이 컬렉션에 추가됨에 따라 상이한 미디어 컨텐츠 아이템(104)이 제시될 수 있다. 전술한 바와 같이, 미디어 컨텐츠 아이템(104)은 컨텐츠의 파일 크기를 감소시키기 위해 전처리될 수 있고, 따라서 미디어 컨텐츠 아이템들(104)이 동적 키보드 인터페이스(122)에 신속하게 렌더링될 수 있게 한다.5B is a high-level block diagram illustrating a process for performing a search to implement an animation input in a dynamic interface, in one embodiment. 5B , the
그런 다음 검색(하는) 사용자는 동적 키보드 사용자 인터페이스(132)를 터치하거나 그와 상호 작용함으로써 동적 키보드 인터페이스(122)로부터 미디어 컨텐츠 아이템을 선택할 수 있다. 선택된 미디어 컨텐츠 아이템(144)은 그 후 메시징 애플리케이션(140)의 메시징 사용자 인터페이스(142)로 전송되거나 그에 붙여넣어질 수 있다. 일 실시예에서, 선택된 미디어 컨텐츠 아이템(144)은 동적 키보드 인터페이스(122)를 클릭, 탭 또는 터치하고 선택된 미디어 컨텐츠 아이템 (144)을 홀딩하여 컨텐츠를 "복사"함으로써 메시징 사용자 인터페이스(142)를 통해 메시징 애플리케이션(140)에 "붙여 넣기"될 수 있도록 선택된다. 이 복사 및 붙여 넣기 방법은 일 실시예에서 선택된 미디어 컨텐츠 아이텐(144)이 사용자 디바이스(102) 상에 영구적으로 저장되지 않도록 사용자 디바이스(102)의 운영 체제를 이용할 수있다. 다른 실시예에서, 검색 사용자는 본 명세서에서 더 설명되는 동적 키보드 인터페이스(122)상의 검색 필드를 통해 미디어 컨텐츠를 검색할 수 있다. 이러한 방식으로, 미디어 컨텐츠 아이템들(104)은 사용자의 디바이스에서 이용 가능한 임의의 메시징 플랫폼을 통해 공유될 수 있다. 개인화 정보는 또한 전술한 바와 같이, 예를 들어 검색 인터페이스 모듈(120)을 통해 개인화 저장소(150)에서 캡처될 수 있다. 적어도 일부 실시예에서, 동적 키보드 인터페이스(122)는 캘리포니아 샌프란시스코에 있는 RIFFSY, INC.에 의해 생성된 GIF 키보드로 구현될 수 있다.A user who searches (does) may then select a media content item from the
도 6a는 일 실시예에 따른 미디어 컨텐츠 관리 시스템의 블록도를 나타내는, 미디어 컨텐츠 관리 시스템에서 검색을 수행하기 위해 조달된 컨텐츠를 범주화하는 시스템의 네트워크 다이어그램이다. 시스템 환경은 하나 이상의 사용자 디바이스(102), 미디어 컨텐츠 소스들(124), 제3자 애플리케이션들(202), 미디어 컨텐츠 관리 시스템(100) 및 네트워크(204)를 포함한다. 대체 구성에서는 다른 및/또는 추가 모듈이 시스템에 포함될 수 있다.6A is a network diagram of a system for categorizing procured content for performing a search in a media content management system, illustrating a block diagram of a media content management system according to an embodiment. The system environment includes one or
사용자 디바이스(102)는 사용자 입력을 수신할 수 있고 네트워크(204)를 통해 데이터를 송수신할 수 있는 하나 이상의 컴퓨팅 디바이스를 포함할 수 있다. 다른 실시예에서, 사용자 디바이스(102)는 PDA(Personal Digital Assistant), 모바일 전화기, 스마트 폰, 웨어러블 디바이스 등과 같이 컴퓨터 기능을 갖는 디바이스일 수 있다. 사용자 디바이스(102)는 네트워크(204)를 통해 통신하도록 구성된다. 사용자 디바이스(102)는 애플리케이션, 예를 들어 사용자 디바이스(102)의 사용자가 미디어 컨텐츠 관리 시스템(100)과 상호 작용할 수 있도록 하는 브라우저 애플리케이션을 실행할 수 있다. 다른 실시예에서, 사용자 디바이스(102)는 사용자 디바이스(102)의 네이티브 운영 체제에서 실행되는 API를 통해 미디어 컨텐츠 관리 시스템(100)과 상호 작용한다.
일 실시예에서, 네트워크(204)는 표준 통신 기술 및/또는 프로토콜을 사용한다. 따라서, 네트워크(204)는 이더넷, 802.11, 마이크로웨이브 액세스를 위한 월브와이드 상호 운용성(WiMAX), 3G, 4G, CDMA, 디지털 가입자 회선(DSL) 등과 같은 기술을 사용하는 링크들을 포함할 수 있다. 유사하게, 네트워크(204)에서 사용되는 네트워킹 프로토콜은 멀티 프로토콜 레이블 전환(MPLS), 전송 제어 프로토콜/인터넷 프로토콜(TCP/IP), 사용자 데이터그램 프로토콜(UDP), 하이퍼 텍스트 전송 프로토콜(HTTP), SMTP(Simple Mail Transfer Protocol) 및 파일 전송 프로토콜(FTP)을 포함할 수 있다. 네트워크(204)를 통해 교환되는 데이터는 HTML(Hypertext Markup Language) 및 XML(Extensible Markup Language)을 포함하는 기술 및/또는 포멧을 사용하여 표현될 수 있다. 또한 보안 소켓 계층(SSL), 전송 계층 보안(TLS) 및 인터넷 프로토콜 보안(IPsec)과 같은 기존 암호화 기술을 사용하여 링크의 전체 또는 일부가 암호화될 수 있다.In one embodiment,
도 6a는 미디어 컨텐츠 관리(100)의 블록도를 포함한다. 미디어 컨텐츠 관리 시스템(100)은 미디어 컨텐츠 저장소(106), 컨텐츠 연관 저장소(118), 개인화 저장소(150), 검색 인터페이스 모듈(120), 컨텐츠 연관 모듈(108), 동적 키보드 인터페이스 모듈(208), 웹 서버(210), 동적 키보드 제시 모듈(212), 컨텐츠 연관 관리 모듈(214), 감정 분석 모듈(220), 이미지 분석(analyzer) 모듈(222), 움직임 분석기(224), 자연어 처리(NLP) 파서(218), 휴리스틱 엔진(216) 및 검색 라우터 규칙 엔진(206)을 포함한다. 다른 실시예에서, 미디어 컨텐츠 관리 시스템(100)은 다양한 애플리케이션을 위한 추가의, 더 적은 또는 상이한 모듈을 포함할 수 있다. 네트워크 인터페이스, 보안 기능, 로드 밸런서, 장애 조치(failover) 서버, 관리 및 네트워크 운영 콘솔 등과 같은 기존 컴포넌트는 시스템의 세부 사항을 모호하게 하지 않기 위해 도시되지 않는다.6A includes a block diagram of
웹 서버(210)는 네트워크(204)를 통해 미디어 컨텐츠 관리 시스템(100)을 하나 이상의 사용자 디바이스(102)에 링크시키고; 웹 서버(210)는 웹 페이지뿐만 아니라 Java, Flash, XML 등과 같은 다른 웹 관련 컨텐츠를 제공한다. 웹 서버(210)는 미디어 컨텐츠 관리 시스템(100)과 사용자 디바이스(102) 사이에서 메시지, 예를 들어 인스턴트 메시지, 큐(queued) 메시지(예를 들어, 이메일), 텍스트 및 SMS(단문 메시지 서비스) 메시지, 또는 다른 적절한 메시징 기술을 사용하여 전송된 메시지를 수신하고 라우팅하는 기능을 제공할 수 있다. 사용자는 정보를 업로드하기 위해 웹 서버(210)에 요청을 보낼 수 있으며, 예를 들어 이미지 또는 미디어 컨텐츠는 미디어 컨텐츠 저장소(106)에 저장된다. 추가적으로, 웹 서버(210)는 데이터를 네이티브 사용자 디바이스 운영 체제로 직접 전송하기 위한 API 기능을 제공할 수 있다.The
컨텐츠 연관 모듈(108)은 미디어 컨텐츠 아이템(104)의 속성에 기초하여 미디어 컨텐츠 관리 시스템(100)에서 미디어 컨텐츠 아이템(104)에 대한 하나 이상의 컨텐츠 연관을 자동으로 생성할 수 있다. 예를 들어, 기계 학습 기술은 컨텐츠 연관 모듈(108)에 의해 사용되어 미디어 컨텐츠 아이템들(104)과 컨텐츠 연관 저장소(118)에 저장된 컨텐츠 연관들 사이의 관계를 결정할 수 있다.The
컨텐츠 연관 모듈(108)은 예를 들어 도 2의 시간 분석 모델(86)을 참조하여 전술한 바와 같이 하나 이상의 시간 분석 모델(109)을 포함할 수 있다. 컨텐츠 연관 모듈(108)은 NLP 파서(218)로부터 미디어 컨텐츠 아이템의 하나 이상의 이미지 프레임에서 인식된 텍스트를 기술하는 데이터(예를 들어, 도 2의 텍스트 인식 데이터(89))를 수신할 수 있다. 이 데이터는 시간 분석 모델(86)에 입력될 수 있다.The
컨텐츠 연관 모듈(108)은 미디어 컨텐츠 아이템들(104)을 분석하고 미디어 컨텐츠 아이템들로부터 추출된 시간 분석 데이터에 기초하여 미디어 컨텐츠 아이템ㄷ들04)에 대한 컨텐츠 연관을 생성하도록 구성될 수 있다.The
일 실시예에서, 컨텐츠 연관 모듈(108)은 영화 스튜디오, 영화, 텔레비전 스튜디오, 텔레비전 쇼, 배우, 장르 등과 같은 하나 이상의 컨텐츠 소스를 식별할 수 있다. 다른 실시예에서, 컨텐츠 연관 모듈(108)은 미디어 컨텐츠 아이템(104) 내의 이미지 프레임의 분석에 기초하여 미디어 컨텐츠 아이템(104)에 대한 컨텐츠 연관을 자동으로 생성할 수 있다. 또 다른 실시예에서, 컨텐츠 연관 모듈(108)은 하나 이상의 컴퓨터 비전 기술 및 다른 이미지 처리 방법을 사용할 수 있다. 예를 들어, 다양한 제3자 애플리케이션(202)이 미디어 컨텐츠 아이템(104) 내의 이미지 프레임을 분석하여 컨텐츠 아이템과 연관될 하나 이상의 컨텐츠 연관을 자동으로 생성하기 위해 사용될 수 있다. 일 실시예에서, 컨텐츠 연관 모듈(108)은 하나 이상의 제3자 애플리케이션(202), NLP 파서(218), 감정 분석 모듈(220), 이미지 분석기 (222), 움직임 분석기(224) 및 휴리스틱 엔진(216)을 활용하여 미디어 컨텐츠 아이템(104)에 포함된 텍스트를 분석 및 파싱할뿐만 아니라 미디어 컨텐츠 아이템 (104)의 동영상 프레임을 분석하여, 컨텐츠 연관을 자동으로 생성하고 및/또는 컨텐츠 연관 저장소(118)에 저장된 컨텐츠 연관을 자동으로 선택할 수 있다. 다른 실시예에서, NLP 파서(218)는 감정 분석 모듈(220)과 결합될 수 있고 미디어 컨텐츠 아이템(104)의 감정을 결정하기 위해 이미지 및/또는 시청각 컨텐츠를 분석하는데 의존할 수 있다. 예를 들어, 이미지 분석기(222) 및 움직임 분석기(224)는 웃는 얼굴을 묘사하는 이미지 시퀀스를 검출 및/또는 분류하는데 사용될 수 있다. 휴리스틱 엔진(216)은 미소를 검출하기 위해 분석된 이미지 시퀀스를 갖는 미디어 컨텐츠 아이템(104)을 그 미디어 컨텐츠 아이템(104)이 미디어 컨텐츠 관리 시스템(100)의 미디어 컨텐츠 저장소(106) 내에 저장될 때 컨텐츠 연관 저장소(118)의 "#happy" 컨텐츠 연관과 자동으로 연관시키는 규칙을 포함할 수있다. 대안적으로 또는 이 분석에 추가하여, NLP 파서(218)는 이미지에 포함된 텍스트 문자열을 파싱하여 "AWESOME"이라는 단어에 대한 매칭을 결정할 수 있다. 추가적으로, NLP 파서(218)는 미소를 긍정적인 감정을 의미하는 것으로 해석할 수 있다. 감정 분석 모듈(220)은 "AWESOME"이라는 단어가 강한 긍정적 감정과 연관되어 있음을 나타낼 수 있으며, 휴리스틱 엔진(216)은 "#happy" 컨텐츠 연관(및/또는 다른 긍정적 컨텐츠 연관)을 강한 긍정적 감정을 갖는 미디어 컨텐츠 아이템들(104)과 자동으로 연관시키는 규칙을 포함할 수 있다.In one embodiment, the
검색 인터페이스 모듈(120)은 일 실시예에서 사용자 디바이스(102)로부터 수신된 미디어 컨텐츠 관리 시스템(100)에서 미디어 컨텐츠 아이템들(104)에 대한 검색 요청 및/또는 검색 질의를 관리할 수 있다. 검색 질의는 일 실시예에서 검색 인터페이스 모듈(120)에서 수신되고 검색 라우터 규칙 엔진(206)에 의해 처리될 수 있다. 다른 실시예에서, 검색 인터페이스모듈(120)은 "#HAPPY", "#RUDE", "#FOMO"등과 같은 컨텐츠 연관 및 애니메이션 키를 선택하거나 텍스트를 검색한 결과에 기초하여 사용자 디바이스(102)로부터 컬렉션에 대한 요청을 수신할 수 있다. 검색 인터페이스 모듈(120)은 일 실시예에서 요청을 처리하기 위해 검색 라우터 규칙 엔진(206)에 검색 질의를 전달할 수 있다.The
컨텐츠 연관 관리 모듈(214)은 미디어 컨텐츠 관리 시스템(100)에서 각각의 미디어 컨텐츠 아이템(104)과 연관된 하나 이상의 컨텐츠 연관을 관리할 수 있다. 컨텐츠 연관은 사용자 인터페이스 및 API(application programming interface)와 같은 다양한 인터페이스를 통해 컨텐츠 연관 관리 모듈(214)을 통하여 미디어 컨텐츠 아이템(104)과 연관될 수 있다. API는 미디어 컨텐츠 소스(124), 제3자 애플리케이션(202)(및/또는 웹 사이트) 및 사용자 디바이스(102)로부터 데이터를 수신, 액세스 및 저장하는데 사용될 수 있다. 컨텐츠 연관 관리 모듈(214)은 일 실시예에서 다양한 조달 방법을 통해 컨텐츠 연관이 미디어 컨텐츠 아이템(104)과 연관되는 방식을 관리할 수 있다.The content
동적 키보드 인터페이스 모듈(208)은 미디어 컨텐츠 관리 시스템(100)과 사용자 디바이스(102) 사이의 인터페이스 통신을 관리할 수 있다. 예를 들어, 도 5a 및도 5b에 도시된 바와 같이, 동적 키보드 인터페이스(122)는 검색하는 사용자가 미디어 컨텐츠 관리 시스템(100)상의 트렌드(trending) 미디어 컨텐츠를 볼 수 있도록 하는 메뉴 선택 요소를 포함할 수 있다. "트렌드" 미디어 컨텐츠는 미디어 컨텐츠 관리 시스템(100)의 사용자들에 의해 자주 시청 및/또는 자주 공유되는 컨텐츠를 포함할 수 있다. 동적 키보드 인터페이스 모듈(208)은 예를 들어, 트렌드 미디어 컨텐츠에 대한 요청을 수신하여, 예를 들어 지난 1 시간 동안 가장 많은 수의 공유를 갖는 미디어 컨텐츠 아이템(104)을 미디어 컨텐츠 저장소(106)로부터 검색할 수 있다. 그런 다음, 동적 키보드 인터페이스 모듈(208)은 일 실시예에서, 동적 키보드 제시 모듈(212)을 통해, 상기 검색된 트렌드 미디어 컨텐츠 아이템을 동적 키보드 애플리케이션(130)을 통해 동적 키보드 인터페이스(122)로 제공할 수 있다. 동적 키보드 제시 모듈(212)은 예를 들어, 미디어 컨텐츠 아이템들이 제시되는 방법 및 순서를 결정할 수 있다. 일 실시예에서, 미디어 컨텐츠 아이템(104)이 검색 질의 또는 사용자 디바이스의 요청을 충족하지 않는 경우, 동적 키보드 인터페이스 모듈(208)은 검색 인터페이스 모듈(120) 및 검색 라우터 규칙 엔진(206)과 함께 또는 이와 연계하여, 인기가 있거나 공유되었던 다른 미디어 컨텐츠 아이템(104)을 전달할 수 있다. 일 실시예에서, 컨텐츠 아이템은 동적 키보드 인터페이스(122)의 검색 결과 또는 애니메이션 키에 포함되도록 제3자 애플리케이션(202)(또는 웹 사이트)으로부터 동적 키보드 인터페이스 모듈(208)에 의해 선택될 수 있다.The dynamic
휴리스틱 엔진(216)은 하나 이상의 결과를 결정하기 위해 하나 이상의 휴리스틱 규칙을 포함할 수 있다. 예를 들어, 컨텐츠 연관 모듈(108)은 휴리스틱 엔진(216)을 사용하여, 미디어 컨텐츠 아이템(104)의 속성들에 기초하여 미디어 컨텐츠 아이템(104)에 대한 후보 컨텐츠 연관들의 순위를 결정할 수 있다. 특정 속성들에는 시각적 움직임(예를 들어, 검출된 미소는 "#HAPPY"컨텐츠 연관과 연관될 수 있음), 시각적 특성(예를 들어, 깜박이는 텍스트는 텍스트 문자열의 중요도를 나타낼 수 있거나 해시 태그 기호는 특정 컨텐츠 연관을 나타낼 수 있음), 컨텐츠 소스, 미디어 컨텐츠 아이템에 포함된 문자 및 기타 속성과 같은 다양한 휴리스틱 규칙이 있을 수 있다. 일 실시예에서, 속성들에 기초하여 컨텐츠 아이템에 대한 컨텐츠 연관을 자동으로 생성하기 위해 관리자에 의해 다양한 휴리스틱 규칙이 생성될 수 있다. 다른 실시예에서, 휴리스틱 규칙은 또한 다양한 속성에 대한 파라미터들의 범위를 사용할 수 있다. 예를 들어, 특정 사용자가 공유하기 위한 미디어 컨텐츠 아이템(104)의 30가지 선택은 검색 결과가 거의없는 특정 사용자로부터의 검색 질의에 응답하여 동일한 미디어 컨텐츠 아이템을 제시하기 위해 휴리스틱 규칙에서 사용될 수 있다. 여기서 범위는 예를 들어 임계 공유 수로 정의될 수 있다.The
감정 분석 모듈(220)은 텍스트가 긍정적, 부정적 또는 중립적 의미를 나타내는지 여부를 결정하기 위해 미디어 컨텐츠 관리 시스템(100)에 의해 수신된 다양한 텍스트의 분석을 제공할 수 있다. 이 정보는 검색하는 사용자의 표현 의도를 추출하기 위해 검색 질의를 효율적으로 번역하기 위한 다양한 모듈에 의해 사용될 수 있다. 예를 들어, 용어 사전을 다수의 언어로 사용하여 텍스트가 긍정적, 부정적 또는 중립적 의미를 갖는 것으로 결정될 수 있는지 여부를 확인할 수 있다. 감정 분석 모듈(220)은 일 실시예에서 다양한 제3자 애플리케이션(202)을 사용하여 이 분석을 수행할 수 있다. 감정 분석 모듈(220)을 사용하여, 검색 라우터 규칙 엔진(206)은 예를 들어 검색 질의의 의미에 기초하여 미디어 컨텐츠 아이템(104)의 하나 이상의 컬렉션을 제공할 수 있다. The
도 6b는 일 실시예에 따른 미디어 컨텐츠 관리 시스템에서 검색을 수행하기 위해 조달된 컨텐츠를 분류하기 위한 시스템의 고레벨 블록도이다. 컨텐츠 연관 관리 모듈(214)은 일 실시예에서 메타 데이터 분석 모듈(240), 사용자 인터페이스 모듈(242), 컨텐츠 연관 선택 모듈(244) 및 연관 관련 모듈(246)을 포함할 수 있다.6B is a high-level block diagram of a system for categorizing procured content to perform a search in a media content management system according to an embodiment. The content
하나 이상의 속성을 갖는 미디어 컨텐츠 아이템들(104)이 미디어 컨텐츠 관리 시스템(100)에서 미디어 컨텐츠 소스(124)로부터 수신됨에 따라, 메타 데이터 분석 모듈(240)은 미디어 컨텐츠 아이템들(104)의 속성에 기초하여 하나 이상의 컨텐츠 연관을 생성할 수 있다. 예를 들어, 특정 영화로부터의 미디어 컨텐츠 아이템(104)은 미디어 컨텐츠 아이템(104)과 관련된 영화 메타 데이터 속성에 기초하여 해당 특정 영화에 대한 컬렉션에 자동으로 연관된 컨텐츠일 수 있다. 일 실시예에서, 미디어 컨텐츠 소스(124)의 관리자는 하나 이상의 메타 데이터 속성을 미디어 컨텐츠 아이템들(104)에 연관시킬 수 있다. 메타 데이터 속성은 소스 파일 내의 헤더 컨텐츠 연관과 같은 미디어 컨텐츠 아이템(104)의 소스 파일뿐만 아니라 미디어 컨텐츠 시스템(100)에 의해 배치(batches)로 조달되는 컨텐츠 아이템을 기술하는 XML 파일과 같은 소스 파일과 관련된 다른 파일에 에 다양한 방식으로 저장될 수 있다. As the
메타 데이터 분석 모듈(240)은 일 실시예에서, 미디어 컨텐츠 아이템들(104)과 관련된 메타 데이터를 파싱하고, 하나 이상의 규칙에 기초하여 컨텐츠 연관 저장소(118)로부터 컨텐츠 연관을 자동으로 생성 및/또는 선택할 수 있다. 도 6b에 도시된 바와 같이, 컨텐츠 연관 저장소(118)는 속성들이 컨텐츠 연관들과 관련되도록 연관-속성 관계(250)를 저장할 수 있다. 이러한 방식으로, 메타 데이터 분석 모듈(240)은 컨텐츠 연관 저장소(118)에 저장된 연관-속성 관계(250)에 기초하여 미디어 컨텐츠 아이템(104)에 컨텐츠 연관을 자동으로 할당할 수 있다.The
메타 데이터 분석 모듈(240)에 의해 분석될 수 있는 다른 메타 데이터 속성은 검색 사용자 또는 큐레이팅 사용자에 의해 사용되는 모바일 디바이스 또는 사용자 디바이스의 인터넷 프로토콜(IP) 주소를 포함한다. IP 주소는 출신 국가를 포함하여 사용자의 지리적 위치의 표시를 제공할 수 있다. 대안으로, 모바일 디바이스의 GPS(Global Position System)는 사용자의 현재 지리적 위치를 포함할 수 있다. 결과적으로, 사용자의 지리적 위치에서 발화된 주요 언어에 기초하여 상이한 컬렉션 또는 컨텐츠 연관이 사용자에게 제시될 수 있다. 다른 실시예에서, 메타 데이터 분석 모듈(240)에 의해 분석될 수 있는 다른 메타 데이터 속성은 보는 사용자에 의해 선택된 하나 이상의 언어를 포함한다. 이러한 방식으로, 언어 선호도는 검색 의도, 큐레이팅 의도 또는 둘 다를 알리는데 도움이 될 수 있다. 예를 들어 프랑스어로 된 단어는 인도네시아어에서 완전히 강이한 의미를 가질 수 있다. 그 결과, 언어 및 출신 국가는 메타 데이터 분석 모듈(240)에 의해 결정될 수 있는 메타 데이터 속성일 수 있다.Other metadata attributes that may be analyzed by the
사용자 인터페이스 모듈(242)은 컴퓨터 또는 모바일 디바이스와 같은 사용자 디바이스(102)에 대한 하나 이상의 사용자 인터페이스를 제공하여, 조달된 미디어 컨텐츠 아이템들(104)에 대한 하나 이상의 컨텐츠 연관을 선택할 수 있다. 예를 들어, 큐레이팅 사용자는 컨텐츠 연관 저장소(118)로부터의 하나 이상의 컨텐츠 연관에 미디어 컨텐츠 아이템들(104)을 할당하는 기능이 주어질 수 있다. 이러한 방식으로, 컨텐츠 연관 관리 모듈(214)은 조달된 미디어 컨텐츠 아이템(104)을 분류하기 위한 컨텐츠 연관의 수동 선택을 가능하게 한다.
컨텐츠 연관 선택 모듈(244)은 일 실시예에 따라 사용자 인터페이스 모듈(242)에 의해 제공되는 하나 이상의 사용자 인터페이스에서 컨텐츠 연관 저장소(118)로부터 하나 이상의 컨텐츠 연관을 제공할 수 있다. 일 실시예에서, 컨텐츠 연관 선택 모듈(244)은 사용자 디바이스(102)를 운영하는 큐레이팅 사용자에 의한 선택 및/또는 확인을 위해 컨텐츠 연관 저장소(118)에 저장된 컨텐츠 연관-속성 연관(250)에 기초하여 예측된 컨텐츠 연관들을 제시할 수 있다. 예를 들어, 미디어 컨텐츠 아이템(104)은 미디어 컨텐츠 소스(124)로부터 미리 채워진 정보에 기초하여 코미디의 장르 속성을 가질 수 있다. "코미디" 속성은 "#HAPPY" 컨텐츠 연관과 관련될 수 있으므로, 일 실시예에서, 미디어 컨텐츠 아이템(104)은 메타 데이터 분석 모듈(240)에 의해 "#HAPPY" 컨텐츠 연관이 할당되었을 수 있다. 컨텐츠 연관 선택 모듈(244)은 큐레이팅 사용자가 상기 연관된 컨텐츠 아이템(104)과 관련된 컨텐츠 연관을 할당 또는 취소하도록 사용자 인터페이스 모듈(242)에 의해 제공되는 사용자 인터페이스에서 다른 관련 컨텐츠 연관과 함께 "#HAPPY" 컨텐츠 연관을 제시할 수 있다. 컨텐츠 연관 저장소(118)에 저장된 연관-속성 연관(250)은 일 실시예에서 다른 컨텐츠 연관과 관련된 컨텐츠 연관을 포함할 수 있다. 예를 들어, "#HAPPY" 컨텐츠 연관은 LOL 및 LMAO 모두 "웃는" 해석을 포함하기 때문에 "LOL" 및 "LMAO"컨텐츠 연관과 관련될 수 있다. 그 결과, 일 실시예에서, 큐레이팅 사용자에 의한 선택을 위해 다른 컨텐츠 연관이 제시될 수 있다.The content association selection module 244 may provide one or more content associations from the
컨텐츠 연관 선택 모듈(244)은 예를 들어 도 2의 시간 분석 모델(86)을 참조하여 전술한 바와 같이 하나 이상의 시간 분석 모델(245)을 포함할 수 있다. 컨텐츠 연관 선택 모듈(244)은 미디어 컨텐츠 아이템(104)을 분석하고 미디어 컨텐츠 아이템들(104)로부터 추출된 시간 분석 데이터에 기초하여 미디어 컨텐츠 아이템들(104)에 대한 컨텐츠 연관을 생성하도록 구성될 수 있다.The content association selection module 244 may include, for example, one or more
조달 프로세스의 일부로서, 미디어 컨텐츠 아이템들은 미디어 컨텐츠 저장소(106)에 저장되기 전에 전처리될 수 있다(252). 이는 미디어 컨텐츠 아이템(104)이 신속하게 검색되고 사용자 디바이스(102)상의 동적 키보드 인터페이스(122)에서 원활하게 렌더링될 수 있게 한다. 미디어 컨텐츠 아이템(252)의 전처리는 픽셀 카운트 감소, 해상도 정의 수정 및 기타 파일 크기 감소 기술을 포함할 수 있다. 동적 키보드 제시 모듈(212)은 일 실시예에서 미디어 컨텐츠 아이템(252)의 이러한 전처리를 수행하기 위해 사용될 수 있다. 유익하게, 미디어 컨텐츠 아이템(252)의 전처리는 사용자 디바이스(102b)에서 사용자에게 제시되는 동적 키보드 인터페이스(122)가 애니메이션으로 적어도 2개의 미디어 컨텐츠 아이템의 적어도 2개의 렌더링을 렌더링하고 이들을 동적 키보드 인터페이스(122)에 동시에 디스플레이할 수 있게 한다. As part of the procurement process, media content items may be preprocessed 252 before being stored in the
연관 관련 모듈(246)은 컨텐츠 연관들을 미디어 컨텐츠 저장소(106)의 미디어 컨텐츠 아이템들(104)에 관련시킬 수 있다. 컨텐츠 연관들은 메타 데이터 분석 모듈(240)(또는 미디어 컨텐츠 관리 시스템(100)의 다른 모듈)에 의해 자동으로 컨텐츠 아이템에 연관딜 수 있거나 컨텐츠 연관들은 사용자 인터페이스 모듈(242)에 의해 제공되는 사용자 인터페이스를 통해 수신된 컨텐츠 연관의 선택의 결과로서 연관될 수 있다. 도 6b에 도시된 바와 같이, 아이템-연관 관계(254)가 미디어 컨텐츠 저장소(106)에 저장된다. 각각의 컨텐츠 아이템은 컨텐츠 식별자를 가질 수 있고, 각각의 컨텐츠 연관은 아이템-연관 관계(254)가 미디어 컨텐츠 저장소(106)에 저장될 수 있도록 컨텐츠 연관 식별자를 가질 수 있다. 도 6b에 도시된 바와 같이, 컨텐츠 아이템("아이템")은 하나 이상의 연관("ass'n")과 관련될 수 있고, 아이템-연관 관계(254)는 예를 들어 미디어 컨텐츠 저장소(106)에 저장된다.
도 6c는 일 실시예에 따른 미디어 컨텐츠 관리 시스템에서 복합 컨텐츠 아이템을 구성하기 위한 시스템의 상위 레벨 블록도이다. 컴포저(composer) 인터페이스(264)가 사용자 디바이스(102)상에 제공될 수 있으며, 이는 보는 사용자가 미디어 컨텐츠 아이템(104)을 검색하고 둘 이상의 컨텐츠 아이템을 선택하여 복합 컨텐츠 아이템을 생성할 수 있게 한다. 도시된 바와 같이, 2개의 선택된 컨텐츠 아이템의 결합된 속성을 갖는 복합 컨텐츠 아이템(266)을 생성하기 위해 2개의 컨텐츠 아이템이 컴포저 인터페이스(264)에서 선택되었다. 예를 들어, 보는 사용자는 검색 인터페이스를 통해 "아니오(No)"를 검색할 수 있으며, 이에 대해서는 나중에 자세히 설명한다. 검색 용어 "아니오"를 충족하는 여러 컨텐츠 아이템(104)이 검색될 수 있다. 제1 선택된 컨텐츠 아이템은 "아니오" 및 "챈들러(Chandler)"의 컨텐츠 연관과 관련되었을 수 있는 반면 제2 선택된 컨텐츠 아이템은 "아니오" 및 "테일러(Taylor)"의 컨텐츠 연관과 관련되었을 수 있다. 그 결과, 복합 컨텐츠 아이템(266)은 컨텐츠 연관 "아니오", "챈들러" 및 "테일러"를 포함할 수 있다. 복합 컨텐츠 아이템(266)은 컴포저 인터페이스 모듈(262)에 의해 수신될 수 있고 복합 아이템 모듈(260)에 의해 미디어 컨텐츠 저장소(106)에 미디어 컨텐츠 아이템(104)으로서 저장될 수 있다. 도 6c에 더 도시된 바와 같이, 복합 아이템 모듈(260)은 컴포저 인터페이스 모듈(262)에 추가하여, 전술한 바와 유사하게 동작하는 메타 데이터 분석 모듈(240), 컨텐츠 연관 선택 모듈(244) 및 연관 관련 모듈(246)과 함께 동작하거나 이를 포함할 수 있다. 6C is a high-level block diagram of a system for organizing complex content items in a media content management system according to an embodiment. A
컨텐츠 연관 선택 모듈(244)은 예를 들어 도 2의 시간 분석 모델(86)을 참조하여 전술한 바와 같이 하나 이상의 시간 분석 모델(245)을 포함할 수 있다. 컨텐츠 연관 선택 모듈(244)은 미디어 컨텐츠 아이템(104)을 분석하고 미디어 컨텐츠 아이템(104)으로부터 추출된 시간 분석 데이터에 기초하여 미디어 컨텐츠 아이템(104)에 대한 컨텐츠 연관을 생성하도록 구성될 수 있다.The content association selection module 244 may include, for example, one or more
적어도 일부 실시예에서, 복합 컨텐츠 아이템(266)은 그 복합 컨텐츠 아이템(266)에 포함된 개별 컨텐츠 아이템과 상이한 의미를 전달하는 표현적 진술과 연관될 수 있다. 위의 예로 돌아가서, "아니오"라는 진술을 표현하는 문자 "챈들러"가 있는 제1 컨텐츠 아이템(104)은 미디어 컨텐츠 관리 시스템(100)의 대부분의 사용자들에게 특정 의미를 전달할 수 있다. 미디어 컨텐츠 관리 시스템(100)의 큐레이팅 사용자는 "#cool" 및 "프렌즈(FRIENDS)"와 같은 다른 컨텐츠 연관을 특정 컨텐츠 아이템(104)과 연관시킬 수있다. 연예인 테일러 로트너(TAYLOR LAUTNER)를 묘사하는 제2 컨텐츠 아이템(104)은 텔레비전 쇼 FRIENDS로부터의 캐릭터 "챈들러"를 묘사하는 제1 컨텐츠 아이템(104)과는 별개의 상이한 의미를 불러일으킬 수 있다. 제2 컨텐츠 아이템(104)은 예를 들어 "아니오"의 공유된 컨텐츠 연관에 추가하여 "cool(멋진)" 및/또는 "유명한(famous)"의 컨텐츠 연관과 자동 또는 수동으로 연관된 컨텐츠일 수 있다. 그 결과, 두 미디어 컨텐츠 아이템의 조합은 개별적으로 제시된 각 미디어 컨텐츠 아이템과 상이한 정보를 제시한다. 일 실시예에서, 복합 컨텐츠 아이템(266)에 의해 제시된 표현적 진술은 복합 컨텐츠 아이템(266)에 포함된 개별 컨텐츠 아이템과 관련된 컨텐츠 연관의 단순한 집합체일 수 있다. 다른 실시예에서, 개별 컨텐츠 아이템에 포함된 컨텐츠 연관들과 상이한 표현적 진술은 복합 컨텐츠 아이템(266)으로부터 추출되거나 이로부터 해석될 수 있다. 복합 컨텐츠 아이템(266)과 관련된 상기 연관된 컨텐츠 연관에 의해 저장된 바와같이, 이 표현적인 진술은 본 명세서에 설명된 바와 같이 검색 사용자의 의도를 관련 컨텐츠 아이템들에 상관시키는데 사용될 것이다.In at least some embodiments, a composite content item 266 may be associated with an expressive statement that conveys a different meaning than the individual content items included in the composite content item 266 . Returning to the example above, the
도 7a는 일부 예에 따라 미디어 컨텐츠 관리 시스템에서 조달된 컨텐츠를 분류하기 위한 시스템의 상위 레벨 블록도이다. 컨텐츠 연관 모듈(108)은 미디어 컨텐츠 아이템(104)에 대한 컨텐츠 연관(402)을 자동으로 선택하기 위한 컨텐츠 연관 알고리즘(406)을 포함할 수 있다. 컨텐츠 연관 모듈(108)은 컨텐츠 연관 저장소(118)로부터 컨텐츠 연관(402)을 선택하기 위한 컨텐츠 연관 선택기(408)를 더 포함할 수 있다. 컨텐츠 연관 모듈(108)은 미디어 컨텐츠 아이템(104)에 대한 컨텐츠 연관(402)을 자동으로 선택하는 것을 보조하기 위해 이미지 분석기(222), 움직임 분석기(224) 및 휴리스틱 엔진(216)과 함께 작동하거나 이를 포함할 수 있다.7A is a high-level block diagram of a system for classifying content procured from a media content management system in accordance with some examples. The
컨텐츠 연관 모듈(108)은 예를 들어 도 2의 시간 분석 모델(86)을 참조하여 전술한 바와 같이 하나 이상의 시간 분석 모델(245)을 포함할 수 있다. 컨텐츠 연관 모듈(108)은 미디어 컨텐츠 아이템(104)을 분석하고 미디어 컨텐츠 아이템(104)으로부터 추출된 시간 분석 데이터에 기초하여 미디어 컨텐츠 아이템(104)에 대한 컨텐츠 연관들을 생성하도록 구성될 수 있다.The
이미지 분석기(222)는 얼굴, 눈, 웃는 입, 찡그린 입 등과 같은 얼굴 특징을 인식하는 컴퓨터 비전 기술을 포함할 수 있다. 이미지 분석기(222)는 이러한 얼굴 특성을 인식하기 위한 베이스라인 트레이닝 세트를 생성하기 위해 다른 컴퓨터 비전 기술 및/또는 패턴 인식 알고리즘을 더 포함할 수 있다. 유사하게, 움직임 분석기(224)는 컴퓨터 비전 기술 및/또는 패턴 인식 알고리즘뿐만 아니라 울음, 웃음, 넘어짐 및 유사한 방식으로 모델링될 수 있는 다른 액션을 인식하기 위한 기계 학습 및 베이지안 추론 기술을 포함할 수 있다. 움직임 분석기(224)는 또한 이미지 세트 또는 애니메이션 이미지 내에서 눈의 위치를 식별하기 위한 시선 추적(eye-tracking) 기능을 포함할 수 있다. 움직임 분석기(224)의 시선 추적 기능은 미디어 컨텐츠 관리 시스템(100)에서 하나 이상의 다른 모듈과 함께 사용되어, 예를 들어, 이미지 내에서 검출된 눈 위의 애니메이션 이미지 세트에 한 쌍의 선글라스을 렌더링하는 것과 같은 새로운 미디어 컨텐츠 아이템(104)을 생성할 수 있다. 새로운 미디어 컨텐츠 아이템(104)을 작성 및/또는 생성하기 위해 "그것을 처리(deal with it)"라는 문구와 같은 텍스트를 미디어 컨텐츠 아이템(104)에 추가하기 위해 다른 모듈이 사용될 수 있다. 전술한 바와 같이 휴리스틱 엔진(216)은 수신된 데이터에 기초하여 결론에 도달하기 위해 다양한 규칙을 사용할 수 있다. 예를 들어, 도 7a에 도시된 바와 같이, 미디어 컨텐츠 아이템(104)은 예를 들어 우는 아기의 GIF를 포함할 수 있다. 이미지 분석기(222)는 미디어 컨텐츠 아이템(104)의 GIF의 프레임을 분석하여, 한 쌍의 눈을 가늘게 뜨고, 찡그린 자세로 입을 벌리고, 눈썹을 올리는 것과 같은 얼굴 특성을 결정할 수 있다. 움직임 분석기(224)는 미디어 컨텐츠 아이템(104)이 우는 아기의 베이스 라인 모델 및 다른 기계 학습 기술에 기초하여 우는 아기를 포함한다는 것을 식별할 수 있다.The
그 결과, 컨텐츠 연관 모듈(108)은 컨텐츠 연관 선택기(408)를 통해 컨텐츠 연관 저장소(118)로부터 하나 이상의 컨텐츠 연관을 선택할 수 있다. 컨텐츠 연관 알고리즘(406)은 미디어 컨텐츠 아이템(104)에 대한 컨텐츠 연관을 자동으로 생성하기 위해 휴리스틱 엔진(216)으로부터의 하나 이상의 휴리스틱 규칙을 포함할 수 있다. 이 예에서, "#sad" 컨텐츠 연관(402)이 미디어 컨텐츠 아이템(104)에 대해 선택되었다. 앞서 설명된 바와 같이, 컨텐츠 연관은 예를 들어, 우는 컨텐츠 연관이 "#sad"컨텐츠 연관(402)과 관련될 수 있는 것과 같이 다른 컨텐츠 연관과 관련될 수 있다. 이러한 방식으로, 우는 아기의 미디어 컨텐츠 아이템(104)은 자동 생성된 컨텐츠 연관에 기초하여 "#sad" 컬렉션(404)에 포함되어 미디어 컨텐츠 저장소(106)에 저장될 수 있다.As a result, the
도 7b는 일부 예에 따라 동적 인터페이스에서 애니메이션 입력을 구현하기 위해 검색을 수행하기 위한 시스템의 상위 레벨 블록도이다. 검색 라우터 규칙 엔진(206)은 질의 분석기(602), 의도 추출기(604), 의도 매칭기(606) 및 기계 학습 모듈(608)을 포함할 수 있다. 일 실시예에서, 질의 분석기(602)는 수신된 텍스트 및/또는 사진을 중첩 윈도우로 분류할 수 있다. 예를 들어, 검색하는 사용자는 검색어로서 "happy birthday"라는 검색어를 입력할 수 있다. 질의 분석기(602)는 "ha", "happy", "birth", "birthday", "happy birth" 및 "happy birthday"와 같이 중첩되는 단어 및 부분 단어로 질의를 분류할 수 있다. 질의 분석기(602)는 일 실시예에서, 관련 미디어 컨텐츠 아이템들의 컨텐츠 연관에 대한 단어 및 부분 단어에 기초하여 미디어 컨텐츠 저장소(106)에서 검색하기 위해 검색 인터페이스 모듈(120)에 단어 및 부분 단어를 제공할 수 있다.7B is a high-level block diagram of a system for performing a search to implement animation input in a dynamic interface in accordance with some examples. The search
다른 실시예에서, 질의 분석기(602)는 의도 추출기(604)에 단어 및 부분 단어를 제공할 수 있다. 예를 들어, 의도 추출기(604)는 생일을 축하하기 위한 의도를 포함하기 위해 질의 "happy birthday"로부터 의도를 이전에 매핑되거나 추출했을 수 있다. 따라서, "happy birthday"라는 용어는 케이크, 양초, 텍스트 문자열 "happy birthday", 파티, 양초를 끄는 사람 등과 같은 생일 요소를 갖는 컨텐츠 아이템들에만 특별히 매핑될 수 있다. 의도 추출기(604)는 자연어 처리(NLP) 파서(218)에 단어 및 부분 단어를 더 제공하여 검색 용어들로부터 의미 및/또는 의도를 도출할 수 있다. NLP 파서(218)는 일 실시예에서 검색 용어가 인식되지 않는 경우 특히 유용할 수 있다. 예를 들어, 검색 용어가 "happy dia de los muertos"이고, "죽은 자의 날(day of the dead)"을 뜻하는 스페인어 "dia de los muertos"라는 용어가 사전 또는 학습 용어의 말뭉치에 포함되지 않은 경우, 의도 추출기(604)는 그 검색 질의에 포함된 "happy(행복한)"라는 단어에 기초하여 행복한 것을 축하하고자 하는 검색 사용자의 의도를 추출할 수 있다. 반면에 컨텐츠 아이템들의 메타 데이터 속성으로서 포함된 문자열이나 사전에 "muertos(죽은)"가 포함되어 있는 경우, NLP 파서(218)는 "happy" 및 "muertos" 컨텐츠 연관 모두와 관련된 컨텐츠 항목들을 제시하는데 의존할 수 있다.In other embodiments,
의도 매칭기(606)는 일 실시예에서 검색 라우터 규칙 엔진(206)에서 사용되어 검색 사용자의 의도를 컨텐츠 연관 저장소(118)에 있는 하나 이상의 컨텐츠 연관에 매칭시킬 수 있다. 이전 예로 돌아가서, 검색 질의 "happy dia de los muertos"에 포함된 "happy"라는 용어는 추가 질의를 위해 그 검색 질의가 의도 매칭기(606)에 의해 "#happy" 컨텐츠 연관에 매칭되게 할 수 있다. "muertos"라는 용어는 일 실시예에서 "dead" 컨텐츠 연관 및 "할로윈" 컨텐츠 연관에 매칭될 수 있다. "dia de los muertos"는 할로윈과 직접적인 관련이 없지만, 실제로는 11월 1일인 멕시코 휴일이므로 일부 컨텐츠가 제시되지 않을 수 있다. 의도 매칭기(606)는 일 실시예에서 검색 문구와 컨텐츠 연관 사이의 매칭을 조정할 수 있다. 그 매칭들은 일 실시예에서 컨텐츠 연관 저장소(118)에 저장될 수 있다.The
다른 실시예에서, 의도 매칭기(606)는 기계 학습 모듈(608)과 함께, 이러한 아이템들이 "happy dia de los muertos" 검색 쿼리에 대한 응답으로 검색 결과에 제시되는 경우 "Halloween" 속성 및 "skull(해골)" 속성을 모두 갖는 컨텐츠 아이템들을 선택하는 것과 같은 사용자 피드백을 분석할 수 있다. 그 결과, 의도 매칭기(606)는 검색 문구 "happy dia de los muertos"와 "Halloween" 및 "skull" 컨텐츠 연관을 모두 갖는 컨텐츠 아이템들 사이에 새로운 매칭을 생성할 수 있다. 일 실시예에서, 의도 매칭기(606)는 확률적 방법 및/또는 각각의 매치에 대한 기계 학습에 기초하여 의도 매칭의 우도 스코어(likelihood score)를 결정할 수 있다. 이 스코어는 각각의 의도 매칭에 대해 컨텐츠 연관 저장소(118)에 저장될 수 있다. 이들 스코어는 NLP 파서(218) 및 기계 학습 모듈(608)에 의해 제공되는 통계적 추론 알고리즘에 추가로 기초할 수 있다.In another embodiment, the
기계 학습 모듈(608)은 지도 및 비지도 학습 방법, 베이지안 지식 기반, 베이지안 네트워크, 최단 이웃점(nearest neighbor), 랜덤 워크(random walk) 및 기타 방법과 같은 다양한 기계 학습 방법을 사용하여, 수신된 트레이닝 데이터 및 수신된 사용자 피드백에 기초하여(보는 사용자가 검색 결과 세트에 제시된 컨텐츠 아이템을 선택/공유했는지 여부에 기초하여) 다양한 결과를 결정할 수 있다. 예를 들어, 종종 "#happy"컨텐츠 연관과 같은 특정 속성을 갖는 컨텐츠 아이템들과 함께 임의 컨텐츠 아이템이 제시된다. 다른 경우에는 "dog"와 같은 상이한 컨텐츠 연관에 대한 검색 결과중에서 동일한 컨텐츠 아이템이 무작위로 제시될 수 있다. 무작위로 제시된 컨텐츠 아이템은 "#happy" 컨텐츠 연관 또는 "dog" 컨텐츠 연관과 연관되지 않을 수 있지만, 검색 및/또는 보는 사용자들은 무작위로 제시된 컨텐츠 아이템을 자주 선택하고 공유할 수 있다. 그 결과, 기계 학습 모듈(608)은 무작위로 제시된 컨텐츠 아이템이 전체 시간의 80%, "#happy"와 연관된 컨텐츠와 함께 제시될 때 그 시간의 70%, "dog"와 관련된 컨텐츠와 함께 제시될 때 그 시간의 60%가 선택되었다고 결정할 수 있다. 기계 학습 모듈(608)은 검색 질의가 "#happy" 및 "dog"라는 용어를 모두 포함하는 경우뿐만 아니라 검색 질의에 그 용어들 중 하나가 포함된 경우 프로세스를 추가로 자동화하고 컨텐츠 아이템을 자동으로 제시하기 위한 휴리스틱 규칙을 생성하는데 사용될 수 있다. 일 실시예에서, 기계 학습 모듈(608)은 50%와 같은 임계 시간 백분율에 걸쳐 공통 속성을 갖는 검색 결과 중에서 선택되는 컨텐츠 아이템에 기초하여 컨텐츠 연관을 컨텐츠 아이템에 연관시키거나 관련시킬 수 있다. 이와 같은 상관은 또한 적어도 하나의 실시예에 따라 사용자 인터페이스를 통한 관리자 승인을 요구할 수 있다.The
기계 학습 모듈(608)은 예를 들어 도 2의 시간 분석 모델(86)을 참조하여 위에서 설명된 바와 같이 하나 이상의 시간 분석 모델(245)을 포함할 수 있다. 기계 학습 모듈(608)은 미디어 컨텐츠 아이템(104)을 분석하고 미디어 컨텐츠 아이템(104)으로부터 추출된 시간 분석 데이터에 기초하여 미디어 컨텐츠 아이템들 (104)에 대한 컨텐츠 연관을 생성하도록 구성될 수 있다.The
검색 라우터 규칙 엔진(206)은 처리 시간을 최적화하고 미디어 컨텐츠 관리 시스템(100)에 직접 매칭이 존재하지 않는 경우에도 검색 결과를 포함하기 위해 검색 질의를 처리하기 위한 규칙을 더 포함할 수 있다. 예를 들어, 검색 라우터 규칙 엔진(206)은 감정 분석 모듈(220), 이미지 분석기(222) 및/또는 움직임 분석기(224)와 함께 동작하여, 연관된 속성을 갖지 않는 미디어 컨텐츠 저장소(106)의 컨텐츠 아이템들을 분석할 수 있다. 감정 분석 모듈(220)은 의도가 긍정적, 부정적 또는 중립적 의미를 포함하는지 여부를 결정하기 위해 단어, 부분 단어 및 검색 질의를 처리하는데 사용될 수 있다. 이미지 분석기(222)는 검색하는 사용자의 의도를 추출하기 위해 검색 질의로서 수신된 수신 이미지들을 처리하기 위해 유사하게 사용될 수 있다. 예를 들어, 이미지가 질의로서 직접 전송된 모바일 디바이스에 의해 캡처된 사진인 경우, 그 사진은 이미지 분석기(222)에 의해 사진이 분석되어 사진에서 발생하는 얼굴 표정 및 활동과 같은 시각적 특성을 검출할 수 있다. 또한, 움직임 분석기(224)는 웃음, 울음, 넘어짐, 악수, 첫 번째 부딪힘, 가슴 치기, 눈 굴림, 머리카락 젖히기 등과 같은 액션, 행동 및 움직임 패턴을 검출하는데 사용될 수 있다. 규칙은 검색 라우터 규칙 엔진(206)에 포함되어, 식별된 행동, 액션, 활동 및/또는 얼굴 표정을 컨텐츠 연관 저장소(118)에 컨텐츠 연관으로서 저장되는 하나 이상의 표현적 진술에 연관시킬 수 있다. 이러한 규칙은 일 실시예에서, 휴리스틱 엔진(216)에 의해 생성된 휴리스틱 규칙일 수 있다.The search
도 9는 다양한 실시예에 따른 미디어 컨텐츠 관리 시스템(100)에서 검색을 수행하기 위해 조달된 컨텐츠를 분류하도록 구성된 디바이스에 배치된 예시적인 컴퓨팅 플랫폼을 도시한다. 일부 예에서, 컴퓨팅 플랫폼(1000)은 전술한 기술들을 수행하기 위해 컴퓨터 프로그램, 애플리케이션, 방법, 프로세스, 알고리즘 또는 다른 소프트웨어를 구현하는데 사용될 수 있다.9 illustrates an example computing platform deployed on a device configured to categorize procured content for performing a search in the media
일부 경우에, 컴퓨팅 플랫폼은 웨어러블 디바이스에 배치될 수 있으며, 모바일 컴퓨팅 디바이스(1090b), 또는 컴퓨팅 디바이스(1090a)와 같은 임의의 다른 디바이스를 구현할 수 있다. 컴퓨팅 플랫폼(1000)은 정보를 전달하기 위한 버스 (1004) 또는 다른 통신 메커니즘을 포함하는데, 이는 프로세서(1006), 시스템 메모리(1010)(예를 들어, RAM 등), 저장 디바이스(1008)(예를 들어, ROM 등), 통신 인터페이스(예를 들어, 이더넷 또는 무선 컨트롤러, 블루투스 컨트롤러 등)와 같은 서브 시스템들 및 디바이스들을 상호 연결하여, 예를 들어, 모바일 컴퓨팅 및/또는 프로세서가 있는 통신 디바이스를 포함하는 컴퓨팅 디바이스와 통신하기 위해 통신 링크(1014)상의 포트를 통한 통신을 용이하게 한다. 프로세서(1006)는 하나 이상의 중앙 처리 장치("CPU") 또는 하나 이상의 가상 프로세서뿐만 아니라 CPU와 가상 프로세서의 임의의 조합으로 구현될 수 있다. 컴퓨팅 플랫폼(1000)은 키보드, 마우스, 오디오 입력(예를 들어, 음성-텍스트 변환 디바이스), 사용자 인터페이스, 디스플레이, 모니터, 커서, 터치 감지 디스플레이, LCD 또는 LED 디스플레이 및 기타 I/O 관련 디바이스를 포함하여 입력 및 출력 디바이스(1002)를 통해 입력 및 출력을 나타내는 데이터를 교환하지만 이에 한정되지는 않는다. In some cases, the computing platform may be deployed in a wearable device and may implement
일부 예에 따르면, 컴퓨팅 플랫폼(1000)은 시스템 메모리(1010)에 저장된 하나 이상의 명령의 하나 이상의 시퀀스를 실행하는 특정 동작을 프로세서(1006)에 의해 수행하고, 컴퓨팅 플랫폼(1000)은 클라이언트-서버 배열, 피어-투-피어 배열, 또는 스마트 폰 등을 포함하는 임의의 모바일 컴퓨팅 디바이스로 구현될 수 있다. 이러한 명령 또는 데이터는 저장 디바이스(1008)와 같은 다른 컴퓨터 판독 가능 매체로부터 시스템 메모리(1010)로 판독될 수 있다. 일부 예에서, 구현을 위한 소프트웨어 명령들 대신에 또는 그와 조합하여 하드 와이어드 회로가 사용될 수 있다. 명령들은 소프트웨어 또는 펌웨어에 임베딩될 수 있다. "컴퓨터 판독 가능 매체"라는 용어는 실행을 위해 프로세서(1006)에 명령들을 제공하는데 참여하는 임의의 유형의(tangible) 매체를 지칭한다. 이러한 매체는 비-휘발성 매체 및 휘발성 매체를 포함여 많은 형태를 취할 수 있지만 이에 한정되지 않는다. 비-휘발성 매체는 예를 들어 광학 또는 자기 디스크 등을 포함한다. 휘발성 매체는 시스템 메모리(1010)와 같은 동적 메모리를 포함한다.According to some examples, the
컴퓨터 판독 가능 매체의 일반적인 형태는 예를 들어 플로피 디스크, 플렉서블 디스크, 하드 디스크, 자기 테이프, 임의의 다른 자기 매체, CD-ROM, 임의의 다른 광학 매체, 펀치 카드, 종이 테이프, 임의의 다른 물리적 매체, 홀 패턴이 있는 임의의 다른 물리적 매체, RAM, PROM, EPROM, FLASH-EPROM, 임의의 다른 메모리 칩 또는 카트리지, 또는 컴퓨터가 판독할 수 있는 임의의 다른 매체를 포함한다. 명령들은 또한 전송 매체를 사용하여 전송 또는 수신될 수 있다. "전송 매체"라는 용어는 기계에 의한 실행을 위한 명령들을 저장, 인코딩 또는 전달할 수 있는 임의의 유형 또는 무형의 매체를 포함할 수 있으며, 이러한 명령들의 통신을 용이하게 하는 디지털 또는 아날로그 통신 신호 또는 다른 무형 매체를 포함한다. 전송 매체는 컴퓨터 데이터 신호를 전송하기 위한 버스(1004)를 포함하는 와이어들을 포함하여 동축 케이블, 구리 와이어 및 광섬유를 포함한다.Common forms of computer readable media include, for example, floppy disks, flexible disks, hard disks, magnetic tape, any other magnetic medium, CD-ROM, any other optical medium, punch card, paper tape, any other physical medium. , any other physical medium having a hole pattern, RAM, PROM, EPROM, FLASH-EPROM, any other memory chip or cartridge, or any other computer readable medium. Instructions may also be transmitted or received using a transmission medium. The term "transmission medium" may include any tangible or intangible medium that can store, encode, or convey instructions for execution by a machine, and may contain a digital or analog communication signal or other Includes intangible media. Transmission media include coaxial cables, copper wires, and optical fibers, including wires including a
일부 예에서, 명령들의 시퀀스의 실행은 컴퓨팅 플랫폼(1000)에 의해 수행될 수 있다. 일부 예에 따르면, 컴퓨팅 플랫폼(1000)은 통신 링크(1014)(예를 들어, LAN, PSTN과 같은 유선 네트워크 또는 다양한 표준 및 프로토콜의 WiFi, Blue Tooth®, Zig-Bee를 포함하는 모든 무선 네트워크)에 의해 임의의 다른 프로세서에 연결되어, 서로 협력하여(또는 비동기적으로) 명령들의 시퀀스를 수행할 수 있다. 컴퓨팅 플랫폼(1000)은 통신 링크(1014) 및 통신 인터페이스(1012)를 통해 프로그램 코드(예를 들어, 애플리케이션 코드)를 포함하는 메시지, 데이터 및 명령을 송수신할 수 있다. 수신된 프로그램 코드는 수신될 때 프로세서(1006)에 의해 실행될 수 있고 및/또는 추후 실행을 위해 메모리(1010) 또는 다른 비-휘발성 저장소에 저장될 수 있다.In some examples, execution of the sequence of instructions may be performed by the
도시된 예에서, 시스템 메모리(1010)는 본 명세서에 설명된 기능을 구현하기 위한 실행 가능한 명령들을 포함하는 다양한 모듈을 포함할 수 있다. 시스템 메모리(1010)는 애플리케이션(1032) 및/또는 논리 모듈(1050)뿐만 아니라 운영 체제("O/S")(1030)를 포함할 수 있다. 도시된 예에서, 시스템 메모리(1010)는 컨텐츠 연관("ass'n"), 선택 모듈(408), 및 컨텐츠 연관("CA") 알고리즘 모듈(1040)을 포함하는 컨텐츠 연관 모듈(108)을 포함한다. 컨텐츠 연관 모듈(108)은 예를 들어 도 2의 시간 분석 모델(86)을 참조하여 전술한 바와 같이 하나 이상의 시간 분석 모델(245)을 포함할 수 있다. 컨텐츠 연관 모듈(108)은 미디어 컨텐츠 아이템(104)을 분석하고 미디어 컨텐츠 아이템(104)으로부터 추출된 시간 분석 데이터에 기초하여 미디어 컨텐츠 아이템(104)에 대한 컨텐츠 연관을 생성하도록 구성될 수 있다.In the illustrated example,
시스템 메모리(1010)는 또한 이미지 분석기(222), 움직임 분석기(224), 휴리스틱 엔진(216), 검색 인터페이스 모듈(120), 동적 키보드 인터페이스 모듈(208), 동적 키보드 제시 모듈(212), 감정 분석 모듈(220), 자연어 처리(NLP) 파서(218), 검색 라우터 규칙 엔진(206)(질의 분석기(602), 의도 추출기(604), 의도 매칭기(606) 및 기계 학습(ML) 모듈(608)을 포함함), 컨텐츠 연관("ass'n") 관리("mgmt) 모듈(메타 데이터 분석 모듈(240), 사용자 인터페이스 모듈(242), 컨텐츠 연관 선택 모듈(244) 및 연관("ass'n") 관련 모듈(246)을 포함함)을 포함한다. 시스템 메모리(1010)는 복합 아이템 모듈(260) 및 컴포저 인터페이스 모듈(262)을 더 포함할 수 있다. 메모리(1010)에 포함된 하나 이상의 모듈은 본 명세서에 설명된 하나 이상의 기능을 구현하기 위해 출력을 제공하거나 소비하도록 구성될 수 있다.
적어도 일부 예에서, 전술한 특징들 중 임의의 것의 구조 및/또는 기능은 소프트웨어, 하드웨어, 펌웨어, 회로, 또는 이들의 조합으로 구현될 수 있다. 위의 구조 및 구성 요소뿐만 아니라 그 기능은 하나 이상의 다른 구조 또는 요소와 통합될 수 있다. 대안적으로, 요소 및 그 기능은 구성 하위 요소(있는 경우)로 세분될 수 있다. 소프트웨어로서, 전술한 기술들은 다양한 유형의 프로그래밍 또는 포맷 언어, 프레임 워크, 구문(syntex), 애플리케이션, 프로토콜, 객체 또는 기술을 사용하여 구현될 수 있다. 하드웨어 및/또는 펌웨어로서, 전술한 기술들은 필드 프로그래밍 가능 게이트 어레이("FPGA"), 주문형 집적 회로("ASIC") 또는 다른 유형의 집적 회로를 설계하도록 구성된 임의의 레지스터 전송 언어("RTL")와 같은 하드웨어 디스크립션 언어를 포함하는 다양한 유형의 프로그래밍 또는 집적 회로 설계 언어를 사용하여 구현될 수 있다. 일부 실시예에 따르면, "모듈"이라는 용어는 예를 들어 알고리즘 또는 그의 일부, 및/또는 하드웨어 회로나 소프트웨어 또는 이들의 조합으로 구현되는 로직을 지칭할 수 있다. 이들은 변경될 수 있으며 제공된 예 또는 설명으로 제한되지 않는다.In at least some examples, the structure and/or functionality of any of the features described above may be implemented in software, hardware, firmware, circuitry, or combinations thereof. The above structures and components, as well as their functionality, may be integrated with one or more other structures or elements. Alternatively, elements and their functions may be subdivided into constituent sub-elements (if any). As software, the techniques described above may be implemented using various types of programming or formatting languages, frameworks, syntax, applications, protocols, objects, or techniques. As hardware and/or firmware, the techniques described above can be implemented in any register transfer language ("RTL") configured to design a field programmable gate array ("FPGA"), application specific integrated circuit ("ASIC"), or other type of integrated circuit. It may be implemented using various types of programming or integrated circuit design languages, including hardware description languages such as According to some embodiments, the term “module” may refer to, for example, an algorithm or a portion thereof, and/or logic implemented as hardware circuitry or software or a combination thereof. These are subject to change and are not limited to the examples or descriptions provided.
일부 실시예에서, 미디어 컨텐츠 관리 시스템 또는 그의 컴포넌트 중 하나 이상, 또는 본 명세서에 설명된 임의의 프로세스 또는 디바이스는 모바일 전화 또는 모바일 디바이스와 같은 모바일 디바이스와 통신(예를 들어, 유선 또는 무선으로)할 수 있거나 그 안에 배치될 수 있다.In some embodiments, the media content management system or one or more of its components, or any process or device described herein, is capable of communicating (eg, wired or wirelessly) with a mobile device, such as a mobile phone or mobile device. may or may be placed therein.
일부 경우에, 모바일 디바이스, 또는 동작 경보 컨트롤러 또는 그의 컴포넌트들 중 하나 이상(또는 본 명세서에 설명된 임의의 프로세스 또는 디바이스)과 통신하는 임의의 네트워크 컴퓨팅 디바이스(미도시)는 본 명세서에 설명된 임의의 특징의 구조 및/또는 기능의 적어도 일부를 제공할 수 있다. 전술한 도면에 도시된 바와 같이, 전술한 특징들 중 임의의 구조 및/또는 기능은 소프트웨어, 하드웨어, 펌웨어, 회로 또는 이들의 임의의 조합으로 구현될 수 있다. 위의 구조 및 구성 요소뿐만 아니라 그들의 기능은 하나 이상의 다른 구조 또는 요소와 집계되거나 결합될 수 있다. 대안적으로, 요소 및 그 기능은 구성 하위 요소(있는 경우)로 세분될 수 있다. 소프트웨어로서, 전술한 기술들 중 적어도 일부는 다양한 유형의 프로그래밍 또는 포맷 언어, 프레임 워크, 구문, 애플리케이션, 프로토콜, 객체 또는 기술을 사용하여 구현될 수 있다. 예를 들어, 임의의 도면에 도시된 요소들 중 적어도 하나는 하나 이상의 알고리즘을 나타낼 수 있다. 또는, 요소들 중 적어도 하나는 구성 구조 및/또는 기능을 제공하도록 구성된 하드웨어의 일부를 포함하는 로직의 일부를 나타낼 수 있다.In some cases, any network computing device (not shown) in communication with a mobile device, or an operational alert controller or one or more of its components (or any process or device described herein), is any may provide at least some of the structure and/or function of the features of As shown in the foregoing drawings, any structure and/or function of the features described above may be implemented in software, hardware, firmware, circuitry, or any combination thereof. The above structures and components, as well as their functions, may be aggregated or combined with one or more other structures or elements. Alternatively, elements and their functions may be subdivided into constituent sub-elements (if any). As software, at least some of the techniques described above may be implemented using various types of programming or formatting languages, frameworks, syntax, applications, protocols, objects, or techniques. For example, at least one of the elements shown in any figure may represent one or more algorithms. Or, at least one of the elements may represent a piece of logic that includes a piece of hardware configured to provide a configuration structure and/or functionality.
예를 들어, 동적 키보드 제시 모듈(212) 또는 그의 하나 이상의 컴포넌트 중 임의의 것, 또는 본 명세서에 설명된 임의의 프로세스 또는 디바이스는 메모리에서 하나 이상의 알고리즘을 실행하도록 구성된 하나 이상의 프로세서를 포함하는 하나 이상의 컴퓨팅 디바이스(즉, 착용 또는 휴대 여부에 관계없이 웨어러블 디바이스, 오디오 디바이스(예컨대, 헤드폰 또는 헤드셋) 또는 모바일 전화기와 같은 임의의 모바일 컴퓨팅 디바이스)에서 구현될 수 있다. 따라서, 전술한 도면의 요소들 중 적어도 일부는 하나 이상의 알고리즘을 나타낼 수 있다. 또는, 요소들 중 적어도 하나는 구성 구조 및/또는 기능을 제공하도록 구성된 하드웨어의 일부를 포함하는 로직의 일부를 나타낼 수 있다. 이들은 변경될 수 있으며 제공된 예 또는 설명으로 제한되지 않는다.For example, any of the dynamic
추가 개시 내용Additional disclosures
본 명세서에서 논의된 기술은 서버, 데이터베이스, 소프트웨어 애플리케이션 및 다른 컴퓨터 기반 시스템뿐만 아니라 이러한 시스템으로(부터) 취해지는 액션 및 송수신되는 정보를 참조한다. 컴퓨터 기반 시스템의 고유한 유연성을 통해 컴포넌트 간의 작업 및 기능에 대한 매우 다양한 구성, 조합 및 분할을 가능케 한다. 예를 들어, 본 명세서에 설명된 프로세스는 단일 디바이스 또는 컴포넌트 또는 조합하여 작동하는 다수의 디바이스 또는 컴포넌트를 사용하여 구현될 수 있다. 데이터베이스와 애플리케이션은 단일 시스템상에서 구현되거나 다수의 시스템에 걸쳐 분산될 수 있다. 분산된 컴포넌트들은 순차적으로 또는 병렬로 작동할 수 있다.The techniques discussed herein refer to servers, databases, software applications, and other computer-based systems, as well as actions taken to and from such systems and information sent and received. The inherent flexibility of computer-based systems allows for a wide variety of configurations, combinations, and divisions of tasks and functions between components. For example, the processes described herein may be implemented using a single device or component or multiple devices or components operating in combination. Databases and applications can be implemented on a single system or distributed across multiple systems. Distributed components can operate sequentially or in parallel.
하드웨어 및/또는 펌웨어로서, 전술한 구조 및 기술은 필드 프로그래밍 가능 게이트 어레이("FPGA"), 주문형 집적 회로("ASIC"), 다중 칩 모듈 또는 임의의 다른 유형의 집적 회로를 설계하도록 구성된 임의의 레지스터 전송 언어("RTL")와 같은 하드웨어 디스크립션 언어를 포함하는 다양한 유형의 프로그래밍 또는 집적 회로 설계 언어를 사용하여 구현될 수 있다.As hardware and/or firmware, the structures and techniques described above may be applied to any structure configured to design a field programmable gate array (“FPGA”), an application specific integrated circuit (“ASIC”), a multi-chip module, or any other type of integrated circuit. It may be implemented using various types of programming or integrated circuit design languages, including hardware description languages such as Register Transfer Language (“RTL”).
예를 들어, 하나 이상의 컴포넌트를 포함하는 미디어 컨텐츠 관리 시스템, 또는 본 명세서에 설명된 임의의 프로세스 또는 디바이스는 하나 이상의 회로를 포함하는 하나 이상의 컴퓨팅 디바이스에서 구현될 수 있다. 따라서, 전술한 도면의 요소들 중 적어도 하나는 하드웨어의 하나 이상의 컴포넌트를 나타낼 수 있다. 또는, 요소들 중 적어도 하나는 구성 구조 및/또는 기능을 제공하도록 구성된 회로의 일부를 포함하는 로직의 일부를 나타낼 수 있다.For example, a media content management system that includes one or more components, or any process or device described herein, may be implemented in one or more computing devices that include one or more circuits. Accordingly, at least one of the elements of the foregoing figures may represent one or more components of hardware. Alternatively, at least one of the elements may represent a portion of logic that includes a portion of circuitry configured to provide configuration structure and/or functionality.
일부 실시예에 따르면, "회로"라는 용어는 예를 들어 하나 이상의 기능을 수행하기 위해 전류가 흐르는 여러 컴포넌트를 포함하는 임의의 시스템을 지칭할 수 있으며, 컴포넌트들은 개별 및 복합 컴포넌트를 포함한다. 개별 컴포넌트의 예로는 트랜지스터, 저항기, 커패시터, 인덕터, 다이오드 등을 포함하고, 복합 컴포넌트의 예로는 필드 프로그래밍 가능 게이트 어레이("FPGA"), 주문형 집적 회로("ASIC")를 포함하여 메모리, 프로세서, 아날로그 회로, 디지털 회로 등이 포함된다. 따라서, 회로는 전자 컴포넌트 및 논리 컴포넌트의 시스템(예를 들어, 알고리즘의 실행 가능한 명령 그룹이 예를 들어 회로의 컴포넌트가 되도록 명령들을 실행하도록 구성된 로직)을 포함할 수 있다. 일부 실시예에 따르면, "모듈"이라는 용어는 예를 들어 알고리즘 또는 그의 일부, 및/또는 하드웨어 회로나 소프트웨어, 또는 이들의 조합으로 구현된 로직을 지칭할 수 있다(즉, 모듈은 회로로 구현될 수 있음). 일부 실시예에서, 알고리즘 및/또는 알고리즘이 저장되는 메모리는 회로의 "컴포넌트"이다. 따라서, "회로"라는 용어는 또한 예를 들어 알고리즘을 포함하는 컴포넌트 시스템을 지칭할 수 있다. 이들은 변경될 수 있으며 제공된 예 또는 설명으로 제한되지 않는다.According to some embodiments, the term “circuit” may refer to any system including, for example, several components through which current flows to perform one or more functions, the components including discrete and composite components. Examples of discrete components include transistors, resistors, capacitors, inductors, diodes, etc., examples of composite components include memory, processors, including field programmable gate arrays (“FPGA”), application specific integrated circuits (“ASICs”), Analog circuits, digital circuits, and the like are included. Accordingly, a circuit may include a system of electronic and logical components (eg, logic configured to execute instructions such that an executable group of instructions of an algorithm is, for example, a component of the circuit). According to some embodiments, the term “module” may refer to, for example, an algorithm or a portion thereof, and/or logic implemented in hardware circuitry or software, or a combination thereof (ie, a module may be implemented as a circuit can). In some embodiments, the algorithm and/or the memory in which the algorithm is stored is a “component” of the circuit. Thus, the term “circuit” may also refer to a component system comprising, for example, an algorithm. These are subject to change and are not limited to the examples or descriptions provided.
이해의 명료함을 위해 전술한 실시예가 일부 상세하게 설명되었지만, 전술한 본 발명의 기술은 제공된 세부 사항에 제한되지 않는다. 전술한 발명 기술을 구현하는 많은 대안적인 방법이 있다. 개시된 예는 예시적이며 제한적이지 않다.Although the foregoing embodiments have been described in some detail for purposes of clarity of understanding, the foregoing description of the present invention is not limited to the details provided. There are many alternative ways of implementing the inventive techniques described above. The disclosed examples are illustrative and not restrictive.
본 발명의 실시예에 대한 전술한 설명은 예시의 목적으로 제시되었다. 이는 본 발명을 개시된 정확한 형태로 제한하거나 포괄적인 것으로 의도되지는 않는다. 관련 기술 분야의 숙련자는 개시 내용에 비추어 많은 수정 및 변경이 가능함을 이해할 수 있다.The foregoing description of embodiments of the present invention has been presented for purposes of illustration. It is not intended to be exhaustive or to limit the invention to the precise form disclosed. Those skilled in the relevant art will appreciate that many modifications and variations are possible in light of the disclosure.
이 설명의 일부 부분은 정보에 대한 연산의 상징적 표현 및 알고리즘의 관점에서 본 발명의 실시예를 설명한다. 이러한 알고리즘 설명 및 표현은 일반적으로 데이터 처리 기술의 숙련자에 의해 작업의 본질을 당업자에게 효과적으로 전달하기 위해 사용된다. 이러한 동작들은 기능적으로, 계산적으로 또는 논리적으로 설명되지만 컴퓨터 프로그램 또는 등가 전기 회로, 마이크로 코드 등에 의해 구현되는 것으로 이해된다. 또한, 일반성을 잃지 않고 이러한 작업 배열을 모듈로 참조하는 것이 편리한 것으로 입증되었다. 설명된 동작 및 관련 모듈은 소프트웨어, 펌웨어, 하드웨어 또는 이들의 임의의 조합으로 구현될 수 있다.Some portions of this description describe embodiments of the invention in terms of algorithms and symbolic representations of operations on information. These algorithmic descriptions and representations are generally used by those skilled in the data processing arts to effectively convey the substance of their work to those skilled in the art. These operations are described functionally, computationally, or logically but are understood to be implemented by a computer program or equivalent electrical circuit, microcode, or the like. Also, it has proven convenient to refer to these task arrays as modules without losing generality. The described operations and related modules may be implemented in software, firmware, hardware, or any combination thereof.
본 명세서에 설명된 임의의 단계들, 동작들 또는 프로세스들은 하나 이상의 하드웨어 또는 소프트웨어 모듈로 단독으로 또는 다른 디바이스와 결합하여 수행되거나 구현될 수 있다. 일 실시예에서, 소프트웨어 모듈은 컴퓨터 프로그램 코드를 포함하는 컴퓨터 판독 가능 매체를 포함하는 컴퓨터 프로그램 제품으로 구현되는데, 이는 설명된 단계들, 동작들, 또는 프로세스들의 일부 또는 전부를 수행하도록 컴퓨터 프로세서에 의해 실행될 수 있다.Any of the steps, operations, or processes described herein may be performed or implemented in one or more hardware or software modules, alone or in combination with other devices. In one embodiment, a software module is implemented as a computer program product comprising a computer readable medium comprising computer program code, which is executed by a computer processor to perform some or all of the described steps, operations, or processes. can be executed
본 발명의 실시예는 또한 본 발명에서 동작들을 수행하기 위한 장치와 관련될 수 있다. 이 장치는 요구된 목적을 위해 특별히 구성될 수 있고 및/또는 컴퓨터에 저장된 컴퓨터 프로그램에 의해 선택적으로 활성화되거나 재구성된 범용 컴퓨팅 디바이스를 포함할 수 있다. 이러한 컴퓨터 프로그램은 비-일시적인 유형의 컴퓨터 판독 가능 저장 매체, 또는 컴퓨터 시스템 버스에 결합될 수 있는 전자 명령들을 저장하기에 적합한 임의의 타입의 매체에 저장될 수 있다. 더욱이, 명세서에서 언급된 임의의 컴퓨팅 시스템은 단일 프로세서를 포함할 수 있거나 증가된 컴퓨팅 성능을 위해 다중 프로세서 설계를 사용하는 아키텍처일 수 있다.Embodiments of the present invention may also relate to an apparatus for performing the operations in the present invention. The apparatus may comprise a general purpose computing device which may be specially constructed for the required purpose and/or selectively activated or reconfigured by a computer program stored in a computer. Such a computer program may be stored in a non-transitory tangible computer readable storage medium, or any type of medium suitable for storing electronic instructions capable of being coupled to a computer system bus. Moreover, any computing system referred to in the specification may include a single processor or may be an architecture that uses a multi-processor design for increased computing performance.
본 발명의 실시예는 또한 본 명세서에 설명된 컴퓨팅 프로세스에 의해 생성되는 제품과 관련될 수 있다. 이러한 제품은 컴퓨팅 프로세스에서 발생하는 정보를 포함할 수 있으며, 여기서 정보는 비-일시적인 유형의 컴퓨터 판독 가능 저장 매체에 저장되고, 본 명세서에 설명된 컴퓨터 프로그램 제품 또는 다른 데이터 조합의 임의의 실시예를 포함할 수 있다.Embodiments of the invention may also relate to products produced by the computing processes described herein. Such products may include information arising from computing processes, wherein the information is stored in a non-transitory tangible computer-readable storage medium and configured to implement any embodiment of the computer program product or other data combination described herein. may include
마지막으로, 본 명세서에서 사용된 언어는 주로 가독성 및 교육 목적을 위해 선택되었으며, 본 발명의 주제를 설명하거나 제한하기 위해 선택되지 않았을 수 있다. 따라서, 본 발명의 범위는 이 상세한 설명에 의해 제한되는 것이 아니라, 이에 기초한 애플리케이션에 대해 발행하는 임의의 청구 범위에 의해 제한되는 것으로 의도된다. 따라서, 본 발명의 실시예의 개시는 다음의 청구 범위에 기재된 본 발명의 범위를 제한하는 것이 아니라 예시적인 것으로 의도된다.Finally, the language used herein has been chosen primarily for readability and educational purposes, and may not be chosen to describe or limit the subject matter of the present invention. Accordingly, it is intended that the scope of the present invention not be limited by this detailed description, but rather by any claims it issues for applications based thereon. Accordingly, the disclosure of the embodiments of the present invention is intended to be illustrative and not limiting of the scope of the present invention as set forth in the following claims.
본 발명은 다양한 특정 실시예에 대하여 상세히 설명하였으나, 각각의 예는 설명의 방식으로 제공되며 개시 내용을 제한하지 않는다. 당업자는 전술한 내용을 이해하면 이러한 실시예에 대한 변경, 변형 및 등가물을 쉽게 생성할 수 있다. 따라서, 본 개시 내용은 당업자에게 쉽게 명백할 바와 같이 본 주제에 대한 이러한 수정, 변경 및/또는 추가의 포함을 배제하지 않는다. 예를 들어, 일 실시예의 일부로서 예시되거나 설명된 특징들은 또 다른 실시예와 함께 사용되어 또 다른 실시예를 생성할 수 있다. 따라서, 본 개시 내용은 이러한 변경, 변형 및 등가물을 포함하는 것으로 의도된다.While the invention has been described in detail with respect to various specific embodiments, each example is provided by way of illustration and not limitation of the disclosure. Those skilled in the art can readily create changes, modifications, and equivalents to these embodiments upon understanding the foregoing. Accordingly, this disclosure does not exclude the inclusion of such modifications, changes and/or additions to the subject matter as will be readily apparent to those skilled in the art. For example, features illustrated or described as part of one embodiment may be used in conjunction with another embodiment to create another embodiment. Accordingly, this disclosure is intended to cover such alterations, modifications and equivalents.
Claims (22)
하나 이상의 컴퓨팅 디바이스를 포함하는 컴퓨팅 시스템에 의해, 순차 디스플레이를 위한 복수의 이미지 프레임을 포함하는 미디어 컨텐츠 아이템을 기술하는 데이터를 수신하는 단계;
컴퓨팅 시스템에 의해, 미디어 컨텐츠 아이템을 기술하는 데이터를 수신하고, 미디어 컨텐츠 아이템을 기술하는 데이터를 수신하는 것에 응답하여 미디어 컨텐츠 아이템의 복수의 이미지 프레임을 순차적으로 시청(view)하는 것과 관련된 시간 정보를 기술하는 시간 분석 데이터 출력하도록 구성된 기계 학습 시간 분석 모델에 상기 미디어 컨텐츠 아이템을 기술하는 데이터를 입력하는 단계; 및
컴퓨팅 시스템에 의해 기계 학습 시간 분석 모델의 출력으로서, 시간 분석 데이터를 수신하는 단계를 포함하는 것을 특징으로 하는 컴퓨터 구현 방법.A computer implemented method comprising:
receiving, by a computing system including one or more computing devices, data describing a media content item comprising a plurality of image frames for sequential display;
Receive, by the computing system, data describing the media content item, and in response to receiving the data describing the media content item, temporal information associated with sequentially viewing a plurality of image frames of the media content item. inputting the data describing the media content item into a machine learning temporal analysis model configured to output the describing temporal analysis data; and
and receiving, by the computing system, temporal analysis data as an output of the machine learning temporal analysis model.
상기 시간 정보는 복수의 이미지 프레임의 개별 이미지 프레임에 의해 기술되지 않는 것을 특징으로 하는 컴퓨터 구현 방법.According to claim 1,
wherein the temporal information is not described by individual image frames of a plurality of image frames.
상기 복수의 이미지 프레임은 제1 텍스트 문자열을 포함하는 제1 이미지 프레임 및 제1 이미지 프레임 이후에 순차적으로 디스플레이하기 위한 제2 텍스트 문자열을 포함하는 제2 이미지 프레임을 포함하고; 그리고
상기 시간 분석 데이터에 의해 기술된 시간 정보는 제1 텍스트 문자열 또는 제2 텍스트 문자열을 개별적으로 판독함으로써 기술되지 않은 제2 텍스트 문자열 이전에 순차적으로 판독되는 제1 텍스트 문자열에 의해 기술된 시맨틱 의미를 기술하는 것을 특징으로 하는 컴퓨터 구현 방법.3. The method according to claim 1 or 2,
the plurality of image frames include a first image frame including a first text string and a second image frame including a second text string for sequentially displaying after the first image frame; and
The time information described by the temporal analysis data describes the semantic meaning described by the first text string or the second text string that is sequentially read before the not described second text string by individually reading the first text string or the second text string. A computer implemented method, characterized in that
상기 제1 텍스트 문자열 및 제2 텍스트 문자열 중 하나 이상은 추가 단어가 없는 단일 단어 또는 추가 문자가 없는 단일 문자를 포함하는 것을 특징으로 하는 컴퓨터 구현 방법.4. The method of claim 3,
wherein at least one of the first text string and the second text string comprises a single word without an additional word or a single character without an additional character.
상기 제1 이미지 프레임의 제1 텍스트 문자열은,
색상, 굵기, 위치 또는 폰트 중 적어도 하나에 의해 제2 이미지 프레임의 제2 텍스트 문자열의 외관과 다른 외관을 갖는 것을 특징으로 하는 컴퓨터 구현 방법.5. The method of claim 3 or 4,
The first text string of the first image frame,
and having an appearance different from the appearance of the second text string of the second image frame by at least one of color, weight, position, or font.
상기 시간 분석 데이터는,
제2 텍스트 문자열의 외관과 다른 외관을 갖는 제1 텍스트 문자열과 관련된 시맨틱 의미를 기술하는 것을 특징으로 하는 컴퓨터 구현 방법.6. The method of claim 5,
The time analysis data is,
A computer-implemented method for describing a semantic meaning associated with a first text string having an appearance different from that of a second text string.
상기 복수의 이미지 프레임은 제1 장면을 기술하는 제1 이미지 프레임 및 제1 이미지 프레임 이후에 순차적으로 디스플레이하기 위한 제2 장면을 기술하는 제2 이미지 프레임을 포함하고; 그리고
상기 시간 분석 데이터에 의해 기술된 시간 정보는 제1 장면 또는 제2 장면을 개별적으로 시청함으로써 기술되지 않은 제2 장면 이전에 순차적으로 시청되는 제1 장면에 의해 기술된 시맨틱 의미를 기술하는 것을 특징으로 하는 컴퓨터 구현 방법.According to any one of the preceding claims,
the plurality of image frames include a first image frame describing a first scene and a second image frame describing a second scene for sequentially displaying after the first image frame; and
The temporal information described by the temporal analysis data describes the semantic meaning described by the first scene sequentially viewed before the second scene not described by individually viewing the first scene or the second scene. computer-implemented method.
상기 시간 분석 데이터는 미디어 컨텐츠 아이템의 감정 컨텐츠를 기술하는 것을 특징으로 하는 컴퓨터 구현 방법.According to any one of the preceding claims,
wherein the temporal analysis data describes the emotional content of the media content item.
시간 분석 데이터에 의해 기술된 시간 정보에 기초하여 미디어 컨텐츠 아이템에 컨텐츠 라벨을 할당하는 단계를 더 포함하는 것을 특징으로 하는 컴퓨터 구현 방법.According to any one of the preceding claims,
and assigning a content label to the media content item based on temporal information described by the temporal analysis data.
상기 미디어 컨텐츠 아이템은 광고를 포함하는 것을 특징으로 하는 컴퓨터 구현 방법.According to any one of the preceding claims,
and the media content item includes an advertisement.
상기 시간 분석 데이터에 의해 기술되는 시간 정보는 광고의 감정 컨텐츠를 기술하는 것을 특징으로 하는 컴퓨터 구현 방법.11. The method of claim 10,
The time information described by the time analysis data describes the emotional content of the advertisement.
컴퓨팅 시스템의 사용자 컴퓨팅 디바이스에 의해, 미디어 컨텐츠 아이템에 대한 검색 질의를 수신하는 단계;
컴퓨팅 시스템에 의해, 시간 정보를 기술하는 시간 분석 데이터에 기초하여 미디어 컨텐츠 아이템을 선택하는 단계; 및
검색 질의에 응답하여 컴퓨팅 시스템에 의해, 미디어 컨텐츠 아이템을 제공하는 단계를 더 포함하는 것을 특징으로 하는 컴퓨터 구현 방법.According to any one of the preceding claims,
receiving, by a user computing device of the computing system, a search query for a media content item;
selecting, by the computing system, a media content item based on temporal analysis data describing temporal information; and
and providing, by the computing system, an item of media content in response to a search query.
컴퓨팅 시스템의 사용자 컴퓨팅 디바이스에 의해, 디스플레이를 위해 동적 키보드 인터페이스를 제공하는 단계를 더 포함하고,
상기 검색 질의는 사용자 컴퓨팅 디바이스에 의해 제공되는 동적 키보드 인터페이스 내에서 수신되는 것을 특징으로 하는 컴퓨터 구현 방법.13. The method of claim 12,
providing, by a user computing device of the computing system, a dynamic keyboard interface for display;
and the search query is received within a dynamic keyboard interface provided by a user computing device.
하나 이상의 프로세서와;
미디어 컨텐츠 아이템을 기술하는 데이터를 수신하고, 미디어 컨텐츠 아이템을 기술하는 데이터를 수신하는 것에 응답하여 미디어 컨텐츠 아이템의 복수의 이미지 프레임을 순차적으로 시청하는 것과 관련된 시간 정보를 기술하는 시간 분석 데이터를 출력하도록 구성된 기계 학습 시간 분석 모델과, 상기 시간 정보는 복수의 이미지 프레임의 개별 이미지 프레임에 의해 기술되지 않고;
하나 이상의 프로세서에 의해 실행될 때 컴퓨팅 시스템으로 하여금 동작들을 수행하게 하는 명령들을 집합적으로 저장하는 하나 이상의 비-일시적 컴퓨터 판독 가능 매체를 포함하고, 상기 동작들은:
순차 디스플레이를 위한 복수의 이미지 프레임을 포함하는 미디어 컨텐츠 아이템을 기술하는 데이터를 수신하는 단계;
미디어 컨텐츠 아이템을 기술하는 데이터를 기계 학습 시간 분석 모델에 입력하는 단계; 및
기계 학습 시간 분석 모델의 출력으로서, 시간 분석 데이터를 수신하는 단계를 포함하는 것을 특징으로 하는 컴퓨팅 시스템.A computing system comprising:
one or more processors;
receive data describing the media content item, and in response to receiving the data describing the media content item, output temporal analysis data describing temporal information associated with sequentially viewing a plurality of image frames of the media content item; a machine learning temporal analysis model constructed, wherein the temporal information is not described by individual image frames of a plurality of image frames;
1 . A computer readable medium comprising one or more non-transitory computer readable media collectively storing instructions that, when executed by one or more processors, cause a computing system to perform operations comprising:
receiving data describing a media content item comprising a plurality of image frames for sequential display;
inputting data describing media content items into a machine learning temporal analysis model; and
and receiving temporal analysis data as an output of the machine learning temporal analysis model.
상기 복수의 이미지 프레임은 제1 텍스트 문자열을 포함하는 제1 이미지 프레임 및 제1 이미지 프레임 이후에 순차적으로 디스플레이하기 위한 제2 텍스트 문자열을 포함하는 제2 이미지 프레임을 포함하고; 그리고
상기 시간 분석 데이터에 의해 기술된 시간 정보는 제1 텍스트 문자열 또는 제2 텍스트 문자열을 개별적으로 판독함에 의해 기술되지 않은 제2 텍스트 문자열 이전에 순차적으로 판독되는 제1 텍스트 문자열에 의해 기술된 시맨틱 의미를 기술하는 것을 특징으로 하는 컴퓨팅 시스템.15. The method of claim 14,
the plurality of image frames include a first image frame including a first text string and a second image frame including a second text string for sequentially displaying after the first image frame; and
The temporal information described by the temporal analysis data has a semantic meaning described by the first text string or the first text string sequentially read before the second text string not described by individually reading the second text string. A computing system, characterized in that it describes.
상기 제1 텍스트 문자열 및 제2 텍스트 문자열 중 하나 이상은 추가 단어가 없는 단일 단어 또는 추가 문자가 없는 단일 문자를 포함하는 것을 특징으로 하는 컴퓨팅 시스템.16. The method of claim 15,
wherein at least one of the first text string and the second text string comprises a single word without an additional word or a single character without an additional character.
상기 제1 이미지 프레임의 제1 텍스트 문자열은,
색상, 굵기, 위치 또는 폰트 중 적어도 하나에 의해 제2 이미지 프레임의 제2 텍스트 문자열의 외관과 상이한 외관을 갖는 것을 특징으로 하는 컴퓨팅 시스템.17. The method of claim 15 or 16,
The first text string of the first image frame,
A computing system, characterized in that it has an appearance different from an appearance of the second text string of the second image frame by at least one of a color, a thickness, a position, or a font.
상기 시간 분석 데이터는,
제2 텍스트 문자열의 외관과 상이한 외관을 갖는 제1 텍스트 문자열과 연관된 시맨틱 의미를 기술하는 것을 특징으로 하는 컴퓨팅 시스템.18. The method of claim 17,
The time analysis data is,
The computing system of claim 1, wherein the computing system describes the semantic meaning associated with the first text string having an appearance different from the appearance of the second text string.
상기 미디어 컨텐츠 아이템은 광고를 포함하고,
상기 시간 분석 데이터에 의해 기술된 시간 정보는 광고의 감정 컨텐츠를 기술하는 것을 특징으로 하는 컴퓨팅 시스템.19. The method according to any one of claims 14 to 18,
The media content item includes an advertisement;
The time information described by the time analysis data is a computing system, characterized in that it describes the emotional content of the advertisement.
하나 이상의 컴퓨팅 디바이스를 포함하는 컴퓨팅 시스템에 의해, 순차 디스플레이를 위한 복수의 이미지 프레임을 포함하는 미디어 컨텐츠 아이템을 기술하는 데이터를 수신하는 단계;
컴퓨팅 시스템에 의해, 미디어 컨텐츠 아이템을 기술하는 데이터를 수신하고, 미디어 컨텐츠 아이템을 기술하는 데이터를 수신하는 것에 응답하여 미디어 컨텐츠 아이템의 복수의 이미지 프레임을 순차적으로 시청하는 것과 관련된 시간 정보를 기술하는 시간 분석 데이터를 출력하도록 구성된 기계 학습 시간 분석 모델에 상기 미디어 컨텐츠 아이템을 기술하는 데이터를 입력하는 단계, 상기 시간 정보는 복수의 이미지 프레임의 개별 이미지 프레임에 의해 기술되지 않고;
컴퓨팅 시스템에 의해 기계 학습 시간 분석 모델의 출력으로서, 시간 분석 데이터를 수신하는 단계; 및
컴퓨팅 시스템에 의해, 시간 분석 데이터와 지상 실측 시간 분석 데이터의 비교에 기초하여 상기 기계 학습 시간 분석 모델의 하나 이상의 파라미터를 조정하는 단계를 포함하는 것을 특징으로 하는 기계 학습 시간 분석 모델을 트레이닝하기 위한 컴퓨터 구현 방법.A computer implemented method for training a machine learning temporal analysis model, the method comprising:
receiving, by a computing system including one or more computing devices, data describing a media content item comprising a plurality of image frames for sequential display;
time describing, by the computing system, data describing the media content item and temporal information associated with sequentially viewing, by the computing system, a plurality of image frames of the media content item in response to receiving the data describing the media content item inputting data describing the media content item into a machine learning temporal analysis model configured to output analysis data, wherein the temporal information is not described by individual image frames of a plurality of image frames;
receiving, by the computing system, temporal analysis data as an output of the machine learning temporal analysis model; and
adjusting, by the computing system, one or more parameters of the machine learning temporal analysis model based on a comparison of the temporal analysis data and the ground truth temporal analysis data; How to implement.
적어도 하나의 프로세서;
적어도 하나의 프로세서에 의해 실행될 때 적어도 하나의 프로세서로 하여금 제1항 내지 제13항 중 어느 한 항의 방법을 수행하게 하는 명령들을 저장하는 적어도 하나의 유형의 비-일시적 컴퓨터 판독 가능 매체를 포함하는 것을 특징으로 하는 컴퓨팅 시스템.A computing system comprising:
at least one processor;
14. A method comprising at least one tangible non-transitory computer readable medium storing instructions that, when executed by at least one processor, cause the at least one processor to perform the method of any one of claims 1-13. Computing system characterized.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202062979624P | 2020-02-21 | 2020-02-21 | |
US62/979,624 | 2020-02-21 | ||
PCT/US2020/030727 WO2021167632A1 (en) | 2020-02-21 | 2020-04-30 | Systems and methods for extracting temporal information from animated media content items using machine learning |
Publications (2)
Publication Number | Publication Date |
---|---|
KR20210107608A true KR20210107608A (en) | 2021-09-01 |
KR102498812B1 KR102498812B1 (en) | 2023-02-10 |
Family
ID=70969002
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
KR1020217001267A KR102498812B1 (en) | 2020-02-21 | 2020-04-30 | System and method for extracting temporal information from animated media content items using machine learning |
Country Status (6)
Country | Link |
---|---|
US (1) | US20220406033A1 (en) |
EP (1) | EP3895036A1 (en) |
JP (1) | JP7192086B2 (en) |
KR (1) | KR102498812B1 (en) |
CN (1) | CN113557521A (en) |
WO (1) | WO2021167632A1 (en) |
Families Citing this family (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20220382766A1 (en) * | 2021-06-01 | 2022-12-01 | Apple Inc. | Automatic Media Asset Suggestions for Presentations of Selected User Media Items |
EP4248415A1 (en) * | 2022-02-04 | 2023-09-27 | Google LLC | Automated video and audio annotation techniques |
CN115426525B (en) * | 2022-09-05 | 2023-05-26 | 北京拙河科技有限公司 | High-speed dynamic frame linkage image splitting method and device |
CN117576785B (en) * | 2024-01-15 | 2024-04-16 | 杭州巨岩欣成科技有限公司 | Swim guest behavior detection method and device, computer equipment and storage medium |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP2605152A1 (en) * | 2010-08-11 | 2013-06-19 | Sony Corporation | Information processing device, information processing method, and program |
KR20190128117A (en) * | 2018-05-07 | 2019-11-15 | 구글 엘엘씨 | Systems and methods for presentation of content items relating to a topic |
US20190373322A1 (en) * | 2018-05-29 | 2019-12-05 | Sony Interactive Entertainment LLC | Interactive Video Content Delivery |
Family Cites Families (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2007043679A1 (en) | 2005-10-14 | 2007-04-19 | Sharp Kabushiki Kaisha | Information processing device, and program |
JP4812733B2 (en) | 2007-11-01 | 2011-11-09 | 日本電信電話株式会社 | Information editing apparatus, information editing method, information editing program, and recording medium recording the program |
US10282599B2 (en) * | 2016-07-20 | 2019-05-07 | International Business Machines Corporation | Video sentiment analysis tool for video messaging |
US11328159B2 (en) * | 2016-11-28 | 2022-05-10 | Microsoft Technology Licensing, Llc | Automatically detecting contents expressing emotions from a video and enriching an image index |
CN109145712B (en) | 2018-06-28 | 2020-10-16 | 南京邮电大学 | Text information fused GIF short video emotion recognition method and system |
CN110020437B (en) * | 2019-04-11 | 2023-04-07 | 江南大学 | Emotion analysis and visualization method combining video and barrage |
CN110427454B (en) * | 2019-06-21 | 2024-03-15 | 平安科技（深圳）有限公司 | Text emotion analysis method and device, electronic equipment and non-transitory storage medium |
KR102244280B1 (en) * | 2019-07-17 | 2021-04-23 | 장호정 | Advertizing method of instant exposure synchronized with emoticon display |
US11200456B2 (en) * | 2019-07-31 | 2021-12-14 | GE Precision Healthcare LLC | Systems and methods for generating augmented training data for machine learning models |
-
2020
- 2020-04-30 KR KR1020217001267A patent/KR102498812B1/en active IP Right Grant
- 2020-04-30 CN CN202080005148.8A patent/CN113557521A/en active Pending
- 2020-04-30 US US17/295,576 patent/US20220406033A1/en active Pending
- 2020-04-30 EP EP20730150.8A patent/EP3895036A1/en not_active Withdrawn
- 2020-04-30 JP JP2021503172A patent/JP7192086B2/en active Active
- 2020-04-30 WO PCT/US2020/030727 patent/WO2021167632A1/en unknown
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP2605152A1 (en) * | 2010-08-11 | 2013-06-19 | Sony Corporation | Information processing device, information processing method, and program |
KR20190128117A (en) * | 2018-05-07 | 2019-11-15 | 구글 엘엘씨 | Systems and methods for presentation of content items relating to a topic |
US20190373322A1 (en) * | 2018-05-29 | 2019-12-05 | Sony Interactive Entertainment LLC | Interactive Video Content Delivery |
Non-Patent Citations (1)
Title |
---|
Rainer Lienhart: "Automatic Text Recognition for Video Indexing" (1997.02.01.) * |
Also Published As
Publication number | Publication date |
---|---|
JP2022524471A (en) | 2022-05-06 |
CN113557521A (en) | 2021-10-26 |
US20220406033A1 (en) | 2022-12-22 |
JP7192086B2 (en) | 2022-12-19 |
EP3895036A1 (en) | 2021-10-20 |
KR102498812B1 (en) | 2023-02-10 |
WO2021167632A1 (en) | 2021-08-26 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10474877B2 (en) | Automated effects generation for animated content | |
Zhao et al. | Predicting personalized image emotion perceptions in social networks | |
US11138207B2 (en) | Integrated dynamic interface for expression-based retrieval of expressive media content | |
US11290775B2 (en) | Computerized system and method for automatically detecting and rendering highlights from streaming videos | |
CN111143610B (en) | Content recommendation method and device, electronic equipment and storage medium | |
US20170212892A1 (en) | Predicting media content items in a dynamic interface | |
KR102498812B1 (en) | System and method for extracting temporal information from animated media content items using machine learning | |
KR102427412B1 (en) | A face-to-target image combination from a source image based on a search query | |
Shah et al. | Multimodal analysis of user-generated multimedia content | |
US20170083519A1 (en) | Platform and dynamic interface for procuring, organizing, and retrieving expressive media content | |
US20170083520A1 (en) | Selectively procuring and organizing expressive media content | |
JP2019531548A (en) | Video capture framework for visual search platform | |
US20220092071A1 (en) | Integrated Dynamic Interface for Expression-Based Retrieval of Expressive Media Content | |
US20230353820A1 (en) | Systems and Methods for Improved Searching and Categorizing of Media Content Items Based on a Destination for the Media Content Machine Learning | |
CN110554782A (en) | Expression input image synthesis method and system | |
CN115114395A (en) | Content retrieval and model training method and device, electronic equipment and storage medium | |
CN110827058A (en) | Multimedia promotion resource insertion method, equipment and computer readable medium | |
Qureshi et al. | Video based sentiment analysis |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
E902 | Notification of reason for refusal | ||
AMND | Amendment | ||
E601 | Decision to refuse application | ||
X091 | Application refused [patent] | ||
AMND | Amendment | ||
X701 | Decision to grant (after re-examination) | ||
GRNT | Written decision to grant |