US10579733B2 - Identifying codemixed text - Google Patents
Identifying codemixed text Download PDFInfo
- Publication number
- US10579733B2 US10579733B2 US15/976,647 US201815976647A US10579733B2 US 10579733 B2 US10579733 B2 US 10579733B2 US 201815976647 A US201815976647 A US 201815976647A US 10579733 B2 US10579733 B2 US 10579733B2
- Authority
- US
- United States
- Prior art keywords
- token
- text
- codemixed
- language
- data processing
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/263—Language identification
-
- G06F17/277—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/284—Lexical analysis, e.g. tokenisation or collocates
-
- G06F17/275—
-
- G06F17/2785—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/205—Parsing
- G06F40/216—Parsing using statistical methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/232—Orthographic correction, e.g. spell checking or vowelisation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/082—Learning methods modifying the architecture, e.g. adding, deleting or silencing nodes or connections
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/01—Dynamic search techniques; Heuristics; Dynamic trees; Branch-and-bound
-
- G06N7/005—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N7/00—Computing arrangements based on specific mathematical models
- G06N7/01—Probabilistic graphical models, e.g. probabilistic networks
Definitions
- This disclosure relates to identifying codemixed text.
- sentence level- and document level language identifiers are unable to provide per-token (e.g., per-word) language identification on codemixed text, which is needed for many multilingual downstream tasks, including syntactic analysis, machine translation, and dialog systems. It is infeasible for humans to obtain token-level labels for hundreds of languages since candidate codemixed examples must be identified and then annotated by multilingual speakers. Moreover, since codemixing is most common in informal contexts, token-level labels would also need to be obtained to account for a seemingly endless amount of non-standard words (e.g. slang), misspellings, transliteration, and abbreviations.
- non-standard words e.g. slang
- One aspect of the disclosure provides a method for identifying codemixed text that includes receiving, at data processing hardware, codemixed text and segmenting, by the data processing hardware, the codemixed text into a plurality of tokens. Each token includes at least one character and is delineated from any adjacent tokens by a space. For each token of the codemixed text, the method also includes extracting, by the data processing hardware, features from the token and predicting, by the data processing hardware, a probability distribution over possible languages for the token using a language identifier model configured to receive the extracted features from the token as feature inputs. The method also includes assigning, by the data processing hardware, a language to each token of the codemixed text by executing a greedy search on the probability distribution over the possible languages predicted for each respective token.
- Implementations of the disclosure may include one or more of the following optional features.
- the method also includes, for each token of the codemixed text, extracting, by the data processing hardware, features from any adjacent tokens.
- the language identifier model is further configured to receive the extracted features from the adjacent tokens as feature inputs for predicting the probability distribution over possible languages for the corresponding token.
- the feature inputs may include at least one of character features, script features, or lexicon features.
- extracting features from the token includes identifying all character n-gram features in the corresponding token, and for each character n-gram feature, calculating a corresponding frequency of the character n-gram feature in the corresponding token by dividing a corresponding number of occurrences for the character n-gram in the corresponding token by a total number of character n-grams identified from the corresponding token. Identifying all character n-gram features may optionally include identifying at least one of character unigram features, character bigram features, character trigram features, or character quadrigram features in the corresponding token.
- extracting features from the token may optionally include identifying all character script features in the corresponding token, determining a unicode value for each identified character script feature, and assigning each identified character script feature a corresponding character script type from a set of possible character script types based on the corresponding unicode value for the identified character script feature.
- the assigned character script type may be associated with only one language.
- extracting features from the token includes: querying a lexicon library stored in memory hardware in communication with the data processing hardware, the lexicon library comprising a pool of word entries and corresponding language probability distributions for each word entry in the pool of word entries; determining whether the token matches one of the word entries of the lexicon library; and when the token matches one of the word entries of the lexicon library, retrieving the corresponding language probability distribution for the word entry that matches the token.
- the language identifier model may optionally include a feed-forward neural network that includes an embedding layer, a hidden layer interconnected to the embedding layer in a feed-forward manner, and an output layer interconnected to the hidden layer in the feed-forward manner.
- the embedding layer is configured to: receive the feature inputs, each feature input including a sparse matrix; map the sparse matrix of each respective feature input to dense embedding vectors, resulting in a learned embedding matrix; and concatenate each learned embedding matrix corresponding to each received feature input.
- the hidden layer is configured to receive the embedding layer and apply a rectified linear unit (ReLU) of the embedding layer.
- the output layer is configured to output a probability for each possible language for each respective token. In some examples, the output layer includes a softmax layer.
- the method also includes receiving, by the data processing hardware, an assignment constraint that assumes at least one language assigned to at least one token of the codemixed text.
- assigning the language to each token of the codemixed text includes selecting the language having a greatest probability distribution for the respective token relative to any adjacent tokens based on the assignment constraint.
- the at least one assumed language of the assignment constraint may optionally include English or French.
- the assignment constraint may include at least one fixed set of language pairs permissible for assignment to each token of the codemixed text.
- the language identifier model may execute a lexicon feature dropout strategy during training that drops a sub-set of extracted lexicon features as feature inputs.
- the system includes data processing hardware and memory hardware in communication with the data processing hardware.
- the memory hardware stores instructions that when executed by the data processing hardware cause the data processing hardware to perform operations that include receiving codemixed text and segmenting the codemixed text into a plurality of tokens.
- Each token includes at least one character and is delineated from any adjacent tokens by a space.
- the operations also include extracting features from the token and predicting a probability distribution over possible languages for the token using a language identifier model configured to receive the extracted features from the token as feature inputs.
- the operations also include assigning a language to each token of the codemixed text by executing a greedy search on the probability distribution over the possible languages predicted for each respective token.
- the operations also include, for each token of the codemixed text, extracting features from any adjacent tokens.
- the language identifier model is further configured to receive the extracted features from the adjacent tokens as feature inputs for predicting the probability distribution over possible languages for the corresponding token.
- the feature inputs may include at least one of character features, script features, or lexicon features.
- extracting features from the token includes identifying all character n-gram features in the corresponding token, and for each character n-gram feature, calculating a corresponding frequency of the character n-gram feature in the corresponding token by dividing a corresponding number of occurrences for the character n-gram in the corresponding token by a total number of character n-grams identified from the corresponding token. Identifying all character n-gram features may optionally include identifying at least one of character unigram features, character bigram features, character trigram features, or character quadrigram features in the corresponding token.
- extracting features from the token may include identifying all character script features in the corresponding token, determining a unicode value for each identified character script feature, and assigning each identified character script feature a corresponding character script type from a set of possible character script types based on the corresponding unicode value for the identified character script feature.
- the assigned character script type may be associated with only one language.
- extracting features from the token includes: querying a lexicon library stored in memory hardware in communication with the data processing hardware, the lexicon library comprising a pool of word entries and corresponding language probability distributions for each word entry in the pool of word entries; determining whether the token matches one of the word entries of the lexicon library; and when the token matches one of the word entries of the lexicon library, retrieving the corresponding language probability distribution for the word entry that matches the token.
- the language identifier model may optionally include a feed-forward neural network that includes an embedding layer, a hidden layer interconnected to the embedding layer in a feed-forward manner, and an output layer interconnected to the hidden layer in the feed-forward manner.
- the embedding layer is configured to: receive the feature inputs, each feature input including a sparse matrix; map the sparse matrix of each respective feature input to dense embedding vectors, resulting in a learned embedding matrix; and concatenate each learned embedding matrix corresponding to each received feature input.
- the hidden layer is configured to receive the embedding layer and apply a rectified linear unit (ReLU) of the embedding layer.
- the output layer is configured to output a probability for each possible language for each respective token. In some examples, the output layer includes a softmax layer.
- the operations also includes receiving an assignment constraint that assumes at least one language assigned to at least one token of the codemixed text.
- assigning the language to each token of the codemixed text includes selecting the language having a greatest probability distribution for the respective token relative to any adjacent tokens based on the assignment constraint.
- the at least one assumed language of the assignment constraint may optionally include English or French.
- the assignment constraint may include at least one fixed set of language pairs permissible for assignment to each token of the codemixed text.
- the language identifier model may execute a lexicon feature dropout strategy during training that drops a sub-set of extracted lexicon features as feature inputs.
- FIG. 1 schematically illustrates an example system for identifying codemixed text.
- FIGS. 2A and 2B schematically illustrates examples of inter-mix and intra-mix text inputs.
- FIG. 3 schematically illustrates an example feed-forward neural network model configured to predict probability distributions over possible languages for tokens in codemixed text.
- FIG. 4 schematically illustrates a plurality of probability distributions over possible languages for corresponding tokens of codemixed text.
- FIGS. 5A and 5B schematically illustrate an example greedy decoding strategy for assigning language labels to tokens of codemixed text.
- FIG. 6 schematically illustrates example applications using language-labels assigned to tokens of codemixed text.
- FIG. 7A is a plot illustrating per-token language prediction accuracy for various dropout rates of lexicon features.
- FIG. 7B is a table illustrating language prediction accuracy of a neural network model on various training datasets of codemixed text.
- FIG. 7C is a table illustrating language prediction accuracy of a neural network model on a corpus of monolingual text.
- FIG. 8 is a flowchart of an example arrangement of operations for a method of per-token language identification in codemixed text.
- FIG. 9 is a schematic view of an example computing device that may be used to implement the systems and methods described herein.
- Implementations herein are directed toward a two-stage process for assigning per-token language labels in codemixed text by determining a probability distribution over possible languages for each token (i.e., a linguistic, such as a word) in the codemixed text during the first stage, and then assigning a language label to each token based on the probability distribution over possible languages and global constraints (e.g., assignment constraints) applied to the codemixed text during the second stage.
- the data processing hardware may receive codemixed text input by a user and parse the codemixed text into tokens, extract features of each token, and then predict the probability distribution over possible languages for each respective token using a feed-forward neural network configured to receive the extracted features as feature inputs.
- the feature inputs may include at least one of character features (e.g., n-grams), script features (e.g., text scripts correlated with specific languages), or lexicon features, and the feed-forward neural network (e.g., efficient feed-forward classifiers) outputs a language distribution for every token independently from one another.
- a local context window may specify that extracting character features and/or lexicon features from a respective token also includes extracting character features and/or lexicon features from previous and following tokens (i.e., adjacent tokens) to provide additional context for the respective token.
- the second stage may employ a decoder that receives the probability distribution over possible languages for each token output from the feed-forward neural network and assigns the language label to each respective token.
- the decoder executes a decoding algorithm (e.g., greedy decoding strategy) configured to select the language having the greatest probability in the probability distribution for each respective token relative to language predictions for adjacent tokens based on global constraints applied over the entire input of codemixed text.
- the decoder may ensure high-quality language predictions on both monolingual texts as well as codemixed text inputs.
- the global constraints improve accuracy in predicting the language labels for each token by permitting the decoding algorithm to assume at least one language assigned to at least one token of the codemixed text input.
- a global or assignment constraint includes two or more fixed sets of language pairs permissible for assignment to each token in the input of text which may be monolingual or codemixed. For instance, for each fixed set of language pairs, the greedy decoding strategy may select the language from the corresponding language pair having the greatest probability in the probability distribution for each respective token and calculate a score for the corresponding fixed set of language pairs by summing the probabilities associated with the selected languages. Thereafter, the greedy decoding strategy assigns the language to each token using the selected languages from the fixed set of language pairs associated with the highest score.
- implementations herein are directed toward using the two-stage process to assign a language to every token in codemixed text (e.g., a sentence, phrase, or utterance).
- the first stage predicts the probability distribution over possible languages for each token independently with a feed-forward neural network using character and token features from a local context window.
- the second stage determines a best assignment of token labels for the entire sentence of codemixed text using the greedy search (e.g., decoding algorithm) subject to global constraints.
- the two-stage process provides major advantages for fine-grained language identification. Namely, the two-stage process does not require annotations for codemixed text samples over hundreds of languages and their mixed parings. Additionally, the learning independent classifiers of the feed-forward neural network model followed by greedy decoding is significantly faster and substantially easier to implement that structured training.
- CRFs conditional random fields
- RNNs recurrent neural networks
- a system 100 includes a user device 102 associated with a user 10 , who may communicate, via a network 130 , with a remote system 140 .
- the user device 102 includes data processing hardware 104 and memory hardware 106 .
- the data processing hardware 104 executes a language identifier 150 , 150 a configured to receive codemixed text 200 including an utterance or sentence containing multiple languages, parse/segment the codemixed text 200 into a plurality of tokens 202 , and assign a corresponding language label 220 to each token 202 .
- a token 202 may refer to a linguistic unit such as a word or one or more successive characters/symbols. Accordingly, each token 202 may be delineated from adjacent tokens 202 by a space in the codemixed text 200 .
- the user device 102 may transmit the codemixed text 200 and corresponding language labels 220 over the network 130 for use by one or more applications/services 600 executing on the remote system 140 .
- the codemixed text 200 may be an entry/query by the user 10 to a message board, a social media application, a product review platform, a search engine, navigation/map application, or other service executing on the remote system 140 .
- an application 600 executing on the user device 102 may receive the codemixed text 200 and corresponding language labels 220 output from the language identifier 150 a (or output from a language identifier 150 b residing on the remote system 140 ).
- the user device 102 may execute a translator for translating the codemixed text 200 into a desired language or a dialog application (e.g., a navigation application with turn-by-turn audible instructions or a virtual assistant) that converts text-to-speech may use the appropriate language label 220 assigned to each word (e.g., token) so that each word is pronounced correctly.
- a dialog application e.g., a navigation application with turn-by-turn audible instructions or a virtual assistant
- the user device 102 can be any computing device capable of communicating with the remote system 140 through the network 130 .
- the user device 102 includes, but is not limited to, desktop computing devices and mobile computing devices, such as laptops, tablets, smart phones, and wearable computing devices (e.g., headsets and/or watches).
- the user 10 may provide the codemixed text 200 to the user device 102 directly via a user input 12 .
- the user 10 may provide the user input 12 using a keyboard, joystick, smart pen, touch screen, track pad, mouse, joystick, or any other interface in communication with the user device 102 .
- the user input 12 includes a voice/speech input captured by a microphone (when explicitly enabled and/or activated by a designated hotword/phrase) of the user device 102 .
- the speech input is converted into digital form and an automatic speech recognizer annotates the speech input into the codemixed text 200 .
- the user device 102 may optionally send the digital speech input to the remote system 140 for conversion by an automatic speech recognizer to annotate the speech into the codemixed text 200 .
- the remote system 140 may be a distributed system (e.g., cloud computing environment) having scalable/elastic resources 142 .
- the resources 142 include computing resources 144 (e.g., data processing hardware) and/or storage resources 146 (e.g. memory hardware).
- the remote system 140 executes a language identifier 150 , 150 b in lieu of the user device 102 executing the language identifier 150 a .
- the language identifier 150 b executing on the remote system 140 receives the codemixed text 200 (without the language labels 220 ) from the user device 102 , parses/segments the codemixed text 200 into the plurality of tokens 202 , and assigns the corresponding language label 220 to each token 202 .
- the language identifier 150 b executing on the remote system 140 requires about 30 megabytes (MB) of storage while the language identifier 150 a executing on the user device 102 only requires about one (1) MB of storage.
- the language identifier 150 b associated with the remote system 140 uses lexicon features 210 , 210 c requiring additional memory/storage capacity for predicting languages for each token 202
- the language identifier 150 a associated with the user device 102 may omit the use of lexicon features 210 c for predicting languages to alleviate memory/storage requirements for the user device 102 .
- lexicon features 210 c provide increased performance for short inputs of codemixed text 200 (e.g., three or less tokens) at the cost of requiring additional memory capacity. However, for larger inputs of codemixed text 200 , performance degradation is negligible when lexicon features 210 c are omitted and only character n-gram features 210 a (and optionally script features 210 b ) are used for language prediction.
- a user input 12 to the user device 102 indirectly generates codemixed text 200 received by the language identifier 150 .
- a user input 12 e.g., voice or text
- the user input 12 may cause a navigation application executing on the user device 102 (or via a web-based application) to send the navigation query 14 to the remote system 140 .
- the navigation query 14 may include a destination address or specify a name of a location or venue the user 10 wants driving directions for (e.g., a suggested route), or the navigation query 14 may not include an address or name of the location or venue—e.g., “Directions to a nearby coffee shop.”
- the remote system 140 may receive the navigation query 14 and generate a search result for the driving directions that includes a navigation instruction along the suggested route having codemixed text 200 stating “Turn right on Rue Monge.”
- the language identifier 150 may receive the codemixed text 200 and assign an English language label 220 to the Turn, right, and on tokens 202 , and assign a French language label 220 to the Rue and Monge tokens 202 .
- the user device 102 may receive the driving directions having the codemixed text 200 and use the assigned language labels 220 to audibly notify the user 10 when to Turn right on Rue Monge, while ensuring that the French street, Rue Monge, is pronounced correctly.
- the user input 12 includes a search query 15 to a search engine executing on the remote system 114 that requests search results for dinner specials at an Italian restaurant.
- the search engine may generate the search result for the requested dinner specials that includes codemixed text 200 .
- the dinner special including the codemixed text 200 may state “COZZE E VONGOLE AL VINO BIANCO: Mussels and claims with garlic and white wine.”
- the language identifier 150 may receive the codemixed text 200 and assign an Italian language label 220 to the COZZE, E, VONGOLE, AL, VINO, and BIANCO tokens 202 , and assign an English language label 220 to the remaining tokens 202 .
- the user device 102 may receive the dinner special having the codemixed text 200 and use the assigned language labels 220 to audibly inform the user 10 of the requested dinner special while pronouncing the Italian words correctly.
- the language identifier 150 includes a parser 160 , an extractor 170 , a feed-forward neural network model 300 , and an encoder 190 configured to assign the corresponding language label 220 to each respective token 202 of the codemixed text 200 .
- the parser 160 receives the codemixed text 200 containing multiple languages and segments/parses the codemixed text 200 into a plurality of tokens 202 .
- Each token 202 includes at least one character (e.g., letter or script) and is delineated from any adjacent tokens 202 by a space.
- the extractor 170 receives each token 202 of the codemixed text 200 from the parser 160 and extracts one or more features 210 , 210 a - c from each token 202 .
- the features 210 may include at least one of character features 210 a , script features 210 b , or lexicon features 210 c.
- the codemixed text 200 may include either one of intra-mix text 200 a or inter-mix text 200 b .
- Intra-mix text 200 a includes a sentence starting with one language and switching to another language, while inter-mix text 200 b includes a sentence having an overall first language with words from a second language dispersed in the middle.
- the intra-mix text 200 a includes the sentence Dame ese book that you told me about (English translation: “Give me this book that you told me about”).
- the parser 160 may parse/segment the intra-mix text 200 a into a plurality of tokens 202 , 202 aa - 202 ah , where the Symbol token 202 aa and the ese token 202 ab are associated with Spanish and the remaining tokens 202 ac - 202 ah are associated with English.
- the example of inter-mix text 200 b includes the sentence Aapni photo send karo (English translation: “Send your photo”).
- the parser 160 may parse/segment the inter-mix text 200 b into a plurality of tokens 202 , 202 ba - 202 bd , where the first and last Aapni and karo tokens 202 ba , 202 bd are associated with Hindi and the middle photo and send tokens 202 bb , 202 bc are associated with English.
- extracting the character features 210 a includes identifying all character n-gram features in the corresponding token 202 .
- An n-gram is a sequence of n consecutive characters, e.g., letters or symbols, and includes an order of size associated with the number characters in the n-gram. For example, a 1-gram (or unigram) includes one character; a 2-gram (or bigram) includes two characters; a 3-gram (or trigram includes three characters; and a 4-gram (or quadrigram) includes four characters.
- the extractor 170 may identify at least one of character unigram features, character bigram features, character trigram features, or character quadrigram features in the corresponding token 202 .
- the extractor 170 is configured to calculate a corresponding frequency of the character n-gram feature 210 a in the token 202 by dividing a corresponding number of occurrences for the character n-gram in the corresponding token 202 by a total number of character n-grams identified from the corresponding token 202 .
- the token 202 is banana
- one of the extracted character trigrams 210 a is ana and the corresponding frequency for ana is 2/6.
- the token 202 banana includes a total of six character trigrams 210 a due to an additional boundary symbol appended to both ends of the token 202 .
- the extractor 170 may use feature hashing to control a vocabulary size V of the extracted character features 210 a from a corresponding token 202 and avoid storing a large string-to-id map in the memory hardware 106 , 146 during run time.
- a feature id for a corresponding n-gram string x is given by H(x)mod V g , where H is a well-behaved hash function.
- the vocabulary size V may be set equal to 1000, 1000, 5000, 5000 for n equal to 1, 2, 3, 4, respectively.
- extracting character features 210 a from each corresponding token 202 includes extracting character features 210 a from any adjacent tokens 202 .
- the extractor 170 may extract character features 210 a from previous and following tokens 202 , as well as from the respective token 202 , in order to provide additional context for the respective token 202 .
- extracting script features 210 b from a corresponding token 202 provides a strong correlation to a specific language. For instance, Hiragana script is only used in Japanese and Hangul script is only used in Korean.
- the language identifier 150 may store a character script library 172 in data storage 180 (residing on the memory hardware 106 , 146 ) that maps different character script types 174 to corresponding unicode values associated with character script features 210 b .
- the number of character script types 174 may correspond to a vocabulary size V of the script features 210 b .
- the character script library 172 is trained on twenty-eight (28) different script types 174 .
- the extractor 170 may output a final vector for each corresponding token 202 that contains normalized counts of all character script features 210 identified in the corresponding token 202 . Accordingly, the extractor 170 may identify all character script features 210 b in a corresponding token 202 , determine a unicode value for each identified character script features 210 a , and assign a corresponding character script type 174 for the identified character script feature 210 b based on the corresponding unicode value.
- Lexicon features 210 c are both prevalent and highly predictive for predicting languages associated with tokens 202 . Especially for smaller codemixed text 200 inputs (e.g., four tokens or less), lexicon features 210 c may provide strong signals for per-token language identification. The use of lexicon features 210 c are not suitable, however, for predicting languages for informal words, such as misspelled words, abbreviated words, or slang, all of which commonly occur in informal contexts.
- the language identifier 150 stores a lexicon library 182 in the data storage 180 that includes a pool of word entries 184 and corresponding language probability distributions 186 for each word entry 184 in the pool of word entries 184 .
- the word entry 184 for the word mango includes a corresponding language probability distribution 186 indicating that mango occurs 50% of the time in English contexts and 13% of the time in Spanish contexts.
- the lexicon library 182 may contain about four million word entries 184 and cover one-hundred and ten (110) different languages.
- the lexicon features 210 c may include a corresponding vocabulary size V equal to the number of languages (e.g., 110 languages) in the lexicon library 182 . Accordingly, the large lexicon library 182 necessitates increased memory/storage requirements that may only be suitable for use by the language identifier 150 b when implemented on the remote system 140 .
- the lexicon library 182 is trained on a public corpus of text such as Wikipedia pages.
- the extractor 170 extracts lexicon features 210 c by querying the lexicon library 182 to determine whether a corresponding token 202 matches one of the word entries 184 in the lexicon library 182 . In these examples, when the corresponding token 202 matches one of the word entries 184 , the extractor 170 retrieves the corresponding language probability distribution 186 for the matching word entry 184 for use by the neural network model 300 .
- the extracted lexicon features 210 c may be set to one (1) for all non-zero probabilities.
- the extractor 170 may provide a one-hot vector to the neural network model 300 whose only non-zero value is a position indicating the corresponding language associated with the token 202 .
- extracting lexicon features 210 c from each corresponding token 202 may include extracting lexicon features 210 c from any adjacent tokens 202 .
- the extractor 170 may extract lexicon features 210 c from previous and following tokens 202 , as well as from the respective token 202 , in order to provide additional context for the respective token 202 .
- An additional prefix lexicon library for language distributions of 6-gram character prefixes may also be constructed and stored in the data storage 180 .
- the extractor 170 may similarly query the additional prefix lexicon library to determine whether a corresponding token 202 matches one of the 6-gram character prefix entries, and when a match occurs, retrieve a corresponding language probability distribution for the matching 6-gram character prefix entry.
- the extractor 170 may optionally query the prefix lexicon library when no matching word entries 184 are found in the lexicon library 182 .
- the neural network model 300 receives the extracted features 210 from the respective token 202 as feature inputs 210 and predicts a probability distribution 400 over possible languages for the respective token 202 using the feature inputs 210 . Accordingly, the feature inputs 210 received by the neural network model 300 may merge one or more of the character n-gram features 210 a , character script features 210 b , or lexicon features 210 c together for predicting the probability distribution 400 for the respective token 202 .
- the feed-forward neural network model 300 may be referred to as a language identifier model.
- FIG. 3 shows an example of the feed-forward neural network model 300 of FIG. 1 .
- the feed-forward neural network model 300 includes an embedding layer 302 , a hidden layer 304 interconnected with the embedding layer 302 in a feed-forward manner, and an output layer 306 interconnected with the hidden layer 304 in the feed-forward manner.
- the output layer 306 is a softmax layer.
- the embedding layer 302 is configured to receive the feature inputs 210 from one or more of the feature groups g of character features 210 a , script features 210 b , or lexicon features 210 c .
- the embedding layer 302 may represent the feature input 210 for each group g by a sparse matrix X g as follows: X g ⁇ R F g ⁇ V g (1) where F g is a number of feature templates and V g is the vocabulary size of the corresponding feature group.
- the feed-forward neural network model 300 may use both discrete and continuous features.
- the hidden layer 304 is configured to receive the embedding layer 302 and apply a rectified linear unit (ReLU) of the embedding layer 302 .
- the hidden layer 304 may include a size 256 and the ReLU may be applied over hidden layer outputs.
- the final output layer 306 (e.g., softmax layer) is configured to output a probability for each possible language for each respective token 202 .
- the probability for each possible language output from the output layer 306 corresponds to the probability distribution 400 over possible languages for each respective token 202 of the codemixed text.
- the feed-forward neural network 300 may be trained per-token with cross-entropy loss.
- FIG. 4 shows example probability distributions 400 , 400 a - 400 e over possible languages output by the feed-forward neural network model 300 for each of a plurality of tokens 202 , 202 a - e of codemixed text 200 .
- the codemixed text 200 reciting “cv bien hmd w enti” is parsed into respective tokens 202 a - e for cv, bien, hmd, w, and enti.
- the probability distribution 400 a for the cv token 202 a includes probability scores p of 0.80, 0.11, 0.05, ⁇ 0.10 for Vietnamese (vi), English (en), Dutch (nl), and French (fr), respectively.
- the probability distribution 400 b for the bien token 202 b includes probability scores p of 0.72, 0.13, ⁇ 0.05 for French (fr), Spanish (es), and Arabic (ar), respectively.
- the probability distribution 400 c for the hmd token 202 c includes a single probability score p of 1.20 for Arabic (ar).
- the probability distribution 400 d for the w token 202 d includes probability scores p of 0.50, 0.15, 0.13 for Polish (pl), English (en), and Arabic (ar), respectively.
- the probability distribution 400 e for the enti token 202 e includes probability scores of 0.70, 0.23, ⁇ 0.09 for Italian (it), Arabic (ar), and Spanish (es), respectively.
- the encoder 190 simply picks the language associated with the highest probability score from each probability distributions 400 a - 400 e for assigning corresponding language labels 220 , the encoder 190 would assign each token 202 a - 202 e of the codemixed text 200 a different language label 220 . In this case, it is very unlikely that a user would input codemixed text containing five different languages.
- FIG. 1 shows the encoder 190 receiving probability distributions 400 over possible languages output from the neural network model 300 for corresponding tokens 202 in codemixed text 200 , as well as an assignment constraint 510 that assumes at least one language assigned to at least one token 202 of the codemixed text 200 .
- the assignment constraint 510 includes at least one fixed set of language pairs permissible for assignment to each token of the codemixed text 200 .
- the language identifier 150 may have access to one-hundred and sixteen (116) fixed sets of language pairs that primary include a combination of English and a non-English language. Accordingly, the encoder 190 executes the greedy decoding strategy 500 to assign the language label 220 to each token 202 of the codemixed text 200 by selecting the language having a greatest probability distribution for the respective token 202 relative to any adjacent tokens 202 based on the assignment constraint 510 .
- FIGS. 5A and 5B show the encoder 190 executing the greedy decoding strategy 500 to assign language labels 220 to each of the plurality of tokens 202 a - e of the codemixed text 200 of FIG. 4 based on an assignment constraint 510 , 510 a - b including two fixed sets of different language pairs (English/Arabic and French/Arabic). Accordingly, the assignment constraint 510 applied in the example assumes that at least one language assigned to at least one token 202 in the codemixed text 200 is English or French.
- the strategy 500 applies the assignment constraint 510 a including the first fixed set of language pairs for English and Arabic.
- the assignment constraint 510 a permits the strategy 500 to only select the language from the corresponding English/Arabic language pair that has a greatest probability (e.g., probability score p) in the probability distribution 400 over possible languages for each respective token 202 of the codemixed text 200 .
- the strategy 500 assigns an English language label 220 a for the cv token 202 a since Arabic is not listed in the associated probability distribution 400 a and assigns Arabic language labels 220 b , 220 c for the bien and hmd tokens 202 b , 202 c since English is not listed in the associated probability distributions 400 b , 400 c .
- the probability distribution 400 d for the w token 202 d lists both English (en) and Arabic (ar) as possible languages
- the strategy 500 assigns an English language label 220 d to the w token 202 d since the probability score of 0.15 for English is greater than the probability score of 0.13 for Arabic.
- the strategy 500 assigns an Arabic language label 220 e to the enti token 202 e since English is not listed in the associated probability distribution 400 e.
- the greedy decoding strategy 500 calculates a score associated the assignment constraint 510 a by summing the probability scores associated with the assigned language labels 220 a - e (e.g., the selected languages for each token 202 a - e ).
- the assignment constraint 510 a including the language pair for English and Arabic includes a score equal to 1.64.
- the greedy decoding strategy 500 applies the assignment constraint 510 b including the second set of fixed language pairs for French and Arabic.
- the greedy decoding strategy 500 is only permitted to select the language from the corresponding French/Arabic language pair of the assignment constraint 510 b that has a greatest probability (e.g., probability score p) in the probability distribution 400 over possible languages for each respective token 202 of the codemixed text 200 .
- FIG. 5B shows the strategy 500 assigning a French language label 220 a to the cv token 202 a since Arabic is not listed in the associated probability distribution 400 a .
- the strategy 500 assigns a French language label 220 b to the bien token 202 b since the probability score of 0.72 for French is greater than the probability score of ⁇ 0.05 for Arabic.
- FIG. 5B further shows the greedy decoding strategy 500 assigning Arabic language labels 220 c , 220 d , 220 e for the hmd, w, and enti tokens 202 c , 202 d , 202 e since French is not listed in the associated probability distributions 400 c , 400 d , 400 e.
- the greedy decoding strategy 500 calculates a score associated the assignment constraint 510 b by summing the probability scores associated with the assigned language labels 220 a - e (e.g., the selected languages for each token 202 a - e ).
- the assignment constraint 510 b including the language pair for French and Arabic includes a score equal to 2.18.
- the greedy decoding strategy 500 is configured to select the French/Arabic language labels 220 a - 220 e for assignment to the corresponding tokens 202 a - 202 e .
- assigning language labels 220 to each corresponding token 202 of codemixed text 200 includes assigning a language to each token using languages selected from a fixed set of language pairs associated with a highest score.
- the greedy decoding strategy 500 assigns the French language labels 220 a , 220 b to the cv and bien tokens 202 a , 202 b and the Arabic language labels 220 c , 220 d , 220 e to the hmd, w, and enti tokens 202 c , 202 d , 202 e .
- the language identifier 150 of FIG. 1 may output the codemixed text 200 with the assigned language labels 220 for use by one or more applications 600 executing on the remote system 140 and/or the user device 102 .
- FIG. 6 provides a non-exhaustive list of example applications/services 600 that may use the per-token language labels 220 assigned to the codemixed text 200 by the language identifier 150 .
- the applications 600 may execute on the remote system 140 and/or the user device 102 .
- a language translator 600 a i.e., machine translation applications uses the per-token language labels 220 assigned to codemixed text 200 for accurately translating words from one language to another. For instance, consider the codemixed text “Como se llama un squirrel en Espa ⁇ ol” that requests a Spanish translation for the English word squirrel.
- per-token language identification Without per-token language identification, the language translator 600 a is left to assume that the entire input is either entirely English or entirely Spanish, and therefore, would not be able to process the requested translation.
- Content/Sentiment Analyzer applications 600 b also provide increased performance when using the per-token language labels to analyze the sentiment or mood of codemixed texts for product/service reviews entered by users.
- the scope of the content associated with codemixed text entered on social media posts or message boards may be obtained using the per-token language labels.
- text to speech models 600 c used by dialog-based applications, such as navigation applications or voice assistant applications may use per-token language labels when converting codemixed text to speech so that each and every word is pronounced properly.
- search result ranking applications 600 d may use per-token language labels for search queries containing codemixed text to provide more accurate search rankings.
- the feed-forward neural network model 300 may inaccurately predict probability distributions over possible languages for tokens associated with informal texts, such as misspellings and slang, when all three types of features (i.e., character n-gram features 210 a , script features 210 b , and lexicon features 210 c ) are simply merged together. For instance, when predicting languages for a misspelled word Ennnnglish, the model 300 when merging both character n-gram features 210 a and lexicon features 210 c does not make effective use of the character n-gram features 210 a and predicts a catastrophically wrong probability distribution over possible languages for the token associated with the misspelled word Ennnglish.
- the model 300 with character n-gram features 210 a and lexicon features 210 c may output a probability distribution 400 for the Ennnglish token 202 that includes a top three probability scores p of 0.27, 0.24, 0.18 for Swedish (sv), Danish (dn), and Dutch (nl), respectively.
- the model 300 is able to maximize the use of the Ennnglish token's 202 character n-gram features 210 a and make an accurate prediction that the misspelled word belongs to the English context.
- the feed-forward neural network model 300 without the lexicon features 210 c includes a top three probability scores p of 0.74, 0.10, 0.06 for English (en), Dutch (nl), and Frisian (fl), respectively.
- the prevalent and highly predictive traits associated with lexicon features 210 c for per-token language identification also results in dampening to the updating of weights of the character n-gram features 210 a during training of the model 300 . This dampening during training diminishes the overall utility of the character n-gram features 210 a for per-token language identification.
- the language identifier 150 selectively applies a grouped feature dropout strategy that stochastically down-weights lexicon features 210 c received as feature inputs during training of the feed-forward neural network model 300 .
- the grouped feature dropout strategy sets the vector associated with lexicon features 210 c to zero for a subset of the lexicon features 210 c .
- the model 300 is relying entirely on the extracted character n-gram features 210 a for the corresponding token 202 (and also the n-gram features 210 a of any adjacent tokens 202 ).
- selectively applying the grouped feature dropout strategy (e.g., a lexicon feature dropout strategy) on a subset of the lexicon features 210 c includes a selected dropout rate in which the vector associated with the lexicon features 210 c is set to zero.
- the selected dropout rate may ranges from 20-percent (20%) to 100-percent (100%) in some examples. In other examples, the selected dropout rate ranges from 25-percent (25%) to 75-percent (75%). In some examples, a 30-percent (30%) dropout rate is selected to improve accuracy during training of misspelled tokens.
- FIG. 7A provides an example plot 700 a depicting accuracy of the model 300 for predicting misspelled tokens as well as a corpus of codemixed texts (GY-Mix) based on different dropout rates for lexicon features.
- the GY-Mix of codemixed texts is sampled from public posts from Google+ and from YouTube that includes three different language pairs of codemixed text: English-Spanish; English-Hindi; and English-Indonesian.
- the plot 700 a shows that misspelled tokens are only accurately predicted 80.5-percent (80.5%) of the time, indicating that n-gram features 210 a are not properly trained.
- the performance of the model 300 for accurately predicting misspelled tokens is robust across dropout rates from 30% (0.3) to 100% (1), indicating that the dropout strategy is effective and not highly sensitive.
- the model 300 may also be trained on synthetic codemixed data covering 110 languages and a monolingual corpus (KB-Mono 54) covering 54 languages. Every token in the training set spawns a training instance.
- the synthetic codemixed data can include samples from public Twitter posts (Twitter-Mix) that codemix between Spanish and English and samples from multilingual web pages (Web-Mix6) that codemix between six languages (Czech, Spanish, Basque, Hungarian, Croatian, Slovac).
- FIG. 7B provides an example table 700 b comparing accuracy of the neural network model with lexicon features (CMX), the neural network model without lexicon features (CMX-small), an EquiLID model, and a LanideNN model for each of the Twitter-Mix, Web-Mix6, and GY-Mix training datasets.
- the neural network models CMX and CMX-small are both more accurate than the benchmark EquiLID and LanideNN models.
- the LanideNN model makes a prediction for every character.
- the neural network model 300 with lexicon features (CMC) is 29.6% more accurate than EquiLID (93.5% vs. 73.9%) and achieves evendrop greater accuracy than the LanideNN model.
- CMC lexicon features
- CMX lexicon features
- CMX-small the neural network model without lexicon features
- EquiLID model the EquiLID model
- LanideNN model the LanideNN model
- Langid.py model for sentence-level language predictions on the KB-Mono54 training dataset.
- FIG. 8 is a flowchart of an example arrangement of operations for a method 800 of per-token language identification in codemixed text.
- the data processing hardware 104 , 144 may execute the operations for the method 800 by executing instructions stored on the memory hardware 106 , 146 .
- the method 800 includes receiving, at the data processing hardware, codemixed text 200 .
- the codemixed text 200 contains multiple languages and may be either one of intra-mix text 200 a or inter-mix text 200 b .
- the method 800 includes segmenting/parsing, by the data processing hardware, the codemixed text 200 into a plurality of tokens 202 . Each token includes at least one character and is delineated from any adjacent tokens by a space.
- the method 800 includes extracting, by the data processing hardware, features 210 from the token 202 .
- the features may include at least one of character features 210 a , script features 210 b , or lexicon features 210 c .
- the method 800 includes predicting, by the data processing hardware, a probability distribution 400 over possible languages for the token 202 using a language identifier model 300 (i.e., feed-forward neural network model) configured to receive the extracted features 210 from the token 202 as feature inputs 210 .
- a language identifier model 300 i.e., feed-forward neural network model
- the language identifier model 300 may include an embedding layer 302 , a hidden layer 304 interconnected to the embedding layer 302 in a feed-forward manner, and an output layer 306 interconnected with the hidden layer 304 in the feed-forward manner.
- the embedding layer 302 is configured to: receive the feature inputs 210 , each including a sparse matrix; map the sparse matrix of each respective feature input 210 to dense embedding vectors, resulting in a learned embedding matrix; and concatenate each learned embedding matrix corresponding to each received feature input 210 .
- the hidden layer 304 is configured to receive the embedding layer 302 and apply a rectified linear unit (ReLU) of the embedding layer 302 .
- the output layer 306 is configured to output a probability distribution 400 for each possible language for each respective token 202 .
- the output layer 306 may include a softmax layer.
- the method 800 includes assigning, by the data processing hardware, a language to each token 202 of the codemixed text 200 by executing a greedy search on the probability distribution over the possible languages predicted for each respective token 202 .
- the method receives an assignment constraint 510 that assumes at least one language assigned to at least one token 202 of the codemixed text 200 .
- the data processing hardware may execute a greedy decoding algorithm that assigns the language to each token 202 by selecting the language having a greatest probability distribution 400 for the respective token relative to any adjacent tokens 202 based on the assignment constraint 510 .
- the assignment constraint 510 may include at least one fixed set of language pairs permissible for assignment to each token of the codemixed text 200 .
- a software application may refer to computer software that causes a computing device to perform a task.
- a software application may be referred to as an “application,” an “app,” or a “program.”
- Example applications include, but are not limited to, system diagnostic applications, system management applications, system maintenance applications, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and gaming applications.
- FIG. 9 is schematic view of an example computing device 900 that may be used to implement the systems and methods described in this document.
- the computing device 900 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.
- the components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
- the computing device 900 includes a processor 910 (e.g., data processing hardware 104 , 144 ), memory 920 (e.g., memory hardware 106 , 146 ), a storage device 930 , a high-speed interface/controller 940 connecting to the memory 920 and high-speed expansion ports 950 , and a low speed interface/controller 960 connecting to a low speed bus 970 and a storage device 930 .
- a processor 910 e.g., data processing hardware 104 , 144
- memory 920 e.g., memory hardware 106 , 146
- storage device 930 e.g., a high-speed interface/controller 940 connecting to the memory 920 and high-speed expansion ports 950
- a low speed interface/controller 960 connecting to a low speed bus 970 and a storage device 930 .
- Each of the components 910 , 920 , 930 , 940 , 950 , and 960 are interconnected using various bus
- the processor 910 can process instructions for execution within the computing device 900 , including instructions stored in the memory 920 or on the storage device 930 to display graphical information for a graphical user interface (GUI) on an external input/output device, such as display 980 coupled to high speed interface 940 .
- GUI graphical user interface
- multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory.
- multiple computing devices 900 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).
- the memory 920 stores information non-transitorily within the computing device 900 .
- the memory 920 may be a computer-readable medium, a volatile memory unit(s), or non-volatile memory unit(s).
- the non-transitory memory 920 may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by the computing device 900 .
- non-volatile memory examples include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs).
- volatile memory examples include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.
- the storage device 930 is capable of providing mass storage for the computing device 900 .
- the storage device 930 is a computer-readable medium.
- the storage device 930 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.
- a computer program product is tangibly embodied in an information carrier.
- the computer program product contains instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 920 , the storage device 930 , or memory on processor 910 .
- the high speed controller 940 manages bandwidth-intensive operations for the computing device 900 , while the low speed controller 960 manages lower bandwidth-intensive operations. Such allocation of duties is exemplary only.
- the high-speed controller 940 is coupled to the memory 920 , the display 980 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 950 , which may accept various expansion cards (not shown).
- the low-speed controller 960 is coupled to the storage device 930 and a low-speed expansion port 990 .
- the low-speed expansion port 990 which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- input/output devices such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- the computing device 900 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 900 a or multiple times in a group of such servers 900 a , as a laptop computer 900 b , or as part of a rack server system 900 c.
- implementations of the systems and techniques described herein can be realized in digital electronic and/or optical circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof.
- ASICs application specific integrated circuits
- These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- the processes and logic flows described in this specification can be performed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data
- a computer need not have such devices.
- Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- one or more aspects of the disclosure can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- Other kinds of devices can be used to provide interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input
Abstract
Description
Xg∈RF
where Fg is a number of feature templates and Vg is the vocabulary size of the corresponding feature group. The embedding
Eg∈RF
Lastly, the embedding
Claims (26)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/976,647 US10579733B2 (en) | 2018-05-10 | 2018-05-10 | Identifying codemixed text |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/976,647 US10579733B2 (en) | 2018-05-10 | 2018-05-10 | Identifying codemixed text |
Publications (2)
Publication Number | Publication Date |
---|---|
US20190347323A1 US20190347323A1 (en) | 2019-11-14 |
US10579733B2 true US10579733B2 (en) | 2020-03-03 |
Family
ID=68464860
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/976,647 Active 2038-08-17 US10579733B2 (en) | 2018-05-10 | 2018-05-10 | Identifying codemixed text |
Country Status (1)
Country | Link |
---|---|
US (1) | US10579733B2 (en) |
Families Citing this family (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9652896B1 (en) | 2015-10-30 | 2017-05-16 | Snap Inc. | Image based tracking in augmented reality systems |
US9984499B1 (en) | 2015-11-30 | 2018-05-29 | Snap Inc. | Image and point cloud based tracking and in augmented reality systems |
JP6819988B2 (en) * | 2016-07-28 | 2021-01-27 | 国立研究開発法人情報通信研究機構 | Speech interaction device, server device, speech interaction method, speech processing method and program |
US10074381B1 (en) | 2017-02-20 | 2018-09-11 | Snap Inc. | Augmented reality speech balloon system |
US10387730B1 (en) * | 2017-04-20 | 2019-08-20 | Snap Inc. | Augmented reality typography personalization system |
US10997760B2 (en) | 2018-08-31 | 2021-05-04 | Snap Inc. | Augmented reality anthropomorphization system |
US11972529B2 (en) | 2019-02-01 | 2024-04-30 | Snap Inc. | Augmented reality system |
US11875102B2 (en) | 2020-02-14 | 2024-01-16 | International Business Machines Corporation | Automatic font selection |
US20230161977A1 (en) * | 2021-11-24 | 2023-05-25 | Beijing Youzhuju Network Technology Co. Ltd. | Vocabulary generation for neural machine translation |
Citations (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6047251A (en) | 1997-09-15 | 2000-04-04 | Caere Corporation | Automatic language identification system for multilingual optical character recognition |
US7917361B2 (en) | 2004-09-17 | 2011-03-29 | Agency For Science, Technology And Research | Spoken language identification system and methods for training and operating same |
US20110246180A1 (en) * | 2010-04-06 | 2011-10-06 | International Business Machines Corporation | Enhancing language detection in short communications |
US20120330989A1 (en) * | 2011-06-24 | 2012-12-27 | Google Inc. | Detecting source languages of search queries |
US20140052436A1 (en) * | 2012-08-03 | 2014-02-20 | Oracle International Corporation | System and method for utilizing multiple encodings to identify similar language characters |
US20150006148A1 (en) * | 2013-06-27 | 2015-01-01 | Microsoft Corporation | Automatically Creating Training Data For Language Identifiers |
US20150161227A1 (en) * | 2011-06-30 | 2015-06-11 | Google Inc. | Cluster-Based Language Detection |
US9483768B2 (en) | 2014-08-11 | 2016-11-01 | 24/7 Customer, Inc. | Methods and apparatuses for modeling customer interaction experiences |
US9779085B2 (en) | 2015-05-29 | 2017-10-03 | Oracle International Corporation | Multilingual embeddings for natural language processing |
-
2018
- 2018-05-10 US US15/976,647 patent/US10579733B2/en active Active
Patent Citations (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6047251A (en) | 1997-09-15 | 2000-04-04 | Caere Corporation | Automatic language identification system for multilingual optical character recognition |
US7917361B2 (en) | 2004-09-17 | 2011-03-29 | Agency For Science, Technology And Research | Spoken language identification system and methods for training and operating same |
US20110246180A1 (en) * | 2010-04-06 | 2011-10-06 | International Business Machines Corporation | Enhancing language detection in short communications |
US20120330989A1 (en) * | 2011-06-24 | 2012-12-27 | Google Inc. | Detecting source languages of search queries |
US20150161227A1 (en) * | 2011-06-30 | 2015-06-11 | Google Inc. | Cluster-Based Language Detection |
US20140052436A1 (en) * | 2012-08-03 | 2014-02-20 | Oracle International Corporation | System and method for utilizing multiple encodings to identify similar language characters |
US20150006148A1 (en) * | 2013-06-27 | 2015-01-01 | Microsoft Corporation | Automatically Creating Training Data For Language Identifiers |
US9483768B2 (en) | 2014-08-11 | 2016-11-01 | 24/7 Customer, Inc. | Methods and apparatuses for modeling customer interaction experiences |
US9779085B2 (en) | 2015-05-29 | 2017-10-03 | Oracle International Corporation | Multilingual embeddings for natural language processing |
Non-Patent Citations (3)
Title |
---|
An Off-the-shelf Language Identification Tool, Marco Lui, Baldwin, Jul. 2012. |
Incorporating Dialectal Variability for Socially Equitable Language Indentification, Jurgens et. al, Jul. 2017. |
Multilingual Language Indentification on Character Window, Tom Kocmi, Apr. 3-7, 2017. |
Also Published As
Publication number | Publication date |
---|---|
US20190347323A1 (en) | 2019-11-14 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10579733B2 (en) | Identifying codemixed text | |
US10140371B2 (en) | Providing multi-lingual searching of mono-lingual content | |
Rijhwani et al. | Estimating code-switching on twitter with a novel generalized word-level language detection technique | |
KR102329127B1 (en) | Apparatus and method for converting dialect into standard language | |
US8660834B2 (en) | User input classification | |
US8972432B2 (en) | Machine translation using information retrieval | |
US20070021956A1 (en) | Method and apparatus for generating ideographic representations of letter based names | |
US20050216253A1 (en) | System and method for reverse transliteration using statistical alignment | |
Torisawa | Inducing gazetteers for named entity recognition by large-scale clustering of dependency relations | |
JP6090531B2 (en) | How to get word translation | |
US8423350B1 (en) | Segmenting text for searching | |
Zhang et al. | A fast, compact, accurate model for language identification of codemixed text | |
US11615779B2 (en) | Language-agnostic multilingual modeling using effective script normalization | |
US7136803B2 (en) | Japanese virtual dictionary | |
US10049108B2 (en) | Identification and translation of idioms | |
Freihat et al. | Towards an optimal solution to lemmatization in Arabic | |
US10354013B2 (en) | Dynamic translation of idioms | |
Naz et al. | Urdu part of speech tagging using transformation based error driven learning | |
Riyadh et al. | Joint approach to deromanization of code-mixed texts | |
Che et al. | A word segmentation method of ancient Chinese based on word alignment | |
de Mendonça Almeida et al. | Evaluating phonetic spellers for user-generated content in Brazilian Portuguese | |
CN112307183B (en) | Search data identification method, apparatus, electronic device and computer storage medium | |
Claeser et al. | Token level code-switching detection using Wikipedia as a lexical resource | |
US10055401B2 (en) | Identification and processing of idioms in an electronic environment | |
Oo et al. | itextmm: Intelligent text input system for myanmar language on android smartphone |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
FEPP | Fee payment procedure |
Free format text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
AS | Assignment |
Owner name: GOGGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:RIESA, JASON;GILLICK, DANIEL;ZHANG, YUAN;AND OTHERS;REEL/FRAME:045775/0889Effective date: 20180508 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |