CN109547790B - Apparatus and method for processing partition mode in high efficiency video codec - Google Patents
Apparatus and method for processing partition mode in high efficiency video codec Download PDFInfo
- Publication number
- CN109547790B CN109547790B CN201811249137.5A CN201811249137A CN109547790B CN 109547790 B CN109547790 B CN 109547790B CN 201811249137 A CN201811249137 A CN 201811249137A CN 109547790 B CN109547790 B CN 109547790B
- Authority
- CN
- China
- Prior art keywords
- codeword
- partition
- mode
- current block
- enabled
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/17—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object
- H04N19/176—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object the region being a block, e.g. a macroblock
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/103—Selection of coding mode or of prediction mode
- H04N19/107—Selection of coding mode or of prediction mode between spatial and temporal predictive coding, e.g. picture refresh
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/103—Selection of coding mode or of prediction mode
- H04N19/109—Selection of coding mode or of prediction mode among a plurality of temporal predictive coding modes
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/119—Adaptive subdivision aspects, e.g. subdivision of a picture into rectangular or non-rectangular coding blocks
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
- H04N19/157—Assigned coding mode, i.e. the coding mode being predefined or preselected to be further used for selection of another element or parameter
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/60—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using transform coding
- H04N19/61—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using transform coding in combination with predictive coding
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/70—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals characterised by syntax aspects related to video coding, e.g. related to compression standards
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/90—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using coding techniques not provided for in groups H04N19/10-H04N19/85, e.g. fractals
- H04N19/96—Tree coding, e.g. quad-tree coding
Abstract
Embodiments of the present invention relate to an apparatus and method for processing a partition mode in an efficient video codec, and disclose a method for providing a reduced number of bits to represent the partition mode in some cases of the efficient video codec. A single conditional statement is evaluated once to determine if an asymmetric motion partition ("AMP") can be used in the current codec tree block. When the AMP flag indicates (851, 853) that AMP is not enabled, then the two-bit codeword (852, 854) is encoded or decoded for the inter-mode partition of the current codec tree block in the bitstream.
Description
The present application is a divisional application of the inventive patent application with the application date of 2013, 7, 17, 201380049417.0, entitled "apparatus and method for processing partition modes in high efficiency video codec".
Technical Field
The present disclosure relates generally to video coding and, more particularly, to systems, devices, and methods for providing a reduced number of bits to represent a partition mode in some cases of high efficiency video coding ("HEVC").
Background
Video compression uses block processing for many operations. In the block processing, blocks of adjacent pixels are grouped into codec units, and the compression operation treats the group of pixels as one unit to utilize correlation between adjacent pixels within the codec unit. Block-based processing often includes predictive and transform codecs. Transform coding with quantization is a type of data compression that is often "lossy" because the quantization of transform blocks taken from the source image often discards data associated with transform blocks in the source image, thereby reducing its bandwidth requirements, and also often results in quality loss when reproducing the original transform blocks from the source image.
Motion picture expert group advanced video coding ("MPEG-4 AVC"), also known as h.264, is an established video compression standard that uses transform coding in block processing. In h.264, an image is divided into macroblocks ("MBs") of 16×16 pixels. Each MB is often further divided into smaller blocks. Intra-picture or inter-picture prediction is used to predict blocks of size equal to or smaller than one MB and spatial transform along with quantization is applied to the prediction residual. The quantized residual transform coefficients are typically encoded using entropy coding methods (e.g., variable length coding or arithmetic coding). Context-adaptive binary arithmetic coding ("CABAC") is introduced into h.264 by combining an adaptive binary arithmetic coding technique with a set of context modes to provide substantially lossless compression efficiency. Contextual mode selection plays a role in CABAC in providing a degree of adaptation and redundancy reduction. 264 defines two scan patterns over a two-dimensional block. Zig-zag scanning is used for images that are encoded using progressive video compression techniques, and alternative scanning is used for images that are encoded using interlaced video compression techniques.
The international video codec standard HEVC developed to take over h.264 expands the transform block size to 16 x 16 and 32 x 32 pixels to benefit high definition video codec. HEVC may also use a variety of scan patterns including zig-zag scanning.
Within video compression standards such as HEVC, codec mechanisms for reducing spatial and temporal redundancy are desired. Efforts are underway to increase the efficiency of encoders and decoders ("codecs") that compress and decompress, respectively, video data streams. Since the purpose of the codec is to reduce the size of the compressed digital video frames, efficient storage and communication of video, development of hardware of the codec, and continued development of encoding and decoding processes are facilitated.
Drawings
While the appended claims set forth the features of the technology herein with particularity, these techniques, together with their objects and advantages, may be best understood from the following detailed description taken in conjunction with the accompanying drawings of which:
FIG. 1A is a video system in which various embodiments of the present disclosure may be used;
FIG. 1B is a computer system upon which embodiments of the present disclosure may be implemented;
FIGS. 2A, 2B, 3A and 3B illustrate some video encoding principles according to embodiments of the present disclosure;
fig. 4A and 4B illustrate possible architectures for an encoder and decoder according to embodiments of the present disclosure;
fig. 5A and 5B illustrate additional video coding principles according to embodiments of the present disclosure;
fig. 6 illustrates some video encoding principles according to an embodiment of the present disclosure;
FIG. 7 illustrates an example of a binarization table describing bit allocation for partition modes according to an embodiment of the present disclosure; and is also provided with
Fig. 8 illustrates an example of a binarization table describing bit allocation for partition modes according to an embodiment of the present disclosure.
Detailed Description
Turning to the drawings, wherein like reference numerals refer to like elements, the techniques of the present disclosure are illustrated as being implemented in a suitable environment. The following description is based on embodiments of the claims and should not be construed as limiting the claims with respect to alternative embodiments not explicitly described herein.
In this disclosure, the term "codec" refers to both encoding that occurs at an encoder and decoding that occurs at a decoder. Similarly, the term codec refers to an encoder, a decoder, or a combined codec. The terms codec, coder, decoder, and codec all refer to a particular machine designed to codec (encode or decode) image or video data consistent with the present disclosure. Image and video data is typically composed of three parts: one for representing the luminance component of the pixel and two chrominance components for the color information of the pixel.
The discussion herein begins with a very brief overview of some terms and techniques known in the art of digital image compression. This summary is not intended to teach known technology about any detail. Those skilled in the art will know how to find more details in textbooks and related standards.
In one aspect of the present disclosure, a method is disclosed that includes: a single conditional statement is evaluated once to determine whether an asymmetric motion partition ("AMP") can be used in the current codec tree block, and a two-bit codeword for an inter-mode partition of the current codec tree block in the bitstream is encoded or decoded when an AMP flag indicates AMP is not enabled.
Examples of video systems in which embodiments of the present disclosure may be used are now described. It should be understood that the elements depicted in the figures as functional blocks may be implemented in hardware, software, or a combination thereof. Furthermore, embodiments of the present disclosure may also be employed on other systems, such as personal computers, smart phones, or tablet computers.
Referring to fig. 1A, a video system, generally designated 10, may include a front end 100 of a cable television network. Front end 100 may be configured to deliver video content to neighborhoods 129, 130, 131. Front-end 100 may operate within a hierarchy of front-ends, where a higher hierarchy front-end typically has more powerful functionality. The front end 100 may be communicatively linked to a satellite dish 112 and receive video signals therefrom for non-local programming. The headend 100 may also be communicatively linked to a local station 114, the local station 114 delivering local programs to the headend 100. The front end 100 may include: a decoder 104 that decodes video signals received from a satellite dish 112, an off-air (off-air) receiver 106 that receives local programming from a local station 114, a switch 102 that routes data traffic between the various components of the front end 100, an encoder 116 that encodes video signals for delivery to users, a modulator 118 that modulates signals for delivery to users, and a combiner 120 that combines the various signals into a single, multi-channel transmission.
The front end 100 may also be communicatively linked to a hybrid fiber optic cable ("HFC") network 122.HFC network 122 may be communicatively linked to a plurality of nodes 124, 126, 128. Each of the nodes 124, 126, 128 may be linked to one of the neighbors 129, 130, 131 by coaxial cable and deliver cable signals to that neighbor. One of the neighborhoods 130 of FIG. 1A is shown in more detail. Neighborhood 130 may include a plurality of living areas including living being 132. Within residence 132, set top box 134 may be communicatively linked to video display 136. The set top box 134 may include a first decoder 138 and a second decoder 140. The first decoder 138 and the second decoder 140 may be communicatively linked to a user interface 142 and a mass storage device 144. The user interface 142 may be communicatively linked to the video display 136.
During operation, head end 100 may receive local and non-local program video signals from satellite dish 112 and local station 114. The non-native program video signal may be received in the form of a digital video stream, while the native program video signal may be received as an analog video stream. In some embodiments, the local program may also be received as a digital video stream. The digital video stream may be decoded by decoder 104 and sent to switch 102 in response to a user request. The front end 100 may also include a server 108 communicatively linked to a mass storage device 110. The mass storage device 110 may store various types of video content, including video on demand ("VOD"), and the server 108 may retrieve the video content and provide it to the switcher 102. The switch 102 may route the local program directly to the modulator 118 that modulates the local program and may route non-local programs (including any VOD) to the encoder 116. Encoder 116 may digitally encode non-local programs. The encoded non-local program may then be transmitted to modulator 118. Combiner 120 may be configured to receive modulated analog video data and modulated digital video data, combine the video data, and transmit them over multiple radio frequency channels to HFC network 122.
The encoder 116 and decoder 138 (and all other steps and functions described herein) of fig. 1A may be implemented as computer code comprising computer readable instructions stored on a computer readable storage device, such as a memory or another type of storage device. The computer code may be executed on a computer system by a processor such as an application specific integrated circuit, or other type of circuit. For example, the computer code for implementing encoder 116 may be executed on a computer system (such as a server) residing in front end 100. Alternatively, the computer code for the decoders 138, 140 may be executed on the set top box 134, the set top box 134 constituting one type of computer system. Code may exist as a software program comprising program instructions in source code, object code, executable code or other formats. It should be appreciated that the computer code for the various components shown in FIG. 1A may reside anywhere in the system 10 or elsewhere (e.g., in a cloud network) determined to be desirable or advantageous. Furthermore, the computer code may be located in one or more components so long as the instructions are efficiently executable by the one or more components.
FIG. 1B illustrates an example of a computer system on which computer code for the encoder 116 and decoders 138, 140 may be executed. A computer system, generally designated 400, includes a processor 401, or processing circuitry, that may implement or execute software instructions that perform some or all of the methods, functions, and other steps described herein. Commands and data from the processor 401 may be transferred, for example, through a communication bus 403. Computer system 400 may also include a computer readable storage device 402, such as random access memory, for software and data for processor 401 to reside therein during execution. The storage device 402 may also include non-volatile data storage. Computer system 400 may include a network interface 404 for connecting to a network. Other known electronic components may be added to or substituted for the components depicted in computer system 400. The computer system 400 may reside in the front end 100 and execute the encoder 116, and may also be embodied in the set top box 134 to execute the decoders 138, 140. In addition, computer system 400 may reside elsewhere than in front end 100 and set top box 134 and may be miniaturized for integration into a smart phone or tablet computer.
Video coding systems may achieve compression by removing redundancy in video data, for example by removing those elements that may be discarded but do not significantly adversely affect reproduction fidelity. Since video signals occur in time and space, most video coding systems exploit the temporal and spatial redundancies that exist in these signals. In general, there is a high degree of temporal correlation between successive frames. This is also true in the spatial domain for pixels that are close to each other. Thus, a high compression gain is achieved by carefully exploiting these spatio-temporal correlations.
A high-level description of how video data is encoded and decoded by encoder 116 and decoders 138, 140 in embodiments of the present disclosure is now provided. In this embodiment, the encoder and decoder operate according to the HEVC method. HEVC is a block-based hybrid spatial and temporal prediction coding method. In HEVC, the input image is first divided into square blocks, called "LCUs" (for the largest codec unit) or "CTBs" (for the codec tree blocks), as shown in fig. 2A. Unlike other video coding standards in which the basic codec unit is a macroblock of 16×16 pixels, in HEVC, the LCU may be as large as 128×128 pixels. One LCU may be divided into four square blocks, called "CUs" (codec units), which may be one quarter of the LCU size. Each CU may be further partitioned into four smaller CUs, which are one-fourth the size of the original CU. The segmentation process may be repeated until certain criteria are met. Fig. 3A shows an example of LCUs partitioned into CUs. In general, for HEVC, the smallest CU used (e.g., leaf node as described in further detail below) is considered to be one CU.
How a particular LCU is partitioned into CUs may be represented by a quadtree. At each node of the quadtree, the flag is set to "1" if the node is further partitioned into child nodes. Otherwise, the flag is not set to "0". For example, the LCU partition of fig. 3A may be represented by the quadtree of fig. 3B. These "split flags" may be jointly encoded with other flags in the video bitstream, including skip mode flags, merge mode signals, prediction unit ("PU") mode flags, and the like. In the case of the quadtree of fig. 3B, the split flag 10100 may be encoded as overhead along with other flags. Syntax information for a given CU may be defined recursively and may depend on whether the CU is partitioned into sub-CUs.
An undivided node (e.g., a node corresponding to a terminal or "leaf" node in a given quadtree) may include one or more PUs. In general, a PU represents all or a portion of a corresponding CU, and includes data for acquiring reference samples for the PU for the purpose of performing prediction for the CU. Thus, at each leaf of the quadtree, a 2nx2n CU may have one of four possible patterns (nxn, nx2n, 2nxn, 2nx2n), as shown in fig. 2B. Although shown for a 2n×2n CU, other PUs having different sizes and corresponding patterns (e.g., square or rectangular) may be used, as shown in fig. 6. The CU may be predictive codec either spatially or temporally. If a CU is coded in intra mode, each PU of the CU may have its own spatial prediction direction. If a CU is coded in inter mode, each PU of the CU may have its own motion vector and associated reference picture. The data defining the motion vector may describe, for example, a horizontal component of the motion vector, a vertical component of the motion vector, a resolution for the motion vector (e.g., one-quarter pixel precision or 1/8 pixel precision), a reference frame to which the motion vector points, or a reference list for the motion vector (e.g., list 0 or list 1). In addition, a motion vector predictor index may be used to identify a motion vector predictor (e.g., a motion vector ("MV") of a left neighbor, an MV of a co-located neighbor. The data for a CU defining one or more PUs of the CU may also describe, for example, partitioning the CU into one or more PUs. The partition modes may differ depending on whether the CU is uncoded, intra prediction mode coded, or inter prediction mode coded.
Generally, in intra-prediction coding, there is a high level of spatial correlation between neighboring blocks in a frame. Thus, a block may be predicted from nearby encoded and reconstructed blocks, resulting in intra prediction. In some embodiments, the prediction may be formed by a weighted average of previously encoded samples positioned above or to the left of the current block. The encoder may select a mode that minimizes the difference and cost between the original and the prediction and signal the selection in the control data.
In general, in inter-frame prediction coding, a video sequence has a high degree of temporal correlation between frames, so that a block in a current frame can be accurately described by a region (or two regions in the case of bi-prediction) in a previously-coded frame, which is referred to as a reference frame. Inter prediction exploits previously encoded and reconstructed reference frames to develop predictions using block-based motion estimation and compensation techniques.
Quantization of the transform system may be performed after intra-prediction or inter-prediction encoding to generate prediction data and residual data, and after any transform (such as a 4 x 4 or 8 x 8 integer transform or discrete cosine transform ("DCT") used in h.264/AVC) is performed to generate transform coefficients. In some embodiments, any transform operations may be bypassed, for example, using a transform skip mode in HEVC. Quantization generally refers to a process of quantizing transform coefficients so that it is possible to reduce the amount of data used to represent a system, for example, by converting high-precision transform coefficients into a limited number of possible values. These steps are discussed in more detail below.
Each CU may also be divided into transform units ("TUs"). In some embodiments, a block transform operation is performed on one or more TUs to decorrelate pixels within the block and compress block energy into low-order coefficients of the transformed block. In some embodiments, an 8×8 or 4×4 transform may be applied. In other embodiments, a different size set of block transforms may be applied to a CU, as shown in fig. 5A, where the left block is the CU partitioned into PUs and the right block is the associated set of TUs. The size and location of each block transform within a CU is described by a separate quadtree. Fig. 5B shows a quadtree representation of TUs for a CU in the example of fig. 5A. In this example, 11000 is encoded and decoded as part of the overhead and transmitted. As understood, the CU, PU, and TU sizes may be n×n or m×n, where n+.m.
TUs and PUs of any given CU may be used for different purposes. TUs are typically used for transform, quantization, and codec operations, while PUs are typically used for spatial and temporal prediction. For a given CU, there is not necessarily a direct relationship between the number of PUs and the number of TUs.
A video block may comprise a block of pixel data in the pixel domain or a block of transform coefficients in the transform domain, e.g. after a transform such as a DCT, an integer transform, a wavelet transform, or a conceptually similar transform is applied to residual data for a given block of video data, where the residual data represents pixel differences between the video data for the block and the prediction data generated for the block. In some cases, a video block may include a block of quantized transform coefficients in the transform domain, wherein after a transform is applied to residual data for given video data, the resulting transform coefficients are also quantized. In video coding, quantization is a step that introduces losses, so that a balance between bit rate and reconstruction quality can be established. These steps are discussed further below.
Block partitioning serves an important purpose in block-based video codec technology. Encoding video data using smaller blocks may result in better prediction of data for the position of video frames that include high levels of detail, and thus may reduce the final error represented by residual data (e.g., deviation of the predicted data from the source video data). In general, prediction exploits spatial or temporal redundancy in video sequences by modeling correlations between blocks of samples of various sizes, so that only small differences between actual and predicted signals need to be encoded. A prediction for the current block is created from the samples that have been encoded. While it is possible to reduce residual data, such techniques may require additional syntax information to indicate how smaller blocks are partitioned relative to video frames, and may result in increased decoded video bit rates. Thus, in some techniques, block partitioning may be dependent on balancing the desired reduction in residual data against the final increase in bit rate in the decoded video data due to additional syntax information.
In general, a block and its various partitions (e.g., sub-blocks) may be considered a video block. In addition, a slice may be considered as a plurality of video blocks (e.g., macro blocks or codec units) or sub-blocks (partitions of macro blocks or sub-codec units, such as sub-blocks of PUs, TUs, etc.). Each slice may be an independently decodable unit of a video frame. Alternatively, the frame itself may be a decodable unit, or other portions of the frame may be defined as decodable units. Furthermore, a group of images may be defined as a decodable unit.
According to an embodiment of the present disclosure, the encoder 116 of fig. 1A may be comprised of several functional modules as shown in fig. 4A. These modules may be implemented as hardware, software, or any combination of the two. Given the current PU, x, the predicted PU, x' may be obtained first by spatial prediction or temporal prediction. The spatial or temporal prediction may be performed by spatial prediction module 129 or by temporal prediction module 130, respectively.
There are several possible spatial prediction directions that spatial prediction module 129 may perform on a PU-by-PU basis, including horizontal, vertical, 45 degree diagonal, 135 degree diagonal, planar, etc. In general, spatial prediction is performed differently for luma PU and chroma PU, e.g., by luma intra prediction mode or chroma intra prediction mode. Syntax indicates the spatial prediction direction per PU.
The encoder 116 may perform temporal prediction by a motion estimation operation. In particular, the temporal prediction module 130 may search for a best match prediction for the current PU on the reference picture. Best match prediction can be described by MV and by associated reference picture ("refIdx"). Typically, a PU in a B picture may have up to two MVs. Both MV and refIdx may be part of the syntax in the bitstream.
The predicted PU may then be subtracted from the current PU, resulting in a residual PU, e. The residual CU, generated by grouping the residual PUs, E associated with the CU, may then be transformed by transform module 117, one TU at a time, resulting in residual PUs, E in the transform domain. To accomplish this, the transform module 117 may, for example, use square or non-square block transforms.
The transform coefficients E may then be quantized by the quantizer module 118 to convert the high-precision transform coefficients into a limited number of possible values. The quantization process may reduce the bit depth associated with some or all of the coefficients. For example, an n-bit value may be rounded down to an m-bit value during quantization, where n is greater than m. In some embodiments, the modified transform coefficients may be generated using external boundary conditions. For example, a lower range or value may be used in determining whether the transform coefficients are given a non-zero value or have just been zeroed. As should be appreciated, quantization is a lossy operation, and quantization loss is generally not recoverable.
The quantized coefficients may then be entropy encoded by the entropy encoding module 120, resulting in the final compressed bits. Specific steps performed by the entropy encoding module 120 will be discussed in more detail below. It should be noted that the above-described prediction, transformation, and quantization may be performed for any block of video data, e.g., for a PU or TU of a CU, or for a macroblock, depending on the codec standard performed.
To facilitate temporal and spatial prediction, the encoder 116 may also take quantized transform coefficients E and dequantize them with the dequantization module 122, producing dequantized transform coefficients E'. The inverse quantized transform coefficients are then inverse transformed by an inverse transform module 124, yielding a reconstructed residual PU, e'. The reconstructed residual PU, e ', is then added to the corresponding prediction, x ', either temporally or spatially, to form a reconstructed PU, x '.
Deblocking filter ("DFB") operations may be performed on the reconstructed PU, x' to first reduce blocking artifacts. A sample adaptive offset ("SAO") process may be conditionally performed after the deblocking filter process for the decoded image is completed, which compensates for pixel value offset between reconstructed pixels and original pixels. In some embodiments, both DBF operation and SAO procedures are implemented by an adaptive loop filter function that may be conditionally performed over the reconstructed PU by loop filter module 126. In some embodiments, the adaptive loop filter function minimizes codec distortion between the input and output images. In some embodiments, the loop filter module 126 operates during inter-image prediction loops. If the reconstructed pictures are reference pictures, they may be stored in a reference buffer 128 for future temporal prediction.
HEVC specifies two loop filters that are applied in sequence with a DBF that is applied first and an SAO filter that is applied later. DBF is similar to that used by h.264/MPEG-4 AVC, but with simpler design and better support for parallel processing. In HEVC, DBF is only applicable to 8×8 sample cells, while with H.264/MPEG-4 AVC, DBF is applicable to 4×4 sample cells. The DBF uses an 8 x 8 sample grid because it does not lead to significant degradation and significantly improves parallel processing because the DBF no longer leads to cascading interactions with other operations. Another variation is that HEVC only allows three DBF intensities of 0 to 2. HEVC also requires that the DBF first apply horizontal filtering for vertical edges to the image, and only after that it applies vertical filtering for horizontal edges to the image. This allows multiple parallel threads to be used for the DBF.
The SAO filtering process is applied after the DBF and is done to allow a better reconstruction of the original signal amplitude by using e.g. a look-up table comprising parameters based on a histogram analysis by the encoder. SAO filters are of two basic types, the edge offset ("EO") type and the band offset ("BO") type, respectively. One of the SAO types may be applied CTB by CTB. EO types have four sub-types corresponding to processing along four possible directions (e.g., horizontal, vertical, 135 degrees, and 45 degrees). For a given EO subtype, EO processing operates by comparing the value of a pixel with two of its neighbors using one of four different gradient patterns. An offset is applied to the pixels in each of the four gradient patterns. For pixel values that are not in one of the gradient patterns, no offset is applied. The BO processing is directly based on the sample amplitude divided into 32 bands. The offset is applied to pixels in 16 of the 32 bands, where the group of 16 bands corresponds to the BO subtype. The SAO filter process is designed to reduce distortion compared to the original signal by adding an offset to the sample values. It can increase edge sharpness and reduce ringing and pulsing artifacts.
In an embodiment of the present disclosure, encoder 116 supports intra-pictures (such as I-pictures) and inter-pictures (such as P-pictures or B-pictures). The intra pictures can be encoded and decoded without reference to other pictures. Thus, spatial prediction may be used for CUs/PUs within an intra picture. The intra pictures provide points at which decoding may begin. Inter-frame images, on the other hand, are typically directed to high compression. Inter-frame images support both intra-frame prediction and inter-frame prediction. CU/PU in inter pictures are prediction coded spatially or temporally. The temporal reference is a previously encoded intra or inter picture.
When the decoders 138, 140 receive the bit streams, they perform the functions as shown in fig. 4B. The entropy decoding module 146 of the decoder 145 may decode the symbol values, significance map, and non-zero coefficients to reconstruct the quantized and transformed coefficients. In decoding the significance map, the entropy decoding module 146 may perform an inverse of the described process in conjunction with the entropy encoding module 120, thereby decoding the significance map along a scan pattern composed of scan lines. The entropy decoding module 146 may then provide the coefficients to an inverse quantizer module 147, which inverse quantizes the coefficient matrix, producing E'. The inverse quantizer module 147 may provide the inverse quantized coefficients to the inverse transform module 149. The inverse transform module 149 may perform an inverse transform operation on the coefficients, resulting in e'. Filtering and spatial prediction may be applied in the manner described in connection with fig. 4A.
As is well known to those skilled in the art, an encoder operates by encoding slices of a video stream. As described above, a slice may be considered to be a plurality of video blocks or sub-blocks. Each slice may be an independently or non-independently decodable unit of a video frame.
Referring now to fig. 6, some video encoding principles according to embodiments of the present disclosure are shown. In particular, different codec structures for PUs are shown. For intra-coding, square PUs of sizes 2n×2n and n×n may be used. For inter-frame codec, PU sizes of 2n×2n,2n× N, N ×2× 2N, N ×n, and AMP may be used. As described above, if PUs are encoded in intra mode, each PU may have its own spatial prediction direction. If the PUs are encoded in inter mode, each PU may have its own motion vector and associated reference picture.
AMP partitions include nr×2N, nL ×2N, 2nxnu, and 2nxnd, where R refers to "right", L refers to "left", U refers to "up", and D refers to "down". In AMP, partitioning typically occurs such that 3/4 of the blocks are located on one side of the partition and 1/4 of the blocks are located on the other side of the partition. AMP may be signaled typically by a flag such as "amp_enabled_flag". The encoder 116 may use a flag amp_enabled_flag indicating whether AMP may be used in encoding the decoding tree block. an amp_enabled_flag equal to 1 may specify an asymmetric motion partition, e.g., a PartMode equal to part_2nxnu, part_2nxnd, part_nl×2n, or part_nr×2n may be used in encoding the decoding tree block. an amp enabled flag equal to 0 may specify that asymmetric motion partitions cannot be used in encoding the decoding tree block.
In some embodiments, the flag amp_enabled_flag is set by user input and is located at the sequence parameter set header.
Fig. 7 illustrates an example binarization table 700 that describes bit allocation for partition modes in current HEVC. The binarization table 700 includes a category CuPredMode 710, a part_mode 720, a PartMode730, and a binary string 740. The CuPredMode 710 further includes sub-categories mode_intra 712 and mode_inter 714. Binary string 740 further includes sub-categories cLog2CbSize > Log2MinCbSize 750 and cLog2 cbsize= =log 2MinCbSize 760.cLog2 cbsize= Log2MinCbSize 760 further includes the sub-categories cLog2 cbsize= 3762 and cLog2CbSize >3 764. The binarization table 700 manages how the encoder 116 encodes CU headers and how the decoders 138, 140 decode CU headers.
The inputs to the PartMode process include a luma position (xC, yC) that specifies the upper left sample of the current luma codec block relative to the upper left luma sample of the current picture and a variable cLog2CbSize that specifies the current luma codec block size.
The output of this process is the binarization of the syntax element, as shown in columns 750, 762, 764. Depending on CuPredMode [ xC ] [ yC ]710 and cLog2CbSize, the binarization for PartMode730 is given by table 700.
At PartMode730, a part_mode flag may specify the partition mode of the current codec unit. Part_mode 720 refers to a partition mode value. PartMode730 and PartMode 720 generally refer to the same thing with different representations. For example, partMode730 refers to interpolation of a person, and part_Mode 720 refers to a digital value for a machine. The semantics of part_mode depend on CuPredMode [ x0] [ y0]710. The value of part_mode may be limited as follows:
● If CuPredMode [ x0] [ y0] is equal to MODE_INTRA, then part_mode should be equal to 0 or 1.
● Otherwise (CuPredMode [ x0] [ y0] equals MODE_INTER), the following applies:
in case Log2CbSize is greater than Log2MinCbSizeY and amp_enabled_flag is equal to 1, part_mode should be included in the range of 0 to 2, head and tail, or in the range of 4 to 7, head and tail.
In the case where Log2CbSize is greater than Log2MinCbSizeY and amp_enabled_flag is equal to 0, part_mode should be in the range of 0 to 2, inclusive.
Otherwise, if log2CbSize is equal to 3, the value of part_mode should be in the range of 0 to 2, inclusive.
Otherwise (log 2CbSize is greater than 3), the value of part_mode should be in the range of 0 to 3, inclusive.
When PredMode is mode_intra and Log2CbSize is greater than Log2MinCbSizeY, partMode is inferred to be equal to part_2n×2n.
In current HEVC, partition modes part_2n×n732 and part_n×2n 734 always consume three bits when the current CU size is greater than the minimum CU size, as shown by codeword 011 (entry 752) and codeword 001 (entry 754). For bit saving purposes, the actual binarization of the partition modes ParT_2N×N732 and ParT_N×2N 734 depends on the AMP flag. That is, if AMP is off, only two bits may need to be generated.
Fig. 8 illustrates an example binarization table 800 that describes bit allocation for partition mode in HEVC, which corrects for AMP. The binarization table 800 includes a category CuPredMode 810, a part_mode 820, a PartMode 830, and a binary string 840.CuPredMode 810 further includes sub-categories MODE_INTRA 812 and MODE_INTER 814. Binary string 840 further includes sub-categories cLog2CbSize > Log2MinCbSize 850 and cLog2 cbsize= =log 2MinCbSize 860.cLog2 cbsize= Log2MinCbSize 860 further includes the sub-categories cLog2 cbsize= 3 862 and cLog2CbSize >3 864.cLog2CbSize > Log2MinCbSize 850 also includes subcategory-! amp_enabled_flag 851 and amp_enabled_flag 853.
As described above, amp_enabled_flag 853 is equal to 1 designating AMP, for example, part2n×nu, part2n×nd, part_nl×2n, or part_nr×2n, partMode may be used in the encoded tree block, and amp_enabled_flag is equal to 0 (or | amp_enabled_flag 851 is equal to 1) designating AMP cannot be used in the encoded tree block. The binarization table 800 manages how the encoder 116 encodes CU headers and how the decoders 138, 140 decode CU headers.
At PartMode 830, a part_mode flag may specify the partition mode of the current codec unit. Part_mode 820 refers to the partition mode value.
In fig. 8, when the current CU size is greater than the minimum CU size, the partition modes part_2n×n832 and part_n×2n 834 consume two or three bits, as shown by entries 852, 854, 856, 858, and depend on whether AMP is enabled. For example, if the disable flag amp_enabled_flag (or | amp_enabled_flag equals 1), the encoder 116 encodes a codeword in the encoded bitstream having two bits for the partition modes part_2n×n 832 and part_n×2n 834 as shown by entries 852, 854. However, if the flag amp_enabled_flag (equal to 1) is enabled, the encoder 116 encodes a codeword in the encoded bitstream having three bits for the partition modes part_2n×n 832 and part_n×2n 834 as shown by entries 856, 858. In other words, if AMP is turned on or enabled for part_2nxn 832, entry 856 is assigned codeword 011. If AMP is turned off or not enabled for PART_2NxN 832, entry 852 is assigned codeword 01. If AMP is turned on or enabled for PART_Nx2N 834, entry 858 is assigned codeword 001. If AMP is turned off or not enabled for PART_Nx2N 834, entry 854 is assigned codeword 00.
Considering the size of a CU, for an INTER CU with mode_inter 814, the following can be derived from table 800:
● For partition mode part_2n×2n, the codeword is always 1.
● For partition mode part_2n×n, if AMP is off or cu=mincu (minimum CU), the codeword is 01.
● For partition mode ParT_2N×N, if AMP is on or CU > minCU, then codeword is 011.
● For partition mode part_n×2n, if AMP is off or mincu=8×8, the codeword is 00.
● For partition mode ParT_Nx2N, if AMP is on or minCU >8 x 8, then codeword is 001.
● For partition mode part_nxn, if minCU >8×8, the codeword is always 000.
In view of the many possible embodiments to which the principles of this discussion may be applied, it should be recognized that the embodiments described herein with respect to the drawing figures are meant to be illustrative only and should not be taken as limiting the scope of the claims. Accordingly, the techniques described herein contemplate all such implementations that may fall within the scope of the following claims or equivalents thereof.
Claims (10)
1. A method, comprising:
determining, by the computing device, whether to enable a value of a flag indicating an asymmetric motion partition for predicting motion inside the current block when a size of the current block of the current frame is greater than a minimum block size;
responsive to determining that a value of a flag indicating an asymmetric motion partition is not enabled, encoding, by the computing device, a first codeword of an inter mode partition of the current block, wherein the first codeword is a two-bit codeword; and
responsive to determining to enable the value of the flag indicating the asymmetric motion partition, encoding or decoding, by the computing device, a second codeword of the inter mode partition of the current block, wherein the second codeword is a three-bit codeword.
2. The method of claim 1, wherein the first codeword is 01 when a partition mode for predicting motion inside the current block is part_2nxn, wherein the first codeword is 00 when the partition mode is part_nr2n.
3. The method of claim 1, wherein the second codeword is 011 when the partition mode for predicting motion inside the current block is part_2nxn, wherein the second codeword is 001 when the partition mode is part_nx 2N.
4. The method of claim 1, wherein the current block comprises luma samples of a current codec tree block.
5. An encoder, comprising:
one or more computer processors; and
a non-transitory computer-readable storage medium comprising instructions that, when executed, cause the one or more computer processors to:
determining whether a value of a flag indicating an asymmetric motion partition is enabled for predicting motion inside a current block of a video frame when the current block is greater than a minimum block size;
in response to determining that the value of the flag indicating the asymmetric motion partition is not enabled, encoding a first codeword of the inter mode partition of the current block as a bitstream, wherein the first codeword is a two-bit codeword;
responsive to determining that a value of a flag indicating an asymmetric motion partition is enabled, encoding a second codeword of an inter mode partition of the current block as the bitstream, wherein the second codeword is a three-bit codeword; and
outputting or storing the bit stream.
6. The encoder of claim 5, wherein the first codeword is 01 when a partition mode for predicting motion inside the current block is part_2nxn, wherein the first codeword is 00 when the partition mode is part_nr2n.
7. The encoder of claim 5, wherein the second codeword is 011 when the partition mode for predicting motion inside the current block is part_2nxn, wherein the second codeword is 001 when the partition mode is part_nx 2N.
8. A decoder, comprising:
one or more computer processors; and
a non-transitory computer-readable storage medium comprising instructions that, when executed, cause the one or more computer processors to:
determining whether a value of a flag indicating an asymmetric motion partition is enabled for predicting motion inside a current block when the size of the current block of the current frame is greater than a minimum block size;
in response to determining that the value of the flag indicating the asymmetric motion partition is not enabled, decoding the current block into an output video stream using a first codeword of the inter mode partition, wherein the first codeword is a two-bit codeword;
in response to determining that the value of the flag indicating the asymmetric motion partition is enabled, decoding the current block into the output video stream using a second codeword of the inter mode partition, wherein the second codeword is a three-bit codeword; and
and outputting or displaying the output video stream.
9. The decoder of claim 8, wherein the first codeword is 01 when a partition mode for predicting motion inside the current block is part_2nxn, wherein the first codeword is 00 when the partition mode is part_nr2n.
10. The decoder of claim 8, wherein the second codeword is 011 when the partition mode for predicting motion inside the current block is part_2nxn, wherein the second codeword is 001 when the partition mode is part_nx 2N.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
CN201811249137.5A CN109547790B (en) | 2012-07-27 | 2013-07-17 | Apparatus and method for processing partition mode in high efficiency video codec |
Applications Claiming Priority (7)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201261676809P | 2012-07-27 | 2012-07-27 | |
US61/676,809 | 2012-07-27 | ||
US13/940,345 US20140029670A1 (en) | 2012-07-27 | 2013-07-12 | Devices and methods for processing of partition mode in high efficiency video coding |
US13/940,345 | 2013-07-12 | ||
PCT/US2013/050901 WO2014018341A1 (en) | 2012-07-27 | 2013-07-17 | Devices and methods for processing of partition mode in high efficiency video coding |
CN201811249137.5A CN109547790B (en) | 2012-07-27 | 2013-07-17 | Apparatus and method for processing partition mode in high efficiency video codec |
CN201380049417.0A CN104685874B (en) | 2012-07-27 | 2013-07-17 | For handling the device and method of compartment model in high efficiency coding and decoding video |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201380049417.0A Division CN104685874B (en) | 2012-07-27 | 2013-07-17 | For handling the device and method of compartment model in high efficiency coding and decoding video |
Publications (2)
Publication Number | Publication Date |
---|---|
CN109547790A CN109547790A (en) | 2019-03-29 |
CN109547790B true CN109547790B (en) | 2023-07-04 |
Family
ID=49994888
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201380049417.0A Active CN104685874B (en) | 2012-07-27 | 2013-07-17 | For handling the device and method of compartment model in high efficiency coding and decoding video |
CN201811249137.5A Active CN109547790B (en) | 2012-07-27 | 2013-07-17 | Apparatus and method for processing partition mode in high efficiency video codec |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201380049417.0A Active CN104685874B (en) | 2012-07-27 | 2013-07-17 | For handling the device and method of compartment model in high efficiency coding and decoding video |
Country Status (4)
Country | Link |
---|---|
US (1) | US20140029670A1 (en) |
EP (1) | EP2878124B1 (en) |
CN (2) | CN104685874B (en) |
WO (1) | WO2014018341A1 (en) |
Families Citing this family (22)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9532059B2 (en) | 2010-10-05 | 2016-12-27 | Google Technology Holdings LLC | Method and apparatus for spatial scalability for video coding |
US9445093B2 (en) * | 2011-06-29 | 2016-09-13 | Qualcomm Incorporated | Multiple zone scanning order for video coding |
CA2942903A1 (en) * | 2014-03-16 | 2015-09-24 | Vid Scale, Inc. | Method and apparatus for the signaling of lossless video coding |
WO2016090568A1 (en) | 2014-12-10 | 2016-06-16 | Mediatek Singapore Pte. Ltd. | Binary tree block partitioning structure |
US10382795B2 (en) | 2014-12-10 | 2019-08-13 | Mediatek Singapore Pte. Ltd. | Method of video coding using binary tree block partitioning |
US10009620B2 (en) * | 2015-06-22 | 2018-06-26 | Cisco Technology, Inc. | Combined coding of split information and other block-level parameters for video coding/decoding |
US10003807B2 (en) | 2015-06-22 | 2018-06-19 | Cisco Technology, Inc. | Block-based video coding using a mixture of square and rectangular blocks |
CN116866617A (en) * | 2016-09-20 | 2023-10-10 | 株式会社Kt | Method for decoding and encoding video and method for transmitting video data |
WO2018056703A1 (en) * | 2016-09-20 | 2018-03-29 | 주식회사 케이티 | Method and apparatus for processing video signal |
CN106407400B (en) * | 2016-09-21 | 2019-08-06 | 中国科学院信息工程研究所 | A kind of real-time abstraction generating method towards stream data |
US20180109812A1 (en) * | 2016-10-14 | 2018-04-19 | Media Tek Inc. | Block partitioning using tree structures |
US11553210B2 (en) | 2018-12-07 | 2023-01-10 | Interdigital Vc Holdings, Inc. | Managing coding tools combinations and restrictions |
US20200252608A1 (en) * | 2019-02-05 | 2020-08-06 | Qualcomm Incorporated | Sub-partition intra prediction |
EP3935839A4 (en) * | 2019-04-09 | 2022-06-22 | Beijing Dajia Internet Information Technology Co., Ltd. | Methods and apparatuses for signaling of merge modes in video coding |
KR20200145773A (en) * | 2019-06-21 | 2020-12-30 | 현대자동차주식회사 | Method and apparatus for controlling coding tools |
WO2020256510A1 (en) | 2019-06-21 | 2020-12-24 | 현대자동차주식회사 | Method and device for controlling coding tools |
KR20220027173A (en) * | 2019-06-25 | 2022-03-07 | 인터디지털 브이씨 홀딩스 프랑스 에스에이에스 | Motion vector prediction in video encoding and decoding |
JP7359936B2 (en) * | 2019-07-20 | 2023-10-11 | 北京字節跳動網絡技術有限公司 | Conditional Encoding of Instructions for Use of Palette Mode |
WO2021023262A1 (en) * | 2019-08-06 | 2021-02-11 | Beijing Bytedance Network Technology Co., Ltd. | Using screen content coding tool for video encoding and decoding |
CN114342400A (en) | 2019-09-02 | 2022-04-12 | 北京字节跳动网络技术有限公司 | Color format based codec mode determination |
CN114503561A (en) * | 2019-10-09 | 2022-05-13 | 北京达佳互联信息技术有限公司 | Method and apparatus for prediction refinement using optical flow, bi-directional optical flow, and decoder-side motion vector refinement |
WO2021188571A1 (en) * | 2020-03-16 | 2021-09-23 | Beijing Dajia Internet Information Technology Co., Ltd. | Improvements on merge mode with motion vector differences |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2010039731A2 (en) * | 2008-10-03 | 2010-04-08 | Qualcomm Incorporated | Video coding with large macroblocks |
CN102474615A (en) * | 2009-08-14 | 2012-05-23 | 三星电子株式会社 | Video coding and decoding methods and video coding and decoding devices using adaptive loop filtering |
Family Cites Families (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN100571390C (en) * | 2006-12-21 | 2009-12-16 | 联想(北京)有限公司 | A kind of H264 video coding fast schema selection method and device |
KR101624649B1 (en) * | 2009-08-14 | 2016-05-26 | 삼성전자주식회사 | Method and apparatus for video encoding considering hierarchical coded block pattern, and method and apparatus for video decoding considering hierarchical coded block pattern |
KR101457418B1 (en) * | 2009-10-23 | 2014-11-04 | 삼성전자주식회사 | Method and apparatus for video encoding and decoding dependent on hierarchical structure of coding unit |
CN101783951B (en) * | 2010-03-02 | 2012-01-04 | 西安交通大学 | Method for rapidly confirming video coding inter-frame predicting mode based on human vision system |
US8885704B2 (en) * | 2010-10-01 | 2014-11-11 | Qualcomm Incorporated | Coding prediction modes in video coding |
US8526495B2 (en) * | 2010-11-22 | 2013-09-03 | Mediatek Singapore Pte. Ltd. | Apparatus and method of constrained partition size for high efficiency video coding |
CN102595123B (en) * | 2011-01-14 | 2014-06-04 | 华为技术有限公司 | Stripe coding method and device thereof as well as stripe decoding method and device thereof |
US9049452B2 (en) * | 2011-01-25 | 2015-06-02 | Mediatek Singapore Pte. Ltd. | Method and apparatus for compressing coding unit in high efficiency video coding |
CN102075759B (en) * | 2011-02-25 | 2012-11-14 | 清华大学 | Low-power consumption encoding method for dynamic memory in video decoding application |
US9332283B2 (en) * | 2011-09-27 | 2016-05-03 | Broadcom Corporation | Signaling of prediction size unit in accordance with video coding |
US20130114691A1 (en) * | 2011-11-03 | 2013-05-09 | Qualcomm Incorporated | Adaptive initialization for context adaptive entropy coding |
-
2013
- 2013-07-12 US US13/940,345 patent/US20140029670A1/en not_active Abandoned
- 2013-07-17 EP EP13740195.6A patent/EP2878124B1/en active Active
- 2013-07-17 CN CN201380049417.0A patent/CN104685874B/en active Active
- 2013-07-17 CN CN201811249137.5A patent/CN109547790B/en active Active
- 2013-07-17 WO PCT/US2013/050901 patent/WO2014018341A1/en active Application Filing
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2010039731A2 (en) * | 2008-10-03 | 2010-04-08 | Qualcomm Incorporated | Video coding with large macroblocks |
CN102474615A (en) * | 2009-08-14 | 2012-05-23 | 三星电子株式会社 | Video coding and decoding methods and video coding and decoding devices using adaptive loop filtering |
Also Published As
Publication number | Publication date |
---|---|
CN104685874B (en) | 2018-11-16 |
WO2014018341A1 (en) | 2014-01-30 |
EP2878124A1 (en) | 2015-06-03 |
CN104685874A (en) | 2015-06-03 |
CN109547790A (en) | 2019-03-29 |
EP2878124B1 (en) | 2018-09-26 |
US20140029670A1 (en) | 2014-01-30 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN109547790B (en) | Apparatus and method for processing partition mode in high efficiency video codec | |
US9888249B2 (en) | Devices and methods for sample adaptive offset coding and/or selection of edge offset parameters | |
KR101674777B1 (en) | Devices and methods for sample adaptive offset coding and/or signaling | |
US9565435B2 (en) | Devices and methods for context reduction in last significant coefficient position coding | |
EP3311573B1 (en) | Video intra prediction using hybrid recursive filters | |
EP2805495B1 (en) | Devices and methods for context reduction in last significant coefficient position coding | |
WO2018129322A1 (en) | Multi-type-tree framework for video coding | |
US9872034B2 (en) | Devices and methods for signaling sample adaptive offset (SAO) parameters | |
WO2018132475A1 (en) | Intra prediction techniques for video coding | |
EP2920971B1 (en) | Devices and methods for processing of non-idr related syntax for high efficiency video coding (hevc) | |
US20140146894A1 (en) | Devices and methods for modifications of syntax related to transform skip for high efficiency video coding (hevc) | |
US20130188741A1 (en) | Devices and methods for sample adaptive offset coding and/or selection of band offset parameters | |
US20140092975A1 (en) | Devices and methods for using base layer motion vector for enhancement layer motion vector prediction | |
WO2012099743A1 (en) | Method and system for processing video data | |
KR20130119475A (en) | Method and system for determining a context model for video data | |
US11039166B2 (en) | Devices and methods for using base layer intra prediction mode for enhancement layer intra mode prediction | |
WO2013109419A1 (en) | Devices and methods for sample adaptive offset coding and/or selection of band offset parameters |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |