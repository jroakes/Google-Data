US9875272B1 - Method and system for designing a database system for high event rate, while maintaining predictable query performance - Google Patents
Method and system for designing a database system for high event rate, while maintaining predictable query performance Download PDFInfo
- Publication number
- US9875272B1 US9875272B1 US14/748,225 US201514748225A US9875272B1 US 9875272 B1 US9875272 B1 US 9875272B1 US 201514748225 A US201514748225 A US 201514748225A US 9875272 B1 US9875272 B1 US 9875272B1
- Authority
- US
- United States
- Prior art keywords
- time
- data
- database
- user
- nodes
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/23—Updating
- G06F16/2365—Ensuring data consistency and integrity
-
- G06F17/30371—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/22—Indexing; Data structures therefor; Storage structures
- G06F16/2291—User-Defined Types; Storage management thereof
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/245—Query processing
- G06F16/2458—Special types of queries, e.g. statistical queries, fuzzy queries or distributed queries
- G06F16/2471—Distributed queries
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/27—Replication, distribution or synchronisation of data between databases or within a distributed database system; Distributed database system architectures therefor
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/27—Replication, distribution or synchronisation of data between databases or within a distributed database system; Distributed database system architectures therefor
- G06F16/275—Synchronous replication
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/27—Replication, distribution or synchronisation of data between databases or within a distributed database system; Distributed database system architectures therefor
- G06F16/278—Data partitioning, e.g. horizontal or vertical partitioning
-
- G06F17/30342—
-
- G06F17/30581—
-
- G06F17/30584—
Definitions
- the present subject matter generally relates to the field of data storage, analysis and manipulation.
- the disclosure pertains to database system performance. It has applicability to the processing of database storage and querying.
- a database system performs various functions, including defining data tables in an organized fashion according to a user/customer's requirements and specifications.
- a database When such a database is defined on a database system, it operates to accept entered data records for storage as data entries in the database. It must also be able to support user queries, which may specify values within certain fields (“columns”) of the stored data, and report back all stored data entries for which the specified columns have the specified values. As needed, the database needs to be able to purge data no longer desired to be kept.
- a database system should perform these functions to acceptable levels of performance. That can include, among other things, acceptable query latency times. Different users have different requirements relating to database system performance, and may prioritize their requirements differently. Therefore, a database system may need flexibility as to how it meets the customers' needs.
- the database system comprises a plurality of nodes, each node including a data storage device having memory and a server; and a database system controller coupled to each of the plurality of nodes and having a processor and software program code for directing the database system to perform the following function of defining, for a user, a node group including at least one of the plurality of nodes, upon which the user's data is to be stored in a user database; wherein the user database is defined in terms of time-partitioned tables residing on the nodes of the node group, and wherein each time-partitioned table is further defined in terms of shards of the user's data, the shards corresponding with respective ones of the time partitions.
- the user data is managed in terms of the shards on respective nodes of the node group, and the time-partitioned tables include a chronologically oldest time-partitioned table and a current time-partitioned table into which a new user data entry will be written.
- the system further performs the functions of determining whether a predetermined criterion has been met; and, responsive to the detecting that the predetermined criterion has been met, purging all shards of the chronologically oldest time-partitioned table.
- FIGS. 2, 3, and 4 are schematic diagrams of a database showing multiple shards within multiple data storage nodes.
- FIG. 5 is a schematic diagram showing query processing.
- FIG. 6 is a high-level system block diagram of a system according to an embodiment.
- FIG. 7 is a flow chart showing operation of an embodiment.
- FIG. 8 is a system block diagram of a computer-based database system in accordance with an embodiment.
- a database designed using a database system, stores information about names and addresses.
- a given entry also called a record, may include a name, street address, city, state and Zip (postal) code.
- Zip code 94301 is assumed to fall entirely within the city of Palo Alto Calif.
- the Zip code 95008 falls entirely within the city of Campbell Calif.
- the Zip code 95035 falls entirely within the city of Milpitas, Calif.
- a given entry (also referred to herein as a “record”) is a “row” of a database table, and the various data fields of the data entries are “columns.”
- the name, street address and Zip code fields of the data entries in the table 105 are in respective columns.
- Relational databases may be designed to avoid such redundancy.
- a database made of two tables, shown as 105 and 110 might include a Zip code field in each table.
- the table 105 showing the unique names and street addresses may use the Zip code as a cross reference to the table 110 , in which there is only one entry for each Zip code value, showing the corresponding city and state values.
- the database system property called “referential integrity” requires that, if an entry in one database table references an entry in another table, the latter table entry must be there. Deleting the latter would be a referential integrity violation.
- nodes can get very large.
- Each node typically includes a quantity of computer memory for storing the data, and a processor, which functions as a data server, to control storage, access, and manipulation of the data.
- data can be partitioned, for instance based on time periods during which the data entries were entered. For instance, a large list of names and addresses (as in the table 105 ) might be time partitioned into distinct tables for each calendar year in which the entries were entered.
- FIG. 9 shows a table of data, which might be called “fact data” or “transition data,” which pertains to the data base information shown in FIG. 1 .
- Entries in the Name column of the table 105 of FIG. 1 appear in the login_name column of the table of FIG. 9 .
- time stamps are also shown in FIG. 9 , among other things, and time stamps, given in the time_stamp column. That data is an illustrated representative of data, such as API traffic data, web logs, etc., which may be supported or produced, for example, by a Web application such as Apache Tomcat.
- the data of FIG. 9 may, in accordance with an embodiment, be stored and processed. As will be discussed further, data partitioning or sharding may be done according to specified time intervals, within which the time stamps for various data entries of FIG. 9 fall.
- FIG. 3 is a schematic representation of a database system in which two nodes, designated N 1 and N 2 , store data from one customer, designated C 2 , in separate time partitioned shards (terminology to be defined below).
- the time periods are designated TP 1 through TP 4 , and could represent recognizable time periods such as calendar years, fiscal quarters, etc.
- index can be defined, which points to the more frequently accessed entries, in order to access them more quickly.
- the index might, for instance, alphabetize the names of the entries in the Name column of the table 105 , to help find a given individual, by name, more quickly.
- indexes may be used, in which each index defines a different ordering of the records.
- an employee database may have several indexes, based on the information being sought.
- a name index may order employees alphabetically by last name, while a department index may order employees by their department.
- a key is specified in each index, the key typically being the value of one of the columns. For an alphabetical index of employee names, the last name field could be the key.
- sharding is here defined as distributing records or data for one customer or one type across different database servers.
- shards are implemented as data, for a given customer or type, which resides on respective database nodes.
- Shards may be defined in terms of time partitions, as is the case with partitions, but a given time partition will consist of a number of shards equal to the number of nodes, upon which portions of that time partition reside. Keeping the various shards of a database for a customer or type relatively small, may decrease the query latency time. Also, shards may easily be distributed over multiple data storage devices, and their access may then be supported by multiple processors that operate the multiple data storage devices, again improving query latency time.
- shard in a more general sense, may be defined as a small part of a whole.
- Sharding a database may be defined as breaking up the big database into many, much smaller databases that share no redundant data, that can be spread across multiple servers, and which have some sort or unifying attribute, beyond that of mere partitioning by time, etc., which may be expressed in terms of a particular customer, a particular type, etc.
- Sharding can be considered analogous to horizontal partitioning; that is, analogous to partitioning by rows.
- the term is often used to refer to any database partitioning that is meant to make a very large database more manageable.
- partitioning and sharding are different, in that sharding gives a more specific and detailed basis for distinguishing, by content, type, customer, etc., which data shall be within or outside the shard.
- partitioning is broad, and generally pertains only to one given parameter, such as time, and does not necessarily take into account whether single or multiple servers or storage devices are employed.
- the advantageousness of sharding is based on the idea that, as the size of a database and the number of transactions per unit of time made on the database increase linearly, the response time for querying the database increases at a faster rate, such as exponentially.
- database sharding can be done fairly simply.
- One common example is splitting a customer database geographically. Customers located on the East Coast can be placed on one server, while customers on the West Coast can be placed on a second server.
- the database programmer may specify explicit sharding rules, to determine on which machines any given piece of data, or category of data, will be stored. Assuming there are no complicating factors, such as, in the above example, customers with multiple locations, the split is easy to maintain and build rules around.
- shards are partitioned based on the periods of time during which the data records were received form storage in the database. This “time partitioning” is discussed herein, and illustrated in the accompanying drawings.
- Data sharding can be a more complex process in some scenarios, however.
- Sharding a database that holds less structured data, for example, can be very complicated, and the resulting shards may be difficult to maintain.
- Data volume growth, or transaction growth represent another challenge which may be address by embodiments of the present subject matter.
- a partitioning model in which events are sharded across five database nodes.
- it is necessary to expand to a sixth node because of disk capacity, query capacity, or higher insertion rates.
- it is extremely difficult to do so because now all old records (or potentially 5 ⁇ 6 th of the old records) can get remapped to a different node, resulting in a lot of disruption to the database performance.
- shards and node groups can be used to handle data volume growth, while maintaining a good query response time.
- data volume may, in many cases, be greater than what one database server can handle with suitable performance.
- multiple nodes may be used, to provide greater data capacity or greater processing power for data entry, queries, etc.
- Some database systems may, for instance, treat the primary objective as being optimizing query performance, such as by minimizing query latency time.
- a database query typically is a complete select statement that specifies 1) the columns and tables from which data is to be retrieved, 2) optionally, conditions that the data must satisfy, 3) optionally, computations that are to be performed on the retrieved column values, and 4) optionally, a desired ordering of the result set.
- Indexing strategies, and data partitioning (sharding) algorithms may be used to try to achieve such query performance optimization.
- indexing can bring down the rate at which new records are added to the database, by increasing the processing overhead associated with the new record entry.
- indexing is sometimes designed to optimize for the most commonly occurring queries.
- other queries may have longer latency times, decreasing the overall performance.
- Database system administrators can fine-tune the available indexes, to try to optimize overall performance and performance as to the most frequently occurring queries, but without adding so much overhead as to slow down overall performance.
- shards are implemented using the concept of time partitions, shards older than a specific time period are easily and efficiently purged, or “dropped.’
- a database system in which a combination of a defined user group of database system nodes and time-partitioned data tables achieves desirable query performance, while allowing for quick and easy purging of the oldest data and for maintenance of desirably high insert rate.
- the customer when registering as a customer of the database system, will be allocated data storage space on the system. That space may be specified in terms of preconfigured dimensions. The customer may contract for the space based on need and payment, for instance in terms of dimensions and measures of the data, and number of indexes the user might employ to query for data records. If the user's dimensional representation is sufficiently large, then the database system might include a separate dimensional table.
- the user is assigned a node group, consisting of space on one or more nodes of the database system.
- a node group consisting of space on one or more nodes of the database system.
- different distinct database tables such as 105 and 110 of FIG. 1
- different columns of a single table might reside on different nodes.
- accessing a given data record might involve reading different columns of the record from different nodes.
- FIG. 2 is a schematic representation of a database system having three nodes, designated N 1 through N 3 , in which five customers, designated C 1 through C 5 , are allocated space.
- a given customer's node group will include as many, or as few, nodes as are reasonably required to meet the customer's needs, without undue cost to the customer.
- the system administrator may also take into consideration how to meet the customer's needs, while enabling the overall database system to keep up with the system's transaction rate, and maintain an acceptable physical insert speed. This could depend on factors such as the customer's data volume, and security considerations that might make it appropriate to isolate the customer's node group from nodes storing data of other customers.
- data purging based on age of data is optimized by designing the customer's node group and data shards in terms of time partitions based on a time factor such as date and time the data record was entered into the database. Then, purging the oldest data may be done simply by purging the oldest data shard or shards. That is, purging may comprise dropping the oldest shard or shards for a particular time-partition or time-partitions.
- a data purge can be responsive to the current data shard becoming full, responsive to the oldest data reaching a certain age, such as a specified number of months, etc.
- Databases are designed according to an organization of data as a blueprint of how a database is constructed; for instance, divided into database tables. This sort of database definition is referred to as a “schema,” and describes the structure of a database system in a formal language which is supported by the database management system (DBMS).
- DBMS database management system
- an exemplary schema will be referred to by the algebraic expression “Schema(C I ).”
- FIG. 6 is a high-level block diagram of an electronic database processing device architecture which will be described in connection with the embodiment under discussion.
- a user 605 employs the electronic database processing device of FIG. 6 to work with a database stored as a data entity.
- a coordination and synchronization server therefor is here referred to by the name “Zookeeper,” and is designated as 610 in FIG. 6 .
- Zookeeper will be used to coordinate various things, for instance, instructing the ingest-server to insert into a new time-partition, add or remove columns into or from the fact table, etc. Zookeeper is also used to store various configuration details, like nodegroup details, etc.
- the name “Zookeeper” refers to a software project of the Apache Software Foundation.
- the Apache Zookeeper provides an open source distributed configuration service, synchronization service, and naming registry for large distributed systems.
- Zookeeper's architecture supports high availability through redundant services. The clients can thus ask another Zookeeper node if the first node fails to answer.
- Zookeeper nodes store their data in a hierarchical name space, much like a file system or a digital tree data structure. Clients can read and write from/to the nodes and in this way have a shared configuration service. Updates are totally ordered.
- other equivalent software implementations may be used in place of the Zookeeper that is specifically referred to herein. Implementations other than Zookeeper may employ a pull model in order to look up for state, as opposed to being communicated by the Zookeeper (that is, a push model).
- the Zookeeper 610 received database content updates through an Ingest server 615 .
- Data ingestion is the process of obtaining, importing, and processing data for later use or storage in a database. This process often involves altering individual files by editing their content and/or formatting them to fit into a larger document. Queries to the database content are received, and results are provided, through a Query server 620 .
- Databases/node groups 625 schematically represent the data storage devices within which the database content updates are entered, and through which the queries are processed.
- the Schema(C I ) is assumed to contain only one set of tables. Further, it may be assumed for the sake of the example that the tables contain “FACTS” which are defined, as per database terminology, as factual data, such as sales or cost data or other business measures, etc.
- the tables may additionally contain indexes associated with those FACT tables, or other suitable integrity constraints.
- C I .FACT Multi-part naming (using the schema name C I ) will be used to disambiguate the tables for different customers on the same node. (Each customer will have their own schema (in the SQL sense), C I , allowing for such flexibility.)
- a separate dimensional table is created only when the dimensional representation is very large (like for URI's). In such a case, a separate “hash table” maps the lengthy expressions as they get stored in the FACT table.
- the Schema design (table and index definitions) of the Schema(C I ) for the customer will be stored in the Zookeeper 610 .
- a customer based on the above characteristics, and a few other things described later, will have a Class(C I ).
- the Class(C I ) is stored in the Zookeeper 610 .
- a customer is not forced to be on only one node. This facilitates keeping up with the transaction rate, since otherwise the physical insert speed can hinder the transaction rate.
- every customer is not forced to be on all nodes. The reasons are as follows:
- nodegroup the concept of the group of nodes employed by a database system, or “nodegroup.”
- nodegroup the nodes used to hold data of a customer C I will be referred to as belonging to a nodegroup NG(C I ).
- customer data occupying subsets of the nodes making up subsets designated algebraically as Subset ⁇ N 1 . . . N 2 ⁇ for customers N 1 . . . N 2 .
- the nodegroup NG(C I ) is also stored in the Zookeeper 610 .
- TP time partitions
- FIG. 2 illustrates an example of such nodegrouping.
- Three nodes, N 1 through N 2 store data for five customers C 1 through C 5 .
- Five nodegroups, designated NG(C 1 ) through NG(C 5 ) store respective customer data on various subsets of the three nodes, as shown.
- time partitioning may additionally be done.
- TP J (C I ) represents the j th time partition for the schema C I .
- maxTP(C I ) representing the maximum number of time partitions for a given customer.
- the value of maxTP(C I ) is stored in the Zookeeper 610 .
- the time partition TP J (C I ) exists on all nodes in the node group NG(C I ).
- the fraction of the time partition TP J (C I ) that exists on a node N K , where the node N K is in the node group NG(C I ), is designated algebraically as a shard (TP J (C I )).
- TP cur (C I ) contains the most recently entered records, and into which new data entries are being inserted.
- the current schema cur(C I ) is stored in Zookeeper 610 .
- TP cur (C I ) is now declared to be full. In general, because of the randomized inserts, all shards of this, or any given time partition, will become full at approximately the same time.
- a new time partition is initiated, and its designation value is given as (cur+1) mod maxTP(C I ). Also, when maxTP(C I ) TPs become full, (and from then on), the oldest time partition is dropped.
- the time partitions are maintained, for instance, as a circular linked list, and accordingly, they are numbered using a modulo numbering system, as shown in the expression (cur+1) mod maxTP(C I ) just mentioned.
- Alternatives to the circular linked list implementation of time partitions include, for instance, a monotonically increasing entity such as a current system date-time tagging/stamping arrangement, or an automatically generated ever increasing address or index number, etc. Yet additional implementations are possible, if they may be dropped at will when the purge interval is achieved.
- shards and node groups can be used to handle data volume growth, including data purging based on considerations such as age; and yet a good query response time is maintained.
- a first dimension D 1 is “time.” In reality, it may be the case that the timestamp of insertion of a database record will be some other logical time that may be chosen for the convenience of the particular user, or embodiment.
- a time partition over a node group of two nodes N 1 and N 2 , has four time partitions TP 1 through TP 4 .
- the result is that there are eight shards.
- Time Start (TP J (C I )) and Time End (TP J (C I )) are nevertheless the same across all the shards of a given time partition. That is, the time is associated with a TP, not merely associated with any one shard of the TP.
- Time Start (TP J (C I )) and Time End (TP J (C I )) are stored in Zookeeper 610 .
- Query response time may primarily measure query processing time.
- the query is forked, in parallel, into multiple sub queries, which will be sent to individual nodes holding the shards for a given customer and for the given time partition. The results from these nodes are then merged and aggregated by the query node.
- a query is processed by simply looking up information in the Zookeeper 610 to determine which time partitions and nodegroups which store this customer's data are needed to answer the query. That information may, for instance, be cached locally on a query node which receives the query from the customer. Also, it may be the case that the query is specified in terms of a column value which reflects the chronology of the data records, and which is a basis for the time partitioning.
- the Query Server retrieves nodegroups (that is, the list of database servers) for a particular customer, all it requires is the start_time and end_time that is given by the user. The database system can now fetch the records from the right time-partitions given this information. In an embodiment, all time partitions are selected by default, unless the query specifies fewer than all of the time partitions. All queries will go against all nodes, for all TPs, that could possibly contain information that is of interest to the query.
- the obtained data entries are aggregated.
- Conventional aggregation techniques may be used. in an embodiment, however, the query request is processed by a split and aggregation logic software module to generate a query, for instance in the Structured Query Language (SQL), which accesses all shards for the desired time partition or partitions, and collects the results, aggregates them, and returns the result as a query response.
- SQL Structured Query Language
- the aggregation is done in three steps, which can be understood in terms of sets of arrows shown in FIG. 5 , overlaying the modules and shards shown schematically.
- a first step is represented by downward-facing arrows on the upper left of FIG. 5 , crossing the DATA APIS, SPLIT AND AGGREGATION LOGIC, and SQL (structured query language) boxes.
- a request to data API is processed by a split and aggregation logic that generates a query in SQL.
- a second step is represented by the diagonal arrows which move to and from the various time partitions and shards in the lower part of FIG. 5 .
- a given TP is searched, then all of the shards relevant to that TP are searched.
- a third step is represented by the upward-facing arrows in the upper right portion of FIG. 5 .
- query results answers
- answers are collected, aggregated, and returned to the user who initiated the query.
- query processing includes a lookup of the Zookeeper information (assumed cached locally on the Query Node that gets the query) to understand the time partitions (TPs) which are needed to answer the query. If the key on which TPs are created is in the query, then some TP's are selected.
- the query proceeds generally as follows:
- the three steps shown in FIG. 5 and described above implement the aggregation as follows: On the down path (first step), a request to data API is processed by a split and aggregation logic that generates SQL (second step) across the various TPs and the relevant shards (typically all) for that TP. On the upward path (third step), the results are collected, aggregated, and returned as the answers to the query.
- the database system may classify the customer into one of a plurality of customer classes, where each customer class is defined in terms of factors such as the level of service provided, the amount of data storage space provided, which may include a time partition data storage period, a number of custom variables, such as dimensions, measures, etc., the customer's database will require, a transaction rate defined in terms of the volume of queries and number of queries that might be concurrently executed in parallel, new data entries, etc., the number and type of indexes into the customer's database, and the price the customer is willing to pay for the service.
- factors such as the level of service provided, the amount of data storage space provided, which may include a time partition data storage period, a number of custom variables, such as dimensions, measures, etc., the customer's database will require, a transaction rate defined in terms of the volume of queries and number of queries that might be concurrently executed in parallel, new data entries, etc., the number and type of indexes into the customer's database, and the price the customer is willing to pay for the service.
- the class might also be defined in terms of the complexity of the queries that will be used to access the customer's database data.
- Query complexity may be defined in different ways, but generally relates to the number of data records that are accessed by the query, as well as the number of records in the query result. For example, if a query is directed to specified values within two of the columns of the customer database, the query must access all records that satisfy either one of the columns, and then do an intersection of the two result sets to determine which records satisfy both of the search criteria. It may be said more broadly, then, that query complexity is related to the union of all records that satisfy any of the query criteria, even if the query result is a smaller set, such as an intersection, of those results.
- the database system stores the class definitions for all customers.
- the database system or its administrator, must make sure the system has the capacity to satisfy all customer requirements by class. Costs to the customers, if assessed accordingly, allow the system to operate while meeting expense and revenue objectives.
- Cost of operations may, for instance, be substantially determined by the integral or fractional number of EC2 (Amazon Webservices' “Elastic Compute Cloud”) instances that are needed to support the customer.
- EC2 Amazon Webservices' “Elastic Compute Cloud”
- the implementation could be run and hosted on Rackspace, or on any public cloud service provider, or could be run on any internal or private cloud. The following factors go into the number of EC2 instances needed:
- the Cost is a function of the above-listed variables: f(T, F, I, H, X, Q). It will be seen, then, that if any of these variables increase in value, the cost increases. Customer classes may thus be defined in terms of cost ranges, in terms of weighted values for the variables, etc.
- three classes could be defined simply as Small, Medium and Large, or as Gold, Silver and Bronze, etc. Their configurations might, for instance, be something like this:
- the cost of the implementation will be measured.
- the price actually charged to the customer may need to have sufficient margins above the calculated cost.
- Embodiments may be implemented on database systems such as the PostreSQLQL open source database system.
- Other relational data base management systems RDBMS that support time-partitions, for instance, Oracle database systems, may be used.
- RDBMS relational data base management systems
- Oracle database systems may be used.
- a PostreSQL server may for instance be configured as follows:
- system capacity may increase the number of nodes, by defining additional servers supporting additional data storage devices. This may be done, for instance, as follows:
- the system may deal with the node failure as follows:
- a process may take place, such as the following:
- a defined node group for the user database and the use of purging by partition, for instance by purging the oldest time partitioned shard.
- Shards and node groups are used to solve query performance problems that can arise because of the combination of increased data ingest rate and data volume growth.
- FIG. 7 is a flowchart showing operation of an embodiment of the present subject matter.
- the embodiment may represent the operational capability of a computer-based database system, of may be a method for operation such a database system.
- a computer program product such as a pre-recorded non-transitory medium bearing software for execution by a computer system, may direct the computer to perform such functionality.
- the system defines a user group ( 705 ) of one or more of the database system's nodes, for storage of the user's data.
- the user database is defined in terms of data storage on each of the nodes of the user group, and further based on time partitioned tables ( 710 ).
- the data segments, residing on various data nodes and pertaining to various partitioned periods of time, are designated as “shards.”
- the age of the data records entered within any one of the shards is maintained ( 715 ). This may be done as a running part of the process of entering each new data record. While the data records themselves need not necessarily contain information that can serve as a date/time stamp, the database system maintains that date/time information.
- a purge criterion is checked ( 720 ).
- the purge criterion may be any suitable criterion such as whether the age of the oldest stored data has reached an age threshold. In an embodiment, however, the threshold may be that a predetermined data capacity, within the user's database, has been achieved.
- the purge criterion it may be that the data is old enough, or great enough in total volume, etc., that purging is needed. Accordingly, the oldest data shard is purged from the user's database ( 725 ).
- FIG. 8 is a block diagram illustrating an exemplary computing device, for example the server 105 in accordance with one embodiment.
- the computing device 810 includes a processor 810 , a hard drive 820 , an I/O port 830 , and a memory 852 , coupled by a bus 899 .
- the bus 899 can be soldered to one or more motherboards.
- the processor 810 includes, but is not limited to, a general purpose processor, an application-specific integrated circuit (ASIC), an FPGA (Field Programmable Gate Array), a RISC (Reduced Instruction Set Controller) processor, or an integrated circuit.
- the processor 510 can be a single core or a multiple core processor.
- the processor 810 is specially suited for processing demands of location-aware reminders (for example, custom micro-code, and instruction fetching, pipelining or cache sizes).
- the processor 810 can be disposed on silicon or any other suitable material.
- the processor 810 can receive and execute instructions and data stored in the memory 852 or the hard drive 820 .
- the hard drive 820 can be a platter-based storage device, a flash drive, an external drive, a persistent memory device, or other types of memory.
- the hard drive 820 provides persistent (long term) storage for instructions and data.
- the I/O port 830 is an input/output panel including a network card 832 with an interface 833 along with a keyboard controller 834 , a mouse controller 836 , a GPS card 838 and I/O interfaces 840 .
- the network card 832 can be, for example, a wired networking card (for example, a USB card, or an IEEE 802.3 card), a wireless networking card (for example, an IEEE 802.11 card, or a Bluetooth card), and a cellular networking card (for example, a 3G card).
- the interface 833 is configured according to networking compatibility.
- a wired networking card includes a physical port to plug in a cord
- a wireless networking card includes an antennae.
- the network card 532 provides access to a communication channel on a network.
- the keyboard controller 834 can be coupled to a physical port 535 (for example PS/2 or USB port) for connecting a keyboard.
- the keyboard can be a standard alphanumeric keyboard with 101 or 104 keys (including, but not limited to, alphabetic, numerical and punctuation keys, a space bar, modifier keys), a laptop or notebook keyboard, a thumb-sized keyboard, a virtual keyboard, or the like.
- the mouse controller 836 can also be coupled to a physical port 837 (for example, mouse or USB port).
- the GPS card 838 provides communication to GPS satellites operating in space to receive location data.
- An antenna 839 provides radio communications (or alternatively, a data port can receive location information from a peripheral device).
- the I/O interfaces 840 are web interfaces and are coupled to a physical port 841 .
- the memory 852 can be a RAM (Random Access Memory), a flash memory, a non-persistent memory device, or other devices capable of storing program instructions being executed.
- the memory 852 comprises an Operating System (OS) module 856 along with a web browser 854 .
- the memory 852 comprises a calendar application that manages a plurality of appointments.
- the OS module 856 can be one of Microsoft Windows®family of operating systems (for example, Windows 95, 98, Me, Windows NT, Windows 2000, Windows XP, Windows XP x64 Edition, Windows Vista, Windows CE, Windows Mobile), Linux, HP-UX, UNIX, Sun OS, Solaris, Mac OS X, Alpha OS, AIX, IRIX32, or IRIX64.
- the web browser 854 can be a desktop web browser (for example, Internet Explorer, Mozilla, or Chrome), a mobile browser, or a web viewer built integrated into an application program.
- a user accesses a system on the World Wide Web (WWW) through a network such as the Internet.
- the web browser 854 is used to download the web pages or other content in various formats including HTML, XML, text, PDF, postscript, python and PHP and may be used to upload information to other parts of the system.
- the web browser may use URLs (Uniform Resource Locators) to identify resources on the web and HTTP (Hypertext Transfer Protocol) in transferring files to the web.
- URLs Uniform Resource Locators
- HTTP Hypertext Transfer Protocol
- computer software products can be written in any of various suitable programming languages, such as C, C++, C#, Pascal, Fortran, Perl, Matlab (from MathWorks), SAS, SPSS, JavaScript, AJAX, and Java.
- the computer software product can be an independent application with data input and data display modules.
- the computer software products can be classes that can be instantiated as distributed objects.
- the computer software products can also be component software, for example Java Beans (from Sun Microsystems) or Enterprise Java Beans (EJB from Sun Microsystems).
- Java Beans from Sun Microsystems
- EJB Enterprise Java Beans
- a computer that is running the previously mentioned computer software can be connected to a network and can interface to other computers using the network.
- the network can be an intranet, internet, or the Internet, among others.
- the network can be a wired network (for example, using copper), telephone network, packet network, an optical network (for example, using optical fiber), or a wireless network, or a combination of such networks.
- data and other information can be passed between the computer and components (or steps) of a system using a wireless network based on a protocol, for example Wi-Fi (IEEE standards 802.11, 802.11a, 802.11b, 802.11e, 802.11g, 802.11i, and 1802.11n).
- signals from the computer can be transferred, at least in part, wirelessly to components or other computers.
- each illustrated component represents a collection of functionalities which can be implemented as software, hardware, firmware or any combination of these.
- a component can be implemented as software, it can be implemented as a standalone program, but can also be implemented in other ways, for example as part of a larger program, as a plurality of separate programs, as a kernel loadable module, as one or more device drivers or as one or more statically or dynamically linked libraries.
- the portions, modules, agents, managers, components, functions, procedures, actions, layers, features, attributes, methodologies and other aspects of the invention can be implemented as software, hardware, firmware or any combination of the three.
- a component of the present invention is implemented as software, the component can be implemented as a script, as a standalone program, as part of a larger program, as a plurality of separate scripts and/or programs, as a statically or dynamically linked library, as a kernel loadable module, as a device driver, and/or in every and any other way known now or in the future to those of skill in the art of computer programming.
- the present invention is in no way limited to implementation in any specific programming language, or for any specific operating system or environment.
- Any form of computer readable medium can be used in this context, such as magnetic or optical storage media.
- software portions of the present invention can be instantiated (for example as object code or executable images) within the memory of any programmable computing device.
Abstract
Description
-
- Some customers will not have enough data to justify occupying multiple nodes.
- Customers might be divided into classes, and in those cases it might be considered desirable to separate the physical nodes by customer class.
-
- 1. The user's query will have a start_time and end_time (if not present in the query, some default start_time and end_time is picked)
- 2. Once the start_time and end_time is known, the nodegroups for the user in question are obtained from Zookeeper.
- 3. Then, the Query Server constructs an SQL query constrained by the “user”, “start_time”, “end_time” and this query accesses the required nodegroups in parallel. The query access in the third step may include forking the query, in parallel, into multiple sub queries which are sent to individual nodes holding the shards for a given customer and for the given time period.
-
- 1. Transaction rate: If, for instance, one system server can handle 1000 inserts/sec, and here are 5000 transactions per second, then five servers are needed to just keep up with the transaction rate. The customer transaction rate will here be represented by the algebraic symbol T. In various embodiments, the transactions may be managed by an external message broker, such as the Apache Qpid open source messaging broker.
- 2. Number of fields (dimensions and measures): The greater the number of fields or variables in the table, the slower the insert rate, and the more nodes that are needed to keep up with the gateway traffic. The number will be designated as F, the “fatness” of the table.
- 3. Indexing on these variables: Indexes help improve query performance, but they cost space and slow down the insert speeds. The number of indexes will be represented by the symbol I.
- 4. Amount of data stored: The amount can be defined in various terms, but for convenient example, it will be given in terms of a time interval, thus lending itself to time partitioning. Keeping 5 days of data, for instance, is less expensive than keeping five years. This amount of data, which in this example can be considered as history, will be represented as the symbol H.
- 5. Complexity of the queries: Complexity may itself be complex to measure, but in some ways it is proportional to the number of records touched in the database, more so than the number of records in the final answer. Suppose, for instance, that a query addressed two record fields, which could alternatively be referred to as “dimensions.” If the query specifies values for each of the two dimensions, such as dim1=x and dim2=y, then the database will have to search, separately, on the records that qualify for either one of the two conditions, and then do a Boolean intersection of the two sets of results, to determine which of the records satisfy both of the dimension criteria. That Boolean intersection will be smaller than or equal to the Boolean union of the two separate sets of results for the criteria taken individually. The complexity of the queries will be represented by the symbol X, which may for instance be defined as the fraction of the history H (see above) that is touched in a query. Other factors may further refine the complexity X.
- 6. Concurrent queries: The more the queries are executed in parallel with inserts, the more system resources will be drawn into play, and the greater the cost. This is represented by the symbol Q.
-
- 1. Small: T<500, F<10, I<5, H<1 month, X=“temporal queries only with time period<1 day”, Q=2 with no sharding
- 2. Medium: some other parameters
- 3. Large: Some other parameters where the customer is charged on multiples of 1000 TPS
The Class Definitions and Pricing for the classes will be stored in Zookeeper.
-
- 1. Decide on the number of nodes (represented here by the variable “z”) of the PostreSQL server.
- 2. A number “z” of EC2 instances are created, for instance by a datacenter automation and server management tool such as Puppet, or any other open source or commercial or homegrown datacenter and server management tool. Possible alternatives include open source tools like “Ansible” or “Cheff”.
- 3. For each instance, the datacenter management tool initializes PostreSQL with the setup parameters. This includes, for instance, setting up EBS (Amazon Webservices' “Elastic Block Storage”).
- 4. IP addresses of the “z” umber of EC2 instances are stored in Zookeeper.
- 5. The servers are monitored, using a suitable tool such as the Nagios open source software application, which offers computer system, monitoring, network monitoring and infrastructure monitoring.
-
- 1. When the system capacity reaches some threshold, Nagios generates alerts which can be used to create “n” more EC2 nodes, where the variable n may be one at a time, but may alternatively be greater than one, so as to allow the number of nodes to grow in quanta.
- 2. Puppet does steps 3, 4 and 5 discussed just above, in connection with PostreSQL.
- 3. The IP addresses of the new servers (in this context, server and node mean the same thing) are put into Zookeeper.
-
- 1. Nagios will have some threshold that will determine that the server is dead. The threshold may, for instance, be a time period over which the node fails to respond to a communication from another system component.
- 2. The system, or a suitable controller, or the Nagios application, will inform Zookeeper that the node is dead.
- 3. The database system will come to know that the node is dead, and they will have to declare the queries that are in flight, and that involve communicating with that node, to return an error.
- 4. All inflight messages from the node's message broker, such as Qpid, will be assumed to be lost. Message brokers act as places to buffer (in-memory or on-disk or both) the incoming messages. In an embodiment, the incoming transactional messages are ‘parked’ in the message broker first. Because database writes can be slow, or there may be network outage, there is a need for a place to safely park messages and insert at a later time when the network connectivity is restored.
- 5. Nagios generates alerts which can be used to launch a new EC2 node, and Puppet performs steps 3, 4 and 5 from above. The new node will be mounted with the EBS of the old node.
- 6. Eventually Qpid will start routing data to this new node. Because of transient issues, a small number of messages would have been lost. Therefore the new queries that touch this new node may not return exactly the right answers, whenever the query needs data records that might have been lost.
-
- 1. Fetch Class(CI) from Zookeeper.
- 2. Determine a node group NG(CI) (calculation given later)
- 3. Generate a Schema(CI) through an administrative user interface, ensuring that any applicable class policies are followed.
- 4. For each node NK (where NK is in the node group NG(CI)) perform the following steps:
-
- 1. Creating maxTP(CI) copies of the FACT table and index.
- 2. For the earliest time partition, where cur=0; set the starting tables and index.
- 3. Set the gateway to collect and send the right information.
- 4. The node group NG(CI) information should already have passed to the Qpid servers, so operation should be correct.
-
- 1. Add the new node to the node group NG(CI).
- 2. Record the time at which the new node was added. This denotes that there are several node groups NG(CI), separated by time intervals at which they became valid.
- 3. For each time partition, perform the Create PostreSQL Schema(CI) steps, listed above, on the new node.
- 4. The new node group NG(CI) information would already have passed to the Qpid servers, so they should be doing the right thing.
- 5. Insert records.
- 6. It should not be necessary to rebalance. Rebalancing is preferably avoided, because of its detrimental effect on performance. Even if some shards of a TP are less loaded than others, once that TP is full, balance will be restored from that point onwards.
Claims (25)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/748,225 US9875272B1 (en) | 2015-06-23 | 2015-06-23 | Method and system for designing a database system for high event rate, while maintaining predictable query performance |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/748,225 US9875272B1 (en) | 2015-06-23 | 2015-06-23 | Method and system for designing a database system for high event rate, while maintaining predictable query performance |
Publications (1)
Publication Number | Publication Date |
---|---|
US9875272B1 true US9875272B1 (en) | 2018-01-23 |
Family
ID=60956868
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/748,225 Active 2036-05-25 US9875272B1 (en) | 2015-06-23 | 2015-06-23 | Method and system for designing a database system for high event rate, while maintaining predictable query performance |
Country Status (1)
Country | Link |
---|---|
US (1) | US9875272B1 (en) |
Cited By (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20170103092A1 (en) * | 2015-10-07 | 2017-04-13 | Oracle International Corporation | Ddl processing in sharded databases |
US10509803B2 (en) * | 2016-02-17 | 2019-12-17 | Talentica Software (India) Private Limited | System and method of using replication for additional semantically defined partitioning |
US10628462B2 (en) * | 2016-06-27 | 2020-04-21 | Microsoft Technology Licensing, Llc | Propagating a status among related events |
CN111552667A (en) * | 2020-04-29 | 2020-08-18 | 杭州海康威视系统技术有限公司 | Data deleting method and device and electronic equipment |
CN111563125A (en) * | 2020-05-08 | 2020-08-21 | 中国工商银行股份有限公司 | Data storage system, data query method and device |
WO2021006778A1 (en) * | 2019-07-08 | 2021-01-14 | Telefonaktiebolaget Lm Ericsson (Publ) | Methods and systems for multidimensional data sharding in distributed databases |
CN112632154A (en) * | 2020-12-30 | 2021-04-09 | 城云科技（中国）有限公司 | Method and device for determining parallel service quantity and time interval based on time data |
CN113254437A (en) * | 2020-02-11 | 2021-08-13 | 北京京东振世信息技术有限公司 | Batch processing job processing method and device |
US11194773B2 (en) * | 2019-09-12 | 2021-12-07 | Oracle International Corporation | Integration of existing databases into a sharding environment |
US11550505B1 (en) * | 2020-09-01 | 2023-01-10 | Amazon Technologies, Inc. | Intra-shard parallelization of data stream processing using virtual shards |
US20230124867A1 (en) * | 2021-10-18 | 2023-04-20 | Sap Se | In-memory storage cluster consistency and availability |
EP4076691A4 (en) * | 2019-12-20 | 2023-06-14 | Niantic, Inc. | Sharded storage of geolocated data with predictable query response times |
CN117521619A (en) * | 2024-01-04 | 2024-02-06 | 北京百灵天地环保科技股份有限公司 | Report generation method based on lower computer, computer equipment and storage medium |
Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20020065919A1 (en) * | 2000-11-30 | 2002-05-30 | Taylor Ian Lance | Peer-to-peer caching network for user data |
-
2015
- 2015-06-23 US US14/748,225 patent/US9875272B1/en active Active
Patent Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20020065919A1 (en) * | 2000-11-30 | 2002-05-30 | Taylor Ian Lance | Peer-to-peer caching network for user data |
Cited By (23)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10983970B2 (en) | 2015-10-07 | 2021-04-20 | Oracle International Corporation | Relational database organization for sharding |
US10496614B2 (en) * | 2015-10-07 | 2019-12-03 | Oracle International Corporation | DDL processing in shared databases |
US20170103092A1 (en) * | 2015-10-07 | 2017-04-13 | Oracle International Corporation | Ddl processing in sharded databases |
US11204900B2 (en) | 2015-10-07 | 2021-12-21 | Oracle International Corporation | Request routing and query processing in a sharded database |
US10509803B2 (en) * | 2016-02-17 | 2019-12-17 | Talentica Software (India) Private Limited | System and method of using replication for additional semantically defined partitioning |
US10628462B2 (en) * | 2016-06-27 | 2020-04-21 | Microsoft Technology Licensing, Llc | Propagating a status among related events |
WO2021006778A1 (en) * | 2019-07-08 | 2021-01-14 | Telefonaktiebolaget Lm Ericsson (Publ) | Methods and systems for multidimensional data sharding in distributed databases |
US11194773B2 (en) * | 2019-09-12 | 2021-12-07 | Oracle International Corporation | Integration of existing databases into a sharding environment |
US11687530B2 (en) | 2019-12-20 | 2023-06-27 | Niantic, Inc. | Sharded storage of geolocated data with predictable query response times |
EP4076691A4 (en) * | 2019-12-20 | 2023-06-14 | Niantic, Inc. | Sharded storage of geolocated data with predictable query response times |
CN113254437A (en) * | 2020-02-11 | 2021-08-13 | 北京京东振世信息技术有限公司 | Batch processing job processing method and device |
CN113254437B (en) * | 2020-02-11 | 2023-09-01 | 北京京东振世信息技术有限公司 | Batch processing job processing method and device |
CN111552667B (en) * | 2020-04-29 | 2023-11-03 | 杭州海康威视系统技术有限公司 | Data deleting method and device and electronic equipment |
CN111552667A (en) * | 2020-04-29 | 2020-08-18 | 杭州海康威视系统技术有限公司 | Data deleting method and device and electronic equipment |
CN111563125A (en) * | 2020-05-08 | 2020-08-21 | 中国工商银行股份有限公司 | Data storage system, data query method and device |
CN111563125B (en) * | 2020-05-08 | 2024-04-16 | 中国工商银行股份有限公司 | Data storage system, data query method and device |
US11550505B1 (en) * | 2020-09-01 | 2023-01-10 | Amazon Technologies, Inc. | Intra-shard parallelization of data stream processing using virtual shards |
CN112632154A (en) * | 2020-12-30 | 2021-04-09 | 城云科技（中国）有限公司 | Method and device for determining parallel service quantity and time interval based on time data |
CN112632154B (en) * | 2020-12-30 | 2024-03-12 | 城云科技（中国）有限公司 | Method and device for determining parallel service quantity and time interval based on time data |
US20230124867A1 (en) * | 2021-10-18 | 2023-04-20 | Sap Se | In-memory storage cluster consistency and availability |
US11768817B2 (en) * | 2021-10-18 | 2023-09-26 | Sap Se | In-memory storage cluster consistency and availability |
CN117521619A (en) * | 2024-01-04 | 2024-02-06 | 北京百灵天地环保科技股份有限公司 | Report generation method based on lower computer, computer equipment and storage medium |
CN117521619B (en) * | 2024-01-04 | 2024-03-29 | 北京百灵天地环保科技股份有限公司 | Report generation method based on lower computer, computer equipment and storage medium |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9875272B1 (en) | Method and system for designing a database system for high event rate, while maintaining predictable query performance | |
US9996565B2 (en) | Managing an index of a table of a database | |
US8555018B1 (en) | Techniques for storing data | |
US9697273B2 (en) | Unique value calculation in partitioned table | |
US9158843B1 (en) | Addressing mechanism for data at world wide scale | |
US9418101B2 (en) | Query optimization | |
US10002170B2 (en) | Managing a table of a database | |
US10180984B2 (en) | Pivot facets for text mining and search | |
KR101976220B1 (en) | Recommending data enrichments | |
US7774318B2 (en) | Method and system for fast deletion of database information | |
CN103620601A (en) | Joining tables in a mapreduce procedure | |
US10255307B2 (en) | Database object management for a shared pool of configurable computing resources | |
US10289707B2 (en) | Data skipping and compression through partitioning of data | |
US20170212930A1 (en) | Hybrid architecture for processing graph-based queries | |
CN112434015B (en) | Data storage method and device, electronic equipment and medium | |
US9514184B2 (en) | Systems and methods for a high speed query infrastructure | |
US20190340179A1 (en) | Result set output criteria | |
US10169083B1 (en) | Scalable method for optimizing information pathway | |
US20220398250A1 (en) | Database optimization using record correlation and intermediate storage media | |
US20240045878A1 (en) | Building and using a sparse time series database (tsdb) | |
JP2013171495A (en) | Data management device, data management method and data management program | |
US20240045852A1 (en) | Performing an operation in a tree structure | |
US20160253372A1 (en) | Database query processing | |
Morton | Snowflake Architecture | |
CN117435589A (en) | Data transfer method, device, computer equipment and storage medium |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: APIGEE CORPORATION, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:JHINGRAN, ANANT;KUMAR, RUPESH;BOSE, SANJOY;SIGNING DATES FROM 20150504 TO 20150803;REEL/FRAME:036387/0061 |
|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:APIGEE CORPORATION;REEL/FRAME:040955/0070Effective date: 20170104 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044567/0001Effective date: 20170929 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |