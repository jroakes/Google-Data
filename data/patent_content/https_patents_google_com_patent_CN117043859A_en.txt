CN117043859A - Lookup table cyclic language model - Google Patents
Lookup table cyclic language model Download PDFInfo
- Publication number
- CN117043859A CN117043859A CN202280023642.6A CN202280023642A CN117043859A CN 117043859 A CN117043859 A CN 117043859A CN 202280023642 A CN202280023642 A CN 202280023642A CN 117043859 A CN117043859 A CN 117043859A
- Authority
- CN
- China
- Prior art keywords
- tokens
- sequence
- insert
- token
- candidate
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 125000004122 cyclic group Chemical group 0.000 title description 8
- 238000012545 processing Methods 0.000 claims abstract description 49
- 238000013518 transcription Methods 0.000 claims abstract description 46
- 230000035897 transcription Effects 0.000 claims abstract description 46
- 238000000034 method Methods 0.000 claims abstract description 36
- 230000015654 memory Effects 0.000 claims description 46
- 238000013528 artificial neural network Methods 0.000 claims description 17
- 230000000306 recurrent effect Effects 0.000 claims description 14
- 238000004891 communication Methods 0.000 claims description 7
- 238000009826 distribution Methods 0.000 description 10
- 238000004590 computer program Methods 0.000 description 8
- 238000012549 training Methods 0.000 description 8
- 230000008569 process Effects 0.000 description 6
- 230000003190 augmentative effect Effects 0.000 description 5
- 238000010586 diagram Methods 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 4
- 230000006870 function Effects 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 230000006855 networking Effects 0.000 description 2
- 239000013598 vector Substances 0.000 description 2
- 230000009471 action Effects 0.000 description 1
- 230000004913 activation Effects 0.000 description 1
- 238000001994 activation Methods 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 230000002452 interceptive effect Effects 0.000 description 1
- 238000002372 labelling Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000012423 maintenance Methods 0.000 description 1
- 239000003550 marker Substances 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000010606 normalization Methods 0.000 description 1
- 238000005457 optimization Methods 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/083—Recognition networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/205—Parsing
- G06F40/216—Parsing using statistical methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/237—Lexical tools
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/284—Lexical analysis, e.g. tokenisation or collocates
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/289—Phrasal analysis, e.g. finite state techniques or chunking
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/289—Phrasal analysis, e.g. finite state techniques or chunking
- G06F40/295—Named entity recognition
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/0464—Convolutional networks [CNN, ConvNet]
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/187—Phonemic context, e.g. pronunciation rules, phonotactical constraints or phoneme n-grams
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/183—Speech classification or search using natural language modelling using context dependencies, e.g. language models
- G10L15/19—Grammatical context, e.g. disambiguation of the recognition hypotheses based on word sequence rules
- G10L15/197—Probabilistic grammars, e.g. word n-grams
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L2015/088—Word spotting
Abstract
A computer-implemented method (400) comprising: audio data (120) corresponding to an utterance (119) spoken by a user (10) and captured by a user device (102) is received. The method further includes processing the audio data to determine candidate transcriptions (132) of a sequence of tokens (133) comprising the spoken utterance. For each token in the sequence of tokens, the method includes determining a token insert for the corresponding token (312), determining an n-gram token insert for a preceding sequence of n-gram tokens (322), and concatenating the token insert and the n-gram token insert to generate a concatenated output for the corresponding token (335). The method further includes rescaling candidate transcriptions of the spoken utterance by processing the generated join outputs for each corresponding token in the sequence of tokens.
Description
Technical Field
The present disclosure relates to a look-up table loop language model.
Background
In recent years, popularity of Automatic Speech Recognition (ASR) systems has increased for assistant-enabled devices. Improving recognition of infrequently spoken words is a continuing problem for ASR systems. Infrequently spoken words are rarely included in acoustic training data, and therefore, ASR systems have difficulty recognizing accurately in speech. In some examples, the ASR system includes a language model trained on text-only data to improve recognition of infrequently spoken words. However, these language models typically include large memory and computational requirements that result in reduced efficiency of the ASR system.
Disclosure of Invention
One aspect of the present disclosure provides a computer-implemented method that, when executed on data processing hardware, causes the data processing hardware to perform operations for performing speech recognition using a look-up table loop language model. The operation includes: audio data corresponding to an utterance spoken by a user and captured by a user device is received. The operations also include processing the audio data using the speech recognizer to determine candidate transcriptions of the spoken utterance including a sequence of tokens. For each token in the sequence of tokens, the operations include determining a token insert for the corresponding token using a first insert table, determining an n-gram token insert for a previous sequence of n-gram tokens using a second insert table, and concatenating the token insert and the n-gram token insert to generate a concatenated output for the corresponding token. The operations also include re-scoring candidate transcriptions of the spoken utterance using the external language model by processing the generated join outputs for each corresponding token in the sequence of tokens.
Implementations of the disclosure may include one or more of the following optional features. In some implementations, the external language model includes a recurrent neural network language model. The external language model may be integrated with the speech recognizer by factoring (factoring) the speech recognizer by a Hybrid Autoregressive Transducer (HAT). In some examples, the speech recognizer includes a convolutional enhanced transformer (transformer) audio encoder and a recurrent neural network transducer decoder. In other examples, the speech recognizer includes a transducer (transducer) audio encoder and a recurrent neural network transducer decoder. Alternatively, each term in the sequence of terms of the candidate transcript may represent a word in the candidate transcript. Each term in the sequence of terms of the candidate transcript may represent a word segment in the candidate transcript
In some implementations, each of the token sequences of the candidate transcriptions represents an n-gram, a phoneme, or a grapheme in the candidate transcription. In some examples, the first embedded table and the second embedded table are sparsely stored on memory hardware in communication with the data processing hardware. Determining the token embedding of the corresponding token may include retrieving the token embedding from the first embedding table via a lookup without accessing any graphics processing unit and/or tensor processing unit. Alternatively, the data processing hardware may reside on the user device.
Another aspect of the present disclosure provides a system that includes data processing hardware and memory hardware storing instructions that, when executed on the data processing hardware, cause the data processing hardware to perform operations. The operation includes: audio data corresponding to an utterance spoken by a user and captured by a user device is received. The operations also include processing the audio data using the speech recognizer to determine candidate transcriptions of the sequence of spoken utterances that include the lemma. For each token in the sequence of tokens, the operations include determining a token insert for the corresponding token using a first insert table, determining an n-gram token insert for a preceding sequence of n-gram tokens using a second insert table, and concatenating the token insert and the n-gram token insert to generate a concatenated output for the corresponding token. The operations also include rescaling candidate transcriptions of the spoken utterance using the external language model by processing the generated join outputs for each corresponding token in the sequence of tokens.
Implementations of the disclosure may include one or more of the following optional features. In some implementations, the external language model includes a recurrent neural network language model. The external language model may be integrated with the speech recognizer by factoring the speech recognizer with a Hybrid Autoregressive Transducer (HAT). In some examples, the speech recognizer includes a convolutional enhanced transducer audio encoder and a recurrent neural network transducer decoder. In other examples, the speech recognizer includes a transducer audio encoder and a recurrent neural network transducer decoder. Alternatively, each term in the sequence of terms of the candidate transcript may represent a word in the candidate transcript. Each term in the sequence of terms of the candidate transcript may represent a word segment in the candidate transcript
In some implementations, each of the token sequences of the candidate transcriptions represents an n-gram, a phoneme, or a grapheme in the candidate transcription. In some examples, the first embedded table and the second embedded table are sparsely stored on memory hardware in communication with the data processing hardware. Determining the token embedding of the corresponding token may include retrieving the token embedding from the first embedding table via a lookup without accessing any graphics processing unit and/or tensor processing unit. Alternatively, the data processing hardware may reside on the user device.
The details of one or more embodiments of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will be apparent from the description and drawings, and from the claims.
Drawings
FIG. 1 is an example system for integrating a speech recognition model with a language model having an n-gram embedded look-up table.
FIG. 2 is an example speech recognition model.
FIG. 3 is a schematic diagram of the example language model of FIG. 1.
FIG. 4 is an example arrangement of operations of a method for performing speech recognition using a look-up table loop language model.
FIG. 5 is a schematic diagram of an example computing device that may be used to implement the systems and methods described herein.
Like reference symbols in the various drawings indicate like elements.
Detailed Description
Improved recognition of rare words or sequences is a continuing problem in speech recognition systems where the misrecognition of many input text utterances that occur at zero or low frequency in the acoustic data. In particular, proper nouns such as street names, cities, etc. are rarely spoken (i.e., long-tailed content) and are often not included in acoustic training data, making long-tailed content difficult to recognize by a speech recognition system. In some implementations, the speech recognition system integrates a language model trained with text-only data that includes long-tail content that is rarely spoken. That is, the language model may be trained on a corpus of text-only data that includes long-tailed content that is missing in acoustic data, and may bias the speech recognition system toward correctly decoding the long-tailed content.
In order to accurately model large amounts of long tail content, the language model needs to expand the size of the embedded vocabulary. The embedded vocabulary represents embedded identities associated with each of the vocabulary of the language model (i.e., words, word pieces (words), n-grams, etc.). In most examples, augmenting the embedded vocabulary includes augmenting the vocabulary of tokens, which requires a significant amount of memory and computing resources that burden the speech recognition system. In addition, downstream tasks of the speech recognition system are also performed by the augmented vocabulary of words.
Embodiments herein are directed to a cyclic language model that augments an embedded vocabulary while maintaining a constant size of the vocabulary of words. That is, the embedded vocabulary is augmented independently of the vocabulary of words, allowing for more accurate modeling without burdening the computing resources of the speech recognition system. In particular, the cyclic language model includes a first embedding table that generates a token embedding for a current token (e.g., word slice, n-gram, etc.) in a token sequence and a second embedding table that generates a sequence embedding for a preceding token (e.g., n-gram sequence) in the token sequence. Here, the token embedding provides a likelihood of transcribing a particular word for the speech recognition system, and the sequence embedding provides a likelihood of transcribing a particular word based on a particular sequence of previously transcribed words. Thus, the second embedded table augments the embedded vocabulary of the language model while the vocabulary of the vocabulary elements remains constant.
Expanding the embedded vocabulary of the language model with the second embedded table does not require additional operations (e.g., computing resources) of the speech recognition system because the size of the embedded vocabulary has no impact on the number of embedded lookups or operations per output step. Thus, the only practical constraint on the size of the embedded vocabulary is memory capacity. In some examples, the embedded tables are accessed sparsely and need not be stored on a Graphics Processing Unit (GPU) and/or a Tensor Processing Unit (TPU). Rather, the embedded tables may be stored on a Computer Processing Unit (CPU) memory, disk, or other storage that includes a capacity that is substantially greater than the GPU and/or TPU memory.
Referring now to FIG. 1, in some implementations, an example speech recognition system 100 includes user devices 102 associated with respective users 10. User device 102 may communicate with remote system 110 via network 104. The user device 102 may correspond to a computing device such as a mobile phone, computer, wearable device, smart appliance, audio infotainment system, smart speaker, etc., and is equipped with data processing hardware 103 and memory hardware 105. The remote system 110 may be a single computer, multiple computers, or a distributed system (e.g., cloud environment) with scalable/resilient computing resources 112 (e.g., data processing hardware) and/or storage resources 114 (e.g., memory hardware).
The user device 102 may receive streaming audio 118 captured by one or more microphones 106 of the user device 102 corresponding to an utterance 119 spoken by the user 10 and extract acoustic features from the streaming audio 118 to generate audio data 120 corresponding to the utterance 119. The acoustic features may include mel-frequency cepstral coefficients (MFCCs) or filter bank energies calculated over a window of audio data 120 corresponding to the utterance 119. The user device 102 passes the audio data 120 corresponding to the utterance 119 to a speech recognizer 200 (also referred to herein as an Automatic Speech Recognizer (ASR) model 200). The ASR model 200 may reside on and execute on the user device 102. In other implementations, the ASR model 200 resides on and executes on the remote system 110.
Referring to FIG. 2, the ASR model 200 may provide end-to-end (E2E) speech recognition by integrating acoustic, pronunciation, and language models into a single neural network, and does not require lexicon or separate text normalization components. Various structures and optimization mechanisms may provide increased accuracy and reduced model training time. The ASR model 200 may include a convolutional enhanced Transducer (Transducer) model architecture that adheres to latency constraints associated with interactive applications. The ASR model 200 provides little computational overhead and utilizes less memory requirements than conventional ASR architectures, such that the ASR model architecture is suitable for performing speech recognition entirely on the user device 102 (e.g., without requiring communication with the remote system 110). The ASR model 200 includes an audio encoder 201, a tag encoder 220, and a federated network 230. An audio encoder 201 that is substantially similar to an Acoustic Model (AM) in a conventional ASR system includes a neural network with multiple convolutional enhanced transducer layers. For example, the audio encoder 201 reads a sequence of d-dimensional feature vectors (e.g., acoustic frames in the streaming audio 118 (fig. 1)) x= (x) 1 ,x 2 ,…,x T ) WhereinAnd a higher order feature representation is generated at each time step. The higher order feature representation is denoted as ah 1 ,...,ah T . Alternatively, the audio encoder 201 may include a transformer (transformer) layer instead of the convolution enhanced transformer layer. Similarly, the tag encoder 220 may also include a neural network or look-up table embedded model of the transformer layer that, like the Language Model (LM), will so far output the non-blank symbol sequence y by the final Softmax layer 240 0 ,...,y ui-1 Processing paired predictive marker historiesDense representation Ih of coding u 。
Finally, the representation generated by the audio and tag encoder 201,220 uses the dense layer J by the union network 230 u,t And (5) combining. The federated network 230 then predicts P (z) as the distribution over the next output symbol u,t |x,t,y 1 ,…,y u-1 ). In other words, the federated network 230 generates a probability distribution over the possible speech recognition hypotheses at each output step (e.g., time step). Here, a "possible speech recognition hypothesis" corresponds to a set of output labels (also referred to as "phonetic units"), each representing a grapheme (e.g., symbol/character) or word piece or word that specifies a natural language. For example, when the natural language is english, the output set of labels may include twenty-seven (27) symbols, e.g., one label for each of the 26 letters of the english alphabet and one label for labeling a space. Thus, the federated network 230 may output a set of values that indicate the likelihood of the occurrence of each of the predetermined set of output tags. The set of values may be vectors and may indicate a probability distribution over the set of output tags. In some cases, the output label is a grapheme (e.g., an individual character, and potentially punctuation and other symbols), but the output label is not so limited. For example, the set of output tags may also include word sheets and/or whole words in addition to or instead of graphemes. The output profile of the federated network 230 may include a posterior probability value for each of the different output tags. Thus, if there are 100 different output labels representing different graphemes or other tokens, then the output z of the union network 230 u,t 100 different probability values may be included, one for each output label. The probability distribution may then be used (e.g., by Softmax layer 240) to select and assign scores to candidate orthographic (orthographic) elements (e.g., graphemes, word sheets, and/or words) in the bundle search process for use in determining transcription.
The Softmax layer 240 may employ any technique to select the output tag/symbol with the highest probability in the distribution as the next output symbol predicted by the ASR model 200 at the corresponding output step. In this way, the ASR model 200 does not make a condition-independent assumption, but rather the predictions for each symbol depend not only on acoustics, but also on the tag sequence output so far.
Referring back to FIG. 1, the ASR model 200 is configured to process the audio data 120 to determine candidate transcriptions 132 of the spoken utterance 119. Here, the candidate transcriptions include sequences of tokens 133,133a-n, where each token 133 represents a portion of the candidate transcription 132 of the utterance 119. That is, each token 133 in the sequence of tokens 133 may represent a potential word, word piece, n-gram, phoneme, and/or grapheme in the candidate transcription 132. For example, the ASR model 200 generates a candidate transcription 132"driving directions to bourbon (the route to bourbon) for the spoken utterance 119"driving directions to Beaubien (the route to Beaubien). In this example, each of the tokens 133 in the sequence of tokens 133 of the candidate transcription 132 may represent a single word. Thus, the sequence of lemmas 133 includes four (4) lemmas 133, each of which represents a single word in the candidate transcription 132 (e.g., "driving directions to bourbon"). Notably, the ASR model 200 misidentifies the correct term "Beaubien" as the fourth lemma 133 representing the term "bourbon".
Further, the ASR model 200 may generate each of the tokens 133 in the sequence of tokens 133 as a corresponding probability distribution over possible candidate tokens. For example, for a fourth lemma 133 in the sequence of lemmas 133, the ASR model 200 may generate the lemmas "bourbon" and "Beaubien" as corresponding possible lemmas, each possible lemma having a corresponding probability or likelihood that indicates the confidence that the ASR model 200 recognizes the possible lemmas for the respective fourth lemma 133. Here, the first and second candidate tokens are similar in terms of speech, however, the ASR model may be more prone to the first candidate token associated with the term "bourbon" than the second candidate token associated with the term "Beaubien" because "Beaubien" is a proper noun (e.g., rare words or long tail content) that may not be included in the acoustic training data of the ASR model 200. That is, because "Beaubien" is not included in the acoustic training data, or is included in only a small number of instances of the acoustic training data, the ASR model 200 may incorrectly recognize the particular term in the utterance 119. In other words, the ASR model 200 may be configured to output a higher probability/likelihood score for a first candidate term (e.g., bourbon) than for a second candidate term (e.g., beaubien). Thus, the ASR model outputs the fourth lemma 133 in the sequence of lemmas 133 as "bourbon" because of the higher probability/likelihood score. Although for simplicity this example describes only two candidate tokens in the probability distribution over possible candidate tokens, the number of possible candidate tokens in the probability distribution may be any number greater than two. In some examples, each of the tokens 133 in the sequence of tokens 133 is represented by an n-best list of possible candidate tokens 133 associated with a ranked list of n possible candidate tokens 133 having the highest probability/likelihood scores. Each candidate word element 133 may be referred to as a speech recognition hypothesis. In an additional example, each of the tokens 133 in the sequence of tokens 133 is represented by a likely candidate token 133 having the highest probability/likelihood score among the probability distributions over the likely candidate tokens 133.
The ASR model 200 passes the candidate transcription 132, which includes the sequence of tokens 133, to a language model (i.e., a cyclic language model) 300. The language model 300 may reside on the memory hardware 105 of the user device 102, or alternatively on the storage resources 114 of the remote system 110, or a combination thereof. The language model 300 is configured to determine a likelihood of each of the tokens 133 in the sequence of output candidate tokens 133. That is, the language model 300 re-scores the candidate transcriptions 132 output by the ASR model 200 by determining which candidate lemma 133, among the possible candidate lemmas 133 for each lemma 133 in the sequence of lemmas 133, most likely represents the corresponding lemma 133 in the candidate transcriptions 132 of the spoken utterance 119. The language model 300 may assist in biasing speech recognition hypotheses output by the ASR model 200 toward rare words, such as proper nouns or long tail content, that are rarely included or not included in training audio data used to train the ASR model 200. In the above example, the language model 300 may bias speech recognition by boosting the probability/likelihood score of the candidate lemma 133 associated with the word "Beaubien" in the probability distribution over possible candidate lemmas 133 of the fourth lemma 133 in the sequence of lemmas 133. Here, the enhanced probability/likelihood score of the candidate token 133 associated with the word "Beaubien" may now be higher than the probability/likelihood score of the candidate token "bourbon" to produce a re-scored transcript 345 that now correctly recognizes the word "Beaubien" from the spoken utterance 119.
The language model 300 may include a Recurrent Neural Network (RNN) language model. More specifically, the language model 300 may include a look-up table language model configured to expand the size of the RNN language model by expanding the number of rows embedded in the table, but with only minimal expansion in floating point operations. That is, the look-up table allows the embedding to be sparsely stored on memory hardware (e.g., CPU memory or disk) and retrieved from the table via a lookup such that the size of the table does not add additional operations to each forward propagation, thereby alleviating the need for storage on limited/constrained GPU/TPU memory.
The language model 300 includes a first embedded table 310, a second embedded table 320, a connector 330, and a Recurrent Neural Network (RNN) 340. In some examples, the language model 300 is trained on text-only data that includes rarely spoken words (i.e., long-tailed content) such that candidate lemmas of the ASR model 200 are biased to correctly transcribe the long-tailed content.
The first embedding table 310 is configured to generate a current token 133, t in a sequence of tokens 133 i Is embedded 312. Here, t i Representing the current token 133 in the sequence of tokens 133. The first embedding table 310 determines a respective token embedding 312 for each token 133 independent of the remainder of the tokens 133 in the sequence of tokens 133. On the other hand, the second embedding table 320 includes receiving the preceding n-gram lemma sequence 133, t at the current output step (e.g., time step) 0 ,...,t n-1 And generates an n-gram embedding table of the corresponding n-gram word embedding 322. At each time step, a preceding sequence of n-gram lemmas (e.g., t 0 ,...,t n-1 ) Providing contextual information about the previously generated lemma 133. FirstFirst n-gram word sequence t 0 ,...,t n-1 And increases exponentially with n at each time step. Thus, the n-gram lemma embedding 322 at each output step assists in short-range dependencies such as spelling out rare words and thereby improves modeling of long-tailed lemmas (e.g., words) in the subword model.
At each time step, the concatenator 330 concatenates the lemma insert 312 and the n-gram lemma insert 322 into a concatenated output 335. The RNN 340 receives the join output 335 and ultimately uses the join output 335 to re-score the candidate transcripts 132 output by the ASR model 200 to generate re-scored transcripts (e.g., final transcripts) 345. In addition, language model 300 may provide re-scored transcript 345 to user device 102 via network 104. The user device 102 may visually display the re-scored transcript 345 on a display of the user device 102 or audibly present the re-scored transcript 345 through one or more speakers of the user device 102. In other examples, the re-scored transcript 345 may be a query that instructs the user device 102 to perform an action.
In a single output step in the illustrated example, the second embedding table 320 may generate an n-gram lemma embedding 322 to represent the entire sequence of three preceding n-gram lemmas (e.g., "driving directions to") of the current token 133 (e.g., "bourbon") represented by the corresponding lemma embedding 312 looked up by the first embedding table 310. Thereafter, the concatenater 330 may concatenate the n-gram word embedment 322 and the corresponding word embedment 312 to generate a concatenated output 335 (e.g., "driving directions to bourbon"). RNN 340 re-scores candidate transcripts 132 by processing join output 335. For example, the RNN 340 can re-score the candidate transcripts 132 (e.g., driving directions to bourbon) such that the RNN 340 promotes likelihood/probability scores of "Beaubien" to have a higher likelihood/probability score for the fourth lemma 133 than "bourbon". The RNN 340 is based in part on determining that "bourbon" is unlikely to be associated with current context information (e.g., a preceding n-gram lemma sequence t 0 ,...,t n-1 ) To promote the likelihood/probability score of "Beaubien". Thus, RNN 340 generates a regenerationScored transcript 345 (e.g., driving directions to Beaubien). Notably, the fourth term 133 is now correctly recognized in the re-scored transcript 345, even if the fourth term 133 was incorrectly recognized in the candidate transcript 132.
Fig. 3 illustrates a schematic diagram of the cyclic language model 300 of fig. 1. The first embedding table 310 includes a plurality of lemma embeddings 312,312a-n and the second embedding table 320 includes a plurality of n-gram lemma embeddings 322,322a-n. The first embedding table 310 is represented by a UXE matrix, where E represents the embedding dimension (i.e., the embedding length) and U represents the number of lemma embeddings 322 in the first embedding table 310. Typically, the number of embeddings is equal to the number of unique tokens V (e.g., the number of words, word pieces, n-grams, etc.). That is, the first embedding table 310 stores a corresponding word-embedding 312 for each possible word 133 (e.g., word piece, etc.). For example, when the token 133 comprises a word segment, the recurring language model 300 may comprise a 4,096 word segment model having two LSTM layers of width 512, and the dimension E of the token insert 312 may be equal to 96. When n is set to four (4), the n-gram word embeddings 322 may each include dimensions 2048. The second embedding table (e.g., n-gram embedding table) 320 may store each preceding sequence of n-gram lemmas (e.g., t) via a modular hash as follows 0 ,...,t n-1 ) An embedded n-gram is assigned to be embedded (e.g., an embedded identifier) 322.
Notably, modular hashing necessarily conflicts such that any different n-gram embeddings will be hashed into the same n-gram lemma embedment 322. However, collisions are reduced and performance is improved by augmenting the number of unique tokens V.
In the example shown, the first and second embedding tables 310, 320 receive sequences of tokens 133 of the candidate transcription 132 corresponding to the spoken utterance 119"hello Pam". Notably, the candidate transcription 132 misidentifies the correct term "Pam" as the term "pan". Here, a first embedded table 310A third token 133c (e.g., t 2 ) The current lemma 133 in the corresponding candidate transcription 132. The first embedding table 310 determines a token embedding 312 from a plurality of token embeddings 312 of the third token 133c independent of the rest of the sequence of tokens 133. In this example, the first embedding table 310 determines "pan" as the third lemma 133c (represented by a black box), because "pan" may have a likelihood/probability score of 0.8, while "Pam" has a likelihood/probability score of 0.2.
The second embedding table 320 receives the preceding n-gram lemma sequence 133 in the candidate transcription 132. Here, the preceding n-gram lemma sequence 133 includes "<s>Hello "(e.g., t) 0 ,t 1 ) Wherein'<s>"a word element 133 representing the start of a sentence. The second embedded table 320 determines "based on the preceding n-gram lemma sequence 133"<s>Hello "is embedded 322 (represented by a black box).
The concatenator 330 concatenates the lemma insert 312 for the current lemma "pan" output from the first insert table 310 and the n-gram lemma insert 322 of "< s > Hello" and provides a concatenated output 335 (e.g., "< s > Hello pan") to the RNN unit of the RNN 340. Notably, the link output 335 causes the RNN 340 to increase the dense parameter in order to process the link output 335, however, since the RNN output dimension remains fixed, the scaling is linear rather than quadratic. The context information from the n-gram lemma insert 322 may be used not only as input to the RNN 340, but also for the middle layer such that the middle layer/unit receives the context information via hidden states specific to that layer. Thus, the RNN 340 can inject a join output 335 into the input activations of each layer in the RNN, each extracted from the embedded tables specific to that layer.
Continuing with the example above, the RNN 340 re-scores the candidate transcripts 132 by processing the join output 335. That is, the RNN 340 can augment the probability of the term "Pam" based on the context information (e.g., the preceding n-gram lemma sequence 133) included in the join output 335. Thus, RNN 340 can adjust the likelihood/probability score of "pan" to 0.4 and the likelihood/probability score of "Pam" to 0.6. In this way, the RNN generates a re-scored transcript that includes "Pam" as the third lemma 133c and correctly recognizes the spoken utterance 119.
The n-gram embedding table 320 can be effectively extended to almost billions of parameters. The cyclic language model 300 may be trained on a billion text-only sentence corpus. Language model 300 may be further preprocessed by misspellings removal against a whitelist of one million common words and ensuring representation on the tail by log (n) expansion of sentence frequencies.
The cyclic language model 300 may be integrated with the end-to-end ASR model 200 by first obtaining an effective likelihood, which is obtained by: the log-posterior (log-posterior) is separated from the internal language model scores of the ASR model 200 via Hybrid Autoregressive Transducer (HAT) Factorization (Factorization) as follows.
log p(y|x)≈log p(y|x)-λ 2 log P ILM (y) (2)
Thereafter, the language model log posterior score is added as follows.
RNN-T decoders (e.g., predictive and federated networks 220, 230 (fig. 2)) may be factored by HAT during training.
FIG. 4 is a flow chart of an exemplary arrangement of operations of a method 400 for performing speech recognition using a look-up table loop language model. At operation 402, the method 400 includes receiving audio data 120 corresponding to an utterance 119 spoken by a user 10 and captured by a user device 102. At operation 404, the method 400 includes processing the audio data 120 using the speech recognizer 200 to determine a candidate transcription 132 for the utterance 119. Here, candidate transcription 132 includes sequences of tokens 133,133 a-n. For each of the tokens 133 in the sequence of tokens 133, the method 400 performs operations 406-410. At operation 406, the method 400 includes determining a token embedding 312 for the corresponding token 133 using the first embedding table 310. In operation 408 The method 400 includes determining a previous n-gram lemma sequence 133, t using the second embedding table 320 0 ,...,t n-1 N-gram word embedding 322. At operation 410, the method 400 includes concatenating the lemma insert 312 and the n-gram lemma insert 322 to generate a concatenated output 335 of the corresponding lemma 133. At operation 412, the method 400 includes rescaling the candidate transcriptions 132 of the spoken utterance 119 using an external language model (e.g., RNN) 340 by processing the join outputs 335 generated for each corresponding token 133 in the sequence of tokens 133.
FIG. 5 is a schematic diagram of an example computing device 500 that may be used to implement the systems and methods described in this document. The computing device 500 is intended to represent various forms of digital computers, such as notebook computers, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. The components shown herein, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
Computing device 500 includes a processor 510, memory 520, storage device 530, high-speed interface/controller 540 connected to memory 520 and high-speed expansion port 550, and low-speed interface/controller 560 connected to low-speed bus 570 and storage device 530. Each of the components 510, 520, 530, 540, 550, and 560 are interconnected using various buses, and may be mounted on a common motherboard or in other manners as appropriate. The processor 510 may process instructions for execution within the computing device 500, including instructions stored in the memory 520 or on the storage device 530, to display graphical information for a Graphical User Interface (GUI) on an external input/output device, such as the display 580 coupled to the high-speed interface 540. In other embodiments, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. In addition, multiple computing devices 500 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multiprocessor system).
Memory 520 stores information non-transitory within computing device 500. Memory 520 may be a computer-readable medium, a volatile memory unit, or a non-volatile memory unit. Non-transitory memory 520 may be a physical device for storing programs (e.g., sequences of instructions) or data (e.g., program state information) for use by computing device 500 on a temporary or permanent basis. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electrically erasable programmable read-only memory (EEPROM) (e.g., commonly used for firmware such as a boot strap). Examples of volatile memory include, but are not limited to, random Access Memory (RAM), dynamic Random Access Memory (DRAM), static Random Access Memory (SRAM), phase Change Memory (PCM), and magnetic disk or tape.
The storage device 530 is capable of providing mass storage for the computing device 500. In some implementations, the storage device 530 is a computer-readable medium. In various embodiments, storage device 530 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory, or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. In additional embodiments, the computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer-or machine-readable medium, such as the memory 520, the storage device 530, or memory on processor 510.
The high speed controller 540 manages bandwidth-dense-type operations for the computing device 500, while the low speed controller 560 manages lower bandwidth-dense-type operations. Such assignment of tasks is merely exemplary. In some implementations, the high-speed controller 540 is coupled to the memory 520, the display 580 (e.g., via a graphics processor or accelerator), and the high-speed expansion port 550, which high-speed expansion port 550 may accept various expansion cards (not shown). In some implementations, a low speed controller 560 is coupled to the storage device 530 and the low speed expansion port 590. The low-speed expansion port 590, which may include various communication ports (e.g., USB, bluetooth, ethernet, wireless ethernet), may be coupled to one or more input/output devices, such as a keyboard, pointing device, scanner, or networking device, such as a switch or router, for example, through a network adapter.
Computing device 500 may be implemented in a number of different forms, as shown. For example, it may be implemented as a standard server 500a or multiple times in a group of such servers 500a, may be implemented as a notebook computer 500b, or as part of a rack server system 500 c.
Various implementations of the systems and techniques described here can be realized in digital electronic and/or optical circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various embodiments may include embodiments in one or more computer programs executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
A software application (i.e., a software resource) may refer to computer software that causes a computing device to perform tasks. In some examples, a software application may be referred to as an "application," app, "or" program. Example applications include, but are not limited to, system diagnostic applications, system management applications, system maintenance applications, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and gaming applications.
These computer programs (also known as programs, software applications or code) include machine instructions for a programmable processor, and may be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. The terms "machine-readable medium" and "computer-readable medium" as used herein refer to any computer program product, non-transitory computer-readable medium, apparatus and/or device (e.g., magnetic discs, optical disks, memory, programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term "machine-readable signal" refers to any signal used to provide machine instructions and/or data to a programmable processor.
The processes and logic flows described herein can be performed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memories for storing instructions and data. Typically, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, the computer need not have such devices. Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disk; CD ROM and DVD-ROM discs. The processor and the memory may be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, one or more aspects of the disclosure can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor or a touch screen for displaying information to the user and, optionally, a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other types of devices may also be used to provide interaction with the user, for example, feedback provided to the user may be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. Further, the computer may interact with the user by sending and receiving documents to and from devices used by the user, for example, by sending Web pages to a Web browser on the user's client device in response to requests received from the Web browser.
Many embodiments have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly, other embodiments are within the scope of the following claims.
Claims (22)
1. A computer-implemented method (400), which when executed on data processing hardware (103), causes the data processing hardware (103) to perform operations comprising:
receiving audio data (120) corresponding to an utterance (119) spoken by a user (10) and captured by a user device (102);
processing the audio data (120) using a speech recognizer (200) to determine candidate transcriptions (132) of the spoken utterance (119), the candidate transcriptions (132) comprising sequences of lemmas (133);
for each token (133) in the sequence of tokens (133):
determining a token embedding (312) of the corresponding token (133) using the first embedding table (310):
determining an n-gram lemma insert (322) of the preceding sequence of n-gram lemmas (133) using a second insert table (320); and
a concatenation output (335) concatenating the lemma insert (312) and the n-gram lemma insert (322) to generate a corresponding lemma (133); and
the candidate transcriptions (132) of the spoken utterance (119) are rescored using an external language model (300) by processing a join output (335) generated for each corresponding token (133) in the sequence of tokens (133).
2. The method (400) of claim 1, wherein the external language model (300) comprises a recurrent neural network (340) language model.
3. The method (400) of claim 1 or 2, wherein the external language model (300) is integrated with the speech recognizer (200) by factoring the speech recognizer (200) by a Hybrid Autoregressive Transducer (HAT).
4. The method (400) of any of claims 1-3, wherein the speech recognizer (200) comprises a convolutional enhanced transducer audio encoder (201) and a recurrent neural network transducer decoder (220).
5. The method (400) of any of claims 1-3, wherein the speech recognizer (200) comprises a transducer audio encoder (201) and a recurrent neural network transducer decoder (220).
6. The method (400) of any of claims 1-5, wherein each of the tokens (133) in the sequence of tokens (133) of the candidate transcription (132) represents a word in the candidate transcription (132).
7. The method (400) of any of claims 1-6, wherein each of the tokens (133) in the sequence of tokens (133) of the candidate transcription (132) represents a word segment in the candidate transcription (132).
8. The method (400) of any of claims 1-7, wherein each of the tokens (133) in the sequence of tokens (133) of the candidate transcription (132) represents an n-gram, phoneme, or grapheme in the candidate transcription (132).
9. The method (400) of any of claims 1-8, wherein the first embedded table (310) and the second embedded table (320) are sparsely stored on memory hardware (105) in communication with the data processing hardware (103).
10. The method (400) of any of claims 1-9, wherein determining a lexeme insert (312) for a corresponding lexeme (133) comprises: retrieving the lemma insert (312) from the first insert table (310) via a lookup without accessing any graphics processing unit and/or tensor processing unit.
11. The method (400) according to any of claims 1-10, wherein the data processing hardware (103) resides on the user equipment (102).
12. A system (100) comprising:
data processing hardware (103); and
memory hardware (105) in communication with the data processing hardware (103), the memory hardware (105) storing instructions that, when executed by the data processing hardware (103), cause the data processing hardware (103) to perform operations comprising:
receiving audio data (120) corresponding to an utterance (119) spoken by a user (10) and captured by a user device (102):
Processing the audio data (120) using a speech recognizer (200) to determine candidate transcriptions (132) of the spoken utterance (119), the candidate transcriptions (132) comprising sequences of lemmas (133);
for each token (133) in the sequence of tokens (133):
determining a token embedding (312) of the corresponding token (133) using the first embedding table (310):
determining an n-gram lemma insert (322) of the preceding sequence of n-gram lemmas (133) using a second insert table (320); and
a concatenation output (335) concatenating the lemma insert (312) and the n-gram lemma insert (322) to generate a corresponding lemma (133); and
the candidate transcriptions (132) of the spoken utterance (119) are rescored using an external language model (300) by processing a join output (335) generated for each corresponding token (133) in the sequence of tokens (133).
13. The system (100) of claim 12, wherein the external language model (300) comprises a recurrent neural network (340) language model.
14. The system (100) of claim 12 or 13, wherein the external language model (300) is integrated with the speech recognizer (200) by factoring the speech recognizer (200) with a Hybrid Autoregressive Transducer (HAT).
15. The system (100) according to any one of claims 12 to 14, wherein the speech recognizer (200) comprises a convolutional enhanced transducer audio encoder (201) and a recurrent neural network transducer decoder (220).
16. The system (100) according to any one of claims 12 to 14, wherein the speech recognizer (200) comprises a transducer audio encoder (201) and a recurrent neural network transducer decoder (220).
17. The system (100) according to any one of claims 12 to 16, wherein each of the tokens (133) in the sequence of tokens (133) of the candidate transcription (132) represents a word in the candidate transcription (132).
18. The system (100) according to any one of claims 12 to 17, wherein each of the tokens (133) in the sequence of tokens (133) of the candidate transcription (132) represents a word segment in the candidate transcription (132).
19. The system (100) of any of claims 1 to 18, wherein each of the tokens (133) in the sequence of tokens (133) of the candidate transcription (132) represents an n-gram, phoneme, or grapheme in the candidate transcription (132).
20. The system (100) of any of claims 12 to 19, wherein the first embedded table (310) and the second embedded table (320) are sparsely stored on memory hardware (105) in communication with the data processing hardware (103).
21. The system (100) of any of claims 12 to 20, wherein determining a lexeme insert (312) for the corresponding lexeme (133) includes: retrieving the lemma insert (312) from the first insert table (310) via a lookup without accessing any graphics processing unit and/or tensor processing unit.
22. The system (100) according to any one of claims 12 to 21, wherein the data processing hardware (103) resides on the user equipment (102).
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163165725P | 2021-03-24 | 2021-03-24 | |
US63/165,725 | 2021-03-24 | ||
PCT/US2022/015956 WO2022203773A1 (en) | 2021-03-24 | 2022-02-10 | Lookup-table recurrent language model |
Publications (1)
Publication Number | Publication Date |
---|---|
CN117043859A true CN117043859A (en) | 2023-11-10 |
Family
ID=80449228
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202280023642.6A Pending CN117043859A (en) | 2021-03-24 | 2022-02-10 | Lookup table cyclic language model |
Country Status (6)
Country | Link |
---|---|
US (1) | US20220310067A1 (en) |
EP (1) | EP4295358A1 (en) |
JP (1) | JP2024512579A (en) |
KR (1) | KR20230156125A (en) |
CN (1) | CN117043859A (en) |
WO (1) | WO2022203773A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20230186898A1 (en) * | 2021-12-14 | 2023-06-15 | Google Llc | Lattice Speech Corrections |
Family Cites Families (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10431210B1 (en) * | 2018-04-16 | 2019-10-01 | International Business Machines Corporation | Implementing a whole sentence recurrent neural network language model for natural language processing |
-
2022
- 2022-02-10 KR KR1020237034901A patent/KR20230156125A/en active Search and Examination
- 2022-02-10 JP JP2023558607A patent/JP2024512579A/en active Pending
- 2022-02-10 EP EP22706188.4A patent/EP4295358A1/en active Pending
- 2022-02-10 WO PCT/US2022/015956 patent/WO2022203773A1/en active Application Filing
- 2022-02-10 CN CN202280023642.6A patent/CN117043859A/en active Pending
- 2022-02-10 US US17/650,566 patent/US20220310067A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
KR20230156125A (en) | 2023-11-13 |
US20220310067A1 (en) | 2022-09-29 |
JP2024512579A (en) | 2024-03-19 |
WO2022203773A1 (en) | 2022-09-29 |
EP4295358A1 (en) | 2023-12-27 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11545142B2 (en) | Using context information with end-to-end models for speech recognition | |
CN113692616B (en) | Phoneme-based contextualization for cross-language speech recognition in an end-to-end model | |
CN113811946A (en) | End-to-end automatic speech recognition of digital sequences | |
EP4085451B1 (en) | Language-agnostic multilingual modeling using effective script normalization | |
CN116250038A (en) | Transducer of converter: unified streaming and non-streaming speech recognition model | |
US20230377564A1 (en) | Proper noun recognition in end-to-end speech recognition | |
US20230104228A1 (en) | Joint Unsupervised and Supervised Training for Multilingual ASR | |
CN117378004A (en) | Supervised and unsupervised training with loss of alignment of sequences | |
CN117099157A (en) | Multitasking learning for end-to-end automatic speech recognition confidence and erasure estimation | |
CN117043859A (en) | Lookup table cyclic language model | |
US20220310097A1 (en) | Reducing Streaming ASR Model Delay With Self Alignment | |
US20220310081A1 (en) | Multilingual Re-Scoring Models for Automatic Speech Recognition | |
US20230107475A1 (en) | Exploring Heterogeneous Characteristics of Layers In ASR Models For More Efficient Training | |
US20240013777A1 (en) | Unsupervised Data Selection via Discrete Speech Representation for Automatic Speech Recognition | |
US20230298591A1 (en) | Optimizing Personal VAD for On-Device Speech Recognition | |
WO2024086265A1 (en) | Context-aware end-to-end asr fusion of context, acoustic and text representations | |
WO2024091426A1 (en) | Parameter-efficient model reprogramming for cross-lingual speech recognition | |
KR20230156795A (en) | Word segmentation regularization |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |