US20230130634A1 - Optimizing Inference Performance for Conformer - Google Patents
Optimizing Inference Performance for Conformer Download PDFInfo
- Publication number
- US20230130634A1 US20230130634A1 US17/936,547 US202217936547A US2023130634A1 US 20230130634 A1 US20230130634 A1 US 20230130634A1 US 202217936547 A US202217936547 A US 202217936547A US 2023130634 A1 US2023130634 A1 US 2023130634A1
- Authority
- US
- United States
- Prior art keywords
- module
- encoder
- causal
- attention
- speech recognition
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000001364 causal effect Effects 0.000 claims abstract description 168
- 238000000034 method Methods 0.000 claims abstract description 37
- 238000013528 artificial neural network Methods 0.000 claims abstract description 9
- 230000000306 recurrent effect Effects 0.000 claims abstract description 9
- 230000015654 memory Effects 0.000 claims description 35
- 238000012549 training Methods 0.000 claims description 26
- 238000012545 processing Methods 0.000 claims description 14
- PXFBZOLANLWPMH-UHFFFAOYSA-N 16-Epiaffinine Natural products C1C(C2=CC=CC=C2N2)=C2C(=O)CC2C(=CC)CN(C)C1C2CO PXFBZOLANLWPMH-UHFFFAOYSA-N 0.000 claims description 5
- 230000006403 short-term memory Effects 0.000 claims description 5
- 230000009466 transformation Effects 0.000 claims description 5
- 230000008569 process Effects 0.000 description 13
- 238000004590 computer program Methods 0.000 description 8
- 238000013518 transcription Methods 0.000 description 7
- 230000035897 transcription Effects 0.000 description 7
- 239000013598 vector Substances 0.000 description 7
- 230000008901 benefit Effects 0.000 description 6
- 238000004891 communication Methods 0.000 description 6
- 230000003287 optical effect Effects 0.000 description 6
- 238000003058 natural language processing Methods 0.000 description 5
- 230000003993 interaction Effects 0.000 description 4
- 230000006870 function Effects 0.000 description 3
- 230000004044 response Effects 0.000 description 3
- 230000004913 activation Effects 0.000 description 2
- 238000001994 activation Methods 0.000 description 2
- 230000003111 delayed effect Effects 0.000 description 2
- 230000001537 neural effect Effects 0.000 description 2
- 230000009471 action Effects 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 238000000354 decomposition reaction Methods 0.000 description 1
- 230000010354 integration Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 230000037361 pathway Effects 0.000 description 1
- 230000004043 responsiveness Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000005236 sound signal Effects 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
- G06N3/0442—Recurrent networks, e.g. Hopfield networks characterised by memory or gating, e.g. long short-term memory [LSTM] or gated recurrent units [GRU]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
- G06N3/0455—Auto-encoder networks; Encoder-decoder networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/0464—Convolutional networks [CNN, ConvNet]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/047—Probabilistic or stochastic networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/09—Supervised learning
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
Definitions
- This disclosure relates to optimizing inference performance for conformers.
- Automated speech recognition (ASR) systems have evolved from multiple models where each model had a dedicated purposed to integrated models were a single neural network is used to directly map an audio waveform (i.e., input sequence) to an output sentence (i.e., output sequence).
- This integration has resulted in a sequence-to-sequence approach, which generates a sequence of words (or graphemes) when given a sequence of audio features.
- these integrated models include multiple self-attention layers that maintain a large number of internal states.
- devices that implement these integrated models have limited memory bandwidth such that reading from each of these internals states results in increased latency of the ASR systems.
- ASR automated speech recognition
- the causal encoder is configured to receive a sequence of acoustic frames as input and generate, at each of a plurality of output steps, a first higher order feature representation for a corresponding acoustic frame in the sequence of acoustic frames.
- the ASR model also includes a decoder configured to receive, as input, the first higher order feature representation generated by the causal encoder at each of the plurality of output steps and generate, at each of the plurality of output steps, a first probability distribution over possible speech recognition hypotheses.
- each causal encoder layer in the stack of causal encoder layers includes a Recurrent Neural Network (RNN) Attention-Performer module that applies linear attention.
- RNN Recurrent Neural Network
- each causal encoder layer includes a first feedforward module, a convolution module, a multi-head attention module, a second feedforward module, and a layernorm module.
- each causal encoder layer includes the first feedforward module, the convolution module, the RNN Attention-Performer module, the second feedforward module, and the layernorm module.
- Each causal encoder layer may be pre-trained using regular conformer training for the multi-head attention module and is fine-tuned by replacing the multi-head attention module with the RNN Attention-Performer module.
- replacing the multi-head attention module with the RNN Attention-Performer module may include applying a trainable affine transformation to convert queries/keys from the pre-trained multi-head attention module into an RNN model of linear time and constant memory complexity in sequence length.
- the causal encoder further includes an initial stack of convolution blocks without self-attention.
- the ASR model further includes a non-causal encoder configured to receive, as input, the first higher order feature representation generated by the causal encoder at each of the plurality of output steps and generate, at each of the plurality of output steps, a second higher order feature representation for a corresponding higher order feature representation.
- the decoder is further configured to receive, as input, the second higher order feature representation generated by the non-causal encoder at each of the plurality of output steps and generate, at each of the plurality of output steps, a second probability distribution over possible speech recognition hypotheses.
- the decoder includes a prediction network configured to receive, as input, a sequence of non-blank symbols output by a final softmax layer and generate, at each of the plurality of output steps, a dense representation.
- the decoder also includes a joint network configured to: receive, as input, the dense representation generated by the prediction network at each of the plurality of output steps and one of the first higher order feature representation generated by the causal encoder at each of the plurality of output steps when the ASR model is operating in a streaming mode or the second higher order feature representation generated by the non-causal encoder at each of the plurality of output steps when the ASR model is operating in a non-streaming mode; and generate, at each of the plurality of output steps, one of the first probability distribution over possible speech recognition hypotheses when the ASR model is operating in the streaming mode or the second probability distribution over possible speech recognition hypotheses when the ASR model is operating in the non-streaming mode.
- the prediction network may include a long short-term memory (LSTM)-based
- the operations include receiving a sequence of acoustic frames as input to an automatic speech recognition (ASR) model.
- the ASR model includes a causal encoder and a decoder.
- the operations also include generating, by the causal encoder at each of a plurality of output steps, a first higher order feature representation for a corresponding acoustic frame in the sequence of acoustic frames.
- the operations also include generating, by the decoder at each of the plurality of output steps, a first probability distribution over possible speech recognition hypotheses.
- the causal encoder includes a stack of causal encoder layers where each causal encoder layer in the stack of causal encoder layers includes a Recurrent Neural Network (RNN) Attention-Performer module that applies linear attention.
- RNN Recurrent Neural Network
- each causal encoder layer includes a first feedforward module, a convolution module, a multi-head attention module, a second feedforward module, and a layernorm module.
- each causal encoder layer includes the first feedforward module, the convolution module, the RNN Attention-Performer module, the second feedforward module, and the layernorm module.
- the operations may further include pre-training each causal encoder layer using regular conformer training for the multi-head attention module and fine-tuning each causal encoder layer by replacing the multi-head attention module with the RNN Attention-Performer module.
- replacing the multi-head attention module with the RNN Attention-Performer module includes applying a trainable affine transformation to convert queries/keys from the pre-trained multi-head attention module into an RNN model of linear time and constant memory complexity in sequence length.
- the causal encoder may further include an initial stack of convolution blocks without self-attention.
- the operations further include generating, by a non-causal encoder of the ASR model at each of the plurality of output steps, a second higher order feature representation for a corresponding first higher order feature representation generated by the causal encoder and generating, by the decoder at each of the plurality of output steps, a second probability distribution over possible speech recognition hypotheses for a corresponding second higher order feature representation.
- the operations may further include: receiving, as input at a prediction network of the decoder, a sequence of non-blank symbols output by a final softmax layer; generating a dense representation by the prediction network; and generating, by a joint network of the decoder, one of the first probability distribution over possible speech recognition hypotheses when the ASR model is operating in a streaming mode or the second probability distribution over possible speech recognition hypotheses when the ASR model is operating in a non-streaming mode.
- the prediction network includes a long short-term memory (LSTM)-based prediction network.
- the prediction network may include a V2 embedding look-up table.
- FIG. 1 is a schematic view of an example speech recognition system.
- FIGS. 2 A- 2 C are schematic views of an ASR model operating in various combinations of streaming and non-streaming modes.
- FIGS. 3 A- 3 C are schematic views of various example causal encoder layers of the ASR model.
- FIGS. 4 A- 4 C are schematic views of various example stacks of causal encoder layers of the ASR model.
- FIG. 5 is a flowchart of an example arrangement of operations for a computer-implemented method of optimizing inference performance of conformers.
- FIG. 6 is a schematic view of an example computing device that may be used to implement the systems and methods described herein.
- End-to-end (E2E) automated speech recognition (ASR) models are traditionally structured to operate in a streaming mode or a non-streaming mode.
- an E2E ASR model includes an encoder and a decoder as the main components.
- Applications that involve end-user interaction like voice-search or on-device dictation, may require the model to perform recognition in a streaming fashion, where the words are expected to be output as they are spoken with as little latency as possible. This prevents the use of models that use future context to improve accuracy, such as bi-directional long short-term memory (LSTM).
- LSTM bi-directional long short-term memory
- applications such as offline video capturing do not require streaming recognition and may make full use of any available future context to improve performance.
- the encoder of the E2E ASR models may include self-attention layers such as conformer or transformer layers.
- self-attention layers such as conformer or transformer layers.
- a drawback of these self-attention layers is that the number of internal states to maintain is much larger than for LSTM layers. Specifically, most of these internals states correspond to key and value tensors used for self-attention. As a result, latency of these E2E ASR models during inference increases due to the computational cost of repeatedly loading the large number of internal states.
- implementations herein are directed towards an ASR model that includes a causal encoder (e.g., first encoder) having a stack of casual encoder layers.
- the causal encoder is configured to receive a sequence of acoustic frames corresponding to an utterance and generate a first higher order feature representation for a corresponding acoustic frame.
- the ASR model also includes a decoder configured to generate a first probability distribution over possible speech recognition hypotheses for a corresponding first higher order feature representation.
- each casual encoder layer in the stack of encoder layers includes a Recurrent Neural Network (RNN) Attention-Performer module that applies linear attention during fine-tune training and inference of the ASR model.
- RNN Recurrent Neural Network
- the causal encoder may retain the benefit derived from self-attention layers (e.g., conformer or transformer layers) during pre-training while using RNN Attention-Performer module layers during fine-tune training and inference thereby reducing latency and model size of the ASR model making the ASR model suitable for on-device applications.
- the ASR model may also include a non-causal encoder (i.e., second encoder) connected in cascade to the causal encoder to further improve accuracy of the ASR model for applications where latency is not a limiting constraint.
- FIG. 1 is an example system 100 within a speech environment 101 .
- a user may be through voice input.
- the user device 10 (also referred to generally as a device 10 ) is configured to capture sounds (e.g., streaming audio data) from one or more users 104 within the speech environment 101 .
- the streaming audio data may refer to a spoken utterance 106 by the user 104 that functions as an audible query, a command for the user device 10 , or an audible communication captured by the device 10 .
- Speech-enabled systems of the user device 10 may field the query or the command by answering the query and/or causing the command to be performed/fulfilled by one or more downstream applications.
- the user device 10 may correspond to any computing device associated with a user 104 and capable of receiving audio data.
- Some examples of user devices 10 include, but are not limited to, mobile devices (e.g., mobile phones, tablets, laptops, etc.), computers, wearable devices (e.g., smart watches), smart appliances, internet of things (IoT) devices, vehicle infotainment systems, smart displays, smart speakers, etc.
- the user device 10 includes data processing hardware 12 and memory hardware 14 in communication with the data processing hardware 12 and stores instructions, that when executed by the data processing hardware 12 , cause the data processing hardware 12 to perform one or more operations.
- the user device 10 further includes an audio system 16 with an audio capture device (e.g., microphone) 16 , 16 a for capturing and converting spoken utterances 106 within the speech environment 101 into electrical signals and a speech output device (e.g., speaker) 16 , 16 b for communicating an audible audio signal (e.g., as output audio data from the user device 10 ). While the user device 10 implements a single audio capture device 16 a in the example shown, the user device 10 may implement an array of audio capture devices 16 a without departing from the scope of the present disclosure, whereby one or more capture devices 16 a in the array may not physically reside on the user device 10 , but be in communication with the audio system 16 .
- an audio capture device e.g., microphone
- a speech output device e.g., speaker
- the system 100 includes an automated speech recognition (ASR) system 118 implementing an ASR model 200 that resides on the user device 10 of the user 104 and/or on a remote computing device 60 (e.g., one or more remote servers of a distributed system executing in a cloud-computing environment) in communication with the user device 10 via a network 40 .
- ASR model 200 includes a recurrent neural network-transducer (RNN-T) model architecture.
- the user device 10 and/or the remote computing device 60 also includes an audio subsystem 108 configured to receive the utterance 106 spoken by the user 104 and captured by the audio capture device 16 a , and convert the utterance 106 into a corresponding digital format associated with input acoustic frames 110 capable of being processed by the ASR system 118 .
- the user speaks a respective utterance 106 and the audio subsystem 108 converts the utterance 106 into corresponding audio data (e.g., sequence of acoustic frames) 110 for input to the ASR system 118 .
- the ASR model 200 receives, as input, the sequence of acoustic frames 110 corresponding to the utterance 106 , and generates/predicts, at each output step, a corresponding transcription 120 (e.g., speech recognition result/hypothesis) of the utterance 106 as the ASR model 200 receives (e.g., processes) each acoustic frame 110 in the sequence of acoustic frames 110 .
- a corresponding transcription 120 e.g., speech recognition result/hypothesis
- the ASR model 200 may perform streaming speech recognition to produce an initial speech recognition result (e.g., candidate hypothesis) 120 , 120 a and generate a final speech recognition result (e.g., final hypothesis) 120 , 120 b by improving the initial speech recognition result 120 a .
- the initial and final speech recognition result 120 a , 120 b may either correspond to a partial speech recognition result or an entire speech recognition result. Stated differently, the initial and final speech recognition result 120 a , 120 b may either correspond to a portion of an utterance 106 or an entire portion of an utterance 106 .
- the partial speech recognition result may correspond to a portion of a spoken utterance or even a portion of a spoken term.
- the ASR model 200 performs additional processing on the final speech recognition result 120 b whereby the final speech recognition result 120 b may be delayed from the initial speech recognition result 120 a .
- the user device 10 and/or the remote computing device 60 also executes a user interface generator 107 configured to present a representation of the transcription 120 of the utterance 106 to the user 104 of the user device 10 .
- the user interface generator 107 may display the initial speech recognition result 120 a in a streaming fashion during time 1 and subsequently display the final speech recognition result 120 b in a streaming fashion during time 2.
- the ASR model 200 outputs the final speech recognition result 120 b in a streaming fashion even though the final speech recognition result 120 b improves upon the initial speech recognition result 120 a .
- the transcription 120 output from the ASR system 118 is processed (e.g., by a natural language understanding (NLU) module executing on the user device 10 or the remote computing device 60 ) to execute a user command/query specified by the utterance 106 .
- NLU natural language understanding
- a text-to-speech system (not shown) (e.g., executing on any combination of the user device 10 or the remote computing device 60 ) may convert the transcription 120 into synthesized speech for audible output by the user device 10 and/or another device.
- the user 104 interacts with a program or application 50 (e.g., the digital assistant application 50 ) of the user device 10 that uses the ASR system 118 .
- FIG. 1 depicts the user 104 communicating with the digital assistant application 50 and the digital assistant application 50 displaying a digital assistant interface 18 on a screen of the user device 10 to depict a conversation between the user 104 and the digital assistant application 50 .
- the user 104 asks the digital assistant application 50 , “What time is the concert tonight?”
- This question from the user 104 is a spoken utterance 106 captured by the audio capture device 16 a and processed by audio systems 16 of the user device 10 .
- the audio system 16 receives the spoken utterance 106 and converts it into a sequence of acoustic frames 110 for input to the ASR system 118 .
- the ASR model 200 while receiving the sequence of acoustic frames 110 corresponding to the utterance 106 as the user 104 speaks, encodes the sequence of acoustic frames 110 and then decodes the encoded sequence of acoustic frames 110 into the initial speech recognition result 120 a .
- the user interface generator 107 presents, via the digital assistant interface 18 , a representation of the initial speech recognition result 120 a of the utterance 106 to the user 104 of the user device 10 in a streaming fashion such that words, word pieces, and/or individual characters appear on the screen as soon as they are spoken.
- the first look ahead audio context is equal to zero.
- the user interface generator 107 presents, via the digital assistant interface 18 , a representation of the final speech recognition result 120 b of the utterance 106 to the user 104 of the user device 10 a streaming fashion such that words, word pieces, and/or individual characters appear on the screen as soon as they are generated by the ASR model 200 .
- the user interface generator 107 replaces the representation of the initial speech recognition result 120 a presented at time 1 with the representation of the final speech recognition result 120 b presented at time 2.
- time 1 and time 2 may include timestamps corresponding to when the user interface generator 107 presents the respective speech recognition result 120 .
- the timestamp of time 1 indicates that the user interface generator 107 presents the initial speech recognition result 120 a at an earlier time than the final speech recognition result 120 b .
- the final speech recognition result 120 b ultimately displayed as the transcription 120 may fix any terms that may have been misrecognized in the initial speech recognition result 120 a .
- the streaming initial speech recognition result 120 a output by the ASR model 200 is displayed on the screen of the user device 10 at time 1 are associated with low latency and provide responsiveness to the user 104 that his/her query is being processed, while the final speech recognition result 120 b output by the ASR model 200 and displayed on the screen at time 2 leverages an additional speech recognition model and/or a language model to improve the speech recognition quality in terms of accuracy, but at increased latency.
- the initial speech recognition result 120 a are displayed as the user speaks the utterance 106 , the higher latency associated with producing, and ultimately displaying the final speech recognition result 120 b is not noticeable to the user 104 .
- the digital assistant application 50 may respond to the question posed by the user 104 using natural language processing.
- Natural language processing generally refers to a process of interpreting written language (e.g., the initial speech recognition result 120 a and/or the final speech recognition result 120 b ) and determining whether the written language prompts any action.
- the digital assistant application 50 uses natural language processing to recognize that the question from the user 104 regards the user’s schedule and more particularly a concert on the user’s schedule.
- the automated assistant returns a response 19 to the user’s query where the response 19 states, “Venue doors open at 6:30 PM and concert starts at 8 pm.”
- natural language processing occurs on a remote server 60 in communication with the data processing hardware 12 of the user device 10 .
- FIGS. 2 A- 2 C include example ASR models 200 a - c operating in various combinations of streaming and non-streaming modes.
- each of the ASR models 200 a - c includes an encoder 202 and a decoder 204 .
- the encoder 202 may include a causal encoder (i.e., first encoder) 210 that includes a stack of causal encoder layers 300 and a non-causal encoder (i.e., second encoder) 220 .
- FIGS. 3 A- 3 C illustrate various example causal encoder layers 300 of the first encoder 210 . More specifically, FIG. 3 A is a schematic view of a first example causal encoder layer (e.g., conformer or transformer layer) 300 , 300 a that may be used to implement one or more of the causal encoder layers 300 from the stack of causal encoder layers 300 of the first encoder 210 .
- a first example causal encoder layer e.g., conformer or transformer layer
- the first example causal encoder layer 300 a includes a first half feed-forward layer 310 , a second half feed-forward layer 340 , with a convolution module 320 and a multi-head self-attention module 330 disposed between the first and second half feed-forward layers 310 , 340 , concatenation operators 305 , and a layernorm module 350 .
- the first half feed-forward layer 310 processes the sequence of acoustic frames 110 (e.g., Mel-spectrogram input sequence) by projecting the acoustic frames 110 into a larger dimension, followed by a non-linear activation, and then another linear layer to project the features back to the original dimensions.
- the convolution module 320 subsamples the sequence of acoustic frames 110 concatenated with the output of the first half feed-forward layer 310 . That is, the convolution module 320 aggregates information from neighboring context to capture relative offset-based local interactions.
- the multi-head self-attention module 330 may include self-attention layers such as conformer or transformer layers.
- the multi-head self-attention module 330 receives the output of the convolution module 320 concatenated with the output of the first half feed-forward layer 310 .
- the role of the multi-head self-attention module 330 is to summarize noise context separately for each input frame that is to be enhanced.
- the multi-head self-attention module 330 looks back L previous frames and converts an output into a fixed-length vector thereby capturing more global patterns.
- the multi-head self-attention module 330 maintains a large number of internal states. A significant portion of these internal states correspond to the key and value tensors of self-attention causing an increase in latency due to repeatedly loading each of these internal states (e.g., quadratic computational cost).
- the second half feed-forward layer 340 receives a concatenation of the output of the multi-head self-attention module 330 and the output of the convolution module 320 .
- the layernorm module 350 processes a concatenation of the output from the second half feed-forward layer 340 and the output of the multi-head self-attention module 330 . That is, the first example causal encoder layer 300 a transforms each acoustic frame 110 in the sequence of acoustic frames 110 (e.g., input features x), using modulation features m, to generate, at each output step, an output 355 for a corresponding acoustic frame 110 in the sequence of acoustic frames 110 . More specifically, the first example causal encoder layer 300 a may generate the output 355 by:
- the output 355 of the first example causal encoder layer 300 a is passed on to the next causal encoder layer 300 in the stack of causal encoder layers 300 of the first encoder 210 .
- a last causal encoder layer 300 in the stack of causal encoder layers 300 generates a first higher order feature representation 212 for a corresponding acoustic frame 110 in the sequence of acoustic frames 110 .
- FIG. 3 B is a schematic view of a second example causal encoder layer 300 , 300 b that may be used to implement one or more of the causal encoder layers 300 from the stack of causal encoder layers 300 .
- the second example causal encoder layer 300 b includes the first half feed-forward layer 310 , the second half feed-forward layer 340 , with the convolution module 320 and a Recurrent Neural Network (RNN) Attention-Performer module 360 disposed between the first and second half feed-forward layers 310 , 340 , concatenation operators 305 , and the layernorm module 350 . That is, the second example causal encoder layer 300 b replaces the multi-head self-attention module 330 from the first example causal encoder layer 300 a ( FIG.
- RNN Recurrent Neural Network
- the RNN Attention-Performer module 360 replaces the quadratic attention of the multi-head self-attention module 330 with a more manageable linear attention and leverages low-rank decomposition of the attention tensor.
- the RNN Attention-Performer 366 includes a unidirectional performer that performs prefix-sum determinations that emulate causal attention of the multi-head self-attention module. More specifically, the RNN Attention-Performer module 360 determines a matrix by summing outer products of kernel features corresponding to keys with value-vectors. At each iteration of the prefix-sum determination, a kernel feature vector corresponding to a query is multiplied by the prior prefix-sum to generate a new embedding. Here, the RNN Attention-Performer module 360 obtains the prior prefix-sum by summing all outer-products corresponding to preceding tokens. Moreover, RNN Attention-Performer module 360 obtains features by applying rectified linear unit (ReLU) elementwise activations to affinely-transformed queries/keys.
- ReLU rectified linear unit
- the first half feed-forward layer 310 processes the sequence of acoustic frames 110 (e.g., Mel-spectrogram input sequence). Subsequently, the convolution module 320 subsamples the sequence of acoustic frames 110 concatenated with the output of the first feed-forward layer 310 .
- the RNN Attention-Performer module 360 receives the output of the convolution module 320 concatenated with the output of the first half feed-forward layer 310 and applies linear attention to generate an output. Thereafter, the second half feed-forward layer 340 receives a concatenation of the output of the RNN Attention-Performer module 360 and the output of the convolution module 320 .
- the layernorm module 350 processes the output from the second half feed-forward layer 340 and the multi-head self-attention module 330 output to generate the output 355 .
- the output 355 of the second example causal encoder layer (i.e., RNN Performer layer) 300 b is passed on to the next causal encoder layer 300 in the stack of causal encoder layers 300 of the first encoder 210 .
- a last causal encoder layer 300 in the stack of causal encoder layers 300 generates the first higher order feature representation 212 for a corresponding acoustic frame 110 in the sequence of acoustic frames 110 .
- FIG. 3 C is a schematic view of a third example causal encoder layer 300 , 300 c that may be used to implement one or more of the causal encoder layers 300 from the stack of causal encoder layers 300 .
- the third example causal encoder layer (i.e., convolution blocks) 300 c includes the first half feed-forward layer 310 , the convolution module 320 , and concatenation operators 305 .
- the third example causal encoder layer 300 c does not include any self-attention modules (e.g., multi-head self-attention module 330 and RNN Attention-Performer module 360 ) thereby reducing the model size and compute resources of the ASR model 200 .
- self-attention modules e.g., multi-head self-attention module 330 and RNN Attention-Performer module 360
- the initial encoder layers 300 of the stack of causal encoder layers 300 may include the third example causal encoder layer 300 c .
- the first half feed-forward layer 310 processes the sequence of acoustic frames 110 (e.g., Mel-spectrogram input sequence).
- the convolution module 320 subsamples the sequence of acoustic frames 110 concatenated with the output of the first half feed-forward layer 310 to generate an output 325 .
- the output 325 of the third example causal encoder layer 300 c is passed on to the next causal encoder layer 300 in the stack of causal encoder layers 300 of the first encoder 210 .
- a last causal encoder layer 300 in the stack of causal encoder layers 300 (e.g., first or second example causal encoder layer 300 a , 300 b ) generates the first higher order feature representation 212 for a corresponding acoustic frame 110 in the sequence of acoustic frames 110 .
- FIGS. 4 A- 4 C include example configurations 400 of the first encoder 210 having various stacks of causal encoder layers 300 .
- the example configurations 400 shown in FIGS. 4 A- 4 C only illustrate the stack of causal encoder layers 300 having five (5) causal encoder layers 300 for the sake of clarity only, as it is understood that the stack of causal encoder layers 300 may include any number of causal encoder layers 300 .
- FIG. 4 A illustrates a first configuration 400 , 400 a of the stack of causal encoder layers 300 for the first encoder 210 during a pre-training stage of the ASR model 200 .
- each causal encoder layer 300 in the stack of causal encoder layers 300 of the first encoder 210 includes the first example causal encoder layers (e.g., conformer layer) 300 a 1 - a 5 .
- each causal encoder layer 300 in the stack of causal encoder layers 300 the first configuration 400 a includes the multi-head self-attention module 330 that performs ordinary self-attention. Since each causal encoder layer 300 in the stack of causal encoder layers of the first configuration 400 a includes a conformer layer, the first configuration 400 a represents a conformer or encoder whereby each first example causal encoder layer (e.g., conformer layer) uses regular conformer training during the pre-training stage.
- FIG. 4 B illustrates a second configuration 400 , 400 b of the stack of causal encoder layers 300 for the first encoder 210 used during fine-tune training, and subsequently inference, of the ASR model 200 .
- each causal encoder layer 300 in the stack of causal encoder layers 300 of the first encoder 210 includes the second example causal encoder layers 300 b 1 - b 5 .
- the second configuration 400 b replaces each of the first example causal encoder layers 300 a ( FIG. 3 A ) with the second example causal encoder layers 300 b ( FIG. 3 B ).
- each causal encoder layer 300 in the stack of causal encoder layers in the second configuration 400 b includes the RNN Attention-Performer module 360 that performs linear attention.
- the first encoder 210 retains the benefit learnt during pre-training with the multi-head self-attention module 330 while also having the advantage of the smaller latency and smaller model size of the RNN Attention-Performer module 360 of the second example causal encoder layers 300 b ( FIG. 3 B ).
- FIG. 4 C illustrates a third configuration 400 , 400 c of the first encoder 210 used during fine-tune training and inference of the ASR model 200 .
- the initial causal encoder layers 300 in the stack of causal encoder layers 300 of the first encoder 210 include the third example causal encoder layers 300 c and the subsequent causal encoder layers 300 in the stack of causal encoder layers 300 of the first encoder 210 include the second example causal encoder layers 300 b .
- the third configuration 400 c replaces each of the first example causal encoder layers 300 a ( FIG. 3 A ) in the initial causal encoder layers 300 with third example causal encoder layers 300 c and replaces each of the first example causal encoder layers 300 a ( FIG.
- the second example causal encoder layer includes the RNN Attention-Performer module 360 rather than the multi-head self-attention module 330 .
- Replacing the multi-head self-attention module 330 with the RNN Attention-Performer module 360 includes applying a trainable affine transformation to convert queries/keys from the multi-head self-attention module 330 into an RNN model (e.g., RNN Attention-Performer module 360 ) of linear time and constant memory complexity in sequence length.
- the first and second causal encoder layers (e.g., initial causal encoder layers) 300 c 1 , 300 c 2 include the third example causal encoder layers 300 c while the third, fourth, and fifth causal encoder layers 300 b 1 - b 3 (e.g., subsequent causal encoder layers) include the second example causal encoder layers 300 b .
- the initial causal encoder layers are not so limited, for example, the initial causal encoder layers may include any number of layers from the stack of causal encoder layers.
- the third configuration 400 c of the first encoder 210 uses the simpler initial causal encoder layers to hypothesize low-level features and uses the subsequent causal encoder layers that perform linear attention.
- the encoder 202 is a cascading encoder.
- the cascading encoder 202 refers to a model structure where the encoding pathway of the first encoder 210 and the second encoder 220 cascade such that the output of the first encoder 210 feeds the input of the second encoder 220 prior to decoding.
- the first and second encoders 210 , 220 can be cascaded irrespective of the underlying architecture for each encoder.
- the first and second encoders 210 , 220 include a stack of multi-headed (e.g., 8 heads) attention layers.
- the stack of multi-headed attention layers may include a stack of conformer layers or a stack of transformer layers.
- Causal convolution and left-context attention layers may be used for each causal encoder layer in the stack of causal encoder layers of the first encoder 210 to restrict the model from using future inputs.
- the cascading encoder may include 17 causal encoder layers.
- the first encoder 210 may include 15 causal encoder layers while the second encoder 220 may include two causal encoder layers that take in additional right context (e.g., 5.04 seconds).
- the first encoder 210 may be referred to as a causal encoder and the second encoder 220 may be referred to as a non-causal encoder.
- the first encoder 210 is a streaming encoder while the second encoder 220 is a non-streaming encoder.
- the second encoder 220 receives the output of the first encoder 210 and may take advantage of the causal encoder layers 300 of the first encoder 210 such that the second encoder includes fewer multi-head attention layers than the first encoder 210 .
- the cascading encoder may reduce the number of more computationally expensive layers making the ASR model 200 more streamlined than simply combining a traditional streaming model with a traditional non-streaming model.
- d-dimensional feature vectors e.g., acoustic frames 110 shown in FIG. 1
- x (x 1 , x 2 , ..., x T )
- the second encoder 220 is connected in cascade to the first encoder 210 and is configured to receive the first higher order feature representation 212 generated by the first encoder 210 at each of the plurality of output steps, as input, and generate, at each output step, a second higher order feature representation 222 for a corresponding first higher order feature representation 212 .
- the second encoder 220 generates the second higher order feature representations 222 using additional right-context (e.g., future input frames) while the first encoder 210 generates the first higher order feature representation 212 without using any additional right-context.
- the second encoder 220 generates the second higher order feature representation 222 using the first higher order feature representation 212 and does not receive any of the acoustic frames 110 .
- the decoder 204 may include a recurrent neural network-transducer (RNN-T) architecture having a joint network 230 and a prediction network 240 .
- the decoder 204 uses the joint network 230 to combine (i.e., when the model operates in a non-streaming mode) the first and second higher order feature representations 212 , 222 output by the encoder 202 at each of the plurality of output steps, as well as a hidden representation 242 output from the prediction network 240 for the previous prediction y r-1 , to generate a decoder output.
- RNN-T recurrent neural network-transducer
- the joint network 230 receives the hidden representation 242 output from the prediction network 240 and only the first higher order feature representation 212 output from the first encoder 210 (e.g., the joint network 230 does not receive the second higher order feature representation 222 ).
- the decoder output can be a probability distribution, P (y i
- the ASR model 200 may include a final softmax layer that receives the output of the decoder 204 and generates the sequence of non-blank symbols.
- the softmax layer is separate from the decoder 204 and processes the output from the decoder 204 .
- the output of the softmax layer is then used in a beam search process to select orthographic elements.
- the final softmax layer is integrated with the decoder 204 such that the output of the decoder 204 represents the output of the final softmax layer.
- the decoder 204 is configured to generate, at each output step, a probability distribution over possible speech recognition hypotheses.
- the decoder 204 When the ASR model 200 is operating in a streaming mode, the decoder 204 generates a first probability distribution 232 over possible speech recognition hypotheses for a corresponding first higher order feature representation 212 .
- the decoder 204 when the ASR model 200 is operating in a non-streaming mode, the decoder 204 generates a second probability distribution 234 over possible speech recognition hypotheses for a corresponding second higher order feature representation 222 .
- the joint network 230 generates, at each output step (e.g. time step), a probability distribution 232 , 234 over possible speech recognition hypotheses.
- the “possible speech recognition hypotheses” correspond to a set of output labels/symbols (also referred to as “speech units”) each representing a grapheme (e.g., symbol/character) or a word piece in a specified natural language.
- the set of output labels may include twenty-seven (27) symbols, e.g., one label for each of the 26-letters in the English alphabet and one label designating a space.
- the joint network may output a set of value indicative of the likelihood of occurrence of each of a predetermined set of output labels.
- This set of values can be a vector (e.g. a one-hot vector) and can indicate a probability distribution over the set of output labels.
- the output labels are graphemes (e.g., individual characters, and potentially punctuation and other symbols), but the set of output labels is not so limited.
- the set of output labels can include wordpieces and/or entire words, in addition to or instead of graphemes.
- the output labels could also be other types of speech units, such as phonemes or sub-phonemes.
- the output distribution of the joint network 230 can include a posterior probability value for each of the different output labels. Thus, if there are 100 different output labels representing different graphemes or other symbols, the output of the joint network 230 can include 100 different probability values, one for each output label.
- the probability distribution can then be used to select and assign scores to candidate orthographic elements (e.g., graphemes, wordpieces, and/or words) in a beam search process (e.g., by the Softmax layer) for determining the transcription 120 .
- candidate orthographic elements e.g., graphemes, wordpieces, and/or words
- the prediction network may have two 2,048-dimensional LSTM layers, each of which is also followed by a 640-dimensional projection layer, such that the LSTM-based prediction network may have about 23.4 million parameters.
- the prediction network 240 may instead include conformer or transformer layers in lieu of LSTM layers.
- the prediction network 240 includes a V2 embedding lookup table that includes an embedding prediction network.
- the V2 embedding lookup table may receive, as input, the previous two predictions (e.g., 1-hot vectors) output by the joint network 230 , determine a respective embedding d 1 , d 2 for each of the previous two predictions, and provide a concatenated output [d 1 , d 2 ] to the joint network 230 .
- the V2 embedding lookup table may have only about two (2) million parameters, whereas an LSTM-based prediction network may include about 23.4 million parameters.
- the joint network 230 may also be a one-layer neural network with 640 hidden units.
- the ASR model 200 a operating in both the streaming and non-streaming modes in parallel.
- the ASR model 200 a first performs streaming speech recognition on the sequence of acoustic frames 110 using the first encoder 210 to generate the first higher order feature representation 212 for both the second encoder 220 and the decoder 204 .
- the decoder 204 generates the first probability distribution 232 over possible speech recognition hypotheses to output partial speech recognition results 120 , 120 a .
- the ASR model 200 b also performs non-streaming speech recognition where the second encoder 220 uses the first higher order feature representation 212 received from the first encoder 210 to generate the second higher order feature representation 222 .
- the decoder 204 then generates the second probability distribution 234 over possible speech recognition hypotheses to output the final speech recognition result 120 , 120 b .
- the decoder 204 output the partial speech recognition results 120 a in a streaming fashion using the output from the first encoder 210 , and then operating in the non-streaming fashion uses the first higher order feature representation 212 to generate the final speech recognition result 120 b .
- the final speech recognition result 120 b for the input utterance 106 may be delayed from the partial speech recognition results 120 a .
- the ASR model 200 b operating only in the streaming mode. This may occur, for instance, when the user 104 ( FIG. 1 ) is using applications such as voice-search or on-device dictation, which requires as little latency as possible.
- the encoder 202 operates in a non-cascading mode such that only the first encoder 210 generates an output. More specifically, the ASR model 200 b performs streaming speechtimion on the sequence of acoustic frames 110 using only the first encoder 210 to generate the first higher order feature representation 212 for the decoder 204 . Thereafter, the decoder 204 generates the first probability distribution 232 over possible speech recognition results. Because the streaming mode of the ASR model 200 b generates the partial speech recognition results 120 a quickly, the inaccuracy of the term “play” is generally acceptable to users.
- the ASR model 200 c operates only in the non-streaming mode.
- the non-streaming mode may occur, for instance, in non-latency intensive applications such as when the user ( FIG. 1 ) is viewing a transcription of a voicemail left on his/her phone.
- this type of application benefits from the second encoder 220 using future context to improve speech recognition accuracy (e.g., word error rate (WER)) in exchange for increased processing times.
- WER word error rate
- the ASR model 200 c first uses the first encoder 210 to generate the first higher order feature representation 212 at each output step for input to the second encoder 220 , but the decoder 204 does not decode any of the first higher order feature representations 212 .
- the ASR model 200 c then performs non-streaming speech recognition on the sequence of acoustic frames 110 (e.g., including the additional right-context) whereby the second encoder 220 uses the first higher order feature representation 212 received from the first encoder 210 to generate the second higher order feature representation 222 .
- the decoder 204 then generates the second probability distribution 234 over possible speech recognition hypotheses and produces the final speech recognition result 120 b .
- the ASR model 200 c may simply operate in only the non-streaming mode to generate the final speech recognition result 120 b .
- FIG. 5 is flowchart of an example arrangement of operations for a method 500 of optimizing inference performance for conformers.
- the method 500 may execute on data processing hardware 610 ( FIG. 6 ) using instructions stored on memory hardware 620 ( FIG. 6 ).
- the data processing hardware 610 and the memory hardware 620 may reside on the user device 10 and/or the remote computing device 60 of FIG. 1 corresponding to a computing device 600 ( FIG. 6 ).
- the method 500 includes receiving a sequence of acoustic frames 110 as input to an ASR model 200 .
- the ASR model 200 includes a causal encoder (i.e., first encoder) 210 and a decoder 204 .
- the method 500 includes generating, by the causal encoder 210 , at each of the plurality of output steps, a first higher order feature representation 212 for a corresponding acoustic frame 110 in the sequence of acoustic frames 110 .
- the method 500 includes generating, by the decoder 204 , at each of the plurality of output steps, a first probability distribution 232 over possible speech recognition hypotheses.
- the causal encoder 210 includes a stack of causal encoder layers 300 each including a RNN Attention-Performer module 360 that applies linear attention.
- FIG. 6 is schematic view of an example computing device 600 that may be used to implement the systems and methods described in this document.
- the computing device 600 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.
- the components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.
- the computing device 600 includes a processor 610 , memory 620 , a storage device 630 , a high-speed interface/controller 640 connecting to the memory 620 and high-speed expansion ports 650 , and a low speed interface/controller 660 connecting to a low speed bus 670 and a storage device 630 .
- Each of the components 610 , 620 , 630 , 640 , 650 , and 660 are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate.
- the processor 610 can process instructions for execution within the computing device 600 , including instructions stored in the memory 620 or on the storage device 630 to display graphical information for a graphical user interface (GUI) on an external input/output device, such as display 680 coupled to high speed interface 640 .
- GUI graphical user interface
- multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory.
- multiple computing devices 600 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).
- the memory 620 stores information non-transitorily within the computing device 600 .
- the memory 620 may be a computer-readable medium, a volatile memory unit(s), or non-volatile memory unit(s).
- the non-transitory memory 620 may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by the computing device 600 .
- non-volatile memory examples include, but are not limited to, flash memory and read-only memory (ROM) / programmable read-only memory (PROM) / erasable programmable read-only memory (EPROM) / electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs).
- volatile memory examples include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes.
- the storage device 630 is capable of providing mass storage for the computing device 600 .
- the storage device 630 is a computer-readable medium.
- the storage device 630 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.
- a computer program product is tangibly embodied in an information carrier.
- the computer program product contains instructions that, when executed, perform one or more methods, such as those described above.
- the information carrier is a computer- or machine-readable medium, such as the memory 620 , the storage device 630 , or memory on processor 610 .
- the high speed controller 640 manages bandwidth-intensive operations for the computing device 600 , while the low speed controller 660 manages lower bandwidth-intensive operations. Such allocation of duties is exemplary only.
- the high-speed controller 640 is coupled to the memory 620 , the display 680 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 650 , which may accept various expansion cards (not shown).
- the low-speed controller 660 is coupled to the storage device 630 and a low-speed expansion port 690 .
- the low-speed expansion port 690 which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- input/output devices such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- the computing device 600 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 600 a or multiple times in a group of such servers 600 a , as a laptop computer 600 b , or as part of a rack server system 600 c .
- implementations of the systems and techniques described herein can be realized in digital electronic and/or optical circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof.
- ASICs application specific integrated circuits
- These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- the processes and logic flows described in this specification can be performed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data
- a computer need not have such devices.
- Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- one or more aspects of the disclosure can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- Other kinds of devices can be used to provide interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input
Abstract
A computer-implemented method includes receiving a sequence of acoustic frames as input to an automatic speech recognition (ASR) model. Here, the ASR model includes a causal encoder and a decoder. The method also includes generating, by the causal encoder, a first higher order feature representation for a corresponding acoustic frame in the sequence of acoustic frames. The method also includes generating, by the decoder, a first probability distribution over possible speech recognition hypotheses. Here, the causal encoder includes a stack of causal encoder layers each including a Recurrent Neural Network (RNN) Attention-Performer module that applies linear attention.
Description
- This U.S. Pat. Application claims priority under 35 U.S.C. §119(e) to U.S. Provisional Application 63/262,140, filed on Oct. 5, 2021. The disclosure of this prior application is considered part of the disclosure of this application and is hereby incorporated by reference in its entirety.
- This disclosure relates to optimizing inference performance for conformers.
- Automated speech recognition (ASR) systems have evolved from multiple models where each model had a dedicated purposed to integrated models were a single neural network is used to directly map an audio waveform (i.e., input sequence) to an output sentence (i.e., output sequence). This integration has resulted in a sequence-to-sequence approach, which generates a sequence of words (or graphemes) when given a sequence of audio features. Oftentimes, these integrated models include multiple self-attention layers that maintain a large number of internal states. However, devices that implement these integrated models have limited memory bandwidth such that reading from each of these internals states results in increased latency of the ASR systems.
- One aspect of the disclosure provides an automated speech recognition (ASR) model including a causal encoder that includes a stack of causal encoder layers. The causal encoder is configured to receive a sequence of acoustic frames as input and generate, at each of a plurality of output steps, a first higher order feature representation for a corresponding acoustic frame in the sequence of acoustic frames. The ASR model also includes a decoder configured to receive, as input, the first higher order feature representation generated by the causal encoder at each of the plurality of output steps and generate, at each of the plurality of output steps, a first probability distribution over possible speech recognition hypotheses. Here, each causal encoder layer in the stack of causal encoder layers includes a Recurrent Neural Network (RNN) Attention-Performer module that applies linear attention.
- Implementations of the disclosure may include one or more of the following optional features. In some implementations, during pre-training, each causal encoder layer includes a first feedforward module, a convolution module, a multi-head attention module, a second feedforward module, and a layernorm module. In these implementations, during fine-tune training, each causal encoder layer includes the first feedforward module, the convolution module, the RNN Attention-Performer module, the second feedforward module, and the layernorm module. Each causal encoder layer may be pre-trained using regular conformer training for the multi-head attention module and is fine-tuned by replacing the multi-head attention module with the RNN Attention-Performer module. Here, replacing the multi-head attention module with the RNN Attention-Performer module may include applying a trainable affine transformation to convert queries/keys from the pre-trained multi-head attention module into an RNN model of linear time and constant memory complexity in sequence length.
- In some examples, the causal encoder further includes an initial stack of convolution blocks without self-attention. In some implementations, the ASR model further includes a non-causal encoder configured to receive, as input, the first higher order feature representation generated by the causal encoder at each of the plurality of output steps and generate, at each of the plurality of output steps, a second higher order feature representation for a corresponding higher order feature representation. Here, the decoder is further configured to receive, as input, the second higher order feature representation generated by the non-causal encoder at each of the plurality of output steps and generate, at each of the plurality of output steps, a second probability distribution over possible speech recognition hypotheses.
- In some examples, the decoder includes a prediction network configured to receive, as input, a sequence of non-blank symbols output by a final softmax layer and generate, at each of the plurality of output steps, a dense representation. In these examples, the decoder also includes a joint network configured to: receive, as input, the dense representation generated by the prediction network at each of the plurality of output steps and one of the first higher order feature representation generated by the causal encoder at each of the plurality of output steps when the ASR model is operating in a streaming mode or the second higher order feature representation generated by the non-causal encoder at each of the plurality of output steps when the ASR model is operating in a non-streaming mode; and generate, at each of the plurality of output steps, one of the first probability distribution over possible speech recognition hypotheses when the ASR model is operating in the streaming mode or the second probability distribution over possible speech recognition hypotheses when the ASR model is operating in the non-streaming mode. The prediction network may include a long short-term memory (LSTM)-based prediction network. In some examples, the prediction network includes a V2 embedding look-up table.
- Another aspect of the disclosure provides a computer-implemented method that when executed on data processing hardware causes the data processing hardware to perform operations for optimizing inference performance for conformers. The operations include receiving a sequence of acoustic frames as input to an automatic speech recognition (ASR) model. The ASR model includes a causal encoder and a decoder. The operations also include generating, by the causal encoder at each of a plurality of output steps, a first higher order feature representation for a corresponding acoustic frame in the sequence of acoustic frames. The operations also include generating, by the decoder at each of the plurality of output steps, a first probability distribution over possible speech recognition hypotheses. Here, the causal encoder includes a stack of causal encoder layers where each causal encoder layer in the stack of causal encoder layers includes a Recurrent Neural Network (RNN) Attention-Performer module that applies linear attention.
- Implementations of the disclosure may include one or more of the following optional features. In some implementations, during pre-training of the ASR model, each causal encoder layer includes a first feedforward module, a convolution module, a multi-head attention module, a second feedforward module, and a layernorm module. In these implementations, during fine-tune training, each causal encoder layer includes the first feedforward module, the convolution module, the RNN Attention-Performer module, the second feedforward module, and the layernorm module. Here, the operations may further include pre-training each causal encoder layer using regular conformer training for the multi-head attention module and fine-tuning each causal encoder layer by replacing the multi-head attention module with the RNN Attention-Performer module. In some examples, replacing the multi-head attention module with the RNN Attention-Performer module includes applying a trainable affine transformation to convert queries/keys from the pre-trained multi-head attention module into an RNN model of linear time and constant memory complexity in sequence length.
- The causal encoder may further include an initial stack of convolution blocks without self-attention. In some implementations, the operations further include generating, by a non-causal encoder of the ASR model at each of the plurality of output steps, a second higher order feature representation for a corresponding first higher order feature representation generated by the causal encoder and generating, by the decoder at each of the plurality of output steps, a second probability distribution over possible speech recognition hypotheses for a corresponding second higher order feature representation. In these implementations, the operations may further include: receiving, as input at a prediction network of the decoder, a sequence of non-blank symbols output by a final softmax layer; generating a dense representation by the prediction network; and generating, by a joint network of the decoder, one of the first probability distribution over possible speech recognition hypotheses when the ASR model is operating in a streaming mode or the second probability distribution over possible speech recognition hypotheses when the ASR model is operating in a non-streaming mode. In some examples, the prediction network includes a long short-term memory (LSTM)-based prediction network. The prediction network may include a V2 embedding look-up table.
- The details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will be apparent from the description and drawings, and from the claims.
-
FIG. 1 is a schematic view of an example speech recognition system. -
FIGS. 2A-2C are schematic views of an ASR model operating in various combinations of streaming and non-streaming modes. -
FIGS. 3A-3C are schematic views of various example causal encoder layers of the ASR model. -
FIGS. 4A-4C are schematic views of various example stacks of causal encoder layers of the ASR model. -
FIG. 5 is a flowchart of an example arrangement of operations for a computer-implemented method of optimizing inference performance of conformers. -
FIG. 6 is a schematic view of an example computing device that may be used to implement the systems and methods described herein. - Like reference symbols in the various drawings indicate like elements.
- End-to-end (E2E) automated speech recognition (ASR) models are traditionally structured to operate in a streaming mode or a non-streaming mode. Conventionally, an E2E ASR model includes an encoder and a decoder as the main components. Applications that involve end-user interaction, like voice-search or on-device dictation, may require the model to perform recognition in a streaming fashion, where the words are expected to be output as they are spoken with as little latency as possible. This prevents the use of models that use future context to improve accuracy, such as bi-directional long short-term memory (LSTM). By contrast, applications such as offline video capturing do not require streaming recognition and may make full use of any available future context to improve performance.
- The encoder of the E2E ASR models may include self-attention layers such as conformer or transformer layers. A drawback of these self-attention layers is that the number of internal states to maintain is much larger than for LSTM layers. Specifically, most of these internals states correspond to key and value tensors used for self-attention. As a result, latency of these E2E ASR models during inference increases due to the computational cost of repeatedly loading the large number of internal states.
- Accordingly, implementations herein are directed towards an ASR model that includes a causal encoder (e.g., first encoder) having a stack of casual encoder layers. The causal encoder is configured to receive a sequence of acoustic frames corresponding to an utterance and generate a first higher order feature representation for a corresponding acoustic frame. The ASR model also includes a decoder configured to generate a first probability distribution over possible speech recognition hypotheses for a corresponding first higher order feature representation. Here, each casual encoder layer in the stack of encoder layers includes a Recurrent Neural Network (RNN) Attention-Performer module that applies linear attention during fine-tune training and inference of the ASR model. Advantageously, the causal encoder may retain the benefit derived from self-attention layers (e.g., conformer or transformer layers) during pre-training while using RNN Attention-Performer module layers during fine-tune training and inference thereby reducing latency and model size of the ASR model making the ASR model suitable for on-device applications. As will become apparent, the ASR model may also include a non-causal encoder (i.e., second encoder) connected in cascade to the causal encoder to further improve accuracy of the ASR model for applications where latency is not a limiting constraint.
-
FIG. 1 is an example system 100 within a speech environment 101. In the speech environment 101, a user’s 104 manner of interacting with a computing device, such as auser device 10, may be through voice input. The user device 10 (also referred to generally as a device 10) is configured to capture sounds (e.g., streaming audio data) from one ormore users 104 within the speech environment 101. Here, the streaming audio data may refer to a spokenutterance 106 by theuser 104 that functions as an audible query, a command for theuser device 10, or an audible communication captured by thedevice 10. Speech-enabled systems of theuser device 10 may field the query or the command by answering the query and/or causing the command to be performed/fulfilled by one or more downstream applications. - The
user device 10 may correspond to any computing device associated with auser 104 and capable of receiving audio data. Some examples ofuser devices 10 include, but are not limited to, mobile devices (e.g., mobile phones, tablets, laptops, etc.), computers, wearable devices (e.g., smart watches), smart appliances, internet of things (IoT) devices, vehicle infotainment systems, smart displays, smart speakers, etc. Theuser device 10 includesdata processing hardware 12 andmemory hardware 14 in communication with thedata processing hardware 12 and stores instructions, that when executed by thedata processing hardware 12, cause thedata processing hardware 12 to perform one or more operations. Theuser device 10 further includes an audio system 16 with an audio capture device (e.g., microphone) 16, 16 a for capturing and converting spokenutterances 106 within the speech environment 101 into electrical signals and a speech output device (e.g., speaker) 16, 16 b for communicating an audible audio signal (e.g., as output audio data from the user device 10). While theuser device 10 implements a single audio capture device 16 a in the example shown, theuser device 10 may implement an array of audio capture devices 16 a without departing from the scope of the present disclosure, whereby one or more capture devices 16 a in the array may not physically reside on theuser device 10, but be in communication with the audio system 16. - The system 100 includes an automated speech recognition (ASR) system 118 implementing an ASR model 200 that resides on the
user device 10 of theuser 104 and/or on a remote computing device 60 (e.g., one or more remote servers of a distributed system executing in a cloud-computing environment) in communication with theuser device 10 via anetwork 40. In some examples, the ASR model 200 includes a recurrent neural network-transducer (RNN-T) model architecture. Theuser device 10 and/or theremote computing device 60 also includes anaudio subsystem 108 configured to receive theutterance 106 spoken by theuser 104 and captured by the audio capture device 16 a, and convert theutterance 106 into a corresponding digital format associated with inputacoustic frames 110 capable of being processed by the ASR system 118. In the example shown, the user speaks arespective utterance 106 and theaudio subsystem 108 converts theutterance 106 into corresponding audio data (e.g., sequence of acoustic frames) 110 for input to the ASR system 118. Thereafter, the ASR model 200 receives, as input, the sequence ofacoustic frames 110 corresponding to theutterance 106, and generates/predicts, at each output step, a corresponding transcription 120 (e.g., speech recognition result/hypothesis) of theutterance 106 as the ASR model 200 receives (e.g., processes) eachacoustic frame 110 in the sequence ofacoustic frames 110. - In the example shown, the ASR model 200 may perform streaming speech recognition to produce an initial speech recognition result (e.g., candidate hypothesis) 120, 120 a and generate a final speech recognition result (e.g., final hypothesis) 120, 120 b by improving the initial speech recognition result 120 a. The initial and final speech recognition result 120 a, 120 b may either correspond to a partial speech recognition result or an entire speech recognition result. Stated differently, the initial and final speech recognition result 120 a, 120 b may either correspond to a portion of an
utterance 106 or an entire portion of anutterance 106. For example, the partial speech recognition result may correspond to a portion of a spoken utterance or even a portion of a spoken term. However, as will become apparent, the ASR model 200 performs additional processing on the finalspeech recognition result 120 b whereby the finalspeech recognition result 120 b may be delayed from the initial speech recognition result 120 a. - The
user device 10 and/or theremote computing device 60 also executes a user interface generator 107 configured to present a representation of the transcription 120 of theutterance 106 to theuser 104 of theuser device 10. As described in greater detail below, the user interface generator 107 may display the initial speech recognition result 120 a in a streaming fashion duringtime 1 and subsequently display the finalspeech recognition result 120 b in a streaming fashion during time 2. Notably, the ASR model 200 outputs the finalspeech recognition result 120 b in a streaming fashion even though the finalspeech recognition result 120 b improves upon the initial speech recognition result 120 a. In some configurations, the transcription 120 output from the ASR system 118 is processed (e.g., by a natural language understanding (NLU) module executing on theuser device 10 or the remote computing device 60) to execute a user command/query specified by theutterance 106. Additionally or alternatively, a text-to-speech system (not shown) (e.g., executing on any combination of theuser device 10 or the remote computing device 60) may convert the transcription 120 into synthesized speech for audible output by theuser device 10 and/or another device. - In the example shown, the
user 104 interacts with a program or application 50 (e.g., the digital assistant application 50) of theuser device 10 that uses the ASR system 118. For instance,FIG. 1 depicts theuser 104 communicating with thedigital assistant application 50 and thedigital assistant application 50 displaying adigital assistant interface 18 on a screen of theuser device 10 to depict a conversation between theuser 104 and thedigital assistant application 50. In this example, theuser 104 asks thedigital assistant application 50, “What time is the concert tonight?” This question from theuser 104 is a spokenutterance 106 captured by the audio capture device 16 a and processed by audio systems 16 of theuser device 10. In this example, the audio system 16 receives the spokenutterance 106 and converts it into a sequence ofacoustic frames 110 for input to the ASR system 118. - Continuing with the example, the ASR model 200, while receiving the sequence of
acoustic frames 110 corresponding to theutterance 106 as theuser 104 speaks, encodes the sequence ofacoustic frames 110 and then decodes the encoded sequence ofacoustic frames 110 into the initial speech recognition result 120 a. Duringtime 1, the user interface generator 107 presents, via thedigital assistant interface 18, a representation of the initial speech recognition result 120 a of theutterance 106 to theuser 104 of theuser device 10 in a streaming fashion such that words, word pieces, and/or individual characters appear on the screen as soon as they are spoken. In some examples, the first look ahead audio context is equal to zero. - During time 2, the user interface generator 107 presents, via the
digital assistant interface 18, a representation of the finalspeech recognition result 120 b of theutterance 106 to theuser 104 of the user device 10 a streaming fashion such that words, word pieces, and/or individual characters appear on the screen as soon as they are generated by the ASR model 200. In some implementations, the user interface generator 107 replaces the representation of the initial speech recognition result 120 a presented attime 1 with the representation of the finalspeech recognition result 120 b presented at time 2. Here,time 1 and time 2 may include timestamps corresponding to when the user interface generator 107 presents the respective speech recognition result 120. In this example, the timestamp oftime 1 indicates that the user interface generator 107 presents the initial speech recognition result 120 a at an earlier time than the finalspeech recognition result 120 b. For instance, as the finalspeech recognition result 120 b is presumed to be more accurate than the initial speech recognition result 120 a, the finalspeech recognition result 120 b ultimately displayed as the transcription 120 may fix any terms that may have been misrecognized in the initial speech recognition result 120 a. In this example, the streaming initial speech recognition result 120 a output by the ASR model 200 is displayed on the screen of theuser device 10 attime 1 are associated with low latency and provide responsiveness to theuser 104 that his/her query is being processed, while the finalspeech recognition result 120 b output by the ASR model 200 and displayed on the screen at time 2 leverages an additional speech recognition model and/or a language model to improve the speech recognition quality in terms of accuracy, but at increased latency. However, since the initial speech recognition result 120 a are displayed as the user speaks theutterance 106, the higher latency associated with producing, and ultimately displaying the finalspeech recognition result 120 b is not noticeable to theuser 104. - In the example shown in
FIG. 1 , thedigital assistant application 50 may respond to the question posed by theuser 104 using natural language processing. Natural language processing generally refers to a process of interpreting written language (e.g., the initial speech recognition result 120 a and/or the finalspeech recognition result 120 b) and determining whether the written language prompts any action. In this example, thedigital assistant application 50 uses natural language processing to recognize that the question from theuser 104 regards the user’s schedule and more particularly a concert on the user’s schedule. By recognizing these details with natural language processing, the automated assistant returns aresponse 19 to the user’s query where theresponse 19 states, “Venue doors open at 6:30 PM and concert starts at 8 pm.” In some configurations, natural language processing occurs on aremote server 60 in communication with thedata processing hardware 12 of theuser device 10. -
FIGS. 2A-2C include example ASR models 200 a-c operating in various combinations of streaming and non-streaming modes. Specifically, each of the ASR models 200 a-c includes anencoder 202 and adecoder 204. Theencoder 202 may include a causal encoder (i.e., first encoder) 210 that includes a stack of causal encoder layers 300 and a non-causal encoder (i.e., second encoder) 220. -
FIGS. 3A-3C illustrate various example causal encoder layers 300 of thefirst encoder 210. More specifically,FIG. 3A is a schematic view of a first example causal encoder layer (e.g., conformer or transformer layer) 300, 300 a that may be used to implement one or more of the causal encoder layers 300 from the stack of causal encoder layers 300 of thefirst encoder 210. The first example causal encoder layer 300 a includes a first half feed-forward layer 310, a second half feed-forward layer 340, with aconvolution module 320 and a multi-head self-attention module 330 disposed between the first and second half feed-forward layers concatenation operators 305, and alayernorm module 350. The first half feed-forward layer 310 processes the sequence of acoustic frames 110 (e.g., Mel-spectrogram input sequence) by projecting theacoustic frames 110 into a larger dimension, followed by a non-linear activation, and then another linear layer to project the features back to the original dimensions. Subsequently, theconvolution module 320 subsamples the sequence ofacoustic frames 110 concatenated with the output of the first half feed-forward layer 310. That is, theconvolution module 320 aggregates information from neighboring context to capture relative offset-based local interactions. The multi-head self-attention module 330 may include self-attention layers such as conformer or transformer layers. The multi-head self-attention module 330 receives the output of theconvolution module 320 concatenated with the output of the first half feed-forward layer 310. Intuitively, the role of the multi-head self-attention module 330 is to summarize noise context separately for each input frame that is to be enhanced. The multi-head self-attention module 330 looks back L previous frames and converts an output into a fixed-length vector thereby capturing more global patterns. The multi-head self-attention module 330 maintains a large number of internal states. A significant portion of these internal states correspond to the key and value tensors of self-attention causing an increase in latency due to repeatedly loading each of these internal states (e.g., quadratic computational cost). - Thereafter, the second half feed-
forward layer 340 receives a concatenation of the output of the multi-head self-attention module 330 and the output of theconvolution module 320. Thelayernorm module 350 processes a concatenation of the output from the second half feed-forward layer 340 and the output of the multi-head self-attention module 330. That is, the first example causal encoder layer 300 a transforms eachacoustic frame 110 in the sequence of acoustic frames 110 (e.g., input features x), using modulation features m, to generate, at each output step, anoutput 355 for a correspondingacoustic frame 110 in the sequence ofacoustic frames 110. More specifically, the first example causal encoder layer 300 a may generate theoutput 355 by: -
- The
output 355 of the first example causal encoder layer 300 a is passed on to the nextcausal encoder layer 300 in the stack of causal encoder layers 300 of thefirst encoder 210. A lastcausal encoder layer 300 in the stack of causal encoder layers 300 generates a first higherorder feature representation 212 for a correspondingacoustic frame 110 in the sequence ofacoustic frames 110. -
FIG. 3B is a schematic view of a second examplecausal encoder layer 300, 300 b that may be used to implement one or more of the causal encoder layers 300 from the stack of causal encoder layers 300. The second example causal encoder layer 300 b includes the first half feed-forward layer 310, the second half feed-forward layer 340, with theconvolution module 320 and a Recurrent Neural Network (RNN) Attention-Performer module 360 disposed between the first and second half feed-forward layers concatenation operators 305, and thelayernorm module 350. That is, the second example causal encoder layer 300 b replaces the multi-head self-attention module 330 from the first example causal encoder layer 300 a (FIG. 3A ) with the RNN Attention-Performer module 360. Advantageously, the RNN Attention-Performer module 360 replaces the quadratic attention of the multi-head self-attention module 330 with a more manageable linear attention and leverages low-rank decomposition of the attention tensor. - The RNN Attention-Performer 366 includes a unidirectional performer that performs prefix-sum determinations that emulate causal attention of the multi-head self-attention module. More specifically, the RNN Attention-
Performer module 360 determines a matrix by summing outer products of kernel features corresponding to keys with value-vectors. At each iteration of the prefix-sum determination, a kernel feature vector corresponding to a query is multiplied by the prior prefix-sum to generate a new embedding. Here, the RNN Attention-Performer module 360 obtains the prior prefix-sum by summing all outer-products corresponding to preceding tokens. Moreover, RNN Attention-Performer module 360 obtains features by applying rectified linear unit (ReLU) elementwise activations to affinely-transformed queries/keys. - The first half feed-
forward layer 310 processes the sequence of acoustic frames 110 (e.g., Mel-spectrogram input sequence). Subsequently, theconvolution module 320 subsamples the sequence ofacoustic frames 110 concatenated with the output of the first feed-forward layer 310. The RNN Attention-Performer module 360 receives the output of theconvolution module 320 concatenated with the output of the first half feed-forward layer 310 and applies linear attention to generate an output. Thereafter, the second half feed-forward layer 340 receives a concatenation of the output of the RNN Attention-Performer module 360 and the output of theconvolution module 320. Thelayernorm module 350 processes the output from the second half feed-forward layer 340 and the multi-head self-attention module 330 output to generate theoutput 355. Theoutput 355 of the second example causal encoder layer (i.e., RNN Performer layer) 300 b is passed on to the nextcausal encoder layer 300 in the stack of causal encoder layers 300 of thefirst encoder 210. A lastcausal encoder layer 300 in the stack of causal encoder layers 300 generates the first higherorder feature representation 212 for a correspondingacoustic frame 110 in the sequence ofacoustic frames 110. -
FIG. 3C is a schematic view of a third examplecausal encoder layer 300, 300 c that may be used to implement one or more of the causal encoder layers 300 from the stack of causal encoder layers 300. The third example causal encoder layer (i.e., convolution blocks) 300 c includes the first half feed-forward layer 310, theconvolution module 320, andconcatenation operators 305. Notably, the third example causal encoder layer 300 c does not include any self-attention modules (e.g., multi-head self-attention module 330 and RNN Attention-Performer module 360) thereby reducing the model size and compute resources of the ASR model 200. As will become apparent, the initial encoder layers 300 of the stack of causal encoder layers 300 may include the third example causal encoder layer 300 c. The first half feed-forward layer 310 processes the sequence of acoustic frames 110 (e.g., Mel-spectrogram input sequence). Subsequently, theconvolution module 320 subsamples the sequence ofacoustic frames 110 concatenated with the output of the first half feed-forward layer 310 to generate anoutput 325. Theoutput 325 of the third example causal encoder layer 300 c is passed on to the nextcausal encoder layer 300 in the stack of causal encoder layers 300 of thefirst encoder 210. A lastcausal encoder layer 300 in the stack of causal encoder layers 300 (e.g., first or second example causal encoder layer 300 a, 300 b) generates the first higherorder feature representation 212 for a correspondingacoustic frame 110 in the sequence ofacoustic frames 110. -
FIGS. 4A-4C includeexample configurations 400 of thefirst encoder 210 having various stacks of causal encoder layers 300. Theexample configurations 400 shown inFIGS. 4A-4C only illustrate the stack of causal encoder layers 300 having five (5) causal encoder layers 300 for the sake of clarity only, as it is understood that the stack of causal encoder layers 300 may include any number of causal encoder layers 300.FIG. 4A illustrates afirst configuration first encoder 210 during a pre-training stage of the ASR model 200. Here, eachcausal encoder layer 300 in the stack of causal encoder layers 300 of thefirst encoder 210 includes the first example causal encoder layers (e.g., conformer layer) 300 a 1-a 5. Thus, eachcausal encoder layer 300 in the stack of causal encoder layers 300 thefirst configuration 400 a includes the multi-head self-attention module 330 that performs ordinary self-attention. Since eachcausal encoder layer 300 in the stack of causal encoder layers of thefirst configuration 400 a includes a conformer layer, thefirst configuration 400 a represents a conformer or encoder whereby each first example causal encoder layer (e.g., conformer layer) uses regular conformer training during the pre-training stage. -
FIG. 4B illustrates asecond configuration first encoder 210 used during fine-tune training, and subsequently inference, of the ASR model 200. Here, eachcausal encoder layer 300 in the stack of causal encoder layers 300 of thefirst encoder 210 includes the second example causal encoder layers 300 b 1-b 5. Stated differently, during fine-tune training thesecond configuration 400 b replaces each of the first example causal encoder layers 300 a (FIG. 3A ) with the second example causal encoder layers 300 b (FIG. 3B ). Thus, eachcausal encoder layer 300 in the stack of causal encoder layers in thesecond configuration 400 b includes the RNN Attention-Performer module 360 that performs linear attention. As such, during fine-tune training and inference thefirst encoder 210 retains the benefit learnt during pre-training with the multi-head self-attention module 330 while also having the advantage of the smaller latency and smaller model size of the RNN Attention-Performer module 360 of the second example causal encoder layers 300 b (FIG. 3B ). -
FIG. 4C illustrates athird configuration first encoder 210 used during fine-tune training and inference of the ASR model 200. Here, the initial causal encoder layers 300 in the stack of causal encoder layers 300 of thefirst encoder 210 include the third example causal encoder layers 300 c and the subsequent causal encoder layers 300 in the stack of causal encoder layers 300 of thefirst encoder 210 include the second example causal encoder layers 300 b. Stated differently, during fine-tune training thethird configuration 400 c replaces each of the first example causal encoder layers 300 a (FIG. 3A ) in the initial causal encoder layers 300 with third example causal encoder layers 300 c and replaces each of the first example causal encoder layers 300 a (FIG. 3A ) in the subsequent casual encoder layers 300 with the second example causal encoder layers 300 b. Notably, the only difference between the first example causal encoder layer 300 a and the second example causal encoder layer 300 b is that the second example causal encoder layer includes the RNN Attention-Performer module 360 rather than the multi-head self-attention module 330. Replacing the multi-head self-attention module 330 with the RNN Attention-Performer module 360 includes applying a trainable affine transformation to convert queries/keys from the multi-head self-attention module 330 into an RNN model (e.g., RNN Attention-Performer module 360) of linear time and constant memory complexity in sequence length. - As shown in
FIG. 4C , the first and second causal encoder layers (e.g., initial causal encoder layers) 300c 1, 300 c 2 include the third example causal encoder layers 300 c while the third, fourth, and fifth causal encoder layers 300 b 1-b 3 (e.g., subsequent causal encoder layers) include the second example causal encoder layers 300 b. Although, the initial causal encoder layers are not so limited, for example, the initial causal encoder layers may include any number of layers from the stack of causal encoder layers. Thethird configuration 400 c of thefirst encoder 210 uses the simpler initial causal encoder layers to hypothesize low-level features and uses the subsequent causal encoder layers that perform linear attention. - Referring back to
FIGS. 2A-2C , in some implementations, theencoder 202 is a cascading encoder. The cascadingencoder 202 refers to a model structure where the encoding pathway of thefirst encoder 210 and thesecond encoder 220 cascade such that the output of thefirst encoder 210 feeds the input of thesecond encoder 220 prior to decoding. Here, the first andsecond encoders second encoders first encoder 210 to restrict the model from using future inputs. The cascading encoder may include 17 causal encoder layers. Here, thefirst encoder 210 may include 15 causal encoder layers while thesecond encoder 220 may include two causal encoder layers that take in additional right context (e.g., 5.04 seconds). Thefirst encoder 210 may be referred to as a causal encoder and thesecond encoder 220 may be referred to as a non-causal encoder. - Here, the
first encoder 210 is a streaming encoder while thesecond encoder 220 is a non-streaming encoder. In a cascadingencoder 202, thesecond encoder 220 receives the output of thefirst encoder 210 and may take advantage of the causal encoder layers 300 of thefirst encoder 210 such that the second encoder includes fewer multi-head attention layers than thefirst encoder 210. By having fewer layers, the cascading encoder may reduce the number of more computationally expensive layers making the ASR model 200 more streamlined than simply combining a traditional streaming model with a traditional non-streaming model. - Referring now to
FIG. 2A , thefirst encoder 210 reads a sequence of d-dimensional feature vectors (e.g.,acoustic frames 110 shown inFIG. 1 ) x = (x1, x2, ..., xT), where xt ∈ ℝd, and generates, at each output step, the first higherorder feature representation 212 for a correspondingacoustic frame 110 in the sequence ofacoustic frames 110. Similarly, thesecond encoder 220 is connected in cascade to thefirst encoder 210 and is configured to receive the first higherorder feature representation 212 generated by thefirst encoder 210 at each of the plurality of output steps, as input, and generate, at each output step, a second higherorder feature representation 222 for a corresponding first higherorder feature representation 212. Notably, thesecond encoder 220 generates the second higherorder feature representations 222 using additional right-context (e.g., future input frames) while thefirst encoder 210 generates the first higherorder feature representation 212 without using any additional right-context. Moreover, thesecond encoder 220 generates the second higherorder feature representation 222 using the first higherorder feature representation 212 and does not receive any of theacoustic frames 110. - The
decoder 204 may include a recurrent neural network-transducer (RNN-T) architecture having ajoint network 230 and aprediction network 240. Thedecoder 204 uses thejoint network 230 to combine (i.e., when the model operates in a non-streaming mode) the first and second higherorder feature representations encoder 202 at each of the plurality of output steps, as well as ahidden representation 242 output from theprediction network 240 for the previous prediction yr-1, to generate a decoder output. When the ASR model 200 operates in the streaming mode, thejoint network 230 receives the hiddenrepresentation 242 output from theprediction network 240 and only the first higherorder feature representation 212 output from the first encoder 210 (e.g., thejoint network 230 does not receive the second higher order feature representation 222). The decoder output can be a probability distribution, P (yi|yi-1, ..., y0, x), over the current sub-word unit, yi, given a sequence of N previous non-blank symbols (yi-1, ..., yi-N), and input, x. Although not illustrated, the ASR model 200 may include a final softmax layer that receives the output of thedecoder 204 and generates the sequence of non-blank symbols. In some implementations, the softmax layer is separate from thedecoder 204 and processes the output from thedecoder 204. The output of the softmax layer is then used in a beam search process to select orthographic elements. In some implementations, the final softmax layer is integrated with thedecoder 204 such that the output of thedecoder 204 represents the output of the final softmax layer. - The
decoder 204 is configured to generate, at each output step, a probability distribution over possible speech recognition hypotheses. When the ASR model 200 is operating in a streaming mode, thedecoder 204 generates afirst probability distribution 232 over possible speech recognition hypotheses for a corresponding first higherorder feature representation 212. Alternatively, when the ASR model 200 is operating in a non-streaming mode, thedecoder 204 generates asecond probability distribution 234 over possible speech recognition hypotheses for a corresponding second higherorder feature representation 222. Stated differently, thejoint network 230 generates, at each output step (e.g. time step), aprobability distribution joint network 230 can include a posterior probability value for each of the different output labels. Thus, if there are 100 different output labels representing different graphemes or other symbols, the output of thejoint network 230 can include 100 different probability values, one for each output label. The probability distribution can then be used to select and assign scores to candidate orthographic elements (e.g., graphemes, wordpieces, and/or words) in a beam search process (e.g., by the Softmax layer) for determining the transcription 120. - Within the
decoder 204, the prediction network may have two 2,048-dimensional LSTM layers, each of which is also followed by a 640-dimensional projection layer, such that the LSTM-based prediction network may have about 23.4 million parameters. In other configurations, theprediction network 240 may instead include conformer or transformer layers in lieu of LSTM layers. In yet other configurations, theprediction network 240 includes a V2 embedding lookup table that includes an embedding prediction network. At each time step, the V2 embedding lookup table may receive, as input, the previous two predictions (e.g., 1-hot vectors) output by thejoint network 230, determine a respective embedding d1, d2 for each of the previous two predictions, and provide a concatenated output [d1, d2] to thejoint network 230. Comparatively, the V2 embedding lookup table may have only about two (2) million parameters, whereas an LSTM-based prediction network may include about 23.4 million parameters. Finally thejoint network 230 may also be a one-layer neural network with 640 hidden units. - Continuing with the example in
FIG. 2A , in some implementations, the ASR model 200 a operating in both the streaming and non-streaming modes in parallel. When operating in both the streaming and non-streaming mode at the same time, the ASR model 200 a first performs streaming speech recognition on the sequence ofacoustic frames 110 using thefirst encoder 210 to generate the first higherorder feature representation 212 for both thesecond encoder 220 and thedecoder 204. Thereafter, thedecoder 204 generates thefirst probability distribution 232 over possible speech recognition hypotheses to output partial speech recognition results 120, 120 a. The ASR model 200 b also performs non-streaming speech recognition where thesecond encoder 220 uses the first higherorder feature representation 212 received from thefirst encoder 210 to generate the second higherorder feature representation 222. Thedecoder 204 then generates thesecond probability distribution 234 over possible speech recognition hypotheses to output the finalspeech recognition result 120, 120 b. As noted by the time, thedecoder 204 output the partial speech recognition results 120 a in a streaming fashion using the output from thefirst encoder 210, and then operating in the non-streaming fashion uses the first higherorder feature representation 212 to generate the finalspeech recognition result 120 b. Thus, the finalspeech recognition result 120 b for theinput utterance 106 may be delayed from the partial speech recognition results 120 a. - Referring to
FIG. 2B , in some implementations, the ASR model 200 b operating only in the streaming mode. This may occur, for instance, when the user 104 (FIG. 1 ) is using applications such as voice-search or on-device dictation, which requires as little latency as possible. Here, theencoder 202 operates in a non-cascading mode such that only thefirst encoder 210 generates an output. More specifically, the ASR model 200 b performs streaming speech recension on the sequence ofacoustic frames 110 using only thefirst encoder 210 to generate the first higherorder feature representation 212 for thedecoder 204. Thereafter, thedecoder 204 generates thefirst probability distribution 232 over possible speech recognition results. Because the streaming mode of the ASR model 200 b generates the partial speech recognition results 120 a quickly, the inaccuracy of the term “play” is generally acceptable to users. - Referring to
FIG. 2C , in some implementations, the ASR model 200 c operates only in the non-streaming mode. The non-streaming mode may occur, for instance, in non-latency intensive applications such as when the user (FIG. 1 ) is viewing a transcription of a voicemail left on his/her phone. As discussed above, this type of application benefits from thesecond encoder 220 using future context to improve speech recognition accuracy (e.g., word error rate (WER)) in exchange for increased processing times. Here, the ASR model 200 c first uses thefirst encoder 210 to generate the first higherorder feature representation 212 at each output step for input to thesecond encoder 220, but thedecoder 204 does not decode any of the first higherorder feature representations 212. The ASR model 200 c then performs non-streaming speech recognition on the sequence of acoustic frames 110 (e.g., including the additional right-context) whereby thesecond encoder 220 uses the first higherorder feature representation 212 received from thefirst encoder 210 to generate the second higherorder feature representation 222. Thedecoder 204 then generates thesecond probability distribution 234 over possible speech recognition hypotheses and produces the finalspeech recognition result 120 b. In this scenario, because producing streaming speech recognition results in real-time has little value to the user and latency is not a factor, the ASR model 200 c may simply operate in only the non-streaming mode to generate the finalspeech recognition result 120 b. -
FIG. 5 is flowchart of an example arrangement of operations for amethod 500 of optimizing inference performance for conformers. Themethod 500 may execute on data processing hardware 610 (FIG. 6 ) using instructions stored on memory hardware 620 (FIG. 6 ). Thedata processing hardware 610 and thememory hardware 620 may reside on theuser device 10 and/or theremote computing device 60 ofFIG. 1 corresponding to a computing device 600 (FIG. 6 ). - At
operation 502, themethod 500 includes receiving a sequence ofacoustic frames 110 as input to an ASR model 200. Here, the ASR model 200 includes a causal encoder (i.e., first encoder) 210 and adecoder 204. Atoperation 504, themethod 500 includes generating, by thecausal encoder 210, at each of the plurality of output steps, a first higherorder feature representation 212 for a correspondingacoustic frame 110 in the sequence ofacoustic frames 110. Atoperation 506, themethod 500 includes generating, by thedecoder 204, at each of the plurality of output steps, afirst probability distribution 232 over possible speech recognition hypotheses. Here, thecausal encoder 210 includes a stack of causal encoder layers 300 each including a RNN Attention-Performer module 360 that applies linear attention. -
FIG. 6 is schematic view of anexample computing device 600 that may be used to implement the systems and methods described in this document. Thecomputing device 600 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document. - The
computing device 600 includes aprocessor 610,memory 620, astorage device 630, a high-speed interface/controller 640 connecting to thememory 620 and high-speed expansion ports 650, and a low speed interface/controller 660 connecting to alow speed bus 670 and astorage device 630. Each of thecomponents processor 610 can process instructions for execution within thecomputing device 600, including instructions stored in thememory 620 or on thestorage device 630 to display graphical information for a graphical user interface (GUI) on an external input/output device, such asdisplay 680 coupled tohigh speed interface 640. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Also,multiple computing devices 600 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system). - The
memory 620 stores information non-transitorily within thecomputing device 600. Thememory 620 may be a computer-readable medium, a volatile memory unit(s), or non-volatile memory unit(s). Thenon-transitory memory 620 may be physical devices used to store programs (e.g., sequences of instructions) or data (e.g., program state information) on a temporary or permanent basis for use by thecomputing device 600. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM) / programmable read-only memory (PROM) / erasable programmable read-only memory (EPROM) / electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware, such as boot programs). Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM) as well as disks or tapes. - The
storage device 630 is capable of providing mass storage for thecomputing device 600. In some implementations, thestorage device 630 is a computer-readable medium. In various different implementations, thestorage device 630 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. In additional implementations, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as thememory 620, thestorage device 630, or memory onprocessor 610. - The
high speed controller 640 manages bandwidth-intensive operations for thecomputing device 600, while thelow speed controller 660 manages lower bandwidth-intensive operations. Such allocation of duties is exemplary only. In some implementations, the high-speed controller 640 is coupled to thememory 620, the display 680 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 650, which may accept various expansion cards (not shown). In some implementations, the low-speed controller 660 is coupled to thestorage device 630 and a low-speed expansion port 690. The low-speed expansion port 690, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter. - The
computing device 600 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as astandard server 600 a or multiple times in a group ofsuch servers 600 a, as alaptop computer 600 b, or as part of arack server system 600 c. - Various implementations of the systems and techniques described herein can be realized in digital electronic and/or optical circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
- These computer programs (also known as programs, software, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms “machine-readable medium” and “computer-readable medium” refer to any computer program product, non-transitory computer readable medium, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term “machine-readable signal” refers to any signal used to provide machine instructions and/or data to a programmable processor.
- The processes and logic flows described in this specification can be performed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- To provide for interaction with a user, one or more aspects of the disclosure can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen for displaying information to the user and optionally a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user’s client device in response to requests received from the web browser.
- A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly, other implementations are within the scope of the following claims.
Claims (20)
1. An automated speech recognition (ASR) model comprising:
a causal encoder comprising a stack of causal encoder layers, the causal encoder configured to:
receive, as input, a sequence of acoustic frames; and
generate, at each of a plurality of output steps, a first higher order feature representation for a corresponding acoustic frame in the sequence of acoustic frames; and
a decoder configured to:
receive, as input, the first higher order feature representation generated by the causal encoder at each of the plurality of output steps; and
generate, at each of the plurality of output steps, a first probability distribution over possible speech recognition hypotheses,
wherein each causal encoder layer in the stack of causal encoder layers includes a Recurrent Neural Network (RNN) Attention-Performer module that applies linear attention.
2. The ASR model of claim 1 , wherein, during pre-training of the ASR model, each causal encoder layer comprises:
a first feedforward module;
a convolution module;
a multi-head attention module;
a second feedforward module; and
a layernorm module.
3. The ASR model of claim 2 , wherein, during fine-tune training of the ASR model, each causal encoder layer comprises:
the first feedforward module;
the convolution module;
the RNN Attention-Performer module;
the second feedforward module; and
the layernorm module.
4. The ASR model of claim 3 , wherein each causal encoder layer is pre-trained using regular conformer training for the multi-head attention module and is fine-tuned by replacing the multi-head attention module with the RNN Attention-Performer module.
5. The ASR model of claim 4 , wherein replacing the multi-head attention module with the RNN Attention-Performer module comprises applying a trainable affine transformation to convert queries/keys from the pre-trained multi-head attention module into an RNN model of linear time and constant memory complexity in sequence length.
6. The ASR model of claim 1 , wherein the causal encoder further comprises an initial stack of convolution blocks without self-attention.
7. The ASR model of claim 1 , further comprising:
a non-causal encoder configured to:
receive, as input, the first higher order feature representation generated by the causal encoder at each of the plurality of output steps; and
generate, at each of the plurality of output steps, a second higher order feature representation for a corresponding first higher order feature representation,
wherein the decoder is further configured to:
receive, as input, the second higher order feature representation generated by the non-causal encoder at each of the plurality of output steps; and
generate, at each of the plurality of output steps, a second probability distribution over possible speech recognition hypotheses.
8. The ASR model of claim 7 , wherein the decoder comprises:
a prediction network configured to:
receive, as input, a sequence of non-blank symbols output by a final softmax layer; and
generate, at each of the plurality of output steps, a dense representation; and
a joint network configured to:
receive, as input, the dense representation generated by the prediction network at each of the plurality of output steps and one of:
when the ASR model is operating in a streaming mode, the first higher order feature representation generated by the causal encoder at each of the plurality of output steps; or
when the ASR model is operating in a non-streaming mode, the second higher order feature representation generated by the non-causal encoder at each of the plurality of output steps; and
generate, at each of the plurality of output steps, one of:
when the ASR model is operating in the streaming mode, the first probability distribution over possible speech recognition hypotheses; or
when the ASR model is operating in the non-streaming mode, the second probability distribution over possible speech recognition hypotheses.
9. The ASR model of claim 8 , wherein the prediction network comprises a long short-term memory (LSTM)-based prediction network.
10. The ASR model of claim 8 , wherein the prediction network comprises a V2 embedding look-up table.
11. A computer-implemented method when executed by data processing hardware causes the data processing hardware to perform operations comprising:
receiving, as input to an automatic speech recognition (ASR) model, a sequence of acoustic frames, wherein the ASR model comprises a causal encoder and a decoder;
generating, by the causal encoder, at each of a plurality of output steps, a first higher order feature representation for a corresponding acoustic frame in the sequence of acoustic frames; and
generating, by the decoder, at each of the plurality of output steps, a first probability distribution over possible speech recognition hypotheses,
wherein the causal encoder comprises a stack of causal encoder layers, each causal encoder layer in the stack of causal encoder layers comprising a Recurrent Neural Network (RNN) Attention-Performer module that applies linear attention.
12. The computer-implemented method of claim 11 , wherein, during pre-training of the ASR model, each causal encoder layer comprises:
a first feedforward module;
a convolution module;
a multi-head attention module;
a second feedforward module; and
a layernorm module.
13. The computer-implemented method of claim 12 , wherein, during fine-tune training of the ASR model, each causal encoder layer comprises:
the first feedforward module;
the convolution module;
the RNN Attention-Performer module;
the second feedforward module; and
the layernorm module.
14. The computer-implemented method of claim 13 , wherein the operations further comprise:
pre-training each causal encoder layer using regular conformer training for the multi-head attention module; and
fine-tuning each causal encoder layer by replacing the multi-head attention module with the RNN Attention-Performer module.
15. The computer-implemented method of claim 14 , wherein replacing the multi-head attention module with the RNN Attention-Performer module comprises applying a trainable affine transformation to convert queries/keys from the pre-trained multi-head attention module into an RNN model of linear time and constant memory complexity in sequence length.
16. The computer-implemented method of claim 11 , wherein the causal encoder further comprises an initial stack of convolution blocks without self-attention.
17. The computer-implemented method of claim 11 , wherein the operations further comprise:
generating, by a non-causal encoder of the ASR model, at each of the plurality of output steps, a second higher order feature representation for a corresponding first higher order feature representation generated by the causal encoder; and
generating, by the decoder, at each of the plurality of output steps, a second probability distribution over possible speech recognition hypotheses for a corresponding second higher order feature representation.
18. The computer-implemented method of claim 17 , wherein the operations further comprise:
receiving, as input at a prediction network of the decoder, a sequence of non-blank symbols output by a final softmax layer;
generating, by the prediction network, a dense representation; and
generating, by a joint network of the decoder, one of:
when the ASR model is operating in a streaming mode, the first probability distribution over possible speech recognition hypotheses; or
when the ASR model is operating in a non-streaming mode, the second probability distribution over possible speech recognition hypotheses.
19. The computer-implemented method of claim 18 , wherein the prediction network comprises a long short-term memory (LSTM)-based prediction network.
20. The computer-implemented method of claim 18 , wherein the prediction network comprises a V2 embedding look-up table.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/936,547 US20230130634A1 (en) | 2021-10-05 | 2022-09-29 | Optimizing Inference Performance for Conformer |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163262140P | 2021-10-05 | 2021-10-05 | |
US17/936,547 US20230130634A1 (en) | 2021-10-05 | 2022-09-29 | Optimizing Inference Performance for Conformer |
Publications (1)
Publication Number | Publication Date |
---|---|
US20230130634A1 true US20230130634A1 (en) | 2023-04-27 |
Family
ID=84044004
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/936,547 Pending US20230130634A1 (en) | 2021-10-05 | 2022-09-29 | Optimizing Inference Performance for Conformer |
Country Status (2)
Country | Link |
---|---|
US (1) | US20230130634A1 (en) |
WO (1) | WO2023060008A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20230419988A1 (en) * | 2022-06-24 | 2023-12-28 | Actionpower Corp. | Method for detecting speech in audio data |
Family Cites Families (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP3948853A1 (en) * | 2019-05-03 | 2022-02-09 | Google LLC | End-to-end automated speech recognition on numeric sequences |
KR20220148245A (en) * | 2020-03-04 | 2022-11-04 | 구글 엘엘씨 | Consistency Prediction for Streaming Sequence Models |
-
2022
- 2022-09-29 US US17/936,547 patent/US20230130634A1/en active Pending
- 2022-09-29 WO PCT/US2022/077247 patent/WO2023060008A1/en unknown
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20230419988A1 (en) * | 2022-06-24 | 2023-12-28 | Actionpower Corp. | Method for detecting speech in audio data |
US11967340B2 (en) * | 2022-06-24 | 2024-04-23 | Actionpower Corp. | Method for detecting speech in audio data |
Also Published As
Publication number | Publication date |
---|---|
WO2023060008A1 (en) | 2023-04-13 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11741947B2 (en) | Transformer transducer: one model unifying streaming and non-streaming speech recognition | |
US20220122622A1 (en) | Cascaded Encoders for Simplified Streaming and Non-Streaming ASR | |
US11961515B2 (en) | Contrastive Siamese network for semi-supervised speech recognition | |
US20230343328A1 (en) | Efficient streaming non-recurrent on-device end-to-end model | |
US20220310065A1 (en) | Supervised and Unsupervised Training with Contrastive Loss Over Sequences | |
US20220310073A1 (en) | Mixture Model Attention for Flexible Streaming and Non-Streaming Automatic Speech Recognition | |
US20230130634A1 (en) | Optimizing Inference Performance for Conformer | |
US20230352006A1 (en) | Tied and reduced rnn-t | |
US20230317059A1 (en) | Alignment Prediction to Inject Text into Automatic Speech Recognition Training | |
US11823697B2 (en) | Improving speech recognition with speech synthesis-based model adapation | |
US20230109407A1 (en) | Transducer-Based Streaming Deliberation for Cascaded Encoders | |
US20230107248A1 (en) | Deliberation of Streaming RNN-Transducer by Non-Autoregressive Decoding | |
US20230306958A1 (en) | Streaming End-to-end Multilingual Speech Recognition with Joint Language Identification | |
US20230326461A1 (en) | Unified Cascaded Encoder ASR model for Dynamic Model Sizes | |
US20240153495A1 (en) | Multi-Output Decoders for Multi-Task Learning of ASR and Auxiliary Tasks | |
US20220310081A1 (en) | Multilingual Re-Scoring Models for Automatic Speech Recognition | |
US20230298570A1 (en) | Rare Word Recognition with LM-aware MWER Training | |
US20240135923A1 (en) | Universal Monolingual Output Layer for Multilingual Speech Recognition | |
US20230107695A1 (en) | Fusion of Acoustic and Text Representations in RNN-T | |
US20230017892A1 (en) | Injecting Text in Self-Supervised Speech Pre-training | |
US20230013587A1 (en) | Advancing the Use of Text and Speech in ASR Pretraining With Consistency and Contrastive Losses | |
US20240028829A1 (en) | Joint Speech and Text Streaming Model for ASR | |
US20230103382A1 (en) | Training for long-form speech recognition | |
US20230107493A1 (en) | Predicting Word Boundaries for On-Device Batching of End-To-End Speech Recognition Models | |
WO2024081332A1 (en) | Universal monolingual output layer for multilingual speech recognition |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:SAINATH, TARA N.;BOTROS, RAMI;GULATI, ANMOL;AND OTHERS;SIGNING DATES FROM 20220928 TO 20220929;REEL/FRAME:062098/0657 |