US20210019599A1 - Adaptive neural architecture search - Google Patents
Adaptive neural architecture search Download PDFInfo
- Publication number
- US20210019599A1 US20210019599A1 US16/933,690 US202016933690A US2021019599A1 US 20210019599 A1 US20210019599 A1 US 20210019599A1 US 202016933690 A US202016933690 A US 202016933690A US 2021019599 A1 US2021019599 A1 US 2021019599A1
- Authority
- US
- United States
- Prior art keywords
- neural network
- architecture
- candidate
- block
- blocks
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000001537 neural effect Effects 0.000 title description 9
- 230000003044 adaptive effect Effects 0.000 title 1
- 238000013528 artificial neural network Methods 0.000 claims abstract description 316
- 238000000034 method Methods 0.000 claims abstract description 64
- 238000012549 training Methods 0.000 claims abstract description 55
- 238000010801 machine learning Methods 0.000 claims abstract description 22
- 238000003860 storage Methods 0.000 claims abstract description 9
- 238000010200 validation analysis Methods 0.000 claims description 16
- 230000004044 response Effects 0.000 claims description 6
- 238000005070 sampling Methods 0.000 claims description 4
- 238000009826 distribution Methods 0.000 claims description 3
- 238000005457 optimization Methods 0.000 claims description 3
- 238000004590 computer program Methods 0.000 abstract description 15
- 230000008569 process Effects 0.000 description 30
- 238000012545 processing Methods 0.000 description 19
- 230000000306 recurrent effect Effects 0.000 description 8
- 230000009471 action Effects 0.000 description 7
- 238000004891 communication Methods 0.000 description 6
- 238000013526 transfer learning Methods 0.000 description 5
- 239000003795 chemical substances by application Substances 0.000 description 4
- 238000010586 diagram Methods 0.000 description 4
- 230000036541 health Effects 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 4
- 238000012546 transfer Methods 0.000 description 4
- 230000003993 interaction Effects 0.000 description 3
- 230000002411 adverse Effects 0.000 description 2
- 238000013527 convolutional neural network Methods 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 241000009334 Singa Species 0.000 description 1
- 230000004913 activation Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000001149 cognitive effect Effects 0.000 description 1
- 238000000354 decomposition reaction Methods 0.000 description 1
- 238000013135 deep learning Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 238000003745 diagnosis Methods 0.000 description 1
- 230000010339 dilation Effects 0.000 description 1
- 238000003709 image segmentation Methods 0.000 description 1
- 230000003116 impacting effect Effects 0.000 description 1
- 230000000977 initiatory effect Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 230000035772 mutation Effects 0.000 description 1
- 238000003058 natural language processing Methods 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
- 238000009827 uniform distribution Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G06N3/0454—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/086—Learning methods using evolutionary algorithms, e.g. genetic algorithms or genetic programming
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/047—Probabilistic or stochastic networks
-
- G06N3/0472—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
Definitions
- This specification relates to determining architectures for neural networks.
- Neural networks are machine learning models that employ one or more layers of nonlinear units to predict an output for a received input.
- Some neural networks include one or more hidden layers in addition to an output layer. The output of each hidden layer is used as input to the next layer in the network, i.e., the next hidden layer or the output layer.
- Each layer of the network generates an output from a received input in accordance with current values of a respective set of parameters.
- a recurrent neural network is a neural network that receives an input sequence and generates an output sequence from the input sequence.
- a recurrent neural network can use some or all of the internal state of the network from a previous time step in computing an output at a current time step.
- An example of a recurrent neural network is a long short term (LSTM) neural network that includes one or more LSTM memory blocks. Each LSTM memory block can include one or more cells that each include an input gate, a forget gate, and an output gate that allow the cell to store previous states for the cell, e.g., for use in generating a current activation or to be provided to other components of the LSTM neural network.
- LSTM long short term
- This specification describes a system implemented as computer programs on one or more computers in one or more locations that determines a network architecture for a task neural network that is configured to perform a particular machine learning task.
- the system can determine a network architecture that achieves or even exceeds state of the art performance on any of a variety of machine learning tasks, e.g., image classification or another image processing task or speech recognition, keyword spotting, or another audio processing task. Additionally, the system can determine this architecture in a manner that is much more computationally efficient than existing techniques, i.e., that consumes many fewer computational resources than existing techniques, and that is faster in terms of wall-clock time than existing techniques.
- the system dynamically selects the number of training steps that a candidate architecture will be trained for based on the size of the candidate, reducing the time and resources consumed by the training even further, as smaller candidate neural networks can be trained for fewer training steps without adversely impacting the quality of the architecture search.
- the system employs parameter value transfer when generating a mutated architecture, reducing the amount of training required for training the mutated architecture.
- FIG. 1 shows an example neural architecture search system.
- FIG. 2 is a flow diagram of an example process for searching for an architecture for a task neural network.
- FIG. 3 is a flow diagram of an example process for selecting a weighted ensemble of candidate neural networks.
- FIG. 4 shows an example of using transfer learning when initializing the parameters of a new neural network for training during the architecture search.
- This specification describes a system implemented as computer programs on one or more computers in one or more locations that determines an architecture for a task neural network that is configured to perform a particular neural network task.
- the neural network can be trained to perform any kind of machine learning task, i.e., can be configured to receive any kind of digital data input and to generate any kind of score, classification, or regression output based on the input.
- the neural network is a neural network that is configured to perform an image processing task, i.e., receive an input image and to process the input image to generate a network output for the input image.
- the task may be image classification and the output generated by the neural network for a given image may be scores for each of a set of object categories, with each score representing an estimated likelihood that the image contains an image of an object belonging to the category.
- the task can be image embedding generation and the output generated by the neural network can be a numeric embedding of the input image.
- the task can be object detection and the output generated by the neural network can identify locations in the input image at which particular types of objects are depicted.
- the task can be image segmentation and the output generated by the neural network can assign each pixel of the input image to a category from a set of categories.
- the task can be to classify the resource or document, i.e., the output generated by the neural network for a given Internet resource, document, or portion of a document may be a score for each of a set of topics, with each score representing an estimated likelihood that the Internet resource, document, or document portion is about the topic.
- the resource or document i.e., the output generated by the neural network for a given Internet resource, document, or portion of a document may be a score for each of a set of topics, with each score representing an estimated likelihood that the Internet resource, document, or document portion is about the topic.
- the output generated by the neural network may be a score that represents an estimated likelihood that the particular advertisement will be clicked on.
- the output generated by the neural network may be a score for each of a set of content items, with each score representing an estimated likelihood that the user will respond favorably to being recommended the content item.
- the output generated by the neural network may be a score for each of a set of pieces of text in another language, with each score representing an estimated likelihood that the piece of text in the other language is a proper translation of the input text into the other language.
- the task may be an audio processing task.
- the output generated by the neural network may be a score for each of a set of pieces of text, each score representing an estimated likelihood that the piece of text is the correct transcript for the utterance.
- the task may be a keyword spotting task where, if the input to the neural network is a sequence representing a spoken utterance, the output generated by the neural network can indicate whether a particular word or phrase (“hotword”) was spoken in the utterance.
- the output generated by the neural network can identify the natural language in which the utterance was spoken.
- the task can be a natural language processing or understanding task, e.g., an entailment task, a paraphrase task, a textual similarity task, a sentiment task, a sentence completion task, a grammaticality task, and so on, that operates on a sequence of text in some natural language.
- a natural language processing or understanding task e.g., an entailment task, a paraphrase task, a textual similarity task, a sentiment task, a sentence completion task, a grammaticality task, and so on, that operates on a sequence of text in some natural language.
- the task can be a text to speech task, where the input is text in a natural language or features of text in a natural language and the network output is a spectrogram or other data defining audio of the text being spoken in the natural language.
- the task can be a health prediction task, where the input is electronic health record data for a patient and the output is a prediction that is relevant to the future health of the patient, e.g., a predicted treatment that should be prescribed to the patient, the likelihood that an adverse health event will occur to the patient, or a predicted diagnosis for the patient.
- a prediction that is relevant to the future health of the patient, e.g., a predicted treatment that should be prescribed to the patient, the likelihood that an adverse health event will occur to the patient, or a predicted diagnosis for the patient.
- the task can be an agent control task, where the input is an observation characterizing the state of an environment and the output defines an action to be performed by the agent in response to the observation.
- the agent can be, e.g., a real-world or simulated robot, a control system for an industrial facility, or a control system that controls a different kind of agent.
- FIG. 1 shows an example neural architecture search system 100 .
- the neural architecture search system 100 is an example of a system implemented as computer programs on one or more computers in one or more locations, in which the systems, components, and techniques described below can be implemented.
- the neural architecture search system 100 is a system that obtains training data 102 for training a neural network to perform a particular task and a validation set 104 for evaluating the performance of the neural network on the particular task and uses the training data 102 and the validation set 104 to determine an architecture for a neural network that is configured to perform the particular task.
- the architecture defines the number of layers in the neural network, the operations performed by each of the layers, and the connectivity between the layers in the neural network, i.e., which layers receive inputs from which other layers in the neural network.
- the training data 102 and the validation set 104 both include a set of neural network inputs and, for each network input, a respective target output that should be generated by the neural network to perform the particular task.
- a larger set of training data may have been randomly partitioned to generate the training data 102 and the validation set 104 .
- the system 100 can receive the training data 102 and the validation set 104 in any of a variety of ways.
- the system 100 can receive training data as an upload from a remote user of the system over a data communication network, e.g., using an application programming interface (API) made available by the system 100 , and randomly divide the uploaded data into the training data 102 and the validation set 104 .
- API application programming interface
- the system 100 can receive an input from a user specifying which data that is already maintained by the system 100 should be used for training the neural network, and then divide the specified data into the training data 102 and the validation set 104 .
- the system 100 determines the architecture for the neural network by repeatedly modifying architectures in a set of candidate architectures, evaluating the performance of the modified architectures on the task, and then adding the modified architectures to the set in association with a performance measure that reflects the performance of the architecture on the task.
- the system 100 maintains population data 130 specifying a set of candidate architectures and associating each candidate architecture in the set with a corresponding performance measure.
- the system 100 repeatedly adds new candidate architectures and corresponding performance measures to the population data 130 by performing a search process and, after the search process has terminated, uses the performance measures for the architectures in the population data 130 to determine the final architecture for the neural network.
- Each candidate architecture in the set is a tower.
- a tower is a neural network that includes a sequence of neural network blocks, with each block after the first block in the sequence receiving input from one or more blocks that are earlier in the sequence, receiving the network input, or both. While different architectures can include different numbers of blocks, the sequence of blocks in any given candidate includes at least one and at most a fixed, maximum number of blocks.
- each tower may optionally include one or more pre-determined components, e.g., one or more input layers before the first block in the sequence, one or more output layers after the last block in the sequence, or both.
- Each neural network block in each candidate architecture is selected from a set of possible neural network blocks.
- the search space for the final architecture is the set of possible combinations of neural network blocks in the set that include at most the maximum number of blocks.
- a neural network block is a combination of one or more neural network layers that receives one or more input tensors and generates as output one or more output tensors.
- the types of neural network blocks that are in the set of possible network blocks will generally differ based on the neural network task.
- the blocks in the set will include blocks with different configurations of convolutional layers and, optionally, blocks with other kinds of neural network layers, e.g., fully-connected layers.
- An example set of blocks for a convolutional neural network is illustrated in Table 1:
- FIXCONV is a convolution with fixed output channels.
- RESNET blocks refer to the residual deep learning connection, i.e., two convolutions with a skip connection.
- DILATEDCONV is dilated convolution layer.
- CONVOLUTION and DOWNSAMPLECONV are convolutional layers with different filter sizes, where DOWNSAMPLECONV has stride greater than 1 and increases the number of channels.
- NAS-A and NAS-A-REDUCTION are the normal and reduction NASNet cells, respectively.
- FULLYCONN is a fully connected layer with different number of nodes.
- the blocks in the set will include blocks with different configurations of recurrent layers and, optionally, other kinds of layers, e.g., fully-connected layers or projection layers.
- An example set of blocks for a recurrent neural network is illustrated in Table 2:
- RNN is a block of one or more recurrent neural network layers of varying dimensions.
- PROJ is a projection layer that projects an input to an output that has varying dimensions.
- SV DF is a single value decomposition filter layer that approximates a fully-connected layer with a low rank approximation.
- the system 100 To determine the final architecture, the system 100 repeatedly performs the search process using an architecture generation engine 110 and a training engine 120 .
- the architecture generation engine 110 repeatedly (i) selects, based on the performance measures in the maintained data 130 , a candidate architecture from the set of candidate architectures and (ii) selects a neural network block from the set of neural network blocks, e.g., randomly or using Bayesian optimization.
- the architecture generation engine 110 determines whether to (i) add the selected neural network block as a new neural network block in the candidate architecture (after the last block in the sequence) or (ii) replace one of the neural network blocks in the selected candidate architecture with the selected neural network block. Thus, the architecture generation engine 110 determines whether to expand the size of the architecture by one block or to replace an existing block in the architecture.
- the architecture generation engine 120 then generates, based on the results of the determining, a mutated architecture 112 by either (i) adding the selected neural network block as a new neural network block in the selected candidate architecture or (ii) replacing one of the neural network blocks in the selected candidate architecture with the selected neural network block.
- the engine 120 grows architectures adaptively and incrementally via greedy mutations to reduce the sample complexity of the search process.
- the training engine 120 trains a neural network having the mutated architecture 112 on the training data 102 and determines a performance measure 122 for the trained neural network that measures the performance of the trained neural network on the particular machine learning task, i.e., by evaluating the performance of the trained neural network on the validation data set 104 .
- the performance measure can be the loss of the trained neural network on the validation data set 104 or the result of some other measure of model accuracy when computed over the validation data set 104 .
- the system 100 then adds, to the maintained data, data specifying the mutated architecture 112 and data associating the mutated architecture 112 with the determined performance measure 122 .
- the system 100 can select a final architecture for the neural network using the architectures and performance measures in the maintained data 130 .
- the neural network search system 100 can then output architecture data 150 that specifies the final architecture of the neural network, i.e., data specifying the layers that are part of the neural network, the connectivity between the layers, and the operations performed by the layers.
- the neural network search system 100 can output the architecture data 150 to the user that submitted the training data.
- the system 100 instantiates an instance of the neural network having the determined architecture and with trained parameters, e.g., either trained from scratch by the system after determining the final architecture, making use of the parameter values generated as a result of the search process, or generated by fine-tuning the parameter values generated as a result of the search process, and then uses the trained neural network to process requests received by users, e.g., through the API provided by the system. That is, the system 100 can receive inputs to be processed, use the trained neural network to process the inputs, and provide the outputs generated by the trained neural network or data derived from the generated outputs in response to the received inputs.
- trained parameters e.g., either trained from scratch by the system after determining the final architecture, making use of the parameter values generated as a result of the search process, or generated by fine-tuning the parameter values generated as a result of the search process.
- FIG. 2 is a flow diagram of an example process 200 for searching for an architecture for a task neural network.
- the process 200 will be described as being performed by a system of one or more computers located in one or more locations.
- a neural architecture search system e.g., the neural architecture search system 100 of FIG. 1 , appropriately programmed, can perform the process 200 .
- the system can then repeatedly perform the process 200 to update the set of candidate architectures in the maintained population data.
- the system can distribute the certain steps of the process 200 across multiple devices within the system.
- multiple different heterogeneous or homogenous devices can asynchronously perform the process 200 to repeatedly update population data that is shared between all of the devices.
- the system selects, based on the performance measures in the population data, a candidate architecture from the set of candidate architectures (step 202 ).
- the system can select, from the set of candidate architectures, a plurality of candidate architectures having the best performance measures, e.g., a fixed size subset of the set that have the best performance measures, and then sample the candidate architecture from the plurality of candidate architectures.
- a plurality of candidate architectures having the best performance measures e.g., a fixed size subset of the set that have the best performance measures
- the system selects a neural network block from the set of neural network blocks (step 204 ).
- the system selects a neural network block randomly from the set of neural network blocks.
- the system selects the block such that blocks that are more likely to increase the performance of the architecture are selected with a higher frequency.
- the system can select a neural network block from the set of neural network blocks using Baysian optimization in order to bias the selection towards blocks that are more likely improve the performance of the candidate neural network.
- the system determines whether to (i) add the selected neural network block as a new neural network block in the candidate architecture or (ii) replace one of the neural network blocks in the selected candidate architecture with the selected neural network block (step 206 ).
- the system can employ any of a variety of techniques that ensure that the search process adequately explores the space of possible architectures with a given number of blocks before moving on to architectures with a larger number of blocks.
- the system can determine to add the selected neural network block as a new block only if the number of neural network blocks in the selected candidate architecture is less than a predetermined maximum number of neural network blocks. That is, the system will not add a new block to the selected candidate architecture if the candidate architecture already includes the maximum number of blocks.
- the system can sample a value from a predetermined distribution and determine to add the selected neural network block as a new block only if the sampled value satisfies a threshold value. For example, the system can sample a value from the uniform distribution between zero and one, inclusive and only determine to add the selected neural network block as a new block only if the sampled value is less than a fixed value between zero and one. The fixed value between zero and one can be selected to govern how aggressively the system will search the space at any given number of blocks.
- the system can determine a number of architectures in the set of candidate architectures that have the same number of neural network blocks as the selected candidate architecture and determine to add the selected neural network block as a new block only if the number of architectures in the set of candidate architectures that have the same number of neural network blocks as the selected candidate architecture exceeds a threshold, e.g., exceeds a threshold value.
- the system can determine a number of architectures that are currently in the set of candidate architectures and determine to add the selected neural network block as a new block only if the number of architectures in the set of candidate architectures exceeds a threshold.
- the system can determine the threshold based on a predetermined exploration factor, i.e., a fixed positive value, and the current number of blocks in the selected candidate architecture, e.g., as the product of the exploration factor and the number of blocks.
- the system may jointly employ multiple ones of these techniques.
- the system can determine to add the new block as a new neural network block if and only if (i) the number of neural network blocks in the selected candidate architecture is less than the predetermined maximum number of neural network blocks, (ii) if the sampled value satisfies a threshold value, and (iii) the number of architectures in the set of candidate architectures exceeds a threshold that is determined based on the current number of blocks in the selected candidate architecture.
- the system generates a mutated architecture by either (i) adding the selected neural network block as a new neural network block in the selected candidate architecture or (ii) replacing one of the neural network blocks in the selected candidate architecture with the selected neural network block (step 208 ).
- the system in response to determining to add the selected neural network block as a new neural network block, adds the selected neural network block as a new neural network block in the selected candidate architecture, i.e., by adding the new neural network block as a new block at the end of the sequence after the block that is currently last in the sequence.
- the system In response to determining to replace one of the neural network blocks in the selected candidate architecture with the selected neural network block, the system replaces one of the neural network blocks in the selected candidate architecture with the selected neural network block.
- the system can randomly identify a neural network block from the selected candidate architecture; and replacing the randomly identified neural network block with the selected neural network block.
- the system trains a neural network having the mutated architecture on the training data, i.e., using a conventional machine learning technique that is appropriate for the task that the task neural network is configured to perform (step 210 ).
- the system trains each neural network for the same predetermined number of training iterations or until convergence.
- the system trains different neural networks for different numbers of training iterations.
- the system can determine a number of training iterations for which to train the neural network based on the number of neural network blocks in the mutated architecture, i.e., with the number of training iterations increasing as the number of neural network blocks in the mutated architecture increases.
- the system can linearly increase the number of iterations with the number of blocks in the architecture.
- shallow architectures having relatively few blocks will train for a shorter time, increasing the computational efficiency of the overall framework.
- the system trains the neural network having the mutated architecture starting from newly initialized values of the parameters of the blocks in the mutated architecture.
- the system makes use of transfer learning to speed up the training of the new neural network.
- the system leverages the previously trained parameters for those blocks of the mutated architecture that are identical with respect to the selected candidate architecture when initiating the training of the neural network.
- the system also includes in the maintained population data, for each candidate architecture, current parameter values for the parameters of each neural network block in the candidate architecture, i.e., the parameter values for each of the blocks after the neural network having the candidate architecture was trained.
- the system can use these current parameter values for the selected candidate architecture when initializing the parameter values of the neural network blocks in the mutated candidate architecture.
- the system can initialize the parameter values differently for different blocks of the mutated architecture depending on where the selected neural network block was inserted into the selected candidate architecture.
- the system can initialize the values of the parameters of the neural network block to the current values of the parameters for the block (within the selected candidate architecture) in the maintained data.
- the system For the selected neural network block and any neural network block in the candidate architecture that is after the selected neural network block in the candidate architecture, the system initializes the values of the parameters of the neural network block to newly initialized values.
- the system determines a performance measure for the trained neural network that measures the performance of the trained neural network on the particular machine learning task (step 212 ).
- the system can determine the performance of the trained neural network on the validation data set, i.e., the performance measure can be an appropriate measure that measures the performance of the trained neural network on the validation data set.
- performance measures that may be appropriate for different tasks include classification accuracy measures, intersection over union (IoU) measures for regression tasks, edit distance measures for text generation tasks, and so on.
- the system adds, to the maintained population data, data specifying the mutated architecture and data associating the mutated architecture with the determined performance measure (step 214 ).
- the system also adds, to the maintained data, the values of the parameters of the neural network blocks in the mutated architecture after the training of the neural network having the mutated architecture.
- the system can repeatedly update the population data to include candidate architectures with better performance measures.
- the system determines a final architecture for the task neural network.
- the system can select one of the candidate architectures in the set as the architecture for the task neural network based on the performance measures.
- the system can select the candidate architecture in the set having the best performance measure as the final architecture for the task neural network.
- the system can select a fixed number of candidate architectures from the set that have the best performance measures, further train a neural network having each of the selected architectures, determine an updated performance measure for each of the selected architectures based on the performance of the further trained neural networks on the validation data set, and select, as the final architecture for the task neural network, the candidate architecture having the best updated performance measure.
- the system can determine the architecture of the task neural network to be a weighted ensemble of a plurality of candidate architectures in the set, i.e., a weighted ensemble that includes a fixed number p of candidate architectures from the set where p is an integer greater than one.
- the architecture of the task neural network is an architecture that generates a final output for the neural network task as a weighted combination of the outputs generated by the plurality of candidate architectures in the ensemble.
- each architecture in the ensemble can be assigned the same weight, i.e., a weight equal to 1 /p, in the combination.
- the weights assigned to each architecture in the combination can be learned.
- FIG. 3 is a flow diagram of an example process 300 for selecting a weighted ensemble of candidate neural networks.
- the process 300 will be described as being performed by a system of one or more computers located in one or more locations.
- a neural architecture search system e.g., the neural architecture search system 100 of FIG. 1 , appropriately programmed, can perform the process 300 .
- the system selects a plurality of highest-performing candidate architectures from the set of candidate architectures based on the performance measures (step 302 ).
- the system generally selects a number of architectures that is greater than the fixed numberp in the ensemble. For example, the system can select the P architectures in the set that have the best performance measures, where P is an integer greater than p.
- the system generates a plurality of candidate ensembles, with each candidate ensemble including a different combination ofp candidate architectures from the plurality of highest-performing candidate architectures (step 304 ).
- the system generates a respective ensemble for each possible different combination ofp candidate architectures from the plurality of highest-performing candidate architectures.
- the system generates a fixed number of candidate ensembles, i.e., by repeatedly randomly sampling sets ofp candidate architectures from the plurality of highest-performing candidate architectures.
- the system selects, as the determined architecture, the ensemble of the plurality of candidate ensembles that performs best on the particular machine learning task (step 306 ).
- the system can determine the performance of each candidate ensemble on the validation data set as described above, but with the performance of an ensemble being based on the weighted combinations of outputs generated by the candidate architectures in the ensemble.
- the system can train each ensemble on all or part of the training data set in order to fine tune the weights (and, optionally, the parameters of the networks in the candidate ensemble) prior to determining the performance of each candidate ensemble on the validation data set.
- FIG. 4 shows an example of using transfer learning when initializing the parameters of a new neural network for training during the architecture search.
- a mutated architecture B(b) has been generated from a selected candidate architecture B(a).
- the selected candidate architecture includes an initial sequence of blocks ai through a 6 , followed by two fully-connected blocks afc and finally followed by a logits layer that generates a respective score for each of multiple categories (although only a 1 through a 3 are shown in the Figure).
- the fully-connected blocks afc and the logits layer can be fixed for all the candidate architectures in the set, while the blocks in the initial sequence of blocks can be learned through the search process.
- the mutated architecture includes an initial sequence of blocks b 1 through b 6 , followed by two fully-connected blocks bfc and finally followed by the logits layer.
- the system replaced the block a 3 in the selected candidate architecture with a new block b 3 .
- block a 1 and a 2 are the same as blocks b 1 and b 2 , respectively, while block a 3 is not the same as block b 3 . Therefore, when initializing the parameters of blocks b 1 and b 2 , the system initializes the values of the parameters of block b 1 to the current values of the parameters for the block a 1 in the maintained data and initializes the values of the parameters initializes the values of the parameters of block b 2 to the current values of the parameters for the block a 2 in the maintained data. This transfer is illustrated by an arrow in FIG. 4 .
- the system initializes the values of the parameters of the neural network block to newly initialized values, e.g., by setting the values to random values using a conventional random parameter initialization technique.
- the system shortens the training time and, accordingly, the amount of computational resources, required to train the new neural network while allowing the lower blocks learn features which can be extrapolated across architectures, improving the quality of the final determined architecture.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non transitory storage medium for execution by, or to control the operation of, data processing apparatus.
- the computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.
- the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- data processing apparatus refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can also be, or further include, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- the apparatus can optionally include, in addition to hardware, code that creates an execution environment for computer programs, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a computer program which may also be referred to or described as a program, software, a software application, an app, a module, a software module, a script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it can be deployed in any form, including as a stand alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub programs, or portions of code.
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
- the term “database” is used broadly to refer to any collection of data: the data does not need to be structured in any particular way, or structured at all, and it can be stored on storage devices in one or more locations.
- the index database can include multiple collections of data, each of which may be organized and accessed differently.
- engine is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more specific functions.
- an engine will be implemented as one or more software modules or components, installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines can be installed and running on the same computer or computers.
- the processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA or an ASIC, or by a combination of special purpose logic circuitry and one or more programmed computers.
- Computers suitable for the execution of a computer program can be based on general or special purpose microprocessors or both, or any other kind of central processing unit.
- a central processing unit will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data.
- the central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.
- PDA personal digital assistant
- GPS Global Positioning System
- USB universal serial bus
- Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto optical disks e.g., CD ROM and DVD-ROM disks.
- embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's device in response to requests received from the web browser.
- a computer can interact with a user by sending text messages or other forms of message to a personal device, e.g., a smartphone that is running a messaging application, and receiving responsive messages from the user in return.
- Data processing apparatus for implementing machine learning models can also include, for example, special-purpose hardware accelerator units for processing common and compute-intensive parts of machine learning training or production, i.e., inference, workloads.
- Machine learning models can be implemented and deployed using a machine learning framework, e.g., a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework.
- a machine learning framework e.g., a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework.
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (LAN) and a wide area network (WAN), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- a server transmits data, e.g., an HTML page, to a user device, e.g., for purposes of displaying data to and receiving user input from a user interacting with the device, which acts as a client.
- Data generated at the user device e.g., a result of the user interaction, can be received at the server from the device.
Abstract
Description
- This application claims priority to U.S. Provisional Application No. 62/876,548, filed on Jul. 19, 2019. The disclosure of the prior application is considered part of and is incorporated by reference in the disclosure of this application.
- This specification relates to determining architectures for neural networks.
- Neural networks are machine learning models that employ one or more layers of nonlinear units to predict an output for a received input. Some neural networks include one or more hidden layers in addition to an output layer. The output of each hidden layer is used as input to the next layer in the network, i.e., the next hidden layer or the output layer. Each layer of the network generates an output from a received input in accordance with current values of a respective set of parameters.
- Some neural networks are recurrent neural networks. A recurrent neural network is a neural network that receives an input sequence and generates an output sequence from the input sequence. In particular, a recurrent neural network can use some or all of the internal state of the network from a previous time step in computing an output at a current time step. An example of a recurrent neural network is a long short term (LSTM) neural network that includes one or more LSTM memory blocks. Each LSTM memory block can include one or more cells that each include an input gate, a forget gate, and an output gate that allow the cell to store previous states for the cell, e.g., for use in generating a current activation or to be provided to other components of the LSTM neural network.
- This specification describes a system implemented as computer programs on one or more computers in one or more locations that determines a network architecture for a task neural network that is configured to perform a particular machine learning task.
- Particular embodiments of the subject matter described in this specification can be implemented so as to realize one or more of the following advantages. By determining the architecture of a task neural network using the techniques described in this specification, the system can determine a network architecture that achieves or even exceeds state of the art performance on any of a variety of machine learning tasks, e.g., image classification or another image processing task or speech recognition, keyword spotting, or another audio processing task. Additionally, the system can determine this architecture in a manner that is much more computationally efficient than existing techniques, i.e., that consumes many fewer computational resources than existing techniques, and that is faster in terms of wall-clock time than existing techniques. In particular, many existing techniques rely on evaluating the performance of a large number of candidate architectures by training a network having the candidate architecture, with each candidate being the same, large size, e.g., the same size as the final candidate architecture that will be the output of the search process. This training is both time consuming and computationally intensive. The described techniques greatly reduce the time and resource consumption of this training by using a number of techniques that also result in improved performance in discovering new architectures. As a particular example, the system incrementally and greedily constructs candidate networks that will be trained (networks having “mutated architectures”), so that “full size” candidate neural networks are only trained once the space of smaller candidate neural networks has been sufficiently explored. Additionally, the system dynamically selects the number of training steps that a candidate architecture will be trained for based on the size of the candidate, reducing the time and resources consumed by the training even further, as smaller candidate neural networks can be trained for fewer training steps without adversely impacting the quality of the architecture search. Moreover, the system employs parameter value transfer when generating a mutated architecture, reducing the amount of training required for training the mutated architecture.
- The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
-
FIG. 1 shows an example neural architecture search system. -
FIG. 2 is a flow diagram of an example process for searching for an architecture for a task neural network. -
FIG. 3 is a flow diagram of an example process for selecting a weighted ensemble of candidate neural networks. -
FIG. 4 shows an example of using transfer learning when initializing the parameters of a new neural network for training during the architecture search. - Like reference numbers and designations in the various drawings indicate like elements.
- This specification describes a system implemented as computer programs on one or more computers in one or more locations that determines an architecture for a task neural network that is configured to perform a particular neural network task.
- The neural network can be trained to perform any kind of machine learning task, i.e., can be configured to receive any kind of digital data input and to generate any kind of score, classification, or regression output based on the input.
- In some cases, the neural network is a neural network that is configured to perform an image processing task, i.e., receive an input image and to process the input image to generate a network output for the input image. For example, the task may be image classification and the output generated by the neural network for a given image may be scores for each of a set of object categories, with each score representing an estimated likelihood that the image contains an image of an object belonging to the category. As another example, the task can be image embedding generation and the output generated by the neural network can be a numeric embedding of the input image. As yet another example, the task can be object detection and the output generated by the neural network can identify locations in the input image at which particular types of objects are depicted. As yet another example, the task can be image segmentation and the output generated by the neural network can assign each pixel of the input image to a category from a set of categories.
- As another example, if the inputs to the neural network are Internet resources (e.g., web pages), documents, or portions of documents or features extracted from Internet resources, documents, or portions of documents, the task can be to classify the resource or document, i.e., the output generated by the neural network for a given Internet resource, document, or portion of a document may be a score for each of a set of topics, with each score representing an estimated likelihood that the Internet resource, document, or document portion is about the topic.
- As another example, if the inputs to the neural network are features of an impression context for a particular advertisement, the output generated by the neural network may be a score that represents an estimated likelihood that the particular advertisement will be clicked on.
- As another example, if the inputs to the neural network are features of a personalized recommendation for a user, e.g., features characterizing the context for the recommendation, e.g., features characterizing previous actions taken by the user, the output generated by the neural network may be a score for each of a set of content items, with each score representing an estimated likelihood that the user will respond favorably to being recommended the content item.
- As another example, if the input to the neural network is a sequence of text in one language, the output generated by the neural network may be a score for each of a set of pieces of text in another language, with each score representing an estimated likelihood that the piece of text in the other language is a proper translation of the input text into the other language.
- As another example, the task may be an audio processing task. For example, if the input to the neural network is a sequence representing a spoken utterance, the output generated by the neural network may be a score for each of a set of pieces of text, each score representing an estimated likelihood that the piece of text is the correct transcript for the utterance. As another example, the task may be a keyword spotting task where, if the input to the neural network is a sequence representing a spoken utterance, the output generated by the neural network can indicate whether a particular word or phrase (“hotword”) was spoken in the utterance. As another example, if the input to the neural network is a sequence representing a spoken utterance, the output generated by the neural network can identify the natural language in which the utterance was spoken.
- As another example, the task can be a natural language processing or understanding task, e.g., an entailment task, a paraphrase task, a textual similarity task, a sentiment task, a sentence completion task, a grammaticality task, and so on, that operates on a sequence of text in some natural language.
- As another example, the task can be a text to speech task, where the input is text in a natural language or features of text in a natural language and the network output is a spectrogram or other data defining audio of the text being spoken in the natural language.
- As another example, the task can be a health prediction task, where the input is electronic health record data for a patient and the output is a prediction that is relevant to the future health of the patient, e.g., a predicted treatment that should be prescribed to the patient, the likelihood that an adverse health event will occur to the patient, or a predicted diagnosis for the patient.
- As another example, the task can be an agent control task, where the input is an observation characterizing the state of an environment and the output defines an action to be performed by the agent in response to the observation. The agent can be, e.g., a real-world or simulated robot, a control system for an industrial facility, or a control system that controls a different kind of agent.
-
FIG. 1 shows an example neuralarchitecture search system 100. The neuralarchitecture search system 100 is an example of a system implemented as computer programs on one or more computers in one or more locations, in which the systems, components, and techniques described below can be implemented. - The neural
architecture search system 100 is a system that obtainstraining data 102 for training a neural network to perform a particular task and a validation set 104 for evaluating the performance of the neural network on the particular task and uses thetraining data 102 and the validation set 104 to determine an architecture for a neural network that is configured to perform the particular task. - The architecture defines the number of layers in the neural network, the operations performed by each of the layers, and the connectivity between the layers in the neural network, i.e., which layers receive inputs from which other layers in the neural network.
- Generally, the
training data 102 and the validation set 104 both include a set of neural network inputs and, for each network input, a respective target output that should be generated by the neural network to perform the particular task. For example, a larger set of training data may have been randomly partitioned to generate thetraining data 102 and the validation set 104. - The
system 100 can receive thetraining data 102 and the validation set 104 in any of a variety of ways. For example, thesystem 100 can receive training data as an upload from a remote user of the system over a data communication network, e.g., using an application programming interface (API) made available by thesystem 100, and randomly divide the uploaded data into thetraining data 102 and the validation set 104. As another example, thesystem 100 can receive an input from a user specifying which data that is already maintained by thesystem 100 should be used for training the neural network, and then divide the specified data into thetraining data 102 and the validation set 104. - Generally, the
system 100 determines the architecture for the neural network by repeatedly modifying architectures in a set of candidate architectures, evaluating the performance of the modified architectures on the task, and then adding the modified architectures to the set in association with a performance measure that reflects the performance of the architecture on the task. - In particular, the
system 100 maintainspopulation data 130 specifying a set of candidate architectures and associating each candidate architecture in the set with a corresponding performance measure. - The
system 100 repeatedly adds new candidate architectures and corresponding performance measures to thepopulation data 130 by performing a search process and, after the search process has terminated, uses the performance measures for the architectures in thepopulation data 130 to determine the final architecture for the neural network. - Each candidate architecture in the set is a tower. A tower is a neural network that includes a sequence of neural network blocks, with each block after the first block in the sequence receiving input from one or more blocks that are earlier in the sequence, receiving the network input, or both. While different architectures can include different numbers of blocks, the sequence of blocks in any given candidate includes at least one and at most a fixed, maximum number of blocks. In addition to the sequence of one or more neural network blocks, each tower may optionally include one or more pre-determined components, e.g., one or more input layers before the first block in the sequence, one or more output layers after the last block in the sequence, or both.
- Each neural network block in each candidate architecture is selected from a set of possible neural network blocks. Thus, the search space for the final architecture is the set of possible combinations of neural network blocks in the set that include at most the maximum number of blocks. A neural network block is a combination of one or more neural network layers that receives one or more input tensors and generates as output one or more output tensors.
- The types of neural network blocks that are in the set of possible network blocks will generally differ based on the neural network task.
- For example, when the neural network is a convolutional neural network, e.g., for performing an image processing task, the blocks in the set will include blocks with different configurations of convolutional layers and, optionally, blocks with other kinds of neural network layers, e.g., fully-connected layers. An example set of blocks for a convolutional neural network is illustrated in Table 1:
-
TABLE 1 BLOCK TYPE # INPUTS DESCRIPTION OF k POSSIBLE k VALUES FIXCONV k1 OUTPUT CHANNELS 32, 64, 96, 120 RESNET k1 FILTER SIZE 3 × 3, 5 × 5 DILATEDCONV k1 DILATION RATE 2, 4 CONVOLUTION k1 FILTER SIZE 3 × 3, 5 × 5, 1 × 7, 1 × 5, 1 × 3, 3 × 1, 5 × 1, 7 × 1 DOWNSAMPLECONV k1 FILTER SIZE 3 × 3, 5 × 5 NAS-A 2 N/A NAS-A-REDUCTION 2 N/ A FULLYCONN k 1 HIDDEN NODES 128, 256, 512, 1024 - In Table 1, the system can select from different versions of each type of block by selecting a value for k. In more detail, FIXCONV is a convolution with fixed output channels. RESNET blocks refer to the residual deep learning connection, i.e., two convolutions with a skip connection. DILATEDCONV is dilated convolution layer. CONVOLUTION and DOWNSAMPLECONV are convolutional layers with different filter sizes, where DOWNSAMPLECONV has stride greater than 1 and increases the number of channels. NAS-A and NAS-A-REDUCTION are the normal and reduction NASNet cells, respectively. Finally, FULLYCONN is a fully connected layer with different number of nodes.
- As another example, when the neural network is a recurrent neural network, the blocks in the set will include blocks with different configurations of recurrent layers and, optionally, other kinds of layers, e.g., fully-connected layers or projection layers. An example set of blocks for a recurrent neural network is illustrated in Table 2:
-
TABLE 2 Type β (dimensions) RNNk 64, 128, 256 PROJk 64, 128, 256 SVDFk−d 64-4, 128-4, 256-4 512-4, 64-8, 128-8 256-8, 64-16 128-16, 256-16 - In table 2, the system can select from different versions of each block by selecting a value for Beta to specify the dimensions of the layers in the block. In Table 2, RNN is a block of one or more recurrent neural network layers of varying dimensions. PROJ is a projection layer that projects an input to an output that has varying dimensions. SV DF is a single value decomposition filter layer that approximates a fully-connected layer with a low rank approximation.
- To determine the final architecture, the
system 100 repeatedly performs the search process using anarchitecture generation engine 110 and atraining engine 120. - The
architecture generation engine 110 repeatedly (i) selects, based on the performance measures in the maintaineddata 130, a candidate architecture from the set of candidate architectures and (ii) selects a neural network block from the set of neural network blocks, e.g., randomly or using Bayesian optimization. - The
architecture generation engine 110 then determines whether to (i) add the selected neural network block as a new neural network block in the candidate architecture (after the last block in the sequence) or (ii) replace one of the neural network blocks in the selected candidate architecture with the selected neural network block. Thus, thearchitecture generation engine 110 determines whether to expand the size of the architecture by one block or to replace an existing block in the architecture. - The
architecture generation engine 120 then generates, based on the results of the determining, amutated architecture 112 by either (i) adding the selected neural network block as a new neural network block in the selected candidate architecture or (ii) replacing one of the neural network blocks in the selected candidate architecture with the selected neural network block. - By generating the mutated architectures in this manner, the
engine 120 grows architectures adaptively and incrementally via greedy mutations to reduce the sample complexity of the search process. - Generating a mutated architecture will be described in more detail below with reference to
FIG. 2 . - For each
mutated architecture 112 that is generated by theengine 110, thetraining engine 120 trains a neural network having the mutatedarchitecture 112 on thetraining data 102 and determines aperformance measure 122 for the trained neural network that measures the performance of the trained neural network on the particular machine learning task, i.e., by evaluating the performance of the trained neural network on thevalidation data set 104. For example, the performance measure can be the loss of the trained neural network on thevalidation data set 104 or the result of some other measure of model accuracy when computed over thevalidation data set 104. - The
system 100 then adds, to the maintained data, data specifying the mutatedarchitecture 112 and data associating the mutatedarchitecture 112 with thedetermined performance measure 122. - Once the search process has been completed, the
system 100 can select a final architecture for the neural network using the architectures and performance measures in the maintaineddata 130. - Selecting a final architecture is described in more detail below with reference to
FIG. 3 . - The neural
network search system 100 can thenoutput architecture data 150 that specifies the final architecture of the neural network, i.e., data specifying the layers that are part of the neural network, the connectivity between the layers, and the operations performed by the layers. For example, the neuralnetwork search system 100 can output thearchitecture data 150 to the user that submitted the training data. - In some implementations, instead of or in addition to outputting the
architecture data 150, thesystem 100 instantiates an instance of the neural network having the determined architecture and with trained parameters, e.g., either trained from scratch by the system after determining the final architecture, making use of the parameter values generated as a result of the search process, or generated by fine-tuning the parameter values generated as a result of the search process, and then uses the trained neural network to process requests received by users, e.g., through the API provided by the system. That is, thesystem 100 can receive inputs to be processed, use the trained neural network to process the inputs, and provide the outputs generated by the trained neural network or data derived from the generated outputs in response to the received inputs. -
FIG. 2 is a flow diagram of anexample process 200 for searching for an architecture for a task neural network. For convenience, theprocess 200 will be described as being performed by a system of one or more computers located in one or more locations. For example, a neural architecture search system, e.g., the neuralarchitecture search system 100 ofFIG. 1 , appropriately programmed, can perform theprocess 200. - As described above, during the search for an architecture the system maintains population data.
- The system can then repeatedly perform the
process 200 to update the set of candidate architectures in the maintained population data. - In some implementations, the system can distribute the certain steps of the
process 200 across multiple devices within the system. As a particular example, multiple different heterogeneous or homogenous devices can asynchronously perform theprocess 200 to repeatedly update population data that is shared between all of the devices. - The system selects, based on the performance measures in the population data, a candidate architecture from the set of candidate architectures (step 202).
- As one example, the system can select, from the set of candidate architectures, a plurality of candidate architectures having the best performance measures, e.g., a fixed size subset of the set that have the best performance measures, and then sample the candidate architecture from the plurality of candidate architectures.
- The system selects a neural network block from the set of neural network blocks (step 204).
- In some implementations, the system selects a neural network block randomly from the set of neural network blocks.
- In some other implementations, the system selects the block such that blocks that are more likely to increase the performance of the architecture are selected with a higher frequency. As a particular example, the system can select a neural network block from the set of neural network blocks using Baysian optimization in order to bias the selection towards blocks that are more likely improve the performance of the candidate neural network.
- The system determines whether to (i) add the selected neural network block as a new neural network block in the candidate architecture or (ii) replace one of the neural network blocks in the selected candidate architecture with the selected neural network block (step 206).
- When determining whether to (i) add the selected neural network block as a new neural network block in the selected candidate architecture or (ii) replace one of the neural network blocks in the selected candidate architecture with the selected neural network block, the system can employ any of a variety of techniques that ensure that the search process adequately explores the space of possible architectures with a given number of blocks before moving on to architectures with a larger number of blocks.
- For example, the system can determine to add the selected neural network block as a new block only if the number of neural network blocks in the selected candidate architecture is less than a predetermined maximum number of neural network blocks. That is, the system will not add a new block to the selected candidate architecture if the candidate architecture already includes the maximum number of blocks.
- As another example, the system can sample a value from a predetermined distribution and determine to add the selected neural network block as a new block only if the sampled value satisfies a threshold value. For example, the system can sample a value from the uniform distribution between zero and one, inclusive and only determine to add the selected neural network block as a new block only if the sampled value is less than a fixed value between zero and one. The fixed value between zero and one can be selected to govern how aggressively the system will search the space at any given number of blocks.
- As yet another example, the system can determine a number of architectures in the set of candidate architectures that have the same number of neural network blocks as the selected candidate architecture and determine to add the selected neural network block as a new block only if the number of architectures in the set of candidate architectures that have the same number of neural network blocks as the selected candidate architecture exceeds a threshold, e.g., exceeds a threshold value.
- As yet another example, the system can determine a number of architectures that are currently in the set of candidate architectures and determine to add the selected neural network block as a new block only if the number of architectures in the set of candidate architectures exceeds a threshold. For example, the system can determine the threshold based on a predetermined exploration factor, i.e., a fixed positive value, and the current number of blocks in the selected candidate architecture, e.g., as the product of the exploration factor and the number of blocks.
- In some cases, the system may jointly employ multiple ones of these techniques. As a particular example, the system can determine to add the new block as a new neural network block if and only if (i) the number of neural network blocks in the selected candidate architecture is less than the predetermined maximum number of neural network blocks, (ii) if the sampled value satisfies a threshold value, and (iii) the number of architectures in the set of candidate architectures exceeds a threshold that is determined based on the current number of blocks in the selected candidate architecture.
- The system generates a mutated architecture by either (i) adding the selected neural network block as a new neural network block in the selected candidate architecture or (ii) replacing one of the neural network blocks in the selected candidate architecture with the selected neural network block (step 208).
- In other words, in response to determining to add the selected neural network block as a new neural network block, the system adds the selected neural network block as a new neural network block in the selected candidate architecture, i.e., by adding the new neural network block as a new block at the end of the sequence after the block that is currently last in the sequence.
- In response to determining to replace one of the neural network blocks in the selected candidate architecture with the selected neural network block, the system replaces one of the neural network blocks in the selected candidate architecture with the selected neural network block. As a particular example, the system can randomly identify a neural network block from the selected candidate architecture; and replacing the randomly identified neural network block with the selected neural network block.
- The system trains a neural network having the mutated architecture on the training data, i.e., using a conventional machine learning technique that is appropriate for the task that the task neural network is configured to perform (step 210).
- In some implementations, the system trains each neural network for the same predetermined number of training iterations or until convergence.
- In other implementations, however, the system trains different neural networks for different numbers of training iterations. In particular, the system can determine a number of training iterations for which to train the neural network based on the number of neural network blocks in the mutated architecture, i.e., with the number of training iterations increasing as the number of neural network blocks in the mutated architecture increases. For example, the system can linearly increase the number of iterations with the number of blocks in the architecture. Thus, during the early stages of the search, shallow architectures having relatively few blocks will train for a shorter time, increasing the computational efficiency of the overall framework.
- Moreover, in some implementations, the system trains the neural network having the mutated architecture starting from newly initialized values of the parameters of the blocks in the mutated architecture.
- In some other implementations, however, the system makes use of transfer learning to speed up the training of the new neural network. In particular, the system leverages the previously trained parameters for those blocks of the mutated architecture that are identical with respect to the selected candidate architecture when initiating the training of the neural network.
- More specifically, when transfer learning is used, the system also includes in the maintained population data, for each candidate architecture, current parameter values for the parameters of each neural network block in the candidate architecture, i.e., the parameter values for each of the blocks after the neural network having the candidate architecture was trained.
- The system can use these current parameter values for the selected candidate architecture when initializing the parameter values of the neural network blocks in the mutated candidate architecture. In particular, the system can initialize the parameter values differently for different blocks of the mutated architecture depending on where the selected neural network block was inserted into the selected candidate architecture.
- More specifically, for any neural network block in the selected candidate architecture that precedes the selected neural network block in the candidate architecture, the system can initialize the values of the parameters of the neural network block to the current values of the parameters for the block (within the selected candidate architecture) in the maintained data.
- For the selected neural network block and any neural network block in the candidate architecture that is after the selected neural network block in the candidate architecture, the system initializes the values of the parameters of the neural network block to newly initialized values.
- This technique is described in more detail below with reference to
FIG. 4 . - The system determines a performance measure for the trained neural network that measures the performance of the trained neural network on the particular machine learning task (step 212).
- In particular, the system can determine the performance of the trained neural network on the validation data set, i.e., the performance measure can be an appropriate measure that measures the performance of the trained neural network on the validation data set. Examples of performance measures that may be appropriate for different tasks include classification accuracy measures, intersection over union (IoU) measures for regression tasks, edit distance measures for text generation tasks, and so on.
- The system adds, to the maintained population data, data specifying the mutated architecture and data associating the mutated architecture with the determined performance measure (step 214). When transfer learning is being used, the system also adds, to the maintained data, the values of the parameters of the neural network blocks in the mutated architecture after the training of the neural network having the mutated architecture.
- Thus, by repeatedly performing iterations of the
process 200, the system can repeatedly update the population data to include candidate architectures with better performance measures. - After criteria for terminating performing iterations of the
process 200 have been satisfied, e.g., after a fixed number of iterations have been performed, after a fixed time has elapsed, after a termination input has been received from a user of the system, or after the performance measure for the highest-performing architecture in the data satisfies a threshold, the system determines a final architecture for the task neural network. - As one example, the system can select one of the candidate architectures in the set as the architecture for the task neural network based on the performance measures. As a particular example, the system can select the candidate architecture in the set having the best performance measure as the final architecture for the task neural network. As another particular example, the system can select a fixed number of candidate architectures from the set that have the best performance measures, further train a neural network having each of the selected architectures, determine an updated performance measure for each of the selected architectures based on the performance of the further trained neural networks on the validation data set, and select, as the final architecture for the task neural network, the candidate architecture having the best updated performance measure.
- As another example, the system can determine the architecture of the task neural network to be a weighted ensemble of a plurality of candidate architectures in the set, i.e., a weighted ensemble that includes a fixed number p of candidate architectures from the set where p is an integer greater than one. In other words, in this example, the architecture of the task neural network is an architecture that generates a final output for the neural network task as a weighted combination of the outputs generated by the plurality of candidate architectures in the ensemble. As a particular example, each architecture in the ensemble can be assigned the same weight, i.e., a weight equal to 1/p, in the combination. As another particular example, the weights assigned to each architecture in the combination can be learned.
- An example technique for generating a weighted ensemble is described in more detail below with reference to
FIG. 3 -
FIG. 3 is a flow diagram of anexample process 300 for selecting a weighted ensemble of candidate neural networks. For convenience, theprocess 300 will be described as being performed by a system of one or more computers located in one or more locations. For example, a neural architecture search system, e.g., the neuralarchitecture search system 100 ofFIG. 1 , appropriately programmed, can perform theprocess 300. - The system selects a plurality of highest-performing candidate architectures from the set of candidate architectures based on the performance measures (step 302). The system generally selects a number of architectures that is greater than the fixed numberp in the ensemble. For example, the system can select the P architectures in the set that have the best performance measures, where P is an integer greater than p.
- The system generates a plurality of candidate ensembles, with each candidate ensemble including a different combination ofp candidate architectures from the plurality of highest-performing candidate architectures (step 304). In some implementations, the system generates a respective ensemble for each possible different combination ofp candidate architectures from the plurality of highest-performing candidate architectures. In some other implementations, the system generates a fixed number of candidate ensembles, i.e., by repeatedly randomly sampling sets ofp candidate architectures from the plurality of highest-performing candidate architectures.
- The system then selects, as the determined architecture, the ensemble of the plurality of candidate ensembles that performs best on the particular machine learning task (step 306). In particular, the system can determine the performance of each candidate ensemble on the validation data set as described above, but with the performance of an ensemble being based on the weighted combinations of outputs generated by the candidate architectures in the ensemble.
- When the weights assigned to different architectures in the weighted combination are learned, the system can train each ensemble on all or part of the training data set in order to fine tune the weights (and, optionally, the parameters of the networks in the candidate ensemble) prior to determining the performance of each candidate ensemble on the validation data set.
-
FIG. 4 shows an example of using transfer learning when initializing the parameters of a new neural network for training during the architecture search. - In the example of
FIG. 4 , a mutated architecture B(b) has been generated from a selected candidate architecture B(a). - The selected candidate architecture includes an initial sequence of blocks ai through a6, followed by two fully-connected blocks afc and finally followed by a logits layer that generates a respective score for each of multiple categories (although only a1 through a3 are shown in the Figure). In some cases, the fully-connected blocks afc and the logits layer can be fixed for all the candidate architectures in the set, while the blocks in the initial sequence of blocks can be learned through the search process.
- The mutated architecture includes an initial sequence of blocks b1 through b6, followed by two fully-connected blocks bfc and finally followed by the logits layer.
- To generate the mutated architecture from the selected candidate architecture, the system replaced the block a3 in the selected candidate architecture with a new block b3. Thus, block a1 and a2 are the same as blocks b1 and b2, respectively, while block a3 is not the same as block b3. Therefore, when initializing the parameters of blocks b1 and b2, the system initializes the values of the parameters of block b1 to the current values of the parameters for the block a1 in the maintained data and initializes the values of the parameters initializes the values of the parameters of block b2 to the current values of the parameters for the block a2 in the maintained data. This transfer is illustrated by an arrow in
FIG. 4 . - Because block b3 does not match block a3, for block b3 and the blocks that are after block b3 in the mutated architecture, the system initializes the values of the parameters of the neural network block to newly initialized values, e.g., by setting the values to random values using a conventional random parameter initialization technique.
- By initializing the parameters using this transfer technique, the system shortens the training time and, accordingly, the amount of computational resources, required to train the new neural network while allowing the lower blocks learn features which can be extrapolated across architectures, improving the quality of the final determined architecture.
- This specification uses the term “configured” in connection with systems and computer program components. For a system of one or more computers to be configured to perform particular operations or actions means that the system has installed on it software, firmware, hardware, or a combination of them that in operation cause the system to perform the operations or actions. For one or more computer programs to be configured to perform particular operations or actions means that the one or more programs include instructions that, when executed by data processing apparatus, cause the apparatus to perform the operations or actions.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or in addition, the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- The term “data processing apparatus” refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can also be, or further include, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). The apparatus can optionally include, in addition to hardware, code that creates an execution environment for computer programs, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- A computer program, which may also be referred to or described as a program, software, a software application, an app, a module, a software module, a script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it can be deployed in any form, including as a stand alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
- In this specification, the term “database” is used broadly to refer to any collection of data: the data does not need to be structured in any particular way, or structured at all, and it can be stored on storage devices in one or more locations. Thus, for example, the index database can include multiple collections of data, each of which may be organized and accessed differently.
- Similarly, in this specification the term “engine” is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more specific functions. Generally, an engine will be implemented as one or more software modules or components, installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines can be installed and running on the same computer or computers.
- The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA or an ASIC, or by a combination of special purpose logic circuitry and one or more programmed computers.
- Computers suitable for the execution of a computer program can be based on general or special purpose microprocessors or both, or any other kind of central processing unit. Generally, a central processing unit will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.
- Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's device in response to requests received from the web browser. Also, a computer can interact with a user by sending text messages or other forms of message to a personal device, e.g., a smartphone that is running a messaging application, and receiving responsive messages from the user in return.
- Data processing apparatus for implementing machine learning models can also include, for example, special-purpose hardware accelerator units for processing common and compute-intensive parts of machine learning training or production, i.e., inference, workloads.
- Machine learning models can be implemented and deployed using a machine learning framework, e.g., a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework.
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (LAN) and a wide area network (WAN), e.g., the Internet.
- The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, a server transmits data, e.g., an HTML page, to a user device, e.g., for purposes of displaying data to and receiving user input from a user interacting with the device, which acts as a client. Data generated at the user device, e.g., a result of the user interaction, can be received at the server from the device.
- While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or on the scope of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially be claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
- Similarly, while operations are depicted in the drawings and recited in the claims in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
- Particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (20)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/933,690 US20210019599A1 (en) | 2019-07-19 | 2020-07-20 | Adaptive neural architecture search |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201962876548P | 2019-07-19 | 2019-07-19 | |
US16/933,690 US20210019599A1 (en) | 2019-07-19 | 2020-07-20 | Adaptive neural architecture search |
Publications (1)
Publication Number | Publication Date |
---|---|
US20210019599A1 true US20210019599A1 (en) | 2021-01-21 |
Family
ID=74344082
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/933,690 Pending US20210019599A1 (en) | 2019-07-19 | 2020-07-20 | Adaptive neural architecture search |
Country Status (1)
Country | Link |
---|---|
US (1) | US20210019599A1 (en) |
Cited By (12)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN112784140A (en) * | 2021-02-03 | 2021-05-11 | 浙江工业大学 | Search method of high-energy-efficiency neural network architecture |
US20210174504A1 (en) * | 2019-12-09 | 2021-06-10 | Case Western Reserve University | Specialized computer-aided diagnosis and disease characterization with a multi-focal ensemble of convolutional neural networks |
CN113128432A (en) * | 2021-04-25 | 2021-07-16 | 四川大学 | Multi-task neural network architecture searching method based on evolutionary computation |
CN113379068A (en) * | 2021-06-29 | 2021-09-10 | 哈尔滨工业大学 | Deep learning architecture searching method based on structured data |
US20230004807A1 (en) * | 2020-03-13 | 2023-01-05 | International Business Machines Corporation | Determining optimal augmentations for a training data set |
WO2022265573A3 (en) * | 2021-06-15 | 2023-01-12 | Lemon Inc. | Automatically and efficiently generating search spaces for neural network |
US11568201B2 (en) | 2019-12-31 | 2023-01-31 | X Development Llc | Predicting neuron types based on synaptic connectivity graphs |
US11593627B2 (en) | 2019-12-31 | 2023-02-28 | X Development Llc | Artificial neural network architectures based on synaptic connectivity graphs |
US11593617B2 (en) | 2019-12-31 | 2023-02-28 | X Development Llc | Reservoir computing neural networks based on synaptic connectivity graphs |
US11620487B2 (en) * | 2019-12-31 | 2023-04-04 | X Development Llc | Neural architecture search based on synaptic connectivity graphs |
US11625611B2 (en) | 2019-12-31 | 2023-04-11 | X Development Llc | Training artificial neural networks based on synaptic connectivity graphs |
US11631000B2 (en) | 2019-12-31 | 2023-04-18 | X Development Llc | Training artificial neural networks based on synaptic connectivity graphs |
Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20200257961A1 (en) * | 2017-11-30 | 2020-08-13 | Google Llc | Neural architecture search using a performance prediction neural network |
-
2020
- 2020-07-20 US US16/933,690 patent/US20210019599A1/en active Pending
Patent Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20200257961A1 (en) * | 2017-11-30 | 2020-08-13 | Google Llc | Neural architecture search using a performance prediction neural network |
Non-Patent Citations (9)
Title |
---|
Cortes, C., Gonzalvo, X., Kuznetsov, V., Mohri, M., & Yang, S.. (2016). AdaNet: Adaptive Structural Learning of Artificial Neural Networks. (Year: 2016) * |
Fang, J., Chen, Y., Zhang, X., Zhang, Q., Huang, C., Meng, G., Liu, W., & Wang, X.. (2019). EAT-NAS: Elastic Architecture Transfer for Accelerating Large-scale Neural Architecture Search. (Year: 2019) * |
James de St. Germain (2007) "Random Numbers" University of Utah https://users.cs.utah.edu/~germain/PPS/Topics/random_numbers.html (Year: 2007) * |
Kandasamy, K., Neiswanger, W., Schneider, J., Poczos, B., & Xing, E. (2018). Neural Architecture Search with Bayesian Optimisation and Optimal Transport. (Year: 2018) * |
Lapedes, A., & Farber, R. (1987). How Neural Nets Work. In Neural Information Processing Systems. American Institute of Physics. (Year: 1987) * |
Lu, Z., Whalen, I., Boddeti, V., Dhebar, Y., Deb, K., Goodman, E., & Banzhaf, W. (2019). NSGA-Net: Neural Architecture Search Using Multi-Objective Genetic Algorithm. In Proceedings of the Genetic and Evolutionary Computation Conference (pp. 419–427). Association for Computing Machinery. (Year: 2019) * |
M. M. Islam, Xin Yao and K. Murase, "A constructive algorithm for training cooperative neural network ensembles," in IEEE Transactions on Neural Networks, vol. 14, no. 4, pp. 820-834, July 2003, doi: 10.1109/TNN.2003.813832. (Year: 2003) * |
Martin Wistuba and Nicolas Schilling and Lars Schmidt-Thieme, " Automatic Frankensteining: Creating Complex Ensembles Autonomously" Proceedings of the 2017 SIAM International Conference on Data Mining (SDM) pg 741-749, doi: 10.1137/1.9781611974973.83 (Year: 2017) * |
Zoph, B., & Le, Q.. (2016). Neural Architecture Search with Reinforcement Learning. (Year: 2016) * |
Cited By (14)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11817204B2 (en) * | 2019-12-09 | 2023-11-14 | Case Western Reserve University | Specialized computer-aided diagnosis and disease characterization with a multi-focal ensemble of convolutional neural networks |
US20210174504A1 (en) * | 2019-12-09 | 2021-06-10 | Case Western Reserve University | Specialized computer-aided diagnosis and disease characterization with a multi-focal ensemble of convolutional neural networks |
US11620487B2 (en) * | 2019-12-31 | 2023-04-04 | X Development Llc | Neural architecture search based on synaptic connectivity graphs |
US11568201B2 (en) | 2019-12-31 | 2023-01-31 | X Development Llc | Predicting neuron types based on synaptic connectivity graphs |
US11593627B2 (en) | 2019-12-31 | 2023-02-28 | X Development Llc | Artificial neural network architectures based on synaptic connectivity graphs |
US11593617B2 (en) | 2019-12-31 | 2023-02-28 | X Development Llc | Reservoir computing neural networks based on synaptic connectivity graphs |
US11625611B2 (en) | 2019-12-31 | 2023-04-11 | X Development Llc | Training artificial neural networks based on synaptic connectivity graphs |
US11631000B2 (en) | 2019-12-31 | 2023-04-18 | X Development Llc | Training artificial neural networks based on synaptic connectivity graphs |
US20230004807A1 (en) * | 2020-03-13 | 2023-01-05 | International Business Machines Corporation | Determining optimal augmentations for a training data set |
US11790231B2 (en) * | 2020-03-13 | 2023-10-17 | International Business Machines Corporation | Determining optimal augmentations for a training data set |
CN112784140A (en) * | 2021-02-03 | 2021-05-11 | 浙江工业大学 | Search method of high-energy-efficiency neural network architecture |
CN113128432A (en) * | 2021-04-25 | 2021-07-16 | 四川大学 | Multi-task neural network architecture searching method based on evolutionary computation |
WO2022265573A3 (en) * | 2021-06-15 | 2023-01-12 | Lemon Inc. | Automatically and efficiently generating search spaces for neural network |
CN113379068A (en) * | 2021-06-29 | 2021-09-10 | 哈尔滨工业大学 | Deep learning architecture searching method based on structured data |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20210019599A1 (en) | Adaptive neural architecture search | |
US11669744B2 (en) | Regularized neural network architecture search | |
US11934956B2 (en) | Regularizing machine learning models | |
US11544536B2 (en) | Hybrid neural architecture search | |
US20190286984A1 (en) | Neural architecture search by proxy | |
US20220092416A1 (en) | Neural architecture search through a graph search space | |
US11803731B2 (en) | Neural architecture search with weight sharing | |
US11922281B2 (en) | Training machine learning models using teacher annealing | |
US20210034973A1 (en) | Training neural networks using learned adaptive learning rates | |
CN111652378B (en) | Learning to select vocabulary for category features | |
CN112101042A (en) | Text emotion recognition method and device, terminal device and storage medium | |
US20220383119A1 (en) | Granular neural network architecture search over low-level primitives | |
US20220188636A1 (en) | Meta pseudo-labels | |
US20220253680A1 (en) | Sparse and differentiable mixture of experts neural networks | |
US20230029590A1 (en) | Evaluating output sequences using an auto-regressive language model neural network | |
EP4322066A1 (en) | Method and apparatus for generating training data | |
US20220253694A1 (en) | Training neural networks with reinitialization | |
US20230121404A1 (en) | Searching for normalization-activation layer architectures | |
US20220383195A1 (en) | Machine learning algorithm search | |
EP4182850A1 (en) | Hardware-optimized neural architecture search | |
US11900222B1 (en) | Efficient machine learning model architecture selection | |
US20230206030A1 (en) | Hyperparameter neural network ensembles |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:MAZZAWI, HANNA;FRUCTUOSO, JAVIER GONZALVO;HOTAJ, EUGEN SURRI;SIGNING DATES FROM 20200807 TO 20200824;REEL/FRAME:053584/0847 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: FINAL REJECTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE AFTER FINAL ACTION FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: ADVISORY ACTION MAILED |