TW201907298A - Determination of per line buffer unit memory allocation - Google Patents
Determination of per line buffer unit memory allocation Download PDFInfo
- Publication number
- TW201907298A TW201907298A TW107103560A TW107103560A TW201907298A TW 201907298 A TW201907298 A TW 201907298A TW 107103560 A TW107103560 A TW 107103560A TW 107103560 A TW107103560 A TW 107103560A TW 201907298 A TW201907298 A TW 201907298A
- Authority
- TW
- Taiwan
- Prior art keywords
- line buffer
- kernel
- image data
- memory
- execution
- Prior art date
Links
- 239000000872 buffer Substances 0.000 title claims abstract description 232
- 230000015654 memory Effects 0.000 title claims abstract description 205
- 238000004088 simulation Methods 0.000 claims abstract description 84
- 238000000034 method Methods 0.000 claims abstract description 74
- 238000012545 processing Methods 0.000 claims abstract description 71
- 238000004891 communication Methods 0.000 claims abstract description 15
- 238000003860 storage Methods 0.000 claims description 55
- 238000004519 manufacturing process Methods 0.000 claims description 47
- 230000008569 process Effects 0.000 claims description 20
- 238000003491 array Methods 0.000 claims description 3
- 238000009826 distribution Methods 0.000 claims description 2
- 239000012634 fragment Substances 0.000 description 31
- 125000001475 halogen functional group Chemical group 0.000 description 18
- 230000006870 function Effects 0.000 description 15
- 230000005540 biological transmission Effects 0.000 description 10
- 238000013461 design Methods 0.000 description 8
- 230000006399 behavior Effects 0.000 description 7
- 230000000694 effects Effects 0.000 description 6
- 238000007726 management method Methods 0.000 description 5
- 238000012546 transfer Methods 0.000 description 5
- 238000005516 engineering process Methods 0.000 description 3
- 239000000463 material Substances 0.000 description 3
- 238000004458 analytical method Methods 0.000 description 2
- 230000005574 cross-species transmission Effects 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 230000000644 propagated effect Effects 0.000 description 2
- 230000004044 response Effects 0.000 description 2
- 239000004065 semiconductor Substances 0.000 description 2
- 238000012935 Averaging Methods 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 238000012512 characterization method Methods 0.000 description 1
- 238000004590 computer program Methods 0.000 description 1
- 230000001186 cumulative effect Effects 0.000 description 1
- 238000013500 data storage Methods 0.000 description 1
- 239000000284 extract Substances 0.000 description 1
- 230000008570 general process Effects 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 238000010801 machine learning Methods 0.000 description 1
- 238000007620 mathematical function Methods 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000002093 peripheral effect Effects 0.000 description 1
- 230000001902 propagating effect Effects 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T1/00—General purpose image data processing
- G06T1/60—Memory management
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0602—Interfaces specially adapted for storage systems specifically adapted to achieve a particular effect
- G06F3/0604—Improving or facilitating administration, e.g. storage management
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0628—Interfaces specially adapted for storage systems making use of a particular technique
- G06F3/0629—Configuration or reconfiguration of storage systems
- G06F3/0631—Configuration or reconfiguration of storage systems by allocating resources to storage systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0628—Interfaces specially adapted for storage systems making use of a particular technique
- G06F3/0655—Vertical data movement, i.e. input-output transfer; data movement between one or more hosts and one or more storage devices
- G06F3/0659—Command handling arrangements, e.g. command buffers, queues, command scheduling
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0628—Interfaces specially adapted for storage systems making use of a particular technique
- G06F3/0662—Virtualisation aspects
- G06F3/0664—Virtualisation aspects at device level, e.g. emulation of a storage device or system
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/06—Digital input from, or digital output to, record carriers, e.g. RAID, emulated record carriers or networked record carriers
- G06F3/0601—Interfaces specially adapted for storage systems
- G06F3/0668—Interfaces specially adapted for storage systems adopting a particular infrastructure
- G06F3/0671—In-line storage system
- G06F3/0673—Single storage device
- G06F3/068—Hybrid storage device
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F30/00—Computer-aided design [CAD]
- G06F30/30—Circuit design
- G06F30/32—Circuit design at the digital level
- G06F30/33—Design verification, e.g. functional simulation or model checking
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F30/00—Computer-aided design [CAD]
- G06F30/30—Circuit design
- G06F30/32—Circuit design at the digital level
- G06F30/33—Design verification, e.g. functional simulation or model checking
- G06F30/3308—Design verification, e.g. functional simulation or model checking using simulation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/50—Allocation of resources, e.g. of the central processing unit [CPU]
- G06F9/5005—Allocation of resources, e.g. of the central processing unit [CPU] to service a request
- G06F9/5011—Allocation of resources, e.g. of the central processing unit [CPU] to service a request the resources being hardware resources other than CPUs, Servers and Terminals
- G06F9/5016—Allocation of resources, e.g. of the central processing unit [CPU] to service a request the resources being hardware resources other than CPUs, Servers and Terminals the resource being the memory
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T1/00—General purpose image data processing
- G06T1/20—Processor architectures; Processor configuration, e.g. pipelining
-
- G—PHYSICS
- G09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS
- G09G—ARRANGEMENTS OR CIRCUITS FOR CONTROL OF INDICATING DEVICES USING STATIC MEANS TO PRESENT VARIABLE INFORMATION
- G09G5/00—Control arrangements or circuits for visual indicators common to cathode-ray tube indicators and other visual indicators
- G09G5/36—Control arrangements or circuits for visual indicators common to cathode-ray tube indicators and other visual indicators characterised by the display of a graphic pattern, e.g. using an all-points-addressable [APA] memory
- G09G5/363—Graphics controllers
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/0802—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches
- G06F12/0806—Multiuser, multiprocessor or multiprocessing cache systems
- G06F12/084—Multiuser, multiprocessor or multiprocessing cache systems with a shared cache
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/0802—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches
- G06F12/0806—Multiuser, multiprocessor or multiprocessing cache systems
- G06F12/0842—Multiuser, multiprocessor or multiprocessing cache systems for multiprocessing or multitasking
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2117/00—Details relating to the type or aim of the circuit design
- G06F2117/08—HW-SW co-design, e.g. HW-SW partitioning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/45—Caching of specific data in cache memory
- G06F2212/455—Image or video data
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/60—Details of cache memory
- G06F2212/601—Reconfiguration of cache memory
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F30/00—Computer-aided design [CAD]
- G06F30/20—Design optimisation, verification or simulation
-
- Y—GENERAL TAGGING OF NEW TECHNOLOGICAL DEVELOPMENTS; GENERAL TAGGING OF CROSS-SECTIONAL TECHNOLOGIES SPANNING OVER SEVERAL SECTIONS OF THE IPC; TECHNICAL SUBJECTS COVERED BY FORMER USPC CROSS-REFERENCE ART COLLECTIONS [XRACs] AND DIGESTS
- Y02—TECHNOLOGIES OR APPLICATIONS FOR MITIGATION OR ADAPTATION AGAINST CLIMATE CHANGE
- Y02D—CLIMATE CHANGE MITIGATION TECHNOLOGIES IN INFORMATION AND COMMUNICATION TECHNOLOGIES [ICT], I.E. INFORMATION AND COMMUNICATION TECHNOLOGIES AIMING AT THE REDUCTION OF THEIR OWN ENERGY USE
- Y02D10/00—Energy efficient computing, e.g. low power processors, power management or thermal management
Abstract
Description
本發明領域大體上係關於運算科學且更明確言之係關於按行緩衝器單元記憶體分配之判定。The field of the invention is generally related to computational science and more specifically to the determination of memory allocation by line buffer unit.
影像處理通常涉及處理組織成一陣列之像素值。此處，一經空間組織之二維陣列擷取影像之二維性質(額外維度可包含時間(例如，二維影像之一序列)及資料類型(例如，色彩))。在一典型案例中，陣列化之像素值係藉由已產生一靜態影像或用以擷取運動影像之一圖框序列之一相機而提供。傳統影像處理器通常落在兩種極端情況之任一側上。 第一種極端情況將影像處理任務作為在通用處理器或通用類處理器(例如，具有向量指令增強之一通用處理器)上執行之軟體程式執行。儘管該第一種極端情況通常提供一高度多樣化應用軟體開發平台，然其使用與相關聯額外耗用(例如，指令提取及解碼、晶片上及晶片外資料的處置、推測執行)組合之較精細粒度資料結構最終導致在程式碼之執行期間每資料單元消耗更大量能量。 第二種相對極端情況對大得多的資料單元應用固定功能硬連線電路。使用直接應用於客製電路之(如與較精細粒度相反)更大資料單元很大程度上減少每資料單元之電力消耗。然而，使用客製固定功能電路通常導致處理器能夠執行的一組有限任務。因而，在該第二種極端情況中缺乏廣泛多樣化的程式設計環境(其與第一種極端情況相關聯)。 提供高度多樣化的應用軟體開發機會結合每資料單元之經改良電力效率兩者之一技術平台仍為一期望但缺失之解決方案。Image processing usually involves processing pixel values organized into an array. Here, the two-dimensional nature of an image captured by a two-dimensional array organized in space (extra dimensions may include time (eg, a sequence of two-dimensional images) and data type (eg, color)). In a typical case, the arrayed pixel values are provided by a camera that has generated a still image or a frame sequence used to capture moving images. Traditional image processors often fall on either side of two extreme cases. In the first extreme case, the image processing task is executed as a software program executed on a general-purpose processor or a general-purpose processor (for example, a general-purpose processor with vector instruction enhancement). Although this first extreme case typically provides a highly diverse application software development platform, its use compares with a combination of associated additional consumption (e.g., instruction fetch and decode, processing of on-chip and off-chip data, speculative execution) The fine-grained data structure ultimately results in a greater amount of energy being consumed per data unit during the execution of the code. The second relatively extreme case applies a fixed-function hard-wired circuit to a much larger data unit. The use of larger data units (as opposed to finer granularity) applied directly to custom circuits greatly reduces power consumption per data unit. However, the use of custom fixed-function circuits often results in a limited set of tasks that the processor can perform. Thus, a widely diversified programming environment (which is associated with the first extreme case) is lacking in this second extreme case. A technology platform that provides highly diversified application software development opportunities combined with improved power efficiency per data unit is still a desired but missing solution.
本發明描述一種方法。該方法包含模擬一影像處理應用軟體程式之執行。該模擬包含利用模擬之行緩衝器記憶體攔截內核至內核通信，該等模擬之行緩衝器記憶體儲存及轉發自生產內核之模型傳送至消耗內核之模型之若干影像資料行。該模擬進一步包含在一模擬運行時間內追蹤儲存於各自行緩衝器記憶體中之各自影像資料量。該方法亦包含自該等經追蹤之各自影像資料量判定用於對應硬體行緩衝器記憶體之各自硬體記憶體分配。該方法亦包含產生組態資訊以供一影像處理器執行該影像處理應用軟體程式。該組態資訊描述用於該影像處理器之該等硬體行緩衝器記憶體之該等硬體記憶體分配。The present invention describes a method. The method includes simulating execution of an image processing application software program. The simulation includes intercepting kernel-to-kernel communication using simulated bank buffer memory, which stores and forwards a number of image data lines transmitted from the model of the production kernel to the model of the consumed kernel. The simulation further includes tracking the amount of respective image data stored in the respective line buffer memory during a simulation run time. The method also includes determining, from the tracked respective amounts of image data, respective hardware memory allocations for corresponding hardware line buffer memories. The method also includes generating configuration information for an image processor to execute the image processing application software program. The configuration information describes the hardware memory allocations for the hardware line buffer memory of the image processor.
1.0 獨特影像處理器架構 如此項技術中已知，用於執行程式碼之基本電路結構包含一執行階段及暫存器空間。該執行階段含有用於執行指令之執行單元。用於待執行之一指令之輸入運算元係自該暫存器空間提供至該執行階段。由執行階段執行一指令而產生之結果係被回寫至暫存器空間。 在一傳統處理器上執行一軟體執行緒需要透過執行階段循序執行一系列指令。最常見的是，在自一單個輸入運算元集產生一單個結果之意義上，操作係「純量的」。然而，在「向量」處理器之情況中，藉由執行階段執行一指令將自輸入運算元之一向量產生一結果向量。 圖1展示包含耦合至一個二維移位暫存器陣列102之一執行道陣列101之一獨特影像處理器架構100的一高階視圖。此處，該執行道陣列中之各執行道可被視為含有執行藉由該處理器100支援之指令集所需之執行單元的一離散執行階段。在各項實施例中，各執行道接收一相同指令以在一相同機器循環中執行，使得該處理器作為一個二維單指令多資料(SIMD)處理器操作。 各執行道在二維移位暫存器陣列102內之一對應位置中具有其自身專屬暫存器空間。例如，邊角執行道103在邊角移位暫存器位置104中具有其自身專屬暫存器空間，邊角執行道105在邊角移位暫存器位置106中具有其自身專屬暫存器空間等。 此外，移位暫存器陣列102能夠使其內容移位，使得各執行道能夠自其自身暫存器空間直接對在一先前機器循環期間駐存於另一執行道之暫存器空間中之一值操作。例如，一+1水平移位引起各執行道之暫存器空間自其最左相鄰者之暫存器空間接收一值。由於使值在沿著一水平軸之左右兩個方向上移位及使值在沿著一垂直軸之上下兩個方向上移位之一能力，處理器能夠有效處理影像資料之模板。 此處，如此項技術中已知，一模板係用作一基本資料單元之影像表面區域之一切片。例如，用於一輸出影像中之一特定像素位置之一新值可被計算為該特定像素位置在其內居中之一輸入影像之一區域中之像素值的一平均值。例如，若該模板具有3像素×3像素之一尺寸，則該特定像素位置可對應於3×3像素陣列之中間像素且該平均值可在該3×3像素陣列內之全部九個像素內予以計算。 根據圖1之處理器100之各種操作實施例，執行道陣列101之各執行道負責計算一輸出影像中之一特定位置之一像素值。因此，繼續上文剛剛提及之3×3模板求平均值實例，在移位暫存器內之初始載入輸入像素資料及八個移位操作之一經協調移位序列之後，執行道陣列中之各執行道將計算其對應像素位置之平均值所需之全部九個像素值接收至其局部暫存器空間中。即，處理器能夠同時處理居中於(例如)相鄰輸出影像像素位置處之多個重疊模板。因為圖1之處理器架構尤其擅長處理影像模板，所以其亦可被稱為一模板處理器。 圖2展示用於具有多個模板處理器202_1至202_N之一影像處理器之一架構200的一實施例。如圖2中所觀察，該架構200包含透過一網路204 (例如，一晶片上網路(NOC)，包含一晶片上交換式網路、一晶片上環形網路或其他種類之網路)互連至複數個模板處理器單元202_1至202_N及對應截片產生器單元203_1至203_N之複數個行緩衝器單元201_1至201_M。在一項實施例中，任何行緩衝器單元201_1至201_M可透過該網路204連接至任何截片產生器單元203_1至203_N及對應模板處理器202_1至202_N。 程式碼經編譯及載入至一對應模板處理器202上以執行早期藉由一軟體開發者定義之影像處理器操作(例如，取決於設計及實施方案，亦可將程式碼載入至該模板處理器之相關聯截片產生器203上)。在至少一些例項中，可藉由將針對一第一管線階段之一第一內核程式載入至一第一模板處理器202_1中，將針對一第二管線階段之一第二內核程式載入至一第二模板處理器202_2中等，而實現一影像處理管線，其中該第一內核執行該管線之該第一階段之功能，該第二內核執行該管線之該第二階段之功能等，且設置額外控制流方法以將輸出影像資料自該管線之一個階段傳遞至該管線之下一階段。 在其他組態中，影像處理器可被實現為具有操作相同內核程式碼之兩個或兩個以上模板處理器202_1、202_2之一平行機。例如，可藉由跨各執行相同功能之多個模板處理器散佈圖框而處理影像資料之一高度緻密及高資料速率串流。 在又其他組態中，基本上內核之任何有向非循環圖(DAG)可藉由以下步驟載入至影像處理器上：在DAG設計中，用各自模板處理器其自身各自程式碼內核組態各自模板處理器；及將適當控制流攔截程式(hook)組態至硬體中以將來自一內核之輸出影像引導至下一內核之輸入。 作為一般流程，藉由一巨集I/O單元205接收影像資料之圖框且將該等圖框逐圖框地傳遞至行緩衝器單元201之一或多者。一特定行緩衝器單元將其影像資料圖框剖析成一較小影像資料區域(被稱為一「行群組」)，且接著透過網路204將該行群組傳遞至一特定截片產生器。一完整或「全」單行群組可(例如)用一圖框之多個連續完整列或行之資料組成(為簡潔起見本說明書將主要指連續列)。截片產生器將影像資料之行群組進一步剖析成一較小影像資料區域(被稱為一「截片(sheet)」)，且將該截片提呈給其對應模板處理器。 在具有一單個輸入之一影像處理管線或一DAG流程之情況中，一般而言，輸入圖框係引導至相同行緩衝器單元201_1，該相同行緩衝器單元201_1將影像資料剖析成行群組且將該等行群組引導至截片產生器203_1，該截片產生器203_1之對應模板處理器202_1執行該管線/DAG中之第一內核之程式碼。在完成該模板處理器202_1對其處理之行群組之操作之後，截片產生器203_1將輸出行群組發送至一「下游」行緩衝器單元201_2 (在一些使用情況中可將輸出行群組發送回至早期已發送輸入行群組之相同行緩衝器單元201_1)。 表示執行於其自身各自其他截片產生器及模板處理器(例如，截片產生器203_2及模板處理器202_2)上之管線/DAG中之下一階段/操作之一或多個「消耗者」內核接著自下游行緩衝器單元201_2接收藉由第一模板處理器202_1產生之影像資料。以此方式，操作於一第一模板處理器上之一「生產者」內核使其輸出資料轉發至操作於一第二模板處理器上之一「消耗者」內核，其中該消耗者內核依據整體管線或DAG之設計在該生產者內核後執行下一組任務。 如上文參考圖1所提及，各模板處理器202_1至202_N經設計以同時對影像資料之多個重疊模板處理。該多個重疊模板及模板處理器之內部硬體處理能力有效判定一截片之大小。又，如上文所論述，在模板處理器202_1至202_N之任一者內，執行道之若干陣列聯合操作以同時處理藉由該多個重疊模板覆蓋之影像資料表面區域。 此外，在各項實施例中，藉由一模板處理器202之對應(例如，局部)截片產生器203將影像資料之若干截片載入至該模板處理器之二維移位暫存器陣列中。據信截片及二維移位暫存器陣列結構的使用藉由將大量資料移動至大量暫存器空間中作為(例如)一單個載入操作(緊接在此之後藉由一執行道陣列對該資料直接執行處理任務)而有效提供電力消耗改良。此外，一執行道陣列及對應暫存器陣列的使用提供可易於程式化/組態之不同模板大小。關於行緩衝器單元、截片產生器及模板處理器之操作之更多細節係在下文章節3.0中進一步提供。2.0 按行緩衝器單元記憶體分配之判定 如可自上文論述理解，硬體平台能夠支援大量不同應用軟體程式結構。即，可支援實際無限數目個不同且複雜的內核至內核連接。 理解針對任何特定軟體應用程式應對各行緩衝器單元201_1至201_M分配多少記憶體空間具有挑戰性。此處，在一項實施例中，行緩衝器單元之不同者存取已(例如)自一實體上共用記憶體分配給其自身各自記憶體。因此，一行緩衝器單元可更一般地被特性化為一行緩衝器記憶體。在程式執行期間，一行緩衝器單元將其(例如)自一生產內核接收之資料暫存儲存至其各自記憶體中。在一消耗內核準備接收該資料之後，該行緩衝器單元自其各自記憶體讀取該資料且將該資料轉發至該消耗內核。 在行緩衝器單元之一或多者或所有者實體耦合至一相同共用記憶體資源之後，執行於影像處理器上之一應用軟體程式之組態因此包含定義應對共用該記憶體資源之各行緩衝器單元個別分配該共用記憶體資源之多少記憶體容量。清晰表述用於各行緩衝器單元之一可行記憶體分配可能非常難以判定，尤其對於具有複雜資料流及相關聯資料相依性之複雜應用軟體程式。 圖3展示一例示性略複雜應用軟體程式(或其部分)及其在影像處理器上之行緩衝器單元組態之一實例。在各項實施方案中，允許一生產內核產生用於不同消耗內核之分離、不同輸出影像串流。此外，亦允許一生產內核產生由兩個或兩個以上不同內核消耗之一單個輸出串流。最後，在各項實施例中，一行緩衝器單元可自唯一生產內核接收一輸入串流但能夠將該串流饋送至一或多個消耗內核。 圖3之應用軟體組態展現此等組態可能性之各者。此處，內核K1產生針對內核K2及K3之兩者之一第一資料串流且產生針對內核K4之一第二、不同資料串流。內核K1將該第一資料串流發送至行緩衝器單元304_1，該行緩衝器單元304_1將該資料轉發至內核K2及K3之兩者。內核K1亦將該第二資料串流發送至行緩衝器單元304_2，該行緩衝器單元304_2將該資料轉發至內核K4。此外，內核K2將一資料串流發送至內核K4且內核K3將一資料串流發送至內核K4。內核K2將其資料串流發送至行緩衝器單元304_3，該行緩衝器單元304_3將該資料轉發至內核K4。內核K3將其資料串流發送至行緩衝器單元304_4，該行緩衝器單元304_4將該資料轉發至內核K4。 此處，難以明確計算唯一分配給行緩衝器單元304_1至304_4之各者之記憶體量。將各此記憶體分配看作一佇列，若行緩衝器單元將隨時間流逝自一生產內核接收大量資料，則所需記憶體量趨於增加。相比而言，若行緩衝器單元將隨時間流逝自該生產內核接收少量資料，則所需記憶體量趨於減少。同樣地，若行緩衝器單元將隨時間流逝將少量資料發送至更多消耗內核，則所需記憶體量趨於增加，或若行緩衝器單元將隨時間流逝將大量資料發送至更少消耗內核，則所需記憶體量趨於減少。 行緩衝器單元將隨時間流逝自生產者內核接收之資料量可依據以下各項之任一者而變化：1)生產內核對其自身輸入資料具有之相依性；2)生產內核產生輸出資料之速率，不考慮上文1)之相依性/速率；及3)生產內核發送至行緩衝器單元之資料單元之大小。同樣地，行緩衝器單元將隨時間流逝發送之資料量可依據以下各項之任一者而變化：1)生產內核饋送之消耗內核之數目；2) 1)之該等消耗內核之各者準備接收新資料之各自速率(其可依據該等消耗內核具有之其他資料相依性而變化)；及3)該(等)消耗內核自行緩衝器單元接收之資料單元之大小。 因為至少對於略複雜應用軟體程式結構，各種相互相依性及連接速率之複雜性質使得明確計算待分配給各行緩衝器單元之正確記憶體空間量變得非常困難，所以在各項實施例中，採取一啟發式方法，該啟發式方法在一模擬環境中模擬運行時間之前應用軟體程式之執行且監測由該模擬程式之內部資料流所引起之各行緩衝器單元處之成佇列資料量。 圖4描繪用於設置模擬環境之針對圖3之應用軟體程式之一預備程序。在一項實施例中，藉由使各內核剝除其載入指令及其儲存指令而建立針對各內核之一模擬模型。一內核之載入指令對應於該內核消耗來自一行緩衝器單元之輸入資料且一內核之儲存指令對應於該內核產生輸出資料以寫入至一行緩衝器單元中。如上文所論述，一內核可經組態以自(例如)多個不同內核/行緩衝器單元接收多個不同輸入串流。因而，實際內核及其模擬模型內核可包含多個載入指令(每個針對各不同輸入串流)。又如上文所論述，一內核(且因此一模擬模型內核)可經組態以對不同內核饋送不同產生串流。因而，一實際內核及其模擬模型內核可包含多個儲存指令。 參考圖4，模擬模型內核K1展示一載入指令(LD_1)及兩個儲存指令(ST_1及ST_2)，此與展示內核K1接收一輸入串流(至影像處理器之輸入資料)及提供兩個輸出串流(一個至行緩衝器單元304_1且另一個至行緩衝器單元304_2)之圖3中之內核K1之描繪一致。圖4亦展示針對模擬模型內核K2之一載入指令及一儲存指令，此與展示內核K2自行緩衝器單元304_1接收一輸入串流且產生至行緩衝器單元304_3之一輸出串流之圖3中之內核K2之描繪一致。圖4亦展示針對模擬模型內核K3之一載入指令及一儲存指令，此與展示內核K3自行緩衝器單元304_1接收一輸入串流且產生至行緩衝器單元304_4之一輸出串流之圖3中之內核K3之描繪一致。最後，圖4展示模擬模型內核K4具有三個載入指令及一儲存指令，此與展示內核K3自行緩衝器單元304_2接收一第一輸入串流，自行緩衝器單元304_3接收一第二輸入串流及自行緩衝器單元304_4接收一第三輸入串流之圖3中之內核K4之描繪一致。內核K4亦在圖3中展示為產生一輸出串流。 如藉由圖4之迴路401_1至401_4所指示，模擬模型內核(如實際內核)重複成迴路。即，在執行開始時一內核執行其(若干)載入指令以接收其輸入資料，在執行結束時一內核執行其儲存指令以自其自其載入指令接收之該輸入資料產生其輸出資料。接著程序重複。在各項實施例中，各模擬模型內核亦可含有指示該內核對輸入資料執行操作以產生其輸出資料時消耗之時間量(其傳播延遲)之一值。即，該模擬模型內核並不允許其儲存指令執行直至已執行其載入指令之後的一些數目個循環為止。此外，在各項實施例中，為減少執行模擬所消耗之時間，內核模型被剝除其實際影像處理常式。即，模擬未執行任何實際影像處理，僅模擬「虛擬」資料之資料傳送。 在已建構模擬模型內核之後，依據整體應用軟體程式之設計/架構使其透過行緩衝器單元之各自模擬模型彼此連接。基本上，繼續使用圖3之應用軟體程式作為一實例，在一模擬環境中建構應用軟體程式300之一模擬模型，其中該模擬模型含有依據圖3中所描繪之架構，透過行緩衝器單元304_1至304_4之各自模擬模型互連之圖4之內核K1至K4之模擬模型。 為研究各行緩衝器單元處之記憶體需求，將一模擬輸入影像資料串流(例如，圖3之輸入影像資料301之一模擬)提呈給應用程式之模擬模型。該應用軟體程式之該模擬模型接著執行，其中模擬模型內核透過其(若干)載入指令之執行重複消耗模擬量之輸入資料，藉由其(若干)儲存指令自該經接收之輸入資料產生模擬量之輸出資料且重複。 此處，各模擬載入指令可併入有或以其他方式基於存在於原始源內核中之一些輸入影像資料格式化(諸如一輸入行群組中之行數目、最大輸入行群組速率、輸入區塊之尺寸/大小、最大輸入區塊速率等)以判定經消耗之輸入資料之模擬量及速率。同樣地，各儲存指令可指定或以其他方式基於存在於原始源內核中之一些輸出影像格式化(諸如一輸出行群組中之行數目、最大輸出行群組速率、輸出區塊之尺寸/大小、最大輸出區塊速率等)以判定經產生之輸出資料之量及速率。在一項實施例中，內核模型之載入/儲存指令及行緩衝器單元模型對其處置反映應用軟體及基礎硬體平台之實際交握，因為(例如)經產生之影像資料之一特定下一部分係藉由一生產模型內核之儲存指令識別且所請求之影像資料之一特定下一部分係藉由消耗模型內核之載入指令識別。 各行緩衝器單元模型自其各自生產模型內核接收其各自模擬輸入串流且將其儲存至具有(例如)無限容量之一模擬記憶體資源中。又，每資料交換(transaction)傳送之資料量係與生產模型內核之原始源內核之量一致。在藉由行緩衝器單元模型接收之影像串流之(若干)消耗內核執行其各自載入指令時，其依據與其原始源內核之每資料交換量請求來自該行緩衝器單元模型之輸入影像串流之下一量。作為回應，行緩衝器單元模型提供來自其記憶體資源之下一、所請求之資料單元。 在應用軟體程式之模型在模擬環境中執行時，各行緩衝器單元模型之各自記憶體狀態將隨著回應於其(若干)消耗內核之載入指令請求自其讀取之活動及回應於其消耗內核之儲存指令請求寫入至其活動而起伏。為最後判定各行緩衝器單元之記憶體容量需求，如圖5a及圖5b中所展示，各行緩衝器單元模擬模型包含一寫入指標及讀取指標。該寫入指標指定目前為止來自一生產內核模型之多少輸入影像資料已寫入至行緩衝器單元模型之記憶體中。該讀取指標指定目前為止已自該行緩衝器單元模型之記憶體讀取多少經寫入輸入影像資料以服務來自該行緩衝器單元模型之(若干)消耗內核模型之載入指令請求。 圖5a之描繪指示一特定消耗內核每載入指令請求請求X量之影像資料(X可對應於(例如)特定數目個影像行、對應於一區塊大小等)。即，在消耗內核模型已被發送引導至讀取指標之資料量的情況下，行緩衝器單元將無法服務來自消耗內核模型之下一載入指令請求直至寫入至記憶體中之資料量達到對應於讀取指標+X之一量為止(即，直至寫入指標指向等於讀取指標+X之一值)。如圖5a中明確描繪，寫入指標尚未達到此位準。因而，若消耗內核已請求下一量(高達讀取指標+X)，則該消耗內核當前停止以等待來自生產內核之更多輸出資料寫入至記憶體中。若消耗內核尚未請求下一量，則其在技術上尚未停止，且因此仍有時間供生產內核至少提供等於((讀取指標+X)—寫入指標)之一量使得其可在消耗內核請求其之前被寫入至記憶體中。此特定事件係在圖5b中描繪。 一行緩衝器單元所需之記憶體容量之最大量係在應用軟體程式之足夠長的模擬運行時間執行期間讀取指標與寫入指標之間的最大所觀察差異。因此，針對各行緩衝器單元之記憶體容量之判定需要模擬程式之執行達足夠數目個循環，同時連續追蹤寫入指標與讀取指標之間的差異且記錄各新的最大所觀察差異。在完成足夠數目個執行循環之後，針對各行緩衝器單元模型之其餘經記錄之最大所觀察差異(其對應於在整個模擬期間觀察之最大差異)對應於各行緩衝器單元所需之記憶體容量。 在各項實施例中，為避免其中一生產者依比其(若干)消耗者可消耗輸出資料快很多之一速率產生輸出資料，藉此引起行緩衝器單元連續寫入至其記憶體且無限制地使用其無限容量之一不現實條件，各內核模型亦包含在其儲存指令之各者下強制執行之一寫入政策。 即，該寫入政策用作對用生產內核模型之輸出資料寫入至之行緩衝器單元記憶體之量的檢查。明確言之，在一項實施例中，未執行一生產內核之儲存指令直至停止(亦被稱為「準備」)所有對應消耗內核為止。即，僅在針對消耗內核之各者之讀取指標+X大於針對一生產內核之影像串流之寫入指標時允許執行該生產內核之載入指令。 在此狀態中，停止消耗內核之各者(其無法執行針對生產內核之影像串流之其下一單元之其各自載入指令，此係因為尚未藉由生產內核產生資料且將其寫入至行緩衝器單元記憶體中)。因而，模擬環境之特徵在於生產者無法執行針對引導至一特定行緩衝器單元之一特定輸出串流之一儲存指令，直至消耗來自該行緩衝器單元之輸出串流之內核之各者在其各自載入指令下停止，該載入指令將自該行緩衝器單元載入該串流之下一資料單元。又，儘管此可能係實際系統之非典型運行時間行為，但其大致對行緩衝器單元處所需之記憶體量設定一上界(如使用實施中之寫入政策藉由最大所觀察寫入指標對讀取指標差異判定)。 若(例如)對各行緩衝器單元實際分配之記憶體量相同於(或略微大於)自最大所觀察寫入指標對讀取指標差異所判定之量，則實際系統可能永遠不會經歷任何消耗者停止，此係因為生產者通常隨意執行儲存指令直至行緩衝器單元記憶體變滿為止(此時，實際系統中之行緩衝器單元將不會允許生產者發送任何更多資料)。然而，因為在模擬期間不允許各生產者執行其儲存指令直至停止所有其消耗者為止，所以如透過模擬判定之記憶體分配針對一實際系統轉化為一生產者大約不遲於其消耗者將停止之時產生新資料供消耗。因而，一般而言，消耗者不應在一真實系統中停止。以此方式，模擬結果基本上判定各行緩衝器單元處所需之最小記憶體容量。 理想上，在足夠數目個模擬運行時間循環之後，可判定待分配至各行緩衝器單元之記憶體量。然而，在各種模擬運行時間體驗中，模擬系統可達到其中資料無法在該系統中之任何地方流動之總體僵局(deadlock)。即，因為尚未產生資料，所以該系統中之所有內核無法執行下一載入指令，且所有生產者無法寫入下一資料量(例如，因為其自身載入指令已停止且生產內核無新輸入以自其產生輸出資料)。 若系統達到如上所描述之總體僵局，則分析系統狀態且找到一僵局循環。一僵局循環係應用程式之資料流內之一閉合迴路，其包含一特定經停止載入等待一特定儲存執行，但該特定儲存無法執行，此係因為其在等待執行該經停止載入(應注意，該經停止載入及該經停止儲存並不一定與彼此直接通信之內核相關聯)。 例如，在圖3之軟體程式之模擬模型中，自行緩衝器單元304_4讀取資料之K4內核之模型之載入指令可等待由內核K3產生資料。此特定載入之停止基本上停止全部內核K4，此因此防止自行緩衝器304_2讀取之K4之載入指令執行。若行緩衝器304_2之狀態使得寫入指標領先於讀取指標+X (例如，因為K1在行緩衝器304_2中寫入大量資料單元)，則寫入至行緩衝器304_2中之K1之儲存指令將停止，此停止全部K1，包含寫入至行緩衝器304_1中之儲存指令。 因為未寫入至行緩衝器304_1，所以停止K3，此完成僵局循環之識別分析。即，僵局循環運行為：1)自K1通過行緩衝器單元304_1至內核K3；2)自內核K3通過行緩衝器單元304_4至內核K4；及3)自內核K4通過行緩衝器304_2返回至內核K1。在存在此特定僵局循環之情況下，K2亦將停止，從而導致整個系統之總體僵局(且此亦產生系統內之更多僵局循環)。在一項實施例中，一旦已識別一僵局循環，即允許沿著該循環之一經停止儲存指令向前推進一個資料單元，希望該推進將「啟動」系統返回操作。例如，若將寫入至行緩衝器單元304_1中之內核K1之儲存指令向前推進一個資料單元，則引起內核K3之經停止載入指令之執行可能係足夠的，此繼而可引起系統再次開始操作。 在一項實施例中，僅允許沿著僵局循環之一經停止儲存指令向前推進一個單元。若該推進並未引起系統再次開始操作，則選取沿著僵局循環之另一儲存指令用於推進。每次選擇一個儲存指令用於推進之程序繼續進行直至在允許沿著僵局循環之所有儲存指令向前推進一個資料單元之後系統開始操作或保持於總體僵局中為止。若達到後者條件(系統保持於總體僵局中)，則選取沿著僵局循環之寫入器之一者且允許其自由寫入以希望系統將再次開始操作。若系統並未開始操作，則選取沿著僵局循環之另一儲存指令且允許其自由寫入等。最後，系統應開始操作。 在各項實施例中，生產/消耗內核模型可根據不同傳輸模式將影像資料發送至其各自行緩衝器單元模型或自其各自行緩衝器單元模型讀取影像資料。根據一第一模式(被稱為「全行群組」)，在一內核模型與一行緩衝器單元模型之間傳輸若干相同寬度影像資料行。 圖6a及圖6b描繪全行群組模式操作之一實施例。如圖6a中所觀察，影像區域600對應於影像資料之一全圖框或影像資料之一全圖框之一區段(讀者將理解所描繪之矩陣展示整個影像之不同像素位置)。如圖6a中所描繪，在一內核模型與行緩衝器單元模型之間發送之影像資料之第一傳送(例如，一第一封包)含有一第一群組之相同寬度影像行601，該等影像行601跨經傳送之圖框完全延伸至經傳送之一圖框之區段600。接著，如圖6b中所描繪，一第二傳送含有跨圖框或其區段600完全延伸之一第二群組之相同寬度影像行602。 此處，圖6a之群組601之傳送將使行緩衝器單元模型之寫入指標及/或讀取指標向前推進一個單元。同樣地，圖6b之群組602之傳送將使行緩衝器單元模型之寫入指標及/或讀取指標向前推進另一單元。因而，上文參考圖5a及圖5b描述之寫入指標及讀取指標行為係與全行群組模式一致。 另一傳送模式(被稱為「實際上高」)可用於傳送影像資料區塊(影像資料之二維表面區域)。此處，如上文參考圖1所論述且如下文更詳細說明，在各項實施例中，整個影像處理器之一或多個處理核心各包含一個二維執行道陣列及一個二維移位暫存器陣列。因而，一處理核心之暫存器空間係載入有影像資料之全部區塊(而非僅純量或單個向量值)。 依據藉由處理核心處理之資料單元之二維性質，實際上高模式能夠傳送影像資料區塊，如圖6c及圖6d中所描繪。參考圖6c，最初一較小高度全寬行群組611係(例如)自一第一生產內核模型傳送至一行緩衝器單元模型。自此開始，至少對於影像區域600，影像資料係依較小寬度行群組612_1、612_2等自生產內核模型傳送至行緩衝器單元模型。 此處，較小寬度行群組612_1係在(例如)一第二生產內核模型中傳送至行緩衝器單元模型資料交換。接著，如圖6d中所觀察，下一較小寬度行群組612_2係在(例如)一第三生產內核模型中傳送至行緩衝器單元模型資料交換中。因而，行緩衝器單元模型寫入指標最初遞增達一大值(以表示全行群組611之傳送)但接著遞增達較小值(例如，一第一較小值，以表示較小寬度行群組612_1之傳送，且接著增加下一較小值以表示較小寬度行群組612_2之傳送)。 如上文所描述，圖6c及圖6d展示將藉由一生產內核模型發送之內容寫入至行緩衝器單元模型記憶體中。消耗內核模型可經組態以亦接收如上所描述之影像資料(在此情況中讀取指標行為係與上文剛描述之寫入指標行為相同)，或在行緩衝器記憶體中形成影像資料區塊時接收該等影像資料區塊。 即，相對於後者，消耗內核模型最初並未被發送第一全行群組611。而是在將第一較小行寬行群組612_1寫入至行緩衝器記憶體中之後，消耗內核模型最初被發送對應於一第一5×5像素值陣列(其底部邊緣係藉由較小寬度行群組612_1之底部邊緣勾勒)之一資料量。接著，在將第二較小行寬行群組612_2寫入至行緩衝器記憶體中之後，消耗模型被發送一第二5×5像素值陣列(其底部邊緣係藉由較小行寬行群組612_2勾勒)。在如上所述之至消耗內核模型之區塊傳送之情況中，如圖6e中所描繪，待傳送之下一量包含更近期已寫入至行緩衝器記憶體中之一較小資料片段及一定時間之前寫入至行緩衝器記憶體中之一較大資料片段。 圖7展示上文所描述之用於判定按行緩衝器單元記憶體分配之一方法。該方法包含模擬一影像處理應用軟體程式之執行701。該模擬包含攔截702與儲存及轉發自生產內核之模型傳送至消耗內核之模型之若干影像資料行之行緩衝器記憶體之模型的內核至內核通信。該模擬進一步包含在一模擬運行時間內追蹤703儲存於各自模擬行緩衝器記憶體中之各自影像資料量。該方法亦包含自該等經追蹤之各自影像資料量判定704用於對應硬體行緩衝器記憶體之各自硬體記憶體分配。 可至少部分藉由鑑於彼此按比例調整模擬行緩衝器記憶體而實施自模擬行緩衝器記憶體儲存狀態之經追蹤觀察判定硬體記憶體分配。例如，若一第一模擬行緩衝器記憶體展現兩倍於一第二模擬行緩衝器記憶體之一最大寫入對讀取指標差異，則用於該第一硬體行緩衝器單元之對應實際硬體記憶體分配將為用於該第二硬體行緩衝器單元之對應實際硬體記憶體分配之約兩倍。將相應按比例調整其餘分配。 在已針對應用軟體程式判定記憶體分配之後，該應用軟體程式可用組態資訊進行組態以運行於目標影像處理器上，其中該組態資訊通知該影像處理器之硬體根據由模擬作出之判定將多少行緩衝器單元記憶體空間分配給各自硬體行緩衝器單元。該組態資訊亦可包含(例如)指派內核執行於影像處理器之特定模板處理器上且產生特定硬體行緩衝器單元及自特定硬體行緩衝器單元消耗。針對應用程式產生之組態資訊之主體(corpus)可接著(例如)載入至影像處理器之組態暫存器空間及/或組態記憶體資源中以「設置」影像處理器硬體以執行該應用程式。 在各項實施例中，前述行緩衝器單元可更一般被特性化為在生產內核與消耗內核之間儲存及轉發影像資料之緩衝器。即，在各項實施例中，一緩衝器並不一定需要使行群組成佇列。此外，影像處理器之硬體平台可包含具有相關聯記憶體資源之複數個行緩衝器單元且一或多個行緩衝器可經組態以自一單個行緩衝器單元操作。即，硬體中之一單個行緩衝器單元可經組態以在不同生產/消耗內核對之間儲存及轉發不同影像資料流。 在各項實施例中，可在模擬期間模擬實際內核而非模擬其模型。又進一步，在模擬期間在內核與行緩衝器單元之間傳輸之影像資料可為影像資料之表示(例如，若干行，其中各行應被理解為對應於一特定資料大小)。為簡潔起見，術語影像資料應被理解為應用於實際影像資料或影像資料之一表示。3.0 影像處理器實施方案實施例 圖8a至圖8e至圖12提供關於上文詳細描述之影像處理器及相關聯模板處理器之各項實施例之操作及設計的額外細節。回顧圖2之一行緩衝器單元饋送行群組至一模板處理器之相關聯截片產生器之論述，圖8a至圖8e以一行緩衝器單元201之剖析活動、一截片產生器單元203之較精細粒度剖析活動兩者以及耦合至該截片產生器單元203之模板處理器702之模板處理活動之一高階實施例進行繪示。 圖8a描繪影像資料801之一輸入圖框之一實施例。圖8a亦描繪一模板處理器經設計以對其操作之三個重疊模板802 (各具有3像素×3像素之一尺寸)之一輪廓。各模板分別針為其產生輸出影像資料之輸出像素係以純黑色突顯。為簡潔起見，該三個重疊模板802係描繪為僅在垂直方向上重疊。應該認識到，實際上一模板處理器可經設計以在垂直及水平兩個方向上具有重疊模板。 由於模板處理器內之垂直重疊模板802，如圖8a中所觀察，圖框內存在一單個模板處理器可對其操作之大量影像資料。如下文將更詳細論述，在一項實施例中，模板處理器以跨影像資料從左至右之一方式處理其重疊模板內之資料(且接著對於下一組行以頂部至底部順序重複)。因此，在模板處理器持續進行其操作時，純黑色輸出像素區塊之數目將水平地向右增長。如上文所論述，一行緩衝器單元201係負責剖析足以使模板處理器在擴展數目個即將來臨的循環內對其操作之來自一傳入圖框之輸入影像資料之一行群組。一行群組之一例示性描繪係繪示為一陰影區域803。在一項實施例中，行緩衝器單元201可理解用於發送一行群組至一截片產生器或自一截片產生器接收一行群組之不同動態。例如，根據一模式(被稱為「全群組」)，在一行緩衝器單元與一截片產生器之間傳遞完整全寬影像資料行。根據一第二模式(被稱為「實際上高」)，最初以全寬列之一子集傳遞一行群組。接著以較小(小於全寬)片段循序傳遞其餘列。 在輸入影像資料之行群組803已藉由行緩衝器單元定義且傳遞至截片產生器單元之情況下，該截片產生器單元將該行群組進一步剖析成更精確匹配模板處理器之硬體限制之更精細截片。更明確言之，如下文將更進一步詳細描述，在一項實施例中，各模板處理器由一個二維移位暫存器陣列組成。該二維移位暫存器陣列基本上使影像資料在一執行道陣列「下方」移位，其中該移位之型樣引起各執行道對其自身各自模板內之資料操作(即，各執行道對其自身資訊模板處理以產生用於該模板之一輸出)。在一項實施例中，截片係輸入影像資料之「填充」或以其他方式載入至二維移位暫存器陣列中之表面區域。 如下文將更詳細描述，在各項實施例中，實際上存在可在任何循環內移位之多層二維暫存器資料。為方便起見，本發明描述的大部分將僅使用術語「二維移位暫存器」及類似者來指代具有可移位之一或多個此等層之二維暫存器資料之結構。 因此，如圖8b中所觀察，截片產生器剖析來自行群組803之一初始截片804且將其提供至模板處理器(此處，該資料截片對應於一般藉由元件符號804識別之陰影區域)。如圖8c及圖8d中所觀察，模板處理器藉由在截片上方以一從左至右方式有效移動重疊模板802而對輸入影像資料之截片操作。截至圖8d，耗盡可針對其自截片內之資料計算一輸出值之像素之數目(其他像素位置可能無自截片內之資訊判定之一輸出值)。為簡潔起見，已忽略影像之邊界區域。 如圖8e中所觀察，截片產生器接著對模板處理器提供下一截片805用以繼續操作。應注意，模板在其開始對該下一截片操作時之初始位置係自第一截片上之耗盡點(如先前圖8d中所描繪)向右之下一進程。關於新截片805，模板將簡單地在模板處理器以與第一截片之處理相同之方式對該新截片操作時持續向右移動。 應注意，歸因於模板之圍繞一輸出像素位置之邊界區域，在第一截片804之資料與第二截片805之資料之間存在一些重疊。可簡單藉由截片產生器兩次重新傳輸該重疊資料而處置該重疊。在交替實施方案中，為饋送下一截片至模板處理器，截片產生器可繼續進行以僅將新資料發送至該模板處理器且該模板處理器重新使用來自先前截片之重疊資料。 圖9展示一模板處理器架構900之一實施例。如在圖9中觀察，模板處理器包含一資料運算單元901、一純量處理器902及相關聯記憶體903以及一I/O單元904。資料運算單元901包含一執行道陣列905、二維移位陣列結構906及與陣列之特定列或行相關聯之分開隨機存取記憶體907_1至907_R。 I/O單元904負責將自截片產生器接收之「輸入」資料截片載入至資料運算單元901中且將來自模板處理器之「輸出」資料截片儲存至截片產生器中。在一實施例中，將截片資料載入至資料運算單元901中需要將一所接收截片剖析成影像資料之列/行且將影像資料之列/行載入至二維移位暫存器結構906或執行道陣列之列/行之各自隨機存取記憶體907中(在下文更詳細描述)。若首先將截片載入至記憶體907中，則執行道陣列905內之個別執行道可接著在適當時將截片資料自隨機存取記憶體907載入至二維移位暫存器結構906中(例如，作為緊接在對截片之資料之操作之前的一載入指令)。在(無論直接自一截片產生器或自記憶體907)將一資料截片載入至暫存器結構906中完成之後，執行道陣列905之執行道對資料進行操作且最終將已完成資料作為一截片直接「寫回」至截片產生器或至隨機存取記憶體907中。若為後者，則I/O單元904自隨機存取記憶體907提取資料以形成一輸出截片，該輸出截片接著經轉發至截片產生器。 純量處理器902包含一程式控制器909，該程式控制器909自純量記憶體903讀取模板處理器之程式碼之指令且將指令發出至執行道陣列905中之執行道。在一實施例中，一單一相同指令經廣播至陣列905內之所有執行道以實現來自資料運算單元901之一類似SIMD行為。在一實施例中，自純量記憶體903讀取且發出至執行道陣列905之執行道之指令之指令格式包含一極長指令字組(VLIW)型格式，該格式包含每指令一個以上運算碼。在又一實施例中，VLIW格式包含引導藉由各執行道之ALU執行之一數學函數之一ALU運算碼(如下文描述，在一實施例中，其可指定一個以上傳統ALU操作)及一記憶體運算碼(其引導一特定執行道或執行道組之一記憶體操作)。 術語「執行道」係指能夠執行一指令之一組一或多個執行單元(例如，可執行一指令之邏輯電路)。然而，在各種實施例中，一執行道可包含除僅執行單元以外的更多類似處理器功能性。例如，除一或多個執行單元以外，一執行道亦可包含解碼一所接收指令之邏輯電路或(在更多類似MIMD設計之情況中)提取及解碼一指令之邏輯電路。關於類似MIMD方法，儘管已在本文中較大程度上描述一集中式程式控制方法，但可在各種替代實施例中實施一更分散式方法(例如包含陣列905之各執行道內之程式碼及一程式控制器)。 一執行道陣列905、程式控制器909及二維移位暫存器結構906之組合提供用於廣泛範圍之可程式功能之一廣泛可調適/可組態硬體平台。例如，考慮到個別執行道能夠執行廣泛範圍之功能且能夠在接近於任何輸出陣列位置處容易地存取輸入影像資料，應用軟體開發者能夠程式化具有廣泛範圍之不同功能能力以及尺寸(例如模板大小)之內核。 除充當用於藉由執行道陣列905操作之影像資料之一資料儲存器以外，隨機存取記憶體907亦可保持一或多個查找表。在各種實施例中，亦可在純量記憶體903內例示一或多個純量查找表。 一純量查找涉及將來自相同索引之相同查找表之相同資料值傳送至執行道陣列905內之執行道之各者。在各種實施例中，上文描述之VLIW指令格式經擴展以亦包含一純量運算碼，該純量運算碼將藉由純量處理器執行之一查找操作引導至一純量查找表中。指定與運算碼結合使用之索引可為一立即運算元或自某其他資料儲存位置提取。無論如何，在一實施例中，自純量記憶體內之一純量查找表之一查找本質上涉及在相同時脈循環期間將相同資料值傳播至執行道陣列905內之所有執行道。在下文進一步提供關於查找表之使用及操作之額外細節。 圖9b概述上文論述之(若干) VLIW指令字組實施例。如在圖9b中觀察，VLIW指令字組格式包含三個分開指令之欄位：1)一純量指令951，其藉由純量處理器執行；2)一ALU指令952，其藉由執行道陣列內之各自ALU以SIMD方式傳播及執行；及3)一記憶體指令953，其以一部分SIMD方式傳播及執行(例如，若沿著執行道陣列中之一相同列之執行道共用一相同隨機存取記憶體，則來自不同列之各者之一個執行道實際上執行指令(記憶體指令953之格式可包含識別來自各列之哪一執行道執行指令之一運算元))。 亦包含一或多個立即運算元之一欄位954。可在指令格式中識別哪一指令951、952、953使用哪一立即運算元資訊。指令951、952、953之各者亦包含其自身各自輸入運算元及所得資訊(例如，用於ALU操作之局部暫存器及用於記憶體存取指令之一局部暫存器及一記憶體位址)。在一實施例中，在執行道陣列內之執行道執行其他指令952、953之任一者之前藉由純量處理器執行純量指令951。即，VLIW字組之執行包含執行純量指令951之一第一循環，其後接著可執行其他指令952、953之一第二循環(應注意，在各種實施例中，可平行執行指令952及953)。 在一實施例中，藉由純量處理器執行之純量指令包含發出至截片產生器以自資料運算單元之記憶體或2D移位暫存器載入截片/將截片儲存至資料運算單元之記憶體或2D移位暫存器中之命令。此處，截片產生器之操作可取決於行緩衝器單元之操作或防止截片產生器完成藉由純量處理器發出之任何命令將花費之循環數目之運行時間之前的理解之其他變量。因而，在一實施例中，任何VLIW字組(其純量指令951對應於或另外導致一命令發出至截片產生器)亦包含其他兩個指令欄位952、953中之無操作(NOOP)指令。程式碼接著進入指令欄位952、953之NOOP指令之一迴圈直至截片產生器完成其至資料運算單元之載入/自資料運算單元之儲存。此處，在將一命令發出至截片產生器之後，純量處理器可設定截片產生器在完成命令之後重設之一互鎖暫存器之一位元。在NOOP迴圈期間，純量處理器監測互鎖暫存器之位元。當純量處理器偵測到截片產生器已完成其命令時，正常執行再次開始。 圖10展示一資料運算組件1001之一實施例。如在圖10中觀察，資料運算組件1001包含邏輯地定位於二維移位暫存器陣列結構1006「上方」之一執行道陣列1005。如上文論述，在各種實施例中，由一截片產生器提供之影像資料之一截片經載入至二維移位暫存器1006中。執行道接著對來自暫存器結構1006之截片資料進行操作。 執行道陣列1005及移位暫存器結構1006相對於彼此固定在適當位置中。然而，移位暫存器陣列1006內之資料以一戰略性且協調方式移位以導致執行道陣列中之各執行道處理資料內之一不同模板。因而，各執行道判定所產生之輸出截片中用於一不同像素之輸出影像值。自圖10之架構，應明白，重疊模板不僅垂直配置而且水平配置，因為執行道陣列1005包含垂直相鄰執行道以及水平相鄰執行道。 資料運算單元1001之一些顯著架構特徵包含具有寬於執行道陣列1005之尺寸之移位暫存器結構1006。即，執行道陣列1005外部存在暫存器之一「光暈(halo)」1009。儘管光暈1009經展示為存在於執行道陣列之兩側上，但取決於實施方案，光暈可存在於執行道陣列1005之較少(一個)或較多(三個或四個)側上。在資料在執行道1005「下方」移位時，光暈1009用於為溢出執行道陣列1005之邊界外部之資料提供「外溢」空間。作為一簡單情況，以執行道陣列1005之右邊緣為中心之一5×5模板在模板之最左像素被處理時，將需要進一步向右之四個光暈暫存器位置。為易於繪製，當在一標稱實施例中，任一側(右側、底側)上之暫存器將具有水平連接及垂直連接兩者時，圖10將光暈之右側之暫存器展示為僅具有水平移位連接且將光暈之底側之暫存器展示為僅具有垂直移位連接。在各種實施例中，光暈區域並不包含用以執行影像處理指令之對應執行道邏輯(例如，不存在ALU)。然而，個別記憶體存取單元(M)存在於光暈區域位置之各者中，使得個別光暈暫存器位置可個別地自記憶體載入資料及將資料儲存至記憶體。 藉由耦合至陣列中之各列及/或各行或其部分之隨機存取記憶體1007_1至1007_R提供額外外溢空間(例如，一隨機存取記憶體可經指派至跨越4個執行道列及2個執行道行之執行道陣列之一「區域」。為簡單起見，本申請案之其餘部分將主要參考基於列及/或行之分配方案。)此處，若一執行道之內核操作要求其處理二維移位暫存器陣列1006外部之像素值(一些影像處理常式可能要求此)，則影像資料平面能夠例如自光暈區域1009進一步外溢至隨機存取記憶體1007中。例如，考量一6×6模板，其中硬體包含在執行道陣列之右邊緣上之一執行道之右側之僅四個儲存元件之一光暈區域。在此情況中，資料將需要進一步移位至光暈1009之右邊緣之右側以完全處理模板。移位至光暈區域1009外部之資料將接著外溢至隨機存取記憶體1007。下文進一步提供圖9之隨機存取記憶體1007及模板處理器之其他應用。 圖11a至圖11k展現影像資料如上文提及般在執行道陣列「下方」之二維移位暫存器陣列內移位之方式之一工作實例。如在圖11a中觀察，在一第一陣列1107中描繪二維移位陣列之資料內容且藉由一圖框1105描繪執行道陣列。而且，簡單化地描繪執行道陣列內之兩個相鄰執行道1110。在此簡單化描繪1110中，各執行道包含可自移位暫存器接受資料、自一ALU輸出(例如，表現為跨循環之一累加器)接受資料或將輸出資料寫入至一輸出目的地中之一暫存器R1。 各執行道亦可在一局部暫存器R2中獲得二維移位陣列中其「下方」之內容。因此，R1係執行道之一實體暫存器，而R2係二維移位暫存器陣列之一實體暫存器。執行道包含可對由R1及/或R2提供之運算元進行操作之一ALU。如將在下文進一步更詳細描述，在一實施例中，實際上使用每陣列位置之多個儲存器/暫存器元件(之一「深度」)實施移位暫存器，但移位活動限於儲存元件之一個平面(例如，儲存元件之僅一個平面可在每循環移位)。圖11a至圖11k描繪如用於儲存來自各自執行道之結果X之此等較深暫存器位置之一者。為易於繪示，較深所得暫存器經繪製成並排於其對應暫存器R2而非在其對應暫存器R2下方。 圖11a至圖11k集中於兩個模板之計算，兩個模板之中心位置與在執行道陣列內描繪之該對執行道位置1111對準。為易於繪示，該對執行道1110經繪製成水平相鄰者，實際上，當根據以下實例時，其係垂直相鄰者。 如首先在圖11a中觀察，執行道以其中心模板位置為中心。圖11b展示藉由兩個執行道執行之目的碼。如在圖11b中觀察，兩個執行道之程式碼導致移位暫存器陣列內之資料向下移位一個位置且向右移位一個位置。此將兩個執行道對準至其各自模板之左上角。程式碼接著導致定位於(R2中)其各自位置中之資料經載入至R1中。 如在圖11c中觀察，程式碼接著導致該對執行道使移位暫存器陣列內之資料向左移位一個單位，此導致在各執行道之各自位置之右側之值移位至各執行道之位置中。R1中之值(先前值)接著與已移位至執行道之位置中(R2中)之新值相加。結果經寫入至R1中。如在圖11d中觀察，重複相同於上文針對圖11c描述之程序，此導致結果R1現在包含上執行道中之值A+B+C及下執行道中之F+G+H。此時，兩個執行道已處理其各自模板之上列。應注意，外溢至執行道陣列之左側上之一光暈區域中(若一個光暈區域存在於左手側上)或至隨機存取記憶體中(若一光暈區域不存在於執行道陣列之左手側上)。 如在圖11e中觀察，程式碼接著導致移位暫存器陣列內之資料向上移位一個單位，此導致兩個執行道與其各自模板之中間列之右邊緣對準。兩個執行道之暫存器R1當前包含模板之頂列及中間列之最右值之總和。圖11f及圖11g展現跨兩個執行道之模板之中間列向左移動之連續進展。累積加法繼續，使得在圖11g之處理結束時，兩個執行道包含其各自模板之頂列及中間列之值之總和。 圖11h展示使各執行道與其對應模板之最下列對準之另一移位。圖11i及圖11j展示完成對兩個執行道之模板之進程之處理之連續移位。圖11k展示使各執行道與其在資料陣列中之正確位置對準且將結果寫入至其額外移位。 在圖11a至圖11k之實例中，應注意，用於移位操作之目的碼可包含識別在(X,Y)座標中表達之移位之方向及量值之一指令格式。例如，用於向上移位一個位置之目的碼可在目的碼中表達為SHIFT 0,+1。作為另一實例，向右移位一個位置可在目的碼中表達為SHIFT +1,0。在各種實施例中，亦可在目的碼中指定具有較大量值之移位(例如SHIFT 0,+2)。此處，若2D移位暫存器硬體僅支援每循環一個位置之移位，則指令可藉由機器解釋為需要多個循環執行，或2D移位暫存器硬體可經設計以支援每循環一個以上位置之移位。下文進一步更詳細描述後者之實施例。 圖12展示用於一執行道及對應移位暫存器結構之單元胞之另一更詳細描繪(在各種實施例中，光暈區域中之暫存器不包含一對應執行道但包含一記憶體單元)。在一實施例中，藉由在執行道陣列之各節點處例示圖12中觀察之電路而實施與執行道陣列中之各位置相關聯之執行道及暫存器空間。如在圖12中觀察，單元胞包含耦合至由四個暫存器R2至R5構成之一暫存器檔案1202之一執行道1201。在任何循環期間，執行道1201可自暫存器R1至R5之任一者讀取或寫入至暫存器R1至R5之任一者。對於需要兩個輸入運算元之指令，執行道可自R1至R5之任一者擷取兩個運算元。 在一實施例中，藉由在一單一循環期間允許暫存器R2至R4之(僅)一者之任一者之內容透過輸出多工器1203移「出」至其相鄰者之暫存器檔案之一者且使暫存器R2至R4之(僅)一者之任一者之內容替換為透過輸入多工器1204自其相鄰者之一對應者移「入」之內容，使得相鄰者之間的移位在一相同方向上(例如所有執行道向左移位、所有執行道向右移位等)而實施二維移位暫存器結構。儘管一相同暫存器使其內容移出且替換為在一相同循環內移入之內容可為常見的，但多工器配置1203、1204允許一相同循環期間之一相同暫存器檔案內之不同移位源及移位目標暫存器。 如在圖12中描繪，應注意，在一移位序列期間，一執行道將使內容自其暫存器檔案1202移出至其左、右、頂部及底部相鄰者之各者。結合相同移位序列，執行道亦將使內容自其左、右、頂部及底部相鄰者之一特定者移入至其暫存器檔案中。再者，移出目標及移入源應與所有執行道之一相同移位方向一致(例如若移出係至右相鄰者，則移入應係自左相鄰者)。 儘管在一項實施例中，每循環每執行道僅允許移位一個暫存器之內容，但其他實施例可允許移入/移出一個以上暫存器之內容。例如，若在圖12中觀察之多工器電路1203、1204之一第二例項經併入至圖12之設計中，則可在一相同循環期間移出/移入兩個暫存器之內容。當然，在其中每循環僅允許移位一個暫存器之內容之實施例中，可藉由消耗更多時脈循環用於數學運算之間的移位而在數學運算之間發生自多個暫存器之移位(例如，藉由消耗數學運算之間的兩個移位操作而在數學運算之間移位兩個暫存器之內容)。 若在一移位序列期間移出一執行道之暫存器檔案之少於所有內容，則應注意，各執行道之未移出暫存器之內容保持在適當位置中(未移位)。因而，未替換為移入內容之任何未移位內容跨移位循環留存在執行道本端。在各執行道中觀察到之記憶體單元(「M」)用於自與執行道陣列內之執行道之列及/或行相關聯之隨機存取記憶體空間載入資料/將資料儲存至該隨機存取記憶體空間。此處，M單元充當一標準M單元，其中其通常用於載入/儲存無法自執行道之自身暫存器空間載入/無法儲存至執行道之自身暫存器空間之資料。在各種實施例中，M單元之主要操作係將資料自一局部暫存器寫入至記憶體中及自記憶體讀取資料且將其寫入至一局部暫存器中。 關於藉由硬體執行道1201之ALU單元支援之ISA運算碼，在各種實施例中，藉由硬體ALU支援之數學運算碼包含(例如ADD、SUB、MOV、MUL、MAD、ABS、DIV、SHL、SHR、MIN/MAX、SEL、AND、OR、XOR、NOT)。恰如上文描述，可藉由執行道1201執行記憶體存取指令以自其相關聯隨機存取記憶體提取資料/將資料儲存至其相關聯隨機存取記憶體。另外，硬體執行道1201支援移位操作指令(右、左、上、下)以使資料在二維移位暫存器結構內移位。如上文描述，較大程度上藉由模板處理器之純量處理器執行程式控制指令。4.0 實施方案實施例 應指出，上文描述之各種影像處理器架構特徵不必限於傳統意義上之影像處理且因此可經應用至可(或可不)導致重新特性化影像處理器之其他應用。例如，若上文描述之各種影像處理器架構特徵之任一者待用於建立及/或產生及/或呈現動畫(相對於處理實際相機影像)，則影像處理器可經特性化為一圖形處理單元。另外，上文描述之影像處理器架構特徵可經應用至其他技術應用，諸如視訊處理、視覺處理、影像辨識及/或機器學習。以此方式應用，影像處理器可(例如作為一協同處理器)與一更通用處理器(例如，其係運算系統之一CPU或其部分)整合或可為一運算系統內之一獨立處理器。 上文論述之硬體設計實施例可體現於一半導體晶片內及/或體現為以一半導體製程為最終目標之一電路設計之一描述。在後一情況中，此等電路描述可採用一(例如，VHDL或Verilog)暫存器轉移層級(RTL)電路描述、一閘極層級電路描述、一電晶體層級電路描述或遮罩描述或其各種組合之形式。電路描述通常體現於一電腦可讀儲存媒體(諸如一CD-ROM或其他類型之儲存技術)上。 自前述章節應認識到，如上文描述之一影像處理器可體現於一電腦系統上之硬體中(例如，作為處理來自手持裝置之相機之資料之一手持裝置之系統單晶片(SOC)之部分)。在其中影像處理器體現為一硬體電路之情況中，應注意，可自一相機直接接收藉由影像處理器處理之影像資料。此處，影像處理器可為一離散相機之部分或具有一整合相機之一運算系統之部分。在後一情況中，可自相機或自運算系統之系統記憶體直接接收影像資料(例如，相機將其影像資料發送至系統記憶體而非影像處理器)。亦應注意，在前述章節中描述之許多特徵可應用至一圖形處理器單元(其呈現動畫)。 圖13提供一運算系統之一例示性描繪。下文描述之運算系統之許多組件可應用至具有一整合相機及相關聯影像處理器之一運算系統(例如，一手持式裝置，諸如一智慧型電話或平板電腦)。一般技術者將能夠容易地區分兩者。另外，圖13之運算系統亦包含一高效能運算系統(諸如一工作站或超級電腦)之許多特徵。 如在圖13中觀察，基本運算系統可包含一中央處理單元1301 (其可包含例如安置於一多核心處理器或應用處理器上之複數個通用處理核心1315_1至1315_N及一主記憶體控制器1317)、系統記憶體1302、一顯示器1303 (例如觸控螢幕、平板)、一局部有線點對點鏈路(例如，USB)介面1304、各種網路I/O功能1305 (諸如乙太網路介面及/或蜂巢式數據機子系統)、一無線區域網路(例如，WiFi)介面1306、一無線點對點鏈路(例如，藍芽)介面1307及一全球定位系統介面1308、各種感測器1309_1至1309_N、一或多個相機1310、一電池1311、一功率管理控制單元1312、一揚聲器及麥克風1313及一音訊編碼器/解碼器1314。 一應用處理器或多核心處理器1350可包含其CPU 1201內之一或多個通用處理核心1315、一或多個圖形處理單元1316、一記憶體管理功能1317 (例如，一記憶體控制器)、一I/O控制功能1318及一影像處理單元1319。通用處理核心1315通常執行運算系統之作業系統及應用軟體。圖形處理單元1316通常執行圖形密集功能以例如產生呈現於顯示器1303上之圖形資訊。記憶體控制功能1317與系統記憶體1302介接以將資料寫入至系統記憶體1302/自系統記憶體1302讀取資料。功率管理控制單元1312通常控制系統1300之功率消耗。 可根據在上文之前述章節中詳細描述之影像處理單元實施例之任一者實施影像處理單元1319。替代地或組合地，IPU 1319可經耦合至GPU 1316及CPU 1301之任一者或兩者以作為其之一協同處理器。另外，在各種實施例中，可使用上文詳細描述之影像處理器特徵之任一者實施GPU 1316。影像處理單元1319可用如上文詳細描述之應用軟體組態。此外，一運算系統(諸如圖13之運算系統)可執行如上所述模擬一影像處理應用軟體程式之程式碼，使得可判定用於各自行緩衝器單元之各自記憶體分配。 觸控螢幕顯示器1303、通信介面1304至1307、GPS介面1308、感測器1309、相機1310及揚聲器/麥克風編解碼器1313、1314之各者皆可被視為關於整個運算系統(亦適當地包含一整合周邊裝置(例如，一或多個相機1310))之I/O (輸入及/或輸出)之各種形式。取決於實施方案，此等I/O組件之各者可整合於應用處理器/多核心處理器1350上或可定位於晶粒之外或應用處理器/多核心處理器1350之封裝外部。 在一實施例中，一或多個相機1310包含能夠量測相機與其視場中之一物件之間的深度之一深度相機。在一應用處理器或其他處理器之一通用CPU核心(或具有用以執行程式碼之一指令執行管線之其他功能區塊)上執行之應用軟體、作業系統軟體、裝置驅動器軟體及/或韌體可執行上文描述之功能之任一者。 本發明之實施例可包含如上文陳述之各種程序。程序可體現為機器可執行指令。指令可用於導致一通用或專用處理器執行某些程序。替代地，可藉由含有用於執行程序之硬接線及/或可程式化邏輯之特定硬體組件或藉由程式化電腦組件及客製硬體組件之任何組合執行此等程序。 本發明之元件亦可提供為用於儲存機器可執行指令之一機器可讀媒體。機器可讀媒體可包含(但不限於)軟碟、光碟、CD-ROM及磁光碟、FLASH記憶體、ROM、RAM、EPROM、EEPROM、磁卡或光學卡、傳播媒體或適於儲存電子指令之其他類型的媒體/機器可讀媒體。例如，本發明可作為一電腦程式下載，該電腦程式可藉由體現於一載波或其他傳播媒體中之資料信號經由一通信鏈路(例如，一數據機或網路連接)自一遠端電腦(例如，一伺服器)傳遞至一請求電腦(例如，一用戶端)。 在前述說明書中，已參考本發明之特定例示性實施例描述本發明。然而，將顯而易見，在不脫離如隨附發明申請專利範圍中陳述之本發明之更廣泛精神及範疇之情況下，可對其作出各種修改及改變。因此，本說明書及圖式應被視為一繪示性意義而非一限制性意義。 在下文描述一些實例。 實例1：一種機器可讀儲存媒體，其含有在藉由一運算系統處理時引起該運算系統執行一方法之程式碼，該方法包括： a)模擬一影像處理應用軟體程式之執行，該模擬包括利用模擬之行緩衝器記憶體攔截內核至內核通信，該等模擬之行緩衝器記憶體儲存及轉發自生產內核之模型傳送至消耗內核之模型之若干影像資料行，該模擬進一步包括在一模擬運行時間內追蹤儲存於各自行緩衝器記憶體中之各自影像資料量；及， b)自該等經追蹤之各自影像資料量判定用於對應硬體行緩衝器記憶體之各自硬體記憶體分配； c)產生組態資訊以供一影像處理器執行該影像處理應用軟體程式，該組態資訊描述用於該影像處理器之該等硬體行緩衝器記憶體之該等硬體記憶體分配。 實例2：如實例1之機器可讀儲存媒體，其中該追蹤進一步包括追蹤一模擬行緩衝器記憶體寫入指標與一模擬行緩衝器記憶體讀取指標之間的一差異。 實例3：如實例1或2之機器可讀儲存媒體，其中該判定係基於該模擬行緩衝器記憶體寫入指標與該模擬行緩衝器記憶體讀取指標之間的一最大所觀察差異。 實例4：如前述實例中至少一項之機器可讀儲存媒體，其中該模擬進一步包括施行一寫入政策，該寫入政策防止影像資料之下一單元寫入至一模擬行緩衝器記憶體中直至消耗該影像資料之內核之一或多個模型等待接收該影像資料之該下一單元為止。 實例5：如前述實例中至少一項之機器可讀儲存媒體，其中該寫入政策係在產生影像資料之該下一單元之一生產內核之一模型處強制執行。 實例6：如前述實例中至少一項之機器可讀儲存媒體，其中該方法進一步包括若該應用軟體程式之該模擬執行出現僵局，則允許違反該寫入政策。 實例7：如前述實例中至少一項之機器可讀儲存媒體，其中該等內核用以在一硬體影像處理器之不同處理核心上操作，該硬體影像處理器包括用以儲存及轉發在該等處理核心之間傳遞之行群組之硬體行緩衝器單元。 實例8：如前述實例中至少一項之機器可讀儲存媒體，其中該等不同處理核心包括一個二維執行道及一個二維移位暫存器陣列。 實例9：如前述實例中至少一項之機器可讀儲存媒體，其中該等生產內核之該等模型及該等消耗內核之該等模型包括發送影像資料至一模擬行緩衝器記憶體之指令且包括自一模擬行緩衝器記憶體讀取影像資料之指令但不包含實質上處理影像資料之指令。 實例10：如前述實例中至少一項之機器可讀儲存媒體，其中一影像處理器架構包括耦合至一個二維移位暫存器陣列之一執行道陣列。 實例11：如前述實例中至少一項之機器可讀儲存媒體，其中該影像處理器之該架構包括一行緩衝器、一截片產生器及/或一模板處理器之至少一者。 實例12：如實例11之機器可讀儲存媒體，其中該模板處理器經組態以處理重疊模板。 實例13：如前述實例中至少一項之機器可讀儲存媒體，其中一資料運算單元包括具有比該執行道陣列寬之尺寸之一移位暫存器結構，特定言之在該執行道陣列外存在暫存器。 實例14：一種運算系統，其包括： 一中央處理單元； 一系統記憶體； 一系統記憶體控制器，其介於該系統記憶體與該中央處理單元之間； 一機器可讀儲存媒體，該機器可讀儲存媒體含有在藉由該運算系統處理時引起該運算系統執行一方法之程式碼，該方法包括： a)模擬一影像處理應用軟體程式之執行，該模擬包括利用模擬之行緩衝器記憶體攔截內核至內核通信，該等模擬之行緩衝器記憶體儲存及轉發自生產內核之模型傳送至消耗內核之模型之若干影像資料行，該模擬進一步包括在一模擬運行時間內追蹤儲存於各自行緩衝器記憶體中之各自影像資料量；及， b)自該等經追蹤之各自影像資料量判定用於對應硬體行緩衝器記憶體之各自硬體記憶體分配； c)產生用於一影像處理器之組態資訊以執行該影像處理應用軟體程式，該組態資訊描述用於該影像處理器之該等硬體行緩衝器記憶體之該等硬體記憶體分配。 實例15：如實例14之運算系統，其中該追蹤進一步包括追蹤一模擬行緩衝器記憶體寫入指標與一模擬行緩衝器記憶體讀取指標之間的一差異。 實例16：如實例14或15之運算系統，其中該判定係基於該模擬行緩衝器記憶體寫入指標與該模擬行緩衝器記憶體讀取指標之間的一最大所觀察差異。 實例17：如實例14至16中至少一項之運算系統，其中該模擬進一步包括施行一寫入政策，該寫入政策防止影像資料之下一單元寫入至一模擬行緩衝器記憶體中直至消耗該影像資料之內核之一或多個模型等待接收該影像資料之該下一單元為止。 實例18：如實例14至17中至少一項之運算系統，其中該寫入政策係在產生影像資料之該下一單元之一生產內核之一模型處強制執行。 實例19：如實例14至18中至少一項之運算系統，其中該方法進一步包括若該應用軟體程式之該模擬執行出現僵局，則允許違反該寫入政策。 實例20：如實例14至19中至少一項之運算系統，其中一影像處理器架構包括耦合至一個二維移位暫存器陣列之一執行道陣列。 實例21：如實例14至20中至少一項之運算系統，其中該影像處理器之該架構包括一行緩衝器、一截片產生器及/或一模板處理器之至少一者。 實例22：如實例21之運算系統，其中該模板處理器經組態以處理重疊模板。 實例23：如實例14至22中至少一項之運算系統，其中一資料運算單元包括具有比該執行道陣列寬之尺寸之一移位暫存器結構，特定言之在該執行道陣列外存在暫存器。 實例24：一種方法，其包括： a)模擬一影像處理應用軟體程式之執行，該模擬包括利用模擬之行緩衝器記憶體攔截內核至內核通信，該等模擬之行緩衝器記憶體儲存及轉發自生產內核之模型傳送至消耗內核之模型之若干影像資料行，該模擬進一步包括在一模擬運行時間內追蹤儲存於各自行緩衝器記憶體中之各自影像資料量；及， b)自該等經追蹤之各自影像資料量判定用於對應硬體行緩衝器記憶體之各自硬體記憶體分配； c)產生用於一影像處理器之組態資訊以執行該影像處理應用軟體程式，該組態資訊描述用於該影像處理器之該等硬體行緩衝器記憶體之該等硬體記憶體分配。 實例25：如實例24之方法，其中該追蹤進一步包括追蹤一模擬行緩衝器記憶體寫入指標與一模擬行緩衝器記憶體讀取指標之間的一差異。 實例26：如實例24或25之方法，其中該判定係基於該模擬行緩衝器記憶體寫入指標與該模擬行緩衝器記憶體讀取指標之間的一最大所觀察差異。 實例27：如實例24至26中至少一項之方法，其中該模擬進一步包括施行一寫入政策，該寫入政策防止影像資料之下一單元寫入至一模擬行緩衝器記憶體中直至消耗該影像資料之內核之一或多個模型等待接收該影像資料之該下一單元為止。 實例28：如實例24至27中至少一項之方法，其中該寫入政策係在產生影像資料之該下一單元之一生產內核之一模型處強制執行。 實例29：如實例24至28中至少一項之方法，其中一影像處理器架構包括耦合至一個二維移位暫存器陣列之一執行道陣列。 實例30：如實例24至29中至少一項之方法，其中該影像處理器之該架構包括一行緩衝器、一截片產生器及/或一模板處理器之至少一者。 實例31：如實例24至30中至少一項之方法，其中該模板處理器經組態以處理重疊模板。 實例32：如實例24至31中至少一項之方法，其中一資料運算單元包括具有比該執行道陣列寬之尺寸之一移位暫存器結構，特定言之在該執行道陣列外存在暫存器。 1.0 Unique image processor architecture As known in the art, the basic circuit structure for executing code includes an execution stage and a register space. The execution phase includes an execution unit for executing instructions. An input operand for an instruction to be executed is provided from the register space to the execution stage. The result generated by executing an instruction during the execution phase is written back to the register space. Running a software thread on a conventional processor requires a series of instructions to be executed sequentially through the execution phase. Most commonly, operations are "scalar" in the sense that a single result is produced from a single set of input operands. However, in the case of a "vector" processor, a result vector is generated from a vector of input operands by executing an instruction during execution. FIG. 1 shows a high-level view of a unique image processor architecture 100 including an execution track array 101 coupled to a two-dimensional shift register array 102. Here, each execution track in the execution track array can be regarded as a discrete execution stage containing execution units required to execute an instruction set supported by the processor 100. In various embodiments, each execution track receives a same instruction to execute in a same machine cycle, so that the processor operates as a two-dimensional single instruction multiple data (SIMD) processor. Each execution track has its own dedicated register space in a corresponding position in the two-dimensional shift register array 102. For example, the corner execution track 103 has its own dedicated register space in the corner shift register position 104, and the corner execution track 105 has its own dedicated register in the corner shift register position 106 Space, etc. In addition, the shift register array 102 is capable of shifting its contents so that each execution track can directly from its own register space to a register space that resides in another execution track during a previous machine cycle. One value operation. For example, a +1 horizontal shift causes the register space of each execution lane to receive a value from the register space of its left-most neighbor. Due to the ability to shift the value in two directions along a horizontal axis and to shift the value in two directions along a vertical axis, the processor can effectively process the template of the image data. Here, it is known in this technology that a template is used as a slice of an image surface area of a basic data unit. For example, a new value for a specific pixel position in an output image may be calculated as an average of the pixel values of the specific pixel position in an area of an input image in which it is centered. For example, if the template has a size of 3 pixels × 3 pixels, the specific pixel position may correspond to the middle pixel of the 3 × 3 pixel array and the average value may be within all nine pixels in the 3 × 3 pixel array. Calculate it. According to various operation embodiments of the processor 100 in FIG. 1, each execution track of the execution track array 101 is responsible for calculating a pixel value at a specific position in an output image. Therefore, following the example of the 3 × 3 template averaging just mentioned above, after the initial loading of input pixel data in the shift register and one of the eight shift operations after coordinated shift sequences, the track array is executed Each execution channel receives all the nine pixel values needed to calculate the average of its corresponding pixel position into its local register space. That is, the processor can simultaneously process multiple overlapping templates centered at, for example, pixel positions of adjacent output images. Because the processor architecture of FIG. 1 is particularly good at processing image templates, it can also be referred to as a template processor. FIG. 2 shows an embodiment of an architecture 200 for an image processor having a plurality of template processors 202_1 to 202_N. As viewed in FIG. 2, the architecture 200 includes interworking via a network 204 (eg, a network on a chip (NOC), including a switch network on a chip, a ring network on a chip, or other types of networks) The line buffer units 201_1 to 201_M are connected to the plurality of template processor units 202_1 to 202_N and the corresponding slice generator units 203_1 to 203_N. In one embodiment, any of the line buffer units 201_1 to 201_M can be connected to any of the slice generator units 203_1 to 203_N and the corresponding template processors 202_1 to 202_N through the network 204. The code is compiled and loaded onto a corresponding template processor 202 to perform early image processor operations defined by a software developer (for example, depending on the design and implementation, the code can also be loaded into the template Processor's associated slice generator 203). In at least some examples, a first kernel program for a first pipeline stage may be loaded into a first template processor 202_1, and a second kernel program for a second pipeline stage may be loaded. To a second template processor 202_2, etc., and implement an image processing pipeline, wherein the first kernel performs the functions of the first phase of the pipeline, the second kernel performs the functions of the second phase of the pipeline, and An additional control flow method is set to pass the output image data from one stage of the pipeline to the next stage of the pipeline. In other configurations, the image processor may be implemented as a parallel machine having two or more template processors 202_1, 202_2 operating the same kernel code. For example, one of the highly dense and high data rate streams of image data can be processed by distributing frames across multiple template processors each performing the same function. In yet other configurations, basically any directed acyclic graph (DAG) of the kernel can be loaded onto the image processor by the following steps: In the DAG design, each template processor uses its own respective code kernel group Configure respective template processors; and configure appropriate control flow interceptors (hardware) into the hardware to direct the output image from one core to the input of the next core. As a general process, a macro I / O unit 205 receives frames of image data and passes the frames to one or more of the line buffer units 201 frame by frame. A specific line buffer unit parses its image data frame into a smaller image data area (referred to as a "line group"), and then passes the line group to a specific slice generator via the network 204 . A complete or "full" single row group may, for example, be composed of multiple consecutive complete rows or rows of data in a frame (for the sake of brevity, this description will mainly refer to consecutive rows). The slice generator further analyzes the group of image data into a smaller image data area (referred to as a "sheet"), and presents the slice to its corresponding template processor. In the case of an image processing pipeline or a DAG process with a single input, generally, the input frame is guided to the same line buffer unit 201_1, which parses the image data into line groups and The row groups are guided to the fragment generator 203_1, and the corresponding template processor 202_1 of the fragment generator 203_1 executes the code of the first kernel in the pipeline / DAG. After completing the template processor 202_1's operation on the row groups it processes, the slice generator 203_1 sends the output row group to a "downstream" line buffer unit 201_2 (the output row group can be used in some use cases The group is sent back to the same line buffer unit 201_1) of the input line group that was sent earlier. Represents one or more "consumers" in the next stage / operation in the pipeline / DAG on its own other fragment generator and template processor (e.g., fragment generator 203_2 and template processor 202_2) The kernel then receives the image data generated by the first template processor 202_1 from the lower buffer unit 201_2. In this way, a "producer" kernel operating on a first template processor causes its output data to be forwarded to a "consumer" kernel operating on a second template processor, where the consumer kernel is based on the whole The pipeline or DAG is designed to perform the next set of tasks after the producer's kernel. As mentioned above with reference to FIG. 1, each of the template processors 202_1 to 202_N is designed to process multiple overlapping templates of image data at the same time. The multiple overlapping templates and the internal hardware processing capabilities of the template processor effectively determine the size of a slice. Also, as discussed above, within any of the template processors 202_1 to 202_N, several array joint operations are performed to simultaneously process the surface area of the image data covered by the multiple overlapping templates. In addition, in various embodiments, a corresponding (for example, partial) clip generator 203 of a template processor 202 loads several clips of image data into the two-dimensional shift register of the template processor. In the array. It is believed that the use of a slice and a two-dimensional shift register array structure by moving a large amount of data into a large amount of register space as, for example, a single load operation (immediately thereafter by an execution track array Directly perform processing tasks on the data) to effectively provide improved power consumption. In addition, the use of a track array and corresponding register array provides different template sizes that can be easily programmed / configured. More details on the operation of the line buffer unit, the slice generator and the template processor are provided further in Section 3.0 below.2.0 Judgement of memory allocation by line buffer unit As can be understood from the discussion above, the hardware platform can support a large number of different application software program structures. That is, a virtually unlimited number of different and complex kernel-to-kernel connections can be supported. Understanding how much memory space should be allocated to each row of buffer units 201_1 to 201_M for any particular software application is challenging. Here, in one embodiment, different accesses to the line buffer unit have been allocated to its own respective memory, for example, from an physically shared memory. Therefore, a line of buffer cells can be more generally characterized as a line of buffer memory. During program execution, a row of buffer units temporarily stores, for example, data it receives from a production core into its respective memory. After a consuming core is ready to receive the data, the row buffer unit reads the data from its respective memory and forwards the data to the consuming core. After one or more of the line buffer units or the owner entity are coupled to a same shared memory resource, the configuration of an application software program executing on the image processor therefore includes defining the line buffers that should be used to share the memory resource. The memory unit individually allocates how much memory capacity of the shared memory resource. Clearly articulating one of the possible memory allocations for each row of buffer units can be very difficult to determine, especially for complex application software programs with complex data streams and associated data dependencies. FIG. 3 shows an example of an exemplary slightly complex application software program (or part thereof) and its buffer unit configuration on the image processor. In various embodiments, a production core is allowed to generate separate, different output video streams for different consuming cores. It also allows a production core to produce a single output stream that is consumed by two or more different cores. Finally, in various embodiments, a row of buffer units can receive an input stream from a single production core but can feed the stream to one or more consuming cores. The application software configuration of Figure 3 illustrates each of these configuration possibilities. Here, the kernel K1 generates a first data stream for one of the kernels K2 and K3 and generates a second, different data stream for the kernel K4. The kernel K1 sends the first data stream to the line buffer unit 304_1, and the line buffer unit 304_1 forwards the data to both the kernels K2 and K3. The kernel K1 also sends the second data stream to the row buffer unit 304_2, and the row buffer unit 304_2 forwards the data to the kernel K4. In addition, kernel K2 sends a data stream to kernel K4 and kernel K3 sends a data stream to kernel K4. The kernel K2 sends its data stream to the line buffer unit 304_3, which forwards the data to the kernel K4. The kernel K3 sends its data stream to the line buffer unit 304_4, which forwards the data to the kernel K4. Here, it is difficult to explicitly calculate the amount of memory uniquely allocated to each of the line buffer units 304_1 to 304_4. Considering each memory allocation as a queue, if the row buffer unit will receive a large amount of data from a production core over time, the amount of memory required will tend to increase. In contrast, if the line buffer unit will receive a small amount of data from the production core over time, the amount of memory required tends to decrease. Similarly, if the line buffer unit will send a small amount of data to more consuming cores over time, the amount of memory tends to increase, or if the line buffer unit will send a large amount of data to less consumption over time Kernel, the amount of memory required tends to decrease. The amount of data that the line buffer unit will receive from the producer kernel over time can vary according to any of the following: 1) the dependence of the production kernel on its own input data; 2) the output data produced by the production kernel Rate, irrespective of 1) dependency / rate above; and 3) the size of the data unit sent by the production kernel to the line buffer unit. Similarly, the amount of data that the line buffer unit will send over time can vary based on any of the following: 1) the number of consumption cores fed by the production core; 2) 1) each of these consumption cores The respective rates at which new data is ready to be received (which may vary based on other data dependencies of the consuming cores); and 3) the size of the data units that the (or other) consuming cores receive on their own buffer units. Because at least for slightly complex application software program structures, the complex nature of various interdependencies and connection rates makes it very difficult to explicitly calculate the correct amount of memory space to allocate to each row of buffer units, so in each embodiment, a Heuristic method, which simulates the execution of an application software program before running time in a simulation environment and monitors the amount of queued data at each row of buffer units caused by the internal data flow of the simulation program. FIG. 4 depicts a preparatory procedure for the application software program of FIG. 3 for setting up a simulation environment. In one embodiment, a simulation model is created for each core by having each core strip its load instruction and its store instruction. A kernel's load instruction corresponds to the kernel consuming input data from a row of buffer units and a kernel's store instruction corresponds to the kernel generating output data to write to a row of buffer units. As discussed above, a core may be configured to receive multiple different input streams from, for example, multiple different core / line buffer units. Thus, the actual kernel and its simulation model kernel may contain multiple load instructions (each for a different input stream). As also discussed above, a kernel (and therefore a simulation model kernel) can be configured to feed different cores to generate streams differently. Therefore, an actual kernel and its simulation model kernel may include multiple storage instructions. Referring to FIG. 4, the simulation model kernel K1 shows a load instruction (LD_1) and two storage instructions (ST_1 and ST_2), and this shows that the kernel K1 receives an input stream (input data to the image processor) and provides two The output streams (one to the line buffer unit 304_1 and the other to the line buffer unit 304_2) have the same description of the kernel K1 in FIG. Figure 4 also shows a load instruction and a store instruction for the simulation model kernel K2, and this shows that the kernel K2 receives an input stream from its buffer unit 304_1 and generates an output stream to one of the line buffer units 304_3 The description of the kernel K2 is the same. Figure 4 also shows a load instruction and a store instruction for the simulation model kernel K3, and this shows that the kernel K3 receives an input stream from the buffer unit 304_1 and generates an output stream to one of the line buffer units 304_4. The description of the kernel K3 is the same. Finally, FIG. 4 shows that the simulation model kernel K4 has three load instructions and a store instruction, and this shows that the kernel K3 receives a first input stream in the self-buffer unit 304_2 and receives a second input stream in the self-buffer unit 304_3. It is consistent with the description of the kernel K4 in FIG. 3 that the self-buffer unit 304_4 receives a third input stream. The kernel K4 is also shown in FIG. 3 to generate an output stream. As indicated by the loops 401_1 to 401_4 in FIG. 4, the simulation model kernel (such as the actual kernel) is repeatedly looped. That is, a kernel executes its load instruction (s) at the start of execution to receive its input data, and a kernel executes its store instruction at the end of execution to generate its output data from the input data it receives from its load instruction. The procedure then repeats. In various embodiments, each simulation model kernel may also include a value that instructs the kernel to perform operations on input data to generate its output data (its propagation delay). That is, the simulation model kernel does not allow its store instructions to execute until some number of cycles after its load instruction has been executed. In addition, in various embodiments, in order to reduce the time taken to execute the simulation, the kernel model is stripped of its actual image processing routine. That is, the simulation does not perform any actual image processing, only the data transfer of the "virtual" data is simulated. After the simulation model kernel has been constructed, it is connected to each other through the respective simulation models of the line buffer unit according to the design / architecture of the overall application software program. Basically, continue to use the application software program of FIG. 3 as an example to construct a simulation model of the application software program 300 in a simulation environment, wherein the simulation model contains a structure according to the structure depicted in FIG. 3 through the line buffer unit 304_1 The respective simulation models to 304_4 interconnect the simulation models of kernels K1 to K4 of FIG. 4. In order to study the memory requirements of the buffer units in each row, a simulation input image data stream (for example, a simulation of one of the input image data 301 in FIG. 3) is presented to the simulation model of the application. The simulation model of the application software program is then executed, wherein the simulation model kernel repeatedly consumes analog input data through the execution of its load load (s), and generates simulations from the received input data through its storage load (s). Output data and repeat. Here, each analog load instruction may be incorporated or otherwise formatted based on some input image data existing in the original source kernel (such as the number of lines in an input line group, maximum input line group rate, input Block size / size, maximum input block rate, etc.) to determine the analog quantity and rate of consumed input data. Similarly, each store instruction may specify or otherwise format some output images (such as the number of rows in an output row group, the maximum output row group rate, the output block size / Size, maximum output block rate, etc.) to determine the amount and rate of output data generated. In one embodiment, the load / store instructions of the kernel model and the line buffer unit model reflect their actual interaction with the application software and the underlying hardware platform because, for example, one of the generated image data is specific One part is identified by a storage instruction of a production model kernel and one of the requested image data is specific. The next part is identified by a load instruction of a consumption model kernel. Each row of buffer cell models receives its respective analog input stream from its respective production model kernel and stores it in an analog memory resource with, for example, unlimited capacity. In addition, the amount of data transmitted per data transaction is the same as the original source kernel of the production model kernel. When the consuming core (s) of the image stream received by the line buffer unit model executes their respective load instructions, it requests the input image string from the line buffer unit model according to each data exchange amount with its original source kernel. A drop. In response, the line buffer unit model provides a requested data unit from under its memory resource. When the model of the application software program is executed in the simulation environment, the respective memory states of each row of the buffer unit model will be in response to the load instruction of its consuming kernel (s) requesting the activity read from it and responding to its consumption Kernel storage instructions request writes to its activity and fluctuate. In order to finally determine the memory capacity requirements of each row of buffer units, as shown in FIGS. 5a and 5b, the simulation model of each row of buffer units includes a write index and a read index. The write indicator specifies how much input image data from a production kernel model has been written into the memory of the line buffer unit model so far. The read indicator specifies how much input image data has been read from the memory of the buffer unit model of the row so far to serve the load instruction request (s) from the row buffer unit model that consumes the kernel model. The depiction of FIG. 5a instructs a specific consuming kernel to request X amount of image data per load instruction request (X may correspond to, for example, a specific number of image lines, correspond to a block size, etc.). That is, in the case where the consumption kernel model has been sent to guide the amount of data to be read, the line buffer unit will not be able to service a load instruction request from the next consumption kernel model until the amount of data written into the memory reaches Corresponds to one amount of reading index + X (ie, until the writing index points to a value equal to one of reading index + X). As clearly depicted in Figure 5a, the write index has not yet reached this level. Therefore, if the consuming core has requested the next amount (up to the reading index + X), the consuming core currently stops waiting for more output data from the production core to be written into the memory. If the consuming kernel has not requested the next amount, it has not yet stopped technically, and therefore there is still time for the production kernel to provide at least an amount equal to ((read indicator + X)-write indicator) so that it can consume the kernel Request it to be written to memory before. This particular event is depicted in Figure 5b. The maximum amount of memory capacity required for a row of buffer units is the largest observed difference between a read indicator and a write indicator during a simulation run time of the application software program that is long enough. Therefore, the determination of the memory capacity of each line of buffer units requires the execution of the simulation program for a sufficient number of cycles, while continuously tracking the difference between the write index and the read index and recording each new maximum observed difference. After completing a sufficient number of execution cycles, the remaining maximum observed difference recorded for each row of buffer unit models (which corresponds to the largest difference observed during the entire simulation period) corresponds to the memory capacity required for each row of buffer units. In various embodiments, in order to prevent one of the producers from generating output data at a faster rate than its consumer (s) can consume the output data, thereby causing the line buffer unit to continuously write to its memory without One of the unrealistic conditions of using its unlimited capacity is restricted, and each kernel model also includes a write policy that is enforced under each of its storage instructions. That is, the write policy is used to check the amount of buffer cell memory to which the output data of the production kernel model is written. Specifically, in one embodiment, the store instruction of a production core is not executed until all corresponding consumption cores are stopped (also referred to as "prepare"). That is, the load instruction of the production core is allowed to be executed only when the read index + X for each of the consuming cores is greater than the write index for the image stream of a production core. In this state, each of the consuming kernels (which cannot execute their respective load instructions for the next unit of the image stream of the production kernel, because the data has not been generated by the production kernel and written to Line buffer unit memory). Therefore, the simulation environment is characterized in that the producer cannot execute a storage instruction directed to a specific output stream leading to a specific line buffer unit, until each of the kernels that consume the output stream from the line buffer unit is in its Stop at each load instruction. The load instruction will load the data unit below the stream from the line buffer unit. Also, although this may be an atypical run-time behavior of the actual system, it roughly sets an upper bound on the amount of memory required at the line buffer unit (e.g., using the implemented write policy with the maximum observed write The index judges the difference of reading index). If, for example, the actual amount of memory allocated to each line of buffer units is the same as (or slightly greater than) the amount determined from the maximum observed write index versus read index difference, the actual system may never experience any consumer Stop because the producer usually executes store instructions at will until the memory of the line buffer unit becomes full (at this time, the line buffer unit in the actual system will not allow the producer to send any more data). However, since the producers are not allowed to execute their storage instructions during the simulation until all their consumers are stopped, the memory allocation, as determined by the simulation, is converted to a producer for an actual system approximately no later than its consumers. New data is generated for consumption. Thus, in general, consumers should not stop in a real system. In this way, the simulation results basically determine the minimum memory capacity required at each line of buffer units. Ideally, after a sufficient number of simulation run time cycles, the amount of memory to be allocated to each line of buffer units can be determined. However, in various simulation runtime experiences, the simulation system can reach the overall deadlock where data cannot flow anywhere in the system. That is, because no data has been generated, all kernels in the system cannot execute the next load instruction, and all producers cannot write the next amount of data (for example, because their own load instructions have stopped and the production kernel has no new inputs To generate output data from it). If the system reaches an overall deadlock as described above, the system state is analyzed and a deadlock cycle is found. A deadlock loop is a closed loop in the application's data stream, which contains a specific load that has been stopped waiting for a specific store to execute, but the specific store cannot be executed because it is waiting to execute the stopped load (should (Note that the stopped loading and the stopped storage are not necessarily associated with a kernel that communicates directly with each other). For example, in the simulation model of the software program in FIG. 3, the load instruction of the model of the K4 kernel that reads data from the buffer unit 304_4 can wait for the data to be generated by the kernel K3. This particular loading stop basically stops all kernels K4, thus preventing the execution of the load instruction of K4 read by the self-buffer 304_2. If the state of the line buffer 304_2 makes the write index ahead of the read index + X (for example, because K1 writes a large number of data units in the line buffer 304_2), write to the storage instruction of K1 in the line buffer 304_2 It will stop, this stop all K1, including the storage instruction written to the line buffer 304_1. Because it is not written to the line buffer 304_1, K3 is stopped, and the identification analysis of the deadlock cycle is completed. That is, the deadlock cycle runs as: 1) from K1 through the line buffer unit 304_1 to kernel K3; 2) from kernel K3 through the line buffer unit 304_4 to kernel K4; and 3) from kernel K4 to the kernel through line buffer 304_2 K1. In the presence of this particular deadlock cycle, K2 will also stop, leading to an overall deadlock in the entire system (and this also creates more deadlock cycles in the system). In one embodiment, once a deadlock cycle has been identified, a data unit is allowed to be advanced along a stop storage instruction along one of the cycles, and it is hoped that this advancement will "start" the system back into operation. For example, if the storage instruction written to the kernel K1 in the line buffer unit 304_1 is advanced by one data unit, the execution of the stopped loading instruction causing the kernel K3 may be sufficient, which in turn may cause the system to start again operating. In one embodiment, only one unit is allowed to advance along one of the deadlock cycles with a stop storage instruction. If the advance does not cause the system to start operating again, another storage instruction along the deadlock cycle is selected for advance. The process of selecting one storage instruction for advancement at a time continues until the system begins to operate or remains in an overall deadlock after all storage instructions along the deadlock cycle are allowed to advance forward by one unit of data. If the latter condition is reached (the system remains in an overall deadlock), one of the writers along the deadlock cycle is selected and allowed to write freely in the hope that the system will begin operation again. If the system has not started operation, another storage instruction along the deadlock loop is selected and allowed to be freely written, etc. Finally, the system should start operating. In various embodiments, the production / consumption kernel model may send image data to or read image data from its respective line buffer unit model according to different transmission modes. According to a first mode (referred to as a "full row group"), a plurality of image data rows of the same width are transmitted between a kernel model and a row of buffer unit models. Figures 6a and 6b depict one embodiment of full row group mode operation. As viewed in FIG. 6a, the image area 600 corresponds to a full frame of the image data or a section of a full frame of the image data (the reader will understand that the depicted matrix shows different pixel positions of the entire image). As depicted in FIG. 6a, a first transmission (e.g., a first packet) of image data sent between a kernel model and a line buffer unit model contains a first group of image rows 601 of the same width. The image line 601 extends completely across the transmitted frame to the section 600 of one transmitted frame. Next, as depicted in FIG. 6b, a second transmission contains the same width image line 602 of a second group that fully extends across the frame or its segment 600. Here, the transmission of the group 601 of FIG. 6a will advance the write index and / or read index of the line buffer cell model by one cell. Similarly, the transmission of the group 602 of FIG. 6b will advance the write index and / or read index of the row buffer cell model to another cell. Therefore, the behaviors of the write index and read index described above with reference to FIGS. 5 a and 5 b are consistent with the full-row group mode. Another transmission mode (called "actually high") can be used to transmit image data blocks (two-dimensional surface areas of image data). Here, as discussed above with reference to FIG. 1 and described in more detail below, in various embodiments, one or more processing cores of the entire image processor each include a two-dimensional execution track array and a two-dimensional shift temporarily. Memory array. Therefore, the register space of a processing core is loaded with all blocks of image data (rather than just scalar or a single vector value). Based on the two-dimensional nature of the data units processed by the processing core, in fact, the high mode is capable of transmitting image data blocks, as depicted in Figures 6c and 6d. Referring to FIG. 6c, initially, a small-height full-width row group 611 is, for example, transferred from a first production kernel model to a row of buffer unit models. From then on, at least for the image area 600, the image data is transferred from the production kernel model to the line buffer unit model according to smaller width row groups 612_1, 612_2, and so on. Here, the smaller-width row group 612_1 is transmitted to the row buffer unit model data exchange in, for example, a second production kernel model. Next, as observed in FIG. 6d, the next smaller width row group 612_2 is transmitted to the row buffer unit model data exchange in, for example, a third production kernel model. Thus, the line buffer unit model write index is initially incremented to a large value (to represent the transmission of the entire row group 611) but then incremented to a smaller value (for example, a first smaller value to represent a smaller width row The transmission of group 612_1, and then increase the next smaller value to indicate the transmission of the smaller width row group 612_2). As described above, FIG. 6c and FIG. 6d show that the content sent by a production kernel model is written into the line buffer unit model memory. The consumption kernel model can be configured to also receive image data as described above (in this case, the behavior of reading the indicator is the same as the behavior of writing the indicator just described above), or forming the image data in the line buffer memory Block when receiving these image data blocks. That is, compared to the latter, the consumption kernel model is not initially sent to the first full row group 611. Instead, after the first smaller row-width row group 612_1 is written into the line buffer memory, the consumption kernel model is initially sent corresponding to a first 5 × 5 pixel value array (the bottom edge of which is The bottom edge of the small-width row group 612_1 is an amount of data. Then, after writing the second smaller line width line group 612_2 into the line buffer memory, the consumption model is sent a second 5 × 5 pixel value array (the bottom edge is Group 612_2 outline). In the case of block transfer to the consuming kernel model as described above, as depicted in FIG. 6e, the next amount to be transferred includes a smaller data fragment that has been more recently written into the line buffer memory and A larger piece of data written to the line buffer memory some time ago. FIG. 7 shows one of the methods described above for determining the memory allocation of the line buffer unit. The method includes simulating execution 701 of an image processing application software program. The simulation includes kernel-to-kernel communication that intercepts 702 and stores and forwards the model from the production kernel to the model that consumes some of the image data lines of the kernel's line buffer memory. The simulation further includes tracking 703 the amount of respective image data stored in the respective analog line buffer memory during a simulation run time. The method also includes determining 704 from the tracked respective image data amounts for respective hardware memory allocations corresponding to the hardware line buffer memory. The hardware memory allocation can be determined at least in part by implementing a tracked observation of the storage state of the analog line buffer memory in view of adjusting the analog line buffer memory in proportion to each other. For example, if a first analog line buffer memory exhibits twice the maximum write-to-read index difference as a second analog line buffer memory, it is used for the correspondence of the first hardware line buffer unit. The actual hardware memory allocation will be approximately twice the corresponding actual hardware memory allocation for this second hardware line buffer unit. The remaining allocations will be adjusted proportionally accordingly. After the memory allocation has been determined for the application software program, the application software program can be configured to run on the target image processor with configuration information, wherein the configuration information informs the hardware of the image processor Determines how much line buffer unit memory space is allocated to the respective hardware line buffer units. The configuration information may also include, for example, assigning a kernel to execute on a specific template processor of the image processor and generate a specific hardware line buffer unit and consume the specific hardware line buffer unit. The corpus of the configuration information generated by the application can then, for example, be loaded into the configuration register space and / or configuration memory resources of the image processor to "set" the image processor hardware to Run the application. In various embodiments, the aforementioned line buffer unit may be more generally characterized as a buffer that stores and forwards image data between the production core and the consumption core. That is, in various embodiments, a buffer does not necessarily need to form a queue of rows. In addition, the hardware platform of the image processor may include a plurality of line buffer units with associated memory resources and one or more line buffers may be configured to operate from a single line buffer unit. That is, a single line buffer unit in hardware can be configured to store and forward different image data streams between different production / consumption core pairs. In various embodiments, the actual kernel may be simulated during simulation rather than its model. Still further, the image data transmitted between the kernel and the line buffer unit during the simulation may be a representation of the image data (eg, several lines, where each line should be understood to correspond to a particular data size). For the sake of brevity, the term image material should be understood as being applied to actual image material or one of the representations of image material.3.0 Image processor implementation example Figures 8a to 8e to 12 provide additional details regarding the operation and design of the embodiments of the image processor and associated template processor described in detail above. Recalling the discussion of the associated slice generator that a row buffer unit feeds a row group to a template processor in FIG. 2, FIGS. 8 a to 8 e use the analysis activity of a row buffer unit 201 and a slice generator unit 203. Both a finer-grained profiling activity and a high-level embodiment of a template processing activity of a template processor 702 coupled to the fragment generator unit 203 are shown. FIG. 8a depicts one embodiment of an input frame of image data 801. FIG. Figure 8a also depicts a contour of one of the three overlapping templates 802 (each having a size of 3 pixels x 3 pixels) that a template processor is designed to operate on. The output pixels for which each template generates output image data are highlighted in pure black. For brevity, the three overlapping templates 802 are depicted as overlapping only in the vertical direction. It should be recognized that a template processor may actually be designed to have overlapping templates in both vertical and horizontal directions. Due to the vertically overlapping templates 802 in the template processor, as viewed in Figure 8a, the frame contains a large amount of image data that a single template processor can operate on. As will be discussed in more detail below, in one embodiment, the template processor processes the data in its overlapping templates in one of the left-to-right ways across the image data (and then repeats top-to-bottom for the next set of rows) . Therefore, as the template processor continues its operations, the number of pure black output pixel blocks will increase horizontally to the right. As discussed above, a row of buffer units 201 is responsible for parsing a row group of input image data from an incoming frame that is sufficient for the template processor to operate on an extended number of upcoming cycles. An exemplary depiction of one line group is shown as a shaded area 803. In one embodiment, the line buffer unit 201 can understand different dynamics of sending a group of lines to a slice generator or receiving a group of slices from a slice generator. For example, according to a pattern (known as "full group"), a full-width image data line is passed between a buffer unit and a slice generator. According to a second pattern (called "actually high"), a row of groups is initially passed as a subset of a full-width column. The remaining columns are then passed sequentially in smaller (less than full width) fragments. In the case where the row group 803 of the input image data has been defined by the row buffer unit and passed to the slice generator unit, the slice generator unit further analyzes the row group into a more precise matching template processor Hardware-limited finer cuts. More specifically, as will be described in further detail below, in one embodiment, each template processor consists of a two-dimensional shift register array. The two-dimensional shift register array basically shifts image data "below" on an execution track array, wherein the shift pattern causes each execution track to perform data operations on its own template (i.e., each execution Tao processes its own information template to produce an output for one of the templates). In one embodiment, the clips are "filled" or otherwise loaded into the surface area of the two-dimensional shift register array of the input image data. As will be described in more detail below, in various embodiments, there are actually multiple layers of two-dimensional register data that can be shifted within any cycle. For convenience, most of the description of this invention will only use the term "two-dimensional shift register" and the like to refer to two-dimensional register data with one or more of these layers that can be shifted. structure. Therefore, as viewed in FIG. 8b, the fragment generator parses one of the initial fragments 804 from the row group 803 and provides it to the template processor (here, this data fragment corresponds to the one generally identified by the component symbol 804 Shaded area). As seen in FIGS. 8c and 8d, the template processor operates on the input image data by effectively moving the overlapping template 802 from left to right above the clip. As of FIG. 8d, the number of pixels that can be used to calculate an output value for the data in the self-slice (other pixel positions may not have one of the output values determined by the information in the self-slice). For brevity, the border area of the image has been ignored. As viewed in Figure 8e, the slice generator then provides the next slice 805 to the template processor for continued operation. It should be noted that the initial position of the template when it starts operating on the next slice is a process from the depletion point on the first slice (as previously depicted in Figure 8d) to the right. Regarding the new slice 805, the template will simply continue to move to the right as the template processor operates on the new slice in the same manner as the first slice. It should be noted that due to the boundary area of the template surrounding an output pixel location, there is some overlap between the data of the first slice 804 and the data of the second slice 805. The overlap can be handled simply by retransmitting the overlapping data twice by a slice generator. In an alternate implementation, to feed the next fragment to the template processor, the fragment generator may proceed to send only new data to the template processor and the template processor reuses overlapping data from the previous fragment. FIG. 9 shows one embodiment of a template processor architecture 900. As seen in FIG. 9, the template processor includes a data operation unit 901, a scalar processor 902 and an associated memory 903, and an I / O unit 904. The data operation unit 901 includes an execution track array 905, a two-dimensional shift array structure 906, and separate random access memories 907_1 to 907_R associated with a specific column or row of the array. The I / O unit 904 is responsible for loading the "input" data fragments received from the fragment generator into the data operation unit 901 and storing the "output" data fragments from the template processor into the fragment generator. In an embodiment, loading the clip data into the data operation unit 901 requires parsing a received clip into rows / rows of image data and loading the rows / rows of image data into a two-dimensional shift buffer. To the respective random access memory 907 of the bank structure 906 or the row / row of the track array (described in more detail below). If the fragment is first loaded into the memory 907, the individual execution tracks in the execution track array 905 may then load the fragment data from the random access memory 907 into the two-dimensional shift register structure when appropriate. 906 (for example, as a load instruction immediately before the operation on the clipped data). After loading a data fragment into the temporary register structure 906 (whether directly from a fragment generator or from the memory 907), the execution path of the execution track array 905 operates on the data and finally completes the data. As a fragment, "write back" directly to the fragment generator or into the random access memory 907. If it is the latter, the I / O unit 904 extracts data from the random access memory 907 to form an output fragment, which is then forwarded to the fragment generator. The scalar processor 902 includes a program controller 909. The program controller 909 reads the instructions of the template processor's code from the scalar memory 903 and sends the instructions to the execution track in the execution track array 905. In one embodiment, a single identical instruction is broadcast to all execution lanes in the array 905 to achieve SIMD-like behavior from one of the data operation units 901. In one embodiment, the instruction format of the instruction read from the scalar memory 903 and issued to the execution channel of the execution channel array 905 includes a very long instruction word (VLIW) type format, which includes more than one operation per instruction. code. In yet another embodiment, the VLIW format includes an ALU operation code (as described below, in one embodiment, which can specify more than one traditional ALU operation) to guide a mathematical function performed by the ALU of each execution channel, and a Memory opcodes (which direct a memory operation on a particular execution channel or group of execution channels). The term "execution path" refers to a group of one or more execution units capable of executing an instruction (e.g., a logic circuit that can execute an instruction). However, in various embodiments, an execution lane may include more similar processor functionality than just an execution unit. For example, in addition to one or more execution units, an execution path may also include logic circuits that decode a received instruction or (in more MIMD-like designs) logic that fetches and decodes an instruction. Regarding similar MIMD methods, although a centralized program control method has been described to a greater extent herein, a more decentralized method may be implemented in various alternative embodiments (e.g., the code and A program controller). The combination of a track array 905, a program controller 909, and a two-dimensional shift register structure 906 provides a widely adaptable / configurable hardware platform for a wide range of programmable functions. For example, considering that individual execution lanes can perform a wide range of functions and can easily access input image data near any output array location, application software developers can program a wide range of different functional capabilities and sizes (such as templates Size) of the kernel. In addition to serving as a data store for image data that is operated by performing the track array 905, the random access memory 907 may also maintain one or more lookup tables. In various embodiments, one or more scalar lookup tables may be instantiated in the scalar memory 903. A scalar lookup involves transmitting the same data values from the same lookup table with the same index to each of the execution tracks in the execution track array 905. In various embodiments, the VLIW instruction format described above is extended to also include a scalar opcode, which will be directed into a scalar lookup table by a lookup operation performed by the scalar processor. The index specified for use with the opcode can be an immediate operand or retrieved from some other data storage location. Regardless, in one embodiment, a lookup from one of the scalar lookup tables in a scalar memory essentially involves propagating the same data value to all execution tracks in the execution track array 905 during the same clock cycle. Additional details on the use and operation of lookup tables are provided below. Figure 9b summarizes the VLIW instruction block embodiment (s) discussed above. As observed in Figure 9b, the VLIW instruction block format contains three separate instruction fields: 1) a scalar instruction 951, which is executed by a scalar processor; 2) an ALU instruction 952, which is executed by The respective ALUs in the array are propagated and executed in the SIMD manner; and 3) a memory instruction 953 is propagated and executed in a part of the SIMD manner (e.g., if an execution lane along the same row in the execution lane array shares a same random To access the memory, one execution lane from each of the different rows actually executes the instruction (the format of the memory instruction 953 may include an operand that identifies which execution lane from each row executes the instruction). It also contains a field 954, one of one or more immediate operands. It is possible to identify which instruction 951, 952, 953 uses which immediate operand information in the instruction format. Each of the instructions 951, 952, and 953 also contains its own input operator and the obtained information (e.g., a local register for ALU operations and a local register for memory access instructions and a memory location site). In one embodiment, the scalar instruction is executed by a scalar processor 951 before any one of the other instructions 952, 953 is executed by the execution lane in the execution lane array. That is, the execution of the VLIW block includes the execution of a first cycle of one of the scalar instructions 951, followed by the execution of a second cycle of one of the other instructions 952, 953 (note that in various embodiments, instructions 952 and 952 can be executed in parallel 953). In one embodiment, the scalar instruction executed by the scalar processor includes issuing to the fragment generator to load the fragment from the memory of the data operation unit or the 2D shift register / storing the fragment to the data. Commands in the memory of the arithmetic unit or in the 2D shift register. Here, the operation of the slice generator may depend on the operation of the line buffer unit or other variables that prevent the slice generator from comprehending any number of cycles that it would take for any command issued by the scalar processor to be understood before run time. Therefore, in one embodiment, any VLIW block (whose scalar instruction 951 corresponds to or otherwise causes a command to be issued to the slice generator) also contains the NOOP in the other two instruction fields 952, 953. instruction. The code then loops into one of the NOOP instructions in the instruction fields 952 and 953 until the slice generator finishes loading / saving to the data operation unit. Here, after issuing a command to the fragment generator, the scalar processor may set the fragment generator to reset a bit of an interlock register after completing the command. During the NOOP loop, the scalar processor monitors the bits of the interlock register. When the scalar processor detects that the fragment generator has completed its command, normal execution begins again. FIG. 10 shows an embodiment of a data computing component 1001. As seen in FIG. 10, the data computing component 1001 includes an execution track array 1005 logically positioned on one of the “upper” two-dimensional shift register array structures 1006. As discussed above, in various embodiments, a slice of the image data provided by a slice generator is loaded into the two-dimensional shift register 1006. The execution channel then operates on the clipped data from the register structure 1006. The track array 1005 and the shift register structure 1006 are fixed in place relative to each other. However, the data in the shift register array 1006 is shifted in a strategic and coordinated manner to cause each execution track in the execution track array to process a different template in the data. Therefore, each execution path determines the output image value for a different pixel in the output clip generated. From the structure of FIG. 10, it should be understood that the overlapping templates are not only configured vertically but also horizontally, because the execution track array 1005 includes vertically adjacent execution tracks and horizontally adjacent execution tracks. Some significant architectural features of the data operation unit 1001 include a shift register structure 1006 having a size wider than the execution track array 1005. That is, one of the registers "halo" 1009 exists outside the track array 1005. Although the halo 1009 is shown to be present on both sides of the execution track array, depending on the implementation, the halo may be present on fewer (one) or more (three or four) sides of the execution track array 1005 . When the data is shifted "below" on the execution track 1005, the halo 1009 is used to provide "overflow" space for data that overflows outside the boundary of the execution track array 1005. As a simple case, when a 5 × 5 template centered on the right edge of the execution track array 1005 is processed, when the leftmost pixel of the template is processed, the four halo register positions further to the right will be needed. For ease of drawing, when a register on either side (right side, bottom side) will have both a horizontal connection and a vertical connection in a nominal embodiment, FIG. 10 shows the register on the right side of the halo. The register with only horizontal shift connection and the bottom side of the halo is shown as having only vertical shift connection. In various embodiments, the halo region does not include corresponding execution track logic (eg, no ALU exists) to execute the image processing instructions. However, the individual memory access units (M) exist in each of the positions of the halo area, so that the individual halo register locations can individually load data from the memory and store the data into the memory. Provide extra spillover space through random access memory 1007_1 to 1007_R coupled to each row and / or row or part of the array (e.g., a random access memory can be assigned to span 4 execution lanes and 2 An execution area is an "area" of the execution area array. For simplicity, the rest of this application will mainly refer to the row and / or row-based allocation scheme.) Here, if the core operation of an execution area requires its When processing pixel values outside the two-dimensional shift register array 1006 (some image processing routines may require this), the image data plane can further overflow from the halo area 1009 into the random access memory 1007, for example. For example, consider a 6 × 6 template in which the hardware includes a halo area of only one of the four storage elements on the right side of one execution track on the right edge of the execution track array. In this case, the data will need to be further shifted to the right of the right edge of halo 1009 to fully process the template. The data shifted outside the halo area 1009 will then overflow to the random access memory 1007. The following further provides other applications of the random access memory 1007 and the template processor of FIG. 9. 11a to 11k show a working example of a method for shifting image data in the two-dimensional shift register array "below" the track array as mentioned above. As viewed in FIG. 11 a, the data content of the two-dimensional shift array is depicted in a first array 1107 and the execution track array is depicted by a frame 1105. Moreover, two adjacent execution lanes 1110 in the execution lane array are simply depicted. In this simplified depiction 1110, each execution path includes a self-shifting register to accept data, to receive data from an ALU output (e.g., represented as an accumulator across a loop) or to write output data to an output destination One of the ground registers R1. Each execution track can also obtain its "below" content in the two-dimensional shift array in a local register R2. Therefore, R1 is a physical register of the execution path, and R2 is a physical register of a two-dimensional shift register array. The execution path contains one of the ALUs that can operate on the operands provided by R1 and / or R2. As will be described in further detail below, in one embodiment, the shift register is actually implemented using multiple memory / register elements (one "depth") per array location, but the shifting activity is limited One plane of the storage element (eg, only one plane of the storage element can be shifted per cycle). Figures 11a to 11k depict one of these deeper register locations as used to store the results X from the respective execution lanes. For ease of illustration, the deeper register is drawn side-by-side below its corresponding register R2 instead of below its corresponding register R2. 11a to 11k focus on the calculation of two templates, and the center positions of the two templates are aligned with the pair of execution track positions 1111 depicted in the execution track array. For ease of illustration, the pair of execution lanes 1110 are drawn as horizontal neighbors. In fact, they are vertical neighbors according to the following example. As first observed in Figure 11a, the execution path is centered on its central template position. Figure 11b shows the object code executed by two execution lanes. As viewed in FIG. 11b, the code of the two execution paths causes the data in the shift register array to be shifted down by one position and shifted to the right by one position. This aligns the two execution tracks to the upper left corner of their respective template. The code then causes the data located in (R2) their respective locations to be loaded into R1. As viewed in Figure 11c, the code then causes the pair of execution lanes to shift the data in the shift register array to the left by one unit, which results in shifting the values to the right of the respective positions of the execution lanes to the execution lanes. The location of the road. The value in R1 (previous value) is then added to the new value in the position (in R2) that has been shifted to the track. The result is written into R1. As observed in FIG. 11d, the same procedure as described above for FIG. 11c is repeated, which results in the result R1 now including the value A + B + C in the upper execution lane and F + G + H in the lower execution lane. At this point, the two execution lanes have been processed above their respective templates. It should be noted that spillover to a halo area on the left side of the execution track array (if a halo area exists on the left-hand side) or to random access memory (if a halo area does not exist on the execution track array On the left hand side). As viewed in Figure 11e, the code then causes the data in the shift register array to shift up by one unit, which causes the two execution lanes to align with the right edge of the middle row of their respective templates. The register R1 of the two execution paths currently contains the sum of the rightmost values of the top and middle columns of the template. Figures 11f and 11g show the continuous progress of moving left across the middle column of the template of the two execution lanes. Cumulative addition continues so that at the end of the processing of FIG. 11g, the two execution lanes include the sum of the values of the top and middle columns of their respective templates. FIG. 11h shows another shift to align each execution track with the bottom of its corresponding template. Figures 11i and 11j show successive shifts to complete the processing of the template for the two execution lanes. Figure 11k shows aligning each execution track with its correct position in the data array and writing the results to its extra shift. In the examples of FIG. 11a to FIG. 11k, it should be noted that the purpose code used for the shift operation may include an instruction format identifying the direction and magnitude of the shift expressed in the (X, Y) coordinates. For example, the destination code used to shift up one position can be expressed as SHIFT 0, + 1 in the destination code. As another example, shifting one position to the right can be expressed as SHIFT +1,0 in the destination code. In various embodiments, a shift with a larger magnitude can also be specified in the destination code (for example, SHIFT 0, + 2). Here, if the 2D shift register hardware only supports one position shift per cycle, the instructions can be interpreted by the machine as requiring multiple cycles to execute, or the 2D shift register hardware can be designed to support More than one position shift per cycle. The latter embodiment is described in further detail below. Figure 12 shows another more detailed depiction of a unit cell for an execution track and corresponding shift register structure (in various embodiments, the register in the halo area does not include a corresponding execution track but includes a memory Body unit). In one embodiment, the execution track and register space associated with each position in the execution track array are implemented by exemplifying the circuit observed in FIG. 12 at each node of the execution track array. As seen in FIG. 12, the unit cell includes an execution path 1201 coupled to one of the register files 1202 composed of four registers R2 to R5. During any cycle, the execution path 1201 can read from or write to any of the registers R1 to R5 to any of the registers R1 to R5. For instructions that require two input operands, the execution path can retrieve two operands from any of R1 to R5. In one embodiment, by allowing the contents of any one (only) of the registers R2 to R4 during a single cycle, the output multiplexer 1203 is used to move "out" to the temporary storage of its neighbors. One of the register files and replaces the content of any one of (only) one of the registers R2 to R4 with the content of "in" from one of its neighbors through the input multiplexer 1204, so that The two-dimensional shift register structure is implemented in the same direction (for example, all execution lanes are shifted to the left, all execution lanes are shifted to the right, etc.). Although it may be common for an identical register to move its content out and replace it with content moved in the same cycle, multiplexer configurations 1203, 1204 allow different movements in the same register file during one of the same cycle Bit source and shift destination registers. As depicted in Figure 12, it should be noted that during a shift sequence, an execution track will move content from its register file 1202 to each of its left, right, top, and bottom neighbors. In combination with the same shift sequence, the execution path will also move content from one of its left, right, top, and bottom neighbors into its register file. Furthermore, the moving target and moving source should be consistent with the same moving direction of one of the execution paths (for example, if moving to the right neighbor, moving into the left neighbor). Although in one embodiment only the contents of one register are allowed to be shifted per cycle per execution, other embodiments may allow the contents of more than one register to be moved in / out. For example, if the second example of one of the multiplexer circuits 1203, 1204 observed in FIG. 12 is incorporated into the design of FIG. 12, the contents of the two registers can be moved in / out during the same cycle. Of course, in an embodiment where only one register is allowed to be shifted per cycle, multiple clock cycles may occur between mathematical operations by consuming more clock cycles for shifting between mathematical operations. Register shift (for example, the contents of two registers are shifted between mathematical operations by consuming two shift operations between mathematical operations). If less than all the contents of the scratchpad file of an execution track are removed during a shift sequence, it should be noted that the contents of the execution track's non-removed scratchpad remain in place (not shifted). Thus, any unshifted content that has not been replaced with shifted content remains at the end of the execution path across the shift loop. The memory unit ("M") observed in each execution lane is used to load / store data to / from the random access memory space associated with the rows and / or rows of execution lanes within the execution lane array. Random access memory space. Here, the M unit serves as a standard M unit, which is generally used to load / store data that cannot be loaded / stored into the own register space of the execution track. In various embodiments, the main operation of the M unit is to write data from a local register to the memory and read data from the memory and write it to a local register. Regarding the ISA operation codes supported by the ALU unit of the hardware execution path 1201, in various embodiments, the mathematical operation codes supported by the hardware ALU include (for example, ADD, SUB, MOV, MUL, MAD, ABS, DIV, SHL, SHR, MIN / MAX, SEL, AND, OR, XOR, NOT). As described above, the memory access instruction can be executed by executing lane 1201 to retrieve data from its associated random access memory / store data to its associated random access memory. In addition, the hardware execution path 1201 supports shift operation instructions (right, left, up, down) to shift data in the two-dimensional shift register structure. As described above, the program control instructions are executed to a greater extent by the scalar processor of the template processor.4.0 实施 例 例 Implementation Examples It should be noted that the various image processor architecture features described above need not be limited to image processing in the traditional sense and may therefore be applied to other applications that may (or may not) cause re-characterization of the image processor. For example, if any of the various image processor architecture features described above are to be used to create and / or generate and / or render animation (as opposed to processing actual camera images), the image processor may be characterized as a graphic Processing unit. In addition, the image processor architecture features described above can be applied to other technical applications, such as video processing, visual processing, image recognition, and / or machine learning. Applied in this manner, the image processor may be integrated (e.g., as a co-processor) with a more general purpose processor (e.g., it is a CPU or part of a computing system) or may be an independent processor within a computing system . The hardware design embodiments discussed above may be embodied in a semiconductor wafer and / or described as a circuit design with a semiconductor process as the ultimate goal. In the latter case, these circuit descriptions may use a (e.g., VHDL or Verilog) register transfer level (RTL) circuit description, a gate level circuit description, a transistor level circuit description or mask description, or Various combinations. The circuit description is typically embodied on a computer-readable storage medium, such as a CD-ROM or other type of storage technology. It should be recognized from the foregoing chapters that an image processor as described above may be embodied in hardware on a computer system (e.g., a system-on-chip (SOC) of a handheld device as one of the processing data from a camera of a handheld device). section). In the case where the image processor is embodied as a hardware circuit, it should be noted that the image data processed by the image processor can be directly received from a camera. Here, the image processor may be part of a discrete camera or part of a computing system with an integrated camera. In the latter case, the image data can be received directly from the camera or the system memory of the computing system (for example, the camera sends its image data to the system memory instead of the image processor). It should also be noted that many of the features described in the preceding sections can be applied to a graphics processor unit (which presents animation). FIG. 13 provides an exemplary depiction of a computing system. Many of the components of the computing system described below can be applied to a computing system (eg, a handheld device such as a smartphone or tablet) with an integrated camera and associated image processor. The average skilled person will be able to easily distinguish between the two. In addition, the computing system of FIG. 13 also includes many features of a high-performance computing system, such as a workstation or a supercomputer. As seen in FIG. 13, the basic computing system may include a central processing unit 1301 (which may include, for example, a plurality of general-purpose processing cores 1315_1 to 1315_N and a main memory controller disposed on a multi-core processor or an application processor). 1317), system memory 1302, a display 1303 (such as a touch screen, a tablet), a local wired point-to-point link (such as USB) interface 1304, various network I / O functions 1305 (such as an Ethernet interface and And / or cellular modem subsystem), a wireless local area network (e.g., WiFi) interface 1306, a wireless point-to-point link (e.g., Bluetooth) interface 1307, and a global positioning system interface 1308, various sensors 1309_1 to 1309_N, one or more cameras 1310, a battery 1311, a power management control unit 1312, a speaker and microphone 1313, and an audio encoder / decoder 1314. An application processor or multi-core processor 1350 may include one or more general-purpose processing cores 1315, one or more graphics processing units 1316, and a memory management function 1317 (e.g., a memory controller) within its CPU 1201. An I / O control function 1318 and an image processing unit 1319. The general processing core 1315 usually executes an operating system and application software of a computing system. The graphics processing unit 1316 typically performs graphics-intensive functions to generate, for example, graphics information presented on the display 1303. The memory control function 1317 interfaces with the system memory 1302 to write data to the system memory 1302 and read data from the system memory 1302. The power management control unit 1312 generally controls the power consumption of the system 1300. The image processing unit 1319 may be implemented according to any one of the image processing unit embodiments described in detail in the preceding sections above. Alternatively or in combination, the IPU 1319 may be coupled to either or both of the GPU 1316 and the CPU 1301 as one of its co-processors. Additionally, in various embodiments, GPU 1316 may be implemented using any of the image processor features described in detail above. The image processing unit 1319 can be configured with application software as described in detail above. In addition, an operating system (such as the operating system of FIG. 13) can execute the code that simulates an image processing application software program as described above, so that the respective memory allocations for the respective line buffer units can be determined. Each of the touch screen display 1303, the communication interfaces 1304 to 1307, the GPS interface 1308, the sensor 1309, the camera 1310, and the speaker / microphone codec 1313, 1314 can be considered as the entire computing system (also appropriately included Various forms of I / O (input and / or output) that integrate peripheral devices (eg, one or more cameras 1310). Depending on the implementation, each of these I / O components may be integrated on the application processor / multi-core processor 1350 or may be located off-die or external to the application processor / multi-core processor 1350 package. In one embodiment, the one or more cameras 1310 include a depth camera capable of measuring the depth between the camera and an object in its field of view. Application software, operating system software, device driver software, and / or firmware running on a general-purpose CPU core (or other functional block with an instruction execution pipeline to execute code) of an application processor or other processor The body can perform any of the functions described above. Embodiments of the invention may include various procedures as stated above. Programs can be embodied as machine-executable instructions. Instructions can be used to cause a general-purpose or special-purpose processor to execute certain programs. Alternatively, such procedures may be performed by specific hardware components containing hard-wired and / or programmable logic for executing the procedures or by any combination of programmed computer components and custom hardware components. Elements of the present invention may also be provided as a machine-readable medium for storing machine-executable instructions. Machine-readable media may include, but is not limited to, floppy disks, optical disks, CD-ROMs and magneto-optical disks, flash memory, ROM, RAM, EPROM, EEPROM, magnetic or optical cards, propagation media, or other suitable for storing electronic instructions Type of media / machine-readable media. For example, the present invention can be downloaded as a computer program which can be transmitted from a remote computer via a communication link (for example, a modem or network connection) by a data signal embodied in a carrier wave or other propagation medium. (E.g., a server) to a requesting computer (e.g., a client). In the foregoing specification, the invention has been described with reference to specific exemplary embodiments thereof. However, it will be apparent that various modifications and changes may be made thereto without departing from the broader spirit and scope of the invention as set forth in the scope of the appended invention application patent. Therefore, the description and drawings should be regarded as illustrative and not restrictive. Some examples are described below. Example 1: A machine-readable storage medium containing code that causes the computing system to execute a method when processed by the computing system, the method includes: a) simulating the execution of an image processing application software program, the simulation includes Kernel-to-kernel communication is intercepted using simulated bank buffer memory, which stores and forwards a number of image data rows transmitted from the model of the production kernel to the model that consumes the kernel. The simulation further includes a simulation Track the respective amount of image data stored in the respective line buffer memory at runtime; and, b) determine the respective hardware memory for the corresponding line buffer memory from the tracked respective amount of image data Allocating; c) generating configuration information for an image processor to execute the image processing application software program, the configuration information describing the hardware memory of the hardware line buffer memory for the image processor distribution. Example 2: The machine-readable storage medium of Example 1, wherein the tracking further includes tracking a difference between an analog line buffer memory write index and an analog line buffer memory read index. Example 3: The machine-readable storage medium of Example 1 or 2, wherein the determination is based on a maximum observed difference between the analog line buffer memory write index and the analog line buffer memory read index. Example 4: The machine-readable storage medium of at least one of the preceding examples, wherein the simulation further includes implementing a write policy that prevents a unit under the image data from being written to an analog line buffer memory Until one or more models that consume the image data are waiting for the next unit to receive the image data. Example 5: The machine-readable storage medium of at least one of the preceding examples, wherein the write policy is enforced at a model of a production core of the next unit that generates the image data. Example 6: The machine-readable storage medium of at least one of the preceding examples, wherein the method further includes allowing the write policy to be violated if the simulation of the application software program is deadlocked. Example 7: The machine-readable storage medium of at least one of the preceding examples, wherein the cores are used to operate on different processing cores of a hardware image processor, the hardware image processor includes Hardware row buffer units for row groups passed between these processing cores. Example 8: The machine-readable storage medium of at least one of the preceding examples, wherein the different processing cores include a two-dimensional execution track and a two-dimensional shift register array. Example 9: The machine-readable storage medium of at least one of the preceding examples, wherein the models of the production cores and the models of the consumption cores include instructions to send image data to a simulated line buffer memory and It includes instructions for reading image data from an analog line buffer memory, but does not include instructions for substantially processing image data. Example 10: The machine-readable storage medium of at least one of the preceding examples, wherein an image processor architecture includes an execution track array coupled to one of a two-dimensional shift register array. Example 11: The machine-readable storage medium of at least one of the preceding examples, wherein the architecture of the image processor includes at least one of a line of buffers, a slice generator, and / or a template processor. Example 12: The machine-readable storage medium of Example 11, wherein the template processor is configured to process overlapping templates. Example 13: The machine-readable storage medium of at least one of the preceding examples, wherein a data operation unit includes a shift register structure having a size wider than the execution track array, specifically outside the execution track array There is a register. Example 14: A computing system including: a central processing unit; a system memory; a system memory controller between the system memory and the central processing unit; a machine-readable storage medium, the The machine-readable storage medium contains code that, when processed by the computing system, causes the computing system to execute a method, the method including: a) simulating the execution of an image processing application software program, the simulation including using a simulated line buffer The memory intercepts the kernel-to-kernel communication. The simulated bank buffer memory stores and forwards some image data rows transmitted from the model of the production kernel to the model of the consuming kernel. The simulation further includes tracking and storing in a simulation runtime The respective amount of image data in the respective line buffer memory; and, b) determining the respective hardware memory allocation for the corresponding hardware line buffer memory from the tracked respective amount of image data; c) generation The configuration information of an image processor is used to execute the image processing application software program. The configuration information describes the image processing application software. Hardware allocation of the hardware like buffer memory of the processor. Example 15: The computing system of Example 14, wherein the tracking further includes tracking a difference between an analog line buffer memory write indicator and an analog line buffer memory read indicator. Example 16: The computing system of Example 14 or 15, wherein the determination is based on a maximum observed difference between the analog line buffer memory write index and the analog line buffer memory read index. Example 17: The computing system of at least one of Examples 14 to 16, wherein the simulation further includes implementing a write policy that prevents the next unit of image data from being written to an analog line buffer memory until One or more models that consume the image data are waiting for the next unit to receive the image data. Example 18: The computing system of at least one of Examples 14 to 17, wherein the write policy is enforced at a model of a production core of the next unit that generates image data. Example 19: The computing system of at least one of Examples 14 to 18, wherein the method further includes allowing the write policy to be violated if the simulation of the application software program is deadlocked. Example 20: The computing system of at least one of Examples 14 to 19, wherein an image processor architecture includes an execution track array coupled to one of the two-dimensional shift register arrays. Example 21: The computing system of at least one of Examples 14 to 20, wherein the architecture of the image processor includes at least one of a line buffer, a slice generator, and / or a template processor. Example 22: The computing system of Example 21, wherein the template processor is configured to process overlapping templates. Example 23: The computing system according to at least one of Examples 14 to 22, wherein a data operation unit includes a shift register structure having a size wider than the execution track array, and specifically exists outside the execution track array Register. Example 24: A method comprising: a) simulating the execution of an image processing application software program, the simulating includes intercepting kernel-to-kernel communication using simulated bank buffer memory, and storing and forwarding such simulated bank buffer memory From the model of the production kernel to the number of image data rows of the model consuming the kernel, the simulation further includes tracking the amount of respective image data stored in the respective line buffer memory during a simulation run time; and, b) from these The traced respective image data amounts are used to determine respective hardware memory allocations corresponding to the hardware line buffer memory; c) generating configuration information for an image processor to execute the image processing application software program, the group The state information describes the hardware memory allocation for the hardware line buffer memory of the image processor. Example 25: The method of Example 24, wherein the tracking further comprises tracking a difference between an analog line buffer memory write indicator and an analog line buffer memory read indicator. Example 26: The method of Example 24 or 25, wherein the determination is based on a maximum observed difference between the analog line buffer memory write index and the analog line buffer memory read index. Example 27: The method of at least one of Examples 24 to 26, wherein the simulation further includes implementing a write policy that prevents a unit below the image data from being written to an analog line buffer memory until consumed One or more models of the kernel of the image data are waiting for the next unit to receive the image data. Example 28: The method of at least one of Examples 24 to 27, wherein the writing policy is enforced at a model of a production core of the next unit that generates the image data. Example 29: The method of at least one of Examples 24 to 28, wherein an image processor architecture includes an execution track array coupled to one of a two-dimensional shift register array. Example 30: The method of at least one of Examples 24 to 29, wherein the architecture of the image processor includes at least one of a line buffer, a slice generator, and / or a template processor. Example 31. The method of at least one of Examples 24 to 30, wherein the template processor is configured to process overlapping templates. Example 32: The method of at least one of Examples 24 to 31, wherein a data operation unit includes a shift register structure having a size wider than the execution track array, in particular, there is a temporary register outside the execution track array Memory.
100‧‧‧影像處理器架構/處理器100‧‧‧Image Processor Architecture / Processor
101‧‧‧執行道陣列101‧‧‧ Execution Road Array
102‧‧‧二維移位暫存器陣列/移位暫存器陣列102‧‧‧Two-dimensional shift register array / shift register array
103‧‧‧邊角執行道103‧‧‧ Corner Execution Road
104‧‧‧邊角移位暫存器位置104‧‧‧ Corner shift register position
105‧‧‧邊角執行道105‧‧‧ Corner Execution Road
106‧‧‧邊角移位暫存器位置106‧‧‧ Corner shift register position
200‧‧‧架構200‧‧‧ architecture
201_1‧‧‧行緩衝器單元201_1‧‧‧line buffer unit
201_2‧‧‧行緩衝器單元/下游行緩衝器單元201_2‧‧‧line buffer unit / down parade buffer unit
201_M‧‧‧行緩衝器單元201_M‧‧‧line buffer unit
202_1‧‧‧模板處理器/模板處理器單元/第一模板處理器202_1‧‧‧template processor / template processor unit / first template processor
202_2‧‧‧模板處理器/模板處理器單元/第二模板處理器202_2‧‧‧Template processor / Template processor unit / Second template processor
202_N‧‧‧模板處理器/模板處理器單元202_N‧‧‧Template Processor / Template Processor Unit
203_1‧‧‧截片產生器單元/截片產生器203_1‧‧‧Clip generator unit / Clip generator
203_2‧‧‧截片產生器單元/截片產生器203_2‧‧‧Clip generator unit / Clip generator
203_N‧‧‧截片產生器單元203_N‧‧‧Clip generator unit
204‧‧‧網路204‧‧‧Internet
205‧‧‧巨集輸入/輸出(I/O)單元205‧‧‧Macro input / output (I / O) unit
300‧‧‧應用軟體程式300‧‧‧ Application Software Program
301‧‧‧輸入影像資料301‧‧‧Enter image data
304_1‧‧‧行緩衝器單元/行緩衝器304_1‧‧‧line buffer unit / line buffer
304_2‧‧‧行緩衝器單元/行緩衝器304_2‧‧‧line buffer unit / line buffer
304_3‧‧‧行緩衝器單元304_3‧‧‧row buffer unit
304_4‧‧‧行緩衝器單元304_4‧‧‧line buffer unit
401_1‧‧‧迴路401_1‧‧‧loop
401_2‧‧‧迴路401_2‧‧‧loop
401_3‧‧‧迴路401_3‧‧‧loop
401_4‧‧‧迴路401_4‧‧‧loop
600‧‧‧影像區域600‧‧‧Image area
601‧‧‧第一群組之相同寬度影像行/群組601‧‧‧The same width image line / group of the first group
602‧‧‧第二群組之相同寬度影像行/群組602‧‧‧Second group of image rows / groups of the same width
611‧‧‧較小高度全寬行群組/全行群組611‧‧‧Small height full width row group / full row group
612_1‧‧‧較小寬度行群組/第一較小行寬行群組612_1‧‧‧Small width row group / First smaller width row group
612_2‧‧‧較小寬度行群組/第二較小行寬行群組612_2‧‧‧Small width row group / second smaller width row group
701‧‧‧模擬701‧‧‧simulation
702‧‧‧攔截/模板處理器702‧‧‧Intercept / Template Processor
703‧‧‧追蹤703‧‧‧Follow
704‧‧‧判定704‧‧‧Judgment
801‧‧‧影像資料801‧‧‧Image data
802‧‧‧重疊模板802‧‧‧ Overlay Template
803‧‧‧陰影區域/行群組803‧‧‧Shadow area / row group
804‧‧‧初始截片/第一截片804‧‧‧Initial Cut / First Cut
805‧‧‧截片/下一截片/新截片/第二截片805‧‧‧Cross section / Next section / New section / Second section
900‧‧‧模板處理器架構900‧‧‧ template processor architecture
901‧‧‧資料運算單元901‧‧‧Data Operation Unit
902‧‧‧純量處理器902‧‧‧ scalar processor
903‧‧‧記憶體/純量記憶體903‧‧‧Memory / scalar memory
904‧‧‧輸入/輸出(I/O)單元904‧‧‧input / output (I / O) unit
905‧‧‧執行道陣列/陣列905‧‧‧Execution track array / array
906‧‧‧二維移位陣列結構/二維移位暫存器結構/暫存器結構906‧‧‧Two-dimensional shift array structure / Two-dimensional shift register structure / register structure
907_1至907_R‧‧‧隨機存取記憶體907_1 to 907_R‧‧‧ Random access memory
909‧‧‧程式控制器909‧‧‧Program Controller
951‧‧‧純量指令/指令951‧‧‧ scalar instruction / instruction
952‧‧‧ALU指令/指令/指令欄位952‧‧‧ALU Instruction / Instruction / Instruction Field
953‧‧‧記憶體指令/指令/指令欄位953‧‧‧Memory Command / Command / Command Field
954‧‧‧欄位954‧‧‧field
1001‧‧‧資料運算組件1001‧‧‧Data Computing Unit
1005‧‧‧執行道陣列/執行道1005‧‧‧Execution track array / Execution track
1006‧‧‧二維移位暫存器陣列結構/二維移位暫存器/暫存器結構/移位暫存器結構/移位暫存器陣列1006‧‧‧Two-dimensional shift register array structure / Two-dimensional shift register / register structure / shift register structure / shift register array
1007_1至1007R‧‧‧隨機存取記憶體1007_1 to 1007R‧‧‧ Random Access Memory
1009‧‧‧光暈/光暈區域1009‧‧‧Halo / Halo Area
1105‧‧‧圖框1105‧‧‧Frame
1107‧‧‧第一陣列1107‧‧‧First Array
1110‧‧‧執行道1110‧‧‧ Execution Road
1111‧‧‧執行道位置對1111‧‧‧ the location of the execution lane
1201‧‧‧執行道/硬體執行道1201‧‧‧ Execution Road / Hardware Execution Road
1202‧‧‧暫存器檔案1202‧‧‧Temporary File
1203‧‧‧輸出多工器/多工器配置/多工器電路1203‧‧‧Output Multiplexer / Multiplexer Configuration / Multiplexer Circuit
1204‧‧‧輸入多工器/多工器配置/多工器電路1204‧‧‧Input Multiplexer / Multiplexer Configuration / Multiplexer Circuit
1300‧‧‧系統1300‧‧‧ system
1301‧‧‧中央處理器單元(CPU)1301‧‧‧ Central Processing Unit (CPU)
1302‧‧‧系統記憶體1302‧‧‧System memory
1303‧‧‧顯示器/觸控螢幕顯示器1303‧‧‧Display / Touch screen display
1304‧‧‧局域有線點對點鏈路介面/通信介面1304‧‧‧Local wired point-to-point link interface / communication interface
1305‧‧‧網路輸入/輸出(I/O)功能/通信介面1305‧‧‧Network input / output (I / O) function / communication interface
1306‧‧‧無線區域網路介面/通信介面1306‧‧‧Wireless LAN Interface / Communication Interface
1307‧‧‧無線點對點鏈路介面/通信介面1307‧‧‧Wireless point-to-point link interface / communication interface
1308‧‧‧全球定位系統(GPS)介面1308‧‧‧Global Positioning System (GPS) interface
1309_1至1309_N‧‧‧感測器1309_1 to 1309_N‧‧‧ sensors
1310‧‧‧相機1310‧‧‧ Camera
1311‧‧‧電池1311‧‧‧ Battery
1312‧‧‧功率管理控制單元1312‧‧‧Power Management Control Unit
1313‧‧‧揚聲器及麥克風/揚聲器及麥克風編解碼器1313‧‧‧Speaker and Microphone / Speaker and Microphone Codec
1314‧‧‧音訊編碼器/音訊解碼器/揚聲器及麥克風編解碼器1314‧‧‧Audio Encoder / Audio Decoder / Speaker and Microphone Codec
1315_1至1315_N‧‧‧通用處理核心1315_1 to 1315_N‧‧‧General Processing Core
1316‧‧‧圖形處理單元(GPU)1316‧‧‧Graphics Processing Unit (GPU)
1317‧‧‧主記憶體控制器/記憶體管理功能/記憶體控制功能1317‧‧‧Main memory controller / memory management function / memory control function
1318‧‧‧輸入/輸出(I/O)控制功能1318‧‧‧Input / output (I / O) control function
1319‧‧‧影像處理單元(IPU)1319‧‧‧Image Processing Unit (IPU)
1350‧‧‧應用處理器或多核心處理器1350‧‧‧Application processor or multi-core processor
K1‧‧‧內核/模擬模型內核K1‧‧‧kernel / simulation model kernel
K2‧‧‧內核/模擬模型內核K2‧‧‧ kernel / simulation model kernel
K3‧‧‧內核/模擬模型內核K3‧‧‧kernel / simulation model kernel
K4‧‧‧內核/模擬模型內核K4‧‧‧kernel / simulation model kernel
LD_1‧‧‧載入指令LD_1‧‧‧Load instruction
R1‧‧‧暫存器R1‧‧‧Register
R2‧‧‧局部暫存器/暫存器R2‧‧‧Local Register / Register
R3‧‧‧暫存器R3‧‧‧Register
R4‧‧‧暫存器R4‧‧‧Register
R5‧‧‧暫存器R5‧‧‧Register
ST_1‧‧‧儲存指令ST_1‧‧‧Save instruction
ST_2‧‧‧儲存指令ST_2‧‧‧Save instruction
以下描述及隨附圖式係用於繪示本發明之實施例。在圖式中： 圖1展示一模板處理器架構之一高階視圖； 圖2展示一影像處理器架構之一更詳細視圖； 圖3展示可藉由一影像處理器執行之一應用軟體程式； 圖4展示複數個內核模型； 圖5a及圖5b展示一行緩衝器單元模型之寫入指標及讀取指標行為； 圖6a、圖6b、圖6c、圖6d及圖6e展示用於區塊影像傳送之全行群組傳送模式、實際上高傳送模式及讀取指標行為； 圖7展示用於判定按行緩衝器單元記憶體分配之一方法； 圖8a、圖8b、圖8c、圖8d及圖8e描繪將影像資料剖析成一行群組、將一行群組剖析成一截片及運用重疊模板對一截片上執行之操作； 圖9a展示一模板處理器之一實施例； 圖9b展示該模板處理器之一指令字組之一實施例； 圖10展示一模板處理器內之一資料運算單元之一實施例； 圖11a、圖11b、圖11c、圖11d、圖11e、圖11f、圖11g、圖11h、圖11i、圖11j及圖11k描繪使用一個二維移位陣列及一執行道陣列來判定具有重疊模板之一對相鄰輸出像素值之一實例； 圖12展示用於一整合式執行道陣列及二維移位陣列之一單位晶格之一實施例； 圖13展示一影像處理器之另一實施例。The following description and accompanying drawings are used to illustrate embodiments of the present invention. In the drawings: FIG. 1 shows a high-level view of a template processor architecture; FIG. 2 shows a more detailed view of an image processor architecture; FIG. 3 shows an application software program executable by an image processor; 4 shows a plurality of kernel models; Figures 5a and 5b show the behavior of the write index and read index of a row of buffer unit models; Figures 6a, 6b, 6c, 6d, and 6e show the block image transmission Full row group transfer mode, actually high transfer mode, and read index behavior; Figure 7 shows one method for determining the memory allocation of the buffer unit per line; Figure 8a, Figure 8b, Figure 8c, Figure 8d, and Figure 8e Describes the operations of parsing image data into a row group, parsing a row group into a slice, and using overlapping templates to perform operations on a slice; Figure 9a shows an embodiment of a template processor; Figure 9b shows the template processor An embodiment of an instruction block; Figure 10 shows an embodiment of a data operation unit in a template processor; Figures 11a, 11b, 11c, 11d, 11e, 11f, 11g, 11h , Figure 11i, Figure 11 j and FIG. 11k depict an example of using a two-dimensional shift array and an execution track array to determine an adjacent pair of output pixel values with overlapping templates; FIG. 12 shows an integrated execution track array and two-dimensional shift An embodiment of a unit lattice of an array; FIG. 13 shows another embodiment of an image processor.
Claims (32)
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/594,512 US10430919B2 (en) | 2017-05-12 | 2017-05-12 | Determination of per line buffer unit memory allocation |
US15/594,512 | 2017-05-12 |
Publications (2)
Publication Number | Publication Date |
---|---|
TW201907298A true TW201907298A (en) | 2019-02-16 |
TWI684132B TWI684132B (en) | 2020-02-01 |
Family
ID=61599563
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
TW107103560A TWI684132B (en) | 2017-05-12 | 2018-02-01 | Determination of per line buffer unit memory allocation |
TW108147270A TWI750557B (en) | 2017-05-12 | 2018-02-01 | Determination of per line buffer unit memory allocation |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
TW108147270A TWI750557B (en) | 2017-05-12 | 2018-02-01 | Determination of per line buffer unit memory allocation |
Country Status (7)
Country | Link |
---|---|
US (2) | US10430919B2 (en) |
EP (1) | EP3622399B1 (en) |
JP (1) | JP7208920B2 (en) |
KR (1) | KR102279120B1 (en) |
CN (1) | CN110574011B (en) |
TW (2) | TWI684132B (en) |
WO (1) | WO2018208334A1 (en) |
Families Citing this family (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10387988B2 (en) * | 2016-02-26 | 2019-08-20 | Google Llc | Compiler techniques for mapping program code to a high performance, power efficient, programmable image processing hardware platform |
US10489878B2 (en) * | 2017-05-15 | 2019-11-26 | Google Llc | Configurable and programmable image processor unit |
US10534639B2 (en) * | 2017-07-06 | 2020-01-14 | Bitfusion.io, Inc. | Virtualization of multiple coprocessors |
CN110706147B (en) * | 2019-09-29 | 2023-08-11 | 阿波罗智联(北京)科技有限公司 | Image processing environment determination method, device, electronic equipment and storage medium |
US11093400B2 (en) * | 2019-10-15 | 2021-08-17 | Sling Media Pvt. Ltd. | Lock-free sharing of live-recorded circular buffer resources |
CN114168524B (en) * | 2021-12-07 | 2023-10-20 | 平头哥(上海)半导体技术有限公司 | Line cache unit, acceleration unit, system on chip and line cache configuration method |
CN114333930B (en) * | 2021-12-23 | 2024-03-08 | 合肥兆芯电子有限公司 | Multi-channel memory storage device, control circuit unit and data reading method thereof |
Family Cites Families (16)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5398079A (en) * | 1993-01-27 | 1995-03-14 | General Instrument Corporation | Half-pixel interpolation for a motion compensated digital video system |
US7499960B2 (en) | 2001-10-01 | 2009-03-03 | Oracle International Corporation | Adaptive memory allocation |
KR100865811B1 (en) | 2004-05-14 | 2008-10-28 | 엔비디아 코포레이션 | Low power programmable processor |
US7331037B2 (en) | 2004-08-12 | 2008-02-12 | National Instruments Corporation | Static memory allocation in a graphical programming system |
US8024549B2 (en) * | 2005-03-04 | 2011-09-20 | Mtekvision Co., Ltd. | Two-dimensional processor array of processing elements |
US7818725B1 (en) | 2005-04-28 | 2010-10-19 | Massachusetts Institute Of Technology | Mapping communication in a parallel processing environment |
JP4923602B2 (en) | 2006-02-10 | 2012-04-25 | 富士ゼロックス株式会社 | Image formation processing simulation apparatus and image formation processing simulation method |
US7890314B2 (en) | 2007-12-05 | 2011-02-15 | Seagate Technology Llc | Method for modeling performance of embedded processors having combined cache and memory hierarchy |
US20110191758A1 (en) | 2010-01-29 | 2011-08-04 | Michael Scharf | Optimized Memory Allocator By Analyzing Runtime Statistics |
US9335977B2 (en) | 2011-07-28 | 2016-05-10 | National Instruments Corporation | Optimization of a data flow program based on access pattern information |
US9256915B2 (en) * | 2012-01-27 | 2016-02-09 | Qualcomm Incorporated | Graphics processing unit buffer management |
US20150055861A1 (en) * | 2013-08-23 | 2015-02-26 | Amlogic Co., Ltd | Methods and Systems for Image Demosaicing |
US10055342B2 (en) | 2014-03-19 | 2018-08-21 | Qualcomm Incorporated | Hardware-based atomic operations for supporting inter-task communication |
US9756268B2 (en) | 2015-04-23 | 2017-09-05 | Google Inc. | Line buffer unit for image processor |
US10095479B2 (en) * | 2015-04-23 | 2018-10-09 | Google Llc | Virtual image processor instruction set architecture (ISA) and memory model and exemplary target hardware having a two-dimensional shift array structure |
US11016742B2 (en) | 2015-06-24 | 2021-05-25 | Altera Corporation | Channel sizing for inter-kernel communication |
-
2017
- 2017-05-12 US US15/594,512 patent/US10430919B2/en active Active
-
2018
- 2018-01-09 KR KR1020197032090A patent/KR102279120B1/en active IP Right Grant
- 2018-01-09 EP EP18709813.2A patent/EP3622399B1/en active Active
- 2018-01-09 CN CN201880028856.6A patent/CN110574011B/en active Active
- 2018-01-09 JP JP2019559299A patent/JP7208920B2/en active Active
- 2018-01-09 WO PCT/US2018/012875 patent/WO2018208334A1/en active Application Filing
- 2018-02-01 TW TW107103560A patent/TWI684132B/en active
- 2018-02-01 TW TW108147270A patent/TWI750557B/en active
-
2019
- 2019-09-27 US US16/585,834 patent/US10685423B2/en active Active
Also Published As
Publication number | Publication date |
---|---|
KR102279120B1 (en) | 2021-07-20 |
WO2018208334A1 (en) | 2018-11-15 |
US10430919B2 (en) | 2019-10-01 |
CN110574011A (en) | 2019-12-13 |
TWI684132B (en) | 2020-02-01 |
US20200098083A1 (en) | 2020-03-26 |
CN110574011B (en) | 2023-06-27 |
JP7208920B2 (en) | 2023-01-19 |
US20180330467A1 (en) | 2018-11-15 |
KR20190135034A (en) | 2019-12-05 |
US10685423B2 (en) | 2020-06-16 |
JP2020519993A (en) | 2020-07-02 |
EP3622399A1 (en) | 2020-03-18 |
TWI750557B (en) | 2021-12-21 |
EP3622399B1 (en) | 2023-10-04 |
TW202014888A (en) | 2020-04-16 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP7202987B2 (en) | Architecture for High Performance, Power Efficient, Programmable Image Processing | |
TWI684132B (en) | Determination of per line buffer unit memory allocation | |
JP6967570B2 (en) | Energy efficient processor core architecture for image processors | |
CN107430760B (en) | Two-dimensional shift array for image processor | |
TWI698832B (en) | Compiler managed memory for image processor | |
TWI670679B (en) | Image processor, non-transitory machine-readable storage media, computing system, and method performed by an image processor | |
TWI736880B (en) | Processor, computer program product and method performed by a processor | |
TWI670968B (en) | Image processor i/o unit | |
JP6775088B2 (en) | Program code variants to improve image processor runtime efficiency | |
CN110300944B (en) | Image processor with configurable number of active cores and supporting internal network | |
JP6820428B2 (en) | Configuring application software on a multi-core image processor |