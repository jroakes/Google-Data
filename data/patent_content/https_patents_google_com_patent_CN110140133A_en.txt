CN110140133A - The implicit bridge joint of machine learning task - Google Patents
The implicit bridge joint of machine learning task Download PDFInfo
- Publication number
- CN110140133A CN110140133A CN201780068195.5A CN201780068195A CN110140133A CN 110140133 A CN110140133 A CN 110140133A CN 201780068195 A CN201780068195 A CN 201780068195A CN 110140133 A CN110140133 A CN 110140133A
- Authority
- CN
- China
- Prior art keywords
- machine learning
- mode input
- input
- language
- data
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/42—Data-driven translation
- G06F40/44—Statistical methods, e.g. probability models
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/42—Data-driven translation
- G06F40/47—Machine-assisted translation, e.g. using translation memory
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/06—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons
- G06N3/063—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons using electronic means
Abstract
For executing the mthods, systems and devices of machine learning task, including encoding the computer program in computer storage medium.A kind of method includes: to receive (i) mode input and (ii) identification to execute the mode input to generate the data of the first machine learning task of the model output of the first kind for the mode input；The mode input is expanded with the identifier of the first machine learning task to generate amplification mode input；And the amplification mode input is handled using machine learning model.The exemplary system training machine learning model implicitly bridged for machine learning task application as described in this description is to execute certain form of machine learning task, without requiring during the training period using the explicit training data for being directed to the certain form of machine learning task.
Description
Background technique
Machine translation system requires a large amount of parallel training data to realize high-caliber accuracy.Generally, with many people
Described language is compared, it is difficult to for a large amount of parallel datas of language acquirement described in a few peoples.For example, find on the internet
Most of texts are English, however the amount of text found in the language of such as Japanese or Korean is less.This makes for smaller language
Speech, which obtains parallel data, becomes challenging.
Traditional machine translation system via third language by bridging compared with the translation between small language i.e. by first language
A part of text translate into third language and then from third language translation at second language overcome the problems, such as this.This bridge joint
Process is by many problems, propagation, increased waiting time and increased system complexity including error.
Summary of the invention
It can realize theme described in this specification in a particular embodiment to realize one or more in following advantages
It is a:
The systematic training machine learning mould implicitly bridged for machine learning task application as described in this description
Type is appointed using for certain form of machine learning with executing certain form of machine learning task without requiring during the training period
The explicit training data of business.For example, even if the training data for such task, system are not used during the training period
Zero sample (zero-shot) translation can be executed, Japanese text segmentation is translated by corresponding Korea Spro with training machine learning model
Language text segmentation.Therefore, the explicit bridge joint for translation is avoided, to avoid the propagation of error, reduces machine learning task
Waiting time and reduce system complexity.For example, decoding speed can be twice fast, because when being Korean from Japanese Translator
It is unnecessary by the explicit bridge joint of third language.
Once what zero sample of execution as described in this description was translated is in addition, machine learning model has been trained to
System can realize high-caliber accuracy, the system explicitly bridged comparable to (if being no better than) for translation application.
Individual machine learning model can be used to hold in the system of zero sample of execution translation as described in this description
Machine translation between row N kind language, rather than there are N^2 individual models, to reduce the required of model parameter tuning
Time and complexity, and reduce the computing resource as consumed by machine learning model.In addition, the reduction of the quantity of model can be with
Make it possible to using more language pair in individual equipment, because service-delivery machine usually has limited memory.In addition, mould
The reduction of the quantity of type can greatly simplified system architecture, to improve production/setting time associated with system.
The system of zero sample of execution translation as described in this description, which can permit, zooms to additional language.For example,
New data can be added to existing model, may have over-sampling or lack sampling, so that all language are all by appropriate earth's surface
Show, and is used together in the case where object language changes with new preposition token.Have no need to change the framework of existing model.
The system of zero sample of execution translation as described in this description can permit the improvement of low-resource language.System
All language that all parameters are all modeled are to implicitly sharing.This forces system to cross over the popularization of language boundary during the training period.
It, can be with when the language with seldom data available is to the language with data abundant to being mixed in individual system
Low-resource language is improved to upper translation accuracy.
Various example embodiments described herein are related to neural network.Neural network is one using non-linear unit
A or multiple layers come be directed to receive input prediction output machine learning model.Some neural networks are in addition to including output layer
Except further include one or more hidden layers.It is i.e. next hiding that the output of each hidden layer is used as next layer in network
The input of layer or output layer.Each layer of network is defeated to generate from the input received according to the current value of one group of corresponding parameter
Out.Training data can be used to train neural network to determine the trained values of layer parameter in machine learning task, and can
Machine learning task is executed to use neural network to input neural network.
In general, this specification can be implemented in for the method for training machine learning model on the training data
Described in theme a novel aspects, wherein machine learning model is configured to: receive amplification mode input, the expansion
Increase mode input include with will to mode input execute machine learning task identifier mode input；And to being connect
The amplification mode input received executes machine learning task to generate the model output of respective type, and its for mode input
In, which comprises obtain the training data including multiple paired data collection, wherein each of paired data collection packet
Include (i) input data set and (ii) output data set；And on the training data training machine learning model to execute multiple machines
Device learning tasks, wherein multiple machine learning tasks include the machine learning task to execute to mode input.
Other embodiments in this respect include corresponding computer system, device and are recorded in one or more computers
The computer program in equipment is stored, respectively is configured to execute the movement of method.One or more system for computer can quilt
It is configured to execute specific operation or movement by means of software, firmware, hardware for being mounted in system or any combination thereof, it is described
Software, firmware, hardware or any combination thereof can make system execute movement in operation.One or more computer programs can quilt
It is configured to execute specific operation or movement by means of including instruction, described instruction makes to fill when being executed by data processing equipment
It sets and executes movement.
Aforementioned and other embodiments individually or in conjunction can respectively optionally include one or more of following characteristics.
In some embodiments, amplification mode input includes the mode input with the preposition token --identifier of machine learning task.
In some embodiments, mode input includes the mode input of the first input type and model output includes the
The model of one output type exports, and multiple paired data collection do not include matching with the output data set of the first output type
The input data set of first input type.
In some embodiments, the data set that multiple paired datas are concentrated includes the text segmentation of different language.
In some embodiments, method further includes generating training data, comprising: is generated admittedly with each of different language
Determine the vocabulary of size V；And by the vocabulary of each generation sequential selection there is highest word and generated to merge
Vocabulary to generate new vocabulary, until the size of new vocabulary reaches V.
In some embodiments, each paired data collection includes the input text segmentation of input language, the input language
The input text segmentation of speech is matched with the text segmentation for the object language for being different from input language.
In some embodiments, multiple machine learning tasks include for each paired data collection, by input text point
Section translates into the text segmentation of object language.
In some embodiments, amplification mode input includes the model with the instruction at least preposition token of object language
Input.
In general, a wound of theme described in this specification can be implemented in the method for following steps
New aspect: mode input will be executed to generate the first kind for mode input by receiving (i) mode input and (ii) identification
Model output the first machine learning task data；With the identifier amplification mode input of the first machine learning task with life
At amplification mode input；And amplification mode input is handled using machine learning model, wherein machine learning model is being instructed
Practice and be trained to execute multiple machine learning tasks including the first machine learning task in data, and wherein, machine learning
Model has passed through training and has been configured to: processing amplification mode input is to be directed to the first engineering that mode input generates the first kind
Practise model output.
Other embodiments in this respect include corresponding computer system, device and are recorded in one or more computers
The computer program in equipment is stored, respectively is configured to execute the movement of the method.One or more system for computer
It can be configured to execute specific operation or movement by means of software, firmware, hardware for being mounted in system or any combination thereof,
The software, firmware, hardware or any combination thereof can make system execute the movement in operation.One or more computers
Program can be configured to execute specific operation or movement by means of including instruction, and described instruction is worked as to be executed by data processing equipment
When, so that device is executed the movement.
Aforementioned and other embodiments individually or in conjunction can respectively optionally include one or more of following characteristics.
It in some embodiments, include arriving the token --identifier of the first machine learning task is preposition with identifier amplification mode input
Mode input.
In some embodiments, training data includes multiple paired data collection, wherein each of paired data collection
Including the input data set matched with output data set, and mode input has the mode input of the first kind, and multiple
Paired data collection does not include the mode input of the first kind containing the output data set pairing with the output of the model of the first kind
Input data set data set pairing.
In some embodiments, the data set that multiple paired datas are concentrated includes the text segmentation of different language, and
Each paired data collection includes the input text segmentation of input language, the input text segmentation of the input language be different from it is defeated
Enter the text segmentation pairing of the object language of language.
In some embodiments, multiple machine learning tasks include for each paired data collection, by input text point
Section translates into the text segmentation of object language.
In some embodiments, with the identifier amplification mode input of machine learning task to generate amplification mode input
Including the token for indicating at least object language is forereached mode input.
The one or more embodiments of the detail of the theme of this specification are illustrated in following attached drawing and description.Theme
Other feature, aspect and advantage will become apparent according to specification, drawings and the claims.
Detailed description of the invention
Fig. 1 shows the implicit bridge system of example for executing machine learning task.
Fig. 2 is the flow chart for executing the instantiation procedure of machine learning task implicitly bridged.
Fig. 3 is the process for training machine learning system to execute the instantiation procedure of machine learning task implicitly bridged
Figure.
Similar appended drawing reference and title indicate similar element in various figures.
Specific embodiment
This specification description is for executing the method and system of machine learning task using implicit bridge joint.For example, zero sample
This translation system is used from one group of language to such as English-Japanese, Japanese-English, English-Korean, the training of Korean-English
Data carry out training machine translation model so that the text of original language to be translated as to the text of object language.By training, even if not yet
Such explicit data is used in training, zero sample translation system also learns the invisible language of translation to such as Korea Spro
Language-Japanese and Japanese-Korean.As another example, systematic learning constructs the parsing tree representation of text and using from one group
The training data of (sentence, analytic tree) and (analytic tree, emotion) pair carrys out training machine learning model to predict given analytic tree
Emotion.By training, even if using such explicit data not yet in training, system also learns directly to predict to give
The emotion of sentence.
Fig. 1 shows the implicit bridge system 100 of the example for executing machine learning task.For example, system 100 can be as
The system for executing the translation of zero sample being described in greater detail below.Other machine learning tasks include sentiment analysis or its
Its natural language processing task.System 100 is to realize as computer program in one or more of one or more positions
The example of system on computer, wherein may be implemented in systems described below, component and technology.
The machine learning task that implicit bridge system 100 receives mode input 102a and identification will execute the mode input
Data 102b.Implicit bridge system 100 handles received mould using amplification module 104 and machine learning model 106
The data 102b for the machine learning task that type input 102a and identification will execute the mode input is to generate model output 108.
Model output 108 generated is certain types of model output.For example, mode input can be original language in some cases
In text segmentation, such as " Hello, how are you? ", and will this to text segmentation execute machine learning task can
Be from the text be segmented from source language translation at object language, for example, by " Hello, how are you? " from English Translation
At Spanish.In this example, object language can indicate the type of model output generated.
Module 104 is expanded to receive mode input 102a and identify the number for the machine learning task to execute the mode input
Mode input 102a is expanded according to 102b, and with the identifier of the first machine learning task to generate amplification mode input 108.?
In some embodiments, amplification module 104 by by the token --identifier of the first machine learning task it is preposition arrive mode input come
Expand mode input 102a.For example, as described above, mode input 102a can be the text segmentation of original language in some cases
And the machine learning task executed is segmented to the text can be text segmentation from source language translation into object language.
In this example, amplification module 104 can be preposition to mode input by the token for indicating at least object language.For example, amplification mould
Block 104 can with preposition token<2xx>, wherein " xx " indicate object language code, for example, for English for EN or for
It is JP for Japanese.It continuing the examples above, amplification mode input 108 can be<2ES>Hello, how are you?
In some embodiments, amplification module 104 can also be by preposition to output text segmentation example by token<2xx>
The output text segmentation of object language is such as expanded instead of standard token<s>.For example, preposition with object language in some cases
Text segmentation can be it is beneficial because with original language to input text segmentation encode then independently of object language.This
It can permit and one text segmentation of original language is translated into many language with a kind of only coding.
In some embodiments, amplification module can give preposition "<the xx>" symbol of text segmentation of original language and to mesh
Preposition "<the xx>" token of correspondence text segmentation of poster speech.For example, in some cases it is such it is preposition can make be
Single language data can be added to machine learning model by system, for example, to obtain low-resource language.In these cases, mould is expanded
Block 104 may be configured to receive mode input 102a and identification will be to the data of the machine learning task of mode input execution
102b and output from machine learning model 106.
Machine learning model 104 receives amplification mode input 108 generated.Machine learning model 104 has passed through training
It is configured to handle amplification mode input 108 to generate machine learning model output 110 for mode input 102a.By engineering
The type for practising the machine learning model output that model generates depends on the defeated i.e. machine learning task mark of received amplification model
Know symbol and machine learning model has been trained to be performed the type of task.For example, continuing the examples above, machine learning mould
Type output 110 can be " Hola, como estas? ".Below with reference to Fig. 3 be more fully described training machine learning model with
Execute the implicit bridge joint of machine learning task.
Machine learning model 104 includes encoder component 112 and decoder component 114.In some embodiments, it encodes
Device assembly 112 and decoder component 114 are all recurrent neural networks.In some embodiments, decoder neural network can be with
It including attention mechanism, and may include softmax output layer.Exemplary neural Machine Translation Model is in " Google's Neural
Machine Translation System:Bridging the Gap between Human and Machine
Translation, " Wu, Yonghui et al., arXiv:1609.08144 are described in more detail in (2016).
As described above, mode input 102a can be the text segmentation of original language in some cases, and will be to this article
The machine learning task that this segmentation executes, which can be, is segmented the text from source language translation into object language.Original language can be
One of many possible original language, and object language can be one of many possible object languages.At these
In the case of machine learning model 106 may include by different original language share encoder, by different object languages
The shared vocabulary of shared decoder and the size V across different language.In some embodiments, vocabulary can be
Shared word block vocabulary, the i.e. vocabulary comprising the sub- word unit of word can be assembled into.
In some embodiments, implicit bridge system 100 may be configured to generate shared vocabulary.For example, system
100 can with each in multilingual generate fixed size V multiple vocabularies and merge vocabulary generated with
Generate the new single vocabulary that size is V.For example, system 100 can be sequentially selected going out in the vocabulary of each generation
Existing highest word, until the size of new vocabulary reaches V.Optionally, system can be from vocabulary generated
Except dittograph, until the size of new vocabulary reaches V.For example, when generating the word for merging English word with German word
When remittance table, system can be with German article " die " to English word " die " duplicate removal.
In some cases, vocabulary may include the probability distribution across the word of different language, for example, not for n kind
It include being uniformly distributed for the V/n word of every kind of language with language vocabulary generated.In other cases, vocabulary can wrap
The data-driven distribution across the word of different language is included, for example, vocabulary generated may include the different numbers of every kind of language
The word of amount.
In some cases, implicit bridge system 100 may be configured to generate model output, and the model output is institute
The mode input received becomes the translation of simple target language.In these cases, machine learning model 106 may include across
The shared encoder 112 of more different original language is shared for the decoder of simple target language and across different original language
Vocabulary.In other cases, implicit bridge system 100 may be configured to generate model output, and the model output is
The mode input of the single original language received becomes the translation of plurality of target language.
Fig. 2 is the flow chart for executing the instantiation procedure of machine learning task implicitly bridged.For convenience, process
200 will be described as being executed by the one or more system for computer being located in one or more positions.For example, according to this theory
Process 200 can be performed in the implicit bridge system 100 of the suitably programmed such as Fig. 1 of the system including machine learning model of bright book.
System receives (i) mode input and (ii) identification and to execute to the mode input to generate for the mode input
Data (the step 202) of first machine learning task of the model output of the first kind.For example, being connect in some embodiments
The mode input received may include the text segmentation of original language such as Japanese.Received identification will hold the mode input
Row may include identification with the data for generating the first machine learning task of the model output of the first kind for the mode input
The text segmentation of original language is translated to generate the data of the task of the correspondence text segmentation of object language such as Korean.In other realities
It applies in mode, received mode input may include text segmentation, and received identification will be to the mode input
Executing to generate the data of the first machine learning task of the model output of the first kind for the mode input may include knowing
Not Yu Ce text segmentation emotion task data.
The identifier amplification mode input of the first machine learning task of system is to generate amplification mode input (step
204).In some embodiments, system can be by preposition defeated to model by the token --identifier of the first machine learning task
Enter to use the identifier of the first machine learning task to expand mode input.For example, being the text segmentation of original language in mode input
And in the case that machine learning task includes the text segmentation that text segmentation is translated into object language, system can give source
Preposition "<the 2xx>" token of the text segmentation of language, wherein xx indicates object language code, for example, for English for EN or
Person is DE for German.It as another example, be text segmentation and machine learning task in mode input include prediction
In the case where the emotion of text segmentation, system can be segmented preposition " < 2sentiment " token to the text.
As another example, system can be to the preposition additional token for also indicating original language of text segmentation of original language, example
Such as,<s><eN><dE>how are you></s><s>wie geht esIhnen?</s>.The method can be in some cases
It is beneficial, such as in the homonym such as English with different meanings from two kinds of different languages of serving as interpreter
When " die " in " die " and German.When to the token of the preposition instruction source language and the target language of text segmentation, it is necessary to every
The sequence of token is safeguarded in a example.Then system may learn the first token instruction original language and the second token instruction mesh
Poster speech, or vice versa.
Alternatively, in some cases system can by by token<2xx>it is preposition to output text segmentation (for example,
Instead of standard token<s>) expand the output text segmentation of object language.In some cases, with the preposition text of object language
Segmentation can be it is beneficial because with original language to input text segmentation encode then independently of object language.This can be with
Allow that one text segmentation of original language is translated into many language with a kind of only coding.
As another alternative solution, for example, practicing in order to enable being able to achieve multilingual and single speech therapy, it is in some cases
System can to original language preposition "<the xx>" symbol of text segmentation and to object language correspondence text segmentation it is preposition "<xx>"
Token.In some cases, this preposition method can enable the system to single language data being added to machine learning model, example
Such as, to obtain low-resource language.For example, when Punjabi (Punjabi) the Lai Xunlian English, the print ground that are used as low-resource language
When language, Punjabi machine learning model, system can via it is following by single language Punjabi data include to model:
<pa>punjabi sentence</s><pa>punjabi sentence</s>.
In these examples, even if system is not segmented directly from Punjabi cypher text or translates text segmentation
At Punjabi, system can also be exposed to the Punjabi text of incrementss and can be appreciated that Punjabi vocabulary and dilute
There is word.Text segmentation is translated into Punjabi or is turned over from Punjabi in this way it is possible to increase machine learning model
Translate the ability of text segmentation.
System handles amplification mode input (step 206) using machine learning model.Machine learning model is to have instructed
Practice to be trained to execute in data and includes one group of machine learning task of the first machine learning task and passed through trained matched
Processing amplification mode input is set to generate the machine of the first machine learning model output of the first kind for the mode input
Learning model.
For example, the first machine learning task can be the task that mode input is translated into language-specific such as Korean, and
And machine learning model may be trained to execute on the training data and translate into setting models input including language-specific
One or more language task, for example, machine learning model may be trained to by setting models input translate into English
Language, Korean and Japanese.As another example, the first machine learning task can be the task of the emotion of prediction text segmentation, and
And machine learning model may be trained to execute two subtasks on the training data --- generate setting models input
Parse the emotion of tree representation and predictive calculation tree representation.
In some cases, the training data for training machine learning model may include one group of paired data collection,
Each of middle paired data collection includes the input data set with output data set pairing.In this case, mode input
It can be the mode input of the first kind and this group of paired data collection can not include defeated with the output of the model of the first kind
The input data set of the mode input of the first kind of data set pairing out.
For example, mode input can be the text segmentation of original language such as Japanese, object language will be translated into for example
The text segmentation of Korean.In this case, the training data for training machine learning model may include different language
Text segmentation to collection, such as (Japanese, English), (English, Japanese), (Korean, English), (English, Korean), wherein different languages
The text segmentation of speech do not include to collection the text of original language and the text of object language pairing, such as (Japanese, Korean).So
And because machine learning model has been trained to receive the mode input of the text segmentation comprising Japanese, for example, working as Japanese
It when text segmentation translates into the text segmentation of English, and has been trained to given text segmentation translating into Korean, for example, logical
Processing training data is crossed to (English, Korean), so machine learning model has been trained to the text segmentation for example by English
It is translated to execute zero sample, and the text segmentation of Japanese can directly be translated into the correspondence text segmentation of Korean.
As another example, mode input, which can be, will predict the text segmentation of its emotion.In this case, for instructing
The training data for practicing machine learning model may include pair of sentence and corresponding parsing tree representation, and parsing tree representation and phase
Pair for the emotion answered.Using this training data, the emotion of given text segmentation can be directly predicted with training system.Below
Training machine learning model is more fully described with reference to Fig. 3 to execute the implicit bridge joint of machine learning task.
Fig. 3 is the instantiation procedure 300 implicitly bridged for training machine learning model to execute machine learning task
Flow chart.For example, process 300 can be used for training the machine learning model 104 of Fig. 1.For convenience, process 300 will be described
To be executed by the one or more system for computer being located in one or more positions.For example, suitably according to this specification
Process 300 can be performed in the system 100 for executing the system such as Fig. 1 implicitly bridged of programming.
System obtains the training data (step 302) for being used for training machine learning model.For training machine learning model
Training data include one group of paired data collection, wherein each of paired data collection include with output data set pairing it is defeated
Enter data set.In some embodiments, by machine learning model, received mode input can be the first input at runtime
The mode input of type, for example, input1, and the model output generated by machine learning model can be the first output type
Model output, for example, output1.It in some cases, can be with for this group of paired data collection of training machine learning model
It does not include the input data set with the first input type of the output data set of the first output type pairing, for example, the group is matched
Data set can not include to (input1,output1).For example, this group of paired data collection may include to (input1,
output2)、(input2,output2)、(input2,output3)、(input3,output1) or (input3,output3) in
One or more, wherein index instruction input or output type.
In some embodiments, the data set that this group of paired data is concentrated can be the text segmentation of different language.?
In this case, each paired data collection then may include input language input text segmentation, the input language it is defeated
The output text segmentation for entering text segmentation and the output language for being different from input language matches.In these embodiments, by machine
Received mode input can be the input text segmentation of first language to device learning model at runtime, for example, the text of Japanese
Segmentation, and the model output generated by machine learning model can be the text segmentation of second language such as Korean.This is assembled
It can not include the text segmentation with the first language of the text segmentation of second language pairing to data set, for example, (Japanese, Korea Spro
Language) it is right.For example, this group of paired data collection may include such as (English, Korean), (Korean, English), (English, Japanese), (day
Language, English) pair.
Each of paired data collection further includes the language of specified machine learning task associated with paired data collection
Identifier, for example, paired data collection (English, Korean) may include the mark that specified English data set is translated into Korean data set
Know symbol.
In some embodiments, the data set that this group of paired data is concentrated can be the text point of one or more language
Section, parsing tree representation and emotion.In this case, paired data collection may include and the text of corresponding parsing tree representation pairing
This segmentation and the parsing tree representation matched with corresponding emotion.By machine learning model, received mode input can at runtime
To be input text segmentation, for example, " I hate flowers ", and the model output generated by machine learning model can be
Emotion, for example, " passive ".Each of paired data collection further includes the mark of specified task associated with paired data collection
Symbol is known, for example, the identifier of specified " generating parsing tree representation " or " prediction emotion ".
System on the training data training machine learning model to execute one group of machine learning task (step 304).System
Carry out training machine learning model using standard machine learning art.For example, in the case where machine learning model is neural network,
System can train neural network by the text segmentation of processing training input such as original language on the training data, with basis
The text segmentation of object language is for example translated into given input by given machine learning task, to generate trained output, for example, mesh
The text segmentation of poster speech.System, which may then pass through, to be calculated loss function and reversely passes relative to Current Situation of Neural Network weight
Broadcasting loss function gradient will train output to compare with known output, to determine one group of update for minimizing loss function
Neural network weight, as described above with reference to Figure 1.
This group of machine learning task include will at runtime to mode input execute machine learning task, such as above with reference to
Described in step 304.In other words, machine learning model is trained to execute the machine that can be provided to system at runtime
Device learning tasks.For example, continuing the above the example provided in step 302, this group of paired data is concentrated in some cases
Data set includes the different language such as text segmentation of Japanese, English or Korean.In this example, each paired data collection includes
With the input text segmentation of the input language of the text segmentation pairing for the object language for being different from input language, such as (Japanese, English
Language), (English, Japanese), (English, Korean), (Korean, English).Then this group of machine learning task may include, for each
Paired data collection will input text segmentation and translate into the text segmentation of object language, for example, input text segmentation is translated into English
Language text segmentation, the text segmentation that input text segmentation is translated into Japanese and the text that input text segmentation is translated into Korean
This segmentation.
By training process 300, even if model, which is not yet explicitly trained to, executes machine to certain types of mode input
Device learning tasks, machine learning model also learn to handle setting models input according to given machine learning task.For example, as above
It is described, Japanese text is translated into third " bridge joint " language and turns over third language even if machine learning model is only trained to
It is translated into Korean, machine learning model can also learn the correspondence text point that the text segmentation of Japanese is directly translated into Korean
Section.
In some embodiments, system can expand training process 300 to improve using available parallel data
The validity of journey 300.As described above, in this context, parallel data description is for will at runtime execute mode input
Machine learning task training data.In other words, parallel data can describe do not included for machine learning task
The training data in training data obtained at step 302.It continues the examples above, parallel data may include that can be used for instructing
Practice machine learning model so that the text segmentation of Japanese to be translated into the training data of the correspondence text segmentation of Korean, for example, pairing
Data set (Japanese, Korean).
System can execute above-mentioned step 302 and 304 and carry out training machine learning model to execute machine learning task
Implicit bridge joint.For example, as described above, even if the training data obtained in step 302 includes being used for training machine learning model
Japanese text to be translated into " bridge joint " language and the training data by third language translation at Korean, machine learning model
It can learn the correspondence text segmentation that the text segmentation of Japanese is directly translated into Korean.
It is not by obtaining at step 302 that system, which obtains attached Parallel training data to carry out training machine learning model to execute,
Machine learning task represented by the training data obtained.Similar to the training data above with reference to described in step 302, obtained
Parallel training data may include one group of paired data collection, wherein each of paired data collection includes and output data set
The input data set of pairing.It continues the examples above, parallel training data may include matching with the corresponding text segmentation of Korean
One group of text segmentation of Japanese.In some embodiments, parallel training data obtained can be a small amount of training data.
For example, the size of parallel training data can be less than the training dataset above with reference to described in step 302, and/or simultaneously
Row training data may include the training example fewer than the training dataset obtained at step 302.
It is not by step 302 that system, which carrys out training machine learning model using parallel training data obtained to execute,
Locate machine learning task represented by the training data obtained.As described above, this may include using standard machine learning art.
For example, system can pass through processing parallel data training input such as source in the case where machine learning model is neural network
The text segmentation of language Japanese carrys out the training neural network in parallel training data, to generate parallel data training output, for example,
The text segmentation of object language Korean.As described above with reference to Figure 1, system may then pass through calculate loss function and
Parallel data training is exported and known and line number relative to Current Situation of Neural Network weight backpropagation loss function gradient
It compares according to output, to determine the neural network weight for one group of update for minimizing loss function.
When becoming to be utilized by system with and when attached Parallel data, system can for attached Parallel Data duplication this
Additional training process.
By being that be directed to appointed by machine learning represented by the training data that originally obtained for zero sample orientation
Multitask machine learning model is incrementally trained in business in attached Parallel data, and system can further refine multitask engineering
It practises model and improves from the accuracy for using machine learning model result obtained at runtime.In some embodiments
In, utilize the mixing i.e. training data including parallel data of available training data to be trained with individualized training process is used
Model compare, as described above execute amplification training process machine learning model can be enabled to hold with higher accuracy
The some machine learning tasks of row.
For illustrative purpose, system and method described in this specification have used machine translation to use as prime example
Example.However, described system and method can be applied to various other settings, including such as parse or sentiment analysis its
Its natural language task.For example, system and method can be used for predicting the emotion from given sentence, for example, to predict phrase
The emotion of " I hate flowers ".In this example, trainable system is to execute two machine learning subtasks, i.e., (1) will
Given text segmentation translates into analytic tree, and the emotion of (2) predictive calculation tree.Such as:
<2parsetree>I love flowers</s>(ROOT(S(NP(PRP I))(VP(VBP love)(NP(NNS
flowers)))(..)))</s>
…
<2sentiment>(ROOT(S(NP(PRP I))(VP(VBP love)(NP(NNS flowers)))(..)))</
s>positive</s>。
Then system can directly learn to be bridged to emotion from sentence:
<2sentiment>I hate flowers</s>“negative”。
Theme described in this specification and the embodiment available digital electronic circuit of feature operation, with visibly specific real
Existing computer software or firmware with computer hardware includes structure and its equivalent structures or use disclosed in this specification
The combination of one or more of which is realized.The embodiment of theme described in this specification can be used as one or more
Computer program encodes to be executed by data processing equipment or controlled data on tangible non-transitory storage medium
The one or more modules for managing the computer program instructions of the operation of device are implemented.Alternatively or in addition, program can be referred to
Enable coding on manually generated transmitting signal, the transmitting signal is, for example, electricity, optics or the electromagnetic signal that machine generates, should
Signal is generated to encode information to be transferred to suitable acceptor device for being executed by data processing equipment.Meter
Calculation machine storage medium can be machine readable storage device, machine readable storage substrate, random or serial access memory equipment,
Or the combination of one or more of which.Computer storage medium so rather than transmitting signal.
Term " data processing equipment " includes device, equipment and machine for handling all kinds of data, as showing
Example includes programmable processor, computer or multiple processors or computer.Device may include dedicated logic circuit, such as FPGA
(field programmable gate array) or ASIC (specific integrated circuit).Device may also include other than including hardware for computer journey
Sequence create performing environment code, for example, constitute processor firmware, protocol stack, data base management system, operating system or they
One or more of combined code.
Computer program (its be also referred to as or be described as program, software, software application, module, software module,
Script or code) it can be write with any type of programming language, the programming language includes compiling or interpretative code, or declaratively
Or procedural language, and it can be disposed in any form, including as stand-alone program or as module, component, son routine
Program is suitable for the other units used in a computing environment.Computer program can with but do not need to correspond in file system
File.It can be in the file for keeping other programs or data (for example, being stored in one or more scripts in marking language document)
A part in, in the single file for being exclusively used in described program or in multiple coordination files (for example, storage is one or more
The file of the part of module, subprogram or code) in store program.Computer program can be deployed on a computer or
Person executes on the multiple computers for being located at a website or being distributed across multiple websites.
As used in this description, " engine " or " any engine ", which refers to, provides the software of the output different from input
The input/output of realization.Engine can be functional encoding block, such as library, platform, software development kit (" SDK ")
Or object.It can include any appropriate type of one or more processors and computer-readable medium by each engine implementation
It calculates in equipment, the calculating equipment is, for example, server, smart phone, tablet computer, notebook computer, music
Device, E-book reader, on knee or desktop computer, PDA, smart phone or other fixations or portable device.Additionally,
It can be by two or more realizations in these engines in identical calculating equipment or in different calculating equipment.
Process and logic flow described in this specification can by one or more programmable calculators execute one or
Multiple computer programs and output is operated and generated to input data to execute function to be performed.Process and logic
Process can also be executed by dedicated logic circuit, and device is alternatively arranged as dedicated logic circuit and is implemented, the special logic electricity
Road is, for example, FPGA (field programmable gate array) or ASIC (specific integrated circuit).
It includes that can be based on general purpose microprocessor or dedicated micro- place that the computer of computer program, which is adapted for carrying out, as example
Manage device or both or any other kind of central processing unit.Generally, central processing unit will from read-only memory or with
Machine accesses memory or both and receives instruction and data.The necessary component of computer is the center for executing or carrying out instruction
Processing unit and for storing instruction with one or more memory devices of data.Generally, computer will also include or
Person operationally couple with from one or more mass-memory units for storing data (for example, disk, magneto-optic disk or light
Disk) data are received, data are perhaps transferred to one or more mass-memory units for storing data or and and are had
It.However, computer need not have such equipment.In addition, computer can be embedded in another equipment, another equipment example
For example mobile phone, personal digital assistant (PDA), Mobile audio frequency or video player, game console, global positioning system
(GPS) receiver or portable memory apparatus, for example, universal serial bus (USB) flash drive etc..
It is suitable for storing computer program instructions and the computer-readable medium of data including the non-volatile of form of ownership
Memory, medium and memory devices include semiconductor memory devices as example, for example, EPROM, EEPROM and flash
Memory devices；Disk, such as internal hard drive or moveable magnetic disc；Magneto-optic disk；And CD ROM and DVD-ROM disk.Processor
Can be by supplemented with memory, or be incorporated in dedicated logic circuit.
In order to provide the interaction with user, the embodiment of theme described in this specification, institute can be realized on computers
Stating computer has for the display equipment such as CRT (cathode-ray tube) or LCD (liquid crystal display) to user's display information
Monitor and user can be used to provide the keyboard and pointer device of input, such as mouse or trackball to the computer.It is other
The equipment of type can also be used for providing the interaction with user；For example, the feedback for being supplied to user may be any type of feeling
Feedback, such as visual feedback, audio feedback or touch feedback；And input from the user can be received in any form, including
Acoustics, voice or tactile input.In addition, computer can be by making to the equipment transmission document used by user and from by user
Equipment receives document and interacts with user；For example, to user's and the request in response to being received from web browser
Web browser on client device sends web page.
It can realize that the embodiment of theme described in this specification, the computing system include rear end group in computing systems
Part (for example, as data server), perhaps including middleware component (for example, application server) or including front end assemblies
(for example, having, user can be used to the graphic user interface interacted with the embodiment of theme described in this specification or Web is clear
Look at the client computer of device), or any combination including one or more such rear ends, middleware or front end assemblies.System
The component of system can be interconnected by any form or the digital data communications (for example, communication network) of medium.Communication network shows
Example includes local area network (LAN) and wide area network (WAN), such as internet.
Computing system may include client and server.Client and server is generally remote from each other and usually passes through
Communication network interacts.The relationship of client and server on corresponding computer by means of running and each other with client
End-relationship server computer program and generate.In some embodiments, server to user equipment send data (for example,
Html page), for example, for interact with the user equipment as client user display data and from as client
The user of the user equipment interaction at end receives the purpose of user's input.It can be received from user equipment in user equipment at server
Locate the data generated, for example, the result of user's interaction.
Although this specification includes many specific embodiment details, these are not construed as to any invention
Or the range of thing that may require protection be construed as limiting, but being interpreted on the contrary can be specific to the specific of specific invention
The description of the feature of embodiment.It can also realize in combination in a single embodiment in the present specification in individual embodiment
Certain features described in context.On the contrary, can also be individually or according to any suitable sub-portfolio in multiple embodiments
Middle realization various features described in the context of single embodiment.In addition, although feature can be described above as by
It works according to certain combinations and even initially so claimed, however the one or more from claimed combination is special
Sign can be removed from the combination in some cases.
Similarly, although describing operation in the accompanying drawings according to certain order, this be not construed as require according to
Shown in certain order perhaps order executes this generic operation in sequence or to execute the operation of all diagrams desired to realize
Effect.In some cases, multitasking and parallel processing can be advantageous.In addition, each in the above embodiments
The separation of kind system module and component is not construed as requiring this separation in all embodiments, and should be understood that
It is that described program assembly and system generally can be integrated together in single software product or be packaged into multiple softwares
In product.
The specific embodiment of theme has been described.Other embodiments are in the range of following claims.For example, right
The movement described in it is required that can be performed in a different order and still realize desired result.As an example, attached drawing
The process of middle description not necessarily require shown in certain order or sequential order to realize desired result.In certain embodiment party
In formula, multitasking and parallel processing can be advantageous.
Claims (20)
1. a kind of system of one or more storage equipment including one or more computers and store instruction, described instruction can
Operation is so that one or more of computers is executed operation when being executed by one or more of computers, comprising:
The mode input will be executed to generate first for the mode input by receiving (i) mode input and (ii) identification
The data of first machine learning task of the model output of type；
The mode input is expanded with the identifier of the first machine learning task to generate amplification mode input；And
The amplification mode input is handled using machine learning model, wherein the machine learning model is in training data
On be trained to execute the multiple machine learning tasks for including the first machine learning task, and wherein, the engineering
Habit model has passed through training and has been configured to handle the amplification mode input to be directed to the mode input and generate the first kind
The machine learning model of type exports.
2. system according to claim 1, wherein the machine learning model includes that (i) is configured to receive amplification mould
The encoder subsystem of type input and (ii) are configured to generate the Decoder Subsystem of model output.
3. system according to claim 2, wherein the encoder subsystem and Decoder Subsystem including passing accordingly
Return neural network.
4. according to claim 2 or system as claimed in claim 3, wherein the Decoder Subsystem includes attention mechanism.
5. system according to any one of the preceding claims, wherein the amplification mode input includes having to be directed to
The mode input of the preposition token --identifier of the machine learning task.
6. system according to any one of the preceding claims, wherein the multiple machine learning task includes will be defeated
Enter the text segmentation that text segmentation translates into object language.
7. a kind of method for training machine learning model on the training data,
Wherein, the machine learning model is configured to
Amplification mode input is received, the amplification mode input includes having to be directed to and to appoint the machine learning that mode input executes
The mode input of the identifier of business, and
The machine learning task is executed to generate respective class for the mode input to received amplification mode input
The model of type exports, and
Wherein, which comprises
Obtain the training data including multiple paired data collection, wherein each of described paired data collection includes that (i) is inputted
Data set and (ii) output data set；And
The training machine learning model is on the training data to execute multiple machine learning tasks, wherein the multiple
Machine learning task includes the machine learning task to execute to the mode input.
8. according to the method described in claim 7, wherein, the amplification mode input includes having to appoint for the machine learning
The mode input of the preposition token --identifier of business.
9. according to claim 7 or method according to any one of claims 8, wherein the mode input includes the first input type
Mode input, and model output includes the model output of the first output type, and
Wherein, the multiple paired data collection does not include described first with the pairing of the output data set of first output type
The input data set of input type.
10. the method according to any one of claims 7 to 9, wherein the data set that the multiple paired data is concentrated
Text segmentation including different language.
11. according to the method described in claim 10, further comprising generating the training data, comprising:
The vocabulary of fixed size V is generated with each in the different language；And
Merge vocabulary generated by being sequentially selected the highest word of appearance in the vocabulary of each generation to generate
New vocabulary, until the size of the new vocabulary reaches V.
12. method described in 0 or 11 according to claim 1, wherein each paired data collection includes the input text of input language
Segmentation, the input text segmentation of the input language are matched with the text segmentation for the object language for being different from the input language.
13. according to the method for claim 12, wherein the multiple machine learning task includes: for each with logarithm
According to collection, input text segmentation is translated into the text segmentation of the object language.
14. according to claim 12 or claim 13 described in method, wherein the amplification mode input include have instruction
The mode input of the preposition token of at least described object language.
15. a method of computer implementation, comprising:
The mode input will be executed to generate first for the mode input by receiving (i) mode input and (ii) identification
The data of first machine learning task of the model output of type；
The mode input is expanded with the identifier of the first machine learning task to generate amplification mode input；And
The amplification mode input is handled using machine learning model, wherein the machine learning model is in training data
On be trained to execute the multiple machine learning tasks for including the first machine learning task, and wherein, the engineering
It practises model and has passed through trained be configured to:
The amplification mode input is handled to generate the first machine learning model of the first kind for the mode input
Output.
16. according to the method for claim 15, wherein expanding the mode input with identifier includes that will be used for described the
The token --identifier of one machine learning task is preposition to arrive the mode input.
17. method according to claim 15 or 16, wherein the training data includes multiple paired data collection, wherein
Each of described paired data collection includes the input data set with output data set pairing, and
Wherein, the mode input be the mode input of the first kind and the multiple paired data collection do not include containing with institute
State the number of the input data set of the mode input of the first kind of the output data set pairing of the model output of the first kind
According to the pairing of collection.
18. according to the method for claim 17, wherein the data set that the multiple paired data is concentrated includes different language
Text segmentation, and each paired data collection includes the input text segmentation of input language, the input text of the input language
This segmentation and the text segmentation for the object language for being different from the input language match.
19. according to the method for claim 18, wherein the multiple machine learning task includes: for each with logarithm
According to collection, input text segmentation is translated into the text segmentation of the object language.
20. method described in 8 or 19 according to claim 1, wherein expand the mould with the identifier of the machine learning task
Type input includes that will indicate that the token of at least described object language is preposition to the mode input to generate amplification mode input.
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201662418098P | 2016-11-04 | 2016-11-04 | |
US62/418,098 | 2016-11-04 | ||
US15/394,708 | 2016-12-29 | ||
US15/394,708 US10713593B2 (en) | 2016-11-04 | 2016-12-29 | Implicit bridging of machine learning tasks |
PCT/US2017/059776 WO2018085577A1 (en) | 2016-11-04 | 2017-11-02 | Implicit bridging of machine learning tasks |
Publications (1)
Publication Number | Publication Date |
---|---|
CN110140133A true CN110140133A (en) | 2019-08-16 |
Family
ID=62063853
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201780068195.5A Pending CN110140133A (en) | 2016-11-04 | 2017-11-02 | The implicit bridge joint of machine learning task |
Country Status (6)
Country | Link |
---|---|
US (3) | US10713593B2 (en) |
EP (1) | EP3520035A1 (en) |
JP (1) | JP6776448B2 (en) |
KR (1) | KR102368519B1 (en) |
CN (1) | CN110140133A (en) |
WO (1) | WO2018085577A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN112131342A (en) * | 2020-09-07 | 2020-12-25 | 北京字节跳动网络技术有限公司 | Model training method, device, equipment and storage medium |
Families Citing this family (26)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN110352423B (en) * | 2016-11-04 | 2021-04-20 | 渊慧科技有限公司 | Method, storage medium, and system for generating a target sequence using a noisy channel model |
US10713593B2 (en) * | 2016-11-04 | 2020-07-14 | Google Llc | Implicit bridging of machine learning tasks |
WO2018126213A1 (en) * | 2016-12-30 | 2018-07-05 | Google Llc | Multi-task learning using knowledge distillation |
EP3723084A1 (en) | 2018-03-07 | 2020-10-14 | Google LLC | Facilitating end-to-end communications with automated assistants in multiple languages |
US10565475B2 (en) * | 2018-04-24 | 2020-02-18 | Accenture Global Solutions Limited | Generating a machine learning model for objects based on augmenting the objects with physical properties |
US10664472B2 (en) | 2018-06-27 | 2020-05-26 | Bitdefender IPR Management Ltd. | Systems and methods for translating natural language sentences into database queries |
CN112889073A (en) | 2018-08-30 | 2021-06-01 | 谷歌有限责任公司 | Cross-language classification using multi-language neural machine translation |
US11568207B2 (en) * | 2018-09-27 | 2023-01-31 | Deepmind Technologies Limited | Learning observation representations by predicting the future in latent space |
KR20200075615A (en) | 2018-12-18 | 2020-06-26 | 삼성전자주식회사 | Method and apparatus for machine translation |
US10963644B2 (en) * | 2018-12-27 | 2021-03-30 | Microsoft Technology Licensing, Llc | Computer-implemented generation and utilization of a universal encoder component |
CN109740741B (en) * | 2019-01-09 | 2023-07-25 | 上海理工大学 | Reinforced learning method combined with knowledge transfer and learning method applied to autonomous skills of unmanned vehicles |
JP2020160917A (en) * | 2019-03-27 | 2020-10-01 | 国立研究開発法人情報通信研究機構 | Method for training neural machine translation model and computer program |
US11960843B2 (en) * | 2019-05-02 | 2024-04-16 | Adobe Inc. | Multi-module and multi-task machine learning system based on an ensemble of datasets |
JP6772394B1 (en) * | 2019-05-21 | 2020-10-21 | 日本電信電話株式会社 | Information learning device, information processing device, information learning method, information processing method and program |
CN111985637A (en) * | 2019-05-21 | 2020-11-24 | 苹果公司 | Machine learning model with conditional execution of multiple processing tasks |
JP7114528B2 (en) * | 2019-07-08 | 2022-08-08 | 株式会社東芝 | Reasoning device, learning device, reasoning method and learning method |
KR20210014949A (en) | 2019-07-31 | 2021-02-10 | 삼성전자주식회사 | Decoding method and apparatus in artificial neural network for speech recognition |
US20210035556A1 (en) * | 2019-08-02 | 2021-02-04 | Babylon Partners Limited | Fine-tuning language models for supervised learning tasks via dataset preprocessing |
US11842165B2 (en) * | 2019-08-28 | 2023-12-12 | Adobe Inc. | Context-based image tag translation |
US20210065051A1 (en) * | 2019-09-04 | 2021-03-04 | Advanced Micro Devices, Inc. | Method and apparatus for predicting kernel tuning parameters |
US20210142005A1 (en) * | 2019-11-11 | 2021-05-13 | Sri International | Machine learning for translation to structured computer readable representation |
CN110991195B (en) * | 2019-12-13 | 2023-09-29 | 北京小米智能科技有限公司 | Machine translation model training method, device and storage medium |
EP4087213A4 (en) * | 2020-01-14 | 2023-01-04 | Guangdong Oppo Mobile Telecommunications Corp., Ltd. | Artificial intelligence operation processing method and apparatus, system, terminal, and network device |
JP7442324B2 (en) | 2020-01-29 | 2024-03-04 | 日本放送協会 | Machine translation devices and programs |
CN112528669B (en) * | 2020-12-01 | 2023-08-11 | 北京百度网讯科技有限公司 | Training method and device for multilingual model, electronic equipment and readable storage medium |
WO2024005787A1 (en) * | 2022-06-28 | 2024-01-04 | Google Llc | Systems and methods for training translation models using source-augmented training examples |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1452552A (en) * | 2000-05-24 | 2003-10-29 | 西尔弗布鲁克研究有限公司 | Printed page tag encoder |
CN105160397A (en) * | 2014-06-06 | 2015-12-16 | 谷歌公司 | Training distilled machine learning models |
CN105378699A (en) * | 2013-11-27 | 2016-03-02 | Ntt都科摩公司 | Automatic task classification based upon machine learning |
Family Cites Families (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7624020B2 (en) * | 2005-09-09 | 2009-11-24 | Language Weaver, Inc. | Adapter for allowing both online and offline training of a text to text system |
CN101030196B (en) * | 2006-02-28 | 2010-05-12 | 株式会社东芝 | Method and apparatus for training bilingual word alignment model, method and apparatus for bilingual word alignment |
US8799307B2 (en) | 2007-05-16 | 2014-08-05 | Google Inc. | Cross-language information retrieval |
US8190420B2 (en) * | 2009-08-04 | 2012-05-29 | Autonomy Corporation Ltd. | Automatic spoken language identification based on phoneme sequence patterns |
JP5548176B2 (en) * | 2011-09-16 | 2014-07-16 | 日本電信電話株式会社 | Translation optimization apparatus, method, and program |
US9195656B2 (en) * | 2013-12-30 | 2015-11-24 | Google Inc. | Multilingual prosody generation |
JP6306376B2 (en) * | 2014-03-06 | 2018-04-04 | 株式会社Ｎｔｔドコモ | Translation apparatus and translation method |
US10181098B2 (en) * | 2014-06-06 | 2019-01-15 | Google Llc | Generating representations of input sequences using neural networks |
JP6262122B2 (en) * | 2014-11-18 | 2018-01-17 | 日本電信電話株式会社 | Translation learning device, proper expression learning device, method, and program |
JP6230987B2 (en) * | 2014-12-01 | 2017-11-15 | 日本電信電話株式会社 | Language model creation device, language model creation method, program, and recording medium |
CN106383818A (en) * | 2015-07-30 | 2017-02-08 | 阿里巴巴集团控股有限公司 | Machine translation method and device |
US11238348B2 (en) * | 2016-05-06 | 2022-02-01 | Ebay Inc. | Using meta-information in neural machine translation |
US10713593B2 (en) * | 2016-11-04 | 2020-07-14 | Google Llc | Implicit bridging of machine learning tasks |
-
2016
- 2016-12-29 US US15/394,708 patent/US10713593B2/en active Active
-
2017
- 2017-11-02 KR KR1020197015572A patent/KR102368519B1/en active IP Right Grant
- 2017-11-02 EP EP17800671.4A patent/EP3520035A1/en active Pending
- 2017-11-02 WO PCT/US2017/059776 patent/WO2018085577A1/en active Search and Examination
- 2017-11-02 JP JP2019522923A patent/JP6776448B2/en active Active
- 2017-11-02 CN CN201780068195.5A patent/CN110140133A/en active Pending
-
2019
- 2019-05-03 US US16/402,787 patent/US10679148B2/en active Active
-
2020
- 2020-07-13 US US16/927,803 patent/US20200410396A1/en active Pending
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1452552A (en) * | 2000-05-24 | 2003-10-29 | 西尔弗布鲁克研究有限公司 | Printed page tag encoder |
CN105378699A (en) * | 2013-11-27 | 2016-03-02 | Ntt都科摩公司 | Automatic task classification based upon machine learning |
CN105160397A (en) * | 2014-06-06 | 2015-12-16 | 谷歌公司 | Training distilled machine learning models |
Non-Patent Citations (1)
Title |
---|
RICO SENNRICH，BARRY HADDOW，ALEXANDRA BIRCH: "Controlling Politeness in Neural Machine Translation via Side Constraints", 《PROCEEDINGS OF NAACL-HLT 2016》 * |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN112131342A (en) * | 2020-09-07 | 2020-12-25 | 北京字节跳动网络技术有限公司 | Model training method, device, equipment and storage medium |
Also Published As
Publication number | Publication date |
---|---|
US20180129972A1 (en) | 2018-05-10 |
EP3520035A1 (en) | 2019-08-07 |
US10713593B2 (en) | 2020-07-14 |
US20190258961A1 (en) | 2019-08-22 |
JP6776448B2 (en) | 2020-10-28 |
KR20190073525A (en) | 2019-06-26 |
US10679148B2 (en) | 2020-06-09 |
US20200410396A1 (en) | 2020-12-31 |
JP2020501228A (en) | 2020-01-16 |
WO2018085577A1 (en) | 2018-05-11 |
KR102368519B1 (en) | 2022-03-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN110140133A (en) | The implicit bridge joint of machine learning task | |
US10635977B2 (en) | Multi-task learning using knowledge distillation | |
US20240070392A1 (en) | Computing numeric representations of words in a high-dimensional space | |
US11093813B2 (en) | Answer to question neural networks | |
US20220350965A1 (en) | Method for generating pre-trained language model, electronic device and storage medium | |
CN106997370B (en) | Author-based text classification and conversion | |
US11775761B2 (en) | Method and apparatus for mining entity focus in text | |
US9460075B2 (en) | Solving and answering arithmetic and algebraic problems using natural language processing | |
CN106663092A (en) | Neural machine translation systems with rare word processing | |
CN110023928A (en) | Forecasting search engine ranking signal value | |
EP4109324A2 (en) | Method and apparatus for identifying noise samples, electronic device, and storage medium | |
CN109313719A (en) | It is parsed using the interdependence that neural network generates text chunk | |
CN109074517A (en) | Global normalization's neural network | |
JP7395553B2 (en) | Text translation methods, devices, electronic devices and storage media | |
Ramati et al. | Uniform multilingualism: A media genealogy of Google Translate | |
CN114648032B (en) | Training method and device of semantic understanding model and computer equipment | |
CN111104796A (en) | Method and device for translation | |
Pan et al. | ChatGPT: A OpenAI platform for society 5.0 | |
CN114580446A (en) | Neural machine translation method and device based on document context | |
Ni et al. | Recurrent neural network based language model adaptation for accent mandarin speech | |
EP4064110A1 (en) | Apparatus and method for training dialogue summary model | |
Popel et al. | TectoMT–a deep linguistic core of the combined Cimera MT system | |
KR20230027925A (en) | Method and device for translating from spoken language to sign language based on a graph | |
CN110909530A (en) | Method, apparatus, electronic device and computer readable medium for generating text |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |