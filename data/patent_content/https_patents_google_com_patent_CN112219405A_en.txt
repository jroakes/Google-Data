CN112219405A - Identifying and controlling smart devices - Google Patents
Identifying and controlling smart devices Download PDFInfo
- Publication number
- CN112219405A CN112219405A CN201980036735.0A CN201980036735A CN112219405A CN 112219405 A CN112219405 A CN 112219405A CN 201980036735 A CN201980036735 A CN 201980036735A CN 112219405 A CN112219405 A CN 112219405A
- Authority
- CN
- China
- Prior art keywords
- smart device
- smart
- virtual assistant
- user
- user interface
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/43—Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware
- H04N21/436—Interfacing a local distribution network, e.g. communicating with another STB or one or more peripheral devices inside the home
- H04N21/43615—Interfacing a Home Network, e.g. for connecting the client to a plurality of peripherals
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F1/00—Details not covered by groups G06F3/00 - G06F13/00 and G06F21/00
- G06F1/16—Constructional details or arrangements
- G06F1/1613—Constructional details or arrangements for portable computers
- G06F1/1633—Constructional details or arrangements of portable computers not specific to the type of enclosures covered by groups G06F1/1615 - G06F1/1626
- G06F1/1684—Constructional details or arrangements related to integrated I/O peripherals not covered by groups G06F1/1635 - G06F1/1675
- G06F1/1686—Constructional details or arrangements related to integrated I/O peripherals not covered by groups G06F1/1635 - G06F1/1675 the I/O peripheral being an integrated camera
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F1/00—Details not covered by groups G06F3/00 - G06F13/00 and G06F21/00
- G06F1/16—Constructional details or arrangements
- G06F1/1613—Constructional details or arrangements for portable computers
- G06F1/1633—Constructional details or arrangements of portable computers not specific to the type of enclosures covered by groups G06F1/1615 - G06F1/1626
- G06F1/1684—Constructional details or arrangements related to integrated I/O peripherals not covered by groups G06F1/1635 - G06F1/1675
- G06F1/1698—Constructional details or arrangements related to integrated I/O peripherals not covered by groups G06F1/1635 - G06F1/1675 the I/O peripheral being a sending/receiving arrangement to establish a cordless communication link, e.g. radio or infrared link, integrated cellular phone
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/03—Arrangements for converting the position or the displacement of a member into a coded form
- G06F3/0304—Detection arrangements using opto-electronic means
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/0482—Interaction with lists of selectable items, e.g. menus
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/04847—Interaction techniques to control parameter settings, e.g. interaction with sliders or dials
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/451—Execution arrangements for user interfaces
- G06F9/453—Help systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T19/00—Manipulating 3D models or images for computer graphics
- G06T19/006—Mixed reality
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/20—Scenes; Scene-specific elements in augmented reality scenes
-
- G—PHYSICS
- G08—SIGNALLING
- G08C—TRANSMISSION SYSTEMS FOR MEASURED VALUES, CONTROL OR SIMILAR SIGNALS
- G08C17/00—Arrangements for transmitting signals characterised by the use of a wireless electrical link
- G08C17/02—Arrangements for transmitting signals characterised by the use of a wireless electrical link using a radio link
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M1/00—Substation equipment, e.g. for use by subscribers
- H04M1/72—Mobile telephones; Cordless telephones, i.e. devices for establishing wireless links to base stations without route selection
- H04M1/724—User interfaces specially adapted for cordless or mobile telephones
- H04M1/72403—User interfaces specially adapted for cordless or mobile telephones with means for local support of applications that increase the functionality
- H04M1/72409—User interfaces specially adapted for cordless or mobile telephones with means for local support of applications that increase the functionality by interfacing with external accessories
- H04M1/72415—User interfaces specially adapted for cordless or mobile telephones with means for local support of applications that increase the functionality by interfacing with external accessories for remote control of appliances
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04W—WIRELESS COMMUNICATION NETWORKS
- H04W88/00—Devices specially adapted for wireless communication networks, e.g. terminals, base stations or access point devices
- H04W88/02—Terminal devices
-
- G—PHYSICS
- G08—SIGNALLING
- G08C—TRANSMISSION SYSTEMS FOR MEASURED VALUES, CONTROL OR SIMILAR SIGNALS
- G08C2201/00—Transmission systems of control signals via wireless link
- G08C2201/30—User interface
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M2250/00—Details of telephonic subscriber devices
- H04M2250/52—Details of telephonic subscriber devices including functional features of a camera
Abstract
Methods, systems, and apparatus for controlling smart devices are described. In one aspect, a method comprises: image data of an image captured by a camera of a mobile device of a user is received and it is determined that the image depicts at least one of a smart device or a physical control of the smart device. In response to determining that the image depicts the smart device or physical controls of the smart device, one or more user interface controls for controlling the smart device are identified and generated and presented on a display of the mobile device. The method may also include detecting, at a display of the mobile device, user interaction with at least one of the one or more user interface controls, and controlling the smart device based on the detected user interaction.
Description
Background
Many mobile devices, such as smartphones, are equipped with cameras for capturing pictures and digital images. Mobile devices also provide access to a variety of information. This information can be viewed in a Web browser or native application running on the mobile device. The user may also use the image to obtain additional information about the object or location depicted in the image.
A virtual assistant is an application that performs a task or service for a user. The virtual assistant may be integrated in a voice assistant device (e.g., a smart speaker or other voice-controlled device). Some virtual assistants also act as smart devices that control other smart devices, such as home automation devices. For example, a user may control the operation of a light by providing voice commands to a voice assistant device configured to control the light.
Disclosure of Invention
This specification describes technologies relating to identifying a smart device and presenting one or more user interface controls for controlling the identified smart device.
In general, one innovative aspect of the subject matter described in this specification can be embodied in the following methods: the method comprises the following steps: receiving image data of an image captured by a camera of a mobile device of a user; determining, based on the image data, that the image depicts at least one of a smart device or a physical control of the smart device; in response to determining that the image depicts the smart device or at least one of physical controls of the smart device, identifying one or more user interface controls for controlling the smart device; generating and presenting one or more user interface controls for controlling the smart device on a display of the mobile device; detecting, on a display of a mobile device, a user interaction with at least one of one or more user interface controls; and controlling the smart device based on the detected user interaction. Other implementations of this aspect include corresponding apparatuses, methods, systems, and computer programs configured to perform the actions of the methods encoded on computer storage devices.
These and other implementations can each optionally include one or more of the following features. In some aspects, controlling a smart device comprises: data is sent to a virtual assistant control device operatively connected to the smart device, the data causing the virtual assistant control device to send an operation corresponding to the detected user interaction to the smart device to perform the operation.
In some aspects, determining, based on the image data, that the image depicts at least one of a smart device or a physical control of the smart device includes: the determination image depicts a virtual assistant control device configured to control other smart devices. In some aspects, the method may further comprise: it is determined that the mobile device is capable of controlling the virtual assistant control device while the mobile device is rendering the image. In some aspects, determining that the mobile device is capable of controlling the virtual assistant control device while the mobile device is presenting the image comprises: the virtual assistant control device is determined to be the particular virtual assistant control device registered with the user account by determining that the mobile device is within a threshold distance of the particular virtual assistant control device registered with the user account when the user device is rendering an image. In some aspects, in response to determining that the mobile device is capable of controlling the virtual assistant control device while the mobile device is presenting the image, one or more user interface controls for controlling the smart device are determined to be presented.
In some aspects, determining, based on the image data, that the image depicts at least one of a smart device or a physical control of the smart device includes: the determination image depicts an image of a virtual secondary control device of the user. In some aspects, the virtual assistant control device includes a virtual assistant smart speaker device that receives voice commands from a user and provides information to the user using speakers of the virtual assistant smart speaker device. In some aspects, identifying one or more user interface controls for controlling a smart device includes: determining a task or service that the virtual assistant smart speaker device is currently performing; and selecting one or more user interface controls for controlling the task or service.
In some aspects, generating and presenting, at a mobile device, one or more user interface controls for controlling a smart device includes: one or more user interface controls are superimposed in an augmented reality manner over a field of view of a viewfinder of a camera of a mobile device.
In some aspects, in response to determining that the image depicts at least one of a smart device or a physical control of the smart device, identifying one or more user interface controls for controlling the smart device includes: a registry file for the smart device is obtained, wherein the registry file includes data specifying a type of the smart device and available user interface controls for controlling the smart device.
The subject matter described in this specification can be implemented in particular embodiments to realize one or more of the following advantages. By facilitating a platform that can identify smart devices in an image and present virtual user interface controls for the identified smart devices, the platform allows virtual user interface controls to be presented that have been customized or adjusted for the object, its capabilities, and/or the context in which the object is currently operating.
Using a conventional user interface, a user may need to scroll and toggle views multiple times to find the correct data/functionality or to provide multiple voice commands to the voice assistant device. When a user views smart devices on a mobile device (e.g., using augmented reality technology), the virtual assistant application of the present system can present virtual user interface controls for the smart devices without having to find a separate application for each smart device. For example, when a virtual assistant control device (e.g., a voice assistant device, such as a smart speaker) at a user's home, office, or other location is playing music, the user may point the camera of the mobile device at the virtual assistant control device. The virtual assistant application (e.g., used alone or with the assistance of a remote system) may detect the virtual assistant control device based on image data received from the camera and present user interface controls that enable the user to control the audio currently being played. By selecting user interface controls based on the smart device detected in the image data and the current mode or action of the smart device, the virtual assistant application may provide a customized user interface that provides efficient and effective control of the smart device. For example, attempting to adjust volume or play music using a voice control may require multiple voice commands and adjustments by the smart speaker to obtain the appropriate volume or play changes. User interface controls presented by the mobile device may make the same changes faster and more intuitive for the user.
The systems and techniques described herein may identify a smart device from image data (e.g., a single frame image, continuous video, image stream, etc.) from a camera of a mobile device. Once the smart device is identified, the mobile device can index the results. This prevents the user from having to make multiple requests, thereby reducing the number of times the computing system processes the requests to identify the smart device and determine which user interface controls (e.g., audio controls for music playing, light switch controls for lights, etc.) should be presented for a particular smart device application. This may significantly improve the computer functionality of a virtual assistant management system that selects user interface controls by reducing the processing requirements for computer settings, as aggregated across many users. The reduction in the number of requests may also reduce the amount of bandwidth consumed since the virtual assistant management system may be accessible over a network, thereby providing bandwidth for other network traffic and/or increasing network speed.
By selecting the appropriate user interface control based on the smart device, the user is not burdened with having to select the control each time the camera of the mobile device is pointed at the same smart device. This allows faster requests by the user and greater flexibility by the user. For example, using the virtual assistant application, a user may hold the mobile device, point the camera at the smart device, and present user interface controls that control certain features of the smart device without further interaction with the mobile device or the smart device. Thus, the systems and techniques described herein provide a guided human-machine interaction process for controlling smart devices.
Various features and advantages of the foregoing subject matter are described below with reference to the drawings. Other features and advantages will be apparent from the subject matter described herein and from the claims.
Drawings
FIG. 1A is a block diagram of an example environment in which a virtual assistant application identifies a smart device and presents user interface controls for controlling the smart device.
FIG. 1B is an example system flow diagram of an example process for a virtual assistant application to present user interface controls for an identified smart device.
Fig. 2 is a sequence of example screen shots presenting a mobile device for controlling one or more user interface controls of an identified smart device based on received image data.
Fig. 3 is another sequence of example screen shots presenting a mobile device for controlling one or more user interface controls of an identified smart device based on received image data.
FIG. 4 is a flow diagram of an example process for identifying a smart device using a virtual assistant application and presenting one or more user interface controls for controlling the identified smart device.
FIG. 5 is a block diagram of an example computer system that may be used to implement the methods, systems, and processes described in this disclosure.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
Systems, methods, and computer program products are described for identifying smart devices (or physical controls of smart devices) based on image data (e.g., a single frame image, a continuous video, a stream of images, etc.) using a virtual assistant application, and for each identified smart device, presenting one or more user interface controls for controlling the smart device. For example, the smart device or a physical control of the smart device may be identified (e.g., recognized using object recognition techniques) in image data representing a viewfinder of a camera of the mobile device. In response, user interface controls for controlling the smart device may be presented within the viewfinder, for example, using augmented reality techniques, so that the user may control the smart device. Smart devices are electronic devices that are connected to other devices through one or more networks. Smart devices may operate autonomously and/or interactively with other smart devices. Exemplary smart devices include virtual assistant control devices (e.g., voice control devices such as smart speakers), home automation devices (e.g., smart lights, smart appliances, thermostats, etc.), smart televisions, and smart radios, to name a few. A virtual assistant control device is an electronic device that includes an integrated virtual assistant that performs tasks and/or services for a user and controls other intelligent devices. An example virtual assistant control device is a smart speaker that performs tasks and controls other smart devices in response to voice commands.
For example, a user may point a camera of the mobile device at the smart speaker and view user interface controls for controlling the smart speaker or other smart devices that the smart speaker has been configured to control (e.g., other smart devices registered with the user account). In another example, a user may point a camera of a mobile device at a smart light or light switch and view user interface controls for adjusting the smart light.
In some aspects, the mobile device connects to the smart device through virtual assistant control devices (e.g., smart speakers), where each virtual assistant control device has been connected to the same network. In some aspects, the mobile device may connect to the smart device without connecting to or through the virtual assistant control device, such as where a particular smart device is configured to communicate data over the same network (e.g., wireless network) as the mobile device.
According to some aspects, a user may register a virtual assistant control device with a user account. In another example, the user may also register other smart devices with the account so that the virtual assistant control device may control each registered smart device, and the user may configure the virtual assistant control device to control one or more smart devices, e.g., on the virtual assistant control device. Then, if the light is a smart device (or connected to a smart device) that has been registered with a user account or otherwise controlled by the virtual assistant control device, the user may request the virtual assistant control device to play music on its own speaker or request the virtual assistant control device to turn off the light in a certain room. The virtual assistant control device may control the smart device by sending control data to the smart device over a wireless network or by sending control data to a virtual assistant management system that relays the control data to the smart device.
The virtual assistant application described herein may determine whether a smart device (such as a virtual assistant control device or a smart light switch) detected in the image data is associated with a user of the mobile device by determining that the location of the mobile device at the time the image is presented on the mobile device (or at the time the image is captured) is within a threshold distance of a virtual assistant control device registered with the user account. For example, the system may use the geographic location of the mobile device and the location of the virtual assistant control device registered with the user account to determine whether the particular smart device represented by the image data is the user's smart device. The virtual assistant application may then control the smart device using the virtual assistant to control the connection between the device and the particular smart device through the generated interface controls. The application of the smart device may be used to configure and connect a particular smart device to a network, such as a WiFi network. The applications of the virtual assistant control device may be used to connect to and control smart devices on the same WiFi network and further configure the particular smart device (e.g., add a nickname, assign it to a room, etc.).
According to some aspects, the virtual assistant application may connect to the smart device without the virtual assistant controlling the device. For example, the virtual assistant app may determine whether the user of the mobile device may access the home network to which the smart device is registered (e.g., grant access to the home network via WiFi passwords, etc.). The virtual assistant app described herein may determine whether a smart device (such as a smart light switch or a smart thermostat) identified in image data is registered with a user by determining that the location of the mobile device is within the location of the smart device (such as the same network) and has been registered with a user account at the time the image is presented on the mobile device (or at the time the image is captured). For example, the system may use the geographic location of the mobile device and access the registration file through the virtual assistant management system to determine whether to allow the mobile device to access a particular smart device. The registration file includes data about the particular smart device, including but not limited to identification and control information. The virtual assistant application may use the data in the registry file to communicate with and control a particular smart device.
The user interface controls described herein may be generated and presented through a user interface of the mobile device, for example, through a virtual assistant application executing on the mobile device. The present system may overlay user interface controls over a real-time image (e.g., a digital viewfinder) presented by a user interface of the mobile device or over a previously captured image using an Augmented Reality (AR) module or the like. For example, a user may view a smart light switch in real-time in a viewfinder of a camera of a mobile device and may virtually view user interface controls superimposed on a real-time image.
In operation, the virtual assistant application receives image data and location data that specifies the location of the mobile device that is executing the virtual assistant application. An object recognizer component of the virtual assistant application (or remote server) may analyze the image data and generate recognized object data. The identified object data specifies an image area occupied by the object. In some aspects, the object recognizer component may determine whether the object is a smart device and embed this information into the identified object data. A smart device control selector of the virtual assistant application (or remote server) receives the identified object data from the object recognizer component and determines whether the image data depicts a specified object, such as a smart device, and if the smart device is identified from the identified object data, the smart device control selector receives location data from the mobile device. The smart device control selector may then determine that the location of the mobile device while presenting the image (or image data of the captured image) is within a threshold distance (e.g., 50 meters, 100 meters, or another suitable distance) of the location of the virtual assistant control device registered with the user account. In some aspects, the system may determine whether the smart device is associated with a user account and whether the virtual assistant control device is capable of controlling the smart device.
In some aspects, if the smart device is identified from the identified object data, the smart device control selector receives location data from the mobile device and then processes the registration file to determine if the smart device is registered to the user account, e.g., if the voice assistant control device is not found on the network. The registration file may be stored on the user device (if known) or may be accessed through a back-end server in the virtual assistant management system, such as a smart device registration engine.
After the smart device control selector determines that the location of the mobile device is within a threshold distance of the location of the virtual assistant control device registered with the user account, the smart device control selector identifies (e.g., selects) one or more user interface controls for controlling the smart device and generates interface control identification data. The interface control identification data includes data specifying one or more controls that can control the identified smart device. For example, if the smart device is a light switch, the controls may include an open icon, a close icon, and/or a dimmer. The interface control identification data is received by the interface generator. For each user interface control belonging to the smart device of interest specified by the interface control identification data, the interface generator generates presentation data that presents the user interface control on the user interface.
The received image data may be from real-time continuous video processed in real-time, which will continue to identify objects of interest within the camera lens field of view even if the camera is moving. In some aspects, the received image data may be a single frame shot (frame shot) or a recorded video.
These functions and others will be described in more detail below.
FIG. 1A is a block diagram of an example environment 100A in which a virtual assistant application 116 identifies smart devices and presents user interface controls for controlling the smart devices. FIG. 1B is an example system flow diagram of an example process by which the virtual assistant application 116 presents user interface controls for an identified smart device.
The virtual assistant application 116 may be installed on and/or executed by the mobile device 110. Mobile device 110 is an electronic device capable of sending and receiving data over data communication network 150. Exemplary mobile devices 110 include smart phones, tablet computing devices, wearable computing devices (e.g., smartwatches), and other devices that can send and receive data over the network 150. The network 150 may include a Local Area Network (LAN), a Wide Area Network (WAN), the internet, a mobile network, or a combination thereof.
The virtual assistant application 116 may be implemented as a native application developed for a particular platform or a particular device. The virtual assistant application 116 may perform tasks or services for the user of the mobile device 110. For example, the virtual assistant application 116 may respond to the user's voice commands (e.g., provide requested information), control the user's smart device, play content (e.g., music or video), and so forth. To make it easier and more efficient for a user to control a smart device, the virtual assistant application 116 may present graphical user interface controls on the display of the mobile device 110.
The virtual assistant application 116 may present user interface controls for a smart device that is identified in image data representing a scene in the field of view of the camera 111 of the mobile device 110. For example, the virtual assistant application 116 may identify smart devices and determine whether to provide user interface controls for the identified smart devices. The virtual assistant app 116 may determine whether to present user interface controls for the identified smart device based on whether the identified smart device is registered with the user account and/or whether the mobile device 110 is within a threshold distance of the virtual assistant control device registered with the user account. The virtual assistant application 116 may also select user interface controls (e.g., audio controls for music, toggle controls for smart device light switches, etc.) of the smart device to be used for recognition and present the interface with the selected controls, e.g., as a graphical overlay over image data (e.g., an image of the camera 111 or an object depicted in a viewfinder).
The virtual assistant application 116 may attempt to detect the smart device from the image data received from the camera 111 of the mobile device 110, e.g., continuously without receiving a user request to identify the smart device. For example, the virtual assistant app 116 may detect and/or recognize an object in the viewfinder of the camera 11 of the mobile device 110 (based on image data) and interpret the fact that the user pointed the camera 111 at the smart device as requesting control of the identified smart device.
In some implementations, the virtual assistant application 116 may receive commands for positioning and selecting smart device controls via one or more inputs. For example, virtual assistant application 116 may receive a voice request from microphone 113 of mobile device 110. The virtual assistant application 116 may include a speech recognizer 120, and the speech recognizer 120 may receive audio input from the microphone 113 and convert the audio to text (if the audio includes spoken words) and provide the text to the virtual assistant application 116.
In some implementations, the virtual assistant app 116 can receive a command identifying a smart device through user interaction (e.g., touching) with a button on an interface of the virtual assistant app 116 displayed on the mobile device 110 to initiate the processes described herein. In some implementations, the virtual assistant application 116 can also receive text requests entered using, for example, a physical or touch keypad of the mobile device 110.
The virtual assistant application 116 includes a smart device control selector 130 that determines whether a smart device is present in an image (e.g., pixel data representing the image) and selects a user interface control based on the identified smart device, its capabilities, and/or the context in which the smart device is currently operating. In certain aspects, the virtual helper app 116 may receive the identified object data 126 from an object recognizer 125 (further described herein) while the virtual helper app 116 is in an active state. For example, when the virtual helper app 116 is launched, the virtual helper app 116 may begin obtaining viewfinder pixel data from the camera 111. The smart device control selector 130 may monitor the data until the smart device control selector 130 has sufficient information to select a user interface control. For example, the smart device control selector 130 may monitor the identified object data 126, and/or other data, such as the location data 124, of the image stream until the smart device control selector 130 obtains sufficient data to make a decision as to whether or not to present the user interface control that it controls to select.
In some implementations, the smart device control selector 130 can select a user interface control based on the identified object data 126 generated by the object recognizer 125 based on the image data 123 of the viewfinder of the camera 111. The image data 123 may include pixel data representing a current scene of the viewfinder of the camera 111. After launching the virtual assistant application 116, the virtual assistant application 116 may obtain image data 123 from the camera 111. For example, the virtual helper app 116 may obtain a stream of pixel data sets. Each pixel data set may represent a pixel of the viewfinder at a particular point in time. The pixel data in each pixel data set may include visual characteristic (e.g., color, intensity, brightness, etc.) data that specifies each pixel of the viewfinder.
The smart device control selector 130 may select a user interface control based on whether the smart device (or a physical control of the smart device) is detected in the viewfinder (e.g., in one or more pixel data sets) and, if so, the identity of the smart device or the category of the smart device. For example, the virtual assistant application 116 may include an object recognizer 125 that attempts to detect and recognize (e.g., identify) the smart device (or its physical controls) in the pixel data (or image). The object identifier 125 may use edge detection and/or other object identification techniques to detect various objects, such as smart speakers, appliances, televisions, physical controls (e.g., light switches, door handles, thermostats, oven/stove controls, etc.), and/or other types of objects. For some smart devices, the smart device and the physical controls for the smart device may be the same (e.g., a smart thermostat, a smart coffee pot, etc.). As described below, the object recognizer 125 may provide the identified object data 126 to the smart device control selector 130, the identified object data 126 identifying the smart device (if any) identified in the image data 123.
In some implementations, the object recognizer 125 includes a coarse classifier that determines whether the pixel dataset includes objects in one or more particular classes (e.g., classes) of objects. For example, the coarse classifier may detect that the pixel dataset includes an object of a particular class (e.g., a class of a smart device such as a lighting control) with or without identifying an actual object. In some aspects, the object recognizer 125 may determine from the image data 123 whether a smart device is in the image data and generate data specific to the identified smart device.
The coarse classifier may detect the presence of the class of objects based on whether the image includes one or more features indicative of the class of objects. The coarse classifier may include a lightweight model that performs low computational analysis to detect the presence of objects within its object class. For example, the coarse classifier may detect, for each object class, a limited set of visual features depicted in the image to determine whether the image depicts objects falling within the object class. In a particular example, the coarse classifier may detect whether the image depicts an object classified into one or more of the following classes: smart speakers, appliances, televisions, physical controls (e.g., light switches, door handles, thermostats, oven/range controls).
In some implementations, the coarse classifier uses a trained machine learning model (e.g., a convolutional neural network) to classify images based on their visual features. For example, machine learning models may be trained using labeled images that are labeled with their respective categories. The machine learning model may be trained to classify images as zero or more of a particular set of object classes. The machine learning model may receive as input data relating to visual features of the image and output to a classification of zero or more object classes in a particular set of object classes.
The coarse classifier may output data specifying whether an object class has been detected in the image. The coarse classifier may also output a confidence value indicating a confidence that the presence of the object class has been detected in the image and/or a confidence value indicating a confidence that an actual object (e.g., a particular type of smart speaker) is depicted in the image.
As shown in fig. 1B, the user is pointing the camera 111 of the mobile device 110 at the object 115. The object 115 is presumably a smart device that the user wishes the virtual assistant application 116 to generate and render controls. Using the virtual assistant application 116, the user is holding the mobile device 110 such that the object 115 is in the field of view of the camera 110 and the object 115 is presented to the user in the user interface 136 of the virtual assistant application 116. For example, the user interface 136 of the virtual assistant application 116 may include a viewfinder of the camera 111. In a particular example, when the virtual assistant app 116 is in an image mode, the virtual assistant app 116 may present a viewfinder for which the virtual assistant app 116 may present user interface controls for smart devices detected in the image and/or provide content related to objects detected in the image.
The object recognizer 125 receives image data 123 representing a scene within the field of view of the camera 111 (and presented in the viewfinder of the user interface 136). The location data may also be received by the smart device control selector 130, for example, from a Global Positioning System (GPS) receiver of the mobile device 110. The object recognizer 125 may provide the identified object data 126 to the smart device control selector 126 specifying whether an object is detected in the image data 123 and, if an object (such as a particular smart device) is recognized, data identifying the recognized object. When using a coarse classifier, object recognizer 125 may provide smart device control selector 130 with data specifying whether the presence of an object within at least one of the object classes has been detected and, if so, the detected class. As described above, the virtual helper app 116 may receive a stream of pixel data sets. In this example, for each pixel data set (or at least a portion) in the stream, the object recognizer 125 may evaluate each pixel data set and provide that data (which specifies whether an object is detected and the identity of any recognized objects) to the smart device control selector 130.
In some implementations, the object recognizer 125 recognizes objects in the image data received from the mobile device 110 using a trained machine learning model (e.g., a convolutional neural network). For example, machine learning models may be trained using labeled images labeled with their respective smart devices and/or physical controls of the smart devices. The machine learning model may be trained to recognize and output data identifying a smart device and/or a physical control of the smart device depicted in an image represented by the image data. The machine learning model may receive as input data relating to visual features of the image and output data identifying the smart device depicted in the image or the smart device whose physical controls are depicted in the image.
If a smart device is identified, the smart device control selector 130 may select a user interface control for the identified smart device based on the identified actual smart device, the class of smart device detected in the pixel dataset, the capabilities of the smart device, and/or the context in which the smart device is operating. For example, if a smart device is detected in the viewfinder (based on the viewfinder's pixel data set), the user is more likely to request user interface controls from content in the viewfinder than if there were no detectable smart device in the viewfinder. Thus, if a smart device or smart device class is detected in the pixel data, the smart device control selector 130 may select a user interface control.
Each smart device or class of smart devices may include one or more corresponding user interface controls. The user interface controls for each smart device or class of smart devices may be stored in the user interface control index 140. For example, a smart light may have corresponding user interface controls that enable a user to turn the light on and off. Dimmable and non-dimmable lights may have different user interface controls. For example, the user interface for a dimmable light may include a rotary dimmer or a slider bar that the user may adjust to adjust the intensity of the light, and if the user points the camera 111 at the toggle light switch and the object recognizer 125 detects that the camera 111 is pointing at the toggle light switch, the smart device control selector 130 may determine that the light is a light that may be turned on and off but cannot be dimmed. Thus, the smart device control selector 130 may select a user interface control that enables a user to turn the light on and off, but not adjust the intensity of the light. Similarly, if the object recognizer 125 recognizes a dimmer, the smart device control selector 130 may select a user interface control that enables a user to adjust the intensity of the light.
Some smart devices (such as virtual assistant control devices) may perform a number of different tasks or execute a number of applications. The smart device control selector 130 may select user interface controls for the smart devices based on the context in which the devices are currently operating. For example, if the smart speaker is playing music when the object recognizer 125 recognizes the smart speaker in the viewfinder of the camera 111, the smart device control selector 130 may select user interface controls for controlling the music, such as controls for adjusting volume, playing different songs, fast forwarding, and so forth. If the smart speaker is currently controlling a home appliance (e.g., an oven), the smart device control selector 130 may select a user interface control that enables a user to control an application (e.g., change the temperature of the oven).
In some implementations, the smart speaker may have a corresponding user interface for operating the smart speaker. The user interface may correspond to a user interface presented by a smart speaker equipped with a graphical display. For example, some smart speakers may include an optional display. In this example, if the user owns a smart speaker that does not have a display, the virtual assistant app 116 may present a user interface that corresponds to the user interface that will be presented by the display when the user points the camera 111 at the smart speaker.
The user interface of the virtual assistant control device (e.g., smart speakers) may include user interface controls for each smart device registered with the user account. For example, the virtual assistant app may select and present user interface controls (described below) for controlling the virtual assistant control device in response to identifying the virtual assistant control device in the image data 123 representing the viewfinder of the camera 111 and determining that the mobile device is within a threshold distance of the virtual assistant control device. These user interface controls may enable a user to select from smart devices registered with a user account. For example, if the user has configured the virtual assistant control device to control a smart oven and a smart light, the user interface controls may cause the user to select any of these devices in response to a selection, and the virtual assistant application 116 may present the user interface controls for controlling the selected device. In another example, the virtual assistant app 116 may present a main panel for all (or at least a plurality of) the smart devices, enabling the user to control each smart device from the main panel.
In some implementations, when a virtual assistant control device is identified in the image data 123, the virtual assistant application 116 can enable a user to select from among the registered smart devices and configure the user interface controls of the smart devices to be presented. For example, the virtual assistant application 116 may present a list of registered smart devices and enable the user to select from the list. In response, each time a virtual assistant control device is identified in the image data 123, the virtual assistant application 116 may present user interface controls for the selected smart device.
The virtual assistant application 116 may also enable the user to present content or user interface controls in response to recognizing other objects in the image data 123. For example, the virtual assistant app 116 may enable a user to configure a dashboard that presents content (e.g., weather information, sports information, financial information, etc. obtained from one or more web pages or other resources) in an AR within the user interface of the virtual assistant app 116 whenever a particular wall or other object is detected in the image data 123. In another example, whenever a particular wall or other object is detected in the image data 123, the user may configure the virtual assistant application 116 to present a real-time web page or application in an AR manner within the user interface of the virtual assistant application 116. The user may make these configurations by pointing the camera 111 at a wall or a particular object, selecting an icon or other user interface control for assigning content to the particular object, and then selecting the content (e.g., by providing one or more URLs, selecting a bookmark, etc.).
In some implementations, the user can use voice commands in conjunction with an image of the smart device to register the smart device to be controlled by the virtual assistant application 116. For example, the user may point the camera 111 at the smart device and speak a voice command (e.g., this is my stereo). In response, the virtual assistant application 116 may associate the image of the smart device with the name of the smart device (e.g., stereo). If the user points the camera 111 at the smart device, the virtual assistant application 116 may identify user interface controls of the smart device and enable the user to control the smart device using these controls.
In some implementations, the virtual assistant application 116 creates an instance of the smart device for a particular location in response to a voice command. For example, the virtual assistant application 116 may also identify a location of the smart device in response to detecting the voice command and associate the location with an image of the smart device and a name of the smart device. If the user points the camera 111 at the smart device (or a copy thereof) when the smart device is located at a different location (e.g., greater than a threshold distance from the location of the smart device), the virtual assistant application 116 may determine not to present the user interface controls of the smart device. If the user points the camera 111 at the smart device while the mobile device 110 is near the smart device (e.g., within a threshold distance of the smart device), the virtual assistant app 116 may present user interface controls of the smart device.
The smart device control selector 130 may also select a user interface control based on, for example, a sequence of pixel data sets in the stream. For example, if an object identified in a pixel dataset changes within a short period of time (e.g., 2-5 seconds), the user may be moving the mobile device 110 around without attempting to gain control of a particular smart device. However, if the same smart device is recognized in the sequence of pixel data sets, the user is more likely to request control of a particular smart device.
The smart device control selector 130 may also determine whether to present and/or select a user interface control based on the location of the mobile device 110, for example, relative to the virtual assistant control device. For example, if the mobile device 110 is located near the user's virtual assistant control device (e.g., within a threshold distance) while the mobile device 110 is presenting images, the user may be requesting control of the smart device in the viewfinder and may select a particular user interface control. If the mobile device is farther away from the user's virtual assistant control device (e.g., farther than a threshold distance), the viewfinder is less likely to view the smart device associated with the user. This also prevents the virtual assistant app 116 from presenting user interface controls in the event of an accidental capture of another user's virtual assistant control device, as the virtual assistant app 116 may not be able to control the other user's virtual assistant control device. In such a case, the presentation of the user interface controls may frustrate the user.
In another example, the user may access a previously captured image, such that in this example the user may control the smart device, for example, from a location remote from the virtual assistant control device, the smart device control selector 130 may access the location of the mobile device 110 at the time the image was captured. The location may be stored with the image, for example as metadata. If the location of the mobile device 110 at the time the image was captured is within a threshold distance of the location of the virtual assistant control device, the smart device control selector 130 may determine that the mobile device 110 may control the smart device recognized at the image and present user interface controls for controlling the smart device.
To determine whether the mobile device 110 is within a threshold distance of the virtual assistant control device, the virtual assistant application 116 may obtain location data of the mobile device 110 from the mobile device's GPS receiver and may obtain the location data of the virtual assistant control device from the virtual assistant control device, the virtual assistant management system 180, or an initial configuration of the virtual assistant control device. In an example, the virtual assistant control device may periodically send data specifying its current location to the virtual assistant management system 160. The virtual assistant application 116 may compare the location of the mobile device 110 to the location of the virtual assistant control device to determine whether the mobile device 110 is within a threshold distance of the virtual assistant control device.
The smart device control selector 130 may also determine whether to present the user interface controls of the smart device based on whether the smart device matching the recognized smart device has been registered with a user account (e.g., a virtual assistant account of the user). In this way, the virtual assistant app 116 does not provide user interface controls for smart devices that have not yet been set up to be controlled by the virtual assistant app 116. For example, the user may use the virtual assistant application 116 to register for smart devices controlled by the virtual assistant control device, such as smart speakers in the user's home. The virtual assistant application 116 may store data about each registered smart device locally, for example, in the control index 140 or another index. The data may include the name of the device, the class of the device, the capabilities of the device, how to control the device, e.g., by controlling a wireless connection between the device and the device via a virtual assistant, and/or other suitable data. In some implementations, the data for the device can also include an image of the device and/or an image of a physical control for controlling the device. The virtual assistant application 116 may provide this data to a virtual assistant management system 160 that manages the user's virtual assistant account.
The smart device control selector 130 may also select a user interface control based on the location of the mobile device 110 (e.g., the geographic location of the mobile device 110). For example, the mobile device 110 may include a GPS receiver that determines the geographic location of the mobile device 110 and provides location data 124 specifying the location of the mobile device 110 to the virtual assistant application 116. The smart device control selector 130 may use the geographic location to determine whether the mobile device is located in a known location (such as their home) where the user's virtual assistant control device is located, or other area where the user owns a registered virtual assistant control device. When the mobile device 110 is located in one of these locations (or within a threshold distance associated therewith), such as a home network associated with a virtual assistant control device, the smart device control selector 130 may register the identified smart device with the virtual assistant control device if the smart device control selectors 130 are each associated with the same network.
The smart device control selector 130 may select a user interface control for controlling the identified smart device based on a user's history of using the user interface control for the identified smart device or similar smart devices. The smart device control selector 130 may also select the user interface control of the identified smart device based on the user-specified preferences of the user interface control. For example, the user may select a user interface control that the user prefers in certain situations. In a particular example, the virtual assistant application 116 may generate various user interface controls for the same object or the same object class. The user may select between various user interface controls, and the virtual assistant application 116 may store the selection for the user.
According to some aspects, the virtual assistant application 116 may send the image data 123 and/or the location data 124 to the virtual assistant management system 160 over the network 150 to perform one or more of the processes discussed herein. For example, using image data 123 and location data 124, virtual assistant management system 160 may determine whether the received image data depicts a smart device, determine that the location of mobile device 110 at the time of capturing image data 123 is within a threshold distance of the location of a virtual assistant control device (e.g., smart speaker) registered with the user account, and select one or more user interface controls for controlling the smart device if mobile device 110 is within the threshold distance of the virtual assistant control device. The virtual helper app 116 may send one or more pixel data sets to the virtual helper management system 160. The pixel data set may include pixel data for selecting a viewfinder of the smart device control and/or a pixel data set captured after selecting the image smart device control. For example, the virtual assistant application 116 may first send a pixel data set for selecting a smart device control. If the viewfinder is subsequently directed to a different smart device, the virtual assistant application 116 may send the pixel data for the different smart device to the virtual assistant management system 160.
The virtual assistant management system 160 includes one or more front-end servers 161 and one or more back-end servers 162. The front-end server 161 may receive data from the mobile device 110 and provide the data to the back-end server 162. Front-end server 161 may also transmit content to mobile device 110 in response to receiving the data.
For example, if the image data 123 received from the mobile device 110 represents an image of a virtual assistant control device (e.g., a smart speaker) and the audio received from the mobile device 110 represents music, the smart device control selection engine 165 may identify the smart device controls represented by the voice assistant device and select the smart device controls for audio related to the particular voice assistant device (e.g., adjustable volume trigger, forward, back, title, like/dislike buttons, etc., as shown in fig. 2). If the image data 123 received from the mobile device 110 represents an image of a lamp or lamp controller (e.g., a physical lamp switch), the smart device control selection engine 165 may select a user interface control (e.g., a virtual adjustable dimmer trigger, an ON/OFF button, etc.) for controlling the lamp, as shown in FIG. 3.
In some embodiments, the particular smart device for which the smart device control selection engine 165 is generating user interface controls is determined to be registered in the account of the user of the mobile device 110 as a registered user of the virtual assistant control device, and the user interface controls are associated with voice-activated controls that the virtual assistant control device is capable of controlling.
The back-end server 162 may provide the user interface control selected by the smart device control selection engine 165 to the front-end server 161. The front-end server 161, in turn, can provide the smart device control to the mobile device 110 from which the data for selecting the smart device control was received.
In some embodiments, the backend server 162 may include a smart device registration engine that selects a registration file and provides it to the mobile device 110 in response to data received from the mobile device 110. The smart device registration engine may select a registration file for a plurality of smart devices. For example, if the smart device control selection engine 165 can identify a smart device represented in the pixel data, the smart device registration engine can determine whether the registration file is associated with the smart device through a user account of the mobile device 110. For example, an Original Equipment Manufacturer (OEM) of smart devices may require a user to register the device through their home network. In some embodiments, the OEM may associate and register the smart device with the user's virtual assistant control device, or may access the smart device through a native application or Web browser managed by the OEM. In certain aspects, the virtual assistant management system 160 may store a registry file that the OEM allows access over the user's home network.
In some embodiments, the virtual assistant management system 160 may allow a particular mobile device 110 to control the smart device through the virtual assistant application 116 without having to associate a virtual assistant control device by determining whether the particular smart device is registered to the user account of the mobile device 110. In some implementations, it may be desirable to grant permission to the user in the registry file to access the particular smart device. For example, a user may set up a smart device light controller through an OEM setup process without connecting through a virtual assistant control device. The virtual assistant application 116 can access the smart device registration engine and retrieve a registration file associated with the identified smart device and control the smart device light controller based on the location data 124 of the mobile device 110 and account information associated with the user in the registration file. In some embodiments, the virtual assistant application may store the registration file on the mobile device.
The virtual assistant application 116 includes a user interface generator 135 that generates and presents a graphical user interface. The user interface may include user interface controls for controlling the smart device that has been recognized in the image data 123 represented in the viewfinder of the camera 11. As described above, the user interface of the virtual assistant application 116 may present a viewfinder for the camera 111. As shown in fig. 1B, smart device control selector 130, upon selecting one or more user interface controls for controlling the smart device, generates and sends interface control identification data 131 to user interface generator 135. Interface control identification data 131 includes data for a selected user interface control that interface generator 135 may use to present the selected user interface control (e.g., a different type of control, a different icon serving as a control, a different layout, etc.). User interface generator 135 generates, updates, and presents user interface 136 on the display of mobile device 110 based at least in part on interface control identification data 131. For example, the user interface generator 135 may present the selected user interface control on a real-time image (e.g., in a viewfinder of the camera 111).
Then, in some implementations, the user may interact with the user interface controls to control the smart device, and the virtual assistant application 116 detects the user interaction and provides data specifying the user interaction to the user's virtual assistant control device. For example, the mobile device 110 may send data to the virtual assistant control device over the wireless network. The virtual assistant control device may then control the device over the wireless network. In another example, the virtual assistant control device may send control data to the virtual assistant management system 160, which in turn controls the smart devices based on the control data, e.g., by sending the control data to the smart devices over the network 150.
In some implementations, the virtual helper app 116 sends control data to the virtual helper management system 160, e.g., over the network 150, without sending the data to the virtual helper control devices. The virtual assistant management system 160 may then send the control data to the smart devices, as described above.
In some implementations, the virtual assistant application 116 may allow multiple users to communicate with each other and leave messages or other content media on the identified objects for communication purposes. For example, a user of the same home network (or the same virtual assistant account) may register a recognized object, such as a refrigerator, and place a virtual note (note) (e.g., using augmented reality technology) for another user or users to access and view in the viewfinder using the virtual assistant application 116. In some embodiments, the user may specify that a particular user has access to the virtual notes, or the user may allow anyone on the home network to view the virtual notes.
Fig. 2 depicts a sequence of example screen shots 210 and 220 of a mobile device presenting one or more user interface controls for controlling an identified smart device based on received image data. A first screenshot 210 depicts an example user interface 212 of a mobile device. In this example, the user is viewing the voice assistant device 215 (e.g., smart speaker) through a viewfinder of a camera presented in the user interface 212, and the voice assistant device 215 is playing music. The user interface 212 may be generated and presented by a virtual assistant application (e.g., the virtual assistant application 116 of FIG. 1).
A second screenshot 220 presents a user interface 222 of the virtual assistant application 116. In this example, user interface 222 presents user interface controls 224 overlaid on the current view of user interface 222. In particular, the user interface control 224 presents a window around the recognized object, which in this example is the voice assistant device 215. The virtual assistant application 116 may identify the voice assistant device 215 from the image data representing the viewfinder. As described above, the virtual assistant app 116 may also determine whether to present the user interface controls of the voice assistant device based on whether the location of the mobile device is within a threshold distance of the location of the user's voice assistant device.
Within the user interface controls 224, there are many example smart device controls for controlling music audio. In particular, a volume switch bar 226 is shown to control the volume level of the voice assistant device 215. In addition, the music control button 228 is shown with several different options that may be presented to the user, including but not limited to a pause/play button, like and dislike, previous song button, next song button, and a placeholder toggle bar for moving to a particular point in the currently playing audio. The options for the music control button 228 may depend on the particular music application being used by the voice assistant device 215. In some aspects, the available buttons may be generic to all music playing applications. In some aspects, the buttons may be customized to a particular music playing application and may be updated by the virtual assistant management system 160.
The virtual assistant application 116 may select a music control based on the context of the voice assistant device 215. For example, the virtual assistant application 116 may communicate with the voice assistant device 215 and request the current operating mode, the current tasks performed by the voice assistant device 215, or an application (e.g., a music application) that the voice assistant device 215 is currently using. The voice assistant device 215 may provide data to the virtual assistant app 116 that specifies the current operating mode or current task being performed, and in turn, the virtual assistant app 116 may select user interface controls that enable the user to control the current operating mode or current task. In this example, the voice assistant device 215 is playing music and the virtual assistant app 116 selects a music control in response to receiving data specifying that the voice assistant device 215 is playing music. In some implementations, the virtual assistant application 116 may determine that the voice assistant device 215 is playing music based on the detected audio.
If the user interacts with (e.g., selects) music control buttons 228, virtual assistant application 116 may communicate with voice assistant device 215 to control the music playing application. According to some aspects, if the user moves the viewfinder away from the voice assistant device 215, the virtual assistant app 116 will remove the user interface control 224 from the user interface 222 because it no longer detects the smart device, and in particular, the voice assistant device 215, in the image data. In some aspects, the virtual assistant application 116 may freeze the image of the voice assistant device 215 in the user interface 222. For example, the virtual assistant application 116 may present the user with a lock screen or lock image button that, when interacted with, causes the virtual assistant application 116 to freeze the image. In this example, the user may move the mobile device 110 without having to point the camera directly at the smart device, but may continue to use the user interface controls 224 to control the smart device. In some implementations, the virtual assistant application 116 may freeze the image in response to the user's interaction with one of the controls, as this indicates that the user wishes to use the control to control the voice assistant device 215.
In some aspects, the user interface control 224 may present content or an animation that indicates that the virtual assistant application 116 is attempting to identify content. For example, the user interface 222 may present a bounce animation that cycles through the animation or signal the virtual assistant application to present to the user another animation of the smart device control being identified.
Fig. 3 depicts another sequence of example screen shots 310, 320, and 330 of a mobile device presenting one or more user interface controls for controlling an identified smart device based on received image data. A first screenshot 310 depicts an example interface 312 of a mobile device. In this example, the user is viewing the physical controls 315 of the smart device through the viewfinder. In this example, the smart device is a smart light and the physical control 315 is a light switch.
The second screen shot 320 and the third screen shot 330 present user interfaces 322 and 332, respectively, for the virtual assistant application 116. In these examples, user interfaces 322 and 332 present user interface controls 324 overlaid on the current view of user interface 322. In particular, the user interface controls 324 present a window under the physical controls 315 of the smart light. The virtual assistant application 116 identifies a physical control 315 from image data representing a viewfinder for a camera of the mobile device. As shown, within the user interface control 324, there are three exemplary smart device controls for controlling the lighting of the smart device 315. As shown, the screen shot 320 shows the smart device 315 with lighting control, the smart device 315 initially previously selected by the user to be "ON" (or representative of the current state of the smart lamp). In some examples, the user may select the "ON" toggle button 325 to turn ON the lamp control. If the user of the mobile device chooses to turn OFF the lights using the virtual assistant application 116, the user selects the "OFF" toggle button 334 as shown in screen shot 330. Further, a dimming switch bar 326 is shown to control the dimming level of the smart lamp.
In these examples, the smart device 315 has registered with the user account and may communicate with and be controlled by a virtual assistant control device (such as the voice assistant device 215 in FIG. 2). The virtual assistant application 116 will receive image data from the camera of the user's mobile device and determine that the received image data depicts the smart device. Next, the virtual assistant application 116 may determine that the location of the mobile device at the time the image data was captured is within a threshold distance of the location of the virtual assistant control device (such as the voice assistant device 215 in FIG. 2). Further, the virtual assistant application 116 may identify one or more user interface controls for controlling the smart device and present the one or more user interface controls (i.e., interface control 324) for controlling the smart device.
According to some aspects, the available buttons may be generic to all lighting control smart devices. In some aspects, the buttons may be customized to a particular lighting control smart device and may be updated by the virtual assistant management system 160.
According to some aspects, if other smart device controls are available (e.g., a second smart device such as a second light controller is in the image data), the user interface 322 may include user interface controls that allow the user to select the other smart device.
According to some aspects, the intelligent device in the image data is a timer or clock of a home appliance such as a furnace or microwave oven. In this example, the virtual assistant application 116 may present user interface controls for setting timers, or provide simple controls to quickly change clock times or set alarms for smart devices.
According to some aspects, the smart device in the image data is a digital dashboard application presented on a display device or projected on a background. For example, the smart device may use a projector on a wall of a living room that presents a virtual dashboard (e.g., weather, news, sports, finance, etc.). The mobile device will acquire image data of the dashboard and provide that data to the virtual assistant application 116, and the virtual assistant application 116 may then provide interface controls to update or modify the dashboard.
FIG. 4 is a flow diagram of an example process 400 for identifying a smart device using a virtual assistant application and presenting one or more user interface controls for controlling the identified smart device. The operations of process 400 may be performed, for example, by one or more data processing apparatus (such as mobile device 110 of fig. 1A and 1B). The operations of process 400 may also be implemented as instructions stored on a non-transitory computer-readable medium. Execution of the instructions causes one or more data processing apparatus to perform the operations of process 400.
Image data from a camera of a mobile device of a user is received (402). As described herein, the object recognizer 125 of the virtual assistant application 116 may receive pixel data (e.g., image data 123 shown in FIG. 1B) for a viewfinder of a camera of the mobile device. The image data may be a single image or a continuous image, such as streaming real-time video.
After receiving image data from a camera of a mobile device, it is determined whether the received image data depicts a smart device or a physical control of the smart device (404). The smart device may be a virtual assistant control device (e.g., a voice assistant device) or another type of smart device. For example, the smart device may be a smart device capable of being controlled by a virtual assistant control device, e.g., by registering the smart device with a user's virtual assistant account or otherwise configuring the virtual assistant control device to control the smart device. As described above, the virtual assistant application on the mobile device may determine whether the smart device or a physical control for the smart device is depicted in the image data. In some aspects, the image data may be analyzed by a remote system (e.g., a virtual assistant management system) and the results sent back to the mobile device.
According to some aspects, if it is determined that a smart device is depicted in the image data, it is determined whether the mobile device is capable of controlling a virtual assistant control device registered with the user account when the image is presented on the mobile device. For example, the determination may be based on whether the mobile device is within a threshold distance of the virtual secondary control device when rendering the image. As described above, the mobile device may send the location data to the virtual assistant app, which may then determine whether the virtual assistant control device (e.g., a voice assistant device) is proximate to the mobile device or within a predetermined distance of the location of the mobile device. For example, the virtual assistant application may compare the location of the mobile device to the location of the virtual assistant control device. The location of the virtual assistant control device may be a location (e.g., determined when the virtual assistant control device initially registers). In another example, the virtual assistant control device may send data identifying its current location (e.g., using a GPS receiver installed on or in the virtual assistant control device) to the mobile device or to a virtual assistant management system (from which the virtual assistant application may obtain location data). The determination of the location data and whether the smart device is within a certain distance of the user's virtual assistant control device may be made by a remote system (e.g., a virtual assistant management system) and the results of the determination sent back to the mobile device.
According to some aspects, determining that the received image depicts a smart device registered with the user account includes: it is determined that the image data depicts an image of the user's virtual assistant control device. According to some aspects, a virtual assistant control device includes a virtual assistant smart speaker device that receives voice commands from a user and provides information to the user using speakers of the virtual assistant smart speaker device.
According to some aspects, determining that the received image depicts a smart device registered with the user account includes: the determination image depicts a smart device controlled by a virtual assistant control device of the user. For example, the virtual assistant application may communicate with the voice assistant device to determine which smart devices it controls, such as to determine which smart devices the voice assistant device has been paired with.
According to some aspects, determining that the received image depicts a smart device registered with the user account includes: data is obtained that specifies smart devices that have registered with the user's virtual assistant account (e.g., smart devices for which the user has configured virtual assistant control devices for control). If the smart device (or physical controls) identified in the image matches the registered smart device (or physical controls of the registered smart device), the virtual assistant application may determine that the user is pointing the camera of the mobile device at the registered smart device.
Responsive to determining that the image depicts at least one of a smart device or a physical control for the smart device, one or more user interface controls for controlling the smart device are identified (408). As described above, when the smart device control selector determines that the location of the mobile device is within a distance of the location of the virtual assistant control device registered with the user account at the time the image data was captured, the smart device control selector identifies one or more user interface controls for controlling the smart device and generates interface control identification data. The interface control identification data includes data specifying particular controls that can control the identified smart device. The user interface control may be selected by a remote system (e.g., a virtual assistant management system) and sent to the mobile device.
According to some aspects, identifying one or more user interface controls for controlling the smart device includes determining a task or service that the virtual assistant control device is currently performing, and selecting one or more user interface controls for controlling the particular task or service. For example, if the virtual assistant control device is a smart speaker and the smart speaker is controlling an appliance, a control for controlling the appliance may be selected.
One or more user interface controls for controlling the smart device are generated and presented to a user of the mobile device (408). As described above, the interface control identification data is received by the interface generator, which generates presentation data that presents the user interface control overlaid on the user interface on the mobile device for each user interface control belonging to the smart device of interest specified by the interface control identification data. In some aspects, for continuous video (e.g., live), the user interface control is superimposed on the user interface only when the smart device is identified in the image data. The user interface controls may be selected by a remote system (e.g., a virtual assistant management system) and sent as presentation data to the mobile device for display.
User interaction with at least one of the one or more user interface controls is detected on a display of the mobile device (410). For example, the virtual assistant application may detect the interaction and determine a corresponding action to perform based on the interaction.
The smart device is controlled based on the detected user interaction with at least one of the one or more user interface controls (412). As described above, if a user interacts with an interface control, the user is able to control a particular smart device based on the user interaction. For example, if a music application is running on a smart speaker, the user may control the application (e.g., volume, skip to next track, etc.) by interacting with interface controls presented to the user on the display of the mobile device.
The mobile device may directly control the smart device, for example, by sending control data to the smart device over a wireless network. If the smart device is controlled by a virtual assistant control device, the mobile device may send control data to the virtual assistant control device, which in turn may control the smart device.
In some aspects, presenting one or more user interface controls for controlling a smart device includes: one or more user interface controls are superimposed in an augmented reality manner over a field of view of a viewfinder of a camera of a mobile device. For example, as shown in FIG. 2, the music control button 228 is a volume toggle bar 226, a user interface control displayed in augmented reality on a viewfinder of a camera of the mobile device that the user may select to control a music playing application of the voice assistant device 215.
In some aspects, for continuous video (e.g., live), the user interface control is superimposed on the user interface only when the smart device is identified in the image data. In some aspects, still frame screenshots of the smart device may be captured and used to control the smart device, so the mobile device would not necessarily be required to continuously acquire image data of the smart device to control it. For example, a user may wish to take a screenshot of a voice assistant device and then remove it from the voice assistant device and run a virtual assistant application to generate user interface controls for controlling the voice assistant device.
FIG. 5 is a block diagram of an example computer system 500 that may be used to perform the operations described above. The system 500 includes a processor 510, a memory 520, a storage device 530, and an input/output device 540. Each of the components 510, 520, 530, and 540 may be interconnected, for example, using a system bus 550. Processor 510 is capable of processing instructions for execution within system 500. In one implementation, the processor 510 is a single-threaded processor. In another implementation, the processor 510 is a multi-threaded processor. The processor 510 is capable of processing instructions stored in the memory 520 or the storage device 530.
The storage device 530 is capable of providing mass storage for the system 500. In one implementation, the storage device 530 is a computer-readable medium in various different implementations, and the storage device 530 may include, for example, a hard disk device, an optical disk device, a storage device shared by multiple computing devices (e.g., cloud storage devices) over a network, or some other mass storage device.
The input/output device 540 provides input/output operations for the system 500. In one implementation, the input/output device 540 may include one or more of a network interface device (e.g., an Ethernet card), a serial communication device (e.g., an RS-232 port), and/or a wireless interface device (e.g., an 802.11 card). In another implementation, the input/output devices can include driver devices configured to receive input data and transmit output data to other input/output devices, such as a keyboard, a printer, and a display device 580. However, other implementations may also be used, such as a mobile computing device, a mobile communication device, a set-top box television client device, and so forth.
Although an example processing system is depicted in fig. 5, implementations and functional operations described in the subject matter may be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
Implementations of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software or firmware, or in computer hardware (including the structures disclosed in this specification and their equivalents, or in combinations of one or more of them). Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible, non-transitory program carrier for execution by, or to control the operation of, data processing apparatus. Alternatively or in addition, the program instructions may be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by the data processing apparatus. The computer storage medium may be a machine-readable storage device, a machine-readable storage substrate, a random or serial access storage device, or a combination of one or more of them.
The term "data processing apparatus" encompasses various apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. An apparatus may comprise special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program (also known as a program, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. A computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array), an ASIC (application-specific integrated circuit), or a GPGPU (general purpose graphics processing unit).
For example, a computer suitable for executing a computer program may be based on a general purpose or special purpose microprocessor or both, or on any other kind of central processing unit. Generally, a central processing unit will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a central processing unit for executing or carrying out instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer may be embedded in another device, e.g., a mobile telephone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a Universal Serial Bus (USB) flash drive, to name a few.
Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disks; and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, implementations of the subject matter described in this specification can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display)) and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer for displaying information to the user. Other kinds of devices may also be used to provide interaction with the user, and the feedback provided to the user may be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. In addition, the computer may interact with the user by sending documents to and receiving documents from the device used by the user; for example, by sending a Web page to a Web browser on a user's client device in response to a request received from the Web browser.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front-end component (e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification), or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network ("LAN") and a wide area network ("WAN"), e.g., the Internet.
The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any inventions or of what may be claimed, but rather as descriptions of features specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain situations, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Specific embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In certain implementations, multitasking and parallel processing may be advantageous.
Claims (20)
1. A method performed by one or more data processing apparatus, the method comprising:
receiving image data of an image captured by a camera of a mobile device of a user;
determining, based on the image data, that the image depicts at least one of a smart device or a physical control of the smart device;
in response to determining that the image depicts the smart device or at least one of physical controls of the smart device, identifying one or more user interface controls for controlling the smart device;
generating and presenting one or more user interface controls for controlling the smart device on a display of the mobile device;
detecting, on a display of a mobile device, a user interaction with at least one of one or more user interface controls; and
controlling the smart device based on the detected user interaction.
2. The method of claim 1, wherein controlling the smart device comprises: data is sent to a virtual assistant control device operatively connected to the smart device, the data causing the virtual assistant control device to send an operation corresponding to the detected user interaction to the smart device to perform the operation.
3. The method of claim 1, wherein determining, based on the image data, that the image depicts at least one of a smart device or a physical control of the smart device comprises: the determination image depicts a virtual assistant control device configured to control other smart devices.
4. The method of claim 3, further comprising:
it is determined that the mobile device is capable of controlling the virtual assistant control device while the mobile device is rendering the image.
5. The method of claim 4, wherein determining that the mobile device is capable of controlling the virtual assistant control device while the mobile device is rendering the image comprises: the virtual assistant control device is determined to be the particular virtual assistant control device registered with the user account by determining that the mobile device is within a threshold distance of the particular virtual assistant control device registered with the user account when the user device is rendering an image.
6. The method of claim 4, further comprising: in response to determining that the mobile device is capable of controlling the virtual assistant control device while the mobile device is presenting the image, determining to present one or more user interface controls for controlling the smart device.
7. The method of claim 1, wherein determining, based on the image data, that the image depicts at least one of a smart device or a physical control of the smart device comprises: the determination image depicts an image of the user's virtual assistant control device.
8. The method of claim 7 wherein the virtual assistant control device comprises a virtual assistant smart speaker device that receives voice commands from the user and provides information to the user using speakers of the virtual assistant smart speaker device.
9. The method of claim 8, wherein identifying one or more user interface controls for controlling a smart device comprises:
determining a task or service that the virtual assistant smart speaker device is currently performing; and
one or more user interface controls for controlling the task or service are selected.
10. The method of claim 1, wherein generating and presenting, at the mobile device, one or more user interface controls for controlling the smart device comprises: one or more user interface controls are superimposed in an augmented reality manner over a field of view of a viewfinder of a camera of a mobile device.
11. The method of claim 1, wherein identifying one or more user interface controls for controlling the smart device in response to determining that the image depicts at least one of a smart device or a physical control of the smart device comprises: a registry file for the smart device is obtained, wherein the registry file includes data specifying a type of the smart device and available user interface controls for controlling the smart device.
12. The method of claim 1, wherein determining, based on the image data, that the received image data depicts at least one of a smart device or a physical control of the smart device comprises:
the user is prompted to confirm that the smart device is the user's device and to receive confirmation from the user.
13. A system, comprising:
one or more data processing devices; and
a memory storage device in data communication with the one or more data processing apparatus, the memory storage device storing instructions executable by the one or more data processing apparatus and which, when executed, cause the one or more data processing apparatus to:
receiving image data of an image captured by a camera of a mobile device of a user;
determining, based on the image data, that the image depicts at least one of a smart device or a physical control of the smart device;
in response to determining that the image depicts the smart device or at least one of physical controls of the smart device, identifying one or more user interface controls for controlling the smart device;
generating and presenting one or more user interface controls for controlling the smart device on a display of the mobile device;
detecting, on a display of a mobile device, a user interaction with at least one of one or more user interface controls; and
controlling the smart device based on the detected user interaction.
14. The system of claim 13, wherein determining, based on the image data, that the image depicts at least one of a smart device or a physical control of the smart device comprises: the determination image depicts a virtual assistant control device configured to control other smart devices.
15. The system of claim 14, the operations further comprising:
it is determined that the mobile device is capable of controlling the virtual assistant control device while the mobile device is rendering the image.
16. The system of claim 15, wherein determining that the mobile device is capable of controlling the virtual assistant control device while the mobile device is rendering the image comprises: the virtual assistant control device is determined to be the particular virtual assistant control device registered with the user account by determining that the mobile device is within a threshold distance of the particular virtual assistant control device registered with the user account when the user device is rendering an image.
17. The system of claim 15, the operations further comprising: in response to determining that the mobile device is capable of controlling the virtual assistant control device while the mobile device is presenting the image, determining to present one or more user interface controls for controlling the smart device.
18. The system of claim 13, wherein determining, based on the image data, that the image depicts at least one of a smart device or a physical control of the smart device comprises: the determination image depicts an image of the user's virtual assistant control device.
19. The system of claim 13, wherein generating and presenting, at the mobile device, one or more user interface controls for controlling the smart device comprises: one or more user interface controls are superimposed in an augmented reality manner over a field of view of a viewfinder of a camera of a mobile device.
20. A non-transitory computer storage medium encoded with a computer program, the program comprising instructions that when executed by data processing apparatus cause the data processing apparatus to:
receiving image data of an image captured by a camera of a mobile device of a user;
determining, based on the image data, that the image depicts at least one of a smart device or a physical control of the smart device;
in response to determining that the image depicts the smart device or at least one of physical controls of the smart device, identifying one or more user interface controls for controlling the smart device;
generating and presenting one or more user interface controls for controlling the smart device on a display of the mobile device;
detecting, on a display of a mobile device, a user interaction with at least one of one or more user interface controls; and
controlling the smart device based on the detected user interaction.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
CN202210264622.XA CN114666650B (en) | 2018-06-25 | 2019-06-14 | Identifying and controlling smart devices |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/017,394 | 2018-06-25 | ||
US16/017,394 US10725629B2 (en) | 2018-06-25 | 2018-06-25 | Identifying and controlling smart devices |
PCT/US2019/037171 WO2020005575A1 (en) | 2018-06-25 | 2019-06-14 | Identifying and controlling smart devices |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202210264622.XA Division CN114666650B (en) | 2018-06-25 | 2019-06-14 | Identifying and controlling smart devices |
Publications (2)
Publication Number | Publication Date |
---|---|
CN112219405A true CN112219405A (en) | 2021-01-12 |
CN112219405B CN112219405B (en) | 2022-03-15 |
Family
ID=67108213
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202210264622.XA Active CN114666650B (en) | 2018-06-25 | 2019-06-14 | Identifying and controlling smart devices |
CN201980036735.0A Active CN112219405B (en) | 2018-06-25 | 2019-06-14 | Identifying and controlling smart devices |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202210264622.XA Active CN114666650B (en) | 2018-06-25 | 2019-06-14 | Identifying and controlling smart devices |
Country Status (7)
Country | Link |
---|---|
US (3) | US10725629B2 (en) |
EP (2) | EP3797521B1 (en) |
JP (3) | JP6915169B2 (en) |
KR (1) | KR102211014B1 (en) |
CN (2) | CN114666650B (en) |
ES (1) | ES2963118T3 (en) |
WO (1) | WO2020005575A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
TWI804294B (en) * | 2022-04-28 | 2023-06-01 | 中華電信股份有限公司 | Remote device control system, method and computer readable medium for augmented reality collaboration |
Families Citing this family (43)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN110709784A (en) | 2017-04-26 | 2020-01-17 | 唯景公司 | Colorable window system computing platform |
US11892738B2 (en) | 2017-04-26 | 2024-02-06 | View, Inc. | Tandem vision window and media display |
US10725629B2 (en) | 2018-06-25 | 2020-07-28 | Google Llc | Identifying and controlling smart devices |
US11132681B2 (en) | 2018-07-06 | 2021-09-28 | At&T Intellectual Property I, L.P. | Services for entity trust conveyances |
WO2020022780A1 (en) * | 2018-07-25 | 2020-01-30 | Samsung Electronics Co., Ltd. | Method and apparatus for establishing device connection |
US10991385B2 (en) | 2018-08-06 | 2021-04-27 | Spotify Ab | Singing voice separation with deep U-Net convolutional networks |
US10977555B2 (en) | 2018-08-06 | 2021-04-13 | Spotify Ab | Automatic isolation of multiple instruments from musical mixtures |
US10923141B2 (en) | 2018-08-06 | 2021-02-16 | Spotify Ab | Singing voice separation with deep u-net convolutional networks |
US10802872B2 (en) | 2018-09-12 | 2020-10-13 | At&T Intellectual Property I, L.P. | Task delegation and cooperation for automated assistants |
EP4145775A1 (en) * | 2018-09-24 | 2023-03-08 | Google LLC | Controlling a device based on processing of image data that captures the device and/or an installation environment of the device |
KR102620363B1 (en) * | 2018-10-12 | 2024-01-04 | 삼성전자주식회사 | A mobile apparatus and a method for controlling the mobile apparatus |
US11481186B2 (en) | 2018-10-25 | 2022-10-25 | At&T Intellectual Property I, L.P. | Automated assistant context and protocol |
US11475664B2 (en) * | 2018-12-03 | 2022-10-18 | Signify Holding B.V. | Determining a control mechanism based on a surrounding of a remove controllable device |
US10482678B1 (en) * | 2018-12-14 | 2019-11-19 | Capital One Services, Llc | Systems and methods for displaying video from a remote beacon device |
US20200193264A1 (en) * | 2018-12-14 | 2020-06-18 | At&T Intellectual Property I, L.P. | Synchronizing virtual agent behavior bias to user context and personality attributes |
WO2020167524A1 (en) * | 2019-02-11 | 2020-08-20 | Spellbound Development Group, Inc. | System and method for object and location-related delivery of real-time messages |
CN111752443A (en) * | 2019-03-28 | 2020-10-09 | 华为技术有限公司 | Method, related device and system for controlling page by display equipment |
US11521384B1 (en) * | 2019-07-01 | 2022-12-06 | Alarm.Com Incorporated | Monitoring system integration with augmented reality devices |
US11445107B2 (en) * | 2019-08-08 | 2022-09-13 | Qorvo Us, Inc. | Supervised setup for control device with imager |
US20210121784A1 (en) * | 2019-10-23 | 2021-04-29 | Sony Interactive Entertainment Inc. | Like button |
US11089109B1 (en) * | 2019-11-20 | 2021-08-10 | Sprint Communications Company L.P. | Smart device management via a mobile communication device based on privacy preferences |
WO2021181604A1 (en) * | 2020-03-12 | 2021-09-16 | マクセル株式会社 | Information terminal device, and application operation mode control method of same |
US11010129B1 (en) * | 2020-05-08 | 2021-05-18 | International Business Machines Corporation | Augmented reality user interface |
US20210358294A1 (en) * | 2020-05-15 | 2021-11-18 | Microsoft Technology Licensing, Llc | Holographic device control |
WO2022050433A1 (en) * | 2020-09-01 | 2022-03-10 | 엘지전자 주식회사 | Display device for adjusting recognition sensitivity of speech recognition starting word and operation method thereof |
JP2022042036A (en) * | 2020-09-02 | 2022-03-14 | 株式会社リコー | Apparatus management system, apparatus management method, and program |
TW202225941A (en) * | 2020-11-03 | 2022-07-01 | 美商視野公司 | Virtually viewing devices in a facility |
CN112783467A (en) * | 2020-12-31 | 2021-05-11 | 读书郎教育科技有限公司 | Wearable device screen capture control system and method |
US11947783B2 (en) * | 2021-01-25 | 2024-04-02 | Google Llc | Undoing application operation(s) via user interaction(s) with an automated assistant |
US20220239523A1 (en) * | 2021-01-27 | 2022-07-28 | AVAST Software s.r.o. | Universal virtual remote control for smart devices |
US11832028B2 (en) * | 2021-03-11 | 2023-11-28 | Objectvideo Labs, Llc | Doorbell avoidance techniques |
JP2022178205A (en) * | 2021-05-19 | 2022-12-02 | キヤノン株式会社 | Control apparatus and method of controlling the same |
AT17724U1 (en) * | 2021-06-10 | 2022-12-15 | Ars Electronica Linz Gmbh & Co Kg | System for spatially limited activation of a control unit |
EP4113987B1 (en) * | 2021-06-29 | 2023-11-08 | Axis AB | System, camera device, method, and computer program for displaying collected sensor data together with images |
US11579752B1 (en) * | 2021-07-20 | 2023-02-14 | Sony Interactive Entertainment Inc. | Augmented reality placement for user feedback |
IT202100022940A1 (en) * | 2021-09-06 | 2023-03-06 | Candy Spa | Method to assist a user in the use and/or exploration of the functions of an appliance, through the use of augmented reality |
US11960652B2 (en) | 2021-10-12 | 2024-04-16 | Qualcomm Incorporated | User interactions with remote devices |
JP2023060654A (en) | 2021-10-18 | 2023-04-28 | パナソニックＩｐマネジメント株式会社 | Sprinkling device and sprinkling method |
CN113849118A (en) * | 2021-10-20 | 2021-12-28 | 锐捷网络股份有限公司 | Image identification method applied to electronic whiteboard and related device |
US11930270B2 (en) | 2021-10-29 | 2024-03-12 | Snap Inc. | Identifying a video camera for an object |
US11579755B1 (en) * | 2022-03-08 | 2023-02-14 | Amazon Technologies, Inc. | Dynamic address-based dashboard customization |
JP2023151540A (en) * | 2022-03-31 | 2023-10-16 | パナソニックホールディングス株式会社 | Display system, display method, and program |
US20240077935A1 (en) * | 2022-09-01 | 2024-03-07 | Youjean Cho | Virtual interfaces for controlling iot devices |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2011009069A2 (en) * | 2009-07-17 | 2011-01-20 | Qualcomm Incorporated | Automatic interfacing between a master device and object device |
CN104977904A (en) * | 2014-04-04 | 2015-10-14 | 浙江大学 | Visible and controllable intelligent household control system and control method thereof |
EP3147826A1 (en) * | 2015-09-28 | 2017-03-29 | Xiaomi Inc. | Method and apparatus for controlling electronic device |
CN107132769A (en) * | 2017-04-21 | 2017-09-05 | 北京小米移动软件有限公司 | Smart machine control method and device |
US20180063569A1 (en) * | 2015-08-30 | 2018-03-01 | EVA Automation, Inc. | User Interface Based on Device-State Information |
Family Cites Families (53)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20140063054A1 (en) * | 2010-02-28 | 2014-03-06 | Osterhout Group, Inc. | Ar glasses specific control interface based on a connected external device type |
US20150309316A1 (en) * | 2011-04-06 | 2015-10-29 | Microsoft Technology Licensing, Llc | Ar glasses with predictive control of external device based on event input |
US9111138B2 (en) * | 2010-11-30 | 2015-08-18 | Cisco Technology, Inc. | System and method for gesture interface control |
US8977971B2 (en) * | 2010-12-24 | 2015-03-10 | General Electric Company | Metadata generation systems and methods |
US20120226981A1 (en) * | 2011-03-02 | 2012-09-06 | Microsoft Corporation | Controlling electronic devices in a multimedia system through a natural user interface |
US9462423B1 (en) * | 2011-07-12 | 2016-10-04 | Google Inc. | Qualitative and quantitative sensor fusion for indoor navigation |
CA2853033C (en) * | 2011-10-21 | 2019-07-16 | Nest Labs, Inc. | User-friendly, network connected learning thermostat and related systems and methods |
US9329678B2 (en) * | 2012-08-14 | 2016-05-03 | Microsoft Technology Licensing, Llc | Augmented reality overlay for control devices |
US9165406B1 (en) | 2012-09-21 | 2015-10-20 | A9.Com, Inc. | Providing overlays based on text in a live camera view |
US20140181683A1 (en) * | 2012-12-21 | 2014-06-26 | Samsung Electronics Co., Ltd. | Method and system for controlling external device |
KR102171444B1 (en) * | 2013-04-22 | 2020-10-29 | 엘지전자 주식회사 | Smart watch and method for controlling thereof |
US9639984B2 (en) * | 2013-06-03 | 2017-05-02 | Daqri, Llc | Data manipulation based on real world object manipulation |
CA2926463A1 (en) * | 2013-10-07 | 2015-04-16 | Google Inc. | Smart-home hazard detector providing useful follow up communications to detection events |
JP6213181B2 (en) * | 2013-11-20 | 2017-10-18 | ヤマハ株式会社 | Synchronous playback system and synchronous playback method |
US9282283B2 (en) * | 2014-01-29 | 2016-03-08 | Microsoft Technology Licensing, Llc | Detecting patterns traced on a screen of a user device |
WO2015120019A1 (en) * | 2014-02-10 | 2015-08-13 | Google Inc. | Smart camera user interface |
JP2015156610A (en) * | 2014-02-21 | 2015-08-27 | ソニー株式会社 | Electronic apparatus and power supply control method |
JP6500477B2 (en) | 2015-02-12 | 2019-04-17 | セイコーエプソン株式会社 | Head-mounted display device, control system, control method of head-mounted display device, and computer program |
US9338493B2 (en) * | 2014-06-30 | 2016-05-10 | Apple Inc. | Intelligent automated assistant for TV user interactions |
SG10201405182WA (en) * | 2014-08-25 | 2016-03-30 | Univ Singapore Technology & Design | Method and system |
US9378467B1 (en) * | 2015-01-14 | 2016-06-28 | Microsoft Technology Licensing, Llc | User interaction pattern extraction for device personalization |
JP2016167385A (en) | 2015-03-09 | 2016-09-15 | パナソニックＩｐマネジメント株式会社 | Portable terminal and equipment control system |
US10921896B2 (en) * | 2015-03-16 | 2021-02-16 | Facebook Technologies, Llc | Device interaction in augmented reality |
CN104714414B (en) | 2015-03-25 | 2018-11-02 | 小米科技有限责任公司 | The control method and device of smart home device, electronic equipment |
US10007413B2 (en) * | 2015-04-27 | 2018-06-26 | Microsoft Technology Licensing, Llc | Mixed environment display of attached control elements |
CN113766636A (en) * | 2015-05-12 | 2021-12-07 | 三星电子株式会社 | Apparatus and method for estimating position in wireless communication system |
WO2016191875A1 (en) * | 2015-06-04 | 2016-12-08 | Griffin Innovation | Device and method for controlling a plurality of targeted devices |
WO2017096082A1 (en) * | 2015-12-01 | 2017-06-08 | Drexel University | Beam visualization and using augmented reality for control and interaction |
US9927614B2 (en) | 2015-12-29 | 2018-03-27 | Microsoft Technology Licensing, Llc | Augmented reality display system with variable focus |
WO2017165705A1 (en) * | 2016-03-23 | 2017-09-28 | Bent Image Lab, Llc | Augmented reality for the internet of things |
US10474297B2 (en) * | 2016-07-20 | 2019-11-12 | Ams Sensors Singapore Pte. Ltd. | Projecting a structured light pattern onto a surface and detecting and responding to interactions with the same |
US10664722B1 (en) * | 2016-10-05 | 2020-05-26 | Digimarc Corporation | Image processing arrangements |
US10621747B2 (en) * | 2016-11-15 | 2020-04-14 | Magic Leap, Inc. | Deep learning system for cuboid detection |
KR102531542B1 (en) | 2016-12-05 | 2023-05-10 | 매직 립, 인코포레이티드 | Virual user input controls in a mixed reality environment |
CN107038408A (en) * | 2017-01-11 | 2017-08-11 | 阿里巴巴集团控股有限公司 | Image-recognizing method and device based on augmented reality |
US10108867B1 (en) * | 2017-04-25 | 2018-10-23 | Uber Technologies, Inc. | Image-based pedestrian detection |
EP3577597A1 (en) * | 2017-05-19 | 2019-12-11 | Google LLC | Efficient image analysis using environment sensor data |
US10602046B2 (en) * | 2017-07-11 | 2020-03-24 | Htc Corporation | Mobile device and control method |
US10922583B2 (en) * | 2017-07-26 | 2021-02-16 | Magic Leap, Inc. | Training a neural network with representations of user interface devices |
WO2019079790A1 (en) * | 2017-10-21 | 2019-04-25 | Eyecam, Inc | Adaptive graphic user interfacing system |
US11131973B2 (en) * | 2017-12-06 | 2021-09-28 | Arris Enterprises Llc | System and method of IOT device control using augmented reality |
US10937240B2 (en) * | 2018-01-04 | 2021-03-02 | Intel Corporation | Augmented reality bindings of physical objects and virtual objects |
TWM569984U (en) * | 2018-01-05 | 2018-11-11 | 英屬開曼群島商麥迪創科技股份有限公司 | Automotive theater apparatus |
US10620721B2 (en) * | 2018-01-29 | 2020-04-14 | Google Llc | Position-based location indication and device control |
US10650597B2 (en) * | 2018-02-06 | 2020-05-12 | Servicenow, Inc. | Augmented reality assistant |
US10504290B2 (en) * | 2018-05-04 | 2019-12-10 | Facebook Technologies, Llc | User interface security in a virtual reality environment |
US20200028841A1 (en) * | 2018-06-15 | 2020-01-23 | Proxy, Inc. | Method and apparatus for providing multiple user credentials |
US10725629B2 (en) * | 2018-06-25 | 2020-07-28 | Google Llc | Identifying and controlling smart devices |
WO2020022780A1 (en) * | 2018-07-25 | 2020-01-30 | Samsung Electronics Co., Ltd. | Method and apparatus for establishing device connection |
US11042992B2 (en) * | 2018-08-03 | 2021-06-22 | Logitech Europe S.A. | Method and system for detecting peripheral device displacement |
US10867210B2 (en) * | 2018-12-21 | 2020-12-15 | Waymo Llc | Neural networks for coarse- and fine-object classifications |
US10701661B1 (en) * | 2019-04-02 | 2020-06-30 | Google Llc | Location determination for device control and configuration |
US11082249B2 (en) * | 2019-04-02 | 2021-08-03 | Google Llc | Location determination for device control and configuration |
-
2018
- 2018-06-25 US US16/017,394 patent/US10725629B2/en active Active
-
2019
- 2019-06-14 CN CN202210264622.XA patent/CN114666650B/en active Active
- 2019-06-14 ES ES19734634T patent/ES2963118T3/en active Active
- 2019-06-14 EP EP19734634.9A patent/EP3797521B1/en active Active
- 2019-06-14 JP JP2020546363A patent/JP6915169B2/en active Active
- 2019-06-14 EP EP23183579.4A patent/EP4231652A1/en active Pending
- 2019-06-14 WO PCT/US2019/037171 patent/WO2020005575A1/en unknown
- 2019-06-14 CN CN201980036735.0A patent/CN112219405B/en active Active
- 2019-06-14 KR KR1020207026995A patent/KR102211014B1/en active IP Right Grant
-
2020
- 2020-06-18 US US16/905,245 patent/US11086493B2/en active Active
-
2021
- 2021-07-13 JP JP2021115685A patent/JP7077463B2/en active Active
- 2021-07-19 US US17/378,856 patent/US11921988B2/en active Active
-
2022
- 2022-05-17 JP JP2022080934A patent/JP2022107645A/en active Pending
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2011009069A2 (en) * | 2009-07-17 | 2011-01-20 | Qualcomm Incorporated | Automatic interfacing between a master device and object device |
CN104977904A (en) * | 2014-04-04 | 2015-10-14 | 浙江大学 | Visible and controllable intelligent household control system and control method thereof |
US20180063569A1 (en) * | 2015-08-30 | 2018-03-01 | EVA Automation, Inc. | User Interface Based on Device-State Information |
EP3147826A1 (en) * | 2015-09-28 | 2017-03-29 | Xiaomi Inc. | Method and apparatus for controlling electronic device |
CN107132769A (en) * | 2017-04-21 | 2017-09-05 | 北京小米移动软件有限公司 | Smart machine control method and device |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
TWI804294B (en) * | 2022-04-28 | 2023-06-01 | 中華電信股份有限公司 | Remote device control system, method and computer readable medium for augmented reality collaboration |
Also Published As
Publication number | Publication date |
---|---|
JP2021514091A (en) | 2021-06-03 |
ES2963118T3 (en) | 2024-03-25 |
KR20200115652A (en) | 2020-10-07 |
EP3797521B1 (en) | 2023-08-02 |
US11921988B2 (en) | 2024-03-05 |
US20210342047A1 (en) | 2021-11-04 |
JP7077463B2 (en) | 2022-05-30 |
CN114666650A (en) | 2022-06-24 |
JP2021170370A (en) | 2021-10-28 |
US20190391716A1 (en) | 2019-12-26 |
CN112219405B (en) | 2022-03-15 |
EP3797521A1 (en) | 2021-03-31 |
US11086493B2 (en) | 2021-08-10 |
WO2020005575A1 (en) | 2020-01-02 |
EP4231652A1 (en) | 2023-08-23 |
US20200319765A1 (en) | 2020-10-08 |
JP2022107645A (en) | 2022-07-22 |
JP6915169B2 (en) | 2021-08-04 |
CN114666650B (en) | 2023-12-26 |
US10725629B2 (en) | 2020-07-28 |
KR102211014B1 (en) | 2021-02-04 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN112219405B (en) | Identifying and controlling smart devices | |
US11507619B2 (en) | Display apparatus with intelligent user interface | |
US11509957B2 (en) | Display apparatus with intelligent user interface | |
US20190354608A1 (en) | Display apparatus with intelligent user interface | |
CN111240555A (en) | Intelligent Internet of things menu using camera device | |
KR101968725B1 (en) | Media selection for providing information corresponding to voice query | |
US20190354603A1 (en) | Display apparatus with intelligent user interface | |
US20200098367A1 (en) | Output for improving information delivery corresponding to voice request | |
US20190356952A1 (en) | Display apparatus with intelligent user interface | |
WO2020107040A2 (en) | Integration of internet of things devices | |
US20190356951A1 (en) | Display apparatus with intelligent user interface | |
EP3892069B1 (en) | Determining a control mechanism based on a surrounding of a remote controllable device | |
CN108398127A (en) | A kind of indoor orientation method and device | |
CN110741652A (en) | Display device with intelligent user interface | |
CN113259583B (en) | Image processing method, device, terminal and storage medium |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |