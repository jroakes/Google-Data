CN116210051A - Enhanced computing device audio representation - Google Patents
Enhanced computing device audio representation Download PDFInfo
- Publication number
- CN116210051A CN116210051A CN202180066234.4A CN202180066234A CN116210051A CN 116210051 A CN116210051 A CN 116210051A CN 202180066234 A CN202180066234 A CN 202180066234A CN 116210051 A CN116210051 A CN 116210051A
- Authority
- CN
- China
- Prior art keywords
- computing device
- sound
- structured
- output
- user interface
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000034 method Methods 0.000 claims abstract description 148
- 238000013518 transcription Methods 0.000 claims abstract description 32
- 230000035897 transcription Effects 0.000 claims abstract description 32
- 230000004044 response Effects 0.000 claims description 26
- 239000000779 smoke Substances 0.000 claims description 23
- 238000003860 storage Methods 0.000 claims description 13
- 206010011469 Crying Diseases 0.000 claims description 11
- 239000011521 glass Substances 0.000 claims description 4
- UGFAIRIUMAVXCW-UHFFFAOYSA-N Carbon monoxide Chemical compound [O+]#[C-] UGFAIRIUMAVXCW-UHFFFAOYSA-N 0.000 claims description 3
- 241001465754 Metazoa Species 0.000 claims description 3
- 229910002091 carbon monoxide Inorganic materials 0.000 claims description 3
- XLYOFNOQVPJJNP-UHFFFAOYSA-N water Substances O XLYOFNOQVPJJNP-UHFFFAOYSA-N 0.000 claims description 3
- 238000010801 machine learning Methods 0.000 description 134
- 238000012549 training Methods 0.000 description 44
- 238000012545 processing Methods 0.000 description 32
- 238000013528 artificial neural network Methods 0.000 description 22
- 230000006870 function Effects 0.000 description 21
- 239000010410 layer Substances 0.000 description 21
- 238000010586 diagram Methods 0.000 description 19
- 230000008569 process Effects 0.000 description 12
- 238000005457 optimization Methods 0.000 description 11
- 230000000306 recurrent effect Effects 0.000 description 11
- 238000004458 analytical method Methods 0.000 description 10
- 238000001514 detection method Methods 0.000 description 10
- 238000003066 decision tree Methods 0.000 description 8
- 238000012544 monitoring process Methods 0.000 description 7
- 230000002787 reinforcement Effects 0.000 description 6
- 238000004891 communication Methods 0.000 description 5
- 238000012417 linear regression Methods 0.000 description 5
- 238000007637 random forest analysis Methods 0.000 description 5
- 230000009467 reduction Effects 0.000 description 5
- 241000009328 Perro Species 0.000 description 4
- 238000013527 convolutional neural network Methods 0.000 description 4
- 230000002996 emotional effect Effects 0.000 description 4
- 238000005516 engineering process Methods 0.000 description 4
- 230000001537 neural effect Effects 0.000 description 4
- 238000007781 pre-processing Methods 0.000 description 4
- 230000009466 transformation Effects 0.000 description 4
- OCKGFTQIICXDQW-ZEQRLZLVSA-N 5-[(1r)-1-hydroxy-2-[4-[(2r)-2-hydroxy-2-(4-methyl-1-oxo-3h-2-benzofuran-5-yl)ethyl]piperazin-1-yl]ethyl]-4-methyl-3h-2-benzofuran-1-one Chemical compound C1=C2C(=O)OCC2=C(C)C([C@@H](O)CN2CCN(CC2)C[C@H](O)C2=CC=C3C(=O)OCC3=C2C)=C1 OCKGFTQIICXDQW-ZEQRLZLVSA-N 0.000 description 3
- 240000007320 Pinus strobus Species 0.000 description 3
- 230000003044 adaptive effect Effects 0.000 description 3
- 238000013473 artificial intelligence Methods 0.000 description 3
- 238000013145 classification model Methods 0.000 description 3
- 238000009826 distribution Methods 0.000 description 3
- 230000000694 effects Effects 0.000 description 3
- 230000007613 environmental effect Effects 0.000 description 3
- 238000004519 manufacturing process Methods 0.000 description 3
- 238000000513 principal component analysis Methods 0.000 description 3
- 230000002776 aggregation Effects 0.000 description 2
- 238000004220 aggregation Methods 0.000 description 2
- 230000008901 benefit Effects 0.000 description 2
- 238000004138 cluster model Methods 0.000 description 2
- 238000000354 decomposition reaction Methods 0.000 description 2
- 238000011982 device technology Methods 0.000 description 2
- 238000011143 downstream manufacturing Methods 0.000 description 2
- 238000000605 extraction Methods 0.000 description 2
- 238000007477 logistic regression Methods 0.000 description 2
- 238000013507 mapping Methods 0.000 description 2
- 238000003058 natural language processing Methods 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 238000010238 partial least squares regression Methods 0.000 description 2
- 238000012628 principal component regression Methods 0.000 description 2
- 238000005070 sampling Methods 0.000 description 2
- 239000004984 smart glass Substances 0.000 description 2
- 230000011273 social behavior Effects 0.000 description 2
- 239000013598 vector Substances 0.000 description 2
- 238000012935 Averaging Methods 0.000 description 1
- 206010011878 Deafness Diseases 0.000 description 1
- 208000032041 Hearing impaired Diseases 0.000 description 1
- 244000024873 Mentha crispa Species 0.000 description 1
- 235000014749 Mentha crispa Nutrition 0.000 description 1
- 244000246386 Mentha pulegium Species 0.000 description 1
- 235000016257 Mentha pulegium Nutrition 0.000 description 1
- 235000004357 Mentha x piperita Nutrition 0.000 description 1
- 230000006978 adaptation Effects 0.000 description 1
- 230000004931 aggregating effect Effects 0.000 description 1
- 230000008485 antagonism Effects 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 238000003339 best practice Methods 0.000 description 1
- 210000004556 brain Anatomy 0.000 description 1
- 238000004422 calculation algorithm Methods 0.000 description 1
- 230000001364 causal effect Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 238000006243 chemical reaction Methods 0.000 description 1
- 239000003795 chemical substances by application Substances 0.000 description 1
- 238000002790 cross-validation Methods 0.000 description 1
- 238000013135 deep learning Methods 0.000 description 1
- 238000003708 edge detection Methods 0.000 description 1
- 238000007636 ensemble learning method Methods 0.000 description 1
- 238000013213 extrapolation Methods 0.000 description 1
- 230000001815 facial effect Effects 0.000 description 1
- 230000002068 genetic effect Effects 0.000 description 1
- 238000009499 grossing Methods 0.000 description 1
- 235000001050 hortel pimenta Nutrition 0.000 description 1
- 239000007943 implant Substances 0.000 description 1
- 238000013178 mathematical model Methods 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 239000000203 mixture Substances 0.000 description 1
- 210000005036 nerve Anatomy 0.000 description 1
- 210000002569 neuron Anatomy 0.000 description 1
- 238000013450 outlier detection Methods 0.000 description 1
- 238000004091 panning Methods 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 238000013139 quantization Methods 0.000 description 1
- 239000002356 single layer Substances 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 238000006467 substitution reaction Methods 0.000 description 1
- 238000012706 support-vector machine Methods 0.000 description 1
- 239000003826 tablet Substances 0.000 description 1
- 238000013526 transfer learning Methods 0.000 description 1
- 238000000844 transformation Methods 0.000 description 1
- 230000001755 vocal effect Effects 0.000 description 1
- 230000002618 waking effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L21/00—Processing of the speech or voice signal to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility
- G10L21/06—Transformation of speech into a non-audible representation, e.g. speech visualisation or speech processing for tactile aids
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L21/00—Processing of the speech or voice signal to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/48—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use
- G10L25/51—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use for comparison or discrimination
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/48—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use
- G10L25/72—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use for transmitting results of analysis
Abstract
An example method includes: receiving, by one or more processors of the computing device, audio data recorded by one or more microphones of the computing device; and generating, by the one or more processors and based on the audio data, one or more structured sound recordings, a first structured sound recording of the one or more structured sound recordings comprising: a description of the first sound, the description including a descriptive label of the first sound, the descriptive label being different from a text transcription of the first sound; and a timestamp indicating a time at which the first sound occurred; and outputting a graphical user interface comprising a timeline representation of the one or more structured sound recordings.
Description
The present application claims the benefit of U.S. provisional patent application No. 63/088,811, filed on 7, 10, 2020, which is incorporated herein by reference in its entirety.
Background
Deaf and/or hearing impaired (DHH) people may have difficulty hearing or understanding various sounds. For example, people who are DHHs may have difficulty getting knowledge about ambient sounds, especially if they are not wearing hearing assistance devices (e.g., hearing aids, cochlear implants, etc.).
Disclosure of Invention
The present disclosure relates generally to techniques for representing sounds detected by a computing device. For example, the computing device may generate a structured sound recording based on the recorded audio data. The structured sound recording may include a description of the sound and a timestamp indicating the time at which the sound occurred. The description may include descriptive labels of sounds or text transcription of sounds. For example, where the sound includes spoken language, the description may include text transcription of the spoken language. Similarly, in the case where the sound corresponds to a predetermined sound, the descriptive label may include a classification of the predetermined sound. Descriptive tags can be categorized into various categories, such as emergency category tags, priority category tags, and other category tags. In some examples, the computing device may utilize artificial intelligence, such as a machine learning model, to determine the tags of the sound.
The computing device may use the structured sound recording to generate various outputs. For example, the computing device may output a non-audio indication of one or more structured sound recordings. The non-audio indication may take the form of one or more output modalities including graphics, haptic, and light. As one illustrative example, where the computing device generates a structured sound recording with descriptive tags in an emergency category (e.g., smoke alarms), the computing device may output a tactile alert and/or strobe to alert a user that may not otherwise be aware of the smoke alarms due to acting as a DHH. This may be particularly useful if the user is located in a location with smoke alarms that do not include strobing.
In some examples, the computing device may cause one or more other computing devices to adjust operation based on the structured sound recording generated by the computing device. For example, the computing device may cause another computing device (e.g., a wearable device) associated with a user of the computing device to output a non-audio indication. Continuing with the previous example, in response to generating the structured audio record with descriptive tags in the emergency category, a computing device (e.g., mobile phone) associated with the user may output a message to a wearable computing device (e.g., smart watch) associated with the user that causes the wearable computing device to output a non-audio indication (e.g., output a haptic alert).
In some examples, a computing device may output a Graphical User Interface (GUI) that includes a timeline representation of one or more structured sound recordings. The timeline representation may help the user to further interpret the sound. For example, by looking at the timeline representation, the user can determine potential causal relationships for the event. As one example, when the timeline indicates that a knock occurred at a first time, that an infant cry occurred at a second time after the first time, and that a third time dog barking after the second time, the user may determine that the knock may cause the infant cry.
As described above, in some examples, the structured sound recording may include a description that includes text transcription. The computing device may output a Graphical User Interface (GUI) including text transcription in a manner that improves user understanding. For example, the computing device may output a timeline view that includes text transcription. In some examples, the timeline view may be shared across multiple computing devices. For example, the first computing device may output a representation of one or more structured sound recordings to the second computing device. Both the first and second computing devices may output timeline views that may be scrolled independently. In this way, the timeline view output by the computing device may cover the same time period, or may cover different time periods.
As one example, a method includes: receiving, by one or more processors of the computing device, audio data recorded by one or more microphones of the computing device; and generating, by the one or more processors and based on the audio data, one or more structured sound recordings, a first structured sound recording of the one or more structured sound recordings comprising: a description of the first sound, the description including a descriptive tag of the first sound, the descriptive tag being different from a text transcription of the first sound, and a timestamp indicating a time at which the first sound occurred; and outputting a graphical user interface comprising a timeline representation of the one or more structured sound recordings.
As another example, a computing device includes one or more microphones; and one or more processors configured to: receiving audio data recorded by one or more microphones of a computing device; and generating, by the one or more processors and based on the audio data, one or more structured sound recordings, a first structured sound recording of the one or more structured sound recordings comprising: a description of the first sound, the description including a descriptive tag of the first sound, the descriptive tag being different from a text transcription of the first sound, and a timestamp indicating a time at which the first sound occurred; and outputting a graphical user interface comprising a timeline representation of the one or more structured sound recordings.
As another example, a computer-readable storage medium stores instructions that, when executed, cause one or more processors of a computing device to: receiving audio data recorded by one or more microphones of a computing device; generating, by the one or more processors and based on the audio data, one or more structured sound recordings, a first structured sound recording of the one or more structured sound recordings comprising: a description of the first sound, the description including a descriptive tag of the first sound, the descriptive tag being different from a text transcription of the first sound, and a timestamp indicating a time at which the first sound occurred; and outputting a graphical user interface comprising a timeline representation of the one or more structured sound recordings.
The details of one or more examples of the disclosure are set forth in the accompanying drawings and the description below. Other features, objects, and advantages will be apparent from the description and drawings, and from the claims.
Drawings
Fig. 1 is a system diagram illustrating a system including a computing device configured to generate structured sound recordings in accordance with one or more techniques of the present disclosure.
Fig. 2 is a conceptual diagram illustrating a system including a first computing device configured to cause a second computing device to output a non-audio indication of a structured sound recording generated at the first computing device in accordance with one or more techniques of this disclosure.
Fig. 3 is a conceptual diagram illustrating a system including a first computing device configured to cause a second computing device to output a non-audio indication of a structured sound recording generated at the first computing device in accordance with one or more techniques of this disclosure.
Fig. 4A and 4B are conceptual diagrams illustrating example Graphical User Interfaces (GUIs) that may be output by a computing device generating a structured sound recording in accordance with one or more techniques of the present disclosure.
Fig. 5A and 5B are conceptual diagrams illustrating example Graphical User Interfaces (GUIs) that may be output by a computing device generating a structured sound recording in accordance with one or more techniques of the present disclosure.
Fig. 6A-6E are conceptual diagrams illustrating aspects of an example machine learning model according to example embodiments of the present disclosure.
FIG. 7 is a conceptual diagram illustrating an example Graphical User Interface (GUI) that may be output by a computing device generating a structured sound recording in accordance with one or more techniques of the present disclosure.
Fig. 8 is a flowchart illustrating an example technique for displaying a timeline representation of structured sound recordings in accordance with one or more techniques of the present disclosure.
Detailed Description
Fig. 1 is a system diagram illustrating a system including a computing device configured to generate structured sound recordings in accordance with one or more techniques of the present disclosure. As shown in fig. 1, system 100 may include computing device 102 and other computing devices 114.
Examples of computing device 102 may include, but are not limited to, mobile phones (including so-called "smart phones"), smart glasses, smart watches, portable speakers (including portable smart speakers), notebook computers, portable gaming systems, wireless gaming system controllers, wireless headset charging cartridges, smart home devices (e.g., smart thermostats, smart smoke detectors, etc.), environmental computing devices (including so-called "smart displays"), and the like. As shown in fig. 1, the computing device 102 may include a user interface device 104, a microphone 106, a processor 108, a structured sound module 110, and a structured sound recording database 112.
As shown in fig. 1, computing device 102 includes user interface device 104. The user interface device 104 may serve as an input and/or output device for the computing device 102. The user interface device 104 may be implemented using a variety of techniques. For example, the user interface device 104 may be used as an input device using a presence-sensitive input screen, infrared sensor technology, or other input device technology for receiving user input. The user interface device 104 may function as an output device configured to present output to a user using any one or more of a display device, speaker technology, haptic feedback technology, or other output device technology for outputting information to a user. The computing device 102 may use the user interface device 104 to output a Graphical User Interface (GUI) such as the user interface 116 for display.
Microphone 106 may generate electrical signals based on sound waves. One or more microphones 106 may be integrated into computing device 102. In some examples, one or more microphones 106 may be external to computing device 102 and connected to computing device 102 via a wired or wireless interface.
The processor 108 may implement functionality and/or execute instructions within the computing device 102. Examples of the processor 108 include, but are not limited to: a Central Processing Unit (CPU); a Vision Processing Unit (VPU); a Graphics Processing Unit (GPU); tensor Processing Unit (TPU); a Neural Processing Unit (NPU); a neural processing engine; CPU, VPU, GPU, TPU, NPU or other processing device; an Application Specific Integrated Circuit (ASIC); a Field Programmable Gate Array (FPGA); a coprocessor; a controller; or a combination of the above processing devices. The processing device may be embedded in other hardware components, such as image sensors, accelerometers, etc.
In accordance with one or more techniques of this disclosure, the computing device 102 may include a structured sound module 110 that may be configured to generate a structured sound recording based on observed sound. For example, the processor 108 may execute the structured sound module 110 to process audio data recorded by the microphone 106 to generate a structured sound recording. The structured sound module 110 can store the generated structured sound record in the structured sound record database 112.
Each structured sound record may include one or more fields. Example fields include, but are not limited to, description fields and timestamp fields. In some examples, the description field in the structured sound record may include a descriptive tag from which sound in the audio data of the structured sound record is generated. As discussed in further detail below, the descriptive tags may be selected from a predetermined set of descriptive tags including emergency category tags, priority category tags, and/or other category tags.
In some examples, the description field may include a text transcription of a spoken language included in the audio data from which the structured sound recording was generated. In other examples, the text transcription may be included in a different field (e.g., different from the description field) of the structured sound record (e.g., the description field may indicate that the structured sound record includes the text transcription).
In some examples, such as where the structured sound record includes a text transcription, the structured sound record may include a field that characterizes the text transcription. For example, the structured sound record may include fields indicating emotional characteristics (e.g., sadness, happiness, and anger) accompanying the transcription of text.
In some examples, all processing to generate the structured sound recording may occur "on-device". For example, the computing device 102 may generate a structured sound recording without the audio data ever leaving the computing device 102. In this way, the techniques of this disclosure may enhance privacy.
The structured sound module 110 can generate a non-audio output based on the generated structured sound recording. For example, the structured sound module 110 can cause one or more devices of the user interface device 104 to output a non-audio indication of one or more structured sound recordings. As one example, the structured sound module 110 can cause a display of the user interface device 104 to display a GUI, such as GUI 116, that includes graphical indications of one or more structured sound recordings. As shown in fig. 1, the graphical indication of the one or more structured sound records may include a notification indicating that a particular type of structured sound record has been generated. Other output modalities may be used in addition to or in lieu of the graphical indications. As one example, the structured sound module 110 can cause the haptic device of the user interface device 104 to output a haptic signal (e.g., vibration or shake) in response to generating a particular type of structured sound recording. As another example, the structured sound module 110 may cause a light device (e.g., a strobe or camera flash) of the user interface device 104 to output a light signal in response to generating a structured sound recording of a particular type. In this way, the structured-sound module 110 can select one or more of a tactile output, a graphical output, and/or a light output as a non-audio indication of one or more structured-sound recordings.
The user may interact with the graphical indication of one or more structured sound recordings in a variety of ways. For example, the user may interact with the graphical notification of the GUI 116 by selecting (e.g., clicking) on the graphical notification. In response to receiving the user input selecting the graphical indication, the computing device 102 may display a timeline view, as discussed in further detail below with reference to fig. 4A and 4B.
As described above, the computing device 102 may both generate structured sound recordings and provide non-audio user output based on the structured sound recordings. In some examples, computing device 102 may cause one or more other (i.e., different) computing devices (e.g., other computing devices 114) to provide non-audio user output based on the structured sound recording. Computing device 102 may cause other computing devices to provide output in addition to or instead of non-audio user output provided by computing device 102.
Although illustrated in fig. 1 as a wearable computing device, other computing devices 114 are not so limited. Examples of other computing devices 114 may include, but are not limited to, mobile phones (including so-called "smart phones"), smart glasses, smart watches, portable speakers (including portable smart speakers), notebook computers, portable gaming systems, wireless gaming system controllers, wireless headset charging boxes, smart home devices (e.g., smart thermostats, smart smoke detectors, etc.), environmental computing devices (including so-called "smart displays"), and the like.
As described above, when generating the structured sound recording, the structured sound module 110 can select one descriptive label from a predetermined set of descriptive labels. For example, the structured-sound module 110 may monitor an incoming audio data stream generated by the microphone 106 to determine whether sounds in the audio data match sounds associated with descriptive tags in a predetermined set of descriptive tags. As discussed in further detail below, in some examples, the structured-sound module 110 may use artificial intelligence and/or machine learning (AI/ML) to determine whether sounds in the audio data match sounds associated with descriptive tags in a predetermined set of descriptive tags. In response to determining that a particular sound in the audio data matches a sound associated with a particular descriptive tag in the predetermined set of descriptive tags, the structured-sound module 110 can generate a new structured-sound record for the particular sound having a description of the particular descriptive tag and a timestamp indicating a time at which the particular sound occurred. The structured sound module 110 can store the newly generated structured sound record in a structured sound record database. As discussed herein, the structured sound module 110 can generate a non-audio user output in response to generating a new structured sound recording.
The predetermined set of descriptive tags may include emergency category tags, priority category tags, and other category tags. The emergency class tags may include one or more of a smoke alarm tag, a fire alarm tag, a carbon monoxide tag, a siren tag, and a shout tag. The priority class tags may include one or more of baby crying tags, doorbell tags, knock tags, animal warning tags, and glass breakage tags. Other categories of tags may include one or more of a running water tag, a landline ring tag, and one or more appliance beeping tags.
As described above, the structured-sound module 110 may select from different output modalities when outputting a non-audio indication of the structured-sound recording. In some examples, the structured sound module 110 may select different modalities for different descriptive tags. As one example, the structured sound module 110 can select an output modality of the non-audio indication of the particular structured sound recording based on the class of descriptive tags of the particular structured sound recording (e.g., such that the non-audio indication is output using a modality common to all descriptive tags in the particular class). As another example, the structured sound module 110 can select an output modality of the non-audio indication of the particular structured sound recording based on the descriptive label of the particular structured sound recording (e.g., such that the non-audio indication can be output using an output modality that is different even for descriptive labels in the particular category).
Fig. 2 is a conceptual diagram illustrating a system including a first computing device configured to cause a second computing device to output a non-audio indication of a structured sound recording generated at the first computing device in accordance with one or more techniques of this disclosure. As shown in fig. 2, the first computing device 202 of the system 200 may be an environmental computing device located in an infant room (e.g., a careroom). The first computing device 202 may be considered an example of the computing device 102 of fig. 1. Although illustrated as including a display, in some examples, the first computing device 202 may not include a display (e.g., where the first computing device 202 is a smoke detector, camera, speaker, etc.). As shown in the example of fig. 2, the second computing device 214 of the system 200 may be a wearable computing device worn by a caretaker of an infant sleeping in an infant room. The second computing device 214 may be considered an example of the other computing devices 114 of fig. 1.
In operation, one or more microphones of the first computing device 202 may generate audio data representative of sound in the baby room. A structured sound module (e.g., similar to structured sound module 110 of fig. 1) may generate one or more structured sound recordings based on the generated audio data. As one example, where an automobile horn is externally horn-pressed, the sound of the horn may propagate to the infant's room and be picked up by the microphone of the first computing device 202. The structured sound module of the first computing device 202 may recognize the sound as a car horn and generate a structured sound record with a descriptive tag of "car horn" and a timestamp of the car horn sound occurrence. Some sounds may last more than a single instant. Thus, in some examples, such as the example of fig. 2, the first computing device 202 may generate a structured sound record to indicate the length of sound generation and may additionally indicate the amplitude (e.g., level) of sound throughout the sound generation.
The first computing device 202 may determine whether to output an indication of the structured sound recording to one or more other computing devices, such as the second computing device 214. In the example of algo 2, the "car horn" descriptive tag may not be associated with the output modality of the external device. Thus, the first computing device 202 may not itself cause the second computing device 214 to output a non-audio indication in response to the first computing device 202 generating a structured sound recording with the descriptive tag "car horn". However, in some examples, the second computing device 214 may function as a monitor. As such, the second computing device 214 (e.g., in response to receiving such a user input request) may output a request to the first computing device 202 to provide the most recently generated structured audio recording (e.g., structured audio recording generated by the first computing device 202 in the past 10 minutes, 5 minutes, 1 minute, 30 seconds, 10 seconds, 5 seconds, etc.).
It is expected that in reality, some sounds may induce the occurrence of other sounds. For example, a horn-operated car horn in the vicinity of a baby's sleeping room may cause crying. This mode can be seen in the example of fig. 2. For example, an automobile horn may wake an infant, and the infant may start crying. The cry may be represented in audio data generated by a microphone of first computing device 202, and a structured sound module of first computing device 202 may identify the cry in the audio data and generate a structured sound record with a descriptive label "baby cry".
The first computing device 202 may determine whether to output an indication of the structured sound recording to one or more other computing devices, such as the second computing device 214. In the example of fig. 2, the "infant crying" descriptive label may be associated with an output modality (e.g., graphics and haptic) that generates a warning at the external device. As such, first computing device 202 may cause second computing device 214 to output a non-audio indication in response to first computing device 202 generating a structured sound recording with a descriptive label of "baby crying.
As shown in the example of fig. 2, the second computing device 214 may display a Graphical User Interface (GUI) that includes a non-audio indication of the structured sound recording. For example, the second computing device 214 may display a graph having a vertical axis representing amplitude (e.g., volume) and a horizontal axis representing time, each structured sound recording having a plot (e.g., a first plot illustrates the amplitude versus time of an "automobile horn" and a second plot illustrates the amplitude versus time of an "infant crying"). In this way, the second computing device 214 may not only alert the caretaker that the infant is awake and crying, but may further enable the caretaker, who may be DHH, to determine the cause of the infant waking.
Fig. 3 is a conceptual diagram illustrating a system including a first computing device configured to cause a second computing device to output a non-audio indication of a structured sound recording generated at the first computing device in accordance with one or more techniques of this disclosure. As shown in fig. 3, the first computing device 302 of the system 300 may be a mobile computing device of a user, such as user 350. The first computing device 302 may be considered an example of the computing device 102 of fig. 1. As shown in the example of fig. 3, the second computing device 314 of the system 300 may be a wearable computing device worn by a user, such as the user 350 (e.g., the first computing device 302 and the second computing device 314 may be used by a general user). The second computing device 314 may be considered an example of the other computing devices 114 of fig. 1.
In operation, one or more microphones of the first computing device 302 may generate audio data representative of sound in a room in which the first computing device 302 is located (e.g., a room in which the user 350 is sleeping). A structured sound module (e.g., similar to structured sound module 110 of fig. 1) may generate one or more structured sound recordings based on the generated audio data. As one example, where the smoke detector begins to sound (e.g., in response to detecting smoke), the sound may be picked up by the microphone of the first computing device 302. The structured sound module of the first computing device 302 can recognize the sound as a smoke alarm and generate a structured sound record with a "smoke alarm" descriptive tag and a timestamp of the occurrence of the ringing sound.
The first computing device 302 may determine whether to output an indication of the structured sound recording to one or more other computing devices, such as the second computing device 314. The "smoke alarm" descriptive tag may be associated with an output modality of the external device. For example, the descriptive label "smoke alarm" may be an emergency category label, and at least in the example of fig. 2, the generation of a structured sound recording with an emergency category label may trigger the output of a non-audio indication using each of the possible output modalities. Thus, potentially in addition to outputting one or more non-audio indications (e.g., haptic, graphical, and light) using the output device of the first computing device, the first computing device 302 may cause the second computing device 314 to output the non-audio indications in response to the first computing device 302 generating a structured sound recording with a descriptive label of "smoke alert". As shown in the example of fig. 2, the first computing device 302 may cause the second computing device 302 to output a tactile indication and a graphical indication of a structured sound recording of a descriptive label with a "smoke alert". In this way, the system 300 may more effectively make the user 350 aware of an emergency situation (e.g., smoke alarm). These techniques may be particularly useful in situations where the user 350 is located in a location where there is no DHH emergency awareness equipment (e.g., smoke alarms with built-in strobes, pillows/shaker, etc.).
In some examples, a computing device may share indications of generating certain structured sound recordings with computing devices of other users. For example, a user's computing device, such as the computing device 302 of the user 350, may output alerts to other computing devices in response to generating a structured sound recording with the tag "smoke alarm". Other computing devices may be associated with other users in a particular geographic area (e.g., a radius or geofence around computing device 302). In some examples, computing device 302 may register as a provider and/or subscribe to be a recipient of such alerts (e.g., via a server system or other service).
Fig. 4A and 4B are conceptual diagrams illustrating example Graphical User Interfaces (GUIs) that may be output by a computing device generating a structured sound recording in accordance with one or more techniques of the present disclosure. As described above, the computing device may generate a non-audio output based on the generated structured sound recording. In some examples, DHH people may have difficulty obtaining insight about sound based solely on sound markers and/or text transcription.
In accordance with one or more techniques of this disclosure, a computing device, such as computing device 102 of fig. 1, may output a non-audio indication of one or more structured sound recordings as a graphical user interface that includes a timeline representation of the one or more structured sound recordings. The timeline representation may graphically indicate an order in which sounds of one or more structured sound recordings occur. For example, as shown in fig. 4A, the timeline representation in GUI 460 indicates that a knock occurs before a smoke alarm sound, a smoke alarm sound occurs before a dog barking, and a dog barking occurs before an appliance beeping. As can also be seen in the example of fig. 4A, the timeline representation may include a horizontal axis representing time. In other examples, the representation may be rotated such that time is represented on the vertical axis. The representation of the structured sound recordings in the graphical timeline representation may include an indication of their descriptive labels (e.g., in text form) and may also include an indication of the category of their descriptive labels (e.g., color-coded, such as: emergency category red, preferably yellow, and others green).
The user may interact with the graphical timeline representation in a variety of ways. As an example, as shown in the example of fig. 4A, a particular representation of the structured sound recording may be selected (e.g., by a user clicking on the representation), and further information about the selected structured sound recording may be displayed (e.g., how long ago the structured sound recording was generated). As another example, the user may provide user input to scroll forward or backward in time. For example, in response to receiving user input to scroll forward in time when GUI 460 is displayed, the computing device may display GUI 462, which is a view of the same graphical timeline representation, but later in time than shown in GUI 460.
In some examples, a first computing device, such as computing device 102 of fig. 1, may output a representation of a structured sound recording to a second computing device (such as computing device 114 of fig. 1) to enable the second computing device to output a timeline representation of the structured sound recording. In some examples, the representation of the structured sound recording may be a copy of the structured sound recording. In other examples, the representation of the structured sound recording may include a subset of data contained in the original structured sound recording. By enabling the second computing device to output the timeline representation, both computing devices can view the same timeline representation of the structured sound recording collection. For example, a first computing device may view a timeline representation of structured sound records comprising a collection of structured sound records in a first time window, and a second computing device may view a timeline representation of structured sound records comprising the set of structured sound records in a second time window (e.g., which may be the same as the first time window or may be different). Thus, the first computing device may output a first graphical user interface that includes a current timeline representation of one or more structured sound recordings, and the second computing device may output a second graphical user interface that includes a past timeline representation of one or more structured sound recordings.
The computing device may output a timeline representation of the structured sound recording that includes descriptive tags (e.g., that does not include text transcription), a timeline representation of the structured sound recording that includes text transcription, and a hybrid timeline representation of the structured sound recording that includes descriptive tags and the structured sound recording that includes text transcription. Combining the timeline representation with the ability to enable the second computing device to output a timeline representation of the structured sound recording may facilitate various use cases.
As one example, where both the first user and the second user are engaged in the same event (e.g., concert, performance, speech, conversation, etc.) and the first user is sitting in a better audio vantage point (e.g., closer to a stage or speaker), a first computing device used by the first user may generate and output a structured sound recording to a second computing device used by the second user. This may enable the second user to view a timeline representation (e.g., which may include a text transcription of the event) that is more accurate than what would have been generated by the second computing device (e.g., based on audio data generated by a microphone of the second computing device) because the second user has a worse audio vantage point than the first user. In some examples, such as where a hybrid timeline view is used, the timeline representation may include marked-up sounds (e.g., applause) and may show the audience's reactions to the performance (frequency, length of time, volume, etc. of their applause).
As another example, where a single user is using a first computing device and a second computing device, the first computing device may generate and output a structured sound recording to the second computing device. The user may then scroll back in time using the second computing device while still occasionally panning across the timeline representation on the first computing device, periodically updating the timeline representation with new structured sound recordings (as they are generated).
In some examples, such as where the timeline view includes text transcription, the timeline view may also include representations of emotional characteristics of the text transcription. For example, the computing device may output an indication of emotional characteristics corresponding to text transcription that approximates a structured sound recording with emotional characteristics (e.g., displaying sad facial emoticons near text transcribed from speech spoken with sad sounds).
Although described above in the context of benefiting users as DHHs, the techniques of this disclosure are not so limited. For example, the techniques of this disclosure may be useful to a person with hearing. For example, parents may return to home and quickly look at sound events that they are not at home.
Fig. 5A and 5B are conceptual diagrams illustrating example Graphical User Interfaces (GUIs) that may be output by a computing device generating a structured sound recording in accordance with one or more techniques of the present disclosure. As described above, the computing device may monitor audio data generated by one or more microphones of the computing device to selectively generate structured audio recordings. Controls are presented to a user of the computing device to enable the user to selectively enable and disable such audio data monitoring. For example, while the computing device is actively monitoring audio data, the computing device may output GUI 570, which includes an indication of monitoring activity and controls (e.g., a pause button) for giving the user control to disable the monitoring. In response to a user providing a user input to disable monitoring (e.g., clicking a pause button), the computing device may output GUI 572, which includes an indication of monitoring not active and controls (e.g., a play button) for providing user control to enable monitoring. The computing device may not monitor the audio data without prior consent from the user.
In addition to the above description, controls may be provided for the user that allow the user to select: whether and when the systems, programs, or features described herein may enable collection of user information (e.g., information about a user's social network, social behavior or activity, profession, user's preferences, or the user's current location); and whether to send content or communications from the server to the user. In addition, some data may be processed in one or more ways before it is stored or used in order to remove personal identification information. For example, the identity of the user may be processed such that personal identity information of the user cannot be determined, or the geographic location of the user may be generalized in the event that location information is obtained (such as reaching a city, zip code, or state level) such that a particular location of the user cannot be determined. Thus, the user can control what information is collected about the user, how that information is used, and what information is provided to the user.
Fig. 6A-6E are conceptual diagrams illustrating aspects of an example machine learning model according to example embodiments of the present disclosure. Fig. 6A through 6E are described below in the context of one or more models used by the structured sound module 110 of fig. 1. For example, in some cases, the machine learning model 300 (described below) may be an example of one or more models used by the structured sound module 110 of fig. 1.
Fig. 6A depicts a conceptual diagram of an example machine learning model, according to an example embodiment of the disclosure. As shown in fig. 6A, in some implementations, the machine learning model 600 is trained to receive one or more types of input data and in response provide one or more types of output data. Thus, FIG. 6A illustrates a machine learning model 600 that performs reasoning.
The input data may include one or more features associated with the instance or example. In some implementations, one or more features associated with an instance or example may be organized into feature vectors. In some implementations, the output data may include one or more predictions. Predictions may also be referred to as inferences. Thus, given features associated with a particular instance, the machine learning model 600 may output predictions for such instance based on the features.
The machine learning model 600 may be or include one or more of a variety of different types of machine learning models. Specifically, in some implementations, the machine learning model 600 may perform classification, regression, clustering, anomaly detection, recommendation generation, and/or other tasks.
In some implementations, the machine learning model 600 may perform various types of classification based on input data. For example, the machine learning model 600 may perform binary classification or multi-component classification. In binary classification, outputting the data may include classifying the input data into one of two different categories. In multivariate classification, outputting the data may include classifying the input data into one (or more) of more than two categories. The classification may be single or multiple labels. The machine learning model 600 may perform discrete class classification in which input data is simply classified into one or more classifications or classes.
In some implementations, the machine learning model 600 may perform classification, wherein the machine learning model 600 provides, for each of one or more categories, a numerical value that describes the degree to which the input data is considered to fall within the corresponding classification. In some cases, the numerical values provided by the machine learning model 600 may be referred to as "confidence scores" that indicate respective confidence associated with classifying the input into the respective components. In some implementations, the confidence score can be compared to one or more thresholds to present discrete classification predictions. In some implementations, only a certain number of classifications with a relative maximum confidence score (e.g., one) may be selected to present discrete classification predictions.
The machine learning model 600 may output a probabilistic classification. For example, the machine learning model 600 may predict probability distributions over a collection of classifications given sample inputs. Thus, rather than outputting only the most likely classification to which the sample input should belong, the machine learning model 600 may output, for each classification, the probability that the sample input belongs to that classification. In some embodiments, the probability distribution over all possible classifications may sum to one. In some implementations, a Softmax function or other type of function or layer may be used to compress the set of real values respectively associated with the possible classifications into a set of real values that sum to one in the range (0, 1).
In some examples, the probability provided by the probability distribution may be compared to one or more thresholds to present discrete classification predictions. In some implementations, only a certain number of classifications (e.g., one) with a relative maximum prediction probability may be selected to present discrete classification predictions.
Where the machine learning model 600 performs classification (e.g., classification of sounds), the machine learning model 600 may be trained using supervised learning techniques. For example, the machine learning model 600 may be trained on a training dataset that includes training examples labeled as belonging to (or not belonging to) one or more classifications (e.g., one or more predetermined descriptive labels). Further details regarding supervised training techniques are provided below in the descriptions of fig. 6B-6E.
In some implementations, the machine learning model 600 may perform regression to provide output data in the form of continuous values. The consecutive values may correspond to any number of different metrics or numerical representations, including, for example, monetary values, scores, or other numerical representations. As an example, the machine learning model 600 may perform linear regression, polynomial regression, or non-linear regression. As an example, the machine learning model 600 may perform a simple regression or a multiple regression. As described above, in some embodiments, a Softmax function or other function or layer may be used to compress the set of real values associated with two or more possible classifications, respectively, into a set of real values that sum to one in the range (0, 1).
The machine learning model 600 may perform various types of clustering. For example, the machine learning model 600 may identify one or more previously defined clusters to which the input data most likely corresponds. The machine learning model 600 may identify one or more clusters within the input data. That is, where the input data includes a plurality of objects, documents, or other entities, the machine learning model 600 may categorize the plurality of entities included in the input data into a plurality of clusters. In some embodiments in which the machine learning model 600 performs clustering, the machine learning model 600 may be trained using unsupervised learning techniques.
The machine learning model 600 may perform anomaly detection or outlier detection. For example, the machine learning model 600 may identify input data that does not conform to an expected pattern or other feature (e.g., as previously observed from previous input data). For example, anomaly detection may be used for fraud detection or system fault detection.
In some cases, the machine learning model 600 may act as a proxy in the environment. For example, the machine learning model 600 may be trained using reinforcement learning, which is discussed in further detail below.
In some implementations, the machine learning model 600 may be a parametric model, while in other implementations, the machine learning model 600 may be a non-parametric model. In some implementations, the machine learning model 600 may be a linear model, while in other implementations, the machine learning model 600 may be a nonlinear model.
As described above, the machine learning model 600 may be or include one or more of a variety of different types of machine learning models. Examples of such different types of machine learning models are provided below for illustration. One or more example models described below may be used (e.g., combined) to provide output data in response to input data. Other models may be used in addition to the example models provided below.
In some implementations, the machine learning model 600 may be or include one or more classifier models, such as, for example: a linear classification model; a secondary classification model; etc. The machine learning model 600 may be or include one or more regression models, such as, for example: a simple linear regression model; a multiple linear regression model; a logistic regression model; stepwise regression model; a multi-element adaptive regression spline; a locally estimated scatter plot smoothing model; etc.
In some examples, the machine learning model 600 may be or include one or more decision tree-based models, such as, for example: classification and/or regression trees; iterative dichotomy 3 decision tree; c4.5 decision tree; the chi-square automatically and interactively detects a decision tree; decision tree stakes; a conditional decision tree; etc.
The machine learning model 600 may be or include one or more kernel machines. In some implementations, the machine learning model 600 may be or include one or more support vector machines. The machine learning model 600 may be or include one or more example-based learning models, such as, for example: learning a vector quantization model; a self-organizing map model; a locally weighted learning model; etc. In some implementations, the machine learning model 600 may be or include one or more nearest neighbor models, such as, for example: k nearest neighbor classification model; k nearest neighbor regression model; etc. The machine learning model 600 may be or include one or more bayesian models, such as, for example: a naive bayes model; a gaussian naive bayes model; a polynomial na iotave bayes model; average single correlation estimates; a bayesian network; a bayesian belief network; a hidden Markov model; etc.
In some implementations, the machine learning model 600 may be or include one or more artificial neural networks (also simply referred to as neural networks). A neural network may include a group of connected nodes, which may also be referred to as neurons or perceptrons. The neural network may be organized into one or more layers. Neural networks comprising multiple layers may be referred to as "deep" networks. The depth network may include an input layer, an output layer, and one or more hidden layers located between the input layer and the output layer. The nodes of the neural network may be connected or not fully connected.
The machine learning model 600 may be or include one or more feed-forward neural networks. In a feed forward network, the connections between nodes do not form loops. For example, each connection may connect a node from an earlier tier to a node from a later tier.
In some cases, the machine learning model 600 may be or include one or more recurrent neural networks. In some cases, at least some nodes of the recurrent neural network may form a loop. Recurrent neural networks are particularly useful for processing input data that is sequential in nature. Specifically, in some cases, the recurrent neural network may pass or retain information from a previous portion of the input data sequence to a subsequent portion of the input data sequence through the use of a recurrent or directed loop node connection.
In some examples, the sequential input data may include time series data (e.g., sensor data versus time or images captured at different times). For example, the recurrent neural network may analyze the sensor data versus time to detect or predict a sliding direction, perform handwriting recognition, and the like. The sequential input data may include: words in sentences (e.g., for natural language processing, speech detection or processing, etc.); notes in a musical composition; sequential operations that the user takes (e.g., to detect or predict sequential application usage); a sequential object state; etc.
An example recurrent neural network includes: long-short term (LSTM) recurrent neural networks; a gating recursion unit; a bi-directional recurrent neural network; a continuous time recurrent neural network; a nerve history compressor; an echo state network; an elman network; a jordan network; a recurrent neural network; a hopfield network; a completely recursive network; sequence-to-sequence configuration; etc.
In some implementations, the machine learning model 600 may be or include one or more convolutional neural networks. In some cases, the convolutional neural network may include one or more convolutional layers that perform convolution on input data using a learning filter.
Filters may also be referred to as kernels. Convolutional neural networks are particularly useful for vision problems, such as when the input data contains images that are still images or video. However, convolutional neural networks may also be applied to natural language processing.
In some examples, the machine learning model 600 may be or include one or more generation networks, such as, for example, generation of an antagonism network. The generation network may be used to generate new data, such as new images or other content.
The machine learning model 600 may be or include an automatic encoder. In some cases, the purpose of an automatic encoder is to learn a representation of a data set (e.g., low-dimensional encoding), typically for the purpose of dimension reduction. For example, in some cases, an automatic encoder may seek to encode input data and provide output data that reconstructs the input data from the encoding. Recently, the automatic encoder concept has been more widely used for learning a model of data generation. In some cases, the auto-encoder may contain additional losses beyond reconstructing the input data.
The machine learning model 600 may be or include one or more other forms of artificial neural networks, such as, for example: a deep boltzmann machine; a deep belief network; stacking an automatic encoder; etc. Any of the neural networks described herein can be combined (e.g., stacked) to form a more complex network.
One or more neural networks may be used to provide input data based embedding. For example, embedding may be abstracting a knowledge representation from the input data to one or more learning dimensions. In some cases, embedding may be a useful source for identifying related entities. In some cases, the embeddings may be extracted from the output of the network, while in other cases, the embeddings may be extracted from any hidden node or layer of the network (e.g., near the final but not final layer of the network). The embedding may be used to perform auto-suggestion of next video, product suggestion, entity or object recognition, etc. In some cases, embedding is a useful input to the downstream model. For example, embedding may be used to generalize input data (e.g., search queries) of a downstream model or processing system.
The machine learning model 600 may include one or more cluster models, such as, for example, a k-means cluster model; k median clustering model; the maximization model is expected; a hierarchical clustering model; etc.
In some implementations, the machine learning model 600 may perform one or more dimension reduction techniques, such as, for example: analyzing principal components; analyzing the main component of the kernel; graph-based kernel principal component analysis; principal component regression; partial least squares regression; sammon mapping; multidimensional scaling; projection tracking; linear discriminant analysis; mixing and discriminant analysis; secondary discriminant analysis; generalized discriminant analysis; flexible discriminant analysis; automatically encoding; etc.
In some implementations, the machine learning model 600 may perform or be amenable to one or more reinforcement learning techniques, such as, for example: a Markov decision process; dynamic planning is carried out; q function or Q learning; a cost function method; a deep Q network; a microcomputer; asynchronous dominant actor critics; a deterministic policy gradient; etc.
In some implementations, the machine learning model 600 may be an autoregressive model. In some cases, the autoregressive model may specify that the output data depends linearly on its own previous values and random terms. In some cases, the autoregressive model may take the form of a random differential equation. One example autoregressive model is WaveNet, which is a model of the generation of raw audio.
In some implementations, the machine learning model 600 may include or form part of a multi-model ensemble. As one example, a boot aggregation (bootstrap aggregating) may be performed, which may also be referred to as "bagging. In the guided aggregation, the training data set is divided into a plurality of subsets (e.g., by random sampling with substitution), and a plurality of models are trained on the plurality of subsets, respectively. In reasoning, the respective outputs of the multiple models may be combined (e.g., by averaging, voting, or other techniques) and used as the output of the ensemble.
One example ensemble is a random forest, which may also be referred to as a random decision forest. Random forests are an ensemble learning method for classification, regression and other tasks. Random forests are generated by generating multiple decision trees during training. In some cases, patterns as classes (classifications) or average predictions of individual trees (regression) can be used as output of the forest at the time of reasoning. Random decision forests can correct the tendency of decision trees to overfit their training sets.
Another example ensemble technique is stacking, which in some cases may be referred to as stacked generalization. Stacking includes training a combiner model to mix or otherwise combine predictions of several other machine learning models. Thus, multiple machine learning models (e.g., of the same or different types) may be trained based on the training data. In addition, the combiner model may be trained to take predictions from other machine learning models as input and to generate final inferences or predictions in response. In some cases, a single-layer logistic regression model may be used as the combiner model.
Another example ensemble technique is lifting. Lifting may include building an ensemble step by iteratively training the weak model and then adding to the final strong model. For example, in some cases, each new model may be trained to emphasize training examples of prior model misunderstandings (e.g., misclassifications). For example, the weight associated with each such misinterpreted example may be increased. One common implementation of lifting is AdaBoost, which may also be referred to as adaptive lifting. Other example boosting techniques include LPBoost; totalBoost; brown boost; xgboost; madaBoost, logitBoost, gradient lifting; etc. Furthermore, any of the models described above (e.g., regression models and artificial neural networks) may be combined to form an ensemble. For example, the ensemble may include a top-level machine learning model or heuristic function to combine and/or weight the outputs of the models that form the ensemble.
In some implementations, multiple machine learning models (e.g., that form an ensemble that can be linked and co-trained (e.g., by error back-propagation sequentially through the model ensemble)). However, in some embodiments, only a subset (e.g., one) of the jointly trained models are used for reasoning.
In some implementations, the machine learning model 600 may be used to pre-process input data for subsequent input into another model. For example, the machine learning model 600 may perform: dimension reduction techniques and embedding (e.g., matrix decomposition, principal component analysis, singular value decomposition, word2vec/GLOVE, and/or related means); clustering; and even classification and regression of downstream consumption. Many of these techniques have been discussed above and will be discussed further below.
As described above, the machine learning model 600 may be trained or otherwise configured to receive input data and provide output data in response. The input data may include different types, forms, or variants of the input data. As an example, in various embodiments, the input data may include features describing the content (or a portion of the content) that the user originally selected, e.g., the content of the document or image that the user selected, links to the user selections, links within the user selections related to other files available on the device or cloud, metadata of the user selections, etc. Further, in the case of user permissions, the input data includes the context of user usage that may be obtained from the app itself or from other sources. Examples of usage contexts include shared breadth (shared publicly, or with a large group of people, or privately, or with a particular person), shared context, and the like. When allowed by the user, the additional input data may include the state of the device, e.g., the location of the device, the app running on the device, etc.
In some implementations, the machine learning model 600 may receive and use input data in raw form. In some implementations, the raw input data may be preprocessed. Thus, the machine learning model 600 may receive and use pre-processed input data in addition to or in place of the original input data.
In some implementations, preprocessing the input data may include extracting one or more additional features from the original input data. For example, feature extraction techniques may be applied to the input data to generate one or more new additional features. Example feature extraction techniques include: edge detection; detecting corner points; spot detection; ridge detection; the scale-invariant feature transformation; detecting motion; optical flow; hough transformation; etc.
In some implementations, the extracted features can include transformations of or derived from input data to other domains and/or dimensions. For example, the extracted features may include or be derived from a transformation of the input data into the frequency domain. For example, a wavelet transform and/or a fast fourier transform may be performed on the input data to generate additional features.
In some implementations, the extracted features may include statistics calculated from the input data or some portion or dimension of the input data. Example statistics include patterns, averages, maxima, minima, or other metrics of the input data or portions thereof.
In some embodiments, as described above, the input data may be sequential in nature. In some cases, sequential input data may be generated by sampling or otherwise splitting the input data stream. As one example, frames may be extracted from a video. In some embodiments, sequential data may be made non-sequential data by summarization.
As another example preprocessing technique, portions of the input data may be estimated. For example, additional synthetic input data may be generated by interpolation and/or extrapolation.
As another example preprocessing technique, some or all of the input data may be scaled, normalized, summarized, and/or regularized. Example regularization techniques include: ridge regression; a Least Absolute Shrinkage and Selection Operator (LASSO); an elastic net; minimum angle regression; cross-validation; regularization of L1; regularization of L2; etc. As one example, some or all of the input data may be normalized by subtracting the average of the eigenvalues of a given dimension from each individual eigenvalue and then dividing by the standard deviation or other metric.
As another example preprocessing technique, some or all of the input data may be quantized or discretized. In some cases, qualitative features or variables contained in the input data may be converted into quantitative features or variables. For example, the thermal encoding may be performed once.
In some examples, dimension reduction techniques may be applied to the input data prior to input to the machine learning model 600. Several examples of dimension reduction techniques are provided above, including, for example: analyzing principal components; analyzing the main component of the kernel; graph-based kernel principal component analysis; principal component regression; partial least squares regression; sammon mapping; multidimensional scaling; projection tracking; linear discriminant analysis; mixing and discriminant analysis; secondary discriminant analysis; generalized discriminant analysis; flexible discriminant analysis; automatically encoding; etc.
In some embodiments, during training, the input data may be intentionally deformed in any number of ways to increase model robustness, generalization, or other quality. Example techniques for deforming input data include: adding noise; changing color, shade or hue; amplifying; dividing; enlarging; etc.
In response to receipt of the input data, the machine learning model 600 may provide output data. The output data may include different types, forms, or variants of the output data. As an example, in various embodiments, the output data may include content stored locally at the user device or in the cloud, which is sharable in association with the initial content selection.
As described above, in some embodiments, the output data may include various types of classification data (e.g., binary classification, multi-class classification, single label, multi-label, discrete classification, regression classification, probability classification, etc.) or may include various types of regression data (e.g., linear regression, polynomial regression, nonlinear regression, simple regression, multiple regression, etc.). In other cases, the output data may include cluster data, anomaly detection data, recommendation data, or any other form of output data discussed above.
In some implementations, the output data may affect downstream processes or decision making. As one example, in some implementations, the output data may be interpreted and/or acted upon by a rule-based regulator.
The present disclosure provides systems and methods that include or otherwise utilize one or more machine learning models to suggest content that is stored locally on a user device or in the cloud that may be sharable in connection with initial content selection based on characteristics of the initial content selection. Any of the different types or forms of input data described above may be combined with any of the different types or forms of machine learning models described above to provide any of the different types or forms of output data described above.
The systems and methods of the present disclosure may be implemented by or otherwise executed on one or more computing devices. An example computing device includes: user computing devices (e.g., notebook computers, desktop computers, and mobile computing devices, such as tablet computers, smartphones, wearable computing devices, etc.); embedded computing devices (e.g., devices embedded in a vehicle, camera, image sensor, industrial machine, satellite, gaming machine or controller, or household appliances such as a refrigerator, thermostat, electric energy meter, home energy manager, smart home assistant, etc.); server computing devices (e.g., database servers, parameter servers, file servers, mail servers, print servers, web servers, game servers, application servers, etc.); dedicated, specialized model processing or training equipment; a virtual computing device; other computing devices or computing infrastructures; or a combination thereof.
Fig. 6B illustrates a conceptual diagram of a computing device 610, the computing device 610 being an example of the computing device 102 of fig. 1. The computing device 610 includes a processing component 602, a memory component 604, and a machine learning model 600. The computing device 610 may store and implement the machine learning model 600 locally (i.e., on the device). Thus, in some implementations, the machine learning model 600 may be stored at and/or implemented locally by an embedded device or a user computing device such as a mobile device. Output data obtained through the local implementation of the machine learning model 600 at an embedded device or user computing device may be used to improve performance of the embedded device or user computing device (e.g., an application implemented by the embedded device or user computing device).
FIG. 6C illustrates a conceptual diagram of an example client computing device that may communicate over a network with an example server computing system that includes a machine learning model. Fig. 6C includes a client device 610A in communication with a server device 660 over a network 630. Client device 610A is an example of computing device 102 of fig. 1. The server device 660 stores and implements the machine learning model 600. In some cases, the output data obtained at the server device 660 through the machine learning model 600 may be used to improve other server tasks or may be used by other non-user devices to improve services performed by such other non-user devices or performed by such other non-user devices. For example, the output data may improve other downstream processes performed by the server device 660 for the user's computing device or embedded computing device. In other cases, output data obtained by implementing the machine learning model 600 at the server device 660 may be sent to and used by a user computing device, an embedded computing device, or some other client device (e.g., the client device 610A). For example, the server device 660 may be said to conduct machine learning as a service.
In still other implementations, different respective portions of the machine learning model 600 may be stored at and/or implemented by some combination of the following portions: a user computing device; an embedded computing device; a server computing device; etc. In other words, portions of the machine learning model 600 may be distributed in whole or in part between the client device 610A and the server device 660.
Devices 610A and 660 may use one or more machine learning platforms, frameworks, and/or libraries (such as, for example, tensorFlow, caffe/Caffe2, theano, torch/PyTorch, MXnet, CNTK, etc.) to perform graphics processing techniques or other machine learning techniques. Devices 610A and 660 may be distributed at different physical locations and connected via one or more networks, including network 630. If configured as a distributed computing device, devices 610A and 660 may operate in accordance with a sequential computing architecture, a parallel computing architecture, or a combination thereof. In one example, the distributed computing device may be controlled or booted through the use of a parameter server.
In some implementations, multiple instances of the machine learning model 600 may be parallelized to provide increased processing throughput. For example, multiple instances of the machine learning model 600 may be parallelized on a single processing device or computing device or parallelized across multiple processing devices or computing devices.
Each computing device implementing the machine learning model 600 or other aspects of the disclosure may include a plurality of hardware components that implement the performance of the techniques described herein. For example, each computing device may include one or more memory devices that store some or all of the machine learning model 600. For example, the machine learning model 600 may be a structured numerical representation stored in memory. The one or more memory devices may also include instructions for implementing the machine learning model 600 or performing other operations. Example memory devices include RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, and the like, as well as combinations thereof.
Each computing device may also include one or more processing devices that implement some or all of the machine learning model 600 and/or perform other related operations. Example processing devices include one or more of the following: a Central Processing Unit (CPU); a Vision Processing Unit (VPU); a Graphics Processing Unit (GPU); tensor Processing Unit (TPU); a Neural Processing Unit (NPU); a neural processing engine; CPU, VPU, GPU, TPU, NPU or other processing device; an Application Specific Integrated Circuit (ASIC); a Field Programmable Gate Array (FPGA); a coprocessor; a controller; or a combination of the above processing devices. The processing device may be embedded in other hardware components, such as image sensors, accelerometers, etc.
Hardware components (e.g., storage devices and/or processing devices) may be distributed across multiple physically distributed computing devices and/or virtually distributed computing systems.
FIG. 6D illustrates a conceptual diagram of an example computing device in communication with an example training computing system that includes a model trainer. Fig. 6D includes a client device 610B in communication with a training device 670 through a network 630. Client device 610B is an example of computing device 102 of fig. 1. The machine learning model 600 described herein may be trained at a training computing system, such as the training device 670, and then provided for storage and/or implementation at one or more computing devices, such as the client device 610B. For example, model trainer 672 may execute locally at training device 670. However, in some examples, training device 670 including model trainer 672 may be included in or separate from client device 610B or any other computing device implementing machine learning model 600.
In some implementations, the machine learning model 600 may be trained in an offline manner or an online manner. In offline training (also referred to as batch learning), the machine learning model 600 is trained on the entire static training dataset. In online learning, the machine learning model 600 is continuously trained (or retrained) as new training data becomes available (e.g., when the model is used to perform reasoning).
The machine learning model 600 described herein may be trained in accordance with one or more of a variety of different training types or techniques. For example, in some implementations, the machine learning model 600 may be trained by the model trainer 672 using supervised learning, wherein the machine learning model 600 is trained on a training data set comprising instances or examples with labels. These tags may be manually applied by an expert, generated by crowdsourcing, or provided by other techniques (e.g., by physics-based or complex mathematical models). In some implementations, the training examples can be provided by the user computing device if the user agrees. In some implementations, this process may be referred to as a personalized model.
Fig. 6E illustrates a conceptual diagram of a training process 690, the training process 690 being an example training process in which the machine learning model 600 is trained on training data 391 including example input data 692 with labels 693. Training process 690 is one example training process; other training procedures may also be used.
In some implementations, the machine learning model 600 may be trained by optimizing an objective function, such as the objective function 695. For example, in some implementations, the objective function 695 may be or include a loss function that compares (e.g., determines differences between) output data generated by the model from the training data with tags (e.g., real tags) associated with the training data. For example, the loss function may evaluate the sum or mean of the squared differences between the output data and the labels. In some examples, the objective function 695 may be or include a cost function describing the cost of a particular result or output data. Other examples of objective function 695 may include margin-based techniques such as, for example, triple loss or maximum margin training.
One or more of a variety of optimization techniques may be performed to optimize the objective function 695. For example, the optimization technique may minimize or maximize the objective function 695. Example optimization techniques include Hessian-based techniques and gradient-based techniques, such as: descending coordinates; gradient descent (e.g., random gradient descent); a secondary gradient method; etc. Other optimization techniques include black box optimization techniques and heuristics.
In some implementations, the back propagation of errors can be used in conjunction with optimization techniques (e.g., gradient-based techniques) to train the machine learning model 600 (e.g., when the machine learning model is a multi-layer model such as an artificial neural network). For example, an iterative loop of propagation and model parameter (e.g., weight) updates may be performed to train the machine learning model 600. Example back propagation techniques include back propagation with time truncation, levenberg-Marquardt back propagation, and the like.
In some implementations, the machine learning model 600 described herein may be trained using unsupervised learning techniques. Unsupervised learning may include inferring a function to describe hidden structures of unlabeled data. For example, the classification or category may not be included in the data. Unsupervised learning techniques may be used to generate machine learning models capable of performing clustering, anomaly detection, learning latent variable models, or other tasks.
The machine learning model 600 may be trained using semi-supervised techniques that combine aspects of supervised learning and unsupervised learning. The machine learning model 600 may be trained or otherwise generated by evolutionary techniques or genetic algorithms. In some implementations, the machine learning model 600 described herein may be trained using reinforcement learning. In reinforcement learning, agents (e.g., models) may take actions in the environment and learn to maximize rewards and/or minimize penalties caused by such actions. Reinforcement learning differs from supervised learning in that correct input/output pairs are not presented nor are sub-optimal actions explicitly corrected.
In some implementations, one or more generalization techniques may be performed during training to improve generalization of the machine learning model 600. Generalization techniques can help reduce overfitting of the machine learning model 600 to training data. Example generalization techniques include dropout techniques; weight decay techniques; normalizing in batches; stopping in advance; selecting a subset; gradually selecting; etc.
In some implementations, the machine learning model 600 described herein may include or otherwise be affected by a plurality of super parameters, such as, for example, a learning rate, a number of layers, a number of nodes in each layer, a number of leaves in a tree, a number of clusters; etc. Hyper-parameters can affect model performance. The hyper-parameters may be selected manually or may be selected automatically by applying techniques such as the following: such as, for example, a grid search; black box optimization techniques (e.g., bayesian optimization, random search, etc.); optimizing based on gradient; etc. Example techniques and/or tools for performing automatic hyper-parametric optimization include Hyperopt; auto-WEKA; peppermint (Spearmint); index optimization engine (MOE); etc.
In some implementations, various techniques may be used to optimize and/or adjust the learning rate as the model is trained. Example techniques and/or tools for performing learning rate optimization or adaptation include: adagard; adaptive moment estimation (ADAM); adadelta; RMSprop; etc.
In some implementations, the transfer learning technique may be used to provide an initial model from which to begin training of the machine learning model 600 described herein.
In some implementations, the machine learning model 600 described herein may be included in different portions of computer readable code on a computing device. In one example, the machine learning model 600 may be included in and used by (e.g., exclusively) a particular application or program. Thus, in one example, a computing device may include multiple applications, and one or more of such applications may contain its own respective machine learning libraries and machine learning models.
In another example, the machine learning model 600 described herein may be included in an operating system of a computing device (e.g., in a central intelligent layer of the operating system) and may be invoked or otherwise used by one or more applications that interact with the operating system. In some implementations, each application can communicate with the central intelligence layer (and the model stored therein) using an Application Programming Interface (API) (e.g., a generic, public API across all applications).
In some implementations, the central intelligence layer can communicate with the central device data layer. The central device data layer may be a central data store of the computing device. The central device data layer may communicate with many other components of the computing device, such as, for example, one or more sensors, a context manager, a device status component, and/or additional components. In some implementations, the central device data layer can communicate with each device component using an API (e.g., a proprietary API).
The technology discussed herein refers to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from such systems. The flexibility inherent in computer-based systems allows for a variety of possible configurations, combinations, and divisions of tasks and functions between components. For example, the processes discussed herein may be implemented using a single device or component or multiple devices or components working in combination.
The database and applications may be implemented on a single system or distributed across multiple systems. Distributed components may run sequentially or in parallel.
Furthermore, the machine learning techniques described herein may be readily interchangeable and combinable. While certain example techniques have been described, there are many other techniques and these many other techniques may be used in connection with aspects of the present disclosure.
The present disclosure has provided a brief overview of an example machine learning model and related techniques. Regarding more details, the reader should see the following references: machine learning probability perspective (Murphy); machine learning rules: machine learning engineering best practices (zinkavich); deep learning (Goodfellow); reinforcement learning: introduction (Sutton); and, artificial intelligence: a modern approach (Norvig).
In addition to the above description, controls may be provided for the user that allow the user to select: whether and when the systems, programs, or features described herein may enable collection of user information (e.g., information about a user's social network, social behavior or activity, profession, user's preferences, or the user's current location); and whether to send content or communications from the server to the user. In addition, some data may be processed in one or more ways before it is stored or used in order to remove personal identification information. For example, the identity of the user may be processed such that personal identity information of the user cannot be determined, or the geographic location of the user may be generalized in the event that location information is obtained (e.g., reaching a city, zip code, or state level) such that a particular location of the user cannot be determined. Thus, the user can control what information is collected about the user, how that information is used, and what information is provided to the user.
FIG. 7 is a conceptual diagram illustrating an example Graphical User Interface (GUI) that may be output by a computing device generating a structured sound recording in accordance with one or more techniques of the present disclosure. As shown in fig. 7, a computing device, such as computing device 102 of fig. 1, may output a GUI with options for a user to enable or disable the output of non-audio indications in response to the generation of a structured audio recording with various tags. For example, the user may individually and selectively cause the computing device to output or not output a non-audio indication in response to the generation of a structured audio recording having a smoke alarm tag, a fire alarm tag, a siren tag, a shout tag, a baby crying tag, a glass break tag, a doorbell sound tag, a knock tag, and a dog bark tag.
Fig. 8 is a flowchart illustrating an example technique for displaying a timeline representation of structured sound recordings in accordance with one or more techniques of the present disclosure. Although described with reference to computing device 102 of fig. 1 and/or computing device 202 of fig. 2, the operations of fig. 8 may be performed by any suitable component of a computing device.
The computing device may receive audio data recorded by one or more microphones of the computing device (802). For example, one or more microphones 106 of computing device 102 may generate electrical signals representative of sound. The processor 108 of the computing device 102 may receive a representation of the electrical signal.
The computing device may generate, by one or more processors, one or more structured sound recordings based on the audio data (804). For example, the processor 108 may process the audio data to generate a structured sound recording. The structured sound recording may comprise one or more parameters. For example, each respective one of the structured sound records may include a respective description and a respective timestamp. Thus, a first structured-sound recording of the one or more structured-sound recordings may comprise: a description of the first sound, the description including a descriptive label of the first sound, the descriptive label being different from a text transcription of the first sound; and a time stamp indicating a time of the first sound generation.
The computing device may output a graphical user interface (806) including a timeline representation of one or more structured sound recordings. For example, computing device 102 may output a graphical user interface similar to GUI 460 of fig. 4A or GUI 462 of fig. 4B. In some examples, computing device 102 may output a graphical user interface locally (e.g., at a display of the computing device). In some examples, computing device 102 may cause another computing device to output a graphical user interface. For example, computing device 202 may cause computing device 214 to output a graphical user interface that includes a timeline representation. In some examples, computing device 102 may output the timeline representation locally and cause other computing devices to output the timeline representation.
In some examples, the computing device may provide an indication to view the timeline representation. For example, in response to receiving a user input at computing device 202 indicating that the timeline representation was viewed at a particular time, computing device 202 may modify the output of the timeline representation at computing device 214 to indicate a previous view. User input indicative of viewing may include, but is not necessarily limited to, touch input (e.g., click, swipe, etc.), verbal input, etc., proximate to a portion of the display of the computing device 202 displaying the timeline representation. To modify the output of the timeline representation, computing device 202 may output a signal to computing device 214 indicating that the timeline representation has been viewed. In response to receiving the signal, the computing device 214 may eliminate or otherwise modify the output of the timeline representation at the computing device 214.
As one illustrative example, after computing device 202 displays a timeline representation of one or more structured sound recordings (e.g., indicating that the infant is crying), a first child caretaker may provide user input to computing device 202 indicating to view the timeline representation. Because the first caregiver has already viewed the timeline representation, the second caregiver (e.g., the user of computing device 214) may no longer be urgent in viewing the timeline representation. As such, in response to receiving the signal indicating that the timeline representation has been viewed, the computing device 214 may eliminate or otherwise modify the output of the timeline representation at the computing device 214.
The following numbered examples may illustrate one or more aspects of the disclosure:
example 1A. A method, comprising: receiving, by one or more processors of the computing device, audio data recorded by one or more microphones of the computing device; and generating, by the one or more processors and based on the audio data, one or more structured sound recordings, a first structured sound recording of the one or more structured sound recordings comprising: a description of the first sound, the description including a descriptive tag of the first sound, the descriptive tag being different from a text transcription of the first sound, and a timestamp indicating a time at which the first sound occurred; and outputting a graphical user interface comprising a timeline representation of the one or more structured sound recordings.
Example 2A the method of example 1A, wherein outputting the non-audio indication comprises outputting a graphical user interface comprising a timeline representation of the one or more structured sound recordings.
Example 3A the method of example 2A, wherein the timeline represents a sequence of sound occurrences indicating one or more structured sound recordings.
Example 4A the method of example 2A or example 3A, wherein the second structured-sound recording of the one or more sound recordings comprises: a description of the second sound, the description including a text transcription of the second sound; and a timestamp indicating the second sound occurrence time.
Example 5A the method of any of examples 2A-4A, wherein outputting the graphical user interface comprises outputting a first graphical user interface comprising a current timeline representation of one or more structured sound recordings, the method further comprising: a second graphical user interface is output, the second graphical user interface including past timeline representations of one or more structured sound recordings.
Example 6A the method of example 5A, wherein outputting the first graphical user interface comprises outputting the first graphical user interface for display at a first display, and wherein outputting the second graphical user interface comprises outputting the second graphical user interface for display at a second display, the second display being different from the first display.
Example 7A the method of any of examples 1A-6A, wherein outputting the non-audio indication comprises outputting the non-audio indication in response to determining that a newly generated sound recording of the one or more structured sound recordings has a descriptive tag included in the predetermined set of descriptive tags.
Example 8A the method of example 7A, wherein the predetermined set of descriptive tags includes an emergency category tag, a priority category tag, and other category tags.
Example 9A the method of example 8A, wherein the emergency class tag comprises one or more of a smoke alarm tag, a fire alarm tag, a carbon monoxide tag, a siren tag, and a shout tag.
Example 10A the method of example 8A or 9A, wherein the priority class labels include one or more of baby crying labels, doorbell labels, knock labels, animal warning labels, and glass breakage labels.
Example 11A the method of any of examples 8A-10A, wherein the other category labels include one or more of a running water label, a landline ringing label, and one or more appliance beeping labels.
Example 12A the method of any of examples 8A-11A, wherein outputting the non-audio indication comprises: based on the class of descriptive tags of the particular structured sound recording, an output modality for the non-audio indication is selected.
Example 13A the method of example 12A, wherein selecting the output modality comprises selecting one or more of a haptic output, a graphical output, and a light output as the non-audio indication of the one or more structured sound recordings.
Example 14A the method of any of examples 7A-13A, wherein outputting the non-audio indication of the newly generated sound recording includes outputting the non-audio indication at the computing device.
The method of any of examples 15A-13A, wherein the computing device is a first computing device, wherein outputting the non-audio indication of the newly generated sound recording comprises causing a second computing device to output the non-audio indication, and wherein the second computing device is different from the first computing device.
The method of any of examples 16A, 7A-13A, wherein the computing device is a first computing device, wherein outputting the non-audio indication of the newly generated sound recording comprises: outputting, at the first computing device, a non-audio indication; and causing the second computing device to output the non-audio indication, and wherein the second computing device is different from the first computing device.
Example 17A. The method of example 15A or example 16A, wherein the second computing device comprises a wearable computing device.
Example 18A the method of example 16A or example 17A, wherein outputting the non-audio indication of the newly generated sound recording further comprises: the third computing device is caused to output the non-audio indication, and wherein the third computing device is different from the second computing device and different from the first computing device.
Example 19A the method of any of examples 1A-18A, wherein generating a structured record of the one or more structured sound records comprises: descriptive labels of the structured sound recordings are determined using a machine learning model.
Example 20A the method of any of examples 1A-19A, wherein the one or more microphones of the mobile computing device comprise one or more of: one or more microphones included in the mobile computing device and/or one or more external microphones connected to the mobile computing device through a wired or wireless connection.
Example 21A the method of any of examples 1A-20A, wherein outputting the non-audio indication comprises outputting a tactile indication of a first structured sound recording of the one or more structured sound recordings.
Example 22A the method of example 21A, wherein outputting the tactile indication of the first structured sound recording comprises: selecting a first haptic mode of the plurality of haptic modes based on the descriptive label of the first structured sound recording; and, causing the haptic output device to output the first haptic pattern.
Example 23A the method of example 22A, further comprising: outputting a tactile indication of the second structured-sound recording by at least: selecting a second haptic mode of the plurality of haptic modes based on the descriptive label of the second structured sound recording, the second haptic mode being different from the first haptic mode; and, causing the haptic output device to output the second haptic pattern.
Example 24A the method of example 23A, wherein the descriptive label of the first structured sound recording is in an emergency category label, wherein the descriptive label of the second structured sound recording is not an emergency category label, and wherein an average amplitude of the first haptic mode is greater than an average amplitude of the second haptic mode.
Example 25A the method of example 21A, wherein outputting the tactile indication of the first structured sound recording comprises: outputting a first tactile indication representing a start of a first sound at a first time; and outputting a second haptic indication representing the end of the first sound at a second time subsequent to the first time.
Example 26A the method of any of examples 1A-25A, wherein outputting the non-audio indication of the one or more structured sound recordings comprises outputting a signal via one or more devices implanted in the brain of the user.
Example 27A the method of any of examples 1A-26A, wherein outputting the non-audio indication of the first structured-sound recording of the one or more structured-sound recordings comprises outputting a representation of an amplitude of the first sound.
Example 28A. A method, comprising: receiving, by one or more processors of the first computing device, audio data recorded by one or more microphones of the first computing device; generating, by the one or more processors and based on the audio data, one or more structured sound recordings, a first structured sound recording of the one or more structured sound recordings comprising: a description of the first sound, the description including a text transcription of the first sound; and a timestamp indicating a first sound occurrence time; outputting, at the first computing device, a non-audio indication of one or more structured sound recordings comprising a text transcription of the first sound; a representation of one or more structured sound recordings including text transcription of a first sound output by a first computing device and to be received by a second computing device used by a user in the same event as the user of the first computing device.
Example 29A the method of example 28A, wherein generating a structured record of the one or more structured sound records comprises: a text transcription of the first sound is determined using a machine learning model.
Example 30A the method of example 28A or 29A, wherein the one or more microphones of the mobile computing device comprise one or more of: one or more microphones included in the mobile computing device; and/or one or more external microphones connected to the mobile computing device through a wired or wireless connection.
Example 31A. A computing device, comprising: one or more microphones; and one or more processors configured to perform the methods of any combination of examples 1A-30A.
Example 32A. A computer-readable storage medium storing instructions that, when executed, cause one or more processors of a computing device to perform the method of any combination of examples 1A-30A.
The techniques described in this disclosure may be implemented, at least in part, in hardware, software, firmware, or any combination thereof. For example, aspects of the described techniques may be implemented within one or more processors, including one or more microprocessors, digital Signal Processors (DSPs), application Specific Integrated Circuits (ASICs), field Programmable Gate Arrays (FPGAs), or any other equivalent integrated or discrete logic circuitry, as well as any combinations of such components. The term "processor" or "processing circuit" may generally refer to any of the foregoing logic circuits, alone or in combination with other logic circuits or any other equivalent circuit. A control unit comprising hardware may also perform one or more of the techniques of this disclosure.
Such hardware, software, and firmware may be implemented within the same device or within separate devices to support the various techniques described in this disclosure. Furthermore, any of the described units, modules, or components may be implemented together or separately as discrete but interoperable logic devices. Depiction of different features as modules or units is intended to highlight different functional aspects and does not necessarily imply that such modules or units must be realized by separate hardware, firmware, or software components. Rather, functionality associated with one or more modules or units may be performed by separate hardware, firmware, or software components, or integrated within common or separate hardware, firmware, or software components.
The techniques described in this disclosure may also be embodied or encoded in an article of manufacture, including a computer-readable storage medium encoded with instructions. Instructions embedded or encoded in an article of manufacture comprising an encoded computer-readable storage medium may cause one or more programmable processors or other processors to implement one or more techniques described herein, such as when the instructions contained or encoded in the computer-readable storage medium are executed by the one or more processors. The computer-readable storage medium may include Random Access Memory (RAM), read-only memory (ROM), programmable read-only memory (PROM), erasable programmable read-only memory (EPROM), electrically erasable programmable read-only memory (EEPROM), flash memory, a hard disk, a compact disc read-only memory (CD-ROM), a floppy disk, a cartridge, magnetic media, optical media, or other computer-readable media. In some examples, an article of manufacture may comprise one or more computer-readable storage media.
In some examples, the computer-readable storage medium may include a non-transitory medium. The term "non-transitory" may indicate that the storage medium is not embodied in a carrier wave or propagated signal. In some examples, a non-transitory storage medium may store data (e.g., in RAM or cache) that may change over time.
Various examples have been described. These and other examples are within the scope of the following claims.
Claims (15)
1. A method, comprising:
receiving, by one or more processors of a computing device, audio data recorded by one or more microphones of the computing device; and
generating, by the one or more processors and based on the audio data, one or more structured sound recordings, a first structured sound recording of the one or more structured sound recordings comprising:
a description of a first sound, the description including a descriptive label of the first sound, the descriptive label being different from a textual transcription of the first sound, an
A timestamp indicating a time at which the first sound occurred; and
a graphical user interface is output that includes a timeline representation of the one or more structured sound recordings.
2. The method of claim 1, wherein the timeline representation indicates an order of sound occurrences of the one or more structured sound recordings.
3. The method of claim 1 or claim 2, wherein a second structured-sound recording of the one or more sound recordings comprises:
a description of a second sound, the description including a text transcription of the second sound, and
a timestamp indicating a time at which the second sound occurred.
4. The method of any of claims 1-3, wherein outputting the graphical user interface comprises: outputting a first graphical user interface comprising a current timeline representation of the one or more structured sound recordings, the method further comprising:
a second graphical user interface is output that includes past timeline representations of the one or more structured sound recordings.
5. The method of claim 4, wherein outputting the first graphical user interface comprises: outputting the first graphical user interface for display on a first display, and wherein outputting the second graphical user interface comprises: the second graphical user interface is output for display on a second display different from the first display.
6. The method of any of claims 1-5, wherein outputting the graphical user interface comprises: the graphical user interface is output in response to determining that a newly generated sound recording of the one or more structured sound recordings has a descriptive label contained in a predetermined set of descriptive labels.
7. The method of claim 6, wherein the predetermined set of descriptive tags includes an emergency category tag, a priority category tag, and other category tags.
8. The method of claim 7, wherein:
the emergency class tags include one or more of a smoke alarm tag, a fire alarm tag, a carbon monoxide tag, a siren tag, and a shouting tag;
the priority class labels comprise one or more of baby crying labels, doorbell labels, knock labels, animal warning labels and glass breaking labels; and
the other category labels include one or more of a running water label, a fixed telephone ring label, and one or more appliance beeping labels.
9. The method of any of claims 1-8, wherein outputting the graphical user interface including the timeline representation comprises: the graphical user interface including the timeline representation is output at the computing device.
10. The method of any of claims 1-9, wherein the computing device is a first computing device, wherein outputting the graphical user interface including the timeline representation comprises: causing a second computing device to output the graphical user interface including the timeline representation, and wherein the second computing device is different from the first computing device.
11. The method of claim 10, further comprising:
responsive to receiving a user input at the computing device indicating that the timeline representation was viewed at a particular time, an output of the timeline representation at the second computing device is modified to indicate a previous viewing.
12. The method of claim 10 or claim 11, wherein the second computing device comprises a wearable computing device.
13. The method of any of claims 10-12, wherein the computing device does not include a display.
14. A computing device, comprising:
one or more microphones; and
one or more processors configured to perform the method of any combination of claims 1-13.
15. A computer-readable storage medium storing instructions that, when executed, cause one or more processors of a computing device to perform the method of any combination of claims 1-13.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202063088811P | 2020-10-07 | 2020-10-07 | |
US63/088,811 | 2020-10-07 | ||
PCT/US2021/048496 WO2022076108A1 (en) | 2020-10-07 | 2021-08-31 | Enhanced computing device representation of audio |
Publications (1)
Publication Number | Publication Date |
---|---|
CN116210051A true CN116210051A (en) | 2023-06-02 |
Family
ID=77897787
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180066234.4A Pending CN116210051A (en) | 2020-10-07 | 2021-08-31 | Enhanced computing device audio representation |
Country Status (7)
Country | Link |
---|---|
US (1) | US20230342108A1 (en) |
EP (1) | EP4200844A1 (en) |
JP (1) | JP2023546013A (en) |
KR (1) | KR20230084138A (en) |
CN (1) | CN116210051A (en) |
DE (1) | DE112021005304T5 (en) |
WO (1) | WO2022076108A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
IT202200008735A1 (en) * | 2022-05-02 | 2023-11-02 | Betadynamiq S R L | Wearable acoustic device and sound identification process |
Family Cites Families (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8682654B2 (en) * | 2006-04-25 | 2014-03-25 | Cyberlink Corp. | Systems and methods for classifying sports video |
US10198697B2 (en) * | 2014-02-06 | 2019-02-05 | Otosense Inc. | Employing user input to facilitate inferential sound recognition based on patterns of sound primitives |
US9668073B2 (en) * | 2015-10-07 | 2017-05-30 | Robert Bosch Gmbh | System and method for audio scene understanding of physical object sound sources |
US11145171B2 (en) * | 2019-02-28 | 2021-10-12 | Arlo Technologies, Inc. | Electronic doorbell system with text communication |
CN111048114A (en) * | 2019-12-30 | 2020-04-21 | 深圳江行联加智能科技有限公司 | Equipment and method for detecting abnormal sound of equipment |
-
2021
- 2021-08-31 DE DE112021005304.3T patent/DE112021005304T5/en active Pending
- 2021-08-31 JP JP2023521411A patent/JP2023546013A/en active Pending
- 2021-08-31 KR KR1020237010080A patent/KR20230084138A/en unknown
- 2021-08-31 CN CN202180066234.4A patent/CN116210051A/en active Pending
- 2021-08-31 EP EP21773993.7A patent/EP4200844A1/en active Pending
- 2021-08-31 WO PCT/US2021/048496 patent/WO2022076108A1/en active Application Filing
- 2021-08-31 US US18/044,831 patent/US20230342108A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
KR20230084138A (en) | 2023-06-12 |
US20230342108A1 (en) | 2023-10-26 |
WO2022076108A1 (en) | 2022-04-14 |
DE112021005304T5 (en) | 2023-08-31 |
EP4200844A1 (en) | 2023-06-28 |
JP2023546013A (en) | 2023-11-01 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
Sarker | Machine learning: Algorithms, real-world applications and research directions | |
US11847858B2 (en) | Vehicle occupant engagement using three-dimensional eye gaze vectors | |
Hoai et al. | Max-margin early event detectors | |
US11544591B2 (en) | Framework for a computing system that alters user behavior | |
CN112955893A (en) | Automatic hyperlink of document | |
US20210174020A1 (en) | Recipient based text prediction for electronic messaging | |
JP7461485B2 (en) | Verifying image authenticity using decoding neural networks | |
US20220249906A1 (en) | On-device activity recognition | |
Khan et al. | Anomalous behavior detection framework using HTM-based semantic folding technique | |
US20230342108A1 (en) | Enhanced computing device representation of audio | |
Gu et al. | A negative selection algorithm with hypercube interface detectors for anomaly detection | |
Felker | System Volume Compensating for Environmental Noise | |
US20240152440A1 (en) | Game performance prediction across a device ecosystem | |
Ali | Anomalous behaviour detection using heterogeneous data | |
Price et al. | Machine Learning to Disable Applications from Using Background Resources Except at Appropriate Times | |
Price et al. | Configuring Alarm Setting Using Machine Learning | |
Price | Identifying Hold State in an Automated Calling System | |
Price | Classifying a User as Driver or Passenger in a Vehicle Using Machine Learning | |
Felker | Using Music to Affect Mood Based on Sentiment Analysis | |
Price et al. | Machine Learning to Automatically Lock Device Screen at Opportune Time | |
Cardinal et al. | Identifying Impaired State for a Driver | |
Price | Machine Learning to Identify Vehicle Maintenance Needs | |
WO2023224672A1 (en) | Deep learning system for navigating feedback | |
Felker | Planning Group Meals Based on Preferences of Attendees | |
Felker | Configuring Alarm System Based on Time to Arrive at Appointment |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |