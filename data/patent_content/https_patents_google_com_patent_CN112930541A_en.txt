CN112930541A - Determining a control strategy by minimizing delusional effects - Google Patents
Determining a control strategy by minimizing delusional effects Download PDFInfo
- Publication number
- CN112930541A CN112930541A CN201980070743.7A CN201980070743A CN112930541A CN 112930541 A CN112930541 A CN 112930541A CN 201980070743 A CN201980070743 A CN 201980070743A CN 112930541 A CN112930541 A CN 112930541A
- Authority
- CN
- China
- Prior art keywords
- policy
- action
- agent
- current
- value
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000011217 control strategy Methods 0.000 title claims abstract description 58
- 230000000694 effects Effects 0.000 title description 4
- 230000009471 action Effects 0.000 claims abstract description 158
- 238000000034 method Methods 0.000 claims abstract description 56
- 238000003860 storage Methods 0.000 claims abstract description 10
- 238000013138 pruning Methods 0.000 claims abstract description 4
- 238000012549 training Methods 0.000 claims description 31
- 230000004044 response Effects 0.000 claims description 22
- 238000013528 artificial neural network Methods 0.000 claims description 10
- 230000007704 transition Effects 0.000 claims description 7
- 238000012886 linear function Methods 0.000 claims description 6
- 230000009466 transformation Effects 0.000 claims description 3
- 238000012544 monitoring process Methods 0.000 claims 1
- 238000004590 computer program Methods 0.000 abstract description 13
- 239000003795 chemical substances by application Substances 0.000 description 68
- 230000008569 process Effects 0.000 description 30
- 230000006870 function Effects 0.000 description 29
- 230000002787 reinforcement Effects 0.000 description 16
- 238000012545 processing Methods 0.000 description 15
- 241000196324 Embryophyta Species 0.000 description 11
- 244000141353 Prunus domestica Species 0.000 description 6
- 238000010586 diagram Methods 0.000 description 6
- 230000003993 interaction Effects 0.000 description 6
- 102000004169 proteins and genes Human genes 0.000 description 6
- 108090000623 proteins and genes Proteins 0.000 description 6
- 238000004088 simulation Methods 0.000 description 6
- 238000004891 communication Methods 0.000 description 5
- 238000010801 machine learning Methods 0.000 description 5
- 230000015654 memory Effects 0.000 description 5
- 239000000126 substance Substances 0.000 description 5
- 230000001133 acceleration Effects 0.000 description 4
- 230000007613 environmental effect Effects 0.000 description 4
- 239000000543 intermediate Substances 0.000 description 3
- 239000002243 precursor Substances 0.000 description 3
- 230000008901 benefit Effects 0.000 description 2
- 230000001186 cumulative effect Effects 0.000 description 2
- 238000009826 distribution Methods 0.000 description 2
- 238000011156 evaluation Methods 0.000 description 2
- 230000006872 improvement Effects 0.000 description 2
- 238000004519 manufacturing process Methods 0.000 description 2
- 238000013507 mapping Methods 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 238000005192 partition Methods 0.000 description 2
- 238000010248 power generation Methods 0.000 description 2
- 230000012846 protein folding Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 238000003786 synthesis reaction Methods 0.000 description 2
- 206010012239 Delusion Diseases 0.000 description 1
- 241000009334 Singa Species 0.000 description 1
- 230000006399 behavior Effects 0.000 description 1
- 230000008827 biological function Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000001149 cognitive effect Effects 0.000 description 1
- 230000001010 compromised effect Effects 0.000 description 1
- 238000007796 conventional method Methods 0.000 description 1
- 231100000868 delusion Toxicity 0.000 description 1
- ZZUFCTLCJUWOSV-UHFFFAOYSA-N furosemide Chemical compound C1=C(Cl)C(S(=O)(=O)N)=CC(C(O)=O)=C1NCC1=CC=CO1 ZZUFCTLCJUWOSV-UHFFFAOYSA-N 0.000 description 1
- 230000005484 gravity Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000037361 pathway Effects 0.000 description 1
- 230000000379 polymerizing effect Effects 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 230000000087 stabilizing effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
- 239000002699 waste material Substances 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/004—Artificial life, i.e. computing arrangements simulating life
- G06N3/006—Artificial life, i.e. computing arrangements simulating life based on simulated virtual individual or collective life forms, e.g. social simulations or particle swarm optimisation [PSO]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
Abstract
The present invention relates to methods, systems, and apparatus, including computer programs encoded on computer storage media, for determining a control policy for an agent interacting with an environment. One of the methods includes updating a control strategy using a strategy-consistent backup using Q-learning. To determine a policy-consistent backup, the system determines a policy-consistent backup for a control policy at a current view-current action pair, comprising: identifying, for each action of a plurality of actions in a set of possible actions that the agent is capable of performing, a Q value assigned by the control policy to a next observation-action pair of the control policy and evidenced by at least one of the sets of information; pruning from the identified Q values any Q values that are only evidenced by a set of information that is not consistent with the policy class; and determining the policy-consistent backup from the reward and the identified Q value that is only untrimmed.
Description
Cross Reference to Related Applications
This application claims priority to U.S. application No. 62/752,306, filed on 2018, 29/10, the entire content of which is incorporated herein by reference.
Technical Field
This description relates to reinforcement learning.
Background
In a reinforcement learning system, an agent interacts with an environment by performing an action selected by the reinforcement learning system in response to receiving observations characterizing a current state of the environment.
Some reinforcement learning systems select an action to be performed by an agent based on the output of a neural network in response to receiving a given observation.
A neural network is a machine learning model that employs one or more layers of non-linear elements to predict output for received inputs. Some neural networks are deep neural networks that include one or more hidden layers in addition to an output layer. The output of each hidden layer is used as an input to the next layer in the network, i.e. the next hidden layer or output layer. Each layer of the network generates an output from the received input in accordance with the current values of the respective set of parameters.
Disclosure of Invention
The present specification generally describes an reinforcement learning system that controls agents interacting with an environment, and in particular determines a control strategy for controlling the agents.
Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages.
Conventional systems that use Q-learning to learn a control strategy for an agent may update the strategy by using backup value estimates derived from unrealizable action choices in the base strategy class. That is, in conventional Q learning, a backup of a state-action pair is generated by independently selecting an action at the corresponding next state using the max operator, i.e., by generating a target Q value using the Q value for the argmax action when the argmax action is selected at the next state. This assumes that an independently selected maximum is feasible, i.e., the action chosen to yield the maximum Q value at the next state is consistent with the selection of other actions taken to reach the next state. In other words, this assumes that there is a control strategy that will both select the action that yields the maximum Q value in the next state and make other action choices to take to reach the next state.
This may often be the case when this assumption is violated, which can cause problems in the control strategy learning process. In particular, violations of these assumptions (as occur in systems implementing conventional Q-learning variant patterns) may lead to a divergence in the learning process, may lead to the control strategy to be learned not performing well on the task, or may lead to the learning process running an excessive number of iterations. On the other hand, the described techniques avoid or minimize violations of this assumption, thereby improving the control strategy learned for the agent, which in turn improves the performance of the agent on the desired task. Additionally, the control strategy can be learned with fewer iterations to reduce the computational resources consumed by the learning process.
In particular, the described techniques determine updates (and backups) to control policies by using techniques that avoid explicitly violating this assumption, e.g., by maintaining information sets and updating them with only policy-consistent Q values, or by employing heuristics that reduce the likelihood of violating the assumption, e.g., by selecting the next action that is locally consistent within a collection of training tuples.
The described techniques, when used in conjunction with real-world environments and agents such as mechanical agents/robots or plants/service facilities, can result in improvements to the control strategies learned for controlling the agents, e.g., energy efficiency, accuracy, speed, and/or output improvements to performing tasks by using the learned control strategies.
The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1A illustrates an example reinforcement learning system.
FIG. 1B illustrates an example environment susceptible to "delusional bias".
FIG. 2 is a flow diagram of an example process for learning a control strategy using Q-learning and a strategy-consistent backup.
FIG. 3 is a flow diagram of an example process for learning a control strategy using value iteration and strategy-consistent backups.
FIG. 4 is a flow diagram of an example process for learning a control strategy using locally consistent backups.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
This specification describes a reinforcement learning system that controls an agent interacting with an environment by, at each of a plurality of time steps, processing data (i.e., "observation") characterizing a current state of the environment at that time step to select an action to be performed by the agent.
At each time step, the environmental state at that time step depends on the environmental state at the previous time step and the action performed by the agent at the previous time step.
In some embodiments, the environment is a real-world environment and the agent is a mechanical agent that interacts with the real-world environment, e.g., a robot or autonomous or semi-autonomous land, air, or marine vehicle that navigates within the environment.
In these embodiments, the observation may, for example, include one or more of: images, object position data, and sensor data used to capture observations when the agent interacts with the environment, such as sensor data from image, distance or position sensors or from actuators.
For example, in the case of a robot, the observations may include data characterizing the current state of the robot, such as one or more of: joint position, joint velocity, joint force, moment or acceleration (e.g., gravity compensated moment feedback), and global or relative pose of an item held by the robot.
In the case of a robot or other mechanical agent or vehicle, the observation may similarly include one or more of: position, linear or angular velocity, force, moment or acceleration, and global or relative pose of one or more portions of the agent. The observation may be defined as 1-dimensional, 2-dimensional or 3-dimensional and may be an absolute observation and/or a relative observation.
The observation may also for example include: sensing an electronic signal, such as a motor current or temperature signal; and/or image or video data, e.g., from a camera or LIDAR sensor, such as data from a sensor of an agent, or data from a sensor located separately from an agent in the environment.
In these embodiments, the action may be a control input for controlling a mechanical agent/robot, such as a torque or higher-level control command for a robot joint, or a control input for controlling an autonomous or semi-autonomous land, air, marine vehicle, such as a torque to a control surface or other control element of the vehicle, or a higher-level control command.
In other words, the action can for example comprise position, velocity or force/moment/acceleration data of one or more joints of the robot or a part of another mechanical agent. The motion data may additionally or alternatively comprise electronic control data, such as motor control data, or more generally data for controlling one or more electronic devices in the environment, which may have an effect on the observed environmental state. For example, in the case of an autonomous or semi-autonomous land or air or marine vehicle, the actions may include actions that control navigation (e.g., steering) and motion (e.g., braking and/or acceleration) of the vehicle.
In the case of an electronic agent, the observations may include data from one or more sensors (e.g., current, voltage, power, temperature, and other sensors) used to monitor a portion of the plant or service facility and/or electronic signals used to represent the electronic and/or mechanical item functions of the device. For example, the real-world environment may be a manufacturing plant or a service facility, the observations may relate to the operation of the plant or facility, e.g., to resource usage, such as power consumption, and the agents may control actions or operations in the plant/facility, e.g., to reduce resource usage. In some other embodiments, the real-world environment may be a renewable energy plant, the observations may relate to the operation of the plant, for example, to maximizing current or future projected power generation, and the agents may control actions or operations in the plant to achieve this goal.
In some other applications, the agent may control actions in a real-world environment (including an item of equipment), such as in a data center, in a power/hydro distribution system, or in a manufacturing plant or service facility. The observations may then relate to the operation of the plant or facility. For example, the observations may include observations of power or hydraulics used by the device, or observations of power generation or distribution control, or observations of usage resources or waste generation. The actions may include actions for controlling or imposing operating conditions on equipment items of the plant/service facility, and/or actions that result in changing settings in the operation of the plant/service facility, for example, to adjust or turn on/off components of the plant/facility.
As another example, the environment may be a chemical synthesis or protein folding environment, such that each state is a respective state of a protein chain or one or more intermediate or precursor chemicals, and the agent is a computer system for determining how to fold the protein chain or synthetic chemical. In this example, the action is a possible folding action for folding a protein chain or an action for polymerizing precursor chemicals/intermediates, and the result to be achieved may for example include folding the protein, stabilizing the protein and achieving a specific biological function, or providing an efficient synthetic pathway for chemicals. As another example, the agent may be a mechanical agent that automatically performs or controls, without human intervention, a protein folding action or a chemical synthesis step selected by the system. The observation may comprise observing the state of the protein or chemical/intermediate/precursor directly or indirectly and/or may be derived from a simulation.
In some implementations, the environment may be a simulated environment and the agent may be implemented as one or more computers that interact with the simulated environment.
The simulated environment may be a motion simulation environment, such as a driving simulation or a flight simulation, and the agent may be a simulated vehicle that navigates through the motion simulation. In these embodiments, the action may be a control input to control a simulated user or a simulated vehicle.
In some implementations, the simulated environment may be a simulation of a particular real-world environment. For example, the system may be used to select actions in a simulated environment during training or evaluation of a control neural network, and after completion of the training or evaluation, or both, the system may be deployed to control real-world agents in a real-world environment simulated by the simulated environment. This can avoid unnecessary wear and damage to the real-world environment or real-world agents, and can allow the neural network to be controlled to train and evaluate rare or difficult to reproduce situations in the real-world.
In general, in the case of a simulated environment, the observations can include simulated versions of one or more previously described observations or observation types, and the actions can include simulated versions of one or more previously described actions or action types.
Fig. 1A illustrates an example reinforcement learning system 100. The reinforcement learning system 100 is an example of a system implemented as a computer program on one or more computers in one or more locations in which the systems, components, and techniques described below are implemented.
The system 100 controls the interaction of the agent 102 with the environment 104 by selecting an action 106 to be performed by the agent 102 and then causing the agent 102 to perform the selected action 106.
The agent 102 performing the selected action 106 substantially transitions the environment 104 to a new state. By repeatedly having the agent 102 act within the environment 104, the system 100 is able to control the agent 102 to accomplish specified tasks.
The system 100 includes a control strategy 110, a training engine 150, and one or more memories for storing a set of control strategy parameters 118 for the control strategy 110.
At each of a plurality of time steps, the control strategy 110 maps a current observation 120 characterizing the current state of the environment 104 to the action 106 in accordance with the control strategy parameters 118 to generate an action selection output 122.
In particular, the control policy 110 generates a respective Q value for each action in the set of actions and then selects one of the actions. The Q value of an action is an estimate of the "reward" that will result from the agent performing the action in response to the current observations 120 and thereafter selecting a future action to be performed by the agent 102 in accordance with a greedy control strategy that selects the action with the highest Q value in response to each observation, i.e., the Q value is generated in accordance with the control strategy parameters 118.
The reward refers to a cumulative measure of the "reward" 124 received by the agent, such as a reward total or a time discount sum of the reward. An agent can receive a respective reward 124 at each time step, where the reward 124 is specified by a scalar value and characterizes, for example, the agent's progress toward completing a specified task.
For example, during training, the control strategy 110 can select an action to be performed by an agent according to an exploration strategy. For example, the exploration policy may be a ∈ -greedy exploration policy, where system 100 selects the action with the highest Q value with a probability of 1 ∈ and randomly selects the action with a probability of ∈. In this example, e is a scalar value between 0 and 1.
After training, control strategy 110 can employ a greedy action selection strategy, i.e., by always selecting the action with the highest Q value.
The system 100 then controls the agent, i.e., by having the agent perform the action 106 selected by the control policy 110.
Control strategy 110 can map the observations to a Q value by using any of a variety of techniques.
As one example, the control strategy 110 can maintain a tabular representation of the Q function that maps (observation-action) pairs to Q values. In this example, during training, the system 100 directly updates the Q value in the tabular representation, i.e., the control strategy parameter is the Q value in the tabular representation.
As another example, the control strategy 110 can maintain a function approximator (approximator) for approximating the Q function. When the observation comprises high dimensional data, such as an image of the environment, the function approximator can be, for example, a neural network (also known as a Q neural network). Alternatively, the function approximator can be a linear model or a generalized linear model. In these cases, during training, the system 100 learns the parameter values of the function approximator, i.e., the control strategy parameters are parameters of the function approximator.
The training engine 150 is configured to train the control strategy 110 by repeatedly updating the control strategy parameters 118 (i.e., the parameters of the function approximator or the Q values in the tabular representation) of the control strategy 110.
In particular, the training engine 150 trains the control strategy 110 by using reinforcement learning that uses the observations 120 and rewards 124 generated as a result of agent interaction with the environment during training.
In general, the training engine 150 can train the control strategy 110 to increase the reward (i.e., the cumulative measure of reward) received by an agent by using Q updates (e.g., by using Q learning or value iteration).
In other words, the training engine 150 repeatedly updates the control strategy parameters 118 using the observations 120, the actions performed in response to the observations, and the rewards 124.
The training engine 150 updates the control strategy parameters 118 in a manner that eliminates or mitigates the effects of "delusional bias" on learning the control strategy parameters 118. In particular, a "delusion bias" occurs whenever a backup value estimate is derived from an action selection that cannot be implemented in the underlying policy class. In other words, if any policy in the allowable class cannot jointly express all past (implicit) action choices, the backup value does not correspond to the Q value that can be achieved by any expressible policy.
Fig. 1B illustrates an environment 170 susceptible to "paranoid bias".
In the environment of FIG. 1B, the scenario (i.e., the series of interactions during which an agent attempts to perform a specified task) is in state s1Starting, and there are two actions: a is1Terminate the episode except at s1Wherein at s1If a is executed1The environment can then enter state s with probability q4. Another action a2Moving the environment to the sequence s with certainty1To s4And when in s4Execution of a2When this happens, episode termination occurs. The reward for performing an action is 0, but when at s1Execution of a1Then there is a non-zero positive reward R(s)1,a1) And when in s4Execution of a2Then there is another non-zero positive reward R(s)4,a2)。
Specifically, q is 0.1, R(s)1,a1) 0.3, and R(s)4,a2)＝2。
At this time, consider a linear function approximator f for operating on the characteristics of a state-action pairθThat is, the characteristic value of the state is observed, and the input of the approximator is the characteristic value of the state and the characteristic value of the action, and the Q value is generated for the state action pair. Specially for treating diabetesIn other words, the linear function approximator operates on a two-dimensional feature vector, where the first dimension is characteristic of a state and the second dimension is characteristic of an action, i.e., state-action characteristics
In view of this, a linear function approximator cannot (i) both
Therefore, any greedy action selection strategy π cannot satisfy π(s) at the same time2)＝a2And pi(s)3)＝a2I.e. no strategy can make the system in state s2Hour selection action a2And in a state s3Hour selection action a2. In other words, any in state s is due to the greedy policy selecting the action with the highest Q value2The timing will select action a2Will be in state s3Hour selection action a1And is in any state s3The timing will select action a2Will be in state s2The timing will select action a1. Thus, the optimal unconstrained strategy (in any case, take a)2The expected value is 2) cannot be realized. Thus, the Q update can never converge to an optimal unconstrained strategy. Instead, the optimal achievable strategy will be at s1Get a when1And at s4Get a when2Thereby obtaining a value of 0.5.
In this example, conventional Q updates cannot find the optimal allowable policy pi. For example, online Q-learning of data generated using an epsilon greedy behavior policy (epsilon 0.5) converges to a fixed point that gives a "compromised" tolerable policy, at s1And s4All take a1(value 0.3).
This example shows how delusional bias prevents Q learning from reaching a reasonable fixed point.
For example, consider(s)2,a2) And(s)3,a2) To backup. Assume that the current function approximator is(s)3,a2) Assign a "high" value (i.e., so as to be for(s)3,a2) Is greater than for(s)3,a1) Q value of) as required by the optimal control strategy.
Intuitively, this requires θ1Less than 0, and for(s)2,a2) A "high" bootstrap value is generated. But for theta used to try to fit the value1And theta2Any update of (i.e., make(s)2,a2) Has a Q value of greater than(s)2,a1) Q value of) Will force theta1Greater than 0, which is consistent with the assumption (i.e., θ) required to generate a high bootstrap value1<0) And are inconsistent. In other words, will be directed to(s)2,a2) Any update that moves Q higher weakens the proof that it is higher.
The result is that the Q updates compete with each other,(s)2,a2) Converges to a compromise value that cannot be achieved by any possible strategy. This is because the backup generated by conventional Q learning is independent of previous actions and does not consider whether the actions required to generate the backup are consistent with the previous actions.
By modifying the manner in which backups are generated (e.g., for Q-learning or value iteration), the described system is able to achieve better performance than conventional update techniques, i.e., the return compiled by the generated control strategy is closer to the optimal return than by control strategies generated using conventional techniques, and thus results in the agent performing better on the specified task.
In some cases, the system maintains information sets and uses these information sets in determining backups for Q learning. That is, the system updates the control strategy by using model-free Q-learning. This process will be described in detail below with reference to fig. 2.
In other cases, the system maintains a transformation model that models the dynamics of the environment. The transition model maps the observation-action pairs to probabilities for each of a plurality of next states. The probability of the next state represents the probability that the next state will be the state to which the environment transitioned when an agent performed an action in the pair in response to an observation in the pair. The system can use the transformation model and the maintained set of information to determine backups using value iterations. This process will be described in detail below with reference to fig. 3.
In other cases, the system does not maintain multiple sets of information, but rather ensures that backups are computed by using locally consistent Q values when performing a batch Q-learning. This process will be described in detail below with reference to fig. 4.
FIG. 2 is a flow diagram of an example process 200 for learning a control strategy by using Q to learn a backup that is consistent with the strategy. For convenience, process 200 will be described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed reinforcement learning system, such as reinforcement learning system 100 of fig. 1, can perform process 200.
The system can repeatedly perform the process 200 to update the control strategy, i.e., repeatedly update the control strategy parameters.
The system maintains data defining one or more information sets (step 202). Each information set corresponds to a respective set of policy constraints and identifies a Q value assigned by the control policy to an observation-action pair under the set of policy constraints.
Where the policy uses a table representation, the system can maintain an additional table of Q values for each information set.
Where the policy uses function approximators, the system can maintain additional function approximators for each information set (or equivalently, separate sets of parameters or weights for the same function approximator for each information set).
Each set of policy constraints generally corresponds to some set of action-observation pairs, and specifies that the policy must be able to select an action in the pair in response to an observation in the pair for all corresponding action-observation pairs. In other words, each set of policy constraints specifies one or more paths through the environment that must be able to be implemented by the policy. As a specific example, in the example of FIG. 1B, one set of information may constrain the policy to be able to be in state s1Time selection action a1And in the state s4Time selection action a2Another set of information may constrain the policy to be able to be in state s1Time selection action a1And in the state s4Time selection action a1And yet another set of information may constrain the policy to be able to be in state s1Time selection action a2And in the state s2Time selection action a2Those policies of (1).
However, in some cases, the pairs corresponding to any given set of policy constraints can also be based on constraints on the operation of the agent that affect which actions can be selected in a given environmental state, such as security constraints or other operational constraints.
In particular, let Θ be the parameter class used to define the Q function. Information set
In general, an information set can be considered as a finite partition of Θ, i.e., a set of non-empty subsets P ≠ X1.., Xk }, such that for all i ≠ j, X1 · · · · Xk ═ Θ, and
additionally, if X ∈ P ' exists for all X ' ∈ P ', such that X ∈ P exists such that
The system receives a current observation characterizing a current state of the environment, a current action performed by the agent in response to the current observation, a next observation characterizing a next state of the environment, and a reward received as a result of the agent performing the current action (step 204).
In other words, the system receives training tuples for determining updates to the control strategy. In some cases, for example, when an update is being calculated per policy, an action is selected based on the current values of the policy parameters. In other cases, for example, when an update is being computed off of a policy, the training tuples are sampled from replay memory and actions may have been selected by using different old values of the policy parameters.
The system determines a policy-consistent Q-backup at the current observation s-current action a pair for the control policy (step 206).
In particular, for each of a plurality of actions in a set of possible actions that the agent is capable of performing, the system identifies a Q value assigned to the next observation-action pair by the control policy and evidenced by at least one of the sets of information. If the control strategy operates under the constraints imposed by the information set, the Q value is justified by the information set when the control strategy has assigned the Q value to the next observation-action pair.
The system then prunes (prune) any Q values from the identified Q values that are only evidenced by a set of information that is not consistent with the policy class. When a greedy policy operating under constraints imposed by a set of information does not select a current action in response to a current observation, the set of information is not policy-class consistent. Thus, any Q values that are only justified by the set of information that does not result in the current action being performed in response to the current observation are pruned from the set of identified Q values.
The system then determines a policy-consistent backup from the reward and the identified Q value that is only untrimmed. In particular, the policy-consistent backup includes a backup, i.e., a target output, for each set of information that proves an untrimmed Q value. In particular, for a given one of these sets of information, the update can be based on the reward in the tuple and the identified Q value evidenced by that information, e.g., the sum of (i) and (ii): (i) a reward, (ii) a product of the discount coefficient and the identified Q value.
The system updates the control policy for the agent by using Q learning by using a policy-consistent backup (step 208). In particular, the system is able to update each information set that is used to prove an untrimmed Q value by using updates to the information set.
When the system uses a tabular representation of the Q function, the system can update the Q value for each information set by calculating a weighted sum of the current Q value and the backup for that information set.
When the system approximates the Q function using a function approximator, the system can calculate an update to the function approximator corresponding to the information set by calculating a supervised learning update based on an error gradient (e.g., mean square error) between the current Q value and the backup for the information set.
Thus, as can be seen from the description of FIG. 2, the system does not independently identify the argmax action when computing a backup (as is done in conventional Q-updates), but rather maintains the information sets and updates each information set only by using a Q value that is consistent with the information set.
The total number of information sets may grow too large in a complex environment as the number of information sets increases with the agent exploration environment due to policy constraints imposed on the information sets. In some implementations, the system can prune the information sets during training, merge the information sets during training, or both when certain criteria are met. For example, when the number of information sets exceeds a threshold number, the system can prune the information set with the lowest Q value or can combine the information sets with the lowest Q value.
Once training is complete, the system can select the policy parameters represented by one of the information sets as the final set of policy parameters. In particular, the system can select the policy parameter that results in assigning the highest Q value to the action selected by the control policy in the initial state of any given episode of control agents as the policy parameter to be employed to control the agent in that episode.
Table 1 below illustrates pseudo code for learning a control strategy through process 200 when the system uses a tabular representation of the Q function. In particular, in table 1, the symbols are as follows:
- (s, a, r, s') are training tuples,
q [ sa ] is a table such that Q [ sa ] (X) is the Q value that takes action a at s and then follows a greedy policy parameterized by θ ∈ X, where the parameter θ represents the constraint imposed by the information set X,
-ConQ[sa]is a table such that ConQ [ sa](X) is the Q value of taking action a at s and then following a greedy policy parameterized by θ ∈ X, with an additional constraint of πθ(s) ═ a. That is, the greedy strategy must select action a at state s when parameterized by parameter θ, such that the Q value at state s, which takes action a, appears in table ConQ [ sa ]]In (1).
ConQ [ s ] is a table formed by the concatenation of all tables ConQ [ sa ] taking action a at state s,
s → a is the set of policy parameters, so that the greedy policy selects action a at state s,
≦ is the sum of the intersections and is defined such that h ≦ h1⊕h2Is defined by the formula: h (X)1∩X2)＝h1(X1)+h2(X2),
dom () is the domain of the function.
Table 1
FIG. 3 is a flow diagram of an example process 300 for learning a control strategy by iterating a backup consistent with the strategy using values. For convenience, process 300 will be described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed reinforcement learning system, such as reinforcement learning system 100 of fig. 1, can perform process 300.
As described above, the system maintains one or more sets of information (step 302).
The system obtains a current observation, and an action to perform in response to the observation, and a reward (step 304).
The system determines a respective probability for each of a plurality of next states by using the transition model (step 306).
In particular, the system uses a transition model that dynamically models the environment to predict a respective probability for each of a plurality of next states, the probability representing the likelihood that the next state is the state to which the environment transitioned as a result of the agent performing the action in response to the current observation.
The system determines a policy-consistent Bellman backup at the current observation s-current action a pair for the control policy (step 308).
Generally, the system performs the following operations independently for each next state assigned a non-zero probability by the transition model to generate a set of Q values for the next state, and then combines the results to generate a policy-consistent Bellman backup of the control policy.
In particular, for each action of a plurality of actions in a set of possible actions that the agent is capable of performing, and for any given next state, the system identifies a Q value assigned by the control policy to the next observation-action pair and evidenced by at least one of the sets of information (where "next observation" is one used to characterize the given next state).
The system then generates a set of Q values for the next state by pruning from the identified Q values any Q values that are only evidenced by a set of information that is not consistent with the policy class. Thus, any Q value that is only justified by the set of information that does not result in the current action being performed in response to the current observation is pruned from the set of identified Q values for the next state.
The policy-consistent backup includes a backup, i.e., a target output, of each information set that proves a Q value that is not clipped for any next state. In particular, for a given one of these information sets, the update can be based on the reward in the tuple and for each next state of the Q-values not clipped being justified for the given information set, the update being based on the Q-values justified by the information set and the probabilities assigned to the next states. For example, to compute a backup of a given set of information, the system can compute, for each next state, the product of the probability assigned to the next state and the Q value evidenced by the information set for the next state, and then sum these products to determine the initial update. The system can then calculate a backup of the information set as the sum of (i) and (ii): (i) reward, (ii) product of discount factor and initial backup of next state.
The system updates the control policy of the agent by using Q learning by using a policy-consistent backup (step 310). In particular, the system can use the updates of the information sets to update each information set that demonstrates a Q value that is not clipped. In some cases, the system also updates the information set that does not justify any non-clipped Q values, e.g., by using only rewards.
When the system uses a tabular representation of the Q function, the system is able to update the Q value of each information set by calculating a weighted sum of the current Q value and the backup of the information set.
When the system approximates the Q function using a function approximator, the system can calculate an update of the function approximator corresponding to the information set by calculating a supervised learning update based on an error gradient (e.g., mean square error) between the current Q value and the backup of the information set.
As described above, in some embodiments, the system can prune the information sets during training, merge the information sets during training, or both when certain criteria are met. For example, when the number of information sets exceeds a threshold number, the system can prune the information set with the lowest Q value or can combine the information sets with the lowest Q value.
Once training is complete, the system can select the policy parameters represented by one of the information sets as the final set of policy parameters. In particular, the system can select the policy parameter that results in assigning the highest Q value to the action selected by the control policy in the initial state of any given episode of control agents as the policy parameter to be employed to control the agent in that episode.
Table 2 below illustrates pseudo code for learning a control strategy using the process 300 when the system uses a tabular representation of the Q function. The symbols used in table 2 are the same as those used in table 1 above, and in addition:
-p (s ' | s, a) is the probability assigned by the transition model to the next state s ', i.e. the next state s ' is the probability of the state to which the environment transitioned due to the agent performing the current action a at the current state s.
Table 2
In both process 200 and process 300, the system maintains a set of information. When the action space is large, when the space of possible constraints (or possible policy parameters) is large, or when the amount of computational resources available to perform the training process is limited, the system may avoid employing multiple sets of information, and instead employ a single function approximator for estimating the observation-action pairs. In these cases, the system may use the updates to enforce local consistency to mitigate the impact of the delusional bias on the training process.
FIG. 4 is a flow diagram of an example process 400 for learning a control strategy using locally consistent backups. For convenience, process 400 will be described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed reinforcement learning system, such as reinforcement learning system 100 of fig. 1, can perform process 400.
The system receives a batch of training tuples (step 402). As described above, each training tuple includes a current observation characterizing a current state of the environment, a current action performed by the agent in response to the current observation, a next observation characterizing a next state of the environment, and a reward received as a result of the agent performing the current action.
The system selects a corresponding next action for each next observation in the batch (step 404). In particular, the system selects the respective next actions such that they are locally consistent. More specifically, the system does not select the argmax action independently for each next observation as in conventional Q learning, but rather selects the next action based on criteria that ensure that the next actions locally coincide with each other. The criteria specify that (i) all next actions can be selected using the same control policy, i.e. there is some set of policy parameters according to which the control policy will select all next actions in response to their respective next observations, and (ii) each next action must be consistent with the current action in the same tuple, i.e. there is some set of policy parameters according to which the control policy will select the current action in response to the current observation and the next action in response to the next observation. In other words, for each next observation, when only actions that satisfy both criteria (i) and (ii) are considered, the system selects the argmax action as the next action, i.e., ignores any actions that do not satisfy (i), (ii), or both.
The system calculates a respective target Q value for each of a batch of training tuples (step 406). In particular, the system determines a target Q value for each tuple from the rewards in the tuples and the Q values generated by the control policy for the next observation-next action pair, where the next observation is the next observation in the tuples and the next action is the next action selected for the tuple in step 404, and then determines an update based on the target Q values. For example, the target Q value can be the sum of the product of the reward and discount coefficients and the Q value generated by the control strategy for the next observation-next action pair.
The system updates the control strategy parameters using the target Q values for the training tuples (step 408). In particular, the system computes supervised learning updates based on the error gradient (e.g., mean square error) between the current Q value of each tuple (i.e., the Q value currently generated by the function approximator for the current observation-current action pair) and the target Q value of the tuple.
This specification uses the term "configured" with respect to systems and computer program components. For a system of one or more computers, configured to perform a particular operation or action means: the system has installed thereon software, firmware, hardware, or a combination thereof that, when executed, causes the system to perform the operations or actions. For one or more computer programs, configured to perform a particular operation or action means: the one or more programs include instructions that, when executed by the data processing apparatus, cause the apparatus to perform the operations or acts.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, tangibly embodied in computer software or firmware, computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on a tangible, non-transitory storage medium for execution by, or to control the operation of, a digital processing apparatus. The computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or additionally, the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
The term "data processing apparatus" refers to data processing hardware and encompasses all types of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can also be or further comprise special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can optionally include, in addition to hardware, code that creates an execution environment for the computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program (which may also be referred to or described as a program, software application, module, software module, script, or code) can be written in any form of programming language, including compiled or interpreted languages or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
In this specification, the term "database" refers broadly to any collection of data: the data need not be structured in any particular way or at all, and can be stored on a storage device in one or more locations. Thus, for example, an index database can include multiple data sets, each of which can be organized and accessed differently.
Similarly, in this specification, the term "engine" broadly refers to a software-based system, subsystem, or process that is programmed to perform one or more particular functions. Typically, the engine will be implemented as one or more software modules or components installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines can be installed and run on the same computer or computers.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and in combination with, special purpose logic circuitry, e.g., an FPGA or an ASIC.
A computer adapted to execute a computer program can be based on a general purpose microprocessor or a special purpose microprocessor or both, or any other kind of central processing unit. Typically, the central processing unit will receive instructions and data from a read-only memory or a random access memory or both. The elements of a computer are a central processing unit for implementing or executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer does not necessarily have such a device. Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a Universal Serial Bus (USB) flash drive, to name a few.
Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example: semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; as well as CDROM discs and DVD-ROM discs.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other types of devices may also be used to provide for interaction with a user; for example, feedback provided to the user can be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. In addition, the computer is able to interact with the user by sending and receiving files to and from the device used by the user; for example, by sending a web page to a web browser on the user device in response to a request received from the web browser. Moreover, the computer is able to interact with the user by sending a text message or other form of message to a personal device (e.g., a smartphone running a messaging application) and receiving a response message from the user.
The data processing apparatus for the machine learning model can also, for example, include dedicated hardware accelerator units, common and computationally intensive portions for processing machine learning training or generating (i.e., reasoning) workloads.
The machine learning model can be implemented and deployed by using a machine learning framework (e.g., a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework).
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface, a web browser, or an App through which a user can interact with an implementation of the subject matter described in this specification, or any combination of such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a Local Area Network (LAN) and a Wide Area Network (WAN), e.g., the Internet.
The computing system may include clients and servers. The client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server transmits data (e.g., HTML pages) to the client device, for example, for the purpose of displaying data to and receiving user input from a user interacting with the device acting as a client. Data generated at the user device, e.g., the result of the user interaction, can be received at the server from the device.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted and described in the drawings and claims as occurring in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Specific embodiments of the subject matter are described herein. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (15)
1. A method of determining a control policy for an agent interacting with an environment, the method comprising:
maintaining data defining a plurality of information sets, each information set corresponding to a respective set of policy constraints and identifying a Q value assigned by the control policy under the set of policy constraints to an observation-action pair;
receiving a current observation characterizing a current state of the environment, a current action performed by the agent in response to the current observation, a next observation characterizing a next state of the environment, and a reward received as a result of the agent performing the current action;
determining a policy-consistent backup of a current view-current action pair for the control policy, comprising:
identifying, for each action of a plurality of actions in a set of possible actions that can be performed by the agent, a Q value assigned by the control policy to a next observation-action pair of the control policy and evidenced by at least one of the sets of information;
pruning from the identified Q values any Q values that are only evidenced by a set of information that is not consistent with the policy class; and
determining the policy-consistent backup from the reward and the identified Q value that is only untrimmed; and
updating the control policy of the agent using the policy-consistent backup by using Q-learning.
2. The method of claim 1, wherein updating the control policy of the agent using the policy-consistent backup using Q learning comprises: updating the control strategy by using model-free Q-learning, and wherein determining the strategy-consistent backup from the reward and the identified Q-value that is only untrimmed comprises: a Q backup is determined.
3. The method of any preceding claim, wherein the policy-consistent backups comprise a respective backup for each set of information that proves an untrimmed Q-value.
4. The method of claim 3, wherein the respective backup is based on (i) the reward and (ii) a Q value that is not pruned and is evidenced by the set of information.
5. The method of any of the preceding claims, wherein information sets that are not policy class consistent are those that impose policy constraints that cause the control policy to not select the current action in response to the current observation.
6. A method of determining a control policy for an agent interacting with an environment, the method comprising:
maintaining data defining a plurality of information sets, each information set corresponding to a respective set of policy constraints and identifying a Q value assigned by the control policy under the set of policy constraints to an observation-action pair;
receiving a current observation characterizing a current state of the environment, a current action performed by the agent in response to the current observation according to a current control policy, and a reward received as a result of the agent performing the current action;
determining a policy-consistent backup of a current view-current action pair for the control policy, comprising:
for each of a plurality of next states:
for each action of a plurality of actions in a set of possible actions that can be performed by the agent, identifying a Q value assigned by the control policy to a next observation-action pair of the control policy and evidenced by at least one of the sets of information, wherein the next observation is an observation that characterizes the next state; and
pruning from the identified Q values any Q values that are only evidenced by a set of information that is not consistent with the policy class; and
determining the policy-consistent backups from the rewards and the identified Q values that are not pruned only for each of the next states; and
updating the control policy of the agent using the policy-consistent backup by using Q-learning.
7. The method of claim 6, further comprising: maintaining a dynamic transformation model of the environment, wherein determining the policy-consistent backup from the reward and the identified Q-value that is only untrimmed comprises: determining a Bellman backup by using the reward and the identified Q value that is not clipped for the next state.
8. The method of claim 7, wherein the transition model maps the current observations and the current actions to respective probabilities for each of the next states, and wherein determining a Bellman backup comprises determining a Bellman backup using the reward, the respective probabilities for the next state, and the identified Q value that is not clipped for the next state.
9. The method of any of the preceding claims, wherein the control strategy selects the action to be performed by the agent by using a neural network, and wherein updating the control strategy comprises training the respective neural network for each set of information that proves a Q value that is not clipped.
10. The method of any of the preceding claims, wherein the control strategy selects an action to be performed by the agent by using a linear function approximator, and wherein updating the control strategy comprises updating a weight of the respective linear function approximator for each set of information that proves a Q value that is not clipped.
11. The method of any of the preceding claims, wherein the control policy selects an action to be performed by the agent by using a table representation that maps observation-action pairs to Q values, and wherein updating the control policy comprises updating the Q value for the current observation-action pair in the respective table representation for each set of information that proves a Q value that is not clipped.
12. The method of any preceding claim, wherein:
the agent comprises a mechanical agent;
the observations for characterizing the current state and/or the next state of the environment comprise or are generated from sensor data;
the current action and/or set of active actions includes an input for controlling the mechanical agent.
13. The method of any preceding claim, wherein:
the agent comprises an electronic agent;
the observations for characterizing the current state and/or the next state of the environment comprise or are generated from sensor data for monitoring a part of a plant or a service facility;
the current action and/or set of possible actions includes an action for controlling and/or imposing an operating condition on an equipment item in the plant or service facility.
14. One or more computer-readable storage media storing instructions that, when executed by one or more computers, cause the one or more computers to perform the respective operations of any one of the methods of any preceding claim.
15. A system comprising one or more computers and one or more storage devices storing instructions that, when executed by the one or more computers, cause the one or more computers to perform respective operations of any of the methods of any of claims 1-13.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201862752306P | 2018-10-29 | 2018-10-29 | |
US62/752,306 | 2018-10-29 | ||
PCT/US2019/058660 WO2020092437A1 (en) | 2018-10-29 | 2019-10-29 | Determining control policies by minimizing the impact of delusion |
Publications (1)
Publication Number | Publication Date |
---|---|
CN112930541A true CN112930541A (en) | 2021-06-08 |
Family
ID=69374351
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980070743.7A Pending CN112930541A (en) | 2018-10-29 | 2019-10-29 | Determining a control strategy by minimizing delusional effects |
Country Status (4)
Country | Link |
---|---|
US (1) | US20210383218A1 (en) |
EP (1) | EP3847583A1 (en) |
CN (1) | CN112930541A (en) |
WO (1) | WO2020092437A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN113610226A (en) * | 2021-07-19 | 2021-11-05 | 南京中科逆熵科技有限公司 | Online deep learning-based data set self-adaptive cutting method |
Families Citing this family (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2018083667A1 (en) * | 2016-11-04 | 2018-05-11 | Deepmind Technologies Limited | Reinforcement learning systems |
JP7263980B2 (en) * | 2019-08-27 | 2023-04-25 | 富士通株式会社 | Reinforcement learning method, reinforcement learning program, and reinforcement learning device |
CN111340227A (en) * | 2020-05-15 | 2020-06-26 | 支付宝(杭州)信息技术有限公司 | Method and device for compressing business prediction model through reinforcement learning model |
CN113377655B (en) * | 2021-06-16 | 2023-06-20 | 南京大学 | Task allocation method based on MAS-Q-learning |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN106056213A (en) * | 2015-04-06 | 2016-10-26 | 谷歌公司 | Selecting reinforcement learning actions using goals and observations |
WO2018156891A1 (en) * | 2017-02-24 | 2018-08-30 | Google Llc | Training policy neural networks using path consistency learning |
-
2019
- 2019-10-29 EP EP19845629.5A patent/EP3847583A1/en active Pending
- 2019-10-29 WO PCT/US2019/058660 patent/WO2020092437A1/en unknown
- 2019-10-29 US US17/289,514 patent/US20210383218A1/en active Pending
- 2019-10-29 CN CN201980070743.7A patent/CN112930541A/en active Pending
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN106056213A (en) * | 2015-04-06 | 2016-10-26 | 谷歌公司 | Selecting reinforcement learning actions using goals and observations |
WO2018156891A1 (en) * | 2017-02-24 | 2018-08-30 | Google Llc | Training policy neural networks using path consistency learning |
Non-Patent Citations (2)
Title |
---|
LIZOTTE DJ等: "Multi-objective Markov decision processes for data-driven decision support", JMLR, vol. 17, no. 1, 16 December 2016 (2016-12-16), pages 1 - 28 * |
TOBIAS POHLEN等: "Observe and look further：Achieving Consistent Performance on Atari", ARXIV, vol. 1805, 29 May 2018 (2018-05-29), pages 1 - 19 * |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN113610226A (en) * | 2021-07-19 | 2021-11-05 | 南京中科逆熵科技有限公司 | Online deep learning-based data set self-adaptive cutting method |
CN113610226B (en) * | 2021-07-19 | 2022-08-09 | 南京中科逆熵科技有限公司 | Online deep learning-based data set self-adaptive cutting method |
Also Published As
Publication number | Publication date |
---|---|
WO2020092437A1 (en) | 2020-05-07 |
EP3847583A1 (en) | 2021-07-14 |
US20210383218A1 (en) | 2021-12-09 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11868894B2 (en) | Distributed training using actor-critic reinforcement learning with off-policy correction factors | |
CN110546653B (en) | Action selection for reinforcement learning using manager and worker neural networks | |
US10860927B2 (en) | Stacked convolutional long short-term memory for model-free reinforcement learning | |
CN112930541A (en) | Determining a control strategy by minimizing delusional effects | |
US20200244707A1 (en) | Multi-agent reinforcement learning with matchmaking policies | |
US11887000B2 (en) | Distributional reinforcement learning using quantile function neural networks | |
US20210397959A1 (en) | Training reinforcement learning agents to learn expert exploration behaviors from demonstrators | |
CN113168566A (en) | Controlling a robot by using entropy constraints | |
WO2021058583A1 (en) | Training action selection neural networks using q-learning combined with look ahead search | |
US20230083486A1 (en) | Learning environment representations for agent control using predictions of bootstrapped latents | |
US20230144995A1 (en) | Learning options for action selection with meta-gradients in multi-task reinforcement learning | |
KR20230119023A (en) | Attention neural networks with short-term memory | |
CN116324818A (en) | Reinforced learning agent using reinforced time difference learning training | |
US20220366246A1 (en) | Controlling agents using causally correct environment models | |
EP4007976A1 (en) | Exploration using hypermodels | |
CN115066686A (en) | Generating implicit plans that achieve a goal in an environment using attention operations embedded to the plans | |
US20240126812A1 (en) | Fast exploration and learning of latent graph models | |
US20230325635A1 (en) | Controlling agents using relative variational intrinsic control | |
US20240086703A1 (en) | Controlling agents using state associative learning for long-term credit assignment | |
US20240046112A1 (en) | Jointly updating agent control policies using estimated best responses to current control policies | |
KR20230153481A (en) | Reinforcement learning using ensembles of discriminator models |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |