US8892422B1 - Phrase identification in a sequence of words - Google Patents
Phrase identification in a sequence of words Download PDFInfo
- Publication number
- US8892422B1 US8892422B1 US13/544,323 US201213544323A US8892422B1 US 8892422 B1 US8892422 B1 US 8892422B1 US 201213544323 A US201213544323 A US 201213544323A US 8892422 B1 US8892422 B1 US 8892422B1
- Authority
- US
- United States
- Prior art keywords
- words
- sequence
- phrase
- identified
- weighting
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/284—Lexical analysis, e.g. tokenisation or collocates
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/289—Phrasal analysis, e.g. finite state techniques or chunking
Definitions
- Text mining involves the processing of text data to make the text data suitable for one or more applications such as, for example, text classification, text clustering, information retrieval, and/or key phrase detection. Text mining attempts to represent unstructured text data in a structured format that can be processed meaningfully for one or more text mining applications.
- Bag-of-Words One text representation is called the “Bag-of-Words” model or the “Vector Space” model.
- the present disclosure is directed to methods and apparatus for phrase identification.
- some implementations are directed to methods and apparatus for determining co-occurrence consistencies for positional word pairings of a plurality of sequences of words in a corpus that may be utilized in identifying a phrase.
- some implementations are directed to inventive methods and apparatus for determining a phrase coherence of a sequence of words based on the co-occurrence consistencies for positional word pairings in the sequence of words.
- inventive methods and apparatus for determining one or more phrase boundaries in a sequence of words are directed to inventive methods and apparatus for determining one or more phrase boundaries in a sequence of words.
- a computer implemented method of identifying a phrase weighting of a sequence of words as a function of the position of words present in the sequence of words includes the steps of: identifying a sequence of words; determining, utilizing one or more processors, a centrality value for each of a plurality of identified words in the sequence of words, the centrality value for each of the identified words based on a co-occurrence consistency with other of the identified words in their respective relative positions in the sequence of words; and determining, utilizing one or more processors, a phrase weighting of the sequence of words based on the determined centrality value for each of the identified words, wherein the phrase weighting provides an indication of the likelihood that the sequence of words is a phrase.
- the step of determining the phrase weighting of the sequence of words may include combining the centrality value for each of the identified words.
- the step of determining the phrase weighting of the sequence of words may include determining a weighted average of the centrality value for each of the identified words.
- the step of determining the phrase weighting of the sequence of words may include determining a minimum centrality value from the centrality value for each of the identified words.
- the method may further include identifying the sequence of words for indexing as a phrase when the phrase weighting is greater than a threshold phrase weighting.
- the step of determining a centrality value for each of the identified words may include performing an auto-associative analysis for each of the identified words with all other of the identified words in their respective relative positions in the sequence of words.
- the step of determining a centrality value for each of the identified words may include performing a link analysis for each of the identified words with all other of the identified words in their respective relative positions in the sequence of words.
- the co-occurrence consistency for each of the identified words with other of the identified words in their respective relative positions in the sequence of words may be extracted from a plurality of positional matrices, each of the positional matrices containing co-occurrence consistency values for a plurality of word pairs in a given positional relation to one another.
- the co-occurrence consistency for each of the identified words with other of the identified words in their respective relative positions in the sequence of words may factor in how much more likely than incidental it is for each of the identified words to co-occur with other identified words in their respective relative positions.
- the method may further include parsing a document to create the sequence of words.
- the method may further include receiving a search query and identifying the sequence of words from the search query.
- the sequence of words may be part of a larger sequence of words and the method may further include: identifying a second sequence of words from the larger sequence of words, wherein the second sequence of words contains the same words as the identified words and one additional word; determining, utilizing one or more processors, an additional centrality value for the additional word, the additional centrality value based on the co-occurrence consistency of the additional word with other of the identified words in their respective relative positions in the second sequence of words; and determining, utilizing one or more processors, a second phrase weighting of the second sequence of words based on the determined additional centrality value, wherein the second phrase weighting provides an indication of the likelihood that the second sequence of words is a phrase; and comparing the phrase weighting to the second phrase weighting to identify which is more likely to be a phrase.
- the sequence of words may be part of a larger sequence of words and the method may further include: identifying additional sequences of words from the larger sequence of words; and determining, utilizing one or more processors, additional phrase weightings of each of the additional sequences of words that provide an indication of phrase likelihood.
- the method may further include comparing a plurality of the additional phrase weightings and the phrase weighting to one another to identify at least one phrase in the larger sequence of words.
- the comparing may include identifying at least one local extrema among the additional phrase weightings and the phrase weighting.
- n is an integer; determining a count of the number of instances each positional bigram occurs in the document collection; determining a co-occurrence probability for each positional bigram based on the positional bigram counts, where the co-occurrence probability is a measure of the probability each positional bigram occurs in the corpus; determining a co-occurrence consistency for each positional bigram based on the co-occurrence probability for each positional bigram; where the co-occurrence consistency is a measure of how much greater the co-occurrence probability factor is than an incidental chance the positional bigram occurs; and storing the co-occurrence consistency for each identified positional bigram in the document collection.
- the method may further include identifying one or more sequences of words including an ordered set of words from the document collection; determining a centrality value for each of a plurality of words in each of the sequences of words based on the co-occurrence consistency determined for each positional bigram, where each centrality value is a measure of how important one of the words is in the sequence of words; and/or determining a phrase weighting for each of the one or more sequences of words based on the centrality value.
- the step of determining the phrase weighting for the one or more sequences of words may optionally include combining the centrality value of all words in the sequence of words to compute the phrase weighting for the sequence of words.
- the step of determining the phrase weighting may optionally include combining the centrality value for the words in the sequence of words by calculating a weighted average of the centrality values for the words and/or determining the minimum of the centrality values for the words.
- the method may further include parsing a document to identify one or more sequences of words and assigning phrase weightings to the sequences of words.
- the method may optionally further include selecting one or more of the sequences of words for indexing the document based on the phrase weightings assigned to the sequences of words.
- the step of selecting one or more of the sequences of words for indexing may include selecting one or more of the sequences of words that have corresponding phrase weightings greater than a predefined value.
- the method further includes omitting certain positional bigrams from one or more steps when the certain positional bigrams have less than a predefined value of a count, a co-occurrence probability, and/or co-occurrence consistency.
- implementations may include a non-transitory computer readable storage medium storing instructions executable by a processor to perform a method such as one or more of the methods described herein.
- implementations may include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform a method such as one or more of the methods described herein.
- Particular implementations of the subject matter described herein process a sequence of words to identify at least one phrase in the sequence of words for utilization in one or more meaningful text mining applications. These identified phrases are derived from analysis of multiple words of the word grouping and the word pairing positions of words in the word grouping. The identified phrases may be utilized by one or more text mining applications to provide improved performance of the text mining applications. Particular implementations of the subject matter described herein process words in a plurality of sequences of words to determine co-occurrence consistencies for the positional words pairings in the sequence of words for utilization in identifying phrases. The determined co-occurrence consistencies represent new values that are derived from analysis of those words and their position in the sequence of words.
- FIG. 1 is a flow chart illustrating an implementation of a method of determining co-occurrence relationships between words in their respective relative positions in a corpus of word groupings.
- FIG. 2 illustrates an example of a sequence of words and positional bigrams of the sequence of words.
- FIG. 3 is a flow chart illustrating an implementation of a method of identifying a phrase weighting of a sequence of words as a function of which words are present in the sequence of words and the position of the words.
- FIG. 4A illustrates the position of “new” relative to other words in the phrase “new york stock exchange.”
- FIG. 4B illustrates the position of “york” relative to other words in the phrase “new york stock exchange.”
- FIG. 4C illustrates the position of “stock” relative to other words in the phrase “new york stock exchange.”
- FIG. 4D illustrates the position of “exchange” relative to other words in the phrase “new york stock exchange.”
- FIG. 5 illustrates an example of positional matrices utilized to determine the co-occurrence consistencies for each of the words of the phrase “new york stock exchange” with other words of the phrase “new york stock exchange.”
- FIG. 6 is a flow chart illustrating an implementation of a method of identifying one or more phrase boundaries in a sequence of words.
- FIG. 7 illustrates an example of a lattice structure of phrase weighting formulas for a plurality of subsequences in a sequence of words that graphically illustrates an example of identifying one or more phrase boundaries in a sequence of words.
- FIG. 8 illustrates an example of a lattice structure of actual phrase weighting values for a plurality of subsequences in a sequence of words that graphically illustrates an example of identifying one or more phrase boundaries in a sequence of words.
- FIG. 9 is a block diagram of an example environment in which co-occurrence consistencies for positional word pairings in a sequence of words may be determined, phrase coherence of a sequence of words may be determined as a function of positional word pairings in the sequence of words, and/or one or more phrase boundaries in a sequence of words may be determined.
- FIG. 10 illustrates a block diagram of an example computer system.
- a corpus of word groupings is a set of electronically stored word groupings from one or more sources that are analyzed to determine one or more characteristics of the words of the word groupings.
- the corpus of word groupings described herein may be utilized to determine co-occurrence consistencies for positional word pairings of a plurality of sequences of words in the corpus that may be utilized in identifying a phrase.
- Other implementations may perform the steps in a different order, omit certain steps, and/or perform different and/or additional steps than those illustrated in FIG. 1 .
- FIG. 1 will be described with reference to a system of one or more computers that perform the process.
- the system may be, for example, the co-occurrence relationship determination engine 910 of FIG. 9 .
- a vocabulary of words is created.
- the vocabulary of words may be obtained from the content database 915 ( FIG. 9 ).
- the content database 915 may include one or more storage mediums having one or more documents and unique words from the documents may be utilized to form the vocabulary of words.
- the documents may optionally include all of those word groupings of the corpus of step 110 .
- the content database 915 may include a plurality of word groupings of one or more tags for media (e.g., videos, images, sounds), all the unique words of the tags may form the vocabulary of words, and the plurality of word groupings may form the corpus.
- the content database 915 may include a plurality of documents each having a plurality of words, the unique words may form the vocabulary of words, and the plurality of documents may form the corpus.
- Each of the documents may include all or portions of one or more documents such as, for example, HTML documents, PDF documents, DOC documents, query documents, keyword documents, and/or media tag documents.
- one document may include a portion of another document such as, for example, tags of an HTML document.
- a document may include portions of a plurality of documents such as, for example, portions of the titles of a plurality of PDF documents and DOC documents.
- One or more canonicalization rules may optionally be applied to words from the documents to modify and/or omit certain words. For example, stemming of words, phrasing of words, removal of low frequency words, removal of numerical values, and/or removal of stop words may be utilized. Also, for example, in some implementations the vocabulary of key words may be derived from one or more preexisting listings of words. For example, a preexisting listing of words that includes a plurality of words from a language that have optionally been modified utilizing canonicalization rules may be utilized.
- a positional bigram is a co-occurrence of a pair of words in a specific relative position with respect to each other.
- a text segment may include any sequence of words with no breaking punctuation.
- the text “in trading today, the new york stock exchange” may include two text segments: “in trading today” and “the new york stock exchange.”
- FIG. 2 illustrates the text segment “new york stock exchange.”
- Positional bigram 2 is for the word pair “new” and “york,” with “new” occurring one position prior to “york.”
- Positional bigram 2 is for the word pair “new” and “stock,” with “new” occurring two positions prior to “stock.”
- Positional bigram 3 is for the word pair “new” and “exchange,” with “new” occurring three positions prior to “exchange.”
- Positional bigram 4 is for the word pair “york” and “stock,” with “york” occurring one position prior to “stock.”
- Positional bigram 5 is for the word pair “york” and “exchange,” with “york” occurring two positions prior to “exchange.”
- Positional bigram 6 is for the word pair “stock” and “exchange,” with “stock” occurring one position prior to “exchange.”
- one or more words in a sequence of words may be replaced with a generic placeholder.
- a positional bigram may optionally formed with such a placeholder.
- the number “4” in the phrase “new york stock exchange rose 4 points” may be replaced with a placeholder such as “#.”
- the percentage “0.25%” in the phrase “new york stock exchange rose 0.25%” may be replaced with a placeholder such as “%.”
- positional bigrams may optionally be formed that include such a placeholder as one of the word pairs. In some implementations positional bigrams may not be formed for such a placeholder.
- the co-occurrence relationship determination engine 910 may identify positional bigrams for each value k from the vocabulary of words that occur at least once in any of the text segments of the corpus of step 110 . In some implementations the co-occurrence relationship determination engine 910 may identify positional bigrams for each value k from the vocabulary of words that occur with at least a threshold frequency in any of the text segments of the corpus of step 110 . In some implementations all possible positional bigrams for each value k for words from the vocabulary of words may be identified as positional bigrams.
- the number of instances in a corpus in which the positional bigram occurs is determined.
- the co-occurrence relationship determination engine 910 may identify the number of sequences of words in a corpus in which both words in the positional bigram co-occur in the relative position k of the positional bigram.
- the co-occurrence relationship determination engine 910 may identify the number of text segments in a corpus in which both words in the positional bigram co-occur in the relative position k of the positional bigram.
- a segment parser may be utilized to parse each of a plurality of documents of the corpus into its one or more fields; parse each field into its one or more sentences; and/or parse each sentence into its one or more text segments.
- a segment parser may be utilized to parse each of a plurality of documents of the corpus into its one or more fields; parse each field into its one or more sentences; and/or parse each sentence into its one or more text segments.
- only the number of instances in which the positional bigram occurs within individual text segments in the corpus is determined.
- the analyzed sequences of words of the corpus may include any sequence of words for which it may be desirable to perform text mining to rank words in the sequence of words.
- one or more of the sequence of words of the corpus may include: a plurality of tags for media such as, for example, images, videos, and/or sounds; a multi-word query such as, for example, a search engine query; adwords such as, for example, a set of keywords associated with an advertisement; a plurality of words from a field in a document such as, for example, a set of keywords in a title, URL, and/or anchor; a plurality of words of a document such as, for example, all or a set of words in an entire document; and/or a plurality of keywords such as, for example, a set of explicitly annotated keywords in a document such as, for example, named entities in a news story, MeSH terms in Medline data, and/or keywords in a scientific publication.
- sequences of words may optionally be parsed into text segments and only positional bigrams within each of the text segments may be counted.
- One of ordinary skill in the art, having had the benefit of the present disclosure, will recognize and appreciate that the methods and apparatus described herein may be applied to one or more of a variety of sequences of words.
- the weight of the m th sequence of words may depend on the relative importance of the field, f, in which the sequence of words occurs (e.g., title may be more important than the header and/or the header may be more important than the body) and/or the number of occurrences of the sequence of words in the field in the corpus.
- w m may be represented by the equation
- w m ⁇ fields : f ⁇ ⁇ f ⁇ freq ⁇ ( x m
- the weight of the m th sequence of words (w m ) may additional and/or alternatively depend on the weight of the document from which the sequence of words is obtained.
- an un-weighted count of the number of instances in a corpus in which the positional bigram occurs is determined.
- an un-weighted count of the number of text segments in a corpus in which the positional bigram occurs is determined. For example, if the first word of a word pair is “new,” the second word of the word pair is “eve,” and the relative position between the two words is two, a count of the number of word groupings in the corpus in which “new” occurs two positions prior to “eve” will be determined. This may be done for each of the identified positional bigrams in the corpus.
- a weighted count of the number of instances in a corpus in which the positional bigram occurs is determined.
- the weighted count still determines the number of instances in a corpus in which the positional bigram occurs, but also takes into account the weight of the sequence of words in each of the sequences of words in which the positional bigram occurs.
- the weighted count of the number of instances in a corpus in which the positional bigram occurs may take into account one or more independence based presumptions such as property weighting (e.g., how often the sequence of words occurs in the word grouping, how early the sequence of words occurs in the document, decorations applied to the sequence of words (bold, italics, font, etc.)), and/or field weighting (e.g., does the sequence of words occur in an important field such as the title). For example, positional bigrams that occur in the title of a document in the corpus may be given more weight than positional bigrams that occur in the body of a document in the corpus.
- property weighting e.g., how often the sequence of words occurs in the word grouping, how early the sequence of words occurs in the document, decorations applied to the sequence of words (bold, italics, font, etc.)
- field weighting e.g., does the sequence of words occur in an important field such as the title.
- At least some of the weights for the sequence of words may be provided.
- a weight may be provided in combination with each of the sequences of words.
- a weight may be provided in combination with each of the documents containing the sequences of words.
- at least some of the weights may additionally or alternatively be determined via analysis of the sequences of words in the corpus.
- the co-occurrence relationship determination engine 910 may determine the weights via analysis of the sequences of words.
- a default weight (e.g., 1) may be assigned to such sequences of words.
- a co-occurrence count matrix for each value k may optionally be formed in step 115 that identifies the number (either weighted or un-weighted) of sequences of words in the corpus in which the bigram occurs for a position k. For example, four separate co-occurrence count matrices may be created for counts of a plurality of bigrams for positions 1 , 2 , 3 , and 4 . In some implementations the co-occurrence relationship determination engine 910 may form each co-occurrence count matrix in step 115 . In some implementations pruning of certain bigrams from the co-occurrence counts of the number of bigrams for a value k may occur.
- the threshold number may be a set number (e.g., 20).
- the threshold number may be statistically determined (e.g., statistical outliers or a set deviation below a statistical mean or medium).
- existing co-occurrence count matrices may optionally be modified. For example, as a corpus of sequences of words grows and adds new sequences of words, it may be desirable to update the co-occurrence count matrices with data from the new sequences of words.
- the number of sequences of words in the new grouping in which the positional bigrams are present may be determined (e.g., as described, for example, with respect to step 110 ) to create new grouping co-occurrence count matrices for the new grouping of sequences of words.
- the new grouping co-occurrence count matrices may then be added to the already existing co-occurrence count matrices to thereby update the co-occurrence count matrices.
- New groupings of sequences of words may be added to the co-occurrence count matrices on one or more of a variety of bases. For example, in some implementations new groupings of sequences of words may be added every hour, every day, every week, as soon as a new groupings are received, as soon as 1,000 new groupings are received, etc.
- the co-occurrence count matrices may be desirable to update to reflect the removal of such sequences of words.
- the number of sequences of words in a corpus in which the bigrams are present for each value of k is determined (e.g., as described, for example, with respect to step 110 ) to create a new sequence co-occurrence count matrix of the removed sequence of words.
- the new sequence co-occurrence count matrix may then be subtracted from the already existing co-occurrence count matrix to thereby update the co-occurrence count matrix.
- Sequences of words may be removed from the co-occurrence count matrix on one or more of a variety of bases. For example, in some implementations sequences of words may be removed when the content with which they are associated is deleted, may be removed on a first-in-first-out basis in which the corpus is set to maintain a fixed amount of the most recent sequences of words, and/or may be removed after being a part of the corpus utilized for more than a certain time period (e.g., more than a day, more than a week, more than two years).
- a certain time period e.g., more than a day, more than a week, more than two years.
- the marginal counts for each of the words of the positional bigrams, totals for each of the words of the positional bigrams, and probabilities for each of the positional bigrams are determined.
- the co-occurrence relationship determination engine 910 may manipulate the co-occurrence count matrices optionally created in step 115 to determine marginal counts for each word for each position k, determine a total count for the words for each matrix of value k, and determine probabilities for each of the positional bigrams of each matrix of value k.
- From marginal counts for each word for each value k may be determined by summing the number (either weighted or un-weighted) of times for the positional bigrams determined in step 110 in which the first word of the bigram occurs k positions before any word. For example, if the word “new” occurred one position before the word “york” in 10 sequences of words in a corpus, occurred one position before the word “jersey” in 5 sequences of words in the corpus, and occurred one position before the word “potatoes” in 2 sequences of words in the corpus (and did not occur one position before any other words in the corpus), then the from marginal count for the unweighted count of the word “new” would be 17 (10+5+2).
- the from marginal count for a word for a value k may be determined by summing values in the co-occurrence count matrix for the value k in which the word is the first word in the bigram. In some implementations the from marginal count for a word for a value k may be determined using the following equation:
- ⁇ k ⁇ ( v ⁇ , • ) ⁇ v ⁇ ⁇ V ⁇ ⁇ k ⁇ ( v ⁇ , v ⁇ )
- To marginal counts for each word for each value k may be determined by summing the number (either weighted or un-weighted) of times for the positional bigrams determined in step 110 in which the some word of the bigram occurs k positions before the second word of the bigram. For example, if the word “happy” occurred one position before the word “new” in 15 sequences of words in a corpus and the word “brand” occurred one position before the word “new” in 5 sequences of words in the corpus (and no other words occurred one position before the word new in the corpus), then the to marginal count for the unweighted count of the word “new” would be 20 (15+5).
- the to marginal count for a word for a value k may be determined by summing values in the co-occurrence count matrix for the value k in which the word is the second word in the bigram. In some implementations the from marginal count for a word for a value k may be determined using the following equation:
- ⁇ k ⁇ ( • , v ⁇ ) ⁇ v ⁇ ⁇ V ⁇ ⁇ k ⁇ ( v ⁇ , v ⁇ )
- Total co-occurrence counts for a position k for all of the words may be determined by summing the number (either weighted or un-weighted) of all the various word groupings determined in step 110 in which some word occurs k positions before some word in the corpus. In some implementations the totals may be determined by summing all of the numerical elements in the co-occurrence count matrix for position k. In some implementations the totals may be determined using the following equation:
- ⁇ k ⁇ ( • , • ) ⁇ v ⁇ ⁇ V ⁇ ⁇ v ⁇ ⁇ V ⁇ ⁇ k ⁇ ( v ⁇ , v ⁇ )
- the determined marginal counts and totals may be utilized to normalize the co-occurrence count values for each matrix to probabilities.
- a co-occurrence probability may be determined for each positional bigram for a position k by dividing the co-occurrence count for that positional bigram for the position k by the determined total co-occurrence counts for all word pairs for the position k.
- the co-occurrence probability for a positional bigram for a position k may be determined using the following equation:
- a from marginal probability for each positional bigram for a position k may be determined by dividing the determined from marginal count for that positional bigram for the position k by the determined total co-occurrence counts for all word pairs for the position k. For example, in some implementations the from marginal probability for a word for a position k may be determined using the following equation:
- a to marginal probability for each positional bigram for a position k may be determined by dividing the determined to marginal count for that positional bigram for the position k by the determined total co-occurrence counts for all word pairs for the position k. For example, in some implementations the to marginal probability for a word for a position k may be determined using the following equation:
- smoothed versions of the counts for the to marginal counts, the from marginal counts, and/or the total counts may be utilized.
- one or more smoothing mechanisms may be utilized to convert raw counts into probabilities such as, for example, Good-Turing smoothing, Kneser-Ney smoothing, and/or Witten-Bell smoothing.
- smoothed to and from marginal probabilities may be calculated utilizing Laplace smoothing and the following equations:
- a smoothed co-occurrence probability for a positional bigram for a position k may be determined using the following equation:
- the co-occurrence relationship determination engine 910 may determine how much more likely than random/incidental it is for the positional bigram to co-occur in a sequence of words.
- it is determined for all of the positional bigrams how much more likely than random/incidental it is for the positional bigram to co-occur in a text segment.
- the co-occurrence probability of a positional bigram for a position k determined in step 120 may be compared to marginal probabilities of words of the positional bigram.
- the co-occurrence probability of a positional bigram compared to the to and from marginal probabilities of the positional bigram is relatively high, then it is likely that the words of the positional bigram actually have a positional correlation and are not positionally co-occurring just by random chance. If the co-occurrence probability of a positional bigram compared to the marginal probabilities of the positional bigram is relatively low, then it is likely that many of the positional co-occurrences of the words of the positional bigram are likely due to “incidental chance.” For example, two fairly common words may have a relatively high co-occurrence probability for a position k in a corpus due to the popularity of both of the terms.
- the comparison of the co-occurrence probability to the marginal probabilities of the positional bigrams for a value k may be utilized to determine a co-occurrence consistency for each of the positional bigrams for a value k at step 130 .
- a calculation based on the ratio of the co-occurrence probability to the marginal probabilities of the positional bigrams for position k may be the co-occurrence consistency for each of the positional bigrams for position k.
- the co-occurrence consistency for each of the positional bigrams may be determined using the following cosine equation:
- ⁇ k ⁇ ( v ⁇ , v ⁇ ) P k ⁇ ( v ⁇ , v ⁇ ) P k ⁇ ( v ⁇ , • ) ⁇ P k ⁇ ( • , v ⁇ )
- the co-occurrence consistency for each of the positional bigrams may be determined using the following pointwise mutual information equation:
- ⁇ k ⁇ ( v ⁇ , v ⁇ ) log ⁇ ( P k ⁇ ( v ⁇ , v ⁇ ) P k ⁇ ( v ⁇ , • ) ⁇ P k ⁇ ( • , v ⁇ ) )
- the co-occurrence consistency for each of the positional bigrams may be determined using the following Jaccard coefficient equation:
- ⁇ ⁇ ( v ⁇ , v ⁇ ) P k ⁇ ( v ⁇ , v ⁇ ) P k ⁇ ( v ⁇ , • ) + P k ⁇ ( • , v ⁇ ) - P k ⁇ ( v ⁇ , v ⁇ )
- equations for determining co-occurrence consistencies are described herein, one of ordinary skill in the art, having had the benefit of the present disclosure, will recognize and appreciate that other determinations of how much more likely than incidental it is for the positional bigram to co-occur in a sequence of words may be utilized. For example, one or more interest equations and/or confidence equations may be utilized.
- a plurality of co-occurrence consistency matrices may optionally be formed in step 135 that each identify the co-occurrence consistencies for a plurality of positional bigrams for a single positional value k.
- the co-occurrence relationship determination engine 910 may form the co-occurrence consistency matrices in step 135 .
- one or more of the co-occurrence consistency matrices created in step 135 may optionally omit certain positional bigrams (completely omit the positional bigrams and/or omit co-occurrence consistency data for certain positional bigrams) to omit those bigrams that are determined to have little correlation to one another in the relative position.
- certain positional bigrams completely omit the positional bigrams and/or omit co-occurrence consistency data for certain positional bigrams
- at least data associated with those bigrams that have less than a threshold co-occurrence consistency value for a given position may be removed from the relative co-occurrence consistency matrix.
- the threshold co-occurrence consistency value may be a set number.
- any co-occurrence consistency values below 0.1 may be removed from the co-occurrence consistency matrices.
- the threshold number may be empirically determined.
- one or more additional and/or alternative bases may optionally be utilized for pruning certain bigrams that occur in a certain position relative to one another with relatively low consistency.
- FIG. 3 a flow chart illustrating an implementation of a method of identifying a phrase weighting of a sequence of words as a function of which other words are present in the sequence of words is provided.
- Other implementations may perform the steps in a different order, omit certain steps, and/or perform different and/or additional steps than those illustrated in FIG. 3 .
- FIG. 3 will be described with reference to a system of one or more computers that perform the process.
- the system may be, for example, the phrase weighting engine 920 of FIG. 9 .
- a sequence of words is identified.
- the phrase weighting engine 920 may identify the sequence of words.
- the sequence of words may be identified from a supplied listing of words such as words from a document from one or more sources such as content database 915 .
- a headline for a news story from a web page document may be provided and the headline may form the sequence of words.
- a plurality of words in a document may be provided and one or more sequences of the words of the document may form the sequence of words.
- all the words of the document may form a word grouping and a segment parser may parse the words into one or more text segments that each form a sequence of words.
- the sequence of words may be identified from a submitted search query (e.g., the entire search query or a segment of the search query).
- the co-occurrence consistencies for each of a plurality of positional bigrams in the sequence of words are identified.
- the phrase weighting engine 320 may identify the co-occurrence consistencies.
- co-occurrence consistencies are identified for all possible positional bigrams of the sequence of words identified in step 300 .
- the co-occurrence consistencies may be identified from the co-occurrence consistency matrices created in step 125 of FIG. 1 .
- the co-occurrence consistency matrices may be stored in content database 915 and consulted to obtain co-occurrence consistencies for each of the positional bigrams of the identified sequence of words.
- one or more co-occurrence consistency matrices for the identified sequence of words may be created based on the co-occurrence consistency matrices created in step 125 of FIG. 1 .
- FIGS. 4A-4D illustrate the positions between each word of the phrase “new york stock exchange” and other words of the phrase.
- FIG. 4A illustrates the position of “new” relative to other words in the phrase “new york stock exchange.”
- FIG. 4B illustrates the position of “york” relative to other words in the phrase “new york stock exchange.”
- FIG. 4C illustrates the position of “stock” relative to other words in the phrase “new york stock exchange.”
- FIG. 4D illustrates the position of “exchange” relative to other words in the phrase “new york stock exchange.”
- the illustrated positions represent positional bigrams for which co-occurrence consistencies may be determined.
- FIG. 4B illustrates that positional bigrams are present for “york” occurring one position after “new”; “york” occurring one position before “stock”; and “york” occurring two positions before “exchange”.
- Co-occurrence consistencies may be determined for one or more of the positional bigrams of the phrase “new york stock exchange.”
- FIG. 5 illustrates an example of positional matrices utilized to determine the co-occurrence consistencies for each of the words of the phrase “new york stock exchange” with other words of the phrase “new york stock exchange.”
- Co-occurrence consistency values for positional bigrams with words occurring one position apart ( ⁇ k (x 1 , x 2 ); ⁇ k (x 2 , x 3 ); ⁇ k (x 3 , x 4 )) are obtained from matrix ⁇ 1 .
- Co-occurrence consistency values for positional bigrams with words occurring two positions apart are obtained from matrix ⁇ 2 .
- the co-occurrence consistency value for the positional bigram with words occurring three positions apart ( ⁇ k (x 1 , x 4 )) is obtained from matrix ⁇ 3 .
- the centrality value for each of a plurality of words in the sequence of words is determined based on identified co-occurrence consistencies for positional bigrams including that word.
- a word has a significant centrality value if it co-occurs with other words in the sequence of words in its relative position and if the other words are also have a significant centrality value in the sequence of words.
- the importance of a word x i in a sequence of words x may be represented by I(x i
- x, ⁇ ), given a sequence of words that is represented by x ⁇ x 1 , x 2 , . . . x n ⁇ .
- the centrality value of one or more words in a sequence of words may be determined via an iterative auto-associative word importance algorithm.
- the auto-associative based centrality values for the words of a sequence of words may be determined via the following iterative equation that may be updated until convergence:
- I t + 1 ⁇ ( new ⁇ new ⁇ ⁇ york ⁇ ⁇ stock ⁇ ⁇ exchange , ⁇ ) I t ⁇ ( york ) ⁇ ⁇ 1 ⁇ ( new , york ) + I t ⁇ ( stock ) ⁇ ⁇ 2 ⁇ ( new , ⁇ stock ) + I t ⁇ ( exchange ) ⁇ ⁇ 3 ⁇ ( new , exchange ) ⁇ ⁇ 1 ⁇ ( new , york ) 2 + ⁇ ⁇ 2 ⁇ ( new , ⁇ stock ) 2 + ⁇ 3 ⁇ ( new , exchange ) 2 ; to determine the centrality value of the word “york” in the sequence of words “new york stock exchange,” the following iterative equation may be used:
- I t + 1 ⁇ ( york ⁇ new ⁇ ⁇ york ⁇ ⁇ stock ⁇ ⁇ exchange , ⁇ ) I t ⁇ ( new ) ⁇ ⁇ 1 ⁇ ( new , york ) + I t ⁇ ( stock ) ⁇ ⁇ 1 ⁇ ( york , ⁇ stock ) + I t ⁇ ( exchange ) ⁇ ⁇ 2 ⁇ ( york , exchange ) ⁇ 1 ⁇ ( new , york ) 2 + ⁇ ⁇ 1 ⁇ ( york , ⁇ stock ) 2 + ⁇ 2 ⁇ ( york , exchange ) 2 ; to determine the centrality value of the word “stock” in the sequence of words “new york stock exchange,” the following iterative equation may be used:
- I t + 1 ⁇ ( stock ⁇ new ⁇ ⁇ york ⁇ ⁇ stock ⁇ ⁇ exchange , ⁇ ) I t ⁇ ( new ) ⁇ ⁇ 2 ⁇ ( new , stock ) + I t ⁇ ( york ) ⁇ ⁇ 1 ⁇ ( york , ⁇ stock ) + I t ⁇ ( exchange ) ⁇ ⁇ 1 ⁇ ( stock , exchange ) ⁇ 2 ⁇ ( new , stock ) 2 + ⁇ ⁇ 1 ⁇ ( york , ⁇ stock ) 2 + ⁇ 1 ⁇ ( stock , exchange ) 2 ; and to determine the centrality value of the word “exchange” in the sequence of words “new york stock exchange,” the following iterative equation may be used:
- I t + 1 ⁇ ( exchange ⁇ new ⁇ ⁇ york ⁇ ⁇ stock ⁇ ⁇ exchange , ⁇ ) I t ⁇ ( new ) ⁇ ⁇ 3 ⁇ ( new , exchange ) + I t ⁇ ( york ) ⁇ ⁇ 2 ⁇ ( york , ⁇ exchange ) + I t ⁇ ( stock ) ⁇ ⁇ 1 ⁇ ( stock , exchange ) ⁇ 3 ⁇ ( new , exchange ) 2 + ⁇ ⁇ 2 ⁇ ( york , ⁇ exchange ) 2 + ⁇ 1 ⁇ ( stock , exchange ) 2 .
- the centrality value of one or more words in a sequence of words may be determined via a link analysis algorithm.
- the link analysis algorithm may be an algorithm such as a PageRank algorithm.
- the link analysis algorithm may assign a numerical weighting to each word of a sequence of words based on the links between that word and other words of the sequence of words.
- the link analysis based centrality values for the words of a word sequence may be determined via the following iterative equation that may be updated until convergence:
- the centrality value of one or more words in a sequence of words may be determined via a HITS based algorithm.
- the HITS analysis based centrality values for the words of a word sequence may be determined via the following iterative equations that may be updated until convergence:
- a phrase weighting is determined for the sequence of words based on the determined centrality values for the sequence of words.
- the phrase weighting provides an indication of the likelihood that the sequence of words is a phrase.
- the centrality values of all words in the sequence of words may be combined to determine the phrase weighting of the sequence of words.
- phrase weighting of the sequence of words may be determined by determining a Gibb's weighted average of the centrality values for the sequence of words. For example, in some implementations the phrase weighting of the sequence of words may be determined via the equation:
- the phrase weighting of the sequence of words may be based on the minimum centrality value of the centrality values for the sequence of words.
- the phrase weighting may be assigned a low phrase weighting due to a low centrality value of “jersey” even though an average based determination taking into account the centrality values of the other words “new, stock, exchange” may result in a relatively high phrase weighting.
- a phrase weighting for a sequence of words. For example, other average calculations may be utilized as an alternative to the Gibb's weighted average calculation. Also, for example, the centrality values of a sequence of words may first be analyzed to determine if any word has a centrality value that is below a minimum threshold centrality value, and, if it is determined that all centrality values are above the threshold centrality value, then an average of the centrality values may be taken to determine a phrase weighting.
- the determination of phrase weighting for a sequence of words may optionally be performed in “offline” applications such as, for example, crawling of documents, web pages, or other groupings of words.
- the determination of phrase weighting for a sequence of words may optionally be performed in real time application such as, for example, analysis of a submitted query (e.g., to determine phrase weighting of one or more text segments of the query).
- the sequence of words may be indexed as a phrase when the phrase weighting of the sequence of words is greater than a threshold phrase weighting.
- the sequence of words may be stored as an identified phrase in content database 915 or elsewhere when the phrase weighting indicates a high likelihood that the sequence of words is a phrase.
- a stored identified phrase may optionally be utilized in one or more text mining applications such as those discussed herein.
- FIG. 6 a flow chart illustrating an implementation of a method of identifying one or more phrase boundaries in a sequence of words is provided. Other implementations may perform the steps in a different order, omit certain steps, and/or perform different and/or additional steps than those illustrated in FIG. 6 .
- FIG. 6 will be described with reference to a system of one or more computers that perform the process.
- the system may be, for example, the phrase segmentation engine 925 of FIG. 9 .
- a sequence of words is identified.
- the phrase segmentation engine 925 may identify the sequence of words.
- the sequence of words may be identified from a supplied listing of words such as words from a document from one or more sources such as content database 915 .
- a headline for a news story from a web page document may be provided and the headline may form the sequence of words.
- a plurality of words in a document may be provided and one or more sequences of the words of the document may form the sequence of words. For instance, all the words of the document may form a word grouping and a segment parser may parse the words into one or more text segments that each form a sequence of words.
- the sequence of words may be identified from a submitted search query (e.g., the entire search query or a segment of the search query).
- the sequence of words may be a text segment.
- steps 300 and/or 600 may share one or more common aspects.
- a plurality of subsequences in the sequence of words are identified.
- the phrase segmentation engine 925 may identify the subsequences of words.
- the plurality of subsequences may include all potential groupings of two or more sequentially arranged words in the sequence of words.
- the plurality of subsequences may include all potential groupings of two or more sequentially arranged words in the sequence of words up to a value k. For example, as illustrated in FIGS.
- each subsequence is represented by a node in the lattice structure of FIGS. 7 and 8 .
- each node in FIG. 7 is illustrated by nodes labeled with ⁇ (x i ⁇ j ), wherein i represents the numerical value of the starting word of the subsequence, j represents the numerical value of the ending word of the subsequence, and ⁇ (x i ⁇ j ) represents the phrase weighting formula of the subsequence.
- ⁇ (x 3 ⁇ 6 ) represents the phrase weighting formula of the four word subsequence “new york stock exchange”.
- each node in FIG. 8 is illustrated by nodes labeled with a numerical value that is indicative of an example phrase weighting of the corresponding subsequence of words.
- numerical value “72” represents the example phrase weighting of the five word subsequence “new york stock exchange traded”.
- a phrase weighting is determined for each of the identified subsequences of words.
- the phrase weighting may be provided in combination with the subsequences of words.
- the phrase weightings may be obtained from a database such as content database 915 .
- the phrase segmentation engine 925 may query the content database 915 to obtain the various phrase weightings for the plurality of subsequences of words.
- the phrase weighting engine 920 may determine the phrase weighting for the identified subsequences of words.
- the phrase weighting engine 920 may perform one or more of the steps of FIG. 3 to determine the phrase weightings for the identified subsequences of words.
- the phrase weighting for each of the subsequences of words is compared with phrase weightings of neighboring subsequences of words.
- the phrase weighting of each node in the lattice structure of FIGS. 7 and 8 is compared with neighboring nodes.
- Neighboring subsequences of words of a given subsequence of words are those that either add or subtract a single word from the left or right of the given subsequence of words.
- the neighbors of “york stock exchange” in FIGS. 7 and 8 are “york stock”; “stock exchange;” “new york stock exchange;” and “york stock exchange traded.”
- the neighbors of “morning new” in FIGS. 7 and 8 are “today morning new” and “morning new york.”
- one or more of the subsequences of words is identified as a phrase if it has a more significant phrase weighting than all neighboring subsequences of words.
- a phrase when a phrase is identified as a phrase in step 620 it may be added to a phrase lexicon database such as content database 915 , optionally in combination with its phrase weighting.
- a larger phrase weighting has a greater significance, it may be determined which subsequences have a greater value than all of its neighbors via the comparison in step 615 . For example, as illustrated in the example of FIGS.
- each local maxima may be identified as a phrase.
- the overall maxima with the largest phrase weighting from all the subsequences of words may also be determined. For example, in the example of FIGS. 7 and 8 , the subsequence “new york stock exchange” is the overall maxima from the sequence of words.
- x i ⁇ j (x i , . . . , x j )
- FIG. 9 illustrates a block diagram of an example environment in which co-occurrence consistencies for positional word pairings in a sequence of words may be determined, phrase coherence of a sequence of words may be determined as a function of positional word pairings in the sequence of words, and/or one or more phrase boundaries in a sequence of words may be determined.
- the environment includes a communication network 901 that allows for communication between various components of the environment.
- the communication network 901 facilitates communication between the various components in the environment.
- the communication network may include the Internet, one or more intranets, and/or one or more bus subsystems.
- the communication network 901 may optionally utilize one or more standard communications technologies, protocols, and/or inter-process communication techniques.
- the co-occurrence relationship determination engine 910 may access a corpus of sequences of words from content database 915 or elsewhere and utilize such corpus to determine co-occurrence consistencies for positional word pairings utilizing techniques as described herein. In some implementations the co-occurrence relationship determination engine 910 may perform one or more of the steps of the method of FIG. 1 .
- the co-occurrence relationship engine 910 may be implemented in hardware, firmware, and/or software running on hardware. For example, the co-occurrence relationship engine 910 may be implemented in one or more computer servers.
- the phrase weighting engine 920 may access a sequence of words from content database 915 or elsewhere (e.g., from a user submitted query via the Internet) and analyze such sequence of words to contextually weight words in the sequence of words utilizing techniques as described herein.
- the phrase weighting engine 920 may perform one or more of the steps of the method of FIG. 3 .
- the phrase weighting engine 920 may be implemented in hardware, firmware, and/or software running on hardware.
- the phrase weighting engine 920 may be implemented in one or more computer servers.
- the phrase weighting engine 920 utilizes one or more identified co-occurrence consistencies to determine a phrase weighting of a sequence of words. In some implementations the co-occurrence consistencies may be obtained, either directly or indirectly (e.g., via content database 915 ) via the co-occurrence relationship determination engine 910 .
- the phrase segmentation engine 925 may access a sequences of words from content database 915 or elsewhere and identify one or more phrase boundaries in the sequence of words utilizing techniques as described herein. In some implementations the phrase segmentation engine 925 may perform one or more of the steps of the method of FIG. 6 .
- the phrase segmentation engine 925 may be implemented in hardware, firmware, and/or software running on hardware. For example, the phrase segmentation engine 925 may be implemented in one or more computer servers.
- the text mining application 905 may utilize the phrase weightings of sequences of words and/or the identification of phrases from sequences of words for one or more applications such as, for example, text classification, text clustering, information retrieval, and/or key phrase detection.
- the text mining application 905 may include a search engine.
- the phrase weighting engine 920 may determine the phrase weighting of all or portions of a submitted search query and/or the phrase segmentation engine 925 may identify phrases from the submitted search query. Identified phrases from the submitted search query may be utilized in formulating a more accurate search request. For example, one or more identified phrases in a submitted search query may be submitted in a search request to be searched as a cohesive sequence.
- the phrase weighting engine 920 and/or the phrase segmentation engine 925 may additionally or alternatively crawl web pages to determine phrases and/or phrase weightings in the webpages and thereby provide more meaningful results to a query submitted to the search engine. For example, identified phrases from web pages may be utilized to promote certain web pages in search results that contain a phrase from a search query over certain results that contain words of the phrase, but do not have those words in their respective relative positions in the phrase.
- the text mining application 905 may include a document clustering and classification engine.
- One or more phrases in a document may be identified via the phrase weighting engine 920 (e.g., determining the phrase weightings of potential phrases) and/or the phrase segmentation engine 925 (e.g., identifying phrases within the document).
- Such identified phrases may be utilized by the document clustering and classification engine to analyze phrase level similarity between documents in addition to or as an alternative to analyzing word level similarity between the documents.
- the text mining application 905 may include an indexing engine.
- One or more phrases in a document may be identified via phrase weighting engine 920 and/or the phrase segmentation engine 925 . Identified phrases may be utilized by the indexing engine to index documents based on phrases within the documents.
- the text mining application 905 may include a machine translation engine.
- One or more phrases in a first language may be identified via the phrase weighting engine 920 and/or the phrase segmentation engine 925 . Identified phrases in the first language may then be converted to corresponding phrases in a second language.
- the text mining application 905 may be implemented in hardware, firmware, and/or software running on hardware.
- the text mining application 905 may be implemented in one or more computer servers.
- co-occurrence relationship determination engine 910 the phrase weighting engine 920 , and the phrase segmentation engine 925 are all illustrated in FIG. 9 , it is understood that in some environments less than all of the co-occurrence relationship determination engine 910 , the phrase weighting engine 920 , and the phrase segmentation engine 925 may be provided. Also, for example, in some environments the text mining application 905 may be omitted. Also, for example, in some environments one or more of the co-occurrence relationship determination engine 910 , the phrase weighting engine 920 , and the phrase segmentation engine 925 may be combined.
- FIG. 10 is a block diagram of an example computer system 1010 .
- Computer system 1010 typically includes at least one processor 1014 which communicates with a number of peripheral devices via bus subsystem 1012 .
- peripheral devices may include a storage subsystem 1024 , including, for example, a memory subsystem 1026 and a file storage subsystem 1028 , user interface input devices 1022 , user interface output devices 1020 , and a network interface subsystem 1016 .
- the input and output devices allow user interaction with computer system 1010 .
- Network interface subsystem 1016 provides an interface to outside networks and is coupled to corresponding interface devices in other computer systems.
- User interface input devices 1022 may include a keyboard, pointing devices such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a touchscreen incorporated into the display, audio input devices such as voice recognition systems, microphones, and/or other types of input devices.
- pointing devices such as a mouse, trackball, touchpad, or graphics tablet
- audio input devices such as voice recognition systems, microphones, and/or other types of input devices.
- use of the term “input device” is intended to include all possible types of devices and ways to input information into computer system 1010 or onto a communication network.
- User interface output devices 1020 may include a display subsystem, a printer, a fax machine, or non-visual displays such as audio output devices.
- the display subsystem may include a cathode ray tube (CRT), a flat-panel device such as a liquid crystal display (LCD), a projection device, or some other mechanism for creating a visible image.
- the display subsystem may also provide non-visual display such as via audio output devices.
- output device is intended to include all possible types of devices and ways to output information from computer system 1010 to the user or to another machine or computer system.
- Storage subsystem 1024 stores programming and data constructs that provide the functionality of some or all of the modules described herein.
- the storage subsystem 1024 may include the logic to determine co-occurrence consistencies for positional word pairings of a plurality of sequences of words, to determine a phrase coherence of a sequence of words based on the co-occurrence consistencies for positional word pairings in the sequence of words, and/or to one or more phrase boundaries in a sequence of words according to one or more processes described herein.
- Memory 1026 used in the storage subsystem can include a number of memories including a main random access memory (RAM) 1030 for storage of instructions and data during program execution and a read only memory (ROM) 1032 in which fixed instructions are stored.
- a file storage subsystem 1028 can provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges.
- the modules implementing the functionality of certain implementations may be stored by file storage subsystem 1028 in the storage subsystem 1024 , or in other machines accessible by the processor(s) 1014 .
- Bus subsystem 1012 provides a mechanism for letting the various components and subsystems of computer system 1010 communicate with each other as intended. Although bus subsystem 1012 is shown schematically as a single bus, alternative implementations of the bus subsystem may use multiple busses.
- Computer system 1010 can be of varying types including a workstation, server, computing cluster, blade server, server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computer system 1010 depicted in FIG. 10 is intended only as a specific example for purposes of illustrating some implementations. Many other configurations of computer system 1010 are possible having more or fewer components than the computer system depicted in FIG. 10 .
- inventive implementations are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, inventive implementations may be practiced otherwise than as specifically described and claimed.
- inventive implementations of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein.
- a reference to “A and/or B”, when used in conjunction with open-ended language such as “comprising” can refer, in one implementation, to A only (optionally including elements other than B); in another implementation, to B only (optionally including elements other than A); in yet another implementation, to both A and B (optionally including other elements); etc.
- the phrase “at least one,” in reference to a list of one or more elements, should be understood to mean at least one element selected from any one or more of the elements in the list of elements, but not necessarily including at least one of each and every element specifically listed within the list of elements and not excluding any combinations of elements in the list of elements.
- This definition also allows that elements may optionally be present other than the elements specifically identified within the list of elements to which the phrase “at least one” refers, whether related or unrelated to those elements specifically identified.
- “at least one of A and B” can refer, in one implementation, to at least one, optionally including more than one, A, with no B present (and optionally including elements other than B); in another implementation, to at least one, optionally including more than one, B, with no A present (and optionally including elements other than A); in yet another implementation, to at least one, optionally including more than one, A, and at least one, optionally including more than one, B (and optionally including other elements); etc.
Abstract
Description
wherein θf is the relative importance of the field in which the sequence of words occurs and freq(xm|f) is the number of occurrences of the sequence of words in the corpus. In some implementations the weight of the mth sequence of words (wm) may additional and/or alternatively depend on the weight of the document from which the sequence of words is obtained.
ηk(να,νβ)=Σm=1 M w mΣi=1 L
-
- Where:
Φ={Φk[Øk(να,νβ)]:∀(να,νβ)εV} k=1 n
where the importance values are represented by It(xi|x,Φ)=1,∀i=1 . . . n.
to determine the centrality value of the word “york” in the sequence of words “new york stock exchange,” the following iterative equation may be used:
to determine the centrality value of the word “stock” in the sequence of words “new york stock exchange,” the following iterative equation may be used:
and to determine the centrality value of the word “exchange” in the sequence of words “new york stock exchange,” the following iterative equation may be used:
where the importance values are represented by
where the authority weights are represented by αt(xi)←1, ∀i=1 . . . n; where the importance values are represented by I(xi|x, Φ)=α∞(xi); and where the hub weights are updated from the authority weights and the authority weights are updated from the hub weights.
π(x)=min{I(x i |x,Φ)}
wherein the phrase weighting will be based on the minimum centrality value of the centrality values for the sequence of words. For example, in some implementations if any of the centrality values of the sequence of words is below a minimum threshold centrality value, then the phrase will be assigned a low phrase weighting in implementations where higher phrase weightings indicate greater likelihood of a phrase. For instance, for the phrase “new jersey stock exchange” the phrase weighting may be assigned a low phrase weighting due to a low centrality value of “jersey” even though an average based determination taking into account the centrality values of the other words “new, stock, exchange” may result in a relatively high phrase weighting.
Claims (25)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/544,323 US8892422B1 (en) | 2012-07-09 | 2012-07-09 | Phrase identification in a sequence of words |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/544,323 US8892422B1 (en) | 2012-07-09 | 2012-07-09 | Phrase identification in a sequence of words |
Publications (1)
Publication Number | Publication Date |
---|---|
US8892422B1 true US8892422B1 (en) | 2014-11-18 |
Family
ID=51870242
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/544,323 Active 2033-02-04 US8892422B1 (en) | 2012-07-09 | 2012-07-09 | Phrase identification in a sequence of words |
Country Status (1)
Country | Link |
---|---|
US (1) | US8892422B1 (en) |
Cited By (20)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20140122316A1 (en) * | 2003-07-22 | 2014-05-01 | Yahoo! Inc. | Concept valuation in a term-based concept market |
WO2017013667A1 (en) | 2015-07-17 | 2017-01-26 | Giridhari Devanathan | Method for product search using the user-weighted, attribute-based, sort-ordering and system thereof |
WO2017051425A1 (en) | 2015-09-23 | 2017-03-30 | Devanathan Giridhari | A computer-implemented method and system for analyzing and evaluating user reviews |
WO2017090051A1 (en) | 2015-11-27 | 2017-06-01 | Giridhari Devanathan | A method for text classification and feature selection using class vectors and the system thereof |
US9754020B1 (en) * | 2014-03-06 | 2017-09-05 | National Security Agency | Method and device for measuring word pair relevancy |
CN107633000A (en) * | 2017-08-03 | 2018-01-26 | 北京微智信业科技有限公司 | File classification method based on tfidf algorithms and related term weight amendment |
US20180089309A1 (en) * | 2016-09-28 | 2018-03-29 | Linkedln Corporation | Term set expansion using textual segments |
US20180113938A1 (en) * | 2016-10-24 | 2018-04-26 | Ebay Inc. | Word embedding with generalized context for internet search queries |
US10242090B1 (en) | 2014-03-06 | 2019-03-26 | The United States Of America As Represented By The Director, National Security Agency | Method and device for measuring relevancy of a document to a keyword(s) |
US10248715B2 (en) * | 2014-04-01 | 2019-04-02 | Tencent Technology (Shenzhen) Company Limited | Media content recommendation method and apparatus |
US11010564B2 (en) | 2019-02-05 | 2021-05-18 | International Business Machines Corporation | Method for fine-grained affective states understanding and prediction |
US11076219B2 (en) * | 2019-04-12 | 2021-07-27 | Bose Corporation | Automated control of noise reduction or noise masking |
CN113191145A (en) * | 2021-05-21 | 2021-07-30 | 百度在线网络技术（北京）有限公司 | Keyword processing method and device, electronic equipment and medium |
US11138389B2 (en) * | 2016-11-17 | 2021-10-05 | Goldman Sachs & Co. LLC | System and method for coupled detection of syntax and semantics for natural language understanding and generation |
US20210365723A1 (en) * | 2020-05-22 | 2021-11-25 | Microsoft Technology Licensing, Llc | Position Masking for Transformer Models |
CN113779259A (en) * | 2021-11-15 | 2021-12-10 | 太平金融科技服务（上海）有限公司 | Text classification method and device, computer equipment and storage medium |
US20220358287A1 (en) * | 2021-05-10 | 2022-11-10 | International Business Machines Corporation | Text mining based on document structure information extraction |
US11580303B2 (en) * | 2019-12-13 | 2023-02-14 | Beijing Xiaomi Mobile Software Co., Ltd. | Method and device for keyword extraction and storage medium |
US11586820B2 (en) * | 2020-08-10 | 2023-02-21 | Ebay Inc. | Techniques for enhancing the quality of human annotation |
CN116436987A (en) * | 2023-06-12 | 2023-07-14 | 深圳舜昌自动化控制技术有限公司 | IO-Link master station data message transmission processing method and system |
Citations (12)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5696962A (en) * | 1993-06-24 | 1997-12-09 | Xerox Corporation | Method for computerized information retrieval using shallow linguistic analysis |
US20020116170A1 (en) * | 2000-12-15 | 2002-08-22 | Corman Steven R. | Method for mining, mapping and managing organizational knowledge from text and conversation |
US20040249637A1 (en) * | 2003-06-04 | 2004-12-09 | Aurilab, Llc | Detecting repeated phrases and inference of dialogue models |
US20070100680A1 (en) * | 2005-10-21 | 2007-05-03 | Shailesh Kumar | Method and apparatus for retail data mining using pair-wise co-occurrence consistency |
US7567959B2 (en) | 2004-07-26 | 2009-07-28 | Google Inc. | Multiple index based information retrieval system |
US7603345B2 (en) | 2004-07-26 | 2009-10-13 | Google Inc. | Detecting spam documents in a phrase based information retrieval system |
US7693813B1 (en) | 2007-03-30 | 2010-04-06 | Google Inc. | Index server architecture using tiered and sharded phrase posting lists |
US20110295903A1 (en) * | 2010-05-28 | 2011-12-01 | Drexel University | System and method for automatically generating systematic reviews of a scientific field |
US8117223B2 (en) | 2007-09-07 | 2012-02-14 | Google Inc. | Integrating external related phrase information into a phrase-based indexing information retrieval system |
US8166045B1 (en) | 2007-03-30 | 2012-04-24 | Google Inc. | Phrase extraction using subphrase scoring |
US8176067B1 (en) * | 2010-02-24 | 2012-05-08 | A9.Com, Inc. | Fixed phrase detection for search |
US8521745B2 (en) * | 2006-06-05 | 2013-08-27 | Accenture Global Services Limited | Extraction of attributes and values from natural language documents |
-
2012
- 2012-07-09 US US13/544,323 patent/US8892422B1/en active Active
Patent Citations (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5696962A (en) * | 1993-06-24 | 1997-12-09 | Xerox Corporation | Method for computerized information retrieval using shallow linguistic analysis |
US20020116170A1 (en) * | 2000-12-15 | 2002-08-22 | Corman Steven R. | Method for mining, mapping and managing organizational knowledge from text and conversation |
US20040249637A1 (en) * | 2003-06-04 | 2004-12-09 | Aurilab, Llc | Detecting repeated phrases and inference of dialogue models |
US7567959B2 (en) | 2004-07-26 | 2009-07-28 | Google Inc. | Multiple index based information retrieval system |
US7603345B2 (en) | 2004-07-26 | 2009-10-13 | Google Inc. | Detecting spam documents in a phrase based information retrieval system |
US20070100680A1 (en) * | 2005-10-21 | 2007-05-03 | Shailesh Kumar | Method and apparatus for retail data mining using pair-wise co-occurrence consistency |
US8521745B2 (en) * | 2006-06-05 | 2013-08-27 | Accenture Global Services Limited | Extraction of attributes and values from natural language documents |
US7693813B1 (en) | 2007-03-30 | 2010-04-06 | Google Inc. | Index server architecture using tiered and sharded phrase posting lists |
US8166045B1 (en) | 2007-03-30 | 2012-04-24 | Google Inc. | Phrase extraction using subphrase scoring |
US8402033B1 (en) | 2007-03-30 | 2013-03-19 | Google Inc. | Phrase extraction using subphrase scoring |
US8117223B2 (en) | 2007-09-07 | 2012-02-14 | Google Inc. | Integrating external related phrase information into a phrase-based indexing information retrieval system |
US8176067B1 (en) * | 2010-02-24 | 2012-05-08 | A9.Com, Inc. | Fixed phrase detection for search |
US20110295903A1 (en) * | 2010-05-28 | 2011-12-01 | Drexel University | System and method for automatically generating systematic reviews of a scientific field |
Cited By (27)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20140122316A1 (en) * | 2003-07-22 | 2014-05-01 | Yahoo! Inc. | Concept valuation in a term-based concept market |
US9754020B1 (en) * | 2014-03-06 | 2017-09-05 | National Security Agency | Method and device for measuring word pair relevancy |
US10242090B1 (en) | 2014-03-06 | 2019-03-26 | The United States Of America As Represented By The Director, National Security Agency | Method and device for measuring relevancy of a document to a keyword(s) |
US10248715B2 (en) * | 2014-04-01 | 2019-04-02 | Tencent Technology (Shenzhen) Company Limited | Media content recommendation method and apparatus |
WO2017013667A1 (en) | 2015-07-17 | 2017-01-26 | Giridhari Devanathan | Method for product search using the user-weighted, attribute-based, sort-ordering and system thereof |
WO2017051425A1 (en) | 2015-09-23 | 2017-03-30 | Devanathan Giridhari | A computer-implemented method and system for analyzing and evaluating user reviews |
WO2017090051A1 (en) | 2015-11-27 | 2017-06-01 | Giridhari Devanathan | A method for text classification and feature selection using class vectors and the system thereof |
US20180089309A1 (en) * | 2016-09-28 | 2018-03-29 | Linkedln Corporation | Term set expansion using textual segments |
US20180113938A1 (en) * | 2016-10-24 | 2018-04-26 | Ebay Inc. | Word embedding with generalized context for internet search queries |
AU2021269302B2 (en) * | 2016-11-17 | 2021-12-23 | Goldman Sachs & Co. LLC | System and method for coupled detection of syntax and semantics for natural language understanding and generation |
AU2021269302C1 (en) * | 2016-11-17 | 2022-03-31 | Goldman Sachs & Co. LLC | System and method for coupled detection of syntax and semantics for natural language understanding and generation |
US11138389B2 (en) * | 2016-11-17 | 2021-10-05 | Goldman Sachs & Co. LLC | System and method for coupled detection of syntax and semantics for natural language understanding and generation |
CN107633000B (en) * | 2017-08-03 | 2020-08-04 | 北京微智信业科技有限公司 | Text classification method based on tfidf algorithm and related word weight correction |
CN107633000A (en) * | 2017-08-03 | 2018-01-26 | 北京微智信业科技有限公司 | File classification method based on tfidf algorithms and related term weight amendment |
US11132511B2 (en) * | 2019-02-05 | 2021-09-28 | International Business Machines Corporation | System for fine-grained affective states understanding and prediction |
US11010564B2 (en) | 2019-02-05 | 2021-05-18 | International Business Machines Corporation | Method for fine-grained affective states understanding and prediction |
US11076219B2 (en) * | 2019-04-12 | 2021-07-27 | Bose Corporation | Automated control of noise reduction or noise masking |
US11580303B2 (en) * | 2019-12-13 | 2023-02-14 | Beijing Xiaomi Mobile Software Co., Ltd. | Method and device for keyword extraction and storage medium |
US20210365723A1 (en) * | 2020-05-22 | 2021-11-25 | Microsoft Technology Licensing, Llc | Position Masking for Transformer Models |
US11893469B2 (en) * | 2020-05-22 | 2024-02-06 | Microsoft Technology Licensing, Llc | Position masking for transformer models |
US11586820B2 (en) * | 2020-08-10 | 2023-02-21 | Ebay Inc. | Techniques for enhancing the quality of human annotation |
US20220358287A1 (en) * | 2021-05-10 | 2022-11-10 | International Business Machines Corporation | Text mining based on document structure information extraction |
CN113191145A (en) * | 2021-05-21 | 2021-07-30 | 百度在线网络技术（北京）有限公司 | Keyword processing method and device, electronic equipment and medium |
CN113191145B (en) * | 2021-05-21 | 2023-08-11 | 百度在线网络技术（北京）有限公司 | Keyword processing method and device, electronic equipment and medium |
CN113779259A (en) * | 2021-11-15 | 2021-12-10 | 太平金融科技服务（上海）有限公司 | Text classification method and device, computer equipment and storage medium |
CN116436987A (en) * | 2023-06-12 | 2023-07-14 | 深圳舜昌自动化控制技术有限公司 | IO-Link master station data message transmission processing method and system |
CN116436987B (en) * | 2023-06-12 | 2023-08-22 | 深圳舜昌自动化控制技术有限公司 | IO-Link master station data message transmission processing method and system |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8892422B1 (en) | Phrase identification in a sequence of words | |
US9201876B1 (en) | Contextual weighting of words in a word grouping | |
US8051080B2 (en) | Contextual ranking of keywords using click data | |
US8819047B2 (en) | Fact verification engine | |
US8266155B2 (en) | Systems and methods of displaying and re-using document chunks in a document development application | |
US8751484B2 (en) | Systems and methods of identifying chunks within multiple documents | |
US7933896B2 (en) | Systems and methods of searching a document for relevant chunks in response to a search request | |
US20120117092A1 (en) | Systems And Methods Regarding Keyword Extraction | |
US20110119262A1 (en) | Method and System for Grouping Chunks Extracted from A Document, Highlighting the Location of A Document Chunk Within A Document, and Ranking Hyperlinks Within A Document | |
US8001140B2 (en) | Systems and methods of refining a search query based on user-specified search keywords | |
US8352485B2 (en) | Systems and methods of displaying document chunks in response to a search request | |
US20120284275A1 (en) | Utilizing offline clusters for realtime clustering of search results | |
US8359533B2 (en) | Systems and methods of performing a text replacement within multiple documents | |
US20100198802A1 (en) | System and method for optimizing search objects submitted to a data resource | |
US9129036B2 (en) | Systems and methods of identifying chunks within inter-related documents | |
US8126880B2 (en) | Systems and methods of adaptively screening matching chunks within documents | |
Bendersky et al. | Joint annotation of search queries | |
US8924421B2 (en) | Systems and methods of refining chunks identified within multiple documents | |
Zhang et al. | Mutual-reinforcement document summarization using embedded graph based sentence clustering for storytelling | |
Qumsiyeh et al. | Enhancing web search by using query-based clusters and multi-document summaries | |
Das et al. | Opinion summarization in Bengali: a theme network model | |
Durao et al. | Expanding user’s query with tag-neighbors for effective medical information retrieval | |
US8001162B2 (en) | Systems and methods of pipelining multiple document node streams through a query processor | |
Rananavare et al. | Automatic summarization for agriculture article | |
Jia et al. | Big Data Text Automation on Small Machines |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:KUMAR, SHAILESH;REEL/FRAME:028515/0606Effective date: 20120708 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044277/0001Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551)Year of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |