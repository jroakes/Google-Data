CN113194260A - Method and device for stabilizing video - Google Patents
Method and device for stabilizing video Download PDFInfo
- Publication number
- CN113194260A CN113194260A CN202110599340.0A CN202110599340A CN113194260A CN 113194260 A CN113194260 A CN 113194260A CN 202110599340 A CN202110599340 A CN 202110599340A CN 113194260 A CN113194260 A CN 113194260A
- Authority
- CN
- China
- Prior art keywords
- particular frame
- frames
- ois
- frame
- series
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 238000000034 method Methods 0.000 title claims abstract description 94
- 230000000087 stabilizing effect Effects 0.000 title description 2
- 230000009466 transformation Effects 0.000 claims abstract description 82
- 230000006641 stabilisation Effects 0.000 claims abstract description 34
- 238000011105 stabilization Methods 0.000 claims abstract description 34
- 230000003287 optical effect Effects 0.000 claims abstract description 13
- 230000033001 locomotion Effects 0.000 claims description 140
- 230000007704 transition Effects 0.000 claims description 60
- 239000011159 matrix material Substances 0.000 claims description 49
- 238000005259 measurement Methods 0.000 claims description 27
- 238000000844 transformation Methods 0.000 claims description 6
- DMSMPAJRVJJAGA-UHFFFAOYSA-N benzo[d]isothiazol-3-one Chemical compound C1=CC=C2C(=O)NSC2=C1 DMSMPAJRVJJAGA-UHFFFAOYSA-N 0.000 claims 4
- 238000004590 computer program Methods 0.000 abstract description 13
- 230000008569 process Effects 0.000 description 36
- 238000012545 processing Methods 0.000 description 33
- 238000013507 mapping Methods 0.000 description 15
- 238000001914 filtration Methods 0.000 description 13
- 230000008859 change Effects 0.000 description 7
- 230000000694 effects Effects 0.000 description 7
- 238000004458 analytical method Methods 0.000 description 6
- 230000006870 function Effects 0.000 description 6
- 238000004891 communication Methods 0.000 description 5
- 230000007246 mechanism Effects 0.000 description 5
- 238000004091 panning Methods 0.000 description 5
- 238000013500 data storage Methods 0.000 description 4
- 238000010586 diagram Methods 0.000 description 4
- 230000029058 respiratory gaseous exchange Effects 0.000 description 4
- 238000005096 rolling process Methods 0.000 description 4
- 230000009286 beneficial effect Effects 0.000 description 3
- 230000008901 benefit Effects 0.000 description 3
- 238000006243 chemical reaction Methods 0.000 description 3
- 238000012417 linear regression Methods 0.000 description 3
- 238000013515 script Methods 0.000 description 3
- 238000013459 approach Methods 0.000 description 2
- 238000004364 calculation method Methods 0.000 description 2
- 238000013481 data capture Methods 0.000 description 2
- 238000006073 displacement reaction Methods 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 238000010801 machine learning Methods 0.000 description 2
- 230000008447 perception Effects 0.000 description 2
- 230000000644 propagated effect Effects 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000005540 biological transmission Effects 0.000 description 1
- 230000015572 biosynthetic process Effects 0.000 description 1
- 238000004422 calculation algorithm Methods 0.000 description 1
- 230000002301 combined effect Effects 0.000 description 1
- 230000007547 defect Effects 0.000 description 1
- 230000000593 degrading effect Effects 0.000 description 1
- 230000003111 delayed effect Effects 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 230000004927 fusion Effects 0.000 description 1
- 238000010191 image analysis Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 230000001360 synchronised effect Effects 0.000 description 1
- 238000003786 synthesis reaction Methods 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G06T5/73—
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/682—Vibration or motion blur correction
- H04N23/685—Vibration or motion blur correction performed by mechanical compensation
- H04N23/687—Vibration or motion blur correction performed by mechanical compensation by shifting the lens or sensor position
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T5/00—Image enhancement or restoration
- G06T5/50—Image enhancement or restoration by the use of more than one image, e.g. averaging, subtraction
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/20—Analysis of motion
- G06T7/246—Analysis of motion using feature-based methods, e.g. the tracking of corners or segments
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/681—Motion detection
- H04N23/6811—Motion detection based on the image signal
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/681—Motion detection
- H04N23/6812—Motion detection based on additional sensors, e.g. acceleration sensors
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/682—Vibration or motion blur correction
- H04N23/683—Vibration or motion blur correction performed by a processor, e.g. controlling the readout of an image memory
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10016—Video; Image sequence
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20172—Image enhancement details
- G06T2207/20201—Motion blur correction
Abstract
A method and apparatus, including a computer program stored on a computer readable storage medium, for video stabilization. In some embodiments, the computer system obtains frames of video captured by the recording device using an Optical Image Stabilization (OIS) system. The computing system receives (i) OIS location data indicating a location of the OIS system during an acquisition frame and (ii) device location data indicating a location of the recording device during the acquisition frame. The computing system determines a first transformation for the particular frame based on the OIS location data for the particular frame and the device location data for the particular frame. The computing system determines a second transformation for a particular frame based on the first transformation and a location of the recording device that occurred after the particular frame was captured. The computing system uses the second transformation to generate a stable version of the particular frame.
Description
The present case is patent application number: 201880030074.6, filing date: 31/7/2018, patent name: a method and apparatus for video stabilization are disclosed.
Background
Typically, video recording is performed using cameras, cell phones, tablet computers and other recording devices. In many cases, videos recorded using handheld recording devices can move unexpectedly, thereby degrading video quality. One of the main reasons for movement is handshaking, e.g. unintentional or natural movement when the user is holding the recording device affects the video quality. Shaking and other movements at non-uniform speeds can also degrade video quality. These unwanted movements may cause blurs, jitter and other visible defects in the recorded video lens.
Unless jitter is compensated for, for example, by an image stabilization mechanism, jitter of the recording device may cause the video to likewise jitter. Optical Image Stabilization (OIS) can reduce jitter and blur in video by mechanically moving components of the recording device, such as a lens or image sensor. However, the operation of OIS systems sometimes introduces artifacts, and OIS systems are typically limited in the amount of motion they can compensate for. Similarly, the OIS system may erroneously compensate for intentional movement of the recording device, such as a user's pan. Electronic Image Stabilization (EIS) may also reduce jitter present in video, for example, by aligning and shifting image frames based on image analysis. However, not all EIS techniques are reliable, as sometimes object motion, noise, and camera motion blur in captured video may confound processing.
Disclosure of Invention
The video stabilization system may use a combination of OIS and EIS to stabilize the video. The OIS module may stabilize the frame during video capture and may capture position information (e.g., lens shift information) from the OIS module and use it in EIS processing. By using the OIS lens shift position, the stabilization process can correct for distortions caused by the OIS system and avoid making adjustments or motions that have already been compensated for by the OIS system.
During video capture, the system may also obtain position data of the recording device from a gyroscope of the recording device. The system may use the device location data and the OIS location data during EIS processing to account for the actual pose of the recording device during capture and the adjustments to the camera view caused by OIS. As a result, the EIS process may compensate for motion of the entire recording apparatus (e.g., camera shake or other extrinsic movement of the recording apparatus) and image shift due to operation of the OIS module (e.g., internal lens movement). This may allow the system to more accurately determine the relationship between the captured image data and the real-world scene to more accurately generate a stable output frame from the input frame.
Video stabilization systems may use various other techniques to provide high quality stabilization. For example, when processing a frame, the system may use future frames (e.g., later captured frames or earlier frames) to detect possible large motion and begin compensating for it even before large motion occurs. This may help to avoid abrupt motion changes in a series of frames. Even when the stabilization operation is performed in substantially real time by video capture, analysis of future frames can be performed by implementing a small delay between frame capture and EIS processing to allow for the use of an appropriate number of later captured frames to stabilize previously captured frames. As another example, the system may correct rolling shutter distortion by considering different camera positions when capturing different regions of a frame. Different image transformations may be applied to different regions of the captured frame, e.g., different scan lines, to properly align the stabilized output frame with the different regions. As another example, the system may detect camera lens movement due to focus changes and may eliminate perspective shifts caused by effective focal length shifts that typically accompany focus shifts (e.g., focus breathing). As another example, the system may evaluate camera motion blur and align a motion path estimated using EIS with the movement indicated by the blur.
In one general aspect, a method includes: receiving, by a computing system, a series of frames of a video captured by a recording device using an Optical Image Stabilization (OIS) system; and receiving, by the computing system, (i) OIS location data indicative of a location of the OIS system during the capturing of the series of frames and (ii) device location data indicative of a location of the recording device during the capturing of the series of frames; determining, by the computing system, a first transformation for a particular frame of the series of frames, the first transformation determined based on the OIS location data for the particular frame and the device location data for the particular frame; determining, by the computing system, a second transformation for the particular frame based on the first transformation and a location of the recording device determined for one or more frames in the series of frames captured after the particular frame based on the device location data; and generating, by the computing system, a stable version of the particular frame using the second transformation.
Implementations may include one or more of the following features. For example, the recording device and the computing system may be integrated into a single device.
In some embodiments, the computing system and the recording device are both part of a mobile phone.
In some embodiments, receiving OIS position data includes receiving lens displacement data indicative of a position of a movable lens of the OIS system, and receiving device position data includes receiving orientation or movement data from a gyroscope or accelerometer of the recording device.
In some embodiments, the OIS position data indicates a plurality of OIS position readings for each frame in a series of frames. The device location data indicates a plurality of device location measurements for each frame in a series of frames.
In some embodiments, the method includes determining a lens focus position corresponding to the particular frame. Determining the first transformation of the particular frame using the lens focus position.
In some embodiments, the method includes determining the image offset for a particular frame based on a position of a movable element of the OIS system indicated by the OIS position data. A first transform for a particular frame is determined using the image offset.
In some implementations, the method includes: determining a set of camera positions that occur over a set of multiple frames based on the OIS position data and the device position data, the set of multiple frames including one or more frames that precede the particular frame and one or more frames that follow the particular frame; and applying a filter to the set of camera positions, wherein the second transformation is determined based on the camera positions determined from applying the filter to the set of camera positions.
In some embodiments, the method comprises: determining an intermediate camera position for a particular frame from an output of applying the filter to a set of camera positions; comparing the intermediate camera position to a virtual camera position of a frame immediately preceding the particular frame. The second transformation is determined based on a camera position determined based on a comparison of the intermediate camera position and the virtual camera position of the frame immediately preceding the particular frame.
In some embodiments, the method includes generating data characterizing an amount of motion blur in the particular frame. Determining the second transformation based on a camera position determined using the data characterizing an amount of motion blur in the particular frame.
In some implementations, generating the data characterizing motion blur in the particular frame includes estimating a motion blur kernel based on a projection matrix determined for a start of an exposure of the particular frame and a projection matrix for an end of an exposure of the particular frame.
In some embodiments, the method includes determining a camera position representative of a position occurring during uniform movement of the recording device over a set of multiple frames. Determining the second transformation based on the camera position representing a position occurring during a uniform movement of the camera.
In some implementations, the camera position is generated by applying a stable filter to data corresponding to a set of frames including one or more frames preceding the particular frame and one or more frames following the particular frame.
In some embodiments, the method comprises: determining a probability of uniform movement of the recording device based on an analysis of camera positions over a set of frames, the set of frames including one or more frames preceding the particular frame and one or more frames following the particular frame; and fusing (i) the camera position representing a position occurring during a uniform motion of the camera with (ii) a second camera position of the particular frame, wherein the camera positions are fused according to the determined probability, wherein the second transformation is determined based on a result of fusing the camera position and the second camera position.
In some implementations, the probability of uniform movement is determined using linear regression over a set of camera positions subsampled from camera positions corresponding to the set of frames. Determining the second camera position using data indicative of an amount of blur in the particular frame.
In some implementations, determining the second transformation for the particular frame includes: for each reference frame in a reference set comprising the particular frame and one or more frames occurring after the particular frame, determining a projection of image data for the frame based on the device location data and OIS location data corresponding to the frame; determining a transformation for the particular frame, the transformation mapping projected image data for the particular frame to an output frame; for each reference frame in the reference set, determining whether the determined transformation for the particular frame, when applied to the reference frame, defines each pixel of the output frame; identifying at least one reference frame for which application of the determined transformation to the projection image data does not define each pixel of the output frame; identifying a minimum degree of transition (transition measure) between the camera position determined for the particular frame and the camera position determined for the at least one reference frame; and determining a second transformation for the camera position determined using the identified minimum degree of transition, the second transformation being determined such that when the second transformation is applied to the projection of each reference frame in the reference set, the second transformation maps image data to each pixel of an output frame.
In some embodiments, the method comprises: determining a current degree of transition for the particular frame, the current degree of transition indicating a difference between a camera position determined for a frame immediately preceding the particular frame and a camera position determined for the particular frame; determining a future degree of transition for each of one or more future frames occurring after the particular frame, wherein each future degree of transition indicates a difference between the camera position determined for the particular frame and the camera position determined for one of the future frames; determining a maximum degree of transition from the current degree of transition and the future degree of transition, the maximum degree of transition indicating a maximum difference in camera position; and determining a camera position based on the identified maximum degree of transition; wherein the second mathematical transformation is determined using the determined camera position.
In some implementations, determining the camera position based on the identified maximum degree of transition includes: determining that the maximum degree of transition corresponds to a particular future frame of the one or more future frames; adjusting a camera position of the particular frame toward the camera position of the particular future frame, wherein the camera position of the particular frame is adjusted by an amount specified in accordance with an amount of time between capturing the particular frame and capturing the particular future frame.
In some embodiments, the method comprises, for each appropriate subset of scan lines of a particular said frame, determining a mapping of virtual camera positions of said scan lines relative to said output frame or said scan lines; and determining a mapping for each scan line of the particular frame by interpolating between the mappings determined for the appropriate subset of scan lines, or interpolating between the virtual camera positions determined for the appropriate subset of scan lines.
In some embodiments, the first transformation is a first projection matrix that maps the particular frame to an image; the second transform is a second projection matrix for projecting the image in the intermediate space to the stabilized version of the particular frame.
Other embodiments of these aspects include corresponding systems, apparatus, and computer programs configured to perform the actions of the methods encoded on non-transitory machine-readable storage devices. The system of one or more devices may be configured by means of software, firmware, hardware or a combination thereof installed on the system which in operation causes the system to act. One or more computer programs may be configured by having instructions which, when executed by a data processing apparatus, cause the apparatus to perform actions.
Various embodiments may provide one or more of the following advantages. For example, the quality of a video segment may be improved by reducing jitter, blur, and other movement. By using the OIS and EIS techniques in combination, a stable video may be smoother than using the two techniques alone. OIS can significantly reduce camera motion blur in individual frames, as well as reduce motion in multiple frames. EIS may use data from a moving gyro sensor tracking a recording device to further reduce apparent motion in a series of video frames. The EIS process may also use OIS position data (e.g., lens shift information) to improve the accuracy of mapping image data to stable output frames. The system may use the OIS position data together with the device gyroscope sensors to estimate the camera position. This may allow EIS processing to avoid correcting device motion that has been counteracted by the OIS module, and also allow EIS to correct for unwanted OIS movements (e.g., movements that may interfere with panning, introduce distortion, or not correspond to device motion).
In some embodiments, the system may use the previous frame to reduce the effect of large or sudden movements to increase the accuracy of EIS processing. Based on the analysis of subsequent frames, the video frames may be adjusted to gradually prepare for large changes in movement before it occurs. The system may evaluate whether a uniform motion (e.g., pan) is occurring and adjust the video frames to reflect the uniform motion. Rolling shutter distortion may be reduced or eliminated. The change in viewing angle due to focused breathing can be reduced or eliminated.
In some embodiments, the system may provide real-time or substantially real-time processing while capturing video. For example, EIS processing may occur when recording a video clip, and thus the recorded clip may apply the EIS processing. Furthermore, the techniques discussed may be performed in a computationally feasible and power efficient manner on a battery-powered mobile electronic device (e.g., a mobile phone).
The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
Fig. 1 is a diagram illustrating an example of a system for video stabilization.
Fig. 2 is a diagram showing an example of data for video stabilization.
Fig. 3A-3C show a flow chart illustrating an example of a process for video stabilization.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
Fig. 1 is a diagram illustrating an example of a system 100 for video stabilization. The system 100 includes a recording device 102, the recording device 102 including a camera module 110 having an OIS module 115. Recording device 102 captures video frames while using OIS module 115 to at least partially counteract movement of recording device 102 during frame capture. The recording device 102 also includes one or more device location sensors 120, one or more data storage devices 130, and an EIS module 155.
The recording device 102 may be any of various types including a camera module, such as a mobile phone, a tablet computer, a camera, and the like. The recording device 102 may include a computing system for performing the operations of the EIS module 155, which may be implemented in software, hardware, or some combination thereof. For example, recording device 102 may include various processing components, such as one or more processors, one or more data storage devices storing executable instructions, memory, input/output components, and so forth. The processor performing EIS processing may include a general purpose processor (e.g., the main CPU of a mobile phone or other device), a graphics processor, a coprocessor, an image processor, a fixed function EIS processor, or any combination thereof.
The EIS module 155 uses the position data from both the device position sensor 120 and the OIS module 115 to stabilize the video captured by the recording device. For example, the location data from the OIS module 115 may be used to determine an offset from an expected camera view to be inferred from the device location data, the offset representing the effect of OIS movement. This enables the EIS module 155 to estimate a valid camera position that reflects the actual view of the image sensor even if the OIS module 115 changes the camera view of the scene relative to the device position. Together with other features discussed herein, these techniques may enable the recording device 102 to efficiently use both OIS and EIS processing, and to achieve the benefits of both techniques.
In general, OIS can be very effective in reducing blur within a single frame due to camera shake, and OIS can be effective in reducing apparent motion over a series of frames. However, the use of OIS alone is often subject to various limitations. The OIS modules may have limitations on the speed at which they respond to motion and the magnitude of the motion that they can compensate for. Furthermore, the operation of the OIS module sometimes results in distortion (e.g., wobbling video) and may erroneously cancel the desired movement (e.g., pan). The EIS module 155 may use location data describing the internal movement of the OIS module to mitigate the effects of these limitations.
Because the OIS module 115 attempts to compensate for movement of the recording device, device motion alone may not indicate the true camera view used during video capture. If the EIS process attempts to compensate for motion based only on device motion, the EIS process may attempt to correct for the motion that has been compensated for by the OIS system. Furthermore, OIS typically only partially removes the effects of device movement, and the amount of compensation may vary from one frame to the next. To provide high quality stabilization, in some embodiments, the EIS module 155 uses the OIS position data along with the device-level position data to vary the amount of stabilization applied to each frame, and even to the individual scan lines of a frame. This process may provide effective stabilization and reduce or eliminate distortion in the video segment. For example, variations in OIS lens shift position when capturing frames may cause distortion, especially when used in conjunction with rolling shutters common in many camera modules. With information about OIS lens shift at different times during frame capture, the EIS module 155 may estimate the lens position and correct the image when capturing different portions of the frame. The EIS module 155 may also compensate to reduce the effects of disturbing the pan or otherwise undesirable OIS lens shift.
Another way in which the EIS module 155 may enhance video is by analyzing data for subsequently captured frames. To process a particular frame, the EIS processing module may evaluate a set of camera positions in a time window that includes a time at which one or more future frames were captured. The information about future frames and corresponding locations may be used in a number of ways. First, the EIS module 155 may apply a filter to a set of camera positions to smooth the motion pattern for defining the image transformation for the changed frame. Second, the EIS module 155 may use a set of camera positions to assess the likelihood of there being a constant motion (e.g., pan) or attempting to make a constant motion (e.g., pan), and then adjust frames of that motion at a constant velocity when possible. Third, the EIS module 155 may evaluate the camera position for the frame relative to future camera positions and adjust for future large movements. For example, if a large, fast motion is identified for a future frame, the EIS module 155 may begin adjusting the content of the frame before motion begins. The EIS module 155 may spread motion over a larger frame rather than making a large apparent motion over several frames, such that incremental image shifts occur during earlier frames and gradually spread motion over more frames.
The EIS module 155 performs region synthesis of the output frame, for example, changing the transform applied to each scan line of the image frame. This allows the system to correct for rolling shutter distortion, movement of the OIS module 115, and various device motions that occur during the capture duration of a single frame.
Still referring to fig. 1, recording device 102 may be any suitable device having a camera that captures video data, such as a camera, cell phone, smart phone, tablet computer, wearable computer, or other device. Although the example of fig. 1 shows a single device capturing video and processing the video, the functionality may alternatively be spread across multiple devices or systems. For example, the first device may capture video frames and also record location data and other parameters as metadata. The first device may provide the video frames and metadata to a second device, such as a local computing system or a remote server, which may perform EIS processing as discussed herein.
The camera module 110 may include a lens element, an image sensor, a sensor reading circuit, and other components. The OIS module 115 may include a sensor, a movable element, a processor, and a drive mechanism for moving the movable element. The movable element is located in the optical path of the camera module 110. For example, the movable element may be a reflective or refractive element, such as a lens, mirror, prism. In some implementations, the movable element is an image sensor of the camera module 110. The sensors may include one or more gyroscopes or other sensors that detect movement. The processor determines the amount and direction of movement of the movable element required to compensate for the movement indicated by the sensor and then instructs the drive mechanism to move the movable element.
In some cases, position data from the gyro sensors of the OIS module 115 may be captured and stored in addition to or in lieu of using the separate position sensors 120 of the recording device 102. However, it may be beneficial for the recording device 102 to use a gyroscope sensor having different characteristics than the OIS sensor. For example, a gyroscope sensor for the recording device 102 may provide measurements at a rate of approximately 400Hz with a reasonable range of rotation of greater than 100 degrees per second. Typical gyro sensors of OIS modules may provide measurements at different rates and ranges (e.g., 5000 measurements per second or higher) with a reasonable range of rotation of about 10 degrees per second compared to device-level sensors. In some embodiments, it may be beneficial to have a larger reasonable range of rotation for the device-level sensor (e.g., to describe larger movements), and it may also be beneficial for the OIS module sensor to measure more frequently (e.g., to detect small changes or high frequency patterns). Thus both types of data may be used together to determine the location of the recording device 102.
The recording device 102 includes one or more data storage devices 130, the data storage devices 130 storing information characterizing the camera module 110 and the frame capture process. For example, the stored data may include calibration data 132, the calibration data 132 indicating a relationship between the position of the OIS module 115 and the resulting offset present in the image data. Similarly, calibration data 132 may indicate the correspondence of camera module lens focal positions and the effective focal lengths of those focal positions, allowing the system to account for focused breathing. The stored data may include scan pattern data 134, which scan pattern data 134 may indicate read attributes of the image sensor in the camera module 110. For example, the scan pattern data 134 may indicate, for example, a scan direction (e.g., scan lines read from top to bottom), whether the scan lines are read individually or in groups, and so forth.
During video capture, the camera module 110, the OIS module 115 and the device position sensor 120 each provide information regarding the video capture process. The camera module 110 provides video frame data 142, e.g., a sequence of video image frames. The camera module 110 also provides frame exposure data 144, which may include, for each frame captured, an indication of an exposure duration and a reference time (e.g., a start time or an end time of the exposure) indicating that the exposure occurred. The camera module 110 also provides lens focus position data 146, the lens focus position data 146 indicating a lens focus position for each captured frame.
The OIS module 115 provides OIS position data 148 indicating the position of the movable element of the OIS module 115 at various times during video capture. For example, when the movable element is a movable lens that is displaced to compensate for motion, the OIS module 115 may provide a lens displacement reading that specifies the current position of the movable lens. The recording device 102 may record the lens shift position and the time at which the position occurs. In some implementations, the OIS position data 148 is captured at a higher frequency (e.g., at a rate higher than the frame rate of the video capture) so that multiple measurements are taken over the duration of each video frame exposure.
Device position sensor 120 provides device position data 150, which device position data 150 indicates rotation and/or other movement of recording device 102 during video capture. The device position may be measured at a high frequency (e.g., 200Hz or higher). Thus, in many cases, measurements may be obtained at a number of different times during the capture of each video frame.
The lens focus position data 146, OIS position data 148 and device position data 150 may all be recorded with time stamps indicating the times at which the specified locations occur. The time stamp can be made accurately, e.g., to the nearest millisecond, so that the data obtained from the various position measurements can be aligned in time. Additionally, the position of the device, OIS system or lens focus mechanism may be interpolated to determine the time value between the two measurements.
An example of a potential timing of data capture is shown in chart 152. As shown, device position data 150 (e.g., gyroscope data) and OIS position data 148 (e.g., lens shift position data) may be captured at a rate higher than the video capture frame rate (e.g., 30 frames per second, 60 frames per second, etc.) so that multiple positions of the device and OIS system may be determined for each video frame. The lens focus position data 146 may be captured at least once per image frame. For example, the position data may be captured asynchronously with respect to the frame exposure, where the gyroscope sensor data and OIS position data are sampled at a rate that exceeds and is not necessarily synchronous with the beginning or end of the image frame exposure.
Data obtained from the camera module 110 and other components is provided to the EIS module 155 for processing. This process may occur while video capture is taking place. For example, the EIS process may be performed in substantially real-time such that a video file accessible to the user at the end of video capture has been stabilized by the EIS module 155. In some embodiments, EIS processing may be performed at a later time (e.g., after video capture has been completed), or by a device other than the device that recorded the video. The EIS module 155 may be implemented in hardware, firmware, software, or a combination or sub-combination thereof.
EIS module 155 includes a motion data processor 156 that periodically or continuously obtains updated device position data 150 from device position sensor 120. The motion data processor estimates the current camera pose from the device position data 150. For example, a gyroscope signal may be obtained and used to estimate the device position of the recording device 102 at a high frequency (e.g., 200 Hz). The position of the device at a given time t is hereinafter referred to as r (t). The device position may indicate a rotational position of the recording device 102 with respect to, for example, one, two, or three axes. The device position may be represented as a rotation matrix, or relative to a coordinate system, or in other forms.
The EIS module 155 includes an OIS position data processor 158 that periodically or continuously obtains OIS position readings, illustrated as OIS position data 148. The OIS position data processor 158 converts the OIS read value to an offset that may be used with the device position. For example, the OIS lens position may be converted to a two-dimensional pixel offset. To generate the offsets, the OIS position data processor 158 may use the stored calibration data 132, which may provide conversion coefficients or matrices to convert from OIS positions to corresponding offsets. The offset due to the OIS position may account for changes in the effective focal length of the camera over time, for example, if the camera module 110 is capable of optical zooming, due to changes in the lens focus position and/or lens zoom position. Like the motion data processor 156, the OIS position data processor 158 marks each measurement and offset with a time of data representation.
The EIS module includes a motion model constructor 160 that receives the device position calculated by the device position data processor 156 and the OIS offset calculated by the OIS position data processor 158. Using this data, as well as the frame exposure data 142 and lens focus position data 146, the motion model builder 160 generates a first transformation 162 of the frame. For example, the first transformation 162 may be a projection matrix that maps the real world scene in the camera view to the captured frames. This process is repeated for each frame.
When generating the first transformation 162 of the frame, the position of the OIS module 115 may be modeled as an offset from the primary device position determined from the gyroscope data. As discussed further below, the offset may validate the effective focal length of the camera at capture by looking up the effective focal length at the then-current lens focus position. The first transformation 162 may describe the relationship of different subsets or regions of a single image frame, respectively. For example, different portions or components of the first transformation 162 may describe how different scan lines of a frame are mapped to the real-world scene. The device position, OIS module position and lens focus position can all be aligned using the measurement time stamps and interpolated as necessary to provide accurate positions for each scan line of a frame at the time of exposure.
The first transformation 162 generated by the motion model constructor 160 is provided to a nonlinear filtering engine 170 that determines a second transformation 172. The second transform 172 may be a second projection matrix P'i，jWhich projects the image data of a frame onto an output frame representing a stable version of the frame. In particular, the second transformation 172 may project the image P made using the initial transformation 162i，jMapping to an output frame, rather than operating on captured image data. In some embodiments, two transforms 162 may then be transformed172 are combined into a single transform that operates on the image data of the originally captured frame and maps it directly to a stable output frame.
To effectively stabilize the movement, the non-linear filtering engine 170 may generate a second transformation 172 to account for movement that will occur in the future after capture of the processed frame. For example, for a current frame being analyzed, the position of the recording device may not have moved significantly since the previous frame. However, if the engine 170 determines that significant motion occurs in future frames, a second transformation 172 may be generated to shift or otherwise alter the current frame to introduce significant movement in the video so that the larger future motion may be interspersed as a series of gradual changes rather than abrupt changes. Likewise, if stabilization of future frames would result in cropping or other changes, a second transformation 172 may be generated to propagate those changes at least partially to earlier frames for more gradual and uniform changes over a series of frames.
The nonlinear filtering engine 170 may generate a second transformation 172 of the frame from the virtual camera position. The virtual camera position may represent an adjusted or assumed pose of the recording device 102 that will stabilize the recorded video, rather than representing the actual position of the camera at the time the exposure occurred. The virtual position may represent a desired position at which to place the virtual camera, e.g., a position that will simulate a particular view or perspective of a scene. In general, any camera position can be represented by its rotation and translation relative to a global reference frame (global reference frame). The virtual camera position may be represented as a rotation matrix, e.g., a matrix indicating a rotational offset relative to a reference position. This may be a 3x3 matrix indicating rotational offsets relative to 3 axes of rotation. In some embodiments, the stabilization process of the EIS module defines the position based only on the rotation components, as these rotation components generally have the greatest impact on the stability of the handheld video.
The virtual camera position of the frame may reflect adjustments to the estimated camera position to enhance video stabilization, correct distortion and manipulation, facilitate panning, and enhance video. The virtual camera position for a frame may be determined by generating an initial camera position that is adjusted based on various factors. For example, the virtual camera position may be adjusted by filtering the device position, by making adjustments to prepare for motion of future frames, and/or to ensure that the image data covers the entire output frame, based on detected movement before and after the frame, based on the amount of blur in the frame, based on the likelihood of a pan occurring. Various factors may be addressed by generating a series of virtual camera positions for the frame that are altered, fused, or otherwise used to determine a final virtual camera position for the frame.
Just as the transforms 162, 172 may have different mappings for different scan lines, different virtual camera positions may be determined for different scan lines of a frame to adjust for changes in device position, OIS module 115 position, and/or lens focus position during frame capture. Thus, different virtual camera positions may be used for different portions of the frame. To improve efficiency, the virtual camera positions and corresponding components of the second conversion 172 may be calculated for a suitable subset of the scan lines of the image sensor, and then the appropriate data may be interpolated for the remaining scan lines. In the following examples, for simplicity, one scan line, e.g., the center scan line of an image sensor, is discussed. The technique of fully calculating the virtual camera position and the corresponding projection matrix components may be used for multiple scanlines of an image frame, or even for each scanline individually, if desired.
As used herein, device location refers to the location of the recording device 102, e.g., as indicated by device location data 150 (e.g., gyroscope data) and the output of the device location data processor 156. The device level position indicates the pose or orientation of the recording device 102 without regard to the internal movement of the lens of the camera module 110 or the movement of the OIS module 115. Also, as used herein, a camera position indication corresponds to a position of an active view or an estimated view of the camera. The camera position may be different from the device position by taking into account shifts due to operation of the OIS module 115, lens breathing, and other factors. Further, the camera position may be a virtual position, e.g., an approximate or hypothetical position that reflects an enhanced or altered view of the camera, rather than the actual view of the camera.
The EIS module 155 then uses the output of the non-linear motion filter engine 170 using the image warping engine 180 and maps each captured image frame to an output frame. The second projection 172 may include a component corresponding to each scan line of the frame, thereby mapping each portion of the frame to an output space and defining each pixel of the output frame. The processing by the EIS module 155 may be performed for each frame of the video.
Fig. 2 is a diagram showing an example of data for video stabilization. The figure shows a series of frames 210 captured by recording device 102. Each frame is marked with a corresponding set of metadata 212, the set of metadata 212 indicating, for example, an exposure duration, an exposure reference time (e.g., a start time, a stop time, or other reference point of exposure), a lens focus position, and the like. Although not shown, device location data and OIS module location data are captured and time stamped at different times during each exposure.
To stabilize the frame 211, a time frame around the captured frame 211 is defined. The time range or frame window is analyzed to determine how to transform the frame 211. The exposure time T of the center scan line of the frame 211 being analyzed may be used as the center of the range. The predetermined time offset L may be used to set a range, such as a time range from [ T-L, T + L ]. In some embodiments, the time offset L is about 200 ms. As a result, the range will include about 7 frames before and about 7 frames after frame 211. A larger or smaller time offset L may be used. Because the EIS module 155 uses the context of the future frame during processing, the processing of the frame is delayed until an appropriate number of subsequent frames have been captured.
In fig. 2, frame 211 is shown as being captured by the image sensor in fig. 2. As described above, the EIS module 155 defines the first transformation 162 from data indicating the true position of the recording device 102 during capture of the frame 211, and the position of camera elements such as OIS module elements and lens focus position. The result of applying the first transformation 162 is a projected image 230 shown relative to an output frame target 235. In an embodiment, the first transformation 162 is determined using only data corresponding to the capture of the frame 211. The transformation 162 corresponds to the real lens position of the camera, so the projected image 230 estimates the mapping between the image data and the actual scene in view of the camera.
The EIS module 155 uses the second transform 162 to further adjust the image data of the frame 211. This second transformation 162 corresponds to a virtual lens position, for example, an assumed position that would result in a more stable video if used to capture the frame 211. When the second transformation 162 is applied to the frame 211, a projected image 240 is generated that fully defines the data of the output frame 235.
Can be synthesized from [ T-L, T + L]Generates the second transformation 172 that produces the projected image 240 from the data corresponding to each frame in the time frame of (a). The position r (t) of the recording device 102 over the time period may be filtered, for example using a gaussian filter, to smooth the motion over the range. The result of the filtering at time T, i.e., the exposure of the center scan line of frame 211, may be used as the initial virtual camera position V0(T). Even if filtering is performed, the device location may experience undesirable movement, or other factors that may cause undesirable movement. As a result, the initial virtual camera position V may be updated through a series of further operations0(T). In some embodiments, the filtered locations r (t) are locations where no OIS movement is assumed, and therefore these locations may be based on the device location data 150 without consideration of the OIS location data 148. In other embodiments, OIS motion and offset may be included in a set of positions that are filtered to generate an initial virtual camera position V0(T)。
For example, the position V may be determined based on the amount of motion occurring over a future frame0(T) and the previous frame's camera position VF(T _ pre) interpolating to generate a second virtual camera position V1(T). The camera position of the previous frame may be a final virtual camera position, e.g., a position corresponding to a transformation used to generate a stable output version of the previous frame. Interpolation may align significant changes in motion between frame 211 and a previous frame with significant changes in motion between frame 211 and a future frame.
May be determined by subtracting V based on the amount of camera motion blur present in frame 2111(T) interpolate with the real device position R (T) to generate a third virtual camera position V2(T). This may reduce the amount of stabilization applied to reduce the viewer's perception of blur. Since motion blur is generally not eliminated, the stability of the video is reduced, where appropriate, to produce a more natural effect.
A fourth virtual camera position V may be generated3(T) to simulate or represent the time range [ T-L, T + L ] at the recording device 102]Of the position occurring during the uniform motion. The position may be determined by applying a stable filter (e.g. a domain transform filter) to the estimated actual device position r (t) over the time horizon. Although the filter is applied to the generation of V0(T), but this step represents a different type of filtering. For example, V may be generated by filtering0(T) that smoothes, but generally follows, the estimated actual device location over time without imposing a predetermined shape or pattern. In contrast, V is generated by filtering device patterns3(T) to conform to a predetermined uniform motion pattern, such as a substantially linear pan or other movement that may be desired by a user of the recording device 102.
Fifth virtual camera position V4(T) may be as V3(T) and V2An interpolation of (T) is generated. The EIS module 155 may evaluate whether the change in device location over time is likely to represent a pan of the recording device 102, and may weight the interpolation accordingly. V if it is determined that the possibility of pan is high4(T) approach to the estimated pan position V3(T). V if it is determined that the possibility of pan is low4(T) will be closer to position V2(T)。
At a fifth virtual camera position V4(T), the EIS module 155 may evaluate the coverage that the corresponding transform will provide to the output frame 235. Since it is desirable to fill the entire output frame 235 without leaving any undefined pixels, the EIS module 155 can determine the representation from the virtual camera position V4(T) A transformation of the viewed scene, e.g., a projection matrix, and verifying that the projected image will cover the output frame 235. For motion in future frames, the transformation may be applied to portions of the scene captured by future image frames. Transformation and corresponding virtual camera position V4(T) may be adjusted so that when using transform mapping, the current frame and each of a set of future frames will fully define the output frame 235. The resulting transform may be set as transform 172 and may be used to generate a stable output frame 235 for frame 211.
In some embodiments, generating stable output frame 235 for frame 211 includes performing the EIS processing techniques discussed for scan lines exposed at time T for one or more other scan lines of frame 211. For example, scan lines may be processed at certain intervals (e.g., every 100 scan lines, every 500 scan lines, etc.) or at certain reference points (e.g., one quarter and three quarters of a full frame, or at the top of a frame and the bottom of a frame). When the virtual camera positions and the second transformation 172 are determined for only the appropriate subset of the scan lines of the frame 211, the transformations of the scan lines (e.g., the corresponding portions of the projection matrix) are interpolated between the calculated positions. In this way, an appropriate transform is determined for each scan line, and as a result, each scan line may have a different transform. In some embodiments, the complete process of generating the virtual camera position and the second transformation 172 may be done for each scan line of each frame without relying on interpolation between data for different scan lines.
Once frame 211 is mapped to output frame 235, the result is saved and EIS module 155 begins processing the next frame. This process continues until each frame of the video has been processed.
The various factors used to generate the virtual camera position and resulting transformation may be used in combination or separately. For example, according to an embodiment, creating the virtual camera position V may be omitted0(T) to V4Some interpolation and adjustment of (T). For example, in various embodiments, any filtered camera position V may be used0(T) to V3(T) to determine projection of data ontoTransformation of output frames, not using V for this purpose4(T). Thus, any filtered camera position V is used0(T)，V1(T) and V2(T) to generate a stable transform may still improve the stability of the video. Similarly, V3(T) it is possible to effectively stabilize a video where panning occurs. Many other variations are within the scope of the disclosure, even if a subset of the different factors discussed are considered.
The techniques discussed may be applied in various ways. For example, rather than applying the two transforms 162, 172 sequentially to the image data, the recording device may generate a single combined transform that reflects the combined effect of the two. Thus, using the transforms 162, 172 to produce stabilized image data may involve generating further transforms or relationships that are ultimately used to stabilize the image data, rather than applying the transforms 162, 172 directly.
Fig. 3A-3C show a flow chart illustrating an example of a process 300 for video stabilization. Process 300 shows a more detailed example of the techniques discussed in fig. 1 and 2. The acts of process 300 may be performed by a computing system such as recording device 102 (e.g., a camera, mobile phone, tablet computer, etc.). The acts of process 300 may be performed by a single computing device or multiple computing devices.
In process 300, a series of video frames are captured (301). Various other types of data will also be captured when capturing frames. Metadata is captured for each frame (302), such as exposure time, exposure duration, lens focus position, zoom position (if applicable), camera identifier (e.g., specifying which multiple cameras or lenses to use if needed), and the like. Device location data is captured (303), such as gyroscope sensor data or other data indicative of the orientation of the recording device. OIS position data is captured (304) indicating the position (e.g., lens shift position) of a movable element of the OIS module. The device location and OIS location measurements may be made more frequently than the frame rate, so that one or more measurements are made for each frame captured. In some embodiments, the position measurement may be made above 200 Hz.
The device location data is processed to determine the device location (305) that occurs over a series of frames. The position may be indicative of a pose or orientation of the recording device in a coordinate system. For example, the position at time t may be represented by a rotation matrix r (t). Several locations of the device may be estimated for each frame, representing the device's location at the time the different portions of the frame were captured. The device locations that occur over a range of multiple frames will be used in processing a particular frame, so the device locations of frames before and after the frame being analyzed can also be determined. In some embodiments, the computing system continuously acquires the gyroscope signals at a high frequency (e.g., 200Hz) and estimates the latest camera position r (t).
The OIS position data is used to generate the OIS offset (306). The stored calibration data may indicate a table, function, or other data that maps OIS locations to offsets. For example, the OIS reading may be converted to a 2D pixel offset. For example, the offset may be referred to as 0_ len (t) (0_ len (x, t), 0_ len (y, t)), showing that the offset has x and y components for each time t.
The manufacturer of the recording device may use an efficient calibration process performed by one or more computers to generate calibration data that converts raw OIS reading data to pixel offsets via linear transformation. The calibration process may include an automatic data capture process controlled by a script to capture video when the OIS movable element (e.g., lens) is in various predetermined positions. The process may move the movable element in a predetermined pattern that covers the entire OIS motion field while capturing video frames of incrementally changing positions. For OIS modules with a movable lens, the motion may be a sequence of spiral motions around the central position of the movable lens. The one or more computers then detect the key frames, for example, by automatically marking the frames representing incremental changes in the position of the drive mechanism. The time of the key frame is then matched to the corresponding OIS position reading based on the time stamp of the key frame and the OIS reading data. The one or more computers then measure the image shift between the keyframes in pixels using a feature detection algorithm. For example, one or more computers may compare one keyframe image to another and determine the pixel offsets in the x and y directions that correspond to changes in OIS reading position for the two keyframes. The one or more computers may then use linear regression to determine conversion coefficients or other data that map the OIS raw readings to the image offset in pixels. In general, the calibration process can be performed for all camera modules of a particular type, without performing the calibration process for each individual device.
To perform the stabilization process, the computing system identifies a particular frame to be processed. The computing system defines a target time for the particular frame (307). For example, the time T at which the exposure of the center scan line occurs may be used as the target time. Typically, this is the middle time of the exposure duration of a particular frame. Because the frame metadata for each frame indicates a reference time (e.g., a start and/or end time of the exposure) and an exposure duration, a time corresponding to the center scan line may be determined for each frame.
The computing system determines a transformation for the scan line exposed at the target time (308). The transformation may be a projection matrix that maps the real world scene to an image. The projection matrix is called Pi，jWhere i is the frame index and j is the scan line index. The component of the matrix corresponding to the central scan line is Pi,j_T. This transformation may be generated using the device location r (t) and the OIS location offset.
OIS lens shift data may be modeled as an additional offset relative to a principal point in the frame (e.g., the center or origin of the frame). This follows the physical intuition that the OIS lens moves about its origin during recording. To determine the projection matrix, the computing system may use Ri,j_T，Ri,j_TIs a camera external matrix (e.g., rotation matrix) determined from the device location data in step (305). The computing system may also determine a camera eigen matrix Ki,j_T(309). For example, the effective focal length f of the camera at time T may be used to generate a camera intrinsic matrix, taking into account the lens focus position at time. The calibration data can be used to find a focal length corresponding to the lens focus position reading value. Camera intrinsic matrix Ki,j_TCan be determined as a 3x3 matrix, for example:
where f is the focal length of the current frame, Pt is the 2D principal point set at the center of the image, and O _ len (j _ T) is the converted OIS reading in pixels on the scan line exposed at T. Can be based on the following equation, Pi,j_T＝Ri,j_T*Ki,j_TA projection matrix is generated using the camera intrinsic matrix and the camera extrinsic matrix (310). A first transformation may be applied to the image data of the center scan line to project the center scan line (311).
In addition, the computing system may determine the first virtual camera position V by filtering the real device positions r (t) that occur during the time window0(T) (312). The first transformation maps the image data in the particular frame under analysis to a coordinate system. The virtual camera position represents a hypothetical view or perspective of the image data of this mapping, which would adjust the mapping of the image data to the frames. The computing system defines a time range around the target time T (313). For example, the range may include a predetermined time L before and after the target time T. For example, L may be 200ms, 500ms, etc. Thus, a defined time range [ T-L, T + L]The exposure time of one or more frames before and one or more frames after a particular frame of processing may be included. The computing system filters the device locations r (t) (e.g., rotation matrices) that occur within a defined time range (314). For this step, the recording device position is used, which represents the actual movement of the recording device, so the filtered position does not typically reflect the adjustment due to the OIS operation. For example, for this step, the computing system may assume that the virtual camera always has zero OIS motion: 0_ len _ virtual (x, y) ═ 0, 0. However, in some embodiments, the change in OIS location may be used to generate the location that is filtered during this process. Filtering can be performed using a gaussian filter with a large kernel (e.g., Sigma ═ 6). This filtering can smooth out changes in device location over time and reduce the effects of random or short-lived changes in location. The filtered device position corresponding to time T is set to a first virtual camera positionV0(T)。
Information about the recording device movement after time T may be used to generate a second virtual camera position V1(T) (315). Future motion may be evaluated by evaluating the device locations that occur after time T to time T + L. By setting an initial virtual camera position V0(T) is interpolated with the final virtual camera position V (T _ pre) of the center scan line of the frame captured immediately prior to frame 211, and the amount of future occurrence of movement can be used to calculate the second virtual camera position V1(T). In this example, the virtual camera position may be represented by a rotation matrix, so spherical interpolation may be used. The computing system accesses the virtual camera position V (T _ pre) for the center scan line of the previous frame (316). The computing system then determines the degree of transition based on the change in device location that occurred after processing the particular frame. For example, the amount of movement of the recording device 102 occurring after time T may be used to determine a transition coefficient A that sets the interpolation result closer to V0(T) is also V (T _ pre). In particular, changes in the position of the device after T can be evaluated. The variability can be measured using a measure of "protrusion". Intuitively, it refers to if the previous virtual camera pose VF(T _ pre) remains unchanged, the warped frame highlights the amount of current and future frames. The larger the projection, the larger the transition ratio or degree will be, and vice versa. The computing system may obtain a saliency value of the current frame relative to each future frame to T + L, calculate a corresponding transition ratio, and average the transition ratios as a final transition ratio or degree of transition.
Interpolation may be performed using spherical linear interpolation, referred to herein as a function
Slerp(p0，p1，α)＝sin((1-α)*θ)/sin(θ)*p0+sin(α*θ)/sin(θ)*p1Where α is the degree of transition or the coefficient of transition, 0<＝α<1 is point p0And p1θ ═ arccos (p)0·p1) Is from point p0To p1The arc formed is diagonal. Thus, the camera position can be determined as V1(T)＝Slerp(VF(T_pre)，V0(T), A). Such asIf the motion is kept constant or changes slowly, the coefficients will be small and therefore the resulting camera position will be close to V (T _ pre). If the future motion is large, the coefficient will be large and the resulting camera position will be closer to V0(T). The computing system interpolates (318) a second virtual camera position V1(T) is set to smooth the position of the apparent rate of change between frames so that the rate at which motion changes does not suddenly increase or decrease.
Third virtual camera pose V2(T) may be based on the amount of motion present during frame capture or the amount of motion blur present in the frame being processed, from V1(T) generation (319). Virtual camera position V1(T) may be adjusted to more closely align with the true camera position r (T) and thus reflect motion that occurred between the previous frame and frame 211. If there is a large amount of motion blur in frame 211 (e.g., due to camera movement rather than object movement) and the processed video becomes very stable, the motion blur may become very noticeable and unnatural. To reduce the viewer's perception of blur, the change in virtual camera position from one frame to the next may be allowed to at least partially track the real camera position, consistent with natural blur in the image. This may hide the blur to some extent. Thus, based on the determination of the amount of image blur in the frame 211, at least a part of the real motion of the recording device 102 may be used, for example, by determining at V from the detected amount of image blur1Interpolating between (T) and R (T) to adjust virtual camera position V1(T)。
To estimate the amount of motion blur for a frame, the computing system may determine a homography of the scan line at the target time T (320). The computing system then estimates a motion blur kernel (321). One technique for characterizing blur is to compare the projection matrix components at two different locations along the frame. For example, for a frame having an exposure duration E, a projection matrix component P may be determinedi，T-E/2And Pi，T+E/2A projection matrix corresponding to the first and last scan lines of a particular frame is represented. Since the projection matrix is generated using OIS position data and lens focal length, theseThe blur caused by the change of the parameter will be reflected in the transformation. The computing system then computes a transformation between the two projection matrices and applies the resulting transformation to an origin representing the center point of the image. The transformation is applied to move the proxels to offsets from the actual origin in projection space. The offset from the origin can be used as a fuzzy estimate.
The computing system then maps the second virtual camera position V1(T) adjusting towards the real device position r (T) to an extent determined based on the identified amount of blur (322). For example, the computing system may determine a coefficient based on the motion blur kernel and use the coefficient to position V1The spherical interpolation between (T) and R (T) is weighted. The coefficients may be based on the offsets discussed above. In some embodiments, a threshold amount of blur is set, and the interpolation of r (t) is only performed if the amount of blur exceeds a minimum amount represented by the threshold. For example, interpolation may be performed only when the motion represented by the offset exceeds the motion that the OIS operation can usually compensate for.
The computing system determines a fourth virtual camera position V3(T) which represents the uniform motion of the recording device (323) over a series of frames. It may be assumed that the user is panning the camera or in the time range T-L, T + L]Generating a virtual camera position V while performing another uniform motion3(T), hence position V3(T) should descend along a stable motion path. Thus, the time range [ T-L, T + L ] can be]The stable filter is applied to the actual plant location r (t) (324). As an example, a domain transform filter may be used. For example, this may infer a best fit pan trajectory and indicate a corresponding position V along the trajectory3(T)。
The computing system determines a probability S (325) that uniform motion (e.g., pan) of the recording device occurs over a time horizon. The time range may correspond to the capture time of one or more future frames captured after the particular frame. For example, the time range of the data evaluated may be [ T + L-K, T + L ], where K > L. In general, a large number of contexts before time T may help in evaluating the likelihood of a pan. For example, a pan may occur for one or more seconds, typically much larger than the value of L. As a result, the value of K may be set to, for example, 1 second, 2 seconds, 3 seconds, or the like. The computing system may utilize a relatively long device location history determined during the video capture process. In some embodiments, the start of the time range may be set at the beginning of the video so that the analysis may evaluate the overall motion pattern of the entire video before time T + L.
When the probability S is generated, the computing system down-samples the device location data r (t) (326). For example, the rotational position r (t) may be sampled at a frame rate of 30fps, which approximates the rate perceived by humans, so the probability calculation is more robust to slow movements and noise. The computing system analyzes a series of down-sampled locations over the time period to determine a probability (327). For example, linear regression can be used to generate a measure of how well the down-sampled position matches the linear trajectory. Other methods may be used for analysis. For example, a machine learning model may be trained to predict the likelihood of a pan based on video examples that exhibit the pan and video examples that do not reflect the pan. In general, the gyroscope signals provide high quality location information, which may allow reliable patterns to be learned through machine learning models. For example, an sigmoid function may be trained to produce a pan probability S.
The computing system determines a fifth virtual camera position V4(T) (328). By setting the third virtual camera position V2(T) and a fourth virtual camera position V3(T) fusing to generate the location (329). For example, the position V may be determined by determining a probability of uniform motion2(T) and V3(T) interpolating between them to achieve fusion (330). V if it is determined that the possibility of pan is high4(T) approach to the estimated pan position V3(T). V if it is determined that the possibility of pan is low4(T) will be closer to position V2(T)。
At this point in process 300, virtual camera position V is due to filtering and evaluating future motion over a range of device positions4(T) provides a stable motion. The position is also adjusted to hide motion blur and allow panning (if possible). However, direct application to this locationThe determined projection may leave certain portions of the output frame uncertain, e.g., no image information from a particular frame is mapped to the output frame. Further processing may adjust the virtual camera position further towards the real camera position r (t) to ensure that each portion of the output frame is mapped to some portion of a particular frame.
The computing system is based on the fifth virtual camera position V4(T) determining a transformation, e.g. a second projection matrix Pr(t) (331). The computing system then determines a first projection matrix Pi，j(based on the true device position r (t)) to a second projection matrix (332). If used for the current video frame or a subsequent video frame captured later in the time period, the computing system determines whether the reference transformation resulted in an undefined pixel (333). For example, for a signal at [ T, T + L]For each frame captured in the time frame, P using the frame can be determinedi，jAnd a reference transformation may be applied to the projected image of the frame. The transformed projection image is then checked against the output frame to determine if each pixel of the output frame has been defined. Can be aimed at]This check is performed every frame captured in the time frame. In some embodiments, the fifth virtual camera position V may be used if an output frame is defined for each frame captured within the time range4(T) corresponding second projection matrix Pr(t) generating a stable version of the particular frame.
If the reference transform pair is in the time range [ T, T + L]The application of the projected version of any frame captured during does not define all pixels of the output frame, then the computing system determines a minimum transition to maintain valid pixels of each frame for a time period [ T, T + L ]]Mapping of the end (334). For example, if a frame with a center scan line at T' is not fully defined using the reference transform, a binary search may be used, from V4(T) and R (T') find the minimum transition coefficient so that all projected pixels are effectively defined.
The computing system determines a fifth transition matrix V for each subsequent frame of the time period4(T) (335). For having a central scan line at TThe degree of transition of the frame may be a transition coefficient which is applied to the location V4At time (T) will be from position V4(T) move to true device location R (T'). The computing system selects a degree of transition from the determined various degrees of transition (336). The computing system may select the highest degree of transition in the set. For slave [ T, T + L]The set of transition degrees, the frame of occurrence, may represent a comparison of the true device position V4(T) for each center scan line relative to the true camera position. Thus, the set may include transition degrees for: v4(T)→R(T)，V4(T)→R(T+1)，……V4(T)→R(T+L))。
For example, if the current degree of transition is higher than the degree of transition for a particular frame to a future frame, then it may be possible to move from camera position VF(T) selecting a current degree of transition to a current true device location r (T). Otherwise, when the slave V4(T) the highest degree of transition to the future frame T ", the computing system increases the current degree of transition (337). The amount by which the current degree of transition is increased may vary depending on the amount of time between time T and time T ". The shorter the amount of time, the more the current degree of transition may be increased, e.g., the current degree of transition with V4The closer the degree of transition from (T) to R (T').
Computing system generates a final virtual location VF(T) (338). This may use V4Interpolation between (T) and R (T), i.e. Slerp (V)4(T), R (T), tr), where tr is the transition ratio or degree determined in step 337. Can be defined by]Modifying the fifth virtual camera position V by a minimum transition coefficient required for output of frames within the range4(T) and setting the interpolation relative to the current actual device position r (T) using the selected degree of transition.
Computing system uses final virtual position VF(T) to determine a final projection matrix P'i，j(339). From an initial projection matrix Pi，jFirst transformation of representation and from a final projection matrix P'i，jThe second transforms of the representations together map the image data in the captured particular frame to the output frame. In other words, the first transformation maps the image data to an intermediate space based on the actual device position r (t), and the second transformation is based on the final virtual camera position VF(T) mapping the image data from the intermediate space to an output frame. The first and second transforms may be combined or otherwise used to generate a single mapping that may be applied to a particular frame to map image data directly to an output frame.
Various steps in process 300 involve determining a transform and virtual camera position specific to a particular scan line (e.g., a scan line exposed at time T). The same techniques can be used to determine the transform and virtual camera position for other scan lines in a particular frame being processed. In general, only a proper subset of the scan lines of a frame may be processed using the techniques described above. For example, only the top, bottom and middle scan lines may be evaluated independently. As another example, a center scan line and other scan lines occurring every 50 scan lines, every 200 scan lines, or every 500 scan lines may be processed. The remaining scan lines may interpolate the transformation between the separately calculated scan lines.
The computing system generates a stable output frame for the particular frame being processed using the final transform with the components determined for each scan line (340). The stabilized output version of the frame is saved to the storage device as part of the video being recorded. Process 300 may be repeated for the next frame in the capture sequence of processed frames, and then for the next frame until video capture ends and all captured frames have been processed. The capture of other frames of the video, and corresponding position information, may be performed while processing the captured frames.
The embodiments of the invention and all of the functional operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their equivalents, or in combinations of one or more of them. Embodiments of the invention may be implemented as one or more computer program products, i.e., one or more modules of computer program instructions encoded on a computer readable medium for execution by, or to control the operation of, data processing apparatus. The computer readable medium can be a non-transitory computer readable storage medium, a machine readable storage device, a machine readable storage substrate, a storage device, a composition of matter effecting a machine readable propagated signal, or a combination of one or more of these. The term "data processing apparatus" encompasses all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of these. A propagated signal is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus.
A computer program (also known as a program, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such devices. Furthermore, the computer may be embedded in another device, e.g., a tablet computer, a mobile phone, a Personal Digital Assistant (PDA), a mobile audio player, a Global Positioning System (GPS) receiver, to name a few. Computer-readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disks; and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, embodiments of the invention may be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other types of devices may also be used to provide for interaction with a user. For example, feedback provided to the user can be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input.
Embodiments of the invention may be implemented in a computing system that includes a back-end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front-end component (e.g., a client computer having a graphical user interface or a Web browser through which a user may interact with an implementation of the invention), or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network ("LAN") and a wide area network ("WAN"), such as the Internet.
The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
While this specification contains many specifics, these should not be construed as limitations on the scope of the invention or of what may be claimed, but rather as descriptions of features specific to particular embodiments of the invention. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various functions described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated within a single software product or packaged into multiple software products.
In each instance where HTML files are mentioned, other file types or formats may be substituted. For example, the HTML file may be replaced with an XML, JSON, plain text, or other type of file. Further, where tables or hash tables are mentioned, other data structures (e.g., spreadsheets, relational databases, or structured files) may be used.
Thus, particular embodiments of the invention have been described. Other embodiments are within the scope of the following claims. For example, the motions recited in the claims may be performed in a different order and still achieve desirable results.
Claims (40)
1. A method, characterized in that the method comprises:
receiving a series of frames of a video captured by a recording device using an Optical Image Stabilization (OIS) system;
receiving (i) OIS position data indicative of a position of the OIS system during acquisition of a series of frames, the OIS position data being indicative of a plurality of OIS position measurements for at least some of the frames in the series of frames at different times during acquisition of the frames; and (ii) device location data indicative of a location of the recording device during capture of the series of frames;
determining one or more transforms for a particular frame of the series of frames, the one or more transforms being determined based on the plurality of OIS location measurements determined during acquisition of the particular frame and the device location data for the particular frame; and
generating a stable version of the particular frame using the one or more transforms.
2. The method of claim 1, comprising associating each of the plurality of OIS position measurements determined during the capturing of the particular frame with a different scan line of the particular frame.
3. The method of claim 1, comprising: determining at least one of the one or more transitions for the particular frame based on a location of the recording device, wherein the location of the recording device is determined based on device location data and one or more frames in the series of frames are captured after the particular frame.
4. The method of claim 1, wherein at least one of the one or more transforms indicates how different scan lines of the particular frame are mapped into the scene represented by the particular frame.
5. The method of claim 1, wherein the one or more transforms are further based on lens focus data indicating one or more lens focus positions that occurred during capture of the particular frame.
6. The method of claim 1, wherein the one or more transforms comprise a plurality of transforms corresponding to different scan lines of the particular frame.
7. The method of claim 1, comprising determining a plurality of virtual camera positions corresponding to different scan lines of the particular frame, and using the plurality of virtual camera positions to determine a plurality of transforms for different scan lines of the particular frame.
8. The method of claim 1, wherein generating the one or more transforms comprises:
for each of the appropriate subsets of the scan lines of the particular frame, computing a transformation corresponding to the scan line; and
for each scan line of the particular frame that is not in the proper subset, a transform is determined using interpolation based on the computed transform.
9. The method of claim 8, wherein the scan lines in the proper subset are spaced apart from each other by a constant number of scan lines.
10. The method of claim 8, wherein the proper subset includes only a top scan line of the particular frame, a bottom scan line of the particular frame, and a middle scan line of the particular frame.
11. The method of claim 1,
receiving the OIS location data comprises: receiving lens shift data indicative of a position of a movable lens of the OIS system; and
receiving the device location data includes receiving orientation or motion data from a gyroscope or accelerometer of the recording device.
12. The method of claim 1,
the OIS position data indicates a plurality of OIS position readings for each frame in the series of frames; and
wherein the device location data indicates a plurality of device location measurements for each frame in a series of frames.
13. The method of claim 1, wherein generating one or more transforms comprises: generating a projection matrix comprising different transformations for different scan lines of the particular frame.
14. A system, characterized in that the system comprises:
one or more processors; and
one or more machine-readable media storing instructions that, when executed by one or more processors, cause the one or more processors to:
receiving a series of frames of a video captured by a recording device using an Optical Image Stabilization (OIS) system;
receiving (i) OIS position data indicative of a position of the OIS system during acquisition of a series of frames, the OIS position data being indicative of a plurality of OIS position measurements for at least some of the frames in the series of frames at different times during acquisition of the frames; and (ii) device location data indicative of a location of the recording device during capture of the series of frames;
determining one or more transforms for a particular frame of the series of frames, the one or more transforms being determined based on the plurality of OIS location measurements determined during acquisition of the particular frame and the device location data for the particular frame; and
generating a stable version of the particular frame using the one or more transforms.
15. The system of claim 14, wherein the operations comprise associating each of the plurality of OIS position measurements determined during the acquisition of the particular frame with a different scan line of the particular frame.
16. The system of claim 14, wherein the operations comprise: determining at least one of the one or more transitions for the particular frame based on a location of the recording device, wherein the location of the recording device is determined based on device location data and one or more frames in the series of frames are captured after the particular frame.
17. The system of claim 14, wherein at least one of the one or more transforms indicates how different scan lines of the particular frame are mapped into the scene represented by the particular frame.
18. The system of claim 14, wherein the one or more transformations are further based on lens focus data indicating one or more lens focus positions that occur during capture of the particular frame.
19. The system of claim 14, wherein the one or more transforms comprise a plurality of transforms corresponding to different scan lines of the particular frame.
20. One or more non-transitory machine-readable media storing instructions that, when executed by one or more processors, cause the one or more processors to:
receiving a series of frames of a video captured by a recording device using an Optical Image Stabilization (OIS) system;
receiving (i) OIS position data indicative of the position of the OIS system during acquisition of a series of frames, the OIS position data indicating, for at least some of the frames in the series of frames, a plurality of OIS position measurements for the frames at different times during acquisition of the frames; and (ii) device location data indicative of a location of the recording device during capture of the series of frames;
determining one or more transforms for a particular frame in the series of frames, the one or more transforms being determined based on the plurality of OIS position measurements determined during acquisition of the particular frame and the device location data for the particular frame; and
a stable version of the particular frame is generated using one or more transforms.
21. A method, characterized in that the method comprises:
receiving a series of frames of a video captured by a recording device using an Optical Image Stabilization (OIS) system;
receiving (i) OIS position data indicative of a position of the OIS system during acquisition of a series of frames, the OIS position data being indicative of a plurality of OIS position measurements for at least some of the frames in the series of frames at different times during acquisition of the frames; and (ii) device location data indicative of a location of the recording device during capture of the series of frames;
determining one or more transforms for a particular frame in the series of frames, the one or more transforms determined based on the OIS location data for the particular frame and the device location data for the particular frame;
determining an amount of blur for the particular frame based on the OIS location data for the particular frame; and
in response to an amount of blur in the particular frame being above a threshold amount, generating a stable version of the particular frame using one or more transforms, the stable version of the particular frame tracking, at least in part, device location data of the particular frame.
22. The method of claim 21, further comprising:
determining a first virtual camera position for the particular frame; and
determining a second virtual camera position using the first virtual camera position and the device position data for the particular frame.
23. The method of claim 22, wherein determining a second virtual camera position comprises: interpolating between the first virtual camera position and the position of the recording device based on the amount of blur in the particular frame during the capturing of the particular frame.
24. The method of claim 23, wherein the amount of blur in the particular frame is determined by:
determining a first projection matrix at the beginning of the exposure of the particular frame;
determining a second projection matrix at the end of the exposure of the particular frame;
determining a projective transformation between the first projection matrix and the second projection matrix;
applying the projective transformation to an origin representing a center point of the particular frame to generate a proxel, the proxel being offset from the center point; and
estimating the amount of blur in the particular frame based on a size of the offset.
25. The method of claim 23, wherein the one or more transformations are determined based on the second virtual camera position.
26. The method of claim 21, further comprising:
determining a future degree of transition of a future frame occurring after the particular frame, the future degree of transition indicating a difference between the location of the recording device of the particular frame and the location of the recording device of the future frame;
determining whether the degree of future transition is above a threshold amount; and
in response to determining that the degree of future transition is above the threshold amount, generating the stable version of the particular frame based at least in part on the degree of future transition.
27. The method of claim 26, wherein the stable version of the particular frame is shifted to introduce apparent motion from the future frame.
28. The method of claim 21, further comprising:
determining whether stabilization of a future frame occurring after the particular frame introduces clipping;
in response to determining that stabilization of a future frame introduces clipping, generating the stabilized version of the particular frame to introduce partial clipping.
29. The method of claim 21, wherein the one or more transforms are further based on lens focus data indicating one or more lens focus positions that occurred during capture of the particular frame.
30. The method of claim 21, wherein the one or more transforms comprise a plurality of transforms corresponding to different scan lines of the particular frame.
31. The method of claim 21, wherein:
receiving the OIS location data comprises: receiving lens shift data indicative of a position of a movable lens of the OIS system; and
receiving the device location data includes receiving orientation or motion data from a gyroscope or accelerometer of the recording device.
32. The method of claim 21, wherein:
the OIS position data indicates a plurality of OIS position readings for each frame in the series of frames; and
the device location data indicates a plurality of device location measurements for each frame in a series of frames.
33. A system, characterized in that the system comprises:
one or more processors; and
one or more machine-readable media storing instructions that, when executed by one or more processors, cause the one or more processors to:
receiving a series of frames of a video captured by a recording device using an Optical Image Stabilization (OIS) system;
receiving (i) OIS position data indicative of a position of the OIS system during acquisition of a series of frames, the OIS position data being indicative of a plurality of OIS position measurements for at least some of the frames in the series of frames at different times during acquisition of the frames; and (ii) device location data indicative of a location of the recording device during capture of the series of frames;
determining one or more transforms for a particular frame in the series of frames, the one or more transforms determined based on the OIS location data for the particular frame and the device location data for the particular frame;
determining an amount of blur for the particular frame based on the OIS location data for the particular frame; and
in response to an amount of blur in the particular frame being above a threshold amount, generating a stable version of the particular frame using one or more transforms, the stable version of the particular frame tracking, at least in part, device location data of the particular frame.
34. The system of claim 33, wherein the one or more machine-readable media store further instructions that, when executed by the one or more processors, cause the one or more processors to:
determining a first virtual camera position for the particular frame; and
determining a second virtual camera position using the first virtual camera position and the device position data for the particular frame.
35. The system of claim 34, wherein determining a second virtual camera position comprises: interpolating between the first virtual camera position and the position of the recording device based on the amount of blur in the particular frame during the capturing of the particular frame.
36. The system according to claim 35, wherein the amount of blur in the particular frame is determined by: determining a first projection matrix for an exposure start of a particular frame;
determining a second projection matrix for the end of exposure of the particular frame;
determining a projective transformation between the first projection matrix and the second projection matrix;
applying a projective transformation to an origin representing a center point of a particular frame to generate a proxel, the proxel being offset from the center point; and
based on the magnitude of the offset, the amount of blur in a particular frame is estimated.
37. The system of claim 35, wherein the one or more transformations are determined based on the second virtual camera position.
38. The system of claim 33, wherein the one or more machine-readable media store further instructions that, when executed by the one or more processors, cause the one or more processors to:
determining a future degree of transition of a future frame occurring after the particular frame, the future degree of transition indicating a difference between the location of the recording device of the particular frame and the location of the recording device of the future frame;
determining whether the degree of future transition is above a threshold amount; and
in response to determining that the degree of future transition is above the threshold amount, generating the stable version of the particular frame based at least in part on the degree of future transition.
39. The system of claim 38, wherein the stable version of the particular frame is shifted to introduce apparent motion from the future frame.
40. One or more non-transitory machine-readable media, wherein stored instructions, which when executed by one or more processors, cause the one or more processors to:
receiving a series of frames of a video captured by a recording device using an Optical Image Stabilization (OIS) system;
receiving (i) OIS position data indicative of a position of the OIS system during acquisition of a series of frames, the OIS position data being indicative of a plurality of OIS position measurements for at least some of the frames in the series of frames at different times during acquisition of the frames; and (ii) device location data indicative of a location of the recording device during capture of the series of frames;
determining one or more transforms for a particular frame in the series of frames, the one or more transforms determined based on the OIS location data for the particular frame and the device location data for the particular frame;
determining an amount of blur for the particular frame based on the OIS location data for the particular frame; and
in response to an amount of blur in the particular frame being above a threshold amount, generating a stable version of the particular frame using one or more transforms, the stable version of the particular frame tracking, at least in part, device location data of the particular frame.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
CN202110599340.0A CN113194260B (en) | 2017-10-03 | 2018-07-31 | Video stabilization method and device |
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/724,241 | 2017-10-03 | ||
US15/724,241 US10462370B2 (en) | 2017-10-03 | 2017-10-03 | Video stabilization |
CN202110599340.0A CN113194260B (en) | 2017-10-03 | 2018-07-31 | Video stabilization method and device |
CN201880030074.6A CN111133747B (en) | 2017-10-03 | 2018-07-31 | Method and device for stabilizing video |
PCT/US2018/044524 WO2019070333A1 (en) | 2017-10-03 | 2018-07-31 | Video stabilization |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880030074.6A Division CN111133747B (en) | 2017-10-03 | 2018-07-31 | Method and device for stabilizing video |
Publications (2)
Publication Number | Publication Date |
---|---|
CN113194260A true CN113194260A (en) | 2021-07-30 |
CN113194260B CN113194260B (en) | 2024-04-16 |
Family
ID=63407513
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202110599340.0A Active CN113194260B (en) | 2017-10-03 | 2018-07-31 | Video stabilization method and device |
CN201880030074.6A Active CN111133747B (en) | 2017-10-03 | 2018-07-31 | Method and device for stabilizing video |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880030074.6A Active CN111133747B (en) | 2017-10-03 | 2018-07-31 | Method and device for stabilizing video |
Country Status (5)
Country | Link |
---|---|
US (4) | US10462370B2 (en) |
EP (1) | EP3603049A1 (en) |
JP (1) | JP6818912B2 (en) |
CN (2) | CN113194260B (en) |
WO (1) | WO2019070333A1 (en) |
Families Citing this family (22)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10462370B2 (en) * | 2017-10-03 | 2019-10-29 | Google Llc | Video stabilization |
US10171738B1 (en) * | 2018-05-04 | 2019-01-01 | Google Llc | Stabilizing video to reduce camera and face movement |
JP2020031264A (en) * | 2018-08-20 | 2020-02-27 | ソニーセミコンダクタソリューションズ株式会社 | Signal processor, imaging apparatus, signal processing method |
CN113170038B (en) * | 2018-11-29 | 2023-06-13 | 富士胶片株式会社 | Jitter correction control device, method for operating jitter correction control device, storage medium, and image pickup device |
JP7152588B2 (en) | 2019-02-20 | 2022-10-12 | 富士フイルム株式会社 | Image stabilization device, imaging device, monitoring system, and program |
US11470254B1 (en) | 2019-06-21 | 2022-10-11 | Gopro, Inc. | Systems and methods for assessing stabilization of videos |
US20200412954A1 (en) * | 2019-06-25 | 2020-12-31 | Qualcomm Incorporated | Combined electronic image stabilization and optical image stabilization |
CN110536064B (en) * | 2019-07-22 | 2021-04-06 | 杭州电子科技大学 | Method for removing jitter of pixel-level precision video image of fixed scene |
EP3979617A4 (en) * | 2019-08-26 | 2022-06-15 | Guangdong Oppo Mobile Telecommunications Corp., Ltd. | Shooting anti-shake method and apparatus, terminal and storage medium |
CN114616820A (en) | 2019-10-29 | 2022-06-10 | 富士胶片株式会社 | Imaging support device, imaging system, imaging support method, and program |
US11580621B2 (en) * | 2019-12-06 | 2023-02-14 | Mediatek Inc. | AI frame engine for mobile edge |
CN111212224A (en) * | 2020-01-10 | 2020-05-29 | 上海摩象网络科技有限公司 | Anti-shake processing method and device applied to image shooting equipment and electronic equipment |
US11694311B2 (en) * | 2020-03-04 | 2023-07-04 | Nec Corporation | Joint rolling shutter image stitching and rectification |
EP3883234B1 (en) * | 2020-03-17 | 2022-02-02 | Axis AB | Wearable camera and a method for power consumption optimization in the wearable camera |
US11265469B1 (en) * | 2020-04-10 | 2022-03-01 | Amazon Technologies, Inc. | System to mitigate image jitter by an actuator driven camera |
US11190689B1 (en) | 2020-07-29 | 2021-11-30 | Google Llc | Multi-camera video stabilization |
CN112153282B (en) * | 2020-09-18 | 2022-03-01 | Oppo广东移动通信有限公司 | Image processing chip, method, storage medium and electronic device |
JP2023553153A (en) * | 2020-12-10 | 2023-12-20 | グーグル エルエルシー | Improving video stabilization based on machine learning models |
CN112734653A (en) * | 2020-12-23 | 2021-04-30 | 影石创新科技股份有限公司 | Motion smoothing processing method, device and equipment for video image and storage medium |
CN113452919B (en) * | 2021-07-21 | 2022-04-19 | 杭州海康威视数字技术股份有限公司 | Camera for realizing cooperative anti-shake by using optical anti-shake and electronic anti-shake |
WO2023003556A1 (en) * | 2021-07-22 | 2023-01-26 | Google Llc | Joint video stabilization and motion deblurring |
CN115546043B (en) * | 2022-03-31 | 2023-08-18 | 荣耀终端有限公司 | Video processing method and related equipment thereof |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN104079819A (en) * | 2013-03-26 | 2014-10-01 | 佳能株式会社 | Image processing apparatus and method, and image capturing apparatus |
CN104769934A (en) * | 2012-11-08 | 2015-07-08 | 苹果公司 | Super-resolution based on optical image stabilization |
US20160006935A1 (en) * | 2014-07-06 | 2016-01-07 | Apple Inc. | Low Light Video Image Stabilization Strength Modulation |
US9407827B2 (en) * | 2011-11-14 | 2016-08-02 | Dxo Labs | Method and system for capturing sequences of images with compensation for variations in magnification |
US20170244881A1 (en) * | 2016-02-19 | 2017-08-24 | Fotonation Limited | Method of Stabilizing a Sequence of Images |
Family Cites Families (118)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US4637571A (en) | 1985-09-03 | 1987-01-20 | The United States Of America As Represented By The Secretary Of The Army | Electronic image stabilization |
GB2220319B (en) | 1988-07-01 | 1992-11-04 | Plessey Co Plc | Improvements in or relating to image stabilisation |
US6466686B2 (en) | 1998-01-07 | 2002-10-15 | International Business Machines Corporation | System and method for transforming fingerprints to improve recognition |
US20030038927A1 (en) | 2001-08-27 | 2003-02-27 | Alden Ray M. | Image projector with integrated image stabilization for handheld devices and portable hardware |
US6877863B2 (en) | 2002-06-12 | 2005-04-12 | Silicon Optix Inc. | Automatic keystone correction system and method |
KR20040077240A (en) | 2003-02-28 | 2004-09-04 | 주식회사 대우일렉트로닉스 | Method for controlling recording speed according to motion detecting in a timelapse image recording and reproducing apparatus |
US8199222B2 (en) | 2007-03-05 | 2012-06-12 | DigitalOptics Corporation Europe Limited | Low-light video frame enhancement |
US7643062B2 (en) | 2005-06-08 | 2010-01-05 | Hewlett-Packard Development Company, L.P. | Method and system for deblurring an image based on motion tracking |
JP4340915B2 (en) | 2006-02-01 | 2009-10-07 | ソニー株式会社 | Captured image signal distortion correction method, captured image signal distortion correction apparatus, imaging method, and imaging apparatus |
US7697725B2 (en) | 2006-04-03 | 2010-04-13 | Sri International | Method and apparatus for autonomous object tracking |
US7952612B2 (en) * | 2006-06-22 | 2011-05-31 | Nokia Corporation | Method and system for image construction using multiple exposures |
US8781162B2 (en) | 2011-01-05 | 2014-07-15 | Ailive Inc. | Method and system for head tracking and pose estimation |
JP4702233B2 (en) | 2006-09-11 | 2011-06-15 | ソニー株式会社 | Image data processing apparatus and image data processing method |
KR100819301B1 (en) | 2006-12-20 | 2008-04-03 | 삼성전자주식회사 | Method and apparatus for optical image stabilizer on mobile camera module |
US7559017B2 (en) | 2006-12-22 | 2009-07-07 | Google Inc. | Annotation framework for video |
US7796872B2 (en) | 2007-01-05 | 2010-09-14 | Invensense, Inc. | Method and apparatus for producing a sharp image from a handheld device containing a gyroscope |
US8213685B2 (en) | 2007-01-05 | 2012-07-03 | American Traffic Solutions, Inc. | Video speed detection system |
WO2008114264A2 (en) | 2007-03-21 | 2008-09-25 | Mantis Vision Ltd | A method and apparatus for video image stabilization |
JP5111088B2 (en) | 2007-12-14 | 2012-12-26 | 三洋電機株式会社 | Imaging apparatus and image reproduction apparatus |
JP4539729B2 (en) | 2008-02-15 | 2010-09-08 | ソニー株式会社 | Image processing apparatus, camera apparatus, image processing method, and program |
JP5075757B2 (en) | 2008-08-05 | 2012-11-21 | オリンパス株式会社 | Image processing apparatus, image processing program, image processing method, and electronic apparatus |
JP5144487B2 (en) | 2008-12-15 | 2013-02-13 | キヤノン株式会社 | Main face selection device, control method thereof, imaging device, and program |
KR101547556B1 (en) | 2009-02-06 | 2015-08-26 | 삼성전자주식회사 | Image display method and apparatus |
US8970690B2 (en) | 2009-02-13 | 2015-03-03 | Metaio Gmbh | Methods and systems for determining the pose of a camera with respect to at least one object of a real environment |
WO2011046633A1 (en) | 2009-10-14 | 2011-04-21 | Zoran Corporation | Method and apparatus for image stabilization |
US8599238B2 (en) | 2009-10-16 | 2013-12-03 | Apple Inc. | Facial pose improvement with perspective distortion correction |
US8553275B2 (en) | 2009-11-09 | 2013-10-08 | Xerox Corporation | Architecture for controlling placement and minimizing distortion of images |
US8416277B2 (en) | 2009-12-10 | 2013-04-09 | Apple Inc. | Face detection as a metric to stabilize video during video chat session |
US8558903B2 (en) | 2010-03-25 | 2013-10-15 | Apple Inc. | Accelerometer / gyro-facilitated video stabilization |
US10614289B2 (en) | 2010-06-07 | 2020-04-07 | Affectiva, Inc. | Facial tracking with classifiers |
US20120050570A1 (en) | 2010-08-26 | 2012-03-01 | Jasinski David W | Audio processing based on scene type |
CN103201765B (en) | 2010-09-28 | 2016-04-06 | 马普科技促进协会 | For recovering the method and apparatus of digital picture from viewed digital image sequence |
WO2012064106A2 (en) | 2010-11-12 | 2012-05-18 | Samsung Electronics Co., Ltd. | Method and apparatus for video stabilization by compensating for view direction of camera |
US9077890B2 (en) | 2011-02-24 | 2015-07-07 | Qualcomm Incorporated | Auto-focus tracking |
GB2492529B (en) | 2011-05-31 | 2018-01-10 | Skype | Video stabilisation |
US8913140B2 (en) | 2011-08-15 | 2014-12-16 | Apple Inc. | Rolling shutter reduction based on motion sensors |
US8493459B2 (en) | 2011-09-15 | 2013-07-23 | DigitalOptics Corporation Europe Limited | Registration of distorted images |
US9070002B2 (en) | 2011-10-18 | 2015-06-30 | King Fahd University Of Petroleum And Minerals | Method of performing fingerprint matching |
TWI469062B (en) | 2011-11-11 | 2015-01-11 | Ind Tech Res Inst | Image stabilization method and image stabilization device |
US9246543B2 (en) | 2011-12-12 | 2016-01-26 | Futurewei Technologies, Inc. | Smart audio and video capture systems for data processing systems |
US8743222B2 (en) | 2012-02-14 | 2014-06-03 | Nokia Corporation | Method and apparatus for cropping and stabilization of video images |
WO2013130082A1 (en) | 2012-03-01 | 2013-09-06 | Geo Semiconductor Inc. | Method and system for adaptive perspective correction of ultra wide-angle lens images |
EP2680616A1 (en) | 2012-06-25 | 2014-01-01 | LG Electronics Inc. | Mobile terminal and audio zooming method thereof |
US9280810B2 (en) | 2012-07-03 | 2016-03-08 | Fotonation Limited | Method and system for correcting a distorted input image |
US8928730B2 (en) | 2012-07-03 | 2015-01-06 | DigitalOptics Corporation Europe Limited | Method and system for correcting a distorted input image |
JP5984574B2 (en) * | 2012-08-14 | 2016-09-06 | キヤノン株式会社 | Imaging system, control method therefor, and imaging apparatus |
JP6097522B2 (en) * | 2012-10-22 | 2017-03-15 | キヤノン株式会社 | Image blur correction apparatus, image blur correction method, and imaging apparatus |
CN113472989A (en) | 2012-11-28 | 2021-10-01 | 核心光电有限公司 | Multi-aperture imaging system and method for acquiring images by multi-aperture imaging system |
CN103853908B (en) * | 2012-12-04 | 2017-11-14 | 中国科学院沈阳自动化研究所 | A kind of maneuvering target tracking method of adaptive interaction formula multi-model |
US9071756B2 (en) | 2012-12-11 | 2015-06-30 | Facebook, Inc. | Systems and methods for digital video stabilization via constraint-based rotation smoothing |
US9232138B1 (en) | 2013-06-03 | 2016-01-05 | Amazon Technologies, Inc. | Image stabilization techniques |
CN108989647B (en) | 2013-06-13 | 2020-10-20 | 核心光电有限公司 | Double-aperture zooming digital camera |
US10474921B2 (en) | 2013-06-14 | 2019-11-12 | Qualcomm Incorporated | Tracker assisted image capture |
EP3779565A3 (en) | 2013-07-04 | 2021-05-05 | Corephotonics Ltd. | Miniature telephoto lens assembly |
US9857568B2 (en) | 2013-07-04 | 2018-01-02 | Corephotonics Ltd. | Miniature telephoto lens assembly |
CN108718376B (en) | 2013-08-01 | 2020-08-14 | 核心光电有限公司 | Thin multi-aperture imaging system with auto-focus and method of use thereof |
WO2015048694A2 (en) | 2013-09-27 | 2015-04-02 | Pelican Imaging Corporation | Systems and methods for depth-assisted perspective distortion correction |
US9952756B2 (en) | 2014-01-17 | 2018-04-24 | Intel Corporation | Dynamic adjustment of a user interface |
CN103984929B (en) | 2014-05-20 | 2017-04-19 | 清华大学 | Method and system for correcting distorted fingerprints |
US10133351B2 (en) | 2014-05-21 | 2018-11-20 | Apple Inc. | Providing haptic output based on a determined orientation of an electronic device |
US9357132B2 (en) * | 2014-05-30 | 2016-05-31 | Apple Inc. | Video rolling shutter correction for lens movement in optical image stabilization cameras |
US9413963B2 (en) * | 2014-05-30 | 2016-08-09 | Apple Inc. | Video image stabilization |
US20150362989A1 (en) | 2014-06-17 | 2015-12-17 | Amazon Technologies, Inc. | Dynamic template selection for object detection and tracking |
WO2015198478A1 (en) | 2014-06-27 | 2015-12-30 | 株式会社 市川ソフトラボラトリー | Image distortion correction apparatus, information processing apparatus and image distortion correction method |
EP3167417A1 (en) | 2014-07-11 | 2017-05-17 | Google, Inc. | Hands-free offline communications |
CN105306804B (en) * | 2014-07-31 | 2018-08-21 | 北京展讯高科通信技术有限公司 | Intelligent terminal and its video image stabilization method and device |
US9392188B2 (en) | 2014-08-10 | 2016-07-12 | Corephotonics Ltd. | Zoom dual-aperture camera with folded lens |
US9596411B2 (en) * | 2014-08-25 | 2017-03-14 | Apple Inc. | Combined optical and electronic image stabilization |
WO2016061565A1 (en) | 2014-10-17 | 2016-04-21 | The Lightco Inc. | Methods and apparatus for using a camera device to support multiple modes of operation |
US20170351932A1 (en) | 2014-12-19 | 2017-12-07 | Nokia Technologies Oy | Method, apparatus and computer program product for blur estimation |
JP6530602B2 (en) * | 2014-12-22 | 2019-06-12 | キヤノン株式会社 | Image pickup apparatus and control method thereof |
WO2016107635A1 (en) | 2014-12-29 | 2016-07-07 | Metaio Gmbh | Method and system for generating at least one image of a real environment |
GB2533788A (en) | 2014-12-30 | 2016-07-06 | Nokia Technologies Oy | Method for determining the position of a portable device |
CN107209404B (en) | 2015-01-03 | 2021-01-15 | 核心光电有限公司 | Miniature telephoto lens module and camera using the same |
JP7106273B2 (en) | 2015-01-27 | 2022-07-26 | インターデジタル マディソン パテント ホールディングス， エスアーエス | Methods, systems and apparatus for electro-optical and opto-electrical conversion of images and video |
KR101914894B1 (en) | 2015-04-02 | 2018-11-02 | 코어포토닉스 리미티드 | Dual voice coil motor structure of dual optical module camera |
WO2016168415A1 (en) | 2015-04-15 | 2016-10-20 | Lytro, Inc. | Light guided image plane tiled arrays with dense fiber optic bundles for light-field and high resolution image acquisition |
KR101963546B1 (en) | 2015-04-16 | 2019-03-28 | 코어포토닉스 리미티드 | Auto focus and optical imagestabilization in a compact folded camera |
CN110687655B (en) | 2015-05-28 | 2022-10-21 | 核心光电有限公司 | Bi-directional stiffness for optical image stabilization and auto-focus in dual aperture digital cameras |
KR101992040B1 (en) | 2015-06-24 | 2019-06-21 | 코어포토닉스 리미티드 | Low-Profile 3-Axis Actuator for Foldable Lens Cameras |
KR102253997B1 (en) | 2015-08-13 | 2021-05-20 | 코어포토닉스 리미티드 | Dual aperture zoom camera with video support and switching/non-switching dynamic control |
KR102225727B1 (en) | 2015-09-06 | 2021-03-10 | 코어포토닉스 리미티드 | Auto focus and optical image stabilization with roll compensation in a compact folded camera |
KR102457617B1 (en) | 2015-09-16 | 2022-10-21 | 한화테크윈 주식회사 | Method and apparatus of estimating a motion of an image, method and apparatus of image stabilization and computer-readable recording medium for executing the method |
US9967461B2 (en) * | 2015-10-14 | 2018-05-08 | Google Inc. | Stabilizing video using transformation matrices |
CN106709932B (en) | 2015-11-12 | 2020-12-04 | 创新先进技术有限公司 | Face position tracking method and device and electronic equipment |
US9953217B2 (en) | 2015-11-30 | 2018-04-24 | International Business Machines Corporation | System and method for pose-aware feature learning |
US9674439B1 (en) | 2015-12-02 | 2017-06-06 | Intel Corporation | Video stabilization using content-aware camera motion estimation |
KR102140882B1 (en) | 2015-12-29 | 2020-08-04 | 코어포토닉스 리미티드 | Dual-aperture zoom digital camera with automatic adjustable tele field of view |
US9773196B2 (en) | 2016-01-25 | 2017-09-26 | Adobe Systems Incorporated | Utilizing deep learning for automatic digital image segmentation and stylization |
US10194089B2 (en) | 2016-02-08 | 2019-01-29 | Qualcomm Incorporated | Systems and methods for implementing seamless zoom function using multiple cameras |
JP6640620B2 (en) * | 2016-03-17 | 2020-02-05 | ソニーモバイルコミュニケーションズ株式会社 | Image stabilizing device, image stabilizing method, and electronic device |
US10482663B2 (en) | 2016-03-29 | 2019-11-19 | Microsoft Technology Licensing, Llc | Virtual cues for augmented-reality pose alignment |
CN105741789B (en) | 2016-05-06 | 2018-06-01 | 京东方科技集团股份有限公司 | A kind of driving method and driving device of high dynamic contrast display screen |
US10027893B2 (en) | 2016-05-10 | 2018-07-17 | Nvidia Corporation | Real-time video stabilization for mobile devices based on on-board motion sensing |
CN111965919B (en) | 2016-05-30 | 2022-02-08 | 核心光电有限公司 | Rotary ball guided voice coil motor |
CN109639954B (en) | 2016-06-19 | 2020-10-23 | 核心光电有限公司 | Frame synchronization system and method in dual aperture camera |
US9774798B1 (en) | 2016-06-29 | 2017-09-26 | Essential Products, Inc. | Apparatus and method for a wide field of view image sensor |
CN107770433B (en) | 2016-08-15 | 2020-08-04 | 广州立景创新科技有限公司 | Image acquisition device and image smooth scaling method thereof |
US9888179B1 (en) | 2016-09-19 | 2018-02-06 | Google Llc | Video stabilization for mobile devices |
KR20180073327A (en) | 2016-12-22 | 2018-07-02 | 삼성전자주식회사 | Display control method, storage medium and electronic device for displaying image |
US10104334B2 (en) | 2017-01-27 | 2018-10-16 | Microsoft Technology Licensing, Llc | Content-adaptive adjustment of display device brightness levels when rendering high dynamic range content |
EP3579040B1 (en) | 2017-02-23 | 2021-06-23 | Corephotonics Ltd. | Folded camera lens designs |
US10645286B2 (en) | 2017-03-15 | 2020-05-05 | Corephotonics Ltd. | Camera with panoramic scanning range |
CN106954024B (en) * | 2017-03-28 | 2020-11-06 | 成都通甲优博科技有限责任公司 | Unmanned aerial vehicle and electronic image stabilizing method and system thereof |
US10222862B2 (en) | 2017-06-05 | 2019-03-05 | Lenovo (Singapore) Pte. Ltd. | Method to sense force exertion on handheld device for behavior setting and input for quick response |
US11295119B2 (en) | 2017-06-30 | 2022-04-05 | The Johns Hopkins University | Systems and method for action recognition using micro-doppler signatures and recurrent neural networks |
US10957297B2 (en) | 2017-07-25 | 2021-03-23 | Louis Yoelin | Self-produced music apparatus and method |
US10462370B2 (en) | 2017-10-03 | 2019-10-29 | Google Llc | Video stabilization |
US10979814B2 (en) | 2018-01-17 | 2021-04-13 | Beijing Xiaoniao Tingling Technology Co., LTD | Adaptive audio control device and method based on scenario identification |
US10310079B1 (en) | 2018-03-02 | 2019-06-04 | Amazon Technologies, Inc. | Presence detection using wireless signals confirmed with ultrasound and/or audio |
US10171738B1 (en) | 2018-05-04 | 2019-01-01 | Google Llc | Stabilizing video to reduce camera and face movement |
US10665250B2 (en) | 2018-09-28 | 2020-05-26 | Apple Inc. | Real-time feedback during audio recording, and related devices and systems |
US20200265211A1 (en) | 2019-02-14 | 2020-08-20 | West Virginia University | Fingerprint distortion rectification using deep convolutional neural networks |
US10665204B1 (en) | 2019-10-08 | 2020-05-26 | Capital One Services, Llc | Automatically adjusting screen brightness based on screen content |
US10726579B1 (en) | 2019-11-13 | 2020-07-28 | Honda Motor Co., Ltd. | LiDAR-camera calibration |
US11190689B1 (en) | 2020-07-29 | 2021-11-30 | Google Llc | Multi-camera video stabilization |
CN111738230B (en) | 2020-08-05 | 2020-12-15 | 深圳市优必选科技股份有限公司 | Face recognition method, face recognition device and electronic equipment |
US11380181B2 (en) | 2020-12-09 | 2022-07-05 | MS Technologies | Doppler radar system with machine learning applications for fall prediction and detection |
-
2017
- 2017-10-03 US US15/724,241 patent/US10462370B2/en active Active
-
2018
- 2018-07-31 CN CN202110599340.0A patent/CN113194260B/en active Active
- 2018-07-31 CN CN201880030074.6A patent/CN111133747B/en active Active
- 2018-07-31 WO PCT/US2018/044524 patent/WO2019070333A1/en unknown
- 2018-07-31 EP EP18762179.2A patent/EP3603049A1/en active Pending
- 2018-07-31 JP JP2019562259A patent/JP6818912B2/en active Active
-
2019
- 2019-09-12 US US16/568,931 patent/US11064119B2/en active Active
-
2021
- 2021-06-01 US US17/336,194 patent/US11683586B2/en active Active
-
2023
- 2023-06-16 US US18/210,812 patent/US20230336873A1/en active Pending
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9407827B2 (en) * | 2011-11-14 | 2016-08-02 | Dxo Labs | Method and system for capturing sequences of images with compensation for variations in magnification |
CN104769934A (en) * | 2012-11-08 | 2015-07-08 | 苹果公司 | Super-resolution based on optical image stabilization |
CN104079819A (en) * | 2013-03-26 | 2014-10-01 | 佳能株式会社 | Image processing apparatus and method, and image capturing apparatus |
US20160006935A1 (en) * | 2014-07-06 | 2016-01-07 | Apple Inc. | Low Light Video Image Stabilization Strength Modulation |
US20170244881A1 (en) * | 2016-02-19 | 2017-08-24 | Fotonation Limited | Method of Stabilizing a Sequence of Images |
Also Published As
Publication number | Publication date |
---|---|
US10462370B2 (en) | 2019-10-29 |
US11064119B2 (en) | 2021-07-13 |
JP2020520603A (en) | 2020-07-09 |
EP3603049A1 (en) | 2020-02-05 |
JP6818912B2 (en) | 2021-01-27 |
US20200007770A1 (en) | 2020-01-02 |
CN113194260B (en) | 2024-04-16 |
WO2019070333A1 (en) | 2019-04-11 |
US20190104255A1 (en) | 2019-04-04 |
CN111133747A (en) | 2020-05-08 |
US20210289139A1 (en) | 2021-09-16 |
US11683586B2 (en) | 2023-06-20 |
CN111133747B (en) | 2021-06-22 |
US20230336873A1 (en) | 2023-10-19 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN111133747B (en) | Method and device for stabilizing video | |
US11856295B2 (en) | Multi-camera video stabilization | |
US9979889B2 (en) | Combined optical and electronic image stabilization | |
KR102003012B1 (en) | Systems and methods for digital video stabilization via constraint-based rotation smoothing | |
CN103563349B (en) | Video signal stabilization method and device | |
JP5243477B2 (en) | Blur correction apparatus and blur correction method | |
EP2688282B1 (en) | Image processing device, image processing method, and image processing program | |
CN107566688B (en) | Convolutional neural network-based video anti-shake method and device and image alignment device | |
JP2008118644A (en) | Method for estimating point spread function of blurred digital image, and medium readable by one or a plurality of computers and having computer-readable command for carrying out method for estimating point spread function of blurred digital image | |
US20170374256A1 (en) | Method and apparatus for rolling shutter compensation | |
WO2013151873A1 (en) | Joint video stabilization and rolling shutter correction on a generic platform | |
CN110493522A (en) | Anti-fluttering method and device, electronic equipment, computer readable storage medium | |
US11823359B2 (en) | Systems and methods for leveling images | |
US20220174217A1 (en) | Image processing method and device, electronic device, and computer-readable storage medium | |
KR101741150B1 (en) | An imaging photographing device and an imaging photographing method using an video editing | |
KR101725932B1 (en) | An imaging photographing device and an imaging photographing method using an video editing | |
JP2020102786A (en) | Image processing device |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |