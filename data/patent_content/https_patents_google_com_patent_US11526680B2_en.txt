US11526680B2 - Pre-trained projection networks for transferable natural language representations - Google Patents
Pre-trained projection networks for transferable natural language representations Download PDFInfo
- Publication number
- US11526680B2 US11526680B2 US16/790,917 US202016790917A US11526680B2 US 11526680 B2 US11526680 B2 US 11526680B2 US 202016790917 A US202016790917 A US 202016790917A US 11526680 B2 US11526680 B2 US 11526680B2
- Authority
- US
- United States
- Prior art keywords
- projection
- input
- layer
- neural network
- network
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
- 238000012549 training Methods 0.000 claims abstract description 114
- 238000000034 method Methods 0.000 claims abstract description 74
- 230000006870 function Effects 0.000 claims description 161
- 238000013528 artificial neural network Methods 0.000 claims description 153
- 239000013598 vector Substances 0.000 claims description 75
- 230000000875 corresponding effect Effects 0.000 claims description 22
- 238000000605 extraction Methods 0.000 claims description 22
- 238000003058 natural language processing Methods 0.000 claims description 17
- 238000005070 sampling Methods 0.000 claims description 5
- 230000002596 correlated effect Effects 0.000 claims description 4
- 230000003068 static effect Effects 0.000 claims description 4
- 238000011068 loading method Methods 0.000 claims description 2
- 230000001537 neural effect Effects 0.000 abstract description 15
- 239000010410 layer Substances 0.000 description 369
- 230000015654 memory Effects 0.000 description 32
- 239000011159 matrix material Substances 0.000 description 25
- 230000008569 process Effects 0.000 description 19
- 230000000306 recurrent effect Effects 0.000 description 13
- 238000012545 processing Methods 0.000 description 12
- 230000008901 benefit Effects 0.000 description 10
- 230000004913 activation Effects 0.000 description 9
- 238000001994 activation Methods 0.000 description 9
- 238000004891 communication Methods 0.000 description 9
- 238000010586 diagram Methods 0.000 description 9
- 230000009466 transformation Effects 0.000 description 9
- 230000004044 response Effects 0.000 description 8
- 238000013459 approach Methods 0.000 description 7
- 238000002372 labelling Methods 0.000 description 7
- 238000010801 machine learning Methods 0.000 description 5
- 238000010606 normalization Methods 0.000 description 5
- 241000282326 Felis catus Species 0.000 description 4
- 238000013500 data storage Methods 0.000 description 4
- 238000013136 deep learning model Methods 0.000 description 4
- 238000001514 detection method Methods 0.000 description 4
- 230000000694 effects Effects 0.000 description 4
- 230000008909 emotion recognition Effects 0.000 description 4
- 239000004744 fabric Substances 0.000 description 4
- 235000013305 food Nutrition 0.000 description 4
- 238000013507 mapping Methods 0.000 description 4
- 238000007781 pre-processing Methods 0.000 description 4
- 238000004088 simulation Methods 0.000 description 4
- 238000013138 pruning Methods 0.000 description 3
- 238000013519 translation Methods 0.000 description 3
- ORILYTVJVMAKLC-UHFFFAOYSA-N Adamantane Natural products C1C(C2)CC3CC1CC2C3 ORILYTVJVMAKLC-UHFFFAOYSA-N 0.000 description 2
- 241000283070 Equus zebra Species 0.000 description 2
- 235000009499 Vanilla fragrans Nutrition 0.000 description 2
- 244000263375 Vanilla tahitensis Species 0.000 description 2
- 235000012036 Vanilla tahitensis Nutrition 0.000 description 2
- 230000009471 action Effects 0.000 description 2
- 230000004075 alteration Effects 0.000 description 2
- 238000004590 computer program Methods 0.000 description 2
- 238000013527 convolutional neural network Methods 0.000 description 2
- 230000036541 health Effects 0.000 description 2
- 230000005055 memory storage Effects 0.000 description 2
- 235000019699 ravioli Nutrition 0.000 description 2
- 230000006403 short-term memory Effects 0.000 description 2
- 239000004753 textile Substances 0.000 description 2
- 238000007792 addition Methods 0.000 description 1
- 230000003190 augmentative effect Effects 0.000 description 1
- 230000001143 conditioned effect Effects 0.000 description 1
- 230000001276 controlling effect Effects 0.000 description 1
- 238000013135 deep learning Methods 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 238000003745 diagnosis Methods 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 238000004880 explosion Methods 0.000 description 1
- 230000006872 improvement Effects 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 238000012886 linear function Methods 0.000 description 1
- 238000005259 measurement Methods 0.000 description 1
- 230000003278 mimic effect Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 238000005457 optimization Methods 0.000 description 1
- 239000002356 single layer Substances 0.000 description 1
- 230000002123 temporal effect Effects 0.000 description 1
- 238000000844 transformation Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/205—Parsing
- G06F40/216—Parsing using statistical methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/082—Learning methods modifying the architecture, e.g. adding, deleting or silencing nodes or connections
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/213—Feature extraction, e.g. by transforming the feature space; Summarisation; Mappings, e.g. subspace methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/21—Design or setup of recognition systems or techniques; Extraction of features in feature space; Blind source separation
- G06F18/214—Generating training patterns; Bootstrap methods, e.g. bagging or boosting
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/268—Morphological analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/284—Lexical analysis, e.g. tokenisation or collocates
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/289—Phrasal analysis, e.g. finite state techniques or chunking
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/289—Phrasal analysis, e.g. finite state techniques or chunking
- G06F40/295—Named entity recognition
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/42—Data-driven translation
- G06F40/44—Statistical methods, e.g. probability models
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/55—Rule-based translation
- G06F40/56—Natural language generation
-
- G06K9/6256—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/047—Probabilistic or stochastic networks
-
- G06N3/0472—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/82—Arrangements for image or video recognition or understanding using pattern recognition or machine learning using neural networks
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/01—Protocols
- H04L67/04—Protocols specially adapted for terminals or networks with limited capabilities; specially adapted for terminal portability
Definitions
- the present disclosure relates generally to machine learning. More particularly, the present disclosure relates to systems and methods to pre-train projection networks for use as transferable natural language representation generators.
- Neural language representations are at the core of many state-of-the-art natural language processing models.
- a widely used approach is to pretrain, store, and look up word or character embedding matrices.
- Some popular word embeddings are word2vec, GloVe, and ELMO.
- Approaches such as these which rely on pre-computed word embeddings can help initialize neural models, lead to faster convergence, and have improved performance for numerous application such as Question Answering, Summarization, Sentiment Analysis, and other similar tasks.
- Such pre-computed language representations occupy huge amounts of memory.
- one entry e.g., a d-dimensional embedding vector
- the amount of memory required to store the embedding matrix grows proportional with vocabulary size.
- a large amount of memory is required for a vocabulary of average size. Pruning the vocabulary may result in reduced memory requirements, but will also hinder the ability of the language representations to generalize to unknown words.
- a look up operation is required to obtain the embedding for a particular language input (e.g., word) from the embedding matrix.
- One example aspect of the present disclosure is directed to a computing system that includes one or more processors and one or more non-transitory computer-readable media that collectively store a pre-trained projection network.
- the pre-trained projection network is configured to receive a language input comprising one or more units of text and to dynamically generate an intermediate representation from the language input.
- the pre-trained projection network includes a sequence of one or more projection layers, wherein each projection layer is configured to receive a layer input and apply a plurality of projection layer functions to the layer input to generate a projection layer output.
- the pre-trained projection network includes a sequence of one or more intermediate layers configured to receive the projection layer output generated by a last projection layer in the sequence of one or more projection layers and to generate one or more intermediate layer outputs, wherein the intermediate representation comprises the intermediate layer output generated by a last intermediate layer in the sequence of one or more intermediate layers.
- the computer-readable media collective store instructions that, when executed by the one or more processors, cause the computing system to perform operations.
- the operations include obtaining the language input; inputting the language input into the pre-trained projection network; and receiving the intermediate representation as an output of the pre-trained projection network.
- Another example aspect of the present disclosure is directed to a computer-implemented method to pre-train a projection network comprising one or more projection layers and one or more intermediate layers, each projection layer configured to apply one or more projection functions to project a layer input into a different dimensional space, the projection network configured to receive an input and to generate an intermediate representation for the input.
- the method includes accessing, by one or more computing devices, a set of training data comprising a plurality of example inputs.
- the method includes inputting, by the one or more computing devices, each of the plurality of example inputs into the projection network.
- the method includes receiving, by the one or more computing devices, a respective intermediate representation for each of the plurality of example inputs as an output of the projection network.
- the method includes inputting, by the one or more computing devices, each respective intermediate representation into a decoder model configured to reconstruct inputs based on intermediate representations.
- the method includes receiving, by the one or more computing devices, a respective reconstructed input for each of the plurality of example inputs as an output of the decoder model.
- the method includes learning, by the one or more computing devices, one or more parameter values for the one or more intermediate layers of the projection network based at least in part on a comparison of each respective reconstructed input to the corresponding example input.
- Another example aspect of the present disclosure is directed to computer-implemented method to pre-train a projection network comprising one or more projection layers and one or more intermediate layers, each projection layer configured to apply one or more projection functions to project a layer input into a different dimensional space, the projection network configured to receive an input and to generate an intermediate representation for the input.
- the method includes accessing, by one or more computing devices, a set of training data comprising a plurality of input words, wherein a respective set of ground truth context words are associated with each of the plurality of input words.
- the method includes inputting, by the one or more computing devices, each of the plurality of input words into the projection network.
- the method includes receiving, by the one or more computing devices, a respective intermediate representation for each of the plurality of input words as an output of the projection network.
- the method includes determining, by the one or more computing devices, a set of predicted context words for each of the plurality of input words based at least in part on the respective intermediate representation for each of the plurality of input words.
- the method includes learning, by the one or more computing devices, one or more parameter values for the one or more intermediate layers of the projection network based at least in part on a comparison, for each input word, of the respective set of predicted context words to the respective set of ground truth context words.
- FIG. 1 shows an example projection neural network system according to example embodiments of the present disclosure.
- FIG. 2 depicts a block diagram of an example data flow for processing a projection layer input to determine a projection layer output according to example embodiments of the present disclosure.
- FIG. 3 depicts an example Self-Governing Neural Network according to example embodiments of the present disclosure.
- FIG. 4 depicts an example Projection Sequence Network according to example embodiments of the present disclosure.
- FIGS. 5 A and 5 B depict block diagrams of example data flows for training a projection neural network according to example embodiments of the present disclosure
- FIGS. 6 A-C depict block diagrams of example computing systems and devices according to example embodiments of the present disclosure.
- FIG. 7 depicts an example neural projection skip-gram model according to example embodiments of the present disclosure.
- FIG. 8 depicts an example neural projection auto-encoder architecture according to example embodiments of the present disclosure.
- FIG. 9 depicts learning a classifier model with pre-trained neural projections according to example embodiments of the present disclosure.
- the present disclosure is directed to systems and methods to pre-train projection networks for use as transferable natural language representation generators.
- example pre-training schemes described herein enable learning of transferable deep neural projection representations over randomized locality sensitive hashing (LSH) projections, thereby surmounting the need to store any embedding matrices because the projections can be dynamically computed at inference time.
- LSH locality sensitive hashing
- aspects of the present disclosure are directed to techniques for pre-training a projection network to produce an intermediate representation based on a language input. Once pre-trained, the intermediate representations generated by the projection network can be transferred or “plugged in” for use in performing any number of different natural language processing (NLP) tasks.
- NLP natural language processing
- a projection network can include one or more projection layers and one or more intermediate layers.
- Each of the projection layers can apply one or more projection layer functions to project a layer input into a compact low-dimensional space.
- the projection layer functions can be modeled using LSH techniques and can be dynamically computed from the input.
- the one or more intermediate layers can follow the one or more projection layers and can process a projection layer output generated by a last projection layer to produce the intermediate representation.
- the intermediate representation can be the output of a last intermediate layer.
- the intermediate layers can be artificial neural network layers such as multi-layer perceptron layers.
- some or all of the intermediate layers e.g., all of the intermediate layers except the last intermediate layer
- a projection network can include projection layer(s) which project an input using projection functions and can further include intermediate layer(s) which include learnable parameters (e.g., weights, biases, and the like) that allow the projection network to be trainable and learn to produce powerful intermediate representations that can be easily plugged into NLP tasks and existing deep learning models.
- learnable parameters e.g., weights, biases, and the like
- a projection network can be pre-trained as part of an autoencoder model that can be trained on unsupervised text.
- a projection network can be used to generate (e.g., encode) a language input (e.g., an input sentence) into an intermediate representation (e.g., a sentence representation).
- a decoder model e.g., a recurrent neural network such as a long short-term memory network
- the projection network and the decoder model can be jointly trained (e.g., as an end-to-end autoencoder) to maximize a probability of the reconstructed language input matching the original language input (e.g., on a token-by-token basis).
- the projection network can learn (in an unsupervised fashion) to produce intermediate representations (e.g., sentence representations) which encode sufficient information about the language inputs to enable reconstruction of the language input.
- a projection network can be pretrained in the form of a neural projection skip-gram model.
- a skip-gram based architecture can be coupled with projection layers (e.g., that perform LSH projections) to learn efficient and dynamically computable representations.
- a training dataset can include a plurality of input words and a plurality of sets of ground truth context words that respectively surround the plurality of input words within a training corpora.
- a projection network can receive one of the input words and can generate an intermediate representation for the input word.
- a skip-gram function can be used to generate a set of predicted context words that are predicted to surround the input word based on the intermediate representation for the input word.
- Parameters of the projection network can be learned based on an objective function that compares the set of predicted context words for each input word to the set of ground truth context words for the input word.
- the parameters of the projection network can be learned through optimization of a negative sampling objective function that, in addition to the set of ground truth context words for an input word, compares the set of predicted context words for the input word to one or more sets of ground truth context words associated with other, different input words.
- the objective function used to learn the parameters of the projection network can further include a regularization term that provides a penalty that has a magnitude that is positively correlated with a sum of a cosine similarity between the respective intermediate representations produced by the projection network for each pair of words in a training batch.
- a regularization term that provides a penalty that has a magnitude that is positively correlated with a sum of a cosine similarity between the respective intermediate representations produced by the projection network for each pair of words in a training batch.
- various perturbations can be applied to the training data (e.g., the input words) to enable the learned network to better generalize to out of vocabulary words and misspellings. Applying perturbations in this fashion can also assist in generating systems that are more resistant to adversarial attacks in natural language, for example, as compared to existing models like LSTMs.
- the projection network can then be used as a transferable representation generator.
- the pre-trained projection network can be easily plugged into various different NLP tasks and existing deep learning models.
- one or more machine-learned prediction models can be trained to produce predictions (e.g., classifications) based on the intermediate representations produced by a pre-trained projection network for a given language input.
- the intermediate representations produced by a pre-trained projection network can be directly used to perform various tasks such as, for example, clustering and/or similarity search.
- the systems and methods of the present disclosure provide a number of technical effects and benefits.
- the pre-trained projection networks do not need to store lookup tables. Instead, the language representations are computed on-the-fly and require low memory footprint.
- aspects of the present disclosure enable the generation and use of language representations with significantly reduced memory requirements, which make the techniques described herein significantly more suitable for use on-device or in other resource-constrained environments.
- the pre-training schemes described herein enable training of the projection networks to produce language representations in an unsupervised fashion.
- significant amounts of time and resources do not need to be dedicated to manual labelling.
- the projection networks can be pre-trained using unsupervised techniques and then re-fined and/or paired with one or more specialized prediction models (e.g., classification heads) trained using a very small amount of labeled training data.
- specialized prediction models e.g., classification heads
- the language representations provided by the projection networks can be easily transferred between or applied to many different NLP tasks.
- a single projection network can be used to generate language representations which can then be used by different downstream models/processes to perform different NLP tasks.
- only a single projection network needs to be trained, rather than multiple different models.
- only a single projection network needs to be run to produce an inference, rather than multiple different models. This saves computing resources since a fewer number of training iterations and/or model runs need to be performed, thereby conserving memory space, processing power, energy expenditure, and the like.
- projection models reduce the memory occupied by the model from O(
- refers to the vocabulary size and refers to number of projection operation.
- a projection network as described in this specification can enable a system to perform tasks faster and with a performance level (e.g., a prediction accuracy) that is similar to that of much larger and more complex conventional neural networks (i.e., neural networks that do not contain projection layers, as described in this specification), while consuming fewer computational resources (e.g., memory and computing power).
- a projection network can enable a system to perform tasks (e.g., text classification) with a performance level comparable to that of a larger neural network, despite the projection network having several orders of magnitude fewer parameters than the larger neural network.
- Projection networks can perform tasks faster and consume fewer computational resources than conventional neural networks because they include projection layers.
- Projection networks as described herein can be more resistant to adversarial attacks in natural language, for example, as compared to existing models like LSTMs.
- the pre-training techniques and resulting projection networks described herein are not limited to natural language processing tasks. Instead, the projection networks can be pre-trained and used to generate intermediate representations of many different modalities of data include image data (e.g., video), audio data (e.g., speech data), and/or other forms of data.
- image data e.g., video
- audio data e.g., speech data
- One example task that can be performed using pre-trained projection networks is text classification.
- the system including the projection network is given text and/or intermediate features derived from text and considers all terms and/or features to make a single classification (e.g., binary or multi-class classification).
- the input is a text and the output is a class label.
- Example applications of the classification task in natural language processing include: dialog act classification; humor & sarcasm detection; sentiment analysis; question classification; news headline classification; emotion recognition; health notifications; intent classification (dialog); and automated essay scoring.
- TeleCorp confirms it is buying ad platform AppCorp, reportedly for between $1.6B$2B
- sequence labeling Another example task that can be performed by a system that includes a pre-trained projection network is sequence labeling.
- the system including the projection network is given text and/or intermediate features derived from text and identifies segments (e.g., sequences of words/phrases) and classifies those segments into multiple classes.
- the input is a text and the output is labeled sequences.
- Example applications of the sequence labeling task in natural language processing include: named entity recognition; keyphrase extraction; noun phrase extraction; chunking; relation extraction; semantic parsing; slot extraction in dialog systems; product (shopping) & attribute extraction; and aspect based sentiment analysis. Additional example applications include translating text between languages; text completion (e.g., sentence completion and/or automatic reply); or other generative tasks. Some example tasks and applications are as follows.
- CorporationA was acquired by CorporationB to create synergies.
- Input Reserve a flight from San Francisco to LAX for two people after 3 pm.
- the task is to identify every entity E and attribute A pair towards which an opinion is expressed in the given text.
- Input I like to dance the tango.
- a zebra has black and white
- FIG. 1 shows an example projection neural network system 100 .
- the projection neural network system 100 is an example of a system implemented as computer programs on one or more computers in one or more locations in which the systems, components, and techniques described below are implemented.
- the projection neural network system 100 includes a projection neural network 102 .
- the projection neural network 102 can be a feed-forward neural network, a recurrent neural network, or any other appropriate type of neural network.
- the projection neural network 102 is configured to receive a projection neural network input 104 and to generate a projection network output 106 from the projection network input 104 .
- the projection neural network input 104 can be any kind of digital data input
- the projection network output 106 can be any kind of score, classification, or regression output based on the input.
- the system 100 described herein is widely applicable and is not limited to one specific implementation. However, for illustrative purposes, a small number of example implementations are described below.
- the output generated by the projection neural network 102 for a given image may be scores for each of a set of object categories, with each score representing an estimated likelihood that the image contains an image of an object belonging to the category.
- the output generated by the projection neural network 102 may be a score for each of a set of pieces of text in another language, with each score representing an estimated likelihood that the piece of text in the other language is a proper translation of the input text into the other language.
- the output generated by the projection neural network 102 may be a score for each of a set of pieces of text, each score representing an estimated likelihood that the piece of text is the correct transcript for the utterance.
- the output generated by the projection neural network 102 may be a score for each of a set of possible diagnoses for the condition of a user, with the score representing an estimated likelihood that the diagnosis is accurate.
- the output generated by the projection neural network 102 may be a score for each of a set of possible responses to the received communication, with the score representing an estimated likelihood that the response matches a user's intent.
- the projection neural network 102 includes a sequence of one or more projection layers (e.g., the projection layer 108 ). Although only a single projection layer 108 is illustrated, the projection neural network 102 can include any number of projection layers (e.g., stacked one after the other).
- the projection layer 108 is configured to receive a projection layer input 110 , and to process the projection layer input 110 in accordance with current values of projection layer parameters to generate a projection layer output 112 .
- the projection layer input 110 may be the projection network input 104 (i.e., if the projection layer 108 is the first layer in the projection network 102 ) or the output of another layer of the projection network 102 (e.g., a conventional layer or another projection layer).
- the projection layer input 110 and the projection layer output 112 may be represented in any appropriate numerical format, for example, as vectors or as matrices.
- FIG. 2 is a block diagram of an example data flow 200 by which a projection layer (e.g., the projection layer 108 ) can process a projection layer input 110 to determine a projection layer output 112 .
- a projection layer e.g., the projection layer 108
- FIG. 2 is a block diagram of an example data flow 200 by which a projection layer (e.g., the projection layer 108 ) can process a projection layer input 110 to determine a projection layer output 112 .
- the projection layer input may be the projection network input or the output of another layer of the projection network, and may be represented in any appropriate numerical format (e.g., as a vector or as a matrix).
- the projection layer provides the projection layer input 110 to each of one or more projection layer functions (e.g., 202 , 204 , 206 ). Each of the projection layer functions processes the projection layer input 110 to generate a respective projection function output (e.g., 208 , 210 , 212 ).
- each projection function can generate the corresponding projection function output by mapping the projection layer input to a different space.
- the dimensionality of the projection function outputs are much less (e.g., by several orders of magnitude) than the dimensionality of the projection layer input 110 .
- each of the projection function outputs (e.g., 208 , 210 , 212 ) is a bit vector.
- each projection function may be defined by a matrix.
- the rows (or columns) of a matrix defining a projection function can be referred to as projection vectors associated with the projection function.
- a projection function may process the projection layer input by determining dot products (i.e., inner products) between the projection layer input and each of the projection vectors associated with the projection function.
- ⁇ , ⁇ > denotes the dot product operation between vectors
- a projection function may process the projection layer input by determining whether the dot product between the projection layer input and each of the projection vectors results in positive or negative values.
- a dot product between the projection layer input and a projection vector results in a positive value
- a first value may be assigned to a corresponding position in the projection function output.
- a second value may be assigned to a corresponding position in the projection function output.
- the projection function output is a binary representation (i.e., a vector with components consisting of 0s and 1s) of the projection layer input.
- the projection functions may be locality sensitive hashing functions.
- the projection function defined by (2) may be an example of a locality sensitive hashing function.
- locality sensitive hashing functions allows projection of similar inputs or intermediate network layers into hidden unit vectors that are nearby in metric space. This allows transformation of the inputs and learning of an efficient and compact network representation that is only dependent on the inherent dimensionality (i.e., observed features) of the data rather than the number of instances or the dimensionality of the actual data vector (i.e., overall feature or vocabulary size). For example, this can be achieved with binary hash functions for the projection functions.
- Projection functions may be selected so that: (i) processing a projection layer input by a projection function to generate a projection function output is computationally efficient (e.g., requires few arithmetic operations), (ii) data defining a projection function can be efficiently stored (e.g., in a logical data storage area or physical data storage device), or both.
- one or more of the projection functions may be defined by sparse matrices (i.e., matrices with only a few non-zero entries). If a projection function is defined by a sparse matrix, then processing the projection layer input by the projection function to generate the projection function output is computationally efficient. Specifically, since the results of many of the arithmetic operations involved in computing the projection function output have value zero (due to the sparsity of the matrix defining the projection function), these arithmetic operations do not actually need to be performed.
- a sparse matrix may be stored as a list of tuples, where each tuple includes an index of a location in the sparse matrix and a corresponding value of the sparse matrix at the index. Since sparse matrices have only a few non-zero entries, such a representation occupies less memory than, for example, a representation of a dense matrix that must include the index of every location in the dense matrix and the corresponding value of the dense matrix at the index.
- the system may determine the values of the components of the matrices defining one or more of the projection functions based on the values of a set of seed parameters.
- the seed parameters are represented as numerical values and the number of seed parameters is typically much smaller than the dimensionality of the matrices defining the projection functions.
- the system may, for example, determine the values of the components of the matrices defining the projection functions based on the outputs of random (or pseudo-random) number generators that are initialized using the seed parameters.
- the random (or pseudo-random) number generators are configured to generate Normally-distributed random numbers (i.e., random numbers drawn from a Normal distribution)
- Normally-distributed random numbers i.e., random numbers drawn from a Normal distribution
- the system can reduce the computational requirements of projection layers compared to conventional layers. For example, the system can reduce the amount of memory storage required for projection layers compared to conventional layers, since only the values of the seed parameters must be stored, as compared to some conventional layers that require storing entire dense matrices of conventional layer parameter values. As another example, the system can reduce the latency in generating layer outputs compared to conventional layers, since the system can dynamically compute the values of the components of the matrices defining the projection functions. In contrast, for some conventional layers, the system reads the conventional layer parameter values from memory, which may be a substantially slower process than dynamically computing these values (i.e., as in an example projection layer).
- the values of the parameters defining the projection layer functions may be predetermined, that is, may be fixed before the projection network is trained, and are not adjusted during training.
- the projection layer can concatenate the projection function outputs and apply the projection layer parameters 214 (e.g., a parameter matrix and a bias vector) to the concatenated projection function outputs.
- the projection layer parameters 214 e.g., a parameter matrix and a bias vector
- the projection layer can generate the projection layer output by applying projection layer parameters to the projection function outputs.
- the projection layer parameters may include a parameter matrix and a bias vector
- the dimensionality of the projection function outputs is generally much lower than the dimensionality of the projection layer input. Therefore, the number of projection layer parameters that are applied to the projection function outputs to generate the projection layer outputs is generally much lower than the number of parameters that are applied to layer inputs by conventional neural network layers (e.g., fully-connected layers) that do not include projection functions.
- the dimensionality of the projection layer parameters defined by the parameter matrix W and the bias vector b in the projection layer may be much smaller than the dimensionality of the corresponding layer parameters of a conventional neural network layer.
- the projection neural network 102 can include a stack of one or more additional hidden layers (e.g., hidden layer 114 ) connected to the sequence of one or more projection layers (e.g., projection layer 108 ).
- additional hidden layers e.g., hidden layer 114
- the one or more hidden layers can include different types of layers including fully connected layers (e.g., featuring non-linear activations), recurrent layers, convolutional layers, additional projection layers, projection sequence layers (described in further detail below), other forms of additional layers, and/or various combinations thereof.
- the sequence of the one or more additional hidden layers can be configured to receive a layer output generated by a highest projection layer in the sequence of one or more projection layers and to generate one or more additional hidden layer outputs. That is, each hidden layer (e.g., hidden layer 114 ) can receive a hidden layer input (e.g., hidden layer input 116 ) and process the layer input to provide a hidden layer output (e.g., hidden layer output 118 ).
- each hidden layer e.g., hidden layer 114
- a hidden layer input e.g., hidden layer input 116
- a hidden layer output e.g., hidden layer output 118
- the projection neural network 102 can include an output layer 120 .
- the output layer 120 can be configured to receive the additional hidden layer output generated by a highest additional hidden layer in the sequence of one or more additional hidden layers and to generate the projection network output 106 .
- a layer of the projection network 102 can serve as the output layer 120 if the output of such layer is included in the projection network output 106 .
- An output layer may be a softmax layer, a projection layer, or any other appropriate neural network layer.
- the output layer 120 may be configured to receive as input an output generated by a projection layer or a conventional layer.
- the system 100 can be implemented in a resource-constrained environment (e.g., a smartwatch or smartphone) more readily than conventional neural network systems.
- a resource-constrained environment e.g., a smartwatch or smartphone
- data defining the parameters of the system 100 can occupy much less storage capacity than data defining the parameters of a conventional neural network system.
- a graph is a data structure that may be represented by a set of nodes (where each node may be associated with a numerical feature vector), a set of edges (where each edge may be associated with a numerical edge strength value), and in some cases, a set of labels.
- the nodes represent entities (e.g., people, objects, locations, or concepts), the edges represent relationships between the entities represented by the nodes (e.g., a “friend” relationship between two people in a social network), and in some cases, the labels may represent characteristics of the nodes (e.g., whether a product represented by a node is a “best-selling” product).
- entities e.g., people, objects, locations, or concepts
- the edges represent relationships between the entities represented by the nodes (e.g., a “friend” relationship between two people in a social network)
- the labels may represent characteristics of the nodes (e.g., whether a product represented by a node is a “best-selling” product).
- Graph-based systems can be trained by machine learning techniques (e.g., supervised or semi-supervised machine learning techniques) to make predictions. For example, a graph-based system may generate a prediction for a value of a label associated with a previously unlabeled node in a graph. In this example, the graph-based system may generate a prediction for a value of a label associated with a given node based on the labels of the neighboring nodes (i.e., nodes that are connected to the given node by an edge) and the edge strengths of the edges connecting the given node to the neighboring nodes.
- machine learning techniques e.g., supervised or semi-supervised machine learning techniques
- a graph-based system can incorporate the operations performed by a projection layer by, for example, generating a projection graph that is a compact representation of a trainer graph.
- Each node of the projection graph may correspond to a different node of the trainer graph, and the feature vectors associated with the nodes of the projection graph may be determined by performing projection layer operations. More specifically, the feature vector associated with a particular node of the projection graph may be determined by applying multiple projection functions followed by a transformation (e.g., defined by a parameter matrix and a bias vector) to the feature vector associated with the corresponding node in the trainer graph. In this manner, the feature vectors associated with the nodes of the projection graph may have a much smaller dimensionality than the feature vectors associated with the nodes of the trainer graph. Therefore, similar to a projection neural network, the projection graph may be suitable for deployment to resource constrained environments (e.g., mobile devices) where the trainer graph could not be deployed.
- resource constrained environments e.g., mobile devices
- system 100 described herein is widely applicable and is not limited to one specific implementation. However, other examples of specific implementations (in addition to those described above) may be as described below.
- the output generated by the projection neural network 102 for a given Internet resource, document, or portion of a document may be a score for each of a set of topics, with each score representing an estimated likelihood that the Internet resource, document, or document portion is about the topic.
- the output generated by the projection neural network 102 may be a score that represents an estimated likelihood that the particular advertisement will be clicked on.
- the output generated by the projection neural network 102 may be a score for each of a set of content items, with each score representing an estimated likelihood that the user will respond favorably to being recommended the content item.
- Example implementations of the network structure shown in FIG. 1 include the Self-Governing network described herein.
- a Self-Governing neural network can include multi-layered locality-sensitive projection model.
- the self-governing property of this network stems from its ability to learn a model (e.g., a classifier) without having to initialize, load, or store any feature or vocabulary weight matrices.
- the SGNN represents a truly embedding-free approach, which is in contrast with the majority of the widely-used state-of-the-art deep learning techniques in natural language processing whose performance depends on embeddings pre-trained on large corpora.
- the SGNN uses the projection functions to dynamically transform each input to a low-dimensional representation. Furthermore, these projection layer(s) can be stacked with additional layers and non-linear activations to achieve deep, non-linear combinations of projections that permit the network to learn complex mappings from inputs x i to outputs y i .
- i p [ 1 ( x i ), . . .
- i p refers to the output of projection operation applied to input x i
- h p is applied to projection output
- h t is applied at intermediate layers of the network with depth k followed by a final softmax activation layer at the top.
- W p , W t , W o and b p , b t , b o represent trainable weights and biases respectively.
- the projection transformations can use pre-computed parameterized functions, i.e., they are not trained during the learning process, and their outputs can be concatenated to form the hidden units for subsequent operations.
- each input text x i can be converted to an intermediate feature vector (e.g., via raw text features such as skip-grams) followed by projections.
- the intermediate feature vector can include one or more of the following intermediate features generated from or associated with the input text: skip-grams; n-grams; part of speech tags; dependency relationships; knowledge graph information; and/or contextual information.
- the SGNN network can be trained from scratch on the task data using a supervised loss defined with respect to ground truth ⁇ i .
- a loss function that can be used is as follows:
- FIG. 5 A An example training structure according to the above-described scheme is provided in FIG. 5 A .
- FIG. 5 B shows an alternative training structure.
- the network learns to choose and apply specific projection operations j (via activations) that are more predictive for a given task.
- the choice of the type of projection matrix as well as representation of the projected space has a direct effect on computation cost and model size.
- an efficient randomized projection method can be leveraged and a binary representation ⁇ 0,1 ⁇ d can be used for . This yields a drastically lower memory footprint both in terms of number and size of parameters.
- an efficient randomized projection method can be employed for the projection step.
- LSH locality sensitive hashing
- LSH enables the network to project similar inputs ⁇ right arrow over (x) ⁇ i or intermediate network layers into hidden unit vectors that are nearby in metric space.
- the random projection vector k never needs to be explicitly stored since they can be computed on the fly using hash functions over feature indices with a fixed row seed rather than invoking a random number generator. This also permit performance of projection operations that are linear in the observed feature size rather than the overall feature or vocabulary size which can be prohibitively large for high-dimensional data, thereby saving both memory and computation cost.
- SGNN can efficiently model high-dimensional sparse inputs and large vocabulary sizes common for text applications instead of relying on feature pruning or other pre-processing heuristics employed to restrict input sizes in standard neural networks for feasible training.
- the binary representation is significant since this results in a significantly compact representation for the projection network parameters that in turn considerably reduces the model size.
- T and d can be varied depending on the projection network parameter configuration specified for and can be tuned to trade-off between prediction quality and model size. Note that the choice of whether to use a single projection matrix of size T ⁇ d or T separate matrices of d columns depends on the type of projection employed (dense or sparse).
- FIG. 3 depicts an example SGNN 250 according to example embodiments of the present disclosure.
- the SGNN 250 receives an input, which, in some implementations, can be sequential in nature (e.g., words in a sequence).
- the SGNN 250 can first include a feature extractor 252 .
- the feature extractor 252 can be viewed as part of the network 250 while in other instances the feature extractor 252 is viewed as a pre-processing step for the network 250 .
- the feature extractor 252 can extract an intermediate feature vector from the input.
- the feature extractor 252 can provide the intermediate feature vector to a projection layer 254 .
- the input to the projection layer 254 can include a single input vector that has been generated on the basis of the entire input x 1 , . . . , x n and additional features derived from or otherwise associated with the input. Such multiple types of information can be concatenated to form the single input vector.
- the SGNN 250 can process the single input vector to produce a classification output that classifies the input as a whole.
- the projection layer 254 can project the received vector into a lower-dimensional space, for example as described with reference to FIG. 2 .
- the example SGNN includes one additional hidden layer, shown here as a fully connected layer 256 .
- a softmax output layer 258 is a prediction (e.g., text classification) for the input over K classes.
- the compact bit units can be used to represent the projection in SGNN.
- the network can learn to move the gradients for points that are nearby to each other in the projected bit space in the same direction.
- the SGNN network can be trained end-to-end using backpropagation. Training can progress efficiently, for example, with stochastic gradient descent with distributed computing on high-performance CPUs or GPUs.
- the overall complexity for SGNN inference, governed by the projection layer, is O(n ⁇ T ⁇ d), where n is the observed feature size (not the overall vocabulary size) which is linear in input size, d is the number of LSH bits specified for each projection vector k , and T is the number of projection functions used in .
- the model size (in terms of number of parameters) and memory storage required for the projection inference step is O(T ⁇ d ⁇ C), where C is the number of hidden units in h p in the multi-layer projection network.
- One example task that can be performed by SGNNs is text classification.
- the neural network is given text and/or intermediate features derived from text and considers all terms and/or features to make a single classification (e.g., binary or multi-class classification).
- the input is a text and the output is a class label.
- Example applications of the classification task in natural language processing include: dialog act classification; humor & sarcasm detection; sentiment analysis; question classification; news headline classification; emotion recognition; health notifications; intent classification (dialog); and automated essay scoring.
- TeleCorp confirms it is buying ad platform AppCorp, reportedly for between $1.6B$2B
- ProSeqoNets can include one or more projection layers followed by, for example, one or more projection sequence layers.
- Each projection sequence layer can pass information forward and/or backward to subsequent and/or previous iterations of such layer as a sequential input is input into the network over a series of time steps.
- each projection sequence layer can include a first set of nodes that pass information forward to subsequent iterations and/or receive information from previous iterations and also a second set of nodes that receive information passed back from subsequent iterations and/or pass information backward to previous iterations.
- FIG. 4 depicts an example projection sequence network 270 according to example embodiments of the present disclosure.
- FIG. 4 depicts n iterations of the projection sequence network 270 implemented over n times steps relative to n inputs from a sequential input source.
- the input to projection sequence network 270 can be sequential in nature and the projection sequence network 270 can operate iteratively (e.g., at each of a plurality of time steps) to process the sequential input.
- the projection sequence network 270 can operate iteratively (e.g., at each of a plurality of time steps) to process the sequential input.
- one input portion x i of the input can be input at each of a plurality of iterations.
- input portion x i can be input.
- information from past time steps e.g., the raw text and/or intermediate feature data
- the projection sequence network 270 can include a feature extractor 272 .
- the feature extractor 272 can extract an intermediate feature vector from the input.
- the feature extractor 272 can provide the intermediate feature vector to a projection layer 274 .
- the feature extractor 272 can be viewed as part of the network 270 while in other instances the feature extractor 272 is viewed as a pre-processing step for the network 270 .
- additional features can be provided as initial or intermediate input to the projection layer 274 in addition to the base input.
- these additional features can be included in one or more additional feature vectors.
- the input to the projection layer 274 can include multiple feature vectors which may expressed according to different dimensions. These feature vectors may or may not include type information that describes the type of features.
- the input to projection layer 274 includes multiple feature vectors
- input of such vectors into the network can be handled in a number of different ways.
- the multiple feature vectors can be concatenated and flattened to form a single input vector.
- each feature vector can be separately input into the projection layer 274 and separately projected by the projection layer 274 .
- the outputs of the projection layer 274 can be concatenated in the projected space (e.g., the bit space).
- some projection functions and/or bit space positions can be reserved and used for encoding the type information respectively associated with the feature vectors, such that the network 270 (e.g., the lowest projection sequence layer 276 ) can learn, in the projected space, to choose or ignore various information based on its feature type as it relates to the input or other feature types.
- the projection layer 274 can project the received vector into a lower-dimensional space, for example as described with reference to FIG. 2 .
- the example projection sequence network 270 includes two projection sequence layers, shown here as projection sequence layers 276 and 278 . Although two projection sequence layers are shown, the network can include any number of projection sequence layer(s).
- a final layer 280 is a final layer 280 .
- the output of the final layer 280 (e.g., over the time steps) is an output sequence (and scores).
- Each projection sequence layer (e.g., 276 , 278 ) can pass information forward and/or backward to subsequent and/or previous iterations of such layer as a sequential input is input into the network over a series of time steps.
- each projection sequence layer can include a first set of nodes that pass information forward to subsequent iterations and/or receive information from previous iterations and also a second set of nodes that receive information passed back from subsequent iterations and/or pass information backward to previous iterations.
- projection sequence layer 276 includes a first set of nodes 282 that pass internal state information from time step 1 to themselves in the subsequent iteration of the layer 276 at time step 2 .
- Projection sequence layer 276 includes a second set of nodes 284 that receive internal state information from time step 2 from the same nodes but in the subsequent iteration of the layer 276 at time step 2 .
- a second set of nodes 284 receives internal state information from time step 2 from the same nodes but in the subsequent iteration of the layer 276 at time step 2 .
- one or more (e.g., all) of the nodes included in the first set of nodes 282 and/or the second set of nodes 284 can be or include recurrent cells that have been modified for inclusion in the projection sequence layer.
- Example recurrent cells include LSTM cells and gated recurrent units (GRUs).
- the projection state can be used to modify the internal state or dependencies of the cell rather than the base input features.
- sequence labeling One example task that can be performed by ProSeqoNets is sequence labeling.
- the neural network is given text and/or intermediate features derived from text and identifies segments (e.g., sequences of words/phrases) and classifies those segments into multiple classes.
- segments e.g., sequences of words/phrases
- the input is a text and the output is labeled sequences.
- Example applications of the sequence labeling task in natural language processing include: named entity recognition; keyphrase extraction; noun phrase extraction; chunking; relation extraction; semantic parsing; slot extraction in dialog systems; product (shopping) & attribute extraction; and aspect based sentiment analysis.
- Additional example applications of the ProSeqoNets include translating text between languages; text completion (e.g., sentence completion and/or automatic reply); or other generative tasks. Some example tasks and applications are as follows.
- CorporationA was acquired by CorporationB to create synergies.
- Input Reserve a flight from San Francisco to LAX for two people after 3 pm.
- the task is to identify every entity E and attribute A pair towards which an opinion is expressed in the given text.
- Input I like to dance the tango.
- a zebra has black and white
- FIG. 5 A is a block diagram of an example data flow by which a projection neural network system (e.g., the projection neural network system 100 ) can train a projection neural network (e.g., the projection neural network 102 ).
- the system provides a training input 302 from a set of training data 304 to the projection network 102 .
- the projection network 102 processes the training input 302 in accordance with current values of projection network parameters to generate a projection network output 106 .
- the system updates the current values of the projection network parameters by computing a gradient (e.g., by a backpropagation procedure) of a loss functions that depends on the projection network output 106 and a target output 310 .
- a gradient e.g., by a backpropagation procedure
- the system can update the current values of the projection network parameters by a gradient of a loss function (referred to as a projection prediction loss function) that depends on an error between the projection network output 106 and the target output 310 .
- a projection prediction loss function a loss function that depends on an error between the projection network output 106 and the target output 310 .
- Updating the current values of the projection network parameters by the gradient of the projection prediction loss function can cause the projection network 102 to generate an output that is more similar to the target output 310 (i.e., in response to processing the training input 302 ).
- the projection neural network 302 can be trained based solely on its own performance relative to the training data 304 as compared with the target output.
- FIG. 5 B is a block diagram of an alternative example data flow by which a projection neural network system (e.g., the projection neural network system 100 ) can train a projection neural network (e.g., the projection neural network 102 ).
- a projection neural network system e.g., the projection neural network system 100
- a projection neural network e.g., the projection neural network 102
- the system provides a training input 302 from a set of training data 304 to the projection network 102 .
- the projection network 102 processes the training input 302 in accordance with current values of projection network parameters to generate a projection network output 106 .
- the system provides the same training input 302 to a trainer network 306 .
- the trainer network 306 processes the training input 302 in accordance with current values of trainer network parameters to generate a trainer network output 308 .
- the trainer network 306 can be a feed-forward neural network, a recurrent neural network, or any other appropriate type of neural network that is configured to generate the same kinds of outputs as the projection network 102 given the same training input. In general, the trainer network 306 has more parameters (in some cases, by several orders of magnitude) than the projection network 102 .
- the system jointly updates the current values of the projection network parameters and the trainer network parameters by computing gradients (e.g., by a backpropagation procedure) of a combination of several different loss functions.
- the loss functions can depend on the projection network output 106 , the trainer network output 308 , or both.
- the system can update the current values of the trainer network parameters by a gradient of a loss function ⁇ (referred to as a trainer prediction loss function) that depends on an error between the trainer network output 308 and the target output 310 associated with the training input 302 in the training data 304 .
- the target output 310 is an output that should be generated by the trainer neural network 306 and the projection network 102 in response to processing the training input 302 .
- Updating the current values of the trainer network parameters by the gradient of the trainer prediction loss function ⁇ can cause the trainer network 306 to generate an output that is more similar to target output 310 (i.e., in response to processing the training input 302 ).
- the system can update the current values of the projection network parameters by a gradient of a loss function (referred to as a projection prediction loss function) that depends on an error between the projection network output 106 and the target output 310 . Updating the current values of the projection network parameters by the gradient of the projection prediction loss function can cause the projection network 102 to generate an output that is more similar to the target output 310 (i.e., in response to processing the training input 302 ).
- a projection prediction loss function a gradient of a loss function that depends on an error between the projection network output 106 and the target output 310 . Updating the current values of the projection network parameters by the gradient of the projection prediction loss function can cause the projection network 102 to generate an output that is more similar to the target output 310 (i.e., in response to processing the training input 302 ).
- the system can also update the current values of the projection network parameters (and, optionally, the trainer network parameters) by a gradient of a loss function p (referred to as a projection simulation loss function) that depends on an error between the trainer network output 308 and the projection network output 106 . Updating the current values of the projection network parameters by the gradient of the projection simulation loss function p can cause the projection network 102 to generate an output that is more similar to the trainer network output 308 .
- a gradient of a loss function p referred to as a projection simulation loss function
- the system can (in some cases) enable the projection network 102 to generate predictions that are similar in accuracy to the predictions generated by the trainer network 306 , despite the projection network 102 having far fewer parameters (e.g., in some cases, by several orders of magnitude) than the trainer network 306 .
- the trainer prediction loss function ⁇ , the projection prediction loss function , and the projection simulation loss function p can be any appropriate loss functions.
- the loss functions may be cross-entropy loss functions.
- the loss functions may be squared-error loss functions.
- the system may adjust the parameters of the projection network 102 and the trainer network 306 by a weighted combination of gradients of each of the described loss functions.
- a graph-based system that generates a projection graph as a compact representation of a trainer graph, can jointly train the projection graph, the trainer graph, and the parameters of the projection layer operations used to determine the projection graph from the trainer graph.
- the parameters of the projection layer operations can be trained (e.g., by a backpropagation procedure) to cause the predictions generated by the graph-based system based on the projection graph to become more similar to the predictions generated by the graph-based system based on the trainer graph.
- the similarity between predictions may be measured by a loss function, such as a cross-entropy loss function.
- the predictions generated by the graph-based system based on the projection graph and the trainer graph may be predicted labels for labeled or unlabeled nodes in the projection graph and trainer graph respectively.
- aspects of the present disclosure are directed to a skip-gram based architecture coupled with Locality-Sensitive Hashing (LSH) projections to learn efficient dynamically computable representations.
- LSH Locality-Sensitive Hashing
- the proposed models do not need to store lookup tables as representations are computed on-the-fly and require low memory footprint.
- the representations can be trained in an unsupervised fashion and can be easily transferred to other NLP tasks.
- example aspects are directed to a novel model (NP-SG) to learn compact neural representations that combines the benefit of representation learning approaches like skipgram model with efficient LSH projections that can be computed on-the-fly.
- NP-SG novel model
- W t is the window size randomly sampled from the set 1-N, where N is the maximum window size.
- v(w) can be replaced with, as one example, a deep n-layer MLP over the binary projection, P(w) as shown in the equation below.
- v P ( w ) ( f n ( P ( w )) where v P (w) ⁇ d , f n is a n-layer deep neural network encoder with ReLU non-linear activations after each layer except for the last layer as shown in FIG. 7 . refers to a normalization applied to the final layer of f n . Batch-normalization, L2-normalization, or layer normalization can be used.
- the binary projection P (w) can be computed using locality-sensitive projection operations (described in further detail herein) which can be performed on-the-fly (i.e., without any embedding look up) to yield a fixed, low-memory footprint binary vector.
- Example NP-SG models can create a trainable deep projection representation for words using LSH projections over character-level features combined with contextual information learned via the skip-gram architecture.
- a similar approach as provided for the base skip gram model can be used for training the neural projection skip-gram model (NP-SG).
- the training objective can be defined to maximize the probability of predicting the context words given the current word.
- the model tries to learn the word embeddings by maximizing the objective, J( ⁇ ) known as negative sampling (NEG), given by the equation below.
- v p (w) can be projected in a narrow sub-space where the cosine similarities of all the words in the dataset were too close to 1:0. This can make the convergence slower and lead to poor generalization. Therefore, care can be taken to avoid the projections having this characteristic, as is described below.
- an additional explicit regularizing L2-loss function can be introduced. With the assumption that the words in each mini-batch are randomly sampled, an L2-loss over the cosine similarities between all the words within a mini-batch can be added, as shown in the equations below.
- CS(w_i,w_j) refers to the cosine similarity between w_i and w_j
- mb refers to the mini-batch size
- w_mb refers to the words in the mini-batch.
- this can be enforced using a simple outer-product trick.
- the cosine-similarities between all the words within a mini-batch can be extracted in a single shot by computing the outer-product of the L2 row normalized word representations corresponding to each minibatch ⁇ circumflex over (v) ⁇ _P(w_mb), as shown in the following equation:
- NP-SG model does not have a fixed vocabulary size, applications of the model can be flexible and leverage a lot more information during training compared to standard skip-gram models which require vocabulary pruning for feasibility.
- the dataset can be augmented with inputs words after applying character level perturbations to them.
- the perturbations are such a way that they are commonly occurring misspellings in documents.
- Example types of perturbation operation that can be performed are as follows:
- insert(word, n) Randomly choose n chars from the character vocabulary and insert them randomly into the input word. Ignore the locations of first and last character in the word for the insert operation. Example transformation: sample>samnple.
- swap(word, n) Randomly swap the location of two characters in the word n times. As with the insert operation, ignore the first and last character in the word for the swap operation.
- Example transformation sample>sapmle.
- duplicate(word, n) Randomly duplicate a character in the word by n times.
- Example transformation sample>saample.
- drop(sentence, n) Randomly drop n words from the sentence.
- Example transformation This is a sample sentence>This is a sentence.
- duplicate(sentence, n) Similar to duplicate(word, n) above, we randomly duplicate a word in the sentence n times.
- Example transformation This is a sample sentence>This is a sample sample sentence.
- swap(sentence, n) Similar to swap(word, n), randomly swap the location of two words in the sentence n times.
- Example transformation This is a sample sentence>This sample a is sentence.
- One example training setup is as follows: Train skipgram models on the wikipedia data XML dump, enwik91. Extract the normalized English text from the XML dump using the Matt Mahoneys pre-processing perl script2. Fix the vocabulary to the top 100 k frequently occurring words. Sub-sample words in the training corpus, dropping them with some probability based on their frequency of occurrence in the corpus. Perturb the input words with some probability.
- Another example aspect of the present disclosure is directed to a novel autoencoder projections model that can be trained on an unsupervised text.
- This section describes a trainable and plug-able version of LSH projection representation. Unlike certain other example implementations that train neural models with static projections in strictly supervised settings, a key advantage of this model is that it can be pre-trained in an unsupervised fashion on a large corpus to obtain better text representations.
- a projection auto-encoder architecture is proposed as shown in FIG. 8 that (1) learns better text representations from unsupervised text, and (2) can be plugged in and used to initialize input representations for models in other supervised NLP tasks.
- One example architecture uses a deep projection encoder, modeled via a deep MLP on top of the LSH projections. All the layers have the ReLU non-linearity activation except for the final layer. The final layer is a full connected layer without any non-linearities.
- the final layer of the encoder, E(P(w)) acts as a sentence representation which can be plugged in as the input representation in other models as shown in FIG. 9 .
- the deep projection encoder can be combined with a decoder, an LSTM model, that predicts the tokens present in the sentences conditioned on the final layer of the encoder.
- ⁇ 1 , ⁇ 2 , . . . , ⁇ n-1 ,E ( ( w ))) Softmax( g ( ⁇ n-1 ,s n ,E ( ( w )))) where s_n and g denote the hidden state of the decoder at time n, and a non-linear function, respectively.
- One example training setup is as follows: Train the projection auto-encoder by learning to reconstruct sentences from the enwik9 dataset. Restrict the vocabulary to top 30000 frequently occurring words and a max sentence length of 20.
- the decoder can be a single layer LSTM with hidden size, 256.
- a vanilla projection model e.g., SGNN
- FIG. 9 illustrates this.
- the final layer of the neural network can apply Softmax to generate a probability distribution over output classes.
- the classifier models can be trained to minimize negative log-likelihood over correct classes in the training data.
- FIG. 6 A depicts a block diagram of an example computing system 600 according to example embodiments of the present disclosure.
- the system 600 includes a user computing device 602 , a server computing system 630 , and a training computing system 650 that are communicatively coupled over a network 680 .
- the user computing device 602 can be any type of computing device, such as, for example, a personal computing device (e.g., laptop or desktop), a mobile computing device (e.g., smartphone or tablet), a gaming console or controller, a wearable computing device, an embedded computing device, or any other type of computing device.
- a personal computing device e.g., laptop or desktop
- a mobile computing device e.g., smartphone or tablet
- a gaming console or controller e.g., a gaming console or controller
- a wearable computing device e.g., an embedded computing device, or any other type of computing device.
- the user computing device 602 includes one or more processors 612 and a memory 614 .
- the one or more processors 612 can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, a FPGA, a controller, a microcontroller, etc.) and can be one processor or a plurality of processors that are operatively connected.
- the memory 614 can include one or more non-transitory computer-readable storage mediums, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof.
- the memory 614 can store data 616 and instructions 618 which are executed by the processor 612 to cause the user computing device 602 to perform operations.
- the user computing device 602 can store or include one or more projection neural networks 620 .
- the projection neural networks 620 can be or can otherwise include various machine-learned models such as neural networks (e.g., deep neural networks) or other types of machine-learned models, including non-linear models and/or linear models.
- Neural networks can include feed-forward neural networks, recurrent neural networks (e.g., long short-term memory recurrent neural networks), convolutional neural networks or other forms of neural networks.
- the one or more projection neural networks 620 can be received from the server computing system 630 over network 680 , stored in the user computing device memory 614 , and then used or otherwise implemented by the one or more processors 612 .
- the user computing device 602 can implement multiple parallel instances of a single projection neural network 620 .
- one or more projection neural networks 640 can be included in or otherwise stored and implemented by the server computing system 630 that communicates with the user computing device 602 according to a client-server relationship.
- the projection neural networks 640 can be implemented by the server computing system 640 as a portion of a web service.
- one or more projection neural networks 620 can be stored and implemented at the user computing device 602 and/or one or more projection neural networks 640 can be stored and implemented at the server computing system 630 .
- the user computing device 602 can also include one or more user input component 622 that receives user input.
- the user input component 622 can be a touch-sensitive component (e.g., a touch-sensitive display screen or a touch pad) that is sensitive to the touch of a user input object (e.g., a finger or a stylus).
- the touch-sensitive component can serve to implement a virtual keyboard.
- Other example user input components include a microphone, a traditional keyboard, or other means by which a user can provide user input.
- the server computing system 630 includes one or more processors 632 and a memory 634 .
- the one or more processors 632 can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, a FPGA, a controller, a microcontroller, etc.) and can be one processor or a plurality of processors that are operatively connected.
- the memory 634 can include one or more non-transitory computer-readable storage mediums, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof.
- the memory 634 can store data 636 and instructions 638 which are executed by the processor 632 to cause the server computing system 630 to perform operations.
- the server computing system 630 includes or is otherwise implemented by one or more server computing devices. In instances in which the server computing system 630 includes plural server computing devices, such server computing devices can operate according to sequential computing architectures, parallel computing architectures, or some combination thereof.
- the server computing system 630 can store or otherwise include one or more machine-learned projection neural networks 640 .
- the projection neural networks 640 can be or can otherwise include various machine-learned models.
- Example machine-learned models include neural networks or other multi-layer non-linear models.
- Example neural networks include feed forward neural networks, deep neural networks, recurrent neural networks, and convolutional neural networks.
- the user computing device 602 and/or the server computing system 630 can train the projection neural networks 620 and/or 640 via interaction with the training computing system 650 that is communicatively coupled over the network 680 .
- the training computing system 650 can be separate from the server computing system 630 or can be a portion of the server computing system 630 .
- the training computing system 650 includes one or more processors 652 and a memory 654 .
- the one or more processors 652 can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, a FPGA, a controller, a microcontroller, etc.) and can be one processor or a plurality of processors that are operatively connected.
- the memory 654 can include one or more non-transitory computer-readable storage mediums, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof.
- the memory 654 can store data 656 and instructions 658 which are executed by the processor 652 to cause the training computing system 650 to perform operations.
- the training computing system 650 includes or is otherwise implemented by one or more server computing devices.
- the training computing system 650 can include a model trainer 660 that trains the machine-learned projection neural networks 620 and/or 640 stored at the user computing device 602 and/or the server computing system 630 using various training or learning techniques, such as, for example, backwards propagation of errors.
- performing backwards propagation of errors can include performing truncated backpropagation through time.
- the model trainer 660 can perform a number of generalization techniques (e.g., weight decays, dropouts, etc.) to improve the generalization capability of the models being trained.
- the model trainer 660 can train the projection neural networks 620 and/or 640 based on a set of training data 662 .
- the training data 662 can include, for example, textual input that has been labeled with target outputs or otherwise has target outputs associated therewith.
- the target outputs can be text classifications and/or segment classifications.
- the training examples can be provided by the user computing device 602 .
- the projection neural network 620 provided to the user computing device 602 can be trained by the training computing system 650 on user-specific data received from the user computing device 602 . In some instances, this process can be referred to as personalizing the model.
- the model trainer 660 includes computer logic utilized to provide desired functionality.
- the model trainer 660 can be implemented in hardware, firmware, and/or software controlling a general purpose processor.
- the model trainer 660 includes program files stored on a storage device, loaded into a memory and executed by one or more processors.
- the model trainer 660 includes one or more sets of computer-executable instructions that are stored in a tangible computer-readable storage medium such as RAM hard disk or optical or magnetic media.
- the network 680 can be any type of communications network, such as a local area network (e.g., intranet), wide area network (e.g., Internet), or some combination thereof and can include any number of wired or wireless links.
- communication over the network 680 can be carried via any type of wired and/or wireless connection, using a wide variety of communication protocols (e.g., TCP/IP, HTTP, SMTP, FTP), encodings or formats (e.g., HTML, XML), and/or protection schemes (e.g., VPN, secure HTTP, SSL).
- FIG. 6 A illustrates one example computing system that can be used to implement the present disclosure.
- the user computing device 602 can include the model trainer 660 and the training dataset 662 .
- the projection neural networks 620 can be both trained and used locally at the user computing device 602 .
- the user computing device 602 can implement the model trainer 660 to personalize the projection neural networks 620 based on user-specific data.
- FIG. 6 B depicts a block diagram of an example computing device 60 according to example embodiments of the present disclosure.
- the computing device 60 can be a user computing device or a server computing device.
- the computing device 60 includes a number of applications (e.g., applications 1 through N). Each application contains its own machine learning library and machine-learned model(s). For example, each application can include a machine-learned model.
- Example applications include a text messaging application, an email application, a dictation application, a virtual keyboard application, a browser application, etc.
- each application can communicate with a number of other components of the computing device, such as, for example, one or more sensors, a context manager, a device state component, and/or additional components.
- each application can communicate with each device component using an API (e.g., a public API).
- the API used by each application is specific to that application.
- FIG. 6 C depicts a block diagram of an example computing device 690 according to example embodiments of the present disclosure.
- the computing device 690 can be a user computing device or a server computing device.
- the computing device 690 includes a number of applications (e.g., applications 1 through N). Each application is in communication with a central intelligence layer.
- Example applications include a text messaging application, an email application, a dictation application, a virtual keyboard application, a browser application, etc.
- each application can communicate with the central intelligence layer (and model(s) stored therein) using an API (e.g., a common API across all applications).
- the central intelligence layer includes a number of machine-learned models. For example, as illustrated in FIG. 6 C , a respective machine-learned model (e.g., a model) can be provided for each application and managed by the central intelligence layer. In other implementations, two or more applications can share a single machine-learned model. For example, in some implementations, the central intelligence layer can provide a single model (e.g., a single model) for all of the applications. In some implementations, the central intelligence layer is included within or otherwise implemented by an operating system of the computing device 690 .
- a respective machine-learned model e.g., a model
- two or more applications can share a single machine-learned model.
- the central intelligence layer can provide a single model (e.g., a single model) for all of the applications.
- the central intelligence layer is included within or otherwise implemented by an operating system of the computing device 690 .
- the central intelligence layer can communicate with a central device data layer.
- the central device data layer can be a centralized repository of data for the computing device 690 . As illustrated in FIG. 6 C , the central device data layer can communicate with a number of other components of the computing device, such as, for example, one or more sensors, a context manager, a device state component, and/or additional components. In some implementations, the central device data layer can communicate with each device component using an API (e.g., a private API).
- an API e.g., a private API
- This specification describes a projection neural network implemented as computer programs on one or more computers in one or more locations.
- a system including one or more computers and one or more storage devices storing instructions that when executed by the one or more computers cause the one or more computers to implement a projection neural network.
- the projection neural network is configured to receive a projection network input and to generate a projection network output from the projection network input.
- the projection neural network includes a sequence of one or more projection layers, wherein each projection layer has multiple projection layer parameters.
- Each projection layer is configured to receive a layer input and apply multiple projection layer functions to the layer input.
- Each projection layer function generates a respective projection function output that projects the layer input to a different space.
- Each projection layer generates a layer output by applying the projection layer parameters for the projection layer to the projection function outputs.
- the projection neural network further includes an output layer configured to receive a layer output generated by a highest projection layer in the sequence and to generate the projection network output.
- the output layer is a softmax output layer.
- the projection neural network includes only the sequence of projection layers and the output layer.
- a layer output of the highest projection layer in the sequence is the projection network output.
- the projection neural network includes only the sequence of projection layers.
- the layer input of a lowest projection layer in the sequence is a network input to the projection neural network.
- the layer input of any projection layer other than the lowest projection layer is a layer output generated by the projection layer immediately below the projection layer in the sequence.
- each projection function is associated with a respective set of projection vectors. Applying each projection function to the layer input includes, for each projection vector: (i) determining a dot product between the layer input and the projection vector, (ii) when the dot product is negative, assigning a first value to a corresponding position in the projection function output, and (iii) when the dot product is positive, assigning a second value to the corresponding position in the projection function output.
- the projection functions are each encoded as sparse matrices and are used to generate a binary representation from the layer input.
- the projection layer parameters include a parameter matrix and a bias vector.
- Generating the layer output by applying the projection layer parameters for the projection layer to the projection function outputs includes applying the parameter matrix to the projection function outputs and then adding the bias vector to the result.
- a method of training the projection neural network includes receiving a training input and a target output for the training input.
- the training input is processed using the projection neural network in accordance with current values of the projection layer parameters to generate a projection network output for the training input.
- the training input is processed using a trainer neural network having multiple trainer neural network parameters.
- the trainer neural network is configured to process the training input in accordance with current values of the trainer neural network parameters to generate a trainer network output that is specific to the particular machine learning task.
- a gradient is determined with respect to the trainer neural network parameters of a loss function that depends on an error between the target output and the trainer network output.
- the output generated by the trainer neural network is a soft target output.
- the method further includes determining a gradient with respect to the trainer network parameters of the loss function that depends on the error between the trainer network output and the projection network output.
- the update to the current values of the trainer network parameters is also based on the gradient with respect to the trainer network parameters of the loss function that depends on the error between the trainer network output and the projection network output.
- the method further includes determining a gradient with respect to the projection layer parameters of a loss function that depends on an error between the target output and the projection network output.
- the update to the current values of the projection layer parameters is also based on the gradient with respect to the projection layer parameters of the loss function that depends on the error between the target output and the projection network output.
- a system including one or more computers and one or more storage devices storing instructions that when executed by the one or more computers cause the one or more computers to perform the operations of the previously described method.
- one or more computer storage media storing instructions that when executed by one or more computers cause the one or more computers to perform the operations of the previously described method.
- one or more computer storage media storing instructions that when executed by one or more computers cause the one or more computers to implement the previously described projection neural network.
- a projection network as described in this specification can perform tasks faster and with a performance level (e.g., a prediction accuracy) that is similar to that of much larger and more complex conventional neural networks (i.e., neural networks that do not contain projection layers, as described in this specification), while consuming fewer computational resources (e.g., memory and computing power).
- a projection network can perform tasks (e.g., image classification) with a performance level comparable to that of a larger neural network, despite the projection network having several orders of magnitude fewer parameters than the larger neural network.
- Projection networks can perform tasks faster and consume fewer computational resources than conventional neural networks because they include projection layers.
- a projection layer reduces the dimensionality of a projection layer input by processing the projection layer input by projection layer functions.
- the projection layer functions generate projection function outputs that have a dimensionality that may be several orders of magnitude smaller than the dimensionality of the projection layer input.
- the projection layer generates a projection layer output by applying projection layer parameters (e.g., a weight matrix and a bias vector) to the low-dimensional projection function outputs.
- a conventional neural network layer e.g., a conventional fully-connected layer
- projection layers may require far fewer layer parameters and may perform far fewer arithmetic operations in generating layer outputs than some conventional neural network layers.
- projection layers can reduce computational resource consumption (e.g., relative to conventional neural network layers) by performing fewer arithmetic operations and therefore consuming less computing power. Moreover, projection layers can reduce computational resource consumption since they can be stored (e.g., in a logical data storage area or physical data storage device) using less memory (e.g., as measured in bytes).
- projection networks may be suitable for deployment in resource-constrained systems, such as mobile device environments (e.g., smartphones and smartwatches), where some conventional neural networks cannot be deployed (e.g., because their computational resource demands exceed the computational resources available).
- resource constrained systems enables these systems to increase data privacy by performing tasks locally instead of remotely.
- Performing a task locally refers to performing the task using computational resources located within the system
- performing a task remotely refers to transmitting data characterizing the task to a remote environment (e.g., a cloud environment) over a communications network (e.g., the Internet), and receiving the results of the completed task back over the communications network.
- Performing tasks locally can increase data privacy since it does not require transmitting data over communications networks.
- a projection network can be trained to achieve a performance level (e.g., prediction accuracy) comparable to that of a much larger neural network by jointly training the projection network and a trainer network.
- a trainer network is a network that is configured to perform the same task as the projection network, but which is generally much larger (i.e., has more parameters) than the projection network.
- the values of the projection network parameters may be iteratively updated during training using a gradient of a loss function that depends on an error between the trainer network output and the projection network output. In this manner, the projection network can learn to mimic the predictions of the trainer network and thereby generate predictions that are nearly as accurate as those of the trainer network, despite the projection network having far fewer parameters than the trainer network.
- Additional example aspects are directed to a computing system, comprising: one or more processors; and one or more non-transitory computer-readable media that collectively store: a pre-trained projection network configured to receive a language input comprising one or more units of text and to dynamically generate an intermediate representation from the language input, the projection network comprising: a sequence of one or more projection layers, wherein each projection layer is configured to receive a layer input and apply a plurality of projection layer functions to the layer input to generate a projection layer output; and a sequence of one or more intermediate layers configured to receive the projection layer output generated by a last projection layer in the sequence of one or more projection layers and to generate one or more intermediate layer outputs, wherein the intermediate representation comprises the intermediate layer output generated by a last intermediate layer in the sequence of one or more intermediate layers; instructions that, when executed by the one or more processors, cause the computing system to perform operations, the operations comprising: obtaining the language input; inputting the language input into the pre-trained projection network; and receiving the intermediate representation as an output of the pre-
- the one or more non-transitory computer-readable media further collectively store a machine-learned prediction model configured to receive the intermediate representation and to generate a prediction from the intermediate representation; and the operations further comprise: inputting the intermediate representation into the machine-learned prediction model; and receiving the prediction as an output of the machine-learned prediction model.
- the pre-trained projection network was previously trained as part of an autoencoder model, the autoencoder model comprising: the pre-trained projection network configured to receive the language input and to generate the intermediate representation; and a decoder model configured to receive the intermediate representation and to generate a reconstructed language input based on the intermediate representation.
- the decoder model comprises a recurrent neural network.
- the language input comprises data descriptive of a sentence
- the intermediate representation comprises a sentence representation
- the decoder model is configured to generate a reconstructed sentence.
- the autoencoder model is trained to maximize a probability of the reconstructed language input matching the language input on a token-by-token basis.
- the pre-trained projection network was previously trained as a projection skip-gram model configured to receive an input word and to predict a plurality of context words surrounding the input word.
- the projection skip-gram model was trained using a negative sampling objective function.
- the projection skip-gram model was trained using an objective function that includes a regularization term that provides a penalty that has a magnitude that is positively correlated with a sum of a cosine similarity between the respective intermediate representations produced by the projection network for each pair of words in a training batch.
- the projection skip-gram model was trained on a training dataset that comprises a plurality of training examples, and wherein one or more perturbation functions were applied to one or more of the plurality of training examples during training of the projection skip-gram model.
- each of the intermediate layers other than the last intermediate layer comprises one or more non-linear activations; and the last intermediate layer comprises a fully connected layer without non-linear activations.
- the projection network was previously trained using an unsupervised learning technique; and at least the machine-learned prediction model was trained using a supervised learning technique.
- the projection network was previously trained using a first set of training data comprising a first plurality of training examples; and at least the machine-learned prediction model was trained using a second, different set of training data comprising a second plurality of training examples.
- the machine-learned prediction model was trained using the second, different set of training data through performance of training operations comprising: inputting each of the second plurality of training examples into the projection network; receiving a respective intermediate representation for each of the second plurality of training examples as an output of the projection network; inputting each respective intermediate representation into the machine-learned prediction model; receiving a respective prediction for each of the second plurality of training examples as an output of the machine-learned prediction model; and backpropagating, through at least the machine-learned prediction model, an objective function that compares to the respective prediction for each of the second plurality of training examples to a respective ground truth associated with such training example.
- the projection network was further refined using the second, different set of training data through further backpropagation of the objective function through the sequence of one or more intermediate layers subsequent to backpropagation of the objective function through the machine-learned prediction model.
- the language input consists of a single word; or comprises a string of a plurality of words.
- the projection network further comprises a feature extraction layer configured to receive the language input and generate a feature vector that comprises features extracted from the language input, wherein the layer input for a first projection layer of the one or more projection layers comprises the feature vector, and wherein the features extracted from the language input comprise one or more of the following: skip-grams; n-grams; part of speech tags; dependency relationships; knowledge graph information; or contextual information.
- the plurality of projection layer functions are precomputed and held static.
- the plurality of projection layer functions are modeled using locality sensitive hashing.
- the operations further comprise: dynamically computing the plurality of projection layer functions at inference time using one or more seeds.
- the projection neural network performs natural language processing without initializing, loading, or storing any feature or vocabulary weight matrices.
- each projection function is associated with a respective set of projection vectors, and wherein applying each projection function to the layer input comprises: for each projection vector: determining a dot product between the layer input and the projection vector; when the dot product is negative, assigning a first value to a corresponding position in the projection function output; and when the dot product is positive, assigning a second value to the corresponding position in the projection function output.
- the projection functions are each encoded as sparse matrices and are used to generate a binary representation from the layer input.
- the intermediate representation comprises a numerical feature vector.
- Another example aspect is directed to computer-implemented method to pre-train a projection network comprising one or more projection layers and one or more intermediate layers, each projection layer configured to apply one or more projection functions to project a layer input into a different dimensional space, the projection network configured to receive an input and to generate an intermediate representation for the input, the method comprising: accessing, by one or more computing devices, a set of training data comprising a plurality of example inputs; inputting, by the one or more computing devices, each of the plurality of example inputs into the projection network; receiving, by the one or more computing devices, a respective intermediate representation for each of the plurality of example inputs as an output of the projection network; inputting, by the one or more computing devices, each respective intermediate representation into a decoder model configured to reconstruct inputs based on intermediate representations; receiving, by the one or more computing devices, a respective reconstructed input for each of the plurality of example inputs as an output of the decoder model; and learning, by the one or more computing devices, one or more parameter values
- the decoder model comprises a recurrent neural network.
- each example input comprises data descriptive of a respective sentence; the respective intermediate representation for each example input comprises a respective sentence representation of the respective sentence; and the respective reconstructed input for each of the plurality of example inputs comprises a respective reconstructed sentence for the respective sentence.
- learning, by the one or more computing devices, the one or more parameter values for the one or more intermediate layers of the projection network based at least in part on the comparison of each respective reconstructed input to the corresponding example input comprises jointly training, by the one or more computing devices, the projection network and the decoder to maximize a probability of each respective reconstructed input matching the corresponding example input on a token-by-token basis.
- Another example aspect is directed to a computer-implemented method to pre-train a projection network comprising one or more projection layers and one or more intermediate layers, each projection layer configured to apply one or more projection functions to project a layer input into a different dimensional space, the projection network configured to receive an input and to generate an intermediate representation for the input, the method comprising: accessing, by one or more computing devices, a set of training data comprising a plurality of input words, wherein a respective set of ground truth context words are associated with each of the plurality of input words; inputting, by the one or more computing devices, each of the plurality of input words into the projection network; receiving, by the one or more computing devices, a respective intermediate representation for each of the plurality of input words as an output of the projection network; determining, by the one or more computing devices, a set of predicted context words for each of the plurality of input words based at least in part on the respective intermediate representation for each of the plurality of input words; and learning, by the one or more computing devices, one or more parameter values for
- learning, by the one or more computing devices, the one or more parameter values comprises optimizing, by the one or more computing devices, a negative sampling objective function.
- learning, by the one or more computing devices, the one or more parameter values comprises optimizing, by the one or more computing devices, an objective function that includes a regularization term that provides a penalty that has a magnitude that is positively correlated with a sum of a cosine similarity between the respective intermediate representation produced by the projection network for each pair of words in the set of training data.
- the technology discussed herein makes reference to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from such systems.
- the inherent flexibility of computer-based systems allows for a great variety of possible configurations, combinations, and divisions of tasks and functionality between and among components.
- processes discussed herein can be implemented using a single device or component or multiple devices or components working in combination.
- Databases and applications can be implemented on a single system or distributed across multiple systems. Distributed components can operate sequentially or in parallel.
Abstract
Description
Dialog Act Classification |
Person Talking | Input | Output |
A | You're a, so you're a senior | DECLARATIVE- |
now | QUESTION | |
B | Yeah | YES-ANSWER |
B | I'm working on my projects | STATEMENT |
trying to graduate | ||
A | Oh, good for you | APPRECIATION |
B | Yeah | BACKCHANNEL |
where <⋅,⋅> denotes the dot product operation between vectors, and {Pi}i=1 n are the projection vectors associated with the projection function
where sgn(⋅) is the sign function, which outputs
y=W·x+b (3)
where W is the parameter matrix (i.e., so that W·x represents a matrix-vector multiplication), b is the bias vector, and x is the concatenation of the projection function outputs.
i p=[
h p=σ(W p ·i p +b p) (5)
h t=σ(W t ·h t−1 +b t) (6)
y i=softmax(W o ·h k +b o) (7)
where ip refers to the output of projection operation applied to input xi, hp is applied to projection output, ht is applied at intermediate layers of the network with depth k followed by a final softmax activation layer at the top. Wp, Wt, Wo and bp, bt, bo represent trainable weights and biases respectively.
Dialog Act Classification |
Person Talking | Input | Output |
A | You're a, so you're a senior | DECLARATIVE- |
now | QUESTION | |
B | Yeah | YES-ANSWER |
B | I'm working on my projects | STATEMENT |
trying to graduate | ||
A | Oh, good for you | APPRECIATION |
B | Yeah | BACKCHANNEL |
where v; v′ are input and context embedding look up tables.
v P(w)=
where vP(w)∈
where k is the number of randomly sampled words from the training corpus according to the noise distribution, Pn(w)∝U(w)3/4, where U(w) is the unigram distribution of the training corpus.
Loss=J(θ)+L 2 cs(w mb)
L 2 cs(w mb)=λ·∥{CS(w i ,w j)|i,j∈[0,mb)}∥2 2
where CS(w_i,w_j) refers to the cosine similarity between w_i and w_j, mb refers to the mini-batch size and w_mb refers to the words in the mini-batch.
p(ŵ n |ŵ 1 ,ŵ 2 , . . . ,ŵ n-1 ,E(
where s_n and g denote the hidden state of the decoder at time n, and a non-linear function, respectively.
Claims (20)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/790,917 US11526680B2 (en) | 2019-02-14 | 2020-02-14 | Pre-trained projection networks for transferable natural language representations |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201962805498P | 2019-02-14 | 2019-02-14 | |
US16/790,917 US11526680B2 (en) | 2019-02-14 | 2020-02-14 | Pre-trained projection networks for transferable natural language representations |
Publications (2)
Publication Number | Publication Date |
---|---|
US20200265196A1 US20200265196A1 (en) | 2020-08-20 |
US11526680B2 true US11526680B2 (en) | 2022-12-13 |
Family
ID=71208001
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/790,917 Active 2041-01-13 US11526680B2 (en) | 2019-02-14 | 2020-02-14 | Pre-trained projection networks for transferable natural language representations |
Country Status (2)
Country | Link |
---|---|
US (1) | US11526680B2 (en) |
CN (1) | CN111368996B (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20200293903A1 (en) * | 2019-03-13 | 2020-09-17 | Cortica Ltd. | Method for object detection using knowledge distillation |
Families Citing this family (25)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10885277B2 (en) | 2018-08-02 | 2021-01-05 | Google Llc | On-device neural networks for natural language understanding |
CN109145315B (en) * | 2018-09-05 | 2022-03-18 | 腾讯科技（深圳）有限公司 | Text translation method, text translation device, storage medium and computer equipment |
US11615311B2 (en) * | 2018-12-10 | 2023-03-28 | Baidu Usa Llc | Representation learning for input classification via topic sparse autoencoder and entity embedding |
CN111368996B (en) * | 2019-02-14 | 2024-03-12 | 谷歌有限责任公司 | Retraining projection network capable of transmitting natural language representation |
US11704573B2 (en) * | 2019-03-25 | 2023-07-18 | Here Global B.V. | Method, apparatus, and computer program product for identifying and compensating content contributors |
US11783130B2 (en) * | 2019-05-06 | 2023-10-10 | John Snow Labs Inc. | Using unsupervised machine learning for automatic entity resolution of natural language records |
US11132513B2 (en) | 2019-05-07 | 2021-09-28 | International Business Machines Corporation | Attention-based natural language processing |
US11176333B2 (en) * | 2019-05-07 | 2021-11-16 | International Business Machines Corporation | Generation of sentence representation |
JP7418780B2 (en) * | 2019-07-18 | 2024-01-22 | 国立研究開発法人情報通信研究機構 | Inference device, inference method and inference program |
US11544456B2 (en) * | 2020-03-05 | 2023-01-03 | Adobe Inc. | Interpretable label-attentive encoder-decoder parser |
US11687723B2 (en) * | 2020-03-23 | 2023-06-27 | International Business Machines Corporation | Natural language processing with missing tokens in a corpus |
US11250853B2 (en) * | 2020-04-30 | 2022-02-15 | Robert Bosch Gmbh | Sarcasm-sensitive spoken dialog system |
CN112464784A (en) * | 2020-11-25 | 2021-03-09 | 西安烽火软件科技有限公司 | Distributed training method based on hybrid parallel |
IT202000029876A1 (en) * | 2020-12-04 | 2022-06-04 | Airesearch S R L | METHODS OF AUTOMATIC PROCESSING OF NATURAL LANGUAGE THROUGH ARTIFICIAL INTELLIGENCE |
IT202000029894A1 (en) * | 2020-12-04 | 2022-06-04 | Airesearch S R L | METHODS OF AUTOMATIC PROCESSING OF NATURAL LANGUAGE THROUGH ARTIFICIAL INTELLIGENCE |
CN112598130B (en) * | 2020-12-09 | 2024-04-09 | 华东交通大学 | Soil moisture data reconstruction method based on self-encoder and singular value threshold and computer readable storage medium |
CN113011555B (en) * | 2021-02-09 | 2023-01-31 | 腾讯科技（深圳）有限公司 | Data processing method, device, equipment and storage medium |
US20220310061A1 (en) * | 2021-03-26 | 2022-09-29 | Google Llc | Regularizing Word Segmentation |
US11853706B2 (en) * | 2021-05-17 | 2023-12-26 | Salesforce.Com, Inc. | Generative language model for few-shot aspect-based sentiment analysis |
US20220383126A1 (en) * | 2021-05-19 | 2022-12-01 | Microsoft Technology Licensing, Llc | Low-Rank Adaptation of Neural Network Models |
KR20230013793A (en) * | 2021-07-20 | 2023-01-27 | 현대모비스 주식회사 | Method and Apparatus for Classifying Document Based on Attension Mechanism and Semantic Analysis |
US20230091581A1 (en) * | 2021-09-21 | 2023-03-23 | Bank Of America Corporation | Personal Data Discovery |
CN114298277B (en) * | 2021-12-28 | 2023-09-12 | 四川大学 | Distributed deep learning training method and system based on layer sparsification |
CN114444485B (en) * | 2022-01-24 | 2023-06-06 | 四川大学 | Cloud environment network equipment entity identification method |
CN115114439B (en) * | 2022-08-30 | 2022-11-18 | 北京百度网讯科技有限公司 | Method and device for multi-task model reasoning and multi-task information processing |
Citations (33)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO1993013487A1 (en) | 1991-12-27 | 1993-07-08 | R & D Associates | Rapidly converging projective neural network |
US20140067735A1 (en) | 2012-08-29 | 2014-03-06 | Microsoft Corporation | Computer-implemented deep tensor neural network |
US20140156575A1 (en) | 2012-11-30 | 2014-06-05 | Nuance Communications, Inc. | Method and Apparatus of Processing Data Using Deep Belief Networks Employing Low-Rank Matrix Factorization |
US20150074027A1 (en) | 2013-09-06 | 2015-03-12 | Microsoft Corporation | Deep Structured Semantic Model Produced Using Click-Through Data |
CN104538028A (en) | 2014-12-25 | 2015-04-22 | 清华大学 | Continuous voice recognition method based on deep long and short term memory recurrent neural network |
US20160078339A1 (en) | 2014-09-12 | 2016-03-17 | Microsoft Technology Licensing, Llc | Learning Student DNN Via Output Distribution |
US20160307564A1 (en) | 2015-04-17 | 2016-10-20 | Nuance Communications, Inc. | Systems and methods for providing unnormalized language models |
US20160307566A1 (en) | 2015-04-16 | 2016-10-20 | Apple Inc. | Parsimonious continuous-space phrase representations for natural language processing |
EP3144859A2 (en) | 2015-09-18 | 2017-03-22 | Samsung Electronics Co., Ltd. | Model training method and apparatus, and data recognizing method |
US20170132528A1 (en) | 2015-11-06 | 2017-05-11 | Microsoft Technology Licensing, Llc | Joint model training |
US20170139913A1 (en) | 2015-11-12 | 2017-05-18 | Yahoo! Inc. | Method and system for data assignment in a distributed system |
US9842106B2 (en) | 2015-12-04 | 2017-12-12 | Mitsubishi Electric Research Laboratories, Inc | Method and system for role dependent context sensitive spoken and textual language understanding with neural networks |
US20180121799A1 (en) * | 2016-11-03 | 2018-05-03 | Salesforce.Com, Inc. | Training a Joint Many-Task Neural Network Model using Successive Regularization |
US20180150744A1 (en) | 2016-11-29 | 2018-05-31 | Microsoft Technology Licensing, Llc | Neural network data entry system |
US9990687B1 (en) | 2017-01-19 | 2018-06-05 | Deep Learning Analytics, LLC | Systems and methods for fast and repeatable embedding of high-dimensional data objects using deep learning with power efficient GPU and FPGA-based processing platforms |
US20180260381A1 (en) * | 2017-03-09 | 2018-09-13 | Xerox Corporation | Prepositional phrase attachment over word embedding products |
US20180336472A1 (en) * | 2017-05-20 | 2018-11-22 | Google Llc | Projection neural networks |
US20180341702A1 (en) * | 2017-05-25 | 2018-11-29 | J.W. Pepper & Son, Inc. | Sheet Music Search and Discovery System |
US20180356771A1 (en) | 2015-09-17 | 2018-12-13 | Nanyang Technologyical University | Computer system incorporating an adaptive model and methods for training the adaptive model |
US20190147371A1 (en) * | 2017-11-13 | 2019-05-16 | Accenture Global Solutions Limited | Training, validating, and monitoring artificial intelligence and machine learning models |
US20190206095A1 (en) * | 2017-12-29 | 2019-07-04 | Tsinghua University | Image processing method, image processing device and storage medium |
US20190294695A1 (en) * | 2018-03-22 | 2019-09-26 | International Business Machines Corporation | Implicit relation induction via purposeful overfitting of a word embedding model on a subset of a document corpus |
US20190318725A1 (en) * | 2018-04-13 | 2019-10-17 | Mitsubishi Electric Research Laboratories, Inc. | Methods and Systems for Recognizing Simultaneous Speech by Multiple Speakers |
US20190370273A1 (en) * | 2018-06-05 | 2019-12-05 | Sap Se | System, computer-implemented method and computer program product for information retrieval |
US20190393903A1 (en) * | 2018-06-20 | 2019-12-26 | Disney Enterprises, Inc. | Efficient encoding and decoding sequences using variational autoencoders |
US20200042596A1 (en) * | 2018-08-02 | 2020-02-06 | Google Llc | On-Device Neural Networks for Natural Language Understanding |
US20200104102A1 (en) * | 2018-09-27 | 2020-04-02 | Microsoft Technology Licensing, Llc | Automated content editor |
US20200265196A1 (en) * | 2019-02-14 | 2020-08-20 | Google Llc | Pre-Trained Projection Networks for Transferable Natural Language Representations |
US10812449B1 (en) * | 2018-09-19 | 2020-10-20 | Verisign | Method for generating a domain name using a learned information-rich latent space |
US11017778B1 (en) * | 2018-12-04 | 2021-05-25 | Sorenson Ip Holdings, Llc | Switching between speech recognition systems |
US20210185066A1 (en) * | 2017-09-15 | 2021-06-17 | Spherical Defence Labs Limited | Detecting anomalous application messages in telecommunication networks |
US11068722B2 (en) * | 2016-10-27 | 2021-07-20 | Nokia Technologies Oy | Method for analysing media content to generate reconstructed media content |
US11106868B2 (en) * | 2018-03-06 | 2021-08-31 | Samsung Electronics Co., Ltd. | System and method for language model personalization |
Family Cites Families (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20140236577A1 (en) * | 2013-02-15 | 2014-08-21 | Nec Laboratories America, Inc. | Semantic Representations of Rare Words in a Neural Probabilistic Language Model |
CN108733653B (en) * | 2018-05-18 | 2020-07-10 | 华中科技大学 | Sentiment analysis method of Skip-gram model based on fusion of part-of-speech and semantic information |
-
2020
- 2020-02-14 CN CN202010093708.1A patent/CN111368996B/en active Active
- 2020-02-14 US US16/790,917 patent/US11526680B2/en active Active
Patent Citations (42)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO1993013487A1 (en) | 1991-12-27 | 1993-07-08 | R & D Associates | Rapidly converging projective neural network |
US5276771A (en) * | 1991-12-27 | 1994-01-04 | R & D Associates | Rapidly converging projective neural network |
US20140067735A1 (en) | 2012-08-29 | 2014-03-06 | Microsoft Corporation | Computer-implemented deep tensor neural network |
US20140156575A1 (en) | 2012-11-30 | 2014-06-05 | Nuance Communications, Inc. | Method and Apparatus of Processing Data Using Deep Belief Networks Employing Low-Rank Matrix Factorization |
US20150074027A1 (en) | 2013-09-06 | 2015-03-12 | Microsoft Corporation | Deep Structured Semantic Model Produced Using Click-Through Data |
US20160078339A1 (en) | 2014-09-12 | 2016-03-17 | Microsoft Technology Licensing, Llc | Learning Student DNN Via Output Distribution |
CN104538028A (en) | 2014-12-25 | 2015-04-22 | 清华大学 | Continuous voice recognition method based on deep long and short term memory recurrent neural network |
US20160307566A1 (en) | 2015-04-16 | 2016-10-20 | Apple Inc. | Parsimonious continuous-space phrase representations for natural language processing |
US20160307564A1 (en) | 2015-04-17 | 2016-10-20 | Nuance Communications, Inc. | Systems and methods for providing unnormalized language models |
US20180356771A1 (en) | 2015-09-17 | 2018-12-13 | Nanyang Technologyical University | Computer system incorporating an adaptive model and methods for training the adaptive model |
EP3144859A2 (en) | 2015-09-18 | 2017-03-22 | Samsung Electronics Co., Ltd. | Model training method and apparatus, and data recognizing method |
US20170132528A1 (en) | 2015-11-06 | 2017-05-11 | Microsoft Technology Licensing, Llc | Joint model training |
US20170139913A1 (en) | 2015-11-12 | 2017-05-18 | Yahoo! Inc. | Method and system for data assignment in a distributed system |
US9842106B2 (en) | 2015-12-04 | 2017-12-12 | Mitsubishi Electric Research Laboratories, Inc | Method and system for role dependent context sensitive spoken and textual language understanding with neural networks |
US11068722B2 (en) * | 2016-10-27 | 2021-07-20 | Nokia Technologies Oy | Method for analysing media content to generate reconstructed media content |
US20180121799A1 (en) * | 2016-11-03 | 2018-05-03 | Salesforce.Com, Inc. | Training a Joint Many-Task Neural Network Model using Successive Regularization |
US20180150744A1 (en) | 2016-11-29 | 2018-05-31 | Microsoft Technology Licensing, Llc | Neural network data entry system |
US9990687B1 (en) | 2017-01-19 | 2018-06-05 | Deep Learning Analytics, LLC | Systems and methods for fast and repeatable embedding of high-dimensional data objects using deep learning with power efficient GPU and FPGA-based processing platforms |
US20180260381A1 (en) * | 2017-03-09 | 2018-09-13 | Xerox Corporation | Prepositional phrase attachment over word embedding products |
US20180336472A1 (en) * | 2017-05-20 | 2018-11-22 | Google Llc | Projection neural networks |
US10748066B2 (en) * | 2017-05-20 | 2020-08-18 | Google Llc | Projection neural networks |
US20200349450A1 (en) * | 2017-05-20 | 2020-11-05 | Google Llc | Projection neural networks |
US20180341702A1 (en) * | 2017-05-25 | 2018-11-29 | J.W. Pepper & Son, Inc. | Sheet Music Search and Discovery System |
US20210185066A1 (en) * | 2017-09-15 | 2021-06-17 | Spherical Defence Labs Limited | Detecting anomalous application messages in telecommunication networks |
US20190147371A1 (en) * | 2017-11-13 | 2019-05-16 | Accenture Global Solutions Limited | Training, validating, and monitoring artificial intelligence and machine learning models |
US20190206095A1 (en) * | 2017-12-29 | 2019-07-04 | Tsinghua University | Image processing method, image processing device and storage medium |
US10984565B2 (en) * | 2017-12-29 | 2021-04-20 | Tsinghua University | Image processing method using convolutional neural network, image processing device and storage medium |
US11106868B2 (en) * | 2018-03-06 | 2021-08-31 | Samsung Electronics Co., Ltd. | System and method for language model personalization |
US20190294695A1 (en) * | 2018-03-22 | 2019-09-26 | International Business Machines Corporation | Implicit relation induction via purposeful overfitting of a word embedding model on a subset of a document corpus |
US10885082B2 (en) * | 2018-03-22 | 2021-01-05 | International Business Machines Corporation | Implicit relation induction via purposeful overfitting of a word embedding model on a subset of a document corpus |
US20190318725A1 (en) * | 2018-04-13 | 2019-10-17 | Mitsubishi Electric Research Laboratories, Inc. | Methods and Systems for Recognizing Simultaneous Speech by Multiple Speakers |
US10811000B2 (en) * | 2018-04-13 | 2020-10-20 | Mitsubishi Electric Research Laboratories, Inc. | Methods and systems for recognizing simultaneous speech by multiple speakers |
US20190370273A1 (en) * | 2018-06-05 | 2019-12-05 | Sap Se | System, computer-implemented method and computer program product for information retrieval |
US20190393903A1 (en) * | 2018-06-20 | 2019-12-26 | Disney Enterprises, Inc. | Efficient encoding and decoding sequences using variational autoencoders |
US20190392302A1 (en) * | 2018-06-20 | 2019-12-26 | Disney Enterprises, Inc. | Efficient encoding and decoding sequences using variational autoencoders |
US10885277B2 (en) * | 2018-08-02 | 2021-01-05 | Google Llc | On-device neural networks for natural language understanding |
US20210124878A1 (en) * | 2018-08-02 | 2021-04-29 | Google Llc | On-Device Projection Neural Networks for Natural Language Understanding |
US20200042596A1 (en) * | 2018-08-02 | 2020-02-06 | Google Llc | On-Device Neural Networks for Natural Language Understanding |
US10812449B1 (en) * | 2018-09-19 | 2020-10-20 | Verisign | Method for generating a domain name using a learned information-rich latent space |
US20200104102A1 (en) * | 2018-09-27 | 2020-04-02 | Microsoft Technology Licensing, Llc | Automated content editor |
US11017778B1 (en) * | 2018-12-04 | 2021-05-25 | Sorenson Ip Holdings, Llc | Switching between speech recognition systems |
US20200265196A1 (en) * | 2019-02-14 | 2020-08-20 | Google Llc | Pre-Trained Projection Networks for Transferable Natural Language Representations |
Non-Patent Citations (87)
Title |
---|
Abadi et al., "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems", arXiv:1603.04467v2, Mar. 16, 2016, 19 pages. |
Adam et al., "The ICSI Meeting Corpus", 2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, April 6-10, 2003, Hong Kong, China, 4 pages. |
Agirre et al., "A Study on Similarity and Relatedness Using Distributional and WordNet-based Approaches", 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (HLT—NAACL—2009). May 31-Jun. 5, 2009, Boulder, Colorado, 9 pages. |
Ahmed et al., "FastEx: Hash Clustering with Exponential Families", Advances in Neural Information Processing Systems (NIPS 2012), Dec. 3-6, 2012, Lake Tahoe, NV, 9 pages. |
Bahdanau et al., "Neural Machine Translation by Jointlv Learning to Align and Translate", arXiv:1409.0473v7, May 19, 2016, 15 pages. |
Bui et al., "Neural Graph Learning: Training Neural Networks Using Graphs", Eleventh AMC International Conference on Web Search and Data Mining, Feb. 5-9, 2018, Los Angeles, CA, 8 pages. |
Bui et al., "Neural Graph Machines: Learning Neural Networks using Graphs". arXiv:1703.04818v1. Mar. 14. 2017, 9 pages. |
Byrne, "Encoding Reality: Prediction-Assisted Cortical Learning Algorithm in Hierarchical Temporal Memory", arXiv:1509.08255v2, Oct. 8, 2015, 28 pages. |
Charikar et al., "Similarity Estimation Techniques from Rounding Algorithms", 34th ACM Symposium on Theory of Computing, May 19-21, 2002, Montreal, Quebec, Canada, 9 pages. |
Chen et al., "Compressing Neural Networks with the Hashing Trick", arXiv:1504.04788v1, Apr. 19, 2015, 10 pages. |
Cheng et al., "Neural Summarization bv Extracting Sentences and Words", 54th Annual Meeting of the Association for Computational Linguistics (vol. 1: Long Papers). Aug. 7-12, 2016, Berlin, Germany, pp. 484-494. |
Cheng et al., "Solving Convex Optimization Problems using Recurrent Neural Networks in Finite Time", 2009 International Joint Conference on Neural Networks, Jun. 14-19, 2009, Atlanta, Georgia, pp. 538-543. |
Chun et al., "Augmented Smartphone Applications through Clone Cloud Execution", Intel Research Berkeley, 5 pages. |
Communication received in European Application No. 18729286.7, dated Feb. 4, 2020, 13 pages. |
Courbariaux et al., "Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or -1", arXiv:1602.02830v3, Mar. 17, 2016, 11 pages. |
Courbariaux et al., "Low Precision Arithmetic for Deep Learning", arXiv:1412.70242v2, Dec. 25, 2014, 9 pages. |
Dahl et al., "Large-Scale Malware Classification using Random Projections and Neural Networks", 38th International Conference on Acoustics, Speech and Signal Processing (ICASSP), May 26-31, 2013, Vancouver Canada, 5 pages. |
Denil et al., "Predicting Parameters in Deep Learning", arXiv:1306.0543v2, Oct. 27, 2014, 9 pages. |
Finkelstein et al., "Placing Search in Context: The Concept Revisited", Tenth International World Wide Web Conference, WWW10, May 1-5, 2001, Hong Kong, China, pp. 406-414. |
Ganchev et al., "Small Statistical Models bv Random Feature Mixing", 46th Annual Meeting of the Association for Computational Linguistics, Jun. 15-20, 2008, Columbus, Ohio, pp. 19-20. |
Gao et al., "Black-box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers", 2018 IEEE Symposium on Security and Privacy Workshops, May 24, 2018, San Francisco, CA, pp. 50-56. |
Godfrey et al., "Switchboard: Telephone Speech Corpus for Research and Development", 1992 IEEE International Conference on Acoustics, Speech and Signal Processing, Mar. 23-26, 1992, San Francisco, CA, pp. 517-520. |
Gong et al., "Compressing Deep Convolutional Networks using Vector Quantization", arXiv:1412.6115v1, Dec. 18, 2014, 10 pages. |
Goodfellow et al., "Generative Adversarial Nets", arXiv:1406.2661v1, Jun. 10, 2014, 9 pages. |
He et al., "Streaming Small-Footprint Keyword Spotting Using Sequence-To-Sequence Models", arXiv:1710.09617v1, Oct. 26, 2017, 8 pages. |
Hinton et al., "Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups", Signal Processing Magazine, vol. 2, Nov. 2012, 16 pages. |
Hinton et al., "Distilling the Knowledge in a Neural Network", arXiv:1503.02531v1, Mar. 9, 2015, 9 pages. |
Iandola et al., "Squeezenet: Alexnet-Level Accuracy with 50x Fewer Parameters and <0.5MB Model Size", arXiv:1602.07360v4, Nov. 4, 2016, 13 pages. |
International Preliminary Report on Patentability for PCT/US2018/033378, dated Nov. 26, 2019, 13 pages. |
International Search Report and Written Opinion for PCT/US2018/033378, dated Jul. 24, 2018, 20 pages. |
Ji et al., "Backoff Model Training Using Partially Observed Data: Application to Dialog Act Tagging", Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, Jun. 4-9, 2006. Brooklyn, New York, 8 pages. |
Jurafsky et al., "Automatic Detection of Discourse Structure for Speech Recognition and Understanding", IEEE Automatic Speech Recognition and Understanding Workshop, Dec. 14-18, 2019, Sentosa, Singapore, 8 pages. |
Kaban, "Improved Bounds on the Dot Product Under Random Projection and Random Sign Projection", Conference on Knowledge Discovery and Data Mining, Aug. 11-14, 2015. Sydney, Australia, 10 pages. |
Kannan et al., "Smart Reply: Automated Response Suggestion for Emails", arXiv:1606.04870v1, Jun. 15, 2016, 10 pages. |
Khanpour et al., "Dialogue Act Classification in Domain-Independent Conversations Using a Deep Recurrent Neural Networks", Conference on Computational Linguistics, Dec. 11-17, 2016. Osaka, Japan, 10 pages. |
Kingma et al., "Adam: A Method For Stochastic Optimization", arXiv:1412.6980v1 Dec. 22, 2014, 9 pages. |
Konecny et al., "Federated Optimization: Distributed Machine Learning for On-Device Intelligence", arXiv:16010.02527v1, Oct. 8, 2016, 38 pages. |
Krizhevsky et al., "ImageNet Classification with Deep Convolutional Neural Networks". Twenty Sixth Annual Conference on Neural Information Processing Systems (NIPS), Dec. 3-8, 2012, Lake Tahoe, NV, 9 pages. |
Krizhevsky, Alex Krizhevsky, "The CIFAR-10 Dataset", https://www.cs.toronto.edu/˜kriz/cifar.html, retrieved on Jun. 24, 2020, 4 pages. |
Lecun et al., Yann.lecun.com, "The MNIST Database", http://yann.lecun.com/exdb/mnist/, retrieved on Jun. 24, 2020, 7 pages. |
Lee et al., "A Low-Power Processor with Configurable Embedded Machine-Learning Accelerators for High-Order and Adaptive Analysis of Medical-Sensor Signals", Journal of Solid-State Circuits, vol. 48, No. 7, Jul. 2013, pp. 1625-1637. |
Lee et al., "Sequential Short-Text Classification with Recurrent and Convolutional Neural Networks", Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Jun. 12-17, 2016. San Diego, California, pp. 515-520. |
Lin et al., "Building a Reusable Test Collection for Question Answering", Journal of the American Society for Informational Science and Technology, vol. 57, No. 7, 2006, pp. 851-861. |
Liu et al., "IJCNLP-2017 Task 4: Customer Feedback Analysis", Joint Conference on Natural Language Processing, Nov. 27-Dec. 1, 2017, Taipei, Taiwan, 8 pages. |
Liu et al., "Stochastic Answer Networks for Machine Reading Comprehension", 56th Annual Meeting of the Association for Computational Linguistics (vol. 1: Long Papers), Jul. 15-20, 2018, Melbourne, Australia, pp. 1694-1704. |
Loshchilov et al., "SGDR: Stochastic Gradient Descent with Warm Restarts", 5th International Conference on Learning Representations, Apr. 24-26, 20174, Toulon, France, 16 pages. |
Lu et al., "Hierarchical Recurrent Neural Hashing for Image Retrieval with Hierarchical Convolutional Features", Transactions on Image Processing, vol. 27, No. 1, Jan. 2018, pp. 106-120. |
Luong et al., "Better Word Representations with Recursive Neural Networks for Morphology", Seventeenth Conference on Computational Natural Language Learning, CoNLL 2013, Aug. 8-9, 2013, Sofia, Bulgaria, 10 pages. |
Manning et al., "An Introduction to Information Retrieval", Online Edition 2009 Cambridge University Press, Cambridge, England, Apr. 1, 2009, 569 pages. |
Mikolov et al., "Distributed Representations of Words and Phrases and their Compositionality", Twenty-Seventh Conference on Neural Information Processing Systems, Dec. 5-10, 2013, Lake Tahoe, NV, 9 pages. |
Mikolov et al., "Efficient Estimation of Word Representations in Vector Space", arXiv:1301.3781v3, Sep. 7, 2013, 12 pages. |
Nair et al., "Rectified Linear Units Improve Restricted Boltzmann Machines", 27th International Conference on Machine Learning (ICML 2010), Jun. 21-24, 2010, Haifa, Israel, 8 pages. |
Neumann, "Regularization by Intrinsic Plasticity and Its Synergies with Recurrent for Random Projection Methods", Journal of Intelligent Learning Svstems and Applications, vol. 4, No. 3, 2012. pp. 230-246. |
Ortega et al., "Neural-based Context Representation Learning for Dialog Act Classification", 18th Annual SIGdial Meeting on Discourse and Dialogue, Jul. 12-14, 2018, Melbourne, Australia, pp. 247-252. |
Pang et al., "Revisiting the Predictability of Language: Response Completion in Social Media". Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Jul. 12-14, 2012. Jeju Island, Korea, pp. 1489-1499. |
Pennington et al.,. "GloVe: Global Vectors for Word Representation", 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), Oct. 25-29, 2014, Delia, Qatar, pp. 1532-1543. |
Peters et al., "Deep contextualized word representations". 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Jun. 1-6, 2018, New Orleans, LA, pp. 2227-2237. |
Plank, "All-in-1 at IJCNLP-2017 Task 4: Short Text Classification with One Model for All Languages", Joint Conference on Natural Language Processing, Shared Tasks, Nov. 27-Dec. 1, 2017, Taipei, Taiwan, pp. 143-148. |
Quora, "What Does Dr. Hinton Mean by Hard vs. Soft Targets", http://quora.com/what-does-Dr-Hinton-mean-by-hard-vs-soft-targets, retrieved on Jun. 24, 2020, 2 pages. |
QUORA. "What is Signum Function, What is its Uses in Machine Learning Neural Networks", http://www.quora.com/What-is-signum-function-what-is-its-uses-in-machine-learning-neural-networks, retrieved on Jun. 24, 2020, 3 pages. |
Radinsky et al., "A Word at a Time: Computing Word Relatedness using Temporal Semantic Analysis", 20th International Conference on World Wide Web, WWW 2011, Mar. 28-Apr. 1, 2011, Hyderabad, India, pp. 337-346. |
Ravi et al., "Large Scale Distributed Semi-Supervised Learning using Streaming Approximation", arXiv:1512.01752v1, Dec. 6, 2015, 10 pages. |
Ravi et al., "Self-Governing Neural Networks for On-Device Short Text Classification", 2018 Conference on Empirical Methods in Natural Language Processing, Oct. 31-Nov. 4, 2018, Brussels, Belgium, pp. 887-893. |
Ravi, "ProjectionNet: Learning Efficient On-Device Deep Networks Using Neural Projections", arXiv:1708.00630v2, Aug. 9, 2017, 12 pages. |
Ravi, "Scalable Decipherment for Machine Translation via Hash Sampling", Meeting of the Association for Computational Linguistics, Aug. 4-9, 2013, Sofia, Bulgaria, pp. 362-371. |
Schuster, "Speech Recognition for Mobile Devices at Google", Pacific Rim International Conference on Trends in Artificial Intelligence, Aug. 30-Sep. 2, 2010, Daegu, Korea, 3 pages. |
Search Report and Written Opinion in Singapore Application No. 10201804213U, dated Dec. 31, 2019, 8 pages. |
Search Report in Irish Application No. 2018/0149, dated Jul. 31, 2018, 10 pages.x. |
Shi et al., "Hash Kernels for Structured Data", Journal of Machine Learning Research, vol. 10, 2009, pp. 2615-2637. |
Shriberg et al., "The ICSI Meeting Recorder Dialog Act (MRDA) Corpus", SIGDIAL 2004 Workshop, The 5th Annual Meeting of the Special Interest Group on Discourse and Dialogue, Apr. 30 -May 1, 2004, Cambridge, MA, 4 pages. |
Singleton, "Android Wear 2.0: Making the Most of Every Minute" Feb. 8, 2017, https://blog.google/products/android-wear/android-wear-20-make-most-every-minute/, Retrieved on Jun. 24, 2020, 6 pages. |
Srivastava et al., "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", Journal of Machine Learning Research, vol. 15, 2014, 30 pages. |
Stolke et al., "Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech", Computational Linguistics, vol. 26, No. 3, 2000, 36 pages. |
Stolke, "Entropy-Based Pruning of Backoff Language Models", arXiv:cs/0006025v1, Jun. 11, 2000, 5 pages. |
Sutskever et al., "On the Importance of Initialization and Momentum in Deep Learning", International Conference on Machine Learning, Jun. 16-21, 2013, Atlanta, GA, 9 pages. |
Sutskever et al., "Sequence to Sequence Learning with Neural Networks", Advances in Neural Information Processing System. Dec. 8-13, 2014, Montreal, Canada, 9 pages. |
Theiler et al., "Sparse Matrix Transform for Fast Projection to Reduced Dimension", IEEE International Geoscience and Remote Sensing Symposium, Jul. 25-30, 2010, Honolulu, Hawaii, 5 pages. |
Tran et al., "A Generative Attentional Neural Network Model for Dialogue Act Classification", 55th Annual Meeting of the Association for Computational Linguistics, Jul. 30-Aug. 4, 2017, Vancouver, Canada,, 6 pages. |
Tur et al., "What Is Left To Be Understood In ATIS?", 2010 IEEE Spoken Language Technology Workshop, Dec. 12-15, 2010, Berkeley, CA, pp. 19-24. |
Wang et al., "Learning to Hash for Indexing Big Data—A Survey", arXiv:1509.05472v1v1, Sep. 17, 2015, 22 pages. |
Weinberger et al., "Feature Hashing for Large Scale Multitask Learning", arXiv:0902.2206v5, Feb. 27, 2010, 10 pages. |
Wilensky et al., "The Projection Neural Network", International Joint Conference on Neural Networks (IJCNN), Jun. 7-11, 1992, Baltimore, MD, 10 pages. |
Wojcik et al., "Training Neural Networks on High-Dimensional Data Using Random Projection", Pattern Analysis and Applications, vol. 22, Aug. 1, 2019, 11 pages. |
Yang et al., "Revisiting Semi-Supervised Learning with Graph Embeddings", arXiv:1603.08861v2, May 26, 2016, 9 pages. |
Yu et al., "Refining Word Embeddings for Sentiment Analysis", 2017 Conference on Empirical Methods in Natural Language Processing, Sep. 7-11, 2017, Denmark, Copenhagen, pp. 534-539. |
Zhong et al., "Deep Hashing Learning Networks", arXiv:1507.04437v1, Jul. 16, 2015, 7 pages. |
Zhu et al., "A Deep Neural Network Based Hashing for Efficient Image Retrieval", Conference on Systems, Man, and Cybernetics, Oct. 9-12, 2016. Budapest, Hungary, 6 pages. |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20200293903A1 (en) * | 2019-03-13 | 2020-09-17 | Cortica Ltd. | Method for object detection using knowledge distillation |
US11694088B2 (en) * | 2019-03-13 | 2023-07-04 | Cortica Ltd. | Method for object detection using knowledge distillation |
Also Published As
Publication number | Publication date |
---|---|
CN111368996A (en) | 2020-07-03 |
CN111368996B (en) | 2024-03-12 |
US20200265196A1 (en) | 2020-08-20 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11526680B2 (en) | Pre-trained projection networks for transferable natural language representations | |
US11934791B2 (en) | On-device projection neural networks for natural language understanding | |
Goyal et al. | Deep learning for natural language processing | |
CN111291181B (en) | Representation learning for input classification via topic sparse self-encoder and entity embedding | |
US11568266B2 (en) | Systems and methods for mutual learning for topic discovery and word embedding | |
US11030997B2 (en) | Slim embedding layers for recurrent neural language models | |
Gallant et al. | Representing objects, relations, and sequences | |
US20200104729A1 (en) | Method and system for extracting information from graphs | |
Beysolow | Applied natural language processing with python | |
CN111930942B (en) | Text classification method, language model training method, device and equipment | |
US11397892B2 (en) | Method of and system for training machine learning algorithm to generate text summary | |
CN113553510B (en) | Text information recommendation method and device and readable medium | |
US20220129638A1 (en) | Systems and Methods for Machine-Learned Prediction of Semantic Similarity Between Documents | |
Chen et al. | Deep neural networks for multi-class sentiment classification | |
Goyal et al. | Introduction to natural language processing and deep learning | |
Sur | Survey of deep learning and architectures for visual captioning—transitioning between media and natural languages | |
Glenn et al. | Emotion classification of Indonesian tweets using bidirectional LSTM | |
De Cnudde et al. | Deep learning on big, sparse, behavioral data | |
Tsakiris et al. | The development of a chatbot using Convolutional Neural Networks | |
Coman et al. | Exploiting deep neural networks for tweet-based emoji prediction | |
Zaman et al. | Convolutional recurrent neural network for question answering | |
Riemer et al. | A deep learning and knowledge transfer based architecture for social media user characteristic determination | |
Julian | Deep learning with pytorch quick start guide: learn to train and deploy neural network models in Python | |
Goyal et al. | Unfolding recurrent neural networks | |
Fergus et al. | Natural language processing |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
FEPP | Fee payment procedure |
Free format text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:RAVI, SUJITH;KOZAREVA, ZORNITSA;SANKAR, CHINNADHURAI;REEL/FRAME:051840/0665Effective date: 20190227 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |