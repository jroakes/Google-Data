CN107810491A - Memory system architecture - Google Patents
Memory system architecture Download PDFInfo
- Publication number
- CN107810491A CN107810491A CN201680020430.7A CN201680020430A CN107810491A CN 107810491 A CN107810491 A CN 107810491A CN 201680020430 A CN201680020430 A CN 201680020430A CN 107810491 A CN107810491 A CN 107810491A
- Authority
- CN
- China
- Prior art keywords
- cache
- cache line
- memory
- policy identifier
- access request
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/0802—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches
- G06F12/0888—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches using selective caching, e.g. bypass
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/0802—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches
- G06F12/0891—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches using clearing, invalidating or resetting means
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/0802—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches
- G06F12/0893—Caches characterised by their organisation or structure
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/10—Address translation
- G06F12/1027—Address translation using associative or pseudo-associative address translation means, e.g. translation look-aside buffer [TLB]
- G06F12/1045—Address translation using associative or pseudo-associative address translation means, e.g. translation look-aside buffer [TLB] associated with a data cache
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/0802—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches
- G06F12/0806—Multiuser, multiprocessor or multiprocessing cache systems
- G06F12/084—Multiuser, multiprocessor or multiprocessing cache systems with a shared cache
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/12—Replacement control
- G06F12/121—Replacement control using replacement algorithms
- G06F12/126—Replacement control using replacement algorithms with special data handling, e.g. priority of data or instructions, handling errors or pinning
- G06F12/127—Replacement control using replacement algorithms with special data handling, e.g. priority of data or instructions, handling errors or pinning using additional replacement algorithms
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/10—Providing a specific technical effect
- G06F2212/1016—Performance improvement
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/60—Details of cache memory
- G06F2212/604—Details relating to cache allocation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/60—Details of cache memory
- G06F2212/6042—Allocation of cache space to multiple users or processors
- G06F2212/6046—Using a specific cache allocation policy other than replacement policy
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/68—Details of translation look-aside buffer [TLB]
Abstract
Provide method, system and the equipment that the system level cache outside those caches near CPU is particularly for managing simultaneously control memory cache.Process and realize that the representative hardware configuration of these processes is designed that the behavior to such system level cache is specifically controlled.Cache policy is developed based on policy identifier, and wherein policy identifier corresponds to the intersection of the parameter of the behavior of one group of cache management structure of control.For giving cache, a policy identifier is stored in each row of the cache.
Description
The priority for the U.S. Provisional Patent Application No. 62/166,993 submitted this application claims on May 27th, 2015, its
Overall disclosure is incorporated herein by reference.
Background technology
Cache (cache) and it is cached the development of (caching) and has greatly increased in recent years, and from super
Almost each modern CPU core of low-power chip to high-end microcontroller all uses cache, or provides at them
It is used as option.Even in ultra low power design, the performance advantage of cache is too big and can not ignore.CPU is slow at a high speed
Deposit the little Chi (small pools) for the memory for being the information for storing the following most probable needs of CPU.The target of cache is
CPU when next bits of the data of needs the data by needs have been made into order to ensure CPU begins look for it
Next bit load into cache.
The content of the invention
Present invention introduces the selection of design to provide the basic reason to some aspects of the disclosure in simplified form
Solution.Present invention is not the extensive overview ot to the disclosure, and be not intended to mark the disclosure key or important elements or
Delimit the scope of the present disclosure.Present invention is only using some designs of the disclosure as the embodiment being presented below
Foreword and present.
Present disclose relates generally to computer memory systems.More specifically, each side of the disclosure is related to memory height
The efficient and effective management of fast caching.
One embodiment of the disclosure is related to following method, and it includes：Receive the access to system memory cache
Request；It is defined as the policy identifier specified by the cache line associated with the access request, wherein the strategy mark
Know at least one strategy that symbol corresponds to the operation for being used to control the system memory cache；And based on the plan
The slightly corresponding at least one strategy of identifier comes to the system memory cache executive control operation.
In another embodiment, the control operation includes cache line filtering, cache-line allocation and delayed at a high speed
Deposit one during row is evicted from.
In another embodiment, the access request includes associated with the hardware unit that the access request is derived from
Policy identifier, and methods described also includes：It is determined that the policy identifier being included in the access request with for
Policy identifier matching specified by the associated cache line of the access request.
In another embodiment, the control operation in methods described filters including cache line, and the height
Fast cache lines filtering includes：The cache line is identified with horizontal less than utilizing for threshold value；And prevent identified height
Fast cache lines are subsequently loaded into the system memory cache.
In another embodiment, the cache line filtering in methods described also includes the cache that will be identified
Row is added to the set for the cache line for being prevented from being loaded into the system memory cache.
In yet another embodiment, the cache line is identified in the process with horizontal less than utilizing for threshold value
Including：By the cache line be accessed number access count with and the cache line filter strategy it is associated
Threshold count compare；And it is less than the threshold count in response to the access count, the cache line is added
To the set for the cache line for being prevented from being loaded into the system memory cache.
In a further embodiment, the cache line is identified in the process with horizontal less than utilizing for threshold value
Including：Check that the individually accessed bit of the sub-block of the cache line is accessed in the cache line to determine
Sub-block number；By the number of the accessed sub-block with and cache line filtering the associated threshold value of strategy
Compare；And be less than the threshold value in response to the number of the accessed sub-block, by the cache line be added to by
Prevent from being loaded into the set of the cache line in the system memory cache.
In yet another embodiment, the control operation in methods described filters including cache line, and described
Cache line filtering includes：Had based on one or more of cache lines less than threshold value using level, use cloth
Grand filter filters out one or more rows of the system memory cache；And the cache line filtered out is added to
The set for the cache line for being prevented from being loaded into the system memory cache.
Another embodiment of the present disclosure is related to following system, and the system includes：System cache memory；System
Memory；It is coupled at least one processor of the system storage and the system cache memory；And with institute
The associated computer-readable medium of non-transitory of at least one processor is stated, the non-transitory medium, which has, is stored in it
On instruction, it is described instruction when by making at least one processor during at least one computing device：Receive to described
The access request of system cache memory；It is defined as the plan specified by the cache line associated with the access request
Slightly identifier, wherein the policy identifier corresponds at least the one of the operation for being used to control the system cache memory
Individual strategy；And the system cache is deposited based at least one strategy corresponding with the policy identifier
Reservoir executive control operation.
In another embodiment, also cause at least one computing device in the system as the control behaviour
The cache line filtering of work, cache-line allocation and cache line evict from one.
In another embodiment, the access request includes associated with the hardware unit that the access request is derived from
Policy identifier, and also cause at least one processor in the system：It is determined that it is included in the access request
The policy identifier with for specified by the cache line associated with the access request it is described strategy identify
Symbol matching.
In another embodiment, the control operation filters including cache line, and also causes described at least one
Processor：The cache line is identified with horizontal less than utilizing for threshold value；And prevent identified cache line quilt
It is subsequently loaded into the system cache memory.
In another embodiment, at least one processor in the system is also caused：The high speed identified is delayed
Deposit the set that row is added to the cache line for being prevented from being loaded into the system cache memory.
In yet another embodiment, at least one processor in the system is also caused：By the cache
The access count of the accessed number of row with and the threshold count that is associated of the strategy that filters of the cache line compared with；And
And it is less than the threshold count in response to the access count, the cache line is added to and is prevented from being loaded into the system
The set of the cache line in system cache memory.
In a further embodiment, at least one processor in the system is also caused：Check that the high speed is delayed
That deposits capable sub-block is individually accessed bit to determine the number of accessed sub-block in the cache line；By described in
The number of accessed sub-block with and the associated threshold value of strategy of cache line filtering compared with；And in response to institute
The number for stating accessed sub-block is less than the threshold value, and the cache line is added to and is prevented from being loaded into the system height
The set of the cache line in fast buffer memory.
In a further embodiment, the control operation filters including cache line, and also causes described at least one
Individual processor：Had based on one or more of cache lines less than threshold value using level, filtered using Bloom filter
Go out one or more rows of the system cache memory；And the cache line filtered out is added to and is prevented from loading
To the set of the cache line in the system cache memory.
In one or more of the other embodiment, disclosed method and system can alternatively include following supplementary features
One or more of：The control operation is during cache line filtering, cache-line allocation and cache line are evicted from
One；The policy identifier is multiple tactful tables comprising the operation for controlling the system memory cache
Index；The policy identifier is included in the access request using physical address bit；The policy identifier with
One or more of MMU or the translation lookaside buffer page are associated；The policy identifier and the visit
Ask that one or more computing engines that request is derived from are associated；For the policy identifier specified by the cache line
It is one in multiple policy identifiers, and each cache line in the system memory cache is specified described
A policy identifier in multiple policy identifiers；And/or the policy identifier specified by each cache line
Based on the source associated with the nearest access request to the cache line.
It should be noted that processor disclosed herein and the embodiment of some or all of accumulator system
It may be configured to implement some or all of method as disclosed above embodiment embodiment of the method.In addition, institute is public above
The embodiment of some or all of the method opened can also be represented as depositing temporary or non-transitory processor is readable
The instruction that is implemented on storage media (such as optics or magnetic memory) is expressed as via such as internet or phone connection
Communication network be supplied to the transmitting signal of processor or data processing equipment.
The scope of the applicability of other disclosed method and system will become according to the embodiment being provided below
Obtain obviously.It should be understood, however, that embodiment and particular example are in indicating means and the embodiment of system
While, only provided by illustrating, because the master of the design according to the embodiment herein disclosed
Various changes and modifications in purport and scope will become obvious for a person skilled in the art.
Brief description of the drawings
According to combination appended claims and accompanying drawing to research following detailed description of, these and other mesh of the disclosure
, features and characteristics will become more apparent from for a person skilled in the art, appended claims and accompanying drawing and with
Lower detailed description forms this specification.In the accompanying drawings：
Fig. 1 is to illustrate to control storage according to the use policy identifier of one or more embodiments described herein
The block diagram of the exemplary high-level operation of device cache operations.
Fig. 2 is to illustrate to be used to control based on policy identifier according to one or more embodiments described herein
The flow chart of the illustrative methods of the operation of system memory cache.
Fig. 3 illustrates the example logic of the director cache according to one or more embodiments described herein
State.
Fig. 4 is to illustrate the high speed for being used for mark and underusing according to one or more embodiments described herein
The flow chart of the illustrative methods of cache lines.
Fig. 5 is to illustrate to utilize level based on cache line according to one or more embodiments described herein
To determine the flow chart of the illustrative methods of the distribution of cache line.
Fig. 6 is to illustrate to distribute to any given plan for control according to one or more embodiments described herein
The slightly flow chart of the illustrative methods of the cache line of identifier.
Fig. 7 is to illustrate being arranged to use specific at a high speed according to one or more embodiments described herein
The cache policy of caching carrys out the block diagram of the exemplary computing devices of control memory cache.
Title used herein is merely for convenience, and not necessarily influences the scope that is claimed in the disclosure or contain
Justice.
In the accompanying drawings, for ease of understanding and conveniently, identical reference and any acronym are identified as tool
There are key element or the behavior of same or similar structure or function.Accompanying drawing is carried out during will be described in detail below detailed
Description.
Embodiment
The various examples and embodiment of disclosed method and system will now be described.It has been depicted below as these examples
Thoroughly understand and make it possible to realize the description of these examples and detail is provided.However, those skilled in the relevant art will manage
Solution, one or more embodiments described herein can be implemented in the case of these no many details.Similarly, phase
The technical staff in pass field also will be understood that one or more other embodiments of the present disclosure may include what is do not described in detail herein
Further feature.Additionally, some well-known structure or functions can be shown or describe in no detail below, to avoid not
Necessarily make associated description unclear.
In the disclosure, term " application " and " program " can include the file with executable content, described executable
Content is, for example, object code, script, making language document, syllabified code etc.." application " can also include not necessarily can perform
File, such as need accessed or the document otherwise opened or other data files.
Term " component " used herein, " module ", " system ", " database " etc. refer to computer related entity,
It can be such as hardware, software, firmware, the combination of hardware and software or executory software." component " can be for example but
It is not limited to processor, object, the process run on a processor, executable file, program, execution thread and/or computer.
The application run on the computing device at least one example and computing device all can be component in itself.
It will also be noted that one or more assemblies may reside within process and/or execution thread, component can collect
In on a computer and/or be distributed between multiple (for example, two or more) computers, and this class component can be with
The various computer-readable media for being stored with various data structures from it perform.
Embodiment of the disclosure be related to for manage and operate memory cache be particularly near CPU those
Method, system and the equipment of system level cache outside cache.Such system cache was usually as designing
A part for journey dividually designs with CPU, then as mode that the component of other loose connections of computer system is connected
Other components equally are linked to via common bus framework, the common bus framework is, for example, north south framework, I/O devices
And the IP blocks in " on-chip system " (SoC) design.As will be described in greater detail herein, the disclosure, which provides, to be designed to
Allow process that the behavior to these General System level caches is specifically controlled and the representativeness for realizing the process
Hardware configuration.
When in the context of bit or field herein in use, term " clearing " and " being cleared " represent that the value is
Zero or it will be set to zero.When in the context of bit or field herein in use, term " set " indicates the value
For that 1 or will be set to 1.It will also be understood that unless expressly indicated otherwise, otherwise all arithmetic in the disclosure to
The minimum value and maximum saturation of counter.
According to one or more other embodiments of the present disclosure, it can be assumed that control cache management and the various aspects of operation
Required configuration status be centrally stored director cache in itself in.It is to be noted, however, that deposited based on distribution
The embodiment of storage, distributed copies or cached copies will not fundamentally change various cache controls described herein
Feature processed.
In addition, one or more embodiments assume one or more structures (for example, table), each self-sustaining of structure be present
For control cache various operations and/or feature strategy or parameter intersection (for example, cache management strategy,
Cache management parameter etc.).As will be described below in more detail, these structures are by being referred to as tactful ID's (PID)
Identifier indexes (for example, associatedly or directly).
PID used herein is the parameter for the behavior for giving or distributing to one group of cache management structure of control
Intersection title (for example, identifier, numeral etc.).Cache management method presented herein and the various spies of system
Function of seeking peace independent of or otherwise influenceed by PID Precise Representation.As general design, PID can be
The ident value that any ident value (being usually numeral) or software through being present in system can explicitly be built.For this
The purpose of disclosed method, system and equipment, there is provided to memory transaction promoter or requestor (for example, CPU0, CPU1,
GPU kernels 0, GPU share kernel 5, video encoder, Video Decoder or usually any bus master) see clearly it is any
Value is used as suitable PID.Therefore, it is described some possible the present disclosure contemplates structure PID some possible methods
Method can be illustrated by following non-limiting representative illustration：
(1) hwid (" HWID ") of requestor's unit in on-chip system (SoC) is used.For example, typical
In SoC, the value is sent by the part as existing protocol with each affairs.
(2) use context ID (it is sometimes referred to as address space ID).This is to apply or program and program can be used
Generate the ID of translation lookaside buffer (TLB)/MMU (MMU) negotiation of every page of authority and behavior.
(3) keep, send by the request outside the band relative to normal affairs payload and control information using HW structures
With " address space identifier (ASID) " of the complete Program Generating of explanation.
Fig. 1 is the advanced behaviour that PID is used in level cache according to one or more embodiments described herein
Make 100.For example, PID is determined for cache control logic 140 and guides various memory caches to operate.
PID can be attached to (e.g., including wherein) requestor and memory construction (for example, main storage, temporary
Buffer or cache memory) between request/affairs (110), wherein requestor can be for example request access system
Any device hardware of cache, and may, for example, be CPU (CPU) (such as CPU0 or CPU1), general place
Manage unit (GPU) (GPU kernels 0 or GPU share kernel 5), video encoder, Video Decoder, any bus master etc..
It is that corresponding Policy Table 120 indexs to be included in the PID in request/affairs (110), and the Policy Table 120, which includes, to be designed to
The various aspects of control memory cache and/or the strategy of operation.For example, it is used to quote corresponding strategy as PID
The result of table 120, it may be considered that exported to particular case based on being defined as one or more strategies suitable for cache
(for example, generation, offer etc.) policy bit (130).Such policy bit (130) can be used for controlling/determining cache 140
The various logic of (for example, director cache).
It should be noted that PID transfer mechanism can be based on particular implementation and change, and the disclosure never by
The limitation for the exemplary transport mechanism being described below.For example, according at least one embodiment, physical address ratio can be used
PID is partly transferred to memory cache by special unused (for example, top).In such an example, it is associated should
With or program will manage PID, and TLB and cache will store unused bit, but can be for all other addressing function
And otherwise ignore their content.In another example, can change draw the various devices of memory-aided system with
Just by PID copy from Qi Yuan (for example, MMU of system) be automatically replicated to the applicable protocols that are used in extra user
In the bit (for example, such bit can be generally used in network-on-chip agreement) of definition.
Present disclose provides the technology for developing the cache policy based on PID.For example, memory cache will be from it
Transfer mechanism receives PID and PID is explained in a manner of being further described below.In at least one embodiment, for certain
A little strategies be can be achieved, and the copy for making at least one PID is maintained in each cache line by cache.Cache line because
This can be block, and it can be fixed size, data, the memory location transmitted between memory and cache
And at least one PID.Forming the specific number of PID bit can change in various embodiments, and can be based on
The particular characteristics of given system are chosen.
Fig. 2 is the example for carrying out the operation of control system memory cache using one or more policy identifiers
Process 200.At block 205, the access that can be received to system memory cache (or system cache memory) please
Ask.The access request received at block 205 can be that the access to the particular cache line of system memory cache please
Ask.For example, the access request can be to the write request of cache line, read requests etc..
At block 210, it may be determined that be the cache associated with access request (for example, being received at block 205)
The specified policy identifier (for example, PID) of row.For example, can be based in the PID fields being stored in cache line
PID determines PID.
, can be based at least one strategy corresponding with the policy identifier of determination at block 210 to system at block 215
Memory cache performs at least one control operation.For example, according at least one embodiment, it is slow at a high speed to system storage
The control operation of counter foil row can be one during cache line filtering, cache-line allocation and cache line are evicted from.
It will be provided in lower part on can be based on being performed to system memory cache by the PID that cache line is specified
The additional detail of these and other exemplary operation.
Fig. 3 illustrates the example logic state 300 of director cache.As described above, implement according to one or more
Example, the various aspects of control memory cache are assumed for illustration purposes and operate required configuration status and are concentrated
It is stored in director cache, but such state can be stored in a distributed way.It should be noted that hereafter only
Example states outside the state that description is generally realized in computer cache.In addition, it is various names defined in state
Claim being merely to illustrate property purpose, and be in no way intended to limit the scope of the present disclosure.
The side for being used to control cache management and operation according to one or more other embodiments of the present disclosure is described below
The various features of method and system.The various logic state being illustrated in Figure 3 required for these features, and can be herein
With reference to corresponding field to help to understand these features.The exemplary controlling mechanism that can be used in the method and system provided
Including but not limited to：Filtering, distribution, quota control and victim select (dirty to evict from), and each of which will be carried out more below
Detailed description.In the case of without loss of generality, one of ordinary skill in the art can reasonably speculate the subset of this function.
Filtering
According to one or more embodiments, method and system described herein can include cache line filtering work(
Energy (for example, the one or more filtering policys that can be implemented).For example, (it can be referred to as " black nextport hardware component NextPort herein
List ") (or bad the utilize) cache line that can be underused by mark and such high speed is being prevented in the future
Cache lines are then taken into cache to perform filter operation.If be activated, blacklist can be checked to check spy
Determine whether cache line is likely to be fully utilized.Once it is aware of the result of blacklist inspection, it is possible to take follow-up
Step.
Fig. 4 is the example mistake for identifying the cache line underused and training (for example, structure) blacklist
Journey 400.Hereinafter, it may be referred to the example cache controller shown in instantiation procedure 400 and Fig. 3 illustrated in fig. 4
Both logic states.
When by checking total one or more words including such as access_count (332) and accessed sub-block
Blacklist can be trained when section (for example, by checking individually accessed bit (340)) is to evict cache line from.Extremely
In a few operation, can (at block 405) will per cache line access_count (332) fields with per PID threshold values
(access_count_threshhold (329)) compare.If the access_ of cache line is determined at block 410
Count (332) is less than applicable filtering policy threshold value (329), then the cache line can be added to black name at block 415
It is single.On the other hand, if being determined that the access_count (332) of cache line is not less than filtering policy threshold at block 410
It is worth (329), then the cache line can be disabled or be excluded from blacklist at block 420.
In another exemplary operations (not shown), the sub-block of the row of accessed bit (340) field can will be crossed over
With compared with blocks_used_threshold (318) value, and if should and less than blocks_used_threshold
(318), then the row is added to blacklist.It should be noted that together with the instantiation procedure 400 shown in Fig. 4 or it can divide
Turn up the soil and use above-mentioned exemplary operations (for example, process).
Fig. 5 is to determine the example mistake of the distribution of cache line using horizontal for the cache line based on determination
Journey 500.For example, according at least one embodiment, if given row is distributed to the determination of cache can be based on from correlation
The data of blacklist acquisition (for example, reception, retrieval, reference etc.) (train/build simultaneously by the instantiation procedure 400 as shown according to Fig. 4
And describe in detail above).Hereinafter, it may be referred to the example shown in instantiation procedure 500 and Fig. 3 shown in Fig. 5
Both cache controller logic states.
At block 505, it can make about whether the determination that blacklist is enabled for specific PID.For example, can be by right
Use_blacklist bits (316), which are programmed on the basis of every PID, enables or disables blacklist.It is if true at block 505
Blacklist is determined and has not been activated (for example, disabling), then process 500 can terminate.If find that blacklist is opened at block 505
With (for example, consult) blacklist then at block 510, can be checked to determine the cache line in cache to be loaded into
With the presence or absence of thereon.
If it find that the cache line that be loaded into cache is not present on blacklist (at block 510), then
The every next stage cache that the cache-line allocation can be determined to cache policies at block 515.If for example,
Blacklist filter is activity and a line is hit in blacklist filter search, then it can force " no distribution " condition.
, can be with " next stage " of assessment strategy (for example, PID is read at block 515 if lacked however, going in blacklist filter
Take/write allocation strategy, quota determination, default cache strategy etc.).This can include for example on how to be based on blacklist
The mechanism (for example, part distribution etc. when reading distribution, write-in distribution, write-in) of other outside filter based on PID are distributed
Capable subsequent examination.
On the other hand, if being determined that the cache line in cache to be loaded into is present in black name at block 510
Dan Shang, then director cache will be distributed based on blacklist allocation strategy.Therefore, director cache can be consulted
(for example, check, analysis, examine etc.) is per PID blacklist_alloc_policy (314) fields to determine (at block 520)
Take any distribution action (if yes).According to one or more embodiments, blacklist allocation strategy can have multiple
Setting.For example, allocation strategy can include but is not limited to following four setting (be represented as in following example " 0 ", " 1 ",
" 2 " and " 3 ")：
No_alloc (0)-do not distribute cache line in particular cache and (for example, by suitable in SoC
When requestor's unit) will to be returned to cache next under the machine row size of the cache for the address asked
Level.
The overall cache row of 128B (1), 256B (2), 512B (3)-only the load size specified by the field from
The part so alignd.Because these are typical line or the multiple of sub-line size (for example, 64B), it can be with this area
Many different modes known to a person of ordinary skill in the art are realized.
Once (for example, inspection, inquiry, analysis etc.) black name is consulted about whether newline is distributed into cache
It is single, it is possible to one or more additional policies to be applied or perform, as being described in detail below.If as blacklist
As a result unallocated cache line, then the further processing in cache may not be necessary
According at least one embodiment of the disclosure, filtering control/function can utilize simplified Bloom filter and
One or more hash functions can be used during searching and training (renewal) action (it is, for example, possible to use at least two dissipate
Array function).Although the fine detail of hash function can change between embodiment, no matter any hash function used
It all should be quick, efficient, and generate incoherent hashed value.It should be noted that can be based on to this parameter
It is final to tune to adjust Bloom filter locating function.As Bloom filter, set for each hash function realized
A bit in fixed (or inspection) filter array.In example embodiment, related physical addresses bit is used as dissipating
The input of array function.
In at least one embodiment, filter embodiment can be included side by side using Bloom filter bit array
Two copies (for example, A and B) are (for example, filterA [N_FILTER_BITS] for copy A and for copy B's
FilterB [N_FILTER_BITS] (312)) aging mechanism (for example, system, component, unit etc.).Searching what is be performed
Whenever, two arrays (for example, copy A and copy B) can be checked.If the instruction hit of any one array, is quoted
It can be considered as being put on the blacklist.In addition, renewal (training) be performed whenever, two arrays can be updated.
In at least this example embodiment, clear (refresh) Bloom filter can be periodically weighed to prevent the mistake
Filter is filled.For example, corresponding application or program (for example, computer software) can specify " weight clear " circulation (cycle) and
Can also safeguard for A and B arrays single counter (for example, bloom_refresh_cycles_counterA and
bloom_refresh_cycles_counterB(310)).It is corresponding when any one such counter is decremented to zero
Bit array (A or B) is cleared and counter is reset.Can also be by can be referred to Bloom filter by the clear number of weight
The limit (for example, bloom_refresh_cycles_limit (311)) is determined to control Bloom filter.It is it should be noted that old
The key changed is that the initial value of A counters is programmed into (counting down) to be equal to the weight clear cycle and be initialized as B arrays
The half in weight clear cycle.For example, if every 100 circulations of Bloom filter will be cleared, the copy A of array follows at 100
Ring, 200 circulation, 300 circulation etc. be cleared, and the copy B of array 50 circulation, 150 circulation, 250 circulation etc.
It is cleared.It should be adjusted based on the experiment of particular implementation interested in chip designer and/or particular job load
The speed that any one counter (for example, the counter for A and/or counter for B) counts.It is preferred that not hard in advance compile
The appropriate aging value of code, and therefore disclosed method and system allow once determine will chip placement goal systems
Just suitably (for example, based on experiment) adjusts (for example, by hardware and/or Software for Design the workload that will be run with chip
Person) threshold value.
It should be understood that above-mentioned example aging mechanism be at least one embodiment according to the disclosure be suitable for
One example of many such mechanism that Bloom filter is used together.In addition to above-mentioned example aging method or replace upper
The example aging method stated, can be similarly used various other ageing systems or technology.For example, another aging mechanism is based on
The percentage of the bit set in filter array (for example, when they are by some threshold value, array is cleared).
Filtering control/function of method and system described herein allows displaying poorly to reuse or finally use
Only bring the grace " throttling " of the cache line (for example, suboptimum is utilized to those rows of cache) of the subset of byte into
(graceful“throttling”).For example, such cache line " can be forbidden " immediately (for example, in the example shown in Fig. 3
In blacklist_alloc_policy (314) under no_alloc), or only allow these rows loading around missing ground
The sub-block of the physical address (PA) of location.Although such technology still consumes cache capacity, they are reduced in primary storage
The amount of the potential unnecessary bandwidth (BW) consumed between device (being generally, for example, dynamic random access memory (DRAM)).With this
If kind of a mode throttles and also implies that blacklist function is too cautious, will not entirely prevent good being advanced into cache.
Distribution control
According to one or more other embodiments of the present disclosure, the method and system provided can include one or more distribution
Control or function (for example, the one or more distribution control strategies that can be implemented).For example, whenever will be given slow at a high speed
Distribution control can be performed when depositing all or part of of middle distribution cache line., can be with above-mentioned filtering control
Various distribution controls and feature per management/control disclosure on the basis of PID, and PID can with various system MMU/TLB
The single page or with it is specific request subframe unit (sub-unit) (for example, CPU0, CPU1, GPU kernel 0, GPU share
Kernel 5, video encoder, Video Decoder, any bus master etc.) it is associated.
The various features of distribution control logic are hereafter described with continued reference to Fig. 3.When set, allocate_on_read
(322) read requests of alllocated lines in the caches are caused, but allocate_on_write (323) causes slow at a high speed
Deposit the write request of middle alllocated lines.Make two bits (for example, allocate_on_read (322) and allocate_on_write
(323)) set distributes all requests, and two bits clearings is prevented by from given PID's (for example, PID (334))
Request is allocated.
According at least one embodiment, as partial_allocate_on_write (324) and allocate_on_
When both write (322) bits are set, write-in missing can only distribute single sub-block.In such scene, if to height
Fast cache lines are subsequently read, then will load remaining sub-block.Follow-up write-in will distribute other sub-block, but need to read
To load whole block.For being not included in the distribution of blacklist (for example, not excluded or blacklist is basic due to checking blacklist
The reference do not consulted), the subset that distribution control logic provides the sub-block for only loading cache line (such as passes through
Indicated by subblock_size fields (328)) ability.Additionally, control logic allows applicable programming (for example, soft
Part) the specific PID of configuration so that for all cache lines by the PID control, entirely it is about to only carry out to the row
It is loaded after the access (such as by indicated by access_count_threshold fields (329)) of certain amount.
Quota control
Fig. 6 is the instantiation procedure 600 for controlling the cache line for distributing to PID.For example, according to described herein
One or more embodiments, disclosed method and system can include being designed to solution being related to cache line and PID
Distribution various situations one or more quota controls or function (for example, the one or more quota controls that can be implemented
System strategy).Hereinafter, it may be referred to the example cache controller shown in instantiation procedure 600 and Fig. 3 shown in Fig. 6
Logic state.
In at least one example, quota control strategy can prevent any given PID from obtaining to exceed given cache
Line_count_max (326) OK.When specific PID reaches the quota control such as provided by line_count_max (326)
During threshold value, no longer cache-line allocation is given to the PID, and ask to be forwarded in the memory hierarchy of applicable system
Next stage.
At block 605, distribution request can be received at cache (for example, for distributing cache to given PID
Capable request).According at least one embodiment, whenever distribution request is received at caching, can be looked at block 610
Quota control threshold value (for example, being provided by line_count_max (326) field) suitable for given PID is provided.
If determine the total number for the row for being already allocated to given PID (for example, passing through line_count at block 610
(327) field is provided) it is greater than or refers to equal to by quota control threshold value (line_count_max (326) field)
Fixed value, then the row can not be distributed in the particular cache at block 615 (it is to be noted, however, that can be at this
The row is cached at the higher or lower level of level).
If determine the total number for the row for being already allocated to given PID (for example, passing through line_count at block 610
(327) field is provided) be less than the value (line_count_max (326) field) specified by quota control threshold value, then exist
The row can be distributed at block 620 in the caches.Whenever to give PID alllocated lines, such as at block 625, counted per PID
Device (for example, line_count (327)) can be incremented by (and similarly successively decreasing when evicting from).
According at least one embodiment, every a line in cache keeps (for example, storage, maintenance etc.) to dominate its behavior
PID copy.Cache it is accessed whenever, if the PID fields (334) of accessed row are different,
The PID of newest access can be updated to.Shift the process of PID ownership for by the generation of hardware unit and with
It is especially important for the row consumed afterwards by other hardware units, and be important in effective management aspect of cache.
When the PID of cache line changes, director cache is suitably updated for previous in such cases
With new PID line_count (327) (for example, number of the allocated row).Therefore, the ownership as cache line
Another PID result is changed into from a PID, new PID line_count (327), which can exceed, passes through line_count_max
(326) threshold value provided.
According at least one embodiment, cache can realize one or more high-level policies to solve super quota bar
Part.For example, in order to which one in multiple (for example, two) PID share cache lines and these PID exceedes its quota
Jolt (thrashing) is avoided in the case of (for example, being provided by line_count_max (326)), cache can
To realize any one in the example below strategy：(1) counting of the accessed number of cache line is kept and according to quilt
The PID for accessing most cache lines carrys out the PID of alllocated lines；Or cache-line allocation is had most quotas by (2)
Or the PID of surplus.It should be noted that if new PID exceedes quota, give way stay in original PID be make the row invalid or
Person makes the alternative solution of other row sacrifices in target PID.
Victim selects control
According to one or more embodiments, disclosed method and system can include one or more victims and select
(for example, dirty evict from) controls or function (for example, the one or more victims that can be implemented select control strategy).For example,
Victim_policy can be realized, the victim_policy is that height is controlled when distribution new in the caches needs space
How fast cache controller selects the strategy of victim's (for example, cache line to be evicted from from cache).Can example
Be used in conjunction with this control with above-mentioned quota control strategy such as when having reached given PID quota, with control/prevent one
Individual PID makes the row of the quota from other PID jolt.In at least one example embodiment：
Victim_policy==0：Strategy/PID can be programmed with basis to possible victim (for example, generally
For certain type of least recently used (LRU) state) global selection control victim's selection course；And
Victim_policy==l：Selection can be only from those victim's candidates in identical PID.
According to one or more of the other embodiment of the disclosure, cache management and control system and/or equipment can be with
Be designed to realize the target similar with control system and equipment with above-mentioned cache management, but use simultaneously one or
Multiple different states store, distribute, lock and/or evicted from strategy.
For example, such cache management can be designed to from control device (1) provide it is different hard for isolating
The mechanism of working set between part agency (for example, CPU, I/O device etc.) in SoC environment, and/or (2) are provided for avoiding
Outside DRAM bandwidth, which wastes, caused by the large high-speed cache lines of sparse reference (in other words, makes the row of sparse reference
Be maintained at outside cache) mechanism.
It should be understood that according to the one or more other embodiments, cache management and control device and/or it is
System can include and some in the feature and function of equipment that is described above and illustrating in figures 1-7 and system or complete
Feature as category and function.It will also be noted that described above (and illustrate in figures 1-7) be used to delay at a high speed
The additional of detailed hereafter can be optionally included in and/or substitute special by depositing method, system and the equipment of management and control
One or more of sign.
In addition to including following additional/alternative features, the cache management and control device that are described below with it is upper
The example management stated and control device have something in common：(1) shadow of the distribution of states span machine and the policy store based on PID
Sub- copy is maintained at middle position；And (2) this method provide capable level locking and writeback policies.
For example, according at least one embodiment, following structure can be built in the caches to control distribution：
(i) under the rank of cache, copy (for example it is assumed that 16 streams) is often flowed：
(a) shadow control register (for example, 32 bits)
(1) quota (number of cache line) (for example, 16 bits, 2^16 × 4KB cache line sizes)；
(2) pattern (for example, 1 bit)；0 → 1 is activated；1 → 0 disabled (if it should be noted that disabled,
Resetting option (being described below) can determine what to do)；And
(3) option (1 bit) is reset；If 0：Make clean row invalid, write back dirty row；If 1：It is no matter clean or dirty
It is invalid that state all makes to go.
(b) status register (for example, 32 bits)
(1) (number of cache line) (for example, 16 bits, 2^16 × 4KB cache line sizes) is used；
(2) (for example, 1 bit) in use；When mode bit is set 0 → 1；When the process of clearing is completed 1 → 0.
(ii) under the rank of cache line：
(a) it is locked (for example, 1 bit)；
(b)PID；
(c) LFU (least conventional) reference count (for example, at most 6 ' d63 6 bits)；And
(d) LRU (least recently used) reference history (layering hash table).
(iii) under the rank of 64B cache lines：
(a) invalid (2 ' b00), effectively and clean (2 ' b01), effective and dirty (2 ' b11).
It should be noted that the accurate size of counter, structure and/or bus can be employed by equipment and control system
In bottom cache determine.This example is (to be for the purpose this specification 4KB, still with big row size
Accurate size can change) DRAM cache and provide.
In addition, according at least one embodiment, example agency's (for example, CPU, I/O device etc.) controller state can be
It is as follows：
(i) 16 stream ID L4 (16stream-ID L4) main control registers (for example, 32 bits)：
(a) L4 quotas (number of cache line) (for example, 16 bits, 2^16 × 4KB cache line sizes)；
(b) stream ID (stream ID) control enables (1 bit)；
(c) L4 allocation strategies：
(1) (1 bit) is distributed when reading and lacking；
(2) (1 bit) is distributed when writing and lacking；
(3) (1 bit) is distributed when partial write lacks；
(4) the sub- cache lines of N are distributed (wherein N=1 means a 64B)；
(5) distribution after n times missing (wherein N=1 means to distribute at once after first time missing)；And
(6) distribute and lock (1 bit)；Once it is locked, it is just not replaceable.
(d) replacement policy (for example, LRU is to LFU).
(ii) 16 stream ID L3 main control registers (for example, 32 bits)：
(a) L3 quotas (number of cache line) (for example, 16 bits, 2^16 × 64B cache line sizes)；
(b) stream ID control enables (1 bit)
(c) L3 allocation strategies：
(1) (1 bit) is distributed when reading and lacking；
(2) (1 bit) is distributed when writing and lacking；
(3) deallocated when hitting (aiming at L2C to be exclusively enjoyed, 1 bit)；
(4) distribute and lock (1 bit)；Once it is locked, it can not just be replaced.
(d) replacement policy (for example, LRU).
According to one or more embodiments, can be changed in a manner of one or more for cache management and control
Method, system and equipment are with addition to the above feature and/or function or instead of the above feature and/or function
Alternatively include further feature and/or function.For example, the system or equipment can be configured with the additional control to cache
System.The programmable threshold and/or " locking " cache that such control can be included for example for starting dirty write-back are new to prevent
Distribute the ability of (if being jolted for example, detecting).
In another example, " rules bit (chicken bits) " can be used only to allow some agencies to use at a high speed
Caching.Be allowed to use such agency of cache leading candidate can include such as GPU, display controller, ISP and
Video Controller.In such embodiment, it would not allow for CPU (or such as ZRAM other agencies) being assigned to and delay at a high speed
In depositing.In one example, cache hardware can match on one group of ID and prevent from distributing.However, it is prevented from distributing to
Those agencies of cache still can read from cache.Limit by this way distribution capability some are non-exhaustive excellent
Point includes making hardware keep strategy decision that is relatively easy and avoiding complexity.
Another example includes adding performance counter to cache to allow computer program (for example, software) to determine
When particular agent makes cache jolt and prevent from newly distributing with " bypass mode ".It can use in this way
The non-limiting and non-exhaustive list of sample count device include：
(i) number in setting time interval to distribution and/or the dirty number evicted from are counted and in the number
Interrupted in the case of more than threshold value；
(ii) ratio (measurement of cache effectiveness) for entering bandwidth and output bandwidth is counted；
(iii) ratio of dirty cache line and clean cache line is counted；With
(iv) dirty evict from is counted with the ratio distributed.
It should be noted that for performance counter in the above-described manner, it may be necessary to have by agency to meter
Number device carries out the ability of amplitude limit, and therefore it may be necessary to the several counters associated with one group of particular agent.
In one example, if it exceeds the threshold, then software can be configured to provisionally lock cache (for example, preventing that new distribution is straight
To the row for being previously assigned/locking by untill explicitly unlocking).
Fig. 7 is being arranged to use specific to cache according to one or more embodiments described herein
Cache policy come control memory cache exemplary computing devices (700) high level block diagram.Match somebody with somebody in very basic
Put in (701), it is high that computing device (700) generally includes one or more processors (710), system storage (720) and system
Speed caching (740).Memory bus (730) can be used for communicating between processor (710) and system storage (720).Its
Its hardware may be connected to processor (710), memory (720), memory bus (730) and/or system cache (740).
Depending on desired configuration, processor (710) can be any kind of, including but not limited to microprocessor (μ
P), microcontroller (μ C), digital signal processor (DSP) etc. or any combinations thereof.Processor (710) may include one-level or
Multilevel cache (such as on-chip cache (711) and second level cache (712)), processor cores (713) and deposit
Device (714).Processor cores (713) may include ALU (ALU), floating point unit (FPU), digital signal processing core
(DSP core) etc. or any combination of them.Memory Controller (715) can be also used together with processor (710), Huo Zhe
Memory Controller (715) can be the interior section of processor (710) in some embodiments.In one or more of the other reality
Apply in mode, Memory Controller (715) can be used together (for example, above-mentioned as being configured with system cache (740)
Various features and function director cache).
Depending on desired configuration, system storage (720) can be any kind of, including but not limited to volatibility
Memory (RAM), nonvolatile memory (ROM, flash memory etc.) or any combination of them.System stores
Device (720) generally includes operating system (721), is one or more using (722) and routine data (724).Can be with using (722)
Including the system (723) for controlling one or more caches using cache policy.According at least one of the disclosure
Embodiment, the system (723) for being controlled one or more caches using cache policies are designed to provide pair
The detailed control of the behavior (for example, operation, function etc.) of such system level cache.
Routine data (724) can include the instruction of storage, described to instruct when being performed by one or more processing units,
Realize the system (723) and method for controlling one or more caches using cache policies.Additionally, according to
At least one embodiment, routine data (724) can include policy identifier (PID) data (725), its can be related to for example to
The title of the intersection of the parameter of the behavior of one group of cache management structure of control is given or distributed to (for example, identifier, numeral
Deng).According at least some embodiments, can be arranged in operating system (721) and routine data (724) one using (722)
Play operation.
Computing device (700) can have supplementary features or a function, and for facilitate basic configuration (701) with it is any required
The additional interface of communication between device and interface.
According at least one embodiment, system cache (740) can be made by multiple different nextport hardware component NextPorts or unit
With.For example, in SoC design (for example, being used in the mancarried device of phone, flat board etc.), system cache
(740) it can be shared common cache between many different hardware cells of system.
System storage (720) is the example of computer-readable storage medium.Computer-readable storage medium include but is not limited to RAM,
ROM, EEPROM, flash memory or other memory technologies, CD-ROM, digital universal disc (DVD) or other optical memory,
Magnetic holder, tape, magnetic disk storage or other magnetic memory apparatus, or available for store desired information and can by calculate fill
Put any other medium of 700 access.Any such computer-readable storage medium can be a part for device (700).
Computing device (700) can the part of (or mobile) electronic installation portable as small form factor be implemented,
The portable electron device is such as cell phone, smart phone, personal digital assistant (PDA), personal media player dress
Put, tablet PC (flat board), wireless web viewing apparatus, personal Headphone device, special purpose device, or including any
The mixing arrangement of above-mentioned function.In addition, computing device (700), which can also be used as, includes laptop computer and non-calculating on knee
Machine configures both personal computer, one or more servers, Internet of things system etc. and is implemented.
Being described in detail above elaborates each of device and/or process via the use of block diagram, flow chart and/or example
Kind embodiment.As long as such block diagram, flow chart and/or example include one or more functions and/or operation, the technology of this area
Personnel will be understood that, each function and/or operation in such block diagram, flow chart or example can be by miscellaneous hardware, soft
Part, firmware or actually any combination of them individually and/or is jointly realized.According at least one embodiment, Ke Yijing
By application specific integrated circuit (ASIC), field programmable gate array (FPGA), digital signal processor (DSP) or other integrated forms
If to realize the stem portion of theme described herein.However, those skilled in the art will recognize that, it is disclosed herein
Embodiment some aspects entirely or in part can by the use of integrated circuit, be used as one run on one or more computers
Individual or multiple computer programs, as the one or more programs run on the one or more processors, as firmware or
Equally realized as actually any combination of them, and design circuit and/or write the code of software and/or firmware
In view of the disclosure is by the technical ability of complete those skilled in the art.
In addition, those skilled in the art will be appreciated that, the mechanism of theme described herein can be with various shapes
Formula is distributed as program product, and no matter the illustrative embodiment of theme described herein is for actually carrying out point
How the particular type of the non-transitory signal bearing medium of hair is applicable.The example of non-transitory signal bearing medium include but
It is not limited to following medium：Recordable-type media, such as floppy disk, hard disk drive, CD (CD), digital video disc (DVD), numeral
Tape, computer storage etc.；And transmission type media, such as numeral and/or analogue communication medium are (for example, fiber optic cables, ripple
Conduit, wired communications links, wireless communication link etc.).
Unless being expressly limited by by respective contexts, otherwise in the disclosure in the case of use, term " generation " refers to
Show any one in its its ordinary meaning, such as calculate or otherwise produce, term " calculating " is indicated in its its ordinary meaning
Any one, such as calculate, assess, estimate and/or selected from multiple values, term " acquisition " indicates any in its its ordinary meaning
It is individual, such as receive (for example, from external device (ED)), derive from, calculate and/or retrieval (for example, from array of memory element), and
Term " selection " indicates any one in its its ordinary meaning, for example, mark, instruction, application and/or using two or more
At least one in set and all or less than the set.
Term " comprising " is in the case where it is used in the disclosure (including claims), however not excluded that other elements or
Operation.Term "based" (for example, " A is based on B ") it is used to indicate any one in its its ordinary meaning, including situation in the disclosure
(i) " derive from " (for example, " B is A precursor "), (ii) " being at least based on " (for example, " A is at least based on B "), and it is specific up and down
In text it is appropriate in the case of (iii) " being equal to " (for example, " A equals B ").Similarly, term " in response to " is used to indicate that its is general
Lead to any one in implication, including such as " at least responsive to ".
It should be understood that unless otherwise instructed, otherwise the operation to the equipment with special characteristic herein is any
It is open to be also clearly intended to disclose the method (and vice versa) with similar characteristics, and to the equipment according to particular configuration
Any disclosure of operation be also clearly intended to the open method (and vice versa) according to similar configuration.Using term
In the case of " configuration ", method, system and/or equipment as indicated by by specific context may be referred to.Unless pass through spy
Determine context to indicate in addition, otherwise term " method ", " process ", " technology " and " operation " universally and is interchangeably used.
Similarly, unless being indicated in addition by specific context, otherwise term " equipment " and " device " are also by universally and interchangeably
Use.Term " element " and " module " are generally used to indicate that a part for larger configuration.Unless clearly limited by its context,
Otherwise term " system " is used to indicate any one in its its ordinary meaning herein, including for example " public purpose is thought in interaction
One set of pieces of service ".
On substantially any plural number and/or singular references use herein, those skilled in the art can be according to
Context and/or application are converted into odd number from plural number and/or are converted into plural number from odd number.For the sake of clarity, can be herein
In be explicitly described the displacement of various singular/plurals.
Therefore, the specific embodiment of theme is described.Model of the other embodiments in claims below
In enclosing.In some cases, the action that secretary carries in claim, which can be performed in a different order and still be realized, wishes
The result of prestige.In addition, the process described in accompanying drawing not necessarily requires shown certain order or sequential order is wished to obtain
The result of prestige.In some embodiments, multitasking and parallel processing can be favourable.
Claims (26)
1. a kind of method, including：
Receive the access request to system memory cache；
It is defined as the policy identifier specified by the cache line associated with the access request, wherein the strategy mark
Symbol corresponds at least one strategy for the operation for being used to control the system memory cache；And
The system memory cache is held based at least one strategy corresponding with the policy identifier
Row control operation.
2. according to the method for claim 1, wherein, the control operation is cache line filtering, cache line point
With one in being evicted from cache line.
3. according to the method for claim 1, wherein, the policy identifier is comprising for controlling the system storage
Multiple tactful tables of the operation of cache are indexed.
4. according to the method for claim 1, wherein, the access request is hard including being derived from the access request
The associated policy identifier of part device, and also include：
It is determined that the policy identifier being included in the access request with for the height associated with the access request
Policy identifier matching specified by fast cache lines.
5. according to the method for claim 1, wherein, the policy identifier is included in described using physical address bit
In access request.
6. according to the method for claim 1, wherein, the policy identifier is delayed with MMU or translation look-aside
Rushing one or more of device page is associated.
7. according to the method for claim 1, wherein, be derived from one of the policy identifier and the access request
Or multiple computing engines are associated.
8. according to the method for claim 1, wherein, the control operation filters including cache line, and the height
Fast cache lines filtering includes：
The cache line is identified with horizontal less than utilizing for threshold value；And
Identified cache line is prevented to be subsequently loaded into the system memory cache.
9. the method according to claim 11, in addition to：
The cache line identified is added to the cache for being prevented from being loaded into the system memory cache
Capable set.
10. according to the method for claim 8, wherein, the cache line is identified with horizontal less than utilizing for threshold value
Including：
By the cache line be accessed number access count with and the cache line filter strategy it is associated
Threshold count compare；And
Be less than the threshold count in response to the access count, the cache line is added to be prevented from being loaded into it is described
The set of the cache line in system memory cache.
11. according to the method for claim 8, wherein, the cache line is identified with horizontal less than utilizing for threshold value
Including：
Check that the individually accessed bit of the sub-block of the cache line is accessed in the cache line to determine
Sub-block number；
By the number of the accessed sub-block with and the associated threshold value of strategy of cache line filtering compare；With
And
It is less than the threshold value in response to the number of the accessed sub-block, the cache line is added to and is prevented from loading
The set of the cache line into the system memory cache.
12. according to the method for claim 2, wherein, the control operation filters including cache line, and the height
Fast cache lines filtering includes：
Had based on one or more of cache lines less than threshold value using level, filtered out using Bloom filter described
One or more rows of system memory cache；And
The cache line filtered out is added to the cache line for being prevented from being loaded into the system memory cache
Set.
13. according to the method for claim 1, wherein, the policy identifier specified by the cache line is
One in multiple policy identifiers, and each cache line in the system memory cache specify it is described more
A policy identifier in individual policy identifier.
14. wherein, it is according to the method for claim 13, the policy identifier base specified by each cache line
In the source associated with the nearest access request to the cache line.
15. a kind of system, including：
System cache memory；
System storage；
It is coupled at least one processor of the system storage and the system cache memory；And
The non-transitory computer-readable medium associated with least one processor, the non-transitory medium, which has, deposits
Store up in instruction thereon, the instruction is by making at least one processor during at least one computing device：
Receive the access request to the system cache memory；
It is defined as the policy identifier specified by the cache line associated with the access request, wherein the strategy mark
Symbol corresponds at least one strategy for the operation for being used to control the system cache memory；And
The system cache memory is held based at least one strategy corresponding with the policy identifier
Row control operation.
16. system according to claim 15, wherein, the control operation is cache line filtering, cache line
One distributed in being evicted from cache line.
17. system according to claim 15, wherein, the policy identifier is comprising for controlling the system high-speed
Multiple tactful tables of the operation of buffer memory are indexed.
18. system according to claim 15, wherein, the access request includes what is be derived from the access request
The associated policy identifier of hardware unit, and also cause at least one processor：
It is determined that the policy identifier being included in the access request with for the height associated with the access request
Policy identifier matching specified by fast cache lines.
19. system according to claim 15, wherein, the policy identifier is included in institute using physical address bit
State in access request.
20. system according to claim 15, wherein, the MMU of the policy identifier and the system
Or one or more of translation lookaside buffer page is associated.
21. system according to claim 15, wherein, the institute that the policy identifier is derived from the access request
Stating one or more of system computing engines is associated.
22. system according to claim 15, wherein, the control operation filters including cache line, and also makes
Obtain at least one processor：
The cache line is identified with horizontal less than utilizing for threshold value；And
Identified cache line is prevented to be subsequently loaded into the system cache memory.
23. system according to claim 22, wherein, also cause at least one processor：
The cache line identified is added to the cache for being prevented from being loaded into the system cache memory
Capable set.
24. system according to claim 22, wherein, also cause at least one processor：
By the cache line be accessed number access count with and the cache line filter strategy it is associated
Threshold count compare；And
Be less than the threshold count in response to the access count, the cache line is added to be prevented from being loaded into it is described
The set of the cache line in system cache memory.
25. system according to claim 22, wherein, also cause at least one processor：
Check that the individually accessed bit of the sub-block of the cache line is accessed in the cache line to determine
Sub-block number；
By the number of the accessed sub-block with and the associated threshold value of strategy of cache line filtering compare；And
And
It is less than the threshold value in response to the number of the accessed sub-block, the cache line is added to and is prevented from loading
To the set of the cache line in the system cache memory.
26. system according to claim 16, wherein, the control operation filters including cache line, and also makes
Obtain at least one processor：
Had based on one or more of cache lines less than threshold value using level, filtered out using Bloom filter described
One or more rows of system cache memory；And
The cache line filtered out is added to the cache line for being prevented from being loaded into the system cache memory
Set.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201562166993P | 2015-05-27 | 2015-05-27 | |
US62/166,993 | 2015-05-27 | ||
PCT/US2016/034360 WO2016191569A1 (en) | 2015-05-27 | 2016-05-26 | Memory system architecture |
Publications (2)
Publication Number | Publication Date |
---|---|
CN107810491A true CN107810491A (en) | 2018-03-16 |
CN107810491B CN107810491B (en) | 2022-02-01 |
Family
ID=56118026
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201680020430.7A Active CN107810491B (en) | 2015-05-27 | 2016-05-26 | Method and system for managing and controlling memory cache |
Country Status (4)
Country | Link |
---|---|
US (1) | US10089239B2 (en) |
CN (1) | CN107810491B (en) |
DE (1) | DE112016002356B4 (en) |
WO (1) | WO2016191569A1 (en) |
Families Citing this family (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11099849B2 (en) * | 2016-09-01 | 2021-08-24 | Oracle International Corporation | Method for reducing fetch cycles for return-type instructions |
US10437732B2 (en) * | 2016-12-14 | 2019-10-08 | Intel Corporation | Multi-level cache with associativity collision compensation |
US10303612B2 (en) * | 2016-12-30 | 2019-05-28 | Intel Corporation | Power and performance-efficient cache design for a memory encryption engine |
US10901907B2 (en) | 2017-10-19 | 2021-01-26 | Samsung Electronics Co., Ltd. | System and method for identifying hot data and stream in a solid-state drive |
US10263919B1 (en) * | 2017-11-06 | 2019-04-16 | Innovium, Inc. | Buffer assignment balancing in a network device |
US11232033B2 (en) | 2019-08-02 | 2022-01-25 | Apple Inc. | Application aware SoC memory cache partitioning |
EP3854041B1 (en) * | 2019-12-02 | 2024-05-01 | DRW Technologies, LLC | System and method for latency critical quality of service using continuous bandwidth control |
US11275688B2 (en) | 2019-12-02 | 2022-03-15 | Advanced Micro Devices, Inc. | Transfer of cachelines in a processing system based on transfer costs |
US11610281B2 (en) * | 2020-08-25 | 2023-03-21 | Samsung Electronics Co., Ltd. | Methods and apparatus for implementing cache policies in a graphics processing unit |
CN112699063B (en) * | 2021-03-25 | 2021-06-22 | 轸谷科技(南京)有限公司 | Dynamic caching method for solving storage bandwidth efficiency of general AI processor |
US11914521B1 (en) | 2021-08-31 | 2024-02-27 | Apple Inc. | Cache quota control |
Citations (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1744058A (en) * | 2004-04-15 | 2006-03-08 | 国际商业机器公司 | System and method for memory management |
US20060112233A1 (en) * | 2004-11-19 | 2006-05-25 | Ibm Corporation | Enabling and disabling cache bypass using predicted cache line usage |
CN1779664A (en) * | 2004-11-26 | 2006-05-31 | 富士通株式会社 | Memory control device and memory control method |
US20060179259A1 (en) * | 2005-02-04 | 2006-08-10 | Arm Limited | Data Processing apparatus and method for controlling access to memory |
CN101122888A (en) * | 2006-08-09 | 2008-02-13 | 国际商业机器公司 | Method and system for writing and reading application data |
CN101589374A (en) * | 2007-01-24 | 2009-11-25 | 高通股份有限公司 | Method and apparatus for setting cache policies in a processor |
US20110082983A1 (en) * | 2009-10-06 | 2011-04-07 | Alcatel-Lucent Canada, Inc. | Cpu instruction and data cache corruption prevention system |
US20120215986A1 (en) * | 2007-04-09 | 2012-08-23 | Ivan Schreter | Wait-Free Parallel Data Cache |
CN104254841A (en) * | 2012-04-27 | 2014-12-31 | 惠普发展公司，有限责任合伙企业 | Shielding a memory device |
Family Cites Families (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7266538B1 (en) | 2002-03-29 | 2007-09-04 | Emc Corporation | Methods and apparatus for controlling access to data in a data storage system |
US8521962B2 (en) * | 2009-09-01 | 2013-08-27 | Qualcomm Incorporated | Managing counter saturation in a filter |
US9035961B2 (en) | 2012-09-11 | 2015-05-19 | Apple Inc. | Display pipe alternate cache hint |
-
2016
- 2016-05-26 US US15/165,839 patent/US10089239B2/en active Active
- 2016-05-26 CN CN201680020430.7A patent/CN107810491B/en active Active
- 2016-05-26 WO PCT/US2016/034360 patent/WO2016191569A1/en active Application Filing
- 2016-05-26 DE DE112016002356.1T patent/DE112016002356B4/en active Active
Patent Citations (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1744058A (en) * | 2004-04-15 | 2006-03-08 | 国际商业机器公司 | System and method for memory management |
US20060112233A1 (en) * | 2004-11-19 | 2006-05-25 | Ibm Corporation | Enabling and disabling cache bypass using predicted cache line usage |
CN1779664A (en) * | 2004-11-26 | 2006-05-31 | 富士通株式会社 | Memory control device and memory control method |
US20060179259A1 (en) * | 2005-02-04 | 2006-08-10 | Arm Limited | Data Processing apparatus and method for controlling access to memory |
CN101122888A (en) * | 2006-08-09 | 2008-02-13 | 国际商业机器公司 | Method and system for writing and reading application data |
CN101589374A (en) * | 2007-01-24 | 2009-11-25 | 高通股份有限公司 | Method and apparatus for setting cache policies in a processor |
US20120215986A1 (en) * | 2007-04-09 | 2012-08-23 | Ivan Schreter | Wait-Free Parallel Data Cache |
US20110082983A1 (en) * | 2009-10-06 | 2011-04-07 | Alcatel-Lucent Canada, Inc. | Cpu instruction and data cache corruption prevention system |
CN104254841A (en) * | 2012-04-27 | 2014-12-31 | 惠普发展公司，有限责任合伙企业 | Shielding a memory device |
Non-Patent Citations (1)
Title |
---|
孙丽珺，王立宏等: "基于多优先级的动态阈值RED算法", 《计算机工程》 * |
Also Published As
Publication number | Publication date |
---|---|
DE112016002356B4 (en) | 2024-02-29 |
US10089239B2 (en) | 2018-10-02 |
DE112016002356T5 (en) | 2018-02-22 |
WO2016191569A1 (en) | 2016-12-01 |
CN107810491B (en) | 2022-02-01 |
US20160350232A1 (en) | 2016-12-01 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN107810491A (en) | Memory system architecture | |
US9075730B2 (en) | Mechanisms to bound the presence of cache blocks with specific properties in caches | |
US8417891B2 (en) | Shared cache memories for multi-core processors | |
US6408362B1 (en) | Data processing system, cache, and method that select a castout victim in response to the latencies of memory copies of cached data | |
Kim et al. | Subspace snooping: Filtering snoops with operating system support | |
US7277992B2 (en) | Cache eviction technique for reducing cache eviction traffic | |
US9251069B2 (en) | Mechanisms to bound the presence of cache blocks with specific properties in caches | |
US20140181402A1 (en) | Selective cache memory write-back and replacement policies | |
Zebchuk et al. | Multi-grain coherence directories | |
US20100268886A1 (en) | Specifying an access hint for prefetching partial cache block data in a cache hierarchy | |
Sembrant et al. | Tlc: A tag-less cache for reducing dynamic first level cache energy | |
US7287122B2 (en) | Data replication in multiprocessor NUCA systems to reduce horizontal cache thrashing | |
Dublish et al. | Cooperative caching for GPUs | |
WO2014085002A1 (en) | Memory management using dynamically allocated dirty mask space | |
KR20180122969A (en) | A multi processor system and a method for managing data of processor included in the system | |
US11151039B2 (en) | Apparatus and method for maintaining cache coherence data for memory blocks of different size granularities using a snoop filter storage comprising an n-way set associative storage structure | |
Li et al. | Elastic-cache: GPU cache architecture for efficient fine-and coarse-grained cache-line management | |
US8473686B2 (en) | Computer cache system with stratified replacement | |
DE102018002480A1 (en) | SYSTEM, DEVICE AND METHOD FOR OUT OF SERVICE OF NONLOKALITY-BASED COMMAND TREATMENT | |
Yao et al. | SelectDirectory: A selective directory for cache coherence in many-core architectures | |
US20220292015A1 (en) | Cache Victim Selection Based on Completer Determined Cost in a Data Processing System | |
US20140258621A1 (en) | Non-data inclusive coherent (nic) directory for cache | |
Shukla et al. | Tiny directory: Efficient shared memory in many-core systems with ultra-low-overhead coherence tracking | |
US6442653B1 (en) | Data processing system, cache, and method that utilize a coherency state to indicate the latency of cached data | |
Sembrant et al. | A split cache hierarchy for enabling data-oriented optimizations |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
CB02 | Change of applicant information |
Address after: American CaliforniaApplicant after: Google limited liability companyAddress before: American CaliforniaApplicant before: Google Inc. |
|
CB02 | Change of applicant information | ||
GR01 | Patent grant | ||
GR01 | Patent grant |