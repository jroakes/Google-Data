CN107305585B - Search query prediction by keyboard - Google Patents
Search query prediction by keyboard Download PDFInfo
- Publication number
- CN107305585B CN107305585B CN201611273011.2A CN201611273011A CN107305585B CN 107305585 B CN107305585 B CN 107305585B CN 201611273011 A CN201611273011 A CN 201611273011A CN 107305585 B CN107305585 B CN 107305585B
- Authority
- CN
- China
- Prior art keywords
- keyboard
- search
- application
- computing device
- graphical
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/02—Input arrangements using manually operated switches, e.g. using keyboards or dials
- G06F3/023—Arrangements for converting discrete items of information into a coded form, e.g. arrangements for interpreting keyboard generated codes as alphanumeric codes, operand codes or instruction codes
- G06F3/0233—Character input methods
- G06F3/0237—Character input methods using prediction or retrieval techniques
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L51/00—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail
- H04L51/04—Real-time or near real-time messaging, e.g. instant messaging [IM]
- H04L51/046—Interoperability with other network applications or services
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/242—Query formulation
- G06F16/2428—Query predicate definition using graphical user interfaces, including menus and forms
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/248—Presentation of query results
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/332—Query formulation
- G06F16/3322—Query formulation using system suggestions
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/332—Query formulation
- G06F16/3322—Query formulation using system suggestions
- G06F16/3323—Query formulation using system suggestions using document space presentation or visualization, e.g. category, hierarchy or range presentation and selection
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/3331—Query processing
- G06F16/334—Query execution
- G06F16/3344—Query execution using natural language analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/903—Querying
- G06F16/9032—Query formulation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/903—Querying
- G06F16/9032—Query formulation
- G06F16/90324—Query formulation using system suggestions
- G06F16/90328—Query formulation using system suggestions using search space presentation or visualization, e.g. category or range presentation and selection
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/951—Indexing; Web crawling techniques
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
- G06F16/9535—Search customisation based on user profiles and personalisation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/953—Querying, e.g. by the use of web search engines
- G06F16/9538—Presentation of query results
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/04842—Selection of displayed objects or displayed text elements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
- G06F3/04886—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures by partitioning the display area of the touch-screen or the surface of the digitising tablet into independently controllable areas, e.g. virtual keyboards or menus
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/0482—Interaction with lists of selectable items, e.g. menus
Abstract
The application relates to search query prediction by a keyboard. A computing device is described that includes at least one processor and memory including instructions that, when executed, cause the at least one processor to output for display a graphical keyboard that includes a plurality of keys, and determine text of an electronic communication based on an indication of a selection of one or more keys from the plurality of keys. The instructions, when executed, further cause the at least one processor to identify a searchable entity or trigger phrase based at least in part on the text, generate a search query based on the searchable entity or trigger phrase, and output for display within a graphical keyboard a graphical indication indicating that the computing device generated the search query.
Description
Technical Field
The present application relates generally to search query predictions made by a keyboard, and in particular to techniques for enabling a computing device to automatically predict a search query based on text entered with a graphical keyboard and display the predicted query within the graphical keyboard.
Background
While capable of executing several applications simultaneously, some mobile computing devices can only present a Graphical User Interface (GUI) for a single application at a time. To interact with multiple applications at once, a user of a mobile computing device may have to provide input to switch between different application GUIs to accomplish a particular task. For example, a user of a mobile computing device may have to stop entering text in a messaging application, provide input to cause the device to switch to a search application, and provide additional input at a GUI of the search application to search for particular message blocks that the user may need to complete composing a message or enter text in the messaging application. Providing the number of inputs required by some computing devices to perform various tasks can be tedious, repetitive, and time consuming.
Disclosure of Invention
In one example, a method includes outputting, by a keyboard application executing at a computing device and for display, a graphical keyboard comprising a plurality of keys, determining, by the keyboard application, text of an electronic communication based on an indication of a selection of one or more keys from the plurality of keys, and identifying, by the keyboard application and based at least in part on the text, a searchable entity. The method further includes generating, by the keyboard application, a search query based on the searchable entity, and outputting, by the keyboard application for display within the graphical keyboard, a graphical indication indicating that the computing device generated the search query.
In another example, a mobile device includes: a presence-sensitive display component, at least one processor, and a memory storing instructions associated with a keyboard application. The instructions, when executed, cause the at least one processor to output, for display at the presence-sensitive display component, a graphical keyboard comprising a plurality of keys, determine text of the electronic communication based on an indication of a selection of one or more keys from the plurality of keys detected at the presence-sensitive display component, and identify a trigger phrase based at least in part on the text. The instructions, when executed, further cause the at least one processor to generate a search query based on the trigger phrase, and generate, at the computing device, a graphical indication of the search query for display at the presence-sensitive display component and within the graphical keyboard.
In another example, a method is described, the method comprising: invoking, by a first application executing at a computing device, a keyboard application configured to provide a graphical keyboard comprising a plurality of keys for receiving text input, the text input determined by the keyboard application based on an indication of one or more keys selected from the plurality of keys; identifying, by the keyboard application, a searchable entity based at least in part on the text input, and generating, by the keyboard application, a search query based on the searchable entity. The method further comprises: outputting, by the keyboard application for display within the graphical keyboard, a graphical indication indicating that the keyboard application generated the search query, and in response to receiving, by the keyboard application, an indication of a selection of the search query, initiating, by the keyboard application, a search at a search application based on the search query.
The details of one or more examples are set forth in the accompanying drawings and the description below. Other features, objects, and advantages of the disclosure will be apparent from the description and drawings, and from the claims.
Drawings
Fig. 1 is a conceptual diagram illustrating an example computing device configured to present a graphical keyboard with integrated search features in accordance with one or more aspects of the present disclosure.
FIG. 2 is a block diagram illustrating an example computing device configured to present a graphical keyboard with integrated search features in accordance with one or more aspects of the present disclosure.
Fig. 3 is a block diagram illustrating an example computing device outputting graphical content for display at a remote device in accordance with one or more techniques of this disclosure.
4A-4E are conceptual diagrams illustrating an example graphical user interface of an example computing device configured to present a graphical keyboard with integrated search features according to one or more aspects of the present disclosure.
FIG. 5 is a flow diagram illustrating example operations of a computing device configured to present a graphical keyboard with integrated search features in accordance with one or more aspects of the present disclosure.
FIG. 6 is a flow diagram illustrating example operations of a computing device configured to present a graphical keyboard with integrated search features in accordance with one or more aspects of the present disclosure.
Detailed Description
In general, this disclosure relates to techniques for enabling a computing device to automatically predict a search query based on text entered with a graphical keyboard and display the predicted query within the graphical keyboard. In various instances, the techniques may also enable the computing device to perform a search based on a displayed query directly from within the graphical keyboard to obtain text that may be related to entry using the graphical keyboard. For example, a user may interact with a graphical keyboard presented at a presence-sensitive screen (e.g., a touch screen) by a keyboard application. The interaction may be associated with a communication application or similar application other than a search application, for example, as part of a messaging or text transfer application Graphical User Interface (GUI). As the computing device detects an input associated with the graphical keyboard (e.g., as a user enters a message), the graphical keyboard application may determine a word from the input and identify a searchable entity, term, or trigger phrase based on the word. The keyboard application may automatically generate and display suggested search queries derived from one or more of the additional information related to the searchable entity, term, or trigger phrase. If the user is interested in searching based on the displayed query, he may select the suggested search query and cause the keyboard application to initiate a search (which may optionally be performed on content stored on the computing device and/or by a search engine on content stored remotely from the computing device). In some examples, within or through a keyboard application, the computing device may present the search results as part of or in place of the keyboard (e.g., in place of a graphical keyboard).
By providing a GUI that includes a graphical keyboard with integrated search query predictions, an example computing device may provide a way for a user to quickly obtain search results for the user that are relevant to input that the user has provided at the graphical keyboard without having to switch between several different applications and application GUIs, re-type text that has been input at the graphical keyboard, or think of a relevant search query by himself or herself. In this way, techniques of this disclosure may reduce the number of user inputs and the amount of time required to obtain search results, which may simplify the user experience and may reduce power consumption of the computing device.
Throughout this disclosure, examples are described in which an application, computing device, and/or computing system analyzes information associated with the computing device and a user of the computing device (e.g., text entered at a graphical keyboard, context, location, speed, search query, etc.) only if the user of the computing device provides permission to analyze the information. For example, in the context discussed below, prior to an application, a computing device or computing system can collect or can utilize information associated with a user, the user can be provided with an opportunity to provide input to control whether the application, computing device, and/or computing system can collect and utilize user information (e.g., information about text entered at a graphical keyboard, etc.), or to specify whether and/or how the application, device, and/or system receives content relevant to the user. Further, certain data may be processed in one or more ways before it is stored or used by an application, computing device, and/or computing system, such that personally identifiable information is removed. For example, the user identity may be processed such that personally identifiable information about the user cannot be determined, or the user's physical location may be generalized where location information is obtained (such as to a city, zip code, or state level), such that the user's particular location cannot be determined. Thus, the user may control how information about the user is collected and used by the computing device and computing system.
FIG. 1 is a conceptual diagram illustrating an example computing system 110 configured to present a graphical keyboard with integrated search features according to one or more aspects of the present disclosure. Computing device 110 may represent a mobile device, such as a smartphone, tablet computer, laptop computer, computerized watch, computerized eyewear, computer glove, or any other type of portable computing device. Additional examples of computing device 110 include desktop computers, televisions, Personal Digital Assistants (PDAs), portable gaming systems, media players, electronic book readers, mobile television platforms, automotive navigation and entertainment systems, vehicle (e.g., automobile, aircraft, or other vehicle) cockpit displays, or any other type of wearable and non-wearable, mobile, or non-mobile computing device that can output a graphical keyboard for display.
Computing device 110 includes a presence-sensitive display (PSD)112, a User Interface (UI) module 120, and a keyboard module 122. Modules 120 and 122 may perform the described operations using software, hardware, firmware, or a mix of hardware, software, and firmware resident in and/or executing at computing device 110. One or more processors of computing device 110 may execute instructions stored at a memory or other non-volatile storage medium of computing device 110 to perform the operations of modules 120 and 122. Computing device 110 may execute modules 120 and 122 as virtual machines executing on the underlying hardware. Modules 120 and 122 may execute as one or more services of an operating system or computing platform. Modules 120 and 122 may execute at the application layer of the computing platform as one or more executable programs.
The PSD 112 of the computing device 110 may serve as a respective input and/or output device for the computing device 110. The PSD 112 may be implemented using various techniques. For example, PSD 112 may function as an input device using a presence-sensitive input screen, such as a resistive touch screen, a surface acoustic wave touch screen, a capacitive touch screen, a projected capacitive touch screen, a pressure-sensitive screen, an acoustic pulse recognition touch screen, or another presence-sensitive display technology. PSD 112 may also function as an output (e.g., display) device using any one or more display devices, such as a Liquid Crystal Display (LCD), a dot matrix display, a Light Emitting Diode (LED) display, an Organic Light Emitting Diode (OLED) display, electronic ink, or similar monochrome or color display capable of outputting visual information to a user of computing device 110.
The PSDs 112 may detect input (e.g., touch or non-touch input) from a user of the respective computing device 110. The PSD 112 can detect an input indication by detecting one or more gestures from the user (e.g., the user touching, pointing, and/or swiping with a finger or stylus at one or more locations of the PSD 112). The PSD 112 may output information to the user in the form of a user interface (e.g., user interface 114), which may be associated with functionality provided by the computing device 110. Such user interfaces may be associated with a computing platform, operating system, application, and/or service (e.g., an electronic messaging application, chat application, internet browser application, mobile or desktop operating system, social media application, electronic game, or other type of application) executing at or accessible from computing device 110. For example, PSD 112 may present user interface 114, as shown in fig. 1, that user interface 114 is a graphical user interface of a chat application executing at computing device 110 and includes various graphical elements displayed at various locations of PSD 112.
As shown in FIG. 1, user interface 114 is a chat user interface. However, user interface 114 may be any graphical user interface including a graphical keyboard that incorporates search features. The user interface 114 includes an output area 116A, a graphical keyboard 116B, and an editing area 116C. A user of computing device 110 may provide input at graphical keyboard 116B to generate text characters within editing area 116C that form the content of the electronic message displayed within output area 116A. The messages displayed within the output area 116A constitute a chat conversation between the user of the computing device 110 and the user of a different computing device.
In some examples, keyboard module 122 may be a stand-alone application, service, or module executing at computing device 110, and in other examples, keyboard module 122 may be a subcomponent of a service that serves other application or device functions. For example, keyboard module 122 may be integrated into a chat or messaging application executing at computing device 110, while in other examples, keyboard module 122 may be a stand-alone application or subroutine called by an application or operating platform of computing device 110 whenever graphical keyboard input functionality is required by the application or operating platform. In some examples, computing device 110 may download and install keyboard module 122 from an application store of a service provider (e.g., via the internet). In other examples, keyboard module 122 may be preloaded during production of computing device 110.
While operating in the text entry mode, the keyboard module 122 of the computing device 110 may perform traditional, graphical keyboard operations used for text entry, such as: generating a graphical keyboard layout for display at PSD 112, mapping input detected at PSD 112 to selection of graphical keys, determining characters based on selected keys, or predicting or automatically correcting words and/or phrases based on characters determined from selected keys.
Each of the graphical keys 118A may be associated with one or more features (e.g., letters, numbers, punctuation, or other characters) displayed within the key. A user of computing device 110 may provide input at a location of PSD 112 that displays one or more of graphical keys 118A to enter content (e.g., characters, search results, etc.) into editing area 116C (e.g., for composing a message that is sent and displayed within output area 116A or for entering a search query executed by device 110 from within graphical keyboard 116B). Keyboard module 122 may receive information from UI module 120 indicating a location associated with an input detected by PSD 112, the location being related to the location of each of the graphical keys. Using the spatial and/or language model, keyboard module 122 may translate the input into a selection of keys and characters, words, and/or phrases.
For example, when a user of computing device 110 provides user input at or near the location of PSD 112 where PSD 112 presents graphical keys 118A, PSD 112 may detect the user's input. The user may type in at the graphical key 118A to enter the phrase "wait to get diner? (do you want to eat). UI module 120 may receive indications of user inputs detected by PSD 112 from PSD 112 and output information about the user inputs to keyboard module 122. Information about the user input may include an indication of one or more touch events detected by PSD 112 (e.g., location or other information about the input).
Based on information received from UI module 120, keyboard module 122 may map detected input at PSD 112 to selection of graphical key 118A, determine characters based on the selected key 118A, and predict or automatically correct words and/or phrases determined based on the characters associated with the selected key 118A. For example, the keyboard module 122 may include a spatial model that may determine that the one or more keys 118A that are most likely to be selected are the keys based on the locations of the keys 118A and information about the inputs. In response to determining the one or more keys 118A that are most likely to be selected, the keyboard module 122 may determine one or more characters, words, and/or phrases. For example, each of the one or more keys 118A selected according to user input at the PSD 112 may represent an individual character or keyboard operation. The keyboard module 122 may determine the selected character sequence based on the one or more selected keys 118A. In some examples, keyboard module 122 may apply a language model to the sequence of characters to determine one or more most likely candidate letters, morphemes 9, words, and/or phrases that the user front view entered based on the selection of key 118A. In the example of fig. 1, keyboard module 122 may determine that the phrase "wait to get diner? "the letter corresponds to a sequence of characters.
The keyboard module 122 may send a sequence of characters and/or candidate words and phrases (e.g., "did the wait to get diner") to the UI module 120 and the UI module 120 may cause the PSD 112 to present the characters and/or candidate words determined from the selection of the one or more keys 118A as text within the editing area 116C. In some examples, when acting as a traditional keyboard for performing text entry operations and in response to receiving user input at image keys 118A (e.g., when a user types at graphical keyboard 116B to enter text within edit area 116C), keyboard module 122 may cause UI module 120 to display candidate words and/or phrases as selectable word or phrase suggestions and/or one or more selectable spelling corrections within suggestion area 118B.
In addition to performing traditional, graphical keyboard operations used for text entry, keyboard module 122 of computing device 110 also provides integrated search capabilities. That is, rather than requiring the user of computing device 110 to navigate away from user interface 114 providing graphical keyboard 116B (e.g., to a different application or service executing at computing device 110 or accessible from computing device 110), keyboard module 122 can operate in a search mode in which keyboard module 122 can perform search operations-predicting and presenting search queries based on text entered at graphical keyboard 116B, and presenting searches in one or more possible locations and formats, such as results within the same area of PSD 112 displaying graphical keyboard 116B.
As indicated above, keyboard module 122 may execute as a stand-alone application, service, or module executing at computing device 110 or as a separate, integrated sub-component therein. Thus, if keyboard module 122 forms part of a chat or messaging application executing at computing device 110, keyboard module 122 may provide text entry capabilities as well as search capabilities to the chat or messaging application. Similarly, if keyboard module 122 is a standalone application or subroutine called by an application or operating platform of computing device 110 at any time that the application or operating platform requires graphical keyboard input functionality, keyboard module 122 may provide text entry capabilities as well as search capabilities to the calling application or operating platform.
In some examples, when operating in the search mode, keyboard module 122 may cause graphical keyboard 116B to include search element 118C. Search element 118C represents a selectable element (e.g., an icon, a key, etc.) of graphical keyboard 116B for manually invoking one or more of the various search features of graphical keyboard 116B. For example, by selecting search element 118C (e.g., by tapping or gesturing at a location or within an area of PSD 112 that displays search element 118C), the user can enable computing device 110 to invoke one or more of the various integrated search features without requiring the user to explicitly navigate to a separate application, service, or other feature that is executed at or accessible from computing device 110.
In some examples, the search element 118C may also be used as an indicator of a status associated with the search feature. For example, if keyboard module 122 predicts a search query that is likely to produce search results related to the user's chat conversation, keyboard module 122 may cause search element 118C to flash, beat, change color, move, or perform some other animation to indicate the identified search query.
In some examples, when operating in the search mode, keyboard module 122 may automatically perform various search functions regardless of whether graphical keyboard 116B includes search element 118C. For example, keyboard module 122 may predict a search query, determine conversions, or generate other suggested content based on text inferred by keyboard module 122 from user input, and display the suggested content within graphical keyboard 116B. For example, keyboard module 122 may configure suggestion area 118B to present suggested content (e.g., predicted search queries, predicted emoticons or so-called "emoji," other suggested content, or any other iconic symbols) as selectable elements within search element 118C in lieu of or in addition to predicted characters, words, or phrases or other primarily linguistic information derived by keyboard module 122 from a language model, dictionary (lexicon), or dictionary (dictionary). In other words, rather than only providing spelling or word suggestions from the dictionary within suggestion area 118B, computing device 110 can include suggested search-related content in addition to or in lieu of suggested linguistic content within suggestion area 118B, which computing device 110 (or other device in conjunction with or in communication with device 110) determines at a current time (e.g., while providing input related to an electronic communication) can assist the user.
When operating in the search mode, keyboard module 122 may perform automatic prediction of search queries that may be related to text entered using graphical keyboard 116B at the current time. In other words, keyboard module 122 may enable computing device 110 to provide a way for a user to quickly obtain information in the form of suggested searches or search results that is relevant to the input that the user has provided at graphical keyboard 116B without having to switch between several different applications or application GUIs, re-type text that has been entered at graphical keyboard 116B, or think of a relevant search query by himself or herself. Keyboard module 122 may automatically generate and display a search query (e.g., at suggestion area 118C). If the user is interested in searching based on the displayed query, the user can optionally provide input at the PSD 112 location where suggested queries are displayed to select the search query and cause the keyboard module 122 to initiate the search. In some examples, keyboard module 122 can cause UI module 120 and PSD 112 to present search results in place of a portion of graphical keyboard 118A.
For example, as shown in FIG. 1, a user of computing device 110 may want to make a plan to have dinner with a friend and may exchange electronic communications (e.g., messages) with the friend from within user interface 114. The user may begin a conversation by providing a gesture input at the PSD 112 at a location where the key 118A is displayed, and a spatial and/or language model of the keyboard module 122 may determine, based on the input, a gesture input corresponding to a selection of the key 118A for entering the phrase "Working music index? (do you work longer)'. The user may provide input at the location of the enter key of the keys 118A and, in response, the messaging application associated with the user interface 114 may include the text "work music index? "to the computing device associated with the friend. Upon receiving a reply message including the text "Nope, wrapping up" (not, being picked up) from the computing device associated with the friend, the messaging application may present the contents of the reply within the user interface 114.
In some examples, keyboard module 122 may initiate a local-only device search for information stored at computing device 110 without accessing a remote computing device, such as a server. For example, the keyboard module 122 may execute itself or may invoke a locally stored search module that executes against information stored by the computing device 110.
In some examples, keyboard module 122 may conduct a search. And in some examples, keyboard module 122 may rely on a separate device resource or remote resource to complete the search. For example, keyboard module 122 may send instructions to perform a search along with the query to an internal search module executing at computing device 110. And in some examples, keyboard module 122 may send instructions to perform the search along with the query to an external search module, such as a server, executing at the remote computing device. In either case, the internal or external search module may return the search results to keyboard module 122.
The user of the computing device 110 may further provide an input at the location of the PSD 112 where the graphical keyboard 116B is displayed to select one or more keys 118A for composing a second message to a friend stating "won to get diner? ". Keyboard module 122 may receive an indication (e.g., one or more touch events) of a selection of one or more keys 118A from UI module 120.
Based on the indication of the selection of one or more keys 118A, keyboard module 122 may determine the text of the electronic communication. For example, the spatial and/or language model of the keyboard module 122 may translate individual key selections associated with an input into a representation phrase "Want to get diner? "and/or a string or sequence of words.
In some examples, keyboard module 122 may perform search query prediction by an off-device-dependent (off-device) annotator or a remote annotator in addition to or as an alternative to using an on-device annotator. For example, keyboard module 122 may access a cloud service for obtaining a search query prediction by sending text inferred by graphical keyboard 116B and/or other information about computing device 110 to the cloud, and in response, receive one or more related search queries.
Using both the global model and the local model, the annotator of keyboard module 122 can parse text entered at graphical keyboard 116B in order to detect searchable entities and trigger phrases. Keyboard module 122 may include in the model those entities and trigger phrases that are likely to be related or occur most frequently in a chat or text messaging conversation. Keyboard module 122 may provide text inferred from user input at graphical keyboard 116B to a local and/or global model and receive one or more searchable entities as output from the model.
The global model may enable the annotators of keyboard module 122 to identify potentially useful search entities (e.g., international brands and celebrities) that are common across various geographic locations. The local model may enable the annotators of keyboard module 122 to identify potentially useful search entities (e.g., names of local businesses, local events and sights, local politicians, etc.) that are not as common across various geographic locations but are common to a particular geographic location.
Alternatively or in addition to the local and global models, the annotator of keyboard module 122 may further rely on artificial intelligence and machine learning techniques to determine with some degree of confidence whether the user is typing in relation to a searchable entity or trigger phrase. For example, keyboard module 122 may rely on machine learning models including artificial neural networks, recurrent neural networks, long-short term memory (LSTM) models, hidden markov models, or any and all other types of machine learning type models that use learning rules to determine with some degree of certainty whether text entered at graphical keyboard 116B is relevant to a searchable entity or trigger phrase. For example, in the case of the LSTM model, the LSTM model may initially be trained on multi-device and multi-user chat conversations to detect with confidence whether the user is currently typing about a particular, searchable entity or trigger phrase. After identifying a potentially searchable entity or trigger phrase using the local and/or global model, the annotator of keyboard module 122 may determine using the LSTM model that the score assigned to the searchable entity or trigger phrase indicates a probability that the searchable entity or trigger phrase is related to one or more words parsed by the annotator from the original text input. If the score assigned to a searchable entity or trigger phrase satisfies a threshold, keyboard module 122 may generate a query using the searchable entity or trigger phrase.
In some examples, the local model and/or the global model may determine initial scores associated with the potential searchable entity or the trigger phrase, and keyboard module 122 may use the LSTM model to enhance or improve the initial scores determined from the local model and/or the global model. For example, the initial score determined by the local model and/or the global model may be increased or decreased by the LSTM model of keyboard module 122 that has been trained on chat conversations to detect with confidence that the user is talking about a particular entity. The LSTM model may rely on context information of computing device 110 and the content of previous and subsequent messages (in a chat context) to determine with greater confidence than the local model or global model alone whether the user is chatting about a particular searchable entity or trigger phrase.
In addition to identifying searchable entities, the local model and/or the global model of the keyboard module 122 may also suggest a rounded phrase (curved phrase) for use in generating search queries. For example, the keyboard module 122 may provide text inferred from user input at the image keyboard 116B (e.g., the phrase "wait to get diner. Keyboard module 122 may enter text into a local model and, in response to the local model recognizing the phrase as a predefined trigger, keyboard module 122 may receive as output a reformulation phrase "Restaurants near me" from the local model. In some examples, the annotator of keyboard module 122 may further rely on artificial intelligence and machine learning techniques to determine with some confidence whether the user is typing with respect to a particular reformulation phrase. For example, using one or more rules about content typed by others in chat conversations for various contexts, the annotator can generate rules for inferring a reformulation phrase based on the textual input and the particular context. Using this rule, the annotator can predict with some degree of certainty whether the user is chatting about "retaurants near me", "food near", "movies showing at a near movie the player", "directions to a place", or some other reformulating phrase that can be related to the text input. If the degree of certainty (e.g., likelihood) satisfies a threshold, keyboard module 122 may use the reformulated phrase in generating the query.
The keyboard module 122 may output a graphical indication to indicate that the computing device generated the search query for display within the graphical keyboard 116. For example, keyboard module 122 may cause search element 118C to flash or change format, change shape, change color, move, or perform other animations to indicate that the search query was identified. Additionally or alternatively, the keyboard module 122 may output the text of the generated search query as a blue hyperlink (e.g., underlined or not underlined). For example, as shown in FIG. 1, keyboard module 122 causes UI module 120 to present the search query "reserves" within suggestion area 118B. As a further indication that keyboard module 122 determined the search query, keyboard module 122 may cause UI module 120 to present an icon associated with the search query. For example, keyboard module 122 may cause UI module 120 to present icons (e.g., forks and knife icons next to the word retaurants within suggestion area 118B) associated with the category of the search query next to the text language of the query. Alternatively, as in the example shown in FIG. 1, PSD 112 may display a magnified icon next to the word retaurants within suggestion region 118B — the magnified icon is not associated with any particular query category.
Upon automatically presenting the search query within graphical keyboard 116B, the user may select the search query and cause keyboard module 122 to perform a search from within user interface 114 based on the query. For example, the user may provide a tap or swipe gesture at the location of the PSD 112 at which the suggestion region 118B is displayed, and in response to receiving an indication of the tap or swipe gesture from the UI module 120, the keyboard module 122 may perform a search based on the query. Keyboard module 122 may cause UI module 120 to present results of the search within user interface 114, from which user may select one or more of the results and enter information associated with the results into a new electronic message.
By providing a GUI that includes a graphical keyboard with integrated search query predictions, an example computing device may provide a user with a way to quickly obtain search results relevant to input that the user has provided at the graphical keyboard without switching between several different application GUIs, retyping text that has been input at the graphical keyboard, or switching between thinking about a relevant search query by himself or herself. In other words, unlike other computing devices that require a user to exit a chat application GUI and provide subsequent text input at a different search application GUI (e.g., by pasting or re-typing text previously entered in the chat application) to search for a topic previously entered at the chat application, the example computing device automatically predicts a search query and executes the search query without requiring the user to provide any additional input beyond what he or she originally entered when typing the original chat message. The search is not limited to the text of the original message but may relate to the subject matter of the original message. In this way, techniques of this disclosure may reduce the amount of time and the number of user inputs needed to obtain search results related to chat conversations, which may simplify the user experience and may reduce power consumption of the computing device.
Fig. 2 is a block diagram illustrating computing device 210 as an example computing device configured to present a graphical keyboard with integrated search features in accordance with one or more aspects of the present disclosure. Computing device 210 of fig. 2 is described below as an example of computing device 110 of fig. 1. Fig. 2 illustrates only one particular example of computing device 210, and many other examples of computing device 210 may be used in other instances and may include a subset of the components included in example computing device 210 or may include components of additional components not shown in fig. 2.
As shown in the example of fig. 2, computing device 210 includes PSD212, one or more processors 240, one or more communication units 242, one or more input components 244, one or more output components 246, and one or more storage components 248. Presence-sensitive display 212 includes display component 202 and presence-sensitive input component 204. Storage components 248 of computing device 210 include UI module 220, keyboard module 222, and one or more application modules 224. Keyboard module 122 can include a spatial model ("SM") module 226, a language model ("LM") module 228, and a search module 230. Storage 248 also includes local and global modules 232A and 232B (collectively "modules 232") and a machine learning module 233 (e.g., LSTM or other machine learning model). Communication channel 250 may interconnect each of components 212, 240, 242, 244, 246, and 248 for inter-component communication (physically, communicatively, and/or operatively). In some examples, communication channel 250 may include a system bus, a network connection, an interprocess communication data structure, or any other method for transferring data.
One or more communication units 242 of computing device 210 may communicate with external devices via one or more wired and/or wireless networks by transmitting and/or receiving network signals over the one or more networks. Examples of communication unit 242 include a network interface card (e.g., such as an ethernet card), an optical transceiver, a radio frequency transceiver, a GPS receiver, or any other type of device capable of sending and/or receiving information. Other examples of the communication unit 242 may include short wave radio, cellular data radio, wireless network radio, and Universal Serial Bus (USB) controller.
One or more input components 244 of computing device 210 may receive input. Examples of inputs are tactile, audio, and video inputs. In one example, input component 244 of computing device 210 includes a presence-sensitive input device (e.g., a touch-sensitive screen, PSD), a mouse, a keyboard, a voice-responsive system, a video camera, a microphone, or any other type of device for detecting input from a human or machine. In some examples, input components 244 may include one or more sensor components, one or more location sensors (GPS components, Wi-Fi components, cellular components), one or more temperature sensors, one or more movement sensors (e.g., accelerometers, gyroscopes), one or more pressure sensors (e.g., barometers), one or more ambient light sensors, and one or more other sensors (e.g., microphones, cameras, infrared proximity sensors, hygrometers, etc.). Other sensors may include heart rate sensors, magnetometers, glucose sensors, hygrometer sensors, olfactory sensors, compass sensors, pedometers, among other non-limiting examples.
One or more output components 246 of the computing device 110 may generate output. Examples of outputs are tactile, audio, and video outputs. In one example, output components 246 of computing device 210 include a PSD, sound card, video graphics adapter card, speaker, Cathode Ray Tube (CRT) monitor, Liquid Crystal Display (LCD), or any other type of device for generating output to a human or machine.
PSD212 of computing device 210 may be similar to PSD 112 of computing device 110 and include display component 202 and presence-sensitive input component 204. Display component 202 may be a screen at which information is displayed by PSD212 and presence-sensitive input component 204 may detect objects at and/or near display component 202. As one example range, presence-sensitive input component 204 may detect an object, such as a finger or stylus within two inches or less of display component 202. Presence-sensitive input component 204 may determine the location (e.g., [ x, y ] coordinates) of display component 202 at which the object was detected. In another example range, presence-sensitive input component 204 may detect objects six inches or less from display component 202 and other ranges are possible. Presence-sensitive input component 204 may determine a position of display component 202 selected by a user's finger using capacitive, inductive, and/or optical recognition techniques. In some examples, presence-sensitive input component 204 provides output to a user using touch, audio, or video stimuli as described with respect to display component 202. In the example of FIG. 2, PSD212 may present a user interface (e.g., graphical user interface 114 of FIG. 1).
Although illustrated as an internal component of computing device 210, PSD212 may also represent an external component that shares a data path with computing device 210 for transmitting and/or receiving input and output. For example, in one example, PSD212 represents a built-in component of computing device 210 (e.g., a screen on a mobile phone) that is located within an enclosure external to computing device 210 and physically connected thereto. In another example, PSD212 represents an external component of computing device 210 that is located outside and physically separate from the enclosure or housing of computing device 210 (e.g., a monitor, projector, etc. that shares a wired/wireless data path with computing device 210).
PSD212 of computing device 210 may detect two-dimensional and/or three-dimensional gestures from a user of computing device 210 as input. For example, a sensor of PSD212 may detect movement of the user (e.g., moving a hand, arm, pen, stylus, etc.) within a threshold distance of the PSD212 sensor. PSD212 may determine a two-dimensional or three-dimensional vector representing movement and associate the vector representation with a gesture input having multiple dimensions (e.g., hand-wave, pinch, tap, pen stroke, etc.). In other words, PSD212 is able to detect multi-dimensional gestures without requiring the user to gesture at or near the screen or surface of PSD212 that outputs information for display. Alternatively, PSD212 can detect multi-dimensional gestures performed at or near the sensor, which may or may not be located near the screen or surface of PSD212 that outputs information for display.
One or more processors 240 may implement the functions associated with computing device 210 and/or execute instructions associated with computing device 210. Examples of processor 240 include an application processor, a display controller, an auxiliary processor, one or more sensor hubs, and any other hardware configured to act as a processor, processing unit, or processing device. Modules 220, 222, 224, 226, 228, and 230 may be operable by processor 240 to perform various actions, operations, or functions of computing device 210. For example, processor 240 of computing device 210 may retrieve and execute instructions stored by storage component 248 that cause processor 240 to execute operational modules 220, 222, 224, 226, 228, and 230. The instructions, when executed by processor 240, may cause computing device 210 to store information within storage component 248.
One or more storage components 248 within computing device 210 may store information for processing during operation of computing device 210 (e.g., computing device 210 may store data accessed by modules 220, 222, 224, 226, 228, and 230 and models 232 and 233 during execution of computing device 210). In some examples, storage component 248 is a temporary memory, meaning that the primary purpose of storage component 248 is not long-term storage. The storage component 248 on the computing device 210 may be configured as volatile memory for short-term storage of information and thus not retain content in the event of a power outage. Examples of volatile memory include Random Access Memory (RAM), Dynamic Random Access Memory (DRAM), Static Random Access Memory (SRAM), and other forms of volatile memory known in the art.
In some examples, storage component 248 also includes one or more computer-readable storage media. In some examples, storage component 248 includes one or more non-transitory computer-readable storage media. Storage component 248 may be configured to store larger amounts of information than is typically stored by volatile memory. Storage component 248 may be further configured as a non-volatile storage space for long-term storage of information and to retain information after power on/off cycling. Examples of non-volatile memory include magnetic hard disks, optical disks, floppy disks, flash memory, or forms of electrically programmable memory (EPROM) or Electrically Erasable and Programmable (EEPROM) memory. Storage component 248 may store program instructions and/or information (e.g., data) associated with models 232 and 233 and modules 220, 222, 224, 226, 228, and 230. Storage component 248 may include a memory configured to store data or other information associated with models 232 and 233 and modules 220, 222, 224, 226, 228, and 230.
In some examples, UI module 220 may receive indications of one or more user inputs detected at PSD212 and may output information regarding the user inputs to keyboard module 222. For example, PSD212 may detect user input and send data regarding the user input to UI module 220. UI module 220 may generate one or more touch events based on the detected input. The touch event may include information characterizing the user input, such as a location component of the user input (e.g., [ x, y ] coordinates), a time component (e.g., when the user input was received), a force component (e.g., an amount of pressure applied by the user input), or other data about the user input (e.g., velocity, acceleration, direction, density, etc.).
Based on the location information of the touch event generated from the user input, UI module 220 may determine that the detected user input is associated with a graphical keyboard. UI module 220 may send an indication of the one or more touch events to keyboard module 222 for further explanation. Based on the touch events received from UI module 220, keyboard module 222 may determine that the detected user input represents an initial selection of one or more keys of the graphical keyboard.
Application module 224 represents all of the various individual applications and services that are executed and accessible from computing device 210 that may rely on a graphical keyboard with integrated search features. A user of computing device 210 may interact with a graphical user interface associated with one or more application modules 224 to cause computing device 210 to perform functions. Numerous examples of application modules 224 may exist and include a fitness application, a calendar application, a personal assistant or prediction engine, a search application, a mapping or navigation application, a transportation service application (e.g., a bus or train tracking application), a social media application, a gaming application, an email application, a chat or messaging application, an internet browser application, or any and all other applications that may be executed at computing device 210.
The SM module 226 can receive one or more touch events as input and output a character or sequence of characters that is likely to represent the one or more touch events, along with a certainty or spatial model score that indicates how likely or with what precision the one or more characters are to define the touch event. In other words, the SM module 226 can infer a touch event as a selection of one or more keys of the keyboard and can output a character or sequence of characters based on the selection of the one or more keys.
When keyboard module 222 operates in a text entry mode, LM module 228 may receive a character or sequence of characters as input and output one or more candidate characters, words, or phrases identified by LM module 228 from the dictionary as potential alternatives to the sequence of characters that LM module 228 receives as input for a given language context (e.g., a sentence in written language). Keyboard module 222 may cause UI module 220 to present one or more of the candidate words at suggestion region 118C of user interface 114.
The dictionary of computing device 210 may include a list of words within a written language vocabulary (e.g., a dictionary). For example, the dictionary may include a database of words (e.g., words in a standard dictionary and/or words added to the dictionary by the user or computing device 210). LM module 228 may perform a lookup of a character string in the dictionary to identify one or more letters, words, and/or phrases that include some or all of the characters in the character string. For example, LM module 228 may assign language model probabilities or similarity coefficients (e.g., Jaccard similarity coefficients, or other similarity coefficients) to one or more candidate words located at a dictionary of computing device 210 that include at least some of the same characters as the input character or sequence of characters. The language model probability assigned to each of the one or more candidate words indicates a degree of certainty or likelihood that the candidate word is typically found to be located after, before, and/or in a sequence of words (e.g., a sentence) generated from a text input detected by presence-sensitive input component 204 before and/or after receiving the current sequence of characters being analyzed by LM module 228. In response to determining the one or more candidate words, LM module 228 may output the one or more candidate words having the highest similarity coefficients from dictionary data store 260A.
The search module 230 of the keyboard module 222 may perform the integrated search function on behalf of the keyboard module 222. That is, when invoked (e.g., manually in response to a user of computing device 210 selecting selectable element 218C of user interface 114 or automatically in response to identifying a searchable entity or trigger phrase from the text input), keyboard module 222 may operate in a search mode in which keyboard module 222 enables computing device 210 to perform search functions from within graphical keyboard 118A, such as predicting or displaying search queries that a user of computing device 210 may find relevant to a chat session.
Model 232 represents a plurality of "on-device" (e.g., locally stored or executed) models for use by an annotator or text analysis engine of search module 230 to parse and analyze text being entered at graphical keyboard 116B to detect whether a user has typed certain content at graphical keyboard 116B that is searchable. The model 232 may receive as input text inferred from user input at the graphical keyboard 116 and, in response, output one or more searchable entities or trigger phrases that may be related to portions of the text input. In some examples, the model 232 may output a score associated with a searchable entity or trigger phrase as an indication of the likelihood that the searchable entity or trigger phrase is relevant to the text input.
By storing and executing the model 232 locally, "on-device" (as opposed to "off-device" like other traditional search systems that may rely on a remote annotator executing at a remote computing system), the computing device 210 may perform search query prediction in real-time or near real-time in order to avoid interruptions or lags in text dialog that a user may have while typing at the graphical keyboard 116B. In other words, the on-device annotator used alone or in conjunction with a remote annotator may enable computing device 210 to more quickly determine a search query because computing device 210 does not have to exchange information with a remote computing device or cloud service that processes the prediction.
In response to a change in the location of computing device 210, search module 230 may automatically download a new local model 232A. For example, search model 230 may receive information about the current location of computing device 210 from communication unit 242 and/or one or more of input components 232 (e.g., a GPS receiver). In response to determining that the current location does not correspond to a location associated with the local model 232A, the search model 230 may query the remote computing system for the local model of the current location. Upon receiving the local model of the current location from the remote computing system, the search model 230 replaces the previous local model 232A with a copy of the local model of the current location. In other words, in some examples, in response to determining a change in the current location of computing device 210 from a first location to a second location, search module 230 may obtain a local model of a searchable entity for the second location from a remote computing system and may replace previous local model 232A with the local model of the second location.
The search module 230 may select one or more search entities from among the identified global and local search entities with the highest scores according to the local and global models 232. From the highest scoring entity, the search module 230 may generate a query.
In addition to identifying searchable entities, the local and/or global models 232 used by the search module 230 may also suggest a reformulation phrase for use in generating a search query. Based at least in part on the searchable entity or the trigger phrase included in the searchable entity, the search model 230 may determine a reformulation phrase associated with the searchable entity and generate a search query from the reformulation phrase. For example, keyboard module 222 may provide text determined from user input at graphical keyboard 116B (e.g., the phrase "what time is kickoff") to local and/or global model 232 and receive as output from model 232 one or more reduced phrases such as "local sports team schedule" in response to local model 232A recognizing the text as including a predetermined trigger.
Machine learning model 233 represents an "on-device" (e.g., locally stored and executed) LSTM model for use by annotators or text analysis engines of search module 230 to enhance and further refine determinations made using local and/or global model 232 as to whether a user has typed in searchable something at graphical keyboard 116B. For example, search module 230 may input one or more words inferred from user interaction with graphical keyboard 116B to machine learning model 233 to determine refined or updated respective scores assigned to each of the one or more global or local search entities identified by model 232. In this manner, the machine learning model 233 may improve the computing device 220's confidence of whether the user is providing input related to a searchable entity.
In some examples, an annotator of search model 230 (e.g., relying on machine learning model 233) may further rely on the current context of computing device 210 to determine searchable entities and/or reformulated phrases for use in generating the predicted query. As used herein, the current context specifies characteristics of the physical and/or virtual environment of the computing device, such as computing device 210 and the user of the computing device, at a particular time. Furthermore, the term "contextual information" is used to describe any information that can be used by a computing device to define virtual and/or physical environmental features that the computing device and a user of the computing device may experience at a particular time.
Examples of contextual information are numerous and may include: sensor information obtained by sensors of computing device 210 (e.g., location sensors, accelerometers, gyroscopes, barometers, ambient light sensors, proximity sensors, microphones, and any other sensors), communication information sent and received by a communication module of computing device 210 (e.g., text-based communication information, audible communication information, video communication information, etc.), and application usage information associated with applications executing at computing device 210 (e.g., application data associated with the applications, internet search history, text communications, voice and video communications, calendar information, social media publication and related information, etc.). Further examples of contextual information include signals and information obtained from a transmitting device external to computing device 210.
Machine learning model 233 may rely on contextual information including information associated with an electronic conversation that includes an electronic communication that a user may be composing when the user provides input at graphical keyboard 116B, as well as one or more other electronic communication information that has been sent or received by computing device 220. For example, keyboard module 222 may modify the scores assigned to the searchable entities by global model 232B or local model 232A based on contextual information (e.g., text or other information associated with previous messages sent by computing device 220 and text or other information associated with previous messages received by computing device 220) and in accordance with machine learning model 233. After modifying or refining the score assigned to the searchable entity and in response to determining that the score assigned to the searchable entity satisfies a threshold, keyboard module 222 may identify the searchable entity.
The search module 230 can rely on artificial intelligence and machine learning techniques performed by the computer learning model 233 to determine with some degree of confidence whether the user is typing with respect to a particular reformulation phrase for a particular context. For example, using one or more rules of machine learning model 233 with respect to content typed by others in chat conversations for various contexts, an annotator of search module 230 can generate rules for inferring a reformulation phrase based on the textual input and the particular context. Using the rules, the annotator can predict with some degree of certainty whether the user is chatting about "retaurants near me", "food near", "movies playing at a near the user", "directions to a place", or some other termizing phrase that may be related to the text input. If the degree of certainty (e.g., probability) satisfies a threshold, keyboard module 122 can use the reformulated phrase in generating the query.
Using machine learning model 233, search model 230 may rely on the current context of computing device 210 to resolve ambiguities in determining searchable entities. For example, if the text of the chat conversation includes the phrase "what time is kickoff," the search module 230 may obtain the names of several professional soccer teams scheduled to play on a different day from the global model 232B. Based on the current location of computing device 210 and using machine learning model 233, search model 230 may infer that the user is likely to refer to the professional team associated with the location closest to the current location of computing device 210 as the professional team referred to by the user.
In addition to relying on the text of the current message being entered at computing device 210, machine learning model 233 may enable search module 230 to suggest search queries in dependence on previous words, sentences, etc. associated with previous messages sent and/or received by computing device 210. In other words, using the machine learning model 233, the search module 230 may rely on the text of the entire conversation, including the plurality of messages that the computing device 210 has sent and received, to determine search queries that are likely to be relevant to the current conversation.
Fig. 3 is a block diagram illustrating an example computing device outputting graphical content for display at a remote device in accordance with one or more techniques of this disclosure. In general, graphical content may include any visual information that may be output for display, such as text, images, groups of moving images, and so forth. The example shown in fig. 3 includes computing device 310, PSD 312, communication unit 342, mobile device 386, and visual display component 390. In some examples, PSD 312 may be a presence-sensitive display as described in fig. 1-2. Although shown in fig. 1 and 2 for purposes of example as a standalone computing device 110, a computing device, such as computing device 310, may generally be any component or system that includes a processor or other suitable computing environment for executing software instructions, and need not include a presence-sensitive display, for example.
As shown in the example of fig. 3, computing device 310 may be a processor that includes the functionality described with respect to processor 240 in fig. 2. In such an example, computing device 310 may be operatively coupled to PSD 312 by a communication channel 362A, which may be a system bus or other suitable connection. Computing device 310 may also be operatively coupled to a communication unit 342, described further below, by a communication channel 362B, which may also be a system bus or other suitable connection. Although shown separately as an example in fig. 3, computing device 310 may be operatively coupled to PSD 312 and communication unit 342 by any number of one or more communication channels.
In other examples, such as previously illustrated by computing device 110 in fig. 1-2, the computing device may refer to a portable or mobile device, such as a mobile phone (including a smartphone), a laptop computer, and so forth. In some examples, the computing device may be a desktop computer, a tablet computer, a smart television platform, a camera, a Personal Digital Assistant (PDA), a server, or a host.
As shown in fig. 3, computing device 310 may also include and/or be operatively coupled with a communication unit 342. The communication unit 342 may include the functionality of the communication unit 242 as described in fig. 2. Examples of communication unit 342 may include a network interface card, an ethernet card, an optical transceiver, a radio frequency transceiver, or any other type of device that can send and receive information. Other examples of such communication units may include bluetooth, 3G, and Wifi radios, Universal Serial Bus (USB) interfaces, and the like. Computing device 310 may also include and/or be operatively coupled to one or more other devices (e.g., input devices, output components, memory, storage devices) that are not illustrated in fig. 3 for purposes of brevity and illustration.
Fig. 3 also illustrates a mobile device 386 and a visual display assembly 390. Mobile device 386 and visual display component 390 can each include computing and connectivity capabilities. Examples of mobile device 386 may include electronic reader devices, convertible notebook devices, hybrid board devices, and so forth. Examples of visual display component 390 may include other devices such as televisions, computer monitors, and the like. In some examples, the visual display assembly 390 may be a vehicle cockpit display or a navigation display (e.g., in an automobile, aircraft, or some other vehicle). In some examples, the visual display component 390 may be a home automation display or some other type of display that is separate from the computing device 310.
As shown in fig. 3, mobile device 386 may include a presence-sensitive display 388. Visual display component 390 may include a presence-sensitive display 392. Presence- sensitive displays 388, 392 may include a subset of the functionality or all of the functionality of presence- sensitive displays 112, 212\ and/or 312 as described in this disclosure. In some examples, presence- sensitive displays 388, 392 may include additional functionality. In any case, for example, presence-sensitive display 392 may receive data from computing device 310 and display graphical content. In some examples, presence-sensitive display 392 may determine one or more user inputs (e.g., continuous gestures, multi-touch gestures, single-touch gestures) at the projection screen using capacitive, inductive, and/or optical recognition techniques and transmit indications of such user inputs to computing device 310 using one or more communication units.
As described above, in some examples, computing device 310 may output graphical content for display at PSD 312, which PSD 312 is coupled to computing device 310 by a system bus or other suitable communication channel. Computing device 310 may also output graphical content for display at one or more remote devices, such as mobile device 386 and visual display component 390. For example, computing device 310 may execute one or more instructions to generate and/or modify graphical content in accordance with the techniques of this disclosure. Computing device 310 may output data that includes the graphical content to a communication unit of computing device 310, such as communication unit 342. Communication unit 342 can transmit the data to one or more remote devices, such as mobile device 386 and/or visual display component 390. In this manner, the computing device 310 may output graphical content for display at one or more remote devices. In some examples, one or more remote devices may output graphical content at a presence-sensitive display included in and/or operatively coupled to the respective remote device.
In some examples, computing device 310 may not output graphical content at PSD 312 operatively coupled to computing device 310. In other examples, computing device 310 may output graphical content for display at both PSD 312 coupled to computing device 310 over communication channel 362A and at one or more remote devices. In such an example, the graphical content may be displayed at each respective device substantially simultaneously. In some examples, the graphical content generated and output by computing device 310 for display at PSD 312 may be different from the graphical content output for display at one or more remote devices.
In some examples, computing device 310 may be operatively coupled to one or more remote devices included in fig. 3 using direct device communication 378. Direct device communication 378 may include communications by which computing device 310 sends and receives data directly with a remote device using wired or wireless communications. That is, in some examples of direct device communication 378, data sent by computing device 310 may not be forwarded by one or more additional devices before being received at a remote device, and vice versa. Examples of direct device communication 378 may include bluetooth, near field communication, universal serial bus, WiFi, infrared, and so forth. One or more of the remote devices illustrated in FIG. 3 may be operatively coupled to computing device 310 by communication links 376A-376C. In some examples, communication links 376A-376C may be connections using bluetooth, near field communication, universal serial bus, infrared, and the like. Such connections may be wireless and/or wired connections.
In accordance with the techniques of this disclosure, computing device 310 may be operatively coupled to visual display component 390 using external network 374. Computing device 310 may output a graphical keyboard for display at PSD 392. For example, computing device 310 may send data comprising a representation of the graphical keyboard to communication unit 342. The communication unit 342 can transmit data comprising a representation of the graphical keyboard to the visual display component 390 using the external network 374. In response to receiving the data using external network 374, visual display component 390 can cause PSD 392 to output the graphical keyboard. In response to receiving a user input at PSD 392 selecting one or more keys of the keyboard, visual display device 130 may send an indication of the user input to computing device 310 using external network 374. The communication unit 342 may receive the indication of the user input and transmit the indication to the computing device 310.
4A-4E are conceptual diagrams illustrating an example graphical user interface of an example computing device configured to present a graphical keyboard with integrated search features according to one or more aspects of the present disclosure. 4A-4E illustrate example graphical user interfaces 414A-414E, respectively (collectively user interfaces 414). However, many other examples of graphical user interfaces may be used in other instances. Each of the graphical user interfaces 414 may correspond to a graphical user interface displayed by the computing device 110 or 210 of fig. 1 and 2, respectively. Fig. 4A-4E are described below in the context of computing device 110.
Graphical user interface 414 includes an output area 416A, an editing area 416C, and a graphical keyboard 416B. Graphical keyboard 416B includes a suggestion region 418B, a plurality of keys 418A, and a search element 418C.
As shown in fig. 4A, computing device 110 may receive an electronic communication (e.g., a text message) from a device associated with a friend. The computing device 110 may output the content of the electronic communication for display within the output region 416A. The content of the message may include the phrase "Wanna see a movie or do so positioning tonight? (do you want to see a movie or do it this evening?
The user of computing device 110 may interact with graphical keyboard 416B to compose a reply to the message from the friend. As shown in fig. 4B by the semi-transparent circles (which may or may not be displayed by the computing device 110) overlaid on the individual graphical keys 418A, the user may tap or gesture at one or more of the keys 418A to enter the reply. Keyboard module 122 of computing device 110 may receive an indication of a tap or gesture at key 418A and determine, based on the user input, text that computing device 110 lays out (formats) and displays within edit region 416C. For example, when the user types "sure," the computing device 110 may cause the edit field 416C to display "sure. Further, when the user types a key at graphical key 418A, keyboard module 122 of computing device 110 may predict one or more candidate words based on the user input and display one or more of the candidate words within suggestion region 418B.
As shown in fig. 4C, keyboard module 122 of computing device 110 may receive an additional indication of a tap or gesture at key 418A and proceed to determine text that computing device 110 lays out and displays within edit region 416C based on the user input. For example, when the user types "playin" after typing "sure wings," the keyboard module 122 of the computing device 110 may cause the edit region 416C to display "playin". Further, when the user types at graphical key 418A, keyboard module 122 of computing device 110 may predict one or more candidate words based on the user input and display one or more of the candidate words within suggestion region 418B. For example, as shown in fig. 4C, computing device 110 may present "playin", "playing", and "playtime" in suggestion region 418B as suggested words picked from the dictionary. In response to detecting selection of the suggested word in suggestion area 418B, and further after detecting selection of the "send" key in graphical key 418A, computing device 110 may compose and send an electronic message including the text "sure hat's playing" to a computing device associated with the friend. As shown in fig. 4D, the computing device 110 may output the content of the electronic message for display in the output region 416A.
After the user is finished typing text associated with the electronic communication, keyboard module 122 of computing device 110 may automatically infer that the user is finished typing and, in response, predict one or more search queries that may interest the user of computing device 110 in composing additional text associated with the electronic conversation that the user is conducting with the friend. For example, keyboard module 122 of computing device 110 may determine the search query in response to determining the end of text associated with the electronic communication by at least determining the last key in the selection of one or more keys 418A corresponding to the punctuation key associated with the punctuation character. In other words, in response to determining that the last character in the text is a punctuation symbol (e.g., ". In other examples, keyboard module 122 of computing device 110 may determine the search query in response to determining that text associated with the electronic communication ended by at least determining a last key in the selection of one or more keys 418A that corresponds to a send key in a graphical keyboard that sends the electronic communication. In other words, in response to determining that the last key selected by the user is a "send key" or a "return" key that, when selected, triggers the chat application of computing device 110 to send a message, keyboard module 122 may predict a search query associated with the text.
In response to determining the end of the text or otherwise inferring that the user has finished composing at least a portion of the content of the electronic communication, keyboard module 122 may enter a search mode and predict a search query that may be relevant to the text. In the example of fig. 4D, the keyboard module 122 may analyze the text "show's playing" and determine that the phrase "show's playing" is the trigger phrase used to refer to a movie, particularly a movie being shown in a movie theater. Whenever keyboard module 122 identifies the trigger phrase "what's playing," keyboard module 122 may automatically associate the reformulation phrase "movies playing near me" to the text. Further, to obtain that the user refers to "movies" rather than "plays" or other products, keyboard module 122 may analyze the content of the text within the current context of computing device 110. That is, the current context of computing device 110 includes previously received messages that include the text "movies" therein. In response to determining that the previous message references "movies," keyboard module 122 may increase the score associated with the reformulation phrase "movies playing near me" and generate a query to perform a search for "movies playing near me" to identify individual movies playing near the current location of computing device 110.
In some examples, computing device 110 may modify the visual format of the search key from the plurality of keys to indicate that the computing device generated the search query. For example, as shown in fig. 4D, search element 418C has changed from a first visual format in which search element 418C has a first color tray to a second visual format in which search element 418C is displayed with a second color tray. In other examples, the computing device 110 may cause the search element 418C to flash, move, change size, or be altered in some other manner that alerts the user that the computing device 110 automatically generated the search query based on text input at the graphical keyboard 416B.
In some examples, the computing device 110 may indicate to the computing device that a search query was generated by outputting text of the generated query. For example, as shown in fig. 4D, the computing device 110 may output text of the automatically generated search query as suggestions within the suggestion region 418B of the graphical keyboard 416B. As shown in fig. 4D, the search query, when displayed within suggestion region 418B, is displayed in and among linguistic, candidate words or phrases (e.g., non-search related suggestions).
In some examples, computing device 110 may display the text of the search query within a separate search area of a graphical keyboard. The search area may be different than suggestion area 418B of graphical keyboard 416B that displays suggested words for text entry. For example, the search area may be located between the graphical key 418A and the suggestion area 418C or the search area may be located between the edit area 416C or the output area 416A and the suggestion area 418B. In some examples, computing device 110 may even replace suggestion region 418B with a search region.
After displaying the suggested query, computing device 110 may receive an indication of a selection of the graphical indication, and in response to receiving the indication of the selection of the graphical indication, computing device 110 may perform a search for information based on the search query. For example, as shown in fig. 4D, keyboard module 122 may receive an indication of a user input detected at a location within suggestion area 418B where the search query "movies playing near me" is displayed. In response to receiving an indication of user input, keyboard module 122 may perform a search based on the selected query.
After performing the search for information, computing device 110 may output, for display, a graphical indication of one or more search results obtained by the search by replacing at least a portion of plurality of keys 418A of graphical keyboard 416B. For example, as shown in fig. 4E, the keyboard module 122 may obtain search results based on the selected query and organize the search results into respective search cards 418D, the keyboard module 122 causing the computing device 110 to display the search cards 418D as overlays on top of the graphical keys 418A. In the example of fig. 4D, search card 418D is presented in a carousel format such that the user can swipe left or right to move to a subsequent search result or can drag a search result from the carousel into edit area 416C or output area 416A. In some examples, the user may further present a search box in addition to search card 418D so that the user can perform subsequent searches. In this manner, keyboard module 122 conveniently presents automatically generated search queries within graphical keyboard 416B, which upon selection causes computing device 110 to present search results directly within graphical keyboard 416B.
FIG. 5 is a flow diagram illustrating example operations of a computing device configured to present a graphical keyboard with integrated search features in accordance with one or more aspects of the present disclosure. The operations of fig. 5 may be performed by one or more processors of a computing device, such as computing device 110 of fig. 1 or computing device 210 of fig. 2. For purposes of illustration only, fig. 5 is described below in the context of computing device 110 of fig. 1.
In operation, computing device 110 may output a graphical keyboard for display (500). For example, a chat application executing at computing device 110 may invoke keyboard module 122 (e.g., a separate application or function of computing device 110 that is separate from the chat application) to present graphical keyboard 116B at PSD 112.
Based on the indication of the selection of the key from the graphical keyboard, the computing device 110 may determine text of the electronic communication (502). For example, keyboard module 122 may receive information about touch inputs detected at the location of PSD 112 that displays graphical keys 118A of graphical keyboard 116B. Based on this information, keyboard module 122 may determine text (e.g., "what's with the president's hair.
Computing device 110 may identify a searchable entity based at least in part on the text (504). For example, the global model of keyboard module 122 may infer from the portion of the "President" in the text input that the user is talking about the American President (e.g., "Barak Obama").
Based on the searchable entity, the computing device 110 may generate a search query (506). For example, keyboard module 122 may generate a search query for obtaining news about the american president as a keyword search for "Barak Obama news.
Computing device 110 may output, for display within the graphical keyboard, a graphical indication indicating that computing device 110 generated the search query (508). For example, the keyboard module 122 may cause the computing device 110 to present the text of the generated query within the graphical keyboard 116B (e.g., as suggestions within the suggestion region 118B).
FIG. 6 is a flow diagram illustrating example operations of a computing device configured to present a graphical keyboard with integrated search features in accordance with one or more aspects of the present disclosure. The operations of fig. 6 may be performed by one or more processors of a computing device, such as computing device 110 of fig. 1 or computing device 210 of fig. 2. For purposes of illustration only, FIG. 6 is described below in the context of computing device 210 of FIG. 2.
In operation, a first application executing at computing device 210 may invoke a keyboard application configured to provide a graphical keyboard comprising a plurality of keys for receiving text input (600). For example, application module 224 may be a chat application that provides instant messaging or a chat application user interface at PSD212 for exchanging electronic messages between users of computing device 210 and users of other computing devices in an electronic conversation. As part of the chat application user interface, application module 224 may invoke (call on) keyboard module 222 to provide graphical keyboard 116B. Application module 224 may send a command to keyboard module 222 to invoke keyboard module 222 (e.g., as an application or service executing at computing device 210 that is different from application module 224).
Once invoked, the keyboard application may determine text input based on an indication of a selection of one or more keys from the plurality of keys (602). For example, once invoked, keyboard module 222 may receive an indication of a user input at a location at PSD212 where one or more keys 118A are displayed and infer text from the user input.
The keyboard application may identify a searchable entity based at least in part on the text input (604). For example, keyboard module 222 may rely on local and/or global models 232 and machine learning model 233 to identify one or more searchable entities from the text input, as well as to identify searchable entities from previous text inputs detected by keyboard module 222.
The keyboard application may generate a search query based on the searchable entity (606). For example, keyboard module 222 may compose or package one or more of the searchable entities identified from the text input into a search query format used by a search engine executing at or remote from computing device 210 for obtaining information about the search entity.
The keyboard application may output, for display within the graphical keyboard, a graphical indication indicating that the keyboard application generated the search query (608). For example, after generating the search query, keyboard module 222 may cause PSD212 to present the query (e.g., as text) at suggestion area 118B, or in place of or in addition to other word suggestions.
The keyboard application may receive an indication of a selection of a search query (610). For example, keyboard module 222 may receive information from UI module 220 indicating that a user of computing device 220 made a selection (e.g., by gesture) at or near the location of PSD212 where the search query is being displayed, and may infer from this information that the user selected the query.
In response to receiving the indication of the selection, the keyboard application may initiate a search at the search application based on the search query (612). For example, based on the search query, keyboard module 222 may invoke a separate search application to perform a local information search for information stored at computing device 220 or a remote information search for information stored remotely from computing device 220 (e.g., by invoking a search application executing at a remote computing device such as a server).
The search application may perform the search based on the search query (614). For example, a search application invoked by keyboard module 222 may execute the search as an on-device search for search results associated with information stored locally at computing device 210 based on the search query. The search application invoked by keyboard module 222 may perform the search as a remote search for search results associated with information stored remotely from computing device 210 based on the search query. In any case, the chat application may receive, from the search application via the keyboard application, one or more search results obtained from the execution of the search.
After the keyboard application initiates the search, the chat application may output, for display in place of at least a portion of the graphical keyboard, a graphical indication of one or more search results obtained by the keyboard application (616). For example, application module 224 may package search results obtained from the search into a carousel of search cards, and application module 224 may cause keyboard module 122 to present some or all of the graphical keys 118A covered by the carousel in front of, or in place of, PSD 212.
The chat application may receive an indication of a selection of a search result (618). For example, application module 224 may receive information from UI module 220 instructing a user of computing device 210 to swipe or otherwise gesture at or near the location of one of the search results being displayed within graphical keyboard 116B at PSD 212.
In response to receiving an indication of a selection of at least one of the one or more search results, the chat application may output the search result as a message of an electronic conversation (620). For example, in response to detecting a user input selecting a search result, application module 224 may format information associated with the search result into a message and output the formatted information as part or all of a message that application module 224 outputs to a different computing device as part of an electronic conversation between a user of computing device 210 and a user of the different computing device.
The following numbered clauses may illustrate one or more aspects of the present disclosure:
clause 1. a method comprises: outputting, by a keyboard application executing at a computing device and for display, a graphical keyboard comprising a plurality of keys; determining, by the keyboard application, text of an electronic communication based on an indication of a selection of one or more keys from the plurality of keys; identifying, by the keyboard application, a searchable entity based at least in part on the text; generating, by the keyboard application, a search query based on the searchable entity; and outputting, by the keyboard application for display within the graphical keyboard, a graphical indication indicating that the keyboard application generated the search query.
The method of clause 1, wherein the search query is determined in response to determining the end of the text, the method further comprising: determining, by the keyboard application, the end of the text in response to determining that a last key in the selection of one or more keys corresponds to a punctuation key associated with a punctuation character.
The method of any of clauses 1-2, wherein the search query is determined in response to determining an end of the text, the method further comprising: determining, by the keyboard application, the end of the text in response to determining that a last key in the selection of one or more keys corresponds to a send key in the graphical keyboard used to send the electronic communication.
Clause 4. the method of any of clauses 1-3, wherein outputting the graphical indication comprises: modifying a visual format of a search key of the plurality of keys to indicate that the computing device generated the search query.
Clause 6. the method of any of clauses 1-5, wherein the graphical indication comprises text of the search query, and outputting the graphical indication comprises outputting the text of the search query as a suggested query for searching for display within a search area of a graphical keyboard, the search area being different from a suggestion area of the graphical keyboard that displays suggested words for text entry.
The method of any of clauses 1-7, wherein identifying the searchable entity comprises: determining, by the keyboard application, one or more words based at least on a portion of the text; determining, by the keyboard application, a score assigned to a searchable entity based on the one or more words according to a local model of the searchable entity that is associated with a current location of the computing device, the score indicating a probability that the searchable entity is relevant to the one or more words; identifying, by the keyboard application, the searchable entity in response to determining that the score assigned to the searchable entity satisfies a threshold.
The method of any of clauses 1-9, wherein identifying the searchable entity comprises: determining, by the keyboard application, one or more words based at least on a portion of the text; determining, by the keyboard application, based on the one or more words and in accordance with a global model of a searchable entity, a score assigned to the searchable entity that indicates a probability that the searchable entity is related to the one or more words, the global model associated with a geographic area that includes respective locations associated with two or more models of searchable entities; and identifying, by the computing device, the searchable entity in response to determining that the score assigned to the searchable entity satisfies a threshold.
The method of any of clauses 1-10, wherein identifying the searchable entity comprises: determining, by the keyboard application, one or more words based at least on a portion of the text; determining, by the keyboard application, a score assigned to a searchable entity based on the one or more words and according to at least one of a global model of the searchable entity or a local model of the searchable entity, the global model associated with a geographic area that includes respective locations associated with two different local models of searchable entities that include the local models of searchable entities, the score indicating a probability that the searchable entity is relevant to the one or more words; modifying, by the keyboard application, the score assigned to the searchable entity based on contextual information and according to a machine learning model; and identifying, by the keyboard application, the searchable entity after modifying the score assigned to the searchable entity and in response to determining that the score assigned to the searchable entity satisfies a threshold.
Clause 12. the method of clause 11, wherein the contextual information comprises information associated with an electronic conversation, the electronic conversation comprising the electronic communication and one or more other electronic communications that have been sent or received by the computing device.
The method of any of clauses 1-12, wherein generating the search query comprises: determining, by the keyboard application, a reformulation phrase associated with the searchable entity based at least in part on the searchable entity; and generating, by the keyboard application, the search query from the reformulation phrase, wherein the searchable entity includes a trigger phrase, and wherein the reformulation phrase associated with the searchable entity is further determined based at least in part on a current context of the computing device.
Clause 14. a mobile device includes: a presence-sensitive display component; at least one processor; and a memory storing instructions associated with a keyboard application that, when executed, cause the at least one processor to: outputting, for display at the presence-sensitive display component, a graphical keyboard comprising a plurality of keys; determining text of an electronic communication based on an indication of a selection of one or more keys from the plurality of keys detected at the presence-sensitive display component; identifying a trigger phrase based at least in part on the text; generating a search query based on the trigger phrase; and outputting, for display at the presence-sensitive display component and within the graphical keyboard, a graphical indication indicating that the computing device generated the search query.
Clause 15. a method comprises: invoking, by a first application executing at a computing device, a keyboard application configured to provide a graphical keyboard comprising a plurality of keys for receiving text input; determining, by the keyboard application, a text input based on an indication of a selection of one or more keys from the plurality of keys; identifying, by the keyboard application, a searchable entity based at least in part on the text input; generating, by the keyboard application, a search query based on the searchable entity; outputting, by the keyboard application for display within the graphical keyboard, a graphical indication indicating that the keyboard application generated the search query; and in response to receiving, by the keyboard application, an indication of a selection of the search query, initiating, by the keyboard application, a search at a search application based on the search query.
The method of clause 15, wherein the search application is executing at the computing device, the method further comprising: performing, by the search application, the search as an on-device search for search results associated with information stored locally at the computing device based on the search query.
The method of any of clauses 15-16, wherein the search application is executing at a remote computing device, the method further comprising: performing, by the search application, the search as a remote search for search results associated with information stored remotely from the computing device based on the search query.
Clause 18. the method of any of clauses 15-17, wherein the first application is a chat application and the text input is associated with an electronic conversation within the chat application.
Clause 19. the method of clause 18, further comprising: after the keyboard application initiates a search, outputting, by the first application, for display in place of at least a portion of the plurality of keys, a graphical indication of one or more search results obtained by the keyboard application.
Clause 20. the method of clause 19, further comprising: in response to receiving, by the first application, an indication of a selection of at least one of the one or more search results, outputting, by the chat application, the at least one of the one or more search results as a message of the electronic conversation.
Clause 21. a system comprising means for performing any one of the methods of clauses 1-13.
Clause 22. a computing device comprising means for performing any of the methods of clauses 1-13.
Clause 23. a computer-readable storage medium comprising instructions that, when executed by at least one processor of a computing device, cause the at least one processor to perform any of the methods of clauses 1-13.
In one or more examples, the functions described may be implemented in hardware, software, firmware, or any combination thereof. If implemented in software, the functions may be stored on or transmitted over as one or more instructions or code on a computer-readable medium and executed by a hardware-based processing unit. The computer readable medium may include: a computer-readable storage medium corresponding to a tangible medium such as a data storage medium; or communication media including any medium that facilitates transfer of a computer program from one place to another, such as according to a communication protocol. In this manner, the computer-readable medium may generally correspond to: (1) a non-transitory tangible computer-readable storage medium; or (2) a communication medium such as a signal or carrier wave. A data storage medium may be any available medium that can be accessed by one or more computers or one or more processors to retrieve instructions, code and/or data structures for use in implementing the techniques described in this disclosure. The computer program product may include a computer-readable medium.
By way of example, and not limitation, such computer-readable storage media can comprise RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage or other magnetic storage devices, flash memory, or any other medium that can be used to store desired program code in the form of instructions or data structures and that can be accessed by a computer. Also, any connection is properly termed a computer-readable medium. For example, if the instructions are transmitted from a website, server, or other remote source using a cable, fiber optic cable, twisted pair, Digital Subscriber Line (DSL), or wireless technologies such as infrared, radio, and microwave, then the coaxial cable, fiber optic cable, twisted pair, DSL, or wireless technologies such as infrared, radio, and microwave are included in the definition of medium. It should be understood, however, that computer-readable storage media and data storage media do not include connections, carrier waves, signals, or other transitory media, but rather refer to non-transitory tangible storage media. Disk or disc, including Compact Disc (CD), laser disc, optical disc, Digital Versatile Disc (DVD), floppy disk and blu-ray disc, are used where disks usually reproduce data magnetically, while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer-readable media.
The instructions may be executed by one or more processors, such as one or more Digital Signal Processors (DSPs), general purpose microprocessors, Application Specific Integrated Circuits (ASICs), field programmable logic arrays (FPGAs), or other equivalent integrated or discrete logic circuitry. Accordingly, the term "processor" as used may refer to any of the foregoing structure or any other structure suitable for implementing the described techniques. Further, in certain aspects, the described functionality may be provided within dedicated hardware and/or software modules. Furthermore, the techniques may be fully implemented in one or more circuits or logic elements.
The techniques of this disclosure may be implemented in a wide variety of devices or apparatuses, including a radio handset, an Integrated Circuit (IC), or a collection of ICs (e.g., a chipset). Various components, modules, or units are described in this disclosure to emphasize functional aspects of devices configured to perform the disclosed techniques, without necessarily being implemented by different hardware units. Rather, as noted above, the various units may be combined in hardware units or provided by a collection of cooperating hardware units, including one or more processors as described above, in conjunction with suitable software and/or firmware.
Various examples have been described. These and other examples are within the scope of the following claims.
Claims (15)
1. A method, comprising:
outputting, by a keyboard application executing at a computing device and for display, a graphical keyboard comprising a plurality of keys and a suggestion area in which a suggested word for text entry is displayed, wherein the keyboard application is a standalone application or subroutine called by a different application executing at the computing device when the different application requires graphical keyboard input functionality;
determining, by the keyboard application, text entered using the graphical keyboard based on the indication of the selection of the one or more keys from the plurality of keys;
identifying, by the keyboard application based at least in part on the text, a searchable entity that includes one or more words of text that can be used as a basis for a search entered using the graphical keyboard;
generating, by the keyboard application, a search query based on the searchable entity, wherein the search query is based at least in part on contextual information included in content of a previous message, and wherein the search query is different from the text that was entered;
outputting, by the keyboard application, for display within a search area of the graphical keyboard that replaces the suggested area of the graphical keyboard, a graphical indication indicating that the keyboard application generated the search query, wherein the graphical indication includes text of the search query as the suggested query for searching, and wherein the search area of the graphical keyboard is positioned within the graphical keyboard between at least some character keys of the plurality of keys and a text input area of a graphical user interface from a particular one of the different applications that invokes the keyboard application to output the graphical keyboard for display; and
initiating, by the keyboard application, a search based on the search query as at least one of: an on-device search performed by a search application for search results associated with information stored locally at the computing device; or a remote search performed by a search application for search results associated with information stored remotely from the computing device.
2. The method of claim 1, wherein the search query is determined in response to determining an end of the text, the method further comprising:
determining, by the keyboard application, the end of the text in response to determining that a last key in the selection of one or more keys corresponds to a punctuation key associated with a punctuation character.
3. The method of claim 1, wherein the search query is determined in response to determining an end of the text, the method further comprising:
determining, by the keyboard application, the end of the text in response to determining that a last key in the selection of one or more keys corresponds to a send key in the graphical keyboard used to send the text-based electronic communication.
4. The method of claim 1, wherein outputting the graphical indication further comprises: modifying a visual format of a search key of the plurality of keys to further indicate that the computing device generated the search query.
5. The method of claim 1, wherein identifying the searchable entity comprises:
determining, by the keyboard application, the one or more words of the text entered using the graphical keyboard based at least on a portion of the text;
determining, by the keyboard application, a score assigned to a searchable entity based on the one or more words according to a local model of the searchable entity that is associated with a current location of the computing device, the score indicating a probability that the searchable entity is relevant to the one or more words;
identifying, by the keyboard application, the searchable entity in response to determining that the score assigned to the searchable entity satisfies a threshold.
6. The method of claim 5, wherein the local model is a first local model, the method further comprising:
in response to determining a change in the current location of the computing device from a first location to a second location:
obtaining, by the keyboard application, a second model of a searchable entity from a remote computing system, the second model associated with the second location; and
replacing, by the keyboard application, the first local model with the second model.
7. The method of claim 1, wherein identifying the searchable entity comprises:
determining, by the keyboard application, the one or more words of the text entered using the graphical keyboard based at least on a portion of the text;
determining, by the keyboard application, based on the one or more words and in accordance with a global model of a searchable entity, a score assigned to the searchable entity that indicates a probability that the searchable entity is relevant to the one or more words, the global model associated with a geographic area that includes respective locations associated with two or more models of searchable entities; and
identifying, by the computing device, the searchable entity in response to determining that the score assigned to the searchable entity satisfies a threshold.
8. The method of claim 1, wherein identifying the searchable entity comprises:
determining, by the keyboard application, the one or more words of the text entered using the graphical keyboard based at least on a portion of the text;
determining, by the keyboard application, a score assigned to a searchable entity based on the one or more words and according to at least one of a global model of the searchable entity or a local model of the searchable entity, the global model associated with a geographic area that includes respective locations associated with two different local models of searchable entities that include the local models of searchable entities, the score indicating a probability that the searchable entity is relevant to the one or more words;
modifying, by the keyboard application, the score assigned to the searchable entity based on contextual information and according to a machine learning model; and
identifying, by the keyboard application, the searchable entity after modifying the score assigned to the searchable entity and in response to determining that the score assigned to the searchable entity satisfies a threshold.
9. The method of claim 8, wherein the contextual information comprises information associated with an electronic conversation, the electronic conversation comprising an electronic communication based on the text and one or more other electronic communications that have been sent or received by the computing device.
10. The method of claim 1, wherein generating the search query comprises:
determining, by the keyboard application, a reformulation phrase associated with the searchable entity based at least in part on the searchable entity; and
generating, by the keyboard application, the search query from the reformulation phrase, wherein the searchable entity includes a trigger phrase, and wherein the reformulation phrase associated with the searchable entity is further determined based at least in part on a current context of the computing device.
11. A method, comprising:
invoking, by a first application executing at a computing device, a keyboard application configured to provide a graphical keyboard comprising a plurality of keys and a suggestion region, wherein a suggested word for text entry is displayed in the suggestion region, for receiving text input, wherein the keyboard application is a standalone application or subroutine invoked by the first application and other applications executing at the computing device when the first application and the other applications require graphical keyboard input functionality;
determining, by the keyboard application, a text input based on an indication of a selection of one or more keys from the plurality of keys;
identifying, by the keyboard application based at least in part on the text input, a searchable entity that includes one or more words of text that can be used as a basis for a search;
generating, by the keyboard application, a search query based on the searchable entity, wherein the search query is based at least in part on contextual information included in content of a previous message, and wherein the search query is different from the text that was entered;
outputting, by the keyboard application, for display within a search area of the graphical keyboard that replaces the suggested area of the graphical keyboard, a graphical indication indicating that the keyboard application generated the search query, wherein the graphical indication includes text of the search query as the suggested query for searching, and wherein the search area of the graphical keyboard is positioned within the graphical keyboard between at least some of the plurality of keys and a text input area of a graphical user interface of the first application that invokes the keyboard application to output the graphical keyboard for display;
in response to receiving, by the keyboard application, an indication of a selection of the search query, initiating, by the keyboard application, a search based on the search query at a search application different from the first application and the keyboard application; and
performing, by the search application based on the search query, the search as at least one of: an on-device search for search results associated with information stored locally at the computing device; or a remote search for search results associated with information stored remotely from the computing device.
12. The method of claim 11, wherein the first application is a chat application and the text input is associated with an electronic conversation within the chat application.
13. The method of claim 11, further comprising:
after the keyboard application initiates a search, outputting, by the first application, for display in place of at least a portion of the plurality of keys, a graphical indication of one or more search results obtained by the keyboard application.
14. The method of claim 13, further comprising:
in response to receiving, by the first application, an indication of a selection of at least one of the one or more search results, outputting, by the first application, the at least one of the one or more search results as a message of an electronic conversation.
15. A mobile device, comprising:
a presence-sensitive display component;
at least one processor; and
a memory storing instructions associated with a keyboard application, the instructions when executed cause the at least one processor to:
outputting, for display at the presence-sensitive display component, a graphical keyboard comprising a plurality of keys and a suggestion area in which a suggested word for text entry is displayed, wherein the keyboard application is a standalone application or subroutine called by a different application executing at the mobile device when the different application requires graphical keyboard input functionality;
determining text entered using the graphical keyboard based on the detected indication at the presence-sensitive display component to select one or more keys from the plurality of keys;
identifying, based at least in part on the text, a trigger phrase comprising one or more words of text that can be used as a basis for a search entered using the graphical keyboard;
generating a search query based on the trigger phrase, wherein the search query is based at least in part on contextual information included in content of a previous message, and wherein the search query is different from the text that was entered;
outputting, for display at the presence-sensitive display component and within a search area of the graphical keyboard that replaces the suggested area of the graphical keyboard, a graphical indication indicating that the keyboard application generated the search query, wherein the graphical indication includes text of the search query as the suggested query for searching, and wherein the search area of the graphical keyboard is positioned within the graphical keyboard between at least some character keys of the plurality of keys and a text input area of a graphical user interface from a particular one of the different applications that invokes the keyboard application to output the graphical keyboard for display; and
initiating, by the keyboard application, a search based on the search query as at least one of: an on-device search performed by a search application executing at the at least one processor for search results associated with information stored locally at the mobile device; or a remote search performed by the search application for search results associated with information stored remotely from the mobile device.
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/133,291 | 2016-04-20 | ||
US15/133,291 US10305828B2 (en) | 2016-04-20 | 2016-04-20 | Search query predictions by a keyboard |
Publications (2)
Publication Number | Publication Date |
---|---|
CN107305585A CN107305585A (en) | 2017-10-31 |
CN107305585B true CN107305585B (en) | 2021-05-14 |
Family
ID=57882144
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201611273011.2A Active CN107305585B (en) | 2016-04-20 | 2016-12-30 | Search query prediction by keyboard |
Country Status (5)
Country | Link |
---|---|
US (2) | US10305828B2 (en) |
CN (1) | CN107305585B (en) |
DE (2) | DE202016008310U1 (en) |
GB (1) | GB2549568A (en) |
WO (1) | WO2017184218A1 (en) |
Families Citing this family (44)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10976922B2 (en) * | 2013-02-17 | 2021-04-13 | Benjamin Firooz Ghassabian | Data entry systems |
US9043196B1 (en) | 2014-07-07 | 2015-05-26 | Machine Zone, Inc. | Systems and methods for identifying and suggesting emoticons |
US10489768B2 (en) * | 2015-12-30 | 2019-11-26 | Visa International Service Association | Keyboard application with third party engagement selectable items |
WO2020219476A1 (en) * | 2019-04-21 | 2020-10-29 | Hyperkey Inc. | Advanced keyboard based search |
US11494547B2 (en) * | 2016-04-13 | 2022-11-08 | Microsoft Technology Licensing, Llc | Inputting images to electronic devices |
US10222957B2 (en) | 2016-04-20 | 2019-03-05 | Google Llc | Keyboard with a suggested search query region |
US10078673B2 (en) | 2016-04-20 | 2018-09-18 | Google Llc | Determining graphical elements associated with text |
US10140017B2 (en) | 2016-04-20 | 2018-11-27 | Google Llc | Graphical keyboard application with integrated search |
US9965530B2 (en) | 2016-04-20 | 2018-05-08 | Google Llc | Graphical keyboard with integrated search features |
US10305828B2 (en) | 2016-04-20 | 2019-05-28 | Google Llc | Search query predictions by a keyboard |
US10831283B2 (en) * | 2016-06-02 | 2020-11-10 | Samsung Electronics Co., Ltd. | Method and electronic device for predicting a response from context with a language model |
US20170357521A1 (en) * | 2016-06-13 | 2017-12-14 | Microsoft Technology Licensing, Llc | Virtual keyboard with intent-based, dynamically generated task icons |
US10409488B2 (en) * | 2016-06-13 | 2019-09-10 | Microsoft Technology Licensing, Llc | Intelligent virtual keyboards |
US10664157B2 (en) | 2016-08-03 | 2020-05-26 | Google Llc | Image search query predictions by a keyboard |
CA2939395A1 (en) * | 2016-08-15 | 2016-10-17 | Richard S. Brown | Method and device for invoking a search from a text message |
US11115463B2 (en) * | 2016-08-17 | 2021-09-07 | Microsoft Technology Licensing, Llc | Remote and local predictions |
US10409487B2 (en) * | 2016-08-23 | 2019-09-10 | Microsoft Technology Licensing, Llc | Application processing based on gesture input |
WO2018057627A1 (en) * | 2016-09-20 | 2018-03-29 | Google Llc | System and method for transmitting a response in a messaging application |
WO2018101671A1 (en) * | 2016-11-29 | 2018-06-07 | Samsung Electronics Co., Ltd. | Apparatus and method for providing sentence based on user input |
US20180210872A1 (en) * | 2017-01-23 | 2018-07-26 | Microsoft Technology Licensing, Llc | Input System Having a Communication Model |
US20180267615A1 (en) * | 2017-03-20 | 2018-09-20 | Daqri, Llc | Gesture-based graphical keyboard for computing devices |
US10572107B1 (en) * | 2017-06-23 | 2020-02-25 | Amazon Technologies, Inc. | Voice communication targeting user interface |
US10788900B1 (en) * | 2017-06-29 | 2020-09-29 | Snap Inc. | Pictorial symbol prediction |
US10339922B2 (en) * | 2017-08-23 | 2019-07-02 | Sap Se | Thematic segmentation of long content using deep learning and contextual cues |
US20190087466A1 (en) * | 2017-09-21 | 2019-03-21 | Mz Ip Holdings, Llc | System and method for utilizing memory efficient data structures for emoji suggestions |
CN107918497A (en) * | 2017-11-14 | 2018-04-17 | 厦门攸信信息技术有限公司 | The Forecasting Methodology and system of a kind of controller |
US11886473B2 (en) | 2018-04-20 | 2024-01-30 | Meta Platforms, Inc. | Intent identification for agent matching by assistant systems |
US11715042B1 (en) | 2018-04-20 | 2023-08-01 | Meta Platforms Technologies, Llc | Interpretability of deep reinforcement learning models in assistant systems |
US10782986B2 (en) | 2018-04-20 | 2020-09-22 | Facebook, Inc. | Assisting users with personalized and contextual communication content |
US11307880B2 (en) | 2018-04-20 | 2022-04-19 | Meta Platforms, Inc. | Assisting users with personalized and contextual communication content |
US11676220B2 (en) | 2018-04-20 | 2023-06-13 | Meta Platforms, Inc. | Processing multimodal user input for assistant systems |
US11210728B2 (en) | 2018-05-25 | 2021-12-28 | Walmart Apollo, Llc | Systems and methods for searching retail products and locations using a universal search bar |
JP7263712B2 (en) * | 2018-08-23 | 2023-04-25 | 富士通株式会社 | Control method, information processing device and control program |
CN112445892B (en) * | 2019-09-02 | 2023-09-29 | 百度在线网络技术（北京）有限公司 | Method, device, electronic equipment and storage medium for determining brand mention rate |
US11593569B2 (en) * | 2019-10-11 | 2023-02-28 | Lenovo (Singapore) Pte. Ltd. | Enhanced input for text analytics |
US11636438B1 (en) | 2019-10-18 | 2023-04-25 | Meta Platforms Technologies, Llc | Generating smart reminders by assistant systems |
US11567788B1 (en) | 2019-10-18 | 2023-01-31 | Meta Platforms, Inc. | Generating proactive reminders for assistant systems |
CN111241398B (en) * | 2020-01-10 | 2023-07-25 | 百度在线网络技术（北京）有限公司 | Data prefetching method, device, electronic equipment and computer readable storage medium |
US11853381B2 (en) * | 2020-11-13 | 2023-12-26 | Google Llc | Hybrid fetching using a on-device cache |
US11563706B2 (en) | 2020-12-29 | 2023-01-24 | Meta Platforms, Inc. | Generating context-aware rendering of media contents for assistant systems |
US11809480B1 (en) | 2020-12-31 | 2023-11-07 | Meta Platforms, Inc. | Generating dynamic knowledge graph of media contents for assistant systems |
WO2022169992A1 (en) | 2021-02-04 | 2022-08-11 | Keys Inc | Intelligent keyboard |
US11861315B2 (en) | 2021-04-21 | 2024-01-02 | Meta Platforms, Inc. | Continuous learning for natural-language understanding models for assistant systems |
US11921692B1 (en) * | 2022-09-16 | 2024-03-05 | Capital One Services, Llc | Computer-based systems configured for automatically updating a database based on an initiation of a dynamic machine-learning verification and methods of use thereof |
Family Cites Families (80)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6104397A (en) | 1997-06-30 | 2000-08-15 | Sun Microsystems, Inc. | Method and system for generating improved progress indicators |
US7599847B2 (en) | 2000-06-09 | 2009-10-06 | Airport America | Automated internet based interactive travel planning and management system |
US20090006543A1 (en) | 2001-08-20 | 2009-01-01 | Masterobjects | System and method for asynchronous retrieval of information based on incremental user input |
US8028250B2 (en) | 2004-08-31 | 2011-09-27 | Microsoft Corporation | User interface having a carousel view for representing structured data |
US7788248B2 (en) * | 2005-03-08 | 2010-08-31 | Apple Inc. | Immediate search feedback |
US7590699B2 (en) | 2005-06-23 | 2009-09-15 | Microsoft Corporation | Instant messaging with built-in search |
US7747639B2 (en) * | 2005-08-24 | 2010-06-29 | Yahoo! Inc. | Alternative search query prediction |
US7676517B2 (en) | 2005-10-14 | 2010-03-09 | Microsoft Corporation | Search results injected into client applications |
US7925716B2 (en) | 2005-12-05 | 2011-04-12 | Yahoo! Inc. | Facilitating retrieval of information within a messaging environment |
US20070300177A1 (en) | 2006-06-23 | 2007-12-27 | Microsoft Corporation | User interface for specifying multi-valued properties |
WO2008011454A2 (en) | 2006-07-18 | 2008-01-24 | Chacha Search, Inc. | Anonymous search system using human searchers |
US8564544B2 (en) | 2006-09-06 | 2013-10-22 | Apple Inc. | Touch screen device, method, and graphical user interface for customizing display of content category icons |
US8005822B2 (en) * | 2007-01-17 | 2011-08-23 | Google Inc. | Location in search queries |
US20080201434A1 (en) | 2007-02-16 | 2008-08-21 | Microsoft Corporation | Context-Sensitive Searches and Functionality for Instant Messaging Applications |
US20080244446A1 (en) | 2007-03-29 | 2008-10-02 | Lefevre John | Disambiguation of icons and other media in text-based applications |
US8745168B1 (en) | 2008-07-10 | 2014-06-03 | Google Inc. | Buffering user interaction data |
CN101876878A (en) * | 2009-04-29 | 2010-11-03 | 深圳富泰宏精密工业有限公司 | Word prediction input system and method |
US10705692B2 (en) | 2009-05-21 | 2020-07-07 | Sony Interactive Entertainment Inc. | Continuous and dynamic scene decomposition for user interface |
US9116615B2 (en) * | 2009-10-13 | 2015-08-25 | Blackberry Limited | User interface for a touchscreen display |
US20110112824A1 (en) | 2009-11-06 | 2011-05-12 | Craig Peter Sayers | Determining at least one category path for identifying input text |
US20110191321A1 (en) | 2010-02-01 | 2011-08-04 | Microsoft Corporation | Contextual display advertisements for a webpage |
US9129012B2 (en) | 2010-02-03 | 2015-09-08 | Google Inc. | Information search system with real-time feedback |
US8650210B1 (en) | 2010-02-09 | 2014-02-11 | Google Inc. | Identifying non-search actions based on a search query |
US8782556B2 (en) * | 2010-02-12 | 2014-07-15 | Microsoft Corporation | User-centric soft keyboard predictive technologies |
US8621379B2 (en) | 2010-03-12 | 2013-12-31 | Apple Inc. | Device, method, and graphical user interface for creating and using duplicate virtual keys |
US9483175B2 (en) | 2010-07-26 | 2016-11-01 | Apple Inc. | Device, method, and graphical user interface for navigating through a hierarchy |
US8918734B2 (en) | 2010-07-28 | 2014-12-23 | Nuance Communications, Inc. | Reduced keyboard with prediction solutions when input is a partial sliding trajectory |
AU2011291544B2 (en) | 2010-08-19 | 2015-03-26 | Google Llc | Predictive query completion and predictive search results |
CN103348353B (en) | 2010-10-06 | 2016-07-06 | 西里克斯系统公司 | Resource access is carried out intermediary's adjustment by the physical location based on mobile equipment |
US10346479B2 (en) | 2010-11-16 | 2019-07-09 | Microsoft Technology Licensing, Llc | Facilitating interaction with system level search user interface |
US8515984B2 (en) | 2010-11-16 | 2013-08-20 | Microsoft Corporation | Extensible search term suggestion engine |
EP2641145A4 (en) | 2010-11-20 | 2017-05-03 | Nuance Communications, Inc. | Systems and methods for using entered text to access and process contextual information |
US9111238B2 (en) | 2010-12-17 | 2015-08-18 | Microsoft Technology Licensing, Llc | Data feed having customizable analytic and visual behavior |
US9104992B2 (en) | 2010-12-17 | 2015-08-11 | Microsoft Technology Licensing, Llc | Business application publication |
US10642934B2 (en) | 2011-03-31 | 2020-05-05 | Microsoft Technology Licensing, Llc | Augmented conversational understanding architecture |
US20120256840A1 (en) | 2011-04-10 | 2012-10-11 | Mahmoud Razzaghi | Virtual keyboard |
US9176944B1 (en) * | 2011-08-23 | 2015-11-03 | Google Inc. | Selectively processing user input |
CN102955658B (en) | 2011-08-25 | 2017-03-22 | 腾讯科技（深圳）有限公司 | Device and method for page switching in interaction interface |
WO2013054957A1 (en) | 2011-10-13 | 2013-04-18 | Lg Electronics Inc. | Input interface controlling apparatus and method thereof |
US9652448B2 (en) | 2011-11-10 | 2017-05-16 | Blackberry Limited | Methods and systems for removing or replacing on-keyboard prediction candidates |
US8914451B2 (en) | 2012-02-17 | 2014-12-16 | Blackberry Limited | Electronic device configured with messaging composition interface |
US9310888B2 (en) | 2012-03-16 | 2016-04-12 | Microsoft Technology Licensing, Llc | Multimodal layout and rendering |
US9685160B2 (en) | 2012-04-16 | 2017-06-20 | Htc Corporation | Method for offering suggestion during conversation, electronic device using the same, and non-transitory storage medium |
US20130285916A1 (en) | 2012-04-30 | 2013-10-31 | Research In Motion Limited | Touchscreen keyboard providing word predictions at locations in association with candidate letters |
US9460237B2 (en) | 2012-05-08 | 2016-10-04 | 24/7 Customer, Inc. | Predictive 411 |
US8484573B1 (en) | 2012-05-23 | 2013-07-09 | Google Inc. | Predictive virtual keyboard |
US9582146B2 (en) * | 2012-05-29 | 2017-02-28 | Nokia Technologies Oy | Causing display of search results |
US9116552B2 (en) | 2012-06-27 | 2015-08-25 | Blackberry Limited | Touchscreen keyboard providing selection of word predictions in partitions of the touchscreen keyboard |
US20140115070A1 (en) | 2012-10-22 | 2014-04-24 | Nokia Corporation | Apparatus and associated methods |
US9305114B2 (en) | 2012-12-17 | 2016-04-05 | Microsoft Technology Licensing, Llc | Building long search queries |
US10228819B2 (en) | 2013-02-04 | 2019-03-12 | 602531 British Cilumbia Ltd. | Method, system, and apparatus for executing an action related to user selection |
WO2014139120A1 (en) | 2013-03-14 | 2014-09-18 | Microsoft Corporation | Search intent preview, disambiguation, and refinement |
US20140282203A1 (en) | 2013-03-15 | 2014-09-18 | Research In Motion Limited | System and method for predictive text input |
US9529856B2 (en) | 2013-06-03 | 2016-12-27 | Google Inc. | Query suggestion templates |
US9449079B2 (en) | 2013-06-28 | 2016-09-20 | Yandex Europe Ag | Method of and system for displaying a plurality of user-selectable refinements to a search query |
US20150100537A1 (en) | 2013-10-03 | 2015-04-09 | Microsoft Corporation | Emoji for Text Predictions |
US9461945B2 (en) * | 2013-10-18 | 2016-10-04 | Jeffrey P. Phillips | Automated messaging response |
KR102157264B1 (en) | 2013-10-30 | 2020-09-17 | 삼성전자주식회사 | Display apparatus and UI providing method thereof |
KR20150081181A (en) | 2014-01-03 | 2015-07-13 | 삼성전자주식회사 | Display apparatus and Method for providing recommendation characters thereof |
KR102225031B1 (en) | 2014-01-14 | 2021-03-09 | 엘지전자 주식회사 | Terminal and operating method thereof |
US10050926B2 (en) | 2014-02-05 | 2018-08-14 | Facebook, Inc. | Ideograms based on sentiment analysis |
US20150242086A1 (en) | 2014-02-21 | 2015-08-27 | Markport Limited | Drag and drop event system and method |
US9471570B2 (en) | 2014-04-30 | 2016-10-18 | Excalibur Ip, Llc | Method and system for user selection of query suggestions |
KR20150126213A (en) | 2014-05-02 | 2015-11-11 | 삼성전자주식회사 | System and method for searching infomation |
KR102177607B1 (en) | 2014-05-16 | 2020-11-11 | 엘지전자 주식회사 | Mobile terminal and method for controlling the same |
US9671956B2 (en) | 2014-06-18 | 2017-06-06 | Lenovo Enterprise Solutions (Singapore) Pte. Ltd. | Presenting search term suggestions on graphical user interfaces |
US9043196B1 (en) | 2014-07-07 | 2015-05-26 | Machine Zone, Inc. | Systems and methods for identifying and suggesting emoticons |
US9930167B2 (en) | 2014-07-07 | 2018-03-27 | Verizon Patent And Licensing Inc. | Messaging application with in-application search functionality |
US20160034977A1 (en) | 2014-08-01 | 2016-02-04 | Yahoo! Inc. | System and method for embedded search within messaging applications |
US10824654B2 (en) | 2014-09-18 | 2020-11-03 | Snap Inc. | Geolocation-based pictographs |
US10275152B2 (en) | 2014-10-28 | 2019-04-30 | Idelan, Inc. | Advanced methods and systems for text input error correction |
US20160224524A1 (en) * | 2015-02-03 | 2016-08-04 | Nuance Communications, Inc. | User generated short phrases for auto-filling, automatically collected during normal text use |
CN104765791A (en) * | 2015-03-24 | 2015-07-08 | 北京搜狗科技发展有限公司 | Information inputting method and device |
US10547571B2 (en) | 2015-05-06 | 2020-01-28 | Kakao Corp. | Message service providing method for message service linked to search service and message server and user terminal to perform the method |
US10496275B2 (en) | 2015-10-12 | 2019-12-03 | Microsoft Technology Licensing, Llc | Multi-window keyboard |
US10305828B2 (en) | 2016-04-20 | 2019-05-28 | Google Llc | Search query predictions by a keyboard |
US10078673B2 (en) | 2016-04-20 | 2018-09-18 | Google Llc | Determining graphical elements associated with text |
US10222957B2 (en) | 2016-04-20 | 2019-03-05 | Google Llc | Keyboard with a suggested search query region |
US9965530B2 (en) | 2016-04-20 | 2018-05-08 | Google Llc | Graphical keyboard with integrated search features |
US10140017B2 (en) | 2016-04-20 | 2018-11-27 | Google Llc | Graphical keyboard application with integrated search |
-
2016
- 2016-04-20 US US15/133,291 patent/US10305828B2/en active Active
- 2016-10-10 US US15/289,661 patent/US9720955B1/en active Active
- 2016-12-23 GB GB1622164.0A patent/GB2549568A/en not_active Withdrawn
- 2016-12-28 DE DE202016008310.5U patent/DE202016008310U1/en active Active
- 2016-12-28 DE DE102016125760.5A patent/DE102016125760A1/en active Pending
- 2016-12-29 WO PCT/US2016/069159 patent/WO2017184218A1/en active Application Filing
- 2016-12-30 CN CN201611273011.2A patent/CN107305585B/en active Active
Also Published As
Publication number | Publication date |
---|---|
DE102016125760A1 (en) | 2017-10-26 |
WO2017184218A1 (en) | 2017-10-26 |
DE202016008310U1 (en) | 2017-07-19 |
US10305828B2 (en) | 2019-05-28 |
US20170310616A1 (en) | 2017-10-26 |
GB2549568A (en) | 2017-10-25 |
GB201622164D0 (en) | 2017-02-08 |
CN107305585A (en) | 2017-10-31 |
US9720955B1 (en) | 2017-08-01 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN107305585B (en) | Search query prediction by keyboard | |
EP3479213B1 (en) | Image search query predictions by a keyboard | |
EP3408733B1 (en) | Keyboard with a suggested search query region | |
CN108700951B (en) | Iconic symbol search within a graphical keyboard | |
EP3400539B1 (en) | Determining graphical elements associated with text | |
US10140017B2 (en) | Graphical keyboard application with integrated search | |
US20180173692A1 (en) | Iconographic symbol predictions for a conversation | |
US20170308290A1 (en) | Iconographic suggestions within a keyboard | |
US9946773B2 (en) | Graphical keyboard with integrated search features | |
KR102204888B1 (en) | Automatic translation by keyboard |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
CB02 | Change of applicant information |
Address after: American CaliforniaApplicant after: Google limited liability companyAddress before: American CaliforniaApplicant before: Google Inc. |
|
CB02 | Change of applicant information | ||
GR01 | Patent grant | ||
GR01 | Patent grant |