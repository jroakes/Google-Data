CN115104305A - Multi-context entropy coding for graph compression - Google Patents
Multi-context entropy coding for graph compression Download PDFInfo
- Publication number
- CN115104305A CN115104305A CN202080096330.9A CN202080096330A CN115104305A CN 115104305 A CN115104305 A CN 115104305A CN 202080096330 A CN202080096330 A CN 202080096330A CN 115104305 A CN115104305 A CN 115104305A
- Authority
- CN
- China
- Prior art keywords
- data
- graph
- entropy encoder
- compressing
- context entropy
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- H—ELECTRICITY
- H03—ELECTRONIC CIRCUITRY
- H03M—CODING; DECODING; CODE CONVERSION IN GENERAL
- H03M7/00—Conversion of a code where information is represented by a given sequence or number of digits to a code where the same, similar or subset of information is represented by a different sequence or number of digits
- H03M7/30—Compression; Expansion; Suppression of unnecessary data, e.g. redundancy reduction
- H03M7/40—Conversion to or from variable length codes, e.g. Shannon-Fano code, Huffman code, Morse code
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/901—Indexing; Data structures therefor; Storage structures
- G06F16/9024—Graphs; Linked lists
-
- H—ELECTRICITY
- H03—ELECTRONIC CIRCUITRY
- H03M—CODING; DECODING; CODE CONVERSION IN GENERAL
- H03M7/00—Conversion of a code where information is represented by a given sequence or number of digits to a code where the same, similar or subset of information is represented by a different sequence or number of digits
- H03M7/30—Compression; Expansion; Suppression of unnecessary data, e.g. redundancy reduction
- H03M7/3068—Precoding preceding compression, e.g. Burrows-Wheeler transformation
- H03M7/3079—Context modeling
-
- H—ELECTRICITY
- H03—ELECTRONIC CIRCUITRY
- H03M—CODING; DECODING; CODE CONVERSION IN GENERAL
- H03M7/00—Conversion of a code where information is represented by a given sequence or number of digits to a code where the same, similar or subset of information is represented by a different sequence or number of digits
- H03M7/30—Compression; Expansion; Suppression of unnecessary data, e.g. redundancy reduction
- H03M7/40—Conversion to or from variable length codes, e.g. Shannon-Fano code, Huffman code, Morse code
- H03M7/4006—Conversion to or from arithmetic code
- H03M7/4012—Binary arithmetic codes
- H03M7/4018—Context adapative binary arithmetic codes [CABAC]
-
- H—ELECTRICITY
- H03—ELECTRONIC CIRCUITRY
- H03M—CODING; DECODING; CODE CONVERSION IN GENERAL
- H03M7/00—Conversion of a code where information is represented by a given sequence or number of digits to a code where the same, similar or subset of information is represented by a different sequence or number of digits
- H03M7/30—Compression; Expansion; Suppression of unnecessary data, e.g. redundancy reduction
- H03M7/40—Conversion to or from variable length codes, e.g. Shannon-Fano code, Huffman code, Morse code
- H03M7/4031—Fixed length to variable length coding
- H03M7/4037—Prefix coding
- H03M7/4043—Adaptive prefix coding
-
- H—ELECTRICITY
- H03—ELECTRONIC CIRCUITRY
- H03M—CODING; DECODING; CODE CONVERSION IN GENERAL
- H03M7/00—Conversion of a code where information is represented by a given sequence or number of digits to a code where the same, similar or subset of information is represented by a different sequence or number of digits
- H03M7/30—Compression; Expansion; Suppression of unnecessary data, e.g. redundancy reduction
- H03M7/60—General implementation details not specific to a particular type of compression
- H03M7/6005—Decoder aspects
-
- H—ELECTRICITY
- H03—ELECTRONIC CIRCUITRY
- H03M—CODING; DECODING; CODE CONVERSION IN GENERAL
- H03M7/00—Conversion of a code where information is represented by a given sequence or number of digits to a code where the same, similar or subset of information is represented by a different sequence or number of digits
- H03M7/30—Compression; Expansion; Suppression of unnecessary data, e.g. redundancy reduction
- H03M7/60—General implementation details not specific to a particular type of compression
- H03M7/6064—Selection of Compressor
- H03M7/6082—Selection strategies
- H03M7/6094—Selection strategies according to reasons other than compression rate or data type
Abstract
Example embodiments relate to encoding a adjacency list using a multi-context entropy encoder. The system may obtain a graph (or graphs) with data and may compress the data of the graph using a multi-context entropy encoder. The multi-context entropy encoder may encode a contiguous list within the data such that each integer is assigned to a different probability distribution. For example, operating the multi-context entropy encoder may involve using a combination of arithmetic coding, huffman coding, and ANS. The assignment of integers to probability distributions may depend on the role of each integer and/or the previous value of a similar kind. By using multi-context entropy coding, a computing system may increase compression rates while maintaining similar processing speeds.
Description
Cross Reference to Related Applications
This application claims priority to U.S. provisional patent application No. 62/975,722, filed on 12.2.2020, which is incorporated herein by reference in its entirety.
Background
Data compression techniques are used to encode digital data into an alternative compressed form having fewer bits than the original data, and then decode (i.e., decompress) the compressed form when the original data is needed. The compression rate of a particular data compression system is the ratio of the size of the encoded output data (during storage or transmission) to the size of the original data. Data compression techniques are increasingly used as the amount of data that is obtained, transmitted and stored in digital form in many different fields increases significantly. These techniques may help reduce the resources required to store and transmit data.
In general, data compression techniques can be classified as lossless or lossy. Lossless compression reduces bits by identifying and eliminating statistical redundancy. No information is lost in lossless compression. Lossy compression involves reducing bits by removing unnecessary or less important information.
Disclosure of Invention
Example embodiments presented herein relate to systems and methods for compressing data, such as graph (graph) data, using multi-context entropy coding.
In a first example embodiment, a method is provided. The method involves obtaining, at a computing system, a graph having data and compressing, by the computing system, the data of the graph using a multi-context entropy encoder. A multi-context entropy encoder encodes a contiguous list within the data such that each integer is assigned to a different probability distribution.
In a second example embodiment, a system is provided. The system includes a computing system, a non-transitory computer readable medium, and program instructions stored on the non-transitory computer readable medium that are executable by the computing system to perform operations. The operations include obtaining a graph with data and compressing the data of the graph using a multi-context entropy encoder. A multi-context entropy encoder encodes a contiguous list within the data such that each integer is assigned to a different probability distribution.
In a third example embodiment, a non-transitory computer-readable medium configured to store instructions is provided. The program instructions may be stored in a data storage device and, when executed by a computing system, may cause the computing system to perform operations according to the first and second example embodiments.
In a fourth example embodiment, a system may comprise various means for performing each of the operations of the example embodiments described above.
These and other embodiments, aspects, advantages, and alternatives will become apparent to one of ordinary skill in the art by reading the following detailed description, with reference where appropriate to the accompanying drawings. Further, it is to be understood that this summary and other descriptions and drawings provided herein are intended to illustrate embodiments by way of example only and that, accordingly, many variations are possible. For example, structural elements and process steps may be rearranged, combined, distributed, eliminated, or otherwise altered while remaining within the scope of the claimed embodiments.
Drawings
Fig. 1 is a block diagram of a computing system in accordance with one or more example embodiments.
Fig. 2 depicts a cloud-based server cluster in accordance with one or more example embodiments.
Fig. 3 depicts an asymmetric digital system implementation in accordance with one or more example embodiments.
Fig. 4 depicts a huffman coding implementation in accordance with one or more example embodiments.
Fig. 5 shows a flow diagram of a method in accordance with one or more example embodiments.
Fig. 6 shows a schematic diagram of a computer program according to an example embodiment.
Detailed Description
Example methods, devices, and systems are described herein. It should be understood that the words "example" and "exemplary" are used herein to mean "serving as an example, instance, or illustration. Any embodiment or feature described herein as "exemplary" or "exemplary" is not necessarily to be construed as preferred or advantageous over other embodiments or features. Other embodiments may be utilized, and other changes may be made, without departing from the scope of the subject matter presented herein.
Accordingly, the example embodiments described herein are not meant to be limiting. The aspects of the present disclosure generally described herein and illustrated in the figures can be arranged, substituted, combined, separated, and designed in a wide variety of different configurations, all of which are contemplated herein. Furthermore, the features shown in each figure may be used in combination with each other, unless the context indicates otherwise. Thus, the drawings are generally to be regarded as forming an integral aspect of one or more embodiments, but it is to be understood that not all illustrated features are required for each embodiment.
1. Overview
Graphs processed by modern computing systems are of increasingly larger size, often growing faster than the resources available to process them. This may require implementing a compression scheme that allows access to the data without decompressing the full graph.
The current implementation of such a structure compresses the graph by storing an adjacency list (adjacency list) using the other lists as references. The edge may be copied from the reference or encoded using a universal integer code. While this scheme may achieve useful compression rates, it does not adapt well to changes in the source data.
Example embodiments may relate to encoding a adjacency list using multi-context entropy coding. Multi-context entropy coding may involve the use of a variety of compression modes (schemas), such as arithmetic coding, huffman coding, or asymmetric digital systems (ANS). For example, the system may use a combination of huffman coding and ANS. Huffman coding may be used to create a file that supports a neighborhood of access to any node, while ANS may be used to create a file that can only be decoded in its entirety. Further, the system may enable a symbol to be encoded to be partitioned into multiple contexts. For each context, the system may use a different probability distribution, which may allow more accurate encoding when the symbols are assumed to belong to different probability distributions.
In some embodiments, the system may use multi-context entropy coding such that each integer is assigned to a different (stored) probability distribution according to its role. For example, multi-context entropy coding may enable the length of a block to be copied from a reference list rather than skipped. Multi-context entropy coding may also involve assigning each integer to a different probability distribution based on previous values of a similar kind. For example, a different probability distribution may be selected for a given increment based on the magnitude of the previous increment (delta). Using multi-context entropy coding may enable the system to achieve compression rate improvements over the prior art while also having similar processing speeds. Further examples are described herein.
2. Example System
Fig. 1 is a simplified block diagram illustrating a computing system 100, showing some components that may be included in a computing device arranged to operate in accordance with embodiments herein. Computing system 100 may be a client device (e.g., a device actively operated by a user), a server device (e.g., a device that provides computing services to client devices), or some other type of computing platform. Some server devices may operate from time to time as client devices to perform certain operations, and some client devices may incorporate server features.
In this example, computing system 100 includes a processor 102, a memory 104, a network interface 106, and an input/output unit 108, all of which may be coupled via a system bus 110 or similar mechanism. In some embodiments, computing system 100 may include other components and/or peripherals (e.g., removable storage, printers, etc.).
The processor 102 may be one or more of any type of computer processing element, such as in the form of a Central Processing Unit (CPU), a coprocessor (e.g., a math, graphics, or cryptographic coprocessor), a Digital Signal Processor (DSP), a network processor, and/or an integrated circuit or controller that performs the processor operations. In some cases, processor 102 may be one or more single-core processors. In other cases, processor 102 may be one or more multi-core processors having multiple independent processing units. The processor 102 may also include register memory for temporarily storing instructions and related data being executed, and cache memory for temporarily storing recently used instructions and data.
The memory 104 may be any form of computer usable memory including, but not limited to, Random Access Memory (RAM), Read Only Memory (ROM), and non-volatile memory. This may include flash memory, hard drives, solid state drives, compact discs rewritable (CDs), digital video discs rewritable (DVDs), and/or tape storage, to name a few examples.
The memory 104 may store program instructions and/or data upon which the program instructions may operate. For example, the memory 104 may store these program instructions on a non-transitory computer-readable medium such that the instructions are executable by the processor 102 to perform any of the methods, processes, or operations disclosed in this specification or the figures.
As shown in fig. 1, memory 104 may include firmware 104A, kernel 104B, and/or application 104C. Firmware 104A may be program code used to boot or otherwise boot some or all of computing system 100. The kernel 104B may be an operating system including modules for memory management, scheduling and management of processes, input/output, and communications. The kernel 104B may also include device drivers that allow the operating system to communicate with hardware modules (e.g., memory units, network interfaces, ports, and buses) of the computing system 100. The application 104C may be one or more user space software programs, such as a web browser or email client, and any software libraries used by these programs. In some examples, the application 104C may include one or more neural network applications. Memory 104 may also store data used by these and other programs and applications.
The network interface 106 may take the form of one or more wired interfaces, such as an ethernet (e.g., fast ethernet, gigabit ethernet, etc.). The network interface 106 may also support communication over one or more non-ethernet media, such as coaxial cable or power line, or over a wide area medium, such as Synchronous Optical Network (SONET) or Digital Subscriber Line (DSL) technology. The network interface 106 may additionally take the form of one or more wireless interfaces, such as IEEE 802.11(Wifi),network interface 106. Further, the network interface 106 may include a plurality of physical interfaces. For example, some embodiments of computing system 100 may include an Ethernet network,
Input/output unit 108 may facilitate user and peripheral device interaction with computing system 100 and/or other computing systems. Input/output unit 108 may include one or more types of input devices, such as a keyboard, a mouse, one or more touch screens, sensors, biometric sensors, and so forth. Similarly, input/output unit 108 may include one or more types of output devices, such as a screen, a monitor, a printer, and/or one or more Light Emitting Diodes (LEDs). Additionally or alternatively, the computing system 100 may communicate with other devices using, for example, a Universal Serial Bus (USB) or high-definition multimedia interface (HDMI) port interface.
The encoder 112 and decoder 114 may be in communication with other components of the computing system 100, such as the memory 104. Further, the encoder 112 and decoder 114 may represent software and/or hardware in some embodiments.
In some embodiments, one or more instances of computing system 100 may be deployed to support a cluster architecture. The exact physical location, connectivity, and configuration of these computing devices may be unknown and/or unimportant to the client device. Thus, the computing device may be referred to as a "cloud-based" device, which may be located at various remote data center locations. Further, the computing system 100 may implement the performance of the embodiments described herein, including using neural networks and implementing neural light transmission.
Fig. 2 depicts a cloud-based server cluster 200, according to an example embodiment. In fig. 2, one or more operations of a computing device (e.g., computing system 100) may be distributed among server device 202, data storage 204, and router 206, all of which may be connected by local cluster network 208. The number of server devices 202, data storage 204, and routers 206 in a server cluster 200 may depend on the computing task(s) and/or application(s) assigned to the server cluster 200. In some examples, server cluster 200 may perform one or more operations described herein, including the use of neural networks and the implementation of neural optical transmission functions.
The data storage 204 may be a data storage array comprising a drive array controller configured to manage read and write access to hard disk drives and/or groups of solid state drives. The drive array controller, alone or in combination with the server devices 202, may also be configured to manage backup or redundant copies of data stored in the data storage 204 to prevent drive failures or other types of failures that prevent one or more of the server devices 202 from accessing the cells of the cluster data storage 204. Other types of memory besides drives may be used.
The router 206 may comprise a network device configured to provide internal and external communication for the server cluster 200. For example, the router 206 may include one or more packet switching and/or routing devices (including switches and/or gateways) configured to provide (i) network communications between the server device 202 and the data storage 204 via the cluster network 208 and/or (ii) network communications between the server cluster 200 and other devices via the communication link 210 to the network 212.
Further, the configuration of the cluster router 206 may be based at least in part on the data communication requirements of the server devices 202 and the data storage 204, latency and throughput of the local cluster network 208, latency, throughput and cost of the communication link 210, and/or other factors that may contribute to the cost, speed, fault tolerance, resiliency, efficiency, and/or other design goals of the system architecture.
As one possible example, the data store 204 may include any form of database, such as a Structured Query Language (SQL) database. Various types of data structures may store information in such databases, including, but not limited to, tables, arrays, lists, trees, and tuples. Further, any of the databases in the data store 204 may be monolithic or distributed across multiple physical devices.
3. Entropy coding
Entropy coding is a type of lossless coding that compresses digital data by representing frequently occurring patterns (patterns) with a small number of bits and rarely occurring patterns with a large number of bits. Thus, the entropy coding technique may be a lossless data compression scheme that does not depend on the specific characteristics of the medium.
The process of Entropy Coding (EC) can be divided into modeling and encoding. Modeling may involve assigning probabilities to symbols, and encoding may involve generating bit sequences from these probabilities. As established in shannon's source coding theorem, there is a relationship between the probability of a symbol and its corresponding bit sequence. For example, a symbol with probability p is assigned a bit sequence of length-log (p). To achieve a good compression rate, probabilistic estimates may be used. In particular, modeling may be a critical task in data compression, since the model is responsible for the probability of each symbol.
One entropy encoding technique may involve creating and assigning a unique prefix-free code for each unique symbol present in the input. These entropy encoders may then compress the data by replacing each fixed length input symbol with a corresponding variable length prefix-free output codeword. The length of each codeword is approximately proportional to the negative logarithm of the probability. In some examples, the optimal code length for a symbol is-log b P, where b is the number of symbols used to form the output code and P is the probability of an input symbol.
Entropy coding can be achieved by different coding schemes. A common scheme that uses a discrete number of bits per symbol is huffman coding. A different approach is arithmetic coding, which can output a sequence of bits representing points within an interval (interval). The interval may be constructed recursively by the probability of the symbol being encoded.
Another compression scheme is the Asymmetric digital System (ANS). The ANS is a lossless compression algorithm or pattern that inputs a list of symbols from some finite set and outputs one or more finite numbers. Each symbol s has a fixed known probability p of appearing in the list s . The ANS pattern attempts to assign a unique integer to each list so that more likely lists get smaller integers. The computing system 100 may use an ANS that may combine the compression rate of arithmetic encoding with a processing cost similar to that of huffman encoding.
FIG. 3 depicts asymmetric numbers according to one or more example embodimentsA system implementation. ANS 300 may involve encoding information as a single natural number x, which may be interpreted as containing log 2 (x) A single bit of information. Adding information from the sign of the probability p adds information content toequation 302 as follows:
x′＝x/p. [1]
as shown in fig. 3, system 300 may add information in the least significant position using equation 302 by a coding rule that specifies "x to the x-th occurrence of a subset of natural numbers corresponding to a currently encoded symbol. "in the example shown in FIG. 3, graph 304 shows that the sequence (01111) is encoded as a natural number 18 that is less than 47 would be obtained using a standard binary system. The system 300 may implement a smaller natural number 18 due to better correspondence with the frequency of the sequence to be encoded. In this way, the system 300 may allow information to be stored in a single natural number, rather than two numbers in a limited range, as shown by the X sub-graph 306 further illustrated in FIG. 3.
Fig. 4 depicts a huffman coding implementation in accordance with one or more example embodiments. As discussed above, huffman coding may be used with integer length codes and may be depicted via a huffman tree. The system 400 can use huffman coding to construct the minimum redundancy code. In this way, the system 400 may use huffman coding to perform data compression that minimizes the cost, time, bandwidth, and storage space used to transfer data from one place to another.
In the embodiment illustrated in fig. 4, system 400 shows a graph 402 that includes nodes arranged according to values and corresponding frequencies. System 400 may be configured to search graph 402 for two nodes that have the lowest frequency and have not yet been assigned to a parent node. The two nodes may be coupled together to a new internal node and the frequency may be added by the system 400 to assign a total to the new internal node. The system 400 may repeat the process of searching for the next two nodes with the lowest frequency that have not been assigned to a parent node until all nodes are combined together in the root node.
The system 400 may initially arrange all values in ascending order of frequency according to huffman coding techniques. For example, the values may be rearranged in the following order: "E, A, C, F, D, B". After reordering, the system 400 may then insert the first two values with the smallest frequencies (i.e., E and A) as the first part of the Huffman tree 404. As shown, the frequencies of E:4 and A:5 are added, as shown in Huffman tree 404, for a total frequency of 9 (i.e., EA: 9).
Next, system 400 can involve combining nodes with subsequent minimum frequencies, which correspond to C:7 and EA: 9. Adding these together creates CEA:16, as shown in Huffman tree 404. The system 400 may then create a sub-tree of the next two nodes (which are F:12 and D:15) with the smallest frequencies. This results in FD:27 as shown. The system 400 may then combine the next two minimal nodes corresponding to CEA:16 and B:25 to produce CEAB: 41. Finally, the system 400 may combine the subtrees FD:27 and CEAB:41 together to create a value FDCEAB of frequency 68, as shown in total by the Huffman tree 404 represented in FIG. 4.
Although both huffman coding and ANS provide compression benefits, there are certain situations where a computing system may benefit from using a combination during data compression. In particular, the computing system 100 may encode the adjacency list using multi-context entropy coding. Multi-context entropy coding may involve the use of multiple modes, such as arithmetic coding, huffman coding, or ANS. For example, the computing system 100 may use huffman coding in creating a file that supports access to the neighborhood of any node, and may use an ANS in creating a file that can only be decoded in its entirety. In both cases, the symbol to be encoded may be partitioned into multiple contexts. For each context, a different probability distribution may be used, which may allow more accurate coding when the symbols are assumed to belong to different probability distributions.
The computing system 100 may use multi-context entropy coding such that each integer is assigned to a different (stored) probability distribution according to its role. For example, multi-context entropy coding may enable the length of a block to be copied from a reference list instead of being skipped. Multi-context entropy coding may also involve assigning each integer to a different probability distribution based on previous values of a similar kind. For example, a different probability distribution may be selected for a given increment based on the magnitude of the previous increment. Using multi-context entropy coding may enable computing system 100 to achieve compression rate improvements over the prior art while also having similar processing speeds.
In some cases, the computing system 100 may use variants of the ANS during multi-context entropy encoding. Variations of the ANS may be based on frequently used variations in a particular format, such as JPEG XL. Unlike other variations, this choice may allow the memory usage of each context to be proportional to the maximum number of symbols that can be encoded by the stream, which may require memory proportional to the quantization probability size of each distribution. As a result, the techniques may achieve better cache locality when decoding is performed by the computing system 100.
One potential drawback of ANS and other coding schemes that may use a non-integer number of bits per coded symbol (e.g., arithmetic coding) may be that the system using the ANS may need to maintain internal states when access to a single contiguous list is involved. In order for decoding to be able to recover successfully from a given position in the bitstream, it may also be necessary to be able to recover the state of the entropy encoder at that point in the bitstream, which may result in a huge per-node overhead. Thus, when random access to the adjacency list is required, the computing system 100 may switch to using huffman coding instead of ANS. Thus, the ability to switch between modes when using multi-context entropy coding may help the computing system 100 avoid the drawbacks associated with separate modes.
Both huffman coding and ANS may utilize reduced alphabet sizes. When computing system 100 is performing tasks that involve encoding integers of arbitrary length, it may not be feasible to use different symbols for each integer due to the resources required. As a result, the system 100 may choose to use mixed integer coding, which may be defined by two parameters h and k. In particular, in defining these two parameters, k can be greater than or equal to h, and h can be greater than or equal to 1(k ≧ h ≧ 1).
In some embodiments, computing system 100 may compare 0,2 k ) Each integer within the range is stored directly as a symbol. Furthermore, any other integer can then be stored by encoding the index of the highest bit (x) into the symbol and h-1 subsequent bits (b) in a base-2 representation of the number, and then by storing all remaining bits directly in the bitstream without using any entropy coding. Thus, the resulting symbols can be represented as follows:
2 k +(x-k-1)·2 h-1 +b [2]
4. Example graph compression method
The computing system 100 may perform graph compression using multi-context coding code. This format can achieve a desired compression rate by taking the following representation of the adjacency list of node n. Hereinafter, the window size (W) and the minimum interval length (L) may be used as global parameters. Each list may start with a degree of n. If the degree is strictly positive, it may be followed by a reference number r, which may be a number in [1, W). This may indicate that the list is represented by a contiguous list of reference nodes n-r (referred to as a reference list, or 0, meaning that the list is represented without reference to any other list).
Furthermore, if r is strictly positive, it may be followed by a list of integers indicating that the reference list should be split to obtain the indices of consecutive blocks. Blocks in even positions may represent edges that should be copied to the current list. The format contains in this order the number of blocks, the length of the first block and the length of all subsequent blocks minus 1 (since no block except the first block may be empty). The last block may not be stored because its length can be inferred from the length of the reference list. A list of intervals may follow, where each interval is represented by a pair s, l. This may mean that there should be edges towards all nodes in the interval s, L + L).
In addition, a list of residuals may be encoded. For example, the list of residuals may be coded with an implicit length, since their length may be inferred by degrees, the number of copied edges, and the number of edges represented by the interval. The list may represent all edges that are not encoded using other schemes and may also be incrementally encoded. In particular, a first residual may be encoded as a delta with respect to the current node, and all subsequent residuals may be represented as a delta minus 1 with respect to the previous residuals.
In some cases, the representation of the first residual may result in a negative number. To address this problem, the computing system 100 may encode the first residual as follows:
this is a bijective that is easy to invert between integer and natural numbers. To enable fast access to a single adjacency list, this mode may limit the length of the reference chain for each node. In particular, the reference chain may be a sequence of nodes (e.g., n) 1 ,....,n r ) So that node n i+1 Using node n i For reference, where r denotes the length of the reference chain. The pattern may enable each reference chain to have a length of at most R, where R is a global parameter.
The pattern may represent the resulting sequence of non-negative integers using a zeta code, such as a set of generic codes that are particularly suited to represent integers that follow a power law distribution.
In some embodiments, the computing system 100 may use the above-described schema with one or more modifications. As previously indicated herein, the computing system 100 may use entropy encoding to represent non-negative integers.
In an embodiment, the computing system 100 may use a pattern having degrees represented via delta coding. Incremental encoding may be used because the representation of the node degree may take a large number of bits in the final compressed file. Since this may produce a negative number, the delta may be represented using the transformation of equation 1 shown above.
Incremental encoding across multiple adjacency lists may be disadvantageous to enable access to any adjacency list without first decoding the rest of the graph. In view of this potential problem, the schema may split the graph into chunks (chunks) when access to a single list is requested. Each chunk may have a fixed length "C". As a result, incremental encoding of degrees may then be performed within a single chunk.
To illustrate an example, consider the case where the adjacency list contains nodes 2, 3, 4, 6, 7 and edges 3, 4, and 6 have been represented by block copies. Then, the residuals will be 2 and 7, and the second residual will be represented as follows: 7-2-1 ═ 4. However, in this example, reading a 0, 1, or 3 from the compressed file may result in an edge value of 3, 4, or 6, which would be redundant. Thus, the computing system 100 may modify the delta encoding of the residual by removing edges known to exist from the length of the gap. In this case, the residual edge 7 will be denoted as 2.
Also, as a simplified form, the representation of the intervals may be removed and replaced with run-length encoding with zero gaps. This change is made possible by the entropy coding improvements previously described herein.
In particular, when reading the residual, as long as a sequence of exactly Z zero slots is read, another integer is read to represent the subsequent number of zero slots, which would otherwise not be represented in the compressed representation. Since the ANS may not require bits per symbol integer and may also enable efficient representation of zero sequences, the system may set Z ∞ifa single adjacency list does not need to be accessed.
The encoder of computing system 100 may use one or more algorithms to select a reference list for use during compression. In some cases, there is no need to access the single list. In this case, the length of the reference chain used by a single node may not be limited, so the system can safely select the reference list that gives the optimal compression from all lists available in the current window (i.e., all contiguous lists of W previous nodes).
The system can estimate the number of bits that an algorithm can use to compress the adjacency list using a given reference. Since the system may use an adaptive entropy model, the estimation may be affected because the selection of one list may affect the probability of all other options.
Thus, the system can use the same iterative approach used by this mode. This may involve initializing symbol probabilities (e.g., all symbols have equal probabilities) using a simple fixed module, and then selecting a reference list, assuming these will be the final cost. The system can then calculate the symbol probabilities given by the selected reference list and repeat the process using the new probability distribution. The process may then be repeated a constant number of times.
When access to a single list is requested, the reference list may need to be correctly selected with more care while avoiding a reference chain that is too long. A simple solution could be to discard all lists in the window that would result in too long a reference chain without changing the decision considered in the previous node.
The system may use different strategies, which may involve initially building the reference tree T. The reference tree T may be disregarded with a maximum reference chain length constraint, where each tree edge is weighted with the number of bits saved by using the parent node as a reference for the child node. In some cases, the optimal tree can be easily constructed by a greedy algorithm that is used when no access to the single list is needed. The system 100 can then solve the dynamic programming problem on the resulting tree. This may produce a result indicating the maximum weight of the sub-forest F contained in the tree that does not have a path of length R + 1. If the process results in some paths shorter than R, the system may attempt to extend them in some way.
The above technique can be demonstrated to provide the following approximation to the maximum number of bits to be preserved, as follows:
if the total weight of T is taken into account
First of all, the first step is to,
fig. 5 is a flow diagram of a method in accordance with one or more example embodiments. Method 300 represents an example method that may include one or more of the operations, functions, or actions depicted in one or more of blocks 502 and 504, where each operation, function, or action may be performed by any of the systems shown in fig. 1-4, possibly other systems.
Those skilled in the art will appreciate that the flow charts described herein illustrate the function and operation of certain embodiments of the present disclosure. In this regard, each block of the flowchart illustrations may represent a module, segment, or portion of program code, which comprises one or more instructions executable by one or more processors to implement a particular logical function or step in the process. The program code can be stored on any type of computer readable medium (e.g., such as a storage device including a disk or hard drive).
Further, each block may represent a circuit wired to perform a particular logical function in the process. Alternative embodiments are included within the scope of the example embodiments of the present application, in which functions may be performed in an order different than illustrated or discussed, including substantially concurrently or in reverse order, depending on the functionality involved, as would be understood by those ordinarily skilled in the art.
At block 502, the method 500 involves obtaining a graph with data. For example, various types of graphs may be available to a computing system. The graph may be obtained from different sources, such as other computing systems, internal memory, and/or external memory.
At block 504, the method 500 involves compressing data of the graph using a multi-context entropy encoder. A multi-context entropy encoder encodes a contiguous list within the data such that each integer is assigned to a different probability distribution.
In some examples, compressing the data may involve compressing the data of the graph using a multi-context entropy encoder to store in memory. Further, compressing the data may involve compressing the data of the graph using a multi-context entropy encoder for transmission to the at least one computing device. In some examples, compressing the data of the graph may involve using a combination of huffman coding and ANS.
In further examples, the method 500 may also involve obtaining a second graph having second data and compressing the second data of the graph using a multi-context entropy encoder. In some cases, compressing the second data of the map is performed concurrently with compressing the data of the map.
In some embodiments, method 500 may also involve decompressing the compressed data of the graph using a decoder. The decoder may be configured to decode data encoded by the multi-context entropy encoder. In some cases, multiple decoders may be used. The decoder and/or encoder may transmit and receive between different types of devices, such as servers, CPUs, GPUs, etc.
In some embodiments, the method 500 may further involve, while compressing the data of the map using the multi-context entropy encoder, determining a processing speed associated with the multi-context entropy encoder. The method 500 may also involve comparing the processing speed to a threshold processing speed and adjusting operation of the multi-context entropy encoder based on comparing the processing speed to the threshold processing speed. For example, the system may determine that the processing speed is below a threshold processing speed and reduce an operation rate of the multi-context entropy encoder based on determining that the processing speed is below the threshold processing speed.
In other embodiments, the computing system 100 may determine and apply different weights when compressing or decompressing one or more graphs. For example, the computing system 100 may assign a weight to compression using huffman compression that is greater than the weight assigned to compression via ANS compression. Compression may also involve switching between each compression technique or performing these techniques simultaneously.
Fig. 6 is a schematic diagram illustrating a conceptual partial view of an example computer program product comprising a computer program for executing a computer process on a computing device, arranged in accordance with at least some embodiments presented herein. In some embodiments, the disclosed methods may be implemented as computer program instructions encoded on a non-transitory computer-readable storage medium in a machine-readable format, or on other non-transitory media or articles of manufacture.
In one embodiment, the example computer program product 600 is provided using a signal bearing medium 602, which signal bearing medium 602 may include one or more programming instructions 604, which one or more programming instructions 604 may provide the functionality or portions of the functionality described above with respect to fig. 1-5 when executed by one or more processors. In some examples, signal bearing medium 602 may encompass a non-transitory computer readable medium 606 such as, but not limited to, a hard disk drive, a Compact Disc (CD), a Digital Video Disc (DVD), a digital tape, memory, and the like. In some embodiments, the signal bearing medium 602 may encompass a computer recordable medium 608 such as, but not limited to, memory, a read/write (R/W) CD, a R/W DVD, and the like.
In some implementations, the signal bearing medium 602 may encompass a communication medium 610, such as, but not limited to, a digital and/or analog communication medium (e.g., a fiber optic cable, a waveguide, a wired communications link, a wireless communication link, etc.). Thus, for example, the signal bearing medium 602 may be conveyed over a wireless form of the communication medium 610.
The one or more programming instructions 604 may be, for example, computer-executable instructions and/or logic-implemented instructions. In some examples, the computing system 100 of fig. 1 may be configured to provide various operations, functions, or actions in response to the programming instructions 604 communicated to the computing system 100 by one or more of the computer-readable media 606, the computer-recordable media 608, and/or the communication media 610.
The non-transitory computer readable medium may also be distributed among a plurality of data storage elements, which may be remotely located from each other. Alternatively, the computing device executing some or all of the stored instructions may be another computing device, such as a server.
5. Conclusion
Embodiments of the present disclosure provide technical improvements specific to computer technology, for example, relating to analyzing large-scale data files having thousands of parameters. Computer-specific technical problems, such as the ability to format data into a standardized form for parameter rationality analysis, may be addressed in whole or in part by embodiments of the present disclosure. For example, rather than using manual inspection, embodiments of the present disclosure allow data received from many different types of sensors to be formatted and inspected for accuracy and rationality in a very efficient manner. Source data files that include outputs from different types of sensors (such as outputs concatenated together in a single file) may be processed together in a computing transaction by one computing device, rather than each sensor output being processed by a separate device or through a separate computing transaction. This is also very advantageous to enable the inspection and comparison of combinations of outputs of different sensors to further provide insight into the rationality of the data that cannot be performed when processing the sensor outputs individually. Accordingly, embodiments of the present disclosure may introduce new and efficient improvements in the way data is analyzed by selectively applying appropriate transformation maps to the data for batch processing of sensor outputs.
The systems and methods of the present disclosure also address computer network-specific issues, e.g., issues related to processing source file(s) including data received from various sensors for comparison to expected data found within multiple databases (generated as a result of causal analysis of each sensor reading). These computing network specific problems may be addressed by embodiments of the present disclosure. For example, by identifying a transformation graph and applying the graph to data, a common format can be associated with multiple source files for more efficient rationality checking. The source files can be processed using much fewer resources than currently performed manually, and the level of accuracy is improved due to the use of a parameter rules database that could otherwise be applied to standardized data. Embodiments of the present disclosure thus introduce new and efficient improvements in the manner in which a database may be applied to data in a source data file to increase the speed and/or efficiency of one or more processor-based systems configured to support or utilize the database.
The present disclosure is not limited in terms of the particular embodiments described in this application, which are intended as illustrations of various aspects. It will be apparent to those skilled in the art that many modifications and variations can be made without departing from the scope thereof. Functionally equivalent methods and apparatuses within the scope of the disclosure, in addition to those enumerated herein, will be apparent to those skilled in the art from the foregoing description. Such modifications and variations are intended to fall within the scope of the appended claims.
The above detailed description describes various features and functions of the disclosed systems, devices, and methods with reference to the accompanying drawings. The example embodiments described herein and in the drawings are not meant to be limiting. Other embodiments may be utilized, and other changes may be made, without departing from the scope of the subject matter presented herein. It will be readily understood that the aspects of the present disclosure, as generally described herein, and illustrated in the figures, can be arranged, substituted, combined, separated, and designed in a wide variety of different configurations, all of which are explicitly contemplated herein.
With respect to any or all of the message flow diagrams, scenarios, and flowcharts in the figures and as discussed herein, each step, block, and/or communication may represent processing of information and/or transmission of information according to an example embodiment. Alternate embodiments are included within the scope of these example embodiments. In such alternative embodiments, for example, functions described as steps, blocks, transmissions, communications, requests, responses, and/or messages may be performed in an order different than illustrated or discussed, including substantially concurrently or in a reverse order, depending on the functionality involved. Further, more or fewer blocks and/or functions may be used with any of the ladder diagrams, scenarios, and flow diagrams discussed herein, and these ladder diagrams, scenarios, and flow diagrams may be partially or fully combined with each other.
The steps or blocks representing information processing may correspond to circuitry that may be configured to perform the particular logical functions of the methods or techniques described herein. Alternatively or additionally, the steps or blocks representing information processing may correspond to modules, segments, or portions of program code (including related data). The program code may include one or more instructions executable by a processor to implement specific logical functions or actions in the described methods or techniques. The program code and/or associated data may be stored on any type of computer-readable medium, such as a storage device including a diskette, hard drive, or other storage medium.
The computer readable medium may also include non-transitory computer readable media, such as computer readable media that store data for short periods of time, such as register memory, processor cache, and Random Access Memory (RAM). The computer-readable medium may also include a non-transitory computer-readable medium that stores program code and/or data for longer periods of time. Thus, for example, a computer-readable medium may include secondary or persistent long-term storage devices such as Read Only Memory (ROM), optical or magnetic disks, compact disk read only memory (CD-ROM). The computer readable medium may also be any other volatile or non-volatile storage system. For example, a computer-readable medium may be considered a computer-readable storage medium or a tangible storage device.
Further, steps or blocks representing one or more transfers of information may correspond to transfers of information between software and/or hardware modules in the same physical device. However, other information transfers may be between software modules and/or hardware modules in different physical devices.
The particular arrangement shown in the figures should not be considered limiting. It should be understood that other embodiments may include more or less of each element shown in a given figure. In addition, some of the illustrated elements may be combined or omitted. Furthermore, example embodiments may include elements not shown in the figures.
Furthermore, any enumeration of elements, blocks or steps in the present description or claims is for clarity. Thus, such enumeration should not be interpreted as requiring or implying any particular arrangement of such elements, blocks, or steps, or performing in a particular order.
While various aspects and embodiments have been disclosed herein, other aspects and embodiments will be apparent to those skilled in the art. The various aspects and embodiments disclosed herein are for purposes of illustration and are not intended to be limiting, with the true scope being indicated by the following claims.
Claims (20)
1. A method, comprising:
obtaining, at a computing system, a graph having data; and
compressing, by the computing system, data of the graph using a multi-context entropy encoder, wherein the multi-context entropy encoder encodes a contiguous list within the data such that each integer is assigned to a different probability distribution.
2. The method of claim 1, wherein compressing the data of the graph using the multi-context entropy encoder comprises:
compressing data of the graph using the multi-context entropy encoder for storage in a memory.
3. The method of claim 1, wherein compressing the data of the graph using the multi-context entropy encoder comprises:
compressing data of the graph using the multi-context entropy encoder for transmission to at least one computing device.
4. The method of claim 1, wherein compressing the data of the graph using the multi-context entropy encoder comprises:
the data of the graph is compressed using a combination of huffman coding and an asymmetric digital system (ANS).
5. The method of claim 1, further comprising:
obtaining a second graph having second data; and
compressing second data of the graph using the multi-context entropy encoder, wherein compressing the second data of the graph is performed concurrently with compressing the data of the graph.
6. The method of claim 1, further comprising:
decompressing the compressed data of the graph using a decoder, wherein the decoder is configured to decode the data encoded by the multi-context entropy encoder.
7. A system, comprising:
a computing system;
a non-transitory computer readable medium; and
program instructions stored on the non-transitory computer-readable medium, wherein the program instructions are executable by the computing system to perform operations comprising:
obtaining a graph with data; and
compressing data of the graph using a multi-context entropy encoder, wherein the multi-context entropy encoder encodes a contiguous list within the data such that each integer is assigned to a different probability distribution.
8. The system of claim 7, wherein compressing the data of the graph using the multi-context entropy encoder comprises:
compressing data of the graph using the multi-context entropy encoder for storage in a memory.
9. The system of claim 7, wherein compressing the data of the graph using the multi-context entropy encoder comprises:
compressing data of the graph using the multi-context entropy encoder for transmission to at least one computing device.
10. The system of claim 7, wherein compressing the data of the graph using the multi-context entropy encoder comprises:
the data of the graph is compressed using a combination of huffman coding and an asymmetric digital system (ANS).
11. The system of claim 7, wherein the operations further comprise:
obtaining a second graph having second data; and
compressing second data of the graph using the multi-context entropy encoder, wherein compressing the second data of the graph is performed concurrently with compressing the data of the graph.
12. The system of claim 7, further comprising:
decompressing the compressed data of the graph using a decoder, wherein the decoder is configured to decode the data encoded by the multi-context entropy encoder.
13. A non-transitory computer-readable medium having stored therein instructions executable by one or more processors to cause a computing system to perform functions comprising:
obtaining a graph with data; and
compressing data of the graph using a multi-context entropy encoder, wherein the multi-context entropy encoder encodes a contiguous list within the data such that each integer is assigned to a different probability distribution.
14. The non-transitory computer-readable medium of claim 13, wherein compressing the data of the graph using the multi-context entropy encoder comprises:
compressing data of the graph using the multi-context entropy encoder for storage in a memory.
15. The non-transitory computer-readable medium of claim 13, wherein compressing the data of the graph using the multi-context entropy encoder comprises:
compressing data of the graph using the multi-context entropy encoder for transmission to at least one computing device.
16. The non-transitory computer-readable medium of claim 13, wherein compressing the data of the graph using the multi-context entropy encoder comprises:
the data of the graph is compressed using a combination of Huffman coding and an asymmetric digital system (ANS).
17. The non-transitory computer-readable medium of claim 13, further comprising:
obtaining a second graph having second data; and
compressing second data of the graph using the multi-context entropy encoder, wherein compressing the second data of the graph is performed concurrently with compressing the data of the graph.
18. The non-transitory computer-readable medium of claim 13, further comprising:
decompressing the compressed data of the graph using a decoder, wherein the decoder is configured to decode the data encoded by the multi-context entropy encoder.
19. The non-transitory computer-readable medium of claim 13, further comprising:
determining a processing speed associated with the multi-context entropy encoder while compressing the data of the graph using the multi-context entropy encoder;
comparing the processing speed to a threshold processing speed; and
adjusting operation of the multi-context entropy encoder based on comparing the processing speed to the threshold processing speed.
20. The non-transitory computer-readable medium of claim 19, wherein adjusting operation of the multi-context entropy encoder based on comparing the processing speed to the threshold processing speed comprises:
determining that the processing speed is below the threshold processing speed; and
based on determining that the processing speed is below the threshold processing speed, reducing an operating rate of the multi-context entropy encoder.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202062975722P | 2020-02-12 | 2020-02-12 | |
US62/975,722 | 2020-02-12 | ||
PCT/US2020/030870 WO2021162722A1 (en) | 2020-02-12 | 2020-04-30 | Multi-context entropy coding for compression of graphs |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115104305A true CN115104305A (en) | 2022-09-23 |
Family
ID=77292551
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202080096330.9A Pending CN115104305A (en) | 2020-02-12 | 2020-04-30 | Multi-context entropy coding for graph compression |
Country Status (4)
Country | Link |
---|---|
US (1) | US20230042018A1 (en) |
EP (1) | EP4078957A4 (en) |
CN (1) | CN115104305A (en) |
WO (1) | WO2021162722A1 (en) |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN117155407A (en) * | 2023-10-31 | 2023-12-01 | 博洛尼智能科技（青岛）有限公司 | Intelligent mirror cabinet disinfection log data optimal storage method |
CN117394866A (en) * | 2023-10-07 | 2024-01-12 | 广东图为信息技术有限公司 | Intelligent flap valve system based on environment self-adaption |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20240022260A1 (en) * | 2022-07-12 | 2024-01-18 | Samsung Display Co., Ltd. | Low complexity optimal parallel huffman encoder and decoder |
CN116600135B (en) * | 2023-06-06 | 2024-02-13 | 广州大学 | Lossless compression-based traceability graph compression method and system |
Family Cites Families (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6549666B1 (en) * | 1994-09-21 | 2003-04-15 | Ricoh Company, Ltd | Reversible embedded wavelet system implementation |
US7545293B2 (en) * | 2006-11-14 | 2009-06-09 | Qualcomm Incorporated | Memory efficient coding of variable length codes |
CA2839560C (en) * | 2011-06-16 | 2016-10-04 | Fraunhofer-Gesellschaft Zur Forderung Der Angewandten Forschung E.V. | Entropy coding of motion vector differences |
EP2823409A4 (en) * | 2012-03-04 | 2015-12-02 | Adam Jeffries | Data systems processing |
GB2513111A (en) * | 2013-04-08 | 2014-10-22 | Sony Corp | Data encoding and decoding |
US10735736B2 (en) * | 2017-08-29 | 2020-08-04 | Google Llc | Selective mixing for entropy coding in video compression |
EP3514968B1 (en) * | 2018-01-18 | 2023-03-08 | BlackBerry Limited | Methods and devices for entropy coding point clouds |
CN112771541A (en) * | 2018-09-27 | 2021-05-07 | 谷歌有限责任公司 | Data compression using integer neural networks |
-
2020
- 2020-04-30 US US17/758,851 patent/US20230042018A1/en not_active Abandoned
- 2020-04-30 EP EP20919163.4A patent/EP4078957A4/en not_active Withdrawn
- 2020-04-30 WO PCT/US2020/030870 patent/WO2021162722A1/en unknown
- 2020-04-30 CN CN202080096330.9A patent/CN115104305A/en active Pending
Cited By (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN117394866A (en) * | 2023-10-07 | 2024-01-12 | 广东图为信息技术有限公司 | Intelligent flap valve system based on environment self-adaption |
CN117394866B (en) * | 2023-10-07 | 2024-04-02 | 广东图为信息技术有限公司 | Intelligent flap valve system based on environment self-adaption |
CN117155407A (en) * | 2023-10-31 | 2023-12-01 | 博洛尼智能科技（青岛）有限公司 | Intelligent mirror cabinet disinfection log data optimal storage method |
CN117155407B (en) * | 2023-10-31 | 2024-04-05 | 博洛尼智能科技（青岛）有限公司 | Intelligent mirror cabinet disinfection log data optimal storage method |
Also Published As
Publication number | Publication date |
---|---|
EP4078957A1 (en) | 2022-10-26 |
EP4078957A4 (en) | 2024-01-03 |
US20230042018A1 (en) | 2023-02-09 |
WO2021162722A1 (en) | 2021-08-19 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20230042018A1 (en) | Multi-context entropy coding for compression of graphs | |
US10187081B1 (en) | Dictionary preload for data compression | |
US10680645B2 (en) | System and method for data storage, transfer, synchronization, and security using codeword probability estimation | |
US11106735B2 (en) | Directed graph compression | |
US10666289B1 (en) | Data compression using dictionary encoding | |
US10706018B2 (en) | Bandwidth-efficient installation of software on target devices using reference code libraries | |
US10476519B2 (en) | System and method for high-speed transfer of small data sets | |
US9137337B2 (en) | Hierarchical bitmasks for indicating the presence or absence of serialized data fields | |
KR100484137B1 (en) | Improved huffman decoding method and apparatus thereof | |
CN116018647A (en) | Genomic information compression by configurable machine learning based arithmetic coding | |
US11733867B2 (en) | System and method for multiple pass data compaction utilizing delta encoding | |
US7796059B2 (en) | Fast approximate dynamic Huffman coding with periodic regeneration and precomputing | |
CN104125475A (en) | Multi-dimensional quantum data compressing and uncompressing method and apparatus | |
US11868616B2 (en) | System and method for low-distortion compaction of floating-point numbers | |
Wang et al. | A simplified variant of tabled asymmetric numeral systems with a smaller look-up table | |
US11700013B2 (en) | System and method for data compaction and security with extended functionality | |
GB2607923A (en) | Power-aware transmission of quantum control signals | |
Williams | Performance Overhead of Lossless Data Compression and Decompression Algorithms: A Qualitative Fundamental Research Study | |
US11914443B2 (en) | Power-aware transmission of quantum control signals | |
US11967974B2 (en) | System and method for data compression with protocol adaptation | |
US20240080040A1 (en) | System and method for data storage, transfer, synchronization, and security using automated model monitoring and training | |
AU2022292146A1 (en) | Power-aware transmission of quantum control signals | |
Baidoo | Comparative Analysis of the Compression of Text Data Using Huffman, Arithmetic, Run-Length, and Lempel Ziv Welch Coding Algorithms | |
CN117811586A (en) | Data encoding method and device, data processing system, device and medium | |
WO2020112917A1 (en) | Hight-speed transfer of small data sets |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |