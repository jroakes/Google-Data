CN115004703A - Intra prediction for image and video compression - Google Patents
Intra prediction for image and video compression Download PDFInfo
- Publication number
- CN115004703A CN115004703A CN202080093804.4A CN202080093804A CN115004703A CN 115004703 A CN115004703 A CN 115004703A CN 202080093804 A CN202080093804 A CN 202080093804A CN 115004703 A CN115004703 A CN 115004703A
- Authority
- CN
- China
- Prior art keywords
- intercept
- pixel
- prediction
- block
- peripheral
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/17—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object
- H04N19/176—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object the region being a block, e.g. a macroblock
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/103—Selection of coding mode or of prediction mode
- H04N19/105—Selection of the reference unit for prediction within a chosen coding or prediction mode, e.g. adaptive choice of position and number of pixels used for prediction
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/103—Selection of coding mode or of prediction mode
- H04N19/11—Selection of coding mode or of prediction mode among a plurality of spatial predictive coding modes
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
- H04N19/136—Incoming video signal characteristics or properties
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
- H04N19/157—Assigned coding mode, i.e. the coding mode being predefined or preselected to be further used for selection of another element or parameter
- H04N19/159—Prediction type, e.g. intra-frame, inter-frame or bidirectional frame prediction
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
- H04N19/167—Position within a video image, e.g. region of interest [ROI]
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/182—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being a pixel
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/189—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the adaptation method, adaptation tool or adaptation type used for the adaptive coding
- H04N19/196—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the adaptation method, adaptation tool or adaptation type used for the adaptive coding being specially adapted for the computation of encoding parameters, e.g. by averaging previously computed encoding parameters
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/46—Embedding additional information in the video signal during the compression process
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/59—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving spatial sub-sampling or interpolation, e.g. alteration of picture size or resolution
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/593—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving spatial prediction techniques
Abstract
A method of coding a current block using an intra prediction mode includes: acquiring a focus, the focus having coordinates (a, b) in a coordinate system; and generating a prediction block for the current block using first peripheral pixels and second peripheral pixels, wherein the first peripheral pixels form a first peripheral pixel line constituting an x-axis, and wherein the second peripheral pixels form a second peripheral pixel line constituting a y-axis. Generating the prediction block includes: for each position of the prediction block at position (i, j) of the prediction block, determining at least one of an x-intercept or a y-intercept; and determining a predicted pixel value for each location of the prediction block using the at least one of the x-intercept or the y-intercept.
Description
Cross Reference to Related Applications
Priority and benefit of this application are claimed in united states provisional patent application No. 62/976,001, filed on 13/2/2020, the entire disclosure of which is incorporated herein by reference.
Background
A digital video stream may represent video using a series of frames or still images. Digital video can be used for a variety of applications including, for example, video conferencing, high definition video entertainment, video advertising, or sharing of user generated video. Digital video streams can contain large amounts of data and consume a significant amount of the computing or communication resources of a computing device for processing, transmission, or storage of the video data. Various methods have been proposed to reduce the amount of data in a video stream, including compression and other encoding techniques.
Spatial similarity-based encoding may be performed by decomposing a frame or image into blocks that are predicted based on other blocks within the same frame or image. The difference (i.e., the residual) between the block and the prediction block is compressed and encoded in the bitstream. The decoder uses the difference and the reference frame to reconstruct the frame or picture.
Disclosure of Invention
Aspects, features, elements, and embodiments for encoding and decoding a block using intra prediction are disclosed herein.
A first aspect is a method of coding a current block using an intra prediction mode. The method comprises acquiring a focal point having coordinates (a, b) in a coordinate system; generating a prediction block for the current block using first and second peripheral pixels, wherein the first peripheral pixels form a first peripheral pixel line constituting an x-axis, wherein the second peripheral pixels form a second peripheral pixel line constituting a y-axis, and wherein the first and second peripheral pixel lines form a coordinate system having an origin; and a residual block corresponding to a difference between the current block and the prediction block is coded. Generating the prediction block includes: for each position of the prediction block at position (i, j) of the prediction block, determining at least one of an x-intercept or a y-intercept, wherein the x-intercept is a first point at which a line formed by a point centered on each position of the prediction block and the focus intersects the first line of peripheral pixels, and wherein the y-intercept is a second point at which a line formed by a point centered on each position of the prediction block and the focus intersects the second line of peripheral pixels; and determining a predicted pixel value for each location of the predicted block using at least one of an x-intercept or a y-intercept.
Another aspect is an apparatus for decoding a current block. The apparatus includes a memory and a processor. The processor is configured to: executing instructions stored in a memory to decode a focus from a compressed bitstream; obtaining a prediction block of prediction pixels of a current block, wherein each prediction pixel is located at a corresponding position in the prediction block; and reconstructing the current block using the prediction block. The prediction block is acquired by: for each position of the prediction block, executing instructions to obtain a line indicative of a respective prediction angle, the line connecting the focus to each position; and determining a pixel value for each location using the line.
Another aspect is a method for encoding a current block. The method comprises the following steps: obtaining a prediction block for prediction pixels of a current block using peripheral pixels, wherein each prediction pixel is located at a respective position within the prediction block; and encoding the focal point in the compressed bitstream. Acquiring the prediction block includes: obtaining a focal point, the focal point having coordinates (a, b) in a coordinate system, the focal point being outside the current block, and the focal point not being any peripheral pixels; for each position of the prediction block, performing a step comprising obtaining a line indicating a respective prediction angle, the line connecting the focus to each position; and determining a pixel value for each location using the line.
It should be appreciated that aspects can be implemented in any convenient form. For example, aspects may be implemented by means of a suitable computer program, which may be carried on a suitable carrier medium which may be a tangible carrier medium (e.g. a disk) or an intangible carrier medium (e.g. a communications signal). Aspects may also be implemented using suitable apparatus, which may take the form of a programmable computer running a computer program arranged to implement the methods. The ability to combine aspects enables features described in the context of one aspect to be implemented in another aspect.
Drawings
The description herein makes reference to the accompanying drawings wherein like reference numerals refer to like parts throughout the several views.
Fig. 1 is a schematic diagram of a video encoding and decoding system.
Fig. 2 is a block diagram of an example of a computing device capable of implementing a transmitting station or a receiving station.
Fig. 3 is a diagram of a video stream to be encoded and subsequently decoded.
Fig. 4 is a block diagram of an encoder according to an embodiment of the present disclosure.
Fig. 5 is a block diagram of a decoder according to an embodiment of the present disclosure.
Fig. 6 is a block diagram of a representation of a portion of a frame according to an embodiment of the present disclosure.
Fig. 7 is a diagram of an example of an intra prediction mode.
Fig. 8 is an example of an image portion including a rail.
Fig. 9 is an example of a flow diagram of a technique for determining bits along a line of peripheral pixels, and therefore for determining predicted pixel values, according to an embodiment of the present disclosure.
Fig. 10 is an example of a bit position computed by the technique of fig. 9, according to an embodiment of the disclosure.
Fig. 11 is an example of a prediction block computed from the example of fig. 10.
FIG. 12 is a flowchart of a technique for intra prediction of a current block according to an embodiment of the present disclosure.
FIG. 13 is a flowchart of a technique for generating a prediction block for a current block using intra prediction according to an embodiment of the present disclosure.
FIG. 14 is a flowchart of a technique for decoding a current block using an intra prediction mode according to an embodiment of the present disclosure.
Fig. 15 is an example 1500 illustrating a focal point according to an embodiment of the present disclosure.
Fig. 16 is an example illustrating x and y intercepts in accordance with an embodiment of the present disclosure.
Fig. 17 illustrates an example of a focal group according to an embodiment of the present disclosure.
Fig. 18 is an example of a flow diagram of a technique for determining bits along a peripheral pixel line so that a predicted pixel value is determined, according to an embodiment of the present disclosure.
FIG. 19 is a flow diagram of a technique for generating a prediction block for a current block using intra-prediction according to an embodiment of the present disclosure.
Fig. 20A to 20B are examples for illustrating the technique of fig. 19.
Fig. 21 shows an example of the technique of fig. 19 when the upper peripheral pixels are used as the main peripheral pixels.
Detailed Description
As described above, compression schemes associated with coded video streams may include breaking an image into blocks and using one or more techniques to generate a digital video output bitstream (i.e., an encoded bitstream) to limit the information included in the output bitstream. The received bitstream can be decoded to recreate the block and source images from limited information. Encoding a video stream or a portion of a video stream such as a frame or block can include using spatial similarities in the video stream to improve coding efficiency. For example, a current block of a video stream may be encoded based on identifying a difference (residue) between a previously coded pixel value or a combination of previously coded pixel values and a pixel value in the current block.
Encoding using spatial similarity can be referred to as intra prediction. Intra-prediction attempts to predict pixel values of a current block of a frame (i.e., image, picture) or a single image of a video stream using pixels peripheral to the current block; that is, pixels in the same frame as the current block but outside the current block are used. Intra-prediction can be performed along prediction directions, referred to herein as prediction angles, where each direction can correspond to an intra-prediction mode. The intra prediction mode uses pixels around the predicted current block. Pixels outside the current block are pixels outside the current block. The intra prediction mode can be signaled by the encoder to the decoder.
Many different intra prediction modes can be supported. Some intra-prediction modes use a single value for all pixels within a prediction block generated using at least one peripheral pixel. The others are called directional intra prediction modes, each having a corresponding prediction angle. The intra prediction modes can include, for example, a horizontal intra prediction mode, a vertical intra prediction mode, and various other directional intra prediction modes. For example, a codec can have available prediction modes corresponding to 50 to 60 prediction angles. An example of intra prediction modes is described with respect to fig. 7.
However, the intra prediction modes of current codecs, such as those described above and with respect to fig. 7, may not optimally code images or scene blocks that contain parallel lines. It is well known that a perspective representation of a scene (e.g., an image) or a scene viewed at an angle where the image includes parallel lines can have one or more vanishing points. That is, the parallel lines can be perceived (i.e., seen) as converging to (or diverging from) a vanishing point (or focal point). Non-limiting examples of images that include parallel lines or checkerboard patterns include striped shirts, bricks on buildings, shutters, train tracks, paneled wood flooring, sun rays, tree trunks, and the like. Although parallel lines are used herein for ease of description, the present disclosure is not so limited. For example, the disclosure herein can be used with parallel edges (such as the edge of a pencil in an image where the image is taken from a point of dots). Furthermore, the present disclosure is not limited to straight lines. The parameters described below can be used to overlay the curves in the prediction line.
Such a pattern can be easily recognized visually. However, such patterns can be significantly more difficult to programmatically resolve and encode when viewed from perspective angles other than 90 degree angles. As already mentioned, parallel lines can appear to go to a point far away, such as described with respect to fig. 8, which shows converging rails.
Although some intra-prediction modes can have an associated direction, the same direction is used to generate each prediction pixel of the prediction block. However, with respect to converging lines (such as the rails of fig. 8), each line can have a different direction. Therefore, the unidirectional intra prediction mode may not generate an optimal prediction block for coding the current block. By optimal prediction block is meant a prediction block that minimizes the residual error between the prediction block and the current block being encoded.
Embodiments in accordance with the present disclosure use novel intra prediction modes that can be used to code blocks that include a convergence line. As indicated above, the intra prediction mode uses pixels that are peripheral to the current block. At a high level, a prediction block generated using an intra prediction mode according to an embodiment of the present disclosure can enable one pixel of a row of the prediction block to be copied from one or more peripheral pixels in one direction, while another pixel of the same row of the prediction block can be copied from one or more other peripheral pixels in a different direction. In addition, scaling (expansion or reduction) can be optionally applied according to parameters of the intra prediction mode, as described further below.
In some embodiments, to generate a prediction block according to the intra-prediction modes of the present disclosure, the same set of peripheral pixels (i.e., upper and/or left peripheral pixels) that are typically used for intra-prediction are repeatedly resampled for each pixel (i.e., pixel location) of the prediction block. Peripheral pixels are considered to be a continuous line of pixel values, which for ease of reference are referred to herein as a peripheral pixel line. To generate the prediction pixels of the prediction block, different bits of the peripheral pixel line are considered. However, as described further below, each time a bit position is considered, the bit position is shifted from the immediately preceding bit according to the parameters of the intra prediction mode. It is noted that the pixel values of only integer positions of the pixel positions of the peripheral pixel line are known: the peripheral pixels themselves. As such, sub-pixel (i.e., non-integer pixel) values for a line of peripheral pixels are obtained from the peripheral pixels using, for example, interpolation or filtering operations.
In other embodiments, the initial prediction block can be generated using a directional prediction mode. Warping (e.g., warping functions, warping parameter sets, etc.) can then be applied to the initial prediction block to generate the prediction block. In an example, the warping can be perspective warping. In an example, the warping can be affine warping.
In yet another embodiment, the intra prediction mode according to the present disclosure can use a focus a distance away from the current block. The focal point can be considered as a point in space from which all pixels of the current block diverge or converge. For each predicted pixel position of the predicted block, a line connecting the pixel position and the focal point is drawn. The x-and y-intercepts of the line are determined from the x-and y-axes of the coordinate system formed by the left and top peripheral pixels. The x-intercept and the y-intercept are used to determine (e.g., identify, select, calculate, etc.) the peripheral pixels used to calculate the predicted pixel value.
In summary, directional prediction modes such as those described with respect to fig. 7 result in parallel prediction lines for all pixels of the prediction block. However, the intra prediction mode according to the embodiments of the present disclosure results in non-parallel prediction lines. In this way, at least two prediction pixels can be calculated from different prediction angles.
In contrast to the design and semantics of conventional directional prediction modes, where each prediction pixel is calculated from the same prediction angle, although the intra prediction mode according to the present disclosure may result in two or more prediction pixels being derived (e.g., calculated) by using parallel prediction lines (i.e., the same prediction angle), this is only by chance. For example, although more than one prediction pixel can have the same prediction angle depending on the position of the focus, the intra prediction mode according to the embodiment of the present disclosure enables not all prediction pixels to have the same prediction angle.
After first describing an environment in which intra prediction for image and video compression disclosed herein may be implemented, details are described herein. Although the intra prediction mode according to the present disclosure is described with respect to a video encoder and a video decoder, the intra prediction mode can also be used in an image codec. The image codec can be or share many aspects of the video codec described herein.
Fig. 1 is a schematic diagram of a video encoding and decoding system 100. Transmitter station 102 can be, for example, a computer having a hardware internal configuration such as that depicted in fig. 2. However, other suitable implementations of transmitting station 102 are possible. For example, the processing of transmitting station 102 can be distributed among multiple devices.
In one example, the receiving station 106 can be a computer having an internal configuration such as the hardware depicted in FIG. 2. However, other suitable implementations of the receiving station 106 are possible. For example, the processing of the receiving station 106 can be distributed among multiple devices.
Other implementations of the video encoding and decoding system 100 are possible. For example, embodiments can omit network 104. In another embodiment, the video stream can be encoded and then stored for later transmission to the receiving station 106 or any other device having memory. In one embodiment, the receiving station 106 receives (e.g., via the network 104, a computer bus, and/or some communication path) the encoded video stream and stores the video stream for later decoding. In an example embodiment, the real-time transport protocol (RTP) is used to transmit encoded video over the network 104. In another embodiment, a transport protocol other than RTP may be used, such as a hypertext transfer protocol (HTTP) -based video streaming protocol.
When used in a videoconferencing system, for example, transmitter station 102 and/or receiving station 106 may include the ability to both encode and decode video streams as described below. For example, the receiving station 106 may be a video conference participant that receives an encoded video bitstream from a video conference server (e.g., the transmitting station 102) to decode and view and further encode its own video bitstream, and sends its own video bitstream to the video conference server for decoding and viewing by other participants.
Fig. 2 is a block diagram of an example of a computing device 200 capable of implementing a transmitting station or a receiving station. For example, computing device 200 may be capable of implementing one or both of transmitting station 102 and receiving station 106 of fig. 1. Computing device 200 can be in the form of a computing system that includes multiple computing devices, or in the form of a single computing device, such as a mobile phone, a tablet computer, a laptop computer, a notebook computer, a desktop computer, and so forth.
The CPU202 in the computing device 200 can be a central processing unit. Alternatively, the CPU202 can be any other type of device or devices capable of manipulating or processing information now existing or later developed. Although the disclosed embodiments can be practiced with a single processor, such as CPU202, as shown, more than one processor can be used to achieve speed and efficiency advantages.
In one implementation, the memory 204 in the computing device 200 can be a Read Only Memory (ROM) device or a Random Access Memory (RAM) device. Any other suitable type of storage device can be used as memory 204. The memory 204 can include code and data 206 that are accessed by the CPU202 using the bus 212. The memory 204 can further include an operating system 208 and application programs 210, the application programs 210 including at least one program that allows the CPU202 to perform the methods described herein. For example, the application programs 210 can include applications 1 through N, with applications 1 through N further including video coding applications that perform the methods described herein. The computing device 200 can also include a secondary memory 214, the secondary memory 214 can be, for example, a memory card used with the removable computing device 200. Because video communication sessions may contain significant amounts of information, they can be stored in whole or in part in secondary memory 214 and loaded into memory 204 as needed for processing.
Although fig. 2 depicts the CPU202 and memory 204 of the computing device 200 as integrated into a single unit, other configurations can be used. The operations of CPU202 can be distributed across multiple machines (each machine having one or more processors) that can be coupled directly or coupled across a local area network or other network. Memory 204 can be distributed across multiple machines, such as a network-based memory or a memory among multiple machines that perform operations for computing device 200. Although depicted here as a single bus, the bus 212 of the computing device 200 can be comprised of multiple buses. Further, the secondary memory 214 can be directly coupled to other components of the computing device 200 or can be accessible via a network and can include a single integrated unit, such as a memory card, or multiple units, such as multiple memory cards. Computing device 200 can thus be implemented in a variety of configurations.
Fig. 3 is a diagram of an example of a video stream 300 to be encoded and subsequently decoded. The video stream 300 includes a video sequence 302. At the next level, the video sequence 302 includes a number of adjacent frames 304. Although three frames are depicted as adjacent frames 304, the video sequence 302 can include any number of adjacent frames 304. The adjacent frames 304 can then be further subdivided into individual frames, such as frame 306. At the next level, the frame 306 can be divided into a series or plane of segments 308. For example, the segments 308 can be a subset of frames that allow parallel processing. The segment 308 can also be a subset of a frame that can separate the video data into individual colors. For example, a frame 306 of color video data can include a luminance plane and two chrominance planes. The segments 308 can be sampled at different resolutions.
Regardless of whether frame 306 is divided into segments 308, frame 306 may be further subdivided into blocks 310, blocks 310 being capable of containing data corresponding to, for example, 16x16 pixels in frame 306. The block 310 can also be arranged to include data from one or more segments 308 of pixel data. The blocks 310 can also have any other suitable size, such as 4x4 pixels, 8x8 pixels, 16x8 pixels, 8x16 pixels, 16x16 pixels, or larger.
Fig. 4 is a block diagram of an encoder 400 according to an embodiment of the present disclosure. As described above, encoder 400 can be implemented in transmitting station 102, such as by providing a computer software program stored in a memory, e.g., memory 204. The computer software program can include machine instructions that, when executed by a processor, such as CPU202, cause transmitting station 102 to encode video data in the manner described herein. Encoder 400 can also be implemented as dedicated hardware included, for example, in transmitting station 102. The encoder 400 has the following stages to perform various functions in the forward path (indicated by the solid connecting lines) to generate an encoded or compressed bitstream 420 using the video stream 300 as input: an intra/inter prediction stage 402, a transform stage 404, a quantization stage 406, and an entropy coding stage 408. The encoder 400 may also include a reconstruction path (indicated by dotted connecting lines) to reconstruct the frame used to encode future blocks. In fig. 4, the encoder 400 has the following stages to perform various functions in the reconstruction path: a dequantization stage 410, an inverse transform stage 412, a reconstruction stage 414 and a loop filtering stage 416. Other structural variations of the encoder 400 can be used to encode the video stream 300.
When the video stream 300 is presented for encoding, the frames 306 can be processed in units of blocks. At the intra/inter prediction stage 402, a block can be encoded using intra-frame prediction (also referred to as intra-prediction) or inter-frame prediction (also referred to as inter-prediction), or a combination of both. In any case, a prediction block can be formed. In the case of intra prediction, all or part of the prediction block may be formed of samples in the current frame that have been previously encoded and reconstructed. In the case of inter-prediction, all or part of the prediction block may be formed from samples in one or more previously constructed reference frames determined using the motion vector.
Next, still referring to FIG. 4, the predicted block can be subtracted from the current block at the intra/inter prediction stage 402 to generate a residual block (also referred to as a residual). The transform stage 404 transforms the residual into transform coefficients, e.g., in the frequency domain, using a block-based transform. Such block-based transforms include, for example, Discrete Cosine Transform (DCT) and Asymmetric Discrete Sine Transform (ADST). Other block-based transforms are possible. Furthermore, a combination of different transforms may be applied to a single residual. In one example of applying a transform, the DCT transforms the residual block to a spatial frequency-based frequency domain of transform coefficient values. The lowest frequency (DC) coefficients are at the top left of the matrix and the highest frequency coefficients are at the bottom right of the matrix. It is noted that the size of the prediction block, and thus the residual block, may be different from the size of the transform block. For example, a prediction block may be partitioned into smaller blocks to which separate transforms are applied.
The quantization stage 406 uses quantizer values or quantization levels to convert the transform coefficients into discrete quantum values, referred to as quantized transform coefficients. For example, the transform coefficients may be divided by the quantizer value and truncated. The quantized transform coefficients are then entropy encoded by the entropy encoding stage 408. Entropy coding may be performed using any number of techniques including tokens and binary trees. The entropy coded coefficients are then output to the compressed bitstream 420 along with other information used to decode the block, which may include, for example, the type of prediction used, the type of transform, the motion vector, and the quantizer value. The information to decode the block may be entropy coded into a block, frame, slice, and/or segment header within the compressed bitstream 420. The compressed bitstream 420 can also be referred to as an encoded video stream or an encoded video bitstream, and these terms will be used interchangeably herein.
The reconstruction path in fig. 4 (shown by the dotted connecting lines) can be used to ensure that both the encoder 400 and the decoder 500 (described below) use the same reference frames and blocks to decode the compressed bitstream 420. The reconstruction path performs functions similar to those occurring during the decoding process discussed in more detail below, including dequantizing the quantized transform coefficients at a dequantization stage 410 and inverse transforming the dequantized transform coefficients at an inverse transform stage 412 to produce a block of derivative residues (also referred to as derivative residuals). At the reconstruction stage 414, the prediction block predicted at the intra/inter prediction stage 402 can be added to the derivative residual to create a reconstructed block. Loop filtering stage 416 can be applied to the reconstructed block to reduce distortion, such as block artifacts.
Other variations of the encoder 400 can be used to encode the compressed bitstream 420. For example, the non-transform based encoder 400 can quantize the residual signal directly without the transform stage 404 for certain blocks or frames. In another embodiment, the encoder 400 can combine the quantization stage 406 and the de-quantization stage 410 into a single stage.
Fig. 5 is a block diagram of a decoder 500 according to an embodiment of the present disclosure. The decoder 500 can be implemented in the receiving station 106, for example, by providing a computer software program stored in the memory 204. The computer software program can include machine instructions that, when executed by a processor, such as CPU202, cause receiving station 106 to decode video data in the manner described herein. Decoder 500 can also be implemented in hardware included in, for example, transmitting station 102 or receiving station 106. Similar to the reconstruction path of the encoder 400 discussed above, the decoder 500 in one example includes the following stages to perform various functions to generate the output video stream 516 from the compressed bitstream 420: an entropy decoding stage 502, a dequantization stage 504, an inverse transform stage 506, an intra/inter prediction stage 508, a reconstruction stage 510, a loop filtering stage 512, and a deblocking filtering stage 514. Other structural changes of the decoder 500 can be used to decode the compressed bitstream 420.
When the compressed bitstream 420 is presented for decoding, data elements within the compressed bitstream 420 can be decoded by the entropy decoding stage 502 to produce a set of quantized transform coefficients. The dequantization stage 504 dequantizes the quantized transform coefficients (e.g., by multiplying the quantized transform coefficients by quantizer values), and the inverse transform stage 506 inverse transforms the dequantized transform coefficients using the selected transform type to produce derivative residuals, which can be the same as those created by the inverse transform stage 412 in the encoder 400. Using the header information decoded from the compressed bitstream 420, the decoder 500 can use the intra/inter prediction stage 508 to create the same prediction block as was created in the encoder 400, e.g., at the intra/inter prediction stage 402. At the reconstruction stage 510, the prediction block can be added to the derivative residual to create a reconstructed block. The loop filtering stage 512 can be applied to the reconstructed block to reduce block artifacts. Other filtering can be applied to the reconstructed block. In this example, a deblocking filtering stage 514 is applied to the reconstructed block to reduce block distortion, and the result is output as an output video stream 516. The output video stream 516 can also be referred to as a decoded video stream, and the terms will be used interchangeably herein.
Other variations of the decoder 500 can be used to decode the compressed bitstream 420. For example, the decoder 500 can generate the output video stream 516 without the deblocking filtering stage 514. In some embodiments of the decoder 500, the deblocking filtering stage 514 is applied before the loop filtering stage 512. Additionally or alternatively, the encoder 400 includes a deblocking filtering stage in addition to the loop filtering stage 416.
Fig. 6 is a block diagram representation of a portion 600 of a frame, such as frame 306 of fig. 3, according to an embodiment of the present disclosure. As shown, a portion 600 of a frame includes four 64 x 64 blocks 610 in two rows and two columns in a matrix or cartesian plane, which may be referred to as a super block. The super-block can have a larger or smaller size. Fig. 6 is explained with respect to a superblock size of 64 x 64, and the description is readily extended to larger (e.g., 128 x 128) or smaller superblock sizes.
In an example, and without loss of generality, a superblock can be a basic or largest Coding Unit (CU). Each super block can include four 32 x 32 blocks 620. Each 32 x 32 block 620 can include four 16x16 blocks 630. Each 16x16 block 630 can include four 8x8 blocks 640. Each 8x8 block 640 can include four 4x4 blocks 650. Each 4x4 block 650 can include 16 pixels, which can be represented in four rows and four columns in each respective block in a cartesian plane or matrix. The pixels can include information representing the image captured in the frame, such as luminance information, color information, and position information. In an example, a block such as the 16 × 16 pixel block shown can include: a luminance block 660, luminance block 660 capable of including luminance pixels 662; and two chroma blocks 670/680, such as a U or Cb chroma block 670 and a V or Cr chroma block 680. The chroma block 670/680 can include chroma pixels 690. For example, luma block 660 can include 16 × 16 luma pixels 662 and each chroma block 670/680 can include 8 × 8 chroma pixels 690, as shown. Although one arrangement of blocks is shown, any arrangement can be used. Although fig. 6 illustrates an nxn block, in some embodiments, an nxm block can be used, where N ≠ M. For example, 32 × 64 blocks, 64 × 32 blocks, 16 × 32 blocks, 32 × 16 blocks, or any other size block can be used. In some embodiments, an N × 2N block, a 2N × N block, or a combination thereof can be used.
In some implementations, video coding can include ordered block-level coding. Ordered block-level coding can include coding blocks of a frame in an order, such as a raster scan order, where blocks can be identified and processed starting with the block in the upper left corner of the frame or portion of the frame, and identifying each block in turn for processing, going from left to right along a line and from top to bottom. For example, the superblock in the top row and left column of the frame can be the first block coded, and the superblock immediately to the right of the first block can be the second block coded. The second row from the top can be a coded second row, such that the superblock in the column to the left of the second row can be coded after the superblock in the column to the right of the first row.
In an example, coding the block can include using quadtree coding, which can include coding smaller block units with the block in raster scan order. For example, the 64 × 64 super block shown in the lower left corner of the frame portion shown in fig. 6 can be coded using quad-tree coding, where the upper left 32 × 32 block can be coded, then the upper right 32 × 32 block can be coded, then the lower left 32 × 32 block can be coded, and then the lower right 32 × 32 block can be coded. Each 32 x 32 block can be coded using quadtree coding, where the top left 16x16 block can be coded, then the top right 16x16 block can be coded, then the bottom left 16x16 block can be coded, and then the bottom right 16x16 block can be coded. Each 16x16 block can be coded using quadtree coding, where the top left 8x8 block can be coded, then the top right 8x8 block can be coded, then the bottom left 8x8 block can be coded, and then the bottom right 8x8 block is coded. Each 8x8 block can be coded using quadtree coding, where the top left 4x4 block can be coded, then the top right 4x4 block can be coded, then the bottom left 4x4 block can be coded, and then the bottom right 4x4 block can be coded. In some implementations, 8x8 blocks can be omitted for 16x16 blocks, and 16x16 blocks can be coded using quadtree coding, where the top-left 4x4 block can be coded, and then the other 4x4 blocks in the 16x16 block can be coded in raster scan order.
In an example, video coding can include compressing information included in the original or input frames by omitting some information in the original frames from the corresponding encoded frames. For example, the coding can include reducing spectral redundancy, reducing spatial redundancy, reducing temporal redundancy, or a combination thereof.
In an example, reducing spectral redundancy can include using a color model based on a luminance component (Y) and two chrominance components (U and V or Cb and Cr), which can be referred to as a YUV or YCbCr color model or color space. Using the YUV color model can include representing a luminance component of a portion of a frame using a relatively large amount of information and representing each corresponding chrominance component of a portion of a frame using a relatively small amount of information. For example, a portion of a frame can be represented by a high resolution luminance component, which can comprise a 16 × 16 block of pixels, and also by two lower resolution chrominance components, each representing a portion of a frame as an 8 × 8 block of pixels. The pixels can indicate values (e.g., values ranging from 0 to 255) and can be stored or transmitted using, for example, eight bits. Although the present disclosure is described with reference to a YUV color model, any color model can be used.
Reducing spatial redundancy can include transforming the block to the frequency domain as described above. For example, a unit of an encoder, such as the entropy encoding stage 408 of fig. 4, can perform a DCT using spatial frequency based transform coefficient values.
Reducing temporal redundancy can include using similarities between frames to encode frames using a relatively small amount of data based on one or more reference frames, which can be previously encoded, decoded, and reconstructed frames of a video stream. For example, a block or pixel of the current frame can be similar to a spatially corresponding block or pixel of the reference frame. The blocks or pixels of the current frame can be similar to the blocks or pixels of the reference frame at different spatial locations. As such, reducing temporal redundancy can include generating motion information indicative of spatial differences (e.g., a translation between a location of a block or pixel in a current frame and a corresponding location of a block or pixel in a reference frame).
Reducing temporal redundancy can include identifying a block or pixel in a reference frame or a portion of a reference frame that corresponds to a current block or pixel of a current frame. For example, a reference frame or a portion of a reference frame that can be stored in memory can be searched to find the best block or pixel for encoding the current block or pixel of the current frame. For example, the search may identify a block in the reference frame for which the difference in pixel values between the reference block and the current block is minimized, and can be referred to as a motion search. The portion of the reference frame searched can be limited. For example, the portion of the reference frame being searched that can be referred to as a search area can include a limited number of lines of the reference frame. In an example, identifying the reference block can include calculating a cost function, such as a Sum of Absolute Differences (SAD), between pixels of the block in the search region and pixels of the current block.
As described above, the current block can be predicted using intra prediction. The intra prediction mode uses pixels outside the current block being predicted. The pixels outside the current block are pixels outside the current block. Many different intra prediction modes can be available. Fig. 7 is a diagram of an example of an intra prediction mode.
Some intra-prediction modes use a single value for all pixels within a prediction block generated using at least one peripheral pixel. For example, the VP9 codec includes an intra prediction mode, referred to as a true motion (TM _ PRED) mode, in which all values of a prediction block have a value prediction pixel (x, y) ═ for all x and y (top neighbor + left neighbor-top-left neighbor). For another example, the DC intra prediction mode (DC _ PRED) is such that each pixel of the prediction block is set to the value prediction pixel (x, y) as the average of the entire top row and left column.
Other intra prediction modes, which may be referred to as directional intra prediction modes, enable each to have a corresponding prediction angle.
The intra-prediction mode may be selected by the encoder as part of a rate-distortion loop. In short, various intra prediction modes may be tested to determine which type of prediction has the lowest distortion for a given rate or number of bits to be transmitted in the encoded video bitstream, including additional bits contained in the bitstream to indicate the type of prediction used.
In an example codec, the following 13 intra prediction modes can be available: DC _ PRED, V _ PRED, H _ PRED, D45_ PRED, D135_ PRED, D117_ PRED, D153_ PRED, D207_ PRED, D63_ PRED, SMOOTH _ V _ PRED, SMOOTH _ H _ PRED, and pass _ PRED. One of the 13 intra prediction modes can be used to predict the luminance block.
The intra prediction mode 710 shows a V _ PRED intra prediction mode, which is generally referred to as a vertical intra prediction mode. In this mode, the prediction block pixels of the first column are set to the values of the peripheral pixels a; the prediction block pixel of the second column is set to the value of pixel B; the prediction block pixel of the third column is set to the value of pixel C; and the prediction block pixel in the fourth column is set to the value of pixel D.
The intra prediction mode 720 shows an H _ PRED intra prediction mode, which is generally referred to as a horizontal intra prediction mode. In this mode, the prediction block pixels of the first row are set to the values of the peripheral pixels I; the prediction block pixel of the second row is set to the value of pixel J; the prediction block pixel of the third row is set to the value of pixel K; and the predicted block pixel of the fourth row is set to the value of pixel L.
The intra prediction mode 730 shows the D117_ PRED intra prediction mode, so called because the direction of the arrow along which the peripheral pixels forming the diagonal will propagate to generate the prediction block is at an angle of about 117 ° to the horizontal. That is, in D117_ PRED, the predicted angle is 117 °. The intra prediction mode 740 shows a D63_ PRED intra prediction mode, which corresponds to a prediction angle of 63 °. The intra prediction mode 750 shows a D153_ PRED intra prediction mode, which corresponds to a prediction angle of 153 °. The intra prediction mode 760 illustrates a D135_ PRED intra prediction mode, which corresponds to a prediction angle of 135 °.
The prediction modes D45_ PRED and D207_ PRED (not shown) correspond to the prediction angles 45 ° and 207 °, respectively. DC _ PRED corresponds to a prediction mode in which all prediction block pixels are set to a single value, which is a combination of the peripheral pixels a to M.
In the PAETH _ PRED intra prediction mode, the prediction value of a pixel is determined as follows: 1) calculating a base value as a combination of some peripheral pixels, and 2) using one of the peripheral pixels that is closest to the base value as a predicted pixel. The PAETH _ PRED intra prediction mode is shown using, for example, pixel 712 (at position x-1, y-2). In some examples of peripheral pixel combinations, the base value can be calculated as base ═ B + K-M. I.e. the base value is equal to: the value of the left peripheral pixel on the same row as the pixel to be predicted + the value of the upper peripheral pixel on the same column as the pixel-the value of the pixel in the upper left corner.
In the SMOOTH _ V intra prediction mode, the prediction pixel of the bottommost row of the prediction block is estimated using the value of the last pixel in the left column (i.e., the value of the pixel at position L). The remaining pixels of the prediction block are calculated by quadratic interpolation in the vertical direction.
In SMOOTH _ H intra prediction mode, the predicted pixel of the rightmost column of the predicted block is estimated using the value of the last pixel in the top row (i.e., the value of the pixel at position D). The remaining pixels of the prediction block are calculated by quadratic interpolation in the horizontal direction.
In the SMOOTH _ PRED intra prediction mode, the prediction pixel of the bottom row of the prediction block is estimated using the value of the last pixel in the left column (i.e., the value of the pixel at position L), and the prediction pixel of the right-most column of the prediction block is estimated using the value of the last pixel in the top row (i.e., the value of the pixel at position D). The remaining pixels of the prediction block are calculated as a scaled weighted sum. For example, the value of the prediction pixel at position (i, j) of the prediction block can be calculated as pixel L j 、R、T i And a scaled weighted sum of the values of B. Pixel L j Is in the left side column andand pixels in the same row as the predicted pixels. The pixel R is a pixel provided by SMOOTH _ H. Pixel T i Are pixels in the upper row and in the same column as the predicted pixels. Pixel B is a pixel provided by SMOOTH _ V. The weights can be equivalent to quadratic interpolation in the horizontal and vertical directions.
The intra prediction mode selected by the encoder can be transmitted to the decoder in a bitstream. The intra prediction mode can be entropy coded (encoded by the encoder and/or decoded by the decoder) using the context model.
Some codecs use the intra prediction modes of the left and upper neighboring blocks as context for coding the intra prediction mode of the current block. Using fig. 7 as an example, the left-side neighboring block can be a block including pixels I to L, and the upper-side neighboring block can be a block including pixels a to D.
Diagram 770 illustrates the intra prediction modes available in a VP9 codec. VP9 coding supports a set of 10 intra prediction modes for block sizes ranging from 4x4 to 32 x 32. These intra prediction modes are DC _ PRED, TM _ PRED, H _ PRED, V _ PRED, and 6 tilt direction prediction modes: d45_ PRED, D63_ PRED, D117_ PRED, D135_ PRED, D153_ PRED, D207_ PRED, approximately corresponding to angles 45, 63, 117, 135, 153 and 207 degrees (measured counterclockwise with respect to the horizontal axis).
Fig. 8 is an example of an image portion 800 including rails. Image portion 800 includes a first rail 802 and a second rail 804. In real life, the first rail 802 and the second rail 804 are parallel. However, in image portion 800, first rail 802 and second rail 804 are shown converging at focal point 803 outside of image portion 800.
The current block 806 is superimposed on a portion of the image portion 800 for illustration and for clearer visualization purposes. It is noted that, in general, each position (i.e., monomer) of the current block corresponds to or represents a pixel. However, for purposes of illustration and clarity, each individual of the current block 806 clearly includes significantly more than one pixel. Note also that, although not specifically labeled, each of the first rail 802 and the second rail 804 includes a pair of an inner line and an outer line. The lines in each pair of lines are also parallel and will converge at another focal point in image portion 800.
The unitary body 810 includes a portion of the first track 802. The portion propagates into the current block 806 in the southwest direction. However, the portion of the second track 804 shown in the monomer 812 propagates into the current block 806 in the southeast direction.
As described above, the unidirectional intra prediction mode cannot sufficiently predict the current block 806 from the peripheral pixels 808.
Fig. 9 is an example of a flow diagram of a technique 900 for determining (e.g., selecting, calculating, deriving, etc.) bits along a peripheral pixel line (i.e., an upper peripheral pixel line) and therefore for determining a predicted pixel value according to an embodiment of the present disclosure. The technique 900 for deriving a location is merely an example and other techniques are possible. For a prediction block of size M × N (or equivalently, a current block), the technique 900 computes a block of size M × N (e.g., a two-dimensional array). The two-dimensional array is referred to below as the array POSITIONS.
Given a current block and a set of upper peripheral pixels (i.e., at integer peripheral pixel positions), the technique 900 determines, for each prediction pixel (i.e., or equivalently, each prediction pixel position) of the prediction block, a location along the upper peripheral pixel line from which to derive the value of the prediction pixel. As described further below, the locations along the upper peripheral pixel line can be sub-pixel locations. Thus, the value at that location of the peripheral pixel line can be derived from the upper peripheral pixel (e.g., using interpolation).
The technique 900 can be generalized by resampling (e.g., repeatedly looking at, considering, etc.) a set of top peripheral pixels (e.g., the bitsrels of the peripheral pixels) for each row of the prediction block, while shifting the bitsrels according to one or more parameters of the intra-prediction mode at each resampling.
The location can then be used to generate (e.g., calculate, etc.) a prediction block for the current block. When the technique 900 is implemented by an encoder, the prediction block can be used to determine a residual block, which is then encoded in a compressed bitstream, such as the compressed bitstream 420 of fig. 4. When the technique 900 is implemented by a decoder, the prediction block can reconstruct the current block by, for example, adding the prediction block to a residual block decoded from a compressed bitstream, such as the compressed bitstream 420 of fig. 5.
The peripheral pixel line can include upper peripheral pixels (e.g., pixels a to D, or pixels a to D and M of fig. 7). The peripheral pixels M of fig. 7 can be considered as a part of the upper pixels, a part of the left pixels, or can be referred to individually. In an example, the upper peripheral pixels can include additional pixels (referred to herein as overhanging upper pixels), such as peripheral pixels E-H of fig. 7 and 15. Note that the overhanging left-side pixel is not shown in fig. 7.
The bits computed by the technique 900 can be associated with a one-dimensional array including peripheral pixels. For example, assume that peripheral pixels usable for predicting the current block are pixels a to H of fig. 7. Pixel values a to H can be stored in an array neighbor _ pixels, such as neighbor _ pixels ═ 0,0, a, B, C, D, E, F, G, H. An explanation of why the first two bits of the periph _ pixels array are 0 is provided below.
In an example, the technique 900 may calculate a negative bit position corresponding to a pixel location outside of the block. The negative bit corresponds to the position to the left of the block. In an example, pixel values that are too far from the current pixel are not used as predictors (predictors) for the pixel; while the more recent pixels can be used for prediction. In the case of negative pixel locations, as described further below, pixels from another boundary (e.g., the left peripheral pixel) may be determined to be closer (e.g., better predictor).
As implementation details, the period _ pixels can explain such a case by including empty locations in the array period _ pixels. The periph _ pixels array is shown to include two (2) empty locations (empty _ slots). Thus, if the technique 900 determines the location of a bit of zero (0) (e.g., calculated _ position), then the bit corresponds to the pixel value a (e.g., neighboring _ pixels [ empty _ slots + calculated _ position ] ═ neighboring _ pixels [2+0] ═ a). Similarly, if technique 900 calculates a bitwise of-2, it corresponds to a pixel value periph _ pixels [2+ (-2) ] -0. Similarly, the periph _ pixels array can contain training empty slots.
In an example, the technique 900 can receive as input, or can access, one or more parameters of an intra-prediction mode. The technique 900 can receive the size and width of the current block (or equivalently, the size and width of the prediction block). The technique 900 can also receive one or more of the parameters: horizontal offset (h _ off), horizontal step size (h _ st), horizontal acceleration (h _ acc), vertical offset (v _ off), vertical step size (v _ st), and vertical acceleration (v _ acc). In an example, no received parameter can be equivalent to a zero value of the received parameter. Although the parameters are described below as addends, in some examples, at least some of the parameters can equivalently be multiplicative values.
The horizontal offset (h _ off) parameter is the offset (i.e., positional offset) for each single vertical pixel step. The horizontal offset (h _ off) can answer this question: for a new row (e.g., row-k) of the prediction block, along the line of peripheral pixels, where is located (e.g., relative to) the pixel bit from which the value of the first pixel of the new row is derived compared to the bit of the first pixel of the previous row (e.g., row-k-1)? The horizontal offset can indicate an initial prediction angle. The pass initial prediction angle refers to an angle at which the first pixel of each line of the prediction block is predicted.
The horizontal step (h _ st) parameter is the offset that can be used for the next pixel in the horizontal direction. That is, for a given prediction pixel on a row of the prediction block, the horizontal step (h _ st) indicates the distance to the next bit along the peripheral pixel line relative to the bit of the immediately preceding pixel on the same row. A horizontal step (h _ st) smaller than 1 (e.g., 0.95, 0.8, etc.) enables a reduction in the prediction block. For example, using a horizontal step (h _ st) less than 1, the rails of fig. 8 will move away from each other in the prediction block. Similarly, using a horizontal step (h _ st) greater than 1 (e.g., 1.05, 1.2, etc.) can achieve an amplification effect in the prediction block.
The horizontal acceleration (h _ acc) parameter is a change added to each subsequent horizontal step (h _ st). The horizontal acceleration (h acc) parameter can be used as a shift to constantly move bits farther and farther, rather than stepping from one bit to the next in constant horizontal steps along the line of peripheral pixels. As such, the horizontal acceleration can implement a transformation similar to a homographic transformation.
The vertical offset (v _ off) parameter is the variation of the horizontal offset (h _ off) to be applied to each subsequent row. That is, if h _ off is used to predict the first row of the block, (h _ off + v _ off) is used for the second row, ((h _ off + v _ off) + v _ off) is used for the third row, and so on. The vertical step (v _ st) parameter is the variation of the horizontal step (h _ st) to be applied to each subsequent row of the prediction block. The vertical acceleration (v _ acc) parameter is the variation of the horizontal acceleration (h _ acc) to be applied to each subsequent row of the prediction block.
Note that the accelerations of both in at least one direction (i.e., the horizontal acceleration (h acc) parameter and/or the vertical acceleration (v acc) parameter) implement curve prediction. That is, the acceleration parameter enables curve prediction.
At 902, the technique 900 initializes variables. The variable h _ step _ start can be initialized to the horizontal step parameter: h _ step _ start is h _ st. Each of the variable h _ offset and the variable h _ start can be initialized to a horizontal offset: h _ offset and h _ start are h _ off.
At 904, technique 900 initializes an outer loop variable i. The technique 900 performs 908-920 for each row of the prediction block. At 906, the technique 900 determines whether there are more rows of the prediction block. If there are more rows, technique 900 proceeds to 908; otherwise, technique 900 ends at 922. When technique 900 ends at 922, each pixel location of the predicted location has a corresponding location along the line of peripheral pixels from which a pixel value for each pixel location in the POSITIONS two-dimensional array is calculated (e.g., derived, etc.).
At 908, the technique 900 sets the set variable p to the variable h _ start (i.e., p ═ h _ start); and setting the variable h _ step to the variable h _ step _ start (i.e., h _ step ═ h _ step _ start).
At 910, the technique 900 initializes an inner loop variable j. The technique 900 performs 914 through 918 on each pixel of row i of the prediction block. At 912, the technique 900 determines whether there are more pixel locations (i.e., more columns) for the row. If more columns are present, technique 900 proceeds to 914; otherwise, the technique 900 proceeds to 920 to reset (i.e., update) variables, if any, for the next row of the prediction block.
At 914, the technique 900 sets position (i, j) of the posions array to a bit variable value p (i.e., posions (i, j) ═ p). At 916, technique 900 advances the place variable p to the next level place by adding h _ step to the place variable p (i.e., p ═ h _ step + p). At 918, where there are more unprocessed columns, the technique 900 prepares a variable h _ step for the next column of row i of the prediction block. As such, the technique 900 adds the horizontal acceleration (h _ acc) to the variable h _ step (i.e., h _ step ═ h _ acc + h _ step). From 918, technique 900 returns to 912.
At 920, the technique 900 prepares (i.e., updates) the variables of the technique 900 to prepare the next row of the prediction block, if any. Thus, for the next row (i.e., row ═ i +1), the technique 900 updates h _ start to h _ start ═ h _ offset + h _ start; the vertical offset (v _ off) is added to h _ offset (i.e., h _ offset is equal to v _ off + h _ offset), the vertical step (v _ st) is added to h _ step _ start (i.e., h _ step _ start is equal to v _ st + h _ step _ start), and the vertical acceleration (v _ acc) is added to h _ acc (i.e., h _ acc is equal to v _ acc + h _ acc).
FIG. 10 is an example 1000 of a bitplace (i.e., array POSITIONS) computed by the techniques of FIG. 9 according to an embodiment of the present disclosure. Example 1000 is generated for the following inputs: the current block size is 8 × 8, the horizontal offset h _ off is-0.2, the horizontal step h _ st is 1.05, the horizontal acceleration h _ acc is 0, the vertical offset v _ off is 0, the vertical step v _ st is 0.06, and the vertical acceleration v _ acc is 0. Example 1000 shows values for two-dimensional array POSITIONS as described above.
Example 1000 shows, for each position (i, j) of a prediction block, the position of the peripheral pixel line from which the prediction for position (i, j) should be derived, where i is 0, …, columns-1, j is 0, …, rows-1. Predictor positions 1002-1008 illustrate examples of bitwise values for example 1000. The predictor position 1002 corresponding to the position (3,1) of the prediction block is the prediction value to be derived from the bit position 2.93 of the peripheral prediction line. The predictor position 1004, corresponding to the position (6,4) of the prediction block, is the prediction value to be derived from the bit positions 6.74 of the peripheral prediction line. The predictor position 1006, corresponding to the position (0,1) of the prediction block, is the prediction value to be derived from the-0.4 position of the peripheral prediction line. The predictor position 1008, corresponding to the position (0,6) of the prediction block, is the prediction value to be derived from the bit position-1.4 of the peripheral prediction line.
Fig. 11 is an example 1100 of a prediction block computed from the example 1000 of fig. 10. The example 1100 includes a prediction block 1102, which may be visualized as a prediction block 1104. The prediction block 1102 (and, equivalently, the prediction block 1104) is derived (e.g., generated, calculated, etc.) using the example 1000 of fig. 10 and the bits of the peripheral prediction pixels 1106, which may be the top (i.e., upper) peripheral pixels. The peripheral prediction pixels 1106 can be, for example, peripheral pixels a to H of fig. 7. Peripheral predicted pixels 1108 are visualizations of peripheral predicted pixels 1106.
In the visualization, a pixel value of zero (0) corresponds to a black square, and a pixel value of (255) corresponds to a white square. Pixel values between 0 and 255 correspond to different shades of gray squares. As such, the luminance block is shown as an example of an intra prediction mode according to the present disclosure. However, the present disclosure is not limited thereto. The disclosed techniques are also applicable to chroma blocks or any other color component blocks. In general, the techniques disclosed herein are applicable to any prediction block to be generated, which may be of any size mxn, where M and N are positive integers.
As described above and as illustrated in the example 1000 of fig. 10, the bits computed by the technique 900 of fig. 9 can be non-integer bits of a peripheral pixel line (i.e., sub-pixel bits). The pixel values of the peripheral pixel lines in the non-integer bits will be derived (e.g., computed, etc.) from the available integer pixel bit values (i.e., peripheral pixels) such as peripheral predicted pixels 1106.
Many techniques can be available for calculating the sub-pixel location (i.e., the pixel value at which the sub-pixel location is located). For example, a multi-tap (e.g., 4-tap, 6-tap, etc.) Finite Impulse Response (FIR) filter can be used. For example, an average value of surrounding pixels can be used. For example, bilinear interpolation can be used. For example, a 4-pixel bicubic interpolation of peripheral pixels (e.g., top row or left column) can be used. For example, a convolution operation can be used. The convolution operation can use pixels other than the peripheral pixels. In an example, a convolution kernel of size N × N may be used. Thus, N rows (columns) of the upper (left) neighboring block can be used. To illustrate, a 4-tap filter or 4 × 1 convolution kernel with weights (-0.1, 0.6, -0.1) can be used. Thus, the calculated pixel value at the bit between pixel1 and pixel2 of the set of four pixels (pixel0, pixel1, pixel2, pixel3) can be calculated as a clomp (-0.10 pixel0+0.6 pixel1+0.6 pixel2-0.10 pixel3,0,255), where the clomp () operation sets the calculated value less than zero to zero and the calculated value greater than 255 to 255.
The prediction block 1102 illustrates the use of bilinear interpolation. For sub-pixel locations of a peripheral pixel line, the nearest two integer pixels are found. The predictor value is calculated as a weighted sum of the nearest two integer pixels. The weights are determined according to the distance of the sub-pixel bits from the two integer pixel bits.
Given the bit position pos of the peripheral pixel line, the pixel value pix _ val at pos can be calculated as pix _ val-left _ weight × left _ pixel + right _ weight × right _ pixel. left _ pixel is the pixel value of the nearest left adjacent integer pixel of bit position pos. Right _ pixel is the pixel value of the nearest right adjacent integer pixel of bit position pos.
The bits of left _ pixel can be computed as left _ pos ═ floor (pos), where floor () is a function that returns the largest integer less than or equal to pos. Thus, floor (6.74) ═ 6. Bit positions 6.74 are the positions for predictor positions 1004 of FIG. 10. The bit of right _ pixel can be computed as right _ pos — ceiling (pos), where ceiling () is a function that returns the smallest integer greater than or equal to pos. Therefore, ceiling (6.74) ═ 7.
In an example, right _ weight can be calculated as right _ weight-pos-left _ pos and left _ weight can be calculated as left _ weight-1-right _ weight. Thus, for bit position 6.74 of predictor position 1004, right _ weight-6-0.74 and left _ weight-1-0.74-0.26. The pixel values 1112 of the prediction block 1102 are the values derived from the predictor positions 1004 of fig. 10. In this manner, the pixel value 1112 is calculated as ((255 × 26) + (0 × 0.74)) -66.
Similarly, predictor value 1110 is calculated from predictor position 1006 (i.e., -0.4) of FIG. 10. Thus left _ pos and right _ pos are-1 and 0, respectively. Right _ weight and left _ weight are 0.6 and 0.4, respectively. The right neighboring pixel value is the peripheral pixel value of the peripheral predicted pixel 1106 at position 0. Thus, the right adjacent pixel value is the pixel 1114 having a value of 0. The left neighboring pixel is not available. Therefore, the left adjacent pixel value is 0. In this manner, the predictor value 1110 is calculated as ((0 × 0.4) + (0 × 0.6)) ═ 0.
How the parameters (i.e., parameter values) of the intra prediction mode are used is discussed above with respect to fig. 10 and 11. There are any number of ways to select the parameter values.
In an example, the mode selection process of the encoder can test all possible parameter values to find the optimal combination of parameter values that yields the smallest residual. In an example, the minimum residual can be the residual that yields the best rate-distortion value. In an example, the minimum residual can be the residual that yields the minimum residual error. The minimum residual error can be a mean square error. The minimum residual error can be the sum of the absolute difference errors. Any other suitable error measure can be used. In addition to the indication of the intra prediction mode itself, the encoder is also able to encode parameter values for an optimal combination of parameter values in the encoded bitstream. The decoder is able to decode the parameter values of the optimal combination of parameter values. In an example, as described herein, the indication of the intra-prediction mode itself can be a number (e.g., an integer) that instructs a decoder to perform intra-prediction of the current block using intra-prediction parameters.
Testing all possible values of each parameter may be an impractical solution. As such, a limited number of values can be selected for each parameter and combinations of the limited number of values tested.
For example, the horizontal offset (h _ off) parameter can be selected from a limited range of values. In an example, the limited range of values can be [ -4, +4 ]. The step value can be used to select a horizontal offset (h _ off) parameter value to test within a limited range. In an example, the step size can be 0.25 (or some other value). As such, values of-4, -3.75, -3.5, -3.25, …, 3.75, 4 can be tested. In an example, the vertical offset (v _ off) parameter can be selected similarly to the horizontal offset parameter.
As for the horizontal step (h _ st) and the vertical step (v _ st), a value relatively close to 1 can be selected. Any other value can result in too fast a scaling of the prediction block. Therefore, values in the range of [0.9,1.1] can be tested for the horizontal step (h _ st) and the vertical step (v _ st). However, in general, the horizontal step (h _ st) and the vertical step (v _ st) can each be selected from the range of [ -4,4] using a step value that can be 0.25. The selected horizontal acceleration (h _ acc) and vertical acceleration (v _ acc) parameter values can be close to 0. In an example, the horizontal acceleration (h _ acc) and vertical acceleration (v _ acc) parameter values can each be 0 or 1. More generally, the horizontal parameter and the corresponding vertical parameter can have values and/or ranges of values.
In another example, the encoder can select the parameter value based on the best set of parameter values possible. The set of possible optimal parameter values is also referred to herein as predicted parameter values. The best set of parameter values possible can be derived by predicting the peripheral pixels from their neighbors. That is, in the case of the upper peripheral pixels, the peripheral pixels constitute the bottommost row of the previous reconstruction block; and in the case of the left peripheral pixels, the peripheral pixels constitute the rightmost column of the previous reconstruction block. Thus, adjacent rows, columns, or both (as the case may be) of peripheral pixels can be used as predictors for the peripheral pixels. Since the predictors of the peripheral pixels and the peripheral pixels are known per se, the parameter values can be derived from them. In this case, the encoder does not need to encode the best possible set of parameter values in the compressed bitstream, since the decoder can perform a procedure exactly similar to the encoder to derive the best possible set of parameter values. Therefore, all that the encoder needs to encode in the bitstream is an indication of the intra prediction mode itself.
In another example, the differential parameter values can be encoded by an encoder. For example, as described above, the encoder can derive the optimal parameter values, and also, as described above, can derive the best set of parameter values possible (i.e., prediction parameter values). Then, in addition to the indication of the intra prediction mode, the encoder encodes the respective differences between the best parameter values and the set of possible best parameter values. That is, for example, with respect to the horizontal offset (h _ off), the encoder can derive the best horizontal offset (opt _ h _ off) and the best horizontal offset (predicted _ h _ offset) possible. The encoder then encodes the difference (opt _ h _ off-predicted _ h _ offset).
FIG. 12 is a flow diagram of a technique 1200 for intra prediction of a current block in accordance with an embodiment of the present disclosure. The intra prediction mode uses pixels surrounding the current block. The pixels peripheral to the current block can be previously predicted pixels in the same video frame or image as the current block. The current block can be a luma block, a chroma block, or any other color component block. The size of the current block may be mxn, where M and N are positive integers. In an example, M is equal to N. In an example, M is not equal to N. For example, the size of the current block can be 4 × 4,4 × 8, 8 × 4, 8 × 8, 16 × 16, or any other current block size. Technique 1200 generates a prediction block for a current block that is the same size as the current block. The technique 1200 can be implemented in an encoder, such as the encoder 400 of fig. 4. The technique 1200 can be implemented in a decoder, such as the decoder 500 of fig. 5.
The technique 1200 can be implemented using specialized hardware or firmware. Some computing devices can have multiple memories, multiple processors, or both. The steps or operations of technique 1200 can be distributed using different processors, memories, or both. The use of the terms "processor" or "memory" in the singular includes computing devices having one processor or one memory as well as devices having multiple processors or multiple memories that can be used to perform some or all of the described steps.
At 1202, the technique 1200 selects peripheral pixels of the current block. The peripheral pixels are used to generate a prediction block for the current block. In an example, the peripheral pixels can be pixels above the current block. In an example, the peripheral pixels can be pixels to the left of the current block. In an example, the peripheral pixels can be a combination of the upper and left side pixels of the current block. When implemented by a decoder, selecting peripheral pixels can include reading (e.g., decoding) an indication (e.g., syntax elements) from the compressed bitstream that indicates which peripheral pixels are to be used.
For each location (i.e., pixel location) of the prediction block, the technique 1200 performs 1206 through 1208. Thus, if the size of the current block is M × N, the prediction block can include M × N pixel positions. As such, at 1204, the technique 1200 determines whether more pixel positions of the prediction block have not been performed 1206 through 1208. If there are more pixel locations, the technique 1200 proceeds to 1206; otherwise, technique 1200 proceeds to 1210.
At 1206, the technique 1200 selects two respective pixels of the peripheral pixels for the pixel locations of the prediction block. In an example, the technique 1200 first selects bits along a continuous peripheral pixel line along which peripheral pixels are integer pixel locations. At 1208, the technique 1200 computes a prediction pixel (i.e., pixel value) by interpolating two corresponding pixels for the pixel location of the prediction block.
In an example, selecting bits along successive lines of peripheral pixels can be as described with respect to fig. 9. Thus, selecting two respective pixels of the peripheral pixels can include selecting a first two respective pixels of the peripheral pixels for computing a first prediction pixel of the prediction block and selecting a second two respective pixels of the peripheral pixels for computing a second prediction pixel of the prediction block. The second predicted pixel can be a horizontal neighbor of the first predicted pixel. The first two respective pixels and the second two respective pixels can be selected according to an intra prediction mode parameter.
As described above, the intra prediction mode parameters can include at least two of a horizontal offset, a horizontal step size, or a horizontal acceleration. In an example, the intra prediction mode parameters can include a horizontal offset, a horizontal step size, and a horizontal acceleration. As described above, the horizontal offset can indicate an initial predicted angle; the horizontal step size can be used as a subsequent offset for subsequent predicted pixels of the same row; and the horizontal acceleration can indicate a change in the horizontal offset added to each subsequent horizontal step.
In an example, the horizontal offset can be selected from a limited range based on the step value. In an example, the limited range can be-4 to 4. In an example, the step value can be 0.25. In an example, the horizontal (vertical) step size can be selected from a range of-4 to 4 based on the step size value. The step value can be 0.25 or some other value. In an example, the horizontal (vertical) acceleration can be 0. In another example, the horizontal (vertical) acceleration can be 1.
As further described above, the intra-prediction mode parameters can also include at least two of a vertical offset, a vertical step size, or a vertical acceleration. In an example, the intra prediction mode parameters can include a vertical offset, a vertical step size, and a vertical acceleration. The vertical offset can indicate a first variation of the horizontal offset to be applied to each subsequent row of the prediction block. The vertical step can indicate a second variation of the horizontal step to be applied to each subsequent row of the prediction block. The vertical acceleration can indicate a third variation of the horizontal acceleration to be applied to each subsequent row of the prediction block.
In an example, calculating the predicted pixel by interpolating two respective pixels can include calculating the predicted pixel using bilinear interpolation.
At 1210, the technique 1200 codes a residual block corresponding to a difference between the current block and the prediction block. When implemented by an encoder, the technique 1200 encodes the residual block in a compressed bitstream. When implemented by a decoder, the technique 1200 decodes a residual block from a compressed bitstream. The decoded residual block can be added to the prediction block to reconstruct the current block.
When implemented by a decoder, the technique 1200 can also include decoding intra-prediction mode parameters from the compressed bitstream. In another example, and as described above, the intra prediction mode parameters can be derived by predicting peripheral pixels from previously reconstructed other pixels including the peripheral pixels according to the intra prediction mode parameters.
FIG. 13 is a flow diagram of a technique 1300 for generating a prediction block for a current block using intra prediction according to an embodiment of the present disclosure. Intra-prediction mode uses pixels peripheral to the current block, which can be as described with respect to the technique 1200 of fig. 12. The current block can be as described with respect to the technique 1200 of fig. 12. The technique 1300 can be implemented in an encoder, such as the encoder 400 of fig. 4. The technique 1300 can be implemented in a decoder, such as the decoder 500 of fig. 5.
The technique 1300 can be implemented using specialized hardware or firmware. Some computing devices can have multiple memories, multiple processors, or both. The steps or operations of technique 1300 can be distributed using different processors, memories, or both. The use of the terms "processor" or "memory" in the singular includes computing devices having one processor or one memory, as well as devices having multiple processors or multiple memories that are used to perform some or all of the described steps.
At 1302, the technique 1300 determines peripheral pixels for generating a prediction block for the current block. Determining peripheral pixels can mean selecting which peripheral pixel to use, such as described with respect to 1202 of fig. 12. Peripheral pixels can be considered as integer pixel positions along a peripheral pixel line (i.e., a continuous line of peripheral pixels).
At 1304, technique 1300 determines, for each pixel of the prediction block, a respective sub-pixel location of the peripheral pixel line. Determining the corresponding sub-pixel location can be as described with respect to fig. 9. Sub-pixel positions of a peripheral pixel line as used herein also include integer pixel positions. That is, for example, the determined sub-pixel location can be the location of one of the peripheral pixels themselves.
At 1306, for each prediction pixel of the prediction block, the technique 1300 computes the prediction pixel as an interpolation of integer pixels of peripheral pixels corresponding to respective sub-pixel positions of each prediction pixel. In an example, the interpolation can be an interpolation of the nearest integer pixel, as described above with respect to fig. 11. In an example, bilinear interpolation can be used. In another example, filtering of integer pixels can be performed to obtain prediction pixels of a prediction block.
In an example, determining the respective sub-pixel locations for the peripheral pixels of each pixel of the prediction block can include, for each row of pixels of the first row of the prediction block, determining the respective sub-pixel locations using parameters including at least two of a horizontal offset, a horizontal step, or a horizontal acceleration. As described above, the horizontal offset can indicate the initial predicted angle. As described above, the horizontal step size can be used as a subsequent offset for subsequent predicted pixels of the same row. As described above, the horizontal acceleration can indicate the change in the horizontal offset added to each subsequent horizontal step.
In an example, determining the respective sub-pixel position of the peripheral pixels of each pixel of the prediction block can include determining the respective sub-pixel position of each pixel of the second row of the prediction block, wherein the parameters further include at least two of a vertical offset, a vertical step, or a vertical acceleration. As described above, the vertical offset can indicate a first variation of the horizontal offset to be applied to each subsequent row of the prediction block. As described above, the vertical step can indicate the second variation of the horizontal step to be applied to each subsequent row of the prediction block. As described above, the vertical acceleration can indicate a third variation of the horizontal acceleration to be applied to each subsequent row of the prediction block.
In an example, and when implemented by a decoder, the technique 1300 can include decoding parameters from a compressed bitstream. In an example, decoding the parameters from the compressed bitstream can include, as described above, decoding the parameter difference; obtaining a prediction parameter value; and for each parameter, adding the respective parameter difference to the respective predicted parameter value.
As described above, in the embodiment, the initial prediction block can be generated using the directional prediction mode. Warping (e.g., warping functions, warping parameter sets, parameters, etc.) can then be applied to the initial prediction block to generate the prediction block.
In an example, the warping parameter can be derived using the current block and the initial prediction block. Any number of techniques can be used to derive the warp parameter. For example, a random sample consensus (RANSAC) method can be used to fit a model (i.e., warp model, parameters) to the matching points between the current block and the initial prediction block. RANSAC is an iterative algorithm that can be used to estimate the warping parameter (i.e., parameter) between two blocks. In an example, the best matching pixels between the current block and the initial prediction block can be used to derive the warp parameter. The warp can be a homographic warp, an affine warp, a similar warp, or some other warp.
Homographic warping can use eight parameters to project some pixels of the current block to some pixels of the initial prediction block. Homographic warping is not constrained by a linear transformation between two spatial coordinates. As such, the eight parameters defining the homographic warping can be used to project the pixels of the current block onto the quadrilateral portion of the initial prediction block. Thus, homographic warping supports translation, rotation, scaling, aspect ratio changes, shearing, and other non-parallelogram warping.
Affine warping uses six parameters to project pixels of the current block to some pixels of the initial prediction block. Affine warping is a linear transformation between two spatial coordinates defined by six parameters. As such, the six parameters defining affine warping can be used to project the pixels of the current block to a parallelogram that is part of the initial prediction block. Thus, affine warping supports translation, rotation, scaling, aspect ratio change, and shearing.
Similar warping uses four parameters to project the pixels of the current block to the pixels of the initial prediction block. Similar warping is a linear transformation between the coordinates of two spaces defined by four parameters. For example, the four parameters can be a translation along the x-axis, a translation along the y-axis, a rotation value, and a scaling value. As such, four parameters defining a similar model can be used to project the pixels of the current block to the square of the initial prediction block. Thus, similar warping supports a square-to-square transformation with rotation and scaling.
In an example, warped parameters can be sent from the encoder to the decoder in the compressed bitstream in addition to the directional intra prediction mode. The decoder can generate an initial prediction block using a directional intra-prediction mode. The decoder can then decode the current block using the parameters of the transmitted parameters.
In another example, the parameters can be derived by a decoder. For example, as described above, the decoder can use previously decoded pixels to determine parameters for warping. For example, as described above, the warping parameter can be determined by predicting peripheral pixels using pixels of the same block as the peripheral pixels. That is, since the peripheral pixels are known, an optimum warping parameter can be determined to predict the peripheral pixels from neighboring pixels of the peripheral pixels.
In another example, the differential warp parameter can be sent by an encoder, as described above. For example, the predicted warp parameters can be derived using neighboring pixels of the peripheral parameters, and the optimal warp parameters are derived as described above. The difference between the optimal warp parameter and the predicted warp parameter can be sent in a compressed bitstream.
As described above, at least with respect to fig. 9 to 11, the case where only the upper peripheral pixels are used is. For example, in the case where other peripheral pixels (e.g., the left peripheral pixel) are unavailable, only the upper peripheral pixel can be used. For example, even when the left peripheral pixel is available, only the upper peripheral pixel can be used.
Using the left peripheral pixels can be similar to using the upper peripheral pixels. In an example, only the left peripheral pixel can be used if the upper peripheral pixel is not available. In another example, even when the upper peripheral pixels are available, only the left peripheral pixels can be used.
Fig. 18 is an example of a flow diagram of a technique 1800 for determining bits along a left peripheral pixel line and therefore determining a predicted pixel value, according to an embodiment of the present disclosure. For a prediction block of size M × N (or equivalently, a current block), technique 1800 computes a block of size M × N (e.g., a two-dimensional array). The two-dimensional array is referred to below as the array POSITIONS.
Given the current block and the set of left peripheral pixels (i.e., at integer peripheral pixel positions), technique 1800 determines, for each predicted pixel of the predicted block (i.e., or, equivalently, each predicted pixel position), a location along the left peripheral pixel line from which to derive the value of the predicted pixel. As described further below, the locations along the left peripheral pixel line can be sub-pixel locations. Thus, the value at that location of the left peripheral pixel line can be derived from the left peripheral pixel (e.g., using interpolation).
The technique 1800 can be generalized such that for each column of the prediction block, a set of left peripheral pixels (e.g., bits of the left peripheral pixels) are resampled (e.g., repeatedly viewed, considered, etc.), while at each resampling, bits are shifted according to one or more parameters of the intra prediction mode. The location can then be used to generate (e.g., calculate, etc.) a prediction block for the current block.
A detailed description of technique 1800 is omitted because technique 1800 is very similar to technique 900. In technique 1800, the effects (i.e., uses) of horizontal offset (h _ off), horizontal step (h _ st), horizontal acceleration (h _ acc), vertical offset (v _ off), vertical step (v _ st), and vertical acceleration (v _ acc) are opposite to their effects in technique 900. That is, wherever the horizontally-related parameters are used in process 900, the corresponding vertical parameters are used instead in process 1800, and vice versa. Accordingly, 1802 to 1822 can be similar to 902 to 922, respectively. It should also be noted that while the outer iteration (at 906) of technique 900 iterates over rows of the prediction block and the inner iteration (at 912) iterates over columns of the prediction block, in technique 1800 the outer iteration (at 1906) iterates over columns of the prediction block and the inner iteration (at 1812) iterates over rows of the prediction block.
FIG. 19 is a flow diagram of a technique 1900 for generating a prediction block for a current block using intra prediction according to an embodiment of the present disclosure. Technique 1900 uses both upper and left peripheral pixels to generate a prediction block. The current block can be as described with respect to the technique 1200 of fig. 12. The technique 1900 can be implemented in an encoder, such as the encoder 400 of fig. 4. The technique 1900 can be implemented in a decoder, such as the decoder 500 of fig. 5.
The technique 1900 can be implemented using specialized hardware or firmware. Some computing devices can have multiple memories, multiple processors, or both. The steps or operations of technique 1900 can be distributed using different processors, memory, or both. The use of the terms "processor" or "memory" in the singular includes computing devices having one processor or one memory, as well as devices having multiple processors or multiple memories that can be used to perform some or all of the described steps.
The technique 1900 is illustrated with reference to fig. 20A and 20B. 20A-20B are illustrated with the following inputs: the current block size of 8 × 8, the horizontal offset h _ off is 0.25, the horizontal step h _ st is 1, the horizontal acceleration h _ acc is 1, the vertical offset v _ off is 4, the vertical step v _ st is 1, and the vertical acceleration v _ acc is 0.
At 1902, the process 1900 selects a first peripheral pixel of the current block. The first peripheral pixel is along a first edge of the current block. The first peripheral pixel is selected as a primary pixel to generate a prediction block. In an example, the first peripheral pixel can be an upper peripheral pixel. In an example, the first peripheral pixel can be a left peripheral pixel. As described further below, the primary pixel is the pixel that is calculated with respect to its bits along the first peripheral pixel line, as described with respect to technique 900 (in the case where the first peripheral pixel is an upper peripheral pixel) or technique 1800 (in the case where the first peripheral pixel is a left peripheral pixel).
When implemented by an encoder, technique 1900 can be performed once using the top peripheral pixels as the first peripheral pixels and a second time using the left peripheral pixels as the first peripheral pixels. The encoder can test both to determine which provides a better prediction of the current block. In an example, the technique 1900 can send a first indication in a compressed bitstream, such as the compressed bitstream 420 of fig. 4, which of the first peripheral pixels or the second peripheral pixels is to be used as a primary pixel for generating the prediction block. That is, for example, when the upper (left) peripheral pixel is used as the main pixel, the first indication may be a bit value of 0 (1). Other values of the first indication are possible.
Thus, when implemented by a decoder, the technique 1900 can select the first peripheral pixel by decoding the first indication from the compressed bitstream. In another example, the decoder can derive whether the first peripheral pixel is an upper peripheral pixel or a left peripheral pixel by predicting the upper row (left column) from its adjacent upper row (left column) according to the techniques described herein.
For each location (i.e., pixel location) of the prediction block, technique 1900 performs 1906 through 1910. Thus, if the size of the current block is M × N, the prediction block can include M × N pixel positions. As such, at 1904, the technique 1900 determines whether there are more pixel positions of the prediction block that have not been performed 1906 to 1910. If there are more pixel locations, technique 1900 proceeds to 1906; otherwise, technique 1900 proceeds to 1910.
At 1906, the technique 1900 determines a first intercept along a first continuous line that includes first peripheral pixels at respective integer positions. Thus, in the case where the first pixel bit is the upper peripheral pixel, then the first intercept is the y-axis intercept; and, in the case where the first pixel bit is the left peripheral pixel, then the first intercept is the x-axis intercept.
Fig. 20A shows a bitfield 2010 of the values of the two-dimensional array POSITIONS described above when the upper peripheral pixel is used as the first peripheral pixel main pixel. Bitfield 2010 can be computed using technique 900. Thus, the bit locations 2010 provide bit locations in the upper peripheral row.
Example fig. 20B shows a bitfield 2050 of values of a two-dimensional array positons when the left peripheral pixel is used as the first peripheral pixel primary pixel. The bitfield 2050 can be computed using the technique 1800. Thus, the bit locations 2050 provide bit locations within the left peripheral column.
At 1908, the technique 1900 uses the location of each predicted pixel and the first intercept to determine a second intercept along a second continuous line including a second peripheral pixel. The second peripheral pixels are along a second edge of the current block that is perpendicular to the first edge.
In the case where the first predicted pixel is an upper predicted pixel, then the second predicted pixel can be a left predicted pixel. In the case where the first predicted pixel is a left predicted pixel, then the second predicted pixel can be an above predicted pixel. Other combinations of first and second predicted pixels are also possible. For example, combinations of top-right peripheral pixels, bottom-right peripheral pixels, or bottom-left peripheral pixels are possible.
In an example, the second intercept can be calculated by connecting a line between the predicted pixel position and the first intercept and extending the line to a second continuous line.
Fig. 16, described further below, is now used as an illustration. If the current predicted pixel is predicted pixel 1604 and the first intercept is y-intercept 1648, the second intercept can be obtained by connecting predicted pixel 1604 and y-intercept 1648 and extending the line to the x-axis. Thus, the second intercept is the x intercept 1646. Similarly, if the current predicted pixel is predicted pixel 1604 and the first intercept is x-intercept 1646, the second intercept can be obtained by connecting predicted pixel 1604 and x-intercept 1646 and extending the line toward the y-axis. Thus, the second intercept is y-intercept 1648.
Block 2012 of fig. 20A shows the x-intercept when the first peripheral pixel is the top peripheral pixel. Block 2052 of fig. 20B illustrates the y-intercept when the first peripheral pixel is the left peripheral pixel. The intercept can be calculated using one of the formulas of equation (1).
To illustrate, consider the predicted pixel 2010A at the predicted position (1, 3). Thus, considering the top and left predicted pixels, predicted pixel 2010A is located at position (2,4) in a coordinate system having an origin at which the top and left peripheral pixels intersect. The first intercept (i.e., the y-intercept) is at 7.5. Therefore, a line can be formed by two points (2,4) and (0, 7.5). Thus, as shown by the x-intercept value 2012A, the x-intercept can be calculated as (-7.5/((7.5-4)/(0-2))) -4.29.
As another example, the prediction pixel 2050A at position (1,1) of the prediction block is at position (2,2) of the coordinate system including the top and left peripheral pixels. The first intercept (i.e., the x intercept) is at 11. Therefore, a line can be formed by two points (2,2) and (11, 0). Thus, as shown by the y-intercept value 2052A, the y-intercept can be calculated as (-11 x (0-2)/(11-2)) -2.44.
At 1910, technique 1900 computes a value for each predicted pixel using at least one of the first and second intercepts.
In an example, the value of the predicted pixel can be calculated as a weighted sum of the first and second intercepts. More particularly, the value of the predicted pixel can be calculated as a weighted sum of a first pixel value at a first intercept and a second pixel value at a second intercept. The weight can be inversely proportional to a distance from the location of the predicted pixel to each of the first and second intercepts. In this way, the value of the predicted pixel can be obtained as a bilinear interpolation of the first pixel value and the second pixel value. As is well known, the distance between two points (a, b) and (c, d) can be calculated as
In another example, the value of the predicted pixel can be calculated based on the closest distance rather than a weighted sum. That is, the closer of the x-intercept and the y-intercept to the predicted pixel (based on the calculated distance) can be used to calculate the value of the predicted pixel.
Thus, for the predicted pixel at position (1,4), the weights of the x-intercept and the y-intercept, using a weighted sum, are (4.61/(4.61+4.03)) -0.53 and (1-0.53) -0.47, respectively. The pixel values for each of the x-intercept and the y-intercept can be calculated as described above (e.g., as an interpolation of the two nearest integer pixel values).
In the case of using the closest distance, the predicted pixel value at the predicted block position (1,4) can be calculated using only the pixel at the y-intercept, because 4.03 (i.e., distance 2014A) is less than 4.61 (i.e., distance 2016A).
As described below with respect to fig. 14, in some cases, the x-intercept or the y-intercept may not exit (e.g., be negative). As also described with respect to fig. 14, in such a case, the predicted value can only be calculated based on the available intercept values.
In an encoder, the technique 1900 can select one of a closest distance, a weight, and or some other function to generate a prediction block. In an example, the technique 1900 can generate a respective prediction block for each possible function and select one of the prediction blocks that yields the optimal prediction. As such, the technique 1900 can encode a second indication of the selected function in the compressed bitstream to combine the first peripheral pixels and the second peripheral pixels to obtain the prediction block. As mentioned, this function can be a weighted sum, a nearest distance, or some other function.
When implemented by a decoder, the technique 1900 receives, in a compressed bitstream, a second indication of a function for combining first peripheral pixels and second peripheral pixels to obtain a prediction block. The decoder is used to generate a prediction block.
Fig. 21 shows an example 2100 of the technique of fig. 19 when an upper peripheral pixel is used as the primary peripheral pixel. An example prediction block generated using the left peripheral block as the main peripheral pixel is not shown.
Example 2100 illustrates different prediction blocks that can be generated using peripheral pixels. The prediction block of example 2100 is generated using upper peripheral pixels 2104 (visualized using upper pixels 2104), left peripheral pixels 2106 (visualized using left pixels 2108), or a combination thereof. As described above, the upper left peripheral pixel 2110 can be the origin of the coordinate system.
The prediction block 2120 (visualized as prediction block 2122) shows that only the primary peripheral pixels (i.e., the upper peripheral pixels) are used. Thus, even if the left-side peripheral pixels are available, the prediction block 2120 is generated as described with respect to fig. 10 and 11.
The prediction block 2130 (visualized as prediction block 2132) shows that only non-primary peripheral pixels (i.e., left peripheral pixels) are used. Thus, the prediction block 2130 can be generated by using the primary peripheral pixels (i.e., upper peripheral pixels) to acquire the bits within the upper row, as described with respect to bits 2010 of fig. 20A. Acquiring an x-intercept, as described with respect to block 2012 of fig. 20A; and the predicted pixel values are calculated using the x-intercept in a similar manner as described with respect to the prediction block 1102 of fig. 11.
The prediction block 2140 (visualized as prediction block 2142) shows a prediction block generated using a weighted sum, as described above with respect to fig. 19. Prediction block 2150 (visualized as prediction block 2152) shows a prediction block generated using the closest distance, as described above with respect to fig. 19.
As described above, the intra prediction mode according to the present disclosure can be defined with respect to the focus. The focus can be defined as all points of the prediction diverging from that point. That is, each point of the prediction block can be considered to be connected to the focal point. Similarly, the focal point can be considered as a point in the distance where the parallel lines of the fluoroscopic image intersect.
FIG. 14 is a flow diagram of a technique 1400 for coding a current block using intra prediction mode according to an embodiment of the present disclosure. The current block is coded using focus. The intra prediction mode uses pixels surrounding the current block. The current block can be as described with respect to the technique 1200 of fig. 12. The technique 1400 can be implemented in an encoder, such as the encoder 400 of fig. 4. The technique 1400 can be implemented in a decoder, such as the decoder 500 of fig. 5.
The technique 1400 can be implemented using specialized hardware or firmware. Some computing devices can have multiple memories, multiple processors, or both. The steps or operations of technique 1400 can be distributed using different processors, memories, or both. The use of the terms "processor" or "memory" in the singular includes computing devices having one processor or one memory as well as devices having multiple processors or multiple memories that can be used to perform some or all of the described steps.
The technique 1400 can be best understood with reference to fig. 15 and 16.
Fig. 15 is an example 1500 illustrating a focal point according to an embodiment of the present disclosure. Example 1500 shows a current block 1502 to be predicted. That is, a prediction block will be generated for the current block 1502. The current block 1502 has a width 1504 and a height 1506. As such, the size of the current block 1502 is W × H. In example 1500, the current block is shown as 8x 4. However, the present disclosure is not limited thereto. The current block can have any size. For ease of illustration, the pixel locations are shown as squares in example 1500. The specific value of the pixel can be more correctly considered as the value located in the middle (i.e. the center) of the square.
The peripheral pixels will be used to predict the current block 1502. The peripheral pixels can be upper peripheral pixels 1508 or can include upper peripheral pixels 1508. Peripheral pixels can be left peripheral pixels 1512 or can include left peripheral pixels 1512. The top peripheral pixels 1508 can include a number of pixels equal to the width 1504 (W). Left peripheral pixels 1512 can include a number of pixels equal to height 1506 (H). For convenience or reference, top left peripheral pixel 1509 can be considered a portion of left peripheral pixel 1512, a portion of top peripheral pixel 1508, or a portion of both left peripheral pixel 1512 and top peripheral pixel 1508.
The upper peripheral pixels 1508 can include overhanging upper peripheral pixels 1510. The number of overhanging top peripheral pixels 1508 is denoted as W 0 . In example 1500, W 0 Shown as being equal to 8 pixels. However, the present disclosure is not limited thereto, and the overhanging upper peripheral pixel can include any number of pixels.
Left peripheral pixel 1512 can include an overhanging left peripheral pixel 1514. The number of overhanging left peripheral pixels 1514 is denoted as H 0 . In example 1500, H 0 Shown as equal to 2 pixels. However, the present disclosure is not so limited, and the overhanging left peripheral pixel 1514 can include any number of pixels.
Although left peripheral pixel 1512 is a discrete pixel, left peripheral pixel 1512 can be considered a pixel value at an integer position of a continuous line of peripheral pixels. Thus, the left peripheral pixels 1512 (e.g., the first peripheral pixels) form a first peripheral pixel line that constitutes the x-axis 1530. Although the upper peripheral pixels 1508 are discrete pixels, the upper peripheral pixels 1508 can be considered to be pixel values at integer positions of a continuous line of peripheral pixels. Thus, the upper peripheral pixels 1508 (e.g., second peripheral pixels) form a second line of peripheral pixels that make up the y-axis 1532.
Three illustrative pixels of the current block 1502 are shown: a pixel 1518 located in the top right corner of the current block 1502, a pixel 1522 located in the bottom left corner of the current block 1502, and a pixel 1526. Each of the pixels 1518, 1522, 1526 can be considered to have a coordinate (i, j), where the center can be located at the top left corner of the current block. Thus, pixel 1518 is located at coordinate (7, 0), pixel 1522 is located at coordinate (0, 3), and pixel 1526 is located at coordinate (5, 3).
The focal point 1516 is shown outside and at a distance from the current block. Focal point 1516 is at coordinate (a, b) in a coordinate system centered on the intersection between x-axis 1530 and y-axis 1532.
As described above, each pixel of the current block 1502 diverges from the focal point 1516. Thus, line 1520 connects pixel 1518 with focus 1516, line 1524 connects pixel 1522 with focus 1516, and line 1528 connects pixel 1526 with focus 1516.
X intercept x of line 1520 0 (i.e., where line 1520 intersects the x-axis 1530) is point 1534, and the y-intercept y of line 1520 0 (i.e., where line 1520 intersects y-axis 1532) is point 1535. X intercept x of line 1524 0 (i.e., where line 1524 intersects the x-axis 1530) is point 1536, and the y-intercept y of line 1524 0 (i.e., where line 1524 intersects y-axis 1532) is point 1537. X intercept x of line 1528 0 (i.e., where line 1528 intersects the x-axis 1530) is point 1538, the y-intercept y of line 1528 0 (i.e., where line 1528 intersects y-axis 1532) is point 1539.
It is well known that given two points of a straight line with coordinates (a, b) and (i, j), the x and y intercepts can be calculated using equation (1)
Fig. 16 is an example illustrating an x-intercept and a y-intercept in accordance with an embodiment of the present disclosure. The example of FIG. 16 shows the positive and negative x-and y-intercepts of a predicted pixel 1604 (or equivalently, the current pixel) at location (i, j) of the current block 1612, given the different bits of focus.
Example 1600 shows a focal point 1602 and a line 1606 passing through (e.g., connecting) a predicted pixel 1604 to the focal point 1602. The x-intercept 1608 is negative. The y-intercept 1610 is positive. Example 1620 illustrates a focal point 1622 and a line 1624 through (e.g., connecting) prediction pixel 1604 to the focal point 1622. The x-intercept 1626 is positive. y-intercept 1628 is negative. Example 1640 shows a focus 1642 and a line 1644 that passes (e.g., connects) predicted pixel 1604 to focus 1642. The x-intercept 1646 is positive. y-intercept 1648 is positive.
Returning again to fig. 14. At 1402, the technique 1400 acquires a focus. As described with respect to fig. 15, the focal point has coordinates (a, b) in the coordinate system.
When implemented by a decoder, obtaining focus can include decoding intra-prediction modes from the compressed bitstream. The compressed bitstream can be the compressed bitstream 420 of fig. 5. The intra prediction mode can indicate a focus.
In an example, each of the available intra-prediction modes can be associated with an index (e.g., a value). Decoding an index from a compressed bitstream instructs a decoder to perform intra prediction of a current block according to an intra prediction mode (i.e., the semantics of the intra prediction mode). In an example, the intra prediction mode can indicate coordinates of the focus. For example, the intra prediction mode value 45 may indicate that the focus is located at coordinates (-1000 ), the intra prediction mode value 46 may indicate that the focus is located at coordinates (-1000, -850), and so on. Thus, for example, if 64 foci are possible, 64 intra prediction modes are possible, each intra prediction mode indicating the location of the focus. In an example, hundreds of foci (and equivalently, intra-prediction modes) are available. Although the position of the focus is given herein in cartesian coordinates, the focus coordinates can be given in polar coordinates. The angle of the polar coordinates can be about an x-axis, such as x-axis 1530 of FIG. 15.
In another example, obtaining the focal point can include decoding coordinates of the focal point from the compressed bitstream. For example, the compressed bitstream can include an intra-prediction mode indicating intra-prediction using a focus followed by focus coordinates.
When implemented by an encoder, acquiring the focus can include selecting the focus from a plurality of candidate focuses. That is, the encoder selects an optimal focus point for encoding the current block. The optimal focus point can be a focus point of optimal encoding that results in the current block. In an example, the plurality of candidate foci can be divided into a group of candidate foci. Each candidate focal group can be arranged on the circumference of a respective circle. In an example, each candidate focal group can include 16 candidate foci.
Fig. 17 illustrates an example 1700 of a focal group according to an embodiment of the present disclosure. As described above, there may be hundreds of focus candidates, which can be located anywhere in space outside the current block. The focus candidates are a subset of all possible focuses in the space outside the current block. The space outside the current block can be centered (i.e., have an origin) at an upper-left peripheral pixel, such as upper-left peripheral pixel 1509 of FIG. 15. The center of the space can be any other point. In an example, the center can be a center point of the current block.
To limit the search space, only a subset of all possible foci can be considered as candidate foci. There can be many ways to narrow the search space to candidate foci. In an example, the candidate foci can be grouped into groups. Each candidate focal group can be arranged on the circumference of a circle. In an example, three circles can be considered. These three circles can be frame 1702, frame 1704, and frame 1706. The focal points are shown as black circles on each frame (such as focal points 1708-1712). However, any number of circles can be useful. Each circle (or frame) roughly corresponds to the slope (e.g., convergence rate) of the line connecting the predicted pixel to the focal point on the circumference of the circle. Note that the example 1700 is merely illustrative and is not drawn to scale.
The frame 1706 can correspond to a nearby focal point. As such, given a focus on frame 1702, lines from the focus to each predicted pixel location can appear to spread out. Thus, the slope of the lines can be very different. The frame 1706 can have a radius in the range of tens of pixels. For example, the radius can be 20, 30, or some other such number of pixels.
The frame 1704 can correspond to a circle having a medium-sized radius. The radius of the frame 1704 can be in the range of hundreds of pixels.
As not mentioned above, while a circle (frame) can have an impractical number of focal points, only a sample of the focal points is used as a candidate focal point. The candidate foci for each group (i.e., on each frame) can be equally spaced. For example, assuming that N (e.g., 8, 16, etc.) candidate foci are included in each group, the N foci can be separated by 360/N (e.g., 45, 22.5, etc.) degrees.
In an example, acquiring the focus at 1402 in the encoder can include testing each candidate focus to identify a best focus. In another example, the outermost optimal focus of the outermost frame (frame 1702) can be identified by performing intra prediction using each focus of the outermost frame. Foci corresponding to the outermost optimal focus (e.g., at the same angle) can then be tried to determine if any of them produced a better prediction block. Other heuristics can also be used. For example, a binary search can be used.
Returning to fig. 14. At 1404, the technique 1400 can generate a prediction block using the first peripheral pixel and the second peripheral pixel. The first peripheral pixel can be a left peripheral pixel, such as left peripheral pixel 1512 (including left upper peripheral pixel 1509). The second peripheral pixel can be an upper peripheral pixel 1508.
As described above, the first peripheral pixels form the first peripheral pixel lines constituting the x-axis, such as the x-axis 1530 of fig. 15; the second peripheral pixels form a second peripheral pixel line constituting a y-axis, for example, the y-axis 1532 of fig. 15; and the first peripheral pixel line and the second peripheral pixel line form a coordinate system having an origin. Generating the prediction block can include performing 1404_4 to 1404_6 on each position (i.e., each pixel) of the prediction block. Each pixel of the prediction block is located at position (i, j). If the block size is M × N, then M × N times 1404_4 through 1404_6 are performed.
At 1404_2, the technique 1400 determines whether there are any more prediction block locations for which pixel values have not been determined (e.g., calculated). If there are more pixel locations, the technique 1400 proceeds to 1404_ 4; otherwise, technique 1400 proceeds to 1406.
At 1404_4, the technique 1400 can determine (e.g., calculate, identify, etc.) at least one of an x-intercept or a y-intercept of the predicted pixel at (i, j).
The x-intercept is a first point (e.g., x-intercept 1608, x-intercept 1626, x-intercept 1646) at which a line (e.g., line 1606, line 1624, line 1644) formed by a point (e.g., predicted pixel 1604) and a focus (e.g., focus 1602, focus 1622, focus 1642) centered on each location of the prediction block intersects a first line of peripheral pixels (e.g., the x-axis).
The y-intercept is a second point (e.g., y-intercept 1609, y-intercept 1627, y-intercept 1647) at which a line (e.g., line 1606, line 1624, line 1644) formed by a point (e.g., predicted pixel 1604) and a focus (e.g., focus 1602, focus 1622, focus 1642) centered at each location of the prediction block intersects a second peripheral pixel line (e.g., y-axis).
The x and/or y intercept can be calculated using equation (1). However, in some cases, a line passing through the predicted pixel and the focal point may not intercept one of the axes. For example, a line nearly parallel to the x-axis may not be considered to intercept the x-axis; and a line nearly parallel to the y-axis may not be considered to intercept the y-axis. When b ═ j + epsilon, the line may not be considered to intercept the x-axis; and when a is i + epsilon, the line may not be considered to intercept the y-axis, where epsilon is a small threshold near zero. As such, the x-intercept can be identified as i and the y-intercept can be identified as j without using equation (1).
At 1404_6, the technique 1400 can determine a predicted pixel value for each location (i.e., (i, j)) of the predicted block using at least one of an x-intercept or a y-intercept. From 1404_6, the technique 1400 returns to 1404_ 2.
In an example, determining the predicted pixel value can include, where one of the at least one of the x-intercept or the y-intercept is a negative value, determining the predicted pixel value for each location using the other of the at least one of the x-intercept or the y-intercept. For example, with respect to the example 1600 of fig. 16, since the x-intercept 1608 is a negative value, the predicted pixel values for the location (i, j) of the predicted block are calculated using only the y-intercept 1610, which is a positive value. For example, with respect to the example 1620 of fig. 16, since the y-intercept 1628 is a negative value, the predicted pixel values for the location (i, j) of the predicted block are calculated using only the x-intercept 1626, which is a positive value.
In an example, determining the predicted pixel value can include, where the x-intercept is positive and the y-intercept is positive, determining the predicted pixel value for each location as a weighted combination of a first pixel value at the x-intercept and a second pixel value at the y-intercept.
In an example, determining the prediction pixel value can include, in the case where i is equal to a (i.e., i is very close to a), setting a pixel value at each position of the prediction block to a value of a first peripheral pixel located at bit i of the first peripheral pixel line among the first peripheral pixels. That is, if i ≈ a, p (i, j) is set to L [ i ]. That is, if the line is almost parallel to the y-axis, the predicted pixel value p (i, j) is set to the horizontally corresponding left peripheral pixel value L [ i ].
Similarly, in an example, determining the prediction pixel value can include, in a case where j is equal to b (i.e., j is very close to b), setting a pixel value at each position of the prediction block to a value of a second peripheral pixel located at bit j of the second peripheral pixel line among the second peripheral pixels. That is, if j ≈ b, p (i, j) is set to T [ i ]. That is, if the line is nearly parallel to the x-axis, the predicted pixel value p (i, j) is set to the vertically corresponding upper (i.e., top) peripheral pixel value T [ j ].
In an example, determining the prediction pixel value can include, in a case where the x-intercept is zero and the y-intercept is zero, setting a pixel value at each position of the prediction block to a pixel value located at an intersection of the first peripheral pixel line and the second peripheral pixel line. That is, if the x-intercept and the y-intercept are zero, the predicted pixel value can be set to the upper left peripheral pixel value.
The pseudo code of table I shows an example of setting a prediction pixel value p (I, j) at position (I, j) of a prediction block using the focus at (a, b), where I ═ 0.,. width-1 and j ═ 0, …, height-1.
As described above, for a given pixel (i, j), a line connecting the focal point at (a, b) and (i, j) is drawn. The x-intercept and the y-intercept are calculated. Depending on these intercepts, the predicted pixel values p (i, j) are obtained by interpolating or extrapolating the intercept values from the top or left boundary pixels.
In particular, let L [ k ]]An array of left boundary pixels (e.g., left peripheral pixel 1512 plus top-left peripheral pixel 1509 of fig. 15) representing a positive integer position k, where k is 0,1, …, H + H 0 (ii) a And T [ k ]]The top boundary pixel array representing a positive integer position k (e.g., the top peripheral pixel 1508 plus the top left peripheral pixel 1509 of fig. 15), where k is 0,1, …, W + W 0 . Note T [0]]＝L[0]. And also make f L (z) and f T (z) denotes the pixel L k from the boundary by suitable interpolation (i.e. plug-in function)]、T[k]The interpolation function at the high precision (real value) point z is obtained. Note that at integer positions, for z-k-0, 1 0 ，f L (z)＝L[k]And for z-k-0, 1, a, W + W0, f T (z)＝T[k]。
In row 1 of table I, if the focus and the predicted pixel at (I, j) are on the same horizontal line, the predicted pixel p (I, j) is set to the value L [ I ] of the left peripheral pixel located on the same horizontal line in row 2. More particularly, the focus and prediction pixels may not be fully horizontally aligned. Therefore, (i ═ a) can mean that a line connecting the focal point and the prediction pixel passes through a square centered at L [ i ].
In line 3, if the focus and the prediction pixel are on the same vertical line, then in line 4, the prediction pixel p (i, j) is set to the value T [ j ] of the upper peripheral pixel located on the same vertical line. More particularly, the focus and prediction pixels may not be perfectly vertically aligned. Thus, (j ═ b) can mean that the line connecting the focal point and the prediction pixel passes through a square centered at T [ j ].
In lines 6 to 7, the x-intercept (x) is calculated according to equation (1) 0 ) And y intercept (y) 0 ). On line 8, if x intercept (x) 0 ) And y intercept (y) 0 ) At the origin, thenOn line 9, the prediction pixel p (i, j) is set to the upper left peripheral pixel L [0]]. More particularly, the x intercept (x) 0 ) And/or y intercept (y) 0 ) May not be completely zero. Thus, x 0 ＝＝0&&y 0 By 0 can be meant that the x-and y-intercepts are within a square (i.e., pixel) centered at the origin.
On line 10, if x intercept (x) 0 ) Is positive but y intercept (y) 0 ) Is negative, then in line 11, the predicted pixel is taken from only the x intercept (x) 0 ) Using an interpolation function f L And (4) obtaining. On line 12, if x intercept (x) 0 ) Is negative and has a y-intercept (y) 0 ) Is positive, then on line 13, the predicted pixel (pi, j) is only from the y-intercept (y) 0 ) Using an interpolation function f T And (6) obtaining.
On line 14, if x intercept (x) 0 ) And y intercept (y) 0 ) Are all positive values, the prediction pixel p (i, j) is calculated as the x-intercept (x) 0 ) Interpolation (i.e. f) L (x 0 ) And y intercept (y) 0 ) Interpolation (i.e. f) T (y 0 ) ) are combined. Which weight is greater depends on the x-intercept (x) 0 ) And y intercept (y) 0 ) Which is farther from the predicted pixel p (i, j). If x intercept (x) 0 ) Further (i.e., row 15), at row 16, the y-intercept (y) 0 ) Is more heavily weighted. On the other hand, if y intercept (y) 0 ) Further (i.e., row 17), at row 18, the x intercept (x) 0 ) The weight is greater. Rows 20 through 21 are for completeness, and are intended to cover an x intercept (x) 0 ) And y intercept (y) 0 ) Both are negative cases, which is not possible.
In some cases, at least some of the top peripheral pixels or the left peripheral pixels may not be available. For example, the current block can be a block located at the top edge of the image or the left edge of the image. In such a case, the unusable peripheral pixels can be considered to have a value of zero.
Interpolation function f L And f T Can be any interpolation function. The interpolation functions can be the same interpolation function or different interpolation functions. The interpolation function can be as described above with respect to fig. 9. Example (b)For example, the interpolation function can be a Finite Impulse Response (FIR) filter. For example, as described above, the interpolation filter can be a bilinear interpolation. That is, given an x-intercept (or y-intercept) value, the nearest integer pixel can be determined, and a weighted sum of the nearest integer pixels can be used in bilinear interpolation. As used herein, interpolation includes both interpolation and extrapolation.
Returning to FIG. 14, at 1406, the technique 1400 codes a residual block corresponding to a difference between the current block and the prediction block. When implemented by an encoder, the technique 1400 computes a residual block as the difference between the current block and the predicted block, and encodes the residual block in a compressed bitstream. When implemented by a decoder, the technique 1400 encodes the residual block by decoding the residual block from the compressed bitstream. The decoder can then add the residual block to the prediction block to reconstruct the current block.
Another aspect of the disclosed embodiments is a technique for encoding a current block. The technique includes obtaining a prediction block of prediction pixels of a current block using peripheral pixels. Each prediction pixel is located at a corresponding position (i, j) within the prediction block. Acquiring the prediction block can include acquiring a focus having coordinates (a, b) in a coordinate system; and, for each position of the prediction block, obtaining a line indicating a corresponding prediction angle, and determining a pixel value for each position using the line. As described above, this line connects the focal point to each location. As described with respect to fig. 15-17, the focal point can be outside the current block, and the focal point may be, but may not be, one of the peripheral pixels. As described above, each prediction pixel of the prediction block can have a prediction angle different from that of each other prediction pixel of the prediction angle. Although more than one prediction pixel can have the same prediction angle depending on the position of the focus, the intra prediction mode according to the embodiment of the present disclosure is such that not all prediction pixels can have the same prediction angle.
Encoding the current block can include encoding an intra-prediction mode indicating a focus in the compressed bitstream. As described above, the value associated with the intra prediction mode can indicate the position of the focus.
As described above, the peripheral pixels can include a left peripheral pixel and a top peripheral pixel. Determining the pixel value for each location using the lines can include: determining an x-intercept of the line; determining a y-intercept of the line; and determining a pixel value using the x-intercept and the y-intercept. The x-intercept is the first point at which the line intersects the left axis comprising the left peripheral pixels of the current block. The y-intercept is the second point at which the line intersects the top axis comprising the top peripheral pixels of the current block.
Acquiring the focus, as described above and with respect to fig. 17, can include selecting the focus from a plurality of candidate focuses. The plurality of focus candidates can be divided into focus group candidates. Each candidate focal group can be arranged on the circumference of a respective circle.
Another aspect of the disclosed embodiments is a technique for decoding a current block. The technology comprises the following steps: decoding the focus from the compressed bitstream; acquiring a prediction block of a prediction pixel of a current block; and reconstructing the current block using the prediction block. Acquiring the prediction block includes: for each position of the prediction block, obtaining a line representing a respective prediction angle, wherein the line connects the focus to each position; and determining a pixel value for each location using the line.
Determining the pixel value for each location using the line can include: determining the x intercept of the line; determining a y-intercept of the line; and determining a pixel value using the x-intercept and the y-intercept. The x-intercept is the first point at which the line intersects the left axis comprising the left peripheral pixels of the current block. The y-intercept is the second point at which the line intersects the top axis comprising the top peripheral pixels of the current block.
Determining the pixel value using the x-intercept and the y-intercept can include determining the pixel value as a weighted combination of a first pixel value at the x-intercept and a second pixel value at the y-intercept. Determining the pixel value using the x-intercept and the y-intercept can include determining the pixel value using one of the x-intercept or the y-intercept if the other is a negative value.
Decoding the focus from the compressed bitstream can include decoding an intra-prediction mode from the compressed bitstream that indicates the focus.
For simplicity of explanation, each of the techniques 900, 1200, 1300, 1400, 1800, and 1900 are depicted and described as a series of blocks, steps, or operations. However, blocks, steps or operations according to the present disclosure can occur in various orders and/or concurrently. Further, other steps or operations not presented and described herein may be used. Moreover, not all illustrated steps or operations may be required to implement techniques in accordance with the subject matter of this disclosure.
The above encoding and decoding aspects illustrate some encoding and decoding techniques. It should be understood, however, that encoding and decoding, as those terms are used in the claims, may refer to compression, decompression, transformation, or any other data processing or variation.
The word "example" or "embodiment" is used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as "exemplary" or "embodiment" is not necessarily to be construed as preferred or advantageous over other aspects or designs. Rather, use of the words "example" or "embodiment" are intended to present concepts in a concrete fashion. As used in this application, the term "or" is intended to mean an inclusive "or" rather than an exclusive "or". That is, unless specified otherwise, or clear from context, "X includes a or B" is intended to mean any of the natural inclusive permutations. That is, if X comprises A; x comprises B; or X includes both A and B, then either of the foregoing examples satisfies "X includes A or B". In addition, the articles "a" and "an" as used in this application and the appended claims should generally be construed to mean "one or more" unless specified otherwise or clear from context to be directed to a singular form. Furthermore, unless so described, the use of the term "embodiment" or "one embodiment" throughout does not denote the same embodiment or embodiment.
Transmitting station 102 and/or receiving station 106 (as well as algorithms, methods, instructions, etc. stored thereon and/or executed thereby, including algorithms, methods, instructions, etc. executed by encoder 400 and decoder 500) may be implemented in hardware, software, or any combination thereof. The hardware can include, for example, a computer, an Intellectual Property (IP) core, an Application Specific Integrated Circuit (ASIC), a programmable logic array, an optical processor, a programmable logic controller, microcode, a microcontroller, a server, a microprocessor, a digital signal processor, or any other suitable circuitry. In the claims, the term "processor" should be understood to include any of the foregoing hardware, alone or in combination. The terms "signal" and "data" can be used interchangeably. Furthermore, portions of transmitting station 102 and receiving station 106 need not be implemented in the same manner.
Further, in an aspect, for example, transmitting station 102 or receiving station 106 can be implemented using a computer or processor having a computer program that, when executed, performs any of the respective methods, algorithms, and/or instructions described herein. Additionally, or alternatively, for example, a special purpose computer/processor can be used that can contain other hardware for performing any of the methods, algorithms, or instructions described herein.
Transmitting station 102 and receiving station 106 can be implemented, for example, on computers in a video conferencing system. Alternatively, transmitting station 102 can be implemented on a server and receiving station 106 can be implemented on a device separate from the server, such as a handheld communication device. In this example, transmitting station 102 is capable of encoding content into an encoded video signal using encoder 400 and transmitting the encoded video signal to a communication device. The communication device can then decode the encoded video signal using the decoder 500. Alternatively, the communication device can decode content stored locally on the communication device, e.g., content not transmitted by transmitting station 102. Other transmitter station 102 and receiving station 106 embodiments are available. For example, the receiving station 106 can be a generally stationary personal computer, rather than a portable communication device and/or a device that includes the encoder 400 and possibly the decoder 500.
Furthermore, all or portions of embodiments of the present disclosure can take the form of a computer program product accessible from, for example, a tangible computer-usable or computer-readable medium. A computer-usable or computer-readable medium can be, for example, any device that can tangibly contain, store, communicate, or transport the program for use by or in connection with any processor. The medium can be, for example, an electronic, magnetic, optical, electromagnetic, or semiconductor device. Other suitable media are also available.
The above-described embodiments, implementations, and aspects have been described in order to allow easy understanding of the present disclosure and do not limit the present disclosure. On the contrary, the disclosure is intended to cover various modifications and equivalent arrangements included within the scope of the appended claims, which scope is to be accorded the broadest interpretation so as to encompass all such modifications and equivalent structures as is permitted under the law.
Claims (20)
1. A method of coding a current block using an intra prediction mode, comprising:
acquiring a focal point having coordinates (a, b) in a coordinate system;
generating a prediction block for the current block using first peripheral pixels and second peripheral pixels,
wherein the first peripheral pixels form a first peripheral pixel line constituting an x-axis,
wherein the second peripheral pixels form a second peripheral pixel line constituting a y-axis,
wherein the first peripheral pixel line and the second peripheral pixel line form the coordinate system having an origin, an
Wherein generating the prediction block comprises:
for each position of the prediction block at a position (i, j) of the prediction block, determining at least one of an x-intercept or a y-intercept,
wherein the x-intercept is a first point at which a line formed by a point centered on the each position of the prediction block and the focus intersects the first peripheral pixel line, and
wherein the y-intercept is a second point at which a line formed by the point centered at the each position of the prediction block and the focus intersects the second peripheral pixel line; and
determining a prediction pixel value for each location of the prediction block using the at least one of the x-intercept or the y-intercept; and
coding a residual block corresponding to a difference between the current block and the prediction block.
2. The method of claim 1, wherein determining the predicted pixel value for each location of the prediction block using the at least one of the x-intercept or the y-intercept comprises:
determining the predicted pixel value for each location using the other of the at least one of the x-intercept or the y-intercept on a condition that the other of the at least one of the x-intercept or the y-intercept is a negative value.
3. The method of claim 2, wherein determining the predicted pixel value for each location of the prediction block using the at least one of the x-intercept or the y-intercept further comprises:
determining the predicted pixel value for each location as a weighted combination of a first pixel value at the x-intercept and a second pixel value at the y-intercept on a condition that the x-intercept is positive and the y-intercept is positive.
4. The method of claim 1, wherein determining the predicted pixel value for each location of the prediction block using the at least one of the x-intercept or the y-intercept comprises:
setting a pixel value at each position of the prediction block to a value of a first peripheral pixel of the first peripheral pixels located at bit i of the first peripheral pixel line on a condition that i is equal to a.
5. The method of claim 1, wherein determining the predicted pixel value for each location of the prediction block using the at least one of the x-intercept or the y-intercept comprises:
setting a pixel value at each position of the prediction block to a value of a second peripheral pixel of the second peripheral pixels located at bit j of the second peripheral pixel line on a condition that j is equal to b.
6. The method of claim 1, wherein determining the predicted pixel value for each location of the prediction block using the at least one of the x-intercept or the y-intercept comprises:
setting a pixel value at each position of the prediction block to a pixel value at an intersection of the first peripheral pixel line and the second peripheral pixel line on a condition that the x-intercept is zero and the y-intercept is zero.
7. The method of claim 1, wherein acquiring the focal point comprises:
decoding the intra-prediction mode from a compressed bitstream, wherein the intra-prediction mode indicates the focus.
8. The method of claim 1, wherein acquiring the focal point comprises:
the focus is selected from a plurality of candidate focuses.
9. The method of claim 8, wherein the plurality of candidate foci is divided into candidate focal groups, wherein each candidate focal group is arranged on a circumference of a respective circle.
10. The method of claim 9, wherein each of the groups of candidate foci comprises 16 candidate foci.
11. An apparatus for decoding a current block, comprising:
a memory; and
a processor configured to execute instructions stored in the memory for:
decoding the focus from the compressed bitstream;
obtaining a prediction block of prediction pixels of the current block, wherein each prediction pixel is located at a respective position within the prediction block, wherein obtaining the prediction block comprises:
for each location of the prediction block, executing instructions to:
obtaining a line indicative of a respective predicted angle, the line connecting the focal point to the each location; and
determining a pixel value for said each location using said line; and
reconstructing the current block using the prediction block.
12. The apparatus of claim 11, wherein determining the pixel value for the each location using the line comprises:
determining an x-intercept of the line, wherein the x-intercept is a first point at which the line intersects a left-side axis comprising left-side peripheral pixels of the current block;
determining a y-intercept of the line, wherein the y-intercept is a second point at which the line intersects a top axis comprising top peripheral pixels of the current block; and
determining the pixel value using the x-intercept and the y-intercept.
13. The apparatus of claim 12, wherein determining the pixel value using the x-intercept and the y-intercept comprises:
determining the pixel value as a weighted combination of a first pixel value at the x-intercept and a second pixel value at the y-intercept.
14. The apparatus of claim 12, wherein determining the pixel value using the x-intercept and the y-intercept comprises:
determining the pixel value using one of the x-intercept or the y-intercept on a condition that the other of the x-intercept or the y-intercept is negative.
15. The apparatus of claim 11, wherein decoding the focal point from the compressed bitstream comprises:
decoding an intra-prediction mode from the compressed bitstream, wherein the intra-prediction mode indicates the focus.
16. A method for encoding a current block, comprising:
obtaining a prediction block of prediction pixels of the current block using peripheral pixels, wherein each prediction pixel is located at a respective position within the prediction block, and wherein obtaining the prediction block comprises:
obtaining a focal point having coordinates (a, b) in a coordinate system, the focal point being outside the current block and the focal point not being any of the peripheral pixels;
for each position of the prediction block, performing steps comprising:
obtaining a line indicative of a respective predicted angle, the line connecting the focal point to the each location; and
determining a pixel value for said each location using said line; and
the focal point is encoded in a compressed bitstream.
17. The method of claim 16, wherein encoding the focal point in the compressed bitstream comprises:
encoding an intra prediction mode indicating the focus in the compressed bitstream.
18. The method of claim 16, wherein the peripheral pixels comprise a left peripheral pixel and a top peripheral pixel, and wherein determining the pixel value for the each location using the line comprises:
determining an x-intercept of the line, wherein the x-intercept is a first point at which the line intersects a left-side axis comprising the left-side peripheral pixel of the current block;
determining a y-intercept of the line, wherein the y-intercept is a second point at which the line intersects a top axis comprising the top peripheral pixels of the current block; and
determining the pixel value using the x-intercept and the y-intercept.
19. The method of claim 16, wherein acquiring the focal point comprises:
the focus is selected from a plurality of candidate focuses.
20. The method of claim 19, wherein the plurality of candidate foci is divided into candidate focal groups, wherein each candidate focal group is arranged on a circumference of a respective circle.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202062976001P | 2020-02-13 | 2020-02-13 | |
US62/976,001 | 2020-02-13 | ||
PCT/US2020/032862 WO2021162724A1 (en) | 2020-02-13 | 2020-05-14 | Intra prediction for image and video compression |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115004703A true CN115004703A (en) | 2022-09-02 |
Family
ID=70861578
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202080093804.4A Pending CN115004703A (en) | 2020-02-13 | 2020-05-14 | Intra prediction for image and video compression |
Country Status (5)
Country | Link |
---|---|
US (2) | US20230058845A1 (en) |
EP (2) | EP4074046A1 (en) |
JP (1) | JP7480319B2 (en) |
CN (1) | CN115004703A (en) |
WO (2) | WO2021162724A1 (en) |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20220191482A1 (en) * | 2020-12-16 | 2022-06-16 | Tencent America LLC | Method and apparatus for video coding |
US20240022709A1 (en) * | 2022-07-13 | 2024-01-18 | Tencent America LLC | Improvements on model storage for warp extend and warp delta modes |
Family Cites Families (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20060098735A1 (en) * | 2004-11-10 | 2006-05-11 | Yu-Chung Chang | Apparatus for motion estimation using a two-dimensional processing element array and method therefor |
CN102595120A (en) * | 2011-01-14 | 2012-07-18 | 华为技术有限公司 | Airspace predication coding method, decoding method, device and system |
US10354364B2 (en) * | 2015-09-14 | 2019-07-16 | Intel Corporation | Automatic perspective control using vanishing points |
US20170347094A1 (en) * | 2016-05-31 | 2017-11-30 | Google Inc. | Block size adaptive directional intra prediction |
CN109792515B (en) * | 2016-08-01 | 2023-10-24 | 韩国电子通信研究院 | Image encoding/decoding method and apparatus, and recording medium storing bit stream |
EP3301931A1 (en) * | 2016-09-30 | 2018-04-04 | Thomson Licensing | Method and apparatus for omnidirectional video coding with adaptive intra prediction |
US10225578B2 (en) * | 2017-05-09 | 2019-03-05 | Google Llc | Intra-prediction edge filtering |
EP3639517B1 (en) * | 2017-06-14 | 2021-02-24 | Huawei Technologies Co., Ltd. | Intra-prediction for video coding using perspective information |
EP3562158A1 (en) * | 2018-04-27 | 2019-10-30 | InterDigital VC Holdings, Inc. | Method and apparatus for combined intra prediction modes |
-
2020
- 2020-05-14 JP JP2022548760A patent/JP7480319B2/en active Active
- 2020-05-14 EP EP20728884.6A patent/EP4074046A1/en active Pending
- 2020-05-14 EP EP20729576.7A patent/EP4074047A1/en active Pending
- 2020-05-14 US US17/793,618 patent/US20230058845A1/en active Pending
- 2020-05-14 CN CN202080093804.4A patent/CN115004703A/en active Pending
- 2020-05-14 WO PCT/US2020/032862 patent/WO2021162724A1/en unknown
- 2020-05-14 WO PCT/US2020/032857 patent/WO2021162723A1/en unknown
- 2020-05-14 US US17/793,620 patent/US20230050660A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
WO2021162723A1 (en) | 2021-08-19 |
US20230050660A1 (en) | 2023-02-16 |
US20230058845A1 (en) | 2023-02-23 |
EP4074047A1 (en) | 2022-10-19 |
EP4074046A1 (en) | 2022-10-19 |
JP7480319B2 (en) | 2024-05-09 |
JP2023514215A (en) | 2023-04-05 |
WO2021162724A1 (en) | 2021-08-19 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
Han et al. | A technical overview of AV1 | |
US10165283B1 (en) | Video coding using compound prediction | |
US9866863B1 (en) | Affine motion prediction in video coding | |
CN107027038B (en) | Dynamic reference motion vector coding mode | |
JP6605726B2 (en) | Motion vector partitioning of previous frame | |
US11039131B2 (en) | Intra-prediction for smooth blocks in image/video | |
US10116957B2 (en) | Dual filter type for motion compensated prediction in video coding | |
KR20190117671A (en) | Video Coding Techniques for Multiview Video | |
US10506249B2 (en) | Segmentation-based parameterized motion models | |
CN113491129B (en) | Adaptive filter intra prediction mode in image/video compression | |
US11115678B2 (en) | Diversified motion using multiple global motion models | |
CN110268716B (en) | Equivalent rectangular object data processing by spherical projection to compensate for distortion | |
CN110741638B (en) | Motion vector coding using residual block energy distribution | |
CN110741641B (en) | Method and apparatus for video compression | |
CN110169059B (en) | Composite Prediction for Video Coding | |
CN115004703A (en) | Intra prediction for image and video compression | |
Han et al. | A technical overview of av1 | |
EP3744101A1 (en) | Adaptive temporal filtering for alternate reference frame rendering | |
WO2023287417A1 (en) | Warped motion compensation with explicitly signaled extended rotations | |
Storch et al. | Exploring ERP Distortions to Reduce the Encoding Time of 360 Videos | |
WO2021219416A1 (en) | Triangulation-based adaptive subsampling of dense motion vector fields |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |