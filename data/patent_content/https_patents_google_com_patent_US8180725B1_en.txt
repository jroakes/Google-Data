US8180725B1 - Method and apparatus for selecting links to include in a probabilistic generative model for text - Google Patents
Method and apparatus for selecting links to include in a probabilistic generative model for text Download PDFInfo
- Publication number
- US8180725B1 US8180725B1 US12/176,621 US17662108A US8180725B1 US 8180725 B1 US8180725 B1 US 8180725B1 US 17662108 A US17662108 A US 17662108A US 8180725 B1 US8180725 B1 US 8180725B1
- Authority
- US
- United States
- Prior art keywords
- links
- link
- model
- new model
- expected
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
- 238000000034 method Methods 0.000 title claims description 82
- 238000012549 training Methods 0.000 claims abstract description 48
- 238000001994 activation Methods 0.000 claims abstract description 21
- 230000004913 activation Effects 0.000 claims abstract description 20
- 230000007246 mechanism Effects 0.000 claims description 12
- 230000008569 process Effects 0.000 description 56
- 241000406668 Loxodonta cyclotis Species 0.000 description 16
- 230000006870 function Effects 0.000 description 14
- 150000001875 compounds Chemical class 0.000 description 9
- 238000009826 distribution Methods 0.000 description 8
- 238000010411 cooking Methods 0.000 description 5
- 230000009193 crawling Effects 0.000 description 4
- 230000004044 response Effects 0.000 description 4
- 241000282372 Panthera onca Species 0.000 description 3
- 241001465754 Metazoa Species 0.000 description 2
- 238000004458 analytical method Methods 0.000 description 2
- 238000012512 characterization method Methods 0.000 description 2
- 238000010304 firing Methods 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 230000008685 targeting Effects 0.000 description 2
- 206010003805 Autism Diseases 0.000 description 1
- 208000020706 Autistic disease Diseases 0.000 description 1
- 241000283080 Proboscidea <mammal> Species 0.000 description 1
- 230000002411 adverse Effects 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 238000012217 deletion Methods 0.000 description 1
- 238000009472 formulation Methods 0.000 description 1
- 239000000463 material Substances 0.000 description 1
- 239000000203 mixture Substances 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 238000012545 processing Methods 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/35—Clustering; Classification
- G06F16/353—Clustering; Classification into predefined classes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/901—Indexing; Data structures therefor; Storage structures
- G06F16/9027—Trees
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/951—Indexing; Web crawling techniques
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/02—Knowledge representation; Symbolic representation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/3331—Query processing
- G06F16/334—Query execution
- G06F16/3344—Query execution using natural language analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/253—Grammatical analysis; Style critique
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
Definitions
- the present invention relates to techniques for modeling textual documents. More specifically, the present invention relates to techniques for selecting links in a probabilistic generative model for textual documents, wherein the model characterizes textual documents based on clusters of conceptually related words.
- Some embodiments of the present invention provide a system that selects links while updating a probabilistic generative model for textual documents.
- the system receives a current model, which contains terminal nodes representing words and cluster nodes representing clusters of conceptually related words, wherein nodes in the current model are coupled together by weighted links, wherein if a node fires, a link from the node to another node is activated and causes the other node to fire with a probability proportionate to the weight of the link.
- the system applies a set of training documents containing words to the current model to produce a new model.
- the system determines expected counts for activations of links and prospective links; determines link-ratings for the links and the prospective links based on the expected counts, and selects links to be included in the new model based on the determined link-ratings. Finally, the system makes the new model the current model.
- selecting the links to be included in the new model involves ensuring source diversity by selecting links which are associated with at least a minimum number of sources. In some embodiments, the minimum number of sources is three sources.
- the function for the link-rating is significance 2 ⁇ expected count.
- determining the expected count for the given link involves reducing the value of the expected count if the given link is a prospective link.
- selecting links to be included in the new model based on the determined link-ratings involves: ranking the links and prospective links based on the determined link ratings and selecting the N top-ranked links and prospective links to be included in the new model.
- applying the set of training documents to the current model involves: applying the set of training documents to the links defined in the current model to produce expected counts for corresponding links and prospective new links in the new model; and using these expected counts to produce weights for links in the new model.
- producing the new model additionally involves selectively introducing new cluster nodes into the current model.
- producing the new model additionally involves selectively deleting nodes from the new model.
- the system selectively merges nodes in the current model.
- FIG. 1 illustrates a probabilistic model in accordance with an embodiment of the present invention.
- FIG. 2 illustrates a state of the probabilistic model in accordance with an embodiment of the present invention.
- FIG. 3 illustrates a model representing states in the United States in accordance with an embodiment of the present invention.
- FIG. 4 illustrates global nodes and a number of local networks in accordance with an embodiment of the present invention.
- FIG. 5 illustrates a reworked model in accordance with an embodiment of the present invention.
- FIG. 6 illustrates the crawling, ranking and searching processes in accordance with an embodiment of the present invention.
- FIG. 7 illustrates data structures involved in characterizing a document in accordance with an embodiment of the present invention.
- FIG. 8 presents a flow chart of the characterization process in accordance with an embodiment of the present invention.
- FIG. 9 presents of a flow chart of the process for selecting candidate clusters in accordance with an embodiment of the present invention.
- FIG. 10 presents a flow chart of the process of approximating probabilities for candidate clusters in accordance with an embodiment of the present invention.
- FIG. 11 presents a flow chart illustrating how states for the probabilistic model are selected in accordance with an embodiment of the present invention.
- FIG. 12 presents a flow chart summarizing the learning process in accordance with an embodiment of the present invention.
- FIG. 13 presents a flow chart of the process of selectively deleting cluster nodes in accordance with an embodiment of the present invention.
- FIG. 14 presents a flow chart illustrating the process of identifying and merging similar cluster nodes in accordance with an embodiment of the present invention.
- FIG. 15 presents a flow chart illustrating the process of determining whether cluster nodes explain each other in accordance with an embodiment of the present invention.
- FIG. 16 presents a flow chart illustrating the process of merging two cluster nodes to form a combined cluster node in accordance with an embodiment of the present invention.
- FIG. 17 presents a flow chart illustrating the process of selecting links in accordance with an embodiment of the present invention.
- FIG. 18 presents a flow chart illustrating the process of determining link-ratings in accordance with an embodiment of the present invention.
- a computer-readable storage medium which may be any device or medium that can store code and/or data for use by a computer system.
- One embodiment of the present invention provides a system that learns concepts by learning an explanatory model of text.
- small pieces of text are generated in a fairly simple, but incredibly powerful way, through the execution of a probabilistic network.
- the system learns the parameters of this network by examining many examples of small pieces of text.
- One embodiment of the system considers the important information in a piece of text to be the words (and compounds) used in the text. For example in the query “cooking classes palo alto” the words are “cooking” and “classes”, and the compounds consist of the simple compound “palo alto”. Distinguishing compounds from words is done on the basis of compositionality. For example, “cooking classes” is not a compound because it is about both cooking and classes. However “palo alto” is not about “palo” and “alto” separately. This is sometimes a hard distinction to make, but good guesses can make such a system better than no guesses at all.
- FIG. 1 shows one such model.
- the circles are called model nodes.
- These nodes represent random variables, each of which models the existence or non-existence of concepts or terminals.
- the only terminals we are considering in this model are “elephant”, “grey” and “skies”.
- This model might be used, for example, to explain why the words “grey” and “skies” often occur together, why the words “grey” and “elephant” often occur together, but yet why the words “elephant” and “skies” rarely occur together. It is because when people are generating text with these words, they have ideas in mind. The system's concepts are supposed to model the ideas in a person's mind before they generate text.
- This model can be used or “executed” to generate text.
- U universal node
- firing means that the idea of that concept is active, and is able to fire terminals.
- terminals the idea of firing is that the terminals exist in the text to be generated.
- FIG. 2 shows this particular execution of the model detailed in FIG. 1 .
- C 1 becoming active (we illustrate this graphically by darkening the node) and the words “elephant” and “grey” becoming active.
- This idea of graphically viewing the execution model of a piece of text is important from the standpoint of examining the whole system to see if it is operating correctly, and we will use it later on.
- Our system learns the intermediate concepts, the links and the link weights in order to explain the co-occurrence of words and compounds in small pieces of text.
- its generative model is slightly more complicated than that described above, in order to be better able to generate and explain text of various sizes (for example, queries are often 2-3 words, while documents are 1000 words or so).
- FIG. 3 shows an example concept representing the states of the United States of America.
- the concept can fire terminals representing each of the 50 states, each with probability 1/50.
- That probability is roughly (1/50)*(49/50) 49 , which is approximately 0.7%.
- For this concept to fire all the states would be (1/50) 50 which is incredibly small.
- we develop such a concept that covers the idea of the states of the United States we would want it to explain pieces of text where all the states occur.
- each concept picks an activation level.
- this activation level chooses “how many” terminals are to be picked from this concept. Note that this activation level is not a quality of our model. In fact, it is only chosen when the model is being executed. What activation does is to modify the probability that this concept fires each of its terminals (but not its sub-concepts, i.e., concept-to-concept linking is unaffected by activation).
- Bayesian networks a certain class of probabilistic models
- Bayesian networks are well-understood probabilistic modeling techniques in which conditional independences are asserted between various random variables in a joint distribution. As in the model above, Bayesian networks have nodes and directed links. These networks compactly represent a joint distribution over a number of random variables while structurally representing conditional independence assumptions about these variables.
- the set of nodes pointing to a node is called its “parents”.
- the set of nodes reachable from a node via following links is called its “descendants” or “children”, and the structure implies that a node is independent of its non-descendants given its parents.
- the entire distribution is therefore encoded in the conditional probability tables of a child given its parents (nodes with no parents have their own distributions). The probability of a particular instantiation of the entire network is simply the product of the probabilities of each child given its parents.
- Bayesian networks are related to our model in the following way: if each node in the execution of our model is considered to be a random variable, then the joint distribution over the set of nodes that are turned on is exactly that which arises from considering our model as a Bayesian network with noisy-or combination functions. noisy-or conditional probabilities turn a Boolean child on independently from each parent. That is, the probability of a child being off is the product of the probability that each parent does not fire it. Note this is exactly the combination function used in our model to decide if multiple active concepts that link to a terminal fire it. Note that Bayesian networks are themselves a subclass of more general probabilistic models.
- some source of text In learning a generative model of text, in one embodiment of the present invention some source of text must be chosen. Some considerations in such a choice are as follows: (1) it should have related words in close proximity; (2) it should present evidence that is independent, given the model we are trying to learn (more on this later); and (3) it should be relevant to different kinds of text. For this reason, the implementation of the model which follows uses exemplary “query sessions” from a search engine as its small pieces of text. We have also implemented and run our model on web pages and other sources of text, but for the purposes of making this exposition more concrete, we focus on the analysis of query sessions.
- a query session (also referred to as a user session or a session) as any set of queries that are deemed to be relevant.
- a query session can include a set of queries issued by a single user on a search engine over a fixed period of time. Note that while issuing queries, a user will often search for related material, issuing several queries in a row about a particular topic. Sometimes, these queries are interspersed with queries associated with other random topics.
- An example query session (not an actual one) might appear as follows:
- FIG. 4 illustrates a number of local networks.
- the terminals for a particular user session are assumed to be active. Note that our model is replicated for each such session. This is because what we observe for each session is only the words that the user used and not, in fact, the concepts that were active in the user's mind when those words came about!
- the local nodes here represent our uncertainty about these concepts. Because the user may have been thinking of anything when they wrote each word, all concepts have to be considered in each local network.
- FIG. 5 shows a slightly reworked version of the model, where variables exist to show explicitly whether or not each concept triggers another concept or terminal. Note that the joint distributions implied by both are the same, once they are projected to the original variables we are interested in (i.e., C 1 and C 2 ). The triangles in this figure represent extra “trigger” variables, and it is often helpful to think about the model with them because they simplify the number of conditional probabilities that are required.
- the “trigger” variable between U and C 2 only needs to know the distributions of U and the weight of the link from U to C 2 to decide the probability that C 2 gets fired from U.
- the other trigger into C 2 only needs to know the values of the C 1 and weight of the link from C 1 to C 2 .
- the system maintains counters for links and prospective links in the current model to count the expected number of times they are activated during the training process, where the expectation is over the probability that the trigger variables are activated given the training data
- EM Expectation Maximization
- FIG. 6 illustrates the crawling, ranking and searching processes in accordance with an exemplary embodiment of the present invention.
- a web crawler 604 crawls or otherwise searches through websites on web 602 to select web pages to be stored in indexed form in data center 608 .
- the selected web pages are then compressed, indexed and ranked in module 605 (using the ranking process described above) before being stored in data center 608 .
- a search engine 612 receives a query 613 from a user 611 through a web browser 614 .
- This query 613 specifies a number of terms to be searched for in the set of documents.
- search engine 612 uses search terms specified in the query to identify highly-ranked documents that satisfy the query.
- Search engine 612 then returns a response 615 through web browser 614 , wherein the response 615 contains matching pages along with ranking information and references to the identified documents.
- the system can characterize the documents (and query phrases) based on the clusters of conceptually related words to improve the searching and/or ranking processes.
- FIG. 7 illustrates data structures involved in characterizing a document in accordance with an embodiment of the present invention. These data structures include order one probability table 702 , parent table 704 , child table 706 and link table 708 .
- Order one probability table 702 includes entries for each node in the probabilistic model that approximate the order one (unconditional) probability that the node is active in generating a given set of words. Hence, an entry in order one probability table 702 indicates how common an associated word or cluster is in sets of words that are generated by the probabilistic model. In one embodiment of the present invention, order one priority table 702 also includes an “activation” for each cluster node indicating how many links from the candidate cluster to other nodes are likely to fire.
- Parent table 704 includes entries that identify parents of associated nodes in the probabilistic model, as well as the link weights from the identified parents.
- child table 706 includes entries that identify children of associated nodes in the probabilistic model, as well as the link weights to the identified children. (Note that child table 706 is optional; it is not necessary for most of the operations we discuss.)
- link table 708 is populated during the process of characterizing a document.
- Link table 708 includes entries for links to consider as evidence while constructing an evidence tree as is discussed below with reference to FIGS. 8-11 . Each entry in link table 708 contains the weight for an associated link as well as the identifier for the associated parent node. Moreover, link table 708 can be sorted by parent identifier as is discussed below.
- FIG. 8 presents a flow chart of the characterization process in accordance with an embodiment of the present invention.
- the system starts by receiving a document containing a set of words (step 802 ).
- this document can include a web page or a set of terms (words) from a query.
- the system selects a set of “candidate clusters” from the probabilistic model that are likely to be active in generating the set of words (step 804 ). This process is described in more detail below with reference to FIG. 9 . Note that by selecting a set of candidate clusters, the system limits the number of clusters that are considered in subsequent computational operations, thereby reducing the amount of computation involved in characterizing the document.
- the system then constructs a vector (set of components) to characterize the document (step 806 ).
- This vector includes components for candidate clusters, wherein each component of the vector indicates a degree to which the corresponding candidate cluster was active in generating the set of words in the document. This process is described in more detail below with reference to FIGS. 10-11 .
- the system can use this vector to facilitate a number of different operations related to the document (step 808 ). Some of these uses are listed below in a following section of this specification entitled “Uses of the Model”.
- FIG. 9 presents a flow chart of the process for selecting candidate clusters in accordance with an embodiment of the present invention.
- This flow chart describes in more detail the operations involved in performing step 804 in FIG. 8 .
- the system starts by constructing an “evidence tree” starting from terminal nodes associated with the set of words in the document and following links to parent nodes (step 902 ). As a node is selected to be part of the evidence tree, links to the node from parent nodes are inserted into link table 808 .
- the system uses the evidence tree to estimate the likelihood that each parent cluster is active in generating the set of words (step 904 ). More specifically, in one embodiment of the present invention, for a cluster node C i that only points to terminal nodes, the system estimates the likelihood that C i was involved in generating the set of words (we refer to this estimated likelihood as the “Guess of C i ”) using the following formula,
- This formula indicates that the guess of C i is the order one probability of C i multiplied by a product of conditional probability contributions from active child nodes w j of C i .
- the numerator of this contribution, ⁇ tilde over (P) ⁇ (C i ⁇ w j ), is the weight of the link from C i to w j multiplied by a guess at the activation of C i .
- the activation of C i is an indicator of the number of active links out of node C i .
- the denominator of this contribution, ⁇ tilde over (P) ⁇ (w j ) is the order one probability of w j multiplied by the number of words in the set of words.
- Score ⁇ ( C i ) ⁇ k ⁇ Contribution ⁇ ( C k , C i ) ⁇ ⁇ ⁇ j ⁇ Contribution ⁇ ( w j , C i ) .
- the guess of C i is the order one probability of C i multiplied by a product of conditional probability contributions.
- these conditional probability contributions come from other cluster nodes C k as well as from child nodes w j .
- Contribution ⁇ ( C k , C i ) P ⁇ ( C k ⁇ C i ) ⁇ Score ⁇ ( C k ) + 1 - P ⁇ ( C k ⁇ C i ) P ⁇ ( C k ) ⁇ Score ⁇ ( C k ) + 1 - P ⁇ ( C k ) , wherein P(C k
- the system marks terminal nodes during the estimation process for a given cluster node to ensure that terminal nodes are not factored into the estimation more than once.
- the system selects parent nodes to be candidate cluster nodes based on these estimated likelihoods (step 906 ).
- the system has a set of candidate clusters to consider along with their activations.
- FIG. 10 presents a flow chart of the process of approximating probabilities for candidate clusters in accordance with an embodiment of the present invention.
- the system first selects states for the probabilistic model that are likely to have generated the set of words (step 1002 ).
- the system constructs the vector, wherein the vector includes components for candidate clusters. Each of these components indicates a likelihood that a corresponding candidate cluster is active in generating the set of words.
- the system considers only selected states in approximating the probability that an associated candidate cluster is active in generating the set of words (step 1004 ).
- P(C i ) can be calculated a:
- P ⁇ ( C i ) ⁇ P network ⁇ ( C i ⁇ ⁇ is ⁇ ⁇ on ) ⁇ P network ⁇ ( explored ) .
- This formula indicates that P(C i ) is the sum of the network probabilities for networks in which C i is discovered to be active divided by the sum of all network probabilities for networks that have been explored.
- the probability of a given network state occurring can be calculated as:
- P network ⁇ nodes ⁇ ⁇ j that ⁇ ⁇ are ⁇ ⁇ on ⁇ ( 1 - ⁇ nodes ⁇ ⁇ i ⁇ ⁇ that ⁇ ⁇ are on ⁇ ⁇ and ⁇ ⁇ point ⁇ ⁇ to ⁇ ⁇ j ⁇ ( 1 - w i -> j ) ) ⁇ ⁇ nodes ⁇ ⁇ k that ⁇ ⁇ are ⁇ ⁇ off ⁇ ( ⁇ nodes ⁇ ⁇ i ⁇ ⁇ that ⁇ ⁇ are on ⁇ ⁇ and ⁇ ⁇ point ⁇ ⁇ to ⁇ ⁇ k ⁇ ( 1 - w i -> k ) ) .
- This probability includes contributions from nodes that are “on”.
- the system computes the probability that at least one link into j (from an active parent node i) fires. This is one minus the probability that no link into j from an active parent node i fires, wherein the probability that a link from an active node does not fire is one minus the link weight.
- the probability also includes contributions from nodes k that are “off”. For a given node k that is off, the contribution is the probability that no link points to k from active node i, which is simply the product of one minus the link weights.
- FIG. 11 illustrates how states for the probabilistic model are selected in accordance with an embodiment of the present invention.
- This flow chart describes in more detail the operations involved in performing step 1002 in FIG. 10 .
- one embodiment of the present invention considers only candidate cluster nodes and terminal nodes associated with the set of words in the document. All other terminals are assumed to be off and are summarized in a simple term which is used to reduce the probability of the parent being on.
- the system starts by randomly selecting a starting state for the probabilistic model (step 1102 ).
- Each starting state indicates which nodes in the probabilistic model are active and which ones are not. Note that any starting state is possible because the universal node can trigger any subset of the candidate nodes to fire.
- link weights in the probabilistic model tend to make some states more likely than others in generating the set of words in the document. Hence, it is unlikely that a random starting state would have generated the set of words in the document.
- the system performs “hill-climbing” operations to reach a state that is likely to have generated the set of words in the document (step 1104 ).
- hill-climbing typically changes the state of the system in a manner that increases the value of a specific objective function.
- the objective function is the probability of a given network state occurring, P network , which is described above.
- the system periodically changes the state of an individual candidate cluster between hill-climbing operations without regard to the objective function. In doing so, the system fixes the changed state so it does not change during subsequent hill-climbing operations. This produces a local optimum for the objective function, which includes the changed state, which enables the system to explore states of the probabilistic model that are otherwise unreachable through hill-climbing operations alone.
- FIG. 12 presents a flow chart summarizing the learning process in accordance with an embodiment of the present invention.
- the system first receives a current model (step 1202 ).
- a current model can be created from a set of words by: (1) generating a universal node that is always active; (2) generating terminal nodes representing words in the set of words; (3) generating cluster nodes by selecting training instances; and (4) directly linking the universal node with all the nodes.
- the system receives a set of training documents (step 1204 ).
- the system starts with a small set of training documents for an initial iteration, and doubles the number of training documents in each subsequent iteration until all available training documents are used. This allows the system to learn larger concepts, which require fewer training documents to learn, during the earlier iterations.
- the system applies the set of training documents to links defined in the current model to produce weights for corresponding links in the new model.
- the system determines expected counts for the number of times the links and prospective links are activated during the training process (step 1206 ), where the expectation is over the probability that the links are activated given the training data.
- the system applies expectation maximization (EM) to the expected counts to produce weights for links in the new model (step 1208 ).
- EM expectation maximization
- the system then selectively deletes clusters from the new model (step 1210 ). This process is described in more detail below with reference to FIG. 13 .
- the system then considers the new model to be the current model (step 1212 ) and repeats the process for a number of iterations to produce a generative model which explains the set of training documents.
- FIG. 13 presents a flow chart of the process of selectively deleting cluster nodes in accordance with an embodiment of the present invention. This flow chart illustrates in more detail what takes place during step 1210 in the flow chart illustrated in FIG. 12 .
- the system determines the number of outgoing links from the cluster node to other nodes (step 1302 ). These other nodes can be terminal nodes associated with words or other cluster nodes. Using the expected counts collected over the training data, the system also determines the frequency with which the cluster node is activated (step 1304 ). If the number of outgoing links is less than a minimum value, or if the determined frequency is less than a minimum frequency, the system deletes the cluster node (step 1306 ).
- deleting the cluster node reduces memory usage, improves the computational efficiency of the model, and improves the quality of clusters returned by the model without adversely affecting the usefulness of the model. Similarly, if a cluster node is rarely activated, deleting the cluster node has similar benefits.
- the process of deleting a cluster node from the model involves: (1) deleting outgoing links from the cluster node; (2) deleting incoming links into the cluster node; and (3) deleting the cluster node itself.
- FIG. 14 presents a flow chart illustrating the process of identifying and merging similar cluster nodes in accordance with an embodiment of the present invention.
- this merging process takes place during a separate iteration of the training process illustrated in FIG. 13 . Note that this merging iteration can be interspersed with the training iterations illustrated in FIG. 13 .
- the system first receives the current model (step 1402 ).
- the system determines whether cluster nodes in the current model “explain” other cluster nodes in the current model (step 1404 ).
- step 1404 determines whether cluster nodes in the current model “explain” other cluster nodes in the current model.
- the system merges the two cluster nodes to form a combined cluster node (step 1406 ).
- This process is described in more detail below with reference to FIG. 16 .
- FIG. 15 presents a flow chart illustrating the process of determining whether cluster nodes explain each other in accordance with an embodiment of the present invention. During this process, the system iteratively analyzes each given cluster node to determine which other cluster nodes explain the given cluster node.
- the system forms a query which includes a set of “top words” associated with the cluster node (step 1502 ).
- This set of top words can include words whose nodes are most likely to fire if the cluster node fires.
- the system runs the query against the current model minus the given cluster node to produce a set of resulting cluster nodes (step 1504 ). More specifically, the given cluster node is temporarily removed from the current model and the query is run against this temporarily modified model. The query produces a set of resulting cluster nodes.
- the system determines that the resulting cluster node explains the given cluster node (step 1506 ).
- FIG. 16 presents a flow chart illustrating the process of merging two cluster nodes to form a combined cluster node in accordance with an embodiment of the present invention.
- the system merges the parents of the two cluster nodes to produce a set of parents for the combined cluster node (step 1602 ).
- the system also merges the children of the two cluster nodes to produce a set of children for the combined cluster node (step 1604 ).
- the system makes the given cluster node a child but not a parent of the combined cluster node (step 1606 ). This makes sense because in our model parent nodes occur more frequently than their child nodes. Because the combined cluster node will by definition be more frequent than the given cluster node, the given cluster node should be a child and not a parent of the combined cluster node.
- the system combines the two weighted links into a single combined weighted link (step 1608 ).
- the weight on this combined weighted link can be calculated by exponentially combining (or even adding) the weights of the two weighted links. Note that any reasonable approximation for the combined weight can be used because subsequent iterations of the training process will further refine the weight on this combined weighted link.
- FIG. 17 presents a flow chart illustrating the process of selecting links in accordance with an embodiment of the present invention. This flow chart illustrates in more detail the sequence of operations which take place in steps 1206 and 1208 of the flow chart which appears in FIG. 12 .
- the system applies training documents to links and prospective links in the current model to determine expected counts for the links and prospective links (step 1702 ). For a given document, the system activates terminals associated with words that occur in the given document. The system then determines a model structure which is consistent with the activated terminals. During this process, the system keeps track of an expected count for each link, which indicates the expected number times the link fired.
- the system also multiplies expected counts for prospective links by a factor, such as 0.25. This reduces expected counts for the prospective links and compensates for an overly optimistic assumption made while counting prospective link activations.
- the assumption is that whenever a cluster and an associated terminal are both activated, the prospective link between the cluster and the terminal caused the terminal to be activated. This assumption is overly optimistic because it is possible that the terminal was not activated by the link, but was instead activated by another link from another cluster.
- the reduction of the expected count by 0.25 helps to compensate for this overly optimistic assumption.
- the system filters links based on “source diversity” (step 1704 ). This involves only considering links which are associated with inputs from multiple sources.
- a source indicates where the training document came from, e.g., a URL of a web document, a user id, and so on. (Note that every session originates from some source.) It has been observed that links associated with inputs from only one or two sources are likely to be less reliable in generating a model for queries from a large number of people. Hence, in one embodiment of the present invention, the system only considers links which are associated with inputs from three or more sources.
- the system automatically considers links with five or more sources and does not consider links with only two or fewer sources.
- the system selectively considers links with three or four sources, but only after reducing their expected count by limiting the contribution from the most significant source. This prevents situations in which a single source provides the dominant contribution for a link.
- the system determines link-ratings for links and prospective links based on the expected counts (step 1706 ).
- significance is derived by calculating the number of times the child arises in the context of a parent divided by the expected number of times the child would arise if the child and the parent were independent. This can be calculated as follows:
- the significance is essentially the weight of the link divided by the probability of the child associated with the link. Hence, if the child is exceedingly rare, the fact that it shows up in a cluster is significant. Conversely, if the child is extremely common, the fact that it shows up in a cluster is insignificant.
- the system calculates the link-rating for the given link as a function of the significance and the expected count for the given link (step 1804 ).
- the function for the link-rating is significance 2 ⁇ expected count. Note that this function gives more weight to significance than count, which discounts the value of commonly occurring terminals that have a low significance. Furthermore, note that as the amount of training data that is considered increases, the expected count increases, but significance does not. This is because significance is a function of probabilities of occurrence, whereas the expected count is associated with an actual expected number of occurrences. The fact that expected counts increase over time is desirable, because as more training data is considered expected counts become more reliable and can hence be given more weight.
- the system ranks the links based on the link-ratings (step 1708 ). For example, this can involve sorting the links based on their link-ratings.
- the system selects the top N links to include in the new model (step 1710 ) and disregards the other links.
- the system can limit the number of links which occur in the model, which helps to limit the memory requirements for the model.
- the system selects the top N links for a number of iterations, and during the last iteration selects fewer links to include in the new model. For example, the system can select N links for all but the last iteration and can select M links for the final iteration, where M ⁇ N. This technique allows a larger number of links to be considered during the training iterations, which gives more links the opportunity to iteratively attain a sufficiently high link-rating to be included in the ultimate model.
Abstract
Description
wherein
{tilde over (P)}{tilde over ( )}(C i →w j)=(weightc
and wherein
{tilde over (P)}(w j)=O1(w j)×(#words).
This formula indicates that the guess of Ci is the order one probability of Ci multiplied by a product of conditional probability contributions from active child nodes wj of Ci. The numerator of this contribution, {tilde over (P)}(Ci→wj), is the weight of the link from Ci to wj multiplied by a guess at the activation of Ci. Recall that the activation of Ci is an indicator of the number of active links out of node Ci. The denominator of this contribution, {tilde over (P)}(wj), is the order one probability of wj multiplied by the number of words in the set of words.
Guess(C i)=O1(C i)·Score(Ci),
wherein
As in the case of a cluster node that only points to terminals, the guess of Ci is the order one probability of Ci multiplied by a product of conditional probability contributions. However, these conditional probability contributions come from other cluster nodes Ck as well as from child nodes wj.
wherein P(Ck|Ci) is the conditional probability of Ck given Ci, P(Ck) is the order one probability of Ck, and Score(Ck) is the previously calculated score of Ck. Note that since the evidence tree is constructed from terminals up, the score of the child node Ck will have been computed before the score of the parent node Ci is computed.
V 1=Activation(C i)×P(C i),
wherein the Activation(Ci) is an indicator of the number of links that will fire if node Ci fires, and wherein P(Ci) is the probability that Ci is active in generating the set of words in the document.
This formula indicates that P(Ci) is the sum of the network probabilities for networks in which Ci is discovered to be active divided by the sum of all network probabilities for networks that have been explored.
This probability includes contributions from nodes that are “on”. More specifically, for each node j that is on in a given network, the system computes the probability that at least one link into j (from an active parent node i) fires. This is one minus the probability that no link into j from an active parent node i fires, wherein the probability that a link from an active node does not fire is one minus the link weight.
-
- (1) Guessing at the concepts behind a piece of text. The concepts can then be displayed to a user allowing the user to better understand the meaning behind the text.
- (2) Comparing the words and concepts between a document and a query. This can be the information retrieval scoring function that is required in any document search engine, including the special case where the documents are web pages.
- (3) A different way of using our model for web searches is to assume that the distribution of clusters extends the query. For example, a query for the word “jaguar” is ambiguous. It could mean either the animal or the car. Our model will identify clusters that relate to both meanings in response to this search. In this case, we can consider that the user typed in one of either two queries, the jaguar (CAR) query or the jaguar (ANIMAL) query. We can then retrieve documents for both of these queries taking into account the ratio of their respective clusters' probabilities. By carefully balancing how many results we return for each meaning, we assure a certain diversity of results for a search.
- (4) Comparing the words and concepts between a document and an advertisement. This can be used as a proxy for how well an advertisement will perform if attached to a certain piece of content. A specialization of this is attaching advertisements to web pages.
- (5) Comparing the words and concepts between a query and an advertisement (or targeting criteria for an advertisement). In search engines, advertisers often select a set of “targeting criteria”, that trigger an ad to be served. The text of these criteria (and the ad copy itself) can be compared to a query via the use of clusters in our model. This comparison can be a proxy for how well the ad will perform if served on a search page resulting from the query.
- (6) Comparing the words and concepts between two documents. This can be used as a distance metric for conceptual clustering of documents, where similar documents are grouped together.
- (7) Projecting text into the space of clusters. The probabilities of clusters in the text can be used as features for an arbitrary classification task. For example, a pornography filter can be produced by projecting the text of a page onto clusters, and then building a classifier that uses the clusters and the words as its input.
- (8) Generalizing a web query to retrieve more results, using the bit cost or probability of a set of words or terminals given their parent clusters.
- (9) Guessing at whether a particular word is a misspelling of another word by looking at the concepts induced by the two words.
Claims (33)
Priority Applications (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/176,621 US8180725B1 (en) | 2007-08-01 | 2008-07-21 | Method and apparatus for selecting links to include in a probabilistic generative model for text |
US13/464,399 US9418335B1 (en) | 2007-08-01 | 2012-05-04 | Method and apparatus for selecting links to include in a probabilistic generative model for text |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US95329507P | 2007-08-01 | 2007-08-01 | |
US12/176,621 US8180725B1 (en) | 2007-08-01 | 2008-07-21 | Method and apparatus for selecting links to include in a probabilistic generative model for text |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/464,399 Continuation US9418335B1 (en) | 2007-08-01 | 2012-05-04 | Method and apparatus for selecting links to include in a probabilistic generative model for text |
Publications (1)
Publication Number | Publication Date |
---|---|
US8180725B1 true US8180725B1 (en) | 2012-05-15 |
Family
ID=46033299
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US12/176,621 Active 2030-11-16 US8180725B1 (en) | 2007-08-01 | 2008-07-21 | Method and apparatus for selecting links to include in a probabilistic generative model for text |
US13/464,399 Active 2031-09-04 US9418335B1 (en) | 2007-08-01 | 2012-05-04 | Method and apparatus for selecting links to include in a probabilistic generative model for text |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/464,399 Active 2031-09-04 US9418335B1 (en) | 2007-08-01 | 2012-05-04 | Method and apparatus for selecting links to include in a probabilistic generative model for text |
Country Status (1)
Country | Link |
---|---|
US (2) | US8180725B1 (en) |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20100332564A1 (en) * | 2008-02-25 | 2010-12-30 | Microsoft Corporation | Efficient Method for Clustering Nodes |
US9418335B1 (en) | 2007-08-01 | 2016-08-16 | Google Inc. | Method and apparatus for selecting links to include in a probabilistic generative model for text |
CN106209635A (en) * | 2015-04-30 | 2016-12-07 | 深圳市中兴微电子技术有限公司 | A kind of weight the route control method of multipath WCMP, device and system |
Families Citing this family (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN106547924A (en) * | 2016-12-09 | 2017-03-29 | 东软集团股份有限公司 | The sentiment analysis method and device of text message |
US10984022B2 (en) * | 2018-07-30 | 2021-04-20 | Sap Se | Clustering process for objects using link counts |
US11423080B2 (en) | 2018-07-30 | 2022-08-23 | Sap Se | Clustering process for objects using comparison structures |
Citations (16)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5794050A (en) | 1995-01-04 | 1998-08-11 | Intelligent Text Processing, Inc. | Natural language understanding system |
US5815830A (en) | 1994-12-23 | 1998-09-29 | Anthony; Andre Charles | Automatic generation of hypertext links to multimedia topic objects |
US6078914A (en) | 1996-12-09 | 2000-06-20 | Open Text Corporation | Natural language meta-search system and method |
US6137911A (en) | 1997-06-16 | 2000-10-24 | The Dialog Corporation Plc | Test classification system and method |
US20020087310A1 (en) | 2000-12-29 | 2002-07-04 | Lee Victor Wai Leung | Computer-implemented intelligent dialogue control method and system |
US20020120619A1 (en) | 1999-11-26 | 2002-08-29 | High Regard, Inc. | Automated categorization, placement, search and retrieval of user-contributed items |
US20030037041A1 (en) | 1994-11-29 | 2003-02-20 | Pinpoint Incorporated | System for automatic determination of customized prices and promotions |
US6651054B1 (en) | 1999-10-30 | 2003-11-18 | International Business Machines Corporation | Method, system, and program for merging query search results |
US6684205B1 (en) | 2000-10-18 | 2004-01-27 | International Business Machines Corporation | Clustering hypertext with applications to web searching |
US20040068697A1 (en) * | 2002-10-03 | 2004-04-08 | Georges Harik | Method and apparatus for characterizing documents based on clusters of related words |
US20040088308A1 (en) | 2002-08-16 | 2004-05-06 | Canon Kabushiki Kaisha | Information analysing apparatus |
US6751611B2 (en) | 2002-03-01 | 2004-06-15 | Paul Jeffrey Krupin | Method and system for creating improved search queries |
US6820093B2 (en) | 1996-07-30 | 2004-11-16 | Hyperphrase Technologies, Llc | Method for verifying record code prior to an action based on the code |
US7013298B1 (en) | 1996-07-30 | 2006-03-14 | Hyperphrase Technologies, Llc | Method and system for automated data storage and retrieval |
US7363308B2 (en) | 2000-12-28 | 2008-04-22 | Fair Isaac Corporation | System and method for obtaining keyword descriptions of records from a large database |
US7739253B1 (en) * | 2005-04-21 | 2010-06-15 | Sonicwall, Inc. | Link-based content ratings of pages |
Family Cites Families (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7231393B1 (en) | 2003-09-30 | 2007-06-12 | Google, Inc. | Method and apparatus for learning a probabilistic generative model for text |
US7877371B1 (en) * | 2007-02-07 | 2011-01-25 | Google Inc. | Selectively deleting clusters of conceptually related words from a generative model for text |
US8180725B1 (en) | 2007-08-01 | 2012-05-15 | Google Inc. | Method and apparatus for selecting links to include in a probabilistic generative model for text |
-
2008
- 2008-07-21 US US12/176,621 patent/US8180725B1/en active Active
-
2012
- 2012-05-04 US US13/464,399 patent/US9418335B1/en active Active
Patent Citations (16)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20030037041A1 (en) | 1994-11-29 | 2003-02-20 | Pinpoint Incorporated | System for automatic determination of customized prices and promotions |
US5815830A (en) | 1994-12-23 | 1998-09-29 | Anthony; Andre Charles | Automatic generation of hypertext links to multimedia topic objects |
US5794050A (en) | 1995-01-04 | 1998-08-11 | Intelligent Text Processing, Inc. | Natural language understanding system |
US6820093B2 (en) | 1996-07-30 | 2004-11-16 | Hyperphrase Technologies, Llc | Method for verifying record code prior to an action based on the code |
US7013298B1 (en) | 1996-07-30 | 2006-03-14 | Hyperphrase Technologies, Llc | Method and system for automated data storage and retrieval |
US6078914A (en) | 1996-12-09 | 2000-06-20 | Open Text Corporation | Natural language meta-search system and method |
US6137911A (en) | 1997-06-16 | 2000-10-24 | The Dialog Corporation Plc | Test classification system and method |
US6651054B1 (en) | 1999-10-30 | 2003-11-18 | International Business Machines Corporation | Method, system, and program for merging query search results |
US20020120619A1 (en) | 1999-11-26 | 2002-08-29 | High Regard, Inc. | Automated categorization, placement, search and retrieval of user-contributed items |
US6684205B1 (en) | 2000-10-18 | 2004-01-27 | International Business Machines Corporation | Clustering hypertext with applications to web searching |
US7363308B2 (en) | 2000-12-28 | 2008-04-22 | Fair Isaac Corporation | System and method for obtaining keyword descriptions of records from a large database |
US20020087310A1 (en) | 2000-12-29 | 2002-07-04 | Lee Victor Wai Leung | Computer-implemented intelligent dialogue control method and system |
US6751611B2 (en) | 2002-03-01 | 2004-06-15 | Paul Jeffrey Krupin | Method and system for creating improved search queries |
US20040088308A1 (en) | 2002-08-16 | 2004-05-06 | Canon Kabushiki Kaisha | Information analysing apparatus |
US20040068697A1 (en) * | 2002-10-03 | 2004-04-08 | Georges Harik | Method and apparatus for characterizing documents based on clusters of related words |
US7739253B1 (en) * | 2005-04-21 | 2010-06-15 | Sonicwall, Inc. | Link-based content ratings of pages |
Non-Patent Citations (13)
Title |
---|
"Automatic Document Classification Based on Probabilistic Reasoning: Model and Performance Analysi." Wai Lam et al. 0-7803-4053-1/97-IEEE. * |
Geiger, Dan et al., "Asymptotic Model Selection for Directed Networks with Hidden Variables", May 1996, Technical Report MSR-TR-96-07, Microsoft Research, Advanced Technology Division. |
Graham, I., The HTML Sourcebook, John Wiley & Sons, 1995 (ISBN 0471118494) (pages on "partial URL's" and ,"BASE element", e.g., pp. 22-27; 167-168). |
Heckerman, David et al., "Learning Bayesian Networks: The Combination of Knowledge and Statistical Data", Microsoft Research, Advanced Technology Division. |
Jordan, Michael I. et al., "Hidden Markov Decision Trees", 1997, Center for Biological and Computational Learning Massachusetts Institute of Technology and Department of Computer Science, University of Toronto Canada. |
Meila, Marina et al., "Estimating Dependency Structure as a Hidden Variable", Massachusetts Institute of Technology, A.I. Memo No. 1648, C.B.C.L. Memo No. 165, Sep. 1998. |
Mills, T., Providing world wide access to historical sources, Computer Networks and ISDN Systems, vol. 29, Nos. 8-13, Sep. 1997, pp. 1317-1325. |
Myka, A., Automatic Hypertext Conversion of Paper Document Collections (ch6), Digital Libraries Workshop DL'94, Newark NJ, May 1994 (selected papers), pp. 65-90. |
Notice of Allowance for related case (U.S. Appl. No. 10/676,571), mailed from USPTO on Sep. 30, 2003. |
Notice of Allowance for related case (U.S. Appl. No. 10/788,837), mailed from USPTO on Feb. 26, 2004. |
Office Action for related case (U.S. Appl. No. 10/676,571), mailed from USPTO on Sep. 30, 2003. |
Office Action for related case (U.S. Appl. No. 10/788,837), mailed from USPTO on Feb. 26, 2004. |
Thistlewaite, P., Automatic construction and management of large open webs, Information Processing and Management: an International Journal, vol. 33, Issue 2, Mar. 1997, pp. 161-173, (ISSN 0306-4573). |
Cited By (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9418335B1 (en) | 2007-08-01 | 2016-08-16 | Google Inc. | Method and apparatus for selecting links to include in a probabilistic generative model for text |
US20100332564A1 (en) * | 2008-02-25 | 2010-12-30 | Microsoft Corporation | Efficient Method for Clustering Nodes |
CN106209635A (en) * | 2015-04-30 | 2016-12-07 | 深圳市中兴微电子技术有限公司 | A kind of weight the route control method of multipath WCMP, device and system |
CN106209635B (en) * | 2015-04-30 | 2019-06-11 | 深圳市中兴微电子技术有限公司 | It is a kind of to weight the route control method of multipath WCMP, device and system |
Also Published As
Publication number | Publication date |
---|---|
US9418335B1 (en) | 2016-08-16 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US7877371B1 (en) | Selectively deleting clusters of conceptually related words from a generative model for text | |
US7383258B2 (en) | Method and apparatus for characterizing documents based on clusters of related words | |
US7231393B1 (en) | Method and apparatus for learning a probabilistic generative model for text | |
Yu et al. | Keyword search in databases | |
US9418335B1 (en) | Method and apparatus for selecting links to include in a probabilistic generative model for text | |
Chickering | Learning equivalence classes of Bayesian-network structures | |
Kaiser et al. | Reinforcement learning from reformulations in conversational question answering over knowledge graphs | |
Laxman et al. | Stream prediction using a generative model based on frequent episodes in event sequences | |
US8185482B2 (en) | Modeling semantic and structure of threaded discussions | |
US9507858B1 (en) | Selectively merging clusters of conceptually related words in a generative model for text | |
Kumar et al. | User profiling based deep neural network for temporal news recommendation | |
Balog et al. | Category-based query modeling for entity search | |
Mottin et al. | Searching with xq: the exemplar query search engine | |
US20060276996A1 (en) | Fast tracking system and method for generalized LARS/LASSO | |
Adami et al. | Clustering documents into a web directory for bootstrapping a supervised classification | |
Jung et al. | Ontology mapping-based search with multidimensional similarity and Bayesian network | |
Khobragade et al. | Effective negative triplet sampling for knowledge graph embedding | |
Sabetghadam et al. | A hybrid approach for multi-faceted IR in multimodal domain | |
Rosen | E-mail Classification in the Haystack Framework | |
Abul-Basher et al. | TGDB: towards a benchmark for graph databases | |
Huo et al. | User behavior sequence modeling to optimize ranking mechanism for e-commerce search | |
de Campos et al. | Implementing relevance feedback in the Bayesian network retrieval model | |
Saeed et al. | Not all embeddings are created equal: Extracting entity-specific substructures for RDF graph embedding | |
Dos Santos et al. | VOGA: Variable ordering genetic algorithm for learning Bayesian classifiers | |
Sowmya et al. | Social recommendation system using network embedding and temporal information |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:LERNER, URI N.;JAHR, MICHAEL;REEL/FRAME:021358/0050Effective date: 20070711 |
|
FEPP | Fee payment procedure |
Free format text: PAYOR NUMBER ASSIGNED (ORIGINAL EVENT CODE: ASPN); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYFree format text: PAYER NUMBER DE-ASSIGNED (ORIGINAL EVENT CODE: RMPN); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
FPAY | Fee payment |
Year of fee payment: 4 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044101/0405Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 12TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1553); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 12 |