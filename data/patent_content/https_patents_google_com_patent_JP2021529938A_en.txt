JP2021529938A - Interfacing between Digital Assistant and Navigation Applications - Google Patents
Interfacing between Digital Assistant and Navigation Applications Download PDFInfo
- Publication number
- JP2021529938A JP2021529938A JP2020569967A JP2020569967A JP2021529938A JP 2021529938 A JP2021529938 A JP 2021529938A JP 2020569967 A JP2020569967 A JP 2020569967A JP 2020569967 A JP2020569967 A JP 2020569967A JP 2021529938 A JP2021529938 A JP 2021529938A
- Authority
- JP
- Japan
- Prior art keywords
- navigation application
- client device
- component
- point
- audio signal
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Images
Classifications
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/34—Route searching; Route guidance
- G01C21/36—Input/output arrangements for on-board computers
- G01C21/3605—Destination input or retrieval
- G01C21/3608—Destination input or retrieval using speech input, e.g. using speech recognition
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/34—Route searching; Route guidance
- G01C21/36—Input/output arrangements for on-board computers
- G01C21/3626—Details of the output of route guidance instructions
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/34—Route searching; Route guidance
- G01C21/36—Input/output arrangements for on-board computers
- G01C21/3679—Retrieval, searching and output of POI information, e.g. hotels, restaurants, shops, filling stations, parking facilities
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/02—Knowledge representation; Symbolic representation
- G06N5/022—Knowledge engineering; Knowledge acquisition
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/226—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics
- G10L2015/228—Procedures used during a speech recognition process, e.g. man-machine dialogue using non-speech characteristics of application context
Abstract
本開示は全般に、ネットワーク化されたコンピュータ環境において複数のアプリケーション間でインターフェースするシステムおよび方法に関する。データ処理システムは、ナビゲーションアプリケーションにアクセスして、ナビゲーションアプリケーションのビューポートに表示されている地理的領域に対応する基準枠内の点位置を取り出すことができる。各点位置は識別子を有し得る。データ処理システムは、入力オーディオ信号を解析して、要求および参照語を識別することができる。データ処理システムは、入力オーディオ信号から解析された参照語および点位置の識別子に基づいて、基準枠内の点位置を識別することができる。データ処理システムは、識別された点位置を含むアクションデータ構造を生成することができる。データ処理システムは、アクションデータ構造をナビゲーションアプリケーションに送信し、点位置を使用したナビゲーション案内プロセスを開始することができる。The present disclosure generally relates to systems and methods of interfacing between multiple applications in a networked computer environment. The data processing system can access the navigation application to retrieve the point position within the reference frame corresponding to the geographic area displayed in the navigation application's viewport. Each point position can have an identifier. The data processing system can analyze the input audio signal to identify the request and reference term. The data processing system can identify the point position within the reference frame based on the reference word and the point position identifier analyzed from the input audio signal. The data processing system can generate an action data structure containing the identified point positions. The data processing system can send the action data structure to the navigation application to initiate a navigation guidance process using point positions.
Description
関連出願の相互参照
本出願は、その全体が参照によって本明細書に組み込まれる、2018年6月26日に出願された「INTERFACING BETWEEN DIGITAL ASSISTANT APPLICATIONS AND NAVIGATION APPLICATIONS」という表題の米国仮特許出願第62/690,049号の優先権の利益を主張する。
Cross-references to related applications This application is a US provisional patent application entitled "INTERFACING BETWEEN DIGITAL ASSISTANT APPLICATIONS AND NAVIGATION APPLICATIONS" filed June 26, 2018, which is incorporated herein by reference in its entirety. Claim the benefit of the priority of / 690,049.
デジタルアシスタントアプリケーションは、クライアントデバイスにおいて提供される機能と関連付けられる処理がネットワークによってクライアントデバイスに接続されるサーバにおいて実行される、ネットワーク化されたコンピュータ環境において動作することができる。サーバは、ネットワークによって、クライアントデバイスにおける要求と関連付けられるデータを提供され得る。コンピューティングデバイス間でのネットワークトラフィックデータの、パケットベースのまたはそれ以外の過剰なネットワーク送信は、コンピューティングデバイスがネットワークトラフィックデータを適切に処理すること、およびネットワークトラフィックデータに関する動作を完了すること、またはネットワークトラフィックデータにタイミング良く応答することを妨げ得る。ネットワークトラフィックデータの過剰なネットワーク送信はまた、応答するコンピューティングデバイスの処理能力以上であるとき、データのルーティングを複雑にし、または応答の品質を劣化させることがあり、これは、非効率な帯域幅の利用、コンピューティングリソースの消費、およびバッテリー持続時間の消耗をもたらし得る。過剰なネットワーク送信の一部には、有効な要求ではない要求に対する送信が含まれることがある。過剰なネットワーク送信を最小にすることが望ましいそのようなネットワーク化された環境においては特に、グラフィカルユーザインターフェースとして通常は動作するアプリケーションに、発話ベースのインターフェースを用意することに、さらなる課題がある。 The digital assistant application can run in a networked computer environment where the processing associated with the functionality provided on the client device is performed on the server connected to the client device by the network. The server may be provided by the network with data associated with the request on the client device. Excessive network transmission of network traffic data between computing devices, packet-based or otherwise, causes the computing device to properly process the network traffic data and complete operations on the network traffic data, or It can prevent you from responding to network traffic data in a timely manner. Excessive network transmission of network traffic data can also complicate the routing of data or degrade the quality of the response when it exceeds the processing power of the responding computing device, which is an inefficient bandwidth. Can result in utilization, consumption of computing resources, and drainage of battery life. Some of the excess network transmissions may include transmissions for requests that are not valid requests. There is a further challenge in providing a speech-based interface for applications that normally act as graphical user interfaces, especially in such networked environments where it is desirable to minimize excessive network transmission.
本開示のある態様によれば、ネットワーク化されたコンピュータ環境において複数のアプリケーション間でインターフェースするためのシステムは、1つまたは複数のプロセッサを有するデータ処理システムを含み得る。データ処理システム上で実行しているナビゲーションインターフェースコンポーネントは、第1のクライアントデバイス上で実行しているナビゲーションアプリケーションにアクセスして、ナビゲーションアプリケーションのビューポートに表示される地理的領域に対応する基準枠内の複数の点位置(point location)を取り出すことができる。複数の位置の各点位置は識別子を有し得る。データ処理システム上で実行される自然言語プロセッサコンポーネントは、第1のクライアントデバイスと第2のクライアントデバイスのうちの少なくとも1つのセンサによって検出される入力オーディオ信号を受信することができる。自然言語プロセッサコンポーネントは、入力オーディオ信号を解析して、要求および参照語を識別することができる。自然言語プロセッサコンポーネントは、要求の識別に応答して、入力オーディオ信号から解析された参照語および点位置の識別子に基づいて、基準枠内の複数の点位置から点位置を識別することができる。データ処理システム上で実行されるアクションハンドラコンポーネントは、入力オーディオ信号の検出に応答して識別された点位置を含む、アクションデータ構造を生成することができる。アクションハンドラコンポーネントは、アクションデータ構造を第1のクライアントデバイスに送信して、ナビゲーションアプリケーションにその点位置を使用したナビゲーション案内プロセスを開始させることができる。 According to certain aspects of the disclosure, a system for interfacing between multiple applications in a networked computer environment may include a data processing system having one or more processors. The navigation interface component running on the data processing system accesses the navigation application running on the first client device and is within the baseline corresponding to the geographic area displayed in the navigation application's viewport. Multiple point locations can be retrieved. Each point position of the plurality of positions may have an identifier. A natural language processor component running on a data processing system can receive an input audio signal detected by at least one sensor of a first client device and a second client device. The natural language processor component can analyze the input audio signal to identify the request and reference term. In response to the identification of the request, the natural language processor component can identify the point position from a plurality of point positions within the reference frame based on the reference word and the point position identifier analyzed from the input audio signal. The action handler component running on the data processing system can generate an action data structure containing the identified point positions in response to the detection of the input audio signal. The action handler component can send the action data structure to the first client device to initiate a navigation guidance process using that point position in the navigation application.
本開示のある態様によれば、ネットワーク化されたコンピュータ環境において複数のアプリケーション間でインターフェースする方法は、第1のクライアントデバイス上で実行しているナビゲーションアプリケーションにアクセスして、ナビゲーションアプリケーションのビューポートに表示される地理的領域に対応する基準枠内の複数の点位置を取り出すステップを含み得る。複数の位置の各点位置は識別子を有し得る。方法は、第1のクライアントデバイスと第2のクライアントデバイスのうちの少なくとも1つのセンサによって検出される入力オーディオ信号を受信するステップを含み得る。方法は、入力オーディオ信号を解析して、要求および参照語を識別するステップを含み得る。方法は、要求を識別したことに応答して、入力オーディオ信号から解析された参照語および点位置の識別子に基づいて、基準枠内の複数の点位置から点位置を識別するステップを含み得る。方法は、入力オーディオ信号の検出に応答して識別された点位置を含む、アクションデータ構造を生成するステップを含み得る。方法は、アクションデータ構造を第1のクライアントデバイスに送信して、ナビゲーションアプリケーションにその点位置を使用したナビゲーション案内プロセスを開始させるステップを含み得る。 According to certain aspects of the disclosure, a method of interfacing between multiple applications in a networked computer environment is to access the navigation application running on the first client device and into the viewport of the navigation application. It may include the step of retrieving multiple point positions within the reference frame corresponding to the displayed geographic area. Each point position of the plurality of positions may have an identifier. The method may include receiving an input audio signal detected by at least one sensor of the first client device and the second client device. The method may include analyzing the input audio signal to identify the request and reference term. The method may include identifying a point position from a plurality of point positions within the reference frame based on the reference word and the point position identifier analyzed from the input audio signal in response to identifying the request. The method may include the step of generating an action data structure, including the identified point positions in response to the detection of the input audio signal. The method may include sending an action data structure to a first client device to initiate a navigation guidance process using that point position in the navigation application.
各態様は、次の特徴のうちの1つまたは複数を含み得る。ナビゲーションインターフェースコンポーネントは、ナビゲーションアプリケーションにアクセスして、入力オーディオ信号の受信と同時に表示される地理的領域に対応する基準枠の第1の部分を決定し、慣性運動ユニットから取得される第1のクライアントデバイスの速度に基づいて、ビューポートに以前に表示されていた地理的領域に対応する基準枠の第2の部分を決定し得る。自然言語プロセッサコンポーネントは、慣性運動ユニットからのデータを使用して決定された第1のクライアントデバイスと第2のクライアントデバイスのうちの少なくとも1つの移動方向に基づいて、基準枠内の複数の点位置からその点位置を識別し得る。ナビゲーションインターフェースコンポーネントは、ナビゲーションアプリケーションにアクセスして、地理的領域に対応する、およびナビゲーション案内プロセスの経路決定動作の目的地から定められた近接の範囲内にある第2の地理的領域に対応する、第1の部分を有する基準枠内の複数の点位置を取り出してもよく、自然言語プロセッサコンポーネントは、参照語が第2の地理的領域に対応する第2の部分に関連するが地理的領域に対応する第1の部分に関連しないと決定し、参照語が第2の部分に関連するという決定に基づいて、部分内の複数の点位置から点位置を識別してもよい。ナビゲーションインターフェースコンポーネントは、ナビゲーションアプリケーションにアクセスして、地理的領域に対応する基準枠内の第1のクライアントデバイスの第1の位置識別子と、基準枠内の複数の点位置に対応する複数の第2の位置識別子とを取り出してもよく、自然言語プロセッサコンポーネントは、第1のクライアントデバイスの第1の位置識別子および複数の点位置に対応する複数の第2の位置識別子に基づいて、複数の点位置から点位置を識別してもよい。ナビゲーションインターフェースコンポーネントは、ナビゲーションアプリケーションにアクセスして、入力オーディオ信号の受信より前の定められた時間枠内に受信された複数の検索語を取り出してもよく、自然言語プロセッサコンポーネントは、複数の点位置の各点位置および複数の検索語の各検索語に対して、セマンティックナレッジグラフを使用して点位置の識別子と検索語との間の意味的距離を決定し、点位置の識別のために、複数の識別子と複数の検索語との間の複数の意味的距離に基づいて、複数の点位置から点位置のサブセットを選択してもよい。自然言語プロセッサコンポーネントは、入力オーディオ信号を解析して参照語と異なる補助語を識別し、補助語に基づいてナビゲーションアプリケーションのビューポートのサブセットエリアを決定し、点位置の識別のために、補助語に基づいて決定されたビューポートのサブセットエリアに対応する複数の点位置から点位置のサブセットを選択し得る。自然言語プロセッサコンポーネントは、第1のクライアントデバイスと第2のクライアントデバイスのうちの少なくとも1つのセンサによって検出される第2の入力オーディオ信号を受信し、第2の入力オーディオ信号の受信と入力オーディオ信号の受信との間に経過した時間が定められた閾値未満であると決定し、経過時間が定められた閾値未満であるという決定に応答して、第2の入力オーディオ信号を解析して第2の参照語を識別し、点位置の識別のために、第2の参照語に基づいて複数の点位置から点位置のサブセットを選択し得る。自然言語プロセッサコンポーネントは、複数の点位置の各点位置に対して、参照語と点位置の識別子との間の指標尺度を決定することであって、指標尺度が、参照語が点位置の識別子を表す尤度を示す、決定することと、対応する複数の点位置に対する複数の指標尺度に基づいて、基準枠内の複数の点位置から点位置を識別することとを行い得る。自然言語プロセッサコンポーネントは、複数の点位置の各点位置に対して、セマンティックナレッジグラフを使用して、参照語と点位置の識別子との間の意味的距離を決定し、対応する複数の点位置に対する複数の意味的距離に基づいて、基準枠内の複数の点位置から点位置を識別し得る。自然言語プロセッサコンポーネントは、要求に基づいてナビゲーションアプリケーションによって実行されるべき複数の動作のうちの位置発見動作に対応する要求タイプを決定してもよく、アクションハンドラコンポーネントは、要求タイプを含むアクションデータ構造を生成し、アクションデータ構造を第1のクライアントデバイスに送信して、ナビゲーションアプリケーションに、ビューポートに表示される地理的領域において点位置を提示するために要求タイプに対応するナビゲーション案内プロセスの位置発見動作を開始させてもよい。自然言語プロセッサコンポーネントは、要求に基づいてナビゲーションアプリケーションによって実行されるべき複数の動作のうちの経路決定動作に対応する要求タイプを決定してもよく、アクションハンドラコンポーネントは、要求タイプを含むアクションデータ構造を生成し、アクションデータ構造を第1のクライアントデバイスに送信して、ナビゲーションアプリケーションに、目的地としての点位置への移動経路を識別するために要求タイプに対応するナビゲーション案内プロセスの経路決定動作を開始させてもよい。アクションハンドラコンポーネントは、テキスト出力または出力オーディオ信号のうちの少なくとも1つのためのナビゲーションアプリケーションを実行している第1のクライアントデバイスからの応答を受信し得る。 Each aspect may include one or more of the following features: The navigation interface component accesses the navigation application to determine the first part of the reference frame corresponding to the geographic area displayed upon receipt of the input audio signal, and the first client obtained from the inertial motion unit. Based on the speed of the device, the second part of the baseline corresponding to the geographic area previously displayed in the viewport can be determined. The natural language processor component has multiple point positions within the reference frame based on the direction of movement of at least one of the first and second client devices determined using data from the inertial motion unit. The point position can be identified from. The navigation interface component accesses the navigation application and corresponds to a geographic area, and corresponds to a second geographic area within a defined proximity to the destination of the routing action of the navigation guidance process. Multiple point positions within a reference frame having a first part may be retrieved, and the natural language processor component may be associated with a second part whose reference word corresponds to a second geographic area, but in the geographic area. Point positions may be identified from multiple point positions within a part based on the determination that it is not related to the corresponding first part and that the reference term is related to the second part. The navigation interface component accesses the navigation application and has a first position identifier of the first client device in the reference frame corresponding to the geographic area and a plurality of second positions corresponding to multiple point positions in the reference frame. The natural language processor component may retrieve a plurality of point positions based on the first position identifier of the first client device and the plurality of second position identifiers corresponding to the plurality of point positions. The point position may be identified from. The navigation interface component may access the navigation application to retrieve multiple search terms received within a defined time frame prior to receiving the input audio signal, and the natural language processor component may have multiple point locations. For each point position and each search term in multiple search terms, a semantic knowledge graph is used to determine the semantic distance between the point position identifier and the search term, and to identify the point position, A subset of point positions may be selected from multiple point positions based on multiple semantic distances between the plurality of identifiers and the plurality of search terms. The natural language processor component analyzes the input audio signal to identify auxiliary languages that differ from the reference language, determines a subset area of the viewport of the navigation application based on the auxiliary language, and uses the auxiliary language to identify point locations. A subset of point positions can be selected from multiple point positions corresponding to the viewport subset area determined based on. The natural language processor component receives a second input audio signal detected by at least one sensor of the first client device and the second client device, and receives the second input audio signal and the input audio signal. In response to the determination that the elapsed time between the reception and the reception is less than the defined threshold and the elapsed time is less than the defined threshold, the second input audio signal is analyzed and the second A subset of point positions can be selected from multiple point positions based on the second reference word to identify the reference words of. The natural language processor component is to determine an index scale between a reference word and a point position identifier for each point position at a plurality of point positions, where the index scale is an identifier where the reference word is a point position identifier. It is possible to indicate and determine the likelihood of representing, and to identify a point position from a plurality of point positions within a reference frame based on a plurality of index scales for a plurality of corresponding point positions. The natural language processor component uses a semantic knowledge graph for each point position at multiple point positions to determine the semantic distance between the reference word and the point position identifier, and the corresponding point positions. A point position can be identified from a plurality of point positions within a reference frame based on a plurality of semantic distances to. The natural language processor component may determine the request type corresponding to the position-finding action of multiple actions to be performed by the navigation application based on the request, and the action handler component is an action data structure containing the request type. Generates and sends the action data structure to the first client device to find the location of the navigation guidance process corresponding to the request type to present the point location in the geographic area displayed in the viewport to the navigation application. The operation may be started. The natural language processor component may determine the request type corresponding to the routing action of multiple actions to be performed by the navigation application based on the request, and the action handler component is an action data structure containing the request type. And send the action data structure to the first client device to give the navigation application the routing action of the navigation guidance process corresponding to the request type to identify the route to the point position as the destination. You may start it. The action handler component may receive a response from a first client device running a navigation application for at least one of the text output or output audio signals.
これらのおよび他の態様と実装形態が以下で詳しく論じられる。前述の情報および以下で詳述される説明は、様々な態様および実装形態の説明のための例を含み、特許請求される態様および実装形態の性質と特性を理解するための概要または枠組みを与える。図面は、様々な態様および実装形態の例示とさらなる理解をもたらし、本明細書に組み込まれ、本明細書の一部を構成する。 These and other aspects and implementations are discussed in detail below. The information described above and the description detailed below include examples for describing various aspects and implementations and provide an overview or framework for understanding the nature and characteristics of the claimed aspects and implementations. .. The drawings provide examples and further understanding of various aspects and implementations, which are incorporated herein by reference and form part of this specification.
添付の図面は、縮尺通りに描かれることは意図されていない。様々な図面における同様の参照番号および指定は、同様の要素を示す。わかりやすくするために、すべての構成要素がすべての図面において標識されてはいないことがある。 The attached drawings are not intended to be drawn to scale. Similar reference numbers and designations in various drawings indicate similar elements. For clarity, not all components may be labeled in all drawings.
ネットワーク化されたコンピュータ環境において複数のアプリケーション間でインターフェースするための方法、装置、およびシステムに関連する様々な概念、ならびにその実装形態の、より詳細な説明が後に続く。上で紹介され、下でより詳しく論じられた様々な概念が、多数の方法のいずれかで実装され得る。 A more detailed description of methods, devices, and various system-related concepts for interfacing between applications in a networked computer environment, as well as their implementations, follows. The various concepts introduced above and discussed in more detail below can be implemented in any of a number of ways.
デジタルアシスタントアプリケーションは、アプリケーションプログラミングインターフェース(API)に従って、アプリケーションデータを交換して機能を呼び出すことを介して、エージェントとインターフェースすることができる。入力オーディオ信号を受信すると、デジタルアシスタントアプリケーションは、入力オーディオ信号を解析して、入力オーディオ信号から語を識別することができる。デジタルアシスタントアプリケーションは、その語が特定のエージェントの機能を指すと決定することができる。この決定に応答して、デジタルアシスタントアプリケーションは、入力オーディオ信号において言及されるエージェントの機能を呼び出すことができる。この機能を使用して、デジタルアシスタントアプリケーションの能力を補強することができる。 Digital assistant applications can interface with agents by exchanging application data and invoking functions according to an application programming interface (API). Upon receiving the input audio signal, the digital assistant application can analyze the input audio signal to identify words from the input audio signal. The digital assistant application can determine that the term refers to the function of a particular agent. In response to this decision, the digital assistant application can call the agent's functions mentioned in the input audio signal. This feature can be used to augment the capabilities of digital assistant applications.
1つのそのようなエージェントは、ナビゲーションアプリケーション(全地球測位システム(GPS)ナビゲータと呼ばれることがある)であり得る。ナビゲーションアプリケーションは、ビューポートを介して地理的領域の地図の見下ろし図を表示することができる。この地図は、等高線、水深、行政区、人工物、および交通網(たとえば、道路、歩道、自転車道、および鉄道)を定義することができる。この地図はまた、交通網を表現する経路を介して一緒につながれた多数の点位置を含み得る。各点位置は、とりわけ、レストラン、ガソリンスタンド、ランドマーク、山、または湖などの、ベクターマップ上の関心地点を指し得る。各点位置は、地理的座標および識別子を用いて標識され得る。識別子は、関心地点の名前または記述子であり得る。たとえば、レストランに対応する点位置は、「ABC Pizzeria」を名前として、「レストラン」および「ピザ」を記述子として有し得る。拡大および視角を使用して、ナビゲーションアプリケーションのビューポートを通じて見える地図の部分を変更することができる。地図を表示する際、ナビゲーションアプリケーションは、エンドユーザに対して基準枠としてビューポートを通じて見える地図の部分を識別することができる。 One such agent can be a navigation application (sometimes referred to as a Global Positioning System (GPS) navigator). The navigation application can display a top-down map of the geographic area through the viewport. This map can define contour lines, water depths, provinces, artifacts, and transportation networks (eg, roads, sidewalks, bike paths, and railroads). The map may also contain a number of point locations connected together via routes that represent the transportation network. Each point location can point to a point of interest on a vector map, among other things, such as a restaurant, gas station, landmark, mountain, or lake. Each point location can be labeled with geographic coordinates and identifiers. The identifier can be the name or descriptor of the point of interest. For example, a point position corresponding to a restaurant may have "ABC Pizzeria" as a name and "restaurant" and "pizza" as descriptors. You can use magnification and viewing angle to change the portion of the map that you see through the viewport of your navigation application. When displaying a map, the navigation application can identify the portion of the map that is visible through the viewport as a reference frame to the end user.
ナビゲーションアプリケーションはまた、ビューポートを通じて表示される地図に関して、様々なナビゲーション案内機能を実行することができる。ナビゲーションアプリケーションのナビゲーション案内機能は、位置発見動作および経路発見動作を含み得る。位置特定動作は、地図上の特定の関心地点を発見するために呼び出され得る。位置発見動作のもとで、ナビゲーションアプリケーションは、地図上の関心地点に対する検索語を受信することができる。受信すると、ナビゲーションアプリケーションは、ナビゲーションアプリケーションのビューポートを通じて見える、検索語と一致する識別子を伴うすべての点位置を識別することができる。経路発見動作は、現在地から地図の関心地点への進路を決定するために呼び出され得る。経路発見動作において、ナビゲーションアプリケーションは、現在地および要求される関心地点に対応する点位置を識別することができる。この点位置は、ビューポートを通じて見える点位置の識別子と一致する検索語を使用して識別されていてもよい。ナビゲーションアプリケーションは、基準枠内で定義されるような、現在地とその点位置をつなぐ経路を介した現在地とその点位置との間の進路を決定するために、経路発見アルゴリズムを適用することができる。 Navigation applications can also perform various navigation guidance functions on the map displayed through the viewport. The navigation guidance function of the navigation application may include a position finding operation and a route finding operation. The locating action can be called to find a particular point of interest on the map. Under the location discovery operation, the navigation application can receive the search term for the point of interest on the map. Upon receipt, the navigation application can identify all point locations with identifiers that match the search term, as seen through the navigation application's viewport. The route finding operation can be called to determine the route from the current location to the point of interest on the map. In the route discovery operation, the navigation application can identify the current location and the point position corresponding to the requested point of interest. This point position may be identified using a search term that matches the point position identifier visible through the viewport. The navigation application can apply a route finding algorithm to determine the path between the current location and the point position via the route connecting the current location and the point position, as defined within the reference frame. ..
デジタルアシスタントアプリケーションをナビゲーションアプリケーションとインターフェースすることの難しさは、デジタルアシスタントアプリケーションがオーディオ入力信号およびオーディオ出力信号に依存するのに対し、ナビゲーションアプリケーションが、視覚的表現と、視覚的表現とのタッチによる対話(たとえば、タッチスクリーン、キーボード、またはマウスを介した)によって受け取られる入力とに依存し得るということにあり得る。加えて、ナビゲーションアプリケーションは、クライアントデバイスに対する基準枠がその周囲で認識され得る、クライアントデバイスの現在地または現在の中心点を入手することができる。対照的に、デジタルアシスタントアプリケーションは、現在地、現在の中心点、またはナビゲーションアプリケーションを通じてアクセス可能な地図内の基準枠のあらゆる要素を欠いていることがある。さらに、デジタルアシスタントアプリケーションは、ビューポートを通じて見える、地図において定義される点位置および経路を入手できないことがある。ナビゲーションアプリケーションのビューポートを通じて見えるデータを入手できないと、または基準枠の考慮がないと、デジタルアシスタントアプリケーションは、入力オーディオ信号から識別された要求が地図上のどの点位置に言及しているかを決定することが不可能であり得る。その上、入力オーディオ信号を解析することから識別された要求がナビゲーションアプリケーションに対するテキスト入力に変換される場合であっても、ナビゲーションアプリケーションは、テキスト入力がどの点位置を参照しているかを区別することが不可能であり得る。ナビゲーションアプリケーションは、自然言語処理能力を欠いていることがあるので、テキスト入力が指標語または直示語を含む自然言語であるとき、上記の区別できないことがさらに悪化する。 The difficulty of interfacing a digital assistant application with a navigation application is that the digital assistant application relies on audio input and audio output signals, whereas the navigation application interacts with the visual representation by touch. It can depend on the input received by (eg, via a touch screen, keyboard, or mouse). In addition, the navigation application can obtain the current location or current center point of the client device from which the reference frame for the client device can be recognized. In contrast, digital assistant applications may lack any element of the reference frame in the map that is accessible through the current location, current center point, or navigation application. In addition, digital assistant applications may not have access to map-defined point locations and paths that are visible through the viewport. If the data visible through the viewport of the navigation application is not available, or without reference frame consideration, the digital assistant application determines which point position on the map the request identified from the input audio signal refers to. Can be impossible. Moreover, the navigation application distinguishes which point position the text input refers to, even if the request identified by parsing the input audio signal is translated into text input to the navigation application. Can be impossible. Navigation applications may lack natural language processing power, which further exacerbates the above indistinguishability when the text input is in natural language, including index words or deixis.
インターフェースすることに起因する技術的課題に対処するために、デジタルアシスタントアプリケーションは、ナビゲーションアプリケーションの機能のうちの1つに言及する入力オーディオ信号における要求に応答して、ナビゲーションアプリケーションにアクセスすることができる。デジタルアシスタントアプリケーションはまた、入力オーディオ信号における要求がどの機能に言及しているかを決定することができる。たとえば、入力オーディオ信号を解析することから「あそこに連れて行って」という語を識別すると、デジタルアシスタントアプリケーションは、「連れて行って」という語がナビゲーションアプリケーションの経路発見動作に言及していると決定することができる。別の例では、「ガソリンスタンドを見せて」という用語が入力オーディオ信号から解析されるとき、デジタルアシスタントアプリケーションは、「見せて」という用語がデジタルアシスタントアプリケーションの位置発見動作に言及していると決定することができる。ナビゲーションアプリケーションにアクセスする際、デジタルアシスタントアプリケーションは、ナビゲーションアプリケーションのビューポートを通じて見える地図の部分に対応する点位置のセットを取り出すことができる。デジタルアシスタントアプリケーションまた、各点位置の識別子と、ナビゲーションアプリケーションに対する入力として使用される検索語の以前のセットとを取得することができる。デジタルアシスタントアプリケーションはまた、ナビゲーションアプリケーションの機能に言及している以前に受信された要求を識別することができる。たとえば、「ABCタワーについて教えて」という句および「パティスリーを見せて」という句を伴う入力オーディオ信号が続いて受信された可能性がある。デジタルアシスタントアプリケーションは、識別子を取得することに対する関心領域を確立する際に「パティスリーを見せて」という句を処理する際に、「ABCタワーについて教えて」という句を使用することができる。 To address the technical challenges posed by the interface, the digital assistant application can access the navigation application in response to a request in the input audio signal that mentions one of the features of the navigation application. .. The digital assistant application can also determine which function the request in the input audio signal refers to. For example, if you identify the word "take me there" from analyzing the input audio signal, the digital assistant application says that the word "take me" refers to the route-finding behavior of the navigation application. Can be decided. In another example, when the term "show gas station" is parsed from the input audio signal, the digital assistant application determines that the term "show" refers to the location-finding behavior of the digital assistant application. can do. When accessing a navigation application, the digital assistant application can retrieve a set of point locations that correspond to the portion of the map that is visible through the navigation application's viewport. Digital Assistant Application You can also get the identifier of each point position and the previous set of search terms used as input to the navigation application. The digital assistant application can also identify previously received requests that mention the functionality of the navigation application. For example, an input audio signal with the phrases "Tell me about the ABC Tower" and "Show me the patisserie" may have been subsequently received. Digital assistant applications can use the phrase "tell me about ABC Tower" when processing the phrase "show me the patisserie" when establishing an area of interest in getting an identifier.
デジタルアシスタントアプリケーションは、自然言語処理技法を使用して、入力オーディオ信号から解析された語のセットから参照語を決定することができる。参照語は、ナビゲーションアプリケーションのビューポートを通じて見える値図上の関心地点のうちの1つに対応し得る。たとえば、入力オーディオ信号から解析された「あそこに連れて行って」という句に対しては、参照語は「あそこ」であり得る。「ピッツェリアに行こう」という句に対しては、参照語は「ピッツェリア」であり得る。ナビゲーションアプリケーションのビューポートを通じて見える点位置の識別子を使用して、デジタルアシスタントアプリケーションは、参照語がどの点位置に言及しているかを識別することができる。デジタルアシスタントアプリケーションは、参照語を各点位置に対する識別子と比較することができる。比較する際、デジタルアシスタントアプリケーションは、セマンティックナレッジグラフを使用して、参照語と各位置の識別子との間の意味的距離を決定することができる。デジタルアシスタントアプリケーションはまた、参照語と、以前に受信された要求または検索語などの以前の語との間の、指標尺度を決定することができる。比較に基づいて、デジタルアシスタントアプリケーションは、入力オーディオ信号の参照語がどの点位置に言及しているかを識別することができる。要求および識別された点位置を使用して、デジタルアシスタントアプリケーションは、識別された点位置を使用して示された動作を行うためにナビゲーションアプリケーションに提供すべき、アクションデータ構造を生成することができる。 Digital assistant applications can use natural language processing techniques to determine reference words from a set of words parsed from an input audio signal. The reference term can correspond to one of the points of interest on the value map that is visible through the viewport of the navigation application. For example, for the phrase "take me over there" analyzed from the input audio signal, the reference term can be "over there." For the phrase "let's go to Pizzeria", the reference word can be "Pizzeria". Using the point position identifier visible through the viewport of the navigation application, the digital assistant application can identify which point position the reference term refers to. The digital assistant application can compare the reference word with the identifier for each point position. When making comparisons, the Digital Assistant application can use the Semantic Knowledge Graph to determine the semantic distance between the reference term and the identifier at each location. The digital assistant application can also determine the metric scale between the reference term and the previous term, such as a previously received request or search term. Based on the comparison, the digital assistant application can identify which point position the reference term of the input audio signal refers to. Using the requested and identified point positions, the digital assistant application can generate action data structures that should be provided to the navigation application to perform the indicated actions using the identified point positions. ..
したがって、自然言語の処理および解釈に基づくリソース集約的な処理を、リモートサーバにおいてクライアントデバイスのために実行することができ、このとき、クライアントデバイスのグラフィカルユーザインターフェースと関連付けられる情報が考慮される。したがって、本明細書で説明される主題は、クライアントデバイスのグラフィカルユーザインターフェースと発話ベースのシステムとの間のインターフェースを提供し得る。インターフェースは、ユーザが、発話を使用してグラフィカルユーザインターフェースと対話することを可能にし、追加で、グラフィカルユーザインターフェースと関連付けられるデータがリモートサーバに効率的に提供されるようにすることを可能にする。発話ベースのシステムはこうして、クライアントデバイスのユーザとのより良い誘導された対話を提供することが可能である。 Therefore, resource-intensive processing based on natural language processing and interpretation can be performed on the remote server for the client device, taking into account the information associated with the client device's graphical user interface. Accordingly, the subject matter described herein may provide an interface between a graphical user interface of a client device and an utterance-based system. The interface allows the user to interact with the graphical user interface using speech, and additionally allows the data associated with the graphical user interface to be efficiently provided to the remote server. .. Speech-based systems can thus provide better guided dialogue with users on client devices.
図1を参照すると、ネットワーク化されたコンピュータ環境において複数のアプリケーション間でインターフェースするための例示的なシステム100が図示されている。システム100は、少なくとも1つのデータ処理システム102、1つまたは複数のクライアントデバイス104、および1つまたは複数のナビゲータサービス106を含み得る。1つまたは複数のクライアントデバイス104は、1つまたは複数のナビゲータサービス106に通信可能に結合されてもよく、その逆であってもよい。少なくとも1つのデータ処理システム102、1つまたは複数のクライアントデバイス104、および1つまたは複数のナビゲータサービス106は、ネットワーク156を介して互いに通信可能に結合され得る。
With reference to FIG. 1, an
データ処理システム102は、デジタルアシスタントアプリケーション108のインスタンスを含み得る。デジタルアシスタントアプリケーション108は、オーディオベースの入力を解析するための自然言語プロセッサ(NLP)コンポーネント114を含み得る。デジタルアシスタントアプリケーション108は、ナビゲーションアプリケーション110とインターフェースするためのナビゲーションインターフェースコンポーネント116を含み得る。デジタルアシスタントアプリケーション108は、場所の測定結果を取得するためのジオロケーション感知コンポーネント118を含み得る。デジタルアシスタントアプリケーション108は、オーディオベースの信号を生成するためのオーディオ信号生成器コンポーネント122を含み得る。デジタルアシスタントアプリケーション108は、ダイレクトアクションハンドラコンポーネント120を含み得る。デジタルアシスタントアプリケーション108は、オーディオベースの入力信号への応答を選択するための応答選択器コンポーネント124を含み得る。NLPコンポーネント114、オーディオ信号生成器コンポーネント122、データリポジトリ126、ダイレクトアクションハンドラコンポーネント120、および応答選択器コンポーネント124は、デジタルアシスタントアプリケーション108とは分かれている。データ処理システム102は、データリポジトリ126を含み得る。データリポジトリ126は、定型表現128、パラメータ130、ポリシー132、応答データ134、およびテンプレート136を記憶することができる。
The
データ処理システム102はまた、とりわけ、ナビゲーション案内プロセスを実行するための、少なくとも1つのナビゲーションアプリケーション110のインスタンスを含み得る。ナビゲーション案内プロセスは、とりわけ、位置発見動作および経路決定動作を含み得る。ナビゲーションアプリケーション110は、デジタルアシスタントアプリケーション108とインターフェースするためのデジタルアシスタントインターフェースコンポーネント138を含み得る。ナビゲーションアプリケーション110は、位置発見動作を実行して、検索語を使用して地理的領域の中の位置を探すための、位置発見器コンポーネント140を含み得る。ナビゲーションアプリケーション110は、経路決定動作を実行して地理的領域においてある位置から別の位置への経路を決定するための、経路決定器コンポーネント142を含み得る。位置発見器コンポーネント140および経路決定器コンポーネント142の機能が、本明細書において以下で詳細に説明される。ナビゲーションアプリケーション110はまた、場所の測定結果を取得するために、ジオロケーション感知コンポーネント118のインスタンスを含み得る。ナビゲーションアプリケーション110は、少なくとも1つのデータリポジトリ144を含み、または別様にそれにアクセスすることができる。ナビゲーションアプリケーション110は、デジタルアシスタントアプリケーション108とは別個のアプリケーションであり得る。データ処理システム102は、1つまたは複数のナビゲーションアプリケーション110のインスタンスを含み得る。
The
データリポジトリ144は、ナビゲーションアプリケーション110の1つまたは複数のインスタンスがアクセス可能なベクターベースの地図146を記憶して保持することができる。データリポジトリ144は、ナビゲーションアプリケーション110とは別個であってもよく、データ処理システム102またはナビゲータサービス106上で維持され得る。ベクターベースの地図146の少なくとも一部分は、ナビゲーションアプリケーション110を実行しているクライアントデバイス104上で維持され得る。ナビゲーションアプリケーション110は、ナビゲーションアプリケーション110のビューポートを通じて見えるベクターベースの地図146の一部分をレンダリングして表示することができる。ビューポートは、ベクターベースの地図146のその部分がそれを通じて見える、ナビゲーションアプリケーション110を実行しているクライアントデバイス104のディスプレイのエリアに対応し得る。ベクターベースの地図146は、ナビゲーションアプリケーション110のビューポートまたはクライアントデバイス104のディスプレイよりサイズが大きくてもよく、ナビゲーションアプリケーション110のビューポートに対応する部分が表示されてもよい。ナビゲーションアプリケーション110のビューポートを通じて現在表示されている、または以前に表示されていた部分は、ナビゲーションアプリケーション110を実行しているクライアントデバイス104に記憶され得る。ベクターベースの地図146は、データ構造(たとえば、リンクされたリスト、ツリー、アレイ、マトリックス、およびヒープ)を使用して、(たとえば地球の)地理的な地図を表現することができる。ベクターベースの地図146は、等高線、水深、(たとえば、国、州、郡、県、市、町、および村の)行政区、自然物(たとえば、湖、山、および川)、人工物(たとえば、建物、駐車場、および公園)、および/もしくは交通網(たとえば、道路、歩道、自転車道、および鉄道)、またはこれらの特徴の組合せを含み得る。ベクターベースの地図146は、等高線、水深、行政区、人工物、および交通網を定義することができる。ベクターベースの地図146は、点位置のセットおよび経路のセットを含み得る。ベクターベースの地図146は、点に対する地理的座標(たとえば、経度および緯度)を定義することができる。各点位置は、人工物および自然物のうちの1つに対応し得る。各点位置は、地理的座標と関連付けられてもよく、1つまたは複数の識別子を有してもよい。点位置の識別子は、点位置の名前およびカテゴリタイプを含み得る。たとえば、ホテルに対応する点位置に対して、名前は「XYZ Inn」であってもよく、カテゴリタイプは「ホテル」であってもよい。点位置は、経路を介して互いにつながれ得る。各経路は、とりわけ、道路、歩道、自転車道、および鉄道などの、交通網に対応し得る。各経路は、点位置の間の地理的距離(たとえば、キロメートルまたはマイルで測られた)を定義することができる。ベクターベースの地図146は、地理的情報符号化フォーマット(たとえば、GIS)に従って符号化され得る。
The
デジタルアシスタントアプリケーション108およびナビゲーションアプリケーション110などの、データ処理システム102の機能は、1つまたは複数のクライアントデバイス104に含まれてもよく、またはそれから別様にアクセス可能であってもよい。データ処理システム102の機能は、クライアントデバイス104上で実行しているデジタルアシスタントアプリケーション108の機能またはそれとのインターフェースに対応し得る。クライアントデバイス104は各々、デジタルアシスタントアプリケーション108の1つまたは複数のコンポーネントの別個のインスタンスを含み、実行することができる。クライアントデバイス104は、別様に、ネットワーク156を介して、リモートデータ処理システム102上のデジタルアシスタントアプリケーション108のコンポーネントの機能へのアクセス権を有し得る。たとえば、クライアントデバイス104は、NLPコンポーネント114の機能を含み、データ処理システム102へのネットワーク156を介してデジタルアシスタントアプリケーション108のコンポーネントの残りにアクセスすることができる。クライアントデバイス104は各々、ナビゲーションアプリケーション110の1つまたは複数のコンポーネントの別個のインスタンスを含み、実行することができる。クライアントデバイス104は、別様に、ネットワーク156を介して、リモートデータ処理システム102上のナビゲーションアプリケーション110のコンポーネントの機能へのアクセス権を有し得る。たとえば、クライアントデバイス104は、位置発見器コンポーネント140および経路決定器コンポーネント142の機能を含んでもよく、ネットワーク156を介してベクターベースの地図146にアクセスすることができる。
The functionality of the
クライアントデバイス104は各々、ネットワーク156を介してデータ処理システム102と互いに通信するためのプロセッサを有するコンピューティングデバイスなどの、少なくとも1つのロジックデバイスを含み得る。クライアントデバイス104は、データ処理システム102に関連して説明されたコンポーネントのいずれのインスタンスも含み得る。クライアントデバイス104は、デジタルアシスタントアプリケーション108のインスタンスを含み得る。クライアントデバイス104は、デスクトップコンピュータ、ラップトップ、タブレットコンピュータ、携帯情報端末、スマートフォン、モバイルデバイス、ポータブルコンピュータ、シンクライアントコンピュータ、仮想サーバ、スピーカベースのデジタルアシスタント、または他のコンピューティングデバイスを含み得る。
Each
システム100のコンポーネントは、ネットワーク156を介して通信することができる。ネットワーク156は、たとえば、ポイントツーポイントネットワーク、ブロードキャストネットワーク、ワイドエリアネットワーク、ローカルエリアネットワーク、遠隔通信ネットワーク、データ通信ネットワーク、コンピュータネットワーク、ATM(非同期転送モード)ネットワーク、SONET(同期光ネットワーク)ネットワーク、SDH(同期デジタルハイアラーキ)ネットワーク、NFC(近距離通信)ネットワーク、ローカルエリアネットワーク(LAN)、ワイヤレスネットワークまたは有線ネットワーク、およびこれらの組合せを含み得る。ネットワーク156は、赤外線チャネルまたは衛星バンドなどのワイヤレスリンクを含み得る。ネットワーク156のトポロジーは、バス、スター、またはリングネットワークトポロジーを含み得る。ネットワーク156は、advanced mobile phone protocol(AMPS)、時分割多元接続(TDMA)、符号分割多元接続(CDMA)、global system for mobile communication(GSM)、general packet radio services(GPRS)、またはuniversal mobile telecommunications system(UMTS)を含む、モバイルデバイス間で通信するために使用される任意の1つまたは複数のプロトコルを使用する、携帯電話ネットワークを含み得る。異なるタイプのデータが異なるプロトコルを介して送信されてもよく、または、同じタイプのデータが異なるプロトコルを介して送信されてもよい。
The components of
クライアントデバイス104は、デジタルアシスタントアプリケーション108の少なくとも1つのインスタンス、ナビゲーションアプリケーション110の少なくとも1つのインスタンス、少なくとも1つのスピーカ148、少なくとも1つのセンサ154、少なくとも1つのトランスデューサ150、および少なくとも1つの周辺デバイス152のうちの1つまたは複数を含み、実行し、それとインターフェースし、または別様にそれと通信することができる。センサ154は、たとえば、カメラ、周辺光センサ、近接センサ、温度センサ、慣性運動ユニット、加速度計、ジャイロスコープ、運動検出器、GPSセンサ、位置センサ、マイクロフォン、ビデオ、画像検出、またはタッチセンサを含み得る。トランスデューサ150は、スピーカもしくはマイクロフォンを含んでもよく、またはその一部であってもよい。クライアントデバイス104はオーディオドライバを含み得る。オーディオドライバは、ハードウェアトランスデューサ150へのソフトウェアインターフェースを提供することができる。オーディオドライバは、対応する音響波または音波を生成するようにトランスデューサ150を制御するために、データ処理システム102によって提供されるオーディオファイルまたは他の指示を実行することができる。周辺デバイス152は、とりわけ、キーボード、ディスプレイ、およびヘッドフォンなどの、ユーザ入力/出力デバイスを含み得る。ディスプレイは、発光ダイオード、有機発光ダイオード、液晶ディスプレイ、レーザー、またはディスプレイなどの、視覚的な指示または光学的な出力を提供するように構成される、1つまたは複数のハードウェアまたはソフトウェアコンポーネントを含み得る。
The
クライアントデバイス104上のデジタルアシスタントアプリケーション108のインスタンスは、1つまたは複数のプロセッサ、ロジックアレイ、もしくはメモリを含んでもよく、またはそれによって実行されてもよい。クライアントデバイス104上のデジタルアシスタントアプリケーション108のインスタンスは、キーワードを検出し、キーワードに基づいて活動を実行することができる。クライアントデバイス104上のデジタルアシスタントアプリケーション108は、データ処理システム102において実行されるデジタルアシスタントアプリケーション108のインスタンスであってもよく、または、デジタルアシスタントアプリケーション108の機能のいずれを実行してもよい。クライアントデバイス104上のデジタルアシスタントアプリケーション108のインスタンスは、さらなる処理のためにデータとして用語をデータ処理システム102(たとえば、データ処理システム102上のデジタルアシスタントアプリケーション108のインスタンス)に送信する前に、1つまたは複数の用語をフィルタリングし、または用語を修正することができる。クライアントデバイス104上のデジタルアシスタントアプリケーション108のインスタンスは、トランスデューサ150によって検出されたアナログオーディオ信号をデジタルオーディオ信号に変換し、デジタルオーディオ信号を搬送する1つまたは複数のデータパケットを、ネットワーク156を介してデータ処置システム102に送信することができる。クライアントデバイス104上のデジタルアシスタントアプリケーション108のインスタンスは、そのような送信を実行するための指示を検出したことに応答して、入力オーディオ信号の一部または全体を搬送するデータパケットを送信することができる。指示は、たとえば、トリガキーワードもしくは他のキーワード、または、データ処理システム102に入力オーディオ信号を備えるデータパケットを送信するための許可を含み得る。
An instance of Digital Assistant Application 108 on
クライアントデバイス104上のデジタルアシスタントアプリケーション108のインスタンスは、オーディオのある周波数を取り除くために、入力オーディオ信号に対してプリフィルタリングまたは前処理を実行することができる。プリフィルタリングは、ローパスフィルタ、ハイパスフィルタ、またはバンドパスフィルタなどのフィルタを含み得る。フィルタは周波数領域において適用され得る。フィルタは、デジタル信号処理技法を使用して適用され得る。フィルタは、人の声または人の発話に相当する周波数を保ちながら、人の発話の典型的な周波数の外側の周波数を除去するように構成され得る。たとえば、バンドパスフィルタは、第1の閾値(たとえば、70Hz、75Hz、80Hz、85Hz、90Hz、95Hz、100Hz、または105Hz)を下回り、第2の閾値(たとえば、200Hz、205Hz、210Hz、225Hz、235Hz、245Hz、または255Hz)を超える周波数を取り除くように構成され得る。バンドパスフィルタを適用することで、ダウンストリーム処理におけるコンピューティングリソースの利用率を下げることができる。クライアントデバイス104上のデジタルアシスタントアプリケーション108のインスタンスは、入力オーディオ信号をデータ処理システム102に送信する前にバンドパスフィルタを適用して、それによりネットワーク帯域幅の利用率を減らすことができる。しかしながら、クライアントデバイス104に対して利用可能なコンピューティングリソースおよび利用可能なネットワーク帯域幅に基づいて、データ処理システム102がフィルタリングを実行することを可能にするために、入力オーディオ信号をデータ処理システム102に提供することがより効率的であり得る。
An instance of Digital Assistant Application 108 on
クライアントデバイス104上のデジタルアシスタントアプリケーション108のインスタンスは、自然言語プロセッサと干渉し得る周辺雑音レベルを下げるために、雑音低減技法などの追加の前処理技法またはプリフィルタリング技法を適用することができる。雑音低減技法は、自然言語プロセッサの正確さと速さを高め、それにより、データ処理システム102の性能を高め、ディスプレイを介して提供されるグラフィカルユーザインターフェースのレンダリングを管理することができる。
Instances of Digital Assistant Application 108 on
クライアントデバイス104は、クライアントデバイス104へのオーディオ入力として音声の問合せを(センサ154またはトランスデューサ150を介して)入力するエンドユーザと関連付けられてもよく、クライアントデバイス104のエンドユーザに提示すべき、表示すべき、またはレンダリングすべきオーディオ(または他の)出力を、データ処理システム102またはナビゲータサービス106から受信する。デジタルコンポーネントは、データ処理システム102またはナビゲータサービス106からクライアントデバイス104に提供され得るコンピュータで生成された音声を含み得る。クライアントデバイス104は、トランスデューサ150(たとえば、スピーカ)を介して、コンピュータで生成された音声をエンドユーザにレンダリングすることができる。コンピュータで生成された音声は、本物の人による録音またはコンピュータで生成された言語を含み得る。クライアントデバイス104は、クライアントデバイス104に通信可能に結合されるディスプレイデバイスを介して、視覚的な出力を提供することができる。
The
音声の問合せをクライアントデバイス104に入力するエンドユーザは、複数のクライアントデバイス104と関連付けられ得る。たとえば、エンドユーザは、スピーカベースのデジタルアシスタントデバイスであり得る第1のクライアントデバイス104、モバイルデバイス(たとえば、スマートフォン)であり得る第2のクライアントデバイス104、およびデスクトップコンピュータであり得る第3のクライアントデバイス104と関連付けられ得る。データ処理システム102は、共通のログイン(たとえば、アカウント識別子および認証証明書)、位置、ネットワーク、または他の結び付けるデータを通じて、クライアントデバイス104の各々を関連付けることができる。たとえば、エンドユーザは、同じアカウントユーザ名およびパスワードを用いて、クライアントデバイス104の各々にログインし得る。
An end user who inputs a voice query to a
クライアントデバイス104は、ナビゲーションアプリケーション110のインスタンスを含み、または実行することができる。クライアントデバイス104は、ナビゲーションアプリケーション110のインスタンスを含み、または実行することができる。ナビゲーションアプリケーション110は、デジタルアシスタントアプリケーション108と同様の機能を伴う1つまたは複数のコンポーネントを含み得る。ナビゲーションアプリケーション110のインスタンスは、データ処理システム102およびナビゲータサービス106上で実行され得る。あらかじめ定められた機能を行うために、デジタルアシスタントアプリケーション108がナビゲーションアプリケーション110とインターフェースし、ナビゲーションアプリケーション110がデジタルアシスタントアプリケーション108とインターフェースすることができる。ナビゲーションアプリケーション110は、入力オーディオ信号において示される機能を行う際に、ナビゲータサービス106上のリソースにアクセスすることができる。クライアントデバイス104は、クライアントデバイス104のセンサ154(たとえば、マイクロフォン)によって検出される入力オーディオ信号を受信することができる。入力オーディオ信号を解析したことに基づいて、デジタルアシスタントアプリケーション108は、入力オーディオ信号を処理する際にどのナビゲーションアプリケーション110とインターフェースすべきかを決定することができる。入力オーディオ信号は、たとえば、問合せ、質問、命令、指示、または自然言語での他の陳述を含み得る。たとえば、音声の問合せは、地理的領域の中の位置を発見するための命令を含み得る。デジタルアシスタントアプリケーション108は、音声の問合せがナビゲーションアプリケーション110の少なくとも1つの機能に言及する命令を含むと決定することができる。この決定に応答して、デジタルアシスタントアプリケーション108は、音声の問合せにおいて示される仕事を完了するためのデータを取り出すために、ナビゲーションアプリケーション110とインターフェースすることができる。入力オーディオ信号は、ナビゲーションアプリケーション110の機能に言及する1つまたは複数のあらかじめ定められたキーワード(たとえば、「連れて行く」、「見つける」、および「経路」)を含み得る。たとえば、入力オーディオ信号は「XYZ高校に連れて行って」を含み得る。この問合せから、デジタルアシスタントアプリケーション108は、音声の問合せが、別のエージェントまたはデジタルアシスタントアプリケーション108自体の機能ではなく、ナビゲーションアプリケーション110に言及していると決定することができる。デジタルアシスタントアプリケーション108は、音声の問合せがナビゲーションアプリケーション110の機能に言及していると決定することができ、ナビゲーションアプリケーション110への命令を生成するために音声の問合せを使用して処理を実行することができる。受信すると、ナビゲーションアプリケーション110は、音声の問合せを使用して生成される命令に基づいて、ベクターベースの地図146の一部を表示または提示することができる。ナビゲータサービス106およびデジタルアシスタントアプリケーション108に関するナビゲーションアプリケーション110の機能が、本明細書において以下で詳述される。
ナビゲーションアプリケーション110なしで第1のクライアントデバイス104上で実行しているデジタルアシスタントアプリケーション108は、第2のクライアントデバイス104上で実行しているナビゲーションアプリケーション110にアクセスすることができる。音声の問合せがナビゲーションアプリケーション110の少なくとも1つの機能に言及するとの決定に応答して、第1のクライアントデバイス104上で実行しているデジタルアシスタントアプリケーション108は、第1のクライアントデバイス104がナビゲーションアプリケーション110を欠いていることを識別することができる。デジタルアシスタントアプリケーション108は、共通のログイン(たとえば、アカウント識別子および認証証明書)、位置、ネットワーク、または他の結び付けるデータを通じて、第1のクライアントデバイス104と関連付けられるナビゲーションアプリケーション110上で実行している1つまたは複数のクライアントデバイス104(たとえば、第2のクライアントデバイス104)を識別することができる。第1のクライアントデバイス104上で実行しているデジタルアシスタントアプリケーション108は、音声の問合せをさらに処理するために、第2のクライアントデバイス104上で実行しているナビゲーションアプリケーション110にアクセスすることができる。
The digital assistant application 108 running on the
データ処理システム102およびナビゲータサービス106は各々、少なくとも1つのプロセッサを有する少なくとも1つのサーバを含み得る。たとえば、データ処理システム102およびナビゲータサービス106は各々、少なくとも1つのデータセンタまたはサーバファームに配置された複数のサーバを含み得る。データ処理システム102は、要求および要求と関連付けられるトリガキーワードをオーディオ入力信号から決定することができる。要求およびトリガキーワードに基づいて、データ処理システム102は、オーディオ入力信号をナビゲータサービス106に転送すべきか、またはオーディオ入力信号を内部で処理すべきかを決定することができる。オーディオ入力信号が内部で処理されるべきであるという決定に応答して、データ処理システム102は、応答データを生成または選択することができる。応答データは、オーディオベースまたはテキストベースであり得る。たとえば、応答データは、レンダリングされるとオーディオ出力または音響波を提供する、1つまたは複数のオーディオファイルを含み得る。応答データ内のデータは、コンテンツ項目とも呼ばれ得る。応答データは、オーディオコンテンツに加えて、他のコンテンツ(たとえば、テキスト、ビデオ、または画像コンテンツ)を含み得る。オーディオ入力信号が転送されるべきであるという決定に応答して、データ処理システム102は、オーディオ入力信号をナビゲータサービス106に送信することができる。ナビゲータサービス106は、オーディオ入力信号を解析して、実行すべき命令を識別することができる。ナビゲータサービス106は、命令を行い、命令の結果をデータ処理システム102またはクライアントデバイス104に返すことができる。
The
データ処理システム102およびナビゲータサービス106は各々、複数の論理的にグループ化されたサーバを含み、分散型コンピューティング技法を容易にし得る。サーバの論理グループは、データセンタ、サーバファーム、またはマシンファームと呼ばれ得る。サーバは地理的に分散していてもよい。データセンタまたはマシンファームは単一のエンティティとして管理されてもよく、またはマシンファームは複数のマシンファームを含んでもよい。各マシンファーム内のサーバは異種であってもよく、サーバまたはマシンの1つまたは複数が、1つまたは複数のタイプのオペレーティングシステムプラットフォームに従って動作することができる。データ処理システム102およびナビゲータサービス106は各々、たとえば企業のデータセンタに配置された1つまたは複数の高密度ラックシステムに、関連するストレージシステムとともに保管されている、データセンタの中のサーバを含み得る。このようにして、統合されたサーバを伴うデータ処理システム102またはナビゲータサービス106は、システムの管理可能性、データセキュリティ、システムの物理セキュリティ、およびシステムの性能を、サーバと高性能のストレージシステムを局限された高性能ネットワーク上に配置することによって、高めることができる。サーバおよびストレージシステムを含む、データ処理システム102またはナビゲータサービス106のコンポーネントのすべてまたは一部の集中化、ならびに、それらを進化したシステム管理ツールと結合することは、サーバリソースのより効率的な使用を可能にし、このことは、電力と処理の要件を軽減し、帯域幅の使用率を下げる。データ処理システム102のコンポーネントの各々は、少なくとも1つの処理ユニット、サーバ、仮想サーバ、回路、エンジン、エージェント、アプライアンス、または、データリポジトリ126および144と、ならびに他のコンピューティングデバイスと通信するように構成されるプログラマブルロジックアレイなどの、他のロジックデバイスを含み得る。ナビゲータサービス106はまた、少なくとも1つの処理ユニット、サーバ、仮想サーバ、回路、エンジン、エージェント、アプライアンス、または、データリポジトリと、および他のコンピューティングデバイスと通信するように構成されるプログラマブルロジックアレイなどの、他のロジックデバイスを含み得る。
The
データ処理システム102は、データリポジトリ126を含み得る。データリポジトリ126は、1つまたは複数のローカルデータベースまたは分散されたデータベースを含んでもよく、データベース管理システムを含んでもよい。データリポジトリ126は、コンピュータデータストレージまたはメモリを含んでもよく、とりわけ、1つまたは複数の定型表現128、1つまたは複数のパラメータ130、1つまたは複数のポリシー132、応答データ134、およびテンプレート136を記憶してもよい。パラメータ130、ポリシー132、およびテンプレート136は、クライアントデバイス104とデータ処理システム102との間の音声ベースのセッションについての規則などの情報を含み得る。定型表現128は、クライアントデバイス104とデータ処理システム102との間の音声ベースのセッションがナビゲーションアプリケーション110およびナビゲータサービス106をいつ含むべきであるかについての規則を含み得る。定型表現128、パラメータ130、ポリシー132、およびテンプレート136はまた、別のソース(たとえば、データ処理システム102およびクライアントデバイス104)からインターフェース112を介して受信された別のデジタルアシスタントアプリケーション108のための情報を含み得る。応答データ134は、オーディオ出力のためのコンテンツ項目または関連するメタデータ、ならびに、クライアントデバイス104との1つまたは複数の通信セッションの一部であり得る入力オーディオメッセージを含み得る。
The
データ処理システム102は、少なくとも1つの計算リソースまたはサーバを含み得る。データ処理システム102は、少なくとも1つのインターフェース112を含み、それとインターフェースし、または別様にそれと通信することができる。データ処理システム102は、データ処理システム102上のデジタルアシスタントアプリケーション108の少なくとも1つのインスタンスを含み、それとインターフェースし、または別様にそれと通信することができる。データ処理システム102上のデジタルアシスタントアプリケーション108のインスタンスは、少なくとも1つのNLPコンポーネント114、少なくとも1つのオーディオ信号生成器コンポーネント122、および少なくとも1つのダイレクトアクションハンドラコンポーネント120を含み、それらとインターフェースし、または別様にそれらと通信することができる。データ処理システム102は、少なくとも1つの応答選択器コンポーネント124を含み、それとインターフェースし、または別様にそれと通信することができる。データ処理システム102は、少なくとも1つのデータリポジトリ126を含み、それとインターフェースし、または別様にそれと通信することができる。少なくとも1つのデータリポジトリ126は、定型表現128、パラメータ130、ポリシー132、応答データ134、およびテンプレート136を、1つまたは複数のデータ構造またはデータベースに含め、または記憶することができる。データリポジトリ126は、1つまたは複数のローカルデータベースまたは分散されたデータベースを含んでもよく、データベース管理を含んでもよい。
The
データ処理システム102のコンポーネントは各々、データベースリポジトリ128または148と通信するように構成されるプログラマブルロジックアレイエンジンまたはモジュールなどの、少なくとも1つの処理ユニットまたは他のロジックデバイスを含み得る。データ処理システム102のコンポーネントは、別々のコンポーネント、単一のコンポーネント、または複数のデータ処理システム102の一部であり得る。データ処理システム102などの、システム100およびそのコンポーネントは、1つまたは複数のプロセッサ、ロジックデバイス、または回路などの、ハードウェア要素を含み得る。
Each component of the
データ処理システム102はインターフェース112を含み得る。インターフェース112は、たとえばデータパケットを使用して情報を受信して送信するように構成され、構築され、または動作可能であり得る。インターフェース112は、ネットワークプロトコルなどの、1つまたは複数のプロトコルを使用して情報を受信して送信することができる。インターフェース112は、ハードウェアインターフェース、ソフトウェアインターフェース、有線インターフェース、またはワイヤレスインターフェースを含み得る。インターフェース112は、システム100のコンポーネントが互いに通信することを可能にするデータインターフェースまたはネットワークインターフェースであり得る。データ処理システム102のインターフェース112は、ネットワーク156を介してクライアントデバイス104またはナビゲータサービス106に、アクションデータ構造、オーディオ信号、または他のデータを含む1つまたは複数のデータパケットを提供または送信することができる。たとえば、データ処理システム102は、データリポジトリ126からの、またはオーディオ信号生成器コンポーネント122からの出力信号を、クライアントデバイス104に提供することができる。データ処理システム102はまた、データパケット送信を介して、アクションデータ構造において示される機能を実行するようにクライアントデバイス104に命令することができる。出力信号は、データ処理システム102(または他のコンピューティングデバイス)からの1つまたは複数のデータパケット(または他の通信プロトコル)として、取得され、生成され、変換され、クライアントデバイス104に送信され得る。インターフェース112は、あるフォーマットから別のフォーマットにデータを変換またはフォーマッティングするのを容易にすることができる。たとえば、インターフェース112は、ソフトウェアコンポーネントなどの様々なコンポーネント間で通信するための定義を含むアプリケーションプログラミングインターフェース(「API」)を含み得る。
The
データ処理システム102は、データ処理システム102のインターフェース112およびクライアントコンピューティングデバイスのドライブコンポーネントに入力オーディオ信号を通信して出力オーディオ信号または視覚出力をレンダリングするための、クライアントデバイス104上のデジタルアシスタントアプリケーション108のインスタンスなどの、クライアントデバイス104にインストールされたアプリケーション、スクリプト、またはプログラムを含み得る。データ処理システム102は、データパケット、デジタルファイル、または、入力オーディオ信号(または複数の入力オーディオ信号)を含む、もしくは識別する他の信号を受信することができる。クライアントデバイス104は、トランスデューサ150を介してオーディオ信号を検出し、アナログデジタルコンバータを介してアナログオーディオ信号をデジタルファイルに変換することができる。たとえば、オーディオドライバは、アナログデジタルコンバータコンポーネントを含み得る。プリプロセッサコンポーネントは、ネットワーク156上でデータパケットを介して送信され得るデジタルファイルにオーディオ信号を変換することができる。
The
データ処理システム102のデジタルアシスタントアプリケーション108のインスタンスは、クライアントデバイス104のセンサ154によって検出される入力オーディオ信号を含むデータパケットを受信または取得するために、NLPコンポーネント114を実行し、または走らせることができる。データパケットはデジタルファイルを提供することができる。NLPコンポーネント114は、オーディオ信号を備えるデジタルファイルもしくはデータパケットを受信または取得し、オーディオ信号を解析することができる。たとえば、NLPコンポーネント114は、人とコンピュータとの間の対話を可能にできる。NLPコンポーネント114は、自然言語を理解して、データ処理システム102が人または自然言語の入力から意図を導くことを可能にするための技法を用いて、構成され得る。NLPコンポーネント114は、統計的機械学習などの、機械学習に基づく技法を含んでもよく、またはそれを用いて構成されてもよい。NLPコンポーネント114は、決定木、統計モデル、または確率モデルを利用して、入力オーディオ信号を解析することができる。NLPコンポーネント114は、たとえば、名前を付けられたエンティティの認識(たとえば、テキストのストリームが与えられると、テキストの中のどの項目が人々または場所などの名前と対応付けられるかを決定し、人物、位置(たとえば「自宅」)、または組織などの、各々のそのような名前がどのようなタイプであるかを決定する)、自然言語の生成(たとえば、コンピュータデータベースからの情報または意味的意図を理解可能な人の言語に変換する)、自然言語の理解(たとえば、コンピュータモジュールが操作できる1次論理構造などのより正式な表現へとテキストを変換する)、機械翻訳(たとえば、ある人の言語から別の人の言語にテキストを自動的に翻訳する)、形態素区分(たとえば、語を個々の形態素に分けて形態素のクラスを識別する、これは、対象の言語の形態論または語の構造の複雑さに基づき困難であり得る)、質問への回答(たとえば、特定の回答または自由な回答であり得る、人の言語の質問に対する回答を決定する)、または意味的処理(たとえば、識別された語を同様の意味をもつ他の語と関連付けるために、語の識別とその意味の符号化の後で行われ得る処理)などの、機能を実行することができる。
An instance of Digital Assistant Application 108 in
NLPコンポーネント114は、(たとえば、データリポジトリ126に)記憶されている音響波の代表的なセットと入力信号を比較し、最も近い一致を選ぶことによって、入力オーディオ信号を認識されるテキストへと変換することができる。音響波のセットは、データリポジトリ126、またはデータ処理システム102がアクセス可能な他のデータベースに記憶され得る。代表的な波形は、ユーザの大きな集合にわたって生成され、次いで、ユーザからの発話サンプルを用いて強化され得る。オーディオ信号が認識されるテキストへと変換された後で、NLPコンポーネント114は、データ処理システム102が役立つことができる活動と、たとえばユーザにわたる訓練を介してまたは手動の指定を通じて関連付けられる、語とテキストを照合する。NLPコンポーネント114は、画像またはビデオ入力をテキストまたはデジタルファイルに変換することができる。NLPコンポーネント114は、画像もしくはビデオ入力を処理し、分析し、または解釈して、活動を実行し、要求を生成し、または、データ構造を選択もしくは識別することができる。
NLP component 114 converts the input audio signal into recognized text by comparing the input signal with a representative set of acoustic waves stored (for example, in the data repository 126) and choosing the closest match. can do. The set of acoustic waves may be stored in the
データ処理システム102は、入力オーディオ信号に加えて、またはその代わりに、画像またはビデオ入力信号を受信することができる。データ処理システム102は、たとえば、画像解釈技法、コンピュータビジョン、機械学習エンジン、または、画像もしくはビデオを認識もしくは解釈するための他の技法を使用して、画像またはビデオ入力信号を処理し、画像またはビデオをデジタルファイルに変換することができる。1つまたは複数の画像解釈技法、コンピュータビジョン技法、または機械学習技法は、集合的にイメージング技法と呼ばれ得る。データ処理システム102(たとえば、NLPコンポーネント114)は、オーディオ処理技法に加えて、またはその代わりに、イメージング技法を用いて構成され得る。
The
NLPコンポーネント114は、入力オーディオ信号を取得することができる。入力オーディオ信号から、NLPコンポーネント114は、少なくとも1つの要求、要求に対応する少なくとも1つのトリガキーワード、および1つまたは複数のキーワードを識別することができる。要求は、入力オーディオ信号の意図、デジタルコンポーネント、または主題を示すことができる。トリガキーワードは、とられる可能性の高い行動のタイプを示すことができる。たとえば、NLPコンポーネント114は、入力オーディオ信号を解析して、エンドユーザの連絡先リストの中のある連絡先を発見するための少なくとも1つの要求を識別することができる。トリガキーワードは、とられるべき行動を示す少なくとも1つの語、句、語根もしくは部分語、または派生語を含み得る。たとえば、入力オーディオ信号からのトリガキーワード「検索する」または「見つける」は、クエリ検索を実行するための要求を示すことができる。この例では、入力オーディオ信号(または識別された要求)は、クエリ検索の意図を直接表さないが、トリガキーワードは、クエリ検索が要求により示される少なくとも1つの他の活動に対する付随する活動であることを示す。 The NLP component 114 can acquire the input audio signal. From the input audio signal, the NLP component 114 can identify at least one request, at least one trigger keyword corresponding to the request, and one or more keywords. The request can indicate the intent, digital component, or subject of the input audio signal. Trigger keywords can indicate the type of action that is likely to be taken. For example, NLP component 114 can parse the input audio signal to identify at least one request to find a contact in the end user's contact list. The trigger keyword can include at least one word, phrase, root or subword, or derivative that indicates the action to be taken. For example, the trigger keyword "search" or "find" from the input audio signal can indicate a request to perform a query search. In this example, the input audio signal (or identified request) does not directly represent the intent of the query search, but the trigger keyword is an incidental activity to which the query search indicates at least one other activity indicated by the request. Show that.
NLPコンポーネント114は、入力オーディオ信号を解析して、要求およびトリガキーワードを識別し、決定し、取り出し、または別様に取得することができる。たとえば、NLPコンポーネント114は、意味的処理技法を入力オーディオ信号に適用して、トリガキーワードまたは要求を識別することができる。NLPコンポーネント114は、意味的処理技法を入力オーディオ信号に適用して、第1のトリガキーワードおよび第2のトリガキーワードなどの1つまたは複数のトリガキーワードを含むトリガ句を識別することができる。たとえば、入力オーディオ信号は文章「アレックスの電話番号を探して」を含み得る。NLPコンポーネント114は、入力オーディオ信号がトリガキーワード「探す」を含むと決定することができる。NLPコンポーネント114は、要求がエンドユーザの連絡先リストを調べるためのものであると決定することができる。 The NLP component 114 can analyze the input audio signal to identify, determine, retrieve, or otherwise obtain the request and trigger keywords. For example, NLP component 114 can apply semantic processing techniques to the input audio signal to identify trigger keywords or requests. NLP component 114 can apply semantic processing techniques to the input audio signal to identify trigger phrases that contain one or more trigger keywords, such as a first trigger keyword and a second trigger keyword. For example, the input audio signal may contain the sentence "Look for Alex's phone number." NLP component 114 can determine that the input audio signal contains the trigger keyword "find". NLP component 114 can determine that the request is for examining the end user's contact list.
NLPコンポーネント114は、入力オーディオ信号から識別された1つまたは複数のキーワードがナビゲーションアプリケーション110の1つまたは複数の機能に言及するかどうかを決定することができる。入力オーディオ信号から識別された1つまたは複数のキーワードは、ナビゲーションアプリケーション110の識別子(たとえば、「GPSナビゲータA」)を含み得る。ナビゲーションアプリケーション110の識別子は、エンドユーザがどのアプリケーションに要求を行わせることを望むかを示すことができる。たとえば、入力オーディオ信号から変換されたテキストは、「GPSナビゲータAを使って自宅までの道順を教えて」を含み得る。この入力オーディオ信号において、キーワード「GPSナビゲータA」は、オーディオ入力信号において示される要求を行うためのナビゲーションアプリケーション110に対する識別子であり得る。NLPコンポーネント114は、ナビゲーションアプリケーション110の識別子を入力オーディオ信号が含むと決定することができる。入力オーディオ信号が識別子を含むとの決定に基づいて、NLPコンポーネント114は、入力オーディオ信号がナビゲーションアプリケーション110に言及すると決定することができる。さらに、デジタルアシスタントアプリケーション108は、本明細書において以下で詳述されるように、ナビゲーションアプリケーション110とインターフェースすることができる。逆に、NLPコンポーネント114は、入力オーディオ信号がナビゲーションアプリケーション110の識別子を含まないと決定することができる。入力オーディオ信号が識別子を含まないとの決定に基づいて、NLPコンポーネント114は、入力オーディオ信号がナビゲーションアプリケーション110に言及しないと決定することができる。加えて、デジタルアシスタントアプリケーション108は、入力オーディオ信号において示される要求を処理することができる。 The NLP component 114 can determine whether one or more keywords identified from the input audio signal refer to one or more functions of the navigation application 110. One or more keywords identified from the input audio signal may include an identifier for the navigation application 110 (eg, "GPS Navigator A"). The identifier of the navigation application 110 can indicate which application the end user wants to make the request. For example, the text converted from the input audio signal may include "Use GPS Navigator A to give directions to your home." In this input audio signal, the keyword "GPS navigator A" can be an identifier for the navigation application 110 for making the request indicated in the audio input signal. NLP component 114 can determine that the input audio signal contains the identifier of the navigation application 110. Based on the determination that the input audio signal contains an identifier, NLP component 114 can determine that the input audio signal refers to the navigation application 110. In addition, the digital assistant application 108 can interface with the navigation application 110, as detailed herein below. Conversely, NLP component 114 can determine that the input audio signal does not contain the identifier of the navigation application 110. Based on the determination that the input audio signal does not contain an identifier, NLP component 114 can determine that the input audio signal does not refer to the navigation application 110. In addition, the digital assistant application 108 can handle the requests indicated in the input audio signal.
NLPコンポーネント114は、ナビゲーションアプリケーション110に対する定型表現128を使用して、入力オーディオ信号から識別された1つまたは複数のキーワードがナビゲーションアプリケーション110の少なくとも1つの機能に言及するかどうかを決定することができる。定型表現128は、入力オーディオ信号から識別されたキーワードがナビゲーションアプリケーション110の少なくとも1つの機能に言及するかどうかを決定するために照合すべきパターンを定義することができる。定型表現128はまた、入力オーディオ信号において示される命令を行うためにどのキーワードを使用すべきかを指定することができる。たとえば、定型表現128は、{[要求]、[参照キーワード]、「補助キーワード」}という形式であり得る。ナビゲーションアプリケーション110の機能に言及していると決定されるべき入力オーディオ信号のキーワードに対して、定型表現128は、1つまたは複数のキーワードが、ナビゲーションアプリケーション110に対する要求および要求を行うためにパラメータとして使用される1つまたは複数の参照語を含むことを、指定することができる。定型表現128は、入力オーディオ信号から識別された1つまたは複数のキーワードにおける、一連の要求および参照キーワードを指定することができる。
The NLP component 114 can use the
定型表現128は、ナビゲーションアプリケーション110の機能に対応する要求に対するあらかじめ定められたキーワードの第1のセットを含み得る。あらかじめ定められたキーワードの第1のセットは、機能識別子(たとえば、「連れて行く」、「行く」、「見せる」、「道順」、および「見つける」)を含み得る。あらかじめ定められたキーワードの第1のセットの中の各々の機能識別子は、ナビゲーションアプリケーション110の機能のうちの1つと関連付けられ得る。定型表現128は、ナビゲーションアプリケーション110が機能に対応する要求を行うためのパラメータとして使用すべき、1つまたは複数の参照語に対するあらかじめ定められたキーワードの第2のセットを含み得る。あらかじめ定められたキーワードの第2のセットは、直示語(たとえば、「ここ」、「そこ」、「あそこ」、および「向こう」)を含み得る。あらかじめ定められたキーワードの第2のセットは、関心地点と関連付けられるキーワード(たとえば、「レストラン」、「ホテル」、「カフェ」、「ガソリンスタンド」、「公園」、および「空港」)も含み得る。定型表現128は、入力オーディオ信号において識別されるがあらかじめ定められたキーワードの第1のセットまたはキーワードの第2のセットと一致しないキーワードが、補助キーワードとして識別されるべきであることを指定することができる。定型表現128は、1つまたは複数の補助キーワードに対するあらかじめ定められたキーワードの第3のセットを含み得る。あらかじめ定められたキーワードの第3のセットは、クライアントデバイス104のディスプレイまたはナビゲーションアプリケーション110のビューポートと関連付けられるキーワード(たとえば、「左隅」、「右隅」、「上」、および「中央」)を含み得る。第3のセットの各キーワードは、クライアントデバイス104のディスプレイのサブセットエリアに対応し得る。定型表現128は、入力オーディオ信号から識別された1つまたは複数のキーワードにおいて、一連の要求および参照キーワードを指定することができる。定型表現128は、入力オーディオ信号が第1のあらかじめ定められたセットのうちの1つと一致する1つまたは複数のキーワードを含むとの決定に応答して、残りのキーワードのうちの少なくとも1つが要求を行うための1つまたは複数のパラメータとして使用されるべきであると指定することができる。
The boilerplate 128 may include a first set of predefined keywords for the requirements corresponding to the functionality of the navigation application 110. The first set of predefined keywords may include functional identifiers (eg, "take", "go", "show", "direction", and "find"). Each function identifier in the first set of predetermined keywords can be associated with one of the functions of the navigation application 110. The boilerplate 128 may include a second set of predefined keywords for one or more reference terms that the navigation application 110 should use as parameters to make functional requests. A second set of predefined keywords may include deixis (eg, "here", "there", "there", and "beyond"). A second set of predefined keywords may also include keywords associated with the point of interest (eg, "restaurant", "hotel", "cafe", "gas station", "park", and "airport"). .. The
1つまたは複数のキーワードがナビゲーションアプリケーション110の少なくとも1つの機能に言及しているかどうかを決定する際、NLPコンポーネント114は、1つまたは複数のキーワードを定型表現128と比較することができる。NLPコンポーネント114は、入力オーディオ信号から識別されたキーワードの1つまたは複数の並べ替え(たとえば、n-gram)を定型表現128と比較することもできる。NLPコンポーネント114は、1つまたは複数のキーワードを、定型表現128によって指定されるあらかじめ定められたキーワードの第1のセットと比較することができる。NLPコンポーネント114は、すべてのキーワードと、あらかじめ定められたキーワードの第1のセットのすべてとの間に一致がないと決定することができる。入力オーディオ信号のすべてのキーワードと第1のセットのいずれとの間にも一致がないとの決定に応答して、NLPコンポーネント114は、入力オーディオ信号がナビゲーションアプリケーション110のいずれの機能にも言及してないと決定することができる。NLPコンポーネント114は、入力オーディオ信号がそうではなくデジタルアシスタントアプリケーション108の機能の1つに言及していると決定することができる。デジタルアシスタントアプリケーション108は、要求を行うためのキーワードを用いてさらなる処理を実行することができる。
In determining whether one or more keywords refer to at least one feature of the navigation application 110, NLP component 114 can compare one or more keywords with
一方、一致の決定に応答して、NLPコンポーネント114は、入力オーディオ信号がナビゲーションアプリケーション110の少なくとも1つの機能に言及していると決定することができる。NLPコンポーネント114は、あらかじめ定められたキーワードの第1のセットからの機能識別子が要求に対応する少なくとも1つのキーワードと一致することを識別することができる。NLPコンポーネント114は、ナビゲーションアプリケーション110のナビゲーション案内プロセスの機能のうちの1つに対応する要求タイプを決定することができる。ナビゲーションアプリケーション110のナビゲーション案内プロセスは、位置発見動作および経路決定動作を含み得る。要求タイプは、位置発見動作および経路決定動作を含み得る。機能識別子は、要求タイプのうちの1つと関連付けられ得る。機能識別子の関連付けに基づいて、NLPコンポーネント114は、入力オーディオ信号から解析された要求によって示される要求タイプを決定することができる。 On the other hand, in response to the match decision, the NLP component 114 can determine that the input audio signal refers to at least one function of the navigation application 110. The NLP component 114 can identify that the functional identifier from the first set of predetermined keywords matches at least one keyword corresponding to the request. The NLP component 114 can determine the request type corresponding to one of the functions of the navigation guidance process of the navigation application 110. The navigation guidance process of the navigation application 110 may include a position finding operation and a routing operation. The request type may include a position finding operation and a routing operation. The function identifier can be associated with one of the request types. Based on the functional identifier association, the NLP component 114 can determine the request type indicated by the request parsed from the input audio signal.
NLPコンポーネント114は、要求を行うために1つまたは複数のパラメータとして使用すべき入力オーディオ信号のキーワードから、1つまたは複数の参照キーワードおよび補助キーワードも識別することができる。NLPコンポーネント114は、1つまたは複数の残りのキーワードをあらかじめ定められたキーワードの第2のセットと比較することができる。NLPコンポーネント114は、少なくとも1つのキーワードと、あらかじめ定められたキーワードの第2のセットのうちの少なくとも1つとの間の一致を決定することができる。一致の決定に応答して、NLPコンポーネント114は、要求を行うために使用すべき参照キーワードのうちの少なくとも1つとして、少なくとも1つのキーワードを識別することができる。NLPコンポーネント114は、要求を行うためにナビゲーションアプリケーション110が参照キーワードおよび補助キーワードとして使用すべき1つまたは複数のキーワードを識別するために、意味分析を実行することもできる。意味分析は、参照キーワードを識別するためのデイクシスおよび首句反復分析を含み得る。NLPコンポーネント114は、要求および参照キーワード以外の入力オーディオ信号から識別された1つまたは複数の残りのキーワードを、補助キーワードとして識別することができる。NLPコンポーネント114は、1つまたは複数の残りのキーワードをあらかじめ定められたキーワードの第3のセットと比較することができる。NLPコンポーネント114は、少なくとも1つのキーワードと、あらかじめ定められたキーワードの第3のセットのうちの少なくとも1つとの間の一致を決定することができる。一致の決定に応答して、NLPコンポーネント114は、少なくとも1つのキーワードを補助キーワードのうちの少なくとも1つとして識別することができる。入力オーディオ信号からの要求および参照キーワードの識別に基づいて、NLPコンポーネント114は、入力オーディオ信号がナビゲーションアプリケーション110の機能に言及していると決定することができる。たとえば、「画面の角にある店ABCに連れて行って」という入力オーディオ信号に対して、NLPコンポーネント114は、「連れて行って」と「店ABC」の両方が含まれていることに基づいて、入力オーディオ信号がナビゲーションアプリケーション110の機能に言及していると決定することができる。この例では、定型表現128および意味分析技法を使用して、NLPコンポーネント114は、「連れて行って」を要求として、「店ABC」を要求を行うための参照キーワードとして、および「画面の角にある」を補助キーワードとして決定することができる。
NLP component 114 can also identify one or more reference and auxiliary keywords from the keywords of the input audio signal that should be used as one or more parameters to make the request. NLP component 114 can compare one or more remaining keywords with a second set of predefined keywords. NLP component 114 may determine a match between at least one keyword and at least one of a second set of predetermined keywords. In response to a match decision, NLP component 114 may identify at least one keyword as at least one of the reference keywords that should be used to make the request. NLP component 114 can also perform semantic analysis to identify one or more keywords that the navigation application 110 should use as reference and auxiliary keywords to make the request. Semantic analysis may include dyxis and anaphora analysis to identify reference keywords. The NLP component 114 can identify one or more remaining keywords identified from the input audio signal other than the request and reference keywords as auxiliary keywords. NLP component 114 can compare one or more remaining keywords with a third set of predefined keywords. NLP component 114 may determine a match between at least one keyword and at least one of a third set of predetermined keywords. In response to a match decision, NLP component 114 can identify at least one keyword as at least one of the auxiliary keywords. Based on the request from the input audio signal and the identification of the reference keyword, NLP component 114 can determine that the input audio signal refers to the functionality of the navigation application 110. For example, for an input audio signal that says "Take me to the store ABC in the corner of the screen," NLP component 114 is based on the fact that it contains both "Take me" and "Store ABC." It can be determined that the input audio signal refers to the function of the navigation application 110. In this example, using
データ処理システム102は、ナビゲーションインターフェースコンポーネント116のインスタンスを実行し、または走らせることができる。入力オーディオ信号がナビゲーションアプリケーション110の少なくとも1つの機能に言及しているとの決定に応答して、ナビゲーションインターフェースコンポーネント116は、クライアントデバイス104またはナビゲータサービス106上で実行しているナビゲーションアプリケーション110にアクセスすることができる。ナビゲーションインターフェースコンポーネント116は、デジタルアシスタントアプリケーション108とナビゲーションアプリケーション110との間で通信するための定義を含むアプリケーションプログラミングインターフェース(API)に従って、ナビゲーションアプリケーション110にアクセスすることができる。ナビゲーションインターフェースコンポーネント116は、ナビゲーションアプリケーション110にアクセスするためにAPIによって定義されるファンクションコールを呼び出すことができる。ナビゲーションインターフェースコンポーネント116は、共通のログイン(たとえば、アカウント識別子および認証証明書)、位置、ネットワーク、または他の結び付けるデータを通じて、デジタルアシスタントアプリケーション108と関連付けられるナビゲーションアプリケーション110を識別することができる。たとえば、エンドユーザは、デジタルアシスタントアプリケーション108およびナビゲーションアプリケーション110に対して、同じアカウントおよびログイン情報を使用した可能性がある。アクセスすることによって、ナビゲーションインターフェースコンポーネント116は、ナビゲーションアプリケーション110からデータを取り出すことができる。データは、ナビゲーションアプリケーション110のビューポートを通じて見えるベクターベースの地図146の部分の内容に関連してもよく、またはそれに対応してもよい。
The
アクセスする前に、ナビゲーションインターフェースコンポーネント116は、データがナビゲーションアプリケーション110から以前に受信されたかどうかを決定することもできる。デジタルアシスタントアプリケーション108は、以前に受信された入力オーディオ信号に応答して、ナビゲーションアプリケーション110にすでにアクセスしている可能性がある。以前に受信されたデータは、クライアントデバイス104に(たとえば、メモリに)保持され得る。ナビゲーションインターフェースコンポーネント116は、以前に受信されたデータおよび以前に受信されたデータの受信時間を識別することができる。ナビゲーションインターフェースコンポーネント116は、現在の入力オーディオ信号の受信の時間に対応する現在の時間を識別することもできる。ナビゲーションインターフェースコンポーネント116は、受信時間と現在の時間との間で経過した時間を、定められた閾値の時間と比較することができる。経過した時間が定められた閾値の時間より長いとの決定に応答して、ナビゲーションインターフェースコンポーネント116は、ナビゲーションアプリケーション110にアクセスすることに進むことができる。それ以外の場合、経過時間が定められた閾値の時間未満であるとの決定に応答して、ナビゲーションインターフェースコンポーネント116は、ナビゲーションアプリケーション110から以前に受信されたデータを取り出して使用することができる。 Prior to access, the navigation interface component 116 can also determine if the data was previously received from the navigation application 110. The digital assistant application 108 may have already accessed the navigation application 110 in response to a previously received input audio signal. Previously received data may be retained on the client device 104 (eg, in memory). The navigation interface component 116 can identify previously received data and previously received data reception times. The navigation interface component 116 can also identify the current time corresponding to the time of reception of the current input audio signal. The navigation interface component 116 can compare the time elapsed between the received time and the current time with a set threshold time. In response to the determination that the elapsed time is longer than the defined threshold time, the navigation interface component 116 can proceed to access the navigation application 110. Otherwise, in response to the determination that the elapsed time is less than a defined threshold time, the navigation interface component 116 can retrieve and use previously received data from the navigation application 110.
ナビゲーションアプリケーション110にアクセスする際、ナビゲーションインターフェースコンポーネント116は、ナビゲーションアプリケーション110の少なくとも1つの機能に言及している入力オーディオ信号を受信したクライアントデバイス104がナビゲーションアプリケーション110を実行しているかどうか、またはそのインスタンスを有するかどうかを、決定することができる。ナビゲーションインターフェースコンポーネント116によってアクセスされるナビゲーションアプリケーション110は、入力オーディオ信号を受信したクライアントデバイス104とは異なるクライアントデバイス104上で実行していてもよく、またはそこに存在していてもよい。クライアントデバイス104がナビゲーションアプリケーション110を実行している、または有するとの決定に応答して、ナビゲーションインターフェースコンポーネント116は、同じクライアントデバイス104上のナビゲーションアプリケーション110にアクセスすることができる。一方、クライアントデバイス104がナビゲーションアプリケーション110を実行していない、またはそれを欠いているとの決定に応答して、ナビゲーションインターフェースコンポーネント116は、ナビゲーションアプリケーション110を実行している別のクライアントデバイス104を識別することができる。ナビゲーションインターフェースコンポーネント116は、共通のログイン(たとえば、アカウント識別子および認証証明書)、位置、ネットワーク、または他の結び付けるデータを通じて、入力オーディオ信号を受信したクライアントデバイス104と関連付けられる別のクライアントデバイス104を識別することができる。ナビゲーションインターフェースコンポーネント116は、他のクライアントデバイス104がナビゲーションアプリケーション110を実行していること、またはそのインスタンスを有することを決定することができる。ナビゲーションインターフェースコンポーネント116は、入力オーディオ信号を受信したクライアントデバイス104と関連付けられる他のクライアントデバイス104上で実行している、またはその上に存在するナビゲーションアプリケーション110にアクセスすることができる。ナビゲーションインターフェースコンポーネント116は、クライアントデバイス104またはナビゲータサービス106上で実行しているナビゲーションアプリケーション110にアクセス要求を送り、または送信することができる。アクセス要求は、デジタルアシスタントアプリケーション108およびナビゲーションアプリケーション110のための結び付けるデータを含み得る。
When accessing the navigation application 110, the navigation interface component 116 refers to whether or not the
データ処理システム102またはナビゲータサービス106は、ナビゲーションアプリケーション110のデジタルアシスタントインターフェースコンポーネント138のインスタンスを実行し、または走らせることができる。デジタルアシスタントインターフェースコンポーネント138は、ナビゲーションアプリケーション110にアクセスするナビゲーションインターフェースコンポーネント116を識別することができる。アクセスの識別に応答して、デジタルアシスタントインターフェースコンポーネント138は、ナビゲーションアプリケーション110のビューポートに表示されるベクターベースの地図146の部分に対応する基準枠内の点位置のセットを識別することができる。上で論じられたように、各点位置は、人工物および自然物のうちの1つに対応してもよく、地理的座標と関連付けられてもよく、少なくとも1つの識別子を有してもよい。点位置のセットを識別するために、デジタルアシスタントインターフェースコンポーネント138は、ナビゲーションアプリケーション110のビューポートにおいて見える、または表示されるベクターベースの地図146の部分を識別することができる。ベクターベースの地図146のその部分は、ベクターベースの地図146の全体より小さくてもよく、ナビゲーションアプリケーション110のビューポートに表示される地理的領域に対応することができる。デジタルアシスタントインターフェースコンポーネント138は、ナビゲーションアプリケーション110のビューポートを通じて見えるベクターベースの地図146の部分の寸法および座標を識別することができる。座標は、左上の座標および右下の座標などの、ナビゲーションアプリケーション110のビューポートを通じて見えるベクターベースの地図146の部分を定義することができる。座標は、地理的な地図上の地理的座標に対応し得る。ベクターベースの地図146のその部分は、クライアントデバイス104上で実行しているナビゲーションアプリケーション110のインスタンスに対する基準枠に対応し得る。
The
デジタルアシスタントインターフェースコンポーネント138は、ビューポートを通じて見えるベクターベースの地図146の部分を、クライアントデバイス104上で実行しているナビゲーションアプリケーション110に対する基準枠として設定または識別することができる。基準枠は、ナビゲーションアプリケーション110のビューポートにおいて表示されるベクターベースの地図146の寸法、座標、および他の尺度に対応してもよく、クライアントデバイス104のエンドユーザに特有であってもよい。ビューポートを通じて見えるベクターベースの地図146の部分の寸法および座標を使用して、デジタルアシスタントインターフェースコンポーネント138は、基準枠の部分を定義する寸法および座標を識別することができる。座標は、左上の座標および右下の座標などの、基準枠上の座標に対応し得る。デジタルアシスタントインターフェースコンポーネント138は、各点位置の地理的座標を、ビューポートにおいて表示されるベクターベースの地図146の部分に対して識別される寸法および座標と比較することができる。比較に基づいて、デジタルアシスタントインターフェースコンポーネント138は、ビューポートを通じて見えるベクターベースの地図146の部分に対応する基準枠内の点位置のセットを選択または識別することができる。デジタルアシスタントインターフェースコンポーネント138は、デジタルアシスタントアプリケーション108のナビゲーションインターフェースコンポーネント116に点位置のセットを提供することができる。
The digital assistant interface component 138 can set or identify the portion of the vector-based
デジタルアシスタントインターフェースコンポーネント138は、ナビゲーションアプリケーション110のビューポートに関する表示情報をデジタルアシスタントアプリケーション108のナビゲーションインターフェースコンポーネント116に提供することができる。デジタルアシスタントインターフェースコンポーネント138は、デジタルアシスタントアプリケーション108のナビゲーションインターフェースコンポーネント116に、ビューポートを通じて見えるベクターベースの地図146の部分の寸法および座標を提供することができる。デジタルアシスタントインターフェースコンポーネント138は、ナビゲーションアプリケーション110自体のビューポートの寸法を識別することができる。ビューポートの寸法は、ピクセルの数を使用して幅対高さで定義され得る。デジタルアシスタントインターフェースコンポーネント138は、デジタルアシスタントアプリケーション108のナビゲーションインターフェースコンポーネント116にナビゲーションアプリケーション110のビューポートの寸法を提供することができる。
The digital assistant interface component 138 can provide display information about the viewport of the navigation application 110 to the navigation interface component 116 of the digital assistant application 108. The digital assistant interface component 138 can provide the navigation interface component 116 of the digital assistant application 108 with the dimensions and coordinates of the portion of the vector-based
点位置のセットを識別することと併せて、デジタルアシスタントインターフェースコンポーネント138は、ナビゲーションアプリケーション110のビューポートの通じて見えるベクターベースの地図146の部分内のクライアントデバイス104の現在地を識別することができる。デジタルアシスタントインターフェースコンポーネント138は、地理的測位システム(GPS)インターフェースにアクセスすることができる。GPSインターフェースはそして、GPS衛星と通信して、ナビゲーションアプリケーション110を実行しているクライアントデバイス104の現在の地理的座標を識別または受信することができる。GPSインターフェースは、GPS衛星から受信されたクライアントデバイス104の地理的座標を、ベクターベースの地図146上の位置識別子に変換することができる。位置識別子は、ベクターベースの地図146への、物理的な世界の地理的座標に割り当てられるインデックスであり得る。地理的座標から位置識別子への変換は、設定されたマッピングまたは関数に従い得る。変換されると、デジタルアシスタントインターフェースコンポーネント138は、クライアントデバイス104の位置識別子をデジタルアシスタントアプリケーション108のナビゲーションインターフェースコンポーネント116に提供することができる。デジタルアシスタントインターフェースコンポーネント138は、各々の識別された点位置の位置識別子をナビゲーションインターフェースコンポーネント116に提供することもできる。
Along with identifying the set of point locations, the Digital Assistant Interface Component 138 can identify the current location of the
デジタルアシスタントインターフェースコンポーネント138は、ナビゲーションアプリケーション110のビューポートにおいて見える、または表示されるベクターベースの地図146の部分の外側にある点位置の別のセットを識別することもできる。ナビゲーションアプリケーション110は、入力オーディオ信号が受信されるとき、ベクターベースの地図146上で出発地から目的地への経路を決定するために、ナビゲーション案内プロセスの経路決定動作を実行していてもよい。目的地および指定された位置は、ナビゲーションアプリケーション110のビューポートの外側にある、ベクターベースの地図146のその部分の外側にあるベクターベースの地図146上の位置に対応し得る。デジタルアシスタントインターフェースコンポーネント138は、経路決定動作から目的地を識別することができる。デジタルアシスタントインターフェースコンポーネント138は、目的地からある定められた近接(たとえば、1kmから5km)の範囲内にあるベクターベースの地図146の部分を決定することができる。定められた近接の範囲内にあるベクターベースの地図146の部分は、目的地を含めるように寸法および座標を使用して定義され得る。定められた近接の範囲内のベクターベースの地図146の部分は、ナビゲーションアプリケーション110のビューポートに現在表示されているベクターベースの地図146の部分に等しいサイズを有し得る。デジタルアシスタントインターフェースコンポーネント138は、目的地から定められた近接の範囲内にあるベクターベースの地図146の部分を、基準枠の一部として設定または識別することができる。
Digital assistant interface component 138 can also identify another set of point locations outside the portion of vector-based
目的地から定められた近接の範囲内にあるベクターベースの地図146の部分の寸法および座標を使用して、デジタルアシスタントインターフェースコンポーネント138は、基準枠の部分を定義する寸法および座標を識別することができる。座標は、ベクターベースの地図146上の左上の座標および右下の座標などの、基準枠上の座標に対応し得る。デジタルアシスタントインターフェースコンポーネント138は、各点位置の地理的座標を、ベクターベースの地図146のその部分に対して識別される寸法および座標と比較することができる。比較に基づいて、デジタルアシスタントインターフェースコンポーネント138は、目的地から定められた近接の範囲内にあるベクターベースの地図146のその部分に対応する、基準枠内の点位置のセットを選択または識別することができる。デジタルアシスタントインターフェースコンポーネント138は、デジタルアシスタントアプリケーション108のナビゲーションインターフェースコンポーネント116に点位置のセットを提供することができる。点位置のセットを提供する際に、デジタルアシスタントインターフェースコンポーネント138は、ナビゲーションアプリケーション110のビューポートを通じて見える、または見えない、ベクターベースの地図146の部分に対応するものとして点位置を標識することができる。
Using the dimensions and coordinates of the portion of the vector-based
ナビゲーションアプリケーション110にアクセスするナビゲーションインターフェースコンポーネント116の識別に応答して、デジタルアシスタントインターフェースコンポーネント138は、ナビゲーションアプリケーション110によって受信される検索語のセットを識別することができる。検索語は、位置発見器コンポーネント140または経路決定器コンポーネント142によって実行される機能などの、ナビゲーション案内プロセスを実行する際にナビゲーションアプリケーション110によって以前に受信された1つまたは複数のキーワードを含み得る。たとえば、ナビゲーションアプリケーション110のエンドユーザは、近隣の文房具店を探すために以前に「文房具店」とタイプした可能性がある。別の例では、ナビゲーションアプリケーション110は、NLPコンポーネント114によって入力オーディオ信号から変換された「タワーABC」という、そのような名称のタワーを見つけるための問合せを以前に受信した可能性がある。以前に受信された検索語は、ナビゲーションアプリケーション110に記憶されて保持され得る。各検索語はまた、検索語がナビゲーションアプリケーション110によっていつ受信されたかを示す受信タイムスタンプと関連付けられてもよく、またはそれによってインデクシングされてもよい。デジタルアシスタントインターフェースコンポーネント138は、デジタルアシスタントアプリケーション108による入力オーディオ信号の受信の前のある定められた時間枠内にナビゲーションアプリケーション110によって以前に受信された、検索語のセットを選択または識別することができる。定められる時間枠は、15分から2時間にわたり得る。デジタルアシスタントインターフェースコンポーネント138は、入力オーディオ信号の受信の時間またはナビゲーションインターフェースコンポーネント116がナビゲーションアプリケーション110にアクセスする時間を識別することができる。デジタルアシスタントインターフェースコンポーネント138は、検索語の受信タイムスタンプを、入力オーディオ信号の受信またはアクセスの時間、および定められた時間枠と比較することができる。デジタルアシスタントインターフェースコンポーネント138は、入力オーディオ信号の受信またはアクセスの時間の、定められた時間枠内の受信タイムスタンプを伴う、検索語のセットを識別または選択することができる。 In response to the identification of the navigation interface component 116 that accesses the navigation application 110, the digital assistant interface component 138 can identify the set of search terms received by the navigation application 110. The search term may include one or more keywords previously received by the navigation application 110 in performing the navigation guidance process, such as the functions performed by the location detector component 140 or the routing device component 142. For example, the end user of navigation application 110 may have previously typed "stationery store" to find a nearby stationery store. In another example, the navigation application 110 may have previously received a query to find a tower with such a name, "Tower ABC," which was converted from the input audio signal by NLP component 114. Previously received search terms may be stored and retained in the navigation application 110. Each search term may also be associated with, or indexed by, a receive time stamp indicating when the search term was received by the navigation application 110. The digital assistant interface component 138 can select or identify a set of search terms previously received by the navigation application 110 within a defined time frame prior to the reception of the input audio signal by the digital assistant application 108. .. The set time frame can range from 15 minutes to 2 hours. The digital assistant interface component 138 can identify the time when the input audio signal is received or the time when the navigation interface component 116 accesses the navigation application 110. The digital assistant interface component 138 can compare the reception time stamp of the search term with the time of reception or access of the input audio signal and the defined time frame. Digital Assistant Interface Component 138 can identify or select a set of search terms with a receive time stamp within a defined time frame for the time of reception or access of an input audio signal.
データ処理システム102は、デジタルアシスタントアプリケーション108またはナビゲーションアプリケーション110のジオロケーション感知コンポーネント118のインスタンスを実行し、または走らせることができる。ナビゲータサービス106は、ナビゲーションアプリケーション110のジオロケーション感知コンポーネント118のインスタンスを実行し、または走らせることができる。入力オーディオ信号がナビゲーションアプリケーション110の少なくとも1つの機能に言及するとの決定に応答して、ジオロケーション感知コンポーネント118は、デジタルアシスタントアプリケーション108を実行しているクライアントデバイス104の少なくとも1つのセンサ154から取得されたデータを取り出すことができる。ジオロケーション感知コンポーネント118によってアクセスされるセンサ154は、とりわけ、慣性運動ユニット、加速度計、ジャイロスコープ、運動検出器、GPSセンサ、および位置センサを含み得る。取り出されたデータを使用して、ジオロケーション感知コンポーネント118は、デジタルアシスタントアプリケーション108を実行しているクライアントデバイス104の尺度の中でもとりわけ、移動の方向、場所、および速度を決定または識別することができる。ジオロケーション感知コンポーネント118はさらに、複数の測定結果を使用して、デジタルアシスタントアプリケーション108を実行しているクライアントデバイス104の尺度の中でもとりわけ、移動の方向の変化、場所の変化、および速度の変化を決定することができる。変化は、定められた間隔でサンプリングされた1つまたは複数の以前の測定結果に対して相対的なものであり得る。ジオロケーション感知コンポーネント118は、ナビゲーションアプリケーション110を実行しているクライアントデバイス104の尺度の中でもとりわけ、移動の方向、場所、および速度を決定または識別することができる。ジオロケーション感知コンポーネント118はさらに、複数の測定結果を使用して、ナビゲーションアプリケーション110を実行しているクライアントデバイス104の尺度の中でもとりわけ、移動の方向の変化、場所の変化、および速度の変化を決定することができる。変化は、定められた間隔でサンプリングされる1つまたは複数の以前の測定結果に対して相対的なものであり得る。
The
ジオロケーション感知コンポーネント118によって識別される測定結果を使用して、デジタルアシスタントインターフェースコンポーネント138は、ナビゲーションアプリケーション110のビューポートに以前に表示されていたベクターベースの地図146の部分の点位置の別のセットを識別することができる。デジタルアシスタントインターフェースコンポーネント138は、ジオロケーション感知コンポーネント118からの移動の方向、速度、および場所の1つまたは複数の測定結果に基づいて、ベクターベースの地図146の以前に表示されていた部分を識別することができる。デジタルアシスタントインターフェースコンポーネント138はまた、ベクターベースの地図146の現在表示されている部分を識別することができる。移動の方向の変化、場所の変化、および速度の変化、ならびに、ベクターベースの地図146の現在表示されている部分の変化を使用して、デジタルアシスタントインターフェースコンポーネント138は、ベクターベースの地図146の以前に表示されていた部分を決定することができる。移動の方向の変化、場所の変化、および速度の変化は、現在より定められた長さ(たとえば、15秒から3分)だけ前の時間に対して相対的なものであり得る。ベクターベースの地図146の現在表示されている部分から、デジタルアシスタントインターフェースコンポーネント138は、以前の測定された場所からの変化に基づいて、ベクターベースの地図146の別の部分に移ることができる。移ると、デジタルアシスタントインターフェースコンポーネント138は、ベクターベースの地図146の以前に表示されていた部分を識別することができる。
Using the measurement results identified by the geolocation sensing component 118, the digital assistant interface component 138 uses another set of point locations for the portion of the vector-based
デジタルアシスタントインターフェースコンポーネント138は、ベクターベースの地図146の以前に表示されていた部分を、基準枠の一部として、ベクターベースの地図146の現在表示されている部分として設定または識別することができる。設定されると、基準枠の一方の部分はベクターベースの地図146の現在表示されている部分に対応してもよく、基準枠の別の部分はベクターベースの地図146の以前に表示されていた部分に対応してもよい。デジタルアシスタントインターフェースコンポーネント138は、ベクターベースの地図146の以前に表示されていた部分の寸法および座標を識別することができる。座標は、ベクターベースの地図146上の左上の座標および右下の座標などの、基準枠上の座標に対応し得る。デジタルアシスタントインターフェースコンポーネント138は、各点位置の地理的座標を、ベクターベースの地図146の以前に表示されていた部分に対して識別された寸法および座標と比較することができる。比較に基づいて、デジタルアシスタントインターフェースコンポーネント138は、ベクターベースの地図146の以前に表示されていた部分に対応する基準枠内の点位置のセットを選択または識別することができる。デジタルアシスタントインターフェースコンポーネント138は、デジタルアシスタントアプリケーション108のナビゲーションインターフェースコンポーネント116に点位置のセットを提供することができる。
The digital assistant interface component 138 can set or identify the previously displayed portion of the vector-based
加えて、デジタルアシスタントインターフェースコンポーネント138は、ジオロケーション感知コンポーネント118から、移動の方向、速度、および場所の1つまたは複数の測定結果に基づいて、ベクターベースの地図146の表示されるべき部分を識別することができる。デジタルアシスタントインターフェースコンポーネント138は、ベクターベースの地図146の現在表示されている部分も識別することができる。移動の方向の変化、場所の変化、および速度の変化、ならびに、ベクターベースの地図146の現在表示されている部分を使用して、デジタルアシスタントインターフェースコンポーネント138は、ベクターベースの地図146の表示されるべき部分を決定することができる。移動の方向の変化、場所の変化、および速度の変化は、現在から定められた長さ(たとえば、15秒から3分)だけ前の時間に対する相対的なものであり得る。移動の方向の変化、場所の変化、および速度の変化を使用して、デジタルアシスタントインターフェースコンポーネント138は、予測される移動の方向、場所、および速度を決定することができる。ベクターベースの地図146の現在表示される部分から、デジタルアシスタントインターフェースコンポーネント138は、予測される移動の方向、場所、および速度に基づいて、ベクターベースの地図146の別の部分に移ることができる。移ると、デジタルアシスタントインターフェースコンポーネント138は、ベクターベースの地図146の表示されるべき部分を識別することができる。
In addition, the Digital Assistant Interface Component 138 identifies the portion of the vector-based
デジタルアシスタントインターフェースコンポーネント138は、ベクターベースの地図146の表示されるべき部分を、基準枠の一部として、ベクターベースの地図146の現在表示されている部分として設定または識別することができる。設定されると、基準枠の一方の部分はベクターベースの地図146の現在表示されている部分に対応してもよく、基準枠の別の部分はベクターベースの地図146の表示されることになる部分に対応してもよい。デジタルアシスタントインターフェースコンポーネント138は、ベクターベースの地図146の表示されるべき部分の寸法および座標を識別することができる。座標は、ベクターベースの地図146上の左上の座標および右下の座標などの、基準枠上の座標に対応し得る。デジタルアシスタントインターフェースコンポーネント138は、各点位置の地理的座標を、ベクターベースの地図146の表示されることになる部分に対して識別された寸法および座標と比較することができる。比較に基づいて、デジタルアシスタントインターフェースコンポーネント138は、ベクターベースの地図146の表示されることになる部分に対応する基準枠内の点位置のセットを選択または識別することができる。デジタルアシスタントインターフェースコンポーネント138は、デジタルアシスタントアプリケーション108のナビゲーションインターフェースコンポーネント116に点位置のセットを提供することができる。
The digital assistant interface component 138 can set or identify the part of the vector-based
ナビゲーションアプリケーション110からのデータの取り出しにより、NLPコンポーネント114は、1つまたは複数の参照キーワードおよび点位置のセットの識別子に基づいて、基準枠内の点位置のセットから1つまたは複数の点位置を区別または識別することができる。NLPコンポーネント114は、1つまたは複数のキーワードと点位置のセットの識別子との相関を決定して、セマンティックナレッジグラフ(セマンティックグラフまたはセマンティックネットワークと呼ばれることがある)を使用して点位置を識別することができる。セマンティックナレッジグラフは、頂点を介して互いに接続されるノードのセットを含み得る。各ノードはキーワードまたは句に対応し得る。各頂点は、2つのノード間の意味的距離を指定することができる。意味的距離は、ノードの語もしくは句の間の意味的な類似性または関連性の尺度を表し、またはそれに対応し得る。セットの各点位置に対して、NLPコンポーネント114は、セマンティックナレッジグラフを使用して、点位置の対応する識別子と1つまたは複数の参照キーワードとの間の意味的距離を計算または決定することができる。前に論じられたように、識別子は名前またはカテゴリタイプを含み得る。セマンティックナレッジグラフにおいて、NLPコンポーネント114は、参照キーワードに対応するノードおよび点位置の識別子に対応するノードを識別することができる。NLPコンポーネント114は次いで、2つのノード間の意味的距離を決定することができる。NLPコンポーネント114は、参照語と点位置のセットの識別子との間の意味的距離に基づいて、1つまたは複数の点位置を識別することができる。セマンティックナレッジグラフを使用して意味的距離を決定すると、NLPコンポーネント114は、1つまたは複数の参照キーワードからの意味的距離が最も短い点位置を識別することができる。複数の点位置を識別するために、NLPコンポーネント114は、参照キーワードからの意味的距離が短い方からn番目までの1つまたは複数の点位置を識別することができる。 By retrieving data from the navigation application 110, NLP component 114 picks one or more point positions from a set of point positions within the reference frame based on the identifier of one or more reference keywords and a set of point positions. Can be distinguished or identified. NLP component 114 determines the correlation between one or more keywords and the identifier of a set of point locations and uses a semantic knowledge graph (sometimes called a semantic graph or semantic network) to identify the point location. be able to. The Semantic Knowledge Graph can contain a set of nodes that are connected to each other through vertices. Each node can correspond to a keyword or phrase. Each vertex can specify a semantic distance between the two nodes. Semantic distance can represent, or correspond to, a measure of semantic similarity or relevance between words or phrases of a node. For each point position in the set, NLP Component 114 can use the Semantic Knowledge Graph to calculate or determine the semantic distance between the corresponding identifier of the point position and one or more reference keywords. can. As discussed earlier, identifiers can include names or category types. In the semantic knowledge graph, NLP component 114 can identify the node corresponding to the reference keyword and the node corresponding to the point position identifier. NLP component 114 can then determine the semantic distance between the two nodes. NLP component 114 can identify one or more point positions based on the semantic distance between the reference term and the identifier of the set of point positions. Using the Semantic Knowledge Graph to determine the semantic distance, NLP component 114 can identify the point position with the shortest semantic distance from one or more reference keywords. To identify multiple point positions, the NLP component 114 can identify one or more point positions from the shortest semantic distance to the nth point position from the reference keyword.
セマンティックナレッジグラフを使用して、NLPコンポーネント114はまた、参照キーワードが基準枠内の点位置のいずれかを指すかどうかを決定することができる。NLPコンポーネント114は、各参照キーワードと各点位置の識別子との間の意味的距離を閾値の距離と比較することができる。閾値の距離は、セマンティックナレッジグラフにおいて、参照キーワードが識別子を指しているとNLPコンポーネント114が決定することができる、最大の意味的距離を示すことができる。NLPコンポーネント114は、参照キーワードの1つと識別子の1つとの間の少なくとも1つの意味的距離が閾値の距離以下であると決定することができる。少なくとも1つの意味的距離が閾値の距離以下であるとの決定に応答して、NLPコンポーネント114は、少なくとも1つの参照キーワードが基準枠内の点位置のうちの1つに言及していると決定することができる。逆に、NLPコンポーネント114は、すべての意味的距離が閾値の距離より長いと決定することができる。すべての意味的距離が閾値の距離より長いとの決定に応答して、NLPコンポーネント114は、参照キーワードが基準枠内のいずれの点位置にも言及していないと決定することができる。 Using the Semantic Knowledge Graph, NLP Component 114 can also determine whether the reference keyword points to any of the point positions within the reference frame. NLP component 114 can compare the semantic distance between each reference keyword and the identifier at each point position with the threshold distance. The threshold distance can indicate the maximum semantic distance that NLP component 114 can determine in a semantic knowledge graph that the reference keyword points to an identifier. NLP component 114 can determine that at least one semantic distance between one of the reference keywords and one of the identifiers is less than or equal to the threshold distance. In response to the determination that at least one semantic distance is less than or equal to the threshold distance, NLP component 114 determines that at least one reference keyword refers to one of the point positions within the reference frame. can do. Conversely, NLP component 114 can determine that all semantic distances are longer than the threshold distances. In response to the determination that all semantic distances are longer than the threshold distance, NLP component 114 can determine that the reference keyword does not refer to any point position within the reference frame.
NLPコンポーネント114はまた、とりわけ、語義の曖昧性解消、談話指示物分析、および直示分析などの、意味分析技法を使用して1つまたは複数の点位置を識別することができる。NLPコンポーネント114は、セマンティックナレッジグラフを使用して決定される意味的距離に基づいて、意味分析技法を使用すべきかどうかを決定することができる。NLPコンポーネント114は、参照キーワードと点位置の識別子との間の意味的距離を閾値の距離と比較することができる。NLPコンポーネント114は、意味的距離のうちのある設定された割合(たとえば、90%を超える)が閾値より大きいと決定することができる。比較的長い意味的距離は、セマンティックナレッジグラフが点位置の識別子の曖昧性を解消するのに有効ではない可能性があることを示し得る。この決定に応答して、NLPコンポーネント114は、意味分析技法を使用して、1つまたは複数の点位置を識別することができる。セットの各点位置に対して、NLPコンポーネント114は、意味分析技法を適用して、点位置の対応する識別子と参照キーワードとの間の指標尺度を計算または決定することができる。指標尺度は、入力オーディオ信号から解析された参照キーワードが点位置の識別子に言及している、またはそれを表している尤度を示すことができる。指標尺度を決定すると、NLPコンポーネント114は、1つまたは複数の参照キーワードについての指標尺度が最大である点位置を識別することができる。複数の点位置を識別するために、NLPコンポーネント114は、参照キーワードに関連する指標尺度が大きい方からn番目までの1つまたは複数の点位置を識別することができる。 NLP component 114 can also identify one or more point positions using semantic analysis techniques, such as word-sense disambiguation, discourse referent analysis, and deixis analysis, among others. NLP component 114 can determine whether semantic analysis techniques should be used based on the semantic distances determined using the Semantic Knowledge Graph. NLP component 114 can compare the semantic distance between the reference keyword and the point position identifier with the threshold distance. NLP component 114 can determine that a set percentage of the semantic distance (eg, greater than 90%) is greater than the threshold. Relatively long semantic distances may indicate that the semantic knowledge graph may not be effective in disambiguating point position identifiers. In response to this decision, NLP component 114 can use semantic analysis techniques to identify one or more point locations. For each point position in the set, NLP component 114 can apply semantic analysis techniques to calculate or determine an index scale between the corresponding identifier of the point position and the reference keyword. The index scale can indicate the likelihood that the reference keyword analyzed from the input audio signal refers to or represents the point position identifier. Once the metric scale is determined, NLP component 114 can identify the point position where the metric scale is maximum for one or more reference keywords. To identify multiple point positions, the NLP component 114 can identify one or more point positions from the largest to the nth index scale associated with the reference keyword.
指標分析技法を使用して、NLPコンポーネント114はまた、参照キーワードが基準枠内の点位置のいずれかに言及するかどうかを決定することができる。NLPコンポーネント114は、各参照キーワードと各点位置の識別子との間の指標尺度を閾値の尺度と比較することができる。閾値の尺度は、参照キーワードが識別子に言及しているとNLPコンポーネント114が決定することができる、最大の指標尺度を示すことができる。NLPコンポーネント114は、参照キーワードの1つと識別子の1つとの間の少なくとも1つの指標尺度が閾値の尺度以下であると決定することができる。少なくとも1つの指標尺度が閾値の尺度以下であるとの決定に応答して、NLPコンポーネント114は、少なくとも1つの参照キーワードが基準枠内の点位置の1つに言及していると決定することができる。逆に、NLPコンポーネント114は、すべての指標尺度が閾値の尺度より大きいと決定することができる。すべての指標尺度が閾値の尺度より大きいとの決定に応答して、NLPコンポーネント114は、参照キーワードが基準枠内のいずれの点位置にも言及していないと決定することができる。 Using index analysis techniques, NLP component 114 can also determine whether the reference keyword refers to any of the point positions within the reference frame. NLP component 114 can compare the metric scale between each reference keyword and the identifier at each point position with the threshold scale. The threshold scale can indicate the largest metric scale that NLP component 114 can determine that the reference keyword refers to an identifier. NLP component 114 can determine that at least one metric scale between one of the reference keywords and one of the identifiers is less than or equal to the threshold scale. In response to the determination that at least one metric scale is below the threshold scale, NLP component 114 may determine that at least one reference keyword refers to one of the point positions within the reference frame. can. Conversely, NLP component 114 can determine that all index scales are greater than the threshold scale. In response to the determination that all index scales are greater than the threshold scale, NLP component 114 can determine that the reference keyword does not refer to any point position within the reference frame.
NLPコンポーネント114は、点位置のセットから1つまたは複数の点位置を識別するために、ナビゲーションアプリケーション110によって以前に受信された検索語のセットを使用することができる。セットの各点位置に対して、NLPコンポーネント114は、点位置の対応する識別子と1つまたは複数の検索語との間の意味的距離を計算または決定することができる。セマンティックナレッジグラフにおいて、NLPコンポーネント114は、検索語に対応するノードおよび点位置の識別子に対応するノードを識別することができる。NLPコンポーネント114は次いで、2つのノード間の意味的距離を決定することができる。NLPコンポーネント114は、検索語と点位置のセットの識別子との間の意味的距離に基づいて、点位置のサブセットを選択することができる。ナビゲーションアプリケーション110から取り出された点位置のセットから、NLPコンポーネント114は、参照キーワードからの意味的距離が短い方からn番目までの点位置のサブセットを選択することができる。点位置のサブセットから、NLPコンポーネント114は、本明細書において上で詳述された機能を使用して1つまたは複数の点位置を識別することができる。 NLP component 114 can use the set of search terms previously received by the navigation application 110 to identify one or more point positions from the set of point positions. For each point position in the set, NLP component 114 can calculate or determine the semantic distance between the corresponding identifier of the point position and one or more search terms. In the semantic knowledge graph, the NLP component 114 can identify the node corresponding to the search term and the node corresponding to the point position identifier. NLP component 114 can then determine the semantic distance between the two nodes. NLP component 114 can select a subset of point positions based on the semantic distance between the search term and the identifier of the set of point positions. From the set of point positions taken from the navigation application 110, the NLP component 114 can select a subset of the point positions from the shortest semantic distance to the nth point position from the reference keyword. From a subset of point positions, NLP component 114 can identify one or more point positions using the features detailed above herein.
ジオロケーション感知コンポーネント118からの測定結果を使用して、NLPコンポーネント114は、セットから1つまたは複数の点位置を識別することができる。上で論じられたように、ジオロケーション感知コンポーネント118は、デジタルアシスタントアプリケーション108またはナビゲーションアプリケーション110を実行しているクライアントデバイス104の尺度の中でもとりわけ、移動の方向、場所、および速度を決定または識別することができる。NLPコンポーネント114は、ジオロケーション感知コンポーネント118からの測定結果に基づいて、セットから点位置のサブセットを識別または選択することができる。NLPコンポーネント114は、ナビゲーションアプリケーション110から取り出された各点位置の地理的座標を識別することができる。NLPコンポーネント114は、点位置のセットの地理的座標をクライアントデバイス104の場所と比較することができる。NLPコンポーネント114は、クライアントデバイス104の場所からある定められた近接の範囲内にある(たとえば、1〜3kmの範囲内にある)地理的座標を伴う点位置のサブセットを識別することができる。サブセットから、NLPコンポーネント114は、移動の方向を使用して点位置のより小さいサブセットを選択することができる。NLPコンポーネント114は、移動の方向に沿った地理的座標を伴う点位置のより小さいサブセットを選択または識別し、移動の方向と反対側にある点位置を除外することができる。たとえば、NLPコンポーネント114は、クライアントデバイス104が北に移動していることが測定されるとき、クライアントデバイス104から北に2km以内にある点位置を選択することができる。点位置のより小さいサブセットから、NLPコンポーネント114は、本明細書において上で詳述された機能を使用して1つまたは複数の点位置を識別することができる。
Using the measurement results from the geolocation sensing component 118, the NLP component 114 can identify one or more point positions from the set. As discussed above, the geolocation sensing component 118 determines or identifies the direction, location, and speed of movement, among other things, among the scales of the
NLPコンポーネント114は、クライアントデバイス104の位置識別子および点位置の位置識別子を使用して、セットから1つまたは複数の点位置を識別することができる。NLPコンポーネント114は、クライアントデバイス104の位置識別子をセットの中の点位置の位置識別子と比較することができる。各点位置に対して、NLPコンポーネント114は、点位置の位置識別子がクライアントデバイス104の位置識別子からある定められた近接の範囲内(たとえば、1kmから3km未満)にあるかどうかを決定することができる。NLPコンポーネント114は、クライアントデバイス104の位置識別子から定められた近接の範囲内にある位置識別子を伴う点位置のサブセットを選択することができる。点位置のサブセットから、NLPコンポーネント114は、本明細書において上で詳述された機能を使用して、1つまたは複数の点位置を識別することができる。
The NLP component 114 can use the location identifier of the
1つまたは複数の点位置を識別する際、NLPコンポーネント114は、入力オーディオ信号において識別される参照キーワードに関する他のキーワードを探すことができる。NLPコンポーネント114は、データ処理システム102がクライアントデバイス104から受信したコンテンツまたは選好に基づいて、拡張されたエンティティを自動的に生成することができる。NLPコンポーネント114は、後続のオーディオベースの入力要求においてデータ処理システム102がクライアントデバイス104から要求するコンテンツまたは選好に基づいて、拡張されたエンティティを生成することができる。データ処理システム102によって受信されるコンテンツまたは選好に基づいて、NLPコンポーネント114は、1つまたは複数の点位置を識別するために、参照キーワードに関する追加のキーワードを探すことができる。たとえば、入力オーディオ信号は「さあ、家に帰ろう」を含むことがあり、NLPコンポーネント114は「家」を参照キーワードとして識別した可能性がある。クライアントデバイス104のエンドユーザは以前に、エンドユーザの自宅の住所を、デジタルアシスタントアプリケーション108を実行しているデータ処理システム102に以前に提供している可能性がある。この例では、NLPコンポーネント114は、エンドユーザの自宅の住所の位置識別子を取り出し、ナビゲーションアプリケーション110から取り出された点位置の位置識別子と比較することができる。位置識別子を比較することによって、NLPコンポーネント114は、「家」という参照キーワードに対応する点位置を識別することができる。
When identifying one or more point locations, the NLP component 114 can look for other keywords related to the referenced keywords identified in the input audio signal. The NLP component 114 can automatically generate extended entities based on the content or preferences received by the
NLPコンポーネント114は、参照キーワードのさらなる分析に基づいて、セットから1つまたは複数の点位置を識別することができる。NLPコンポーネント114は、参照キーワードがベクターベースの地図146のどの部分に言及しているかを決定または識別することができる。上で論じられたように、ナビゲーションインターフェースコンポーネント116は、ナビゲーションアプリケーション110にアクセスして、ビューポートを通じて見えるベクターベースの地図146の部分の点位置を取り出すことができる。ナビゲーションインターフェースコンポーネント116は、ナビゲーションアプリケーション110にアクセスして、目的地の周りのビューポートの外側にある、近接についてのベクターベースの地図146の別の部分にアクセスすることができる。点位置は、ビューポート内に見えるものとして、またはビューポートの外側にあるものとして標識され得る。NLPコンポーネント114は、意味分析技法を実行して、参照キーワードが近接を表す語であるか遠隔を表す語であるかを決定することができる。近接を表す語は、近くの点位置を表すことができ、ナビゲーションアプリケーション110のビューポートを通じて見えるベクターベースの地図146の部分の中の点位置の1つと相関することができる。遠隔を表す語は、離れた点位置を表すことができ、ナビゲーションアプリケーション110のビューポートの外側にあるベクターベースの地図146の部分の中の点位置の1つと相関することができる。NLPコンポーネント114は、1つまたは複数の参照キーワードをあらかじめ定められた近接を表す語のセット(たとえば、「ここ」、「近くの」、および「近所の」)およびあらかじめ定められた遠隔を表す語のセット(たとえば、「目的地の近く」、「あそこ」、「沿い」)と比較することができる。NLPコンポーネント114は、参照語が近接を表す語であると決定することができる。この決定に応答して、NLPコンポーネント114は、ビューポートを通じて見えるベクターベースの地図146の部分上の点位置に対応する点位置のサブセットを選択または識別することができる。NLPコンポーネント114は、参照語が遠隔を表す語であると決定することができる。この決定に応答して、NLPコンポーネント114は、ビューポートの外側にあるベクターベースの地図146の部分上の点位置に対応する点位置のサブセットを選択または識別することができる。点位置のサブセットから、NLPコンポーネント114は、本明細書において上で詳述された機能を使用して1つまたは複数の点位置を識別することができる。
NLP component 114 can identify one or more point locations from a set based on further analysis of reference keywords. NLP component 114 can determine or identify which part of the vector-based
NLPコンポーネント114は、入力オーディオ信号から解析された1つまたは複数の補助キーワードを使用して、基準枠内の点位置のセットから1つまたは複数の点位置を識別することができる。上で論じられたように、補助キーワードは、要求および1つまたは複数の参照キーワード以外の、入力オーディオ信号から解析されたキーワードであってもよく、クライアントデバイス104のディスプレイに言及するキーワードに対応してもよい。入力オーディオ信号から解析されたキーワードを補助キーワードとして識別する際、NLPコンポーネント114は、補助キーワードのための、ナビゲーションアプリケーション110のビューポートまたはナビゲーションアプリケーション110を実行しているクライアントデバイス104のディスプレイのサブセットエリアを識別することができる。前に説明されたように、補助キーワードを識別するために使用されるあらかじめ定められたキーワードの第3のセットの中の各キーワードは、ナビゲーションアプリケーション110のビューポートのサブセットエリアに対応してもよく、またはそれと関連付けられてもよい。たとえば、補助キーワード「左上の角」は、ナビゲーションアプリケーション110のビューポートの左上の4分の1に対応し得る。ナビゲーションアプリケーション110のビューポートのサブセットエリアは、ピクセル座標(たとえば、長さ対幅)を使用して定義され得る。NLPコンポーネント114は、補助キーワードと関連付けられるサブセットエリアに対応するビューポートを通じて見えるベクターベースの地図146の部分のサブセットエリアを識別または決定することができる。NLPコンポーネント114は、補助キーワードと関連付けられるビューポートのサブセットエリアに対して定義されるピクセル座標を、ビューポートを通じて見えるベクターベースの地図146の部分に対する寸法および座標に変換することができる。
The NLP component 114 can use one or more auxiliary keywords analyzed from the input audio signal to identify one or more point positions from a set of point positions within the reference frame. As discussed above, the auxiliary keywords may be keywords parsed from the input audio signal, other than the request and one or more reference keywords, and correspond to keywords that refer to the display of
補助キーワードと関連付けられるビューポートのサブセットエリアに対応するベクターベースの地図146の部分のサブセットエリアに対する寸法および座標を使用して、NLPコンポーネント114は、点位置のサブセットを選択または識別することができる。NLPコンポーネント114は、各点位置の地理的座標を寸法および座標と比較することができる。比較に基づいて、NLPコンポーネント114は、ベクターベースの地図146の部分のサブセットエリアの内部の点位置を選択または識別することができる。点位置のサブセットから、NLPコンポーネント114は、本明細書において上で詳述された機能を使用して1つまたは複数の点位置を識別することができる。
Using the dimensions and coordinates for the subset area of the portion of the vector-based
NLPコンポーネント114は、セットから1つまたは複数の点位置を識別する際に、以前に受信された入力オーディオ信号を使用することができる。NLPコンポーネント114は、ナビゲーションアプリケーション110の少なくとも1つの機能に言及するものとして決定された入力オーディオ信号を記憶して保持することができる。NLPコンポーネント114はまた、ナビゲーションアプリケーション110の少なくとも1つの機能に言及するものとして決定された、以前に受信された入力オーディオ信号から解析された1つまたは複数のキーワードを記憶して保持することができる。NLPコンポーネント114は、各々の記憶された入力オーディオ信号の受信から経過した時間を識別することができる。各入力オーディオ信号に対して、NLPコンポーネント114は、経過した時間が定められた閾値の時間(たとえば、15秒から60分)以上であるかどうかを決定することができる。NLPコンポーネント114は、定められた閾値の時間より経過時間の短い、以前に受信された入力オーディオ信号のセットを識別することができる。セットの中の各々に対して、NLPコンポーネント114は、入力オーディオ信号を解析し、本明細書において上で説明された機能を使用して1つまたは複数の参照キーワードを識別することができる。 The NLP component 114 can use previously received input audio signals in identifying one or more point locations from the set. The NLP component 114 can store and retain the input audio signal determined to refer to at least one function of the navigation application 110. The NLP component 114 can also store and retain one or more keywords analyzed from a previously received input audio signal, determined to refer to at least one function of the navigation application 110. .. The NLP component 114 can identify the time elapsed since the reception of each stored input audio signal. For each input audio signal, the NLP component 114 can determine if the elapsed time is greater than or equal to a defined threshold time (eg, 15 seconds to 60 minutes). The NLP component 114 can identify a set of previously received input audio signals that have elapsed less than a defined threshold of time. For each of the sets, NLP component 114 can analyze the input audio signal and identify one or more reference keywords using the features described above herein.
以前の入力オーディオ信号からの参照キーワードを使用して、NLPコンポーネント114は、点位置のセットから点位置のサブセットを選択または識別することができる。NLPコンポーネント114は、以前の入力オーディオ信号からの参照キーワードと現在の入力オーディオ信号からの参照キーワードとの一致を決定することができる。一致に基づいて、NLPコンポーネント114は、一致に対応する参照キーワードと点位置の識別子との間の意味的距離を(たとえば、短くすることによって)調整することができる。たとえば、以前の入力オーディオ信号と現在の入力オーディオ信号の両方が、参照語「レストラン」を含むことがある。一致を決定すると、NLPコンポーネント114は、参照語「レストラン」と識別子との間の意味的距離を短くして、それにより、レストランに対応する点位置が選択される尤度を高めることができる。 Using reference keywords from previous input audio signals, NLP component 114 can select or identify a subset of point positions from a set of point positions. NLP component 114 can determine the match between the reference keyword from the previous input audio signal and the reference keyword from the current input audio signal. Based on the match, the NLP component 114 can adjust the semantic distance (eg, by shortening) between the reference keyword corresponding to the match and the point position identifier. For example, both the previous input audio signal and the current input audio signal may include the reference term "restaurant." Upon determining a match, the NLP component 114 can reduce the semantic distance between the reference word "restaurant" and the identifier, thereby increasing the likelihood that the point position corresponding to the restaurant will be selected.
NLPコンポーネント114はまた、意味分析技法を使用して、現在の入力オーディオ信号の参照語と以前に受信された入力オーディオ信号の参照語との間の指標尺度を計算または決定することができる。意味分析技法は、とりわけ、語義の曖昧性解消、談話指示物分析、および直示分析を含み得る。以前に受信された入力オーディオ信号の参照語の各々に対して、NLPコンポーネント114は、指標尺度を計算または決定することができる。前に論じられたように、指標尺度は、入力オーディオ信号から解析された参照キーワードが点位置の識別子に言及している、またはそれを表す尤度を示すことができる。指標尺度を決定すると、NLPコンポーネント114は、1つまたは複数の参照キーワードについての指標尺度が最大である、以前に受信された入力オーディオ信号からの参照語を識別することができる。複数の点位置を識別するために、NLPコンポーネント114は、現在の入力オーディオ信号の参照キーワードに関して、指標尺度が大きい方からn番目までの、以前に受信された入力オーディオ信号からの1つまたは複数の参照語を識別することができる。この識別により、NLPコンポーネント114は、以前に受信された入力オーディオ信号からの1つまたは複数の参照キーワードを使用して、点位置のサブセットを選択することができる。 NLP component 114 can also use semantic analysis techniques to calculate or determine an index scale between the reference term for the current input audio signal and the reference term for the previously received input audio signal. Semantic analysis techniques can include, among other things, word-sense disambiguation, discourse referent analysis, and deixis analysis. For each of the reference terms of a previously received input audio signal, NLP component 114 can calculate or determine an index scale. As discussed earlier, the metric scale can indicate the likelihood that the reference keyword parsed from the input audio signal refers to or represents the point position identifier. Once the metric scale is determined, NLP component 114 can identify the reference term from the previously received input audio signal that has the largest metric scale for one or more reference keywords. To identify multiple point positions, the NLP component 114 refers to one or more of the previously received input audio signals from the largest index scale to the nth with respect to the reference keyword of the current input audio signal. Can identify the reference word of. This identification allows the NLP component 114 to select a subset of point locations using one or more reference keywords from previously received input audio signals.
セットの各点位置に対して、NLPコンポーネント114は、点位置の対応する識別子と以前に受信された入力オーディオ信号からの1つまたは複数の参照キーワードとの間の意味的距離を計算または決定することができる。セマンティックナレッジグラフにおいて、NLPコンポーネント114は、参照キーワードに対応するノードおよび点位置の識別子に対応するノードを識別することができる。NLPコンポーネント114は次いで、2つのノードの間の意味的距離を決定することができる。NLPコンポーネント114は、参照キーワードと点位置のセットの識別子との間の意味的距離に基づいて、点位置のサブセットを選択することができる。ナビゲーションアプリケーション110から取り出された点位置のセットから、NLPコンポーネント114は、参照キーワードからの意味的距離が短い方からn番目までの点位置のサブセットを選択することができる。点位置のサブセットから、NLPコンポーネント114は、本明細書において上で詳述された機能を使用して1つまたは複数の点位置を識別することができる。 For each point position in the set, the NLP component 114 calculates or determines the semantic distance between the corresponding identifier of the point position and one or more reference keywords from a previously received input audio signal. be able to. In the semantic knowledge graph, NLP component 114 can identify the node corresponding to the reference keyword and the node corresponding to the point position identifier. NLP component 114 can then determine the semantic distance between the two nodes. NLP component 114 can select a subset of point positions based on the semantic distance between the reference keyword and the identifier of the set of point positions. From the set of point positions taken from the navigation application 110, the NLP component 114 can select a subset of the point positions from the shortest semantic distance to the nth point position from the reference keyword. From a subset of point positions, NLP component 114 can identify one or more point positions using the features detailed above herein.
データ処理システム102は、ダイレクトアクションハンドラコンポーネント120のインスタンスを実行し、または走らせることができる。ダイレクトアクションハンドラコンポーネント120は、NLPコンポーネント114から受信された入力に基づいて、スクリプトまたはプログラムを実行することができる。ナビゲータサービス106は、スクリプトまたはプログラムを提供することができる。ナビゲータサービス106は、スクリプトまたはプログラムを、APIを通じてデータ処理システム102に対して利用可能にすることができる。ダイレクトアクションハンドラコンポーネント120は、入力フィールドへのパラメータまたは応答を決定することができ、データをアクションデータ構造へとパッケージングすることができる。アクションデータ構造は、APIを通じてデータ処理システム102に提供され得る。ダイレクトアクションハンドラコンポーネント120は、遂行のためにアクションデータ構造をナビゲーションアプリケーション110に送信することができ、または、データ処理システム102が、アクションデータ構造の指示を遂行することができる。
The
ダイレクトアクションハンドラコンポーネント120は、入力オーディオ信号から解析された要求および参照キーワードに基づいて、スレッドのアクションもしくは会話のためのデータ構造を生成または選択することができる。上で説明されたように、NLPコンポーネント114は、入力オーディオ信号がナビゲーションアプリケーション110に言及していることと、ナビゲーションアプリケーション110のどの機能に言及しているかということとを決定することができる。アクションデータ構造は、ナビゲーションアプリケーション110が要求を完了するための情報を含み得る。情報は、入力オーディオ信号において示されるナビゲーションアプリケーション110の機能のうちの1つに対応する要求タイプを含み得る。情報は、機能タイプに対応するナビゲーションアプリケーション110の機能を行うための1つまたは複数のパラメータを含み得る。1つまたは複数のパラメータは、入力オーディオ信号から解析された参照キーワードおよび補助キーワードを使用して識別された1つまたは複数の点位置を含み得る。1つまたは複数のパラメータは、1つまたは複数の識別された点位置の識別子を含み得る。1つまたは複数のパラメータは、アカウント識別子および認証証明書などの、クライアントデバイス104を実行しているデジタルアシスタントアプリケーション108またはナビゲーションアプリケーション110に対する結び付けるデータを含み得る。ダイレクトアクションハンドラコンポーネント120はまた、要求を使用してナビゲーションアプリケーション110を発動し、または呼び出すことができる。ダイレクトアクションハンドラコンポーネント120は、別の要求(メッセージと呼ばれることもある)としてナビゲータサービス106へ送信するために、要求をアクションデータ構造へとパッケージングすることができる。
The direct action handler component 120 can generate or select data structures for thread actions or conversations based on requests and reference keywords parsed from the input audio signal. As described above, the NLP component 114 can determine whether the input audio signal refers to the navigation application 110 and which function of the navigation application 110 it refers to. The action data structure may include information for the navigation application 110 to complete the request. The information may include a request type corresponding to one of the functions of the navigation application 110 indicated in the input audio signal. The information may include one or more parameters for performing the functions of the navigation application 110 corresponding to the function type. One or more parameters may include one or more point positions identified using reference and auxiliary keywords analyzed from the input audio signal. One or more parameters may include an identifier for one or more identified point positions. One or more parameters may include data associated with the digital assistant application 108 or navigation application 110 running
ダイレクトアクションハンドラコンポーネント120は、ナビゲーションアプリケーション110のためのアクションデータ構造にどのフィールドを含めるべきかを決定するために、データリポジトリ126から少なくとも1つのテンプレート136を取り出すことができる。ダイレクトアクションハンドラコンポーネント120は、データ構造のフィールドに対する情報を取得するために、テンプレート136を取り出すことができる。要求タイプおよび1つまたは複数のパラメータを使用して、ダイレクトアクションハンドラコンポーネント120は、アクションデータ構造を生成するために、テンプレート136からのフィールドを埋めることができる。テンプレート136は、アクションデータ構造の作成のために、ナビゲーションアプリケーション110もしくはナビゲーションサービス106に対して設定または構成され得る。たとえば、ナビゲーションアプリケーション110のためのテンプレート136は、{[アカウント識別子]、「認証証明書」、「要求タイプ」、「パラメータ」}という形式であり得る。ナビゲーションアプリケーション110のためのテンプレート136を埋める際、ダイレクトアクションハンドラコンポーネント120は、情報の中でもとりわけ、アカウント識別子、認証証明書、要求タイプ(または機能識別子)、および1つまたは複数のパラメータを識別して挿入することができる。
The direct action handler component 120 can retrieve at least one
少なくとも1つの参照キーワードが基準枠内の点位置の1つに言及しているとの決定に応答して、ダイレクトアクションハンドラコンポーネント120は、データの中でもとりわけ、点位置の識別子、クライアントデバイス104の座標、および点位置の位置識別子を含むように1つまたは複数のパラメータを設定することができる。パラメータに含まれる識別子は、参照キーワードを使用して識別された点位置の識別子を含み得る。1つまたは複数のパラメータはまた、少なくとも1つの参照キーワードが基準枠内の点位置の1つに言及していることのインジケータを含み得る。参照キーワードが基準枠内の点位置のいずれにも言及していないとの決定に応答して、ダイレクトアクションハンドラコンポーネント120は、とりわけ、クライアントデバイス104の座標および参照キーワードを含むように1つまたは複数のパラメータを設定することができる。1つまたは複数のパラメータはまた、参照キーワードが基準枠内の点位置のいずれにも言及していないというインジケータを含み得る。
In response to the determination that at least one reference keyword refers to one of the point positions in the reference frame, the Direct Action Handler component 120, among other things in the data, is the point position identifier, the coordinates of the
ダイレクトアクションハンドラコンポーネント120は、エンティティを拡張して、ナビゲータサービス106のためのアクションデータ構造の所与のフィールドのためのフォーマットへとエンティティを変換することができる。エンティティは、ナビゲータサービス106にとって不明瞭または不明確であり得る情報を含み得る。たとえば、ナビゲータサービス106が街路住所を要求したとき、エンドユーザは、位置または店の正式な名前であるエンティティを提供することがある。NLPコンポーネント114は、データ処理システム102がクライアントデバイス104から受信したコンテンツまたは選好に基づいて、拡張されたエンティティを自動的に生成することができる。NLPコンポーネント114は、後続のオーディオベースの入力要求においてデータ処理システム102がクライアントデバイス104から要求するコンテンツまたは選好に基づいて、拡張されたエンティティを生成することができる。たとえば、データ処理システム102は、「さあ、家に帰ろう」を含む入力オーディオ信号を受信することがある。NLPコンポーネント114は、ナビゲーションアプリケーション110から取り出されたどの点位置の識別子が参照キーワードに対応するかを決定した可能性がある。たとえば、NLPコンポーネント114は、機能のための1つまたは複数のパラメータのうちの1つとして、位置エンティティとしての「家」を識別することができる。しかしながら、アクションデータ構造の中の位置フィールドは、街路住所、都市、州、および郵便番号を必要とし得る。この例では、位置エンティティ「家」は、ナビゲータサービス106によって要求されるフォーマットではない。クライアントデバイス104のエンドユーザが以前にデータ処理システム102またはナビゲータサービス106にエンドユーザの自宅の住所を提供しているとき、NLPコンポーネント114は、サービスプロバイダデバイスのアクションデータ構造のフィールドによって要求されるフォーマット(たとえば、{street_address:"123 Main St.",city:"Anytown",state:"CA"})へと「家」を拡張することができる。エンドユーザが以前にエンドユーザの自宅の住所をデータ処理システム102に提供していない場合、データ処理システム102は、「家」ではなく特定の住所を示すようにエンドユーザに要求するオーディオベースの入力要求を生成して送信することができる。エンティティをナビゲータサービス106に送信する前にエンティティを拡張することで、ナビゲータサービス106が拡張されていないエンティティを受信した後で情報を明確化する別の要求または追加の情報を送信しなくてもよいので、必要とされるネットワーク送信の回数を減らすことができる。
The Direct Action Handler component 120 can extend the entity to transform it into a format for a given field of the action data structure for the
アクションデータ構造が生成されると、ダイレクトアクションハンドラコンポーネント120は、アクションデータ構造をナビゲーションアプリケーション110に送り、送信し、または提供することができる。前に説明されたように、デジタルアシスタントアプリケーション108を実行しているクライアントデバイス104は、ナビゲーションアプリケーション110を欠いていることがあり、それに応答して、ナビゲーションインターフェースコンポーネント116は、ナビゲーションアプリケーション110にアクセスするために別の関連するクライアントデバイス104にアクセスすることができる。入力オーディオ信号を受信したクライアントデバイス104がナビゲーションアプリケーション110を実行している、または有するとの決定に応答して、ダイレクトアクションハンドラコンポーネント120は、アクションデータ構造をナビゲーションアプリケーション110に提供することができる。逆に、入力オーディオ信号を受信したクライアントデバイス104がナビゲーションアプリケーション110を実行していない、またはそれを欠いているとの決定に応答して、ダイレクトアクションハンドラコンポーネント120は、ナビゲーションアプリケーション110を実行している、または有するものとして識別される別のクライアントデバイス104に、アクションデータ構造を提供することができる。
Once the action data structure is generated, the direct action handler component 120 can send, send, or provide the action data structure to the navigation application 110. As previously described, the
デジタルアシスタントインターフェースコンポーネント138は、ダイレクトアクションハンドラコンポーネント120によって生成されるアクションデータ構造を受信することができる。デジタルアシスタントインターフェースコンポーネント138は、テンプレート136に従ってアクションデータ構造を解析することができる。デジタルアシスタントインターフェースコンポーネント138はまた、(たとえば、ナビゲータサービス106によってアクセス可能なデータベース上に)テンプレート136のコピーを保持することができる。テンプレート136を適用することによって、デジタルアシスタントインターフェースコンポーネント138は、アカウント識別子、認証証明書、要求タイプ、および1つまたは複数のパラメータをデータアクション構造から識別することができる。デジタルアシスタントインターフェースコンポーネント138は、認証証明書のローカルコピーをアクションデータ構造からの認証証明書のコピーと比較することによって、アカウント識別子を認証することができる。デジタルアシスタントインターフェースコンポーネント138は、アカウント識別子を使用して、クライアントデバイス104上で実行しているナビゲータサービス106またはナビゲーションアプリケーション110から認証証明書のローカルコピーを取り出すことができる。アカウント識別子の認証に成功するための認証証明書の一致を決定したことに応答して、デジタルアシスタントインターフェースコンポーネント138は、要求タイプおよび1つまたは複数のパラメータを使用してナビゲーション案内プロセスを開始することができる。ナビゲーション案内プロセスは、位置発見動作および経路決定動作を含み得る。デジタルアシスタントインターフェースコンポーネント138は、位置発見動作に対応するものとして要求タイプを識別することができる。識別に応答して、デジタルアシスタントインターフェースコンポーネント138は、位置発見動作を開始するために位置発見器コンポーネント140を呼び出すことができる。位置発見動作のもとでは、アクションデータ構造は1つまたは複数の点位置を含み得る。デジタルアシスタントインターフェースコンポーネント138は、経路決定動作に対応するものとして要求タイプを識別することができる。経路決定動作のもとでは、アクションデータ構造は単一の点位置を含み得る。識別に応答して、デジタルアシスタントインターフェースコンポーネント138は、経路決定動作を開始するために経路決定器コンポーネント142を呼び出すことができる。
The digital assistant interface component 138 can receive the action data structure generated by the direct action handler component 120. Digital assistant interface component 138 can analyze the action data structure according to
データ処理システム102またはナビゲータサービス106は、ナビゲーションアプリケーション110の位置発見器コンポーネント140のインスタンスを実行し、または走らせることができる。呼び出しに応答して、位置発見器コンポーネント140は、ナビゲーションアプリケーション110のビューポートを通じて見えるベクターベースの地図146の部分上に1つまたは複数の点位置を提示することができる。位置発見器コンポーネント140は、アクションデータ構造を解析してインジケータを識別することができる。インジケータを使用して、位置発見器コンポーネント140は、デジタルアシスタントアプリケーション108における入力オーディオ信号の参照キーワードが少なくとも1つの点位置に言及していると決定することができる。決定に応答して、位置発見器コンポーネント140は、アクションデータ構造から1つまたは複数の点位置を識別することができる。各点位置に対して、位置発見器コンポーネント140は、ベクターベースの地図146上の点位置に対応する位置識別子を識別することができる。
The
逆に、位置発見器コンポーネント140は、デジタルアシスタントアプリケーション108における入力オーディオ信号の参照キーワードがアクションデータ構造のインジケータに基づいて少なくとも1つの点位置に言及していると決定することができる。決定に応答して、位置発見器コンポーネント140は、基準枠の外側のベクターベースの地図146にアクセスすることができる。ベクターベースの地図146にアクセスすると、位置発見器コンポーネント140は、基準枠の外側の1つまたは複数の点位置の識別子を探すことができる。位置発見器コンポーネント140は次いで、アクションデータ構造の参照キーワードと一致する、ベクターベースの地図146の中の基準枠の外側にある1つまたは複数の点位置の識別子を識別することができる。たとえば、受信されたアクションデータ構造に含まれる参照キーワード「タワーABC」は、基準枠内の点位置のいずれかを指し得る。この例では、位置発見器コンポーネント140は、初期の基準枠の外側にある、ベクターベースの地図146の中で識別子「タワーABC」と一致する点位置を探すことができる。位置発見器コンポーネント140は、参照キーワードと一致する識別子を伴う複数の点位置を識別することができる。アクションデータ構造からのクライアントデバイス104の位置識別子を使用して、位置発見器コンポーネント140は、クライアントデバイス104に最も近い点位置を識別することができる。各点位置の識別により、位置発見器コンポーネント140は、識別された点位置の地理的座標を識別することができる。
Conversely, the position detector component 140 can determine that the reference keyword of the input audio signal in the digital assistant application 108 refers to at least one point position based on the indicator of the action data structure. In response to the decision, the position detector component 140 can access the vector-based
初期の基準枠の外側にある点位置を識別したことに応答して、位置発見器コンポーネント140は、参照キーワードと一致する識別子を伴う点位置を含むように基準枠を修正することができる。位置発見器コンポーネント140は、ベクターベースの地図146の見える部分に対応する初期の基準枠の寸法および座標を識別することができる。位置発見器コンポーネント140は、参照キーワードと一致する識別子を伴う点位置の座標を含むように、基準枠の座標を移動することができる。点位置の座標は、たとえば、新しい基準枠の中心にあり得る。位置発見器コンポーネント140はまた、基準枠の寸法を保持することができる。基準枠が移動すると、ナビゲーションアプリケーション110は、ビューポートを通じてベクターベースの地図146の異なる部分を表示することができる。この部分は、参照キーワードと一致する識別子を伴う点位置を含むように移動された基準枠に対応し得る。このようにして、デジタルアシスタントアプリケーション108およびナビゲーションアプリケーション110は、ビューポートを通じて表示されるベクターベースの地図146の部分の内側と外側で、点位置を提示して他の機能を実行するために使用され得る。たとえば、NLPコンポーネント114によって解析された第1の音声の問合せは、「タワーABCを見せて」であり得る。NLPコンポーネント114は、ナビゲーションアプリケーション110のビューポートを通じて表示されるベクターベースの地図146において現在見えるいずれの点位置にも第1の音声の問合せが言及していないと決定した可能性がある。参照キーワード「タワーABC」により、位置発見器コンポーネント140は、「タワーABC」に対応する識別子を伴う点位置を発見することができる。続いて、NLPコンポーネント114によって解析された第2の音声の問合せは、「パティスリーを見せて」であり得る。NLPコンポーネント114は、ビューポートを通じて見えるベクターベースの地図146の部分において現在見えている点位置のいくつかが、参照キーワード「パティスリー」によって言及されていると決定することができる。位置発見器コンポーネント140は次いで、ベクターベースの地図146の部分の中の対応する点位置を強調することができる。
In response to identifying a point position outside the initial reference frame, the position detector component 140 can modify the reference frame to include a point position with an identifier that matches the reference keyword. The position detector component 140 can identify the dimensions and coordinates of the initial reference frame corresponding to the visible portion of the vector-based
位置発見器コンポーネント140は、ナビゲーションアプリケーション110のビューポートを通じて見えるベクターベースの地図146の部分において、位置識別子に対応する点位置を提示することができる。たとえば、位置発見器コンポーネント140は、点または丸を挿入し、またはベクターベースの地図146上の点位置に対応するグラフィカル表現を強調することができる。位置発見器コンポーネント140は、テキストで点位置の識別子を表示することもできる。ビューポートを通じてベクターベースの地図146の部分において点位置を表示すると、位置発見器コンポーネント140は、表示のために、または出力オーディオ信号のために、提供すべき応答をテキストとして生成することができる。応答は、位置発見動作に対応する要求タイプを含み得る。応答は、ナビゲーションアプリケーション110のビューポートを通じて見えるベクターベースの地図146の部分内に表示される点位置の識別子を含み得る。応答は、表示される点位置の数も含み得る。応答は、表示のための、または出力オーディオ信号のための、1つまたは複数の語を伴う少なくとも1つの応答句も含み得る。応答句は、テンプレートを使用して定義され得る。たとえば、応答句のテンプレートは、「[点位置の個数][識別子]がエリア内で発見されました」という形式であり得る。応答を生成する際、位置発見器コンポーネント140は、要求タイプ、表示される点位置の識別子、表示される点位置の数、および少なくとも1つの応答句を識別することができる。応答が生成されると、デジタルアシスタントインターフェースコンポーネント138は、デジタルアシスタントアプリケーション108に応答を送り、送信し、または提供することができる。
The position detector component 140 can present the point position corresponding to the position identifier in the portion of the vector-based
データ処理システム102またはナビゲータサービス106は、ナビゲーションアプリケーション110の経路決定器コンポーネント142のインスタンスを実行し、または走らせることができる。呼び出しに応答して、経路決定器コンポーネント142は、アクションデータ構造において識別される点位置への移動経路を生成し、決定し、または識別することができる。経路決定器コンポーネント142は、ジオロケーション感知コンポーネント118を使用して、ナビゲーションアプリケーション110を実行しているクライアントデバイス104の現在の地理的座標を識別することができる。経路決定器コンポーネント142は、クライアントデバイス104の地理的座標をベクターベースの地図146上の位置識別子に変換することができる。経路決定器コンポーネント142は、クライアントデバイス104の位置識別子を出発地として設定することができる。経路決定器コンポーネント142は、アクションデータ構造の点位置に対応する位置識別子を識別することができる。経路決定器コンポーネント142は、点位置の位置識別子を目的地として設定することができる。経路決定器コンポーネント142は、ベクターベースの地図146の経路上の出発地と目的地との間の移動経路を決定するために、経路発見アルゴリズム(たとえば、ダイクストラのアルゴリズム、A*アルゴリズム、およびクラスカルのアルゴリズム)を適用することができる。上で説明されたように、ベクターベースの地図146は、交通網に対応する経路を含み得る。経路決定器コンポーネント142は、ナビゲーションアプリケーション110のビューポートを通じて見えるベクターベースの地図146の部分に移動経路の少なくとも一部を提示または表示することができる。
The
移動経路を決定したことに応答して、経路決定器コンポーネント142は、表示のために、または出力オーディオ信号のために、提供すべき応答をテキストとして生成することができる。応答は、経路決定動作に対応する要求タイプを含み得る。応答は、ベクターベースの地図146上の目的地に対応する点位置の識別子を含み得る。応答はまた、目的地までの推定移動時間を含み得る。応答はまた、表示のための、または出力オーディオ信号のための、1つまたは複数の語を伴う少なくとも1つの応答句を含み得る。応答句はテンプレートを使用して定義され得る。たとえば、応答句のテンプレートは、「[目的地]までのルートが見つかりました。推定所要時間は[推定移動時間]です。」という形式であり得る。応答を生成する際、位置発見器コンポーネント140は、要求タイプ、点位置の識別子、推定移動時間、および少なくとも1つの応答句を識別することができる。応答が生成されると、デジタルアシスタントインターフェースコンポーネント138は、デジタルアシスタントアプリケーション108に応答を送り、送信し、または提供することができる。
In response to determining the travel path, the routing device component 142 can generate the response as text to provide for display or for the output audio signal. The response may include a request type corresponding to the routing operation. The response may include an identifier for the point location corresponding to the destination on the vector-based
ナビゲーションアプリケーション110からの応答の受信に応答して、オーディオ信号生成器コンポーネント122は、応答を解析して、テキスト出力のための、または出力オーディオ信号のための応答句を識別することができる。オーディオ信号生成器コンポーネント122は、ナビゲータサービス106からの応答の中の応答句の1つまたは複数の語に基づいて、出力オーディオファイルを生成することができる。オーディオ信号生成器コンポーネント122は、応答句の1つまたは複数の語の出力オーディオファイルを(たとえば、クライアントデバイス104のスピーカ148を介して)再生することができる。デジタルアシスタントアプリケーション108は、応答句の1つまたは複数の語をテキストで表示することもできる。テキスト出力または出力オーディオファイルを生成する際、応答選択器コンポーネント124は、データリポジトリ126に保持されているポリシー132または応答データ134を使用して、応答句を選択または識別することができる。ポリシー132は、要求タイプ(たとえば、位置発見動作または経路決定動作)に特有であってもよく、要求タイプのための応答データ134を指定することができる。応答選択器コンポーネント124は、ナビゲーションアプリケーション110からの応答の要求タイプを使用して出力を生成するためのポリシー132を探すことができる。ポリシー132が識別されると、応答選択器コンポーネント124は、ナビゲーションアプリケーション110からの応答の内容を応答データ134と照合することができる。位置発見動作のためのポリシー132を識別したことに応答して、応答選択器コンポーネント124は、表示される点位置の識別子および表示される点位置の数をポリシー132のための応答データ134に合わせることができる。経路決定動作のためのポリシー132を識別したことに応答して、応答選択器コンポーネント124は、点位置の識別子および推定移動時間をポリシー132のための応答データ134に合わせることができる。
In response to receiving a response from the navigation application 110, the audio signal generator component 122 can parse the response to identify the response phrase for text output or for the output audio signal. The audio signal generator component 122 can generate an output audio file based on one or more words in the response clause in the response from the
ここで図2を参照すると、図1に示されるシステムのデジタルアシスタントアプリケーション108とインターフェースしているナビゲーションアプリケーション110の動作ステータスを決定するための、例示的なデータフロー200のシーケンス図が示されている。データフロー200は、図1に関連して上で説明されたシステム100または図6に関連して以下で詳述されるシステム600によって、実装または実行され得る。
Referring now to FIG. 2, a sequence diagram of an
クライアントデバイス104上で実行しているデジタルアシスタントアプリケーション108のローカルインスタンスは、センサ158を介して入力オーディオ信号を検出し、入力オーディオ信号に対して初期の処理を実行して要求205を生成することができる。要求205は、入力オーディオ信号自体、または、機械学習技法を使用して入力オーディオ信号において識別された1つまたは複数の語を含み得る。クライアントデバイス104は、要求205をデータ処理システム102に送信することができる。データ処理システム102上で実行しているデジタルアシスタントアプリケーション108のリモートインスタンスは、要求205に対して追加の処理を実行することができる。データ処理システム102上で実行しているNLPコンポーネント114は、要求205を解析して、ナビゲーションアプリケーション110によって実行されるべき機能に要求205が言及していると決定することができる。NLPコンポーネント114はまた、意味分析技法を使用して、入力オーディオ信号から機能および参照キーワードに対応する要求を識別することができる。決定に応答して、ナビゲーションインターフェースコンポーネント116は、ナビゲーションアプリケーション110を実行しているナビゲータサービス106(または別のクライアントデバイス104)にアクセス要求210を送信することができる。
A local instance of Digital Assistant Application 108 running on
アクセス要求210を受信すると、ナビゲータサービス106上で実行しているデジタルアシスタントインターフェースコンポーネント138は、ナビゲーションアプリケーション110のビューポートを通じて見える情報を識別することができる。この情報は、ナビゲーションアプリケーション110のビューポートを通じて見えるベクターベースの地図146によって表現される地理的領域の点位置および点位置の識別子を含み得る。デジタルアシスタントインターフェースコンポーネント138は、ナビゲーションアプリケーション110のビューポートを通じて見える情報を、基準枠データ215として設定することができる。デジタルアシスタントインターフェースコンポーネント138は次いで、基準枠データ215をデータ処理システム102に提供することができる。
Upon receiving the
基準枠データ215を使用して、NLPコンポーネント114は、意味分析技法を使用して、入力オーディオ信号の参照キーワードがどの点位置を表しているかを決定することができる。たとえば、NLPコンポーネント114は、参照キーワードを点位置の識別子と比較することができる。点位置の識別により、データ処理システム102上で実行しているダイレクトアクションハンドラコンポーネント120は、ダイレクトアクションデータ構造220を生成することができる。ダイレクトアクションデータ構造220は、ナビゲーションアプリケーション110によって実行されるべき機能(たとえば、位置発見または経路決定)に対応する要求タイプを含み得る。ダイレクトアクションデータ構造220はまた、参照キーワードを使用して識別される点位置を含み得る。ダイレクトアクションハンドラコンポーネント120は、ダイレクトアクションデータ構造220を、ナビゲーションアプリケーション110を実行しているナビゲータサービス106(またはクライアントデバイス104)に送信することができる。
Using
ダイレクトアクションデータ構造220に従って、ナビゲーションアプリケーション110は、ナビゲーション案内プロセスを実行することができる。デジタルアシスタントインターフェースコンポーネント138は、ダイレクトアクションデータ構造220を解析して、要求タイプを識別することができる。要求タイプを使用して、デジタルアシスタントインターフェースコンポーネント138は、ナビゲータサービス106上で実行している位置発見器コンポーネント140および経路決定器コンポーネント142のうちの1つを呼び出すことができる。要求タイプが位置発見機能に対応するものとして識別されるとき、位置発見器コンポーネント140は、ナビゲーションアプリケーション110のビューポートを通じて表示される地理的領域上に点位置を(たとえば、強調を介して)提示することができる。要求タイプが経路決定機能に対応するものとして識別されるとき、経路決定器コンポーネント142は、出発地(たとえば、クライアントデバイス104)とダイレクトアクションデータ構造220の点位置に対応する目的地との間の移動経路を決定することができる。経路決定器コンポーネント142は、ナビゲーションアプリケーション110のビューポート上に表示される地理的領域において移動経路の一部を提示することができる。位置発見器コンポーネント140および経路決定器コンポーネント142は各々、データ処理システム102上で実行しているデジタルアシスタントアプリケーション108に返信すべき応答225を生成することができる。応答225は、応答句ならびに他のパラメータを含み得る。応答225を使用して、オーディオ信号生成器コンポーネント122は、クライアントデバイス104に提供すべき別の応答230を生成することができる。受信されると、クライアントデバイス104上で実行しているデジタルアシスタントアプリケーション108は、ディスプレイ上にテキストとして、またはスピーカ148を通じて出力されるオーディオファイルとして、応答230を提示することができる。
According to the direct
ここで図3を参照すると、構成300のもとで、左側のデジタルアシスタントアプリケーション108および右側のナビゲーションアプリケーション110を実行しているクライアントデバイス104が図示されている。デジタルアシスタントアプリケーション108を実行しているクライアントデバイス104とナビゲーションアプリケーション110を実行しているクライアントデバイス104は、同じであっても異なっていてもよい。
With reference to FIG. 3, a
左側のクライアントデバイス104上で実行しているデジタルアシスタントアプリケーション108は、センサ158を介して入力オーディオ信号を検出することができる。デジタルアシスタントアプリケーション108は、自然言語処理技法を適用して、検出された入力オーディオ信号の中の1つまたは複数の語を識別することができる。デジタルアシスタントアプリケーション108は、入力オーディオ信号から識別された「近くの喫茶店を見せて」という語を含むテキストコンテンツ項目305として出力を表示することができる。デジタルアシスタントアプリケーション108は、入力オーディオ信号がナビゲーションアプリケーション110の位置発見動作に言及していると決定することができる。デジタルアシスタントアプリケーション108は、「見せて」という語を要求として、「近くの喫茶店」という語を参照キーワードとして識別することができる。入力オーディオ信号が位置発見動作に言及しているとの決定に応答して、デジタルアシスタントアプリケーション108は、ナビゲーションアプリケーション110にアクセスすることができる。
The digital assistant application 108 running on the
右側のクライアントデバイス104上で実行しているナビゲーションアプリケーション110は、ナビゲーションアプリケーション110のビューポート310を通じてベクターベースの地図146の一部分を表示することができる。ナビゲーションアプリケーション110のビューポート310は、クライアントデバイス104のディスプレイのサイズに対応し得る。ベクターベースの地図146は、建物に対応する点位置320のセットと、示されるような道路および鉄道などの交通網を表す点位置320のうちの経路のセットを含み得る。各点位置320は、「カフェ」、「ガソリンスタンド」、「ホテル」、および「オフィス」などの、建物の名前またはカテゴリタイプなどの識別子を有し得る。ナビゲーションアプリケーション110は、識別子「カフェB」、「カフェC」、および「オフィス」を伴う点位置320などの、ビューポート310に現れる点位置320を識別することができる。ナビゲーションアプリケーション110は、識別子「カフェA」および「カフェD」を伴う点位置320などの、ビューポート310の外側の点位置を除外することができる。ナビゲーションアプリケーション110は、マーク(たとえば、十字の星)を使用して、ベクターベースの地図146上にクライアントデバイス104の現在地315を表示することができる。ナビゲーションアプリケーション110は、識別子を伴う点位置をデジタルアシスタントアプリケーション108に提供することができる。
The navigation application 110 running on the
ナビゲーションアプリケーション110からの点位置の取り出しにより、デジタルアシスタントアプリケーション108は、意味分析技法を実行して、参照キーワードがどの点位置に言及しているのかを識別することができる。示される例では、デジタルアシスタントアプリケーション108は、「近くの喫茶店」を参照キーワードとして識別した可能性がある。意味分析技法を使用して、デジタルアシスタントアプリケーション108は、「近くの喫茶店」という参照キーワードが識別子「カフェB」および「カフェC」を伴う点位置320を表していると決定することができる。デジタルアシスタントアプリケーション108は、参照キーワードが識別子「オフィス」を伴う点位置320を表していないと決定することができる。識別子「カフェB」および「カフェC」を有する点位置320の識別により、デジタルアシスタントアプリケーション108は、ナビゲーションアプリケーション110に提供すべきダイレクトアクションデータ構造を生成することができる。ダイレクトアクションデータ構造は、識別された点位置320と、ナビゲーションアプリケーション110の位置発見動作に対応する要求タイプとを有し得る。受信すると、ナビゲーションアプリケーション110は、ダイレクトアクションデータ構造を解析して、実行されるべき機能が位置発見動作であることを識別することができる。ナビゲーションアプリケーション110はまた、ダイレクトアクションデータ構造を解析して、識別子「カフェB」および「カフェC」を伴う点位置320を識別することができる。これらの識別に基づいて、ナビゲーションアプリケーション110は、識別子「カフェB」および「カフェC」を伴う点位置320を表す建物を強調することができる。加えて、ナビゲーションアプリケーション110は、応答を生成し、デジタルアシスタントアプリケーション108に返信することができる。応答は、「2軒の喫茶店が見つかりました」という応答句を含み得る。デジタルアシスタントアプリケーション108は次いで、クライアントデバイス104の画面にテキストコンテンツ項目325を表示することができる。
Extraction of the point position from the navigation application 110 allows the digital assistant application 108 to perform a semantic analysis technique to identify which point position the reference keyword refers to. In the example shown, digital assistant application 108 may have identified "nearby coffee shop" as a reference keyword. Using semantic analysis techniques, the digital assistant application 108 can determine that the reference keyword "near coffee shop" represents
続いて、デジタルアシスタントアプリケーション108は、センサ158を介して別の入力オーディオ信号を検出することができる。デジタルアシスタントアプリケーション108は、自然言語処理技法を適用して、検出された入力オーディオ信号の中の1つまたは複数の語を識別することができる。デジタルアシスタントアプリケーション108は、入力オーディオ信号から識別された「左の店に連れて行って」という語を含むテキストコンテンツ項目330として出力を表示することができる。デジタルアシスタントアプリケーション108は、ナビゲーションアプリケーション110の経路決定動作に入力オーディオ信号が言及していると決定することができる。自然言語処理技法を使用して、デジタルアシスタントアプリケーション108は、「連れて行って」という語を要求として、「店」という語を参照キーワードとして、「左の」という語を補助キーワードとして識別することができる。ナビゲーションアプリケーション110から以前に取り出された点位置を用いて、デジタルアシスタントアプリケーション108は、参照キーワードが補助キーワードと一緒に、ビューポート310の左にある識別子「カフェC」を伴う点位置320を表していると識別することができる。識別子「カフェC」を伴う点位置320の識別に基づいて、デジタルアシスタントアプリケーション108は、ナビゲーションアプリケーション110に提供すべきダイレクトアクションデータ構造を生成することができる。ダイレクトアクションデータ構造は、識別された点位置320と、ナビゲーションアプリケーション110の経路決定動作に対応する要求タイプとを有し得る。受信すると、ナビゲーションアプリケーション110は、ダイレクトアクションデータ構造を解析して、実行されるべき機能が経路決定動作であると識別することができる。ナビゲーションアプリケーション110はまた、ダイレクトアクションデータ構造を解析して、識別子「カフェC」を伴う点位置320を識別することができ、点位置320を目的地として設定することができる。ナビゲーションアプリケーション110は、クライアントデバイス104の現在地を出発地として識別することもできる。これらの識別に基づいて、ナビゲーションアプリケーション110は、経路発見アルゴリズムを使用して、ベクターベースの地図146を通じて移動経路335を決定することができる。移動経路335に基づいて、ナビゲーションアプリケーション110は、推定所要時間を決定することができる。ナビゲーションアプリケーション110は、ベクターベースの地図146上に移動経路335をレンダリングして表示することができる。加えて、ナビゲーションアプリケーション110は、応答を生成して、デジタルアシスタントアプリケーション108に返信することができる。応答は、「ルートが見つかりました。推定所要時間は15分です」という応答句を含み得る。デジタルアシスタントアプリケーション108は次いで、クライアントデバイス104画面にテキストコンテンツ項目340を表示することができる。
The digital assistant application 108 can then detect another input audio signal via the sensor 158. The digital assistant application 108 can apply natural language processing techniques to identify one or more words in the detected input audio signal. The digital assistant application 108 can display the output as a
図4は、ネットワーク化されたコンピュータ環境における、音声で作動するスレッドを生成するための例示的な方法400のブロック図を示す。方法400は、図1〜図3に関連して上で説明されたシステム100、または図6に関連して以下で詳述されるシステム600によって、実施または実行され得る。方法は、入力オーディオ信号を受信するステップ(405)を含み得る。方法400は、入力オーディオ信号を解析するステップ(410)を含み得る。方法400は、アクションデータ構造を選択するステップ(415)を含み得る。方法400は、応答エンティティを拡張するステップ(420)を含み得る。方法は、アクションデータ構造を埋めるステップ(425)を含み得る。方法400は、デジタルコンポーネントを送信するステップ(430)を含み得る。
FIG. 4 shows a block diagram of an
方法400は、入力信号を受信するステップ(405)を含み得る。方法は、データ処理システムによって実行されるNLPコンポーネントによって、入力信号を受信するステップを含み得る。入力信号は、第1のクライアントデバイスにおいてセンサによって検出されデータ処理システムに送信される、入力オーディオ信号であり得る。センサは、第1のクライアントデバイスのマイクロフォンであり得る。たとえば、1つまたは複数のプロセッサおよびメモリを含むデータ処理システムによって少なくとも部分的に実行されるデジタルアシスタントコンポーネントが、入力オーディオ信号を受信することができる。入力オーディオ信号は、デジタルアシスタントによって支援される会話を含み得る。会話は、1つまたは複数の入力および出力を含み得る。会話は、オーディオベース、テキストベース、またはオーディオとテキストの組合せであり得る。入力オーディオ信号は、テキスト入力、または会話情報を提供することができる他のタイプの入力を含み得る。データ処理システムは、会話に対応するセッションに対するオーディオ入力を受信することができる。
方法400は、入力信号を解析するステップ(410)を含み得る。データ処理システムのNLPコンポーネントは、入力信号を解析して要求を識別することができる。NLPコンポーネントは、入力信号の中の少なくとも1つのエンティティを識別することができる。要求は、1つまたは複数のサービスプロバイダデバイスによって満たされ得る意図または要求であり得る。要求は、会話の句の一部であり得る。たとえば、要求は「OK、家まで車を用意して」であり得る。NLPコンポーネントによって識別されるエンティティは、要求を満たすときにサービスプロバイダデバイスが要求する入力フィールドまたはタイプと対応付けられる、要求の中の句または用語であり得る。たとえば、配車サービスを提供するサービスプロバイダデバイスは、現在地入力フィールドおよび目的地入力フィールドを要求し得る。上の例を続けると、NLPコンポーネントは、「家」という用語を目的地入力フィールドと対応付け得る。
方法400は、アクションデータ構造を選択するステップ(415)を含み得る。データ処理システムは、入力信号から解析された要求に基づいてアクションデータ構造を選択することができる。データ処理システムは、要求を満たすことができるサービスプロバイダデバイスに基づいてアクションデータ構造を選択することができる。アクションデータ構造は、サービスプロバイダデバイスによって作成されるデータ構造またはオブジェクトであり得る。サービスプロバイダデバイスは、アクションデータ構造をデータ処理システムに提供することができる。アクションデータ構造は、サービスプロバイダデバイスが要求を満たすために使用するフィールド、データ、または情報を示すことができる。サービスプロバイダデバイスは、そのフィールドに対して返されるエンティティを拡張するようにデータ処理システムに要求するための、フィールドのうちの1つまたは複数にフラグを立てることができる。フィールドが拡張のためにフラグを立てられるとき、サービスプロバイダデバイス160が会話ベースのデータ交換を設計するのではなく、データ処理システムが、フラグを立てられたフィールドのための情報またはデータを取り出すために、クライアントデバイス104との会話ベースのデータ交換を設計して生成することができる。
方法400は、応答エンティティを拡張するステップ(420)を含み得る。データ処理システムは、エンティティがサービスプロバイダデバイスによって指定されるフォーマットではない場合、入力フィールドと対応付けられるエンティティが拡張される必要があると決定することができる。上の例を続けると、NLPコンポーネントは、「家」が目的地と対応付けられるエンティティであると決定することができる。ダイレクトアクションハンドラコンポーネントは、目的地フィールドにエンティティ「家」を含むようにアクションデータ構造を更新することを決定することができる。ダイレクトアクションハンドラコンポーネントは、応答エンティティのフォーマットが目的地フィールドのフォーマットと一致しないと決定することができる。たとえば、目的地フィールドは、街路住所、都市、州、および郵便番号を要求するオブジェクトのフォーマットを有し得る。応答エンティティのフォーマットとフィールドのフォーマットの不一致を検出すると、データ処理システムは、エンティティを街路住所、都市、州、および郵便番号のフォーマットに拡張することができる。たとえば、データ処理システムは、エンドユーザの「家」の住所としてエンドユーザがデータ処理システムに提供した住所を調べることができる。データ処理システムは、拡張ポリシーに基づいてエンティティを拡張することができる。拡張ポリシーは、データ処理システムが用語を拡張する許可を得ているかどうかを示すことができ、または、エンドユーザまたはクライアントコンピューティングデバイスが提供したどのデータが拡張されたエンティティに含まれ得るかを示すことができる。
データ処理システムは、サービスプロバイダデバイスからの要求に基づいてエンティティを拡張することができる。たとえば、データ処理システムは、拡張されていないエンティティを伴う第1のアクションデータ構造を生成することができる。データ処理システムは、要求を満たすために、第1のアクションデータ構造を処理のためにサービスプロバイダデバイスに送信することができる。サービスプロバイダデバイスは、アクションデータ構造のフィールドの1つまたは複数の中のデータを処理または理解できない場合、アクションデータ構造(またはその一部分)をデータ処理システムに返すことができる。たとえば、サービスプロバイダデバイスは、目的地フィールドの中の「家」エンティティを処理することを試み、次いで、エンティティを処理または理解できないとサービスプロバイダデバイスが決定した後で、「家」エンティティを拡張するようにデータ処理システムに要求することができる。 The data processing system can extend the entity based on the request from the service provider device. For example, a data processing system can generate a first action data structure with unextended entities. The data processing system can send a first action data structure to the service provider device for processing in order to satisfy the request. The service provider device may return the action data structure (or part thereof) to the data processing system if it cannot process or understand the data in one or more of the fields of the action data structure. For example, a service provider device attempts to process a "house" entity in a destination field, and then extends the "house" entity after the service provider device determines that it cannot process or understand the entity. Can be requested from the data processing system.
方法400は、アクションデータ構造を埋めるステップ(425)を含み得る。ダイレクトアクションハンドラコンポーネントは、拡張されたエンティティを用いてアクションデータ構造を埋めることができる。ダイレクトアクションハンドラコンポーネントは、エンティティを用いてアクションデータ構造を埋めることができる。たとえば、アクションデータ構造は、エンティティまたは拡張されたエンティティが記憶されるオブジェクトであり得る。アクションデータ構造を埋めることは、アクションデータ構造を更新することとしても言及され得る。
方法400は、アクションデータ構造を送信する(430)ステップを含み得る。データ処理システムは、埋められたアクションデータ構造をサービスプロバイダデバイスに送信することができる。アクションデータ構造を受信すると、サービスプロバイダデバイスは、要求を満たし、またはデータ処理システムもしくはクライアントコンピューティングデバイスから追加の情報を要求することができる。
ここで図5を参照すると、ネットワーク化されたコンピュータ環境において複数のアプリケーション間でインターフェースするための例示的な方法500が図示されている。方法500は、図1〜図3に関連して上で説明されたシステム100、または図6に関連して下で詳述されるシステム600によって、実施または実行され得る。簡単な概略において、方法500は、ビューポートを通じて見える点位置を取り出すステップ(505)を含み得る。方法500は、参照語に対応する識別子を用いて点位置を識別するステップ(510)を含み得る。方法500は、識別子を伴うアクションデータ構造を生成するステップ(515)を含み得る。方法500は、ナビゲーション案内プロセスを開始するステップ(520)を含み得る。
Here, with reference to FIG. 5, an
方法500は、ビューポートを通じて見える点位置を取り出すステップ(505)を含み得る。デジタルアシスタントアプリケーションを実行するデータ処理システム(たとえば、データ処理システム102)は、自然言語処理技法を使用して、入力オーディオ信号から解析された要求および参照語を識別することができる。データ処理システムは、要求がクライアントデバイス上で実行しているナビゲーションアプリケーションの機能に言及していると決定することができる。この機能は、位置発見機能および経路決定機能を含み得る。要求がナビゲーションアプリケーションの機能に言及していると決定したことに応答して、データ処理システムは、ナビゲーションアプリケーションのビューポートを通じて表示される地理的領域上の点位置を取り出すために、ナビゲーションアプリケーションにアクセスすることができる。各点位置は、地理的領域上の地物に対応してもよく、識別子を有してもよい。
方法500は、参照語に対応する識別子を用いて点位置を識別するステップ(510)を含み得る。ナビゲーションアプリケーションのビューポートを通じて表示される点位置の取り出しにより、データ処理システムは、入力オーディオ信号の参照語がどの点位置に言及しているかを識別することができる。データ処理システムは、意味分析技法を使用して、参照語が表している点位置に対応する識別子を識別することができる。意味分析技法は、とりわけ、セマンティックナレッジグラフを使用すること、直示分析を実行すること、およびn-gramを生成することを含み得る。
方法500は、識別子を伴うアクションデータ構造を生成するステップ(515)を含み得る。データ処理システムは、識別された要求および点位置を使用して、テンプレートに従ってアクションデータ構造を生成することができる。要求は、ナビゲーションアプリケーションの機能のうちの1つに対応し得る。点位置は、入力オーディオ信号から解析された参照語に対応するものを含み得る。アクションデータ構造はまた、とりわけ、アカウント識別子および認証証明書を含み得る。
方法500は、ナビゲーション案内プロセスを開始するステップ(520)を含み得る。データ処理システムは、アクションデータ構造をナビゲーションアプリケーションに送信して、ナビゲーション案内プロセスを開始することができる。ナビゲーション案内プロセスは、位置発見動作および経路決定動作を含み得る。位置発見動作は、アクションデータ構造の中の識別子に対応する点位置のグラフィカル表現を提示または表示することを含み得る。経路決定動作は、現在地と、アクションデータ構造の中の識別子に対応する点位置に対応する目的地との間の、移動経路を決定して提示することを含み得る。
図6は、例示的なコンピュータシステム600のブロック図である。コンピュータシステムまたはコンピューティングデバイス600は、システム100、もしくはデータ処理システム102などのそのコンポーネントを含んでもよく、またはそれを実装するために使用されてもよい。コンピューティングシステム600は、情報を通信するためのバス605または他の通信コンポーネントと、情報を処理するためのバス605に結合されたプロセッサ610または処理回路を含む。コンピューティングシステム600はまた、情報を処理するためのバスに結合される1つまたは複数のプロセッサ610または処理回路を含み得る。コンピューティングシステム600はまた、プロセッサ610によって実行されるべき情報および命令を記憶するための、バス605に結合された、ランダムアクセスメモリ(RAM)または他のダイナミックストレージデバイスなどのメインメモリ615を含む。メインメモリ615は、データリポジトリ126もしくは148であってもよく、またはそれを含んでもよい。メインメモリ615はまた、位置情報、一時変数、またはプロセッサ610による命令の実行の間の他の中間情報を記憶するために使用され得る。コンピューティングシステム600はさらに、プロセッサ610のための不変の情報および命令を記憶するための、バス605に結合された読み取り専用メモリ(ROM)620または他のスタティックストレージデバイスを含み得る。ソリッドステートデバイス、磁気ディスク、または光学ディスクなどのストレージデバイス625は、情報および命令を永続的に記憶するためにバス605に結合され得る。ストレージデバイス625は、データリポジトリ126もしくは144を含んでもよく、またはその一部であってもよい。
FIG. 6 is a block diagram of an
コンピューティングシステム600は、情報をユーザに表示するための、液晶ディスプレイまたはアクティブマトリックスディスプレイなどのディスプレイ635にバス605を介して結合され得る。英数字キーおよび他のキーを含むキーボードなどの入力デバイス630は、情報および命令の選択をプロセッサ610に通信するためにバス605に結合され得る。入力デバイス630は、タッチスクリーンディスプレイ635を含み得る。入力デバイス630はまた、方向情報および命令の選択をプロセッサ610に通信し、ディスプレイ635上でのカーソルの動きを制御するための、マウス、トラックボール、またはカーソル方向キーなどのカーソルコントロールを含み得る。ディスプレイ635は、たとえば、データ処理システム102、クライアントデバイス104、または図1の他のコンポーネントの一部であり得る。
The
本明細書で説明されるプロセス、システム、および方法は、メインメモリ615に含まれる命令の構成をプロセッサ610が実行したことに応答して、コンピューティングシステム600によって実施され得る。そのような命令は、ストレージデバイス625などの別のコンピュータ可読媒体からメインメモリ615に読み込まれ得る。メインメモリ615に含まれる命令の構成の実行は、コンピューティングシステム600に、本明細書で説明される例示的なプロセスを実行させる。マルチプロセシング構成の1つまたは複数のプロセッサも、メインメモリ615に含まれる命令を実行するために利用され得る。本明細書で説明されるシステムおよび方法と一緒に、ソフトウェア命令の代わりに、またはそれと組み合わせて、ハードワイヤード回路が使用され得る。本明細書で説明されるシステムおよび方法は、ハードウェア回路とソフトウェアのどのような特定の組合せにも限定されない。
The processes, systems, and methods described herein may be performed by the
例示的なコンピューティングシステムが図6において説明されたが、本明細書で説明される動作を含む主題が、他のタイプのデジタル電子回路において、または、本明細書で開示される構造およびそれらの構造的な等価物、もしくはそれらの1つまたは複数の組合せを含む、コンピュータソフトウェア、ファームウェア、もしくはハードウェアにおいて、実装され得る。 Although an exemplary computing system has been described in FIG. 6, the subject matter including the operations described herein is in other types of digital electronic circuits, or the structures disclosed herein and theirs. It may be implemented in computer software, firmware, or hardware, including structural equivalents, or one or more combinations thereof.
本明細書で論じられるシステムがユーザについての個人情報を収集する状況、または個人情報を利用し得る状況では、ユーザは、プログラムまたは特徴が個人情報(たとえば、ユーザのソーシャルネットワーク、社会的行動、もしくは活動、ユーザの好み、またはユーザの位置についての情報)を収集し得るかどうかを制御するための、または、ユーザにより関連のあり得る、コンテンツサーバもしくは他のデータ処理システムからのコンテンツを、受信するかどうか、もしくはどのように受信するかを制御するための、機会を与えられ得る。加えて、パラメータを生成するときに個人を識別可能な情報が除去されるように、一部のデータは記憶され使用される前に1つまたは複数の方法で匿名化され得る。たとえば、ユーザの識別情報は、ユーザに対して個人を識別可能な情報を決定できないように、または、位置情報が取得されるユーザの地理的位置が(都市、郵便番号、または州レベルまで)一般化され得るようにして、ユーザの具体的な位置を決定できないようにするために、匿名化され得る。したがって、ユーザは、ユーザについての情報がどのように収集されてコンテンツサーバによって使用されるかについて、管理することができる。 In situations where the systems discussed herein collect personal information about you, or where personal information is available, you may find that the program or feature is personal information (eg, your social network, social behavior, or. Receiving content from content servers or other data processing systems, to control whether information about activity, user preferences, or user location) can be collected, or that may be more relevant to the user. You may be given the opportunity to control whether or how it is received. In addition, some data may be anonymized in one or more ways before it is stored and used so that personally identifiable information is removed when generating the parameters. For example, a user's identity may prevent the user from determining personally identifiable information, or the location of the user from which the location may be obtained is general (up to city, zip code, or state level). It can be anonymized so that it can be anonymized so that the specific location of the user cannot be determined. Therefore, the user can manage how information about the user is collected and used by the content server.
本明細書において説明される主題および動作は、デジタル電子回路において、または、本明細書で開示される構造およびそれらの構造的な等価物を含むコンピュータソフトウェア、ファームウェア、もしくはハードウェアにおいて、またはそれらの1つまたは複数の組合せにおいて実装され得る。本明細書で説明される主題は、データ処理装置による実行のために、またはその動作を制御するために、1つまたは複数のコンピュータ記憶媒体に符号化される1つまたは複数のコンピュータプログラム、たとえばコンピュータプログラム命令の1つまたは複数の回路として実装され得る。代替的に、または追加で、プログラム命令は、データ処理装置による実行のために適切な受信機装置への送信のために情報を符号化するために生成される人工的に生成される伝播信号、たとえば、機械で生成される電気信号、光学信号、または電磁信号上に符号化され得る。コンピュータ記憶媒体は、コンピュータ可読記憶デバイス、コンピュータ可読記憶基板、ランダムもしくはシリアルアクセスメモリアレイもしくはデバイス、またはそれらの1つまたは複数の組合せであってもよく、またはそれに含まれてもよい。コンピュータ記憶媒体は伝播信号ではないが、コンピュータ記憶媒体は、人工的に生成された伝播信号において符号化されているコンピュータプログラム命令のソースまたは目的地であり得る。コンピュータ記憶媒体はまた、1つまたは複数の別個のコンポーネントもしくは媒体(たとえば、複数のCD、ディスク、または他のストレージデバイス)であってもよく、またはそれに含まれてもよい。本明細書で説明される動作は、1つまたは複数のコンピュータ可読記憶デバイスに記憶されている、または他のソースから受信されたデータに対して、データ処理装置によって実行される動作として実施され得る。 The subjects and operations described herein are in digital electronic circuits, or in computer software, firmware, or hardware, including the structures disclosed herein and their structural equivalents, or theirs. It can be implemented in one or more combinations. The subject matter described herein is one or more computer programs, eg, encoded in one or more computer storage media, for execution by a data processor or to control its operation. It can be implemented as one or more circuits of computer program instructions. Alternatively or additionally, the program instruction is an artificially generated propagating signal, which is generated to encode information for transmission to a receiver device suitable for execution by the data processing device. For example, it can be encoded on a machine-generated electrical, optical, or electromagnetic signal. The computer storage medium may be, or may be included in, a computer-readable storage device, a computer-readable storage board, a random or serial access memory array or device, or a combination thereof. Although the computer storage medium is not a propagating signal, the computer storage medium can be the source or destination of computer program instructions encoded in the artificially generated propagating signal. The computer storage medium may also be or include one or more separate components or media (eg, multiple CDs, disks, or other storage devices). The operations described herein may be performed as operations performed by a data processor on data stored in one or more computer-readable storage devices or received from other sources. ..
「データ処理システム」、「コンピューティングデバイス」、「コンポーネント」、または「データ処理システム」という用語は、例として、プログラマブルプロセッサ、コンピュータ、システムオンチップ、またはそれらの複数、または前述のものの組合せを含む、データを処理するための様々な装置、デバイス、および機械を包含する。装置は、専用論理回路、たとえばFPGA(フィールドプログラマブルゲートアレイ)またはASIC(特定用途向け集積回路)を含み得る。装置はまた、ハードウェアに加えて、対象のコンピュータプログラムのための実行環境を作成するコード、たとえば、プロセッサファームウェア、プロトコルスタック、データベース管理システム、オペレーティングシステム、クロスプラットフォームランタイム環境、仮想機械、またはそれらの1つまたは複数の組合せを構成するコードを含み得る。装置および実行環境は、ウェブサービス、分散コンピューティング、およびグリッドコンピューティングインフラストラクチャなどの、様々な異なるコンピューティングモデルインフラストラクチャを実現することができる。システム100のコンポーネントは、1つまたは複数のデータ処理装置、システム、コンピューティングデバイス、もしくはプロセッサを含み、または共有することができる。
The terms "data processing system", "computing device", "component", or "data processing system" include, for example, programmable processors, computers, system-on-chips, or combinations thereof, or a combination of those described above. Includes various devices, devices, and machines for processing data. The device may include dedicated logic circuits, such as FPGAs (Field Programmable Gate Arrays) or ASICs (Application Specific Integrated Circuits). In addition to hardware, the device also creates code that creates an execution environment for the target computer program, such as processor firmware, protocol stacks, database management systems, operating systems, cross-platform runtime environments, virtual machines, or theirs. It may contain code that constitutes one or more combinations. Equipment and execution environments can implement a variety of different computing model infrastructures, such as web services, distributed computing, and grid computing infrastructures. The components of
コンピュータプログラム(プログラム、ソフトウェア、ソフトウェアアプリケーション、アプリ、スクリプト、またはコードとしても知られている)は、コンパイル型言語またはインタプリタ型言語、宣言型言語または手続型言語を含む、任意の形式のプログラミング言語で書かれてもよく、スタンドアロンプログラムとして、または、モジュール、コンポーネント、サブルーチン、オブジェクト、もしくはコンピューティング環境における使用に適した他のユニットとしてを含めて、任意の形式で展開されてもよい。コンピュータプログラムは、ファイルシステムの中のファイルに対応し得る。コンピュータプログラムは、他のプログラムまたはデータ(たとえば、マークアップ言語ドキュメントに記憶されている1つまたは複数のスクリプト)を保持するファイルの一部分に、対象のプログラムに専用の単一のファイルに、または複数の協調したファイル(たとえば、1つまたは複数のモジュール、サブプログラム、またはコードの部分を記憶するファイル)に記憶され得る。コンピュータプログラムは、ある場所に位置している、または複数の場所に分散しており通信ネットワークによって相互接続される、1つのコンピュータまたは複数のコンピュータ上で実行されるように展開され得る。 Computer programs (also known as programs, software, software applications, apps, scripts, or code) are in any form of programming language, including compiled or interpreted languages, declarative or procedural languages. It may be written and deployed in any format, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. Computer programs may correspond to files in the file system. A computer program may be part of a file that holds other programs or data (for example, one or more scripts stored in a markup language document), a single file dedicated to that program, or more than one. Can be stored in a coordinated file (for example, a file that stores one or more modules, subprograms, or parts of code). Computer programs can be deployed to run on one or more computers that are located in one location or distributed in multiple locations and interconnected by communication networks.
本明細書において説明されるプロセスおよび論理フローは、入力データに対して動作して出力を生成することによって活動を実行するように、1つまたは複数のコンピュータプログラムを実行する1つまたは複数のプログラマブルプロセッサ(たとえば、データ処理システム102のコンポーネント)によって実行され得る。また、専用論理回路、たとえばFPGA(フィールドプログラマブルゲートアレイ)またはASIC(特定用途向け集積回路)として、プロセスおよび論理フローは実行されてもよく、装置が実装されてもよい。コンピュータプログラム命令およびデータを記憶するのに適したデバイスは、例として、半導体メモリデバイス、たとえばEPROM、EEPROM、およびフラッシュメモリデバイス、磁気ディスク、たとえば内部ハードディスクまたはリムーバブルディスク、磁気光学ディスク、ならびにCD-ROMディスクおよびDVD-ROMディスクを含む、すべての形式の不揮発性メモリ、媒体、およびメモリデバイスを含む。プロセッサおよびメモリは、専用論理回路によって補助されてもよく、またはそれに組み込まれてもよい。 The processes and logical flows described herein are programmable one or more computers that execute one or more computer programs to perform activities by acting on input data and producing output. It can be executed by a processor (eg, a component of data processing system 102). Processes and logic flows may also be implemented as dedicated logic circuits, such as FPGAs (Field Programmable Gate Arrays) or ASICs (Application Specific Integrated Circuits), or devices may be implemented. Suitable devices for storing computer program instructions and data include, for example, semiconductor memory devices such as EPROM, EEPROM, and flash memory devices, magnetic disks such as internal hard disks or removable disks, magnetic optical disks, and CD-ROMs. Includes all types of non-volatile memory, media, and memory devices, including discs and DVD-ROM discs. The processor and memory may be assisted by or incorporated into a dedicated logic circuit.
本明細書で説明される主題は、バックエンドコンポーネントを、たとえばデータサーバとして含む、または、ミドルウェアコンポーネント、たとえばアプリケーションサーバを含む、または、フロントエンドコンポーネント、たとえば、ユーザが本明細書で説明される主題の実装形態とそれを通じて対話することができるグラフィカルユーザインターフェースもしくはウェブブラウザを有するクライアントコンピュータを含む、または、1つまたは複数のそのようなバックエンドコンポーネント、ミドルウェアコンポーネント、もしくはフロントエンドコンポーネントの組合せを含む、コンピューティングシステムにおいて実装され得る。システムのコンポーネントは、デジタルデータ通信の任意の形式または媒体、たとえば通信ネットワークによって相互接続され得る。通信ネットワークの例には、ローカルエリアネットワーク(「LAN」)およびワイドエリアネットワーク(「WAN」)、インターネットワーク(たとえば、インターネット)、およびピアツーピアネットワーク(たとえば、アドホックピアツーピアネットワーク)がある。 The subject matter described herein includes a backend component, eg, as a data server, or a middleware component, eg, an application server, or a frontend component, eg, a subject described herein by a user. Includes implementations of and client computers with a graphical user interface or web browser through which they can interact, or includes a combination of one or more such back-end, middleware, or front-end components. It can be implemented in computing systems. The components of the system can be interconnected by any form or medium of digital data communication, such as a communication network. Examples of communication networks include local area networks (“LAN”) and wide area networks (“WAN”), internetworks (eg, the Internet), and peer-to-peer networks (eg, ad hoc peer-to-peer networks).
システム100またはシステム500などのコンピューティングシステムは、クライアントおよびサーバを含み得る。クライアントおよびサーバは一般に互いに離れており、通常は通信ネットワーク(たとえば、ネットワーク156)を通じて対話する。クライアントとサーバの関係は、それぞれのコンピュータ上で実行しており互いにクライアントとサーバの関係を有するコンピュータプログラムにより生じる。いくつかの実装形態では、サーバは、データ(たとえば、コンテンツ項目を表すデータパケット)をクライアントデバイスに送信する(たとえば、クライアントデバイスと対話するユーザにデータを表示し、ユーザからユーザ入力を受け取る目的で)。(たとえば、ユーザ対話の結果として)クライアントデバイスにおいて生成されるデータは、サーバにおいてクライアントデバイスから受信され得る(たとえば、クライアントデバイス104またはナビゲータサービス106からデータ処理システム102によって受信される)。
Computing systems such as
動作は図面において特定の順序で描かれているが、そのような動作は、示される特定の順序または逐次的な順序で実行される必要はなく、すべての示される動作が実行される必要もない。本明細書で説明される活動は、異なる順序で実行され得る。 The actions are drawn in a particular order in the drawing, but such actions need not be performed in the specific order shown or in sequential order, nor do all the shown actions need to be performed. .. The activities described herein can be performed in a different order.
様々なシステムコンポーネントの分離は、すべての実装形態において分離を要求せず、説明されるプログラムコンポーネントは、単一のハードウェアまたはソフトウェア製品に含まれ得る。たとえば、NLPコンポーネント114およびダイレクトアクションハンドラコンポーネント122は、単一のコンポーネント、アプリ、プログラム、または1つまたは複数の処理回路を有するロジックデバイス、またはデータ処理システム102の1つまたは複数のサーバの一部であり得る。
Separation of various system components does not require separation in all implementations, and the program components described may be contained in a single hardware or software product. For example, NLP component 114 and direct action handler component 122 are a single component, an app, a program, or a logic device with one or more processing circuits, or part of one or more servers in
いくつかの例示的な実装形態をここで説明したが、前述のことは限定するものではなく例示であり、例として提示されたことは明らかである。特に、本明細書で提示される例の多くは、方法の行為またはシステム要素の具体的な組合せを伴い、それらの行為およびそれらの要素は、同じ目的を達成するために他の方法で組み合わされてもよい。一実装形態に関連して論じられた行為、要素、および特徴が、他の実装形態において類似する役割から除外されることは意図されない。 Although some exemplary implementations have been described here, it is clear that the above is not a limitation but an example and is presented as an example. In particular, many of the examples presented herein involve specific combinations of method actions or system elements, and those actions and those elements are combined in other ways to achieve the same purpose. You may. It is not intended that the actions, elements, and features discussed in relation to one implementation be excluded from similar roles in other implementations.
本明細書において使用される語句および用語は、説明を目的としており、限定するものと見なされるべきではない。「含む(including)」、「備える」、「有する」、「含む(containing)」、「伴う」、「よって特徴付けられる」、「を特徴とする」、および本明細書におけるそれらの変形の使用は、その後に列挙される項目、それらの等価物、および追加の項目、ならびに、それらの後で列挙される項目のみからなる代替的な実装形態を排他的に包含することが意図される。一実装形態では、本明細書で説明されるシステムおよび方法は、説明される要素、行為、またはコンポーネントの1つ、1つより多くの各々の組合せ、またはすべてからなる。 The terms and terms used herein are for illustration purposes only and should not be considered limiting. "Including", "including", "having", "containing", "accompanied", "characterized by", "featuring", and the use of those variants herein. Is intended to exclusively include alternative implementations consisting only of the items listed thereafter, their equivalents, and additional items, as well as the items listed after them. In one implementation, the systems and methods described herein consist of one, more than one, each combination, or all of the elements, actions, or components described.
単数形で言及される本明細書のシステムおよび方法の実装形態、要素、または行為へのあらゆる言及は、複数のこれらの要素を含む実装形態も包含してもよく、本明細書での任意の実装形態、要素、または行為への複数形でのあらゆる言及は、単一の要素だけを含む実装形態も包含してもよい。単数形または複数形での言及は、そこで開示されるシステムもしくは方法、それらのコンポーネント、行為、または要素を、単一の構成または複数の構成に限定することは意図されない。任意の情報、行為、または要素に基づく任意の行為または要素への言及は、行為または要素が任意の情報、行為、または要素に少なくとも一部基づくような実装形態を含み得る。 Any reference to an implementation, element, or action of the systems and methods herein referred to in the singular may also include an implementation that includes more than one of these elements, and is arbitrary herein. Any plural reference to an implementation, element, or action may also include an implementation that includes only a single element. References in the singular or plural are not intended to limit the systems or methods disclosed therein, their components, actions, or elements to a single or multiple configurations. References to any action or element based on any information, action, or element may include an implementation in which the action or element is at least partially based on any information, action, or element.
本明細書で開示される任意の実装形態が、任意の他の実装形態または実施形態と組み合わされてもよく、「ある実装形態」、「いくつかの実装形態」、「1つの実装形態」などへの言及は、必ずしも相互に排他的ではなく、実装形態に関連して説明される具体的な特徴、構造、または特性が、少なくとも1つの実装形態または実施形態に含まれ得ることを示すことが意図されている。本明細書で使用されるそのような用語は、必ずしもすべてが同じ実装形態に言及していない。任意の実装形態が、本明細書で開示される態様および実装形態と矛盾しない任意の方式で、包含的にまたは排他的に、任意の他の実装形態と組み合わされ得る。 Any of the embodiments disclosed herein may be combined with any other implementation or embodiment, such as "some implementations", "several implementations", "one implementation", etc. References to are not necessarily mutually exclusive and may indicate that specific features, structures, or properties described in relation to an implementation may be included in at least one implementation or embodiment. Intended. Such terms as used herein do not necessarily all refer to the same implementation. Any implementation can be combined with any other implementation, inclusively or exclusively, in any manner consistent with the embodiments disclosed herein and the implementation.
「または」への言及は、「または」を使用して説明されるあらゆる用語が、説明される用語の単数、1つより多く、およびすべてのいずれをも示し得るように、包含的であるものとして解釈され得る。「『A』と『B』のうちの少なくとも1つ」への言及は、「A」のみ、「B」のみ、ならびに「A」と「B」の両方を含み得る。「備える」または他のオープンな用語に関連して使用されるそのような言及は、追加の項目を含み得る。 References to "or" are inclusive so that any term described using "or" can indicate the singular, more than one, and all of the terms described. Can be interpreted as. References to "at least one of'A'and'B'" can include only "A", only "B", and both "A" and "B". Such references used in connection with "provide" or other open terms may include additional items.
図面、詳細な説明、または任意の請求項における技術的な特徴の後に、参照符号が続く場合、参照符号は、図面、詳細な説明、および請求項の明瞭さを高めるために含まれている。したがって、参照符号、またはそれらがないことはいずれも、どの請求項の要素の範囲に対しても限定する効果を有しない。 If a drawing, detailed description, or technical feature in any claim is followed by a reference code, the reference code is included to enhance the clarity of the drawing, detailed description, and claim. Therefore, neither the reference codes, nor their absence, has the effect of limiting the scope of the elements of any claim.
本明細書で説明されるシステムおよび方法は、それらの特性から逸脱することなく、他の具体的な形式で具現化され得る。前述の実装形態は、説明されるシステムおよび方法を限定するものではなく例示するものである。したがって、本明細書で説明されるシステムおよび方法の範囲は、前述の説明ではなく、添付の特許請求の範囲によって示され、請求項の意図および等価性の範囲内にある変化は、請求項に包含される。 The systems and methods described herein can be embodied in other specific forms without departing from their properties. The implementations described above are exemplary, but not limited to, the systems and methods described. Therefore, the scope of the systems and methods described herein is indicated by the appended claims, rather than by the aforementioned description, and any changes that fall within the intent and equivalence of the claims are in the claims. Included.
102 データ処理システム
104 クライアントデバイス
106 ナビゲータサービス
108 デジタルアシスタントアプリケーション
110 ナビゲーションアプリケーション
112 インターフェース
114 NLPコンポーネント
116 ナビゲーションインターフェースコンポーネント
118 ジオロケーション感知コンポーネント
120 ダイレクトアクションハンドラコンポーネント
122 オーディオ信号生成器コンポーネント
124 応答選択器コンポーネント
126 データリポジトリ
128 定型表現
130 パラメータ
132 ポリシー
134 応答データ
136 テンプレート
138 デジタルアシスタントインターフェースコンポーネント
140 位置発見器コンポーネント
142 経路決定器コンポーネント
144 データリポジトリ
146 ベクターベースの地図
148 スピーカ
150 トランスデューサ
152 周辺デバイス
154 センサ
156 ネットワーク
210 アクセス要求
215 基準枠データ
220 ダイレクトアクションデータ構造
225 応答
230 応答
310 ビューポート
315 現在地
320 点位置
335 移動経路
600 システム
605 バス
610 プロセッサ
615 メインメモリ
620 ROM
625 ストレージデバイス
630 入力デバイス
635 ディスプレイ
102 Data processing system
104 Client device
106 Navigator service
108 Digital Assistant Application
110 Navigation application
112 interface
114 NLP components
116 Navigation interface component
118 Geoposition Sensing Component
120 Direct Action Handler Component
122 Audio signal generator component
124 Response Selector Component
126 Data repository
128 boilerplate
130 parameters
132 Policy
134 Response data
136 template
138 Digital Assistant Interface Component
140 Position Discoverer Component
142 Router component
144 data repository
146 Vector-based map
148 speaker
150 Transducer
152 Peripheral devices
154 sensor
156 network
210 Access request
215 Reference frame data
220 Direct action data structure
225 response
230 response
310 viewport
315 Current location
320 point position
335 Travel route
600 system
605 bus
610 processor
615 Main memory
620 ROM
625 storage device
630 input device
635 display
Claims (20)
第1のクライアントデバイス上で実行しているナビゲーションアプリケーションにアクセスして、前記ナビゲーションアプリケーションのビューポートに表示される地理的領域に対応する基準枠内の複数の点位置を取り出すための、1つまたは複数のプロセッサを有するデータ処理システム上で実行されるナビゲーションインターフェースコンポーネントであって、前記複数の位置の各点位置が識別子を有する、ナビゲーションインターフェースコンポーネントと、
前記データ処理システム上で実行される自然言語プロセッサコンポーネントであって、
前記第1のクライアントデバイスと第2のクライアントデバイスのうちの少なくとも1つのセンサによって検出される入力オーディオ信号を受信し、
前記入力オーディオ信号を解析して、要求および参照語を識別し、
前記要求の前記識別に応答して、前記入力オーディオ信号から解析された前記参照語および前記点位置の前記識別子に基づいて、前記基準枠内の前記複数の点位置から点位置を識別する
ための、自然言語プロセッサコンポーネントと、
前記データ処理システム上で実行されるアクションハンドラコンポーネントであって、
前記入力オーディオ信号の前記検出に応答して識別された前記点位置を含む、アクションデータ構造を生成し、
前記アクションデータ構造を前記第1のクライアントデバイスに送信して、前記ナビゲーションアプリケーションに前記点位置を使用したナビゲーション案内プロセスを開始させる
ための、アクションハンドラコンポーネントとを備える、システム。 A system for interfacing between multiple applications in a networked computer environment.
One or one to access a navigation application running on the first client device to retrieve multiple point locations within the baseline corresponding to the geographic area displayed in the navigation application's viewport. A navigation interface component that is executed on a data processing system having a plurality of processors, wherein each point position of the plurality of positions has an identifier.
A natural language processor component that runs on the data processing system.
Upon receiving the input audio signal detected by at least one sensor of the first client device and the second client device,
The input audio signal is analyzed to identify the request and reference term.
To identify a point position from the plurality of point positions within the reference frame based on the reference word analyzed from the input audio signal and the identifier of the point position in response to the identification of the request. , Natural language processor components,
An action handler component that is executed on the data processing system.
Generate an action data structure that includes the point location identified in response to said detection of the input audio signal.
A system comprising an action handler component for transmitting the action data structure to the first client device and causing the navigation application to initiate a navigation guidance process using the point position.
前記自然言語プロセッサコンポーネントであって、
前記参照語が前記第2の地理的領域に対応する第2の部分に関連するが前記地理的領域に対応する前記第1の部分に関連しないと決定し、
前記参照語が前記第2の部分に関連するという前記決定に基づいて、前記部分内の前記複数の点位置から前記点位置を識別する
ための、前記自然言語プロセッサコンポーネントとを備える、請求項1から3のいずれか一項に記載のシステム。 A first, accessing the navigation application, corresponding to the geographic area, and corresponding to a second geographic area within a defined proximity to the destination of the routing operation of the navigation guidance process. The navigation interface component for retrieving the plurality of point positions within the reference frame having the portion of the
The natural language processor component
It is determined that the reference term relates to the second part corresponding to the second geographical area but not to the first part corresponding to the geographical area.
1. A claim comprising the natural language processor component for identifying the point position from the plurality of point positions within the portion, based on the determination that the reference term relates to the second portion. The system according to any one of 3 to 3.
前記第1のクライアントデバイスの前記第1の位置識別子および前記複数の点位置に対応する前記複数の第2の位置識別子に基づいて、前記複数の点位置から前記点位置を識別するための、前記自然言語プロセッサコンポーネントとを備える、請求項1から4のいずれか一項に記載のシステム。 By accessing the navigation application, a first position identifier of the first client device in the reference frame corresponding to the geographical area and a plurality of first positions corresponding to the plurality of point positions in the reference frame. The navigation interface component for retrieving the location identifier of 2 and
The method for identifying the point position from the plurality of point positions based on the first position identifier of the first client device and the plurality of second position identifiers corresponding to the plurality of point positions. The system according to any one of claims 1 to 4, comprising a natural language processor component.
前記自然言語プロセッサコンポーネントであって、
前記複数の点位置の各点位置および前記複数の検索語の各検索語に対して、セマンティックナレッジグラフを使用して前記点位置の前記識別子と前記検索語との間の意味的距離を決定し、
前記点位置の前記識別のために、前記複数の識別子と前記複数の検索語との間の前記複数の意味的距離に基づいて、前記複数の点位置から点位置のサブセットを選択する
ための、前記自然言語プロセッサコンポーネントとを備える、請求項1から5のいずれか一項に記載のシステム。 The navigation interface component for accessing the navigation application and retrieving a plurality of search terms received within a predetermined time frame prior to the reception of the input audio signal.
The natural language processor component
For each point position of the plurality of point positions and each search term of the plurality of search terms, a semantic knowledge graph is used to determine the semantic distance between the identifier of the point position and the search term. ,
To select a subset of point positions from the plurality of point positions, based on the plurality of semantic distances between the plurality of identifiers and the plurality of search terms for the identification of the point positions. The system according to any one of claims 1 to 5, comprising the natural language processor component.
前記補助語に基づいて前記ナビゲーションアプリケーションの前記ビューポートのサブセットエリアを決定し、
前記点位置の前記識別のために、前記補助語に基づいて決定された前記ビューポートの前記サブセットエリアに対応する前記複数の点位置から点位置のサブセットを選択する
ための、前記自然言語プロセッサコンポーネントを備える、請求項1から6のいずれか一項に記載のシステム。 The input audio signal is analyzed to identify an auxiliary language different from the reference word.
A subset area of the viewport of the navigation application is determined based on the auxiliary language.
The natural language processor component for selecting a subset of point positions from the plurality of point positions corresponding to the subset area of the viewport determined based on the auxiliary language for the identification of the point positions. The system according to any one of claims 1 to 6, wherein the system comprises.
前記第2の入力オーディオ信号の前記受信と前記入力オーディオ信号の前記受信との間に経過した時間が定められた閾値未満であると決定し、
前記経過時間が前記定められた閾値未満であるという前記決定に応答して、前記第2の入力オーディオ信号を解析して第2の参照語を識別し、
前記点位置の前記識別のために、前記第2の参照語に基づいて前記複数の点位置から点位置のサブセットを選択する
ための、前記自然言語プロセッサコンポーネントを備える、請求項1から7のいずれか一項に記載のシステム。 Upon receiving the second input audio signal detected by the sensor at least one of the first client device and the second client device,
It is determined that the time elapsed between the reception of the second input audio signal and the reception of the input audio signal is less than a defined threshold.
In response to the determination that the elapsed time is less than the defined threshold, the second input audio signal is analyzed to identify the second reference term.
Any of claims 1-7, comprising said natural language processor component for selecting a subset of point positions from said plurality of point positions based on said second reference term for said identification of said point position. The system described in one paragraph.
前記対応する複数の点位置に対する前記複数の指標尺度に基づいて、前記基準枠内の前記複数の点位置から前記点位置を識別することと
を行うための、前記自然言語プロセッサコンポーネントを備える、請求項1から8のいずれか一項に記載のシステム。 For each point position of the plurality of point positions, an index scale between the reference word and the identifier of the point position is determined, in which the index scale is such that the reference word is the point position. Determining and determining the likelihood of representing the identifier
A claim comprising the natural language processor component for identifying the point position from the plurality of point positions within the reference frame based on the plurality of index scales for the corresponding plurality of point positions. The system according to any one of items 1 to 8.
前記対応する複数の点位置に対する前記複数の意味的距離に基づいて、前記基準枠内の前記複数の点位置から前記点位置を識別する
ための、前記自然言語プロセッサコンポーネントを備える、請求項1から9のいずれか一項に記載のシステム。 For each point position at the plurality of point positions, a semantic knowledge graph is used to determine the semantic distance between the reference term and the identifier at the point position.
1 to claim 1, further comprising the natural language processor component for identifying the point positions from the plurality of point positions within the reference frame based on the plurality of semantic distances to the corresponding plurality of point positions. The system according to any one of 9.
前記要求タイプを含む前記アクションデータ構造を生成し、前記アクションデータ構造を前記第1のクライアントデバイスに送信して、前記ナビゲーションアプリケーションに、前記ビューポートに表示される前記地理的領域において前記点位置を提示するために前記要求タイプに対応する前記ナビゲーション案内プロセスの前記位置発見動作を開始させるための、前記アクションハンドラコンポーネントとを備える、請求項1から10のいずれか一項に記載のシステム。 A natural language processor component for determining a request type corresponding to a position-finding operation among a plurality of operations to be performed by the navigation application based on the request.
Generate the action data structure containing the request type and send the action data structure to the first client device to provide the navigation application with the point location in the geographic area displayed in the viewport. The system according to any one of claims 1 to 10, comprising said action handler component for initiating said position finding operation of said navigation guidance process corresponding to said request type for presentation.
前記要求タイプを含む前記アクションデータ構造を生成し、前記アクションデータ構造を前記第1のクライアントデバイスに送信して、前記ナビゲーションアプリケーションに、目的地としての前記点位置への移動経路を識別するために前記要求タイプに対応する前記ナビゲーション案内プロセスの前記経路決定動作を開始させるための、前記アクションハンドラコンポーネントとを備える、請求項1から11のいずれか一項に記載のシステム。 A natural language processor component for determining a request type corresponding to a routing action among a plurality of actions to be performed by the navigation application based on the request.
To generate the action data structure containing the request type and transmit the action data structure to the first client device to identify the navigation application with a travel route to the point position as a destination. The system according to any one of claims 1 to 11, comprising the action handler component for initiating the routing operation of the navigation guidance process corresponding to the request type.
1つまたは複数のプロセッサを有するデータ処理システムによって、第1のクライアントデバイス上で実行しているナビゲーションアプリケーションにアクセスして、前記ナビゲーションアプリケーションのビューポートに表示される地理的領域に対応する基準枠内の複数の点位置を取り出すステップであって、前記複数の位置の各点位置が識別子を有する、ステップと、
前記データ処理システムによって、前記第1のクライアントデバイスと第2のクライアントデバイスのうちの少なくとも1つのセンサによって検出される入力オーディオ信号を受信するステップと、
前記データ処理システムによって、前記入力オーディオ信号を解析して、要求および参照語を識別するステップと、
前記データ処理システムによって、前記要求を識別したことに応答して、前記入力オーディオ信号から解析された前記参照語および前記点位置の前記識別子に基づいて、前記基準枠内の前記複数の点位置から点位置を識別するステップと、
前記データ処理システムによって、前記入力オーディオ信号の前記検出に応答して識別された前記点位置を含む、アクションデータ構造を生成するステップと、
前記データ処理システムによって、前記アクションデータ構造を前記第1のクライアントデバイスに送信して、前記ナビゲーションアプリケーションに前記点位置を使用したナビゲーション案内プロセスを開始させるステップとを備える、方法。 A method for interfacing between multiple applications in a networked computer environment.
A data processing system with one or more processors accesses a navigation application running on the first client device and is within the baseline corresponding to the geographic area displayed in the viewport of the navigation application. A step of taking out a plurality of point positions of the above, wherein each point position of the plurality of positions has an identifier.
A step of receiving an input audio signal detected by the data processing system by at least one sensor of the first client device and the second client device.
A step of analyzing the input audio signal by the data processing system to identify a request and a reference word.
From the plurality of point positions within the reference frame, based on the reference word and the identifier of the point position analyzed from the input audio signal in response to the identification of the request by the data processing system. Steps to identify point positions and
A step of generating an action data structure by the data processing system, including said point positions identified in response to said detection of the input audio signal.
A method comprising the steps of transmitting the action data structure to the first client device by the data processing system and causing the navigation application to initiate a navigation guidance process using the point position.
前記データ処理システムによって、前記ナビゲーションアプリケーションにアクセスして、前記入力オーディオ信号の前記受信と同時に表示される前記地理的領域に対応する前記基準枠の第1の部分を決定し、前記第1のクライアントデバイスの前記測定された速度に基づいて、前記ビューポートに以前に表示されていた前記地理的領域に対応する前記基準枠の第2の部分を決定するステップとを備える、請求項14に記載の方法。 A step of identifying the measured velocity of the first client device based on the data acquired from the inertial motor unit by the data processing system.
The data processing system accesses the navigation application to determine a first portion of the reference frame corresponding to the geographical area that is displayed at the same time as the reception of the input audio signal, and the first client. 14. Method.
前記データ処理システムによって、前記参照語が前記第2の地理的領域に対応する第2の部分に関連するが前記地理的領域に対応する前記第1の部分に関連しないと決定するステップと、
前記データ処理システムによって、前記参照語が前記第2の部分に関連するという前記決定に基づいて、前記部分内の前記複数の点位置から前記点位置を識別するステップとを備える、請求項14または15に記載の方法。 A second geographic area that accesses the navigation application by the data processing system, corresponds to the geographic area, and is within a range of proximity defined from the destination of the routing operation of the navigation guidance process. The step of taking out the plurality of point positions in the reference frame having the first portion corresponding to
A step of determining by the data processing system that the reference term relates to the second part corresponding to the second geographic area but not to the first part corresponding to the geographic area.
14. The method described in 15.
前記データ処理システムによって、前記第1のクライアントデバイスの前記第1の位置識別子および前記複数の点位置に対応する前記複数の第2の位置識別子に基づいて、前記複数の点位置から前記点位置を識別するステップとを備える、請求項14から16のいずれか一項に記載の方法。 The data processing system accesses the navigation application to provide a first location identifier of the first client device within the reference frame corresponding to the geographic area and the plurality of point locations within the reference frame. And the step of retrieving multiple second position identifiers corresponding to
The data processing system determines the point position from the plurality of point positions based on the first position identifier of the first client device and the plurality of second position identifiers corresponding to the plurality of point positions. The method of any one of claims 14-16, comprising the step of identifying.
前記データ処理システムによって、前記複数の点位置の各点位置および前記複数の検索語の各検索語に対して、セマンティックナレッジグラフを使用して前記点位置の前記識別子と前記検索語との間の意味的距離を決定するステップと、
前記データ処理システムによって、前記点位置を識別するために、前記複数の識別子と前記複数の検索語との間の前記複数の意味的距離に基づいて、前記複数の点位置から点位置のサブセットを選択するステップとを備える、請求項14から17のいずれか一項に記載の方法。 A step of accessing the navigation application by the data processing system and retrieving a plurality of search terms received within a predetermined time frame prior to the reception of the input audio signal.
By the data processing system, for each point position of the plurality of point positions and each search term of the plurality of search terms, a semantic knowledge graph is used between the identifier of the point position and the search term. Steps to determine the semantic distance and
In order to identify the point position by the data processing system, a subset of the point position is selected from the plurality of point positions based on the plurality of semantic distances between the plurality of identifiers and the plurality of search terms. The method of any one of claims 14-17, comprising a step of selection.
前記データ処理システムによって、前記要求タイプを含む前記アクションデータ構造を生成し、前記アクションデータ構造を前記第1のクライアントデバイスに送信して、前記ナビゲーションアプリケーションに、前記ビューポートに表示される前記地理的領域において前記点位置を提示するために前記要求タイプに対応する前記ナビゲーション案内プロセスの前記位置発見動作を開始させるステップとを備える、請求項14から18のいずれか一項に記載の方法。 A step of determining a request type corresponding to a position-finding operation among a plurality of operations to be performed by the navigation application based on the request by the data processing system.
The data processing system generates the action data structure containing the request type and transmits the action data structure to the first client device so that the navigation application displays the geographic location in the viewport. The method of any one of claims 14-18, comprising the step of initiating the position finding operation of the navigation guidance process corresponding to the request type to present the point position in the region.
前記データ処理システムによって、前記要求タイプを含む前記アクションデータ構造を生成し、前記アクションデータ構造を前記第1のクライアントデバイスに送信して、前記ナビゲーションアプリケーションに、目的地としての前記点位置への移動経路を識別するために前記要求タイプに対応する前記ナビゲーション案内プロセスの前記経路決定動作を開始させるステップとを備える、請求項14から19のいずれか一項に記載の方法。 A step of determining a request type corresponding to a routing action among a plurality of actions to be performed by the navigation application based on the request by the data processing system.
The data processing system generates the action data structure including the request type, transmits the action data structure to the first client device, and moves the navigation application to the point position as a destination. The method of any one of claims 14-19, comprising the step of initiating the routing operation of the navigation guidance process corresponding to the request type to identify the route.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201862690049P | 2018-06-26 | 2018-06-26 | |
US62/690,049 | 2018-06-26 | ||
PCT/US2018/044756 WO2020005304A1 (en) | 2018-06-26 | 2018-08-01 | Interfacing between digital assistant applications and navigation applications |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2021529938A true JP2021529938A (en) | 2021-11-04 |
JP7176011B2 JP7176011B2 (en) | 2022-11-21 |
Family
ID=63174419
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2020569967A Active JP7176011B2 (en) | 2018-06-26 | 2018-08-01 | Interfacing between digital assistant applications and navigation applications |
Country Status (6)
Country | Link |
---|---|
US (1) | US20210199458A1 (en) |
EP (1) | EP3607274B1 (en) |
JP (1) | JP7176011B2 (en) |
KR (1) | KR102569372B1 (en) |
CN (1) | CN110869706B (en) |
WO (1) | WO2020005304A1 (en) |
Families Citing this family (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11842731B2 (en) * | 2020-01-06 | 2023-12-12 | Salesforce, Inc. | Method and system for executing an action for a user based on audio input |
KR20210089295A (en) * | 2020-01-07 | 2021-07-16 | 엘지전자 주식회사 | Data processing method based on artificial intelligence |
US11240366B2 (en) * | 2020-02-03 | 2022-02-01 | Microsoft Technology Licensing, Llc | Digital assistant for emergency calling |
US20220198063A1 (en) * | 2020-02-14 | 2022-06-23 | Balcony Labs Inc. | System and Process for Selective Location-Based Anonymity and Privacy |
KR20220059629A (en) * | 2020-11-03 | 2022-05-10 | 현대자동차주식회사 | Vehicle and method for controlling thereof |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2003150192A (en) * | 2001-11-08 | 2003-05-23 | Denso Corp | Voice recognition device |
JP2009264796A (en) * | 2008-04-22 | 2009-11-12 | Clarion Co Ltd | Information displaying apparatus, its controlling method, and controlling program |
JP2014065359A (en) * | 2012-09-25 | 2014-04-17 | Fujitsu Ten Ltd | Display control device, display system and display control method |
JP2016071865A (en) * | 2014-09-25 | 2016-05-09 | 富士通株式会社 | Reconcile method, controller, and storage media |
JP2016529580A (en) * | 2013-06-08 | 2016-09-23 | アップル インコーポレイテッド | Device, method and graphical user interface for synchronizing two or more displays |
Family Cites Families (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8255224B2 (en) * | 2008-03-07 | 2012-08-28 | Google Inc. | Voice recognition grammar selection based on context |
US20110099507A1 (en) * | 2009-10-28 | 2011-04-28 | Google Inc. | Displaying a collection of interactive elements that trigger actions directed to an item |
US8700655B2 (en) * | 2010-11-08 | 2014-04-15 | At&T Intellectual Property I, L.P. | Systems, methods, and computer program products for location salience modeling for multimodal search |
US9818401B2 (en) | 2013-05-30 | 2017-11-14 | Promptu Systems Corporation | Systems and methods for adaptive proper name entity recognition and understanding |
WO2016200381A1 (en) | 2015-06-10 | 2016-12-15 | Nuance Communications, Inc. | Motion adaptive speech recognition for enhanced voice destination entry |
US10268756B2 (en) * | 2015-12-18 | 2019-04-23 | Here Global B.V. | Method and apparatus for providing natural language input in a cartographic system |
-
2018
- 2018-08-01 EP EP18753524.0A patent/EP3607274B1/en active Active
- 2018-08-01 JP JP2020569967A patent/JP7176011B2/en active Active
- 2018-08-01 WO PCT/US2018/044756 patent/WO2020005304A1/en unknown
- 2018-08-01 US US16/076,193 patent/US20210199458A1/en active Pending
- 2018-08-01 CN CN201880028321.9A patent/CN110869706B/en active Active
- 2018-08-01 KR KR1020217002453A patent/KR102569372B1/en active IP Right Grant
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2003150192A (en) * | 2001-11-08 | 2003-05-23 | Denso Corp | Voice recognition device |
JP2009264796A (en) * | 2008-04-22 | 2009-11-12 | Clarion Co Ltd | Information displaying apparatus, its controlling method, and controlling program |
JP2014065359A (en) * | 2012-09-25 | 2014-04-17 | Fujitsu Ten Ltd | Display control device, display system and display control method |
JP2016529580A (en) * | 2013-06-08 | 2016-09-23 | アップル インコーポレイテッド | Device, method and graphical user interface for synchronizing two or more displays |
JP2016071865A (en) * | 2014-09-25 | 2016-05-09 | 富士通株式会社 | Reconcile method, controller, and storage media |
Also Published As
Publication number | Publication date |
---|---|
EP3607274A1 (en) | 2020-02-12 |
CN110869706B (en) | 2023-10-27 |
WO2020005304A1 (en) | 2020-01-02 |
KR102569372B1 (en) | 2023-08-22 |
CN110869706A (en) | 2020-03-06 |
JP7176011B2 (en) | 2022-11-21 |
US20210199458A1 (en) | 2021-07-01 |
EP3607274B1 (en) | 2021-10-06 |
KR20210025076A (en) | 2021-03-08 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20200258508A1 (en) | Interfacing between digital assistant applications and navigation applications | |
JP7176011B2 (en) | Interfacing between digital assistant applications and navigation applications | |
KR102271264B1 (en) | Dynamic language model | |
JP7032523B2 (en) | Graphical user interface rendering management with voice-driven computing infrastructure | |
US10204292B2 (en) | User terminal device and method of recognizing object thereof | |
KR102502617B1 (en) | Distributed identification in networked system | |
US11848009B2 (en) | Adaptive interface in a voice-activated network | |
TW201921336A (en) | Systems and methods for speech recognition | |
Janarthanam et al. | Evaluating a city exploration dialogue system combining question-answering and pedestrian navigation | |
US10922321B2 (en) | Interpreting user queries based on device orientation | |
KR20220077258A (en) | Big data based voice recognition poi system, method for the system processing, computer program for the system, and medium with the computer program | |
CN113468299A (en) | Data processing method and device, electronic equipment and computer storage medium |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20210205 |
|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20210205 |
|
A977 | Report on retrieval |
Free format text: JAPANESE INTERMEDIATE CODE: A971007Effective date: 20220204 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20220214 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20220512 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20220725 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20220913 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20221011 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20221109 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 7176011Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |