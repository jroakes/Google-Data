US8543576B1 - Classification of clustered documents based on similarity scores - Google Patents
Classification of clustered documents based on similarity scores Download PDFInfo
- Publication number
- US8543576B1 US8543576B1 US13/479,188 US201213479188A US8543576B1 US 8543576 B1 US8543576 B1 US 8543576B1 US 201213479188 A US201213479188 A US 201213479188A US 8543576 B1 US8543576 B1 US 8543576B1
- Authority
- US
- United States
- Prior art keywords
- cluster
- documents
- quality assurance
- requirement
- clusters
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/35—Clustering; Classification
- G06F16/353—Clustering; Classification into predefined classes
Definitions
- This specification relates to information management.
- ⁇ can receive a large number of messages from customers, potential customers, users and/or other people.
- a business and/or organization can receive documents from its customers and potential customers, such as email messages, messages from online forums, e.g., support forums or message boards, and other types of documents.
- documents can be related to a variety of different topics or issues.
- the documents can be related to problems experienced by a user and can include a request for assistance to solve the problems.
- This document describes techniques for information management, including clustering of inbound communications to an organization or business.
- one innovative aspect of the subject matter described in this specification can be embodied in methods that include receiving a set of clusters of documents.
- the methods include calculating a similarity score for each cluster wherein the similarity score is based at least in part on features included in the documents in the cluster and indicates a measure of similarity of the documents in the cluster.
- the methods include, for each cluster associated with a respective similarity score greater than a first threshold, identifying the cluster as satisfying a quality assurance requirement.
- the methods include, for each cluster associated with a respective similarity score less than a second threshold, identifying the cluster as failing the quality assurance requirement.
- the second threshold is less than the first threshold.
- the methods include, for each cluster associated with a similarity score less than or equal to the first threshold value and greater than or equal to the second threshold value, reviewing at least a subset of documents in the cluster to determine whether the cluster satisfies the quality assurance requirement.
- the methods include associating a label with each cluster in the set of clusters. The label indicates whether the cluster satisfied the quality assurance requirement.
- the method can include ranking the set of clusters based on the similarity scores associated with the clusters before identifying the clusters as satisfying the quality assurance requirement and before identifying the clusters as failing the quality assurance requirement. Identifying the cluster as satisfying the quality assurance requirement can include identifying the cluster as satisfying the quality assurance requirement without reviewing a subset of documents in the cluster to identify the cluster as satisfying the quality assurance requirement. Identifying the cluster as failing the quality assurance requirement can include identifying the cluster as failing the quality assurance requirement without reviewing a subset of documents in the cluster to identify the cluster as failing the quality assurance requirement.
- Reviewing at least a subset of documents in the cluster to determine whether the cluster satisfies the quality assurance requirement can include determining a topic purity score and a confidence interval score associated with the cluster.
- the topic purity score is based at least in part on the subset of documents in the cluster and can include a percentage of the documents in the cluster that are related to the same topic.
- the confidence interval score is based at least in part on the subset of documents and comprises an indication of the reliability of the topic purity score.
- the quality assurance requirement can include a topic purity requirement and a confidence interval requirement.
- Reviewing at least a subset of documents in the cluster to determine whether the cluster satisfies the quality assurance requirement can include comparing the topic purity score associated with the cluster and the confidence interval score associated with the cluster to the topic purity requirement and the confidence interval requirement, respectively, to determine whether the cluster satisfies the quality assurance requirement.
- the topic purity requirement is substantially 80% and the confidence interval requirement is substantially 80%.
- the method can include prior to receiving the set of clusters of documents, determining the first and second thresholds, wherein determining the first and second thresholds includes receiving a second set of clusters of documents; for each cluster in the second set of clusters: determining the topic purity score and the confidence level associated with the cluster, wherein the topic purity score comprises a percentage of the documents in the cluster that are related to the same topic and wherein the confidence interval score includes an indication of the reliability of the topic purity score; and for each cluster, calculating a similarity score based at least in part on features included in the documents; ranking the second set of clusters based on the similarity score; analyzing the second set of clusters in a descending order to determine the first threshold value, wherein the first threshold value corresponds to a similarity score associated with a first cluster that fails the quality assurance requirement; and analyzing the second set of clusters in an ascending order to determine the second threshold value, wherein the second threshold value corresponds to a similarity score associated with the first cluster that satisfies the quality assurance requirement.
- FIG. 1 illustrates an example system to cluster and review documents.
- FIG. 2 illustrates an example ranking of clusters.
- FIG. 3 is a flowchart of an example process to determine threshold values.
- FIG. 4 is a flowchart of an example process to cluster and review documents.
- FIG. 5 is a block diagram of an example computer system that can be used to implement the clustering system.
- this document describes a system (and methods) that receives and clusters user documents (e.g., user feedback documents or requests for support).
- a similarity score can be calculated for each cluster.
- the similarity score (e.g., a mutual information value) can provide an indication or a measure of the similarity of the documents in the cluster.
- the clusters can be ranked based on the similarity scores and a high quality threshold and low quality threshold can be determined based on the similarity scores and a quality assurance requirement.
- the high quality threshold is greater than the low quality threshold.
- additional user documents can be received and added to the existing clusters.
- new clusters can be formed.
- the similarity score can be calculated for each of the new/old clusters.
- the clusters can be ranked according to the similarity scores.
- the system can determine that each of the clusters having a score greater than the high quality threshold satisfies the quality assurance requirement.
- the system can determine that each of the clusters having a similarity score less than the low quality threshold does not satisfy the quality assurance requirement.
- each cluster having a score that is less than or equal to the high quality threshold and greater than or equal to the low quality threshold can be further analyzed to determine whether the cluster satisfies the quality assurance requirement.
- FIG. 1 illustrates an example system 100 to cluster and review documents.
- the system 100 includes a user device 102 , one or more user documents 104 , a network 106 and a document organization system 107 that includes a clustering module 108 , a stored documents database 110 , a quality assurance module 112 , a scoring module 114 and a filter 116 .
- the user device 102 can be any appropriate data processing apparatus.
- the user device 102 can be a personal computer, a laptop, a mobile communication device, a personal digital assistant or a smart phone.
- FIG. 1 illustrates a single user device 102 , multiple user devices can be included in system 100 .
- the user device 102 can include various software applications such as web browsers or word processors that can be used to create user documents 104 .
- the user documents 104 can be any appropriate type of document that includes user input, feedback or support requests.
- the user document 104 can include a request for help relating to a user's login problems, technical support issues or billing problems.
- the user documents 104 can include HTML forms, email messages, online forum posts/entries, word processing documents, bug tracking messages/documents, help desk messages, portable document format (PDF) documents and/or other types of documents transmitted, for example, from the user device 102 .
- PDF portable document format
- the user documents 104 can be transmitted to the document organization system 107 .
- the network 106 can be any type of network such as a local area network (LAN), wide area network (WAN), the Internet, or a combination thereof.
- the network 106 facilitates connectivity between the user device 102 and the document organization system 107 .
- the document organization system 107 can receive the user documents 104 and organize the user documents 104 according to topics or issues associated with the user documents 104 .
- the document organization system 107 includes a clustering module 108 , a stored documents database 110 , a quality assurance module 112 , a scoring module 114 and a filter 116 .
- the clustering module 108 can be configured to receive user documents 104 and form clusters of user documents according to the topic or issue associated with the user documents 104 (e.g., billing issues, technical support requests, requests for instructions to use a particular feature, etc.). For example, the clustering module 108 can access the stored documents database 110 and analyze a set of stored user documents 104 to identify similarities among the documents. In some implementations, the clustering module 108 attempts to group user documents 104 in various groupings until an optimal or near optimal grouping is identified. For example, the clustering module 108 can attempt to group the user documents 104 such that substantially all user documents in a particular cluster are related to or describe a similar topic or issue (e.g., 85%, 95% of the documents are related to the same issue).
- the topic or issue associated with the user documents 104 e.g., billing issues, technical support requests, requests for instructions to use a particular feature, etc.
- the clustering module 108 can access the stored documents database 110 and analyze a set of stored user documents 104
- the clusters can be associated with a label that indicates the topic or issue associated with the majority of user documents included in the cluster.
- the user documents can be associated with labels that indicate the topic or issue associated with the user document.
- the clustering module 108 can store the clusters of user documents in the stored documents database 110 (e.g., any appropriate type of database, memory or storage device) or can provide the clusters of user documents to the quality assurance module 114 .
- the clustering module 108 can use various hierarchical or partitional algorithms to analyze and identify the co-occurrence of features or keywords across the user documents 104 (e.g., words, phrases, numbers or text strings that are used as a basis for forming the clusters). For example, the clustering module 108 can use a distributed exchange algorithm, a k-means clustering algorithm or a quality threshold (“QT”) algorithm to form the clusters.
- features or keywords e.g., words, phrases, numbers or text strings that are used as a basis for forming the clusters.
- the clustering module 108 can use a distributed exchange algorithm, a k-means clustering algorithm or a quality threshold (“QT”) algorithm to form the clusters.
- QT quality threshold
- the clustering module 108 can identify a feature set before clustering the user documents 106 .
- the clustering module 108 can determine the feature set based on a domain-specific dictionary.
- the domain-specific dictionary can be derived from documents that correspond to the domain (e.g., technical support manuals, billing support documents, product marketing or instructional documents, etc.), for example, by performing a search of the documents to identify commonly occurring words, phrases, codes, numbers, etc.
- an information retrieval/text mining algorithm such as a term frequency-inverse document frequency algorithm, can be applied to the user documents 104 to identify statistically relevant features or keywords. The information retrieval/text mining algorithm can be applied before clustering the user documents 104 .
- the clustering module 108 can be configured such that common words that have little or minimal value as a feature can be excluded from being identified as a feature (i.e., blacklisted).
- common blacklisted terms include stop words (e.g., “the,” “and,” “a,” etc.), salutations (e.g., “Dear,” “Hello,” etc.) and boilerplate text (e.g., “From:,” “To:,” “Subject:,” “Cc:”, etc.).
- the quality assurance module 112 can access or receive the clusters of user documents and determine whether each user cluster satisfies a quality assurance requirement. For example, for each cluster of user documents, the quality assurance module 112 can randomly or pseudo-randomly select a subset of user documents from the cluster of documents and determine how many of the user documents included in the subset are related to the same topic or issue.
- the subset of user documents can include a predetermined number of user documents 104 that are randomly or pseudo-randomly selected from the cluster of user documents.
- the predetermined number of user documents can be any number of documents (e.g., 10, 20, 100) or any percentage of the total number of documents in the cluster (e.g., 10%, 50%, 80%) but is less than the total number of documents in the cluster.
- a user interacting with the quality assurance module 112 or the document organization system 107 can determine how many of the user documents included in the subset are related to the same topic or issue.
- the user interacting with the quality assurance module 112 or the document organization system 107 can analyze the subset of documents and determine a topic purity score associated with the cluster. For example, the user can analyze each user document 104 in the subset of documents and determine the percentage of the user documents 104 that are related to the same topic or issue.
- the subset of user documents can include twenty documents of which fourteen documents are related to overcharges to users' accounts and six documents are related to a variety of other topics.
- the user can assign a topic purity score of 70% to the cluster of user documents from which the subset of user documents was selected.
- the documents can be associated with labels.
- a document in a cluster can be associated with a label indicating the topic or issue associated with document.
- the labels can be associated with the documents as a result of being reviewed before clustering or as a result of clustering.
- the quality assurance module 112 can determine a confidence interval score associated with each cluster.
- the confidence interval score can be an indication of the reliability of the topic purity score and can be based on the total number of documents in the subset of user documents, the number of user documents in the subset of user documents that shared the same topic or issue and a distribution curve. If the number of user documents in a cluster is large, then a normal distribution can be used to calculate the confidence interval score. If the number of user documents in a cluster is small, then a binomial distribution can be used to calculate the confidence interval score.
- a binomial distribution can be used when the number of user documents in a cluster is less than 30, a normal distribution can be used when the number of user documents in a cluster is greater than 60 documents and a t-distribution can be used when the when the number of user documents in a cluster is less than or equal to 60 and greater than or equal to 30.
- the number of user documents in a cluster can be determined to be large or small based on empirical analysis of the specific data set.
- Various algorithms can be used to calculate the confidence interval score associated with each cluster. For example, the confidence interval score associated with a cluster of user documents can be calculated using the Adjusted Wald Method.
- the quality assurance module 112 can determine whether each cluster of user documents satisfies the quality assurance requirement.
- An example quality assurance requirement can be a requirement that each cluster has a topic purity score of at least 80% and a confidence interval score of at least 80% (e.g., a topic purity requirement of at least 80% and a confidence interval requirement of at least 80%).
- Other topic purity requirements and confidence interval requirements can be used and can be determined based on system requirements.
- the quality assurance module 112 can mark each cluster with a label to indicate whether the cluster satisfied the quality assurance requirement. For example, the quality assurance module 112 can associate a label of “Yes,” “Pass,” “Good,” etc. with a cluster to indicate that the cluster satisfied the quality assurance requirement and a label of “No,” “Fail,” “Bad,” etc. with a cluster to indicate that the cluster did not satisfy the quality assurance requirement. In addition, the quality assurance module 112 can associate a label of “To be reviewed” with a cluster to indicate that the cluster should be further reviewed to determine whether the cluster satisfies the quality assurance requirement. In some implementations, the quality assurance module 112 can label the cluster with the topic purity score and confidence interval score associated with the cluster. In some implementations, the quality assurance module 112 uses metadata to label the cluster.
- the scoring module 114 can access the clusters of user documents in the stored documents database 110 or can receive the clusters of user documents from the clustering module 108 or quality assurance module 112 .
- the scoring module 114 can analyze each cluster of user documents and determine a similarity score, which provides an indication or a measure of the similarity of the documents in the cluster. For example, the scoring module 114 can determine a mutual information value associated with each cluster of user documents as the similarity score.
- the mutual information value can be a probabilistic measure of how many features a user document 104 has in common with the other user documents 104 in the cluster. In some implementations, the mutual information value is a logarithmic function.
- the probability distribution of the feature t, in a user document d j can be ratio of the number of occurrences of the feature t, in the user document d j to the number of occurrences of the feature t i in the corpus of user documents.
- the filter 114 can access the clusters of user documents in the stored documents database 110 or receive the clusters of user documents from the quality assurance module 112 .
- the filter 114 can determine values corresponding to a high quality threshold and a low quality threshold based on the similarity scores associated with the clusters of user documents and a quality assurance requirement. For example, the filter 114 can rank the clusters of user documents according the similarity scores associated with each cluster of user documents such that a first cluster is ranked higher than a second cluster if the similarity score associated with the first cluster is greater than the similarity score associated with the second cluster.
- the similarity score can be an indication of the topic purity score. For example, a cluster having a low topic purity score (e.g., 30%) can be ranked lower because the cluster will likely have a lower similarity score.
- the filter 114 can determine the high quality threshold value. For example, the filter 114 can analyze the clusters of documents in a descending order (e.g., from the highest ranked cluster to the lowest ranked cluster) and determine the ranking of the first cluster that failed the quality assurance requirement. The filter 114 can set the high quality threshold value to be the similarity score associated with the first cluster that failed the quality assurance requirement.
- the filter 114 can determine the low quality threshold value by analyzing the clusters of documents in an ascending order (e.g., from the lowest ranked cluster to the highest ranked cluster) and determine the ranking of the first cluster that satisfies the quality assurance requirement.
- the filter 114 can set the low quality threshold value to be the similarity score associated with the first cluster that satisfies the quality assurance requirement.
- the filter 114 can store the low quality threshold value and the high quality threshold value.
- the filter 114 can determine whether each cluster satisfies the quality assurance requirement by analyzing the label that indicates whether the cluster satisfied the quality assurance requirement. For example, the filter 114 can determine whether a cluster satisfied the quality assurance requirement by determining whether the label is equal to “Yes” or “No.” In some implementations, the filter 114 can determine whether a cluster satisfied the quality assurance requirement by analyzing the topic purity score and the confidence interval score to determine whether the cluster satisfied the quality assurance requirement.
- FIG. 2 provides an example ranking of clusters 200 .
- the ranking of clusters 200 is organized in a descending order according to the similarity score associated with each cluster. For example, Cluster 145 is associated with the largest similarity score and is ranked highest, and Cluster 4 is associated with the smallest similarity score and is ranked lowest.
- the filter 116 can provide the low quality threshold value and the high quality threshold value to the quality assurance module 112 .
- the quality assurance module 112 can use the threshold values to reduce the number of clusters that need to be reviewed. For example, after a new set of user documents 104 (e.g., a set of user documents that are different from the user documents used to generate the low quality threshold value and the high quality threshold value) is clustered by the clustering module 108 , the quality assurance module 112 can determine the similarity scores associated with each of the newly formed clusters and compare the similarity scores to the high quality and/or low quality threshold values.
- a new set of user documents 104 e.g., a set of user documents that are different from the user documents used to generate the low quality threshold value and the high quality threshold value
- the quality assurance module 112 can automatically determine that the cluster satisfies the quality assurance requirement without determining the topic purity score and confidence interval score associated with the cluster. Similarly, if the similarity score associated with a cluster is less than the low quality threshold value, the quality assurance module 112 can automatically determine that the cluster fails the quality assurance requirement without determining the topic purity score and confidence interval score associated with the cluster. If the similarity score associated with a cluster is less than or equal to the high quality threshold and greater than or equal to the low quality threshold, the quality assurance module 112 can calculate the topic purity score and confidence interval score associated with the cluster to determine whether the cluster satisfies the quality assurance requirement. The quality assurance module 112 can mark these clusters with a label to indicate that these clusters should be further reviewed to determine whether the clusters satisfy the quality assurance requirement.
- the quality assurance module 112 ranks the new clusters according to the similarity score associated with each cluster and then automatically determines which of the clusters satisfy the quality assurance requirement and automatically determines which of the clusters fail the quality assurance requirement. For example, the quality assurance module 112 can rank the clusters in a descending order and determine that all the clusters having a similarity score greater than the high quality threshold value satisfy the quality assurance requirement without first determining the topic purity scores and confidence interval scores. Similarly, the quality assurance module 112 can determine that all the clusters having a similarity score less than the low quality threshold value do not satisfy the quality assurance requirement without first determining the topic purity scores and confidence interval scores.
- FIG. 3 is a flowchart of an example process 300 to determine threshold values.
- Process 300 begins by receiving user documents (at 302 ).
- the clustering module 108 can receive user documents 104 over a period of time (e.g., a day, a week, a month, etc.).
- the clustering module 108 can store the user of documents 104 in the stored documents database 110 .
- the corpus of stored user documents can be clustered (at 304 ).
- the clustering module 108 can access the stored documents database 110 and analyze the user documents 104 to identify similarities among the documents and group the user documents according to the identified similarities.
- the clustering module 108 can attempt to group user documents 104 in various groupings until an optimal or near optimal grouping is identified.
- the clustering module 108 can attempt to group the user documents 104 such that substantially all the user documents in a particular cluster are related to or describe a similar topic or issue (e.g., 85%, 95% of the documents are related to the same issue).
- the clustering module 108 can identify a feature set or keyword set before clustering the user documents 104 .
- the clustering module 108 can determine a feature set based on a domain-specific dictionary derived from documents corresponding to the domain (e.g., technical support manuals, billing support documents, etc.).
- an information retrieval/text mining algorithm e.g., a term frequency-inverse document frequency algorithm can be applied to the user documents 104 to identify statistically relevant features before clustering the user documents 104 .
- the quality assurance module can determine whether the cluster satisfies a quality assurance requirement (at 306 ).
- the quality assurance module 112 can randomly or pseudo-randomly select a subset of documents from a cluster of user documents.
- the subset of documents can include a predetermined number of documents (e.g., 20, 40, 100, etc.) or a predetermined percentage of the total number of documents in the cluster (e.g., 20%, 40%, etc.).
- a user interacting with the quality assurance module 108 or the document organization system 107 can analyze the user documents in the subset of documents and determine a topic purity score associated with the cluster.
- the user can analyze each user document 104 in the subset of documents and determine the percentage of the user documents 104 that are related to the same topic or issue.
- the quality assurance module 108 can determine the confidence interval score associated with the cluster.
- the confidence interval score can be an indication of the reliability of the topic purity score and can be based on the number of documents in the subset of user documents, the number of user documents in the subset of user documents that agreed on the same topic or issue and a distribution curve (e.g., a normal distribution or a beta distribution).
- Various algorithms can be used to calculate confidence interval score associated with each cluster. For example, the confidence interval score associated with the cluster of user documents can be calculated using the Adjusted Wald Method.
- the quality assurance module 112 can determine whether the cluster of user documents satisfies the quality assurance requirement.
- An example quality assurance requirement can be a requirement that each cluster has a topic purity score of at least 80% and a confidence interval score of at least 80%.
- Other topic purity requirements and confidence interval requirements can be used and can be determined based on system requirements.
- the quality assurance module 112 can mark the cluster with a label to indicate whether the cluster satisfied the quality assurance requirement. For example, the quality assurance module 112 can associate a label of “Yes” or “No” to the cluster.
- the quality assurance module 108 can analyze each cluster and determine whether each cluster satisfies the quality assurance requirement.
- the scoring module can determine a similarity score for each of the clusters (at 308 ). For example, the scoring module 114 can analyze each cluster of user documents and determine a similarity score that can provide an indication or a measure of the similarity of the documents in the cluster. For example, the scoring module 114 can determine a mutual information value associated with each cluster of user documents.
- the mutual information value can be a probabilistic measure of how many features a user document 104 has in common with the other user documents 104 in the cluster. In some implementations, the mutual information value is a logarithmic function.
- the filter can rank the clusters based on the similarity scores associated with each of the clusters (at 310 ). For example, the filter 114 can rank the clusters of user documents in a descending order according the similarity scores associated with each cluster of user documents.
- the filter can determine a high quality threshold value (at 312 ). For example, the filter 114 can analyze the clusters of documents in a descending order (e.g., from the highest ranked cluster to the lowest ranked cluster) and determine the ranking of the first cluster that failed the quality assurance requirement. The filter 114 can base the high quality threshold value on the similarity score associated with the first cluster that failed the quality assurance requirement.
- the filter can determine a low quality threshold value (at 314 ).
- the filter 114 can determine the low quality threshold value by analyzing the clusters of documents in an ascending order (e.g., from the lowest ranked cluster to the highest ranked cluster) and determine the ranking of the first cluster that satisfies the quality assurance requirement.
- the filter 114 can base the low quality threshold value on the similarity score associated with the first cluster that satisfies the quality assurance requirement.
- FIG. 4 is a flowchart of an example process 400 to cluster and review documents.
- the process 400 begins by receiving user documents from user devices (at 402 ).
- the clustering module 108 can receive the new user documents 104 over a period of time (e.g., a day, a week, a month, etc.).
- the new user documents 104 are different from the user documents 104 used to determine the high quality and low quality threshold values described above in connection with FIG. 3 .
- the clustering module 108 can store the user of documents 104 in the stored documents database 110 .
- the corpus of user documents can be clustered (at 404 ).
- the corpus of user documents can be clustered similar to the process described above in connection with FIG. 3 (at 404 ).
- the similarity score for each cluster can be calculated similar to the process described above in connection with FIG. 3 (at 406 ).
- the filter can rank the clusters according to the similarity score (at 408 ). For example, the filter 114 can rank the clusters in a descending order based on the similarity scores associated with the clusters.
- the quality assurance module can then automatically identify the clusters that satisfy the quality assurance requirement (at 410 ). For example, the quality assurance module 112 can analyze the similarity score associated with each cluster to automatically determine whether or not the cluster satisfies the quality assurance requirement. The quality assurance module 112 can analyze the ranked clusters in a descending order (e.g., analyze the highest ranked cluster first) and compare the similarity score to the high quality threshold. If the similarity score is greater than the high quality threshold value, then the quality assurance module 112 can determine that the cluster satisfies the quality assurance requirement.
- the quality assurance module 112 can mark or label each cluster with an appropriate label (e.g., “Yes,” “Passed,” “Good,” etc.) after determining that the cluster is associated with a similarity score greater than the high quality threshold.
- the quality assurance module 112 can stop analyzing clusters in a descending order after the quality assurance module identifies the first cluster associated with a similarity score less than or equal to the high quality threshold.
- the quality assurance module can then automatically identify the clusters that fail the quality assurance requirement (at 412 ). For example, after identifying the first cluster associated with a similarity score less than or equal to the high quality threshold, the quality assurance module 112 can analyze the ranked clusters in an ascending order (e.g., analyze the lowest ranked cluster first) and compare the similarity score associated with each cluster to the low quality threshold. If the similarity score is less than the low quality threshold, the quality assurance module 112 can determine that the cluster does not satisfy the quality assurance requirement. The quality assurance module 112 can mark or label each cluster with an appropriate label indicating that the cluster failed the quality assurance requirement (e.g., “No,” “Failed,” “Bad,” etc.). The quality assurance module 112 can stop analyzing clusters in an ascending order after it identifies the first cluster associated with a similarity score greater than or equal to the low quality threshold.
- the quality assurance module 112 can stop analyzing clusters in an ascending order after it identifies the first cluster associated with a similarity score greater than
- FIG. 4 describes the process 400 as identifying the clusters that satisfy the quality assurance requirement (at 410 ) and then identifying the clusters that do not satisfy the quality assurance requirement (at 412 ), the order of these steps can be changed. For example, the clusters that do not satisfy the quality assurance requirement can be identified and then the clusters that satisfy the quality assurance requirement can be identified. In some implementations, the clusters that satisfy the quality assurance requirement and the clusters that do not satisfy the quality assurance requirement can be identified in parallel.
- the quality assurance module can identify the remaining clusters as clusters that should be further reviewed (at 414 ). For example, the quality assurance module 112 can mark or label the clusters associated with a similarity score that is less than or equal to the high quality threshold and is greater than or equal to the low quality threshold as clusters that should be further reviewed to determine whether the cluster satisfies the quality assurance requirement.
- Each of the remaining clusters can be analyzed to determine whether the each of the remaining clusters satisfy the quality assurance requirement (at 416 ).
- a the quality assurance module 112 or the document organization system 107 can determine whether each cluster satisfies the quality assurance requirement as explained above in connection with FIG. 3 (e.g., reference 306 in FIG. 3 ).
- FIG. 5 is block diagram of an example computer system 500 that can be used to implement the document organization system 107 .
- the system 500 includes a processor 510 , a memory 520 , a storage device 530 , and an input/output device 540 .
- Each of the components 510 , 520 , 530 , and 540 can be interconnected, for example, using a system bus 550 .
- the processor 510 is capable of processing instructions for execution within the system 500 .
- the processor 510 is a single-threaded processor.
- the processor 510 is a multi-threaded processor.
- the processor 510 is capable of processing instructions stored in the memory 520 or on the storage device 530 .
- the system 500 includes multiple processors 510 each capable of processing instructions for execution within the system 500 .
- the memory 520 stores information within the system 500 .
- the memory 520 is a computer-readable medium.
- the memory 520 is a volatile memory unit.
- the memory 520 is a non-volatile memory unit.
- the storage device 530 is capable of providing mass storage for the system 500 .
- the storage device 530 is a computer-readable medium.
- the storage device 530 can include, for example, a hard disk device, an optical disk device, or some other large capacity storage device.
- the input/output device 540 provides input/output operations for the system 500 .
- the input/output device 540 can include one or more of a network interface device, e.g., an Ethernet card, a serial communication device, e.g., and RS-232 port, and/or a wireless interface device, e.g., an IEEE 802.11 card.
- the input/output device can include driver devices configured to receive input data and send output data to other input/output devices, e.g., keyboard, printer and display devices 560 .
- Other implementations, however, can also be used, such as mobile computing devices, mobile communication devices, set-top box television client devices, etc.
- the various functions of the document organization system 107 can be realized by instructions that upon execution cause one or more processing devices to carry out the processes and functions described above.
- Such instructions can comprise, for example, interpreted instructions, such as script instructions, e.g., JavaScriptTM or ECMAScript instructions, or executable code, or other instructions stored in a computer readable medium.
- the document organization system 107 can be distributively implemented over a network, such as a server farm, or can be implemented in a single computer device.
- implementations of the subject matter and the functional operations described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Implementations of the subject matter described in this specification can be implemented as one or more computer program products, i.e., one or more modules of computer program instructions encoded on a tangible program carrier for execution by, or to control the operation of, a processing system.
- the computer readable medium can be a machine readable storage device, a machine readable storage substrate, a memory device, a composition of matter effecting a machine readable propagated signal, or a combination of one or more of them.
- Implementations of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Implementations of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on a computer storage medium for execution by, or to control the operation of, data processing apparatus.
- the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- a computer storage medium can be, or be included in, a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them.
- a computer storage medium is not a propagated signal, a computer storage medium can be a source or destination of computer program instructions encoded in an artificially-generated propagated signal.
- the computer storage medium can also be, or be included in, one or more separate physical components or media (e.g., multiple CDs, disks, or other storage devices).
- the computer storage medium is a non-transitory medium.
- the operations described in this specification can be implemented as operations performed by a data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
- the term “data processing apparatus” encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or multiple ones, or combinations, of the foregoing.
- the apparatus can include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- the apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them.
- the apparatus and execution environment can realize various different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures.
- a computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment.
- a computer program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code).
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output.
- the processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing actions in accordance with instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a universal serial bus (USB) flash drive), to name just a few.
- Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- implementations of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that is used
- Implementations of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network.
- Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), an inter-network (e.g., the Internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
- LAN local area network
- WAN wide area network
- inter-network e.g., the Internet
- peer-to-peer networks e.g., ad hoc peer-to-peer networks.
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- a server transmits data (e.g., an HTML page) to a client device (e.g., for purposes of displaying data to and receiving user input from a user interacting with the client device).
- client device e.g., for purposes of displaying data to and receiving user input from a user interacting with the client device.
- Data generated at the client device e.g., a result of the user interaction
Abstract
Description
Purity(L)=sum(Positive(L))/(Sum(Positive(all labels)+Negative(L))
where Positive(L)=documents in the cluster positively associated with label L, Negative(L)=documents in the cluster known not to have label L and Positive(all labels)=documents in the cluster positively associated with label L or other labels.
MI(t i ,d j)=sum(Prob(t i ,d j)*log(Prob(t i ,d j)/(Prob(t i)*Prob(d j))))
where t=features in the corpus of user documents represented by the clusters; d=the set of documents in the corpus of user documents represented by the clusters; Prob(ti, dj)=the probability distribution of the feature t, in a user document dj; and i=index of keywords; and j=index of document in the corpus of user documents represented by the clusters. The probability distribution of the feature t, in a user document dj can be ratio of the number of occurrences of the feature t, in the user document dj to the number of occurrences of the feature ti in the corpus of user documents. In some implementations, Prob (ti, dj) can be calculated using the following function:
Prob(t i ,d j)=N(t i ,d j)/N,
where N=Σij N(ti, dj) and N(ti, dj) is the number of times feature ti appears in document dj.
Claims (24)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/479,188 US8543576B1 (en) | 2012-05-23 | 2012-05-23 | Classification of clustered documents based on similarity scores |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/479,188 US8543576B1 (en) | 2012-05-23 | 2012-05-23 | Classification of clustered documents based on similarity scores |
Publications (1)
Publication Number | Publication Date |
---|---|
US8543576B1 true US8543576B1 (en) | 2013-09-24 |
Family
ID=49181565
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/479,188 Active US8543576B1 (en) | 2012-05-23 | 2012-05-23 | Classification of clustered documents based on similarity scores |
Country Status (1)
Country | Link |
---|---|
US (1) | US8543576B1 (en) |
Cited By (22)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20130339373A1 (en) * | 2012-01-10 | 2013-12-19 | Ut-Battelle Llc | Method and system of filtering and recommending documents |
US20140025667A1 (en) * | 2012-07-17 | 2014-01-23 | Tennille D. Lowe | Systems and methods for computing compatibility ratings for an online collaborative environment |
US20140156660A1 (en) * | 2012-06-05 | 2014-06-05 | uTest, Inc. | Methods and systems for quantifying and tracking software application quality |
US20140289229A1 (en) * | 2013-03-22 | 2014-09-25 | International Business Machines Corporation | Using content found in online discussion sources to detect problems and corresponding solutions |
US20140303912A1 (en) * | 2013-04-07 | 2014-10-09 | Kla-Tencor Corporation | System and method for the automatic determination of critical parametric electrical test parameters for inline yield monitoring |
US9002848B1 (en) * | 2011-12-27 | 2015-04-07 | Google Inc. | Automatic incremental labeling of document clusters |
US20150161248A1 (en) * | 2011-09-30 | 2015-06-11 | Google Inc. | Merging semantically similar clusters based on cluster labels |
US20160019300A1 (en) * | 2014-07-18 | 2016-01-21 | Microsoft Corporation | Identifying Files for Data Write Operations |
WO2016175866A1 (en) * | 2015-04-30 | 2016-11-03 | Hewlett Packard Enterprise Development Lp | Identifying groups |
US20160335243A1 (en) * | 2013-11-26 | 2016-11-17 | Uc Mobile Co., Ltd. | Webpage template generating method and server |
US20180068222A1 (en) * | 2016-09-07 | 2018-03-08 | International Business Machines Corporation | System and Method of Advising Human Verification of Machine-Annotated Ground Truth - Low Entropy Focus |
US9928233B2 (en) | 2014-11-12 | 2018-03-27 | Applause App Quality, Inc. | Computer-implemented methods and systems for clustering user reviews and ranking clusters |
WO2019013998A1 (en) * | 2017-07-10 | 2019-01-17 | General Electric Company | Self-feeding deep learning method and system |
US10204153B2 (en) * | 2015-03-31 | 2019-02-12 | Fronteo, Inc. | Data analysis system, data analysis method, data analysis program, and storage medium |
CN109977988A (en) * | 2018-12-29 | 2019-07-05 | 天津南大通用数据技术股份有限公司 | The machine learning method and system classified in batches for magnanimity categorical data |
CN110825878A (en) * | 2018-08-09 | 2020-02-21 | 埃森哲环球解决方案有限公司 | Generating data associated with inadequately represented data based on received data input |
CN111201524A (en) * | 2018-08-30 | 2020-05-26 | 谷歌有限责任公司 | Percentile chaining clustering |
US10762065B2 (en) * | 2013-09-27 | 2020-09-01 | Intel Corporation | Mechanism for facilitating dynamic and proactive data management for computing devices |
WO2020197599A1 (en) * | 2019-03-27 | 2020-10-01 | BigID Inc. | Dynamic document clustering and keyword extraction |
CN113516112A (en) * | 2021-09-14 | 2021-10-19 | 长沙鹏阳信息技术有限公司 | Clustering-based method for automatically identifying and numbering regularly arranged objects |
US11200259B2 (en) | 2019-04-10 | 2021-12-14 | Ivalua S.A.S. | System and method for processing contract documents |
CN117708545A (en) * | 2024-02-01 | 2024-03-15 | 华中师范大学 | Viewpoint contribution degree evaluation method and system integrating theme extraction and cosine similarity |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20030233350A1 (en) * | 2002-06-12 | 2003-12-18 | Zycus Infotech Pvt. Ltd. | System and method for electronic catalog classification using a hybrid of rule based and statistical method |
US20100082626A1 (en) * | 2008-09-19 | 2010-04-01 | Esobi Inc. | Method for filtering out identical or similar documents |
US20100318526A1 (en) * | 2008-01-30 | 2010-12-16 | Satoshi Nakazawa | Information analysis device, search system, information analysis method, and information analysis program |
-
2012
- 2012-05-23 US US13/479,188 patent/US8543576B1/en active Active
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20030233350A1 (en) * | 2002-06-12 | 2003-12-18 | Zycus Infotech Pvt. Ltd. | System and method for electronic catalog classification using a hybrid of rule based and statistical method |
US20100318526A1 (en) * | 2008-01-30 | 2010-12-16 | Satoshi Nakazawa | Information analysis device, search system, information analysis method, and information analysis program |
US20100082626A1 (en) * | 2008-09-19 | 2010-04-01 | Esobi Inc. | Method for filtering out identical or similar documents |
Cited By (36)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9336301B2 (en) * | 2011-09-30 | 2016-05-10 | Google Inc. | Merging semantically similar clusters based on cluster labels |
US20150161248A1 (en) * | 2011-09-30 | 2015-06-11 | Google Inc. | Merging semantically similar clusters based on cluster labels |
US9002848B1 (en) * | 2011-12-27 | 2015-04-07 | Google Inc. | Automatic incremental labeling of document clusters |
US20130339373A1 (en) * | 2012-01-10 | 2013-12-19 | Ut-Battelle Llc | Method and system of filtering and recommending documents |
US9256649B2 (en) * | 2012-01-10 | 2016-02-09 | Ut-Battelle Llc | Method and system of filtering and recommending documents |
US9704171B2 (en) * | 2012-06-05 | 2017-07-11 | Applause App Quality, Inc. | Methods and systems for quantifying and tracking software application quality |
US20140156660A1 (en) * | 2012-06-05 | 2014-06-05 | uTest, Inc. | Methods and systems for quantifying and tracking software application quality |
US20140025667A1 (en) * | 2012-07-17 | 2014-01-23 | Tennille D. Lowe | Systems and methods for computing compatibility ratings for an online collaborative environment |
US20140289229A1 (en) * | 2013-03-22 | 2014-09-25 | International Business Machines Corporation | Using content found in online discussion sources to detect problems and corresponding solutions |
US9892193B2 (en) * | 2013-03-22 | 2018-02-13 | International Business Machines Corporation | Using content found in online discussion sources to detect problems and corresponding solutions |
US20140303912A1 (en) * | 2013-04-07 | 2014-10-09 | Kla-Tencor Corporation | System and method for the automatic determination of critical parametric electrical test parameters for inline yield monitoring |
US10762065B2 (en) * | 2013-09-27 | 2020-09-01 | Intel Corporation | Mechanism for facilitating dynamic and proactive data management for computing devices |
US20160335243A1 (en) * | 2013-11-26 | 2016-11-17 | Uc Mobile Co., Ltd. | Webpage template generating method and server |
US10747951B2 (en) * | 2013-11-26 | 2020-08-18 | Uc Mobile Co., Ltd. | Webpage template generating method and server |
US20160019300A1 (en) * | 2014-07-18 | 2016-01-21 | Microsoft Corporation | Identifying Files for Data Write Operations |
US9928233B2 (en) | 2014-11-12 | 2018-03-27 | Applause App Quality, Inc. | Computer-implemented methods and systems for clustering user reviews and ranking clusters |
US10204153B2 (en) * | 2015-03-31 | 2019-02-12 | Fronteo, Inc. | Data analysis system, data analysis method, data analysis program, and storage medium |
US10534800B2 (en) | 2015-04-30 | 2020-01-14 | Micro Focus Llc | Identifying groups |
WO2016175866A1 (en) * | 2015-04-30 | 2016-11-03 | Hewlett Packard Enterprise Development Lp | Identifying groups |
US20180068222A1 (en) * | 2016-09-07 | 2018-03-08 | International Business Machines Corporation | System and Method of Advising Human Verification of Machine-Annotated Ground Truth - Low Entropy Focus |
CN110869942A (en) * | 2017-07-10 | 2020-03-06 | 通用电气公司 | Self-feedback deep learning method and system |
WO2019013998A1 (en) * | 2017-07-10 | 2019-01-17 | General Electric Company | Self-feeding deep learning method and system |
CN110869942B (en) * | 2017-07-10 | 2023-05-09 | 通用电气公司 | Self-feed deep learning method and system |
US11657316B2 (en) * | 2017-07-10 | 2023-05-23 | General Electric Company | Self-feeding deep learning method and system |
CN110825878A (en) * | 2018-08-09 | 2020-02-21 | 埃森哲环球解决方案有限公司 | Generating data associated with inadequately represented data based on received data input |
CN110825878B (en) * | 2018-08-09 | 2023-09-29 | 埃森哲环球解决方案有限公司 | Generating data associated with insufficiently represented data based on received data input |
CN111201524A (en) * | 2018-08-30 | 2020-05-26 | 谷歌有限责任公司 | Percentile chaining clustering |
CN111201524B (en) * | 2018-08-30 | 2023-08-25 | 谷歌有限责任公司 | Percentile linked clustering |
CN109977988A (en) * | 2018-12-29 | 2019-07-05 | 天津南大通用数据技术股份有限公司 | The machine learning method and system classified in batches for magnanimity categorical data |
US11243990B2 (en) * | 2019-03-27 | 2022-02-08 | BigID Inc. | Dynamic document clustering and keyword extraction |
WO2020197599A1 (en) * | 2019-03-27 | 2020-10-01 | BigID Inc. | Dynamic document clustering and keyword extraction |
US11200259B2 (en) | 2019-04-10 | 2021-12-14 | Ivalua S.A.S. | System and method for processing contract documents |
CN113516112B (en) * | 2021-09-14 | 2021-11-30 | 长沙鹏阳信息技术有限公司 | Clustering-based method for automatically identifying and numbering regularly arranged objects |
CN113516112A (en) * | 2021-09-14 | 2021-10-19 | 长沙鹏阳信息技术有限公司 | Clustering-based method for automatically identifying and numbering regularly arranged objects |
CN117708545A (en) * | 2024-02-01 | 2024-03-15 | 华中师范大学 | Viewpoint contribution degree evaluation method and system integrating theme extraction and cosine similarity |
CN117708545B (en) * | 2024-02-01 | 2024-04-30 | 华中师范大学 | Viewpoint contribution degree evaluation method and system integrating theme extraction and cosine similarity |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8543576B1 (en) | Classification of clustered documents based on similarity scores | |
AU2018383346B2 (en) | Domain-specific natural language understanding of customer intent in self-help | |
Ahmed Abbasi et al. | Benchmarking twitter sentiment analysis tools | |
US20190163711A1 (en) | Ranking and recommending hashtags | |
US20190349320A1 (en) | System and method for automatically responding to user requests | |
US8386487B1 (en) | Clustering internet messages | |
US20180373788A1 (en) | Contrastive multilingual business intelligence | |
US8972413B2 (en) | System and method for matching comment data to text data | |
US8290927B2 (en) | Method and apparatus for rating user generated content in search results | |
US8423551B1 (en) | Clustering internet resources | |
US9152625B2 (en) | Microblog summarization | |
US20170147676A1 (en) | Segmenting topical discussion themes from user-generated posts | |
US20130159277A1 (en) | Target based indexing of micro-blog content | |
US9189470B2 (en) | Generation of explanatory summaries | |
US9521189B2 (en) | Providing contextual data for selected link units | |
US9183312B2 (en) | Image display within web search results | |
US9990359B2 (en) | Computer-based analysis of virtual discussions for products and services | |
US11586684B2 (en) | Serving multiple content items responsive to a single request | |
US11789946B2 (en) | Answer facts from structured content | |
US8620918B1 (en) | Contextual text interpretation | |
US20170228464A1 (en) | Finding users in a social network based on document content | |
US10467300B1 (en) | Topical resource recommendations for a displayed resource | |
US11615245B2 (en) | Article topic alignment | |
US10733221B2 (en) | Scalable mining of trending insights from text | |
US10896461B2 (en) | Method and apparatus for data mining based on users' search behavior |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:BURYAK, KIRILL;PENG, JUN;LEWIS, GLENN M.;AND OTHERS;SIGNING DATES FROM 20120730 TO 20120807;REEL/FRAME:028743/0283 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
FPAY | Fee payment |
Year of fee payment: 4 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044101/0299Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |