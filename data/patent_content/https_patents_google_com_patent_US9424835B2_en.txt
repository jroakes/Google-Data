US9424835B2 - Statistical unit selection language models based on acoustic fingerprinting - Google Patents
Statistical unit selection language models based on acoustic fingerprinting Download PDFInfo
- Publication number
- US9424835B2 US9424835B2 US14/850,249 US201514850249A US9424835B2 US 9424835 B2 US9424835 B2 US 9424835B2 US 201514850249 A US201514850249 A US 201514850249A US 9424835 B2 US9424835 B2 US 9424835B2
- Authority
- US
- United States
- Prior art keywords
- acoustic
- unit
- fingerprint
- new
- database
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000000034 method Methods 0.000 claims abstract description 49
- 238000012549 training Methods 0.000 claims description 52
- 238000011524 similarity measure Methods 0.000 claims description 24
- 238000004590 computer program Methods 0.000 abstract description 12
- 230000015654 memory Effects 0.000 description 39
- 230000015572 biosynthetic process Effects 0.000 description 32
- 238000003786 synthesis reaction Methods 0.000 description 32
- 230000008569 process Effects 0.000 description 26
- 238000004891 communication Methods 0.000 description 18
- 238000012545 processing Methods 0.000 description 12
- 238000004422 calculation algorithm Methods 0.000 description 7
- 230000003287 optical effect Effects 0.000 description 6
- 238000004519 manufacturing process Methods 0.000 description 4
- 238000013459 approach Methods 0.000 description 3
- 230000008901 benefit Effects 0.000 description 3
- 230000001413 cellular effect Effects 0.000 description 3
- 238000010586 diagram Methods 0.000 description 3
- 230000006870 function Effects 0.000 description 3
- 238000013507 mapping Methods 0.000 description 3
- 230000003190 augmentative effect Effects 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 239000004973 liquid crystal related substance Substances 0.000 description 2
- 230000002085 persistent effect Effects 0.000 description 2
- 230000000644 propagated effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000002194 synthesizing effect Effects 0.000 description 2
- 239000013598 vector Substances 0.000 description 2
- 230000002730 additional effect Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 238000011161 development Methods 0.000 description 1
- 230000018109 developmental process Effects 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 238000010295 mobile communication Methods 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 150000003839 salts Chemical class 0.000 description 1
- 230000011218 segmentation Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 238000010561 standard procedure Methods 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 239000013589 supplement Substances 0.000 description 1
- 230000001360 synchronised effect Effects 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000007704 transition Effects 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L19/00—Speech or audio signals analysis-synthesis techniques for redundancy reduction, e.g. in vocoders; Coding or decoding of speech or audio signals, using source filter models or psychoacoustic analysis
- G10L19/018—Audio watermarking, i.e. embedding inaudible data in the audio signal
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/08—Text analysis or generation of parameters for speech synthesis out of text, e.g. grapheme to phoneme translation, prosody generation or stress or intonation determination
Definitions
- This disclosure generally relates to acoustic unit sequence modeling.
- Text-to-speech synthesis systems are used to produce artificial human speech. Often, synthesized speech can be created by concatenating pieces of recorded speech that are stored in some database.
- Unit selection speech synthesis is one type of text-to-speech synthesis which builds a statistical language model representing the probability of a certain sequence of acoustic units.
- a unit selection synthesis system creates a large database of recorded speech, where each recorded utterance is segmented into individual phones, or acoustic units. An index of the acoustic units in the database is then created based on the segmentation and various acoustic parameters.
- the synthesizer selects a sequence of acoustic units by determining and selecting the best chain of candidate acoustic units from the database.
- an aspect of the subject matter described in this specification may involve a process for synthesizing speech using a unit selection synthesis system.
- the system may obtain a database of recorded training utterances and output a probabilistic language model that can be efficiently encoded in a finite state transducer framework to enable speech synthesis.
- the unit selection synthesis system may first require access to a large database of training utterances. This may be provided to the system, for example, using a high-quality synthesis engine that may be used to synthesize a very large corpus of text, such as the text found in the entire English-language Wikipedia database. The synthesized utterances may then be segmented into individual phones, or acoustic units. The acoustic units can be stored in the training utterances database along with any corresponding linguistic information.
- the acoustic units in the training utterances database can be indexed using some corresponding acoustic unit identity.
- the unit identities can take some sequential integer identities.
- the system can also use other notions of identities, for example an acoustic fingerprint.
- the system may estimate a probability of the acoustic unit occurring in a new corpus of text, thus determining a complete probabilistic language model.
- the individual probability estimations can be achieved, for example, using statistical language modeling techniques.
- the system can associate the probability estimations with their corresponding acoustic unit and acoustic unit index in data triples which may be stored in a data store.
- a finite state transducer framework can access at least a portion of the stored data triples to enable speech synthesis.
- the unit selection synthesis system may receive an update to the training utterances database.
- linguistic and acoustic changes to the recorded training utterances database can be frequent.
- the finite state transducer framework is provided with reliable statistical estimates of the acoustic units.
- the system could become unreliable and may be prone to selecting invalid audio and linguistic features resulting in bad speech synthesis.
- rebuilding the probabilistic language model from an updated training utterances database that can include updated acoustic units is costly both regarding time and the required computational resources.
- the system may avoid rebuilding the probabilistic language model from an updated training utterances database in such scenarios. Instead, the system may generate acoustic fingerprints of the acoustic units and introduce similarity measures to replace hardcoded database-specific acoustic unit identities. Such a use of acoustic fingerprints and similarity measures can ensure that the originally recorded training utterances database remains persistent, thus allowing for infrequent estimation of the probabilistic language model whilst accommodating additional and potentially frequent linguistic and acoustic updates.
- the system may proceed as above, obtaining a database of recorded training utterances segmented into individual acoustic units.
- the system may index the database by generating an acoustic fingerprint of each acoustic unit.
- the system may estimate a probability of the acoustic unit occurring in a new corpus of text, thus determining a complete probabilistic language model.
- the system may calculate a new set of acoustic fingerprints for each updated acoustic unit.
- the system may need not to estimate an entirely new probabilistic model incorporating both the original set of acoustic units and the updated acoustic units. Rather, the system may instead define a similarity measure between the acoustic fingerprints of two acoustic units. The system may then select the most similar acoustic fingerprint of an original acoustic unit to the new acoustic fingerprint of the updated acoustic unit.
- the system can associate the corresponding probability estimate of the original acoustic unit occurring in the new corpus of text to the updated acoustic unit and acoustic fingerprint in a data triple.
- the newly generated data triples comprising an updated acoustic unit, acoustic fingerprint of the updated acoustic unit and previously estimated corresponding probability can be stored in an additional data store.
- a finite state transducer framework can access at least a portion of the stored data triples in both data stores to enable speech synthesis.
- the subject matter described in this specification may be embodied in methods that may include the actions of obtaining a unit database of acoustic units and, for each acoustic unit, linguistic data corresponding to the acoustic unit. Additional actions include, for each acoustic unit: generating an acoustic fingerprint; determining a probability of the linguistic data corresponding to the acoustic unit occurring in a text corpus; and storing data that associates the acoustic unit with (i) the acoustic fingerprint of the acoustic unit and (ii) the probability of the linguistic data corresponding to the acoustic unit occurring in the text corpus.
- Further actions include providing at least a portion of the stored data to a finite state transducer training engine that is configured to train one or more finite state transducers that are used in generating speech from text; determining that the unit database of acoustic units has been updated to include one or more new acoustic units; for each new acoustic unit in the updated unit database: generating an acoustic fingerprint for the new acoustic unit; identifying an acoustic unit that (i) has an acoustic fingerprint that is indicated as similar to the fingerprint of the new acoustic unit, and (ii) has a stored associated probability; storing data that associates the new acoustic unit with (i) the acoustic fingerprint of the new acoustic unit and (ii) the probability associated with the acoustic unit that has the acoustic fingerprint that is indicated as similar to the fingerprint of the new acoustic unit; and providing at least a portion of the new stored data to the fi
- the generated acoustic fingerprint for each acoustic unit indexes the unit database.
- the linguistic data is a tri-phone.
- the unit database is considered to be updated when new acoustic units have been added or existing acoustic units have been deleted.
- a new acoustic unit is an acoustic unit that did not exist prior to the database update.
- determining that the unit database has been updated comprises determining that the unit database includes acoustic units that do not have an associated probability of the linguistic data corresponding to the acoustic unit occurring in the text corpus.
- identifying an acoustic unit that has an acoustic fingerprint that is indicated as similar to the fingerprint of the new acoustic unit comprises: calculating a respective similarity measure between the acoustic fingerprints of the new acoustic units and the acoustic fingerprints of each other acoustic unit in the unit database; determining a nearest acoustic fingerprint to the acoustic fingerprint of the new acoustic units according to the similarity measure; and identifying the acoustic unit associated with the nearest acoustic fingerprint.
- the similarity measure is a Hamming distance measuring the minimum number of errors that could have transformed one fingerprint into another.
- Particular embodiments can be implemented, in certain instances, to realize one or more of the following advantages.
- a highly accurate, reliable probabilistic model of unit sequences selected from the inventory may be built and maintained.
- This highly accurate probabilistic model may be used in a finite state transducer framework, for example, for the development of future synthesizers.
- the present subject matter may enable the building of a very large footprint and very high-quality synthesis system without the need to optimize the system for memory or disk footprint.
- the high-quality synthesis system constructed by the disclosed methods may allow for bootstrapping highly optimized, mathematically principled synthesis systems very quickly and efficiently without having to worry about databases being completely synchronized.
- the computational resources and memory required by the system may be reduced, whilst the runtime performance may be increased.
- FIG. 1 is a block diagram of an example system for unit selection language modeling based on acoustic fingerprinting.
- FIG. 2 is a flowchart of an example process of unit selection language modeling based on acoustic fingerprinting.
- FIG. 3 is a diagram showing an example of a system that may be used to implement the systems and methods described in this document.
- an aspect of the subject matter described in this specification may involve a process for synthesizing speech using a unit selection synthesis system.
- the system may obtain a database of recorded training utterances and output a probabilistic language model that can be efficiently encoded in a finite state transducer framework to enable speech synthesis.
- the unit selection synthesis system may first require access to a large database of training utterances. This may be provided to the system using standard techniques, for example a high-quality synthesis engine may be used to synthesize a gigantic corpus of text, for example the entire English Wikipedia. The synthesized utterances may then be segmented into individual phones, or acoustic units. The acoustic units can be stored in the training utterances database along with any corresponding linguistic information.
- the acoustic units in the training utterances database can be indexed via some corresponding acoustic unit identity.
- the unit identities can be assumed to take some sequential integer identities.
- the system can also use other notions of identities, for example an acoustic fingerprint.
- the system may estimate a probability of the acoustic unit occurring in a new corpus of text, thus determining a complete probabilistic language model.
- the individual probability estimations can be achieved, for example, using standard statistical language modeling techniques.
- the system can associate the probability estimations with their corresponding acoustic unit and acoustic unit index in data triples which may be stored in a data store.
- a finite state transducer framework can access at least a portion of the stored data triples to enable speech synthesis.
- the unit selection synthesis system may receive an update to the training utterances database.
- linguistic and acoustic changes to the recorded training utterances database can be frequent.
- the finite state transducer framework is provided with reliable statistical estimates of the acoustic units.
- the system could become unreliable and may be prone to selecting invalid audio and linguistic features resulting in bad speech synthesis.
- rebuilding the probabilistic language model from an updated training utterances database that can include updated acoustic units is costly both regarding time and the required computational resources.
- the system may avoid rebuilding the probabilistic language model from an updated training utterances database in such scenarios. Instead, the system may generate acoustic fingerprints of the acoustic units and introduce similarity measures to replace hardcoded database-specific acoustic unit identities. Such a use of acoustic fingerprints and similarity measures can ensure that the originally recorded training utterances database remains persistent, thus allowing for infrequent estimation of the probabilistic language model whilst accommodating additional and potentially frequent linguistic and acoustic updates.
- the system may proceed as above, obtaining a database of recorded training utterances segmented into individual acoustic units.
- the system may index the database by generating an acoustic fingerprint of each acoustic unit.
- the system may estimate a probability of the acoustic unit occurring in a new corpus of text, thus determining a complete probabilistic language model.
- the system may calculate a new set of acoustic fingerprints for each updated acoustic unit.
- the system may need not to estimate an entirely new probabilistic model incorporating both the original set of acoustic units and the updated acoustic units. Rather, the system may instead define a similarity measure between the fingerprints of two acoustic units. The system may then identify the most similar acoustic fingerprint of an original acoustic unit to the new acoustic fingerprint of the updated acoustic unit.
- the system can assign the corresponding probability estimate of the original acoustic unit occurring in the new corpus of text to the updated acoustic unit and acoustic fingerprint in a data triple.
- the newly generated data triples comprising an updated acoustic unit, similar acoustic fingerprint and assigned probability can be stored in an additional data store.
- a finite state transducer framework can access at least a portion of the stored data triples in both data stores to enable speech synthesis.
- FIG. 1 is a block diagram of an example system 100 for modeling audio sequences.
- the system 100 may include a database of recorded training utterances 110 that stores recorded utterances of speech segmented into acoustic units, an acoustic feature extractor 120 , which includes a fingerprinter 121 , that receives the acoustic units and generates an acoustic unit index, a probability estimator 130 that receives the indexed acoustic units and determines the probability of each acoustic unit occurring in a corpus of text, a database 140 that stores the acoustic unit, its corresponding index and determined probability of occurrence in data triples, and a FST trainer 150 that uses the stored data triples to generate synthesized speech.
- a database of recorded training utterances 110 that stores recorded utterances of speech segmented into acoustic units
- an acoustic feature extractor 120 which includes a fingerprinter 121 , that receives the acoustic
- the system may further include a database of updated recorded training utterances 160 that stores updated recorded utterances of speech segmented into acoustic units, a fingerprint similarity engine 170 that compares acoustic fingerprints of differing acoustic units and identifies similar acoustic fingerprints, a prior probability assigner 180 that assigns an existing probability estimate of occurrence of an acoustic unit in a corpus of text to the updated acoustic units, and a database 190 that stores the updated acoustic unit, its identified similar acoustic fingerprint and assigned probability of occurrence in data triples.
- a database of updated recorded training utterances 160 that stores updated recorded utterances of speech segmented into acoustic units
- a fingerprint similarity engine 170 that compares acoustic fingerprints of differing acoustic units and identifies similar acoustic fingerprints
- a prior probability assigner 180 that assigns an existing probability estimate of occurrence of an
- the database of training utterances 110 may include recorded utterances that can be stored in association with linguistic data. Each recorded utterance may be segmented into individual phones, or acoustic units.
- the database of training utterances may include an acoustic unit that is a recording of a human pronouncing the phone “/s/” in the text “salt”.
- the associated linguistic data corresponding to the acoustic units can include information about parts of speech, supra-segmental phonological information, articulation features and so on.
- the database of training utterances may also include a recording of a human pronouncing the phone “/s/” in the text “post”. The phone “/s/” preceded by silence and followed by the phone “/a/” may sound slightly different from the phone “/s/” preceded by the phone “/o/” and followed by the phone “/t/”.
- the acoustic feature extractor 120 may access the training utterances database 110 (in stage A) and index the acoustic units according to some acoustic unit identity. For example, the acoustic unit identities can be assumed to take some sequential integer identities.
- the acoustic feature extractor 120 may also include a fingerprinter 121 that can generate an acoustic fingerprint for each acoustic unit using an acoustic fingerprinting algorithm.
- the acoustic fingerprints may act as an index for the database of acoustic units.
- An acoustic fingerprint is a condensed, digital summary of an acoustic unit and may be used as an acoustic unit identifier.
- the fingerprints may be represented as fixed-length, integer valued vectors.
- the complete set of acoustic fingerprints may act as an index for the database of acoustic units.
- the indexed acoustic units can be sent to a probability estimator 130 (stage B).
- the probability estimator calculates the probabilities of each acoustic unit occurring in some text corpus, generating a probabilistic language model.
- Several standard statistical language modeling techniques may be employed to generate the probabilistic language model, for example one approach to unit selection speech synthesis is described in ALLAUZEN et al. ‘Statistical modeling for unit selection in speech synthesis’. In: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, 2004, p. 55.
- Each acoustic unit, corresponding acoustic fingerprint and estimated probability of occurrence may be sent to a data store 140 .
- the data store 140 may store the received data as data triples comprising an acoustic unit, corresponding acoustic fingerprint and estimated probability of occurrence.
- the finite state transducer trainer 150 may receive (stage D) at least a portion of the stored data triples in the data store 140 .
- the finite state transducer trainer 150 may efficiently encode the received data in a finite state transducer system for speech synthesis (stage E).
- the finite state transducer system may comprise a first transducer configured to determine the probabilities of all audio unit sequences occurring in a corpus of text, and a second configured to determine the probabilities of phonemes occurring given the acoustic unit sequences.
- a final transducer is obtained through which the best path of acoustic units can be determined. Any appropriate standard speech recognition decoding techniques can be applied in order to produce an audio output comprising an optimal sequence of acoustic units which are concatenated to produce the final output.
- the system may further include an updated training utterances database 160 of acoustic units.
- the updated training utterances database 160 may include, for example, recorded utterances that can be stored with linguistic data, wherein each recorded utterance may be segmented into individual acoustic units.
- the training utterances database 110 may be augmented and the updated training utterances database 160 may introduce additional, new acoustic units that did not exist prior to the database update and therefore do not have an associated probability of occurrence stored in the data store 140 .
- some acoustic units stored in the training utterances database 110 may have been deleted and the updated training database 110 may have been reduced.
- the acoustic feature extractor 120 may access the updated training utterances database 160 (stage F) and index the acoustic units according to some acoustic unit identity. For example, the acoustic unit identities can be assumed to take some sequential integer identities.
- the acoustic feature extractor 120 may also include a fingerprinter 121 that can generate an acoustic fingerprint for each new acoustic unit using an acoustic fingerprinting algorithm. The generated acoustic fingerprints may act as an index for the database of acoustic units.
- the fingerprint similarity engine 170 may receive the new, updated acoustic units and their corresponding acoustic fingerprints from the acoustic feature extractor 120 (stage G). The fingerprint similarity engine 170 can access the data store 140 and may compare the newly received updated acoustic fingerprints to the originally generated acoustic fingerprints stored in the data store 140 . The fingerprint similarity engine may define a similarity measure and calculate the similarity measure between acoustic fingerprints. The fingerprint similarity engine may then accordingly identify an original acoustic unit whose corresponding acoustic fingerprint is indicated as similar to the fingerprint of the new acoustic unit.
- acoustic similarity measures between the acoustic fingerprints may be employed, for example a Hamming distance.
- the Hamming distance may calculate the similarity between acoustic fingerprints represented as vectors of integers by performing a XOR logical operation between their respective bit representations and counting the number of bit errors.
- the prior probability assigner 180 may receive the set of new acoustic units (stage H) and identify similar acoustic fingerprints from the fingerprint similarity engine. The prior probability assigner may also access the data store 140 and may identify the probability estimates developed by the probability estimator 130 that correspond to the set of identified similar acoustic fingerprints received from the fingerprint similarity engine. The probability assigner may then assign the identified probability estimates to the new acoustic units. Each new acoustic unit, acoustic fingerprint and assigned probability of occurrence may be sent to a data store 190 .
- the data store 190 may store the received data (stage I) as data triples comprising an acoustic unit, acoustic fingerprint and assigned probability of occurrence.
- the finite state transducer trainer 150 may also receive at least a portion of the stored data triples in the data store 190 (stage J).
- the finite state transducer trainer may efficiently encode the received data in a finite state transducer system for speech synthesis (stage K).
- system 100 may be used where functionality of the training utterances database 110 , updated training utterances database 160 , acoustic feature extractor 120 , probability estimator 130 , fingerprint similarity engine 170 , prior probability assigner 180 , fingerprint-audio-probability triple databases 140 and 190 and finite state transducer trainer 150 may be combined, further distributed, or interchanged.
- the system 100 may be implemented in a single device or distributed across multiple devices.
- FIG. 2 is a flowchart of an example process 200 for unit selection language modeling based on acoustic fingerprinting. The following describes the process 200 as being performed by components of the system 100 that are described with reference to FIG. 1 . However, the process 200 may be performed by other systems or system configurations.
- the process 200 may include obtaining an acoustic unit database of acoustic units and, for each acoustic unit, linguistic data corresponding to the acoustic unit ( 201 ).
- the acoustic unit database may be created offline using an existing very high-quality synthesis engine that synthesizes a gigantic corpus of text, resulting in many sequences of acoustic units with corresponding linguistic information.
- Each acoustic unit may be associated with a hard-coded, database-specific unit identifier.
- the process 200 may include, for each acoustic unit in the database 110 , generating an acoustic fingerprint ( 202 ).
- the acoustic feature extractor 120 may include a fingerprinter 121 that can derive an acoustic fingerprint a u for each acoustic unit u using any acoustic fingerprinting algorithm, for example an algorithm that satisfies Equation (1), below.
- a ( u ) [ a 1 ( u ), . . .
- Equation (1) for a given unit u comprised of l frames, an acoustic fingerprint a (u) is given by a (finite) sequence of n integers ⁇ a i (u) ⁇ .
- the process may include performing a one-to-one mapping of the acoustic unit database-specific unit identifiers to the respective acoustic fingerprints uniquely identifying individual acoustic units.
- the process 200 may include, for each acoustic unit in the database 110 , determining a probability of the linguistic data corresponding to the acoustic unit occurring in a text corpus ( 203 ).
- the probability estimator 130 may receive the acoustic units and their corresponding indices and use standard statistical language modeling techniques to determine individual probability estimates of each acoustic unit occurring in some corpus of text, thus determining a complete probabilistic language model.
- One example of such a statistical language modeling approach employs the following approximation using the n-grams. That is, given a particular unit sequence u 1 , . . . , u k , deriving accurate probabilities for the sequence using n-gram modeling as in Equation (2).
- Equation (2) u ⁇ U refer to individual units from the unit database U from a high quality unit selection voice.
- the universe of these estimated unit transition probabilities comprise the probabilistic language model P(u). Because the corpus of text used to create the acoustic unit database may be enormous, the probabilistic language model may be estimated reliably.
- the process 200 may include, for each acoustic unit in the database 110 , storing data that associates the acoustic unit with (i) the acoustic fingerprint of the acoustic unit, and (ii) the probability of the linguistic data corresponding to the acoustic unit occurring in the text corpus ( 204 ).
- the data store 140 may receive the estimated probabilities calculated by the probability estimator 130 along with the associated acoustic units and acoustic fingerprints.
- the data store 140 may store the received data in data triples corresponding to each acoustic unit.
- the process 200 may include providing at least a portion of the stored data in the unit database to a finite state transducer training engine that is configured to train one or more finite state transducers that are used in generating speech from text ( 205 ).
- the finite state transducer trainer 150 may implement the data triples and, in turn, the determined probabilistic language model, in an efficient finite state transducer framework to produce an efficient, statistically principled method to perform acoustic unit selection and speech synthesis.
- the process 200 may include determining that the unit database of acoustic units has been updated ( 206 ) to include one or more new acoustic units u′ ⁇ U. For example, in some production environments linguistic and acoustic changes to the recorded training utterances database can be frequent, and often results are expected to be live in extremely short time frames.
- the unit database of acoustic units may be augmented to include new acoustic units that did not exist prior to the database update.
- the updated acoustic units may be stored in an updated audio unit database of acoustic units 160 . If the database is updated, a similar mapping as described above may be performed, which may allow for an updated probabilistic language model without having to go through the incredibly time consuming process of reconstructing the database 140 that corresponds to the new version of the database.
- the process 200 may include, for each new acoustic unit in the database 160 , generating an acoustic fingerprint a i (u′) for each new acoustic unit u′ using any acoustic fingerprinting algorithm ( 207 ).
- the process may include performing a one-to-one mapping of the acoustic unit database-specific unit identifiers to the respective acoustic fingerprints uniquely identifying individual acoustic units.
- the process 200 may include, for each new acoustic unit in the database 160 , identifying an existing acoustic unit that (i) has an acoustic fingerprint that is indicated as similar to the fingerprint of the new acoustic unit, and (ii) has an associated probability stored in the unit database ( 208 ).
- a similar acoustic fingerprint is defined as in Equation (3).
- a ⁇ ⁇ ( u ′ ) argmin u ⁇ U ⁇ D ⁇ ( a ⁇ ( u ′ ) , a ⁇ ( u ) ) ( 3 )
- D(a(u′),a(u)) is a similarity measure.
- the fingerprint similarity engine 170 may define a similarity measure between acoustic fingerprints.
- the simplest similarity measure between acoustic fingerprints is a Hamming distance measuring the minimum number of errors that could have transformed one fingerprint into another. The Hamming distance provides a robust measure of similarity.
- the fingerprint similarity engine 170 may have access to the stored data triples in the data store 140 and use the similarity measure to identify an original acoustic unit that has a similar acoustic fingerprint to the acoustic fingerprint of the new acoustic unit ( 209 ).
- the probability assigner 180 may also access the data store 140 .
- the probability assigner may receive the identified original acoustic unit that has a similar acoustic fingerprint to the acoustic fingerprint of the new acoustic unit and identify the corresponding probability estimate stored in the database 140 . That is the system estimates the probability as given in Equation (4).
- P ( u′ 1 , . . . ,u′ k ) P ( a 1 ( u ′), . . . , a k ( u ′)) (4)
- Equation (4) the system then fetches the probabilities of the sequence of nearest fingerprints P(â 1 (u′), . . . , â k (u′)).
- the process 200 may include, for each acoustic unit in the database 110 , storing data that associates the new acoustic unit with (i) the acoustic fingerprint of the new acoustic unit, and (ii) the probability associated with the acoustic unit that has the acoustic fingerprint that is indicated as similar to the fingerprint of the new acoustic unit ( 210 ).
- the data store 190 may receive the assigned probabilities calculated by the prior probability assigner 180 along with the new acoustic units and new acoustic fingerprints.
- the data store 190 may store the received data in data triples corresponding to each acoustic unit.
- the process 200 may include also providing at least a portion of the stored data in the updated unit database to a finite state transducer training engine that is configured to train one or more finite state transducers that are used in generating speech from text ( 211 ).
- the finite state transducer trainer 150 may implement the data triples and, in turn, the determined probabilistic language model, in an efficient finite state transducer framework to produce an efficient, statistically principled method to perform acoustic unit selection and speech synthesis.
- the probabilistic language model P(u) may be very efficiently encoded in a finite state transducer framework that offers many attractive features exploited during the runtime stage.
- the runtime stage it is easy to perform single-best or n-best search through the resulting transducer to obtain the most optimal sequence of acoustic units that need to be selected for speech synthesis.
- the runtime stage is very efficient and can perform extremely well.
- FIG. 3 shows an example of a computing device 300 and a mobile computing device 350 that can be used to implement the techniques described here.
- the computing device 300 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.
- the mobile computing device 350 is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smart-phones, and other similar computing devices.
- the components shown here, their connections and relationships, and their functions, are meant to be examples only, and are not meant to be limiting.
- the computing device 300 includes a processor 302 , a memory 304 , a storage device 306 , a high-speed interface 308 connecting to the memory 304 and multiple high-speed expansion ports 310 , and a low-speed interface 312 connecting to a low-speed expansion port 314 and the storage device 306 .
- Each of the processor 302 , the memory 304 , the storage device 306 , the high-speed interface 308 , the high-speed expansion ports 310 , and the low-speed interface 312 are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate.
- the processor 302 can process instructions for execution within the computing device 300 , including instructions stored in the memory 304 or on the storage device 306 to display graphical information for a GUI on an external input/output device, such as a display 316 coupled to the high-speed interface 308 .
- an external input/output device such as a display 316 coupled to the high-speed interface 308 .
- multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory.
- multiple computing devices may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).
- the memory 304 stores information within the computing device 300 .
- the memory 304 is a volatile memory unit or units.
- the memory 304 is a non-volatile memory unit or units.
- the memory 304 may also be another form of computer-readable medium, such as a magnetic or optical disk.
- the storage device 306 is capable of providing mass storage for the computing device 300 .
- the storage device 306 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations.
- Instructions can be stored in an information carrier.
- the instructions when executed by one or more processing devices (for example, processor 302 ), perform one or more methods, such as those described above.
- the instructions can also be stored by one or more storage devices such as computer- or machine-readable mediums (for example, the memory 304 , the storage device 306 , or memory on the processor 302 ).
- the high-speed interface 308 manages bandwidth-intensive operations for the computing device 300 , while the low-speed interface 312 manages lower bandwidth-intensive operations. Such allocation of functions is an example only.
- the high-speed interface 308 is coupled to the memory 304 , the display 316 (e.g., through a graphics processor or accelerator), and to the high-speed expansion ports 310 , which may accept various expansion cards (not shown).
- the low-speed interface 312 is coupled to the storage device 306 and the low-speed expansion port 314 .
- the low-speed expansion port 314 which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet) may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- input/output devices such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.
- the computing device 300 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server 320 , or multiple times in a group of such servers. In addition, it may be implemented in a personal computer such as a laptop computer 322 . It may also be implemented as part of a rack server system 324 . Alternatively, components from the computing device 300 may be combined with other components in a mobile device (not shown), such as a mobile computing device 350 . Each of such devices may contain one or more of the computing device 300 and the mobile computing device 350 , and an entire system may be made up of multiple computing devices communicating with each other.
- the mobile computing device 350 includes a processor 352 , a memory 364 , an input/output device such as a display 354 , a communication interface 366 , and a transceiver 368 , among other components.
- the mobile computing device 350 may also be provided with a storage device, such as a micro-drive or other device, to provide additional storage.
- a storage device such as a micro-drive or other device, to provide additional storage.
- Each of the processor 352 , the memory 364 , the display 354 , the communication interface 366 , and the transceiver 368 are interconnected using various buses, and several of the components may be mounted on a common motherboard or in other manners as appropriate.
- the processor 352 can execute instructions within the mobile computing device 350 , including instructions stored in the memory 364 .
- the processor 352 may be implemented as a chipset of chips that include separate and multiple analogue and digital processors.
- the processor 352 may provide, for example, for coordination of the other components of the mobile computing device 350 , such as control of user interfaces, applications run by the mobile computing device 350 , and wireless communication by the mobile computing device 350 .
- the processor 352 may communicate with a user through a control interface 358 and a display interface 356 coupled to the display 354 .
- the display 354 may be, for example, a TFT (Thin-Film-Transistor Liquid Crystal Display) display or an OLED (Organic Light Emitting Diode) display, or other appropriate display technology.
- the display interface 356 may comprise appropriate circuitry for driving the display 354 to present graphical and other information to a user.
- the control interface 358 may receive commands from a user and convert them for submission to the processor 352 .
- an external interface 362 may provide communication with the processor 352 , so as to enable near area communication of the mobile computing device 350 with other devices.
- the external interface 362 may provide, for example, for wired communication in some implementations, or for wireless communication in other implementations, and multiple interfaces may also be used.
- the memory 364 stores information within the mobile computing device 350 .
- the memory 364 can be implemented as one or more of a computer-readable medium or media, a volatile memory unit or units, or a non-volatile memory unit or units.
- An expansion memory 374 may also be provided and connected to the mobile computing device 350 through an expansion interface 372 , which may include, for example, a SIMM (Single In Line Memory Module) card interface.
- SIMM Single In Line Memory Module
- the expansion memory 374 may provide extra storage space for the mobile computing device 350 , or may also store applications or other information for the mobile computing device 350 .
- the expansion memory 374 may include instructions to carry out or supplement the processes described above, and may include secure information also.
- the expansion memory 374 may be provide as a security module for the mobile computing device 350 , and may be programmed with instructions that permit secure use of the mobile computing device 350 .
- secure applications may be provided via the SIMM cards, along with additional information, such as placing identifying information on the SIMM card in a non-hackable manner.
- the memory may include, for example, flash memory and/or NVRAM memory (non-volatile random access memory), as discussed below.
- instructions are stored in an information carrier.
- the instructions when executed by one or more processing devices (for example, processor 352 ), perform one or more methods, such as those described above.
- the instructions can also be stored by one or more storage devices, such as one or more computer- or machine-readable mediums (for example, the memory 364 , the expansion memory 374 , or memory on the processor 352 ).
- the instructions can be received in a propagated signal, for example, over the transceiver 368 or the external interface 362 .
- the mobile computing device 350 may communicate wirelessly through the communication interface 366 , which may include digital signal processing circuitry where necessary.
- the communication interface 366 may provide for communications under various modes or protocols, such as GSM voice calls (Global System for Mobile communications), SMS (Short Message Service), EMS (Enhanced Messaging Service), or MMS messaging (Multimedia Messaging Service), CDMA (code division multiple access), TDMA (time division multiple access), PDC (Personal Digital Cellular), WCDMA (Wideband Code Division Multiple Access), CDMA2000, or GPRS (General Packet Radio Service), among others.
- GSM voice calls Global System for Mobile communications
- SMS Short Message Service
- EMS Enhanced Messaging Service
- MMS messaging Multimedia Messaging Service
- CDMA code division multiple access
- TDMA time division multiple access
- PDC Personal Digital Cellular
- WCDMA Wideband Code Division Multiple Access
- CDMA2000 Code Division Multiple Access
- GPRS General Packet Radio Service
- a GPS (Global Positioning System) receiver module 370 may provide additional navigation- and location-related wireless data to the mobile computing device 350 , which may be used as appropriate by applications running on the mobile computing device 350 .
- the mobile computing device 350 may also communicate audibly using an audio codec 360 , which may receive spoken information from a user and convert it to usable digital information.
- the audio codec 360 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of the mobile computing device 350 .
- Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on the mobile computing device 350 .
- the mobile computing device 350 may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a cellular telephone 380 . It may also be implemented as part of a smart-phone 382 , personal digital assistant, or other similar mobile device.
- Embodiments of the subject matter, the functional operations and the processes described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible nonvolatile program carrier for execution by, or to control the operation of, data processing apparatus.
- the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- the computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.
- data processing apparatus encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- the apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a computer program (which may also be referred to or described as a program, software, a software application, a module, a software module, a script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a standalone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code).
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- special purpose logic circuitry e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- Computers suitable for the execution of a computer program include, by way of example, can be based on general or special purpose microprocessors or both, or any other kind of central processing unit.
- a central processing unit will receive instructions and data from a read-only memory or a random access memory or both.
- the essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a universal serial bus (USB) flash drive), to name just a few.
- PDA personal digital assistant
- GPS Global Positioning System
- USB universal serial bus
- Computer readable media suitable for storing computer program instructions and data include all forms of nonvolatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD-ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto optical disks e.g., CD-ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Abstract
Description
a (u)=[a 1(u), . . . ,a n(u)] (1)
In Equation (1), for a given unit u comprised of l frames, an acoustic fingerprint a(u) is given by a (finite) sequence of n integers {ai(u)}. The process may include performing a one-to-one mapping of the acoustic unit database-specific unit identifiers to the respective acoustic fingerprints uniquely identifying individual acoustic units.
P(u′ 1 , . . . ,u′ k)=P(a 1(u′), . . . ,a k(u′)) (4)
Claims (17)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/850,249 US9424835B2 (en) | 2014-09-30 | 2015-09-10 | Statistical unit selection language models based on acoustic fingerprinting |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201462057588P | 2014-09-30 | 2014-09-30 | |
US14/850,249 US9424835B2 (en) | 2014-09-30 | 2015-09-10 | Statistical unit selection language models based on acoustic fingerprinting |
Publications (2)
Publication Number | Publication Date |
---|---|
US20160093295A1 US20160093295A1 (en) | 2016-03-31 |
US9424835B2 true US9424835B2 (en) | 2016-08-23 |
Family
ID=55585146
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/850,249 Active US9424835B2 (en) | 2014-09-30 | 2015-09-10 | Statistical unit selection language models based on acoustic fingerprinting |
Country Status (1)
Country | Link |
---|---|
US (1) | US9424835B2 (en) |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20110153050A1 (en) * | 2008-08-26 | 2011-06-23 | Dolby Laboratories Licensing Corporation | Robust Media Fingerprints |
US20150019220A1 (en) * | 2012-01-24 | 2015-01-15 | Auraya Pty Ltd | Voice authentication and speech recognition system and method |
-
2015
- 2015-09-10 US US14/850,249 patent/US9424835B2/en active Active
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20110153050A1 (en) * | 2008-08-26 | 2011-06-23 | Dolby Laboratories Licensing Corporation | Robust Media Fingerprints |
US20150019220A1 (en) * | 2012-01-24 | 2015-01-15 | Auraya Pty Ltd | Voice authentication and speech recognition system and method |
Non-Patent Citations (4)
Title |
---|
Allauzen et al., "Statistical Modeling for Unit Selection in Speech Synthesis," ACL '04 Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, Article No. 55, 2004, 8 pages. |
Cano et al., "Robust Sound Modeling for Song Detection in Broadcast Audio," in Proc. Audio Engineering Society 112th International Convention, May 2002, pp. 1-7. |
Doets, "Modeling Audio Fingerprints: Structure, Distortion, Capacity," PhD thesis, Technical University of Delft, Oct. 2010, 175 pages. |
Haitsma and Kalker, "A Highly Robust Audio Fingerprinting System," ISMIR 2002, Oct. 2002, pp. 107-115. |
Also Published As
Publication number | Publication date |
---|---|
US20160093295A1 (en) | 2016-03-31 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11854534B1 (en) | Asynchronous optimization for sequence training of neural networks | |
US11093813B2 (en) | Answer to question neural networks | |
US20230410796A1 (en) | Encoder-decoder models for sequence to sequence mapping | |
US20210217408A1 (en) | Dialogue systems | |
US9741339B2 (en) | Data driven word pronunciation learning and scoring with crowd sourcing based on the word's phonemes pronunciation scores | |
US10535354B2 (en) | Individualized hotword detection models | |
US8959014B2 (en) | Training acoustic models using distributed computing techniques | |
US11675975B2 (en) | Word classification based on phonetic features | |
US20210081503A1 (en) | Utilizing a gated self-attention memory network model for predicting a candidate answer match to a query | |
US20160372118A1 (en) | Context-dependent modeling of phonemes | |
US11138965B2 (en) | Generating phonemes of loan words using two converters | |
US20200364576A1 (en) | Utilizing deep recurrent neural networks with layer-wise attention for punctuation restoration | |
Schumann et al. | Incorporating asr errors with attention-based, jointly trained rnn for intent detection and slot filling | |
KR20230156144A (en) | Supervised and unsupervised training with contrast loss across sequences | |
JP6391925B2 (en) | Spoken dialogue apparatus, method and program | |
US9424835B2 (en) | Statistical unit selection language models based on acoustic fingerprinting | |
US20220050971A1 (en) | System and Method for Generating Responses for Conversational Agents | |
KR101971696B1 (en) | Apparatus and method for creating optimum acoustic model |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:GUTKIN, ALEXANDER;FRUCTUOSO, JAVIER GONZALVO;ALLAUZEN, CYRIL GEORGES;SIGNING DATES FROM 20151005 TO 20151008;REEL/FRAME:036762/0720 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044566/0657Effective date: 20170929 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044129/0001Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |