JP2023534367A - Simultaneous acoustic event detection across multiple assistant devices - Google Patents
Simultaneous acoustic event detection across multiple assistant devices Download PDFInfo
- Publication number
- JP2023534367A JP2023534367A JP2022569600A JP2022569600A JP2023534367A JP 2023534367 A JP2023534367 A JP 2023534367A JP 2022569600 A JP2022569600 A JP 2022569600A JP 2022569600 A JP2022569600 A JP 2022569600A JP 2023534367 A JP2023534367 A JP 2023534367A
- Authority
- JP
- Japan
- Prior art keywords
- additional
- assistant
- audio data
- event
- assistant device
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000001514 detection method Methods 0.000 title claims abstract description 134
- 238000000034 method Methods 0.000 claims abstract description 94
- 230000004044 response Effects 0.000 claims abstract description 27
- 230000009471 action Effects 0.000 claims abstract description 24
- 238000012545 processing Methods 0.000 claims description 75
- 238000013481 data capture Methods 0.000 claims description 25
- 239000011521 glass Substances 0.000 claims description 22
- 238000003860 storage Methods 0.000 claims description 15
- 230000015654 memory Effects 0.000 claims description 12
- UGFAIRIUMAVXCW-UHFFFAOYSA-N Carbon monoxide Chemical compound [O+]#[C-] UGFAIRIUMAVXCW-UHFFFAOYSA-N 0.000 claims description 9
- 241000282326 Felis catus Species 0.000 claims description 9
- 229910002091 carbon monoxide Inorganic materials 0.000 claims description 8
- 206010011469 Crying Diseases 0.000 claims description 7
- 230000003213 activating effect Effects 0.000 claims description 6
- 230000008569 process Effects 0.000 abstract description 18
- 238000012549 training Methods 0.000 description 24
- 230000006870 function Effects 0.000 description 18
- 238000003058 natural language processing Methods 0.000 description 18
- 230000000694 effects Effects 0.000 description 15
- 238000010411 cooking Methods 0.000 description 11
- 230000002452 interceptive effect Effects 0.000 description 11
- 241000282472 Canis lupus familiaris Species 0.000 description 8
- 238000002372 labelling Methods 0.000 description 8
- 238000004891 communication Methods 0.000 description 7
- 230000003993 interaction Effects 0.000 description 6
- 230000026676 system process Effects 0.000 description 6
- 230000000007 visual effect Effects 0.000 description 6
- 230000008859 change Effects 0.000 description 5
- 241000282412 Homo Species 0.000 description 4
- 230000015572 biosynthetic process Effects 0.000 description 4
- 238000010801 machine learning Methods 0.000 description 4
- 238000003786 synthesis reaction Methods 0.000 description 4
- 238000013475 authorization Methods 0.000 description 3
- 230000008878 coupling Effects 0.000 description 3
- 238000010168 coupling process Methods 0.000 description 3
- 238000005859 coupling reaction Methods 0.000 description 3
- 235000013305 food Nutrition 0.000 description 3
- 238000004590 computer program Methods 0.000 description 2
- 238000010586 diagram Methods 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 230000002093 peripheral effect Effects 0.000 description 2
- 238000003825 pressing Methods 0.000 description 2
- 238000012546 transfer Methods 0.000 description 2
- 238000002604 ultrasonography Methods 0.000 description 2
- 241000473391 Archosargus rhomboidalis Species 0.000 description 1
- OKTJSMMVPCPJKN-UHFFFAOYSA-N Carbon Chemical compound [C] OKTJSMMVPCPJKN-UHFFFAOYSA-N 0.000 description 1
- 241000699670 Mus sp. Species 0.000 description 1
- 230000003190 augmentative effect Effects 0.000 description 1
- 229910052799 carbon Inorganic materials 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 239000003795 chemical substances by application Substances 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 239000003550 marker Substances 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000008520 organization Effects 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 230000002062 proliferating effect Effects 0.000 description 1
- 238000009877 rendering Methods 0.000 description 1
- 238000012031 short term test Methods 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
- 238000005406 washing Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/01—Assessment or evaluation of speech recognition systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01S—RADIO DIRECTION-FINDING; RADIO NAVIGATION; DETERMINING DISTANCE OR VELOCITY BY USE OF RADIO WAVES; LOCATING OR PRESENCE-DETECTING BY USE OF THE REFLECTION OR RERADIATION OF RADIO WAVES; ANALOGOUS ARRANGEMENTS USING OTHER WAVES
- G01S3/00—Direction-finders for determining the direction from which infrasonic, sonic, ultrasonic, or electromagnetic waves, or particle emission, not having a directional significance, are being received
- G01S3/80—Direction-finders for determining the direction from which infrasonic, sonic, ultrasonic, or electromagnetic waves, or particle emission, not having a directional significance, are being received using ultrasonic, sonic or infrasonic waves
- G01S3/8006—Multi-channel systems specially adapted for direction-finding, i.e. having a single aerial system capable of giving simultaneous indications of the directions of different signals
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/16—Sound input; Sound output
- G06F3/167—Audio in a user interface, e.g. using voice commands for navigating, audio feedback
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/30—Distributed recognition, e.g. in client-server systems, for mobile phones or network applications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/32—Multiple recognisers used in sequence or in parallel; Score combination systems therefor, e.g. voting systems
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04R—LOUDSPEAKERS, MICROPHONES, GRAMOPHONE PICK-UPS OR LIKE ACOUSTIC ELECTROMECHANICAL TRANSDUCERS; DEAF-AID SETS; PUBLIC ADDRESS SYSTEMS
- H04R29/00—Monitoring arrangements; Testing arrangements
- H04R29/004—Monitoring arrangements; Testing arrangements for microphones
- H04R29/005—Microphone arrays
- H04R29/006—Microphone matching
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L2015/088—Word spotting
Abstract
実装形態は、複数のアシスタントデバイスを含むエコシステムの中の多様なアシスタントデバイスにおいて音響イベントを捉えるそれぞれのオーディオデータを検出し、多様なアシスタントデバイスの各々においてローカルにそれぞれのオーディオデータを処理して、それぞれのイベント検出モデルを使用して音響イベントに関連するそれぞれの尺度を生成し、それぞれの尺度を処理して、検出された音響イベントが実際の音響イベントであるかどうかを決定し、検出された音響イベントが実際の音響イベントであると決定したことに応答して、実際の音響イベントに関連する行動が実行されるようにすることができる。いくつかの実装形態では、それぞれのオーディオデータを検出した多様なアシスタントデバイスは、過去の複数の音響イベントが多様なアシスタントデバイスの各々において検出されていることに基づいて、実際の音響イベントを捉えるそれぞれのオーディオデータを検出することが予想される。Implementations detect respective audio data capturing acoustic events at various assistant devices in an ecosystem that includes multiple assistant devices, process respective audio data locally at each of the various assistant devices, Each event detection model is used to generate respective measures associated with the acoustic event, each measure is processed to determine whether the detected acoustic event is an actual acoustic event, and Actions associated with the actual sound event may be performed in response to determining that the sound event is the actual sound event. In some implementations, the various assistant devices that detected the respective audio data each capture the actual acoustic event based on past multiple acoustic events being detected at each of the various assistant devices. of audio data is expected to be detected.
Description
人は、「自動化アシスタント」(「チャットボット」、「双方向型パーソナルアシスタント」、「インテリジェントパーソナルアシスタント」、「パーソナル音声アシスタント」、「会話エージェント」などとも呼ばれる)と本明細書で呼ばれる双方向型ソフトウェアアプリケーションとの、人対コンピュータの対話に関わることができる。たとえば、人(自動化アシスタントと対話しているときは「ユーザ」と呼ばれ得る)は、入力(たとえば、コマンド、クエリ、および/または要求)を自動化アシスタントに与えてもよく、これは、自動化アシスタントに、応答出力を生成および提供させることができ、1つまたは複数のInternet of things(IoT)デバイスを制御させることができ、かつ/または1つまたは複数の他の機能を実行させることができる。ユーザによって与えられる入力は、たとえば、場合によってはテキスト(または他のセマンティック表現)に変換されて次いでさらに処理され得る話される自然言語入力(すなわち、発話)であってもよく、および/またはタイプされた自然言語入力であってもよい。 Humans are interactive assistants, referred to herein as "automated assistants" (also called "chatbots," "interactive personal assistants," "intelligent personal assistants," "personal voice assistants," "conversation agents," etc.). Can engage in human-to-computer interactions with software applications. For example, a person (which may be referred to as a "user" when interacting with the automation assistant) may provide input (eg, commands, queries, and/or requests) to the automation assistant, which is called the automation assistant can be caused to generate and provide responsive output, can be caused to control one or more Internet of things (IoT) devices, and/or can be caused to perform one or more other functions. The input provided by the user may, for example, be spoken natural language input (i.e., speech), which may optionally be converted to text (or other semantic representation) and then further processed, and/or typed It may be natural language input that has been processed.
いくつかの場合、自動化アシスタントは、アシスタントデバイスによってローカルで実行されユーザが直接関わり合う自動化アシスタントクライアント、ならびに、自動化アシスタントがユーザの入力に応答するのを助けるために実質的に無限のクラウドのリソースを活用するクラウドベースの対応物を含み得る。たとえば、自動化アシスタントは、クラウドベースの対応物に、ユーザの発話のオーディオ記録(またはそのテキスト変換)、および任意選択でユーザの識別情報(たとえば、証明書)を示すデータを提供することができる。クラウドベースの対応物は、クエリに対して様々な処理を実行して結果を自動化アシスタントクライアントに返してもよく、自動化アシスタントクライアントは次いで、対応する出力をユーザに提供してもよい。 In some cases, the automated assistant utilizes an automated assistant client that runs locally on the assistant device and that the user directly engages with, as well as virtually limitless cloud resources to help the automated assistant respond to user input. May include cloud-based counterparts to leverage. For example, an automated assistant can provide its cloud-based counterpart with an audio recording of the user's utterance (or a text translation thereof), and optionally data indicating the user's identity (eg, a certificate). The cloud-based counterpart may perform various operations on the query and return the results to the automated assistant client, which may then provide corresponding output to the user.
多くのユーザが、複数のアシスタントデバイスを使用して自動化アシスタントと関わり得る。たとえば、一部のユーザは、自動化アシスタントに向けられたユーザ入力を受信することができる、かつ/または、アシスタントデバイスの中でもとりわけ、1つまたは複数のスマートフォン、1つまたは複数のタブレットコンピュータ、1つまたは複数の車両コンピューティングシステム、1つまたは複数のウェアラブルコンピューティングデバイス、1つまたは複数のスマートテレビジョン、1つまたは複数の双方向型スタンドアロンスピーカー、および/または1つまたは複数のIoTデバイスなどの自動化アシスタントによって制御され得る、アシスタントデバイスの協調した「エコシステム」を保有し得る。ユーザは、これらのアシスタントデバイスのいずれかを使用して(自動化アシスタントクライアントがインストールされており、アシスタントデバイスが入力を受け取ることが可能であると仮定して)、自動化アシスタントとの人とコンピュータの対話に関わり得る。いくつかの場合、これらのアシスタントデバイスは、ユーザの本邸、別邸、職場、および/または他の構造物の周りに散らばっていることがある。たとえば、スマートフォン、タブレット、スマートウォッチなどのモバイルアシスタントデバイスは、ユーザが身につけていることがあり、および/またはユーザが最後にそれらを置いたところにあることがある。従来のデスクトップコンピュータ、スマートテレビジョン、双方向型スタンドアロンスピーカー、およびIoTデバイスなどの他のアシスタントデバイスは、より据え置き型であり得るが、それでも、ユーザの自宅または職場の様々な場所(たとえば、部屋)に位置していることがある。 Many users may engage with automated assistants using multiple assistant devices. For example, some users may receive user input directed to automated assistants and/or one or more smart phones, one or more tablet computers, one or more tablet computers, among other assistant devices. or multiple vehicle computing systems, one or more wearable computing devices, one or more smart televisions, one or more interactive stand-alone speakers, and/or one or more IoT devices. It may possess a coordinated "ecosystem" of assistant devices that may be controlled by an automated assistant. Using one of these assistant devices (assuming the Automation Assistant client is installed and the assistant device is capable of receiving input), the user can interact with the automation assistant in a human-computer manner. can be involved in In some cases, these assistant devices may be scattered around the user's main residence, villa, workplace, and/or other structures. For example, mobile assistant devices such as smartphones, tablets, and smartwatches may be on the user and/or where the user last left them. Other assistant devices, such as traditional desktop computers, smart televisions, interactive stand-alone speakers, and IoT devices, may be more stationary, but are still used in various locations (e.g., rooms) of a user's home or workplace. may be located in
アシスタントデバイスのエコシステムの中の所与のアシスタントデバイスにおいて音響イベントが検出されるときに、ユーザ(たとえば、単一のユーザ、家族の中の複数のユーザ、同僚、同居人など)に警告するための技法が存在する。しかしながら、そのような技法は、フォールスポジティブイベントを検出することにつながり得る。言い換えると、所与のアシスタントデバイスは、実際には音響イベントが本当は発生しなかったときに、音響イベントが発生したと決定することがある。結果として、所与のアシスタントデバイスの自動化アシスタントクライアントは、自動化アシスタントクライアントがどのような行動も実行するべきではなかったときに、1つまたは複数の行動が実行されるようにすることがある。 To alert a user (e.g., a single user, multiple users in a family, coworkers, cohabitants, etc.) when an acoustic event is detected on a given assistant device in an ecosystem of assistant devices techniques exist. However, such techniques can lead to detection of false positive events. In other words, a given assistant device may determine that a sound event has occurred when in fact the sound event did not actually occur. As a result, the automated assistant client of a given assistant device may cause one or more actions to be performed when the automated assistant client should not have performed any actions.
本明細書において説明される実装形態は、複数のアシスタントデバイスを含むエコシステムの中の多様なアシスタントデバイスにおいて音響イベントを捉えるオーディオデータを同時に検出することに関する。音響イベントを捉える多様なアシスタントデバイスの各々において検出されるそれぞれのオーディオデータは、イベント検出モデルを使用して、音響イベントに関連するそれぞれの尺度を生成するために処理され得る。それぞれの尺度の各々は、音響イベントが実際の音響イベントであるかどうかを決定するために処理され得る。音響イベントが実際の音響イベントであると決定したことに応答して、音響イベントに関連する行動が実行され得る。それぞれのオーディオデータは、イベント検出モデルを使用してアシスタントデバイスにおいてローカルで処理され、イベント検出モデルを使用して遠隔システムにおいて遠隔で処理され、かつ/またはイベント検出モデルを使用してエコシステムの中の所与のアシスタントデバイス(たとえば、音響イベントを捉えるオーディオデータを検出した可能性のある、またはしなかった可能性のある)において処理され得る。 Implementations described herein relate to simultaneously detecting audio data capturing acoustic events at various assistant devices in an ecosystem that includes multiple assistant devices. Using an event detection model, the respective audio data detected at each of the various assistant devices that capture the acoustic event can be processed to generate respective measures associated with the acoustic event. Each of the respective measures can be processed to determine whether the acoustic event is an actual acoustic event. Actions associated with the acoustic event may be performed in response to determining that the acoustic event is an actual acoustic event. The respective audio data is processed locally on the assistant device using the event detection model, processed remotely on the remote system using the event detection model, and/or in the ecosystem using the event detection model. given assistant device (eg, which may or may not have detected audio data that captures the acoustic event).
いくつかの実装形態では、音響イベントはホットワードイベントに対応し得る。これらの実装形態では、音響イベント検出モデルは、検出されると自動化アシスタントの1つまたは複数のコンポーネントおよび/または機能がエコシステムの中のアシスタントデバイスのうちの1つまたは複数においてアクティブにされるようにする特定の語または語句を検出するように訓練される、ホットワード検出モデルに対応することができる。たとえば、マイクロフォンを有する所与のアシスタントデバイスは、エコシステムに関連するユーザの本邸に位置すると仮定する。所与のアシスタントデバイスは、音響イベントを捉えるマイクロフォンを介してオーディオデータを検出するとさらに仮定し、オーディオデータは、オーディオデータが特定の語または語句を含むかどうかを示す確率を生成するためにホットワード検出モデルを使用して処理されるとさらに仮定する。その上、追加のマイクロフォンを有する少なくとも1つの追加のアシスタントデバイスは、エコシステムの中で所与のアシスタントデバイスと位置的に近いと仮定する。少なくとも1つの追加のアシスタントデバイスは、同様に音響イベントを捉える追加のマイクロフォンを介して追加のオーディオデータを検出するとさらに仮定し、追加のオーディオデータは、追加のオーディオデータが特定の語または語句を含むかどうかを同様に示す追加の確率を生成するためにホットワード検出モデルを使用して処理されるとさらに仮定する。この例では、この確率と追加の確率は、音響イベントが実際にその特定の語または語句の出現に対応するかどうかを決定するために処理され得る。 In some implementations, acoustic events may correspond to hotword events. In these implementations, the acoustic event detection model is such that when detected, one or more components and/or functions of the automated assistant are activated on one or more of the assistant devices in the ecosystem. A hot word detection model can be supported that is trained to detect specific words or phrases that For example, assume that a given assistant device with a microphone is located at the main residence of the user associated with the ecosystem. It is further assumed that a given assistant device detects audio data via a microphone that picks up acoustic events, and that the audio data is analyzed by hotwords to generate probabilities indicating whether the audio data contains a particular word or phrase. Further assume that it is processed using the detection model. Moreover, at least one additional assistant device with an additional microphone is assumed to be positionally close to the given assistant device in the ecosystem. It is further assumed that at least one additional assistant device detects the additional audio data via an additional microphone that likewise captures the acoustic event, the additional audio data including the particular word or phrase. Suppose further that it is processed using a hotword detection model to generate additional probabilities that similarly indicate whether or not. In this example, this probability and additional probabilities can be processed to determine whether the acoustic event actually corresponds to the occurrence of that particular word or phrase.
たとえば、確率は0.70という値であり、追加の確率は0.65という値であると仮定する。この事例では、確率と追加の確率の両方が閾値(たとえば、0.60という値)を満たす場合、ホットワードイベントを実際のホットワードイベントであるものとして検証することができ、自動化アシスタントの1つまたは複数のコンポーネントおよび/または機能を、確率のいずれもがあまり高くなくてもアクティブにすることができる。また、たとえば、確率は0.90という値であり、追加の確率は0.55という値であると仮定する。この事例では、追加の確率は閾値を満たさない可能性があるが、示される確率が比較的高い(および任意選択で追加の確率が閾値の範囲内(たとえば、0.10という値以内)にある)ので、それでもホットワードイベントを実際のホットワードイベントとして検証してもよく、自動化アシスタントの1つまたは複数のコンポーネントおよび/または機能をアクティブにすることができる。 For example, suppose the probability has a value of 0.70 and the extra probability has a value of 0.65. In this case, if both the probability and the additional probability meet a threshold (e.g. a value of 0.60), the hotword event can be verified as being an actual hotword event and one or more of the automation assistants components and/or functions of can be activated without any of the probabilities being very high. Also assume, for example, that the probability has a value of 0.90 and the additional probability has a value of 0.55. In this case, the additional probabilities may not meet the threshold, but since the indicated probabilities are relatively high (and optionally the additional probabilities are within the threshold (e.g., within a value of 0.10)) , the hotword event may still be verified as an actual hotword event, and may activate one or more components and/or functions of the Automation Assistant.
いくつかの追加または代替の実装形態では、音響イベントは音イベントに対応することができる。これらの実装形態では、音響イベント検出モデルは、検出されると、エコシステムに関連するユーザのクライアントデバイスにおいて通知が視覚的および/または聴覚的にレンダリングされるようにする1つまたは複数の特定の音を検出するように訓練される、音検出モデルに対応することができる。特定の音は、たとえば、ガラスが割れること、犬が吠えること、猫が鳴くこと、呼び鈴が鳴ること、火災報知器が鳴動すること、一酸化炭素検出器が鳴動すること、乳児が泣くこと、ドアをノックすること、および/またはエコシステムにおいて発生し得る任意の他の音を含み得る。それらの実装形態のいくつかのバージョンでは、それぞれの特定の音を検出するために多様な音検出モデルを訓練することができ、一方、他の実装形態では、多様な音を検出するために単一の音検出モデルを訓練することができる。オーディオデータは、ホットワードイベントに関して上で説明されるのと同じまたは同様の方式で処理され得るが、追加または代替として、音イベントが実際の音イベントであるかどうかを検証するために音検出モデルを使用してオーディオデータを処理することを含み得る。 In some additional or alternative implementations, acoustic events may correspond to sound events. In these implementations, the acoustic event detection model is one or more specific events that, when detected, cause notifications to be visually and/or audibly rendered on a user's client device associated with the ecosystem. It can correspond to a sound detection model that is trained to detect sounds. Specific sounds include, for example, breaking glass, barking dogs, meowing cats, ringing doorbells, ringing fire alarms, ringing carbon monoxide detectors, crying babies, It may include knocking on a door and/or any other sound that may occur in the ecosystem. In some versions of those implementations, different sound detection models can be trained to detect each specific sound, while in other implementations, a single model can be trained to detect different sounds. A single sound detection model can be trained. Audio data may be processed in the same or similar manner as described above with respect to hotword events, but additionally or alternatively, using a sound detection model to verify whether a sound event is an actual sound event. may include processing the audio data using .
様々な実装形態において、エコシステムの中の少なくとも1つの追加のアシスタントデバイスは、音響イベントを捉えるオーディオデータを所与のアシスタントデバイスが検出したことに応答して特定され得る。所与のアシスタントデバイスにおいて検出される音響イベントが実際に本物の音響イベントである場合、エコシステムにおいて特定される少なくとも1つの追加のアシスタントデバイスは、音響イベントを同様に捉える時間的に対応するオーディオデータを検出したはずである。アシスタントデバイスによって捉えられるオーディオデータは、たとえば、アシスタントデバイスにおいて捉えられるオーディオデータに関連するそれぞれのタイムスタンプに基づいて、時間的に対応するオーディオデータであると見なされ得る。たとえば、オーディオデータは、タイムスタンプが一致するとき、またはタイムスタンプが互いに閾値の時間長以内にある(たとえば、数ミリ秒、数秒、または任意の他の適切な長さの時間内にある)とき、時間的に対応するオーディオデータであると見なされ得る。それらの実装形態のいくつかのバージョンでは、少なくとも1つの追加のアシスタントデバイスは、たとえば、所与のアシスタントデバイスおよび少なくとも1つの追加のアシスタントデバイスが同じ音響イベントを捉えるオーディオデータを過去に検出していることに基づいて特定され得る。それらの実装形態のいくつかの追加または代替のバージョンでは、少なくとも1つの追加のアシスタントデバイスは、たとえば、所与のアシスタントデバイスおよび少なくとも1つの追加のアシスタントデバイスがエコシステムのデバイストポロジー表現においてアシスタントデバイスの同じグループに属していることに基づいて特定され得る。 In various implementations, at least one additional assistant device in the ecosystem may be identified in response to a given assistant device detecting audio data capturing the acoustic event. If an acoustic event detected in a given assistant device is in fact a genuine acoustic event, at least one additional assistant device identified in the ecosystem will generate temporally corresponding audio data that similarly captures the acoustic event. should have detected The audio data captured by the assistant device can be considered to be temporally corresponding audio data, eg, based on respective timestamps associated with the audio data captured at the assistant device. For example, the audio data is processed when the timestamps match, or when the timestamps are within a threshold length of time of each other (e.g., within a few milliseconds, seconds, or any other suitable length of time). , can be considered to be the temporally corresponding audio data. In some versions of those implementations, the at least one additional assistant device has, for example, previously detected audio data in which the given assistant device and the at least one additional assistant device capture the same acoustic event. can be identified on the basis of In some additional or alternative versions of those implementations, the at least one additional assistant device is, for example, a given assistant device and the at least one additional assistant device is the assistant device in the device topology representation of the ecosystem. They can be identified based on belonging to the same group.
本明細書において説明される技法を使用して、エコシステムの中の多様なアシスタントデバイスを使用して音響イベントの発生を検出して検証することによって、音響イベントのフォールスポジティブの量を減らすことができる。結果として、計算リソースとネットワークリソースの両方を節約することができる。たとえば、エコシステムにおいてホットワードイベントの発生を検出して検証する際に本明細書において説明される技法を使用することによって、自動化アシスタントのコンポーネントおよび/または機能を、それらがアクティブにされることが意図されていなかったとしてもこれらの技法がなければアクティブにされていた可能性があるときに、休止状態のままにすることができる。別の例として、エコシステムの中の特定の音の発生を検出して検証する際に本明細書において説明される技法を使用することによって、エコシステムに関連するユーザに提示される通知を、実際には音響イベントが発生しなかったにもかかわらずこれらの技法がなければ提示された可能性があるときに、差し控えることができる。その上、本明細書において説明される技法を使用して、エコシステムの中のどのアシスタントデバイスが、音響イベントを捉える時間的に対応するオーディオデータを検出するはずであるかを予想することによって、オーディオデータを処理するアシスタントデバイスの量を減らすことができる。また結果として、計算リソースとネットワークリソースの両方を節約することができる。たとえば、音響イベントを検出したはずであるエコシステムの中のアシスタントデバイスを予想する際に本明細書において説明される技法を使用することによって、音響イベントに対応しない可能性のある時間的に対応するオーディオデータを捉えた可能性のある他のアシスタントデバイスが、音響イベントが実際の音響イベントであるかどうかを決定する際に考慮されなくてもよい。 Using the techniques described herein, the amount of false positives in acoustic events can be reduced by detecting and verifying the occurrence of acoustic events using various assistant devices in the ecosystem. can. As a result, both computational and network resources can be saved. For example, by using the techniques described herein in detecting and verifying the occurrence of hotword events in the ecosystem, automation assistant components and/or functions can It can remain dormant when it might have been activated without these techniques, even though it was not intended. As another example, by using the techniques described herein in detecting and verifying the occurrence of particular sounds in the ecosystem, notifications presented to users associated with the ecosystem can be: It can be withheld when no acoustic event actually occurred but could have been presented without these techniques. Moreover, using the techniques described herein, by predicting which assistant devices in the ecosystem should detect the temporally corresponding audio data that captures the acoustic event, It can reduce the amount of assistant devices that process audio data. Also, as a result, both computational and network resources can be saved. For example, by using the techniques described herein in anticipating an assistant device in the ecosystem that would have detected an acoustic event, we can Other assistant devices that may have captured the audio data may not be considered in determining whether the sound event is an actual sound event.
上の説明は、本開示のいくつかの実装形態のみの概要として与えられる。それらの実装形態および他の実装形態のさらなる説明は、本明細書においてより詳しく説明される。1つの限定しない例として、様々な実装形態が、本明細書に含まれる特許請求の範囲においてより詳しく説明される。 The above description is given as a summary of only some implementations of the disclosure. Further description of those and other implementations are described in greater detail herein. As one non-limiting example, various implementations are described in more detail in the claims included herein.
加えて、いくつかの実装形態は、1つまたは複数のコンピューティングデバイスの1つまたは複数のプロセッサを含み、1つまたは複数のプロセッサは関連するメモリに記憶されている命令を実行するように動作可能であり、命令は本明細書において説明される方法のいずれかの実行を引き起こすように構成される。いくつかの実装形態はまた、本明細書において説明される方法のいずれかを実行するように1つまたは複数のプロセッサによって実行可能なコンピュータ命令を記憶する1つまたは複数の非一時的コンピュータ可読記憶媒体を含む。 In addition, some implementations include one or more processors of one or more computing devices, the one or more processors operating to execute instructions stored in associated memory. possible, the instructions are configured to cause execution of any of the methods described herein. Some implementations also include one or more non-transitory computer-readable storage storing computer instructions executable by one or more processors to perform any of the methods described herein. Including media.
前述の概念および本明細書においてより詳しく説明される追加の概念のすべての組合せが、本明細書において開示される主題の一部であると考えられることを理解されたい。たとえば、本開示の終わりに現れる特許請求される主題のすべての組合せが、本明細書において開示される主題の一部であると考えられる。 It should be understood that all combinations of the above concepts and the additional concepts described in more detail herein are considered to be part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are considered part of the subject matter disclosed herein.
スマートフォン、タブレットコンピュータ、車両コンピューティングシステム、ウェアラブルコンピューティングデバイス、スマートテレビジョン、双方向型スタンドアロンスピーカー(たとえば、ディスプレイのある、またはない)、サウンドスピーカー、家庭用警報器、扉の錠、カメラ、照明システム、トレッドミル、サーモスタット、体重計、スマートベッド、散水システム、車庫開扉器、家電機器、ベビーモニタ、火災報知器、水分計などの、スマートマルチセンシングネットワーク接続デバイス(アシスタントデバイスとも本明細書では呼ばれる)が急増している。しばしば、複数のアシスタントデバイスが、自宅などの構造物の境界内に位置し、または、ユーザの本邸およびユーザの別邸、ユーザの車両、ならびに/もしくはユーザの職場などの、多様な関連する構造内に位置する。 Smart phones, tablet computers, vehicle computing systems, wearable computing devices, smart televisions, interactive stand-alone speakers (e.g. with or without displays), sound speakers, home alarms, door locks, cameras, lights systems, treadmills, thermostats, scales, smart beds, watering systems, garage door openers, home appliances, baby monitors, fire alarms, moisture gauges, etc. ) are rapidly increasing. Often, multiple assistant devices are located within the boundaries of a structure such as a home, or within various related structures such as the user's main residence and user's second residence, the user's vehicle, and/or the user's workplace. To position.
さらに、自動化アシスタントの論理インスタンスを形成できる自動化アシスタントクライアントを各々含むアシスタントデバイス(アシスタント入力デバイスとも本明細書で呼ばれる)が急増している。これらのアシスタント入力デバイスは、アシスタント機能に専用であってもよく(たとえば、アシスタントクライアントおよび関連するインターフェースのみを含み、アシスタント機能に専用の、双方向型スタンドアロンスピーカーおよび/またはスタンドアロンオーディオ/ビジュアルデバイス)、または、他の機能に加えてアシスタント機能を実行することができる(たとえば、多様なアプリケーションの1つとしてアシスタントクライアントを含む携帯電話またはタブレット)。その上、一部のIoTデバイスもアシスタント入力デバイスであり得る。たとえば、一部のIoTデバイスは、自動化アシスタントクライアントと、自動化アシスタントクライアントのアシスタントインターフェースのためのユーザインターフェース出力および/または入力デバイスとして(少なくとも一部)役割を果たす少なくともスピーカーおよび/またはマイクロフォンとを含み得る。一部のアシスタントデバイスは、自動化アシスタントクライアントを実装せず、または、ユーザとインターフェースするための手段(たとえば、スピーカーおよび/またはマイクロフォン)を有しないことがあるが、それらはそれでも、自動化アシスタントによって制御され得る(本明細書ではアシスタント非入力デバイスとも呼ばれる)。たとえば、スマート電球は、自動化アシスタントクライアント、スピーカー、および/またはマイクロフォンを含まないことがあるが、スマート照明の機能を制御する(たとえば、照明をつける/消す、減光する、色を変更するなど)ために、自動化アシスタントを介して、コマンドおよび/または要求をスマート電球に送信することができる。 Additionally, assistant devices (also referred to herein as assistant input devices) are proliferating, each containing an automated assistant client capable of forming a logical instance of an automated assistant. These assistant input devices may be dedicated to assistant functionality (e.g., interactive stand-alone speakers and/or stand-alone audio/visual devices containing only assistant clients and associated interfaces and dedicated to assistant functionality); Alternatively, it can perform assistant functions in addition to other functions (eg, a mobile phone or tablet that includes an assistant client as one of its many applications). Besides, some IoT devices can also be assistant input devices. For example, some IoT devices may include an automated assistant client and at least a speaker and/or microphone that serves (at least in part) as a user interface output and/or input device for an assistant interface of the automated assistant client. . Some assistant devices may not implement an Automated Assistant client or have means (e.g., speakers and/or microphones) for interfacing with the user, but they are still controlled by the Automated Assistant. (also referred to herein as an assistant non-input device). For example, a smart light bulb may not include an automated assistant client, speaker, and/or microphone, but it controls the functionality of smart lighting (e.g., turn light on/off, dim, change color, etc.) To do so, commands and/or requests can be sent to the smart bulbs via an automation assistant.
アシスタントデバイスのエコシステムにアシスタントデバイス(アシスタント入力デバイスとアシスタント非入力デバイスの両方を含む)を追加するための、および/または、エコシステム内のアシスタントデバイスをグループ化するための、様々な技法が提案されている。たとえば、新しいアシスタントデバイスをエコシステムに追加すると、エコシステムに関連するユーザは、エコシステムのデバイストポロジー表現において、ソフトウェアアプリケーションを介して(たとえば、自動化アシスタントアプリケーション、エコシステムに関連するソフトウェアアプリケーション、新しいアシスタントデバイスに関連するソフトウェアアプリケーションなどを介して)エコシステムの中のアシスタントデバイスのグループに新しいアシスタントデバイスを手動で追加することができる。さらに、アシスタントデバイスがエコシステム内で移動する場合、ユーザは、ソフトウェアアプリケーションを介してアシスタントデバイスが割り当てられるグループを手動で変更し得る。そうしなければ、アシスタントデバイスが割り当てられるグループは、エコシステム内のアシスタントデバイスの位置を正確に反映しないことがある。たとえば、「居間のスピーカー」と標識されたスマートスピーカーがユーザの本邸の居間に位置し、アシスタントデバイスの「居間」グループと関連付けられるが、スマートスピーカーがユーザの本邸の台所に移動する場合、標識とグループがアシスタントデバイスの位置を表していなくても、ユーザがユーザの本邸のエコシステムのためのデバイストポロジー表現において標識およびグループを手動で変更しない限り、スマートスピーカーは依然として「居間のスピーカー」として標識され、アシスタントデバイスの「居間」グループに含まれ得る。 Various techniques have been proposed for adding assistant devices (including both assistant input devices and assistant non-input devices) to the assistant device ecosystem and/or for grouping assistant devices within the ecosystem. It is For example, when a new assistant device is added to the ecosystem, users associated with the ecosystem will be able to access the device topology representation of the ecosystem through software applications (e.g., automation assistant applications, ecosystem-related software applications, new assistant A new assistant device can be manually added to the group of assistant devices in the ecosystem (such as via a software application associated with the device). Additionally, if the assistant device moves within the ecosystem, the user may manually change the group to which the assistant device is assigned via the software application. Otherwise, the group to which the assistant device is assigned may not accurately reflect the assistant device's position within the ecosystem. For example, if a smart speaker labeled "living room speaker" is located in the living room of the user's main residence and is associated with the "living room" group of assistant devices, but the smart speaker moves to the kitchen of the user's main residence, the label and Even if the group does not represent the location of the assistant device, the smart speaker is still labeled as "living room speaker" unless the user manually changes the label and group in the device topology representation for the user's home ecosystem. , may be included in the “living room” group of assistant devices.
デバイストポロジー表現は、それぞれのアシスタントデバイスと関連付けられる標識(または固有の識別子)を含み得る。さらに、デバイストポロジー表現は、それぞれのアシスタントデバイスと関連付けられる標識(または固有の識別子)を指定することができる。所与のアシスタントデバイスに対するデバイス属性は、たとえば、それぞれのアシスタントデバイスによってサポートされる1つまたは複数の入力および/または出力様式を示すことができる。たとえば、スタンドアロンスピーカーのみのアシスタントクライアントデバイスに対するデバイス属性は、それが聴覚的な出力を提供することは可能であるが視覚的な出力を提供することは不可能であることを示すことができる。所与のアシスタントデバイスに対するデバイス属性は、追加または代替として、たとえば、所与のアシスタントデバイスの制御できる1つもしくは複数の状態を特定し、アシスタントデバイスのファームウェアを製造し、配布し、かつ/もしくは作成する当事者(たとえば、ファーストパーティ(1P)またはサードパーティ(3P))を特定し、かつ/または、1Pもしくは3Pにより提供される不変の識別子などの所与のアシスタントデバイスに対する固有の識別子、もしくはユーザによって所与のアシスタントデバイスに割り当てられる標識を特定することができる。本明細書において開示される様々な実装形態によれば、デバイストポロジー表現は任意選択でさらに、どのスマートデバイスがどのアシスタントデバイスによってローカルで制御され得るか、ローカルに制御可能なアシスタントデバイスのローカルアドレス(またはそれらのアシスタントデバイスを直接ローカルに制御することができるハブのローカルアドレス)、ローカル信号強度、および/またはそれぞれのアシスタントデバイスの間の他の優先インジケータを指定することができる。さらに、本明細書において開示される様々な実装形態によれば、デバイストポロジー表現(またはその変形)は、アシスタントデバイスをローカルで制御する際に、および/またはそれに標識をローカルで割り当てる際に利用するために、複数のアシスタントデバイスの各々にローカルで記憶され得る。その上、デバイストポロジー表現は、様々なレベルの粒度で定義され得るそれぞれのアシスタントデバイスと関連付けられるグループを指定することができる。たとえば、ユーザの本邸の居間の複数のスマート照明が、「居間の照明」グループに属すると考えられ得る。さらに、本邸の居間がスマートスピーカーも含む場合、居間に位置するアシスタントデバイスのすべてが、「居間のアシスタントデバイス」グループに属すると考えられ得る。 A device topology representation may include an indicator (or unique identifier) associated with each assistant device. Additionally, the device topology representation can specify an indicator (or unique identifier) associated with each assistant device. Device attributes for a given assistant device may indicate, for example, one or more input and/or output modalities supported by the respective assistant device. For example, a device attribute for a standalone speaker-only assistant client device may indicate that it is capable of providing audible output but not visual output. Device attributes for a given assistant device may additionally or alternatively, for example, identify one or more controllable states of the given assistant device, manufacture, distribute, and/or create firmware for the assistant device. and/or a unique identifier for a given assistant device, such as a permanent identifier provided by a 1P or 3P, or by the user A label assigned to a given assistant device can be specified. According to various implementations disclosed herein, the device topology representation optionally further includes which smart devices can be locally controlled by which assistant devices, local addresses of locally controllable assistant devices ( (or the local address of a hub that can directly control those assistant devices locally), local signal strength, and/or other priority indicators between each assistant device. Further, according to various implementations disclosed herein, the device topology representation (or variations thereof) is utilized in locally controlling the assistant device and/or in assigning labels to it locally. For this purpose, it can be stored locally on each of the plurality of assistant devices. Moreover, the device topology representation can specify groups associated with each assistant device that can be defined at various levels of granularity. For example, smart lights in the living room of the user's main residence may be considered to belong to the "living room lights" group. Further, if the living room of the main residence also contains a smart speaker, all the assistant devices located in the living room can be considered to belong to the "living room assistant device" group.
自動化アシスタントは、アシスタントデバイスの1つまたは複数によって生成される1つまたは複数の信号に基づいて、エコシステムにおいて発生する様々なイベントを検出することができる。たとえば、自動化アシスタントは、イベント検出モデルまたはイベント検出ルールを使用して、信号の1つまたは複数を処理してこれらのイベントを検出することができる。さらに、自動化アシスタントは、エコシステムにおいて発生するイベントのための信号の1つまたは複数に基づいて生成される出力に基づいて1つまたは複数の行動が実行されるようにすることができる。いくつかの実装形態では、検出されるイベントは、1つまたは複数のアシスタントデバイスのそれぞれのマイクロフォンを介して捉えられる音響イベントであり得る。自動化アシスタントは、音響イベントを捉えるオーディオデータが音響イベントモデルを使用して処理されるようにすることができる。音響イベントモデルによって検出される音響イベントは、たとえば、ホットワード検出モデルを使用して発話に含まれる自動化アシスタントを呼び出すホットワードを検出すること、周辺雑音検出モデルを使用してエコシステムの中で(および任意選択で、アシスタントデバイスの所与の1つにおいて発話受け入れがアクティブである間に)周辺雑音を検出すること、音検出モデルを使用してエコシステムにおいて特定の音(たとえば、ガラスが割れること、犬が吠えること、猫が鳴くこと、呼び鈴が鳴ること、火災報知器が鳴動すること、一酸化炭素検出器が鳴動すること、乳児が泣くこと、ドアをノックすること、および/または他の音響イベント)を検出すること、および/または、それぞれの音響イベント検出モデルを使用して検出され得る他の音響関連イベントを含み得る。たとえば、オーディオデータがアシスタントデバイスの少なくとも1つのそれぞれのマイクロフォンを介して検出されると仮定する。この例では、自動化アシスタントは、オーディオデータが自動化アシスタントを呼び出すためのホットワードを捉えるかどうかを決定するために、アシスタントデバイスの少なくとも1つのホットワード検出モデルによってオーディオデータが処理されるようにすることができる。さらに、自動化アシスタントは、追加または代替として、周辺雑音の1つまたは複数の異種のセマンティックカテゴリ(たとえば、映画もしくはテレビの音、料理の音、および/または他の異種の音のカテゴリ)へとオーディオデータにおいて捉えられたあらゆる周辺(または背景)雑音を分類するために、アシスタントデバイスの少なくとも1つの周辺雑音検出モデルによってオーディオデータが処理されるようにすることができる。その上、自動化アシスタントは、追加または代替として、何らかの特定の音がオーディオデータにおいて捉えられるかどうかを決定するために、アシスタントデバイスの少なくとも1つの音検出モデルによってオーディオデータが処理されるようにすることができる。 The automated assistant can detect various events occurring in the ecosystem based on one or more signals generated by one or more of the assistant devices. For example, an automated assistant can use event detection models or event detection rules to process one or more of the signals to detect these events. Further, the automated assistant can cause one or more actions to be performed based on outputs generated based on one or more of the signals for events occurring in the ecosystem. In some implementations, the detected events may be acoustic events captured via respective microphones of one or more assistant devices. The automated assistant can cause audio data capturing acoustic events to be processed using acoustic event models. Acoustic events detected by the acoustic event model can be used, for example, to detect hotwords that invoke automated assistants in utterances using the hotword detection model, or in the ecosystem using the ambient noise detection model ( and optionally detecting ambient noise (while speech acceptance is active in a given one of the assistant devices), using sound detection models to detect specific sounds (e.g. breaking glass) in the ecosystem. , dogs barking, cats meowing, doorbells ringing, fire alarms ringing, carbon monoxide detectors ringing, babies crying, door knocking, and/or other acoustic events) and/or other acoustic-related events that may be detected using the respective acoustic event detection model. For example, assume that audio data is detected via at least one respective microphone of the assistant device. In this example, the Automation Assistant ensures that the audio data is processed by at least one hotword detection model on the assistant device to determine whether the audio data captures a hotword for invoking the Automation Assistant. can be done. Furthermore, the automated assistant additionally or alternatively classifies the audio into one or more disparate semantic categories of ambient noise (e.g., movie or television sounds, cooking sounds, and/or other disparate sound categories). The audio data may be processed by at least one ambient noise detection model of the assistant device to classify any ambient (or background) noise captured in the data. Moreover, the automated assistant additionally or alternatively causes the audio data to be processed by at least one sound detection model of the assistant device to determine whether any particular sound is captured in the audio data. can be done.
本明細書において説明される実装形態は、時間的に対応するオーディオデータが多様なアシスタントデバイスのそれぞれのマイクロフォンによって捉えられることに基づいて、エコシステムにおいて音響イベントが実際に発生したと決定することに関する。それらの実装形態はさらに、アシスタントデバイスにおいてローカルでオーディオデータを処理して、音響イベントが実際に発生したかどうかを示す尺度を生成することに関する。またさらに、それらの実装形態は、エコシステムのデバイストポロジー表現に基づいてオーディオデータに時間的に対応するオーディオデータをエコシステムの中のどのアシスタントデバイスが検出したはずであるかを特定することと、多様なアシスタントデバイスが時間的に対応するオーディオデータを検出するとき、時間的に対応するオーディオデータがそれぞれのイベント検出モデルによって処理されるようにすることとに関する。 Implementations described herein relate to determining that an acoustic event has actually occurred in an ecosystem based on temporally corresponding audio data captured by the respective microphones of various assistant devices. . Those implementations further relate to processing audio data locally at the assistant device to generate a measure of whether an acoustic event has actually occurred. Still further, those implementations identify which assistant device in the ecosystem should have detected audio data temporally corresponding to the audio data based on the device topology representation of the ecosystem; and causing the temporally corresponding audio data to be processed by respective event detection models when various assistant devices detect the temporally corresponding audio data.
ここで図1を見ると、本明細書において開示される技法が実装され得る例示的な環境が示される。例示的な環境は、複数のアシスタント入力デバイス1061-N(単に「アシスタント入力デバイス106」とも本明細書では呼ばれる)、1つまたは複数のクラウドベースの自動化アシスタントコンポーネント119、1つまたは複数のアシスタント非入力システム180、1つまたは複数のアシスタント非入力デバイス1851-N(単に「アシスタント非入力デバイス185」とも本明細書では呼ばれる)、デバイス活動データベース191、機械学習(「ML」)モデルデータベース、およびデバイストポロジーデータベース193を含む。図1のアシスタント入力デバイス106およびアシスタント非入力デバイス185は、本明細書では集合的に「アシスタントデバイス」とも呼ばれることがある。
Turning now to FIG. 1, an exemplary environment is shown in which the techniques disclosed herein may be implemented. An exemplary environment includes a plurality of assistant input devices 106 1-N (also referred to herein simply as "assistant input devices 106"), one or more cloud-based automated assistant components 119, one or more assistant non-input system 180, one or more assistant non-input devices 185 1-N (also referred to herein simply as "assistant
アシスタント入力デバイス106の1つまたは複数(たとえば、すべて)が、それぞれの自動化アシスタントクライアント1181-Nのそれぞれのインスタンスを実行することができる。しかしながら、いくつかの実装形態では、アシスタント入力デバイス106の1つまたは複数は任意選択で、それぞれの自動化アシスタントクライアント1181-Nのインスタンスを欠いていてもよく、それでも、自動化アシスタントに向けられるユーザ入力を受け取って処理するためのエンジンおよびハードウェアコンポーネント(たとえば、マイクロフォン、スピーカー、発話認識エンジン、自然言語処理エンジン、発話合成エンジンなど)を含んでもよい。自動化アシスタントクライアント1181-Nのインスタンスは、それぞれのアシスタント入力デバイス106のオペレーティングシステムとは別のアプリケーション(たとえば、オペレーティングの「上に」インストールされる)であってもよく、または代替として、それぞれのアシスタント入力デバイス106のオペレーティングシステムによって直接実装されてもよい。以下でさらに説明されるように、自動化アシスタントクライアント1181-Nの各インスタンスは任意選択で、それぞれのアシスタント入力デバイス106のいずれか1つのそれぞれのユーザインターフェースコンポーネント1071-Nによって提供される様々な要求に応答する際に、1つまたは複数のクラウドベースの自動化アシスタントコンポーネント119と対話することができる。さらに、以下でも説明されるように、アシスタント入力デバイス106の他のエンジンは任意選択で、クラウドベースの自動化アシスタントコンポーネント119の1つまたは複数と対話することができる。 One or more (eg, all) of the assistant input devices 106 can run respective instances of respective automated assistant clients 118 1-N . However, in some implementations, one or more of the assistant input devices 106 may optionally lack an instance of a respective automated assistant client 118 1-N and still receive user input directed to the automated assistant. may include engines and hardware components (eg, microphones, speakers, speech recognition engines, natural language processing engines, speech synthesis engines, etc.) for receiving and processing the . Instances of automation assistant clients 118 1-N may be separate applications (e.g., installed "on top of" the operating system of their respective assistant input devices 106), or alternatively, their respective It may be implemented directly by the operating system of the assistant input device 106. As further described below, each instance of the automated assistant client 118 1-N optionally includes various user interface components 107 1-N provided by the respective user interface components 107 1-N of any one of the respective assistant input devices 106. In responding to requests, one or more cloud-based automation assistant components 119 may be interacted with. Additionally, other engines of the assistant input device 106 may optionally interact with one or more of the cloud-based automated assistant components 119, as also described below.
1つまたは複数のクラウドベースの自動化アシスタントコンポーネント119は、1つまたは複数のローカルエリアネットワーク(Wi-Fi LAN、Bluetoothネットワーク、近距離通信ネットワーク、メッシュネットワークなどを含む「LAN」)および/またはワイドエリアネットワーク(インターネットなどを含む「WAN」)を介してそれぞれのアシスタント入力デバイス106に通信可能に結合される、1つまたは複数のコンピューティングシステム(たとえば、「クラウド」または「リモート」コンピューティングシステムと集合的に呼ばれるサーバ)上で実装され得る。アシスタント入力デバイス106とのクラウドベースの自動化アシスタントコンポーネント119の通信結合は、図1の1101によって全般に示される。また、いくつかの実施形態では、アシスタント入力デバイス106は、図1の1102によって全般に示される、1つまたは複数のネットワーク(たとえば、LANおよび/またはWAN)を介して互いに通信可能に結合され得る。 One or more cloud-based automation assistant components 119 may be connected to one or more local area networks ("LAN", including Wi-Fi LANs, Bluetooth networks, near field communication networks, mesh networks, etc.) and/or wide area One or more computing systems (e.g., "cloud" or "remote" computing systems and collectively communicatively coupled to each assistant input device 106 via a network ("WAN" including the Internet, etc.) server). The communicative coupling of the cloud-based automated assistant component 119 with the assistant input device 106 is indicated generally by 1101 in FIG. Also, in some embodiments, assistant input devices 106 are communicatively coupled to each other via one or more networks (eg, LAN and/or WAN), generally indicated by 1102 in FIG. obtain.
1つまたは複数のクラウドベースの自動化アシスタントコンポーネント119はまた、1つまたは複数のネットワーク(たとえば、LANおよび/またはWAN)を介して1つまたは複数のアシスタント非入力システム180と通信可能に結合され得る。アシスタント非入力システム180とのクラウドベースの自動化アシスタントコンポーネント119の通信結合は、図1の1103によって全般に示される。さらに、アシスタント非入力システム180は各々、1つまたは複数のネットワーク(たとえば、LANおよび/またはWAN)を介してアシスタント非入力デバイス185の1つまたは複数(たとえば、グループ)に通信可能に結合され得る。たとえば、第1のアシスタント非入力システム180は、アシスタント非入力デバイス185の1つまたは複数の第1のグループと通信可能に結合され、それらからデータを受信することができ、第2のアシスタント非入力システム180は、アシスタント非入力デバイス185の1つまたは複数の第2のグループと通信可能に結合され、それらからデータを受信することができ、以下同様である。アシスタント非入力デバイス185とのアシスタント非入力システム180の通信結合は、図1の1104によって全般に示される。
One or more cloud-based automated assistant components 119 may also be communicatively coupled to one or more assistant non-input systems 180 via one or more networks (eg, LAN and/or WAN). . The communicative coupling of the cloud-based automated assistant component 119 with the assistant non-input system 180 is indicated generally by 1103 in FIG. Further, assistant non-input systems 180 may each be communicatively coupled to one or more (eg, groups) of assistant
自動化アシスタントクライアント118のインスタンスは、クラウドベースの自動化アシスタントコンポーネント119の1つまたは複数とのその対話によって、ユーザから見ると、ユーザが人対コンピュータの対話に関わり得る際に用いる自動化アシスタント120の論理インスタンスであるように見えるものを形成し得る。そのような自動化アシスタントの2つのインスタンスが図1に示されている。破線で囲まれる第1の自動化アシスタント120Aは、アシスタント入力デバイス1061の自動化アシスタントクライアント1181、および1つまたは複数のクラウドベースの自動化アシスタントコンポーネント119を含む。dash-dash-dot線で囲まれる第2の自動化アシスタント120Bは、アシスタント入力デバイス106Nの自動化アシスタントクライアント118Nおよび1つまたは複数のクラウドベースの自動化アシスタントコンポーネント119を含む。したがって、アシスタント入力デバイス106の1つまたは複数で実行される自動化アシスタントクライアント118と関わる各ユーザは、実質的に、自動化アシスタント120のユーザ固有の論理インスタンス(または、家庭もしくはユーザの他のグループの間で共有される自動化アシスタント120の論理インスタンス)と関わり得ることを理解されたい。簡潔で簡単にするために、本明細書で使用される「自動化アシスタント」という用語は、アシスタント入力デバイス106のそれぞれ1つで実行される自動化アシスタントクライアント118と、クラウドベースの自動化アシスタントコンポーネント119(多様な自動化アシスタントクライアント118の間で共有され得る)の1つまたは複数との組合せを指す。複数のアシスタント入力デバイス106のみが図1に示されているが、クラウドベースの自動化アシスタントコンポーネント119は追加で、アシスタント入力デバイスの多くの追加のグループにサービスできることが理解される。 An instance of the automation assistant client 118 is, from the user's point of view, a logical instance of the automation assistant 120 with which the user can engage in human-computer interaction through its interaction with one or more of the cloud-based automation assistant components 119. can form what appears to be Two instances of such automated assistants are shown in Figure 1. A first automation assistant 120A, surrounded by a dashed line, includes an automation assistant client 118i of an assistant input device 106i and one or more cloud-based automation assistant components 119. A second automated assistant 120B, bounded by dash-dash-dot lines, includes automated assistant client 118 N and one or more cloud-based automated assistant components 119 of assistant input device 106 N. Thus, each user involved with an automated assistant client 118 running on one or more of the assistant input devices 106 is effectively a user-specific logical instance of the automated assistant 120 (or between a home or other group of users). It should be understood that the logic instance of the automation assistant 120 shared in the . For brevity and simplicity, the term "automation assistant" as used herein refers to the automation assistant client 118 running on each one of the assistant input devices 106 and the cloud-based automation assistant component 119 (various (which may be shared between automated assistant clients 118). Although only multiple assistant input devices 106 are shown in FIG. 1, it is understood that the cloud-based automated assistant component 119 can additionally service many additional groups of assistant input devices.
アシスタント入力デバイス106は、たとえば、デスクトップコンピューティングデバイス、ラップトップコンピューティングデバイス、タブレットコンピューティングデバイス、携帯電話コンピューティングデバイス、ユーザの車両のコンピューティングデバイス(たとえば、車載通信システム、車載娯楽システム、車載ナビゲーションシステム)、双方向型スタンドアロンスピーカー(たとえば、ディスプレイを伴う、または伴わない)、スマートテレビジョンなどのスマートアプライアンス、コンピューティングデバイスを含むユーザのウェアラブル装置(たとえば、コンピューティングデバイスを有するユーザの腕時計、コンピューティングデバイスを有するユーザの眼鏡、仮想現実または拡張現実コンピューティングデバイス)、および/または自動化アシスタント120に向けられるユーザ入力を受け取ることが可能な任意のIoTデバイスのうちの1つまたは複数を含み得る。追加および/または代替のアシスタント入力デバイスが提供され得る。アシスタント非入力デバイス185は、アシスタント入力デバイス106と同じデバイスの多くを含み得るが、自動化アシスタント120に向けられたユーザ入力を受け取ることが可能ではない(たとえば、ユーザインターフェース入力コンポーネントを含まない)。アシスタント非入力デバイス185は、自動化アシスタント120に向けられたユーザ入力を受け取らないが、アシスタント非入力デバイス185はそれでも自動化アシスタント120によって制御され得る。
The assistant input device 106 may be, for example, a desktop computing device, a laptop computing device, a tablet computing device, a mobile phone computing device, a computing device in the user's vehicle (e.g., an in-vehicle communication system, an in-vehicle entertainment system, an in-vehicle navigation system, etc.). systems), interactive stand-alone speakers (e.g., with or without a display), smart appliances such as smart televisions, user wearable devices including computing devices (e.g., users' watches with computing devices, computing devices). a user's glasses with a viewing device, a virtual reality or augmented reality computing device), and/or any IoT device capable of receiving user input directed at the automated assistant 120. Additional and/or alternative assistant input devices may be provided. Assistant
いくつかの実装形態では、複数のアシスタント入力デバイス106およびアシスタント非入力デバイス185は、本明細書において説明される技法の実行を容易にするための様々な方法で互いに関連付けられ得る。たとえば、いくつかの実装形態では、複数のアシスタント入力デバイス106およびアシスタント非入力デバイス185は、1つまたは複数のネットワークを介して(たとえば、図1のネットワーク110を介して)通信可能に結合されることにより互いに関連付けられ得る。これは、たとえば、複数のアシスタント入力デバイス106およびアシスタント非入力デバイス185が、自宅、建物などの特定のエリアまたは環境にわたって配備されている場合に当てはまり得る。追加または代替として、いくつかの実装形態では、複数のアシスタント入力デバイス106およびアシスタント非入力デバイス185は、1名または複数のユーザ(たとえば、個人、家族、組織の従業員、他のあらかじめ定められたグループなど)によって少なくとも選択的にアクセス可能な協調したエコシステムのメンバーであることにより、互いに関連付けられ得る。それらの実装形態のいくつかでは、複数のアシスタント入力デバイス106およびアシスタント非入力デバイス185のエコシステムは、デバイストポロジーデータベース193に記憶されたエコシステムのデバイストポロジー表現において互いに手動および/または自動で関連付けられ得る。
In some implementations, multiple assistant input devices 106 and assistant
アシスタント非入力システム180は、1つまたは複数のファーストパーティ(1P)システムおよび/または1つまたは複数のサードパーティ(3P)システムを含み得る。1Pシステムは、本明細書において言及されている自動化アシスタント120を制御する当事者と同じである当事者により制御されるシステムを指す。3Pシステムは、本明細書において使用される場合、本明細書において言及されている自動化アシスタント120を制御する当事者とは別の当事者により制御されるシステムを指す。 Assistant non-input systems 180 may include one or more first party (1P) systems and/or one or more third party (3P) systems. A 1P system refers to a system controlled by the same party that controls the automated assistant 120 referred to herein. A 3P system, as used herein, refers to a system controlled by a party other than the party controlling the automated assistant 120 referred to herein.
アシスタント非入力システム180は、アシスタント非入力デバイス185および/またはそれに(たとえば、図1のネットワーク110を介して)通信可能に結合される1つまたは複数のクラウドベースの自動化アシスタントコンポーネント119からデータを受信し、アシスタント非入力デバイス185および/または1つまたは複数のクラウドベースの自動化アシスタントコンポーネント119にデータ(たとえば、状態、状態変化、および/または他のデータ)を選択的に送信することができる。たとえば、アシスタント非入力デバイス1851はスマート呼び鈴IoTデバイスであると仮定する。個人が呼び鈴IoTデバイスのボタンを押したことに応答して、呼び鈴IoTデバイスは、アシスタント非入力システム180の1つ(たとえば、1Pシステムまたは3Pシステムであり得る呼び鈴の製造業者によって管理されるアシスタント非入力システムの1つ)に対応するデータを送信することができる。アシスタント非入力システム180の1つは、そのようなデータに基づいて呼び鈴IoTデバイスの状態の変化を決定することができる。たとえば、アシスタント非入力システム180の1つは、非アクティブ状態(たとえば、直近のボタンの押下なし)からアクティブ状態(直近のボタンの押下)への呼び鈴の変化を決定することができ、呼び鈴状態の変化は、1つまたは複数のクラウドベースの自動化アシスタントコンポーネント119および/またはアシスタント入力デバイス106の1つまたは複数に(たとえば、図1のネットワーク110を介して)送信され得る。特に、ユーザ入力はアシスタント非入力デバイス1851において受け取られる(たとえば、呼び鈴のボタンの押下)が、ユーザ入力は自動化アシスタント120に向けられない(したがって、「アシスタント非入力デバイス」という用語である)。別の例として、アシスタント非入力デバイス1851は、マイクロフォンを有するスマートサーモスタットIoTデバイスであるが、スマートサーモスタットは自動化アシスタントクライアント118を含まないと仮定する。個人は、スマートサーモスタットを操作して(たとえば、タッチ入力または話される入力を使用して)、温度を変更し、スマートサーモスタットを介してHVACシステムを制御するための設定点として特定の値を設定することなどができる。しかしながら、スマートサーモスタットが自動化アシスタントクライアント118を含まない限り、個人はスマートサーモスタットを介して自動化アシスタント120と直接意思疎通することができない。
Assistant non-input system 180 receives data from assistant
様々な実装形態において、1つまたは複数のクラウドベースの自動化アシスタントコンポーネント119はさらに、様々なエンジンを含み得る。たとえば、図1に示されるように、1つまたは複数のクラウドベースの自動化アシスタントコンポーネント119はさらに、イベント検出エンジン130、デバイス特定エンジン140、イベント処理エンジン150、セマンティック標識エンジン160、およびクエリ/コマンド処理エンジン170を含み得る。これらの様々なエンジンは、図1の1つまたは複数のクラウドベースの自動化アシスタントコンポーネント119として示されているが、それは例示のためであり、限定することは意図されないことを理解されたい。たとえば、アシスタント入力デバイス106および/またはアシスタント非入力デバイス185は、これらの様々なエンジンの1つまたは複数を含み得る。別の例として、これらの様々なエンジンは、アシスタント入力デバイス106にわたって分散していてもよく、アシスタント非入力デバイス185は、これらの様々なエンジン、および/または1つまたは複数のクラウドベースの自動化アシスタントコンポーネント119の1つまたは複数を含んでもよい。
In various implementations, one or more cloud-based automation assistant components 119 may further include various engines. For example, as shown in FIG. 1, one or more cloud-based automation assistant components 119 further include event detection engine 130, device identification engine 140, event processing engine 150, semantic labeling engine 160, and query/command processing. An engine 170 may be included. Although these various engines are shown as one or more cloud-based automation assistant components 119 in FIG. 1, it should be understood that this is for illustrative purposes and is not intended to be limiting. For example, assistant input device 106 and/or assistant
いくつかの実装形態では、イベント検出エンジン130は、エコシステムにおいて発生する様々なイベントを検出することができる。それらの実装形態のいくつかのバージョンでは、イベント検出エンジン130は、アシスタント入力デバイス106の所与の1つおよび/またはアシスタント非入力デバイス185の所与の1つ(たとえば、アシスタントデバイスの所与の1つ)がいつ新しくエコシステムに追加されるか、またはエコシステム内でいつ移動するかを検出することができる。たとえば、イベント検出エンジン130は、ネットワーク110を介して検出される1つまたは複数のワイヤレス信号に基づいて、およびデバイス特定エンジン140を介して、アシスタントデバイスの所与の1つがいつエコシステムに新しく追加されるかを決定することができる。たとえば、アシスタントデバイスの所与の1つがネットワーク110の1つまたは複数に新しく接続されるとき、アシスタントデバイスの所与の1つは、それがネットワーク110に新しく追加されることを示す信号をブロードキャストすることができる。別の例として、イベント検出エンジン130は、ネットワーク110を介して検出される1つまたは複数のワイヤレス信号に基づいて、アシスタントデバイスの所与の1つがエコシステム内でいつ移動したかを決定することができる。これらの例では、デバイス特定エンジン140は、信号を処理して、アシスタントデバイスの所与の1つがネットワーク110に新しく追加されると決定し、かつ/または、アシスタントデバイスの所与の1つがエコシステム内で移動したと決定することができる。デバイス特定エンジン140によって検出される1つまたは複数のワイヤレス信号は、たとえば、人が知覚できず、アシスタントデバイスの所与の1つおよび/またはアシスタントデバイスの所与の1つに位置的に近い他のアシスタントデバイスのそれぞれの固有の識別子を任意選択で含む、ネットワーク信号および/または音響信号であり得る。たとえば、アシスタントデバイスの所与の1つがエコシステム内で移動するとき、デバイス特定エンジン140は、アシスタントデバイスの所与の1つに位置的に近い他のアシスタントデバイスによって送信されている1つまたは複数のワイヤレス信号を検出することができる。これらの信号を処理して、アシスタントデバイスの所与の1つに位置的に近い1つまたは複数の他のアシスタントデバイスが、アシスタントデバイスの所与の1つに以前は位置的に近かった1つまたは複数のアシスタントデバイスと異なると決定することができる。 In some implementations, the event detection engine 130 can detect various events occurring in the ecosystem. In some versions of those implementations, the event detection engine 130 controls a given one of the assistant input devices 106 and/or a given one of the assistant non-input devices 185 (e.g., a given one of the assistant devices). 1) is newly added to the ecosystem or moved within the ecosystem. For example, the event detection engine 130 determines when a given one of the assistant devices is newly added to the ecosystem based on one or more wireless signals detected over the network 110 and via the device identification engine 140. You can decide whether For example, when a given one of the assistant devices is newly connected to one or more of the networks 110, the given one of the assistant devices broadcasts a signal indicating that it is newly added to the network 110. be able to. As another example, the event detection engine 130 may determine when a given one of the assistant devices has moved within the ecosystem based on one or more wireless signals detected over the network 110. can be done. In these examples, the device identification engine 140 processes signals to determine that the given one of the assistant devices is newly added to the network 110 and/or that the given one of the assistant devices is added to the ecosystem. can be determined to have moved within The one or more wireless signals detected by the device identification engine 140 are, for example, imperceptible to humans and/or located near the given one of the assistant devices and/or the given one of the assistant devices. may be a network signal and/or an acoustic signal optionally containing a unique identifier for each of the assistant devices. For example, as a given one of the assistant devices moves within the ecosystem, the device identification engine 140 may identify the one or more that are being sent by other assistant devices that are positionally close to the given one of the assistant devices. of wireless signals can be detected. These signals are processed so that one or more other assistant devices that are positionally close to the given one of the assistant devices are the one that was previously positionally close to the given one of the assistant devices. Or you can decide to be different with multiple assistant devices.
それらの実装形態のいくつかのさらなるバージョンでは、自動化アシスタント120は、エコシステムに新しく追加される、またはエコシステム内で移動したアシスタントデバイスの所与の1つが、(たとえば、デバイストポロジーデータベース193に記憶されているエコシステムのデバイストポロジー表現において)アシスタントデバイスのグループに割り当てられるようにすることができる。たとえば、アシスタントデバイスの所与の1つがエコシステムに新しく追加される実装形態では、アシスタントデバイスの所与の1つはアシスタントデバイスの既存のグループに追加されてもよく、またはアシスタントデバイスの所与の1つを含むアシスタントデバイスの新しいグループが作成されてもよい。たとえば、アシスタントデバイスの所与の1つが、「台所」グループに属する複数のアシスタントデバイス(たとえば、スマートオーブン、スマートコーヒーメーカー、台所に位置していることを示す固有の識別子もしくは標識と関連付けられる双方向型スタンドアロンスピーカー、および/または他のアシスタントデバイス)に位置的に近い場合、アシスタントデバイスの所与の1つが「台所」グループに追加されてもよく、または新しいグループが作成されてもよい。別の例として、アシスタントデバイスの所与の1つがエコシステム内で移動する実装形態では、アシスタントデバイスの所与の1つがアシスタントデバイスの既存のグループに追加されてもよく、またはアシスタントデバイスの所与の1つを含むアシスタントデバイスの新しいグループが作成されてもよい。たとえば、アシスタントデバイスの所与の1つが、前述の「台所」グループに属する複数のアシスタントデバイスに位置的に近かったが、今では「車庫」グループに属する複数のアシスタントデバイス(たとえば、スマート車庫扉、スマート扉錠、および/または他のアシスタントデバイス)に位置的に近い場合、アシスタントデバイスの所与の1つは「台所」グループから削除され、「車庫」グループに追加され得る。 In some further versions of those implementations, the Automation Assistant 120 determines that a given one of the assistant devices newly added to the ecosystem or moved within the ecosystem is (e.g., stored in the device topology database 193) assigned to a group of assistant devices (in the device topology representation of the ecosystem being defined). For example, in implementations where a given one of assistant devices is newly added to the ecosystem, the given one of assistant devices may be added to an existing group of assistant devices, or a given one of assistant devices A new group of assistant devices containing one may be created. For example, a given one of the assistant devices is associated with multiple assistant devices belonging to the "kitchen" group (e.g. smart oven, smart coffee maker, a unique identifier or indicator that it is located in the kitchen). type stand-alone speakers, and/or other assistant devices), a given one of the assistant devices may be added to the "kitchen" group, or a new group may be created. As another example, in implementations where a given one of the assistant devices moves within the ecosystem, the given one of the assistant devices may be added to an existing group of assistant devices, or a given one of the assistant devices may be added to an existing group of assistant devices. A new group of assistant devices may be created containing one of the For example, a given one of the assistant devices was positionally close to multiple assistant devices belonging to the aforementioned "kitchen" group, but now has multiple assistant devices belonging to the "garage" group (e.g. smart garage door, smart door locks, and/or other assistant devices), the given one of the assistant devices may be removed from the "kitchen" group and added to the "garage" group.
それらの実装形態のいくつかの追加または代替のバージョンでは、イベント検出エンジン130は、音響イベントの発生を検出することができる。音響イベントの発生は、アシスタント入力デバイス106の1つまたは複数および/またはアシスタント非入力デバイス185の1つまたは複数(たとえば、アシスタントデバイスの1つまたは複数)において受信されるオーディオデータに基づいて検出され得る。アシスタントデバイスの1つまたは複数において受信されるオーディオデータは、MLモデルデータベース192に記憶されているイベント検出モデルによって処理され得る。これらの実装形態では、音響イベントの発生を検出する1つまたは複数のアシスタントデバイスの各々は、それぞれのマイクロフォンを含む。 In some additional or alternative versions of those implementations, the event detection engine 130 can detect the occurrence of acoustic events. The occurrence of the acoustic event is detected based on audio data received at one or more of the assistant input devices 106 and/or one or more of the assistant non-input devices 185 (eg, one or more of the assistant devices). obtain. Audio data received at one or more of the assistant devices may be processed by event detection models stored in the ML model database 192 . In these implementations, each of the one or more assistant devices that detect the occurrence of acoustic events includes a respective microphone.
それらの実装形態のいくつかのさらなるバージョンでは、音響イベントの発生は、アシスタントデバイスの1つまたは複数においてオーディオデータに捉えられる周辺雑音を含み得る(および任意選択で、アシスタントデバイスの1つまたは複数において発話受け入れがアクティブであるときに検出される周辺雑音の発生のみを含み得る)。1つまたは複数のアシスタントデバイスの各々において検出される周辺雑音は、デバイス活動データベース191に記憶され得る。これらの実装形態では、イベント処理エンジン150は、周辺雑音検出モデルを使用して周辺雑音を処理する際に生成される尺度に基づいて周辺ノイズを複数の異種のセマンティックカテゴリの1つまたは複数へと分類するように訓練される周辺雑音検出モデルを使用して、1つまたは複数のアシスタントデバイスにおいて検出される周辺雑音を処理することができる。複数の異種のカテゴリは、たとえば、映画もしくはテレビの音のカテゴリ、料理の音のカテゴリ、音楽の音のカテゴリ、車庫もしくは作業場の音のカテゴリ、中庭の音のカテゴリ、および/またはセマンティック上意味のある他の異種の音のカテゴリを含み得る。たとえば、周辺雑音検出モデルを使用して処理された周辺雑音が、電子レンジの鳴動、フライパンで焼かれる食べ物、食べ物を処理するフードプロセッサなどに対応する音を含むと、イベント処理エンジン150が決定する場合、イベント処理エンジン150は、周辺雑音を料理の音のカテゴリへと分類することができる。別の例として、周辺雑音検出モデルを使用して処理された周辺雑音が、電動丸鋸の作動、金槌を打つことなどに対応する音を含むと、イベント処理エンジン150が決定する場合、イベント処理エンジン150は、周辺雑音を車庫または作業場カテゴリへと分類することができる。特定のデバイスにおいて検出される周辺雑音の分類はまた、アシスタントデバイスに対するセマンティック標識(たとえば、セマンティック標識エンジン160に関して説明される)を推測する際に利用されるデバイス固有信号としても利用され得る。 In some further versions of those implementations, the occurrence of acoustic events may include ambient noise captured in audio data at one or more of the assistant devices (and optionally at one or more of the assistant devices). may only include ambient noise occurrences detected when speech acceptance is active). Ambient noise detected at each of the one or more assistant devices may be stored in device activity database 191 . In these implementations, the event processing engine 150 classifies the ambient noise into one or more of a plurality of disparate semantic categories based on measures generated in processing the ambient noise using the ambient noise detection model. Ambient noise detection models that are trained to classify can be used to process ambient noise detected in one or more assistant devices. The multiple disparate categories may be, for example, a movie or television sound category, a cooking sound category, a music sound category, a garage or workshop sound category, a courtyard sound category, and/or semantically semantically semantic sound categories. It may contain some other disparate sound categories. For example, the event processing engine 150 determines that the ambient noise processed using the ambient noise detection model includes sounds corresponding to a microwave oven humming, food being pan-fried, a food processor processing food, etc. If so, the event processing engine 150 may classify the ambient noise into the category of cooking sounds. As another example, if the event processing engine 150 determines that the ambient noise processed using the ambient noise detection model includes sounds corresponding to the actuation of a power circular saw, hammering, etc., then the event processing The engine 150 can classify ambient noise into garage or workshop categories. A classification of ambient noise detected at a particular device can also be utilized as a device-specific signal utilized in inferring semantic markings (eg, described with respect to semantic marking engine 160) for assistant devices.
それらのさらなる実装形態のいくつかの追加または代替のバージョンでは、音響イベントの発生は、アシスタントデバイスの1つまたは複数において検出されるホットワードまたは特定の音を含み得る。これらの実装形態では、イベント処理エンジン150は、ホットワード検出モデルを使用してオーディオデータを処理する際に生成される尺度に基づいて自動化アシスタント120を呼び出す特定の語または語句をオーディオデータが含むかどうかを決定するように訓練されるホットワード検出モデルを使用して、1つまたは複数のアシスタントデバイスにおいて検出されるオーディオデータを処理することができる。たとえば、イベント処理エンジン150は、オーディオデータを処理して、「アシスタント」、「ヘイアシスタント」、「OKアシスタント」、および/または自動化アシスタントを呼び出す任意の他の語もしくは語句を含むユーザの発話をオーディオデータが捉えるかどうかを決定することができる。さらに、ホットワード検出モデルを使用して生成される尺度は、自動化アシスタント120を呼び出す語または語句をオーディオデータが含むかどうかを示すそれぞれの信頼性レベルまたは確率を含み得る。これらの実装形態のいくつかのバージョンでは、イベント処理エンジン150は、尺度が閾値を満たす場合、オーディオデータが語または語句を捉えると決定することができる。たとえば、イベント処理エンジン150が、自動化アシスタント120を呼び出す語または語句を捉えるオーディオデータと関連付けられる0.70という尺度を生成し、閾値が0.65である場合、イベント処理エンジン150は、自動化アシスタント120を呼び出す語または語句をオーディオデータが捉えると決定し得る。ホットワード検出モデルは、たとえば教師あり学習技法を使用して訓練され得る。たとえば、複数の訓練インスタンスが取得され得る。訓練インスタンスの各々は、オーディオデータ(または、メル周波数ケプストラム係数、オーディオ波形、メルバンク特徴、および/または他の音響特徴などの、オーディオデータの特徴)を含む訓練インスタンス入力、および自動化アシスタント120を呼び出す特定の語または語句を訓練インスタンス入力が含むかどうかの標示を含む対応する訓練インスタンス出力を含み得る。たとえば、特定の語または語句を捉えるオーディオデータを訓練インスタンス入力が含む場合、対応する訓練インスタンス出力は、特定の語または語句を訓練インスタンス入力が含むことを示す標識(たとえば、「はい」)または値(たとえば、「1」)割り当てられてもよく、特定の語または語句を訓練インスタンス入力が含まない場合、対応する訓練インスタンス出力は異なる標識(たとえば、「いいえ」)または値(たとえば、「0」)を割り当てられてもよい。 In some additional or alternative versions of those further implementations, the occurrence of acoustic events may include hotwords or specific sounds detected in one or more of the assistant devices. In these implementations, the event processing engine 150 determines whether the audio data contains a particular word or phrase that invokes the automated assistant 120 based on measures generated when processing the audio data using the hot word detection model. Audio data detected in one or more assistant devices can be processed using a hot word detection model that is trained to determine if. For example, the event processing engine 150 may process the audio data to audio the user's utterances including "Assistant", "Hey Assistant", "OK Assistant", and/or any other word or phrase that invokes an automated assistant. You can decide whether the data captures. Additionally, measures generated using the hot word detection model may include respective confidence levels or probabilities indicating whether the audio data contains the word or phrase that invokes the automated assistant 120 . In some versions of these implementations, event processing engine 150 may determine that audio data captures a word or phrase if the measure meets a threshold. For example, if the event processing engine 150 produces a scale of 0.70 that is associated with audio data that captures the word or phrase that invokes the automation assistant 120, and the threshold is 0.65, the event processing engine 150 will detect the word or phrase that invokes the automation assistant 120. It can be determined that the phrase is captured by the audio data. A hot word detection model may be trained using, for example, supervised learning techniques. For example, multiple training instances may be obtained. Each of the training instances has a training instance input containing audio data (or features of the audio data, such as Mel-frequency cepstrum coefficients, audio waveforms, Melbank features, and/or other acoustic features), and a specific call to the automation assistant 120. A corresponding training instance output containing an indication of whether the training instance input contains the word or phrase of . For example, if a training instance input contains audio data that captures a particular word or phrase, then the corresponding training instance output is either an indicator (e.g. "yes") or a value indicating that the training instance input contains the particular word or phrase. (e.g., "1"), and if no training instance input contains a particular word or phrase, the corresponding training instance output will be a different indicator (e.g., "no") or value (e.g., "0"). ) may be assigned.
これらの実装形態では、イベント処理エンジン150は、追加または代替として、音検出モデルを使用してオーディオデータを処理する際に生成される尺度に基づいてオーディオデータが特定の音を含むかどうかを決定するように訓練される音検出モデルを使用して、1つまたは複数のアシスタントデバイスにおいて検出されるオーディオデータを処理することができる。特定の音は、たとえば、ガラスが割れること、犬が吠えること、猫が鳴くこと、呼び鈴が鳴ること、火災報知器が鳴動すること、一酸化炭素検出器が鳴動すること、乳児が泣くこと、またはドアをノックすることを含み得る。たとえば、イベント処理エンジン150は、オーディオデータを処理して、これらの特定の音のいずれかをオーディオデータが捉えるかどうかを決定することができる。この例では、単一の音検出モデルが、複数の特定の音がオーディオデータにおいて捉えられるかどうかを決定するように訓練されてもよく、または、複数の音検出モデルが、所与の特定の音がオーディオデータにおいて捉えられるかどうかを決定するために訓練されてもよい。さらに、音検出モデルを使用して生成される尺度は、オーディオデータが特定の音を含むかどうかを示すそれぞれの信頼性レベルまたは確率を含み得る。これらの実装形態のいくつかのバージョンでは、イベント処理エンジン150は、尺度が閾値を満たす場合、オーディオデータが特定の音を捉えると決定することができる。たとえば、イベント処理エンジン150が、ガラスが割れる音を捉えるオーディオデータと関連付けられる0.70という尺度を生成し、閾値が0.65である場合、イベント処理エンジン150は、ガラスが割れる音をオーディオデータが捉えると決定し得る。 In these implementations, the event processing engine 150 additionally or alternatively determines whether the audio data contains a particular sound based on measures generated when processing the audio data using the sound detection model. A sound detection model trained to do so can be used to process audio data detected in one or more assistant devices. Specific sounds include, for example, breaking glass, barking dogs, meowing cats, ringing doorbells, ringing fire alarms, ringing carbon monoxide detectors, crying babies, Or it may involve knocking on the door. For example, event processing engine 150 may process audio data to determine whether the audio data captures any of these particular sounds. In this example, a single sound detection model may be trained to determine whether multiple specific sounds are captured in the audio data, or multiple sound detection models may be trained to determine whether a given specific sound is captured. It may be trained to determine if sounds are captured in the audio data. Additionally, measures generated using sound detection models may include respective confidence levels or probabilities that indicate whether audio data contains a particular sound. In some versions of these implementations, event processing engine 150 may determine that audio data captures a particular sound if the measure meets a threshold. For example, if event processing engine 150 generates a scale of 0.70 that is associated with audio data that captures the sound of breaking glass, and the threshold is 0.65, event processing engine 150 determines that the audio data captures the sound of breaking glass. can.
様々な実装形態において、音響イベントの発生は、エコシステムの中の多様なアシスタントデバイスによって捉えられ得る。たとえば、環境の中の多様なアシスタントデバイスが、時間的に対応するオーディオデータ(たとえば、同じ時間に、または閾値の長さの時間内に多様なアシスタントデバイスにおいてそれぞれのオーディオデータが検出されるという点で時間的に対応する)を捉え得る。これらの実装形態では、所与のアシスタントデバイスがエコシステムにおいてオーディオデータを検出したことに応答して、デバイス特定エンジン140は、同様に音響イベントを捉える時間的に対応するオーディオデータを同様に検出したはずである1つまたは複数の追加のアシスタントデバイスを特定することができる。たとえば、デバイス特定エンジン140は、音響イベントを同様に捉える時間的に対応するオーディオデータを1つまたは複数の追加のアシスタントデバイスが過去に検出していることに基づいて、同様に音響イベントを捉える時間的に対応するオーディオデータを同様に検出したはずである追加のアシスタントデバイスの1つまたは複数を特定することができる。言い換えると、デバイス特定エンジン140は、(たとえば、図2A、図2B、および図3に関して説明されたように)同じ音響イベントを含む時間的に対応するオーディオデータを所与のアシスタントデバイスおよび1つまたは複数の追加のアシスタントデバイスが過去に捉えているので、音響イベントを含むオーディオデータを1つまたは複数の追加のアシスタントデバイスが同様に捉えるはずであると予想することができる。 In various implementations, the occurrence of an acoustic event can be captured by various assistant devices in the ecosystem. For example, different assistant devices in the environment have temporally corresponding audio data (e.g., the point that each audio data is detected in different assistant devices at the same time or within a threshold length of time). ) can be captured. In these implementations, in response to a given assistant device detecting audio data in the ecosystem, the device identification engine 140 also detected temporally corresponding audio data that also captured the acoustic event. One or more additional assistant devices can be identified that should. For example, the device identification engine 140 may determine the time to similarly capture the acoustic event based on one or more additional assistant devices having previously detected temporally corresponding audio data that similarly captures the acoustic event. One or more of the additional assistant devices that would have similarly detected the corresponding audio data can be identified. In other words, the device identification engine 140 distributes temporally corresponding audio data containing the same acoustic event (eg, as described with respect to FIGS. 2A, 2B, and 3) to a given assistant device and one or more Since multiple additional assistant devices have captured in the past, it can be expected that one or more additional assistant devices should likewise capture the audio data containing the acoustic event.
様々な実装形態において、それぞれのアシスタントデバイスによって生成または検出される1つまたは複数のデバイス固有の信号が、デバイス活動データベース191に記憶され得る。いくつかの実装形態では、デバイス活動データベース191は、その特定のアシスタントデバイスのデバイス活動に専用のメモリの一部分に対応することができる。いくつかの追加または代替の実装形態では、デバイス活動データベース191は、アシスタントデバイスと(たとえば、図1のネットワーク110を介して)通信している遠隔システムのメモリに対応することができる。このデバイス活動は、(たとえば、セマンティック標識エンジン160に関して説明される)アシスタントデバイスの所与の1つに対するセマンティック標識候補を生成する際に利用され得る。デバイス活動は、たとえば、それぞれのアシスタントデバイスにおいて受け取られるクエリもしくは要求(および/または複数のクエリもしくは要求の各々に関連するセマンティックカテゴリ)、それぞれのアシスタントデバイスにおいて実行されるコマンド(および/または複数のコマンドの各々に関連するセマンティックカテゴリ)、それぞれのアシスタントデバイスにおいて検出される周辺雑音(および/または周辺雑音の様々なインスタンスに関連するセマンティックカテゴリ)、(たとえば、イベント検出エンジン140を介して特定される)所与のアシスタントデバイスに位置的に近い任意のアシスタントデバイスの固有の識別子もしくは標識、ならびに/または、それぞれのアシスタントデバイスによって受信される、生成される、および/もしくは実行される任意の他のデータを含み得る。 In various implementations, one or more device-specific signals generated or detected by each assistant device may be stored in device activity database 191 . In some implementations, device activity database 191 may correspond to a portion of memory dedicated to device activity for that particular assistant device. In some additional or alternative implementations, device activity database 191 may correspond to memory of a remote system in communication with the assistant device (eg, via network 110 of FIG. 1). This device activity may be utilized in generating candidate semantic markers for a given one of the assistant devices (eg, described with respect to semantic marker engine 160). Device activity may include, for example, a query or request received at the respective assistant device (and/or a semantic category associated with each of the multiple queries or requests), a command executed at the respective assistant device (and/or multiple commands ), the ambient noise detected in each assistant device (and/or the semantic categories associated with various instances of ambient noise), (e.g., identified via event detection engine 140) A unique identifier or indicator of any assistant device that is geographically close to the given assistant device and/or any other data received, generated and/or executed by the respective assistant device can contain.
いくつかの実装形態では、セマンティック標識エンジン160は、1つまたは複数のデバイス固有の信号を処理して、1つまたは複数のデバイス固有の信号に基づいてアシスタントデバイスの所与の1つ(たとえば、アシスタント入力デバイス106の所与の1つおよび/またはアシスタント非入力デバイス185の所与の1つ)に対するセマンティック標識候補を生成することができる。セマンティック標識候補は、1つまたは複数のルール(任意選択でヒューリスティックに定義される)または機械学習モデル(たとえば、MLモデルデータベース192に記憶される)を使用して生成され得る。たとえば、1つまたは複数のヒューリスティックに定義されたルールは、1つまたは複数のデバイス固有の信号が分類される、セマンティックカテゴリの各々に関連するセマンティック標識候補が生成されるべきであることを示し得る。たとえば、デバイス固有の信号が、「台所」カテゴリ、「料理」カテゴリ、「寝室」カテゴリ、および「居間」カテゴリへと分類されると仮定する。この例では、セマンティック標識候補は、「台所のアシスタントデバイス」という第1のセマンティック標識候補、「料理のアシスタントデバイス」という第2のセマンティック標識候補、「寝室のアシスタントデバイス」という第3のセマンティック標識候補、および「居間のアシスタントデバイス」という第4のセマンティック標識を含み得る。別の例として、1つまたは複数のデバイス固有の信号(またはそれに対応する1つまたは複数のセマンティックカテゴリ)は、セマンティック標識候補を生成するように訓練される機械学習モデルを使用して処理され得る。たとえば、機械学習モデルは複数の訓練インスタンスに基づいて訓練され得る。訓練インスタンスの各々は、訓練インスタンス入力および対応する訓練インスタンス出力を含み得る。訓練インスタンス入力は、たとえば、1つもしくは複数のデバイス固有の信号および/または1つもしくは複数のセマンティックカテゴリを含んでもよく、対応する訓練インスタンス出力は、たとえば、訓練インスタンス入力に基づいて割り当てられるべきセマンティック標識に対応するグラウンドトゥルース出力を含んでもよい。 In some implementations, the semantic labeling engine 160 processes the one or more device-specific signals to provide a given one of the assistant devices (e.g., A candidate semantic label can be generated for a given one of assistant input devices 106 and/or a given one of assistant non-input devices 185). Semantic label candidates may be generated using one or more rules (optionally heuristically defined) or machine learning models (eg, stored in ML model database 192). For example, one or more heuristically defined rules may indicate that candidate semantic labels associated with each of the semantic categories into which one or more device-specific signals are classified should be generated. . For example, assume that device-specific signals are classified into a "kitchen" category, a "cooking" category, a "bedroom" category, and a "living room" category. In this example, the candidate semantic tags are a first candidate semantic label of "kitchen assistant device", a second candidate semantic label of "cooking assistant device", and a third candidate semantic label of "bedroom assistant device". , and a fourth semantic indicator "living room assistant device". As another example, one or more device-specific signals (or one or more corresponding semantic categories) may be processed using a machine learning model trained to generate semantic label candidates. . For example, a machine learning model may be trained based on multiple training instances. Each training instance may include a training instance input and a corresponding training instance output. The training instance inputs may include, for example, one or more device-specific signals and/or one or more semantic categories, and the corresponding training instance outputs are, for example, the semantics to be assigned based on the training instance inputs. A ground truth output corresponding to the sign may also be included.
それらの実装形態のいくつかのバージョンでは、所与のアシスタントデバイスがエコシステムに新しく追加される、および/またはエコシステム内で移動すると決定したことに応答して、セマンティック標識候補がそのために生成される所与のアシスタントデバイスを特定することができる。それらの実装形態のいくつかの追加または代替のバージョンでは、セマンティック標識候補がそのために生成される所与のアシスタントデバイスは、定期的に(たとえば、1か月に1回、6か月に1回、1年に1回など)特定され得る。それらの実装形態のいくつかの追加または代替のバージョンでは、セマンティック標識候補がそのために生成される所与のアシスタントデバイスは、所与のアシスタントデバイスが位置するエコシステムの部分の目的が変えられた(たとえば、エコシステムの本邸の中のある部屋が書斎から寝室に目的が変えられた)と決定したことに応答して特定され得る。これらの実装形態では、所与のアシスタントデバイスは、イベント検出エンジン130を利用して特定され得る。これらおよび他の方式で所与のアシスタントデバイスを特定することは、図2Aおよび図2Bに関して説明される。 In some versions of those implementations, in response to a given assistant device deciding to be newly added to and/or move within the ecosystem, candidate semantic markers are generated for it. can identify a given assistant device that In some additional or alternative versions of those implementations, a given assistant device for which semantic label candidates are generated is periodically (e.g., once a month, once every six months) , once a year, etc.). In some additional or alternative versions of those implementations, a given assistant device for which semantic label candidates are generated is repurposed for the part of the ecosystem in which the given assistant device is located ( For example, it may be identified in response to determining that a room in the main residence of the ecosystem has been repurposed from a study to a bedroom. In these implementations, a given assistant device may be identified utilizing event detection engine 130 . Identifying a given assistant device in these and other ways is described with respect to FIGS. 2A and 2B.
いくつかの実装形態では、セマンティック標識エンジン160は、セマンティック標識候補の中から、デバイス固有の信号の1つまたは複数に基づいて所与のアシスタントデバイスに対する所与のセマンティック標識を選択することができる。所与のアシスタントデバイスに対するセマンティック標識候補が、デバイス活動データベース191(またはそれに対応するテキスト)に記憶されているクエリ、要求、および/またはコマンドに基づいて生成される実装形態では、クエリ、要求、および/またはコマンドは、所与のアシスタントデバイスに対するデバイス活動を、異種のクエリ、要求、および/またはコマンドに対応する1つまたは複数の異なるセマンティックカテゴリへとインデクシングするために、セマンティック分類器(たとえば、MLモデルデータベース192に記憶されている)を使用して処理され得る。セマンティック標識候補は、クエリ、コマンド、および/または要求がそれらへと分類されるセマンティックカテゴリに基づいて生成されてもよく、所与のアシスタントデバイスのために選択される所与のセマンティック標識は、所与のセマンティックカテゴリにおいて分類される複数のクエリ、要求、および/またはコマンドの量に基づいて選択されてもよい。たとえば、所与のアシスタントデバイスが、料理のレシピを取得することに関する9個のクエリ、およびエコシステムの中のスマート照明を制御することに関する2個のコマンドを以前に受け取っていると仮定する。この例では、セマンティック標識候補は、たとえば、「台所のデバイス」という第1のセマンティック標識および「スマート照明制御デバイス」という第2のセマンティック標識を含み得る。さらに、セマンティック標識エンジン160は、所与のアシスタントデバイスが主に料理関連の活動に使用されることを所与のアシスタントデバイスの過去の使用が示すので、所与のアシスタントデバイスに対する所与のセマンティック標識として「台所のデバイス」という第1のセマンティック標識を選択することができる。 In some implementations, the semantic label engine 160 may select from among the candidate semantic labels a given semantic label for a given assistant device based on one or more of the device-specific signals. In implementations in which the candidate semantic markers for a given assistant device are generated based on queries, requests, and/or commands stored in the device activity database 191 (or their corresponding text), the queries, requests, and/or commands /or the command uses a semantic classifier (e.g., ML stored in model database 192). Candidate semantic indicators may be generated based on semantic categories into which queries, commands, and/or requests fall, and a given semantic indicator selected for a given assistant device may The selection may be based on the amount of multiple queries, requests, and/or commands that fall into a given semantic category. For example, assume a given assistant device has previously received 9 queries for retrieving cooking recipes and 2 commands for controlling smart lighting in the ecosystem. In this example, the candidate semantic indicators may include, for example, a first semantic indicator of "kitchen device" and a second semantic indicator of "smart lighting control device." In addition, the semantic labeling engine 160 uses the given semantic labeling for a given assistant device because past use of the given assistant device indicates that the given assistant device is primarily used for cooking-related activities. can choose the first semantic indicator of "kitchen device" as
いくつかの実装形態では、MLモデルデータベース192に記憶されているセマンティック分類器は、自然言語理解エンジン(たとえば、以下で説明されるNLPモジュール122によって実装される)であり得る。アシスタントデバイスにおいて以前に受け取られたクエリ、コマンド、および/または要求を処理したことに基づいて決定される意図は、セマンティックカテゴリの1つまたは複数にマッピングされ得る。特に、本明細書において説明される複数の異種のセマンティックカテゴリは、様々なレベルの粒度で定義され得る。たとえば、セマンティックカテゴリは、スマート照明コマンドというカテゴリ、スマートサーモスタットコマンドというカテゴリ、および/またはスマートカメラコマンドというカテゴリなどの、スマートデバイスコマンドの部類カテゴリ、および/またはその部類クラスの種別カテゴリと関連付けられ得る。言い換えると、各カテゴリは、セマンティック分類器によって決定される、各カテゴリに関連する意図の固有のセットを有し得るが、カテゴリのいくつかの意図は、追加のカテゴリとも関連付けられ得る。いくつかの追加または代替の実装形態では、MLモデルデータベース192に記憶されているセマンティック分類器が、クエリ、コマンド、および/または要求のテキストに対応するテキスト埋め込み(たとえば、word2vec表現などの、低次元の表現)を生成するために利用され得る。これらの埋め込みは、セマンティック上類似している語または語句が埋め込み空間の同じまたは類似する部分と関連付けられるような、埋め込み空間内の点であり得る。さらに、埋め込み空間のこれらの部分は、複数の異種のセマンティックカテゴリの1つまたは複数と関連付けられてもよく、埋め込みの所与の1つは、埋め込みの所与の1つと埋め込み空間の部分の1つまたは複数との間の距離の尺度が距離の閾値を満たす場合、セマンティックカテゴリの所与の1つへと分類されてもよい。たとえば、料理に関連する語または語句は、「料理」というセマンティック標識に関連する埋め込み空間の第1の部分と関連付けられてもよく、気象に関連する語または語句は、「気象」というセマンティック標識に関連する埋め込み空間の第2の部分と関連付けられてもよく、以下同様である。 In some implementations, the semantic classifiers stored in ML model database 192 may be natural language understanding engines (eg, implemented by NLP module 122 described below). Intents determined based on processing previously received queries, commands, and/or requests at the assistant device may be mapped to one or more of the semantic categories. In particular, the multiple disparate semantic categories described herein may be defined with varying levels of granularity. For example, a semantic category may be associated with a category category of smart device commands, such as a category of smart lighting commands, a category of smart thermostat commands, and/or a category of smart camera commands, and/or a classification category of the category class. In other words, each category may have a unique set of intents associated with it as determined by the semantic classifier, although some intents of a category may also be associated with additional categories. In some additional or alternative implementations, the semantic classifiers stored in the ML model database 192 include text embeddings (e.g., low-dimensional can be used to generate a representation of These embeddings can be points in the embedding space such that semantically similar words or phrases are associated with the same or similar portions of the embedding space. Moreover, these portions of the embedding space may be associated with one or more of a plurality of heterogeneous semantic categories, such that a given one of the embeddings is associated with a given one of the embeddings and one of the portions of the embedding space. If the distance measure between one or more satisfies a distance threshold, it may be classified into a given one of the semantic categories. For example, words or phrases related to cooking may be associated with the first portion of the embedding space associated with the semantic label "cooking", and words or phrases related to weather may be associated with the semantic label "weather". It may be associated with a second portion of the associated embedding space, and so on.
1つまたは複数のデバイス固有の信号が、追加または代替として周辺雑音活動を含む実装形態では、周辺雑音のインスタンスは、所与のアシスタントデバイスに対するデバイス活動を、異種の周辺雑音に対応する1つまたは複数の異なるセマンティックカテゴリへとインデクシングするために、周辺雑音検出モデル(たとえば、MLモデルデータベース192に記憶されている)を使用して処理され得る。セマンティック標識候補は、周辺雑音のインスタンスがそれらへと分類されるセマンティックカテゴリに基づいて生成されてもよく、所与のアシスタントデバイスのために選択される所与のセマンティック標識は、所与のセマンティックカテゴリにおいて分類される周辺雑音のインスタンスの量に基づいて選択されてもよい。たとえば、所与のアシスタントデバイスにおいて(および任意選択で、発話認識がアクティブであるときにだけ)検出される周辺雑音が主に、料理の音として分類される周辺雑音を含むと仮定する。この例では、セマンティック標識エンジン160は、デバイスが料理に関連する活動の近くに位置していることをオーディオデータにおいて捉えられる周辺雑音が示すので、所与のアシスタントデバイスに対する所与のセマンティック標識として「台所のデバイス」というセマンティック標識を選択することができる。 In implementations in which the one or more device-specific signals additionally or alternatively include ambient noise activity, an instance of ambient noise may refer to device activity for a given assistant device as one or more corresponding to disparate ambient noises. It can be processed using ambient noise detection models (eg, stored in ML model database 192) to index into multiple different semantic categories. Candidate semantic labels may be generated based on semantic categories into which instances of ambient noise are classified, and a given semantic label selected for a given assistant device may be classified into a given semantic category may be selected based on the amount of instances of ambient noise classified in . For example, assume that the ambient noise detected in a given assistant device (and optionally only when speech recognition is active) comprises primarily ambient noise classified as cooking sounds. In this example, the semantic indicator engine 160 assigns the given semantic indicator for the given assistant device as " The semantic label "kitchen device" can be selected.
いくつかの実装形態では、MLモデルデータベース192に記憶されている周辺雑音検出モデルを、特定の音を検出するように訓練することができ、周辺雑音検出モデルにわたって生成される出力に基づいて、周辺雑音のインスタンスが特定の音を含むかどうかを決定することができる。周辺雑音検出モデルは、たとえば、教師あり学習技法を使用して訓練され得る。たとえば、複数の訓練インスタンスが取得され得る。訓練インスタンスの各々は、周辺雑音を含む訓練インスタンス入力と、周辺雑音検出モデルが検出するように訓練されている特定の音を訓練インスタンス入力が含むかどうかの標示を含む対応する訓練インスタンス出力とを含み得る。たとえば、周辺雑音検出モデルが、ガラスが割れる音を検出するように訓練されている場合、ガラスが割れる音を含む訓練インスタンスは、標識(たとえば、「はい」)または値(たとえば、「1」)を割り当てられてもよく、ガラスが割れる音を含まない訓練インスタンスは、異なる標識(たとえば、「いいえ」)または値(たとえば、「0」)を割り当てられてもよい。いくつかの追加または代替の実装形態では、MLモデルデータベース192に記憶されている周辺雑音検出モデルは、周辺雑音のインスタンス(または、メル周波数ケプストラム係数、生のオーディオ波形、および/または他の音響特徴などの、それらの音響特徴)に基づいて、オーディオ埋め込み(たとえば、周辺雑音のインスタンスの低次元の表現)を生成するために利用され得る。これらの埋め込みは、類似する音(または音を捉える音響特徴)が埋め込み空間の同じまたは類似する部分と関連付けられるような、埋め込み空間内の点であり得る。さらに、埋め込み空間のこれらの部分は、複数の異種のセマンティックカテゴリの1つまたは複数と関連付けられてもよく、埋め込みの所与の1つは、埋め込みの所与の1つと埋め込み空間の部分の1つまたは複数との間の距離の尺度が距離の閾値を満たす場合、セマンティックカテゴリの所与の1つへと分類され得る。たとえば、ガラスが割れるというインスタンスは、「ガラスが割れる」音に関連する埋め込み空間の第1の部分と関連付けられてもよく、呼び鈴が鳴るというインスタンスは、「呼び鈴」の音に関連する埋め込み空間の第2の部分と関連付けられてもよく、以下同様である。 In some implementations, the ambient noise detection models stored in the ML model database 192 can be trained to detect specific sounds, and based on the output generated across the ambient noise detection models, ambient It can be determined whether an instance of noise contains a particular sound. Ambient noise detection models may be trained, for example, using supervised learning techniques. For example, multiple training instances may be obtained. Each of the training instances has a training instance input containing ambient noise and a corresponding training instance output containing an indication of whether the training instance input contains the particular sound that the ambient noise detection model is trained to detect. can contain. For example, if an ambient noise detection model is trained to detect the sound of breaking glass, the training instances containing the sound of breaking glass will be either an indicator (e.g. "yes") or a value (e.g. "1") and training instances that do not contain the sound of breaking glass may be assigned a different indicator (eg, "no") or value (eg, "0"). In some additional or alternative implementations, the ambient noise detection models stored in the ML model database 192 use instances of ambient noise (or Mel-frequency cepstrum coefficients, raw audio waveforms, and/or other acoustic features , etc.) can be utilized to generate audio embeddings (eg, low-dimensional representations of instances of ambient noise). These embeddings can be points in the embedding space such that similar sounds (or acoustic features that capture the sounds) are associated with the same or similar portions of the embedding space. Moreover, these portions of the embedding space may be associated with one or more of a plurality of heterogeneous semantic categories, such that a given one of the embeddings is associated with a given one of the embeddings and one of the portions of the embedding space. If the distance measure between one or more satisfies a distance threshold, it may be classified into a given one of the semantic categories. For example, an instance of glass breaking may be associated with a first portion of the embedded space associated with the "glass breaking" sound, and an instance of a doorbell ringing may be associated with the embedded space associated with the "doorbell" sound. It may be associated with a second portion, and so on.
1つまたは複数のデバイス固有の信号が、追加または代替として、所与のアシスタントデバイスに位置的に近い追加のアシスタントデバイスの固有の識別子または標識を含むような実装形態では、セマンティック標識候補は、それらの固有の識別子または標識に基づいて生成されてもよく、所与のアシスタントデバイスのために選択される所与のセマンティック標識は、追加のアシスタントデバイスの固有の識別子または標識の1つまたは複数に基づいて選択されてもよい。たとえば、「スマートオーブン」という第1の標識は、所与のアシスタントデバイスに位置的に近い第1のアシスタントデバイスと関連付けられ、「スマートコーヒーメーカー」という第2の標識は、所与のアシスタントデバイスに位置的に近い第2のアシスタントデバイスと関連付けられると仮定する。この例では、セマンティック標識エンジン160は、所与のアシスタントデバイスに位置的に近い追加のアシスタントデバイスと関連付けられる標識が料理に関連しているので、所与のアシスタントデバイスのための所与のセマンティック標識として「台所のデバイス」というセマンティック標識を選択することができる。固有の識別子または標識は、クエリ、コマンド、および/または要求を処理したことに関連して上で説明されたのと同じまたは同様の方式で、MLモデルデータベース192に記憶されているセマンティック分類器を使用して処理され得る。 In implementations in which the one or more device-specific signals additionally or alternatively include unique identifiers or indicators of additional assistant devices that are positionally close to the given assistant device, the candidate semantic indicators may include those and a given semantic indicator selected for a given assistant device may be generated based on one or more of the unique identifiers or indicators of additional assistant devices. may be selected by For example, a first sign "smart oven" is associated with a first assistant device that is positionally close to a given assistant device, and a second sign "smart coffee maker" is associated with the given assistant device. Suppose it is associated with a second assistant device that is geographically close. In this example, the semantic labeling engine 160 selects the given semantic labeling for the given assistant device because the signs associated with additional assistant devices that are positionally close to the given assistant device are related to cooking. , the semantic label "kitchen device" can be chosen. A unique identifier or indicator identifies a semantic classifier stored in the ML model database 192 in the same or similar manner as described above in connection with processing queries, commands, and/or requests. can be processed using
いくつかの実装形態では、セマンティック標識エンジン160は、(たとえば、デバイストポロジーデータベース193に記憶されている)エコシステムのデバイストポロジー表現において、所与のセマンティック標識を所与のアシスタントデバイスに自動的に割り当てることができる。いくつかの追加または代替の実装形態では、セマンティック標識エンジン160は、自動化アシスタント120に、セマンティック標識候補を含むプロンプトを生成させることができる。プロンプトは、エコシステムに関連するユーザに対して、所与のセマンティック標識として標識候補のうちの1つを選択することを求めることができる。さらに、プロンプトは、アシスタントデバイスの所与の1つ(所与のセマンティック標識が割り当てられている所与のアシスタントデバイスであることもまたはないこともある)および/またはユーザのクライアントデバイス(たとえば、モバイルデバイス)において、視覚的におよび/または聴覚的にレンダリングされ得る。所与のセマンティック標識としての標識候補のうちの1つの選択を受け取ったことに応答して、選択される所与のセマンティック標識は、(たとえば、デバイストポロジーデータベース193に記憶されている)エコシステムのデバイストポロジー表現において所与のアシスタントデバイスに割り当てられ得る。これらの実装形態のいくつかのバージョンでは、所与のアシスタントデバイスに割り当てられる所与のセマンティック標識は、所与のアシスタントデバイスのためのセマンティック標識のリストに追加され得る。言い換えると、複数のセマンティック標識が所与のアシスタントデバイスと関連付けられ得る。これらの実装形態の他のバージョンでは、所与のアシスタントデバイスに割り当てられる所与のセマンティック標識が、所与のアシスタントデバイスのためのあらゆる他のセマンティック標識に取って代わることができる。言い換えると、単一のセマンティック標識のみが所与のアシスタントデバイスと関連付けられ得る。 In some implementations, the semantic labeling engine 160 automatically assigns a given semantic label to a given assistant device in the device topology representation of the ecosystem (eg, stored in the device topology database 193). be able to. In some additional or alternative implementations, the semantic labeling engine 160 can cause the automated assistant 120 to generate prompts containing candidate semantic labels. The prompt may ask a user associated with the ecosystem to select one of the candidate indicators as a given semantic indicator. Furthermore, the prompt may be a given one of the assistant devices (which may or may not be a given assistant device assigned a given semantic label) and/or the user's client device (e.g., mobile device) can be visually and/or audibly rendered. In response to receiving a selection of one of the label candidates as the given semantic label, the selected given semantic label is stored in the ecosystem (e.g., stored in device topology database 193). It can be assigned to a given assistant device in the device topology representation. In some versions of these implementations, a given semantic label assigned to a given assistant device may be added to a list of semantic labels for the given assistant device. In other words, multiple semantic markers can be associated with a given assistant device. In other versions of these implementations, a given semantic indicator assigned to a given assistant device may supersede any other semantic indicator for the given assistant device. In other words, only a single semantic label can be associated with a given assistant device.
いくつかの実装形態では、クエリ/コマンド処理エンジン170は、自動化アシスタント120に向けられ、アシスタント入力デバイス106の1つまたは複数を介して受け取られる、クエリ、要求、またはコマンドを処理することができる。クエリ/コマンド処理エンジン170は、クエリ、要求、またはコマンドを処理して、クエリまたはコマンドを満足させるようにアシスタントデバイスの1つまたは複数を選択することができる。特に、クエリまたはコマンドを満足させるように選択されるアシスタントデバイスの1つまたは複数は、クエリまたはコマンドを受信したアシスタント入力デバイス106の1つまたは複数と異なり得る。クエリ/コマンド処理エンジン170は、1つまたは複数の基準に基づいて、発話を満足させるように1つまたは複数のアシスタントデバイスを選択することができる。1つまたは複数の基準は、たとえば、発話を提供したユーザへのデバイスの1つもしくは複数の近接度(たとえば、以下で説明される存在センサ105を使用して決定される)、エコシステムの中のデバイスの1つもしくは複数のデバイスの能力、1つもしくは複数のアシスタントデバイスに割り当てられるセマンティック標識、および/または、発話を満足させるようにアシスタントデバイスを選択するための他の基準を含み得る。 In some implementations, query/command processing engine 170 can process queries, requests, or commands directed at automated assistant 120 and received via one or more of assistant input devices 106 . A query/command processing engine 170 can process a query, request, or command to select one or more of the assistant devices to satisfy the query or command. In particular, one or more of the assistant devices selected to satisfy the query or command may be different than one or more of the assistant input devices 106 that received the query or command. Query/command processing engine 170 may select one or more assistant devices to satisfy the utterance based on one or more criteria. The one or more criteria may be, for example, one or more proximity of the device to the user who provided the utterance (eg, determined using the presence sensor 105 described below); device capabilities, semantic labels assigned to one or more assistant devices, and/or other criteria for selecting assistant devices to satisfy an utterance.
たとえば、発話を満足させるために表示デバイスが必要であると仮定する。この例では、発話を満足させるように所与のアシスタントデバイスを選択する際に考慮されるアシスタントデバイス候補は、表示デバイスを含むものに限定され得る。エコシステムの中の複数のアシスタントデバイスが表示デバイスを含む場合、表示デバイスを含み、ユーザに最も近い所与のアシスタントデバイスが、発話を満足させるために選択され得る。対照的に、発話を満足させるためにスピーカーしか必要とされない(たとえば、発話を満足させるために表示デバイスが必要とされない)実装形態では、発話を満足させるように所与のアシスタントデバイスを選択する際に考慮されるアシスタントデバイス候補は、表示デバイスを含むかどうかとは無関係に、スピーカーを有するものを含み得る。 For example, suppose a display device is needed to satisfy speech. In this example, the candidate assistant devices considered in selecting a given assistant device to satisfy an utterance may be limited to those including display devices. If multiple assistant devices in the ecosystem include a display device, a given assistant device that includes a display device and is closest to the user may be selected to satisfy the utterance. In contrast, in implementations where only a speaker is required to satisfy an utterance (e.g., no display device is required to satisfy an utterance), when choosing a given assistant device to satisfy an utterance Candidate assistant devices considered for may include those with speakers, whether or not they include a display device.
別の例として、所与のアシスタントデバイスに割り当てられるセマンティック標識と一致するセマンティック特性を発話が含むと仮定する。クエリ/コマンド処理エンジン170は、発話(またはそれに対応するテキスト)の1つまたは複数の語に対応する第1の埋め込みおよび所与のアシスタントデバイスに割り当てられるセマンティック標識の1つまたは複数の語に対応する第2の埋め込みを生成し、それらの埋め込みを比較して、それらの埋め込みが一致する(たとえば、それが厳密な一致かまたは大まかな一致かにかかわらず)ことを示す距離の閾値をそれらの埋め込み間の距離の尺度が満たすかどうかを決定することによって、発話のセマンティック特性が所与のアシスタントデバイスに割り当てられるセマンティック標識と一致することを決定することができる。この例では、クエリ/コマンド処理エンジン170は、(任意選択で、所与のアシスタントデバイスへの、発話を提供したユーザの近接度に加えて、またはその代わりに)発話がセマンティック標識と一致したことに基づいて、発話を満足させるように所与のアシスタントデバイスを選択することができる。このようにして、発話を満足させるようにアシスタントデバイスを選択することを、本明細書において説明されるようなアシスタントデバイスに割り当てられるセマンティック標識に向かって偏らせることができる。 As another example, assume that an utterance contains semantic characteristics that match the semantic labels assigned to a given assistant device. Query/command processing engine 170 corresponds to first embeddings corresponding to one or more words of the utterance (or text corresponding thereto) and one or more words of semantic markers assigned to a given assistant device. generates a second embedding that By determining whether the distance measure between embeddings is met, it can be determined that the semantic properties of the utterance match the semantic labels assigned to a given assistant device. In this example, the query/command processing engine 170 determines (optionally in addition to or instead of the proximity of the user who provided the utterance to a given assistant device) that the utterance matched the semantic indicator. , a given assistant device can be selected to satisfy the utterance. In this way, the selection of assistant devices to satisfy utterances can be biased toward semantic labels assigned to assistant devices as described herein.
様々な実装形態において、アシスタント入力デバイス106の1つまたは複数は、対応するユーザからの承認により、検出された存在、特に人の存在を示す信号を提供するように構成される、1つまたは複数のそれぞれの存在センサ1051-N(本明細書では単に「存在センサ105」とも呼ばれる)を含み得る。それらの実装形態のいくつかでは、自動化アシスタント120は、エコシステムに関連するユーザからの発話を満足させるように、アシスタント入力デバイス106の1つまたは複数において、そのユーザの存在に少なくとも一部基づいて、アシスタント入力デバイス106の1つまたは複数を特定することができる。発話は、アシスタント入力デバイス106の1つまたは複数において応答するコンテンツを(たとえば、聴覚的におよび/または視覚的に)レンダリングすることによって、アシスタント入力デバイス106の1つまたは複数が発話に基づいて制御されるようにすることによって、および/またはアシスタント入力デバイス106の1つまたは複数に発話を満足させるためのあらゆる他の行動を実行させることによって、満足させられ得る。本明細書において説明されるように、自動化アシスタント120は、ユーザがどこにいるか、または直近にどこにいたかに基づいてそれらのアシスタント入力デバイス106を決定する際に、それぞれの存在センサ105に基づいて決定されるデータを活用し、対応するコマンドをそれらのアシスタント入力デバイス106だけに提供することができる。いくつかの追加または代替の実装形態では、自動化アシスタント120は、アシスタント入力デバイス106のいずれかの近くにユーザ(任意のユーザまたは特定のユーザ)が現在いるかどうかを決定する際に、それぞれの存在センサ105に基づいて決定されるデータを活用することができ、アシスタント入力デバイス106のいずれの近くにもユーザ(任意のユーザまたは特定のユーザ)がいないと決定されることに基づいて、コマンドの提供を任意選択で抑制することができる。 In various implementations, one or more of the assistant input devices 106 are configured, upon approval from the corresponding user, to provide a signal indicative of a detected presence, particularly a human presence. of presence sensors 105 1-N (also referred to herein simply as "presence sensors 105"). In some of those implementations, the automated assistant 120 satisfies an utterance from a user associated with the ecosystem based at least in part on the user's presence at one or more of the assistant input devices 106 . , may identify one or more of the assistant input devices 106 . The utterance controls one or more of the assistant input devices 106 based on the utterance by rendering (e.g., audibly and/or visually) responsive content on one or more of the assistant input devices 106. and/or having one or more of the assistant input devices 106 perform any other action to satisfy the utterance. As described herein, the automated assistants 120 make decisions based on their respective presence sensors 105 in determining their assistant input devices 106 based on where the user is or was most recently located. The data received can be leveraged to provide corresponding commands only to those assistant input devices 106 . In some additional or alternative implementations, the automated assistant 120 uses respective presence sensors in determining whether a user (any user or a specific user) is currently near any of the assistant input devices 106. The data determined based on 105 can be leveraged to provide commands based on determining that no user (either any user or a particular user) is near any of the assistant input devices 106. Can be optionally suppressed.
それぞれの存在センサ105は様々な形式をとることがある。一部のアシスタント入力デバイス106は、その視野において検出された動きを示す信号を捉えて提供するように構成される1つまたは複数のデジタルカメラを装備し得る。追加または代替として、一部のアシスタント入力デバイス106は、その視野内の物体から放射する赤外(「IR」)光を測定する受動赤外線(「PIR」)センサなどの、他のタイプの光に基づく存在センサ105を装備し得る。追加または代替として、一部のアシスタント入力デバイス106は、1つまたは複数のマイクロフォンなどの音波(または圧力波)を検出する存在センサ105を装備し得る。その上、アシスタント入力デバイス106に加えて、アシスタント非入力デバイス185の1つまたは複数が、追加または代替として、本明細書において説明されるそれぞれの存在センサ105を含んでもよく、発話を満足させるかどうか、および/または発話をどのように満足させるかを本明細書において説明される実装形態に従って決定する際に、そのようなセンサからの信号が、自動化アシスタント120によって追加で利用されてもよい。
Each presence sensor 105 may take various forms. Some assistant input devices 106 may be equipped with one or more digital cameras configured to capture and provide signals indicative of detected motion in its field of view. Additionally or alternatively, some assistant input devices 106 are sensitive to other types of light, such as passive infrared (“PIR”) sensors that measure infrared (“IR”) light emitted from objects within their field of view. can be equipped with a presence sensor 105 based on Additionally or alternatively, some assistant input devices 106 may be equipped with a presence sensor 105 that detects sound waves (or pressure waves), such as one or more microphones. Moreover, in addition to the assistant input device 106, one or more of the assistant
追加または代替として、いくつかの実装形態では、存在センサ105は、エコシステムにおける人の存在またはデバイスの存在に関連する他の現象を検出するように構成され得る。たとえば、いくつかの実施形態では、アシスタントデバイスの所与の1つは、たとえば、特定のユーザによって携帯/操作される他のアシスタントデバイス(たとえば、モバイルデバイス、ウェアラブルコンピューティングデバイスなど)および/またはエコシステムの中の他のアシスタントデバイス(たとえば、イベント検出エンジン130に関して説明される)によって放出される様々なタイプのワイヤレス信号(たとえば、無線波、超音波、電磁波などの波)を検出する、存在センサ105を装備し得る。たとえば、アシスタントデバイスの一部は、(たとえば、超音波対応マイクロフォンなどの超音波/赤外線受信機を介して)アシスタント入力デバイス106の1つまたは複数によって検出され得る、超音波または赤外波などの人が知覚できない波を放出するように構成され得る。 Additionally or alternatively, in some implementations, presence sensor 105 may be configured to detect other phenomena associated with the presence of a person or device in the ecosystem. For example, in some embodiments, a given one of the assistant devices is, for example, other assistant devices (e.g., mobile devices, wearable computing devices, etc.) and/or eco-friendly devices carried/operated by a particular user. Presence sensors that detect various types of wireless signals (e.g., radio, ultrasonic, electromagnetic, etc. waves) emitted by other assistant devices in the system (e.g., described with respect to the event detection engine 130) 105 can be equipped. For example, some of the assistant devices may be detected by one or more of the assistant input devices 106 (e.g., via an ultrasound/infrared receiver such as an ultrasound-enabled microphone), such as ultrasound or infrared waves. It can be configured to emit waves imperceptible to humans.
追加または代替として、様々なアシスタントデバイスは、特定のユーザによって携帯/操作される他のアシスタントデバイス(たとえば、モバイルデバイス、ウェアラブルコンピューティングデバイスなど)によって検出され、操作しているユーザの具体的な位置を決定するために使用され得る、無線波(たとえば、Wi-Fi、Bluetooth、セルラーなど)などの人が知覚できない他のタイプの波を放出し得る。いくつかの実装形態では、Wi-Fi三角測量が、たとえばアシスタントデバイスへ/からのWi-Fi信号に基づいてある人物の位置を検出するために使用され得る。他の実装形態では、特定のユーザによって携帯/操作される他のアシスタントデバイスによって放出される信号に基づいてある特定の人物の位置を決定するために、time-of-flight、信号強度などの他のワイヤレス信号特性が、単独でまたは集合的に、様々なアシスタントデバイスによって使用され得る。 Additionally or alternatively, various assistant devices may be detected by other assistant devices (e.g., mobile devices, wearable computing devices, etc.) carried/operated by a particular user and may be used to determine the specific location of the operating user. It may emit other types of waves that are imperceptible to humans, such as radio waves (eg, Wi-Fi, Bluetooth, cellular, etc.) that can be used to determine . In some implementations, Wi-Fi triangulation may be used to detect a person's location based on Wi-Fi signals to/from an assistant device, for example. In other implementations, other techniques such as time-of-flight, signal strength, etc. may be used to determine the location of a particular person based on signals emitted by other assistant devices carried/operated by the particular user. of wireless signal characteristics can be used, singly or collectively, by various assistant devices.
追加または代替として、いくつかの実装形態では、アシスタント入力デバイス106の1つまたは複数は、音声認識を実行してユーザの声からユーザを認識し得る。たとえば、自動化アシスタント120のいくつかのインスタンスは、たとえば様々なリソースへのアクセスを提供/制限する目的で、声をユーザのプロファイルと照合するように構成され得る。いくつかの実装形態では、次いで、たとえばアシスタントデバイスの存在センサ105によって、話者の動きが決定され得る。いくつかの実装形態では、そのような検出された動きに基づいて、ユーザの位置が予測されてもよく、アシスタントデバイスがユーザの位置に近いことに少なくとも一部基づいてそれらのアシスタントデバイスにおいて何らかのコンテンツがレンダリングさせられるとき、この位置がユーザの位置であると見なされてもよい。いくつかの実装形態では、ユーザは単に、ユーザが自動化アシスタント120と関わった最後の位置にいるものとして、特にその最後の関わりから時間があまり経っていない場合には見なされてもよい。 Additionally or alternatively, in some implementations, one or more of the assistant input devices 106 may perform voice recognition to recognize the user from the user's voice. For example, some instances of automated assistant 120 may be configured to match a voice to a user's profile, eg, for purposes of providing/restricting access to various resources. In some implementations, the speaker's movements may then be determined, for example by the presence sensor 105 of the assistant device. In some implementations, based on such detected motion, a user's location may be predicted, and some content on those assistant devices based at least in part on the proximity of the assistant device to the user's location. is rendered, this position may be considered to be the user's position. In some implementations, the user may simply be viewed as being in the last position the user interacted with the automated assistant 120, especially if not too much time has passed since that last interaction.
アシスタント入力デバイス106の各々はさらに、それぞれのユーザインターフェースコンポーネント1071-N(単に「ユーザインターフェースコンポーネント107」とも本明細書では呼ばれる)を含み、それらは各々、1つまたは複数のユーザインターフェース入力デバイス(たとえば、マイクロフォン、タッチスクリーン、キーボード)および/または1つまたは複数のユーザインターフェース出力デバイス(たとえば、ディスプレイ、スピーカー、プロジェクタ)を含み得る。一例として、アシスタント入力デバイス1061のユーザインターフェースコンポーネント1071は、スピーカーおよびマイクロフォンのみを含み得るが、アシスタント入力デバイス106Nのユーザインターフェースコンポーネント107Nは、スピーカー、タッチスクリーン、およびマイクロフォンを含み得る。追加または代替として、いくつかの実装形態では、アシスタント非入力デバイス185は、ユーザインターフェースコンポーネント107の1つまたは複数のユーザインターフェース入力デバイスおよび/または1つまたは複数のユーザインターフェース出力デバイスを含み得るが、アシスタント非入力デバイス185のためのユーザ入力デバイス(もしあれば)は、ユーザが自動化アシスタント120と直接対話することを可能にしないことがある。
Each of the assistant input devices 106 further includes respective user interface components 107 1-N (also referred to herein simply as "user interface components 107"), each of which includes one or more user interface input devices ( microphone, touch screen, keyboard) and/or one or more user interface output devices (eg, display, speakers, projector). As an example, user interface component 107 1 of assistant input device 106 1 may include only a speaker and a microphone, while user interface component 107 N of assistant input device 106 N may include a speaker, touch screen, and microphone. Additionally or alternatively, in some implementations, assistant
アシスタント入力デバイス106および/またはクラウドベースの自動化アシスタントコンポーネント119の1つまたは複数を動作させる任意の他のコンピューティングデバイスの各々は、データおよびソフトウェアアプリケーションの記憶のための1つまたは複数のメモリ、データにアクセスしてアプリケーションを実行するための1つまたは複数のプロセッサ、ならびにネットワークを介した通信を容易にする他のコンポーネントを含み得る。アシスタント入力デバイス106の1つまたは複数によって、および/または自動化アシスタント120によって実行される動作は、複数のコンピュータシステム間で分散され得る。自動化アシスタント120は、たとえば、ネットワーク(たとえば、図1のネットワーク110のいずれか)を通じて互いに結合される1つまたは複数の位置にある1つまたは複数のコンピュータ上で実行されるコンピュータプログラムとして実装され得る。 Each of the assistant input devices 106 and/or any other computing device that operates one or more of the cloud-based automated assistant components 119 has one or more memories for storage of data and software applications. and to run applications, as well as other components that facilitate communication over a network. The actions performed by one or more of assistant input devices 106 and/or by automated assistant 120 may be distributed among multiple computer systems. Automation assistant 120 may be implemented, for example, as a computer program running on one or more computers at one or more locations coupled together through a network (eg, any of network 110 in FIG. 1). .
上で述べられたように、様々な実装形態において、アシスタント入力デバイス106の各々は、それぞれの自動化アシスタントクライアント118を動作させ得る。様々な実装形態において、各自動化アシスタントクライアント118は、それぞれの発話捕捉/テキストトゥスピーチ(TTS)/スピーチトゥテキスト(STT)モジュール1141-N(本明細書では単に「発話捕捉/TTS/STTモジュール114」とも呼ばれる)を含み得る。他の実装形態では、それぞれの発話捕捉/TTS/STTモジュール114の1つまたは複数の態様は、それぞれの自動化アシスタントクライアント118とは別に実装され得る。 As noted above, in various implementations, each of the assistant input devices 106 may operate a respective automated assistant client 118 . In various implementations, each automated assistant client 118 has a respective speech capture/text-to-speech (TTS)/speech-to-text (STT) module 114 1-N (herein simply "speech capture/TTS/STT module"). 114”). In other implementations, one or more aspects of each speech capture/TTS/STT module 114 may be implemented separately from each automated assistant client 118 .
各々のそれぞれの発話捕捉/TTS/STTモジュール114は、たとえば、ユーザの発話を捉えること(発話捕捉、たとえばそれぞれのマイクロフォン(いくつかの場合には存在センサ105を備え得る)を介した)、MLモデルデータベース192に記憶されている発話認識モデルを使用して、その捉えられたオーディオをテキストおよび/もしくは他の表現もしくは埋め込み(STT)に変換すること、ならびに/または、MLモデルデータベース192に記憶されている発話合成モデルを使用してテキストを発話に変換すること(TTS)を含む、1つまたは複数の機能を実行するように構成され得る。これらのモデルのインスタンスは、それぞれのアシスタント入力デバイス106の各々にローカルに記憶され、かつ/または、アシスタント入力デバイスによって(たとえば、図1のネットワーク110を介して)アクセス可能であり得る。いくつかの実装形態では、アシスタント入力デバイス106の1つまたは複数は、計算リソース(たとえば、プロセッササイクル、メモリ、電池など)が比較的限られていることがあるので、アシスタント入力デバイス106の各々のローカルにあるそれぞれの発話捕捉/TTS/STTモジュール114は、発話認識モデルを使用して、有限の数の異なる話された語句をテキストに(または低次元埋め込みなどの他の形式に)変換するように構成されることがある。他の発話入力がクラウドベースの自動化アシスタントコンポーネント119の1つまたは複数に送信されてもよく、これらは、クラウドベースのTTSモジュール116および/またはクラウドベースのSTTモジュール117を含んでもよい。 Each respective speech capture/TTS/STT module 114, for example, captures user speech (speech capture, eg, via a respective microphone (which in some cases may comprise a presence sensor 105)), ML converting the captured audio into text and/or other representations or embeddings (STTs) using speech recognition models stored in model database 192 and/or speech recognition models stored in ML model database 192; It may be configured to perform one or more functions, including converting text to speech (TTS) using a speech synthesis model. Instances of these models may be stored locally on each respective assistant input device 106 and/or accessible by the assistant input devices (eg, via network 110 of FIG. 1). In some implementations, one or more of the assistant input devices 106 may have relatively limited computational resources (eg, processor cycles, memory, batteries, etc.), so each of the assistant input devices 106 Each local speech capture/TTS/STT module 114 is configured to convert a finite number of different spoken phrases to text (or other forms such as low-dimensional embeddings) using a speech recognition model. may be configured to Other speech inputs may be sent to one or more of cloud-based automated assistant components 119 , which may include cloud-based TTS module 116 and/or cloud-based STT module 117 .
クラウドベースのSTTモジュール117は、発話捕捉/TTS/STTモジュール114によって捉えられたオーディオデータを、MLモデルデータベース192に記憶されている発話認識モデルを使用してテキスト(これは次いで自然言語プロセッサ122に提供され得る)へと変換するために、クラウドの実質的に無限のリソースを活用するように構成され得る。クラウドベースのTTSモジュール116は、テキストデータ(たとえば、自動化アシスタント120によって編成されるテキスト)を、MLモデルデータベース192に記憶されている発話合成モデルを使用してコンピュータで生成される発話出力へと変換するために、クラウドの実質的に無限のリソースを活用するように構成され得る。いくつかの実装形態では、クラウドベースのTTSモジュール116は、たとえばそれぞれのアシスタントデバイスのそれぞれのスピーカーを使用して直接出力されるように、コンピュータで生成された発話の出力をアシスタントデバイスの1つまたは複数に提供し得る。他の実装形態では、クラウドベースのTTSモジュール116を使用して自動化アシスタント120によって生成されるテキストデータ(たとえば、コマンドに含まれるクライアントデバイス通知)は、それぞれのアシスタントデバイスの発話捕捉/TTS/STTモジュール114に提供されてもよく、発話捕捉/TTS/STTモジュール114は次いで、発話合成モデルを使用してテキストデータをコンピュータで生成された発話へとローカルで変換し、コンピュータで生成された発話がそれぞれのアシスタントデバイスのローカルのスピーカーを介してレンダリングされるようにしてもよい。 The cloud-based STT module 117 converts the audio data captured by the speech capture/TTS/STT module 114 into text (which is then sent to the natural language processor 122) using speech recognition models stored in the ML model database 192. It can be configured to leverage the virtually limitless resources of the cloud to transform into . The cloud-based TTS module 116 converts text data (eg, text organized by the automation assistant 120) into computer-generated speech output using speech synthesis models stored in the ML model database 192. It can be configured to leverage the virtually limitless resources of the cloud to do so. In some implementations, the cloud-based TTS module 116 directs the output of the computer-generated speech to one of the assistant devices, e.g., directly using the respective speaker of the respective assistant device. Can provide multiple. In other implementations, text data generated by automated assistants 120 using cloud-based TTS modules 116 (e.g., client device notifications included in commands) are sent to the respective assistant device's speech capture/TTS/STT modules. 114, the speech capture/TTS/STT module 114 then locally transforms the text data into computer-generated utterances using the utterance synthesis model, and the computer-generated utterances are respectively may be rendered through the local speakers of your assistant device.
自動化アシスタント120(および特に、1つまたは複数のクラウドベースの自動化アシスタントコンポーネント119)は、自然言語処理(NLP)モジュール122、前述のクラウドベースのTTSモジュール116、前述のクラウドベースのSTTモジュール117、および他のコンポーネントを含んでもよく、それらの一部が以下でより詳しく説明される。いくつかの実装形態では、自動化アシスタント120のエンジンおよび/またはモジュールの1つまたは複数が、省略され、組み合わせられ、かつ/または自動化アシスタント120とは別のコンポーネントにおいて実装されてもよい。NLPモジュール122のインスタンスは、追加または代替として、アシスタント入力デバイス106においてローカルで実装され得る。 The automation assistant 120 (and in particular one or more cloud-based automation assistant components 119) includes a natural language processing (NLP) module 122, the aforementioned cloud-based TTS module 116, the aforementioned cloud-based STT module 117, and Other components may be included, some of which are described in more detail below. In some implementations, one or more of the engines and/or modules of automation assistant 120 may be omitted, combined, and/or implemented in separate components from automation assistant 120. An instance of NLP module 122 may additionally or alternatively be implemented locally on assistant input device 106 .
いくつかの実装形態では、自動化アシスタント120は、自動化アシスタント120との人のコンピュータの対話セッションの間にアシスタント入力デバイス106の1つのユーザによって生成される様々な入力に応答して、応答コンテンツを生成する。自動化アシスタント120は、アシスタント入力デバイス106および/またはアシスタント非入力デバイス185を介した対話セッションの一部として、ユーザへの提示のために応答コンテンツを(たとえば、アシスタントデバイスとは別であるとき、図1のネットワーク110の1つまたは複数を介して)提供し得る。たとえば、自動化アシスタント120は、アシスタント入力デバイス106の1つを介して提供される自由形式の自然言語入力に応答して、応答コンテンツを生成し得る。本明細書において使用される場合、自由形式の入力は、ユーザによる選択のために提示される選択肢のグループに制約されない、ユーザによって編成される入力である。 In some implementations, the automated assistant 120 generates responsive content in response to various inputs generated by one user of the assistant input device 106 during a human computer interaction session with the automated assistant 120. do. Automated assistant 120 may send responsive content for presentation to the user as part of an interactive session via assistant input device 106 and/or assistant non-input device 185 (e.g., when separate from the assistant device, in FIG. via one or more of one's networks 110). For example, automated assistant 120 may generate responsive content in response to free-form natural language input provided via one of assistant input devices 106 . As used herein, free-form input is user-organized input that is not constrained to a group of choices presented for selection by the user.
自動化アシスタント120のNLPモジュール122は、アシスタント入力デバイス106を介してユーザによって生成される自然言語入力を処理し、自動化アシスタント120、アシスタント入力デバイス106、および/またはアシスタント非入力デバイス185の1つまたは複数の他のコンポーネントによる使用のために、アノテートされた出力を生成し得る。たとえば、NLPモジュール122は、アシスタント入力デバイス106の1つまたは複数のそれぞれのユーザインターフェース入力デバイスを介してユーザによって生成される、自由形式の自然言語入力を処理し得る。自由形式の自然言語入力を処理したことに基づいて生成されるアノテートされた出力は、自然言語入力の1つまたは複数のアノテーション、および任意選択で、自然言語入力の語の1つまたは複数(たとえば、すべて)を含み得る。
The NLP module 122 of the automated assistant 120 processes natural language input generated by the user via the assistant input device 106 and one or more of the automated assistant 120, the assistant input device 106, and/or the assistant
いくつかの実装形態では、NLPモジュール122は、自然言語入力の中の様々なタイプの文法情報を特定してアノテートするように構成される。たとえば、NLPモジュール122は、語をその文法上の役割でアノテートするように構成される、発話タガーの一部を含み得る。いくつかの実装形態では、NLPモジュール122は、追加および/または代替として、人々(たとえば、文学の登場人物、有名人、公人などを含む)、組織、位置(現実のおよび想像上の)などへの言及などの、1つまたは複数のセグメントにおけるエンティティへの言及をアノテートするように構成される、エンティティタガー(図示されない)を含み得る。いくつかの実装形態では、エンティティについてのデータは、ナレッジグラフ(図示されない)などの1つまたは複数のデータベースに記憶され得る。いくつかの実装形態では、ナレッジグラフは、知られているエンティティ(および場合によっては、エンティティ属性)を表すノード、ならびに、ノードを接続してエンティティ間の関係を表すエッジを含み得る。 In some implementations, the NLP module 122 is configured to identify and annotate various types of grammatical information in natural language input. For example, NLP module 122 may include a portion of speech taggers configured to annotate words with their grammatical roles. In some implementations, the NLP module 122 may additionally and/or alternatively identify people (e.g., including literary characters, celebrities, public figures, etc.), organizations, locations (real and imaginary), etc. may include an entity tagger (not shown) configured to annotate references to entities in one or more segments, such as mentions of . In some implementations, data about entities may be stored in one or more databases, such as a knowledge graph (not shown). In some implementations, a knowledge graph may include nodes representing known entities (and possibly entity attributes) and edges connecting the nodes to represent relationships between the entities.
NLPモジュール122のエンティティタガーは、高いレベルの粒度で(たとえば、人々などのあるエンティティクラスへのすべての言及の特定を可能にするために)、および/または低いレベルの粒度で(たとえば、特定の人物などの特定のエンティティへのすべての言及の特定を可能にするために)、あるエンティティへの言及をアノテートし得る。エンティティタガーは、特定のエンティティについて解決するために自然言語入力の内容に頼ってもよく、かつ/または、特定のエンティティについて解決するためにナレッジグラフもしくは他のエンティティデータベースと任意選択で通信してもよい。 Entity taggers in NLP module 122 can be used at a high level of granularity (e.g., to allow identification of all mentions of an entity class such as people) and/or at a low level of granularity (e.g., specific References to a certain entity may be annotated (to enable identification of all references to a particular entity, such as a person). Entity taggers may rely on the content of natural language input to resolve for specific entities and/or may optionally communicate with a knowledge graph or other entity database to resolve for specific entities. good.
いくつかの実装形態では、NLPモジュール122は、追加および/または代替として、1つまたは複数の文脈上の合図に基づいて、言及を同じエンティティへとグループ化する、または「群がらせる」ように構成される、共参照解決器(図示されない)を含み得る。たとえば、共参照解決器は、自然言語入力「lock it」を受け取る直前にレンダリングされたクライアントデバイス通知において「front door lock」が言及されていることに基づいて、自然言語入力「lock it」における語「it」を「front door lock」へと解決するために利用され得る。 In some implementations, the NLP module 122 is additionally and/or alternatively configured to group or "clump" mentions into the same entity based on one or more contextual cues. may include a coreference resolver (not shown) that For example, the co-reference resolver may find the word It can be used to resolve "it" to "front door lock".
いくつかの実装形態では、NLPモジュール122の1つまたは複数のコンポーネントは、NLPモジュール122の1つまたは複数の他のコンポーネントからのアノテーションに頼り得る。たとえば、いくつかの実装形態では、指定されたエンティティタガーは、特定のエンティティへのすべての言及をアノテートする際に、共参照解決器および/または依存関係分析器からのアノテーションに頼ってもよい。また、たとえば、いくつかの実装形態では、共参照解決器は、同じエンティティへの言及を群がらせる際に、依存関係解析器からのアノテーションに頼ってもよい。いくつかの実装形態では、特定の自然言語入力を処理する際、NLPモジュール122の1つまたは複数のコンポーネントは、アシスタント入力デバイス通知が基づく自然言語入力を受け取る直前にレンダリングされたアシスタント入力デバイス通知などの、特定の自然言語入力の外側の関連するデータを使用して、1つまたは複数のアノテーションを決定してもよい。 In some implementations, one or more components of NLP module 122 may rely on annotations from one or more other components of NLP module 122 . For example, in some implementations, a given entity tagger may rely on annotations from a coreference resolver and/or dependency analyzer in annotating all references to a particular entity. Also, for example, in some implementations, the coreference resolver may rely on annotations from the dependency analyzer in clustering references to the same entity. In some implementations, when processing a particular natural language input, one or more components of the NLP module 122 may generate an assistant input device notification rendered immediately prior to receiving the natural language input on which the assistant input device notification is based, such as an assistant input device notification. , the associated data outside of the specific natural language input may be used to determine one or more annotations.
図1は、アシスタントデバイスおよび/またはサーバによって実装されるコンポーネントの特定の構成を有するものとして図示され、特定のネットワークを介して通信するアシスタントデバイスおよび/またはサーバを有するものとして図示されるが、それは例示のためであり、限定することは意図されないことを理解されたい。たとえば、アシスタント入力デバイス106およびアシスタント非入力デバイスは直接、1つまたは複数のネットワーク(図示せず)を介して互いに通信可能に直接結合され得る。別の例として、1つまたは複数のクラウドベースの自動化アシスタントコンポーネント119の動作は、アシスタント入力デバイス106のうちの1つまたは複数および/またはアシスタント非入力デバイスの1つまたは複数においてローカルに実装され得る。さらに別の例として、MLモデルデータベース192に記憶されている様々なMLモデルのインスタンスは、アシスタントデバイスにおいてローカルに記憶されてもよく、および/または、デバイストポロジーデータベース193に記憶されているエコシステムのデバイストポロジー表現のインスタンスは、アシスタント入力デバイスにローカルに記憶されてもよい。さらに、データ(たとえば、デバイス活動、それに対応するオーディオデータもしくは認識されるテキスト、デバイストポロジー表現、および/または本明細書において説明される任意の他のデータ)が図1の1つまたは複数のネットワーク110のいずれかを介して送信される実装形態では、データは暗号化され、フィルタリングされ、またはユーザのプライバシーを確保するために任意の方式で別様に保護され得る。 Although FIG. 1 is illustrated as having a particular configuration of components implemented by an assistant device and/or server, and illustrated as having the assistant device and/or server communicating over a particular network, it It should be understood that this is for illustration and is not intended to be limiting. For example, assistant input device 106 and assistant non-input device may be directly communicatively coupled to each other via one or more networks (not shown). As another example, operations of one or more cloud-based automated assistant components 119 may be implemented locally in one or more of the assistant input devices 106 and/or one or more of the assistant non-input devices. . As yet another example, the various ML model instances stored in the ML model database 192 may be stored locally on the assistant device and/or in the ecosystem stored in the device topology database 193. An instance of the device topology representation may be stored locally on the assistant input device. Additionally, data (e.g., device activity, corresponding audio data or recognized text, device topology representations, and/or any other data described herein) may be stored in one or more networks of FIG. In implementations transmitted via any of 110, the data may be encrypted, filtered, or otherwise protected in any manner to ensure user privacy.
本明細書において説明される技法を使用して、エコシステムの中の多様なアシスタントデバイスを使用して音響イベントの発生を検出して検証することによって、音響イベントのフォールスポジティブの量を減らすことができる。結果として、計算リソースとネットワークリソースの両方を節約することができる。たとえば、エコシステムにおけるホットワードイベントの発生を検出して検証する際に本明細書において説明される技法を使用することによって、自動化アシスタントのコンポーネントおよび/または機能を、それらがアクティブにされることが意図されていなかったとしてもこれらの技法がなければアクティブにされていた可能性があるときに、休止状態のままにすることができる。別の例として、エコシステムの中の特定の音の発生を検出して検証する際に本明細書において説明される技法を使用することによって、エコシステムに関連するユーザに提示される通知を、実際には音響イベントが発生しなかったにもかかわらずこれらの技法がなければ提示された可能性があるときに、差し控えることができる。その上、エコシステムの中のどのアシスタントデバイスが、音響イベントを捉える時間的に対応するオーディオデータを検出するはずであるかを予想するために、本明細書において説明される技法を使用することによって、オーディオデータを処理するアシスタントデバイスの量を減らすことができる。また結果として、計算リソースとネットワークリソースの両方を節約することができる。たとえば、音響イベントを検出したはずであるエコシステムの中のアシスタントデバイスを予想する際に本明細書において説明される技法を使用することによって、音響イベントに対応しない可能性が高い時間的に対応するオーディオデータを捉えた可能性のある他のアシスタントデバイスが、音響イベントが実際の音響イベントであるかどうかを決定する際に考慮されなくてもよい。 Using the techniques described herein, the amount of false positives in acoustic events can be reduced by detecting and verifying the occurrence of acoustic events using various assistant devices in the ecosystem. can. As a result, both computational and network resources can be saved. For example, by using the techniques described herein in detecting and verifying the occurrence of hotword events in the ecosystem, automation assistant components and/or functions can It can remain dormant when it might have been activated without these techniques, even though it was not intended. As another example, by using the techniques described herein in detecting and verifying the occurrence of particular sounds in the ecosystem, notifications presented to users associated with the ecosystem can be: It can be withheld when no acoustic event actually occurred but could have been presented without these techniques. Moreover, by using the techniques described herein to predict which assistant devices in the ecosystem should detect the temporally corresponding audio data that captures the acoustic event, , can reduce the amount of assistant devices to process audio data. Also, as a result, both computational and network resources can be saved. For example, by using the techniques described herein in anticipating assistant devices in the ecosystem that would have detected an acoustic event, temporally corresponding Other assistant devices that may have captured the audio data may not be considered in determining whether the sound event is an actual sound event.
図1の様々なコンポーネントの追加の説明がここで、図2A、図2B、および図3を参照して与えられる。ある家の間取りが図2A、図2B、および図3に図示されている。図示される間取りは、複数の部屋250～262を含む。複数のアシスタント入力デバイス1061-5が、部屋の少なくともいくつかにわたって配備されている。アシスタント入力デバイス1061-5の各々は、本開示の選択された態様を用いて構成される自動化アシスタントクライアント118のインスタンスを実装してもよく、近くの人により話される発話を捉えることが可能なマイクロフォンなどの1つまたは複数の入力デバイスを含んでもよい。たとえば、双方向型スタンドアロンスピーカーおよび表示デバイス(たとえば、表示画面、プロジェクタなど)の形態をとる第1のアシスタント入力デバイス1061が、この例では台所である部屋250に配備されている。いわゆる「スマート」テレビジョンの形態をとる第2のアシスタント入力デバイス1062(たとえば、自動化アシスタントクライアント118のそれぞれのインスタンスを実装する1つまたは複数のプロセッサを伴うネットワーク接続されたテレビジョン)が、この例では書斎である部屋252に配備されている。ディスプレイのない双方向型スタンドアロンスピーカーの形態をとる第3のアシスタント入力デバイス1063が、この例では寝室である部屋254に配備されている。別の双方向型スタンドアロンスピーカーの形態をとる第4のアシスタント入力デバイス1064が、この例では居間である部屋256に配備されている。スマートテレビジョンの形態を同様にとる第5のアシスタント入力デバイス1065も、この例では台所である部屋250に配備されている。
Additional description of the various components of FIG. 1 will now be provided with reference to FIGS. 2A, 2B, and 3. FIG. A house layout is illustrated in FIGS. 2A, 2B, and 3. FIG. The illustrated floor plan includes multiple rooms 250-262. Multiple assistant input devices 106 1-5 are deployed throughout at least some of the rooms. Each of the assistant input devices 106 1-5 may implement an instance of an automated assistant client 118 configured using selected aspects of the present disclosure, capable of capturing speech spoken by a nearby person. may include one or more input devices such as a microphone. For example, a first assistant input device 1061 in the form of an interactive stand-alone speaker and display device (eg, display screen, projector, etc.) is deployed in
図2A、図2B、および図3には示されていないが、複数のアシスタント入力デバイス1061-4は、1つまたは複数の有線またはワイヤレスのWANおよび/またはLANを介して(たとえば、図1のネットワーク110を介して)、互いにおよび/または他のリソース(たとえば、インターネット)と通信可能に結合され得る。加えて、他のアシスタント入力デバイス、特に、スマートフォン、タブレット、ラップトップ、ウェアラブルデバイスなどの特定のモバイルデバイスも存在してもよく、たとえば、家の中で1人または複数の人により携帯されてもよく、それらも同じWANおよび/またはLANに接続されてもよく、またはされなくてもよい。図2A、図2B、および図3に図示されるアシスタント入力デバイスの構成は1つの例にすぎず、より多数もしくは少数の、および/または異なるアシスタント入力デバイス106が、家の任意の数の他の部屋および/もしくはエリアにわたって、ならびに/または、住宅以外の位置(たとえば、事業所、ホテル、公共の場所、空港、車両、および/または他の位置もしくは空間)に配備されてもよい。 Although not shown in FIGS. 2A, 2B, and 3, multiple assistant input devices 106 1-4 may be connected via one or more wired or wireless WANs and/or LANs (eg, FIG. 1 network 110), may be communicatively coupled to each other and/or to other resources (eg, the Internet). In addition, other assistant input devices may also be present, in particular certain mobile devices such as smartphones, tablets, laptops, wearable devices, for example carried by one or more people in the house. Well, they too may or may not be connected to the same WAN and/or LAN. The configurations of assistant input devices illustrated in FIGS. 2A, 2B, and 3 are but one example, and more, fewer, and/or different assistant input devices 106 may be used in any number of other homes. It may be deployed across rooms and/or areas and/or in non-residential locations (eg, businesses, hotels, public places, airports, vehicles, and/or other locations or spaces).
複数のアシスタント非入力デバイス1851-5が、図2A、図2B、および図3にさらに示されている。たとえば、スマート呼び鈴の形態をとる第1のアシスタント非入力デバイス1851が、家の正面玄関の近くに、家の外部に配備されている。スマートロックの形態をとる第2のアシスタント非入力デバイス1852が、自宅の正面玄関に接して、家の外部に配備されている。スマート洗濯機の形態をとる第3のアシスタント非入力デバイス1853が、この例では洗濯室である部屋262に配備されている。扉開閉センサの形態をとる第4のアシスタント非入力デバイス1854が、部屋262の裏口の近くに配備されており、裏口が開いているかまたは閉まっているかを検出する。スマートサーモスタットの形態をとる第5のアシスタント非入力デバイス1855が、この例では書斎である部屋252に配備されている。
A number of assistant
アシスタント非入力デバイス185の各々は、それぞれのアシスタント非入力システム180(図1に示されている)と(たとえば、図1のネットワーク110を介して)通信して、データをそれぞれのアシスタント非入力システム180に提供し、任意選択でデータがそれぞれのアシスタント非入力システム180によって提供されるコマンドに基づいて制御されるようにすることができる。アシスタント非入力デバイス185の1つまたは複数は、追加または代替として、アシスタント入力デバイス106の1つまたは複数と直接(たとえば、図1のネットワーク110を介して)通信して、データをアシスタント入力デバイス106の1つまたは複数に提供し、任意選択でデータがアシスタント入力デバイス106の1つまたは複数によって提供されるコマンドに基づいて制御されるようにすることができる。図2A、図2B、および図3に示されるアシスタント非入力デバイス185の構成は一例にすぎず、より多数もしくは少数の、および/または異なるアシスタント非入力デバイス185が、家の任意の数の他の部屋および/もしくはエリアにわたって、ならびに/または、住宅以外の位置(たとえば、事業所、ホテル、公共の場所、空港、車両、および/または他の位置もしくは空間)に配備されてもよい。
Each of the assistant
様々な実装形態において、音響イベントは、エコシステムの中に併置される多様なアシスタントデバイス(たとえば、アシスタント入力デバイス106、および/またはそれぞれのマイクロフォンを含むアシスタント非入力デバイス185のいずれか)において検出され得る。たとえば、音響イベントは、エコシステムの中の所与のアシスタントデバイスのマイクロフォンを介して検出されるオーディオデータにおいて取り込まれてもよく、エコシステムの中の少なくとも1つの追加のアシスタントのそれぞれのマイクロフォンを介して検出されるそれぞれのオーディオデータにおいても捉えられてもよい。アシスタントデバイスによって捉えられるオーディオデータは、音響イベントに関連するそれぞれの尺度を生成するために、それぞれのアシスタントデバイスにおいてローカルに記憶されている、および/または遠隔でサーバに記憶され、それぞれのアシスタントデバイスによってアクセス可能である、イベント検出モデルを使用して処理され得る。さらに、これらの尺度の各々は、音響イベントがエコシステムにおいて発生した実際の音響イベントに対応するかどうかを決定するために処理され得る。音響イベントが実際の音響イベントである場合、エコシステムにおける実際の音響イベントの発生に基づいて活動を実行することができる。そうではない場合、オーディオデータを廃棄することができる。
In various implementations, acoustic events are detected at various assistant devices collocated in the ecosystem (eg, either assistant input device 106 and/or assistant
いくつかの実装形態では、アシスタントデバイス(たとえば、所与のアシスタントデバイスおよび少なくとも1つの追加のアシスタントデバイス)の1つまたは複数は、これらのアシスタントデバイスの各々において検出されるそれぞれのオーディオデータが音響イベントを捉えるかどうかを示すそれぞれの尺度を生成するために、それぞれのアシスタントデバイスにおいてローカルに、それぞれのイベント検出モデルを使用して、それぞれのオーディオデータを処理することができる。いくつかの追加または代替の実装形態では、アシスタントデバイスの1つまたは複数は各々、これらのアシスタントデバイスの各々において検出されるそれぞれのオーディオデータが音響イベントを捉えるかどうかを示すそれぞれの尺度を生成するために、エコシステムの中の所与のアシスタントデバイス(たとえば、オーディオデータを検出したアシスタントデバイスまたは何らオーディオデータを検出しなかった別個のアシスタントデバイスのうちの1つ)にオーディオデータを送信して、所与のアシスタントデバイスにおいてローカルに、イベント検出モデルを使用して、それぞれのオーディオデータを処理することができる。いくつかの追加または代替の実装形態では、アシスタントデバイスの1つまたは複数は各々、これらのアシスタントデバイスの各々において検出されるそれぞれのオーディオデータが音響イベントを捉えるかどうかを示すそれぞれの尺度を生成するために、エコシステムとは別個の遠隔システムにオーディオデータを送信して、遠隔システムにおいて遠隔で、イベント検出モデルを使用して、それぞれのオーディオデータを処理することができる。 In some implementations, one or more of the assistant devices (e.g., a given assistant device and at least one additional assistant device) interprets the respective audio data detected in each of these assistant devices as an acoustic event. The respective audio data can be processed using the respective event detection models locally on the respective assistant devices to generate respective measures indicating whether to capture the . In some additional or alternative implementations, one or more of the assistant devices each generate a respective measure indicating whether the respective audio data detected at each of these assistant devices captures the acoustic event. to send audio data to a given assistant device in the ecosystem (e.g., one of the assistant devices that detected audio data or a separate assistant device that did not detect any audio data), Locally on a given assistant device, the event detection model can be used to process the respective audio data. In some additional or alternative implementations, one or more of the assistant devices each generate a respective measure indicating whether the respective audio data detected at each of these assistant devices captures the acoustic event. For this purpose, the audio data can be sent to a remote system separate from the ecosystem, and the respective audio data can be processed remotely at the remote system using an event detection model.
いくつかの実装形態では、エコシステムの中の少なくとも1つの追加のアシスタントデバイスは、所与のアシスタントデバイスが音響イベントを捉えるオーディオデータを検出したことに応答して特定され得る。所与のアシスタントデバイスにおいて検出される音響イベントが実際に本物の音響イベントである場合、エコシステムにおいて特定される少なくとも1つの追加のアシスタントデバイスは、同様に音響イベントを捉える時間的に対応するオーディオデータを検出したはずである。アシスタントデバイスによって捉えられるオーディオデータは、たとえば、アシスタントデバイスによって捉えられるオーディオデータと関連付けられるそれぞれのタイムスタンプに基づいて、時間的に対応するオーディオデータであると見なされ得る。たとえば、オーディオデータは、タイムスタンプが一致するとき、またはそれらが互いに閾値の時間長以内(たとえば、数ミリ秒、数秒、または任意の他の適切な長さの時間内)にあるとき、時間的に対応するオーディオデータであると見なされ得る。それらの実装形態のいくつかのバージョンでは、少なくとも1つの追加のアシスタントデバイスは、たとえば、所与のアシスタントデバイスおよび少なくとも1つの追加のアシスタントデバイスが、同じ音響イベントを捉えるオーディオデータを過去に検出したことに基づいて特定され得る。それらの実装形態のいくつかの追加または代替のバージョンでは、少なくとも1つの追加のアシスタントデバイスは、たとえば、所与のアシスタントデバイスおよび少なくとも1つの追加のアシスタントデバイスがエコシステムのデバイストポロジー表現においてアシスタントデバイスの同じグループに属することに基づいて特定され得る。 In some implementations, at least one additional assistant device in the ecosystem may be identified in response to a given assistant device detecting audio data capturing the acoustic event. If the sound event detected in a given assistant device is in fact a real sound event, at least one additional assistant device identified in the ecosystem will also generate temporally corresponding audio data that similarly captures the sound event. should have detected The audio data captured by the assistant device may be considered temporally corresponding audio data, eg, based on respective timestamps associated with the audio data captured by the assistant device. For example, audio data is temporally separated when the timestamps match, or when they are within a threshold length of time of each other (e.g., within a few milliseconds, seconds, or any other suitable length of time). can be considered to be audio data corresponding to . In some versions of those implementations, the at least one additional assistant device detects, for example, that the given assistant device and the at least one additional assistant device have previously detected audio data that captures the same acoustic event. can be identified based on In some additional or alternative versions of those implementations, the at least one additional assistant device is, for example, a given assistant device and the at least one additional assistant device is the assistant device in the device topology representation of the ecosystem. They can be identified based on belonging to the same group.
いくつかの実装形態では、音響イベントはホットワードイベントであり、イベント検出モデルはホットワード検出モデルに対応する。ホットワードイベントは、検出されると自動化アシスタントの1つまたは複数のコンポーネントまたは機能がアクティブにされるようにする、特定の語または語句を検出することに対応し得る。たとえば、図2Aおよび図2Bを特に参照すると、図2Aおよび図2Bに示されるエコシステムと関連付けられるユーザ101が、エコシステムの中の多様なアシスタントデバイスによって検出される「ヘイアシスタント...」という発話を提供すると仮定する。たとえば、図2Aに示されるように、ユーザ101は、発話が与えられるとき、この例では台所である部屋250に位置し得る。この例では、「ヘイアシスタント...」という発話に対応するオーディオデータが第1のアシスタント入力デバイス1061において検出される(たとえば、発話に対応する吹き出しから第1のアシスタント入力デバイス1061への破線により示されるように)と仮定する。さらに、第5のアシスタント入力デバイス1065および第5のアシスタント非入力デバイス1855(スマートサーモスタットがマイクロフォンを含むと仮定する)も、(たとえば、やはり破線により示されるように)ホットワードを捉えるオーディオデータを検出することが予想されるとさらに仮定する。この例では、第5のアシスタント入力デバイス1065は、第1のアシスタント入力デバイス1061および第5のアシスタント入力デバイス1065がデバイスの同じグループ(たとえば、「台所」グループ)に含まれることに基づいて、ならびに/または、第1のアシスタント入力デバイス1061および第5のアシスタント入力デバイス1065が同じ発話を捉える時間的に対応するオーディオデータを以前に検出したことがあることに基づいて、オーディオデータを検出することが予想され得る。さらに、第5のアシスタント非入力デバイス1855は、第1のアシスタント入力デバイス1061および第5のアシスタント非入力デバイス1855が同じ発話を捉える時間的に対応するオーディオデータを以前に検出したことがあることに基づいて(たとえば、第1のアシスタント入力デバイス1061が「台所」グループに属し、第5のアシスタント非入力デバイス1855が「寝室」グループに属すると仮定して)、オーディオデータを検出することが予想され得る。
In some implementations, the acoustic event is a hotword event and the event detection model corresponds to the hotword detection model. A hotword event may correspond to detecting a particular word or phrase that, when detected, causes one or more components or functions of the automated assistant to be activated. For example, with particular reference to FIGS. 2A and 2B,
その上、第1のアシスタント入力デバイス1061において検出されるオーディオデータに基づいて生成される尺度が、オーディオデータがホットワードを捉えることを示すと仮定し、第5のアシスタント入力デバイス1065において検出されるオーディオデータに基づいて生成される尺度は、オーディオデータがホットワードを捉えることを示すが、第5のアシスタント非入力デバイス1855において検出されるオーディオデータに基づいて生成される尺度は、オーディオデータがホットワードを捉えないことを示すと仮定する。この例では、エコシステムにおいて検出されるホットワードが実際にホットワードであると決定されてもよく、オーディオデータがホットワードを捉えないことを第5のアシスタント非入力デバイス1855が示していても(または第5のアシスタント非入力デバイス1855がまったくオーディオデータを検出しなかった場合でも)、自動化アシスタントの1つまたは複数の構成要素または機能をアクティブにすることができ、それは、ホットワードが発話において捉えられたことを他のアシスタントデバイスが示すからである。対照的に、第1のアシスタント入力デバイス1061において検出されるオーディオデータに基づいて生成される尺度は、オーディオデータがホットワードを捉えることを示すが、第5のアシスタント入力デバイス1065および第5のアシスタント非入力デバイス1855において検出されるオーディオデータに基づいて生成される尺度は、オーディオデータがホットワードを捉えないことを示すと仮定する。この例では、エコシステムにおいて検出されるホットワードが実際にはホットワードではないと決定されることがあり、オーディオデータがホットワードを捉えることを第1のアシスタント入力デバイス1061が示していても、自動化アシスタントの1つまたは複数のコンポーネントまたは機能を休止状態のままにすることができ、それは、ホットワードが発話において捉えられなかったことを他のアシスタントデバイスが示すからである。
Moreover, assuming that the measure generated based on the audio data detected at the first assistant input device 106 1 indicates that the audio data captures the hot word, the detected at the fifth assistant input device 106 5 A measure generated based on audio data detected at the fifth
別の例として、図2Bに示されるように、ユーザ101は、この例では書斎である、発話が与えられる部屋252に位置し得る。この例では、「ヘイアシスタント...」という発話に対応するオーディオデータが第1のアシスタント入力デバイス1061において検出される(たとえば、第1のアシスタント入力デバイス1061への発話に対応する吹き出しからの破線により示されるように)と仮定する。第5のアシスタント非入力デバイス1855(スマートサーモスタットがマイクロフォンを含むと仮定する)も、ホットワードを捉えるオーディオデータを検出することが(たとえば、破線によっても示されるように)予想されると、さらに仮定する。この例では、第5のアシスタント非入力デバイス1855は、第2のアシスタント入力デバイス1062および第5のアシスタント非入力デバイス1855がデバイスの同じグループ(たとえば、「書斎」グループ)に含まれることに基づいて、ならびに/または、第2のアシスタント入力デバイス1065および第5のアシスタント非入力デバイス1855が同じ発話を捉える時間的に対応するオーディオデータを以前に検出したことがあることに基づいて、オーディオデータを検出することが予想され得る。
As another example, as shown in FIG. 2B,
その上、第2のアシスタント入力デバイス1062において検出されるオーディオデータに基づいて生成される尺度が、オーディオデータがホットワードを捉えることを示すと仮定し、第5のアシスタント非入力デバイス1855において検出されるオーディオデータに基づいて生成される尺度が、オーディオデータがホットワードを捉えることを示すと仮定する。この例では、エコシステムにおいて検出されるホットワードが実際にホットワードであると決定されてもよく、自動化アシスタントの1つまたは複数のコンポーネントまたは機能をアクティブにすることができる。対照的に、第2のアシスタント入力デバイス1062において検出されるオーディオデータに基づいて生成される尺度は、オーディオデータがホットワードを捉えることを示すが、第5のアシスタント非入力デバイス1855において検出されるオーディオデータに基づいて生成される尺度は、オーディオデータがホットワードを捉えないことを示すと仮定する。この例では、エコシステムにおいて検出されるホットワードが実際にホットワードであると決定されるかどうかは、第5のアシスタント非入力デバイス1855において検出されるオーディオデータに基づいて生成される尺度が、その尺度がホットワードを示すものであることを示す閾値の範囲内にあるかどうかに基づき得る。たとえば、ホットワードイベントに関連する尺度は確率であると仮定する。第2のアシスタント入力デバイス1062によって検出されるオーディオデータに基づいて生成される尺度は0.7であり、第5のアシスタント非入力デバイス1855によって検出されるオーディオデータに基づいて生成される尺度は0.6であり、ホットワードが実際のホットワードであると決定するための閾値は0.65であるとさらに仮定する。結果として、音響イベントはホットワードであると見なされないことがあり、それは、第2のアシスタント入力デバイス1062に関連する尺度はホットワードの発生を示すが、第5のアシスタント非入力デバイス1062に関連する尺度はホットワードの発生を示さないからである。しかしながら、第5のアシスタント非入力デバイス1855によって検出されるオーディオデータに基づいて生成される尺度(たとえば、0.6)が閾値(たとえば、0.65)から0.1以内にある場合、音響イベントはホットワードであると見なされなくてもよいとさらに仮定する。したがって、ホットワードがエコシステムにおいて検出されたことを両方のデバイスが確信していなくても、音響イベントはホットワードの発生であると見なされ得る。対照的に、第5のアシスタント非入力デバイス1855によって検出されるオーディオデータに基づいて生成される尺度が0.1であると仮定する。この例では、第2のアシスタント入力デバイス1062によって検出されるオーディオデータに基づいて生成される尺度がホットワードを示す0.7であっても、音響イベントはホットワードの発生であると見なされず、それは、第5のアシスタント非入力デバイス1855によって検出されるオーディオデータに基づいて生成される尺度が、閾値(たとえば、0.65)を満たさず、閾値の範囲内にないからである。これらの例では、ホットワードイベントが実際のホットワードイベントであるかどうかを決定するために、尺度の追加または代替の処理が利用され得る。たとえば、アシスタントデバイスにおいて検出されるそれぞれのオーディオデータに基づいて生成される尺度の平均、中央値、最高値、最低値、またはパーセンタイルの1つまたは複数が、ホットワードイベントが実際のホットワードイベントであるかどうかを決定するために利用され得る。尺度の1つまたは複数は任意選択で、たとえば、ユーザ101への近接度(たとえば、図1の存在センサ105を使用して決定される)、アシスタントデバイスのコンポーネントのタイプ(たとえば、複数のマイクロフォンを有するアシスタントデバイスvs単一のマイクロフォンを有するアシスタントデバイス)に基づいて、および/または他の要因に基づいて、これらの統計の1つまたは複数を決定する際に重み付けられ得る。
Moreover, assuming that the measure generated based on the audio data detected at the second assistant input device 106 2 indicates that the audio data captures the hot word, at the fifth
図2Aおよび図2Bは、自動化アシスタントコンポーネントまたは機能の1つまたは複数をアクティブにするホットワードイベントが音響イベントであることに関して本明細書において説明されるが、これは例示のためであり、限定することは意図されないことを理解されたい。追加または代替として、ホットワードフリーイベントも、エコシステムの中の多様なアシスタントデバイスによって検出され得る。ホットワードフリーイベントは、たとえば、検出されると自動化アシスタントコンポーネントまたは機能の1つまたは複数がアクティブにされるようにする、アシスタントデバイスの所与の1つに向けられる(および任意選択で、ユーザ101の口の動きと組み合わせられる)視線を検出することを含み得る。たとえば、それぞれの画像データは、アシスタントデバイスの複数(たとえば、存在センサに関連して説明されるビジョンコンポーネントを含む)によって捉えられてもよく、それぞれの尺度を生成するために、ホットワードフリーモデル(たとえば、MLモデルデータベース192に記憶されている)を使用して、それぞれの画像データが処理されてもよく、ホットワードフリーイベントが実際のホットワードフリーイベントであるかどうかを決定するために、それぞれの尺度が処理されてもよい。これらの技法は、エコシステムにおいて発生し得る他の視覚的なイベントを検出するためにも利用され得る。 Although FIGS. 2A and 2B are described herein with respect to hotword events that activate one or more of the automation assistant components or functions are acoustic events, this is for purposes of illustration and limitation. It should be understood that this is not intended. Additionally or alternatively, hotword-free events may also be detected by various assistant devices in the ecosystem. Hotword-free events are directed to a given one of the assistant devices (and optionally the user 101 (combined with mouth movements). For example, each image data may be captured by a plurality of assistant devices (e.g., including vision components described in conjunction with presence sensors), and hotword-free models ( For example, using the ML model database 192), each image data may be processed to determine whether the hotword-free event is an actual hotword-free event, each A measure of may be processed. These techniques can also be used to detect other visual events that may occur in the ecosystem.
その上、図2Aおよび図2Bは、音響イベントがホットワードイベントであることに関連して本明細書において説明されるが、これは例示のためであり、限定することは意図されないことを理解されたい。別の限定しない例として、特に図3を参照すると、環境において捉えられる任意のオーディオデータが、追加または代替として、エコシステムにおける特定の音の発生を検出するように訓練される音検出モデルを使用して処理され得る。たとえば、(ユーザ101が発話を提供するのではなく)部屋250の窓が割れていると仮定し、第1のアシスタント入力デバイス1061によって捉えられるオーディオデータが、割れたガラスの発生を検出することに関連する尺度(および任意選択で、呼び鈴が鳴ること、火災報知器の鳴動、および/またはエコシステムにおいて発生し得る他の音などの、他の音に関連する追加の尺度)を生成するように訓練されるそれぞれの音検出モデルを使用して処理されるとさらに仮定する。図2Aに関して上で説明されたのと同様に、エコシステムの中の追加のアシスタントデバイス(たとえば、第5のアシスタント入力デバイス1065および第5のアシスタント非入力デバイス1855)が、時間的に対応するオーディオデータを検出し、時間的に対応するオーディオデータを処理して追加の尺度を生成し、ガラスが割れることに対応する音がエコシステムにおいて実際に捉えられたかどうかを尺度および追加の尺度に基づいて決定することが予想されるものとして、特定され得る。結果として、ガラスが割れる音がエコシステムにおいて検出されたことを示す通知が、ユーザ101のクライアントデバイス(たとえば、エコシステムの中のモバイルデバイスおよび/またはアシスタントデバイスの1つまたは複数)においてレンダリングされてもよく、任意選択で、ガラスが割れることに検出したアシスタントデバイスの標示、および/または、ガラスが割れる音が検出されたエコシステムの中の部屋の標示を含んでもよい。
Moreover, although FIGS. 2A and 2B are described herein in connection with the acoustic event being a hotword event, it is understood that this is for illustration and is not intended to be limiting. sea bream. As another non-limiting example, with particular reference to Figure 3, any audio data captured in the environment may additionally or alternatively use a sound detection model trained to detect the occurrence of specific sounds in the ecosystem. can be processed as For example, suppose a window in
図2A、図2B、および図3は音響イベント(および視覚イベント)に関して本明細書において説明されるが、それは例示のためであり、限定することは意図されないことを理解されたい。本明細書において説明される技法は、エコシステムの中の多様なアシスタントデバイスによって検出され得るあらゆるイベントの発生を検証するために利用され得る。たとえば、第1のアシスタントデバイスが、加速度計を含むユーザ101により携帯されているモバイルデバイスであると仮定し、第2のアシスタントデバイスが、同様に加速度計を含むユーザ101により装着されているスマートウォッチであると仮定する。この例では、モバイルデバイスおよびスマートウォッチによって生成されるそれぞれの加速度計データは、検出された加速度計イベントが実際の加速度計イベント(たとえば、ユーザ101の歩行、ジョギング、ランニング、および/または加速度計により検出され得る任意の他の運動)であるかどうかを決定するために、上で説明されたのと同じまたは同様の方式で処理され得る。
2A, 2B, and 3 are described herein in terms of acoustic events (and visual events), it should be understood that this is for illustration and is not intended to be limiting. The techniques described herein can be utilized to verify the occurrence of any event that can be detected by various assistant devices in the ecosystem. For example, suppose a first assistant device is a mobile device carried by
ここで図4を見ると、エコシステムの中の複数のアシスタントデバイスにおいて捉えられるオーディオデータに基づいて、音響イベントが実際の音響イベントであるかどうかを決定する例示的な方法400を示すフローチャートが図示されている。便宜的に、方法400の動作は、動作を実行するシステムに関連して説明される。方法400のシステムは、コンピューティングデバイスの1つまたは複数のプロセッサおよび/または他のコンポーネントを含む。たとえば、方法400のシステムは、図1、図2A、図2B、もしくは図3のアシスタント入力デバイス106、図1、図2A、図2B、もしくは図3のアシスタント非入力デバイス185、図6のコンピューティングデバイス610、1つもしくは複数のサーバ、他のコンピューティングデバイス、および/またはそれらの任意の組合せによって実装され得る。その上、方法400の動作は特定の順序で示されるが、これは限定することは意図されない。1つまたは複数の動作が、並べ替えられ、省略され、かつ/または追加されてもよい。
Turning now to FIG. 4, illustrated is a flowchart illustrating an exemplary method 400 for determining whether an acoustic event is an actual acoustic event based on audio data captured at multiple assistant devices in an ecosystem. It is For convenience, the operations of method 400 are described in terms of a system that performs the operations. The system of method 400 includes one or more processors and/or other components of a computing device. For example, the system of method 400 may include the assistant input device 106 of FIG. 1, FIG. 2A, FIG. 2B, or FIG. 3, the assistant
ブロック452において、システムは、複数のアシスタントデバイスを含むエコシステムの中に位置するアシスタントデバイスのマイクロフォンを介して、音響イベントを捉えるオーディオデータを検出する。検出された音響イベントは、たとえば、エコシステムに関連するユーザによって提供される特定の語もしくは語句(たとえば、ホットワード)、人の発話に対応しないエコシステムにおける特定の音、および/またはエコシステムにおいて発生し得る他の音響イベントであり得る。 At block 452, the system detects audio data that captures the acoustic event via microphones of assistant devices located in an ecosystem that includes multiple assistant devices. Detected acoustic events may be, for example, specific words or phrases (e.g., hot words) provided by users associated with the ecosystem, specific sounds in the ecosystem that do not correspond to human speech, and/or It can be other acoustic events that can occur.
ブロック454において、システムは、アシスタントデバイスにおいてローカルに記憶されているイベント検出モデルを使用して、音響イベントを捉えるオーディオデータを処理し、音響イベントに関連する尺度を生成する。いくつかの実装形態では、アシスタントデバイスにおいて検出されるオーディオデータが(たとえば、自動化アシスタントの1つまたは複数のコンポーネントをアクティブにするための)特定の語または語句を含むかどうかを示す尺度を生成するために、アシスタントデバイスにおいてローカルに記憶されているホットワード検出モデルを使用して、音響イベントを捉えるオーディオデータが処理され得る。いくつかの追加または代替の実装形態では、特定の音(たとえば、ガラスが割れること、犬が吠えること、猫が鳴くこと、呼び鈴が鳴ること、火災報知器が鳴動すること、一酸化炭素検出器が鳴動すること、乳児が泣くこと、ドアをノックすること、および/またはエコシステムにおいて発生し得る任意の他の音)をアシスタントデバイスにおいて検出されるオーディオデータが含むかどうかを示す尺度を生成するために、アシスタントデバイスにおいてローカルに記憶されている音検出モデルを使用して、音響イベントを捉えるオーディオデータが処理され得る。 At block 454, the system processes the audio data capturing the acoustic event using event detection models stored locally on the assistant device to generate measures associated with the acoustic event. Some implementations generate a measure indicating whether audio data detected at the assistant device contains a particular word or phrase (e.g., for activating one or more components of the automated assistant). To this end, audio data capturing acoustic events may be processed using hot word detection models that are stored locally on the assistant device. In some additional or alternative implementations, certain sounds (e.g., breaking glass, dogs barking, cats meowing, doorbells ringing, fire alarms ringing, carbon monoxide detectors ringing, baby crying, knocking on the door, and/or any other sound that may occur in the ecosystem)). To that end, audio data capturing acoustic events may be processed using sound detection models stored locally in the assistant device.
ブロック456において、システムは、エコシステムの中に位置する追加のアシスタントデバイスの追加のマイクロフォンを介して、音響イベントを同様に捉える追加のオーディオデータを検出する。追加のアシスタントデバイスにおいて検出される追加のオーディオデータは、アシスタントデバイスにおいて検出されるオーディオデータに時間的に対応し得る。システムは、オーディオデータおよび追加のオーディオデータがそれぞれいつ受信されたかに対応するそれぞれのタイムスタンプに基づいて、追加のオーディオデータがオーディオデータに時間的に対応すると決定することができる。さらに、タイムスタンプが一致するとき、またはタイムスタンプが互いに閾値の時間長以内にある(たとえば、数ミリ秒、数秒、または任意の他の適切な長さの時間内にある)場合、追加のオーディオデータは、オーディオデータに時間的に対応すると見なされ得る。 At block 456, the system detects additional audio data that similarly captures the acoustic event via additional microphones of additional assistant devices located within the ecosystem. The additional audio data detected at the additional assistant device may temporally correspond to the audio data detected at the assistant device. The system can determine that the additional audio data temporally corresponds to the audio data based on respective timestamps corresponding to when the audio data and the additional audio data were each received. Additionally, additional audio when the timestamps match, or if the timestamps are within a threshold length of time of each other (e.g., within milliseconds, seconds, or any other suitable length of time) The data can be viewed as temporally corresponding to the audio data.
ブロック458において、システムは、追加のアシスタントデバイスにおいてローカルに記憶されている追加のイベント検出モデルを使用して、音響イベントを捉える追加のオーディオデータを処理して、音響イベントに関連する追加の尺度を生成する。いくつかの実装形態では、音響イベントを捉える追加のオーディオデータは、追加のアシスタントデバイスにおいて検出される追加のオーディオデータが(たとえば、自動化アシスタントの1つまたは複数のコンポーネントをアクティブにするための)特定の語または語句を含むかどうかを示す追加の尺度を生成するために、追加のアシスタントデバイスにおいてローカルに記憶されている追加のホットワード検出モデルを使用して処理され得る。いくつかの追加または代替の実装形態では、音響イベントを捉える追加のオーディオデータは、特定の音(たとえば、ガラスが割れること、犬が吠えること、猫が鳴くこと、呼び鈴が鳴ること、火災報知器が鳴動すること、一酸化炭素検出器が鳴動すること、および/またはエコシステムにおいて発生し得る任意の他の音)を追加のアシスタントデバイスにおいて検出される追加のオーディオデータが含むかどうかを示す追加の尺度を生成するために、追加のアシスタントデバイスにおいてローカルに記憶されている追加の音検出モデルを使用して処理され得る。 At block 458, the system processes the additional audio data capturing the acoustic event using additional event detection models stored locally on the additional assistant device to generate additional measures associated with the acoustic event. Generate. In some implementations, the additional audio data that captures the acoustic event is a specific (e.g., for activating one or more components of the automated assistant) additional audio data detected in the additional assistant device. can be processed using additional hot word detection models stored locally in additional assistant devices to generate additional measures indicating whether the word or phrase contains the In some additional or alternative implementations, the additional audio data capturing the acoustic event is a specific sound (e.g., breaking glass, barking dog, barking cat, ringing doorbell, ringing fire alarm, etc.). sounds, carbon monoxide detectors sound, and/or any other sounds that may occur in the ecosystem), indicating whether additional audio data detected in the additional assistant device includes may be processed using additional sound detection models stored locally in additional assistant devices to generate a measure of .
ブロック460において、システムは、尺度と追加の尺度の両方を処理して、少なくともアシスタントデバイスと追加のアシスタントデバイスの両方によって検出される音響イベントが実際の音響イベントであるかどうかを決定する。尺度および追加の尺度は、たとえば、オーディオデータが音響イベントを捉えるかどうかを示す二進値(たとえば、yesに対して「1」、noに対して「0」)、音響イベントが発生したかどうかに関連する確率(たとえば、オーディオデータが音響イベントを捉える0.7vsオーディオデータが音響イベントを捉えない0.3)、および/または音響イベントに関連する他の尺度であり得る。 At block 460, the system processes both the measure and the additional measure to determine whether the sound events detected by at least both the assistant device and the additional assistant device are actual sound events. Measures and additional measures are, for example, binary values that indicate whether the audio data captures the acoustic event (eg, "1" for yes, "0" for no), whether the acoustic event occurred (eg, 0.7 that the audio data captures the acoustic event vs. 0.3 that the audio data does not capture the acoustic event), and/or other measures associated with the acoustic event.
ブロック462において、システムは、ブロック460において尺度および追加の尺度を処理したことに基づいて、音響イベントが実際の音響イベントであるかどうかを決定する。ブロック462の反復において、音響イベントが実際の音響イベントではないとシステムが決定する場合、システムは、ブロック464に進み、さらなる処理なしでオーディオデータおよび追加のオーディオデータを廃棄する。システムは次いでブロック452に戻る。たとえば、尺度および追加の尺度は、オーディオデータおよび追加のオーディオデータがそれぞれホットワードを捉える確率に対応すると仮定する。たとえば、尺度は、オーディオデータがホットワードを捉える0.7の確率であると仮定し、追加の尺度は、追加のオーディオデータがホットワードを捉える0.3の確率であると仮定する。この例では、システムは、尺度だけに基づいて0.7という確率が閾値の確率を満たすと仮定して、音響イベントが実際の音響イベントである(たとえば、ホットワードを含む発話をユーザが提供した)と決定し得る。しかしながら、システムは、追加の尺度に基づいて0.3という確率が閾値の確率レベルを満たさないと仮定して、音響イベントが実際の音響イベントではない(たとえば、ホットワードを含む発話をユーザが提供しなかった)と決定し得る。結果として、システムは、音響イベントが実際の音響イベントであったことを尺度が示していても、音響イベントが実際の音響イベントではないと決定してもよく、それは、音響イベントが実際の音響イベントであったことを追加の尺度も示さなければならないからであり、オーディオデータおよび追加のオーディオデータは廃棄され得る。追加または代替として、システムは、音響イベントが実際の音響イベントであるかどうかを決定するために利用され得るアシスタントデバイスにおいて検出されるそれぞれのオーディオデータに基づいて生成される、尺度の平均、中央値、最高値、最低値、またはパーセンタイルのうちの1つまたは複数などの、尺度に関連する統計に基づいて、音響イベントが実際の音響イベントであるかどうかを決定することができる。 At block 462 , the system determines whether the sound event is an actual sound event based on processing the scale and the additional scale at block 460 . If in the block 462 iteration the system determines that the sound event is not an actual sound event, the system proceeds to block 464 and discards the audio data and any additional audio data without further processing. The system then returns to block 452. For example, assume that the measure and the additional measure correspond to the probability that the audio data and the additional audio data respectively catch the hotword. For example, suppose the measure is a 0.7 probability that the audio data catches the hotword, and the additional measure is a 0.3 probability that the additional audio data catches the hotword. In this example, the system assumes that the acoustic event is an actual acoustic event (e.g., a user provided an utterance containing a hotword), assuming that a probability of 0.7 satisfies the threshold probability based on the scale alone. can decide. However, the system assumes that a probability of 0.3 does not meet the threshold probability level based on an additional measure and assumes that the acoustic event is not an actual acoustic event (e.g., the user did not provide an utterance containing a hotword). ) can be determined. As a result, the system may determine that the acoustic event is not an actual acoustic event, even though the measures indicate that the acoustic event was an actual acoustic event. audio data and additional audio data may be discarded. Additionally or alternatively, the system generates mean, median measures based on respective audio data detected in the assistant device that can be utilized to determine whether a sound event is an actual sound event. Based on statistics associated with a metric, such as one or more of , highest, lowest, or percentile, it can be determined whether an acoustic event is an actual acoustic event.
ブロック462の反復において、音響イベントが実際の音響イベントであるとシステムが決定する場合、システムはブロック466に進む。たとえば、オーディオデータおよび追加のオーディオデータがそれぞれホットワードを捉える確率に、尺度および追加の尺度が対応すると仮定する。たとえば、尺度は、オーディオデータがホットワードを捉える0.7の確率であると仮定し、追加の尺度は、追加のオーディオデータがホットワードを捉える0.8の確率であると仮定する。この例では、システムは、0.7という確率および0.8という確率(またはそれらに基づく1つまたは複数の統計)が、尺度および追加の尺度にそれぞれ基づいて閾値の確率レベルを満たすと仮定して、音響イベントが実際の音響イベントである(たとえば、ホットワードを含む発話をユーザが提供した)と決定し得る。結果として、システムは、音響イベントが実際の音響イベントであったことを尺度と追加の尺度の両方が示すので、音響イベントが実際の音響イベントであると決定し得る。 In the iteration of block 462, if the system determines that the acoustic event is an actual acoustic event, the system proceeds to block 466; For example, assume that the measure and the additional measure correspond to the probability that the audio data and the additional audio data catch the hot word, respectively. For example, suppose the measure is a 0.7 probability that the audio data catches the hotword, and the additional measure is a 0.8 probability that the additional audio data catches the hotword. In this example, the system assumes that probabilities of 0.7 and 0.8 (or one or more statistics based on them) satisfy the threshold probability level based on the scale and the additional scale, respectively. is an actual acoustic event (eg, a user provided an utterance containing a hotword). As a result, the system can determine that the sound event is a real sound event because both the scale and the additional scale indicate that the sound event was a real sound event.
ブロック466において、システムは、実際の音響イベントに関連する行動が実行されるようにする。システムは次いでブロック452に戻る。システムによって実行される行動は、オーディオデータおよび追加のオーディオデータにおいて捉えられる実際の音響イベントに基づき得る。たとえば、実際の音響イベントがホットワードの発生に対応する実装形態では、システムは、自動化アシスタントの1つまたは複数のコンポーネントまたは機能(たとえば、音声処理、自然言語処理、および/または自動化アシスタントの他のコンポーネントもしくは機能)をアクティブにすることができる。別の例として、実際の音響イベントが特定の音の発生に対応する実装形態では、システムは、エコシステムにおいて特定の音が検出されたことを示す通知を生成する(および任意選択で、特定の音を検出したアシスタントデバイスを特定する)ことができ、エコシステムに関連するユーザのクライアントデバイス(たとえば、ユーザのモバイルデバイス、アシスタントデバイスの1つもしくは複数、および/またはユーザに関連する任意の他のクライアントデバイス)において通知がレンダリングされるようにすることができる。 At block 466, the system causes actions associated with the actual acoustic event to be performed. The system then returns to block 452. Actions performed by the system may be based on actual acoustic events captured in the audio data and additional audio data. For example, in implementations in which the actual acoustic event corresponds to the occurrence of a hotword, the system may use one or more components or functions of the automated assistant (e.g., speech processing, natural language processing, and/or other components of the automated assistant). component or function) can be activated. As another example, in implementations where the actual acoustic event corresponds to the occurrence of a particular sound, the system generates a notification indicating that a particular sound was detected in the ecosystem (and optionally identify the assistant device that detected the sound) and the user's client device associated with the ecosystem (e.g., the user's mobile device, one or more of the assistant devices, and/or any other associated with the user). client device) to render the notification.
ここで図5を見ると、所与のアシスタントデバイスが音響イベントを検出したことに基づいて、どのアシスタントデバイスがエコシステムにおける音響イベントを検出したはずであるかを特定する、例示的な方法500を示すフローチャートが図示されている。便宜的に、方法500の動作は、動作を実行するシステムに関連して説明される。方法500のシステムは、コンピューティングデバイスの1つまたは複数のプロセッサおよび/または他のコンポーネントを含む。たとえば、方法500のシステムは、図1、図2A、図2B、もしくは図3のアシスタント入力デバイス106、図1、図2A、図2B、もしくは図3のアシスタント非入力デバイス185、図6のコンピューティングデバイス610、1つもしくは複数のサーバ、他のコンピューティングデバイス、および/またはそれらの任意の組合せによって実装され得る。その上、方法500の動作は特定の順序で示されているが、これは限定することは意図されない。1つまたは複数の動作が、並べ替えられ、省略され、かつ/または追加されてもよい。
Turning now to FIG. 5, an exemplary method 500 for identifying which assistant devices should have detected an acoustic event in the ecosystem based on the acoustic event detected by a given assistant device. An illustrative flow chart is shown. For convenience, the operations of method 500 are described in terms of a system that performs the operations. The system of method 500 includes one or more processors and/or other components of a computing device. For example, the system of method 500 may include assistant input device 106 of FIG. 1, FIG. 2A, FIG. 2B, or FIG. 3, assistant
ブロック552において、システムは、複数のアシスタントデバイスを含むエコシステムの中に位置するアシスタントデバイスのマイクロフォンを介して、音響イベントを捉えるオーディオデータを検出する。検出された音響イベントは、たとえば、エコシステムに関連するユーザによって提供される特定の語もしくは語句(たとえば、ホットワード)、人の発話に対応しないエコシステムにおける特定の音、および/またはエコシステムにおいて発生し得る他の音響イベントであり得る。 At block 552, the system detects audio data that captures the acoustic event via microphones of assistant devices located in an ecosystem that includes multiple assistant devices. Detected acoustic events may be, for example, specific words or phrases (e.g., hot words) provided by users associated with the ecosystem, specific sounds in the ecosystem that do not correspond to human speech, and/or It can be other acoustic events that can occur.
ブロック554において、システムは、エコシステム内のアシスタントデバイスの位置に基づいて、アシスタントデバイスにおいて検出されるオーディオデータに時間的に対応する追加のオーディオデータをそのそれぞれのマイクロフォンを介して検出したはずである少なくとも1つの追加のアシスタントデバイスを特定する。システムは、アシスタントデバイスおよび少なくとも1つの追加のアシスタントデバイスが時間的に対応するオーディオデータを過去に検出していることに基づいて、追加のオーディオデータを検出したはずである少なくとも1つの追加のアシスタントデバイスを特定することができる。さらに、システムは、オーディオデータおよび追加のオーディオデータがそれぞれいつ受信されるかに対応するそれぞれのタイムスタンプに基づいて、追加のオーディオデータがオーディオデータに時間的に対応すると決定することができる。その上、追加のオーディオデータは、タイムスタンプが一致する、またはタイムスタンプが互いに閾値の時間長以内にある(たとえば、数ミリ秒、数秒、または任意の他の適切な長さの時間内にある)場合、オーディオデータに時間的に対応すると見なされ得る。 At block 554, the system should have detected additional audio data corresponding in time to the audio data detected at the assistant device via its respective microphone based on the location of the assistant device within the ecosystem. Identify at least one additional assistant device. At least one additional assistant device that the system should have detected additional audio data based on the assistant device and at least one additional assistant device having detected temporally corresponding audio data in the past can be specified. Further, the system can determine that the additional audio data temporally corresponds to the audio data based on respective timestamps corresponding to when the audio data and the additional audio data are each received. Moreover, the additional audio data must be either timestamp-matching or timestamps within a threshold length of time of each other (e.g., within milliseconds, seconds, or any other suitable length of time). ), it can be considered to correspond temporally to the audio data.
ブロック556において、システムは、アシスタントデバイスにおいて検出されるオーディオデータに時間的に対応する追加のオーディオデータを追加のアシスタントデバイスが検出したかどうかを決定する。ブロック556の反復において、アシスタントデバイスにおいて検出されるオーディオデータに時間的に対応する追加のオーディオデータがないとシステムが決定する場合、システムはブロック566に進み、さらなる処理なしでオーディオデータおよび追加のオーディオデータを廃棄する。ブロック556の反復において、アシスタントデバイスにおいて検出されるオーディオデータに時間的に対応する追加のオーディオデータがあるとシステムが決定する場合、システムはブロック558に進む。言い換えると、音響イベントを捉えるオーディオデータを検出したはずである他のアシスタントデバイスがいずれも、どのような時間的に対応するオーディオデータも捉えていない場合、システムは、イベント検出モデルを使用してオーディオデータを処理することなく、音響イベントが実際の音響イベントではないと決定し得る。 At block 556, the system determines whether the additional assistant device has detected additional audio data that temporally corresponds to the audio data detected at the assistant device. If, in the iteration of block 556, the system determines that there is no additional audio data temporally corresponding to the audio data detected in the assistant device, the system proceeds to block 566 and processes the audio data and the additional audio data without further processing. Discard data. If, in the iteration of block 556 , the system determines that there is additional audio data temporally corresponding to the audio data detected at the assistant device, the system proceeds to block 558 . In other words, if none of the other assistant devices that should have detected audio data that captures the acoustic event does not capture any temporally corresponding audio data, the system uses the event detection model to detect the audio data. Without processing the data, it can be determined that the acoustic event is not an actual acoustic event.
ブロック558において、システムは、アシスタントデバイスにおいてローカルに記憶されているイベント検出モデルを使用して、音響イベントを捉えるオーディオデータを処理して、音響イベントに関連する尺度を生成する。いくつかの実装形態では、音響イベントを捉えるオーディオデータは、アシスタントデバイスにおいて検出されるオーディオデータが(たとえば、自動化アシスタントの1つまたは複数のコンポーネントをアクティブにするための)特定の語または語句を含むかどうかを示す尺度を生成するために、アシスタントデバイスにおいてローカルに記憶されているホットワード検出モデルを使用して処理され得る。いくつかの追加または代替の実装形態では、音響イベントを捉えるオーディオデータは、特定の音(たとえば、ガラスが割れること、犬が吠えること、猫が鳴くこと、呼び鈴が鳴ること、火災報知器が鳴動すること、一酸化炭素検出器が鳴動すること、乳児が泣くこと、ドアをノックすること、および/またはエコシステムにおいて発生し得る任意の他の音)をアシスタントデバイスにおいて検出されるオーディオデータが含むかどうかを示す尺度を生成するために、アシスタントデバイスにおいてローカルに記憶されている音検出モデルを使用して処理され得る。 At block 558, the system processes the audio data capturing the acoustic event using event detection models stored locally on the assistant device to generate measures associated with the acoustic event. In some implementations, the audio data that captures the acoustic event is such that the audio data detected at the assistant device contains specific words or phrases (e.g., for activating one or more components of the automated assistant). It can be processed using a hot word detection model stored locally in the assistant device to generate a measure of whether. In some additional or alternative implementations, audio data that captures acoustic events may include specific sounds (e.g., breaking glass, dogs barking, cats meowing, doorbells ringing, fire alarms ringing). sounds, carbon monoxide detectors ringing, babies crying, door knocks, and/or any other sounds that may occur in the ecosystem). It can be processed using a locally stored sound detection model in the assistant device to generate a measure indicating whether.
ブロック560において、システムは、追加のアシスタントデバイスにおいてローカルに記憶されている追加のイベント検出モデルを使用して、音響イベントを捉える追加のオーディオデータを処理して、音響イベントに関連する追加の尺度を生成する。いくつかの実装形態では、音響イベントを捉える追加のオーディオデータは、特定の語または語句(たとえば、自動化アシスタントの1つまたは複数のコンポーネントをアクティブにするための)を追加のアシスタントデバイスにおいて検出される追加のオーディオデータが含むかどうかを示す追加の尺度を生成するために、追加のアシスタントデバイスにおいてローカルに記憶されている追加のホットワード検出モデルを使用して処理され得る。いくつかの追加または代替の実装形態では、音響イベントを捉える追加のオーディオデータは、特定の音(たとえば、ガラスが割れること、犬が吠えること、猫が鳴くこと、呼び鈴が鳴ること、火災報知器が鳴動すること、一酸化炭素検出器が鳴動すること、および/またはエコシステムにおいて発生し得る任意の他の音)を追加のアシスタントデバイスにおいて検出される追加のオーディオデータが含むかどうかを示す追加の尺度を生成するために、追加のアシスタントデバイスにおいてローカルに記憶されている追加の音検出モデルを使用して処理され得る。 At block 560, the system processes the additional audio data capturing the acoustic event using additional event detection models stored locally on the additional assistant device to generate additional measures associated with the acoustic event. Generate. In some implementations, the additional audio data capturing the acoustic event is detected in an additional assistant device for specific words or phrases (e.g., for activating one or more components of the automated assistant). Additional hot word detection models stored locally in additional assistant devices may be used and processed to generate additional measures of whether additional audio data is included. In some additional or alternative implementations, the additional audio data capturing the acoustic event is a specific sound (e.g., breaking glass, barking dog, barking cat, ringing doorbell, ringing fire alarm, etc.). sounds, carbon monoxide detectors sound, and/or any other sounds that may occur in the ecosystem), indicating whether additional audio data detected in the additional assistant device includes may be processed using additional sound detection models stored locally in additional assistant devices to generate a measure of .
ブロック562において、システムは、尺度と追加の尺度の両方を処理して、少なくともアシスタントデバイスと追加のアシスタントデバイスの両方によって検出される音響イベントが実際の音響イベントであるかどうかを決定する。尺度および追加の尺度は、たとえば、オーディオデータが音響イベントを捉えるかどうかを示す二進値(たとえば、yesに対して「1」、noに対して「0」)、音響イベントが発生したかどうかに関連する確率(たとえば、オーディオデータが音響イベントを捉える0.7vsオーディオデータが音響イベントを捉えない0.3)、および/または音響イベントに関連する他の尺度であり得る。 At block 562, the system processes both the measure and the additional measure to determine whether the sound events detected by at least both the assistant device and the additional assistant device are actual sound events. Measures and additional measures are, for example, binary values indicating whether the audio data captures the acoustic event (e.g. "1" for yes, "0" for no), whether the acoustic event occurred (eg, 0.7 that the audio data captures the acoustic event vs. 0.3 that the audio data does not capture the acoustic event), and/or other measures associated with the acoustic event.
ブロック564において、システムは、ブロック562において尺度および追加の尺度を処理したことに基づいて、音響イベントが実際の音響イベントであるかどうかを決定する。ブロック564の反復において、音響イベントが実際の音響イベントではないとシステムが決定する場合、システムは、ブロック566に進み、さらなる処理なしでオーディオデータおよび追加のオーディオデータを廃棄する。システムは次いでブロック552に戻る。たとえば、尺度および追加の尺度は、オーディオデータと追加のオーディオデータがそれぞれホットワードを捉える確率に対応すると仮定する。たとえば、尺度は、オーディオデータがホットワードを捉える0.7の確率であると仮定し、追加の尺度は、追加のオーディオデータがホットワードを捉える0.3の確率であると仮定する。この例では、システムは、尺度だけに基づいて0.7という確率が閾値の確率を満たすと仮定して、音響イベントが実際の音響イベントである(たとえば、ホットワードを含む発話をユーザが提供した)と決定し得る。しかしながら、システムは、追加の尺度に基づいて0.3という確率が閾値の確率レベルを満たさないと仮定して、音響イベントが実際の音響イベントではない(たとえば、ホットワードを含む発話をユーザが提供しなかった)と決定し得る。結果として、システムは、音響イベントが実際の音響イベントであったことを尺度が示していても、音響イベントが実際の音響イベントではないと決定してもよく、それは、音響イベントが実際の音響イベントであったことを追加の尺度も示さなければならないからであり、オーディオデータおよび追加のオーディオデータは廃棄され得る。追加または代替として、システムは、音響イベントが実際の音響イベントであるかどうかを決定するために利用され得るアシスタントデバイスにおいて検出されるそれぞれのオーディオデータに基づいて生成される、尺度の平均、中央値、最高値、最低値、またはパーセンタイルのうちの1つまたは複数などの、尺度に関連する統計に基づいて、音響イベントが実際の音響イベントであるかどうかを決定することができる。 At block 564 the system determines whether the sound event is an actual sound event based on processing the scale and the additional scale at block 562 . If in the block 564 iteration the system determines that the sound event is not an actual sound event, the system proceeds to block 566 and discards the audio data and any additional audio data without further processing. The system then returns to block 552. For example, assume that the measure and the additional measure correspond to the probability that the audio data and the additional audio data each catch the hot word. For example, suppose the measure is a 0.7 probability that the audio data catches the hotword, and the additional measure is a 0.3 probability that the additional audio data catches the hotword. In this example, the system assumes that the acoustic event is an actual acoustic event (e.g., a user provided an utterance containing a hotword), assuming a probability of 0.7 satisfies the threshold probability based on the scale alone. can decide. However, the system assumes that a probability of 0.3 does not meet the threshold probability level based on an additional measure and assumes that the acoustic event is not an actual acoustic event (e.g., the user did not provide an utterance containing a hotword). ) can be determined. As a result, the system may determine that the acoustic event is not an actual acoustic event, even though the measures indicate that the acoustic event was an actual acoustic event. audio data and additional audio data may be discarded. Additionally or alternatively, the system generates mean, median measures based on respective audio data detected in the assistant device that can be utilized to determine whether a sound event is an actual sound event. Based on statistics associated with a metric, such as one or more of , highest, lowest, or percentile, it can be determined whether an acoustic event is an actual acoustic event.
ブロック564の反復において、音響イベントが実際の音響イベントであるとシステムが決定する場合、システムはブロック568に進む。たとえば、尺度および追加の尺度は、オーディオデータと追加のオーディオデータがそれぞれホットワードを捉える確率に対応すると仮定する。たとえば、尺度は、オーディオデータがホットワードを捉える0.7の確率であると仮定し、追加の尺度は、追加のオーディオデータがホットワードを捉える0.8の確率であると仮定する。この例では、システムは、尺度および追加の尺度にそれぞれ基づいて0.7という確率および0.8という確率(またはそれらに基づく1つまたは複数の統計)が閾値の確率レベルを満たすと仮定して、音響イベントが実際の音響イベントである(たとえば、ホットワードを含む発話をユーザが提供した)と決定し得る。結果として、システムは、音響イベントが実際の音響イベントであったことを尺度と追加の尺度の両方が示すので、音響イベントが実際の音響イベントであると決定し得る。 In the iteration of block 564 , if the system determines that the sound event is an actual sound event, the system proceeds to block 568 . For example, assume that the measure and the additional measure correspond to the probability that the audio data and the additional audio data each catch the hot word. For example, suppose the measure is a 0.7 probability that the audio data catches the hotword, and the additional measure is a 0.8 probability that the additional audio data catches the hotword. In this example, the system assumes that probabilities of 0.7 and 0.8 (or one or more statistics based on them) satisfy the threshold probability level based on the scale and the additional scale, respectively, and that the acoustic event is It may be determined that it is an actual acoustic event (eg, a user provided an utterance containing a hotword). As a result, the system can determine that the sound event is a real sound event because both the scale and the additional scale indicate that the sound event was a real sound event.
ブロック568において、システムは、実際の音響イベントに関連する行動が実行されるようにする。システムは次いでブロック552に戻る。システムによって実行される行動は、オーディオデータおよび追加のオーディオデータにおいて捉えられる実際の音響イベントに基づき得る。たとえば、実際の音響イベントがホットワードの発生に対応する実装形態では、システムは、自動化アシスタントの1つまたは複数のコンポーネントまたは機能(たとえば、音声処理、自然言語処理、および/または自動化アシスタントの他のコンポーネントもしくは機能)をアクティブにすることができる。別の例として、実際の音響イベントが特定の音の発生に対応する実装形態では、システムは、エコシステムにおいて特定の音が検出されたことを示す通知を生成する(および任意選択で、特定の音を検出したアシスタントデバイスを特定する)ことができ、エコシステムに関連するユーザのクライアントデバイス(たとえば、ユーザのモバイルデバイス、アシスタントデバイスの1つもしくは複数、および/またはユーザに関連する任意の他のクライアントデバイス)において通知がレンダリングされるようにすることができる。 At block 568, the system causes actions associated with the actual acoustic event to be performed. The system then returns to block 552. Actions performed by the system may be based on actual acoustic events captured in the audio data and additional audio data. For example, in implementations in which the actual acoustic event corresponds to the occurrence of a hotword, the system may use one or more components or functions of the automated assistant (e.g., speech processing, natural language processing, and/or other components of the automated assistant). component or function) can be activated. As another example, in implementations where the actual acoustic event corresponds to the occurrence of a particular sound, the system generates a notification indicating that a particular sound was detected in the ecosystem (and optionally identify the assistant device that detected the sound) and the user's client device associated with the ecosystem (e.g., the user's mobile device, one or more of the assistant devices, and/or any other associated with the user). client device) to render the notification.
図6は、本明細書において説明される技法の1つまたは複数の態様を実行するために任意選択で利用され得る例示的なコンピューティングデバイス610のブロック図である。いくつかの実装形態では、アシスタント入力デバイスの1つもしくは複数、クラウドベースの自動化アシスタントコンポーネントの1つもしくは複数、1つもしくは複数のアシスタント非入力システム、1つもしくは複数のアシスタント非入力デバイス、および/または他のコンポーネントは、例示的なコンピューティングデバイス610の1つまたは複数のコンポーネントを備え得る。 FIG. 6 is a block diagram of an exemplary computing device 610 that can optionally be utilized to perform one or more aspects of the techniques described herein. In some implementations, one or more of the assistant input devices, one or more of the cloud-based automated assistant components, one or more assistant non-input systems, one or more assistant non-input devices, and/or Or other components may comprise one or more components of exemplary computing device 610 .
コンピューティングデバイス610は通常、バスサブシステム612を介していくつかの周辺デバイスと通信する少なくとも1つのプロセッサ614を含む。これらの周辺デバイスは、たとえば、メモリサブシステム625およびファイルストレージサブシステム626、ユーザインターフェース出力デバイス620、ユーザインターフェース入力デバイス622、ならびにネットワークインターフェースサブシステム616を含む、ストレージサブシステム624を含み得る。入力デバイスおよび出力デバイスは、コンピューティングデバイス610とのユーザ対話を可能にする。ネットワークインターフェースサブシステム616は、外部ネットワークへのインターフェースを提供し、他のコンピューティングデバイスにおいて対応するインターフェースデバイスに結合される。 Computing device 610 typically includes at least one processor 614 that communicates with several peripheral devices via bus subsystem 612 . These peripheral devices may include, for example, storage subsystem 624 , which includes memory subsystem 625 and file storage subsystem 626 , user interface output device 620 , user interface input device 622 , and network interface subsystem 616 . Input and output devices allow user interaction with computing device 610 . Network interface subsystem 616 provides an interface to external networks and couples to corresponding interface devices in other computing devices.
ユーザインターフェース入力デバイス622は、キーボード、マウス、トラックボール、タッチパッド、もしくはグラフィクスタブレットなどのポインティングデバイス、スキャナ、ディスプレイに組み込まれたタッチスクリーン、音声認識システム、マイクロフォンなどのオーディオ入力デバイス、および/または他のタイプの入力デバイスを含み得る。一般に、「入力デバイス」という用語の使用は、コンピューティングデバイス610または通信ネットワークへと情報を入力するための、すべてのあり得るタイプのデバイスおよび方法を含むことが意図される。 User interface input devices 622 may include pointing devices such as keyboards, mice, trackballs, touch pads, or graphics tablets, scanners, touch screens embedded in displays, voice recognition systems, audio input devices such as microphones, and/or other devices. type of input device. In general, use of the term "input device" is intended to include all possible types of devices and methods for entering information into the computing device 610 or communication network.
ユーザインターフェース出力デバイス620は、ディスプレイサブシステム、プリンタ、ファックスマシン、またはオーディオ出力デバイスなどの非視覚ディスプレイを含み得る。ディスプレイサブシステムは、陰極線管(CRT)、液晶ディスプレイ(LCD)などのフラットパネルデバイス、プロジェクションデバイス、または目に見える画像を作成するための何らかの他の機構を含み得る。ディスプレイサブシステムは、またオーディオ出力デバイスなどを介して非視覚ディスプレイを提供し得る。一般に、「出力デバイス」という用語の使用は、コンピューティングデバイス610からユーザまたは別の機械またはコンピューティングデバイスに情報を出力するための、すべてのあり得るタイプのデバイスおよび方法を含むことが意図される。 User interface output devices 620 may include display subsystems, printers, fax machines, or non-visual displays such as audio output devices. A display subsystem may include a cathode ray tube (CRT), a flat panel device such as a liquid crystal display (LCD), a projection device, or some other mechanism for producing a viewable image. The display subsystem may also provide non-visual displays, such as through audio output devices. In general, use of the term "output device" is intended to include all possible types of devices and methods for outputting information from the computing device 610 to a user or another machine or computing device. .
ストレージサブシステム624は、本明細書において説明されるモジュールの一部またはすべての機能を提供するプログラミングおよびデータ構成物を記憶する。たとえば、ストレージサブシステム624は、本明細書において説明される方法の選択された態様を実行するための、ならびに図1に示される様々なコンポーネントを実装するための論理を含み得る。 Storage subsystem 624 stores programming and data constructs that provide the functionality of some or all of the modules described herein. For example, storage subsystem 624 may include logic for performing selected aspects of the methods described herein, as well as for implementing the various components shown in FIG.
これらのソフトウェアモジュールは一般に、単独で、または他のプロセッサと組み合わせて、プロセッサ614によって実行される。ストレージサブシステム624によって使用されるメモリ625は、プログラム実行の間に命令およびデータを記憶するためのメインランダムアクセスメモリ(RAM)630、ならびに固定された命令が記憶される読み取り専用メモリ(ROM)632を含む、いくつかのメモリを含み得る。ファイルストレージサブシステム626は、プログラムおよびデータファイルのための永続的なストレージを提供することができ、ハードディスクドライブ、関連するリムーバブルメディアを伴うフロッピーディスクドライブ、CD-ROMドライブ、光学ドライブ、またはリムーバブルメディアカートリッジを含み得る。いくつかの実装形態の機能を実装するモジュールは、ストレージサブシステム624にファイルストレージサブシステム626によって記憶され、またはプロセッサ614によってアクセス可能な他の機械に記憶され得る。 These software modules are typically executed by processor 614, either alone or in combination with other processors. The memory 625 used by the storage subsystem 624 includes main random access memory (RAM) 630 for storing instructions and data during program execution, and read only memory (ROM) 632 where fixed instructions are stored. may contain several memories, including The file storage subsystem 626 can provide persistent storage for program and data files, including hard disk drives, floppy disk drives with associated removable media, CD-ROM drives, optical drives, or removable media cartridges. can include Modules implementing the functionality of some implementations may be stored by file storage subsystem 626 in storage subsystem 624 or stored on other machines accessible by processor 614 .
バスサブシステム612は、コンピューティングデバイス610の様々なコンポーネントおよびサブシステムに互いに意図されたように通信させるための機構を提供する。バスサブシステム612は単一のバスであるものとして概略的に示されているが、バスサブシステムの代替の実装形態は複数のバスを使用し得る。 Bus subsystem 612 provides a mechanism for causing the various components and subsystems of computing device 610 to communicate with each other as intended. Although bus subsystem 612 is schematically shown as being a single bus, alternate implementations of the bus subsystem may use multiple buses.
コンピューティングデバイス610は、ワークステーション、サーバ、コンピューティングクラスタ、ブレードサーバ、サーバファーム、または任意の他のデータ処理システムもしくはコンピューティングデバイスを含む、様々なタイプであり得る。コンピュータおよびネットワークの変化し続ける性質により、図6に示されるコンピューティングデバイス610の説明は、いくつかの実装形態を例示することを目的とする特定の例にすぎないことが意図される。コンピューティングデバイス610の多くの他の構成は、図6に示されるコンピューティングデバイスより多数または少数のコンポーネントを有することが可能である。 Computing device 610 may be of various types, including a workstation, server, computing cluster, blade server, server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computing device 610 shown in FIG. 6 is intended only as a specific example intended to illustrate some implementations. Many other configurations of computing device 610 can have more or fewer components than the computing device shown in FIG.
本明細書において論じられるいくつかの実装形態が、ユーザについての個人情報(たとえば、他の電子通信から抽出されるユーザデータ、ユーザのソーシャルネットワークについての情報、ユーザの位置、ユーザの時間、ユーザのバイオメトリック情報、ならびにユーザの活動および属性情報、ユーザ間の関係など)を収集または使用し得る状況では、情報が収集されるかどうか、個人情報が記憶されるかどうか、個人情報が使用されるかどうか、およびユーザについての情報がどのように収集され、記憶され使用されるかを制御するための1つまたは複数の機会が、ユーザに与えられる。すなわち、本明細書において論じられるシステムおよび方法は、ユーザの個人情報の収集、記憶、および/または使用を、関連するユーザからそうすることについての明確な承認を受け取った場合にのみ行う。 Some implementations discussed herein use personal information about the user (e.g., user data extracted from other electronic communications, information about the user's social networks, the user's location, the user's time, the user's biometric information, as well as user activity and demographic information, relationships between users, etc.), whether the information is collected, whether personal information is stored, and how personal information is used Users are given one or more opportunities to control whether and how information about them is collected, stored and used. That is, the systems and methods discussed herein will only collect, store, and/or use a user's personal information upon receiving explicit authorization to do so from the relevant user.
たとえば、ユーザは、プログラムまたは特徴が、その特定のユーザまたはそのプログラムもしくは特徴に関連する他のユーザについてのユーザ情報を収集するかどうかについて、制御権を与えられる。個人情報が収集されることになる各ユーザは、情報が収集されるかどうかについての、および情報のどの部分が収集されるべきかについての許可または承認を提供するために、そのユーザに関連する情報収集についての制御を可能にするための1つまたは複数の選択肢を提示される。たとえば、ユーザは、通信ネットワークを介して1つまたは複数のそのような制御の選択肢を与えられ得る。加えて、あるデータは、個人を特定可能な情報が取り除かれるように、それが記憶される前または使用される前に1つまたは複数の方法で取り扱われ得る。一例として、ユーザの識別情報は、個人を特定可能な情報を決定できないように扱われ得る。別の例として、ユーザの地理的位置は、ユーザの特定の位置を決定できないように、より広い領域に一般化され得る。 For example, a user is given control over whether a program or feature collects user information about that particular user or other users associated with the program or feature. Each user for whom personal information is to be collected will have an authorization to associate with that user to provide permission or authorization as to whether the information is collected and what parts of the information should be collected. You will be presented with one or more options to allow control over information collection. For example, a user may be provided with one or more such control options via a communication network. In addition, certain data may be treated in one or more ways before it is stored or used so that personally identifiable information is removed. As an example, a user's identity may be treated such that personally identifiable information cannot be determined. As another example, a user's geographic location may be generalized to a wider area such that the user's specific location cannot be determined.
いくつかの実装形態では、1つまたは複数のプロセッサによって実施される方法が提供され、複数のアシスタントデバイスを含むエコシステムの中に位置するアシスタントデバイスの1つまたは複数のマイクロフォンを介して、音響イベントを捉えるオーディオデータを検出するステップと、アシスタントデバイスにおいてローカルに記憶されているイベント検出モデルを使用して、音響イベントを捉えるオーディオデータを処理して、音響イベントに関連する尺度を生成するステップと、エコシステムの中に位置する追加のアシスタントデバイスの1つまたは複数のマイクロフォンを介して、音響イベントを同様に捉える追加のオーディオデータを検出するステップであって、追加のアシスタントデバイスがアシスタントデバイスに追加するものであり、追加のアシスタントデバイスがアシスタントデバイスとエコシステムにおいて同じ位置にある、ステップと、追加のアシスタントデバイスにおいてローカルに記憶されている追加のイベント検出モデルを使用して、音響イベントを捉える追加のオーディオデータを処理して、音響イベントに関連する追加の尺度を生成するステップと、尺度と追加の尺度の両方を処理して、少なくともアシスタントデバイスと追加のアシスタントデバイスの両方によって検出される音響イベントが実際の音響イベントであるかどうかを決定するステップと、音響イベントが実際の音響イベントであると決定したことに応答して、実際の音響イベントに関連する行動が実行されるようにするステップとを含む。 In some implementations, a method implemented by one or more processors is provided to transmit acoustic events via one or more microphones of assistant devices located in an ecosystem including multiple assistant devices. and processing the audio data capturing the acoustic event using an event detection model stored locally in the assistant device to generate a measure associated with the acoustic event; Detecting additional audio data that also captures the acoustic event via one or more microphones of additional assistant devices located in the ecosystem, the additional assistant devices adding to the assistant device. , where the additional assistant device is co-located in the ecosystem with the assistant device, steps and additional steps to capture acoustic events using additional event detection models stored locally in the additional assistant device. processing the audio data to generate an additional measure associated with the acoustic event; and processing both the measure and the additional measure so that at least the acoustic event detected by both the assistant device and the additional assistant device is determining whether the acoustic event is an actual acoustic event; and causing an action associated with the actual acoustic event to be performed in response to determining that the acoustic event is an actual acoustic event. include.
本明細書において開示される技術のこれらおよび他の実装形態は、以下の特徴の1つまたは複数を含み得る。 These and other implementations of the technology disclosed herein can include one or more of the following features.
いくつかの実装形態では、音響イベントはホットワード検出イベントを含んでもよく、アシスタントデバイスにおいてローカルに記憶されているイベント検出モデルは、特定の語または語句がオーディオデータにおいて捉えられるかどうかを検出するように訓練されるホットワード検出モデルであってもよく、追加のアシスタントデバイスにおいてローカルに記憶されている追加のイベント検出モデルは、特定の語または語句が追加のオーディオデータにおいて捉えられるかどうかを検出するように訓練される追加のホットワード検出モデルであってもよい。 In some implementations, the acoustic events may include hot word detection events, and an event detection model stored locally on the assistant device is configured to detect whether a particular word or phrase is captured in the audio data. An additional event detection model, which may be a hot word detection model trained on the additional assistant device, is locally stored in the additional assistant device to detect whether a particular word or phrase is captured in the additional audio data. There may be additional hotword detection models trained as follows.
それらの実装形態のいくつかのバージョンでは、音響イベントに関連する尺度は、オーディオデータが特定の語または語句を捉えるかどうかに対応する信頼性レベルであってもよく、音響イベントに関連する追加の尺度は、追加のオーディオデータが特定の語または語句を捉えるかどうかに対応する追加の信頼性レベルであってもよい。音響イベントが実際の音響イベントであると決定するそれらの実装形態のいくつかのさらなるバージョンは、信頼性レベルおよび追加の信頼性レベルに基づいて、特定の語または語句がオーディオデータと追加のオーディオデータの両方において捉えられると決定することを含み得る。それらの実装形態のまたさらなるバージョンでは、実際の音響イベントに関連する行動が実行されるようにすることは、オーディオデータまたは追加のオーディオデータが特定の語または語句を捉えることを音響イベントデータが示すと決定したことに応答して、自動化アシスタントの1つまたは複数のコンポーネントをアクティブ化することを含み得る。 In some versions of those implementations, the measure associated with the acoustic event may be a confidence level corresponding to whether the audio data captures a particular word or phrase, and additional measures associated with the acoustic event. A measure may be an additional confidence level corresponding to whether additional audio data captures a particular word or phrase. Some further versions of those implementations that determine that a sound event is an actual sound event are based on a confidence level and an additional confidence level, based on the confidence level and the additional confidence level, that the particular word or phrase is the audio data and the additional audio data. can include determining that it is captured in both In still further versions of those implementations, causing the action associated with the actual sound event to be performed is the sound event data indicating that the audio data or additional audio data captures a particular word or phrase. activating one or more components of the automated assistant in response to determining that.
それらの実装形態のいくつかの追加または代替のバージョンでは、アシスタントデバイスにおいてローカルに記憶されているホットワード検出モデルは、追加のアシスタントデバイスにローカルに記憶されている追加のホットワード検出モデルとは別の別個のホットワードモデルであり得る。 In some additional or alternative versions of those implementations, the hotword detection models stored locally on the assistant device are separate from the additional hotword detection models locally stored on the additional assistant devices. can be a separate hotword model of
いくつかの実装形態では、音響イベントは音検出イベントであってもよく、アシスタントデバイスにおいてローカルに記憶されているイベント検出モデルは、特定の音がオーディオデータにおいて捉えられるかどうかを検出するように訓練される音検出モデルであってもよく、追加のアシスタントデバイスにおいてローカルに記憶されている追加のイベント検出モデルは、捉えられる特定の音が追加のオーディオデータであるかどうかを検出するように訓練される追加の音検出モデルであってもよい。 In some implementations, the acoustic event may be a sound detection event, and an event detection model stored locally on the assistant device is trained to detect whether a particular sound is captured in the audio data. and an additional event detection model locally stored in the additional assistant device is trained to detect whether the particular sound being caught is additional audio data. additional sound detection models.
それらの実装形態のいくつかのバージョンでは、音響イベントに関連する尺度は、オーディオデータが特定の音を捉えるかどうかに対応する信頼性レベルであってもよく、音響イベントに関連する追加の尺度は、追加のオーディオデータが特定の音を捉えるかどうかに対応する追加の信頼性レベルであってもよい。それらの実装形態のいくつかのさらなるバージョンでは、音響イベントが実際の音響イベントであると決定することは、信頼性レベルおよび追加の信頼性レベルに基づいて、オーディオデータと追加のオーディオデータの両方において特定の音が捉えられると決定することを含み得る。それらの実装形態のまたさらなるバージョンでは、実際の音響イベントに関連する行動が実行されるようにすることは、実際の音響イベントの発生を示す通知を生成することと、ユーザのコンピューティングデバイスを介してエコシステムに関連するユーザに通知が提示されるようにすることとを含み得る。 In some versions of those implementations, the measure associated with the acoustic event may be a confidence level corresponding to whether the audio data captures the particular sound, and the additional measure associated with the acoustic event is , an additional confidence level corresponding to whether the additional audio data captures a particular sound. In some further versions of those implementations, determining that an acoustic event is an actual acoustic event is performed in both the audio data and the additional audio data based on the confidence level and the additional confidence level. It may involve determining that a particular sound is captured. In still further versions of those implementations, causing the action associated with the actual acoustic event to be performed includes generating a notification indicating the occurrence of the actual acoustic event; and causing notifications to be presented to users associated with the ecosystem.
それらの実装形態のいくつかの追加または代替のバージョンでは、特定の音は、ガラスが割れること、犬が吠えること、猫が鳴くこと、呼び鈴が鳴ること、火災報知器が鳴動すること、一酸化炭素検出器が鳴動すること、乳児が泣くこと、またはドアをノックすることのうちの1つまたは複数を含み得る。 In some additional or alternative versions of those implementations, the specific sounds are breaking glass, dogs barking, cats meowing, doorbells ringing, fire alarms ringing, monoxide It may include one or more of a carbon detector ringing, a baby crying, or a knock on the door.
いくつかの実装形態では、尺度と追加の尺度の両方を処理して、アシスタントデバイスと追加のアシスタントデバイスの両方によって検出される音響イベントが実際の音響イベントであるかどうかを決定することは、所与のアシスタントデバイスによるものであり、所与のアシスタントデバイスは、アシスタントデバイス、追加のアシスタントデバイス、または、アシスタントデバイスおよび追加のアシスタントデバイスとエコシステムにおいて同じ位置にあるさらなる追加のアシスタントデバイスのうちの1つまたは複数であり得る。 In some implementations, processing both the scale and the additional scale to determine whether an acoustic event detected by both the assistant device and the additional assistant device is an actual acoustic event is not an option. by a given assistant device, where the given assistant device is one of an assistant device, an additional assistant device, or an assistant device and a further additional assistant device co-located in the ecosystem with the additional assistant device can be one or more.
いくつかの実装形態では、方法はさらに、アシスタントデバイスによって、オーディオデータを遠隔システムに送信するステップと、追加のアシスタントデバイスによって、追加のオーディオデータを遠隔システムに送信するステップとを含み得る。これらの実装形態では、尺度と追加の尺度の両方を処理して、アシスタントデバイスと追加のアシスタントデバイスの両方によって検出される音響イベントが実際の音響イベントであるかどうかを決定することは、遠隔システムによるものであり得る。 In some implementations, the method may further include sending the audio data to the remote system via the assistant device and sending additional audio data to the remote system via the additional assistant device. In these implementations, processing both the metric and the additional metric to determine whether the acoustic events detected by both the assistant device and the additional assistant device are actual acoustic events is the remote system can be due to
いくつかの実装形態では、オーディオデータは一時的に、追加のオーディオデータに対応する。それらの実装形態のいくつかのバージョンでは、尺度と追加の尺度の両方を処理して、アシスタントデバイスと追加のアシスタントデバイスの両方によって検出される音響イベントが実際の音響イベントであるかどうかを決定することは、オーディオデータに関連するタイムスタンプが追加のオーディオデータに関連する追加のタイムスタンプに時間的に対応すると決定することに応答したものであってもよい。 In some implementations, the audio data temporarily corresponds to additional audio data. Some versions of those implementations process both the scale and the additional scale to determine whether the acoustic events detected by both the assistant device and the additional assistant device are actual acoustic events. This may be in response to determining that the timestamp associated with the audio data temporally corresponds to the additional timestamp associated with the additional audio data.
いくつかの実装形態では、方法はさらに、アシスタントデバイスの1つまたは複数のマイクロフォンを介してオーディオデータを検出したことに応答して、複数の過去の音響イベントがアシスタントデバイスと追加のアシスタントデバイスの両方において検出されていることに基づいて、追加のアシスタントデバイスの1つまたは複数の追加のマイクロフォンを介した追加のオーディオデータの検出を予想するステップを含み得る。 In some implementations, the method further comprises detecting the plurality of past acoustic events on both the assistant device and the additional assistant device in response to detecting audio data via one or more microphones of the assistant device. anticipating detection of additional audio data via one or more additional microphones of the additional assistant device based on what has been detected in the.
いくつかの実装形態では、方法はさらに、アシスタントデバイスの1つまたは複数のマイクロフォンを介して、後続の音響イベントを捉える後続のオーディオデータを検出するステップと、イベント検出モデルを使用して、後続の音響イベントを捉える後続のオーディオデータを処理して、後続の音響イベントに関連する後続の尺度を生成するステップと、エコシステムの中に位置するさらなる追加のアシスタントデバイスの1つまたは複数のさらなる追加のマイクロフォンを介して、後続の音響イベントを同様に捉える追加の後続のオーディオデータを検出するステップであって、さらなる追加のアシスタントデバイスがアシスタントデバイスに追加するものであり、さらなる追加のアシスタントデバイスがアシスタントデバイスとエコシステムにおいて同じ位置にある、ステップと、さらなる追加のアシスタントデバイスにおいてローカルに記憶されているさらなる追加のイベント検出モデルを使用して、音響イベントを捉える追加の後続のオーディオデータを処理して、音響イベントに関連する追加の後続の尺度を生成するステップと、尺度と追加の尺度の両方を処理して、アシスタントデバイスとさらなる追加のアシスタントデバイスの両方によって検出される後続の音響イベントが実際の後続の音響イベントであるかどうかを決定するステップと、後続の音響イベントが実際の後続の音響イベントであると決定したことに応答して、実際の後続の音響イベントに関連する後続の行動が実行されるようにするステップとを含み得る。 In some implementations, the method further detects, via one or more microphones of the assistant device, subsequent audio data capturing subsequent acoustic events; processing subsequent audio data capturing the acoustic event to generate subsequent measures associated with the subsequent acoustic event; and one or more additional additional assistant devices located within the ecosystem. detecting, via a microphone, additional subsequent audio data that also captures subsequent acoustic events, the additional assistant device adding to the assistant device, the additional assistant device adding to the assistant device; processing additional subsequent audio data that captures the acoustic event using further additional event detection models stored locally in steps and further additional assistant devices co-located in the ecosystem with generating an additional subsequent measure associated with the acoustic event and processing both the measure and the additional measure such that the subsequent acoustic event detected by both the assistant device and the further additional assistant device is the actual subsequent measure; and performing subsequent actions associated with the actual subsequent sound event in response to determining that the subsequent sound event is the actual subsequent sound event. and the step of
それらの実装形態のいくつかの追加バージョンでは、方法はさらに、アシスタントデバイスの1つまたは複数のマイクロフォンを介して後続のオーディオデータを検出したことに応答して、さらなる追加のアシスタントデバイスの1つまたは複数のさらなる追加のマイクロフォンを介した追加の後続のオーディオデータの検出を予想するステップと、追加の複数の過去の音響イベントがアシスタントデバイスとさらなる追加のアシスタントデバイスの両方において検出されていることに基づいて、追加のアシスタントデバイスの1つまたは複数の追加のマイクロフォンを介したどのようなオーディオデータの検出も予想しないステップとを含み得る。 In some additional versions of those implementations, the method further comprises one or more additional assistant devices in response to detecting subsequent audio data via one or more microphones of the assistant device. Based on anticipating the detection of additional subsequent audio data via the plurality of additional microphones and the additional plurality of past acoustic events being detected at both the assistant device and the additional assistant device. and not anticipating detection of any audio data via one or more additional microphones of the additional assistant device.
いくつかの実装形態では、1つまたは複数のプロセッサによって実施される方法が提供され、複数のアシスタントデバイスを含むエコシステムの中に位置するアシスタントデバイスの1つまたは複数のマイクロフォンを介して、音響イベントを捉えるオーディオデータを検出するステップと、エコシステム内のアシスタントデバイスの位置に基づいて、エコシステムの中の少なくとも1つの追加のアシスタントデバイスの1つまたは複数のそれぞれのマイクロフォンを介してオーディオデータに時間的に対応する追加のオーディオデータを検出したはずである少なくとも1つの追加のアシスタントデバイスを特定するステップと、少なくとも1つの追加のアシスタントデバイスが、オーディオデータに時間的に対応する追加のオーディオデータを検出したと決定したことに応答して、アシスタントデバイスにおいてローカルに記憶されているイベント検出モデルを使用して、音響イベントを捉えるオーディオデータを処理して、音響イベントに関連する尺度を生成するステップと、少なくとも1つの追加のアシスタントデバイスにおいてローカルに記憶されているそれぞれのイベント検出モデルを使用して、音響イベントを捉える追加のオーディオデータを処理して、音響イベントに関連する追加の尺度を生成するステップと、尺度と追加の尺度の両方に基づいて、アシスタントデバイスと追加のアシスタントデバイスの両方によって検出される音響イベントが実際の音響イベントであるかどうかを決定するステップと、音響イベントが実際の音響イベントであると決定したことに応答して、実際の音響イベントに関連する行動が実行されるようにするステップとを含む。 In some implementations, a method implemented by one or more processors is provided to transmit acoustic events via one or more microphones of assistant devices located in an ecosystem including multiple assistant devices. and time-to-audio data via one or more respective microphones of at least one additional assistant device in the ecosystem based on the position of the assistant device in the ecosystem. identifying at least one additional assistant device that would have detected additional audio data temporally corresponding to the audio data; processing the audio data capturing the acoustic event using an event detection model stored locally in the assistant device to generate a measure associated with the acoustic event, in response to determining that the acoustic event has been performed; processing additional audio data capturing the acoustic event using respective event detection models stored locally in at least one additional assistant device to generate additional measures associated with the acoustic event; , determining whether an acoustic event detected by both the assistant device and the additional assistant device is an actual acoustic event based on both the measure and the additional measure; and determining whether the acoustic event is an actual acoustic event. and causing an action associated with the actual acoustic event to be performed in response to determining that there is.
本明細書において開示される技術のこれらおよび他の実装形態は、以下の特徴のうちの1つまたは複数を含み得る。 These and other implementations of the technology disclosed herein can include one or more of the following features.
いくつかの実装形態では、方法はさらに、少なくとも1つの追加のアシスタントデバイスがオーディオデータに時間的に対応するどのようなオーディオデータも検出しなかったと決定したことに応答して、オーディオデータを廃棄するステップを含み得る。 In some implementations, the method further discards the audio data in response to determining that the at least one additional assistant device did not detect any audio data temporally corresponding to the audio data. can include steps.
加えて、いくつかの実装形態は、1つまたは複数のコンピューティングデバイスの1つまたは複数のプロセッサ(たとえば、中央処理装置(CPU)、グラフィクス処理装置(GPUおよび/またはテンソル処理ユニット(TPU)))を含み、1つまたは複数のプロセッサは関連するメモリに記憶されている命令を実行するように動作可能であり、命令は前述の方法のいずれかの実行を引き起こすように構成される。いくつかの実装形態はまた、前述の方法のいずれかを実行するように1つまたは複数のプロセッサによって実行可能なコンピュータ命令を記憶する、1つまたは複数の非一時的コンピュータ可読記憶媒体を含む。いくつかの実装形態は、前述の方法のいずれかを実行するように1つまたは複数のプロセッサによって実行可能な命令を含むコンピュータプログラム製品も含む。 In addition, some implementations use one or more processors (e.g., central processing units (CPUs), graphics processing units (GPUs and/or tensor processing units (TPUs))) of one or more computing devices. ), the one or more processors being operable to execute instructions stored in associated memories, the instructions being configured to cause execution of any of the aforementioned methods. Some implementations also include one or more non-transitory computer-readable storage media storing computer instructions executable by one or more processors to perform any of the methods described above. Some implementations also include a computer program product comprising instructions executable by one or more processors to perform any of the methods described above.
本明細書においてより詳しく説明される前述の概念および追加の概念のすべての組合せが、本明細書において開示される主題の一部であるものとして考えられることを理解されたい。たとえば、本開示の終わりに現れる特許請求される主題のすべての組合せが、本明細書において開示される主題の一部であると考えられる。 It should be understood that all combinations of the foregoing concepts and additional concepts more fully described herein are considered to be part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are considered part of the subject matter disclosed herein.
105 存在センサ
106 アシスタント入力デバイス
107 ユーザインターフェースコンポーネント
110 ネットワーク
114 発話捕捉/TTS/STTモジュール
116 クラウドベースのTTSモジュール
117 クラウドベースのSTTモジュール
118 自動化アシスタントクライアント
119 クラウドベースの自動化アシスタントコンポーネント
120 自動化アシスタント
122 NLPモジュール
130 イベント検出エンジン
140 デバイス特定エンジン
150 イベント処理エンジン
160 セマンティック標識エンジン
170 クエリ/コマンド処理エンジン
180 アシスタント非入力システム
185 アシスタント非入力デバイス
191 デバイス活動データベース
192 MLモデルデータベース
193 デバイストポロジーデータベース
250 部屋
252 部屋
254 部屋
256 部屋
258 部屋
260 部屋
262 部屋
610 コンピューティングデバイス
612 バスサブシステム
614 プロセッサ
616 ネットワークインターフェース
620 ユーザインターフェース出力デバイス
622 ユーザインターフェース入力デバイス
624 ストレージサブシステム
625 メモリサブシステム
626 ファイルストレージサブシステム
630 RAM
632 ROM
105 Presence sensor
106 assistant input device
107 User Interface Components
110 network
114 Speech Capture/TTS/STT Module
116 cloud-based TTS module
117 cloud-based STT module
118 Automation Assistant Client
119 Cloud-Based Automation Assistant Components
120 Automation Assistant
122 NLP module
130 event detection engine
140 device identification engine
150 event processing engine
160 Semantic Signage Engine
170 query/command processing engine
180 assistant non-input system
185 assistant non-input device
191 device activity database
192 ML model database
193 Device Topology Database
250 rooms
252 rooms
254 rooms
256 rooms
258 rooms
260 rooms
262 rooms
610 Computing Device
612 Bus Subsystem
614 processor
616 network interface
620 User Interface Output Device
622 user interface input device
624 storage subsystem
625 memory subsystem
626 file storage subsystem
630 RAM
632 ROMs
Claims (24)
複数のアシスタントデバイスを含むエコシステムの中に位置するアシスタントデバイスの1つまたは複数のマイクロフォンを介して、音響イベントを捉えるオーディオデータを検出するステップと、
前記アシスタントデバイスにおいてローカルに記憶されているイベント検出モデルを使用して、前記音響イベントを捉える前記オーディオデータを処理して、前記音響イベントに関連する尺度を生成するステップと、
前記エコシステムの中に位置する追加のアシスタントデバイスの1つまたは複数の追加のマイクロフォンを介して、前記音響イベントを同様に捉える追加のオーディオデータを検出するステップであって、前記追加のアシスタントデバイスが前記アシスタントデバイスに追加するものであり、前記追加のアシスタントデバイスが前記アシスタントデバイスと前記エコシステムにおいて同じ位置にある、ステップと、
前記追加のアシスタントデバイスにおいてローカルに記憶されている追加のイベント検出モデルを使用して、前記音響イベントを捉える前記追加のオーディオデータを処理して、前記音響イベントに関連する追加の尺度を生成するステップと、
前記尺度と前記追加の尺度の両方を処理して、少なくとも前記アシスタントデバイスと前記追加のアシスタントデバイスの両方によって検出される前記音響イベントが実際の音響イベントであるかどうかを決定するステップと、
前記音響イベントが前記実際の音響イベントであると決定したことに応答して、前記実際の音響イベントに関連する行動が実行されるようにするステップとを備える、方法。 A method performed by one or more processors comprising:
detecting audio data capturing the acoustic event via one or more microphones of an assistant device located in an ecosystem including multiple assistant devices;
processing the audio data capturing the acoustic event using an event detection model stored locally in the assistant device to generate a measure associated with the acoustic event;
detecting additional audio data that also captures the acoustic event via one or more additional microphones of additional assistant devices located in the ecosystem, wherein the additional assistant devices adding to said assistant device, said additional assistant device being co-located in said ecosystem with said assistant device;
Processing the additional audio data capturing the acoustic event using an additional event detection model stored locally in the additional assistant device to generate additional measures associated with the acoustic event. and,
processing both the measure and the additional measure to determine whether the sound event detected by at least both the assistant device and the additional assistant device is an actual sound event;
and responsive to determining that the sound event is the actual sound event, causing an action associated with the actual sound event to be performed.
前記実際の音響イベントの発生を示す通知を生成するステップと、
前記エコシステムに関連するユーザに、前記ユーザのコンピューティングデバイスを介して前記通知が提示されるようにするステップとを備える、請求項7から9のいずれか一項に記載の方法。 causing the action associated with the actual acoustic event to be performed;
generating a notification indicating the occurrence of the actual acoustic event;
and causing a user associated with said ecosystem to be presented with said notification via said user's computing device.
前記追加のアシスタントデバイスによって、前記追加のオーディオデータを前記遠隔システムに送信するステップとをさらに備え、
前記尺度と前記追加の尺度の両方を処理して、前記アシスタントデバイスと前記追加のアシスタントデバイスの両方によって検出される前記音響イベントが前記実際の音響イベントであるかどうかを決定することが、前記遠隔システムによるものである、請求項1から12のいずれか一項に記載の方法。 transmitting, by the assistant device, the audio data to a remote system;
transmitting said additional audio data to said remote system by said additional assistant device;
processing both the measure and the additional measure to determine whether the acoustic event detected by both the assistant device and the additional assistant device is the actual acoustic event; 13. The method of any one of claims 1-12, by a system.
複数の過去の音響イベントが前記アシスタントデバイスと前記追加のアシスタントデバイスの両方において検出されていることに基づいて、前記追加のアシスタントデバイスの前記1つまたは複数の追加のマイクロフォンを介した前記追加のオーディオデータの検出を予想するステップをさらに含む、請求項1から15のいずれか一項に記載の方法。 In response to detecting said audio data via said one or more microphones of said assistant device,
said additional audio via said one or more additional microphones of said additional assistant device based on a plurality of past sound events being detected in both said assistant device and said additional assistant device; 16. The method of any one of claims 1-15, further comprising anticipating detection of data.
前記イベント検出モデルを使用して、前記後続の音響イベントを捉える前記後続のオーディオデータを処理して、前記後続の音響イベントに関連する後続の尺度を生成するステップと、
前記エコシステムの中に位置するさらなる追加のアシスタントデバイスの1つまたは複数のさらなる追加のマイクロフォンを介して、前記後続の音響イベントを同様に捉える追加の後続のオーディオデータを検出するステップであって、前記さらなる追加のアシスタントデバイスが前記アシスタントデバイスに追加するものであり、前記さらなる追加のアシスタントデバイスが前記アシスタントデバイスと前記エコシステムにおいて同じ位置にある、ステップと、
前記さらなる追加のアシスタントデバイスにおいてローカルに記憶されているさらなる追加のイベント検出モデルを使用して、前記音響イベントを捉える前記追加の後続のオーディオデータを処理して、前記音響イベントに関連する追加の後続の尺度を生成するステップと、
前記尺度と前記追加の尺度の両方を処理して、前記アシスタントデバイスと前記さらなる追加のアシスタントデバイスの両方によって検出される前記後続の音響イベントが実際の後続の音響イベントであるかどうかを決定するステップと、
前記後続の音響イベントが前記実際の後続の音響イベントであると決定したことに応答して、前記実際の後続の音響イベントに関連する後続の行動が実行されるようにするステップとをさらに備える、請求項1から16のいずれか一項に記載の方法。 detecting subsequent audio data capturing subsequent acoustic events via the one or more microphones of the assistant device;
processing the subsequent audio data capturing the subsequent acoustic event using the event detection model to generate a subsequent measure associated with the subsequent acoustic event;
detecting additional subsequent audio data that similarly captures the subsequent acoustic event via one or more additional microphones of additional additional assistant devices located within the ecosystem; said further additional assistant device is in addition to said assistant device, said further additional assistant device is co-located in said ecosystem with said assistant device;
processing the additional subsequent audio data capturing the acoustic event using a further additional event detection model stored locally in the additional assistant device to generate additional subsequent audio data associated with the acoustic event; generating a measure of
processing both said measure and said additional measure to determine whether said subsequent sound event detected by both said assistant device and said further additional assistant device is an actual subsequent sound event; and,
and causing subsequent actions associated with the actual subsequent sound event to be performed in response to determining that the subsequent sound event is the actual subsequent sound event. 17. A method according to any one of claims 1-16.
前記さらなる追加のアシスタントデバイスの前記1つまたは複数のさらなる追加のマイクロフォンを介した前記追加の後続のオーディオデータの検出を予想するステップと、追加の複数の過去の音響イベントが前記アシスタントデバイスと前記さらなる追加のアシスタントデバイスの両方において検出されていることに基づいて、前記追加のアシスタントデバイスの前記1つまたは複数の追加のマイクロフォンを介したどのようなオーディオデータの検出も予想しないステップとをさらに備える、請求項17に記載の方法。 in response to detecting said subsequent audio data via said one or more microphones of said assistant device;
anticipating detection of the additional subsequent audio data via the one or more additional microphones of the additional assistant device; not anticipating detection of any audio data via said one or more additional microphones of said additional assistant device based on being detected on both additional assistant devices; 18. The method of claim 17.
複数のアシスタントデバイスを含むエコシステムの中に位置するアシスタントデバイスの1つまたは複数のマイクロフォンを介して、音響イベントを捉えるオーディオデータを検出するステップと、
前記エコシステム内の前記アシスタントデバイスの位置に基づいて、前記エコシステムの中の前記少なくとも1つの追加のアシスタントデバイスの1つまたは複数のそれぞれのマイクロフォンを介して前記オーディオデータに時間的に対応する追加のオーディオデータを検出したはずである少なくとも1つの追加のアシスタントデバイスを特定するステップと、
前記少なくとも1つの追加のアシスタントデバイスが、前記オーディオデータに時間的に対応する前記追加のオーディオデータを検出したと決定したことに応答して、
前記アシスタントデバイスにおいてローカルに記憶されているイベント検出モデルを使用して、前記音響イベントを捉える前記オーディオデータを処理して、前記音響イベントに関連する尺度を生成するステップと、
前記少なくとも1つの追加のアシスタントデバイスにおいてローカルに記憶されているそれぞれのイベント検出モデルを使用して、前記音響イベントを捉える前記追加のオーディオデータを処理して、前記音響イベントに関連する追加の尺度を生成するステップと、
前記尺度と前記追加の尺度の両方に基づいて、前記アシスタントデバイスと前記追加のアシスタントデバイスの両方によって検出される前記音響イベントが実際の音響イベントであるかどうかを決定するステップと、
前記音響イベントが前記実際の音響イベントであると決定したことに応答して、前記実際の音響イベントに関連する行動が実行されるようにするステップとを備える、方法。 A method performed by one or more processors comprising:
detecting audio data capturing the acoustic event via one or more microphones of an assistant device located in an ecosystem including multiple assistant devices;
Based on the location of the assistant device within the ecosystem, an addition temporally corresponding to the audio data via one or more respective microphones of the at least one additional assistant device within the ecosystem. identifying at least one additional assistant device that should have detected the audio data of
In response to determining that the at least one additional assistant device has detected the additional audio data temporally corresponding to the audio data;
processing the audio data capturing the acoustic event using an event detection model stored locally in the assistant device to generate a measure associated with the acoustic event;
processing the additional audio data capturing the acoustic event using respective event detection models locally stored in the at least one additional assistant device to generate additional measures associated with the acoustic event; a step of generating;
determining whether the acoustic event detected by both the assistant device and the additional assistant device is an actual acoustic event based on both the measure and the additional measure;
and responsive to determining that the sound event is the actual sound event, causing an action associated with the actual sound event to be performed.
複数のアシスタントデバイスを含むエコシステムの中に位置するアシスタントデバイスの1つまたは複数のマイクロフォンを介して、音響イベントを捉えるオーディオデータを検出するステップであって、前記音響イベントがホットワード検出イベントを備える、ステップと、
前記アシスタントデバイスにおいてローカルに記憶されているイベント検出モデルを使用して、前記音響イベントを捉える前記オーディオデータを処理して、前記音響イベントに関連する尺度を生成するステップであって、前記アシスタントデバイスにおいてローカルに記憶されている前記イベント検出モデルが、特定の語または語句が前記オーディオデータにおいて捉えられるかどうかを検出するように訓練されるホットワード検出モデルを備える、ステップと、
前記エコシステムの中に位置する追加のアシスタントデバイスの1つまたは複数の追加のマイクロフォンを介して、前記音響イベントを同様に捉える追加のオーディオデータを検出するステップであって、前記追加のアシスタントデバイスが前記アシスタントデバイスに追加するものであり、前記追加のアシスタントデバイスが前記アシスタントデバイスと前記エコシステムにおいて同じ位置にある、ステップと、
前記追加のアシスタントデバイスにおいてローカルに記憶されている追加のイベント検出モデルを使用して、前記音響イベントを捉える前記追加のオーディオデータを処理して、前記音響イベントに関連する追加の尺度を生成するステップであって、前記追加のアシスタントデバイスにおいてローカルに記憶されている前記追加のイベント検出モデルが、前記特定の語または語句が前記追加のオーディオデータにおいて捉えられるかどうかを検出するように訓練される追加のホットワード検出モデルを備える、ステップと、
前記尺度と前記追加の尺度の両方を処理して、少なくとも前記アシスタントデバイスと前記追加のアシスタントデバイスの両方によって検出される前記音響イベントが実際の音響イベントであるかどうかを決定するステップと、
前記音響イベントが前記実際の音響イベントであると決定したことに応答して、前記実際の音響イベントに関連する行動が実行されるようにするステップとを備える、方法。 A method performed by one or more processors comprising:
Detecting, via one or more microphones of an assistant device located in an ecosystem comprising a plurality of assistant devices, audio data capturing acoustic events, said acoustic events comprising hot word detection events. , step and
processing the audio data capturing the acoustic event to generate a measure associated with the acoustic event using an event detection model stored locally at the assistant device; said locally stored event detection model comprising a hot word detection model trained to detect whether a particular word or phrase is captured in said audio data;
detecting additional audio data that also captures the acoustic event via one or more additional microphones of additional assistant devices located in the ecosystem, wherein the additional assistant devices adding to said assistant device, said additional assistant device being co-located in said ecosystem with said assistant device;
Processing the additional audio data capturing the acoustic event using an additional event detection model stored locally in the additional assistant device to generate additional measures associated with the acoustic event. wherein said additional event detection model stored locally in said additional assistant device is trained to detect whether said specific word or phrase is captured in said additional audio data. a step comprising a hotword detection model of
processing both the measure and the additional measure to determine whether the sound event detected by at least both the assistant device and the additional assistant device is an actual sound event;
and responsive to determining that the sound event is the actual sound event, causing an action associated with the actual sound event to be performed.
複数のアシスタントデバイスを含むエコシステムの中に位置するアシスタントデバイスの1つまたは複数のマイクロフォンを介して、音響イベントを捉えるオーディオデータを検出するステップであって、前記音響イベントが音検出イベントを備える、ステップと、
前記アシスタントデバイスにおいてローカルに記憶されているイベント検出モデルを使用して、前記音響イベントを捉える前記オーディオデータを処理して、前記音響イベントに関連する尺度を生成するステップであって、前記アシスタントデバイスにおいてローカルに記憶されている前記イベント検出モデルが、特定の音が前記オーディオデータにおいて捉えられるかどうかを検出するように訓練される音検出モデルを備える、ステップと、
前記エコシステムの中に位置する追加のアシスタントデバイスの1つまたは複数の追加のマイクロフォンを介して、前記音響イベントを同様に捉える追加のオーディオデータを検出するステップであって、前記追加のアシスタントデバイスが前記アシスタントデバイスに追加するものであり、前記追加のアシスタントデバイスが前記アシスタントデバイスと前記エコシステムにおいて同じ位置にある、ステップと、
前記追加のアシスタントデバイスにおいてローカルに記憶されている追加のイベント検出モデルを使用して、前記音響イベントを捉える前記追加のオーディオデータを処理して、前記音響イベントに関連する追加の尺度を生成するステップと、
前記尺度と前記追加の尺度の両方を処理して、少なくとも前記アシスタントデバイスと前記追加のアシスタントデバイスの両方によって検出される前記音響イベントが実際の音響イベントであるかどうかを決定するステップであって、前記追加のアシスタントデバイスにおいてローカルに記憶されている前記追加のイベント検出モデルが、前記特定の音が前記追加のオーディオデータにおいて捉えられるかどうかを検出するように訓練される追加の音検出モデルを備える、ステップと、
前記音響イベントが前記実際の音響イベントであると決定したことに応答して、前記実際の音響イベントに関連する行動が実行されるようにするステップとを備える、方法。 A method performed by one or more processors comprising:
detecting audio data capturing an acoustic event via one or more microphones of an assistant device located in an ecosystem including multiple assistant devices, said acoustic event comprising a sound detection event; a step;
processing the audio data capturing the acoustic event to generate a measure associated with the acoustic event using an event detection model stored locally at the assistant device; said locally stored event detection model comprising a sound detection model trained to detect whether a particular sound is captured in said audio data;
detecting additional audio data that also captures the acoustic event via one or more additional microphones of additional assistant devices located in the ecosystem, wherein the additional assistant devices adding to said assistant device, said additional assistant device being co-located in said ecosystem with said assistant device;
Processing the additional audio data capturing the acoustic event using an additional event detection model stored locally in the additional assistant device to generate additional measures associated with the acoustic event. and,
processing both the measure and the additional measure to determine whether the sound event detected by at least both the assistant device and the additional assistant device is an actual sound event; The additional event detection model locally stored in the additional assistant device comprises an additional sound detection model trained to detect whether the particular sound is captured in the additional audio data. , step and
and responsive to determining that the sound event is the actual sound event, causing an action associated with the actual sound event to be performed.
実行されると、前記少なくとも1つのプロセッサに請求項1から22のいずれか一項に記載の方法を実行させる命令を記憶するメモリとを備える、少なくとも1つのコンピューティングデバイス。 at least one processor;
and a memory that stores instructions that, when executed, cause the at least one processor to perform the method of any one of claims 1 to 22.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/085,926 US11798530B2 (en) | 2020-10-30 | 2020-10-30 | Simultaneous acoustic event detection across multiple assistant devices |
US17/085,926 | 2020-10-30 | ||
PCT/US2020/064988 WO2022093291A1 (en) | 2020-10-30 | 2020-12-15 | Simultaneous acoustic event detection across multiple assistant devices |
Publications (1)
Publication Number | Publication Date |
---|---|
JP2023534367A true JP2023534367A (en) | 2023-08-09 |
Family
ID=74003965
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2022569600A Pending JP2023534367A (en) | 2020-10-30 | 2020-12-15 | Simultaneous acoustic event detection across multiple assistant devices |
Country Status (6)
Country | Link |
---|---|
US (2) | US11798530B2 (en) |
EP (1) | EP4042414A1 (en) |
JP (1) | JP2023534367A (en) |
KR (1) | KR20230015980A (en) |
CN (1) | CN115605949A (en) |
WO (1) | WO2022093291A1 (en) |
Families Citing this family (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11756531B1 (en) * | 2020-12-18 | 2023-09-12 | Vivint, Inc. | Techniques for audio detection at a control system |
US20210225374A1 (en) * | 2020-12-23 | 2021-07-22 | Intel Corporation | Method and system of environment-sensitive wake-on-voice initiation using ultrasound |
KR20220111078A (en) * | 2021-02-01 | 2022-08-09 | 삼성전자주식회사 | Electronic apparatus, system comprising sound i/o device and controlling method thereof |
Family Cites Families (19)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8364481B2 (en) * | 2008-07-02 | 2013-01-29 | Google Inc. | Speech recognition with parallel recognition tasks |
US9058805B2 (en) * | 2013-05-13 | 2015-06-16 | Google Inc. | Multiple recognizer speech recognition |
US9002835B2 (en) * | 2013-08-15 | 2015-04-07 | Google Inc. | Query response using media consumption history |
US8719039B1 (en) * | 2013-12-05 | 2014-05-06 | Google Inc. | Promoting voice actions to hotwords |
JP6118838B2 (en) * | 2014-08-21 | 2017-04-19 | 本田技研工業株式会社 | Information processing apparatus, information processing system, information processing method, and information processing program |
US9812128B2 (en) * | 2014-10-09 | 2017-11-07 | Google Inc. | Device leadership negotiation among voice interface devices |
US10438593B2 (en) * | 2015-07-22 | 2019-10-08 | Google Llc | Individualized hotword detection models |
US9875081B2 (en) * | 2015-09-21 | 2018-01-23 | Amazon Technologies, Inc. | Device selection for providing a response |
US9899035B2 (en) * | 2015-11-04 | 2018-02-20 | Ioannis Kakadiaris | Systems for and methods of intelligent acoustic monitoring |
US9728188B1 (en) * | 2016-06-28 | 2017-08-08 | Amazon Technologies, Inc. | Methods and devices for ignoring similar audio being received by a system |
US10242673B2 (en) * | 2016-12-07 | 2019-03-26 | Google Llc | Preventing of audio attacks using an input and an output hotword detection model |
US10276161B2 (en) * | 2016-12-27 | 2019-04-30 | Google Llc | Contextual hotwords |
US10380852B2 (en) | 2017-05-12 | 2019-08-13 | Google Llc | Systems, methods, and devices for activity monitoring via a home assistant |
US10665232B2 (en) | 2017-05-24 | 2020-05-26 | Harman International Industries, Inc. | Coordination among multiple voice recognition devices |
WO2019029783A1 (en) * | 2017-08-07 | 2019-02-14 | Sonova Ag | Online automatic audio transcription for hearing aid users |
US10665223B2 (en) * | 2017-09-29 | 2020-05-26 | Udifi, Inc. | Acoustic and other waveform event detection and correction systems and methods |
DE102017220266B3 (en) * | 2017-11-14 | 2018-12-13 | Audi Ag | Method for checking an onboard speech recognizer of a motor vehicle and control device and motor vehicle |
US11195522B1 (en) * | 2019-06-17 | 2021-12-07 | Amazon Technologies, Inc. | False invocation rejection for speech processing systems |
US10769203B1 (en) * | 2019-10-17 | 2020-09-08 | Samsung Electronics Co., Ltd. | System and method for prediction and recommendation using collaborative filtering |
-
2020
- 2020-10-30 US US17/085,926 patent/US11798530B2/en active Active
- 2020-12-15 WO PCT/US2020/064988 patent/WO2022093291A1/en unknown
- 2020-12-15 KR KR1020227045231A patent/KR20230015980A/en unknown
- 2020-12-15 CN CN202080100908.3A patent/CN115605949A/en active Pending
- 2020-12-15 JP JP2022569600A patent/JP2023534367A/en active Pending
- 2020-12-15 EP EP20829156.7A patent/EP4042414A1/en active Pending
-
2023
- 2023-09-13 US US18/367,859 patent/US20230419951A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
WO2022093291A1 (en) | 2022-05-05 |
KR20230015980A (en) | 2023-01-31 |
CN115605949A (en) | 2023-01-13 |
US11798530B2 (en) | 2023-10-24 |
US20220139371A1 (en) | 2022-05-05 |
EP4042414A1 (en) | 2022-08-17 |
US20230419951A1 (en) | 2023-12-28 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
KR102551715B1 (en) | Generating iot-based notification(s) and provisioning of command(s) to cause automatic rendering of the iot-based notification(s) by automated assistant client(s) of client device(s) | |
JP7341171B2 (en) | Dynamic and/or context-specific hotwords to invoke automated assistants | |
JP2023534367A (en) | Simultaneous acoustic event detection across multiple assistant devices | |
US11886510B2 (en) | Inferring semantic label(s) for assistant device(s) based on device-specific signal(s) | |
US20220272055A1 (en) | Inferring assistant action(s) based on ambient sensing by assistant device(s) | |
US20230215422A1 (en) | Multimodal intent understanding for automated assistant |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20230113 |
|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20230113 |
|
A977 | Report on retrieval |
Free format text: JAPANESE INTERMEDIATE CODE: A971007Effective date: 20231215 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20240115 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20240222 |
|
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20240415 |