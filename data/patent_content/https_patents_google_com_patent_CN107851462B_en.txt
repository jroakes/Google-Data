CN107851462B - Analyzing health events using a recurrent neural network - Google Patents
Analyzing health events using a recurrent neural network Download PDFInfo
- Publication number
- CN107851462B CN107851462B CN201680038249.9A CN201680038249A CN107851462B CN 107851462 B CN107851462 B CN 107851462B CN 201680038249 A CN201680038249 A CN 201680038249A CN 107851462 B CN107851462 B CN 107851462B
- Authority
- CN
- China
- Prior art keywords
- health
- internal state
- time series
- time
- neural network
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G16—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR SPECIFIC APPLICATION FIELDS
- G16H—HEALTHCARE INFORMATICS, i.e. INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR THE HANDLING OR PROCESSING OF MEDICAL OR HEALTHCARE DATA
- G16H50/00—ICT specially adapted for medical diagnosis, medical simulation or medical data mining; ICT specially adapted for detecting, monitoring or modelling epidemics or pandemics
- G16H50/30—ICT specially adapted for medical diagnosis, medical simulation or medical data mining; ICT specially adapted for detecting, monitoring or modelling epidemics or pandemics for calculating health indices; for individual health risk assessment
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/049—Temporal neural networks, e.g. delay elements, oscillating neurons or pulsed inputs
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/10—Interfaces, programming languages or software development kits, e.g. for simulating neural networks
-
- G—PHYSICS
- G16—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR SPECIFIC APPLICATION FIELDS
- G16H—HEALTHCARE INFORMATICS, i.e. INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR THE HANDLING OR PROCESSING OF MEDICAL OR HEALTHCARE DATA
- G16H50/00—ICT specially adapted for medical diagnosis, medical simulation or medical data mining; ICT specially adapted for detecting, monitoring or modelling epidemics or pandemics
- G16H50/20—ICT specially adapted for medical diagnosis, medical simulation or medical data mining; ICT specially adapted for detecting, monitoring or modelling epidemics or pandemics for computer-aided diagnosis, e.g. based on medical expert systems
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, are provided for analyzing health events using a recurrent neural network. One method comprises the following steps: processing each of a plurality of initial time series of health events to generate, for each of the initial time series, a respective network internal state of the recurrent neural network for each time step in the initial time series; for each of the initial time series, storing one or more of the network internal states for a time step in the time series in a repository; obtaining a first time series; processing the first time series using a recurrent neural network to generate a series internal state of the first time series; and selecting one or more initial temporal sequences of health events that are likely to include a prediction of future health events in the first temporal sequence.
Description
Technical Field
The present specification relates to analyzing health events using a recurrent neural network.
Background
Neural networks are machine learning models that employ one or more layers of nonlinear units to predict output for received inputs. Some neural networks include one or more hidden layers in addition to an output layer. The output of each hidden layer is used as input to the next layer in the network, i.e. the next hidden layer or output layer. Each layer of the network generates an output from the received input in accordance with current values of the respective set of parameters.
Some neural networks are recurrent neural networks. A recurrent neural network is a neural network that receives an input sequence and generates an output sequence from the input sequence. In particular, the recurrent neural network may use some or all of the internal state of the network from the previous time step in calculating the output for the current time step.
Disclosure of Invention
In general, one innovative aspect of the subject matter in this specification is embodied in methods that include the actions of: obtaining a plurality of initial temporal sequences of health events, wherein each of the initial temporal sequences includes respective health-related data at each of a plurality of time steps; processing each of a plurality of initial time sequences of health events using a recurrent neural network to generate, for each of the initial time sequences, a respective network internal state of the recurrent neural network for each time step in the initial time sequence, wherein the recurrent neural network has been trained to receive an input time sequence, and for each time step in each input time sequence, generating a network internal state for the time step and predicting future events occurring after the health event identified at the time step from the network internal state for the time step; for each of a plurality of initial time series, storing one or more of the network internal states for a time step in the time series in an internal state repository; obtaining a first temporal sequence of health events; processing a first temporal sequence of health events using a recurrent neural network to generate a sequence internal state of the first temporal sequence; and selecting, from the plurality of initial time series, one or more initial time series that are likely to include health events that are predictive of future health events in the first time series using the sequence internal state of the first time series and the network internal state in the internal state repository.
Selecting one or more initial time series may include determining: network internal states in the internal state repository that are similar to the sequence internal states. The method may further include selecting an initial time series of health events from the plurality of initial time series that is likely to include a prediction of future health events in the first time series, generating a network internal state similar to the initial time series for the initial time series.
Determining network internal states in the internal state repository that are similar to the sequence internal states may include: for each of the network internal states in the internal state library, a respective similarity measure between the network internal state and the sequence internal state is calculated. Similar network internal states may be determined from the similarity measures.
Each network internal state in the internal state repository may be associated with a respective time step and a respective initial time sequence for which the network internal state was generated.
The method may further include providing data for presentation to a user that identifies, for each of the selected initial temporal sequences, health data in the selected initial temporal sequence at a time step subsequent to the time step for which the corresponding network internal state was generated.
The method may further include calculating statistics of particular health events identifying a frequency of occurrence of the particular health events from health events in the selected initial temporal sequence at time steps subsequent to the time step for which the corresponding network internal state was generated. Providing the calculated statistics for presentation to a user.
The recurrent neural network may be trained to generate, for each of a plurality of time steps in each input training sequence, a respective score for each of a plurality of possible health events from the network internal state at the time step. The respective score for each of the possible health events may indicate a likelihood that the possible health event is a health event at a time step subsequent to the time step in the input training sequence.
Processing the first temporal sequence of health events using the recurrent neural network to generate the sequence internal state for the first temporal sequence may include, for each time step in the first temporal sequence: processing data identifying the health event for the time step using a recurrent neural network to generate a network internal state for the time step; and selecting the network internal state of the last time step in the first time series as the sequence internal state of the first time series.
Each of the plurality of initial temporal sequences of health events may be associated with a respective patient. The health-related data at each time step in the initial temporal sequence may be associated with a respective patient.
For one or more time steps in each of the initial temporal sequences, the health-related data at that time step may be a respective token from a predetermined vocabulary of tokens. Each token in the vocabulary represents a different health event.
For one or more time steps of each of the initial time series, the health-related data for that time step may be other health-related data classified as affecting the health of the respective patient.
Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs, recorded on one or more computer storage devices, each configured to perform the actions of the methods.
A system of one or more computers can be configured to perform operations or actions by virtue of software, firmware, hardware, or a combination thereof installed on the system that causes the system to perform the specified actions when operated. The one or more computer programs may be configured to perform particular operations or actions by virtue of comprising instructions that, when executed by a data processing apparatus, cause the apparatus to perform the actions.
The above and other embodiments may each optionally include one or more of the following features, either alone or in combination.
Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages. The recurrent neural network can be effectively used to analyze a series of health events, such as a series of health events derived from the current patient's electronic medical record. The recurrent neural network can be effectively used to predict the likelihood of an event occurring within a particular time period of the most recent event in the time series, even if the event is not included in the set of possible inputs to the recurrent neural network. The recurrent neural network internal state can be effectively used to identify other time series corresponding to other patients, which can include health events that predict future health events that may become associated with the current patient. Accordingly, embodiments of the present subject matter provide improved systems and methods for processing a temporal sequence of health events.
A user, such as a doctor or other healthcare professional, can be provided with information characterizing the output of the recurrent neural network, or an output derived from the output generated from the recurrent neural network, thereby improving the healthcare professional's ability to provide superior healthcare to a professional patient. For example, a healthcare professional may be provided with useful information regarding the likelihood that a future health event that may become associated with the current patient, such as a health event that is likely to be the next health event associated with the patient or an event occurring within a specified time period of the most recent event in the sequence, will satisfy certain conditions. Further, the healthcare professional can be provided with information identifying the potential impact of the contemplated treatment on the likelihood of an incident occurring, e.g., whether the proposed treatment can reduce or increase the likelihood of undesirable health-related conditions being met by the patient in the future. Further, a healthcare professional may be provided with a medical record of patients whose medical records are similar to the current patient at some point in the history, or provided with a summary of the healthcare effects of these patients. Further, in some cases, an alert may be generated for a healthcare professional that is triggered if the action that the healthcare professional intends to take causes a significant increase in the risk of future predicted outcomes for the patient. In addition, health care analysis systems including recurrent neural networks can be used to codify standard medical practices to discover patterns of treatment and effect, analyze existing medical techniques or medical systems, or propose novel recommendations or facilitate scientific discovery.
The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 illustrates an example healthcare analysis system.
FIG. 2 is a flow diagram of an example process for generating time series health event data.
FIG. 3 is a flow diagram of an example process for generating a time series of health analysis data from a next input score.
FIG. 4 is a flow diagram of an example process for generating time series of health event data from network internal states.
FIG. 5 is a flow diagram of an example process for generating time series of health event data from future condition scores.
FIG. 6 is a flow diagram of an example process for determining the impact of adding an event to a time series on a future condition score-based.
FIG. 7 illustrates an example recurrent neural network configured to generate future condition scores.
FIG. 8 is a flow diagram of an example process for generating a future condition score for a given time step.
FIG. 9 is a flow diagram of an example process for training a recurrent neural network to generate future condition scores.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
The present specification generally describes a system that can generate health analysis data from a time series that includes data identifying a plurality of health events using a recurrent neural network.
Fig. 1 illustrates an example healthcare analysis system 100. The healthcare analysis system 100 is an example of a system implemented as a computer program on one or more computers in one or more locations in which the systems, components, and techniques described below may be implemented.
The health care analysis system 100 receives the time series and generates health analysis data from the received time series by processing the time series using the recurrent neural network 110. For example, the healthcare analysis system 100 can receive the time series 102 and generate the health analysis data 122 from the time series 102.
A temporal sequence is a sequence that includes health-related data, e.g., data identifying health events, for each of a plurality of time steps. Each time series includes health-related data associated with a given patient, wherein health events are identified by the health-related data in the time series in a chronological order such that the most recently occurring health event is the health event at the last time step in the series.
In some implementations, the time series generation system 104 generates the time series 102 from an electronic medical record of the corresponding patient. An electronic medical record is an electronic collection of health information corresponding to a patient. For example, the time series generation system can obtain an electronic medical record of the patient from the electronic medical record repository 106 and generate the time series 102 from the electronic medical record by identifying health events in the electronic medical record and chronologically ordering the health events. In particular, the time series 102 can include a sequence of tokens for each of a plurality of time steps, where each token represents a health event identified in the electronic medical record. In some implementations, the time series generation system can append data identifying the time at which the health event occurred to the data of the health events identified in the time series 102.
In general, the health events identified in the time series received by the health care analytics system 100 may include one or more of symptoms, tests, test results, diagnoses, medications, effects, and the like, each of which is represented by a token from a predetermined vocabulary of tokens. Optionally, each token is combined with data in the time series identifying the time at which the health event occurred. Additionally, in some cases, the time series may identify health events other than health events identified by tokens in the vocabulary. For example, in some embodiments, the health events in the time series may also include health-related images, such as X-ray or other diagnostic images, health-related electronic documents, such as free-form notes generated by a physician during a visit, or both.
Further optionally, the health-related data may comprise other health-related data that may be classified as affecting the health of the patient. For example, the other data may include data characterizing the activity of the patient or other health-related data collected by the patient's device, such as an activity tracking device or activity tracking application executing on a mobile device. For example, the activity data may include data identifying the distance traveled by the patient on a particular day, exercises or other fitness activities engaged in by the patient, food consumed by the patient, and the like. Other health-related data may also include other data believed to affect the patient's health, such as prescription fulfillment data for the patient or data identifying purchases made by the patient.
The healthcare analysis system 100 processes the time series 102 using the recurrent neural network 110 to generate a network output for the time series 102. The healthcare analysis system 100 also includes a healthcare analysis engine 120 that receives the network output for the time series 102 and generates analysis data 122 for the time series 102 from the network output.
In general, the network output of the time series 102 includes one or more of the following: a set of next input scores 112 for the recurrent neural network 110, a set of future condition scores 114, or a network internal state 116.
The recurrent neural network 110 includes one or more recurrent neural network layers that generate the internal state of the network for each time step of a given input time sequence. In some embodiments, the recurrent neural network 110 also includes an output layer, a set of logistic regression nodes, or both, that receives the network internal state and processes the network internal state to generate a time-stepped network output. In addition, in some embodiments, the recurrent neural network may also include one or more other kinds of neural network layers, such as feed-forward layers, e.g., fully-connected layers, convolutional layers, pooling layers, regularization layers, and the like.
In particular, each of the recurrent neural network layers is configured to receive a layer input for a time step and to compute a layer internal state for the layer for the time step. And the recurrent neural network layer calculates the layer internal state of the layer of the current time step according to the current value of the parameter set of the layer and the layer internal state of the layer of the previous time step and the layer input of the current time step. In some embodiments, one or more recurrent neural network layers are configured to also use other internal states for computing the layer internal state for a time step, e.g., internal states for layers from other previous time steps, current time steps for other recurrent layers, or internal states for previous time steps. If the current time step is the first time step in the sequence, the layer internal state for the previous time step is the initial layer internal state, e.g., as specified by a system administrator or as generated by the healthcare analysis system 100.
If there is only one recurrent neural network layer in the recurrent neural network 110, the network internal state for a given time step is the layer internal state of the recurrent neural network layer for that time step.
If there are multiple recurrent neural network layers in the recurrent neural network 110, the layers are arranged in order from the lowest layer in the sequence to the highest layer in the sequence, and the health events at that time step are collectively processed to calculate the network internal state at that time step. If other types of neural network layers are present in the recurrent neural network 110, the other neural network layers may be interspersed at different locations in the sequence, such as before the first recurrent layer, between two recurrent layers, after all recurrent layers, or some combination of these. For a given time step, the recurrent neural network 110 may provide the layer internal state from each recurrent neural network layer as a layer input to the recurrent neural network layer above that layer in the sequence. In some embodiments, one or more of the recurrent neural network layers is configured to also receive input from one or more other layers in the sequence other than the layer below the recurrent layer.
In some implementations, one or more layers in the sequence may be configured to receive global input, per-record input, or both as part of a layer's layer input at a subset of time steps, e.g., at a first time step or at each time step. The global input is an input that is not dependent on the current time series being processed by the recurrent neural network 110. One example of a global input is data characterizing the current time of year, e.g., the current date. Each recorded input is an input that may be different for different time sequences. Examples of per-record inputs may include a gene sequence of the patient associated with the current time series or other information characterizing the patient, such as population characteristic information of the patient.
In some embodiments, if there are multiple recurrent neural network layers, the network internal state for a time step is the layer internal state of the highest layer in the sequence of time steps. In some other implementations, the healthcare analysis system 100 combines the layer internal states for the time step to generate the network internal state for the time step. For example, the healthcare analysis system 100 may calculate a sum, product, or average of the layer internal states, or may concatenate (contain) the layer internal states to generate the network internal state.
In some embodiments, the recurrent neural network layer is a Long Short Term Memory (LSTM) layer. Each LSTM layer includes one or more LSTM memory blocks. Each LSTM memory block may include one or more cells, each cell including an input gate, a forgetting gate, and an output gate, which allows the cell to store a previous state of the cell, for example, for use in generating current activations or to be provided to other components of the LSTM neural network.
In embodiments where the recurrent neural network 110 includes an output layer, the output layer is configured to receive, for each time step, the network internal state for that time step and generate the next set of input scores for that time step. The next set of input scores for the time step includes a respective score for each health event represented by a token in the vocabulary of tokens. Once the recurrent neural network 110 has been trained, the next input score for a given health event indicates the likelihood that the health event will be the next health event in the time series. Thus, when the recurrent neural network 110 includes an output layer, the recurrent neural network 110 is a network that has been trained to predict, for each time step of a given input time series, a future health event, i.e., a health event for the next time step in the time series. The recurrent neural network 110 can be trained on a training sequence using conventional machine learning training techniques, such as back propagation through a temporal training technique.
In these embodiments, the next input score 112 for the time series 102 is the next input score generated by the output layer for the last time step in the time series 102.
In embodiments where the recurrent neural network 110 includes a set of logistic regression nodes configured to receive, at each time step, the network internal state for that time step and generate a set of future condition scores for that time step. The set of future condition scores includes a respective score for each condition in the predetermined set of conditions. The score for a given condition indicates a likelihood that the condition will be met within a specified time period of the health event at the current time step.
The condition may include a condition that is satisfied by the occurrence of an event, such as the occurrence of a health event represented by a token in a vocabulary. In some cases, the conditions in the set of predetermined conditions may include conditions that are satisfied when an event not represented by a token in the vocabulary, i.e., not a possible health event included in the time series processed by the recurrent neural network 110, occurs within a specified time period of the health event for the current time step, in addition to or instead of including conditions that are satisfied by the occurrence of the event represented by the token in the vocabulary. Thus, while an event that can satisfy a condition in a predetermined set of conditions may overlap with an event represented by a token, the set of conditions may also include conditions that are satisfied by the occurrence of other events that are not in the set.
Referring to fig. 7 and 8, a recurrent neural network comprising a set of logistic regression nodes is described in more detail. Hereinafter, training the recurrent neural network to predict the likelihood of the condition being satisfied is described in more detail with reference to fig. 9.
In these embodiments, the conditional score 114 for the time series 102 is a future conditional score generated by the logistic regression node for the last time step in the time series 102.
In embodiments where the network internal states 116 are included in the network output of the time series 102, the network internal states 116 of the time series 102 are either the network internal states generated by the recurrent neural network 110 for the last time step in the series, or a combination of the network internal states generated by the recurrent neural network 110 for multiple time steps in the series, such as a weighted sum, product, or concatenation of the network internal states.
The healthcare analysis engine 120 receives the network output of the time series 102 and generates health analysis data 122 for the time series 102 and provides the health analysis data 122 for presentation to a user, such as a physician treating a patient corresponding to the time series 102. In general, health analysis data 122 is data that characterizes future events that may be associated with time series 102, i.e., health events or other events that may occur after the current last health event in time series 102.
In embodiments where the neural network output of the time series 102 includes the next input score 112, the health care analysis engine 120 generates health analysis data 122, the health analysis data 122 identifying the health events that are likely to occur next in the time series 102. In the following, the generation of time series of health analysis data from the next input score is described in more detail with reference to fig. 3.
In embodiments where the neural network output of the time series 102 includes the network internal state 116, the health analysis engine 120 generates health analysis data 122 that identifies health events from other time series that are likely to predict future events in the time series 102. In particular, the healthcare analysis engine 120 identifies similar internal states from the internal states stored in the internal state repository 130 as the network internal states 116, and uses the similar internal states to determine from other time series a health event that is likely to predict a future event in the time series 102. The internal state repository 130 stores the network internal states generated at the respective time steps in the respective time series and associates each network internal state with data identifying the time step and the time series for which it was generated. The generation of time series of health analysis data from network internal states is described in more detail below with reference to fig. 4.
In embodiments where the neural network output of time series 102 includes future condition scores 114, health analysis engine 120 generates health analysis data 122 characterizing the scores of the conditions. The generation of time series of health analysis data from future health conditions is described in more detail below with reference to fig. 5.
Fig. 2 is a flow diagram of an example process 200 for generating time series of health event data. For convenience, process 200 will be described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed neural network training system, such as the healthcare analysis system 100 of fig. 1, may perform the process 200.
The system receives an input time series (step 202). The temporal sequence includes data identifying a respective health event for each of a plurality of time steps. In some embodiments, the time series is derived from an electronic medical record and includes data identifying a corresponding health event from the electronic medical record at each of a plurality of time steps. The health events in the sequence may be ordered in time such that the most recently occurring health event is the health event at the last time step in the sequence.
The system processes the input time series using a recurrent neural network, such as recurrent neural network 110 of FIG. 1, to generate neural network outputs of the input time series (step 204).
Depending on the implementation and the architecture of the recurrent neural network, the neural network output generated by the recurrent neural network by processing the input time series may include the next input score, the future condition score, or the network internal state.
The system generates a time series of health analysis data from the neural network output (step 206). As described above, the health analysis data depends on the kind of neural network output generated by the recurrent neural network.
FIG. 3 is a flow diagram of an example process 300 for generating a temporal sequence of health analysis data from a next input score. For convenience, process 300 will be described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed neural network training system, such as the healthcare analysis system 100 of fig. 1, may perform the process 300.
The system receives an input time series (step 302).
The system processes the input time series using the recurrent neural network to generate a next input score for the input time series (step 304). The recurrent neural network includes one or more recurrent neural network layers and an output layer configured, for each time step in the time sequence, to receive a network internal state generated by the recurrent neural network layer for that time step and generate a next set of input scores for that time step. The next set of input scores for a time step includes a respective score for each health event represented by a token in the vocabulary of tokens, where the next input score for a given health event represents a likelihood that the health event will be the next health event in the time series, i.e., the health event at the next time step in the time series.
The next input score for the input time sequence is the next input score generated by the output layer for the last time step in the time sequence.
The system identifies one or more highest scoring health events using the next input score (step 306). For example, the system may select a predetermined number of health events with the highest next input score or each health event with a next input score above a threshold.
The system provides data identifying the highest scoring health events and, optionally, data characterizing the next input score for each highest scoring health event for presentation to the user (step 308). Thus, a doctor or other user may be able to view information about health events that are likely to be the next health event to be associated with a patient corresponding to the input time series.
Fig. 4 is a flow diagram of an example process 400 for generating time series of health event data from network internal states. For convenience, process 400 will be described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed neural network training system, such as the healthcare analysis system 100 of fig. 1, may perform the process 400.
The system processes each of the set of time series using a recurrent neural network, such as recurrent neural network 110, to generate a network internal state for each time step of each time series (step 402). Each time series in the set corresponds to a different patient, e.g., generated from a different electronic medical record. The recurrent neural network includes one or more recurrent neural network layers and an output layer, a set of logistic regression nodes, or both. In particular, the recurrent neural network has been trained to predict, for each time step in a given input time sequence, future events, i.e., events that occur after the event at the current time step, from the internal state generated by the neural network for the current time step. For example, if the recurrent neural network includes an output layer, the recurrent neural network may have been trained to predict the next event in the time series, i.e., the event at the next time step after the current time step in the time series. As another example, if the recurrent neural network includes a set of logistic regression nodes, the recurrent neural network may be trained to predict whether each event in the set of events occurs within a specified time period of the event at the current time step in the time series.
The system stores the network internal states in an internal state repository and associates each network internal state with data identifying the time step and time sequence for which the network internal state was generated (step 404). In some embodiments, for each time series, the system stores the network internal state generated by the system for each time step in the time series in a repository. In some other embodiments, the system stores only a subset of the network internal states in the repository, e.g., only the network internal states of health events that precede at least a threshold number of other health events in the time series.
The system receives an input time series of health events (step 406).
The system processes the input time series using a recurrent neural network to determine a sequence internal state of the input time series (step 408). The sequence internal state of the input time sequence is the network internal state of the health event at the last time step in the sequence.
The system selects one or more network internal states from the internal state repository that are similar to the sequence internal states (step 410). The system selects a network internal state by computing a similarity metric, e.g., a cosine similarity metric, between the sequence internal state and the network internal states in the repository. For example, the system may select a predetermined number of network internal states having the largest cosine similarity to the sequence internal states, or each network internal state having cosine similarity to the sequence internal states that exceeds a threshold similarity. In some embodiments, the system determines similarity between internal states using different distance metrics, such as euclidean distance, Hamming (Hamming) distance, and so forth. Likewise, the system may also regularize the internal states and then calculate the distances between the regularized internal states.
The system provides data for presentation to the user that is identified as a time series for which similar network internal states are generated (step 412). In particular, the system provides, for a given similar network internal state, data identifying health events that occur after the time step for which the network internal state was generated in the time sequence for which the similar network internal state was generated. Because the recurrent neural network that generates both the sequence internal state and the similar network internal state is trained to predict future events from the network internal state, and the similar network internal state is similar to the sequence internal state, events that occur after the time step for which the given network internal state is generated are likely to predict future events in the input time series, i.e., events that occur after the current last event in the input time series. That is, starting at the time step for which the given similar network internal state was generated, the recurrent neural network expects the corresponding patient to have a similar future as the future expected by the recurrent neural network for the current patient corresponding to the input time series. Thus, by observing subsequent events according to the network internal state, a user, e.g. a doctor, may be given the idea of an event that may follow the current last event in the input time sequence, i.e. a future health event that may occur for the current patient.
In some other embodiments, the system also provides for identifying other health event data in the time series for presentation to the user as part of the data identifying the time series for which a given network internal state was generated.
In some implementations, rather than providing data identifying a time series for presentation to a user, the system calculates statistics from subsequent events in the time series and provides the calculated statistics for presentation to the user. For example, the system may determine a proportion of a time series that includes a particular health event, such as a heart attack or stroke, after a time step for which similar network internal states are generated. The system may then provide data identifying the ratio for presentation to the user, for example in the form of "X% of patients are expected to have a future similar to the current patient who experienced the particular health event.
In some embodiments, rather than storing the internal states in an internal state repository, the system may recalculate the internal state of each other time series each time an input time series is received that is to be compared to the other time series.
FIG. 5 is a flow diagram of an example process 500 for generating time series of health event data from future condition scores. For convenience, process 500 will be described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed neural network training system, such as the healthcare analysis system 100 of fig. 1, may perform the process 500.
The system receives an input time series (step 502).
The system processes the input time series using a recurrent neural network, such as recurrent neural network 110, to generate future condition scores for the input time series (step 504). The future condition score includes a respective future condition score for each of a set of predetermined conditions. The future condition score for a given condition represents the likelihood that the condition will be met within a specified time period of the event at the last time step in the input time series.
In some embodiments, the recurrent neural network includes one or more recurrent neural network layers and a set of logistic regression nodes. And each logistic regression node generates a future condition score of the corresponding condition according to the preset condition set at each time step in the input time sequence. In the following, a recurrent neural network comprising logistic regression nodes generating future condition scores is described in detail with reference to fig. 7-9. In these embodiments, the set of future conditional scores generated by the recurrent neural network for the last time step in the input time series is the set of future conditional scores for the input time series.
In some other embodiments, the recurrent neural network includes an output layer that generates a set of next input scores for each time step in the input time series, and does not include logistic regression nodes. In these embodiments, the system generates a plurality of possible time series that each include a specified number of additional time steps after the current last time step in the time series and a corresponding possible health event at each additional time step. The system generates a plurality of possible time series by performing a beam search (beam search) having a specified width for each additional time step. The width of the bundle search defines the number of highest scoring events considered by the system at each future time step. The system then determines, for each of the conditions satisfied by the occurrence of one of the events that will generate its future condition score, a proportion of the possible temporal sequences in the sequence that include events that satisfy the condition at one of the additional time steps. The system may then use this ratio as the future condition score for the corresponding condition. Alternatively, the system may weight each occurrence of an event using the likelihood of occurrence of a possible time sequence of events occurring therein. The likelihood of a possible time series occurring may be, for example, the product of the next input score of a health event at each additional time step in the series.
The system provides data identifying the future condition score for presentation to the user (step 506). For example, the system may provide data identifying each condition and a future condition score for each condition, or only provide data identifying one or more highest scoring conditions for presentation to the user.
In some embodiments, in addition to or instead of providing data identifying future condition scores for presentation to the user, the system may determine an effect of the treatment on the future condition scores and provide data identifying the effect for presentation to the user.
FIG. 6 is a flow diagram of an example process 600 for determining the impact of adding an event to a time series on a future condition score. For convenience, process 600 will be described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed neural network training system, such as the healthcare analysis system 100 of fig. 1, may perform the process 600.
The system receives an initial input time sequence (step 602).
The system determines future condition scores for the initial input time series (step 604). For example, the system may determine future condition scores for the initial input time series as described above with reference to FIG. 5.
The system receives data from the user identifying additional health events (step 606). For example, the additional health event may be a potential treatment that the physician will prescribe to the patient.
The system generates a modified input time series by appending data identifying the additional health event, e.g., a token representing the health event, to the end of the initial input time series (step 608).
The system determines a future condition score for the modified input time series. For example, the system may determine future condition scores for the initial input time series as described above with reference to FIG. 5.
The system determines a change in the future condition score caused by adding additional health events to the input time series (step 610) and provides data identifying the change for presentation to the user (step 612). That is, the system calculates a difference between the future condition score of the modified input time series and the corresponding future condition score of the initial input time series, and provides data identifying the difference for presentation to the user. Thus, the physician is able to view the effect of potential treatment on the likelihood of meeting certain conditions in the future.
In some implementations, the system can automatically perform the process 600 in response to a new event being added to the time series. If a new event causes the future condition score for a condition to increase by more than a threshold or to exceed a threshold, the system may generate an alert to automatically notify the user of the change. For example, a system administrator or other user may specify one or more particular conditions to be met as undesirable. The system may then automatically perform process 600 in response to a new event being added to the time series and generate an alert to notify the user whether the future condition score for one of the undesirable conditions crosses the threshold score or increases by more than a threshold increment.
Additionally, in some implementations, the system can automatically generate a plurality of modified time series according to the time step in response to receiving the time series, where each modified time series adds a different possible input health event to the time series. The possible input health events may be a subset of the health events represented by the tokens in the vocabulary, such as some or all of the possible treatments represented by the tokens in the vocabulary. The system may then perform process 600 for each modified time series and determine whether the future condition score for one or more undesirable conditions is reduced by more than a threshold reduction amount for any of the modified time series. In response to determining that the future condition score for the undesirable condition decreases by more than the threshold amount of decrease for a given modified time series, the system may provide information to the user identifying the health event that was added to the time series to generate the modified time series. Thus, the physician may be given the opportunity to consider additional treatment methods that would reduce the likelihood of undesirable conditions being met in the future.
FIG. 7 illustrates an example recurrent neural network 700 configured to generate future condition scores. The recurrent neural network 700 is an example of a system implemented as a computer program on one or more computers in one or more locations, in which the systems, components, and techniques described below can be implemented.
The recurrent neural network 700 receives an input sequence that includes a respective input at each of a plurality of time steps, and generates, for each time step, a respective future condition score for each condition in the set of predetermined events. The future condition score for a given condition at a given time step represents a likelihood that the condition is satisfied within a specified time period of the input at the time step.
The recurrent neural network 700 includes one or more recurrent neural network layers 710, a plurality of logistic regression nodes 720A-N, and optionally an output layer 740.
As described above with reference to fig. 1, for each time step, one or more recurrent neural network layers 710 receive the input at that time step and collectively process the input to generate a network internal state for the time step.
Each of the logistic regression nodes 720A-720N corresponds to a respective condition from a predetermined set of conditions, and is configured to receive, at each time step, a network internal state for that time step, and process the network internal state according to current values of the respective set of parameters to generate a future condition score for the respective event. Thus, at each time step, each of the logistic regression nodes 720A-720N generates a future condition score for a respective one of the conditions in the predetermined set of conditions.
If the recurrent neural network 700 includes the output layer 740, the output layer 740 is configured to receive the network internal state for that time step, and process that internal state to generate, for each possible input in the set of possible inputs, a respective next input score. The next input score for a given possible input indicates the likelihood that the possible input is the next input in the input sequence, i.e., the input that immediately follows the current time step in the input sequence.
The input in the time series comprises an input selected from tokens in a predetermined vocabulary representing a set of possible input events. The condition of the predetermined set of conditions for which the recurrent neural network 700 generates the future condition score may include a condition that is satisfied by the occurrence of an event that is not represented by a token in the predetermined vocabulary, i.e., a possible input event that is not likely to be included in the time series processed by the recurrent neural network 700, an event represented by a token, or both. Thus, while an event in the set of events that satisfies any condition in the set of predetermined conditions for which the recurrent neural network 700 generates a future condition score may overlap with an event represented by the token, the set of events may also include other events that are not in the set.
FIG. 8 is a flow diagram of an example process 800 for generating a future condition score for a given time step. For convenience, process 800 will be described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed recurrent neural network, such as recurrent neural network 700 of fig. 7, can perform process 800.
The system receives input at a time step, such as a token representing a health event (step 802).
The system processes the input using one or more recurrent neural network layers, such as recurrent neural network layer 710 of FIG. 7, to generate a network internal state of the recurrent neural network for that time step (step 804). One or more neural network layers generate network internal states, for example as described above with reference to fig. 1.
The system processes the network internal states using each of a set of logistic regression nodes, such as logistic regression nodes 720A-720N of FIG. 7, to generate a set of future condition scores (step 806). Each logistic regression node corresponds to a respective condition from a predetermined set of conditions, and generates a future condition score for the corresponding condition by processing the internal state according to current values of the set of parameters of the logistic regression node.
Optionally, the system also processes the network internal state using an output layer, such as output layer 740 of FIG. 7, to generate a respective next input score for each of the set of possible inputs (step 808). The output layer generates a corresponding next input score by processing the network internal state according to the current values of the output layer parameter set.
The process 800 may be performed for neural network inputs for which the required output is unknown, i.e., for which the neural network inputs should be generated by the system. The system may also perform process 800 on inputs in a set of training sequences, i.e., a set of inputs for which the output that should be predicted by the system is known, in order to train the system, i.e., determine training values for parameters of the recurrent neural network layer, logistic regression nodes, and, in some embodiments, the output layer. In particular, process 800 may be repeatedly performed on inputs from a set of training sequences as part of a machine learning training technique to train a neural network, such as a back-propagation through time (back-propagation) training technique. An example training process is described in more detail below with reference to fig. 9.
FIG. 9 is a flow diagram of an example process 900 for training a recurrent neural network to generate future condition scores. For convenience, process 900 will be described as being performed by a system of one or more computers located at one or more locations. For example, a suitably encoded recurrent neural network, such as recurrent neural network 700 of fig. 7, can perform process 900.
The system obtains a labeled training sequence (step 902). Each obtained training sequence is an input sequence for each of a plurality of time steps. Each training sequence further comprises, at each time step, a respective indicator variable for each condition of a predetermined set of conditions for which the recurrent neural network generates a future condition score. The indicator for a given condition at a given time step indicates whether the condition is met within a certain time period since the input at that time step. For example, the indicator variable may have a value of 1 if the condition is satisfied and a value of 0 if the condition is not satisfied. Thus, at each time step, the labeled training sequence includes an input for each condition in the predetermined set of conditions and a corresponding indicator variable.
In some embodiments, the system receives a training sequence that has been tagged with an indicator variable. In some other embodiments, the system generates a labeled training sequence by calculating an indicator variable for each condition at each time step. For example, the system may determine, for a given input at a given time step of the training sequence, when the input occurs and access data identifying the occurrence of events that satisfy a condition in a predetermined set of conditions. The system may then determine, for each condition, whether the condition is satisfied within a specified time period when the input at that time step occurs, and set the value of the indicator variable for the event accordingly.
The system trains one or more recurrent neural network layers, logistic regression nodes, and optionally an output layer on the labeled training sequence (step 904). Specifically, the system determines training values for parameters of the recurrent neural network layer, logistic regression nodes, and output layer from initial values of the parameters by performing multiple iterations of a machine learning training technique. As part of the training technique, the system minimizes or maximizes the objective function. If the system includes only logistic regression nodes and no output layers, the objective function depends, for a given time step in a given training sequence, on the error between the future condition score generated by the logistic regression node for that time step and the indicative variable of the corresponding condition at that time step. If the system further includes an output layer, the objective function for that time step also depends on the error between the next input score generated by the output layer for that time step and the input at the next time step in the training sequence.
As described above, the recurrent neural network 700 may process a time series of data including data identifying health events associated with a patient to generate a future condition score. However, the recurrent neural network 700 may be trained to generate future condition scores for a temporal sequence that includes data identifying any type of temporal event, i.e., any temporal sequence that includes events that identify a ranking by when those events occur over time.
For example, the recurrent neural network 700 may be trained to generate future conditional scores for a time series that includes data identifying transactions found in the financial statement of the user, such as bank transactions that may occur on a bank statement, credit card transactions that may occur on a credit card statement, and so forth. Future condition scores in this context may include scores for conditions satisfied by various types of financial transactions made, scores for conditions satisfied by events occurring that are not financial transactions occurring in the financial statement, such as tax audits, or both.
As another example, the recurrent neural network 700 may be trained to generate future condition scores that include a temporal sequence of data identifying stock market deals. In this context, a time series may include the purchase and sale of stocks by a single entity or all entities participating in a stock market.
As another example, the recurrent neural network 700 may be trained to generate future condition scores for a temporal sequence that includes data identifying maintenance records for mechanical or electronic devices, such as aircraft, vehicles, data center components, and the like. Future condition scores in this context may include scores for conditions met by various types of maintenance-related events as well as scores for conditions met by the occurrence of events that do not normally appear in a maintenance record, such as an in-flight fault of an aircraft.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on a tangible, non-transitory program carrier for execution by, or to control the operation of, data processing apparatus. Alternatively or additionally, the program instructions may be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus. The computer storage medium may be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory, or a combination of one or more of them.
The term "data processing apparatus" encompasses various apparatuses, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. An apparatus may comprise special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program (also known as or described as a program, software application, module, software module, script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a circuit, component, subroutine, or other unit suitable for use in a computing environment. A computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, such as one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files such as files that store one or more modules, sub programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
Computers suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, or both, and any kind of central processing unit. Generally, a central processing unit will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a central processing unit for implementing or executing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such a device. Moreover, a computer may be embedded in another device, e.g., a mobile telephone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a Universal Serial Bus (USB) flash drive, to name a few.
Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and storage devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having: a display device, such as a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, such as a mouse or a trackball, by which the user can provide input to the computer. Other types of devices may also be used to provide for interaction with a user; for example, in most embodiments, feedback provided to the user can be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. Further, the computer may interact with the user by sending and receiving documents to and from the device used by the user: for example, by sending a web page to a web browser on a user's client device in response to a request received from the web browser.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network ("LAN") and a wide area network ("WAN"), such as the Internet.
The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of the invention, but rather as descriptions of features specific to particular embodiments described herein. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated in a single software product or packaged into multiple software products.
Specific embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some embodiments, multitasking and parallel processing may be advantageous.
Claims (10)
1. A method for analyzing health events using a recurrent neural network, comprising:
a plurality of initial time series of health events is obtained,
wherein each of the plurality of initial time series of health events is associated with a different patient,
wherein each of the initial temporal sequence includes respective health-related data at each of a plurality of time steps, the respective health-related data being associated with a respective patient associated with the initial temporal sequence, an
Wherein, for one or more time steps of each of the initial temporal sequences, the health-related data at the time step is a respective token from a predetermined vocabulary of tokens, each token in the vocabulary representing a different health event;
processing each of a plurality of initial time series of the health event using a recurrent neural network to generate, for each of the initial time series, a respective network internal state of the recurrent neural network for each time step in the initial time series,
wherein the recurrent neural network has been trained to receive input time sequences, and for each time step in each input time sequence, to generate a network internal state for the time step and to predict future events occurring after a health event identified at the time step from the network internal state for the time step;
for each of the plurality of initial time series, storing one or more of the network internal states for a time step in the time series in an internal state repository;
obtaining a first temporal sequence of health events associated with a current patient;
processing a first temporal sequence of the health events using the recurrent neural network to generate a sequence internal state of the first temporal sequence;
comparing the first time series of sequence internal states with the initial time series of network internal states stored in the internal state repository to determine a plurality of network internal states from the initial time series of network internal states, wherein a respective similarity metric between each determined network internal state and the sequence internal state exceeds a threshold;
an initial time series corresponding to the determined network internal state is selected as the time series comprising health events predicted to be able to become future health events associated with the current patient.
2. The method of claim 1, wherein determining the plurality of network internal states in the internal state repository for the sequence internal state comprises:
for each network internal state in the internal state repository, computing a respective similarity metric between the network internal state and the sequence internal state; and
determining, as the plurality of network internal states, a network internal state having the respective similarity metric exceeding the threshold.
3. The method of claim 1, further comprising:
associating each network internal state in the internal state repository with a respective time step and a respective initial time sequence for which the network internal state was generated.
4. The method of claim 3, further comprising:
providing data for presentation to a user, the data identifying, for each of the selected initial temporal sequences, a health event in the selected initial temporal sequence at a time step subsequent to a time step for which the corresponding network internal state was generated.
5. The method of claim 3, further comprising:
calculating statistics of a particular type of health event identifying a frequency of occurrence of the particular type of health event from health events in the selected initial temporal sequence at time steps subsequent to the time step for which the corresponding network internal state was generated; and
providing the calculated statistics for presentation to a user.
6. The method of any of claims 1-5, wherein the recurrent neural network is trained to, for each of the plurality of time steps in each input training sequence, generate a respective score for each of a plurality of possible health events as a function of a network internal state for the time step, wherein the respective score for each possible health event represents a likelihood that the possible health event is a health event at a time step subsequent to the time step in the input training sequence.
7. The method of any of claims 1 to 5, wherein processing the first temporal sequence of health events using the recurrent neural network to generate the sequence internal states of the first temporal sequence comprises, for each time step in the first temporal sequence:
processing data identifying the health event for the time step using the recurrent neural network to generate a network internal state for the time step; and
selecting a network internal state of a last time step in the first time series as a sequence internal state of the first time series.
8. The method of claim 1, wherein, for one or more time steps in each of the initial temporal sequences, the health-related data at the time step is other health-related data classified as affecting the health of the respective patient.
9. A system for analyzing health events using a recurrent neural network, comprising one or more computers and one or more storage devices storing instructions that, when executed by the one or more computers, cause the one or more computers to perform the method of any of claims 1-8.
10. One or more non-transitory computer-readable media comprising instructions that, when executed by one or more computers, cause the one or more computers to perform the method of any of claims 1-8.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/810,384 US9652712B2 (en) | 2015-07-27 | 2015-07-27 | Analyzing health events using recurrent neural networks |
US14/810,384 | 2015-07-27 | ||
PCT/US2016/044107 WO2017019707A1 (en) | 2015-07-27 | 2016-07-26 | Analyzing health events using recurrent neural networks |
Publications (2)
Publication Number | Publication Date |
---|---|
CN107851462A CN107851462A (en) | 2018-03-27 |
CN107851462B true CN107851462B (en) | 2022-03-04 |
Family
ID=56609968
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201680038249.9A Active CN107851462B (en) | 2015-07-27 | 2016-07-26 | Analyzing health events using a recurrent neural network |
Country Status (6)
Country | Link |
---|---|
US (2) | US9652712B2 (en) |
EP (1) | EP3274888A1 (en) |
JP (1) | JP6530085B2 (en) |
KR (1) | KR101953814B1 (en) |
CN (1) | CN107851462B (en) |
WO (1) | WO2017019707A1 (en) |
Families Citing this family (28)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9336482B1 (en) | 2015-07-27 | 2016-05-10 | Google Inc. | Predicting likelihoods of conditions being satisfied using recurrent neural networks |
US10452961B2 (en) * | 2015-08-14 | 2019-10-22 | International Business Machines Corporation | Learning temporal patterns from electronic health records |
KR20170061222A (en) * | 2015-11-25 | 2017-06-05 | 한국전자통신연구원 | The method for prediction health data value through generation of health data pattern and the apparatus thereof |
WO2017136077A1 (en) * | 2016-02-04 | 2017-08-10 | Google Inc. | Associative long short-term memory neural network layers |
US10282546B1 (en) * | 2016-06-21 | 2019-05-07 | Symatec Corporation | Systems and methods for detecting malware based on event dependencies |
EP3516566A1 (en) | 2016-09-22 | 2019-07-31 | nference, inc. | Systems, methods, and computer readable media for visualization of semantic information and inference of temporal signals indicating salient associations between life science entities |
US10372814B2 (en) * | 2016-10-18 | 2019-08-06 | International Business Machines Corporation | Methods and system for fast, adaptive correction of misspells |
US10579729B2 (en) | 2016-10-18 | 2020-03-03 | International Business Machines Corporation | Methods and system for fast, adaptive correction of misspells |
WO2018175972A1 (en) | 2017-03-24 | 2018-09-27 | Google Llc | Device placement optimization with reinforcement learning |
US11139048B2 (en) * | 2017-07-18 | 2021-10-05 | Analytics For Life Inc. | Discovering novel features to use in machine learning techniques, such as machine learning techniques for diagnosing medical conditions |
US11108787B1 (en) * | 2018-03-29 | 2021-08-31 | NortonLifeLock Inc. | Securing a network device by forecasting an attack event using a recurrent neural network |
CN108985501B (en) * | 2018-06-29 | 2022-04-29 | 平安科技（深圳）有限公司 | Index feature extraction-based stock index prediction method, server and storage medium |
US11150875B2 (en) * | 2018-09-27 | 2021-10-19 | Microsoft Technology Licensing, Llc | Automated content editor |
US11416733B2 (en) * | 2018-11-19 | 2022-08-16 | Google Llc | Multi-task recurrent neural networks |
CN111312349A (en) * | 2018-12-11 | 2020-06-19 | 深圳先进技术研究院 | Medical record data prediction method and device and electronic equipment |
CN109887606B (en) * | 2019-02-28 | 2022-10-18 | 莫毓昌 | Attention-based diagnosis and prediction method for bidirectional recurrent neural network |
US20200342968A1 (en) * | 2019-04-24 | 2020-10-29 | GE Precision Healthcare LLC | Visualization of medical device event processing |
US11487902B2 (en) | 2019-06-21 | 2022-11-01 | nference, inc. | Systems and methods for computing with private healthcare data |
EP3987426A4 (en) * | 2019-06-21 | 2023-07-26 | nference, inc. | Systems and methods for computing with private healthcare data |
US20210065038A1 (en) * | 2019-08-26 | 2021-03-04 | Visa International Service Association | Method, System, and Computer Program Product for Maintaining Model State |
WO2021050061A1 (en) * | 2019-09-11 | 2021-03-18 | Visa International Service Association | Method, system, and computer program product for managing model updates |
US11842263B2 (en) * | 2020-06-11 | 2023-12-12 | Optum Services (Ireland) Limited | Cross-temporal predictive data analysis |
US11470037B2 (en) | 2020-09-09 | 2022-10-11 | Self Financial, Inc. | Navigation pathway generation |
US20220075877A1 (en) | 2020-09-09 | 2022-03-10 | Self Financial, Inc. | Interface and system for updating isolated repositories |
US11475010B2 (en) | 2020-09-09 | 2022-10-18 | Self Financial, Inc. | Asynchronous database caching |
US11641665B2 (en) * | 2020-09-09 | 2023-05-02 | Self Financial, Inc. | Resource utilization retrieval and modification |
CN112380126B (en) * | 2020-11-16 | 2023-06-16 | 泰康保险集团股份有限公司 | Web system health prediction device and method |
CN114819925B (en) | 2022-06-29 | 2022-10-11 | 成都秦川物联网科技股份有限公司 | Industrial Internet of things system based on event sequence analysis and prediction and control method thereof |
Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6272480B1 (en) * | 1997-10-17 | 2001-08-07 | Siemens Aktiengesellschaft | Method and arrangement for the neural modelling of a dynamic system with non-linear stochastic behavior |
CN1527992A (en) * | 2001-03-15 | 2004-09-08 | �ʼҷ����ֵ�������˾ | Automatic system for monitoring independent person requiring occasional assistance |
CN102879728A (en) * | 2012-10-16 | 2013-01-16 | 南京航空航天大学 | Health evaluation index and failure predication method for DC (Direct Current)-DC convertor |
CN103810401A (en) * | 2014-03-13 | 2014-05-21 | 兰州大学 | Two-dimensional runoff restoration method for separating influences of human activities |
Family Cites Families (34)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5109475A (en) | 1988-09-19 | 1992-04-28 | Hitachi, Ltd. | Method and a system for selection of time series data |
JP3168779B2 (en) * | 1992-08-06 | 2001-05-21 | セイコーエプソン株式会社 | Speech recognition device and method |
US5579439A (en) * | 1993-03-24 | 1996-11-26 | National Semiconductor Corporation | Fuzzy logic design generator using a neural network to generate fuzzy logic rules and membership functions for use in intelligent systems |
US5446829A (en) | 1993-06-24 | 1995-08-29 | The United States Of America As Represented By The Department Of Health And Human Services | Artificial network for temporal sequence processing |
JP3538906B2 (en) | 1994-09-02 | 2004-06-14 | 松下電器産業株式会社 | Monitoring device |
WO1996012187A1 (en) | 1994-10-13 | 1996-04-25 | Horus Therapeutics, Inc. | Computer assisted methods for diagnosing diseases |
JPH0973440A (en) | 1995-09-06 | 1997-03-18 | Fujitsu Ltd | System and method for time-series trend estimation by recursive type neural network in column structure |
US5839438A (en) | 1996-09-10 | 1998-11-24 | Neuralmed, Inc. | Computer-based neural network system and method for medical diagnosis and interpretation |
US6090044A (en) | 1997-12-10 | 2000-07-18 | Bishop; Jeffrey B. | System for diagnosing medical conditions using a neural network |
JP2002239960A (en) * | 2001-02-21 | 2002-08-28 | Sony Corp | Action control method of robot device, program, recording medium, and robot device |
US7254565B2 (en) * | 2001-07-26 | 2007-08-07 | International Business Machines Corporation | Method and circuits to virtually increase the number of prototypes in artificial neural networks |
US20040010481A1 (en) | 2001-12-07 | 2004-01-15 | Whitehead Institute For Biomedical Research | Time-dependent outcome prediction using neural networks |
US7647320B2 (en) | 2002-01-18 | 2010-01-12 | Peoplechart Corporation | Patient directed system and method for managing medical information |
JP3661686B2 (en) | 2002-12-19 | 2005-06-15 | 松下電器産業株式会社 | Monitoring device |
US7107107B2 (en) * | 2003-01-31 | 2006-09-12 | Matsushita Electric Industrial Co., Ltd. | Predictive action decision device and action decision method |
WO2005110029A2 (en) | 2004-05-07 | 2005-11-24 | Intermed Advisor, Inc. | Method and apparatus for real time predictive modeling for chronically ill patients |
JP2006120136A (en) | 2004-09-27 | 2006-05-11 | Kyoto Univ | Language processor, language processing method, language processing program and computer readable recording medium with the same recorded thereon |
JP4885463B2 (en) | 2005-03-03 | 2012-02-29 | 株式会社日立製作所 | Sensor network system, sensor data processing method and program |
US20100204920A1 (en) | 2005-04-25 | 2010-08-12 | Caduceus Information Systems Inc. | System for development of individualised treatment regimens |
JP2007200108A (en) | 2006-01-27 | 2007-08-09 | Nippon Telegr & Teleph Corp <Ntt> | Singular event detection device and singular event detection method |
JP2007299366A (en) * | 2006-01-31 | 2007-11-15 | Sony Corp | Learning system and method, recognition device and method, creation device and method, recognition and creation device and method, and program |
JP2007243342A (en) | 2006-03-06 | 2007-09-20 | Yokogawa Electric Corp | Image-monitoring apparatus and image-monitoring system |
JP2009015493A (en) | 2007-07-03 | 2009-01-22 | Toshiba Corp | Action execution device and method |
JP4450063B2 (en) * | 2007-12-17 | 2010-04-14 | ソニー株式会社 | Information processing apparatus, information processing method, and program |
US8099306B2 (en) * | 2008-02-06 | 2012-01-17 | The Trizetto Group, Inc. | Pharmacy episodes of care |
US8244656B2 (en) | 2008-09-25 | 2012-08-14 | Air Products And Chemicals, Inc. | System and method for predicting rare events |
EP2238897B1 (en) * | 2009-04-06 | 2011-04-13 | Sorin CRM SAS | Active medical device including means for reconstructing a surface electrocardiogram using an intracardiac electrogram |
US20100324936A1 (en) * | 2009-04-22 | 2010-12-23 | Suresh-Kumar Venkata Vishnubhatla | Pharmacy management and administration with bedside real-time medical event data collection |
EP2910081B1 (en) * | 2012-10-16 | 2020-12-02 | Telefonaktiebolaget LM Ericsson (publ) | Methods for deciding when to switch between communication channel states, and network nodes therefor |
US9119529B2 (en) * | 2012-10-30 | 2015-09-01 | Dexcom, Inc. | Systems and methods for dynamically and intelligently monitoring a host's glycemic condition after an alert is triggered |
JP2014178800A (en) * | 2013-03-14 | 2014-09-25 | Gifu Univ | Medical information processing device and program |
US9721202B2 (en) * | 2014-02-21 | 2017-08-01 | Adobe Systems Incorporated | Non-negative matrix factorization regularized by recurrent neural networks for audio processing |
US20170032241A1 (en) | 2015-07-27 | 2017-02-02 | Google Inc. | Analyzing health events using recurrent neural networks |
US9336482B1 (en) | 2015-07-27 | 2016-05-10 | Google Inc. | Predicting likelihoods of conditions being satisfied using recurrent neural networks |
-
2015
- 2015-07-27 US US14/810,384 patent/US9652712B2/en active Active
-
2016
- 2016-07-26 CN CN201680038249.9A patent/CN107851462B/en active Active
- 2016-07-26 EP EP16747965.8A patent/EP3274888A1/en not_active Ceased
- 2016-07-26 KR KR1020177031460A patent/KR101953814B1/en active IP Right Grant
- 2016-07-26 WO PCT/US2016/044107 patent/WO2017019707A1/en unknown
- 2016-07-26 JP JP2017556922A patent/JP6530085B2/en active Active
-
2017
- 2017-05-15 US US15/595,644 patent/US10402721B2/en active Active
Patent Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6272480B1 (en) * | 1997-10-17 | 2001-08-07 | Siemens Aktiengesellschaft | Method and arrangement for the neural modelling of a dynamic system with non-linear stochastic behavior |
CN1527992A (en) * | 2001-03-15 | 2004-09-08 | �ʼҷ����ֵ�������˾ | Automatic system for monitoring independent person requiring occasional assistance |
CN102879728A (en) * | 2012-10-16 | 2013-01-16 | 南京航空航天大学 | Health evaluation index and failure predication method for DC (Direct Current)-DC convertor |
CN103810401A (en) * | 2014-03-13 | 2014-05-21 | 兰州大学 | Two-dimensional runoff restoration method for separating influences of human activities |
Also Published As
Publication number | Publication date |
---|---|
WO2017019707A1 (en) | 2017-02-02 |
EP3274888A1 (en) | 2018-01-31 |
US20170316313A1 (en) | 2017-11-02 |
JP2018527636A (en) | 2018-09-20 |
US10402721B2 (en) | 2019-09-03 |
CN107851462A (en) | 2018-03-27 |
KR101953814B1 (en) | 2019-03-04 |
US9652712B2 (en) | 2017-05-16 |
JP6530085B2 (en) | 2019-06-12 |
KR20170132853A (en) | 2017-12-04 |
US20170032243A1 (en) | 2017-02-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN107851462B (en) | Analyzing health events using a recurrent neural network | |
US11790216B2 (en) | Predicting likelihoods of conditions being satisfied using recurrent neural networks | |
CN107995992B (en) | Analyzing health events using a recurrent neural network | |
Jiang et al. | Continuous missing data imputation with incomplete dataset by generative adversarial networks–based unsupervised learning for long-term bridge health monitoring | |
US20180285969A1 (en) | Predictive model training and selection for consumer evaluation | |
Tan et al. | Investigation on the data augmentation using machine learning algorithms in structural health monitoring information | |
Folino et al. | A recommendation engine for disease prediction | |
US10482999B2 (en) | Systems and methods for efficient handling of medical documentation | |
Rabiepour et al. | Structural performance and damage prediction using a novel digital cloning technique | |
US10335092B1 (en) | Building predicted future medical profiles | |
Nasarian et al. | Designing Interpretable ML System to Enhance Trustworthy AI in Healthcare: A Systematic Review of the Last Decade to A Proposed Robust Framework | |
US20240105289A1 (en) | Clinical endpoint adjudication system and method | |
Epifano | Better Models for High-Stakes Tasks |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |