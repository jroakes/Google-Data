WO2022250732A1 - Ascertaining and/or mitigating extent of effective reconstruction, of predictions, from model updates transmitted in federated learning - Google Patents
Ascertaining and/or mitigating extent of effective reconstruction, of predictions, from model updates transmitted in federated learning Download PDFInfo
- Publication number
- WO2022250732A1 WO2022250732A1 PCT/US2021/063122 US2021063122W WO2022250732A1 WO 2022250732 A1 WO2022250732 A1 WO 2022250732A1 US 2021063122 W US2021063122 W US 2021063122W WO 2022250732 A1 WO2022250732 A1 WO 2022250732A1
- Authority
- WO
- WIPO (PCT)
- Prior art keywords
- reconstruction
- model
- prediction
- machine learning
- generating
- Prior art date
Links
- 230000000116 mitigating effect Effects 0.000 title description 2
- 238000000034 method Methods 0.000 claims abstract description 164
- 238000010801 machine learning Methods 0.000 claims abstract description 125
- 239000011159 matrix material Substances 0.000 claims description 68
- 238000012549 training Methods 0.000 claims description 23
- 230000004044 response Effects 0.000 claims description 18
- 238000012545 processing Methods 0.000 claims description 13
- 230000004048 modification Effects 0.000 claims description 11
- 238000012986 modification Methods 0.000 claims description 11
- 238000013507 mapping Methods 0.000 claims description 8
- 230000003044 adaptive effect Effects 0.000 claims description 3
- 238000005457 optimization Methods 0.000 claims description 3
- 238000004590 computer program Methods 0.000 claims 1
- 238000009826 distribution Methods 0.000 description 7
- 230000008569 process Effects 0.000 description 7
- 230000005540 biological transmission Effects 0.000 description 6
- 230000015654 memory Effects 0.000 description 6
- 239000013598 vector Substances 0.000 description 6
- 230000006870 function Effects 0.000 description 4
- 230000000007 visual effect Effects 0.000 description 4
- 238000004891 communication Methods 0.000 description 3
- 238000000354 decomposition reaction Methods 0.000 description 3
- 238000013145 classification model Methods 0.000 description 2
- 238000013136 deep learning model Methods 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 230000002093 peripheral effect Effects 0.000 description 2
- 238000009877 rendering Methods 0.000 description 2
- 238000012216 screening Methods 0.000 description 2
- 230000003190 augmentative effect Effects 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 238000010586 diagram Methods 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 238000007689 inspection Methods 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
Definitions
- Federated learning of machine learning (ML) model(s) is an increasingly popular ML technique for training ML model(s).
- an on-device ML model is stored locally on a client device of a user, and a global ML model, that is a cloud-based counterpart of the on-device ML model, is stored remotely at a remote system (e.g., a cluster of servers).
- the client device using the on-device ML model, can process input detected at the client device to generate a prediction, and can compare the prediction to ground truth output to generate a client gradient. Further, the client device can transmit, to the remote system, a client model update that is based on the client gradient.
- the client model update can be the client gradient or can be based on the client gradient and additional generated client gradient(s).
- the client model update can be generated from a mini-batch of client gradients (e.g., 1-step, N-samples), from client gradients over several steps (e.g., N-steps, 1- sample each), or, more generally, based on gradients from K-step(s) with N-sample(s) at each step.
- the remote system can utilize the client model update, and optionally additional client model updates generated in a similar manner at additional client devices, to update weights of the global ML model.
- the remote system can transmit the global ML model, or updated weights of the global ML model, to the client device and/or to other client devices. Each client device can then replace the on-device ML model with the global ML model, or replace the weights of the on-device ML model with the updated weights of the global ML model, thereby updating the on-device ML model.
- federated learning enables a client device to transmit a locally generated model update, without transmitting the underlying data utilized to generate the model update (i.e., without transmitting the corresponding input(s), prediction(s), or ground truth output(s)). Further, the remote system can effectively update the global ML model utilizing the model update, and without any need to access or utilize the underlying data. In these and other manners, federated learning can provide a degree of data security by obviating the need to transmit the underlying (and potentially sensitive) data and instead transmitting only the model update generated based on such data.
- model updates cannot be reverse engineered to reveal information regarding the underlying data utilized to generate the model update (e.g., to reveal the input(s), the prediction(s), and/or the ground truth output(s)).
- Implementations disclosed herein relate to various techniques for ascertaining to what extent predictions, generated using a machine learning model, can be effectively reconstructed from model updates, where the model updates are generated based on those predictions and based on applying a particular loss technique (e.g., a particular cross-entropy loss technique).
- a particular loss technique e.g., a particular cross-entropy loss technique.
- some examples described herein will be described with respect to a model update that is a single gradient. However, as described herein, implementations disclosed herein can be utilized in conjunction with model updates that are based on multiple gradients.
- the predictions can each be a probability distribution or a sequence of probability distributions and the gradients can each be generated based on applying a cross entropy based loss technique in view of the prediction and in view of a corresponding ground truth one-hot vector (when the prediction is the probability distribution) or a corresponding sequence of ground truth one-hot vectors (when the prediction is the sequence of probability distributions).
- a corresponding reconstruction of each of the predictions can be generated using matrix factorization on the gradient and using a known vocabulary of a projection output layer of the machine learning model. More generally, a corresponding reconstruction of each model update can be generated using matrix factorization on the model update and using a known vocabulary of the projection output layer.
- each reconstruction of a model update can include, for example, a bag of vocabulary reconstruction (e.g., a bag of words reconstruction when the vocabulary elements include words or word pieces) that reconstructs the vocabulary elements of the prediction(s) used in generating the model update, but not necessarily their order.
- a bag of vocabulary reconstruction e.g., a bag of words reconstruction when the vocabulary elements include words or word pieces
- Such reconstructions can each be generated using the model update and the known vocabulary, and without any reference to corresponding current weights of the machine learning model when the corresponding prediction(s) were generated and/or without reference to any other feature(s).
- each reconstruction can additionally or alternatively include an ordered sequence reconstruction.
- the ordered sequence reconstruction can be generated using a language model (or other model(s) that dictate probabilities of various sequences of the vocabulary elements) and optionally without reference to corresponding current weights of the machine learning model.
- the language model can be utilized to determine which, of multiple candidate ordered sequences of the bag of vocabulary reconstruction, is most probable, and that candidate ordered sequence utilized as the ordered sequence reconstruction.
- the ordered sequence reconstructions can be generated based on the bag of vocabulary reconstruction and further based on the corresponding current weights of the machine learning model when the corresponding prediction(s) were generated.
- a gradients matching reconstruction technique and/or other reconstruction technique(s), that rely on corresponding current weights can be utilized in generating the ordered sequence reconstructions.
- reconstruction techniques can be used with a search space that is constrained in view of (e.g., constrained to) the bag of vocabulary reconstruction. This can enable such reconstruction techniques to be performed more efficiently (i.e., with less utilization of processor resources) and/or to be more accurate (i.e., by constraining the search space to the resolved bag of vocabulary reconstruction).
- Some implementations disclosed herein generate measures that each indicate a degree of conformity between a corresponding reconstruction, generated using a corresponding model update, and corresponding prediction(s).
- the measures collectively reflect how effectively predictions can be generated from model update generated using the particular loss technique. Accordingly, the measures and/or an overall measure generated based on the measures, can indicate a degree of data security that is provided by the gradients generated using the particular loss technique.
- the measures are utilized in determining whether to utilize the particular loss technique (utilized in generating the gradients) in federated learning of the machine learning model and/or of additional machine learning model(s).
- the measures, and/or overall measure(s) generated based on the measures can be compared to threshold(s) and the particular loss technique utilized in federated learning only when the measures and/or overall measure(s) satisfy the threshold(s).
- the measures and/or overall measure(s) that are generated based on model updates generated utilizing a particular loss technique can additionally or alternatively be compared to alternate measures and/or alternate overall measures that are each generated based on model updates generated utilizing a corresponding alternative particular loss technique.
- the particular loss technique can be utilized only when the comparison indicates that the particular loss technique provides a greater degree of data security than the alternate particular loss technique(s).
- the particular loss technique can be cross-entropy loss with sign gradient descent
- an alternate loss technique can be cross-entropy loss with adaptive federated optimization
- an additional alternate loss can be cross-entropy loss with gradient sparsification
- a further additional alternate loss technique can be cross-entropy loss without any gradient modification technique.
- the particular loss technique can be utilized only when its measure(s) are more indicative of data security than the measure(s) for the alternate loss technique, the measure(s) for the additional alternate loss technique, and the measure(s) for the further additional loss technique. In these and other manners, a certain degree of data security that is provided by gradients, generated using the particular loss technique, can be ensured prior to utilization of the particular loss technique in federated learning.
- a request that is transmitted by a computing device can be received over one or more networks and the request can include model update, prediction(s) pairs.
- the model update of the pairs can each be generated based on the prediction(s) of the pair and based on applying a particular loss technique.
- a reconstruction for each pair can be generated based on the model update of the pair, and measure(s) then generated that indicates a degree of conformity between the reconstruction and the prediction(s) of the pair.
- the measure(s) can reflect how effectively (e.g., whether and/or to what extent) the reconstruction conforms to the prediction(s).
- the measure(s) can include: a measure that indicates whether the bag of vocabulary reconstruction includes all elements of the prediction(s) and does not include any extra elements not in the prediction(s); and/or a measure that indicates a quantity of elements that differ between the bag of vocabulary reconstruction and the prediction(s) (e.g., a quantity of element(s) that are in the reconstruction but not the prediction(s) and a quantity of element(s) that are in the prediction(s) but not the reconstruction).
- the measure(s) can include: a measure that indicates whether the reconstruction includes all elements of the prediction(s) and in the order of the prediction(s) and does not include any extra element(s) not in the prediction(s); and/or a measure that indicates an extent to which the reconstruction and the prediction(s) differ, if at all (e.g., an edit-distance based measure or other measure that reflects difference(s) in element(s) and/or order between the reconstruction and the prediction(s)).
- the generated measures, and/or overall measure(s) generated based on the measures can be transmitted to the computing device in response to the request.
- the computing device can utilize the measure(s) and/or overall measure(s) in automatically determining whether to utilize the particular loss technique in federated learning and/or other machine learning model training.
- the transmission can additionally or alternatively cause the measures and/or overall measure(s) to be rendered (e.g., visually) at the computing device. This can enable user(s) of the computing device to ascertain (e.g., through viewing of the visual rendering) a degree a data security that is provided by the gradients and to determine, based on the degree, whether to utilize the particular loss technique in federated learning and/or other machine learning model training.
- the machine learning model is one that includes a projection layer having a projection input layer, weight matrix layer(s), and a projection output layer.
- the projection input layer can accept a lower dimensional generated embedding as input and the weight matrix layer(s) can be used to process the generated embedding, using current weights of the weight matrix layer(s), to generate corresponding projection output of the projection output layer.
- the projection output layer has a size that conforms to a vocabulary for the machine learning model.
- the quantity of output nodes of the projection output layer can conform to the vocabulary size and each node will correspond to a particular discrete element of the vocabulary.
- the output generated over the projection output layer can be, for example, a probability distribution over the vocabulary.
- a sequence of inputs is applied to the projection input layer, a sequence of outputs can be generated over the projection output layer and will be of a size that conforms to the vocabulary and to a length of the input sequence.
- the machine learning model is an automatic speech recognition model (e.g., a listen-attend-spell LAS model)
- a sequence of audio data embeddings of dimension S by d (where S is the quantity of audio data embeddings and d is the dimension of each embedding) can be provided to the projection input layer and sequence and the projection output can be a sequence of outputs that are collectively of length S by V, where V is the vocabulary size.
- the elements of the vocabulary in such an example can be words or word pieces.
- the embedding provided as input to the projection input layer can be an image embedding, of an image, of dimension d (where d is the dimension of the embedding) and the projection output can be of length V, where V is the vocabulary size.
- the elements of the vocabulary in such an example can be classifications.
- Additional and/or alternative machine learning models can be utilized that can include different vocabularies and/or can accept different types of embeddings as input.
- various implementations set forth techniques to ensure at least a certain degree of security is afforded by a particular loss technique utilized in federated learning, and can be utilized to ensure that degree of security is afforded before the particular loss technique is utilized in federated learning of particular machine learning model(s).
- security of data can be enhanced for various client devices that participate in the federated learning. This can enable the benefits of federated learning to be achieved, while ensuring a certain degree of security.
- FIG. 1 illustrates an example environment in which implementations described herein can be implemented.
- FIG. 2 is a flowchart illustrating an example method of: generating, utilizing corresponding model updates, corresponding reconstructions of corresponding predictions utilized in generating the corresponding model updates; determining measures based on comparing the corresponding reconstructions to the corresponding model updates; and, optionally, performing one or more further actions based on the determined measures.
- FIG. 3 is a flowchart illustrating an example method of generating a reconstruction of a prediction using matrix factorization on a corresponding gradient and using a known vocabulary of projection output of a machine learning model utilized in generating the prediction.
- FIG. 4 illustrates an example of a projection layer of a machine learning model.
- FIG. 5 illustrates an example of an invertible matrix, an orthogonal matrix generated based on decomposing a gradient, and a resulting matrix from performing a cross product of the invertible matrix and the orthogonal matrix.
- FIG. 6 schematically depicts an example architecture of a computer system.
- Equation (1) applies to a loss computed from a single sample with a single label. Since introducing a new label means adding a new term to the loss, equation (1) can be generalized to various settings. For example, a model update of a /V-sample mini-batch or a sequence of length N is averaged from model updates computed from each sample in the batch or each label in the sequence. In such a scenario, equation (1) can be generalized by equation (2):
- equation (1) can be generalized by equation (3): (3)
- ⁇ W can be represented as the product of two lower-rank matrices , where S is the number of terms used to compute the model update ⁇ W. For example, if the model update is computed from a batch, S is the batch size. As another example, if the model update is aggregated from several step updates, S is the total numbers of samples at these steps.
- the softmax cross-entropy loss is defined on the output of the projection layer Z and ground-truth labels y:L — log softmax Differentiating L with respect to z yields:
- each row in G has a unique negative coordinate corresponding to the ground-truth label.
- Neg(u) define the indices of negative coordinates in a vector u.
- ⁇ W can be decomposed into P ⁇ Q, where are orthogonal matrices, and is a diagonal matrix with non-negative elements on the diagonal.
- a screening round can be applied to filter inseparable columns.
- the screening round returns all points that are separable from a sampled subset of remaining points (e.g., using the Perceptron algorithm). This can be significantly faster and/or more computationally efficient than solving the LP problem.
- the below algorithm provides an overview of some implementations of obtaining a set of labels (i.e., a bag of vocabulary) from a model update.
- FIG. 1 illustrates an example environment in which implementations described herein can be implemented.
- the example environment includes client devices 106A-N, a federated learning system 110, a reconstruction system 120, and one or more networks 108.
- the client devices 106A-N, the federated learning system 110, and/or the reconstruction system 120 can communicate with one another via the network(s) 108.
- the network(s) 108 can include wide area network(s) (WAN(s)) (e.g., the Internet) and/or local area network(s) (LAN(s)).
- WAN(s) wide area network
- LAN(s) local area network
- the client devices 106A-N can include a client device via which a user can interact with the reconstruction system 120, which can be located remote from the client device (in other implementations reconstruction system 120 can be implemented in whole or in part on the client device).
- the user can interact with client device 108A (via user interface input device(s) of the client device 108A) to cause the client device to transmit model update, prediction(s) pairs to reconstruction system 120.
- the reconstruction system 120 can generate measure(s) based on the transmitted pairs and then transmit the measure(s) to the client device 108A.
- the client device 108A can utilize the measure(s) and/or overall measure(s) in automatically determining whether to utilize the particular loss technique in federated learning and/or other machine learning model training.
- the client device 108A can additionally or alternatively cause the measures and/or overall measure(s) to be rendered (e.g., visually) at the client device 108A. This can enable user(s) of the client device 108A to ascertain (e.g., through viewing of the visual rendering via a screen of the client device 108A) a degree a data security that is provided by the gradients and to determine, based on the degree, whether to utilize the particular loss technique in federated learning and/or other machine learning model training.
- the user can interact with client device 108A (via user interface input device(s) of the client device 108A) to cause the client device to transmit model updates to reconstruction system 120.
- the reconstruction system 120 can generate reconstructions that each correspond to one of the transmitted model updates, and then transmit, to the client device 108A, the reconstructions and indications of which reconstructions correspond to which model updates.
- the client device 108A can generate measure(s) and/or overall measure(s) based on comparing the reconstructions to actual predictions that are stored locally at the client device 108A or otherwise accessible at the client device 108A.
- the client device 108A can match the received reconstructions to corresponding predictions based on the received indications of which reconstructions correspond to which model updates (e.g., using a locally stored mapping of the model updates to the predictions). Accordingly, in such an example, the client device 108A transmits only the model updates to the reconstruction system 120, without transmitting the predictions. Moreover, the reconstruction system 120 returns reconstructions generated based on the model updates, enabling the client device 108A to generate the measures based on the returned reconstructions.
- the client devices 106A-N can additionally or alternatively include client devices that interact with the federated learning system 110 in participating in federated learning of a global machine learning (ML) model 118.
- ML machine learning
- each of the client devices 106A-N is illustrated as including a corresponding one of local ML models 108A-N stored locally at the client device.
- the local ML models 108A-N are each a local counterpart to the global ML model 118, which is managed by the federated learning system 110.
- each of the client devices 106A-N using its corresponding one of the on-device ML models 108A-N, can process corresponding input (e.g., an input based on user interface input detected at the client device and/or based on corresponding locally stored data at the client device) to generate a prediction, and can compare the prediction to ground truth output to generate a client gradient.
- corresponding input e.g., an input based on user interface input detected at the client device and/or based on corresponding locally stored data at the client device
- a cross-entropy based loss technique can be utilized in generating the client gradients.
- the ground truth output can be based on other data generated locally at the client device, and can optionally be based on user input(s) (e.g., that explicitly or implicitly confirm the prediction or that explicitly or implicitly indicate an alternate ground truth that is different from the prediction).
- the client devices 106A-N can transmit model updates, that are based on their locally generated client gradients, to the federated learning system 110.
- the model updates can be transmitted to the federated learning system 110 without transmission of the predictions or the ground truth outputs that were utilized in generating the model updates.
- the federated learning system 110 can utilize received client model updates, and optionally additional client model updates generated in a similar manner at additional client devices, to update weights of the global ML model 118.
- the federated learning system 110 can transmit the updated global ML model 118, or updated weights of the global ML model 118, to the client devices 108A-N and/or to other client devices.
- Each client device can then replace the on-device ML model with the updated global ML model, or replace the weights of the on-device ML model with the updated weights of the global ML model 118, thereby updating the on-device ML models.
- Further federated learning can optionally occur based on the updated on-device ML models, resulting in a further updated global ML model 118, which can again be provided (or weights thereof provided) with the client devices 106A-N. This process can continue for multiple iterations, optionally until the ML models are deemed final based on one or more conditions being satisfied.
- the federated learning system 110 can be implemented, for example, by one or more servers, such as a cluster of optionally distributed high-performance servers.
- the client devices 106A-N can include one or more of: a desktop computing device, a laptop computing device, a standalone hardware device at least in part dedicated to an automated assistant, a tablet computing device, a mobile phone computing device, a computing device of a vehicle (e.g., an in-vehicle communications system, and in-vehicle entertainment system, an in-vehicle navigation system, an in-vehicle navigation system), or a wearable apparatus of the user that includes a computing device (e.g., a watch of the user having a computing device, glasses of the user having a computing device, a virtual or augmented reality computing device). Additional and/or alternative client devices may be provided.
- Client devices 106A-N can each include one or more memories for storage of data and software applications, one or more processors for accessing data and executing applications, and other components that facilitate communication over a network.
- the reconstruction system 120 can be implemented, for example, by a client device and/or by one or more servers, such as a cluster of optionally distributed high-performance servers.
- the reconstruction system 120 is illustrated in FIG. 1 as including a reconstruction engine 122, a measure engine 124, and a selection engine 126.
- the reconstruction engine 122 processes model updates 134 and generates, for each of the model updates 134, a corresponding reconstruction.
- the model updates 134 processed at a given time can be provided by one of the client devices 106A, the federated learning system 110, or even locally generated by the reconstruction system 120. Further, the model updates 134 can optionally each be paired with corresponding one(s) of the predictions 136 as described herein (e.g., paired via mappings defining associations between corresponding model updates and predictions).
- the reconstruction engine 122 can use matrix factorization on the model update and use a known vocabulary 132 of a projection output layer of a corresponding ML model.
- a known vocabulary 132 of the projection output layer of the local ML models 108A-N can be used in generating the reconstruction.
- the known vocabulary 132 is provided by a developer or other user with knowledge of the vocabulary of the machine learning model (e.g., provided in or along with a request that includes corresponding gradients).
- the known vocabulary 132 is determined from inspection of the machine learning model and/or from providing multiple known inputs to the machine learning model and inspecting corresponding predictions and/or model updates. Providing multiple known inputs to the machine learning model and inspecting corresponding predictions and/or model update enables resolution of which output dimensions correspond to which elements of the vocabulary. For example, since the prediction, that should be generated from the known input, is also known, it can be determined from the prediction and/or the model update, which output dimensions correspond to element(s) of the vocabulary for the prediction. Through utilization of multiple known inputs and corresponding known predictions, some, or all, of the vocabulary can be effectively derived through analysis of actually generated predictions and/or model updates.
- the reconstruction engine 122 generates a reconstruction that includes, or is restricted to, a bag of vocabulary reconstruction that reconstructs the vocabulary elements of the prediction(s), but not necessarily their order. Put another way, the reconstruction seeks to reconstruct the vocabulary elements of the prediction(s) without regard to their order. While reconstruction could, by happenstance, include the vocabulary elements in the correct order, the reconstruction does not seek to determine the correct order.
- Such reconstructions can each be generated the reconstruction engine 122 using the model update and the known vocabulary, and without any reference to corresponding current weights of the machine learning model when the corresponding prediction was generated and/or without reference to any other feature(s).
- the reconstruction engine 122 can additionally or alternatively generate a reconstruction that is an ordered sequence reconstruction.
- the reconstruction engine 122 can generate the ordered sequence reconstruction based on the bag of vocabulary reconstruction and further based on a language model (or more generally, a vocabulary model) and/or based on the corresponding current weights of the machine learning model when the corresponding prediction was generated. In various implementations, in generating a reconstruction, the reconstruction engine 122 performs some or all aspects of step 256A of FIG. 3 (described below).
- the measure engine 124 compares reconstructions, generated by the reconstruction engine, to their corresponding predictions 136, and generates measure(s) based on the comparisons. For example, the measure engine can compare a generated reconstruction, generated based on a given one of the model updates 134, to a given one of the predictions 136 that is indicated as paired with the given one of the model updates 134.
- the given one of the predictions 136 can be prediction(s) that were actually generated, using the corresponding ML model, and that were utilized in generating the given one of the model updates 134 (e.g., based on comparing the prediction(s) to ground truth output(s)).
- the measure generated by the measure engine 124 for a reconstruction can reflect how effectively (e.g., whether and/or to what extent) the reconstruction conforms to the prediction(s). For example, if the reconstruction is a bag of vocabulary reconstruction, the measure engine 124 can generate a measure that is a "1.0" if the bag of vocabulary reconstruction includes all elements of the prediction(s) and does not include any extra elements not in the prediction(s), and that otherwise is a "0.0". As another example, if the reconstruction is a bag of vocabulary reconstruction, the measure engine 124 can additionally or alternatively generate a measure that is non-binary and that reflects a quantity of elements that differ between the bag of vocabulary reconstruction and the prediction(s).
- the measure can be "1.0" is no elements differ, "0.75" is one of four elements differ, "0.5” if three of six elements differ, “0.0” if all elements differ, and so forth.
- the measure engine 124 can also optionally generate overall measure(s) as a function of the individual measures for the reconstructions.
- the overall measure(s) can include a mean of the individual measures, a median of the individual measures, a standard deviation the individual measures, and/or other overall measure(s) that are a function of the individual measures.
- the measure engine 124 in generating an individual measure, performs some or all aspects of step 258 of FIG. 2 (described below).
- the selection engine 126 analyzes measure(s) (e.g., individual and/or overall), generated by the measures engine 124 for model updates generated according to a particular loss technique, in determining whether to utilize the particular loss technique (e.g., in federated learning of the corresponding machine learning model and/or of additional machine learning model(s)). Accordingly, in various implementations the selection engine 126 can determine whether to select the particular loss technique for usage or, instead, to select an alternative loss technique for usage.
- measure(s) e.g., individual and/or overall
- the measures engine 124 for model updates generated according to a particular loss technique, in determining whether to utilize the particular loss technique (e.g., in federated learning of the corresponding machine learning model and/or of additional machine learning model(s)). Accordingly, in various implementations the selection engine 126 can determine whether to select the particular loss technique for usage or, instead, to select an alternative loss technique for usage.
- the selection engine 126 compares the individual measures and/or overall measure(s), generated by the measures engine 124 for model updates generated according to a particular loss technique, to threshold(s). In those implementations, the selection engine 126 can determine whether to utilize the particular loss technique based at least in part (e.g., solely and/or based on other consideration(s)) based on whether the measures and/or overall measure(s) satisfy the threshold(s).
- the selection engine 126 compares: (a) the individual measures and/or overall measure(s), generated by the measures engine 124 for model updates generated according to a particular loss technique to (b) alternate individual measures and/or alternate overall measure(s), generated by the measures engine 124 for alternate gradients generated according to an alternate particular loss technique.
- the selection engine 126 can determine whether to select the particular loss technique or, instead, the alternate particular loss technique, based on the comparison (e.g., solely based on the comparison or also based on the threshold being satisfied as described in the preceding paragraph).
- the measures engine 124 can determine to select the particular loss technique for utilization only when the comparison indicates that the particular loss technique provides a greater degree of data security than the alternate particular loss technique(s).
- the preceding example is provided with respect to comparing corresponding measures for two different particular loss techniques, more than two particular loss techniques can be considered and the comparison (and resulting selection) can consider corresponding measures for all.
- the selection engine 126 performs some or all aspects of step 262 of FIG.
- FIG. 2 is a flowchart illustrating an example method 200 of: generating, utilizing corresponding model updates, corresponding reconstructions of corresponding predictions utilized in generating the corresponding gradients; determining measures based on comparing the corresponding reconstructions to the corresponding predictions; and, optionally, performing one or more further actions based on the determined measures.
- This system may include various components of various computer systems, such as one or more components of reconstruction system 120 of FIG. 1.
- process 200 operations of process 200 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, and/or added.
- the system receives model update, prediction(s) pairs.
- the model update, prediction(s) pairs each include: prediction(s) generated based on processing corresponding input(s) using a machine learning model; and a model update generated based on applying a particular loss technique and based on corresponding ground truth input(s) (e.g., generated based on gradient(s) each generated based on comparing the corresponding ground truth input(s) to the prediction(s).
- the model update can be a single gradient generated based on comparing a single prediction and ground truth pair or can be a model update generated based on a batch of gradients generated based on comparing multiple predictions and their corresponding ground truths.
- the model update, prediction(s) pairs received at block 252 can be generated by a component of the system and received from that component, or can be received in a transmission, and via a network, from another system or client device.
- the system identifies a model update, prediction(s) pair from those received at block 252.
- block 256 the system generates, using the model update of the identified pair and independent of the prediction of the identified pair, a reconstruction of the prediction.
- block 256 includes sub-block 256A, in which the system generates the reconstruction using matrix factorization on the model update and using a known vocabulary of projection output of the machine learning model.
- block 256A is described below with respect to FIG. 3.
- the system generates a measure based on comparing the reconstruction, of block 256 for the pair, to the prediction(s) of the pair.
- the system can store (e.g., in ROM or RAM) the measure.
- the system determines whether there are any unprocessed model update, prediction(s) pairs. If so, the system proceeds back to block 254 and identifies an unprocessed pair. If not, the system optionally proceeds to optional block 262 and/or optional block 264. It is noted that, although shown serially in FIG. 2 for convenience, in various implementations the system can perform multiple iterations of blocks 254, 256, and 258 in parallel (i.e., each iteration being performed in parallel will involve processing a different pair).
- the system determines, based on the measures generated via multiple iterations of block 258 (e.g., retrieving them from RAM or ROM), whether to utilize the particular loss technique in federated training. In some implementations, the system determines whether to utilize the particular loss technique in federated training based on the measures themselves and/or based on overall measure(s) that are generated based on the individual measure generated via multiple iterations of block 258.
- the system determines to utilize the particular loss technique in federated training only when some (e.g., X% of) or all of the individual measures satisfy an individual threshold and/or only when some (e.g., X% of) or all of the overall measure(s) satisfy a corresponding overall threshold. In some additional or alternative implementations, the system determines whether to utilize the particular loss technique in federated training based on comparing the individual measures and/or overall measure(s), for the particular loss technique, to individual measures and/or overall measure(s) for one or more alternate particular loss techniques. The individual measures and/or overall measure(s) for an alternate particular loss technique can be generated based on performing blocks 252, 254, 256, and 258 based on pairs that include model updates generated using the alternate particular loss technique.
- the system transmits, in response to receiving the pairs at block 252, the individual measures generated via multiple iterations of block 258 and/or overall measure(s) generated based on the generated individual measures.
- the pairs at block 252 can be received in a request from a server or a client device, and the system can transmit the individual measures and/or the overall measure(s) to the server or the client device.
- the individual measures and/or the overall measure(s) can be included in a graphical user interface that is generated by the system, and the graphical user interface transmitted to the client device. Transmitting of the graphical user interface to the client device can cause (e.g., after corresponding user input(s) at the client device) the client device to visually render the individual measures and/or overall measure(s).
- FIG. 3 is a flowchart illustrating one non-limiting example of block 256A of FIG. 2.
- the system identifies a gradient.
- the gradient can be the model update from one of the pairs of FIG. 2.
- the gradient can optionally be generated based on applying a cross-entropy based loss technique and based on a ground truth output and a prediction.
- the prediction is one generated based on processing input using a machine learning model.
- the system decomposes the gradient into at least an S by V orthogonal matrix (Q), where S corresponds to a number of sequences in the prediction and V corresponds to a vocabulary size of the machine learning model.
- Q an S by V orthogonal matrix
- Each column, in the matrix Q can represent an S-dimensional point that corresponds to an element in the vocabulary.
- the system can decompose the gradient into orthogonal matrix Q using singular value decomposition.
- the system can use singular value decomposition to decompose the gradient into two orthogonal matrices, Q and P (which can also be an S by V matrix) and a diagonal matrix ⁇ .
- Block 256A3 can include sub-block 256A3A, in which the system performs a dot product of Q and Z, where Z is an S by S invertible matrix, and identifies, based on the result, resulting column(s) of Q that include a separating value (e.g., negative value).
- a separating value e.g., negative value
- the system can identify, in the matrix VO that results from the dot product of Q and Z, row(s) that include a separating value (e.g., a negative value) and identify the column(s) of Q that have the same index value as the row(s).
- a row that has a separating value in the resulting matrix will indicate that the corresponding column of Q likewise has a separating value.
- an example Z invertible matrix 123A (of size S by S) is illustrated being crossed with an example Q orthogonal matrix 123B (of size S by V), resulting in an example VO matrix 123C (of size S by V).
- the second row of VO matrix 123C (illustrated with shading) is the result of the cross product of the second row of Z invertible matrix 123A (illustrated with shading) and the second column of Q orthogonal matrix 123B (illustrated with shading).
- the second row of VO matrix 123C includes a separating value, indicated by the vertical shading of the cell in the second row and second column (as opposed to the diagonal shading of the other cells of the second row.
- the second row of VO matrix 123C can be determined to have a separating value based on one of the cells being differentiable with respect to all other cells of the row. For example, one of the cells of the second can be negative and all other cells of the row can be positive. This can indicate that the second column of Q orthogonal matrix 123B (having the same "second" index value) likewise has a separating value. It is noted that additional rows of VO matrix 123C can have separating values and, as a result, additional columns of Q can be determined to have additional separating values. However, only one such example is illustrated in FIG. 5 for simplicity.
- block 256A4 the system generates a reconstruction of the prediction, using the column(s) of Q determined to include a separating classifier and a mapping of the columns of Qto the vocabulary of the machine learning models.
- block 256A4 includes sub-block 256A4A and optionally sub-block 256A4B.
- sub-block 256A4A the system generates a bag of vocabulary reconstruction which can include an unordered listing of those elements of the vocabulary that correspond to the columns of Q determined to include a separating classifier.
- the system generates, optionally using the current state of model and using the bag of vocabulary reconstruction of sub-block 256A4A, an ordered sequence reconstruction. It is noted that the current state of the model is not utilized in generating the bag of vocabulary reconstruction at sub-block 256A4A. In some implementations, at sub-block 256A4B, the system does not utilize the current state of the model but, rather, relies on the bag of vocabulary reconstruction and a vocabulary model that dictates probabilities of various sequences of the vocabulary elements. For example, where the vocabulary includes words or word sequences, the vocabulary model can be a language model.
- the system can utilize the language model to determine which, of multiple candidate ordered sequences of the bag of vocabulary reconstruction, is most probable, and that candidate ordered sequence utilized as the ordered sequence reconstruction.
- the system at sub-block 256A4B, the system generates the ordered sequence reconstruction based on the bag of vocabulary reconstruction and further based on the corresponding current weights of the machine learning model when the corresponding prediction was generated.
- the system uses gradients matching reconstruction technique and/or other reconstruction technique(s), that rely on corresponding current weights, in generating the ordered sequence reconstructions.
- the system uses such reconstruction technique(s) with a search space that is constrained in view of (e.g., constrained to) the bag of vocabulary reconstruction.
- the system stores the reconstruction, generated at block 256A4, and an association of the reconstruction to the gradient utilized in generating the reconstruction.
- (P ⁇ Z -1 ) is equivalent to A T , meaning that VO is equal to ( ZQ ) and, thus, VO can be resolved by a cross product of Z and Q.
- a row in VO that includes a separating value e.g., a negative value
- a column of Q having the same index value as the row, likewise has a separating value. This indicates that an element, of the vocabulary, that corresponds to that column of Q, was included in the prediction used to generate the gradient VW.
- FIG. 4 illustrates an example of a projection layer of a machine learning model, such as global model 118 (FIG. 1) and local models 108A-N (FIG. 1).
- the projection layer includes a projection input layer 118A, weight matrix layer(s) 118B, and a projection output layer 118C.
- the projection input layer 118A can accept a lower dimensional generated embedding (of dimension d) as input and the weight matrix layer(s) 118B can be used to process the generated embedding, using current weights of the weight matrix layer(s) 118B, to generate corresponding projection output (of dimension V) of the projection output layer 118C.
- the projection output layer 118C has a size (V) that conforms to a vocabulary for the machine learning model.
- the quantity of output nodes of the projection output layer 118C can conform to the vocabulary size and each node will correspond to a particular discrete element of the vocabulary.
- the output generated over the projection output layer 118C can be, for example, a probability distribution over the vocabulary.
- FIG. 6 is a block diagram of an example computing device 610 that may optionally be utilized to perform one or more aspects of techniques described herein.
- a client device can include one or more aspects of the example computing device 610 and/or a server can include one or more aspects of the example computing device 610.
- Computing device 610 typically includes at least one processor 614 which communicates with a number of peripheral devices via bus subsystem 612. These peripheral devices may include a storage subsystem 624, including, for example, a memory subsystem 625 and a file storage subsystem 626, user interface output devices 620, user interface input devices 622, and a network interface subsystem 616. The input and output devices allow user interaction with computing device 610.
- Network interface subsystem 616 provides an interface to outside networks and is coupled to corresponding interface devices in other computing devices.
- User interface input devices 622 may include a keyboard, pointing devices such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a touchscreen incorporated into the display, audio input devices such as voice recognition systems, microphones, and/or other types of input devices.
- use of the term "input device” is intended to include all possible types of devices and ways to input information into computing device 610 or onto a communication network.
- User interface output devices 620 may include a display subsystem, a printer, a fax machine, or non-visual displays such as audio output devices.
- the display subsystem may include a cathode ray tube (CRT), a flat-panel device such as a liquid crystal display (LCD), a projection device, or some other mechanism for creating a visible image.
- the display subsystem may also provide non-visual display such as via audio output devices.
- output device is intended to include all possible types of devices and ways to output information from computing device 610 to the user or to another machine or computing device.
- Storage subsystem 624 stores programming and data constructs that provide the functionality of some or all of the modules described herein.
- the storage subsystem 624 may include the logic to perform selected aspects of the methods of FIGS. 2, S, and/or other methods described herein.
- Memory 625 used in the storage subsystem 624 can include a number of memories including a main random access memory (RAM) 630 for storage of instructions and data during program execution and a read only memory (ROM) 632 in which fixed instructions are stored.
- a file storage subsystem 626 can provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges.
- the modules implementing the functionality of certain implementations may be stored by file storage subsystem 626 in the storage subsystem 624, or in other machines accessible by the processor(s) 614.
- Bus subsystem 612 provides a mechanism for letting the various components and subsystems of computing device 610 communicate with each other as intended. Although bus subsystem 612 is shown schematically as a single bus, alternative implementations of the bus subsystem may use multiple busses.
- Computing device 610 can be of varying types including a workstation, server, computing cluster, blade server, server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computing device 610 depicted in FIG. 6 is intended only as a specific example for purposes of illustrating some implementations. Many other configurations of computing device 610 are possible having more or fewer components than the computing device depicted in FIG. 6.
- the systems described herein collect personal information about users (or as often referred to herein, "participants"), or may make use of personal information
- the users may be provided with an opportunity to control whether programs or features collect user information (e.g., information about a user's social network, social actions or activities, profession, a user's preferences, or a user's current geographic location), or to control whether and/or how to receive content from the content server that may be more relevant to the user.
- user information e.g., information about a user's social network, social actions or activities, profession, a user's preferences, or a user's current geographic location
- certain data may be treated in one or more ways before it is stored or used, so that personal identifiable information is removed.
- a user's identity may be treated so that no personal identifiable information can be determined for the user, or a user's geographic location may be generalized where geographic location information is obtained (such as to a city, ZIP code, or state level), so that a particular geographic location of a user cannot be determined.
- geographic location information such as to a city, ZIP code, or state level
- the user may have control over how information is collected about the user and/or used.
- a method implemented by one or more processors includes receiving a plurality of model update, prediction(s) pairs.
- Each of the model update, prediction(s) pairs include: (a) at least one corresponding prediction, generated based on processing a corresponding input using a machine learning model with corresponding current weights; and (b) a corresponding model update generated based on at least one gradient, where the at least one gradient is generated based on applying a particular loss technique and generated based at least in part on the corresponding prediction and a corresponding ground truth output.
- the method further includes, for each of the model update, prediction(s) pairs: generating, using the corresponding model update and a known vocabulary of a projection output of the machine learning model, a reconstruction of the corresponding prediction; and generating, based on comparing the reconstruction to the corresponding prediction, a corresponding measure that reflects a degree of conformity between the reconstruction to the corresponding prediction. Generating the reconstruction is performed independent of the corresponding prediction.
- the method further includes determining, based on the corresponding measures for the model update, prediction(s) pairs, whether to utilize the particular loss technique in federated training of the machine learning model or of an additional machine learning model.
- the method further includes, in response to determining to utilize the particular loss technique in federated training of the machine learning model or of the additional machine learning model: causing the machine learning model or the additional machine learning model to be locally stored on a plurality of client devices, along with corresponding instructions.
- the corresponding instructions cause the client devices to locally generate model updates, for the machine learning model or the additional machine learning model, using the particular loss techniques, and transmit the model updates to one or more remote servers.
- determining, based on the corresponding measures, whether to utilize the particular loss technique in federated training of the machine learning model or of an additional machine learning model includes: generating an overall measure based on the corresponding measures; comparing the overall measure to a threshold; and determining to utilize the particular loss technique in federated training in response to the overall measure satisfying the threshold.
- determining, based on the corresponding measures, whether to utilize the particular loss technique in federated training of the machine learning model or of an additional machine learning model includes: generating an overall measure based on the corresponding measures; comparing the overall measure to an alternate overall measure, the alternate overall measure generated based on alternate model update, prediction(s) pairs having alternate corresponding model updates generated based on an alternate particular loss technique that differs from the particular loss technique; and determining, in response to the comparing, to utilize the particular loss technique in federated training.
- the particular loss technique is cross-entropy loss without any gradient modification technique and the alternate particular loss technique is cross-entropy loss with at least one gradient modification technique.
- the at least one gradient modification technique includes sign gradient descent and/or adaptive federated optimization.
- the particular loss technique is cross-entropy loss with a first gradient modification technique (or a first combination of gradient modification techniques) and the alternate particular loss technique is cross-entropy loss with a second gradient modification technique (or a second combination of gradient modification techniques).
- generating, using the corresponding model update and known labels of a projection output of the machine learning model, the reconstruction of the corresponding prediction includes generating the reconstruction using matrix factorization on the model update and using the known vocabulary of projection output of the machine learning model.
- the reconstruction can include (e.g., be restricted to) a bag of vocabulary reconstruction.
- the reconstruction can additionally or alternatively include an ordered sequence reconstruction and generating the reconstruction can further include generating the ordered sequence reconstruction using the corresponding current weights of the model.
- generating the reconstruction using matrix factorization on the model update and using a known vocabulary of projection output of the machine learning model includes: decomposing the model update into an S by V orthogonal matrix, where S corresponds to a number of sequences in the prediction and where V corresponds to a size of the known vocabulary; determining which columns, in the S by V orthogonal matrix, include a separating classifier; and generating the reconstruction using the columns, determined to include the separating classifier, and a mapping of the columns to the known vocabulary.
- determining which columns, in the S by V orthogonal matrix, include the separating classifier includes: performing a dot product of the S by V orthogonal matrix and an S by S invertible matrix; and determining, based on analysis of rows of the resulting matrix from the dot product, which rows include a negative value; and determining the columns include the separating classifier based on the columns corresponding to (e.g., having the same index value as) the rows that include the negative value.
- a method implemented by one or more processors includes receiving, via a network, a request from a computing device.
- the request includes a plurality of model update, prediction(s) pairs.
- Each of the model update, prediction(s) pairs include: (a) at least one corresponding prediction, generated based on processing a corresponding input using a machine learning model with corresponding current weights; and (b) a corresponding model update generated based on at least one gradient, where the at least one gradient is generated based on applying a particular loss technique and generated based at least in part on the corresponding prediction and a corresponding ground truth output.
- the method further includes, for each of the model update, prediction(s) pairs: generating, using the corresponding model update and a known vocabulary of a projection output of the machine learning model, a reconstruction of the corresponding prediction; and generating, based on comparing the reconstruction to the corresponding prediction, a corresponding measure that reflects conformity of the reconstruction to the corresponding prediction. Generating the reconstruction is performed independent of the corresponding prediction.
- the method further includes transmitting, via the network and to the computing device in response to the request, the corresponding measures for the model update, prediction(s) pairs, and/or an overall measure based on the corresponding measures.
- a method implemented by one or more processors includes receiving, via a network, a request from a computing device.
- the request includes a plurality of model updates.
- Each of the model updates is generated based on applying a particular loss technique and based at least in part on a corresponding prediction and a corresponding ground truth output.
- the corresponding prediction is generated based on processing a corresponding input using a machine learning model with corresponding current weights.
- the method further includes, for each of the model updates, generating, using the corresponding model update and a known vocabulary of a projection output of the machine learning model, a reconstruction of the corresponding prediction. Generating the reconstruction is performed independent of the corresponding prediction.
- the method further includes transmitting, via the network and to the computing device in response to the request, the reconstructions of the corresponding predictions.
- some implementations include one or more processors (e.g., central processing unit(s) (CPU(s)), graphics processing unit(s) (GPU(s), and/or tensor processing unit(s) (TPU(s)) of one or more computing devices, where the one or more processors are operable to execute instructions stored in associated memory, and where the instructions are configured to cause performance of any of the methods described herein.
- Some implementations also include one or more transitory or non-transitory computer readable storage media storing computer instructions executable by one or more processors to perform any of the methods described herein.
Abstract
Description
Claims
Priority Applications (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
EP21844108.7A EP4241208A1 (en) | 2021-05-28 | 2021-12-13 | Ascertaining and/or mitigating extent of effective reconstruction, of predictions, from model updates transmitted in federated learning |
CN202180088371.8A CN116783601A (en) | 2021-05-28 | 2021-12-13 | Determining and/or mitigating an effective degree of reconstruction of predictions based on model updates transmitted in federal learning |
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163194663P | 2021-05-28 | 2021-05-28 | |
US63/194,663 | 2021-05-28 | ||
US17/535,405 US20220383204A1 (en) | 2021-05-28 | 2021-11-24 | Ascertaining and/or mitigating extent of effective reconstruction, of predictions, from model updates transmitted in federated learning |
US17/535,405 | 2021-11-24 |
Publications (1)
Publication Number | Publication Date |
---|---|
WO2022250732A1 true WO2022250732A1 (en) | 2022-12-01 |
Family
ID=80112470
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
PCT/US2021/063122 WO2022250732A1 (en) | 2021-05-28 | 2021-12-13 | Ascertaining and/or mitigating extent of effective reconstruction, of predictions, from model updates transmitted in federated learning |
Country Status (2)
Country | Link |
---|---|
EP (1) | EP4241208A1 (en) |
WO (1) | WO2022250732A1 (en) |
-
2021
- 2021-12-13 WO PCT/US2021/063122 patent/WO2022250732A1/en active Application Filing
- 2021-12-13 EP EP21844108.7A patent/EP4241208A1/en active Pending
Non-Patent Citations (3)
Title |
---|
ABHISHEK BHOWMICK ET AL: "Protection Against Reconstruction and Its Applications in Private Federated Learning", ARXIV.ORG, CORNELL UNIVERSITY LIBRARY, 201 OLIN LIBRARY CORNELL UNIVERSITY ITHACA, NY 14853, 3 December 2018 (2018-12-03), XP081368836 * |
TRUNG DANG ET AL: "A Method to Reveal Speaker Identity in Distributed ASR Training, and How to Counter It", ARXIV.ORG, CORNELL UNIVERSITY LIBRARY, 201 OLIN LIBRARY CORNELL UNIVERSITY ITHACA, NY 14853, 15 April 2021 (2021-04-15), XP081939085 * |
WENQI WEI ET AL: "A Framework for Evaluating Gradient Leakage Attacks in Federated Learning", ARXIV.ORG, CORNELL UNIVERSITY LIBRARY, 201 OLIN LIBRARY CORNELL UNIVERSITY ITHACA, NY 14853, 22 April 2020 (2020-04-22), XP081650205 * |
Also Published As
Publication number | Publication date |
---|---|
EP4241208A1 (en) | 2023-09-13 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11829880B2 (en) | Generating trained neural networks with increased robustness against adversarial attacks | |
CN112633419B (en) | Small sample learning method and device, electronic equipment and storage medium | |
US11804069B2 (en) | Image clustering method and apparatus, and storage medium | |
US20210150412A1 (en) | Systems and methods for automated machine learning | |
US10860829B2 (en) | Data-parallel parameter estimation of the Latent Dirichlet allocation model by greedy Gibbs sampling | |
EP3542319A1 (en) | Training neural networks using a clustering loss | |
US11373117B1 (en) | Artificial intelligence service for scalable classification using features of unlabeled data and class descriptors | |
WO2020232874A1 (en) | Modeling method and apparatus based on transfer learning, and computer device and storage medium | |
Luo et al. | Graph entropy guided node embedding dimension selection for graph neural networks | |
US20190311258A1 (en) | Data dependent model initialization | |
CN112395487A (en) | Information recommendation method and device, computer-readable storage medium and electronic equipment | |
CN112667979A (en) | Password generation method and device, password identification method and device, and electronic device | |
US20230005572A1 (en) | Molecular structure acquisition method and apparatus, electronic device and storage medium | |
CN115358397A (en) | Parallel graph rule mining method and device based on data sampling | |
WO2021012691A1 (en) | Method and device for image retrieval | |
US20220383204A1 (en) | Ascertaining and/or mitigating extent of effective reconstruction, of predictions, from model updates transmitted in federated learning | |
US20220374655A1 (en) | Data summarization for training machine learning models | |
EP4241208A1 (en) | Ascertaining and/or mitigating extent of effective reconstruction, of predictions, from model updates transmitted in federated learning | |
CN116783601A (en) | Determining and/or mitigating an effective degree of reconstruction of predictions based on model updates transmitted in federal learning | |
CN113760407A (en) | Information processing method, device, equipment and storage medium | |
US20230111978A1 (en) | Cross-example softmax and/or cross-example negative mining | |
CN113505838B (en) | Image clustering method and device, electronic equipment and storage medium | |
JP2020030702A (en) | Learning device, learning method, and learning program | |
JP7384279B2 (en) | Data processing method, data processing device and data processing program | |
CN112509640B (en) | Gene ontology item name generation method and device and storage medium |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
121 | Ep: the epo has been informed by wipo that ep was designated in this application |
Ref document number: 21844108Country of ref document: EPKind code of ref document: A1 |
|
WWE | Wipo information: entry into national phase |
Ref document number: 2021844108Country of ref document: EP |
|
ENP | Entry into the national phase |
Ref document number: 2021844108Country of ref document: EPEffective date: 20230608 |
|
WWE | Wipo information: entry into national phase |
Ref document number: 202180088371.8Country of ref document: CN |
|
NENP | Non-entry into the national phase |
Ref country code: DE |