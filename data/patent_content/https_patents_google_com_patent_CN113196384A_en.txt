CN113196384A - Dynamic insertion of supplemental audio content into an audio recording at demand time - Google Patents
Dynamic insertion of supplemental audio content into an audio recording at demand time Download PDFInfo
- Publication number
- CN113196384A CN113196384A CN201980004868.XA CN201980004868A CN113196384A CN 113196384 A CN113196384 A CN 113196384A CN 201980004868 A CN201980004868 A CN 201980004868A CN 113196384 A CN113196384 A CN 113196384A
- Authority
- CN
- China
- Prior art keywords
- content
- audio
- processing system
- data processing
- client device
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000000153 supplemental effect Effects 0.000 title claims abstract description 88
- 238000003780 insertion Methods 0.000 title claims description 15
- 230000037431 insertion Effects 0.000 title claims description 15
- 238000012545 processing Methods 0.000 claims abstract description 163
- 230000005236 sound signal Effects 0.000 claims abstract description 108
- 230000009471 action Effects 0.000 claims abstract description 86
- 230000004044 response Effects 0.000 claims abstract description 49
- 238000000034 method Methods 0.000 claims description 88
- 230000003993 interaction Effects 0.000 claims description 53
- 238000001514 detection method Methods 0.000 claims description 45
- 230000007704 transition Effects 0.000 claims description 40
- 230000008569 process Effects 0.000 claims description 24
- 239000003550 marker Substances 0.000 claims description 15
- 238000012549 training Methods 0.000 claims description 8
- 235000014510 cooky Nutrition 0.000 claims description 7
- 238000012544 monitoring process Methods 0.000 claims description 7
- 230000033001 locomotion Effects 0.000 claims description 4
- 230000005540 biological transmission Effects 0.000 description 11
- 238000004891 communication Methods 0.000 description 11
- 238000004590 computer program Methods 0.000 description 11
- 230000006870 function Effects 0.000 description 10
- 238000003058 natural language processing Methods 0.000 description 8
- 238000010586 diagram Methods 0.000 description 7
- 238000010801 machine learning Methods 0.000 description 7
- 238000004422 calculation algorithm Methods 0.000 description 6
- 238000006243 chemical reaction Methods 0.000 description 5
- 238000013213 extrapolation Methods 0.000 description 5
- 230000014509 gene expression Effects 0.000 description 5
- 230000003287 optical effect Effects 0.000 description 5
- 238000001914 filtration Methods 0.000 description 4
- 238000013515 script Methods 0.000 description 4
- 230000002452 interceptive effect Effects 0.000 description 3
- 230000000670 limiting effect Effects 0.000 description 3
- 230000000644 propagated effect Effects 0.000 description 3
- 238000012546 transfer Methods 0.000 description 3
- 230000002730 additional effect Effects 0.000 description 2
- 238000013459 approach Methods 0.000 description 2
- 238000013528 artificial neural network Methods 0.000 description 2
- 239000003795 chemical substances by application Substances 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 238000011156 evaluation Methods 0.000 description 2
- 238000003384 imaging method Methods 0.000 description 2
- 230000000977 initiatory effect Effects 0.000 description 2
- 238000005259 measurement Methods 0.000 description 2
- 230000000877 morphologic effect Effects 0.000 description 2
- 238000007781 pre-processing Methods 0.000 description 2
- 230000009467 reduction Effects 0.000 description 2
- 238000009877 rendering Methods 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 238000013179 statistical model Methods 0.000 description 2
- 230000001360 synchronised effect Effects 0.000 description 2
- 238000013519 translation Methods 0.000 description 2
- 230000000007 visual effect Effects 0.000 description 2
- IRLPACMLTUPBCL-KQYNXXCUSA-N 5'-adenylyl sulfate Chemical compound C1=NC=2C(N)=NC=NC=2N1[C@@H]1O[C@H](COP(O)(=O)OS(O)(=O)=O)[C@@H](O)[C@H]1O IRLPACMLTUPBCL-KQYNXXCUSA-N 0.000 description 1
- 241001465754 Metazoa Species 0.000 description 1
- 230000006978 adaptation Effects 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 230000015556 catabolic process Effects 0.000 description 1
- 230000006835 compression Effects 0.000 description 1
- 238000007906 compression Methods 0.000 description 1
- 238000010276 construction Methods 0.000 description 1
- 238000003066 decision tree Methods 0.000 description 1
- 238000006731 degradation reaction Methods 0.000 description 1
- 238000011143 downstream manufacturing Methods 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 230000010354 integration Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 238000010295 mobile communication Methods 0.000 description 1
- 238000007637 random forest analysis Methods 0.000 description 1
- 230000011218 segmentation Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 238000000926 separation method Methods 0.000 description 1
- 230000011273 social behavior Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012706 support-vector machine Methods 0.000 description 1
- 230000009466 transformation Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L51/00—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail
- H04L51/02—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail using automatic reactions or user delegation, e.g. automatic replies or chatbot-generated messages
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/43—Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware
- H04N21/439—Processing of audio elementary streams
- H04N21/4394—Processing of audio elementary streams involving operations for analysing the audio stream, e.g. detecting features or characteristics in audio streams
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/60—Information retrieval; Database structures therefor; File system structures therefor of audio data
- G06F16/61—Indexing; Data structures therefor; Storage structures
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/18—Speech classification or search using natural language modelling
- G10L15/1822—Parsing for meaning understanding
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/48—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use
- G10L25/51—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use for comparison or discrimination
- G10L25/54—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use for comparison or discrimination for retrieval
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/02—Editing, e.g. varying the order of information signals recorded on, or reproduced from, record carriers
- G11B27/022—Electronic editing of analogue information signals, e.g. audio or video signals
- G11B27/029—Insert-editing
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L51/00—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail
- H04L51/07—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail characterised by the inclusion of specific contents
- H04L51/10—Multimedia information
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/01—Protocols
- H04L67/02—Protocols based on web technology, e.g. hypertext transfer protocol [HTTP]
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/01—Protocols
- H04L67/06—Protocols specially adapted for file transfer, e.g. file transfer protocol [FTP]
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/23—Processing of content or additional data; Elementary server operations; Server middleware
- H04N21/233—Processing of audio elementary streams
- H04N21/2335—Processing of audio elementary streams involving reformatting operations of audio signals, e.g. by converting from one coding standard to another
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/23—Processing of content or additional data; Elementary server operations; Server middleware
- H04N21/235—Processing of additional data, e.g. scrambling of additional data or processing content descriptors
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/43—Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware
- H04N21/435—Processing of additional data, e.g. decrypting of additional data, reconstructing software from modules extracted from the transport stream
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/43—Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware
- H04N21/439—Processing of audio elementary streams
- H04N21/4398—Processing of audio elementary streams involving reformatting operations of audio signals
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/80—Generation or processing of content or additional data by content creator independently of the distribution process; Content per se
- H04N21/81—Monomedia components thereof
- H04N21/8106—Monomedia components thereof involving special audio data, e.g. different tracks for different languages
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
Abstract
The present disclosure generally relates to inserting supplemental audio content into primary audio content via a digital assistant application. The data processing system can maintain an audio recording and content point tags for the content publisher to specify content points that define when supplemental audio content is inserted. The data processing system may receive an input audio signal from a client device. The data processing system may parse the input audio signal to determine that the input audio signal corresponds to the request, and may identify an audio recording of the content publisher. The data processing system may identify a content selection parameter in response to the determination. The data processing system may select an audio content item using the content selection parameter. The data processing system may generate and transmit an action data structure that includes an audio recording with an inserted audio content item.
Description
Background
People may engage in human-computer conversations using an interactive software application, referred to herein as an "automated assistant" (also referred to as a "chat bot," "interactive personal assistant," "intelligent personal assistant," "personal voice assistant," "session agent," etc.). For example, a user may provide a command, query, and/or request (collectively referred to herein as a "query") using free-form natural language input, which may include a voice utterance that is converted to text and then processed, and/or a typed free-form natural language input.
Disclosure of Invention
According to an aspect of the disclosure, a system for inserting supplemental audio content into primary audio content via a digital assistant application may include a record indexer component executing on a data processing system having one or more processors. The record indexer can maintain on the database audio records of the content publisher and content point markers set by the content publisher to specify content points that define times at which supplemental audio content is inserted during presentation of the audio records. The system may include a natural language processor component executing on a data processing system. The natural language processor component may receive an audio data packet that includes an input audio signal detected by a sensor of the client device. The natural language processor component may parse an input audio signal from the audio data packet to determine that the input audio signal corresponds to a request for an audio recording from a content publisher. The natural language processor component may identify an audio recording of the content publisher from the database based on a request determined from the input audio signal. The system can include a content placement component executing on a data processing system. The content placement component may identify an identifier associated with the client device as the content selection parameter in response to determining that the input audio signal corresponds to the request for the audio recording. The content placement component may select an audio content item of the content provider from the plurality of audio content items using the content selection parameter for the content point of the audio recording. The system may include an action handler component executing on the data processing system. The action handler component may insert the audio content item into a content point of the audio recording specified by the content point marker. The action handler component may generate an action data structure comprising an audio recording with an audio content item inserted at a time defined by the content point marker. The action handler component may transmit the action data structure to the client device to present an audio recording with the audio content item inserted at the content point.
According to one aspect of the disclosure, a method of inserting supplemental audio content into primary audio content via a digital assistant application may comprise: an audio recording of a content publisher and a content point marker set by the content publisher to specify a content point defining a time to insert supplemental audio content during presentation of the audio recording are maintained on a database by a data processing system having one or more processors. The method may include receiving, by a data processing system, an audio data packet including an input audio signal detected by a sensor of a client device. The method may include parsing, by a data processing system, an input audio signal from an audio data packet to determine that the input audio signal corresponds to a request for an audio recording from a content publisher. The method may include identifying, by the data processing system, an audio recording of the content publisher from a database based on a request determined from the input audio signal. The method can comprise the following steps: in response to determining that the input audio signal corresponds to a request for audio recording, an identifier associated with the client device is identified by the data processing system as a content selection parameter. The method may include selecting, by the data processing system, an audio content item of a content provider from the plurality of audio content items using a content selection parameter for a content point of the audio recording. The method may include inserting, by the data processing system, an audio content item into a content point of the audio recording defining a time specified by the content point marker. The method may include generating, by a data processing system, a motion data structure including an audio recording with an audio content item inserted at a time defined by a content point marker. The method may include transmitting, by the data processing system, the action data structure to the client device to present an audio recording with the audio content item inserted at the content point.
These and other aspects and embodiments are discussed in detail below. The foregoing information and the following detailed description include illustrative examples of various aspects and embodiments, and provide an overview or framework for understanding the nature and character of the claimed aspects and embodiments. The accompanying drawings are included to provide an illustration and a further understanding of the various aspects and embodiments, and are incorporated in and constitute a part of this specification.
Drawings
The accompanying drawings are not intended to be drawn to scale. Like reference numbers and designations in the various drawings indicate like elements. For purposes of clarity, not every component may be labeled in every drawing.
In the drawings:
fig. 1 illustrates a block diagram of an example system for inserting supplemental audio content into primary audio content via a digital assistant application, according to an example of the present disclosure.
Fig. 2 illustrates a sequence diagram of an example data stream for inserting supplemental audio content into primary audio content via a digital assistant application in the system shown in fig. 1, according to an example of the present disclosure.
Fig. 3 illustrates an example client computing device presenting primary audio content with supplemental audio content inserted therein according to an example of the present disclosure.
Fig. 4 illustrates a flow chart of a method of inserting supplemental audio content into primary audio content via a digital assistant application using the example system shown in fig. 1, according to an example of the present disclosure.
FIG. 5 is a block diagram of an example computer system.
Detailed Description
The following are more detailed descriptions of various concepts related to, and implementations of, methods, apparatus, and systems for inserting supplemental audio content into primary audio content via a digital assistant application. The various concepts introduced above and discussed in greater detail below may be implemented in any of numerous ways.
The audio file may include audio content to be presented via speakers on the client device. To generate audio content for an audio file, a content publisher may record sound sources of various origins-natural (e.g., human voice, animal or weather) or artificial (e.g., musical instruments, sound synthesizers or other machines) using one or more microphones. For example, to create a podcast recording, the voice of a human announcer who continuously reads the text recording may be recorded via a microphone. During recording, sound acquired via the microphone may be sampled, quantized, and encoded to form a digital representation (e.g., a binary code) of the sound for the audio content of the audio file. Once available on an information resource (e.g., a web page), the client device may download an audio file from the information resource and play the audio file using a media player at a later time.
However, after the audio file is generated, it may be difficult to add supplemental audio content provided by another entity, such as a content provider, without interfering or splitting the audio content already included in the file. This can be particularly problematic because, unlike streaming media content that is provided in near real-time and thus can be easily interrupted to insert such content, audio files can be played off-line at some time after download. One method of inserting supplemental audio content may include manually recording the supplemental audio content along with the primary audio content of the content distributor. Continuing from the previous podcast example, the text recording to be recorded that is read aloud by the human announcer may itself include portions for supplemental audio content between portions for primary audio content. But this approach typically results in listeners of the audio file having to constantly hear the same supplemental audio content regardless of their relevance. Furthermore, this type of approach may not be able to adjust the content to a dynamically changing or context driven technical environment, such as the fidelity of the audio device used to playback the content, the current audio environment of the listener, and current network conditions.
The lack of adaptation of the supplemental audio content to the current environment may result from a content selection process that does not consider the requesting client device and the primary audio content in identifying the supplemental audio content to insert. Without such a process, from the perspective of the content provider, it may be difficult for the content provider to evaluate or identify which audio file or content publisher is to provide supplemental audio content. As a result, without the associated supplemental audio content, the client device may consume more computing resources and network bandwidth from the generation and transmission of additional requests for potentially relevant content and initially providing a technically sub-optimal output. This may also lead to degradation of human-computer interaction (HCI) between the user and the client device playing the audio file.
In the context of digital assistant applications, these technical challenges may be exacerbated. The digital assistant application may parse an input voice command acquired via a microphone on the client device to identify a request for audio content from a particular content provider. For example, the input voice command parsed by the digital assistant application may be "download podcasts from 'news source a'. The audio files provided to the content provider of the digital assistant application may include supplemental audio content that is inserted into the audio file and selected without any content selection process. With digital assistant applications, generating additional requests for content may consume significant computing resources and network bandwidth due to the use of computationally complex natural language processing algorithms.
To address these and other challenges in inserting supplemental audio content into the primary audio content of an audio file, the present systems and methods may dynamically select and insert supplemental audio content at the time of the request. The audio file may have a content point specified by the content provider that may define a time period for inserting and playing the supplemental audio content in the primary audio content. Upon identifying that the voice command is a request for an audio file from a particular content provider, the digital assistant application may invoke a content selection service. The content placement service may identify content selection parameters for selecting from the supplemental audio content for insertion into the content points. The content selection parameters may include an identifier, such as a device identifier, cookie identifier, account identifier, or account profile, etc. Using these parameters, the content placement service may identify supplemental audio content from a collection of content from different content providers for the content points of the audio content in the file. Once identified, the digital assistant application can insert supplemental audio content into a content point within the audio content. The digital assistant application can provide and return audio files with supplemental audio content to the client device for presentation.
By incorporating the content selection parameters, the selection and identification of supplemental audio content can be enhanced with additional parameters for insertion into content points of audio content in an audio file. For example, using the identifier, the digital assistant application can monitor whether one of the predefined interactions (e.g., a voice command query for the content provider) occurred at the client device after the audio file was provided. Based on the number of client devices at which the predefined interaction is detected, the content selection service may determine a content point parameter for a content point within the audio content of the audio file. The value of the content point parameter may be proportional to a likelihood of one of the predefined interactions occurring after the audio file with the supplemental audio content is presented. In addition to the content selection parameters, the content selection service may use the content point parameters to identify supplemental audio content to be inserted into the file audio content.
Since various parameters are used to select content when identifying a request, supplemental audio content inserted into a content point may be technically more appropriate for current conditions or may be more relevant to a user listening to the audio content via a client. Due to the increase in adaptability and/or relevance, the likelihood of subsequent relevant interactions via the client device may increase, thereby improving human-machine interaction (HCI) between the user and the client device. Further, the chances of receiving subsequent voice commands by the digital assistant application to make additional requests for content may be reduced, thereby reducing consumption of computing resources and network bandwidth.
Fig. 1 depicts, among other things, a block diagram of an example system 100 for inserting supplemental audio content into primary audio content via a digital assistant application. System 100 may include at least one data processing system 102. The data processing system 102 may include at least one processor and memory, i.e., processing circuitry. The memory stores processor-executable instructions that, when executed by the processor, cause the processor to perform one or more of the operations described herein. The processor may comprise a microprocessor, an Application Specific Integrated Circuit (ASIC), a Field Programmable Gate Array (FPGA), the like, or a combination thereof. The memory may include, but is not limited to, electronic, optical, magnetic, or any other storage or transmission device capable of providing program instructions to the processor. The memory may further include a floppy disk, a CD-ROM, a DVD, a magnetic disk, a memory chip, an ASIC, an FPGA, a Read Only Memory (ROM), a Random Access Memory (RAM), an electrically erasable programmable ROM (eeprom), an erasable programmable ROM (eprom), a flash memory, an optical medium, or any other suitable memory from which a processor may read instructions. The instructions may include code from any suitable computer programming language. Data processing system 102 may include one or more computing devices or servers that may perform various functions.
The data processing system 102 may include a plurality of servers grouped logically and facilitate distributed computing techniques. A logical group of servers may be referred to as a data center, a server farm, or a machine farm. The servers may be geographically dispersed. A data center or machine farm may be managed as a single entity, or a machine farm may include multiple machine farms. The servers within each machine farm may be heterogeneous — one or more of the servers or machines may operate according to one or more types of operating system platforms. The data processing systems 102 may each include servers housed in a data center in one or more high-density rack systems and associated storage systems located, for example, in an enterprise data center. In this manner, the data processing system 102 with integrated servers may improve system manageability, data security, physical security of the system, and system performance by locating the servers and high performance storage systems on a localized high performance network. The centralization of all or some of the data processing system 102 components, including servers and storage systems, and their integration with advanced system management tools, allows for more efficient use of server resources, which saves power and processing requirements and reduces bandwidth usage. Each component of data processing system 102 may include at least one processing unit, server, virtual server, circuit, engine, agent, appliance, or other logic device such as a programmable logic array configured to communicate with other computing devices of system 100.
The system 100 may include at least one client device 104. The client device 104 may include at least one logic device, such as a computing device having a processor to communicate with other components of the system 100. Client device 104 may include an instance of any of the components described with respect to data processing system 102. The client device 104 may include a desktop computer, laptop computer, tablet computer, personal digital assistant, smart phone, mobile device, portable computer, thin client computer, virtual server, speaker-based digital assistant, or other computing device.
The system 100 may include at least one content publisher 106. The content publisher 106 may include a server or other computing device operated by a content publishing entity to provide primary audio content. For example, the content publisher 106 may be associated with an audio recording entity that records primary audio content. The primary audio content may be a recording of an audio broadcast (sometimes referred to herein as a podcast or audio podcast). The primary audio content may include one or more breaks (breaks) defined by the audio recording entity for inserting other audio content from entities other than the content distribution entity. The interruption may correspond to a period of time (e.g., of silence or other sound) within the recording into which other content may be inserted. Once recorded, the content publisher 106 may package and generate one or more audio files and make the files available for download via an information resource (e.g., a web page), a web application, or another program. The audio files may be in any format, such as WAV, MPEG, MP3, RIFF, AAC, OGG, WMA, and the like.
The system 100 may include at least one content provider 108. The content provider 108 may include a server or other computing device operated by a content provider entity to provide supplemental audio content. For example, the content provider 108 may be associated with another audio recording entity that records supplemental audio content (sometimes referred to herein as third-party audio content). The supplemental audio content may be included or inserted into the primary audio content created by the content publisher 106. For example, supplemental audio content recorded by the content provider 108 is in an interruption defined by an audio recording entity associated with the content publisher 106. Once recorded, the content provider 108 may package and generate one or more audio files and make those files available for presentation in conjunction with the primary audio content via an information resource (e.g., a web page), a web application, or another program. The audio files may be in any format, such as WAV, MPEG, MP3, RIFF, AAC, OGG, WMA, and the like.
The system 100 may include at least one network 112. Components of the system 100, such as the data processing system 102, the client device 104, the content publisher 106, and the content provider 108 may communicate over a network 112. The network 112 may include, for example, a point-to-point network, a broadcast network, a wide area network, a local area network, a telecommunications network, a data communications network, a computer network, an ATM (asynchronous transfer mode) network, a SONET (synchronous optical network) network, an SDH (synchronous digital hierarchy) network, an NFC (near field communication) network, a Local Area Network (LAN), a wireless network, or a wired network, as well as combinations thereof. The network 112 may include a wireless link, such as an infrared channel or a satellite band. The topology of the network 112 may include a bus, star, or ring network topology. The network 112 may include a mobile telephone network using any one or more protocols for communicating between mobile devices, including advanced mobile phone protocol (AMPS), Time Division Multiple Access (TDMA), Code Division Multiple Access (CDMA), global system for mobile communications (GSM), General Packet Radio Service (GPRS), or Universal Mobile Telecommunications System (UMTS). Different types of data may be transmitted via different protocols, or the same type of data may be transmitted via different protocols.
The data processing system 102 may include at least one instance of a digital assistant application 110. The digital assistant application 110 can include at least one Natural Language Processor (NLP) component 116 to parse audio-based input. The digital assistant application 110 can include at least one audio signal generator component 118 to generate audio-based signals. The digital assistant application 110 can include at least one direct action handler component 120 to generate an action data structure. The digital assistant application 110 can include at least one response selector component 122 to select a response to the audio-based input signal. The digital assistant application may include at least one data repository 124 to maintain data for the digital assistant application 110. One or more of the NLP component 116, the audio signal generator component 118, the direct action handler component 120, the response selector component 122, and the data store 124 can be separate from the instances of the digital assistant application 110 on the data processing system 102 or from each other.
The data store 124 may include one or more local or distributed databases, and may include a database management system. The data store 124 can include a computer data store or memory and can store data such as one or more regular expressions 126, one or more parameters 128, one or more policies 130, response data 132, templates 134, and at least one identifier 136. Parameters 128, policies 130, and templates 134 may include information such as rules regarding a voice-based session between client device 104 and data processing system 102. The regular expression 126 may include rules regarding conducting a voice-based session between the client device 104 and the data processing system 102 via the digital assistant application 110. The regular expression 126, parameters 128, policies 130, and templates 134 may also include information of another digital assistant application 110 received from another source (e.g., the data processing system 102 and the client device 104). The response data 132 may include content items for audio output or associated metadata, and may be input audio messages that are part of one or more communication sessions with the client device 104. The identifier 138 may include information about the client device 104 (e.g., authentication credentials to access and execute the digital assistant application 110).
The data processing system 102 can include at least one record indexer component 138 to receive and maintain audio content from the content publishers 106 and the content providers 108. The data processing system 102 can include at least one content placement component 140 to select supplemental audio content from the content provider 108 for insertion into the primary audio content of the content publisher 106. The data processing system 102 may include at least one transition detection component 142 to monitor interactions related to the provision of audio content. The data processing system 102 can also include at least one data repository 144 to maintain data for the record indexer component 138, the content placement component 140, or the transformation detection component 142, among other things. One or more of the record indexer component 138, the content placement component 140, the conversion detection component 142, and the data store 144 can be separate from the instance of the digital assistant application 110 on the data processing system 102 (e.g., as depicted). One or more of the record indexer component 138, the content placement component 140, the conversion detection component 142, and the data store 144 can be part of an instance of the digital assistant application 110 on the data processing system 102.
The data processing system 102 may include at least one interface 114. The interface 114 may be configured, constructed or operated to receive and transmit information using, for example, data packets. The interface 114 may receive and transmit information using one or more protocols, such as a network protocol. The interface 114 may include a hardware interface, a software interface, a wired interface, or a wireless interface. The interface 114 may be a data interface or a network interface that enables the components of the system 100 to communicate with each other. The interface 114 of the data processing system 102 may provide or transmit one or more data packets including motion data structures, audio signals, or other data via the network 112. For example, the data processing system 102 may provide output signals to the client device 104 from the data store 124 or from the audio signal generator component 118.
The data processing system 102 may also instruct the client device 104 via data packet transmission to perform the functions indicated in the action data structure. The output signals may be obtained, generated, converted, or transmitted as one or more data packets (or other communication protocols) from the data processing system 102 (or other computing device) to the client device 104. The interface 114 may facilitate or format data from one format to another. For example, the interface 114 may include an application programming interface ("API") that includes definitions for communicating between various components, such as software components. An application, script, program, or other component associated with data processing system 102 may be installed at client device 104. The application may enable the client device 104 to communicate input audio signals (and other data) to the interface 114 of the data processing system 102.
The data processing system 102 may include an application, script, or program installed at the client device 104, such as an instance of the digital assistant application 110 on the client device 104, to communicate input audio signals to the interface 114 of the client device 104 and to drive the drive components of the client computing device to render output audio signals or visual output. The data processing system 102 may receive data packets, digital files, or other signals that include or identify an input audio signal (or multiple input audio signals). The client device 104 may detect the audio signal via the speaker 152 and convert the analog audio signal to a digital file via an analog-to-digital converter. For example, the audio driver may include an analog-to-digital converter component. The pre-processor component may convert the audio signal into a digital file that may be transmitted via data packets over the network 112.
The client device 104 may include at least an instance of a digital assistant application 110. The functionality of the data processing system 102, such as the digital assistant application 110, may be included within the client device 104 or may be accessed from the client device 104 (e.g., via the interface 114). The functionality of the data processing system 102 may correspond to the functionality executing on the client device 104 or interface with the digital assistant application 110 executing on the client device 104. Client devices 104 may each include and execute separate instances of one or more components of digital assistant application 110. Client device 104 may also access the functionality of the components of digital assistant application 110 on data processing system 102 via network 112. For example, the client device 104 may include the functionality of the NLP component 116 and may access the remaining components of the digital assistant application 110 of the data processing system 102 via the network 112.
The client device 104 may be associated with an end user that inputs a voice query as audio input (via the microphone 154 or speaker 152) into the client device 104 and receives audio (or other) output from the data processing system 102 for presentation, display, or rendering to the end user of the client device 104. The digital components may include computer-generated speech that may be provided from the data processing system 102 to the client device 104. Client device 104 may render computer-generated speech to an end user via speaker 152. The computer-generated speech may include recordings from real persons or computer-generated languages. The client device 104 may provide visual output via a display device communicatively coupled to the client device 104.
The client device 104 may include or interface or communicate with at least one speaker 152 and at least one microphone 154. The client device 104 may include an audio driver to provide a software interface to the speaker 152 and microphone 154. The audio driver may execute instructions provided by the data processing system 102 to control the speaker 152 to generate corresponding sound waves or sound waves. The audio driver may execute audio files or other instructions to convert sound waves or sound waves acquired from the microphone 154 to generate audio data. For example, the audio driver may implement an analog-to-digital converter (ADC) to convert sound waves or acoustic waves into audio data.
The functionality of the digital assistant application 110 may be distributed or performed by instances on the data processing system 102 and the client device 104. For example, an instance of the digital assistant application 110 on the client device 104 may detect a keyword and perform an action based on the keyword. The digital assistant application 110 on the client device 104 may be an instance of the digital assistant application 110 executing at the data processing system 102, or may perform any of the functions of the digital assistant application 110. The client device 104 can filter out one or more terms or modify terms for further processing before transmitting the terms as data to the data processing system 102 (e.g., an instance of the digital assistant application 110 on the data processing system 102). The instance of the digital assistant application 110 on the client device 104 may convert the analog audio signal detected by the speaker 152 to a digital audio signal and transmit one or more data packets carrying the digital audio signal to the data processing system 102 via the network 112. An instance of the digital assistant application 110 on the client device 104 may transmit data packets carrying some or the entire input audio signal in response to detecting instructions to perform such transmission. The instructions may include, for example, a trigger key or other key or approval to transmit a data packet including the input audio signal to the data processing system 102.
An instance of the digital assistant application 110 on the client device 104 may perform pre-filtering or pre-processing on the input audio signal to remove certain frequencies of audio. The pre-filtering may include filters such as low pass filters, high pass filters, or band pass filters. The filter may be applied in the frequency domain. The filters may be applied using digital signal processing techniques. The filter may be configured to maintain frequencies corresponding to human speech or human speech while eliminating frequencies that fall outside of the typical frequencies of human speech. For example, the band pass filter may be configured to remove frequencies below a first threshold (e.g., 70Hz, 75Hz, 80Hz, 85Hz, 90Hz, 95Hz, 100Hz, or 105Hz) and above a second threshold (e.g., 200Hz, 205Hz, 210Hz, 225Hz, 235Hz, 245Hz, 255Hz, or 3 kHz). Applying a band pass filter may reduce computational resource utilization in downstream processing. The instance of the digital assistant application 110 on the client device 104 may apply a band pass filter before transmitting the input audio signal to the data processing system 102, thereby reducing network bandwidth utilization. Based on the computing resources available to the client device 104 and the network bandwidth available, it may be more efficient in some cases to provide the input audio signal to the data processing system 102 to allow the data processing system 102 to perform the filtering. The instance of the digital assistant application 110 on the client device 104 may apply additional pre-processing or pre-filtering techniques (e.g., noise reduction techniques) to reduce the level of ambient noise that may interfere with the natural language processor. Noise reduction techniques may improve the accuracy and speed of the natural language processor, thereby improving the performance of the data processing system 102 and managing rendering of a graphical user interface provided via a display.
The NLP component 116 of the instance of the digital assistant application 110 running on the data processing system 102 can receive audio data packets that include input audio signals detected by the microphone 154 of the client device 104. The data packets may provide digital files. The NLP component 116 may receive or obtain digital files or data packets that include audio signals and parse the audio signals. Upon providing the input audio signal to the data processing system 102, the NLP component 116 on the client device 104 may generate at least one audio data packet (sometimes referred to herein generally as a data packet). The audio data packet may include an input audio signal captured by the microphone 154 of the client device 104. The audio data packets may include data related to the client device 104, the digital assistant application 110 running on the client device, or the transmission of the input audio signals, such as one or more identifiers 136. The identifier 136 may include, for example: a device identifier referencing the client device 104, an account identifier associated with the user of the digital assistant application 110 (e.g., as part of the authentication credentials), or a session or cookie identifier assigned to the current use of the digital assistant application 110, etc. The data in the audio data packet may also include metadata such as device attributes associated with the client device 104, application attributes associated with the digital assistant application 110, and characteristic features (trait characteristics) associated with an account used to log into the digital assistant application 110, among others. Upon generation, the NLP component 116 on the client device 104 may transmit the audio data packets to the data processing system 102. Subsequently, an instance of the digital assistant application 110 running on the data processing system 102 may receive the audio data packets and may process the audio data packets including the input audio signals and any additional data.
To parse the input audio signal, the NLP component 116 may be configured with techniques for understanding natural language and enabling the data processing system 102 to derive meaning from human or natural language input. Using various natural language processing techniques, the NLP component 116 may provide for interaction between a person (e.g., a user of the client device 104) and a computer (e.g., the client device 104 running on the digital assistant application 110). The NLP component 116 can include or be configured with techniques based on machine learning, such as statistical machine learning. The NLP component 116 can utilize decision trees, statistical models, or probabilistic models to parse the input audio signal. The NLP component 116 can perform functions such as, for example, named entity recognition (e.g., given a stream of text, determining which terms in the text map to names (such as people or places) and what the type of each such name is, such as people, locations (e.g., "home"), natural language generation (e.g., converting information from a computer database or semantic intent into an understandable human language), natural language understanding (e.g., converting text into a more formal representation, such as a first-order logical structure that a computer module can manipulate), machine translation (e.g., automatically translating text from one human language to another human language), morphological segmentation (e.g., separating words into individual morphemes and identifying categories of morphemes, which can be challenging based on the morphological or structural complexity of the words of the language in question); NLP component 116, Question answering (e.g., determining answers to human language questions, which may be specific or open) or semantic processing (e.g., processing that may occur after a word is identified and its meaning is encoded in order to associate the identified word with other words having similar meanings).
The NLP component 116 may convert the input audio signal into a recognized string by comparing the input signal to a stored set of representative audio waveforms (e.g., in the data store 124) and selecting the closest match. The set of audio waveforms may be stored in the data store 124 or other database accessible to the data processing system 102. A representative waveform is generated across a large number of users and can then be enhanced with speech samples from the users. After the audio signal is converted to recognized text, NLP component 116 matches the text to words that are associated with actions that data processing system 102 can service, e.g., via cross-user training or through manual specification. The NLP component 116 may determine that the input audio signal obtained from the microphone 154 does not contain any recognizable character strings. Upon determining that the input audio signal does not contain any recognizable character strings, NLP component 116 may determine that the input audio signal contains silence (e.g., a maximum amplitude of less than 0 dB). Additionally, the NLP component 116 may determine a signal-to-noise ratio (SNR) of the input audio signal. The NLP component 116 may compare the SNR of the input audio signal to a threshold SNR (e.g., -20 dB). In response to determining that the SNR of the input audio signal is greater than the threshold SNR, the NLP component 116 can determine that the input audio signal does not contain any recognizable strings.
The NLP component 116 may obtain or identify an input audio signal from the input audio packet that is acquired by the microphone 154 of the client device 104. After acquisition, the NLP component 116 segments or divides the input audio signal into one or more segments of audio time period (e.g., 15 seconds to 2 minutes) to process or parse each segment. Upon parsing, NLP component 116 can identify one or more words from the input audio signal. Based on the identified words, the NLP component 116 can determine whether the input audio signal corresponds to a request for audio content from a particular content publisher 106. The determination may be based on whether the identified format setting matches a format setting of the request for audio content. The format settings may be indicated or specified by regular expressions 126, parameters 128, policies 130, and templates 134 maintained on the data store 124. Formatting of a request for audio content may include: a trigger indicating an intent to retrieve, a type of audio content to retrieve, and an entity from which the audio content is to be retrieved. For example, the words identified by the NLP component 116 from the input audio signal may include "download podcasts from news channel XYZ. In this example, the trigger keyword may be "download", the object may be "podcast", and the originating entity may be "news channel XYZ".
By identifying words from the input audio signal, NLP component 116 can determine whether the words correspond or match the formatting of the request for audio content. The determination may be performed using one or more natural language processing techniques. For example, question answering may be employed to determine trigger keywords, and entity recognition may be used to identify the type of audio content and the originating entity. In response to determining that the words from the input audio request do not match the format settings, NLP component 116 can determine that the input audio signal does not correspond to a request for audio content. Additionally, the NLP component 116 can perform additional actions to perform another type of request indicated in the words of the input audio signal. Instead, in response to determining that the word matches the format setting, NLP component 116 can determine that the input audio corresponds to a request for audio content. Further, the digital assistant application 110 and the data processing system 102 can perform additional actions to satisfy the request when audio content is retrieved from the content publisher 106.
The record indexer component 138 executing on the data processing system 102 can maintain a collection of audio records 146 on a data store 144. In maintaining the data store 144, the record indexer component 138 can receive audio records 146 from one of the content publishers 106. Upon receipt, the record indexer component 138 can store the audio records 146 on the data store 144. The record indexer component 138 can also identify the content publisher 106 from which the audio record 146 was received and associate the audio record 146 with the content publisher 106 and store the association between the audio record 146 and the content publisher 106. Each audio recording 146 may be provided or received from one of the content publishers 106 and may be provided via the digital assistant application 110 downloaded onto the client device 104. The audio recording 146 may include one or more audio files of any format type, such as WAV, MPEG, MP3, RIFF, AAC, OGG, WMA, and the like. The audio recording 146 may include primary audio content created by the associated content publisher 106. In addition to the primary audio content, the audio recording 146 may include at least one content point 148. The content points 148 may define a portion of the primary audio content during which supplemental audio content is to be presented. The content points 148 may be specified or provided by the content publisher 106 using corresponding content point tags. The content point markers may define times or time windows during which the supplemental content is to be played during presentation of the primary audio content in the audio recording 146.
In addition, the record indexer component 138 maintains supplemental audio content items 150 (hereinafter referred to generally as audio content items 150) on the data store 144. In maintaining the data store 144, the record indexer component 138 can receive audio content items 150 from the content providers 108. Upon receipt, the record indexer component 138 can store the audio records 146 on the data store 144. The record indexer component 138 can also identify the content provider 108 from which the audio content item 150 was received, associate the audio content item 150 with the content provider 108, and store the association between the audio content item 150 and the content provider 108. Each audio content item 150 may be provided or received from one of the content providers 108 and may be provided as part of the audio recording 146 via download onto the digital assistant application 110 on the client device 104. Each audio content item 150 may include one or more audio files of any type of format, such as WAV, MPEG, MP3, RIFF, AAC, OGG, WMA, and so forth. The audio content item 150 may include supplemental audio content created by the associated content provider 108. The supplemental audio content of the audio content item 150 may be inserted into the content point 148 of one of the audio recordings 146. Once inserted, the supplemental audio content of the audio content item 150 is presented before, during, or after the primary audio content as specified by the content point tags of the content points 148.
In response to determining that the input audio signal corresponds to a request for audio content, NLP component 116 may identify audio recording 146 from data store 144 to provide to digital assistant application 110 on client device 104. Based on the words parsed from the input audio signal, the NLP component 116 can identify the content publishers 106 whose audio content is associated with the request. For example, the NLP component 116 can use entity recognition to identify a content publisher entity associated with the content publisher 106. By identifying the content publisher 106 associated with the request, the NLP component 116 can access the data store 144 to identify a subset of audio recordings 146 that belong to the content publisher 106. The NLP component 116 can invoke the record indexer component 138 to search for and retrieve at least one of the audio records 146 from the data store 144 using the identified content publisher 106. For example, the record indexer component 138 can search the content publisher 106 for the most recent audio records 146 stored and maintained on the data store 144. From the subset, the NLP component 116 can identify one audio recording 146 to provide based on the word parsed from the input audio signal.
The content placement component 140 executing on the data processing system 102 can select or identify at least one of the audio content items 150 for insertion into each content point 148 of the audio recording 146 associated with the request. By determining that the input audio signal corresponds to a request for content, the content placement component 140 can identify at least one content selection parameter. The content selection parameters may be used to select one of the audio content items 150 for inclusion in the content point 148 of the audio recording 146. The content selection parameters may include, for example: an identifier 136 (e.g., a device identifier, an account identifier, or a session or cookie identifier); at least one device attribute associated with the client device 104 (e.g., device type, device capability, network address, and geographic location); at least one application attribute (e.g., application name, version, or presentation capability) associated with the digital assistant application 110 on the client device 104; and at least one characteristic feature (e.g., an account profile, an interest identifier, or a user snippet) associated with an account identifier used to login the digital assistant application 110, and so on.
To identify the content selection parameters, the content placement component 140 can parse the audio data packets provided by the digital assistant application 110 on the client device 104 to identify the data included therein. Through parsing, the content placement component 140 can extract or identify the identifier 136 included in the audio data packet. Once identified, the content placement component 140 can use the identifier 136 from the audio data packet as one of the content selection parameters. Instead of or in addition to parsing, the content placement component 140 can retrieve the identifier 136 from the digital assistant application 110 on the client device 104 via an Application Programming Interface (API). For example, the audio data packet may lack the identifier 136, and the digital assistant application 110 may have authenticated using the identifier 136 to operate. In this example, the content placement component 140 can invoke a function call to grab the identifier 136 according to the specification of the API used by the digital assistant application 110. Further, the content placement component 140 can parse the audio data packets to identify metadata included therein, such as device attributes, application attributes, and characteristic features. From this identification, the content placement component 140 can use the device attributes, application attributes, or characteristic features identified from the audio data packets as content selection parameters.
Further, the content placement component 140 may determine content point parameters for each content point 148 included in the audio recording 146 associated with the request for the input audio signal. The content point parameters may be used to evaluate the content points 148 and may be proportional to the measured or estimated number of viewings of the audio recording 146 or supplemental audio content inserted into the content points 148, or a combination thereof. The number of listens to the audio recording 146 itself and the number of listens to the supplemental audio content inserted into the content point 148 of the audio recording 146 may be detected from previous services of the audio recording 146 across multiple client devices 104 (events). For example, the number of listens to audio recording 145 may be measured from detecting playback of audio recording 145 via multiple instances of digital assistant application 110 across different client devices 104. The number of listens for supplemental audio content inserted into one content point 148 of an audio recording 146 may be measured from the number of times a predefined interaction event is detected across multiple client devices 104 that have been provided with the audio recording 146. The predefined interaction events may include, for example: a subsequent voice query detected via the microphone 154 of the client device 104 that includes a set of words related to the supplemental audio content or interactions with information resources associated with the content provider 108 associated with the supplemental audio content. The estimated number of listens to the audio recording 146 and the estimated number of listens to the supplemental audio content inserted into the content points 148 may be calculated from the measured times (e.g., via reconstruction, trend estimation, or extrapolation techniques). The number of measurements and the estimated number of times may be determined and maintained by a counter of the data processing system 102.
Upon determining the number of times, the content placement component 140 can build at least one predictive model to estimate the number of listens to the audio recording 146 and the number of listens to the supplemental audio content inserted into the content point 148. The predictive model may be generated according to any number of machine learning algorithms or models, such as a regression model (e.g., linear or logical), a support vector machine, an Artificial Neural Network (ANN), a random forest classifier, a bayesian statistical model, or a k-nearest neighbor algorithm, etc. The predictive model may be built using training data sets maintained on the data repository 124 or 144. The training data set may include previous or sample measurements of the number of listens to other audio recordings 146 and the number of listens to supplemental audio content inserted into the audio recordings 146 at various content points 148. Further, the training data set may include one or more characteristics of the audio recording 146 and the content points 148 themselves, such as: the length of each audio recording 146, the length of the content point 148 in the audio recording 146, the time within the audio recording 148 at which the content point 148 was defined, the subject matter category of the audio recording 146, and the subject matter category of the supplemental audio content inserted into the content point 148, etc. The training data set may also include data about the audience of the audio recording 146, such as: device attributes, application attributes or characteristic features, etc. Using the training data set, the content placement component 140 can train the predictive model according to a machine learning algorithm or type of model (e.g., until convergence). When trained, the predictive models can be used to determine a predicted (or estimated) number of listens for the audio recording 146 and the number of listens for supplemental audio content to be inserted into any content point 148 within the audio recording 146.
To determine the content point parameters for the content point 148, the content placement component 140 may calculate, determine, or identify the number of listens for the audio recording 146 across multiple client devices 104. The content placement component 140 can identify the measured number of listens for the audio recording 146 maintained by the counter. The content placement component 140 can identify an estimated number of listens to the audio recording 146 (e.g., calculated using extrapolation). The content placement component 140 may also apply the audio recording 146 to determine a predicted number of listens for the audio recording 146. When applied, the content placement component 140 can identify various characteristics (e.g., length and subject matter category) of the audio recording 146. After application, the content placement component 140 can determine or identify a predicted number of listens output by the predictive model.
Further, for each content point 148 in the audio recording 146, the content placement component 140 can calculate, determine, or identify a number of listens for supplemental audio content inserted into the content point 148 across multiple client devices 104. The supplemental audio content may correspond to one or more of the audio content items 150 maintained on the data store 144. The content placement component 140 can identify a measured number of listens for supplemental audio content inserted into the content points 148 maintained by the counter. The content placement component 140 can identify an estimated number of listens to supplemental audio content inserted into the content points 148 (e.g., calculated using extrapolation). The content placement component 140 can also apply the audio recording 146 to determine a predicted number of listens for supplemental audio content inserted into the content points 148. When applied, the content placement component 140 can identify various characteristics (e.g., length and subject matter category) and content points 148 (e.g., time within the audio recording 146) of the audio recording 146. After application, the content placement component 140 can identify the predicted number of listens output by the predictive model.
Based on the measured, estimated, or predicted number of listens, the content placement component 140 may calculate or determine content point parameters for the content points 148 in the audio recording 146. The content placement component 140 can use the number of listens for the audio recording 146 to determine a content point parameter. The content placement component 140 can also use the number of listens for supplemental audio content in the content points 148 of the audio recording 146 to determine another individual content point parameter. The content placement component 140 may also determine a single content point parameter based on a combination of the number of listens for the audio recording 146 and the number of listens for the content point 148 in the audio recording 146. The combination may include, for example, a summation, an average, a weighted average or function, or the like, or any combination thereof.
In selecting supplemental audio content to be inserted into the content point 148, the content placement component 140 may run or perform a content placement process to select an audio content item 150 from the set of candidate audio content items 150. In operation, the content placement component 150 can request, retrieve, or identify content submission parameters from each content provider 108. The content submission parameter may represent or indicate an evaluation of the content point 148 in the audio recording 146 by the corresponding content provider 108. The content submission parameters may be associated with the audio content items 150 provided by the respective content providers 108. The higher the value of the content submission parameter, the more likely the audio content item 150 of the content provider 108 is to be selected. To retrieve the content submission parameters, the content placement component 150 may send a request for the parameters to each content provider 108. The request may include a content selection parameter and a content point parameter. Upon receipt, each content provider 108 may determine or generate content submission parameters based on the content selection parameters and the content point parameters. Once generated, the content provider 108 may respond with content submission parameters for transmission to the data processing system 102.
Using the one or more parameters, the content placement component 140 can select at least one of the audio content items 150 for insertion into the content point 148 of the identified audio recording 146. The content placement component 140 may select an audio content item 150 from a set of candidate audio content items 150 maintained on the data store 144 based on the content selection parameters. For example, the content placement component 140 may find the audio content item 150 by matching the specifications (e.g., device attributes, application attributes, and characteristic attributes) with the content selection parameters. The content placement component 140 may also select the audio content item 150 based on the content point parameters (in conjunction with the content selection parameters). For example, the content placement component 140 may identify a subset of the audio content items 150 that have a classification topic that matches the interest indicated by the characteristic attribute and that have the same length as the content points 148. In this example, from the subset, the content placement component 140 can select one audio content item 150 with the highest predicted number of listens for supplemental audio content to insert into the audio recording 146 at the content point 148.
The content placement component 140 may also use the content submission parameters when selecting the content item 150, and may continue the content placement process if the content submission parameters are received. The content placement component 140 can rank the content submission parameters received from the various content providers 108. Based on the ranking, the content placement component 140 can identify the content provider 108 with the highest content submission parameter. With this identification, the content placement component 140 can identify or select the audio content item 150 from the content provider 108 having the highest content submission parameter in the ranking. Any combination of content selection parameters, content point parameters, or content submission parameters may be used to identify or select an audio content item 150 for insertion into the content point 148 of the audio recording 146. The content placement component 140 may repeat the process of selecting one audio content item 150 for each content point 148 defined for the audio recording 146.
The direct action handler component 120 of the digital assistant application 110 can insert the audio content item 150 selected by the content placement component 115 into the content point 148 of the audio recording 146. By selecting an audio content item 150 from the data store 144, the direct action handler component 120 may access the data store 144 to identify the audio recording 146 identified by the NLP component 116. For example, the direct action handler component 120 can retrieve one or more audio files corresponding to the sound recordings 146 generated by the content publisher 106 and identified by the NLP component 116. After identification, the direct action handler component 120 may identify a content point 148 in the audio recording 146 for which an audio content item 150 was selected. The direct action handler component 120 may add, embed, or insert the audio content item 150 into the content point 148 at a time defined by the corresponding content point marker. For example, the direct action handler component 120 may delete or empty audio content from the audio recording 146 during a time window defined by the content point markers of the content points 148. After removal, the direct action handler component 120 may overlay the selected audio content item 150 into a time window of the content point 148 within the audio recording 146. Upon insertion of the audio content item 150, the direct action handler component 120 may execute one or more signal processing algorithms to convert the supplemental audio content to be compatible with the remainder of the audio recording 146. The signal processing algorithms may include, for example, various audio mixing techniques such as equalization, compression, and equalization. The direct action handler component 120 may insert all audio content items 150 selected for the content points 148 of the audio recording 146. By inserting the audio content item 150, the direct action handler component 120 may cache or store the audio recording 146 (e.g., as one or more edited audio files) for provision to the client device 104.
The direct action handler component 120 may generate at least one action data structure to perform a request indicated in an input audio signal detected by a microphone 154 on the client device 104. The action data structure may be generated according to the hypertext transfer protocol (HTTP) or the like. For example, the action data structure may be included in the body (or payload) of the HTTP response along with other data to complete the request specified in the input audio signal. The direct action handler component 120 may invoke the response selector component 122 format or generate direct action structures according to data stored in the data store 124, such as regular expressions 126, parameters 128, policies 130, response data 132, templates 134, and the like. For example, the response selector component 122 can retrieve the template 134 from the data store 124 to determine which fields are to be included in the action data structure. The response selector component 122 can retrieve content from the data store 124 to obtain field information for a data structure for a response to a request for audio content. The response selector component 122 (or the direct action handler component 120) can include one or more words in the message for inclusion in the action data structure as a response to the request for audio content. For example, the response message may contain the word "Found podcast. downloading from Talk Show PQR".
In generating the action data structure, the direct action handler component 120 may include an audio recording 146 having one or more audio content items 150 inserted into a content point 148. The direct action handler component 120 may insert one or more audio files corresponding to the audio recording 146 with the inserted audio content item 150 into the action data structure. The direct action handler component 120 may include an address (e.g., a URL address or a network address) that references the audio recording 146 with the audio content item 150 into an action data structure. This address may be used by an instance of the digital assistant application 110 on the client device 104 to retrieve or download the audio recording 146 with the audio content item 150 inserted from the data store 144. The direct action handler component 120 may include the audio recording 146 as part of the body of an HTTP response that includes the action data structure. After insertion, the direct action handler component 120 may provide, send, or transmit the action data structure to the instance of the digital assistant application 110 on the client device 104. The transmission of the action data structure may involve or correspond to uploading the action data structure onto the client device 104 rather than streaming the audio recording 146 to the client device 104. For example, rather than providing the blocks of audio recordings 146 via streaming, the direct action handler component 120 may transmit files corresponding to the audio recordings 146 for download onto the client device 104.
By transmission, the instance of the digital assistant application 110 on the client device 104 may receive an action data structure that includes an audio recording 146 with an inserted audio content item 150. The receipt of the action data structure may involve or correspond to the retrieval or downloading of the audio recording 146 itself by the digital assistant application 110, rather than streaming the audio content included in the audio recording 146. The digital assistant application 110 on the client device 104 can parse the action data structure to extract, retrieve, or identify the audio recording 146. For example, when including audio files, the digital assistant application 110 can extract one or more audio files corresponding to the audio recording 146 from the body of the HTTP response. When including the address of the audio recording 146 with the inserted audio content item 150, the digital assistant application 110 can use the address to retrieve the audio recording 146 and download the audio recording 146 to the client device 104 (e.g., on a hard drive or memory). After retrieval, the digital assistant application 110 can render the audio recording 146 with the audio content item 150 inserted at the content point 148 via the speaker 152 of the client device 104. For example, the digital assistant application 110 on the client device 104 may include a media player component to process downloaded audio content that is played back via the speaker 152 of the audio recording 146.
The audio signal generator component 118 of the digital assistant application 110 (on the data processing system 102 or client device 104) can parse the action data structure to identify words for response. The audio signal generator component 118 can generate an output audio file based on one or more words of the response phrase in response to a request indicated in the input audio signal. The audio signal generator component 118 can play (e.g., via the speaker 152) an output audio file responsive to one or more words of the phrase. For example, the audio signal generator component 118 can generate an output audio file that includes the word "Found podcast. downloading from Talk Show PQR". The digital assistant application 110 on the client device 104 can also display one or more words of the response phrase.
The transition detection component 142 executing on the data processing system 102 can monitor one or more interaction events occurring on the client device 104 after providing the audio recording 146 with the inserted audio content item 150. The interaction event may include another input audio signal, a click event, a screen touch event or a playback start event, a playback pause event, or other event detected via the client device 104, and so forth. The transition detection component 142 can use a variety of techniques to monitor interaction events. For example, the transition detection component 142 may use the identifier 136 (e.g., in the form of a session identifier) to access interaction events received via a web application (e.g., a web browser). The translation detection component 142 can use the identifier 136 (e.g., in the form of a device identifier or an account identifier) to access interaction events detected via an Application Programming Interface (API) of the digital assistant application 110 on the client device 104. The API may define function calls for retrieving predefined interaction events detected at least on the digital assistant application 110 on the client device 104. For example, upon detecting a playback initiation and subsequent playback completion event, the digital assistant application 110 can send an indication to the transition detection component 142 via the API. The transition detection component 142 may access a location within the playback of the audio recording 146 by the digital assistant application 110 via the API. The location may indicate a point in time within which the audio recording 146 is being played via the client device 104.
By monitoring, the transition detection component 142 can maintain and update counters of the number of listens to the audio recordings 146 across multiple client devices 104. The counter can indicate the number of listengths of the measured audio recording 146 and can be used by the content placement component 140 in determining the content point parameters. To maintain the counters, transition detection component 142 may monitor locations within playback of audio recording 146 (e.g., via an API of digital assistant application 110). The transition detection component 142 can begin monitoring playback in response to detecting a playback initiation event from the digital assistant application 110 on the client device 104. The transition detection component 142 may determine whether the location matches a predefined duration of the audio recording 146. The predefined duration may correspond to the entire length of the audio recording 146 in time or a percentage (e.g., 75% to 95%) of the entire length of the audio recording 146 as specified by the content publisher 106.
The transition detection component 142 may compare the monitored location to a predefined duration. By comparison, the transition detection component 142 can determine whether playback of the audio recording 146 has been completed on the digital assistant application 110 on the client device 104. In response to determining that the location matches the predefined duration, the transition detection component 142 may determine that playback of the audio recording 146 is complete. In addition, transition detection component 142 can increment a counter for the number of listens for audio recording 146. Transition detection component 142 can also use the detection of a playback completion event to increment a counter independent of the monitored position. Conversely, in response to determining that the location does not match the predefined duration, transition detection component 142 may continue to monitor the location of playback.
Additionally, transition detection component 142 can maintain a counter of the number of listens for supplemental audio content inserted into content points 148 of audio recording 146 across multiple client devices. The counter may indicate a measured number of listens to supplemental audio content (e.g., the selected audio content item 150 or another audio content item 150) inserted into the content point 148 of the audio recording 146. The counter can be used by the content placement component 140 in determining the content point parameter. To maintain a counter of the number of listens to the audio content item 150, the transition detection component 142 may compare one or more detected interactivity events to a set of predefined interactivity events. A set of interaction events may be predefined for audio content items 150 inserted into the content points 148 of the audio recording 148. The set of interaction events may be designated by the content provider 108 for the audio content item 150 as corresponding to a conversion and may include one or more events intended for conversion. For example, the set of interaction events for the audio content item 150 may include retrieving an input audio signal including the name of the content provider 108 via the microphone 154 of the client device 104.
Based on the comparison, the transition detection component 142 may determine whether the detected interaction event matches a predefined interaction event of the audio content item 150. In response to determining a match between the detected interactivity event and the predefined interactivity event, the transition detection component 142 may determine that supplemental audio content inserted into the content point 148 of the audio recording 146 is listened to. Further, transition detection component 142 can increment a counter for the number of listens for supplemental audio content inserted into content points 148 of audio recording 146. The transition detection component 142 may also maintain and update a counter for the number of listens to the audio content item 150 itself. On the other hand, in response to determining a lack of match between the detected interactivity event and the predefined interactivity event, the transition detection component 142 may determine that the supplemental audio content inserted into the content point 148 did not result in a transition. In addition, transition detection component 142 can maintain a counter value for the number of listens for supplemental audio content inserted into content points 148 of audio recording 146.
Based on the measured times, the transition detection component 142 can calculate or determine an expected number of listens for the audio recording 146 and an expected number of listens for supplemental audio content inserted into each content point 148 of the audio recording 146. The transition detection component 142 can access a counter to identify the measured number of listens to the audio recording 146 and the measured number of listens to the supplemental audio content inserted into the content point 148 of the audio recording 146. From this identification, the conversion detection component 142 can apply any number of techniques (such as regression, reconstruction, trend estimation, or extrapolation) to determine the expected number of listens. For example, the transition detection component 142 may identify the number of listens to the audio recording 146 (or supplemental audio content at one of the content points 148) measured over time. The transition detection component 142 can construct a polynomial function to characterize the measured number of listens. As a function of the construction, the transition detection component 142 can determine an expected number of listens to the audio recording 146 at a future point in time.
In this manner, the audio content item 150 presented with the audio recording 146 may have a higher relevance to the user of the digital assistant application 110 of the client device 104 making the initial request. As the relevance increases, the likelihood of subsequent interaction with respect to the audio content item 150 increases. Additionally, the chance that a subsequent voice command to the digital assistant application 110 is inconsistent with a previous voice command or audio recording 146 may be reduced. Accordingly, inclusion of such audio content items 150 in the audio recordings 146 may improve human interaction between the user and the digital assistant application 110 on the client device 104 while conserving computing resources and conserving network bandwidth.
Fig. 2 depicts, among other things, a sequence diagram of an example data stream 200 for inserting supplemental audio content into primary audio content in the system shown in fig. 1. Data flow 200 may be implemented or performed by system 100 described above in connection with fig. 1 or system 500 described in detail below in connection with fig. 5. Data stream 200 may include communications in the form of packets (e.g., HTTP messages) between data processing system 102, client device 104, content publisher 106, content provider 108, speaker 152, and microphone 154, among others.
The instance of the digital assistant application 110 on the client device 104 may detect the audio signal 205 via the microphone 156. The digital assistant application 110 may initially process the audio signal 205 to generate data packets 210 (sometimes referred to herein as audio data packets). The data packet 210 may include the input audio signal 205 itself or one or more character strings identified from the audio signal 205 using natural language processing techniques. The client device 104 can send the data packet 210 to a remote instance of the digital assistant application 110 on the data processing system 102.
Meanwhile, the data processing system 102 may receive the audio file 215 from the content publisher 106. The audio file 215 may be an instance of the audio recording 146 and may include primary audio content generated by the content publisher 106. The audio file 215 may be received using content point tags that define a time window within the primary audio content of the audio file 215 into which the supplemental audio content is to be inserted. Upon receipt, the data processing system 102 may store and maintain the audio file 215 on the data store 144.
The instance of the digital assistant application 110 on the data processing system 102 may receive the data packet 210 from the client device 104. Upon receipt, the digital assistant application 110 can parse the input audio signal included in the data packet 210 to identify one or more words. From this identification, the digital assistant application 110 can determine that the word corresponds to a request for audio content from one of the content publishers 106. In response to this determination, the digital assistant application 110 can identify the audio file 210 from the specified content publisher 106. Further, the digital assistant application 110 can invoke a content placement process (e.g., via the content placement component 140).
In performing the processing, the data processing system 102 may determine content selection parameters for the requesting client device 104 and may determine content point parameters for content points in the audio file 215. The data processing system 102 may also send a grab request 220 to the content provider 108 for content placement parameters. Upon receipt, each content provider 108 may generate a value parameter 225 (sometimes referred to herein as a content submission parameter). The value parameter 225 may indicate the evaluation of the content point 148 within the audio file 215 by the corresponding content provider 108. Once generated, each content provider 108 may transmit the value parameter 225 to the data processing system 102. Using various parameters, the data processing system 102 may select one audio content item 150 to include in the content point 148 of the audio file 215 according to a content placement process.
By selecting the audio content item 150, the instance of the digital assistant application 110 on the data processing system 102 can insert the audio content item 150 into the content point 148 of the audio file 215. The digital assistant application 110 may also generate an action data structure 230 to package or include the audio file 215 with the audio content item 150 inserted into the content point 148. Upon generation, the digital assistant application 110 on the data processing system 102 may transmit the action data structure 230 to the instance of the digital assistant application 110 on the client device 104. The transmission of the audio file 215 may be a download onto the client device 104, rather than a stream. Accordingly, the digital assistant application 110 on the client device 104 may receive and parse the action data structure 230 to identify the audio file 215 with the inserted audio content item 150. With this identification, the digital assistant application 110 can convert the audio file 215 with the inserted audio content item 250 into an audio signal 235 for playback. The speaker 152 may output audio signals 235 to present and playback the primary and supplemental audio content included in the audio file 215.
Fig. 3 depicts, among other things, an example client device 104 having request and response messages when presenting primary and supplemental audio content in a configuration 300. In configuration 300, an instance of the digital assistant application 110 running on the client device 104 may receive an input audio signal via the microphone 154. The NLP component 116 can use natural language processing techniques to identify one or more words in the input audio signal. The digital assistant application 110 can display the output of the natural language processing technique as a textual content item 305. The textual content item 305 may include the word "Download a podcast from Talk Show a" parsed from the input audio signal acquired via the microphone 154. NLP component 116 may also use natural language processing techniques to identify that words parsed from the input audio signal correspond to requests for audio content. The NLP component 116 can identify the content publisher 106 associated with the request (e.g., "talk show a"). The digital assistant application 110 can display the results of the execution request with the textual content item 310. The text content item 310 may include the word "last post from Talk Show a" and may include a media player interface 315. The media player interface 315 may include, for example, a pause button, a play button, a progress bar, and the like as depicted.
Upon satisfying the request as indicated in the input audio signal, the NLP component 116 can identify one of the audio recordings 146 associated with the content publisher 106 from the data repository 144. The audio recording 146 may include primary audio content 320 and at least one content point marker defining content points 148. The content points 148 may define a time window within which supplemental audio content 325 is to be inserted. To insert the supplemental audio content 325, the content placement component 140 may determine content selection parameters for the client device 104 and content point parameters for the content point 148. In addition, the content placement component 140 can collect content submission parameters from various content providers 108 associated with the candidate audio content items 150. The content placement component 140 may run a content placement process using the set of parameters to select one of the audio content items 150 to be inserted into the content point 148. Once inserted, the data processing system 102 can provide the audio recording 146 with the audio content item 150 for download onto the client device 104 and playback via the digital assistant application 110 on the client device 104. Upon receiving interaction with the play button on the media player interface 315, the digital assistant application 110 can play back the audio recording 146 with the audio content item 150.
Fig. 4 depicts, among other things, a flow diagram of an example method 400 of inserting supplemental audio content into primary audio content via a digital assistant application. The method 400 may be implemented or performed by the system 100 described above in conjunction with fig. 1-3 or the system 500 described in detail below in conjunction with fig. 5. The method 400 may include parsing an input audio signal (405). An instance of the digital assistant application 110 on the client device 104 may receive an input audio signal acquired via the microphone 154. The digital assistant application 110 may perform initial processing and encapsulate the input audio signal into audio data packets for transmission to the data processing system 102. An instance of the digital assistant application 110 on the data processing system 102 may receive audio data packets from the client device 104. The NLP component 116 may parse the audio data packets to identify the input audio signal. For processing, the NLP component 116 may identify one or more words in the input audio signal using natural language processing, and may determine that the input audio signal corresponds to a request for audio content based on the identified words.
The method 400 may include identifying the audio recording 146 (410). In response to determining that the input audio signal corresponds to a request for audio content, NLP component 116 may identify a content provider 108 associated with the request. Through this identification, the NLP component 116 can access the data store 144 to identify the audio recording 146 belonging to the content publisher 106 indicated in the request. The data store can be maintained by the record indexer component 138 and can include audio records 146 from various content publishers 106. The audio recording 146 can include primary audio content and at least one content point 148 defined by a content point marker for inserting supplemental audio content.
The method 400 may include identifying content selection parameters (415). In response to determining that the input audio signal corresponds to a request for content, a content placement component 140 on the data processing system 102 can determine content point parameters for the client device 104. The content placement component 140 can identify an identifier 136 (e.g., a device identifier, an account identifier, a session or cookie identifier) associated with the digital assistant application 110 on the requesting client device 104. The content placement component 140 can also identify data associated with the audio data packets, such as device attributes, application attributes, or characteristic features, and the like. The content placement component 140 can use the identifier 136 and associated data as a content selection parameter.
The method 400 may include determining a content point parameter (420). To determine the content point parameters for one of the content points 148, the content placement component 140 can identify the number of listens for the identified audio recording 146. The content placement component 140 can also identify the number of listens for supplemental audio content inserted into the content points 148 of the audio recording 146. Each number of listens can be measured using a counter, estimated using extrapolation, or predicted using a machine learning model. Once identified, the content placement component 140 can calculate or determine content point parameters based on the number of listens.
The method 400 may include selecting the audio content item 150 (425). Using the content selection parameters and the content point parameters, the content placement component 140 may select one of the audio content items 150 to be inserted into the content point 148 of the audio recording 146. The content placement component 140 may also perform a content placement process to select the audio content item 150. In running this process, the content placement component 140 may crawl content submission parameters from each content provider 108 associated with one of the candidate audio content items 150. In addition to the content selection parameters and the content point parameters, the content placement component 140 may also use the content submission parameters when selecting the audio content item 150 for insertion.
The method 400 may include inserting the audio content item 150 into the audio recording 146 (430). The direct action handler component 120 of the digital assistant application 110 may insert the selected audio content item 150 into the content point 148 of the audio recording 146. For insertion, the direct action handler component 120 may identify a content point 148 defined by the content point tag of the audio recording 146. Once identified, the direct action handler component 120 may overlay or insert the selected audio content item 150 into the content point 148 of the audio recording 146. The direct action handler component 120 may also perform additional signal processing techniques to facilitate insertion of the audio content item 150 into the audio recording 146.
The method 400 may include transmitting the action data structure (435). The direct action handler component 120 may generate the action data structure to include the audio recording 146 with the inserted audio content item 150. The generation of the action data structure may be according to the text transfer protocol (HTTP), where the action data structure is included as part of the body of the HTTP response. Once generated, the direct action handler component 120 may send or provide the action data structure to the instance of the digital assistant application 110 on the client device 104. In providing the action data structure, the direct action handler component 120 may provide the audio recording 146 with the inserted audio content item 150 as a download, rather than as a streaming. Upon receipt, the instance of the digital assistant application 110 on the client device 104 can begin playback of the audio recording 146 with the inserted audio content item 150.
The method 400 may include monitoring playback and interaction (440). After sending the action data structure, a transition detection component 142 on the data processing system 102 can monitor the interaction and playback. The transition detection component 142 can monitor using any number of techniques, such as using a session cookie, or accessing the client device 104 via an application programming interface of the digital assistant application 110. Using the detected interactions and playback, transition detection component 142 can maintain and update a counter of the number of listens for audio recording 146. Transition detection component 142 can maintain and update a counter of the number of listens for supplemental audio content inserted into content points 148 of audio recording 146.
Fig. 5 is a block diagram of an example computer system 500. Computer system or computing device 500 may include or be used to implement system 100 or components thereof, such as data processing system 102. Computing system 500 includes a bus 505 or other communication component for communicating information, and a processor 510 or processing circuit coupled to bus 505 for processing information. Computing system 500 can also include one or more processors 510 or processing circuits coupled to the bus to process information. Computing system 500 also includes a main memory 515, such as a Random Access Memory (RAM) or other dynamic storage device, coupled to bus 505 for storing information and instructions to be executed by processor 510. The main memory 515 may be or include the data store 124 or 144. Main memory 515 also may be used for storing location information, temporary variables, or other intermediate information during execution of instructions by processor 510. Computing system 500 may also include a Read Only Memory (ROM)520 or static storage device coupled to bus 505 for storing static information and instructions for processor 510. A storage device 525, such as a solid state device, magnetic disk or optical disk, may be coupled to bus 505 for persistently storing information and instructions. The storage device 525 may comprise or be part of the data store 124 or 144.
The processes, systems, and methods described herein may be implemented by the computing system 500 in response to the processor 510 executing an arrangement of instructions contained in main memory 515. Such instructions may be read into main memory 515 from another computer-readable medium, such as storage device 525. Execution of the arrangement of instructions contained in main memory 515 causes computing system 500 to perform the illustrative processes described herein. One or more processors in a multi-processing arrangement may also be employed to execute the instructions contained in main memory 515. Hardwired circuitry may be used in place of or in combination with software instructions in the systems and methods described herein. The systems and methods described herein are not limited to any specific combination of hardware circuitry and software.
Although an example computing system has been described in FIG. 5, the subject matter including the operations described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and structural equivalents, or in combinations of one or more of them.
For the case where the system discussed herein collects personal information about a user or is likely to use personal information, the user may be provided with the following opportunities: control whether programs or functions may gather personal information (e.g., information about a person's social network, social behavior or activity, a user's preferences, or a user's location), or control whether or how content is received from a content server or other data processing system that may be more relevant to the user. In addition, certain data may be anonymized in one or more ways prior to storage or use, so that personally identifiable information is deleted when parameters are generated. For example, the identity of the user may be anonymized so that no personally identifiable information can be determined for the user, or the geographic location of the user may be generalized (such as at a city, zip code, or state level) if location information is obtained so that a particular location of the user cannot be determined. Thus, the user may control how information is collected about him or her and used by the content server.
The subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. The subject matter described in this specification can be implemented as one or more computer programs, e.g., one or more circuits of computer program instructions, encoded on one or more computer storage media for execution by, or to control the operation of, data processing apparatus. Alternatively or additionally, the program instructions may be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by the data processing apparatus. The computer storage media may be or be included in a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. Although a computer storage medium is not a propagated signal, a computer storage medium can be a source or destination of computer program instructions encoded in an artificially generated propagated signal. The computer storage media may also be or be included in one or more separate components or media (e.g., multiple CDs, disks, or other storage devices). The operations described in this specification can be implemented as operations performed by data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
The terms "data processing system," "computing device," "component," or "data processing apparatus" encompass various devices, apparatuses, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip or multiple chips, or a combination of the foregoing. The apparatus can comprise special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus and execution environment may implement a variety of different computing model infrastructures, such as web services, distributed computing, and grid computing infrastructures. The components of system 100 may include or share one or more data processing apparatuses, systems, computing devices, or processors.
A computer program (also known as a program, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. The computer program may correspond to a file in a file system. A computer program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs (e.g., components of data processing system 102) to perform actions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and storage devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
The subject matter described herein can be implemented in a computing system that includes a back-end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front-end component (e.g., a client computer having a graphical user interface or a web browser through which a user can interact with an implementation of the subject matter described in this specification), or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include local area networks ("LANs") and wide area networks ("WANs"), the internet (e.g., the internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
A computing system, such as system 100 or system 500, may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network, such as network 112. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some implementations, the server transmits data (e.g., data packets representing content items) to the client device (e.g., for the purpose of displaying data to and receiving user input from a user interacting with the client device). Data generated at the client device (e.g., the result of the user interaction) may be received from the client device at the server (e.g., received by the data processing system 102 from the client device 104).
Although operations are depicted in the drawings in a particular order, such operations need not be performed in the particular order shown or in sequential order, and all illustrated operations need not be performed. The actions described herein may be performed in a different order.
The separation of various system components need not be separate in all embodiments, and the described program components may be included in a single hardware or software product. For example, NLP component 116 and direct action handler component 120 may be part of a single component, application, or program, or one or more servers having one or more processing circuit logic devices, or data processing system 102.
Having now described some illustrative embodiments, it is apparent that the foregoing has been given by way of example only, and not by way of limitation. In particular, although many of the examples presented herein involve specific combinations of method acts or system elements, those acts and those elements may be combined in other ways to accomplish the same objectives. Acts, elements and features discussed in connection with one embodiment are not intended to be excluded from a similar role in other embodiments.
The phraseology and terminology used herein is for the purpose of description and should not be regarded as limiting. The use of "including," "comprising," "having," "containing," "involving," "characterized by," and variations thereof herein, is meant to encompass the items listed thereafter and equivalents thereof as well as additional items and alternative embodiments consisting only of the items listed thereafter. In one embodiment, the systems and methods described herein consist of one, more than one combination of each, or all of the described elements, acts, or components.
Any reference to an embodiment, element, or act of the systems and methods herein referred to in the singular may also encompass embodiments comprising a plurality of such elements, and any reference to any embodiment, element, or act herein in the plural may also encompass embodiments comprising only a single element. References in the singular or plural form are not intended to limit the presently disclosed systems or methods, their components, acts, or elements to a single or multiple configurations. References to any action or element based on any information, action, or element may include embodiments in which the action or element is based, at least in part, on any information, action, or element.
Any embodiment disclosed herein may be combined with any other embodiment or examples, and references to "an embodiment," "some embodiments," "one embodiment," etc. are not necessarily mutually exclusive and are intended to indicate that a particular feature, structure, or characteristic described in connection with the embodiment may be included in at least one embodiment or example. Such terms as used herein do not necessarily all refer to the same embodiment. Any embodiment may be combined with any other embodiment, inclusively or exclusively, in any manner consistent with aspects and embodiments disclosed herein.
References to "or" may be construed as inclusive such that any term described using "or" may indicate any single one, more than one, or all of the described terms. A reference to at least one of the conjunctive list of terms may be interpreted as inclusive or to indicate any of the individual, more than one, and all of the described terms. For example, a reference to "at least one of a ' and ' B ' may include only" a ", only" B ", and both" a "and" B ". Such references used in connection with "including" or other open-ended terms may include additional items.
Where technical features in the drawings, detailed description or any claim are followed by reference signs, the reference signs have been included to increase the intelligibility of the drawings, detailed description, and claims. Accordingly, neither the reference signs nor their absence have any limiting effect on the scope of any claim elements.
The systems and methods described herein may be embodied in other specific forms without departing from the characteristics thereof. The foregoing embodiments are illustrative, and not limiting of the described systems and methods. The scope of the systems and methods described herein is, therefore, indicated by the appended claims rather than by the foregoing description, and all changes that come within the meaning and range of equivalency of the claims are intended to be embraced therein.
Claims (20)
1. A system for inserting supplemental audio content into primary audio content via a digital assistant application, comprising:
a record indexer component executing on a data processing system having one or more processors, the record indexer component maintaining on a database an audio record of a content publisher and content point markers set by the content publisher to specify content points defining times at which supplemental audio content is inserted during presentation of the audio record;
a natural language processor component executing on the data processing system to:
receiving an audio data packet comprising an input audio signal detected by a sensor of a client device;
parsing an input audio signal from the audio data packet to determine that the input audio signal corresponds to a request for the audio recording from the content publisher; and
identifying the audio recording of the content publisher from the database based on a request determined from the input audio signal;
a content placement component executing on the data processing system to:
in response to determining that the input audio signal corresponds to a request for the audio recording, identifying an identifier associated with the client device as a content selection parameter; and
selecting, for the content point of the audio recording, an audio content item of a content provider from a plurality of audio content items using the content selection parameter; and
an action handler component executing on the data processing system to:
inserting the audio content item into a content point of the audio recording specified by the content point marker;
generating an action data structure comprising an audio recording with an audio content item inserted at a time defined by the content point marker; and
transmitting the action data structure to the client device to present an audio recording with the audio content item inserted at the content point.
2. The system of claim 1, comprising a transition detection component executing on the data processing system, the transition detection component to:
after transmitting the action data structure, monitoring for an interaction event performed via the client device, the interaction event matching a predefined interaction for the audio content item selected for insertion into the audio recording; and
in response to detecting an interaction event from the client device that matches the predefined interaction, determining that the audio content item inserted into the audio recording is to be listened to via the client device.
3. The system of claim 1 or claim 2, comprising a transition detection component executing on the data processing system, the transition detection component to:
after transmitting the action data structure, monitoring a location within playback of an audio recording inserted with the audio content item via an Application Programming Interface (API) of an application running on the client device using the identifier, the application to process playback of the audio recording; and
in response to the location matching a duration of an audio recording detected via the API, determining that playback of the audio recording with the audio content item inserted is complete.
4. The system of any of claims 1 to 3, comprising a transition detection component executing on the data processing system, the transition detection component to:
determining, based on the measured number of client devices from which the predefined interactivity event was detected, an expected number of client devices from which a predefined interactivity event for one of a plurality of audio content items will be detected after playback of the audio recording; and
determining an expected number of client devices from which playback of the audio recording inserted with one of the plurality of audio content items will be completed based on the measured number of client devices from which completion of playback of the audio recording was detected.
5. The system of any one of the preceding claims, comprising the content placement component to:
building a predictive model using the training data to estimate a number of client devices from which a predefined interaction event for one of the plurality of content items is expected to be detected after playback of an audio recording in which the one of the plurality of audio content items is inserted;
applying the predictive model to an audio recording having a content point specified by the content point marker to determine a content point parameter corresponding to an expected number of client devices on which interaction events matching a predefined interaction for each of the plurality of audio content items inserted into the audio recording at the content point are detected; and
selecting the audio content item of the content provider from the plurality of audio content items based on the content point parameter for the content point and a content submission parameter for each of the plurality of audio content items.
6. The system of any one of the preceding claims, comprising the content placement component to:
identifying a number of client devices on which interaction events matching predefined interactions for each of the plurality of audio content items inserted into the audio recording at the content point are detected;
determining a content point parameter for the content point defined in the audio recording based on a number of client devices on which the interaction event matches the predefined interaction; and
selecting the audio content item of the content provider from the plurality of audio content items based on the content point parameter for the content point and a content submission parameter for each of the plurality of audio content items.
7. The system of any one of the preceding claims, comprising the content placement component to:
identifying a number of client devices that completed playback of an audio recording in which one of the plurality of audio content items was inserted;
determining a content point parameter for the content point defined in the audio recording based on a number of client devices that completed the playback; and
selecting the audio content item of the content provider from the plurality of audio content items based on the content point parameter for the content point and a content submission parameter for each of the plurality of audio content items.
8. The system of any one of the preceding claims, comprising the content placement component to:
in response to determining that the input audio signal corresponds to the request, identifying a plurality of content selection parameters, the plurality of content selection parameters including at least one of: a device identifier, a cookie identifier associated with a session of the client device, an account identifier for authenticating an application executing on the client device to playback to the audio recording, and a characteristic feature associated with the account identifier; and
selecting the audio content item from the plurality of audio content items using the plurality of content selection parameters.
9. The system of any of the preceding claims, comprising the content placement component to identify, via an Application Programming Interface (API) of an application running on the client device, an identifier associated with the client device in response to determining that the input audio signal corresponds to the request.
10. The system of any one of the preceding claims, comprising:
the natural language processor component to receive the audio data packet including an identifier associated with the client device, the identifier to authenticate the client device to retrieve the audio recording; and
the content placement component to, in response to determining that the input audio signal corresponds to the request, parse the audio data packet to identify the identifier as the content selection parameter.
11. The system of any of the preceding claims, comprising a record indexer component to maintain on the database audio records of the content publisher corresponding to at least one audio file to be downloaded on the client device for presentation.
12. The system of any one of the preceding claims, comprising the action handler component to transmit the action data structure to load an audio recording with the audio content item inserted at the content point onto the client device without streaming.
13. A method of inserting supplemental audio content into primary audio content via a digital assistant application, comprising:
maintaining, by a data processing system having one or more processors, on a database, an audio recording of a content publisher and a content point marker set by the content publisher to specify a content point defining a time at which supplemental audio content is inserted during presentation of the audio recording;
receiving, by the data processing system, an audio data packet comprising an input audio signal detected by a sensor of a client device;
parsing, by the data processing system, an input audio signal from the audio data packet to determine that the input audio signal corresponds to a request for the audio recording from the content publisher;
identifying, by the data processing system, the audio recording of the content publisher from the database based on a request determined from the input audio signal;
identifying, by the data processing system, an identifier associated with the client device as a content selection parameter in response to determining that the input audio signal corresponds to the request for the audio recording;
selecting, by the data processing system, an audio content item of a content provider from a plurality of audio content items for the content point of the audio recording using the content selection parameter;
inserting, by the data processing system, the audio content item into a content point of the audio recording, the content point defining a time specified by the content point marker;
generating, by the data processing system, a motion data structure comprising an audio recording with an audio content item inserted at a time defined by the content point marker; and
transmitting, by the data processing system, the action data structure to the client device to present the audio recording with the audio content item inserted at the content point.
14. The method of claim 13, comprising:
after transmitting the action data structure, monitoring, by the data processing system, an interaction event performed via the client device, the interaction event matching a predefined interaction for the audio content item selected for insertion into the audio recording; and
determining, by the data processing system, that the audio content item inserted into the audio recording is being listened to via the client device in response to detecting the interaction event from the client device that matches the predefined interaction.
15. The method according to claim 13 or 14, comprising:
after transmitting the action data structure, monitoring, by the data processing system, a location within playback of an audio recording inserted with the audio content item via an Application Programming Interface (API) of an application running on the client device using the identifier, the application for processing playback of the audio recording; and
determining, by the data processing system, that playback of the audio recording with the audio content item inserted is complete in response to the location matching a duration of the audio recording detected via the API.
16. The method of any of claims 13 to 15, comprising:
establishing, by the data processing system, a predictive model using training data to estimate a number of client devices from which a predefined interaction event for one of the plurality of content items is expected to be detected after playback of an audio recording in which the one of the plurality of audio content items is inserted;
applying, by the data processing system, the predictive model to an audio recording having a content point specified by the content point marker to determine a content point parameter corresponding to an expected number of client devices on which interaction events matching a predefined interaction for each of the plurality of audio content items inserted into the audio recording at the content point are detected; and
selecting, by the data processing system, the audio content item of the content provider from the plurality of audio content items based on the content point parameter for the content point and a content submission parameter for each of the plurality of audio content items.
17. The method of any of claims 13 to 16, comprising:
identifying, by the data processing system, a number of client devices on which interaction events matching predefined interactions for each of the plurality of audio content items inserted into the audio recording at the content point are detected;
determining, by the data processing system, a content point parameter for the content point defined in the audio recording based on a number of client devices on which the interaction event matches the predefined interaction; and
selecting, by the data processing system, the audio content item of the content provider from the plurality of audio content items based on the content point parameter for the content point and a content submission parameter for each of the plurality of audio content items.
18. The method of any of claims 13 to 17, comprising:
identifying, by the data processing system, a number of client devices that completed playback of the audio recording into which the one of the plurality of audio content items was inserted;
determining, by the data processing system, a content point parameter for the content point defined in the audio recording based on a number of client devices that completed the playback; and
selecting, by the data processing system, the audio content item of the content provider from the plurality of audio content items based on the content point parameter for the content point and a content submission parameter for each of the plurality of audio content items.
19. The method of any of claims 13 to 18, comprising:
in response to determining that the input audio signal corresponds to the request, identifying, by the data processing system, a plurality of content selection parameters, the plurality of content selection parameters including at least one of: a device identifier, a cookie identifier associated with a session of the client device, an account identifier for authenticating an application executing on the client device to playback to the audio recording, and a characteristic feature associated with the account identifier; and
selecting, by the data processing system, the audio content item from the plurality of audio content items using the plurality of content selection parameters.
20. The method of any of claims 13 to 19, comprising:
transmitting, by the data processing system, the action data structure to load, onto the client device without streaming, an audio recording inserted with the audio content item at the content point.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2019/063317 WO2021107932A1 (en) | 2019-11-26 | 2019-11-26 | Dynamic insertion of supplemental audio content into audio recordings at request time |
Publications (1)
Publication Number | Publication Date |
---|---|
CN113196384A true CN113196384A (en) | 2021-07-30 |
Family
ID=69061445
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980004868.XA Pending CN113196384A (en) | 2019-11-26 | 2019-11-26 | Dynamic insertion of supplemental audio content into an audio recording at demand time |
Country Status (6)
Country | Link |
---|---|
US (1) | US11949946B2 (en) |
EP (1) | EP3854037B1 (en) |
JP (1) | JP7174755B2 (en) |
KR (1) | KR102389776B1 (en) |
CN (1) | CN113196384A (en) |
WO (1) | WO2021107932A1 (en) |
Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5625570A (en) * | 1994-06-07 | 1997-04-29 | Technicolor Videocassette, Inc. | Method and system for inserting individualized audio segments into prerecorded video media |
US6223210B1 (en) * | 1998-10-14 | 2001-04-24 | Radio Computing Services, Inc. | System and method for an automated broadcast system |
US6684249B1 (en) * | 2000-05-26 | 2004-01-27 | Sonicbox, Inc. | Method and system for adding advertisements over streaming audio based upon a user profile over a world wide area network of computers |
US20070078709A1 (en) * | 2005-09-30 | 2007-04-05 | Gokul Rajaram | Advertising with audio content |
CN102945074A (en) * | 2011-10-12 | 2013-02-27 | 微软公司 | Population of lists and tasks from captured voice and audio content |
CN104205791A (en) * | 2011-12-20 | 2014-12-10 | 奥德伯公司 | Managing playback of supplemental information |
US20150206168A1 (en) * | 2014-01-23 | 2015-07-23 | Apple Inc. | Technologies for inserting dynamic content into podcast episodes |
CN106688251A (en) * | 2014-07-31 | 2017-05-17 | 杜比实验室特许公司 | Audio processing systems and methods |
US20180157745A1 (en) * | 2016-12-07 | 2018-06-07 | At&T Intellectual Property I, L.P. | User configurable radio |
CN110192246A (en) * | 2017-06-09 | 2019-08-30 | 谷歌有限责任公司 | Modification to the computer program output based on audio |
Family Cites Families (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2003244677A (en) | 2002-02-13 | 2003-08-29 | Red Rice Medium Inc | Moving picture distribution and reproduction control system and computer program for moving picture distribution and reproduction control |
EP1842369B1 (en) | 2005-01-12 | 2020-04-08 | Invidi Technologies Corporation | Targeted impression model for broadcast network asset delivery |
US8676900B2 (en) | 2005-10-25 | 2014-03-18 | Sony Computer Entertainment America Llc | Asynchronous advertising placement based on metadata |
JP2007201742A (en) | 2006-01-25 | 2007-08-09 | Ntt Software Corp | Content distribution system |
US20150154632A1 (en) | 2007-04-30 | 2015-06-04 | Deepak Jindal | Determining a number of view-through conversions for an online advertising campaign |
WO2011049235A1 (en) | 2009-10-23 | 2011-04-28 | シャープ株式会社 | Content delivery system, content delivery device, content viewing device, content delivery method and content viewing method |
US11218434B2 (en) | 2013-06-12 | 2022-01-04 | Google Llc | Audio data packet status determination |
WO2019173577A1 (en) | 2018-03-08 | 2019-09-12 | Bose Corporation | Audio content engine for audio augmented reality |
WO2019173573A1 (en) * | 2018-03-08 | 2019-09-12 | Bose Corporation | User-interfaces for audio-augmented-reality |
-
2019
- 2019-11-26 US US16/621,334 patent/US11949946B2/en active Active
- 2019-11-26 WO PCT/US2019/063317 patent/WO2021107932A1/en unknown
- 2019-11-26 KR KR1020207009966A patent/KR102389776B1/en active IP Right Grant
- 2019-11-26 CN CN201980004868.XA patent/CN113196384A/en active Pending
- 2019-11-26 JP JP2020519272A patent/JP7174755B2/en active Active
- 2019-11-26 EP EP19829374.8A patent/EP3854037B1/en active Active
Patent Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5625570A (en) * | 1994-06-07 | 1997-04-29 | Technicolor Videocassette, Inc. | Method and system for inserting individualized audio segments into prerecorded video media |
US6223210B1 (en) * | 1998-10-14 | 2001-04-24 | Radio Computing Services, Inc. | System and method for an automated broadcast system |
US6684249B1 (en) * | 2000-05-26 | 2004-01-27 | Sonicbox, Inc. | Method and system for adding advertisements over streaming audio based upon a user profile over a world wide area network of computers |
US20070078709A1 (en) * | 2005-09-30 | 2007-04-05 | Gokul Rajaram | Advertising with audio content |
CN102945074A (en) * | 2011-10-12 | 2013-02-27 | 微软公司 | Population of lists and tasks from captured voice and audio content |
CN104205791A (en) * | 2011-12-20 | 2014-12-10 | 奥德伯公司 | Managing playback of supplemental information |
US20150206168A1 (en) * | 2014-01-23 | 2015-07-23 | Apple Inc. | Technologies for inserting dynamic content into podcast episodes |
CN106688251A (en) * | 2014-07-31 | 2017-05-17 | 杜比实验室特许公司 | Audio processing systems and methods |
US20180157745A1 (en) * | 2016-12-07 | 2018-06-07 | At&T Intellectual Property I, L.P. | User configurable radio |
CN110192246A (en) * | 2017-06-09 | 2019-08-30 | 谷歌有限责任公司 | Modification to the computer program output based on audio |
Also Published As
Publication number | Publication date |
---|---|
JP7174755B2 (en) | 2022-11-17 |
WO2021107932A1 (en) | 2021-06-03 |
EP3854037B1 (en) | 2024-04-17 |
KR20210068316A (en) | 2021-06-09 |
EP3854037A1 (en) | 2021-07-28 |
US20220286732A1 (en) | 2022-09-08 |
KR102389776B1 (en) | 2022-04-22 |
JP2022515686A (en) | 2022-02-22 |
US11949946B2 (en) | 2024-04-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10819811B2 (en) | Accumulation of real-time crowd sourced data for inferring metadata about entities | |
JP6876752B2 (en) | Response method and equipment | |
JP7044916B2 (en) | Feedback controller for data transmission | |
US20180121547A1 (en) | Systems and methods for providing information discovery and retrieval | |
JP7139295B2 (en) | System and method for multimodal transmission of packetized data | |
US11908459B2 (en) | Detection of potential exfiltration of audio data from digital assistant applications | |
JP2020129145A (en) | Modulation of packetized audio signal | |
CN101477798A (en) | Method for analyzing and extracting audio data of set scene | |
TW201214173A (en) | Methods and apparatus for displaying content | |
Thorogood et al. | Computationally Created Soundscapes with Audio Metaphor. | |
US20220157314A1 (en) | Interruption detection and handling by digital assistants | |
US11451601B2 (en) | Systems and methods for dynamic allocation of computing resources for microservice architecture type applications | |
CN111883131B (en) | Voice data processing method and device | |
CN111279333A (en) | Language-based search of digital content in a network | |
CN110889008B (en) | Music recommendation method and device, computing device and storage medium | |
CN112262371A (en) | Invoking functionality of a proxy via a digital assistant application using an address template | |
CN110858234A (en) | Method and device for pushing information according to human emotion | |
US11949946B2 (en) | Dynamic insertion of supplemental audio content into audio recordings at request time | |
EP3671735B1 (en) | Method and system for determining speaker-user of voice-controllable device | |
CN110830595B (en) | Personalized music pushing method and system | |
KR20230014680A (en) | Bit vector based content matching for 3rd party digital assistant actions | |
Thiruvengatanadhan et al. | Indexing and retrieval of speech using perceptual linear prediction and sonogram |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |