US20230350657A1 - Translating large source code using sparse self-attention - Google Patents
Translating large source code using sparse self-attention Download PDFInfo
- Publication number
- US20230350657A1 US20230350657A1 US17/731,593 US202217731593A US2023350657A1 US 20230350657 A1 US20230350657 A1 US 20230350657A1 US 202217731593 A US202217731593 A US 202217731593A US 2023350657 A1 US2023350657 A1 US 2023350657A1
- Authority
- US
- United States
- Prior art keywords
- source code
- snippet
- tokens
- function
- graphs
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000013519 translation Methods 0.000 claims abstract description 55
- 238000000034 method Methods 0.000 claims abstract description 44
- 238000010801 machine learning Methods 0.000 claims abstract description 39
- 230000006870 function Effects 0.000 claims description 72
- 230000015654 memory Effects 0.000 claims description 21
- 238000012545 processing Methods 0.000 claims description 11
- 230000008569 process Effects 0.000 claims description 7
- 230000004044 response Effects 0.000 claims 2
- 230000014616 translation Effects 0.000 description 39
- 230000007246 mechanism Effects 0.000 description 14
- 239000011159 matrix material Substances 0.000 description 10
- 230000001419 dependent effect Effects 0.000 description 5
- 239000000463 material Substances 0.000 description 5
- 238000004458 analytical method Methods 0.000 description 4
- 238000013528 artificial neural network Methods 0.000 description 3
- 230000008901 benefit Effects 0.000 description 3
- 238000009635 antibiotic susceptibility testing Methods 0.000 description 2
- 230000002457 bidirectional effect Effects 0.000 description 2
- 238000004364 calculation method Methods 0.000 description 2
- 238000010586 diagram Methods 0.000 description 2
- 238000003058 natural language processing Methods 0.000 description 2
- 230000008520 organization Effects 0.000 description 2
- 230000002093 peripheral effect Effects 0.000 description 2
- 230000000306 recurrent effect Effects 0.000 description 2
- 230000000007 visual effect Effects 0.000 description 2
- 238000003491 array Methods 0.000 description 1
- 238000004891 communication Methods 0.000 description 1
- 238000013527 convolutional neural network Methods 0.000 description 1
- 230000001934 delay Effects 0.000 description 1
- 239000000796 flavoring agent Substances 0.000 description 1
- 235000019634 flavors Nutrition 0.000 description 1
- 230000010006 flight Effects 0.000 description 1
- 230000014509 gene expression Effects 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000012423 maintenance Methods 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000001537 neural effect Effects 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 238000013442 quality metrics Methods 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 229920002803 thermoplastic polyurethane Polymers 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F8/00—Arrangements for software engineering
- G06F8/40—Transformation of program code
- G06F8/51—Source to source
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
Definitions
- Computer software programming often requires developers to read and/or write source code (i.e., to program) in a specific programming language, e.g. Java, C++, C, Python, etc.
- a specific programming language e.g. Java, C++, C, Python, etc.
- Each programming language has its own strengths, weaknesses, nuances, idiosyncrasies, etc.
- some programming languages are more suitable for certain stages of software development and/or a software life cycle than others.
- scripting languages such as Python, JavaScript, Perl, etc., are often more effectively used near the very beginning of software development because programmers using these languages are able to turn around functional software relatively quickly.
- Most programmers obtain at least a superficial understanding of multiple programming languages, but only master a few. Consequently, each programming language tends to have its own talent pool.
- transformer networks have become increasingly popular for performing natural language processing.
- Transformer networks were designed in part to mitigate a variety of shortcomings of prior natural language processing models, such as overfitting, the vanishing gradient problem, and exceedingly high computational costs, to name a few.
- transformer networks still require large amounts of memory when processing large sequences of data, such as large pieces of source code.
- attention-based networks such as transformer networks impose significant computational costs (especially memory) when used to process large sequences of data is that they include attention mechanisms that attend across all possible pairs of input tokens.
- Such an attention mechanism may be referred to as an “all pair attention network” because every input (e.g., source code token) is conceptually connected to, and hence, attended against, every other input token, regardless of whether those tokens are dependent on each other logically.
- the computational costs associated with an all-pair attention network may be acceptable, if not ideal.
- the sequence of inputs grows in length, the memory requirements grow as well, quickly becoming unwieldy.
- Implementations are described herein for encoding large structured textual data for purposes such as translation between domain-specific languages, with reduced memory requirements. More particularly, but not exclusively, implementations are described herein for translating source code between programming languages using machine learning models with sparse self-attention mechanisms. Using sparse attention mechanisms avoids wasting computing resources, particularly memory, on calculations that have limited to no utility for performing downstream tasks such as translation.
- a method for translating a source code snippet from a first programming language to a second programming language may be implemented by one or more processors and may include: obtaining one or more graphs representing snippet tokens, and relationships between the snippet tokens, contained in the source code snippet written in the first programming language; based on the one or more graphs, identifying, from a superset of all possible pairs of the snippet tokens in the source code snippet, a subset of snippet token pairs, wherein each token pair of the subset includes snippet tokens that are represented by nodes connected by one or more edges of the one or more graphs; adapting a self-attention network of a translation machine learning model to sparsely attend across the identified subset of token pairs; and processing the source code snippet based on the adapted translation machine learning model to generate a translation of the source code snippet in the second programming language.
- the one or more graphs may include at least one of a data flow graph (DFG), a control flow graph (CFG), or an abstract syntax tree (AST) representing the source code snippet.
- the edges of one or more of the graphs may represent dependencies between the snippet tokens.
- the source code snippet may be a function
- the method may further include processing an entire source code file that contains the function to identify global tokens defined in a portion of the source code file outside of the function, wherein the self-attention network is adapted based at least in part on the global tokens.
- the method may include adapting the self-attention network of the translation machine learning model to attend between each of the global tokens and all other tokens of the source code file.
- the method may further include: analyzing the one or more graphs to identify, as inter-function token pairs, tokens from different functions that are connected by one or more edges of one or more of the graphs; and adapting the self-attention network to further sparsely attend across the inter-function token pairs.
- one or more of the inter-function pairs may be a function definition and a function call.
- the method may further include: identifying dependencies between one or more other functions of the source code file and the function defined in the source code snippet; and adapting the self-attention network of the translation machine learning model to further sparsely attend based on the identified dependencies.
- the method may include adapting the self-attention network to attend across other randomly-selected token pairs of the superset.
- implementations include one or more processors of one or more computing devices, where the one or more processors are operable to execute instructions stored in associated memory, and where the instructions are configured to cause performance of any of the aforementioned methods. Some implementations also include one or more non-transitory computer readable storage media storing computer instructions executable by one or more processors to perform any of the aforementioned methods.
- FIG. 1 schematically depicts an example environment in which selected aspects of the present disclosure may be implemented, in accordance with various implementations.
- FIG. 2 A and FIG. 2 B depict various types of graphs that can be generated to represent aspects of source code, in accordance with various implementations.
- FIG. 3 is a block diagram showing an example of how source code may be processed to generate a sparse attention network, in accordance with various implementations.
- FIG. 4 depicts an example application of techniques described herein, in accordance with various implementations.
- FIG. 5 depicts a flowchart illustrating an example method for practicing selected aspects of the present disclosure.
- FIG. 6 illustrates an example architecture of a computing device.
- Implementations are described herein for encoding large structured textual data for purposes such as translation between domain-specific languages, with reduced memory requirements. More particularly, but not exclusively, implementations are described herein for translating source code between programming languages using machine learning models with sparse self-attention mechanisms. Using sparse attention mechanisms avoids wasting computing resources, particularly memory, on calculations that have limited to no utility for performing downstream tasks such as translation.
- Machine learning models configured with selected aspects of the present disclosure may take various forms, with various types of attention mechanisms.
- the machine learning model may take the form of a transformer network, such as a BERT (Bidirectional Encoder Representations from Transformers) transformer and/or a GPT (Generative Pre-trained Transformer).
- the transformer model may be trained using one or more corpuses of documents and other data that is relevant to structured text in general, or programming language(s) in particular. These documents may include, for instance, source code examples, programming handbooks or textbooks, general programming documentation, natural language comments embedding in source code, and so forth.
- a machine learning model configured with selected aspects of the present disclosure may include a sparse self-attention mechanism.
- This sparse self-attention mechanism may be represented conceptually as a self-attention network that includes nodes representing inputs (e.g., source code tokens) and edges between the nodes that dictate which inputs are attended against each other.
- the self-attention network may be adapted to sparsely attend across subsets of token pairs that are related (e.g., dependent) to each other within source code, rather than across all possible token pairs.
- matrices that represent these sparse self-attention networks may be instantiated such that memory is only allocated for attended token pairs, instead of simply being allocated for all possible token pairs, attended or otherwise.
- source code may first be processed to generate one or more graphs that represent tokens in the source code, as well as relationships (e.g., dependencies) between those tokens.
- These one or more graphs may take various forms, such as a data flow graph (DFG), a control flow graph (CFG), or an abstract syntax tree (AST). Edges between nodes of these graphs may represent dependencies between the tokens underlying the nodes. Accordingly, the graphs may be analyzed to identify, from a superset of all possible token pairs in the source code, a subset of related token pairs.
- DDG data flow graph
- CFG control flow graph
- AST abstract syntax tree
- Each related token pair of this subset may include tokens that are represented by nodes that are connected by one or more edges of the one or more of the graphs, and therefore are dependent on each other. For example, any tokens represented in the graph(s) by nodes that are connected by no more than some integer n (e.g., one, two, etc.) of edges may be identified as a token pair of the subset. Token pairs represented in the graph(s) by nodes that are connected by a greater number of edges-which suggests a weaker relationship, lack of dependency, etc.-may not be selected for the subset.
- n e.g., one, two, etc.
- a self-attention network of a translation machine learning model may be adapted to include edges that correspond to the token pairs of the subset.
- source code may be processed based on the adapted translation machine learning model to generate a translation of the original source code in a second programming language.
- an encoding of the original source code may be generated by an encoder portion of the translation machine learning model based at least in part on the sparse self-attention network. This encoding may then be processed using a decoding portion of the translation machine learning model to generate the translation in the second programming language. If translation is not the ultimate goal, then the decoder portion may be trained for performance of another task, such as code summarization, quality metric prediction, etc.
- self-attention networks may be adapted to attend across source code tokens not only within functions (“intra-function token pairs”), but across tokens contained in different functions (“inter-function token pairs”).
- global tokens and/or class definitions may be identified in portion(s) of a source code file outside of functions.
- the self-attention network of the translation machine learning model may be adapted to attend between the global tokens and all other tokens contained in the input source code, akin to an all-pair attention mechanism, while still attending sparsely across tokens within function(s).
- one or more graphs e.g., CFGs, ASTs, DFGs
- the self-attention network may be adapted based at least in part on these inter-function token pairs.
- FIG. 1 schematically depicts an example environment in which selected aspects of the present disclosure may be implemented, in accordance with various implementations.
- Any computing devices depicted in FIG. 1 or elsewhere in the figures may include logic such as one or more microprocessors (e.g., central processing units or “CPUs”, graphical processing units or “GPUs”, tensor processing units or “TPUs”) that execute computer-readable instructions stored in memory, or other types of logic such as application-specific integrated circuits (“ASIC”), field-programmable gate arrays (“FPGA”), and so forth.
- ASIC application-specific integrated circuits
- FPGA field-programmable gate arrays
- Some of the systems depicted in FIG. 1 such as a code knowledge system 102 , may be implemented using one or more server computing devices that form what is sometimes referred to as a “cloud infrastructure,” although this is not required.
- a code knowledge system 102 may be provided for helping clients 110 - 1 to 110 -P manage their respective code bases 112 - 1 to 112 -P.
- Code knowledge system 102 may include, among other things, a code translator 104 that is configured to perform selected aspects of the present disclosure in order to help one or more clients 110 - 1 to 110 -P to manage and/or make changes to one or more corresponding code bases 112 - 1 to 112 -P.
- Each client 110 may be, for example, an entity or organization such as a business (e.g., financial institute, bank, etc.), non-profit, club, university, government agency, or any other organization that operates one or more software systems.
- a bank may operate one or more software systems to manage the money under its control, including tracking deposits and withdrawals, tracking loans, tracking investments, and so forth.
- An airline may operate one or more software systems for booking/canceling/rebooking flight reservations, managing delays or cancellations of flight, managing people associated with flights, such as passengers, air crews, and ground crews, managing airport gates, and so forth.
- Code translator 104 may be configured to leverage knowledge of multiple different programming languages in order to aid clients 110 - 1 to 110 -P in translating between programming languages when editing, updating, re-platforming, migrating, or otherwise acting upon their code bases 112 - 1 to 112 -P.
- code translator 104 may be configured to translate code snippets from one programming language to another, e.g., on the fly or in batches. This may, for instance, enable a developer fluent in a first programming language to view and/or edit source code that was originally written in a second, less-familiar programming language in the first programming language. It may also significantly decrease the time and/or costs associated with migrating code bases between different programming languages.
- code knowledge system 102 may include a machine learning (“ML” in FIG. 1 ) database 105 that includes data indicative of one or more trained machine learning models 106 - 1 to 106 -N.
- These trained machine learning models 106 - 1 to 106 -N may take various forms that will be described in more detail below, including but not limited to BERT (Bidirectional Encoder Representations from Transformers) transformers, GPT (Generative Pre-trained Transformer) transformers, a graph-based network such as a graph neural network (“GNN”), graph attention neural network (“GANN”), or graph convolutional neural network (“GCN”), other types of sequence-to-sequence models and/or encoder-decoders, various flavors of a recurrent neural network (“RNN”, e.g., long short-term memory, or “LSTM”, gate recurrent units, or “GRU”, etc.), and any other type of machine learning model that may be applied to facilitate selected aspects of the present disclosure.
- RNN recurrent neural network
- code knowledge system 102 may also have access to one or more programming-language-specific corpuses 108 - 1 to 108 -M.
- these programming-language-specific corpuses 108 - 1 to 108 -M may be used, for instance, to train one or more of the machine learning models 106 - 1 to 106 -N.
- the programming-language-specific corpuses 108 - 1 to 108 -M may include examples of source code (e.g., entire code bases, libraries, etc.), inline comments, textual metadata associated with source code (e.g., commits), documentation such as textbooks and programming manuals, programming language-specific discussion threads, presentations, academic papers, and so forth.
- a client 110 that wishes to enable manipulation of its code base 112 in programming language(s) other than that/those used originally to write the source code may establish a relationship with an entity (not depicted in FIG. 1 ) that hosts code knowledge system 102 .
- code translator 104 may provide one or more versions of the source code snippet that is translated to a target programming language preferred by the developer. In some such implementations, code translator 104 may generate the translated source code snippet on the fly, e.g., in real time.
- code translator 104 may operate, e.g., in a batch mode, to preemptively translate all or selection portions of an entity’s code base 112 into a targeted programming language.
- the edited version may be translated back into the native programming language or left in the new, target programming language, assuming other necessary infrastructure is in place.
- trained translation models may be deployed closer to or at the edge, e.g., at client devices 110 - 1 to 110 -P. Because these trained translation models utilize sparse self-attention mechanisms, they may be effectively applied at relatively resource-constrained resources of the edge, rather than in the cloud. Edge-based deployment may give rise to a variety of benefits, such as maintenance of privacy, protection of sensitive source code, and so forth.
- FIG. 2 A and FIG. 2 B depict examples of how graphs may be generated from source code for purposes of sparsely attending across tokens that are logically related to each other.
- FIG. 2 A depicts an example graph 218 that represents a source code hierarchy.
- Nodes of graph 218 may represent types of programmatic structures.
- node 220 may represent the AST of an entire program represented by the source code, with some nodes (e.g., representing function declarations) that have their own subtrees being collapsed.
- Nodes 222 - 1 and 222 - 2 may represent functions declarations included in the source code.
- Node 224 may represent a block statement that corresponds to a function definition of each function declaration.
- nodes 226 - 1 to 226 - 4 may represent individual expressions and/or statements.
- the edges between these nodes of graph 218 may represent hierarchal relationships between the various source code components represented by the nodes.
- FIG. 2 B depicts an example function definition called “add.”
- the “add” function calculates the sum of its three arguments, variables a, b, and c, outputs the sum (e.g., at a console), and returns the sum.
- the arrows depicted in FIG. 2 B demonstrate logical relationships between various tokens of the function, and may correspond to edges of a graph (e.g., an AST, CFG, DFG) between nodes representing those tokens. For example, an arrow is shown between the argument a and its corresponding variable a within the function. The same goes for arguments b and c. Another arrow is shown between the variable s defined within the function and its use in a subsequent statement.
- any of the edges/arrows depicted in FIGS. 2 A and 2 B may be used to adapt a self-attention network of a translation machine learning model to sparsely attend across only those tokens that are related to each other logically and/or functionally.
- a subset of token pairs may be identified from a superset of all possible token pairs of the tokens in the source code in question.
- Each token pair of the subset may include snippet token that are represented by nodes connected by one or more edges of the one or more graphs.
- a self-attention network of a translation machine learning model may be adapted to sparsely attend across the identified subset of token pairs, e.g., by not attending across other pairs outside of the subset.
- memory may only be allocated for the identified subset of token pairs, not for all possible token pairs, thereby preserving significant memory, which may allow the model to encode much larger input contexts.
- FIG. 3 schematically depicts an example of how a source code file containing some number of functions may be processed using techniques described herein to adapt a self-attention network of a translation machine learning model to sparsely attend across pairs of tokens in which the individual tokens are logically related to each other, e.g., directly, or indirectly with one or more operator tokens (e.g., +, -, ⁇ , ⁇ , x).
- Source code file 330 may include any number of global tokens, such as global variables, function declarations, class definitions, imported libraries, etc.
- Source code file 330 may also include any number of function definitions, e.g., corresponding to the global function declarations.
- Source code file 330 may be processed to generate one or more graphs 332 .
- One or more graphs may take various forms, such as ASTs, CFGs, DFGs, etc.
- source code file 330 may be processed to generate multiple different types of graphs, and the edges of the multiple graphs may be used to identify token pairs for sparse attention.
- One or more graphs 332 may be evaluated to identify tokens that are represented by nodes connected by edges of the graph(s). For example, within a source code snippet that defines a function, a subset of snippet token pairs may be identified from a superset of all possible pairs of the snippet tokens in the source code snippet defining the function. Each token pair of the subset may include snippet tokens that are represented by nodes connected by one or more edges of the one or more graphs that correspond to the function definition.
- a self-attention matrix 334 is depicted in FIG. 3 that demonstrates how various tokens may be attended against each other, given logical dependencies contained in the source code file 330 .
- Each row represents a token and likewise each column represents a token. The cells therefore represent pair of tokens that can potentially be attended across.
- a legend is also provided at bottom left indicating which fill patterns represent which type of attention (or no attention).
- source code file 330 defines two global tokens outside of any function definitions. These global tokens are represented by the first two rows and the first two columns (as indicated at 336 ). Two tokens are shown in FIG. 3 for illustrative purposes—it is likely that in a real world example, there would be more than two global tokens. In some implementations, including that depicted in FIG. 3 , the global tokens may be attended across all other tokens, as indicated by the “global attention” shading of the first two rows and first two columns indicated at 336 . White cells in matrix 334 represent token pairs that are not attended (“no attention”). In FIG. 3 , random tokens are attended across each other as well (“random attention” indicated with 342 at various locations), although this is not required.
- Tokens within functions (“intra-function tokens”) may be selectively attended across each other based on their dependencies on each other, as indicated by the “intra-function attention” shaded cells of self-attention matrix 334 . More particularly, and as described previously, edges of a graph generated for a given function—whether a standalone graph or as a portion of a larger graph (e.g., representing the entire source code file 330 )—may be used to determine which intra-function token pairs to attend across (e.g., nodes directly connected by edges), and which intra-function token pairs not to attend across (e.g., nodes not directly connected in graph, or connected by greater than some threshold number of edges).
- source code file 330 defines three different functions.
- the tokens within these functions are shown paired with each other in self-attention matrix 334 in annotated (thick black lines) sub-grids of cells: 338 - 1 (a 3 ⁇ 3 grid), 338 - 2 (a 4 ⁇ 4 grid), and at 338 - 3 (another 3 ⁇ 3 grid).
- each annotated sub-grid 338 represents a self-attention matrix for a given function of source code file 330 .
- shaded cells (“intra-function attention” from the legend) represent token pairs that are directly related (e.g., dependent upon each other, represented in graph(s) 332 with nodes that are directly connected by edges), and therefore are attended across.
- Non-shaded cells within each sub-grid 338 represent token pairs that are not attended against each other, e.g., because nodes representing those tokens in graph(s) 332 are not directly (or closely enough) connected by edges.
- one function may include a function call to one or more other functions, in which case the two or more functions (and at least some of their respective tokens) are dependent on each other, and therefore may be connected by edge(s) of graph(s) 332 .
- a function f-2() includes, at the kth (positive integer) token, a function call to another function f- 1().
- the token pair ⁇ ind(f-2)+k, ind(f-1)> can be selectively attended to impose the inter-function attention.
- Self-attention matrix 334 includes, as black cells, several examples of “inter-function attention” being applied to inter-function token pairs. For example, on the bottom row of self-attention matrix 334 , the third cell from the left is shaded black to annotate an inter-function token pair between the first function represented by sub-grid 338 - 1 and the third function represented by sub-grid 338 - 3 . As another example, in the fifth row from bottom, the second cell from the right is shaded black to annotate an inter-function token pair between the second function represented by sub-grid 338 - 2 and the third function represented by sub-grid 338 - 3 .
- self-attention matrix 334 may correspond to (e.g., represent) the self-attention network used by a translation machine learning model to translate source code between programming languages.
- self-attention matrix 334 may correspond to (e.g., represent) the self-attention network used by a translation machine learning model to translate source code between programming languages.
- the token pairs corresponding to the non-shaded, white cells are not attended.
- no memory is allocated for these unattended token pairs.
- FIG. 4 depicts an example scenario in which a code snippet written in one programming language may be translated to another programming language.
- the base source code snippet 460 is written in Java and prints the integers one to five.
- GUI graphical user interface
- the code snippet 460 written in Java is converted by code translator 104 into Python and rendered as part of GUI 462 .
- the developer operating GUI 462 may view the source code in a programming language with which he or she is more familiar.
- the developer may be able to edit the translated source code.
- the edits made by the developer may be translated back to Java before being stored and/or more permanently incorporated into the code base.
- the edited Python code may be incorporated into the code base.
- the original source code 460 may be sent to code knowledge system 102 for translation by code translator 104 prior to being sent to the computing device (not depicted) that renders GUI 462 .
- GUI 462 may be part of a software development application that performs the programming language translation locally, e.g., using a plug-in or built-in functionality.
- FIG. 4 is for illustrative purposes only.
- Source code may be translated between programming languages using techniques described herein for any number of applications.
- the source code in the based programming language may be translated into a target programming language en route to the second user, e.g., by code translator 104 .
- the second user’s email application or an email server that stores emails of the second user
- a single user may operate a software development application to view multiple different source code snippets written in multiple different programming languages that are unfamiliar to the user.
- multiple respective translation models may be used to translate the source code snippets from the multiple different programming languages to a language (or languages) that are better understood to the user.
- techniques described herein may be used to automatically convert source code written in one programming language into source code in another programming language, without necessarily presenting translated source code to users as described previously.
- a company may decide to re-platform an existing code base 112 to a new programming language, e.g., to obtain new functionality and/or technical benefits (e.g., security features, processing speed features, etc.) that were unavailable with the original programming language.
- Such a company may be able to deploy techniques described herein, or request that an entity associated with code knowledge system 102 deploy techniques described herein, to automatically convert all or a portion of a code base 112 from one programming language to another.
- These multiple different candidate translations may be determined, for instance, using techniques such as beam searching performed as part of a decoding process associated with a transformer network. Accordingly, in FIG. 4 , a selectable link is presented (“CLICK HERE TO VIEW NEXT CANDIDATE TRANSLATION”) that a user can select to see an alternative translation of the original source code snippet.
- these candidate translations may be presented to the user in a ranked order.
- This ranked order may be determined in various ways, such as by how many (or few) errors or warnings are raised when attempts are made to parse and/or compile the candidate translations (e.g., in the background without the user being aware). For example, various types of analysis associated with compiling, such as lexical analysis, syntax analysis, semantic analysis, and so forth, may be applied to each candidate translation to determine its score (which may be inversely proportional to the number of errors or warnings generated). The candidates with the “best” scores may be presented to the programmer first. In some implementations, candidate translations may be presented (or at least made available for presentation) until various criteria are met, such as a candidate no longer being capable of being compiled.
- FIG. 5 is a flowchart illustrating an example method 500 of translating source code from a first programming language to a second programming language, in accordance with various implementations.
- This system may include various components of various computer systems, such as one or more components of code knowledge system 102 .
- operations of method 500 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted or added.
- the system may process source code (e.g., source code file 330 ) written in a first programming language (e.g., Python). Based on the processing at block 502 , at block 504, the system, e.g., by way of code translator 104 , may generate one or more graphs (e.g., 332 ) representing the source code. As noted previously, these graph(s) may include, for instance, an AST, a CFG, a DFG, or other types of graphs that can be generated to represent tokens and logical relationships defined in source code.
- the system may identify global tokens that are defined outside of function definitions. These global tokens may include, for instance, global variables, class definitions, object definitions, function declarations, and so forth.
- the system e.g., by way of code translator 104 , may identify, e.g., from a superset of all intra-function token pairs within each function defined by the source code, a subset of intra-function token pairs that should be attended, e.g., based on edge(s) of the graph(s) generated at block 504.
- any intra-function token pairs with nodes directly connected by an edge in the graph(s) may be considered related, and therefore may be identified as part of a subset of intra-function token pairs that should be attended.
- These intra-function token pairs were annotated by the shaded cells in self-attention matrix in FIG. 3 .
- the system may identify, as worthy of having attention applied between them, inter-function token pairs that correspond to dependencies across functions. For example, one function may call another function, thereby introducing inter-dependencies between the functions. These inter-function token pairs were annotated by the black cells in self-attention matrix in FIG. 3 .
- the system may adapt a self-attention network of a translation machine learning model to sparsely attend across token pairs identified in blocks 508 - 510 , including intra-function attention and inter-function attention.
- the translation machine learning model may take the form of a BERT transformer, and the self-attention network may be applied during encoding, although this is not required.
- the system may adapt the self-attention network to attend between all global tokens and all other tokens. An example of this was depicted in FIG. 3 at 336 .
- the system e.g., by way of code translator 104
- the system e.g., by way of code translator 104
- a second programming language e.g., Java, C++
- FIG. 6 is a block diagram of an example computing device 610 that may optionally be utilized to perform one or more aspects of techniques described herein.
- Computing device 610 typically includes at least one processor 614 which communicates with a number of peripheral devices via bus subsystem 612 .
- peripheral devices may include a storage subsystem 624 , including, for example, a memory subsystem 625 and a file storage subsystem 626 , user interface output devices 620 , user interface input devices 622 , and a network interface subsystem 616 .
- the input and output devices allow user interaction with computing device 610 .
- Network interface subsystem 616 provides an interface to outside networks and is coupled to corresponding interface devices in other computing devices.
- User interface input devices 622 may include a keyboard, pointing devices such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a touch screen incorporated into the display, audio input devices such as voice recognition systems, microphones, and/or other types of input devices.
- pointing devices such as a mouse, trackball, touchpad, or graphics tablet
- audio input devices such as voice recognition systems, microphones, and/or other types of input devices.
- use of the term “input device” is intended to include all possible types of devices and ways to input information into computing device 610 or onto a communication network.
- User interface output devices 620 may include a display subsystem, a printer, a fax machine, or non-visual displays such as audio output devices.
- the display subsystem may include a cathode ray tube (CRT), a flat-panel device such as a liquid crystal display (LCD), a projection device, or some other mechanism for creating a visible image.
- the display subsystem may also provide non-visual display such as via audio output devices.
- output device is intended to include all possible types of devices and ways to output information from computing device 610 to the user or to another machine or computing device.
- Storage subsystem 624 stores programming and data constructs that provide the functionality of some or all of the modules described herein.
- the storage subsystem 624 may include the logic to perform selected aspects of the method of FIG. 5 , as well as to implement various components depicted in FIG. 1 .
- Memory 625 used in the storage subsystem 624 can include a number of memories including a main random access memory (RAM) 630 for storage of instructions and data during program execution and a read only memory (ROM) 632 in which fixed instructions are stored.
- a file storage subsystem 626 can provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges.
- the modules implementing the functionality of certain implementations may be stored by file storage subsystem 626 in the storage subsystem 624 , or in other machines accessible by the processor(s) 614 .
- Bus subsystem 612 provides a mechanism for letting the various components and subsystems of computing device 610 communicate with each other as intended. Although bus subsystem 612 is shown schematically as a single bus, alternative implementations of the bus subsystem may use multiple buses.
- Computing device 610 can be of varying types including a workstation, server, computing cluster, blade server, server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computing device 610 depicted in FIG. 6 is intended only as a specific example for purposes of illustrating some implementations. Many other configurations of computing device 610 are possible having more or fewer components than the computing device depicted in FIG. 6 .
Abstract
Description
- Computer software programming often requires developers to read and/or write source code (i.e., to program) in a specific programming language, e.g. Java, C++, C, Python, etc. Each programming language has its own strengths, weaknesses, nuances, idiosyncrasies, etc. Additionally, some programming languages are more suitable for certain stages of software development and/or a software life cycle than others. As one example, scripting languages such as Python, JavaScript, Perl, etc., are often more effectively used near the very beginning of software development because programmers using these languages are able to turn around functional software relatively quickly. Most programmers obtain at least a superficial understanding of multiple programming languages, but only master a few. Consequently, each programming language tends to have its own talent pool.
- Large language models (also referred to as “neural translators”) such as transformer networks have become increasingly popular for performing natural language processing. Transformer networks were designed in part to mitigate a variety of shortcomings of prior natural language processing models, such as overfitting, the vanishing gradient problem, and exceedingly high computational costs, to name a few. However, transformer networks still require large amounts of memory when processing large sequences of data, such as large pieces of source code.
- One reason attention-based networks such as transformer networks impose significant computational costs (especially memory) when used to process large sequences of data is that they include attention mechanisms that attend across all possible pairs of input tokens. Such an attention mechanism may be referred to as an “all pair attention network” because every input (e.g., source code token) is conceptually connected to, and hence, attended against, every other input token, regardless of whether those tokens are dependent on each other logically. For relatively short sequences of inputs, such as source code snippets having relatively few tokens (e.g., 500 or less), the computational costs associated with an all-pair attention network may be acceptable, if not ideal. However, as the sequence of inputs grows in length, the memory requirements grow as well, quickly becoming unwieldy.
- Implementations are described herein for encoding large structured textual data for purposes such as translation between domain-specific languages, with reduced memory requirements. More particularly, but not exclusively, implementations are described herein for translating source code between programming languages using machine learning models with sparse self-attention mechanisms. Using sparse attention mechanisms avoids wasting computing resources, particularly memory, on calculations that have limited to no utility for performing downstream tasks such as translation.
- In some implementations, a method for translating a source code snippet from a first programming language to a second programming language may be implemented by one or more processors and may include: obtaining one or more graphs representing snippet tokens, and relationships between the snippet tokens, contained in the source code snippet written in the first programming language; based on the one or more graphs, identifying, from a superset of all possible pairs of the snippet tokens in the source code snippet, a subset of snippet token pairs, wherein each token pair of the subset includes snippet tokens that are represented by nodes connected by one or more edges of the one or more graphs; adapting a self-attention network of a translation machine learning model to sparsely attend across the identified subset of token pairs; and processing the source code snippet based on the adapted translation machine learning model to generate a translation of the source code snippet in the second programming language.
- In various implementations, the one or more graphs may include at least one of a data flow graph (DFG), a control flow graph (CFG), or an abstract syntax tree (AST) representing the source code snippet. In various implementations, the edges of one or more of the graphs may represent dependencies between the snippet tokens.
- In various implementations, the source code snippet may be a function, and the method may further include processing an entire source code file that contains the function to identify global tokens defined in a portion of the source code file outside of the function, wherein the self-attention network is adapted based at least in part on the global tokens. In various implementations, the method may include adapting the self-attention network of the translation machine learning model to attend between each of the global tokens and all other tokens of the source code file.
- In various implementations, the method may further include: analyzing the one or more graphs to identify, as inter-function token pairs, tokens from different functions that are connected by one or more edges of one or more of the graphs; and adapting the self-attention network to further sparsely attend across the inter-function token pairs. In various implementations, one or more of the inter-function pairs may be a function definition and a function call. In various implementations, the method may further include: identifying dependencies between one or more other functions of the source code file and the function defined in the source code snippet; and adapting the self-attention network of the translation machine learning model to further sparsely attend based on the identified dependencies.
- In various implementations, the method may include adapting the self-attention network to attend across other randomly-selected token pairs of the superset.
- In addition, some implementations include one or more processors of one or more computing devices, where the one or more processors are operable to execute instructions stored in associated memory, and where the instructions are configured to cause performance of any of the aforementioned methods. Some implementations also include one or more non-transitory computer readable storage media storing computer instructions executable by one or more processors to perform any of the aforementioned methods.
- It should be appreciated that all combinations of the foregoing concepts and additional concepts described in greater detail herein are contemplated as being part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are contemplated as being part of the subject matter disclosed herein.
-
FIG. 1 schematically depicts an example environment in which selected aspects of the present disclosure may be implemented, in accordance with various implementations. -
FIG. 2A andFIG. 2B depict various types of graphs that can be generated to represent aspects of source code, in accordance with various implementations. -
FIG. 3 is a block diagram showing an example of how source code may be processed to generate a sparse attention network, in accordance with various implementations. -
FIG. 4 depicts an example application of techniques described herein, in accordance with various implementations. -
FIG. 5 depicts a flowchart illustrating an example method for practicing selected aspects of the present disclosure. -
FIG. 6 illustrates an example architecture of a computing device. - Implementations are described herein for encoding large structured textual data for purposes such as translation between domain-specific languages, with reduced memory requirements. More particularly, but not exclusively, implementations are described herein for translating source code between programming languages using machine learning models with sparse self-attention mechanisms. Using sparse attention mechanisms avoids wasting computing resources, particularly memory, on calculations that have limited to no utility for performing downstream tasks such as translation.
- Machine learning models configured with selected aspects of the present disclosure may take various forms, with various types of attention mechanisms. In some implementations, the machine learning model may take the form of a transformer network, such as a BERT (Bidirectional Encoder Representations from Transformers) transformer and/or a GPT (Generative Pre-trained Transformer). The transformer model may be trained using one or more corpuses of documents and other data that is relevant to structured text in general, or programming language(s) in particular. These documents may include, for instance, source code examples, programming handbooks or textbooks, general programming documentation, natural language comments embedding in source code, and so forth.
- As mentioned previously, to conserve computing resources such as memory, a machine learning model configured with selected aspects of the present disclosure may include a sparse self-attention mechanism. This sparse self-attention mechanism may be represented conceptually as a self-attention network that includes nodes representing inputs (e.g., source code tokens) and edges between the nodes that dictate which inputs are attended against each other. The self-attention network may be adapted to sparsely attend across subsets of token pairs that are related (e.g., dependent) to each other within source code, rather than across all possible token pairs. In various implementations, matrices that represent these sparse self-attention networks may be instantiated such that memory is only allocated for attended token pairs, instead of simply being allocated for all possible token pairs, attended or otherwise.
- To identify source code token pairs to attend across, source code may first be processed to generate one or more graphs that represent tokens in the source code, as well as relationships (e.g., dependencies) between those tokens. These one or more graphs may take various forms, such as a data flow graph (DFG), a control flow graph (CFG), or an abstract syntax tree (AST). Edges between nodes of these graphs may represent dependencies between the tokens underlying the nodes. Accordingly, the graphs may be analyzed to identify, from a superset of all possible token pairs in the source code, a subset of related token pairs. Each related token pair of this subset may include tokens that are represented by nodes that are connected by one or more edges of the one or more of the graphs, and therefore are dependent on each other. For example, any tokens represented in the graph(s) by nodes that are connected by no more than some integer n (e.g., one, two, etc.) of edges may be identified as a token pair of the subset. Token pairs represented in the graph(s) by nodes that are connected by a greater number of edges-which suggests a weaker relationship, lack of dependency, etc.-may not be selected for the subset.
- Once the subset of related token pairs is identified, a self-attention network of a translation machine learning model (e.g., a BERT transformer) may be adapted to include edges that correspond to the token pairs of the subset. Then, source code may be processed based on the adapted translation machine learning model to generate a translation of the original source code in a second programming language. For example, an encoding of the original source code may be generated by an encoder portion of the translation machine learning model based at least in part on the sparse self-attention network. This encoding may then be processed using a decoding portion of the translation machine learning model to generate the translation in the second programming language. If translation is not the ultimate goal, then the decoder portion may be trained for performance of another task, such as code summarization, quality metric prediction, etc.
- Techniques described herein allow larger pieces of source code to be translated than would be feasible with conventional machine learning models with all-pair self-attention mechanisms. In fact, techniques described herein facilitate translation of entire source code files, which may contain any number of functions, classes, global variables, etc. To this end, in some implementations, self-attention networks may be adapted to attend across source code tokens not only within functions (“intra-function token pairs”), but across tokens contained in different functions (“inter-function token pairs”).
- For example, global tokens and/or class definitions may be identified in portion(s) of a source code file outside of functions. In some implementations, the self-attention network of the translation machine learning model may be adapted to attend between the global tokens and all other tokens contained in the input source code, akin to an all-pair attention mechanism, while still attending sparsely across tokens within function(s). In some implementations, one or more graphs (e.g., CFGs, ASTs, DFGs) representing relationships between tokens contained within multiple different function(s) may be evaluated to identify inter-function token pairs that are functionally and/or logically related. The self-attention network may be adapted based at least in part on these inter-function token pairs.
-
FIG. 1 schematically depicts an example environment in which selected aspects of the present disclosure may be implemented, in accordance with various implementations. Any computing devices depicted inFIG. 1 or elsewhere in the figures may include logic such as one or more microprocessors (e.g., central processing units or “CPUs”, graphical processing units or “GPUs”, tensor processing units or “TPUs”) that execute computer-readable instructions stored in memory, or other types of logic such as application-specific integrated circuits (“ASIC”), field-programmable gate arrays (“FPGA”), and so forth. Some of the systems depicted inFIG. 1 , such as acode knowledge system 102, may be implemented using one or more server computing devices that form what is sometimes referred to as a “cloud infrastructure,” although this is not required. - A
code knowledge system 102 may be provided for helping clients 110-1 to 110-P manage their respective code bases 112-1 to 112-P.Code knowledge system 102 may include, among other things, acode translator 104 that is configured to perform selected aspects of the present disclosure in order to help one or more clients 110-1 to 110-P to manage and/or make changes to one or more corresponding code bases 112-1 to 112-P. Eachclient 110 may be, for example, an entity or organization such as a business (e.g., financial institute, bank, etc.), non-profit, club, university, government agency, or any other organization that operates one or more software systems. For example, a bank may operate one or more software systems to manage the money under its control, including tracking deposits and withdrawals, tracking loans, tracking investments, and so forth. An airline may operate one or more software systems for booking/canceling/rebooking flight reservations, managing delays or cancellations of flight, managing people associated with flights, such as passengers, air crews, and ground crews, managing airport gates, and so forth. -
Code translator 104 may be configured to leverage knowledge of multiple different programming languages in order to aid clients 110-1 to 110-P in translating between programming languages when editing, updating, re-platforming, migrating, or otherwise acting upon their code bases 112-1 to 112-P. For example,code translator 104 may be configured to translate code snippets from one programming language to another, e.g., on the fly or in batches. This may, for instance, enable a developer fluent in a first programming language to view and/or edit source code that was originally written in a second, less-familiar programming language in the first programming language. It may also significantly decrease the time and/or costs associated with migrating code bases between different programming languages. - In various implementations,
code knowledge system 102 may include a machine learning (“ML” inFIG. 1 )database 105 that includes data indicative of one or more trained machine learning models 106-1 to 106-N. These trained machine learning models 106-1 to 106-N may take various forms that will be described in more detail below, including but not limited to BERT (Bidirectional Encoder Representations from Transformers) transformers, GPT (Generative Pre-trained Transformer) transformers, a graph-based network such as a graph neural network (“GNN”), graph attention neural network (“GANN”), or graph convolutional neural network (“GCN”), other types of sequence-to-sequence models and/or encoder-decoders, various flavors of a recurrent neural network (“RNN”, e.g., long short-term memory, or “LSTM”, gate recurrent units, or “GRU”, etc.), and any other type of machine learning model that may be applied to facilitate selected aspects of the present disclosure. - In some implementations,
code knowledge system 102 may also have access to one or more programming-language-specific corpuses 108-1 to 108-M. In some implementations, these programming-language-specific corpuses 108-1 to 108-M may be used, for instance, to train one or more of the machine learning models 106-1 to 106-N. In some implementations, the programming-language-specific corpuses 108-1 to 108-M may include examples of source code (e.g., entire code bases, libraries, etc.), inline comments, textual metadata associated with source code (e.g., commits), documentation such as textbooks and programming manuals, programming language-specific discussion threads, presentations, academic papers, and so forth. - In some implementations, a
client 110 that wishes to enable manipulation of itscode base 112 in programming language(s) other than that/those used originally to write the source code may establish a relationship with an entity (not depicted inFIG. 1 ) that hostscode knowledge system 102. When a developer wishes to view/edit a source code snippet of the entity’scode base 112 but is unfamiliar with the native programming language,code translator 104 may provide one or more versions of the source code snippet that is translated to a target programming language preferred by the developer. In some such implementations,code translator 104 may generate the translated source code snippet on the fly, e.g., in real time. In other implementations,code translator 104 may operate, e.g., in a batch mode, to preemptively translate all or selection portions of an entity’scode base 112 into a targeted programming language. In some implementations in which the developer then edits the translated source code snippet, the edited version may be translated back into the native programming language or left in the new, target programming language, assuming other necessary infrastructure is in place. - In other implementations, trained translation models may be deployed closer to or at the edge, e.g., at client devices 110-1 to 110-P. Because these trained translation models utilize sparse self-attention mechanisms, they may be effectively applied at relatively resource-constrained resources of the edge, rather than in the cloud. Edge-based deployment may give rise to a variety of benefits, such as maintenance of privacy, protection of sensitive source code, and so forth.
-
FIG. 2A andFIG. 2B depict examples of how graphs may be generated from source code for purposes of sparsely attending across tokens that are logically related to each other.FIG. 2A depicts anexample graph 218 that represents a source code hierarchy. Nodes ofgraph 218 may represent types of programmatic structures. For example,node 220 may represent the AST of an entire program represented by the source code, with some nodes (e.g., representing function declarations) that have their own subtrees being collapsed. Nodes 222-1 and 222-2 may represent functions declarations included in the source code.Node 224 may represent a block statement that corresponds to a function definition of each function declaration. And nodes 226-1 to 226-4 may represent individual expressions and/or statements. The edges between these nodes ofgraph 218 may represent hierarchal relationships between the various source code components represented by the nodes. -
FIG. 2B depicts an example function definition called “add.” The “add” function calculates the sum of its three arguments, variables a, b, and c, outputs the sum (e.g., at a console), and returns the sum. The arrows depicted inFIG. 2B demonstrate logical relationships between various tokens of the function, and may correspond to edges of a graph (e.g., an AST, CFG, DFG) between nodes representing those tokens. For example, an arrow is shown between the argument a and its corresponding variable a within the function. The same goes for arguments b and c. Another arrow is shown between the variable s defined within the function and its use in a subsequent statement. Additional arrows are depicted between the variable s and the operator “+”, as well as between the operator “+” and the variable c, to demonstrate that these tokens are related to each other. By contrast, no arrows are included, for instance, between the variables a and result, because those variables are not directly related to each other logically (although they may be indirectly related via one or more other variables, such as s). - In various implementations, any of the edges/arrows depicted in
FIGS. 2A and 2B may be used to adapt a self-attention network of a translation machine learning model to sparsely attend across only those tokens that are related to each other logically and/or functionally. Put another way, based on these types of graphs, a subset of token pairs may be identified from a superset of all possible token pairs of the tokens in the source code in question. Each token pair of the subset may include snippet token that are represented by nodes connected by one or more edges of the one or more graphs. Then, a self-attention network of a translation machine learning model may be adapted to sparsely attend across the identified subset of token pairs, e.g., by not attending across other pairs outside of the subset. In some implementations, memory may only be allocated for the identified subset of token pairs, not for all possible token pairs, thereby preserving significant memory, which may allow the model to encode much larger input contexts. -
FIG. 3 schematically depicts an example of how a source code file containing some number of functions may be processed using techniques described herein to adapt a self-attention network of a translation machine learning model to sparsely attend across pairs of tokens in which the individual tokens are logically related to each other, e.g., directly, or indirectly with one or more operator tokens (e.g., +, -, <, ≥, x).Source code file 330 may include any number of global tokens, such as global variables, function declarations, class definitions, imported libraries, etc.Source code file 330 may also include any number of function definitions, e.g., corresponding to the global function declarations. -
Source code file 330 may be processed to generate one ormore graphs 332. One or more graphs may take various forms, such as ASTs, CFGs, DFGs, etc. In some implementations,source code file 330 may be processed to generate multiple different types of graphs, and the edges of the multiple graphs may be used to identify token pairs for sparse attention. - One or
more graphs 332 may be evaluated to identify tokens that are represented by nodes connected by edges of the graph(s). For example, within a source code snippet that defines a function, a subset of snippet token pairs may be identified from a superset of all possible pairs of the snippet tokens in the source code snippet defining the function. Each token pair of the subset may include snippet tokens that are represented by nodes connected by one or more edges of the one or more graphs that correspond to the function definition. - A self-
attention matrix 334 is depicted inFIG. 3 that demonstrates how various tokens may be attended against each other, given logical dependencies contained in thesource code file 330. Each row represents a token and likewise each column represents a token. The cells therefore represent pair of tokens that can potentially be attended across. A legend is also provided at bottom left indicating which fill patterns represent which type of attention (or no attention). - In this example, it can be assumed that
source code file 330 defines two global tokens outside of any function definitions. These global tokens are represented by the first two rows and the first two columns (as indicated at 336). Two tokens are shown inFIG. 3 for illustrative purposes—it is likely that in a real world example, there would be more than two global tokens. In some implementations, including that depicted inFIG. 3 , the global tokens may be attended across all other tokens, as indicated by the “global attention” shading of the first two rows and first two columns indicated at 336. White cells inmatrix 334 represent token pairs that are not attended (“no attention”). InFIG. 3 , random tokens are attended across each other as well (“random attention” indicated with 342 at various locations), although this is not required. - Tokens within functions (“intra-function tokens”) may be selectively attended across each other based on their dependencies on each other, as indicated by the “intra-function attention” shaded cells of self-
attention matrix 334. More particularly, and as described previously, edges of a graph generated for a given function—whether a standalone graph or as a portion of a larger graph (e.g., representing the entire source code file 330)—may be used to determine which intra-function token pairs to attend across (e.g., nodes directly connected by edges), and which intra-function token pairs not to attend across (e.g., nodes not directly connected in graph, or connected by greater than some threshold number of edges). - For illustrative purposes, assume that
source code file 330 defines three different functions. The tokens within these functions are shown paired with each other in self-attention matrix 334 in annotated (thick black lines) sub-grids of cells: 338-1 (a 3×3 grid), 338-2 (a 4×4 grid), and at 338-3 (another 3×3 grid). Put another way, each annotated sub-grid 338 represents a self-attention matrix for a given function ofsource code file 330. Within each sub-grid 338, shaded cells (“intra-function attention” from the legend) represent token pairs that are directly related (e.g., dependent upon each other, represented in graph(s) 332 with nodes that are directly connected by edges), and therefore are attended across. Non-shaded cells within each sub-grid 338 represent token pairs that are not attended against each other, e.g., because nodes representing those tokens in graph(s) 332 are not directly (or closely enough) connected by edges. - In addition to selectively attending within functions across intra-function token pairs, attention may be selectively applied across different functions as well, e.g., between inter-function token pairs. For example, one function may include a function call to one or more other functions, in which case the two or more functions (and at least some of their respective tokens) are dependent on each other, and therefore may be connected by edge(s) of graph(s) 332. Suppose a function f-2() includes, at the kth (positive integer) token, a function call to another function f- 1(). The token pair <ind(f-2)+k, ind(f-1)> can be selectively attended to impose the inter-function attention.
- Self-
attention matrix 334 includes, as black cells, several examples of “inter-function attention” being applied to inter-function token pairs. For example, on the bottom row of self-attention matrix 334, the third cell from the left is shaded black to annotate an inter-function token pair between the first function represented by sub-grid 338-1 and the third function represented by sub-grid 338-3. As another example, in the fifth row from bottom, the second cell from the right is shaded black to annotate an inter-function token pair between the second function represented by sub-grid 338-2 and the third function represented by sub-grid 338-3. - In various implementations, self-
attention matrix 334 may correspond to (e.g., represent) the self-attention network used by a translation machine learning model to translate source code between programming languages. As can be seen, rather than attending across all possible tokens, only those token pairs that are depend on/from each other (as evidenced by their respective nodes being connected in graph(s) 332), as well as global tokens (with all other tokens) and some random pairings, are used to perform self-attention. The token pairs corresponding to the non-shaded, white cells are not attended. In some implementations, no memory is allocated for these unattended token pairs. These memory savings make translating between large source code files feasible, e.g., for performance at the edge, or within a reasonable or commercially-viable amount of time and memory. -
FIG. 4 depicts an example scenario in which a code snippet written in one programming language may be translated to another programming language. In this example, the basesource code snippet 460 is written in Java and prints the integers one to five. At bottom, a graphical user interface (“GUI”) 462 is depicted that may be presented to a developer who is unfamiliar with Java, but who has expertise in another programming language. In this example, thecode snippet 460 written in Java is converted bycode translator 104 into Python and rendered as part ofGUI 462. In this way, thedeveloper operating GUI 462 may view the source code in a programming language with which he or she is more familiar. In some cases, the developer may be able to edit the translated source code. In some such implementations, the edits made by the developer (i.e. to the Python code inFIG. 4 ) may be translated back to Java before being stored and/or more permanently incorporated into the code base. In other implementations, the edited Python code may be incorporated into the code base. - In some implementations, the
original source code 460 may be sent to codeknowledge system 102 for translation bycode translator 104 prior to being sent to the computing device (not depicted) that rendersGUI 462. In other implementations,GUI 462 may be part of a software development application that performs the programming language translation locally, e.g., using a plug-in or built-in functionality. The scenario ofFIG. 4 is for illustrative purposes only. Source code may be translated between programming languages using techniques described herein for any number of applications. - For example, suppose a first user who is trained in a base programming language sends a source code snippet in the base programming language to a second user, e.g., as an attachment or in the body of an email. In some implementations, the source code in the based programming language may be translated into a target programming language en route to the second user, e.g., by
code translator 104. Additionally or alternatively, in some implementations, the second user’s email application (or an email server that stores emails of the second user) may have a plugin configured with selected aspects of the present disclosure. - In some implementations, a single user may operate a software development application to view multiple different source code snippets written in multiple different programming languages that are unfamiliar to the user. In some such examples, multiple respective translation models may be used to translate the source code snippets from the multiple different programming languages to a language (or languages) that are better understood to the user.
- In some implementations, techniques described herein may be used to automatically convert source code written in one programming language into source code in another programming language, without necessarily presenting translated source code to users as described previously. For example, a company may decide to re-platform an existing
code base 112 to a new programming language, e.g., to obtain new functionality and/or technical benefits (e.g., security features, processing speed features, etc.) that were unavailable with the original programming language. Such a company may be able to deploy techniques described herein, or request that an entity associated withcode knowledge system 102 deploy techniques described herein, to automatically convert all or a portion of acode base 112 from one programming language to another. - It may be desirable to present a programmer with multiple different candidate translations of a source code snippet, e.g., so that the programmer can use their judgment to determine which candidate is best. These multiple different candidate translations may be determined, for instance, using techniques such as beam searching performed as part of a decoding process associated with a transformer network. Accordingly, in
FIG. 4 , a selectable link is presented (“CLICK HERE TO VIEW NEXT CANDIDATE TRANSLATION”) that a user can select to see an alternative translation of the original source code snippet. In some implementations, these candidate translations may be presented to the user in a ranked order. This ranked order may be determined in various ways, such as by how many (or few) errors or warnings are raised when attempts are made to parse and/or compile the candidate translations (e.g., in the background without the user being aware). For example, various types of analysis associated with compiling, such as lexical analysis, syntax analysis, semantic analysis, and so forth, may be applied to each candidate translation to determine its score (which may be inversely proportional to the number of errors or warnings generated). The candidates with the “best” scores may be presented to the programmer first. In some implementations, candidate translations may be presented (or at least made available for presentation) until various criteria are met, such as a candidate no longer being capable of being compiled. -
FIG. 5 is a flowchart illustrating anexample method 500 of translating source code from a first programming language to a second programming language, in accordance with various implementations. For convenience, the operations of the flow chart are described with reference to a system that performs the operations. This system may include various components of various computer systems, such as one or more components ofcode knowledge system 102. Moreover, while operations ofmethod 500 are shown in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted or added. - At
block 502, the system, e.g., by way ofcode translator 104, may process source code (e.g., source code file 330) written in a first programming language (e.g., Python). Based on the processing atblock 502, atblock 504, the system, e.g., by way ofcode translator 104, may generate one or more graphs (e.g., 332) representing the source code. As noted previously, these graph(s) may include, for instance, an AST, a CFG, a DFG, or other types of graphs that can be generated to represent tokens and logical relationships defined in source code. - At
block 506, the system, e.g., by way ofcode translator 104, may identify global tokens that are defined outside of function definitions. These global tokens may include, for instance, global variables, class definitions, object definitions, function declarations, and so forth. Atblock 508, the system, e.g., by way ofcode translator 104, may identify, e.g., from a superset of all intra-function token pairs within each function defined by the source code, a subset of intra-function token pairs that should be attended, e.g., based on edge(s) of the graph(s) generated atblock 504. For instance, any intra-function token pairs with nodes directly connected by an edge in the graph(s) may be considered related, and therefore may be identified as part of a subset of intra-function token pairs that should be attended. These intra-function token pairs were annotated by the shaded cells in self-attention matrix inFIG. 3 . - At
block 510, the system, e.g., by way ofcode translator 104, may identify, as worthy of having attention applied between them, inter-function token pairs that correspond to dependencies across functions. For example, one function may call another function, thereby introducing inter-dependencies between the functions. These inter-function token pairs were annotated by the black cells in self-attention matrix inFIG. 3 . - At
block 512, the system, e.g., by way ofcode translator 104, may adapt a self-attention network of a translation machine learning model to sparsely attend across token pairs identified in blocks 508-510, including intra-function attention and inter-function attention. In some implementations, the translation machine learning model may take the form of a BERT transformer, and the self-attention network may be applied during encoding, although this is not required. - At
optional block 514, the system, e.g., by way ofcode translator 104, may adapt the self-attention network to attend between all global tokens and all other tokens. An example of this was depicted inFIG. 3 at 336. Atoptional block 516, the system, e.g., by way ofcode translator 104, may adapt the self-attention network to randomly attend across various tokens, regardless of whether those randomly-attended tokens are related or not. Atblock 518, the system, e.g., by way ofcode translator 104, may process the source code based on the adapted translation machine learning model to generate a translation of the source code in a second programming language (e.g., Java, C++). -
FIG. 6 is a block diagram of anexample computing device 610 that may optionally be utilized to perform one or more aspects of techniques described herein.Computing device 610 typically includes at least oneprocessor 614 which communicates with a number of peripheral devices viabus subsystem 612. These peripheral devices may include astorage subsystem 624, including, for example, amemory subsystem 625 and afile storage subsystem 626, userinterface output devices 620, userinterface input devices 622, and anetwork interface subsystem 616. The input and output devices allow user interaction withcomputing device 610.Network interface subsystem 616 provides an interface to outside networks and is coupled to corresponding interface devices in other computing devices. - User
interface input devices 622 may include a keyboard, pointing devices such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a touch screen incorporated into the display, audio input devices such as voice recognition systems, microphones, and/or other types of input devices. In general, use of the term “input device” is intended to include all possible types of devices and ways to input information intocomputing device 610 or onto a communication network. - User
interface output devices 620 may include a display subsystem, a printer, a fax machine, or non-visual displays such as audio output devices. The display subsystem may include a cathode ray tube (CRT), a flat-panel device such as a liquid crystal display (LCD), a projection device, or some other mechanism for creating a visible image. The display subsystem may also provide non-visual display such as via audio output devices. In general, use of the term “output device” is intended to include all possible types of devices and ways to output information fromcomputing device 610 to the user or to another machine or computing device. -
Storage subsystem 624 stores programming and data constructs that provide the functionality of some or all of the modules described herein. For example, thestorage subsystem 624 may include the logic to perform selected aspects of the method ofFIG. 5 , as well as to implement various components depicted inFIG. 1 . - These software modules are generally executed by
processor 614 alone or in combination with other processors.Memory 625 used in thestorage subsystem 624 can include a number of memories including a main random access memory (RAM) 630 for storage of instructions and data during program execution and a read only memory (ROM) 632 in which fixed instructions are stored. Afile storage subsystem 626 can provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges. The modules implementing the functionality of certain implementations may be stored byfile storage subsystem 626 in thestorage subsystem 624, or in other machines accessible by the processor(s) 614. -
Bus subsystem 612 provides a mechanism for letting the various components and subsystems ofcomputing device 610 communicate with each other as intended. Althoughbus subsystem 612 is shown schematically as a single bus, alternative implementations of the bus subsystem may use multiple buses. -
Computing device 610 can be of varying types including a workstation, server, computing cluster, blade server, server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description ofcomputing device 610 depicted inFIG. 6 is intended only as a specific example for purposes of illustrating some implementations. Many other configurations ofcomputing device 610 are possible having more or fewer components than the computing device depicted inFIG. 6 . - While several implementations have been described and illustrated herein, a variety of other means and/or structures for performing the function and/or obtaining the results and/or one or more of the advantages described herein may be utilized, and each of such variations and/or modifications is deemed to be within the scope of the implementations described herein. More generally, all parameters, dimensions, materials, and configurations described herein are meant to be exemplary and that the actual parameters, dimensions, materials, and/or configurations will depend upon the specific application or applications for which the teachings is/are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific implementations described herein. It is, therefore, to be understood that the foregoing implementations are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, implementations may be practiced otherwise than as specifically described and claimed. Implementations of the present disclosure are directed to each individual feature, system, article, material, kit, and/or method described herein. In addition, any combination of two or more such features, systems, articles, materials, kits, and/or methods, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, is included within the scope of the present disclosure.
Claims (20)
Priority Applications (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/731,593 US20230350657A1 (en) | 2022-04-28 | 2022-04-28 | Translating large source code using sparse self-attention |
PCT/US2023/020007 WO2023212069A1 (en) | 2022-04-28 | 2023-04-26 | Translating large source code using sparse self-attention |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/731,593 US20230350657A1 (en) | 2022-04-28 | 2022-04-28 | Translating large source code using sparse self-attention |
Publications (1)
Publication Number | Publication Date |
---|---|
US20230350657A1 true US20230350657A1 (en) | 2023-11-02 |
Family
ID=86604632
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/731,593 Pending US20230350657A1 (en) | 2022-04-28 | 2022-04-28 | Translating large source code using sparse self-attention |
Country Status (2)
Country | Link |
---|---|
US (1) | US20230350657A1 (en) |
WO (1) | WO2023212069A1 (en) |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20220138266A1 (en) * | 2020-11-03 | 2022-05-05 | International Business Machines Corporation | Learning-based automated machine learning code annotation with graph neural network |
US20220237368A1 (en) * | 2021-01-22 | 2022-07-28 | Bao Tran | Systems and methods for machine content generation |
US11720346B2 (en) * | 2020-10-02 | 2023-08-08 | International Business Machines Corporation | Semantic code retrieval using graph matching |
Family Cites Families (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11842174B2 (en) * | 2019-07-09 | 2023-12-12 | Google Llc | Translating between programming languages using machine learning |
-
2022
- 2022-04-28 US US17/731,593 patent/US20230350657A1/en active Pending
-
2023
- 2023-04-26 WO PCT/US2023/020007 patent/WO2023212069A1/en unknown
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11720346B2 (en) * | 2020-10-02 | 2023-08-08 | International Business Machines Corporation | Semantic code retrieval using graph matching |
US20220138266A1 (en) * | 2020-11-03 | 2022-05-05 | International Business Machines Corporation | Learning-based automated machine learning code annotation with graph neural network |
US20220237368A1 (en) * | 2021-01-22 | 2022-07-28 | Bao Tran | Systems and methods for machine content generation |
Also Published As
Publication number | Publication date |
---|---|
WO2023212069A1 (en) | 2023-11-02 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11169786B2 (en) | Generating and using joint representations of source code | |
US11604628B2 (en) | Generation and/or recommendation of tools for automating aspects of computer programming | |
US20210192321A1 (en) | Generation and utilization of code change intents | |
US11842174B2 (en) | Translating between programming languages using machine learning | |
CA3141560A1 (en) | Automated identification of code changes | |
US11656867B2 (en) | Conditioning autoregressive language model to improve code migration | |
US11822909B2 (en) | Adapting existing source code snippets to new contexts | |
US20190303115A1 (en) | Automated source code sample adaptation | |
US11537797B2 (en) | Hierarchical entity recognition and semantic modeling framework for information extraction | |
US11886850B2 (en) | Transformation templates to automate aspects of computer programming | |
US11960867B1 (en) | Using natural language latent representation in automated conversion of source code from base programming language to target programming language | |
US11487522B1 (en) | Training and/or using neural network model to generate target source code from lower-level representation | |
US9141344B2 (en) | Hover help support for application source code | |
US11775271B1 (en) | Annotations for developers | |
CN106484389B (en) | Action stream segment management | |
US20230350657A1 (en) | Translating large source code using sparse self-attention | |
US11893384B2 (en) | Refactoring and/or rearchitecting source code using machine learning | |
US20230325164A1 (en) | Translating between programming languages independently of sequence-to-sequence decoders | |
US20240086164A1 (en) | Generating synthetic training data for programming language translation | |
US11775267B2 (en) | Identification and application of related source code edits | |
Druckemiller | Context Aware Data Generation Through Domain Specific Language | |
US20120151434A1 (en) | System And Method For Generating One Or More Plain Text Source Code Files For Any Computer Language Representing Any Graphically Represented Business Process | |
Naik et al. | A STUDY ON EVOLUTION OF GENERAL PURPOSE PROGRAMMING |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: X DEVELOPMENT LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:SINGH, RISHABH;NI, BIN;ZAHEER, MANZIL;SIGNING DATES FROM 20220427 TO 20220428;REEL/FRAME:059707/0443 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:X DEVELOPMENT LLC;REEL/FRAME:062572/0565Effective date: 20221227 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |