US9888179B1 - Video stabilization for mobile devices - Google Patents
Video stabilization for mobile devices Download PDFInfo
- Publication number
- US9888179B1 US9888179B1 US15/269,447 US201615269447A US9888179B1 US 9888179 B1 US9888179 B1 US 9888179B1 US 201615269447 A US201615269447 A US 201615269447A US 9888179 B1 US9888179 B1 US 9888179B1
- Authority
- US
- United States
- Prior art keywords
- camera orientation
- input frame
- determining
- computing device
- camera
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- H04N5/23267—
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/681—Motion detection
- H04N23/6815—Motion detection by distinguishing pan or tilt from motion
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/20—Analysis of motion
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/681—Motion detection
- H04N23/6812—Motion detection based on additional sensors, e.g. acceleration sensors
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/682—Vibration or motion blur correction
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/682—Vibration or motion blur correction
- H04N23/683—Vibration or motion blur correction performed by a processor, e.g. controlling the readout of an image memory
-
- H04N5/23258—
-
- H04N5/23261—
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/689—Motion occurring during a rolling shutter mode
-
- H04N5/2329—
Definitions
- Handheld mobile devices such as smartphones, commonly include video cameras that enable a user to record digital video.
- digital videos recorded on a handheld mobile device frequently include undesired motion resulting from the user's shaky or erratic hand movements during recording.
- Other distortions such as wobble and skew, may occur due to a digital camera's use of a rolling shutter sensor.
- Further distortions may be caused by a phenomenon known as focus breathing, which involves a change in focal length that occurs when the focus distance of the lens is changed, causing the image boundary to unexpectedly shift.
- FIG. 1 depicts a block diagram of illustrative computing device architecture, according to an example implementation.
- FIG. 2 is a top view of a series of real camera orientations and their associated virtual camera orientations, according to an example implementation.
- FIG. 3 is a flow diagram of a method, according to an example implementation.
- FIG. 4 illustrates a transformation from a physical camera image to a virtual camera image using a stabilization mesh, according to an example implementation.
- FIG. 5 is a example of an inverse stabilization mesh, according to an example implementation.
- FIG. 6 is a flow diagram of a method, according to an example implementation.
- FIG. 7 is a flow diagram of a method, according to an example implementation.
- FIG. 8 illustrates a protrusion resulting from a virtual-to-real transform, according to an example implementation.
- FIG. 9 is a chart of results comparing actual camera motion to stabilized camera motion of a handheld video taken while the user was stationary, according to an example implementation.
- FIG. 10 is a chart of results comparing actual camera motion to stabilized camera motion of a handheld video taken while the user was panning, according to an example implementation.
- FIG. 11 is a chart of the probability of panning calculated for the stabilized video of FIG. 10 , according to an example implementation.
- FIG. 12 is a chart of results comparing actual camera motion to stabilized camera motion of a handheld video taken while the user was walking and panning, according to an example implementation.
- FIG. 13 is a chart of the probability of panning calculated for the stabilized video of FIG. 12 , according to an example implementation.
- FIG. 14 is a chart of results comparing actual camera motion to stabilized camera motion of a handheld video taken while the user was running, according to an example implementation.
- Implementations of the disclosed technology include systems and methods for providing improved video stabilization on a mobile device.
- improved video stabilization may be achieved by correlating gyroscope data and focal length sensor data of an image capture device associated with the mobile device with the hand motion of the user and movement in the video to estimate the actual (i.e., real or physical) camera orientation as a video is recorded over time.
- Systems implemented according to the disclosed technology can continuously model a virtual (i.e., corrected or stabilized) camera orientation that represents the position of the camera absent undesired motions, such as jittery hand motions.
- a grid or stabilization mesh can be used to transform the pixels of an input frame associated with the real camera orientation to an output frame associated with the virtual camera orientation, such that the video may be stabilized across a series of frames. Implementations of the disclosed technology may also be effective in removing the effects of focus breathing by accounting for changes in focal length of the camera.
- a goal of the improved video stabilization method is to maintain the same virtual camera orientation from frame to frame, except when the camera is determined to be panning or when the crop region is tending to go out of bounds.
- This approach may be referred to as a “constant pose” filter.
- the systems and methods described herein seek to fix the virtual camera orientation at a constant position if the system determines no panning is occurring, or if the system determines that panning is occurring, the system estimates the panning and determines the virtual camera orientation based on the estimate of the panning.
- the modeling of a virtual camera orientation may be achieved by comparing the physical camera orientation to historical data regarding previous camera orientations and making a determination regarding whether the camera is panning.
- the system may model the movement of the physical camera and may use a non-linear filter, such as a domain transform, to smooth out the projected motion path of the physical camera.
- a virtual camera orientation may then be determined based on the smoothed motion of the real camera, the previous virtual camera orientation, and a probability of panning that is determined by the system.
- the system and methods may use a multiple stage filtering that sequentially revises the estimate of the camera motion used in determining the virtual camera orientation.
- the systems and methods described herein may also include functionality that prevents a black boundary from appearing in the output image due to undefined pixels being included in the transformation between the real image and the virtual image. This may be achieved by proposing an initial virtual camera orientation and then testing it against the transformation from real image to virtual image to determine if the resultant virtual image includes any undefined pixels. If it does, the method may include adjusting the proposed virtual camera orientation by blending it with the corresponding real camera orientation to make the new final virtual camera orientation more similar to the corresponding real camera orientation in order to eliminate the undefined pixels.
- a stabilized video with a smooth motion path free of the effects of jitter, black boundaries, and focus breathing can be generated in real time on a mobile device such that the preview view of the video matches the final video recording.
- each frame of a video may have a plurality of scan lines, and each scan line may have an associated real and virtual camera orientation because each scan line may be obtained at slightly different points in time.
- a computing device may be referred to as a mobile device, mobile computing device, a mobile station (MS), terminal, cellular phone, cellular handset, personal digital assistant (PDA), smartphone, wireless phone, organizer, handheld computer, desktop computer, laptop computer, tablet computer, set-top box, television, appliance, game device, medical device, display device, or some other like terminology.
- a computing device may be a processor, controller, or a central processing unit (CPU).
- a computing device may be a set of hardware components.
- a presence-sensitive input device may be a device that accepts input by the proximity of a finger, a stylus, or an object near the device.
- a presence-sensitive input device may also be a radio receiver (for example, a Wi-Fi receiver) and processor which is able to infer proximity changes via measurements of signal strength, signal frequency shifts, signal to noise ratio, data error rates, and other changes in signal characteristics.
- a presence-sensitive input device may also detect changes in an electric, magnetic, or gravity field.
- a presence-sensitive input device may be combined with a display to provide a presence-sensitive display.
- a user may provide an input to a computing device by touching the surface of a presence-sensitive display using a finger.
- a user may provide input to a computing device by gesturing without physically touching any object.
- a gesture may be received via a video camera or depth camera.
- a presence-sensitive display may have two main attributes. First, it may enable a user to interact directly with what is displayed, rather than indirectly via a pointer controlled by a mouse or touchpad. Secondly, it may allow a user to interact without requiring any intermediate device that would need to be held in the hand.
- Such displays may be attached to computers, or to networks as terminals. Such displays may also play a prominent role in the design of digital appliances such as a PDA, satellite navigation devices, mobile phones, and video games. Further, such displays may include a capture device and a display.
- a computer-readable medium may include, for example: a magnetic storage device such as a hard disk, a floppy disk or a magnetic strip; an optical storage device such as a compact disk (CD) or digital versatile disk (DVD); a smart card; and a flash memory device such as a card, stick or key drive, or embedded component.
- a carrier wave may be employed to carry computer-readable electronic data including those used in transmitting and receiving electronic data such as electronic mail (e-mail) or in accessing a computer network such as the Internet or a local area network (LAN).
- a person of ordinary skill in the art will recognize many modifications may be made to this configuration without departing from the scope or spirit of the claimed subject matter.
- FIG. 1 depicts a block diagram of illustrative computing device architecture 100 , according to an example implementation. Certain aspects of FIG. 1 may be embodied in a computing device, such as, a mobile computing device or smartphone. As desired, implementations of the disclosed technology may include a computing device with more or less of the components illustrated in FIG. 1 . It will be understood that the computing device architecture 100 is provided for example purposes only and does not limit the scope of the various implementations of the present disclosed systems, methods, and computer-readable mediums.
- the computing device architecture 100 of FIG. 1 includes a CPU 102 , where computer instructions are processed; a display interface 104 that acts as a communication interface and provides functions for rendering video, graphics, images, and texts on the display.
- the display interface 104 may be directly connected to a local display, such as a touch-screen display associated with a mobile computing device.
- the display interface 104 may be configured for providing data, images, and other information for an external/remote display that is not necessarily physically connected to the mobile computing device.
- a desktop monitor may be utilized for mirroring graphics and other information that is presented on a mobile computing device.
- the display interface 104 may wirelessly communicate, for example, via a Wi-Fi channel or other available network connection interface 112 to the external/remote display.
- the network connection interface 112 may be configured as a communication interface and may provide functions for rendering video, graphics, images, text, other information, or any combination thereof on the display.
- a communication interface may include a serial port, a parallel port, a general purpose input and output (GPIO) port, a game port, a universal serial bus (USB), a micro-USB port, a high definition multimedia (HDMI) port, a video port, an audio port, a Bluetooth port, a near-field communication (NFC) port, another like communication interface, or any combination thereof.
- the computing device architecture 100 may include a keyboard interface 106 that provides a communication interface to a keyboard.
- the computing device architecture 100 may include a presence-sensitive display interface 107 for connecting to a presence-sensitive display.
- the presence-sensitive display interface 107 may provide a communication interface to various devices such as a pointing device, a touch screen, a depth camera, etc. which may or may not be associated with a display.
- the computing device architecture 100 may be configured to use an input device via one or more of input/output interfaces (for example, the keyboard interface 106 , the display interface 104 , the presence sensitive display interface 107 , network connection interface 112 , camera interface 114 , sound interface 116 , etc.) to allow a user to capture information into the computing device architecture 100 .
- the input device may include a mouse, a trackball, a directional pad, a track pad, a touch-verified track pad, a presence-sensitive track pad, a presence-sensitive display, a scroll wheel, a digital camera, a digital video camera, a web camera, a microphone, a sensor, a smartcard, and the like.
- the input device may be integrated with the computing device architecture 100 or may be a separate device.
- the input device may be an accelerometer, a magnetometer, a digital camera, a microphone, and an optical sensor.
- Example implementations of the computing device architecture 100 may include an antenna interface 110 that provides a communication interface to an antenna; a network connection interface 112 that provides a communication interface to a network.
- a camera interface 114 is provided that acts as a communication interface and provides functions for capturing digital images and video from an image capture device 160 , such as a camera.
- a sound interface 116 is provided as a communication interface for converting sound into electrical signals using a microphone and for converting electrical signals into sound using a speaker.
- a random access memory (RAM) 118 is provided, where computer instructions and data may be stored in a volatile memory device for processing by the CPU 102 .
- Example implementations of the computing device architecture 100 may include various interfaces that provide communication interfaces to various sensors for data gathering.
- an ambient light sensor interface 140 is provided as a communication interface and provides functions for obtaining light data from an ambient light sensor.
- a thermometer interface 142 is provided as a communication interface and provides functions for capturing temperature data from a temperature sensor.
- an accelerometer interface 144 is provided as a communication interface and provides functions for obtaining accelerometer data from an accelerometer.
- a gyroscope interface 146 is provided as a communication interface and provides functions for obtaining gyroscope data from a gyroscope.
- a GPS location interface 148 is provided as a communication interface and provides functions for obtaining location data from a GPS receiver.
- an atmospheric pressure interface 152 is provided as a communication interface and provides functions for obtaining pressure data from a pressure sensor.
- a focal length interface 154 is provided as a communication interface and provides functions for obtaining focal length data of a camera from a focal length sensor.
- the computing device architecture 100 includes a read-only memory (ROM) 120 where invariant low-level system code or data for basic system functions such as basic input and output (I/O), startup, or reception of keystrokes from a keyboard are stored in a non-volatile memory device.
- ROM read-only memory
- the computing device architecture 100 includes a storage medium 122 or other suitable type of memory (e.g., RAM, ROM, programmable read-only memory (PROM), erasable programmable read-only memory (EPROM), electrically erasable programmable read-only memory (EEPROM), magnetic disks, optical disks, floppy disks, hard disks, removable cartridges, flash drives), where the files include an operating system 124 , application programs 126 (including, for example, a web browser application, a widget or gadget engine, and or other applications, as necessary) and data files 128 are stored.
- the computing device architecture 100 includes a power source 130 that provides an appropriate alternating current (AC) or direct current (DC) to power components.
- the computing device architecture 100 includes a telephony subsystem 132 that allows the transmission and receipt of sound over a telephone network. The constituent devices and the CPU 102 communicate with each other over a bus 134 .
- the CPU 102 has appropriate structure to be a computer processor.
- the CPU 102 may include more than one processing unit.
- the RAM 118 interfaces with the computer bus 134 to provide quick RAM storage to the CPU 102 during the execution of software programs such as the operating system application programs, and device drivers. More specifically, the CPU 102 loads computer-executable process steps from the storage medium 122 or other media into a field of the RAM 118 in order to execute software programs. Data may be stored in the RAM 118 , where the data may be accessed by the computer CPU 102 during execution.
- the device architecture 100 includes at least 125 MB of RAM, and 256 MB of flash memory.
- the storage medium 122 itself may include a number of physical drive units, such as a redundant array of independent disks (RAID), a floppy disk drive, a flash memory, a USB flash drive, an external hard disk drive, thumb drive, pen drive, key drive, a High-Density Digital Versatile Disc (HD-DVD) optical disc drive, an internal hard disk drive, a Blu-Ray optical disc drive, or a Holographic Digital Data Storage (HDDS) optical disc drive, an external mini-dual in-line memory module (DIMM) synchronous dynamic random access memory (SDRAM), or an external micro-DIMM SDRAM.
- RAID redundant array of independent disks
- HD-DVD High-Density Digital Versatile Disc
- HD-DVD High-Density Digital Versatile Disc
- HDDS Holographic Digital Data Storage
- DIMM mini-dual in-line memory module
- SDRAM synchronous dynamic random access memory
- micro-DIMM SDRAM an external micro-DIMM SDRAM
- Such computer readable storage media allow a computing device to access computer-executable process steps, application programs and the like, stored on removable and non-removable memory media, to off-load data from the device or to upload data onto the device.
- a computer program product such as one utilizing a communication system may be tangibly embodied in storage medium 122 , which may comprise a machine-readable storage medium.
- the term computing device may be a CPU, or conceptualized as a CPU (for example, the CPU 102 of FIG. 1 ).
- the computing device may be coupled, connected, and/or in communication with one or more peripheral devices, such as display.
- the term computing device, as used herein may refer to a mobile computing device, such as a smartphone or tablet computer.
- the computing device may output content to its local display and/or speaker(s).
- the computing device may output content to an external display device (e.g., over Wi-Fi) such as a TV or an external computing system.
- the computing device may include any number of hardware and/or software applications that are executed to facilitate any of the operations.
- one or more I/O interfaces may facilitate communication between the computing device and one or more input/output devices.
- a universal serial bus port, a serial port, a disk drive, a CD-ROM drive, and/or one or more user interface devices such as a display, keyboard, keypad, mouse, control panel, touch screen display, microphone, etc.
- the one or more I/O interfaces may be utilized to receive or collect data and/or user instructions from a wide variety of input devices. Received data may be processed by one or more computer processors as desired in various implementations of the disclosed technology and/or stored in one or more memory devices.
- One or more network interfaces may facilitate connection of the computing device inputs and outputs to one or more suitable networks and/or connections; for example, the connections that facilitate communication with any number of sensors associated with the system.
- the one or more network interfaces may further facilitate connection to one or more suitable networks; for example, a local area network, a wide area network, the Internet, a cellular network, a radio frequency network, a Bluetooth enabled network, a Wi-Fi enabled network, a satellite-based network any wired network, any wireless network, etc., for communication with external devices and/or systems.
- implementations of the disclosed technology include techniques for providing improved video stabilization on a mobile device that may include determining an estimate of the physical camera orientation (which may be interchangeably referred to as the real camera orientation or real pose) of the camera in relation to each frame of the video.
- the physical camera orientation may represent the camera's actual position at the time of filming a video frame.
- Implementations of the disclosed technology may further include modeling successive virtual camera orientations that correspond to each physical camera orientation.
- a virtual camera orientation may represent the camera's theoretical position when the undesired motion is removed by the system. This concept is illustrated in FIG.
- a camera orientation may refer to the position of the camera, which may include the yaw, pitch, roll, and focal length of the camera.
- a mobile device may include a gyroscope that can repeatedly provide sensor data indicative of the mobile device's yaw, pitch, and roll at successive points in time.
- a mobile device may include a focal length sensor that may provide data indicative of the focal length of the camera.
- each real and virtual camera orientation may correspond to a unique time stamp of the video.
- each real and virtual camera orientation may be associated with a particular video frame or scan line of a video frame.
- the system may use the first real camera orientation 202 a as a starting point or initial reference point, and the system may designate that the first virtual camera orientation 204 a has the same or substantially the same position as the first real camera orientation 202 a . As shown in the example in FIG.
- the real camera orientation may rotate clockwise to arrive at the second real camera orientation 202 b , then counterclockwise to arrive a the third real camera orientation 202 c , and then clockwise to arrive at the fourth real camera orientation 202 d .
- the system may create virtual camera orientations 204 a - d that corresponds to each real camera orientation 202 a - d .
- Each virtual camera orientation 204 a - d may represent the camera position of the corresponding real camera orientation 202 a - d after the real camera orientation 202 a - d has been adjusted to remove undesired rotational movement, providing for a stabilized path of the camera. As shown in FIG. 2 , while the real camera orientations 202 a - d wobble back and forth, the virtual camera orientations 202 a - d provide a stabilized position devoid of unwanted rotation. According to some implementations, once a virtual camera orientation has been determined for a particular frame or scan line of the video, the image from the frame or scan line may be modified to show to reflect an estimate of what would have been filmed had the camera been positioned at the virtual camera orientation instead of the real camera orientation. In this way, the system may adjust each frame and provide a stabilized video.
- FIG. 3 is a flow diagram of a method 300 according to an example implementation of the disclosed technology.
- the method 300 begins with a computing device receiving 302 camera orientation data and image data.
- camera orientation data may include gyroscope data from a mobile device that may be indicative of a mobile device's yaw, pitch, and roll.
- camera orientation data may also include focal length sensor data from a mobile device that may be indicative of the focal length of a camera of the mobile device.
- camera orientation data may correspond to one or more frames of a digital video and/or one or more scanlines of a frame.
- image data may be representative of one or more images of one or more input frames of a digital video.
- the method further includes determining 304 by a computing device, an estimate of the physical camera orientation for a frame or scan line based on the camera orientation data. For example, in some implementations, a computing device may determine a first and second physical camera orientation estimate based on the gyroscope data and that are associated with a first and second input frame, respectively. The method further includes determining 600 a corrected (or virtual) camera orientation for the same frame or scan line, as will be described in greater detail below. According to some implementations, once the physical camera orientation and virtual camera orientation for a particular frame or scan line have been determined, the method 300 may further includes determining 306 a transformation, such as a stabilization mesh, for generating a stabilized image from the input image.
- a transformation such as a stabilization mesh
- a stabilization mesh may be a grid that can be used to transform the real image (i.e., an image associated with the real camera orientation) to a virtual image (i.e., an image associated with the virtual camera orientation).
- the method 300 further includes generating 308 an output frame based on the corrected camera orientation.
- the output frame may be generated based on a transformation, such as a stabilization mesh.
- the output frame may represent a stabilized version of a corresponding input frame, having, for example, undesired motion removed.
- the output frame may be displayed on a screen of a mobile device.
- the system may use the data indicative of a stabilized image to map the pixels of the input frame to an output frame to create a stabilized output image.
- the method 300 may be performed repeatedly on subsequent frames of a video in real time on a mobile device in order to generate a stabilized video.
- a stabilized image of an input video frame may be created by generating an estimate of a real camera orientation associated with a particular frame or scan line, determining a virtual camera orientation corresponding to the same frame or scan line, and then generating and applying a transformation to the input image associated with the real camera orientation to generate a stabilized image associated with the virtual camera orientation.
- the transformation may include a stabilization mesh.
- FIG. 4 shows an illustration of how a stabilization mesh may be used to warp an input image 402 a associated with the perspective of the real camera orientation to generate a stabilized image 402 b associated with the perspective of the virtual camera orientation. As shown in FIG.
- the input image 402 a may be rotated to one side as a result of unintentional hand movement by a user when recording a video with a mobile device, but the stabilized image 402 b may be devoid of this rotation.
- the stabilization mesh 404 a , 304 b may be made up of a grid having a plurality of regions, such as quadrilaterals.
- the stabilization mesh shown in FIG. 4 is made up of a grid of four rows and four columns of rectangular boxes. The system may overlay the stabilization mesh on the image such that each point in the image lies within a region of the mesh.
- the system may overlay the input image 402 a with the grid 404 a such that each pixel of the input image 402 a is positioned within a region of the grid 404 a .
- each vertex on the real image grid 404 a is associated with a vertex on the virtual image grid 404 b , forming a mapping mesh that may be used to transform the input image 402 a to a stabilized output image 402 b .
- the stabilization mesh may be used to project pixels from a predetermined number of scanlines from the input image to an output image and may interpolate the projection of the remaining of the pixels using known techniques.
- a stabilization homography may be used instead of a stabilization mesh.
- a stabilization homography may be a global 3-by-3 transform between an input homogenous coordinate and an output homogenous coordinate.
- a stabilization homography may be less complex than a stabilization mesh and may provide lower quality than a stabilization mesh, but because it requires less parameters to transmit, it may increase the speed at which an output image may be generated.
- the system may use stabilization homography by defining a rectangular box on the image plane of the physical camera, and determine the corresponding 2D coordinate in the virtual camera for each vertex using Equation (5) below.
- a homography may be computed from input and output image quadrilaterals.
- an output image resulting from the transformation from the real camera image to the virtual camera image may have black boundaries around the edges if some image pixels become undefined during the mapping.
- black boundaries representing undefined pixels
- the system may crop the virtual camera image (i.e., output image) by a fixed margin.
- the system may crop the virtual camera image by 10% on each side.
- FIG. 5 shows an example of an inverse stabilization mesh (i.e., from output image to input image) illustrating how a portion of the edges of the input image are left out of the mesh and are therefore cropped in the output image.
- the systems and methods for providing improved video stabilization on a mobile device may use a camera model to aid in describing the relationship between a real image (i.e., an image taken from the perspective of the physical camera orientation) and a virtual image (i.e., an image created from the perspective of the virtual camera orientation).
- a virtual image may represent an approximation for a corresponding real image that has been adjusted to remove distortions caused by unwanted motion that occurred during recording.
- the system may project pixels (i.e., points) from a 2-dimensional real image (i.e., an image from a recorded video) into a 3-dimensional space, and the system may then project the pixels onto a 2-dimensional virtual (“2D”) image after accounting for the effects of the movement of the camera on the pixels in the 3-dimensional space.
- K(t) may be the camera intrinsic matrix at time t
- R(t) may be the camera rotation matrix at time t.
- the camera intrinsic matrix may be used in transforming points between 3-dimensional space and a 2-dimensional image plane. According to some implementations, the intrinsic matrix K(t) may be:
- K ⁇ ( t ) [ f ⁇ ( t ) 0 o x 0 f ⁇ ( t ) o y 0 0 1 ] ( 2 )
- focal length may be modeled as a function of time because the distance between the lens of the camera of the mobile device and the focal length sensor may change due to focus adjustment or focus breathing (i.e., lens movement).
- a computing device may receive a signal from a focal length sensor that provides data indicative of changes in focal length of the camera over time.
- the focal length may be set to be a constant, in which case the systems and methods described herein may provide an improved stabilized video without correcting for distortions caused by focus breathing.
- the intrinsic matrix and rotation matrix for the physical camera may be denoted by K p (t) and R p (t) respectively.
- the intrinsic matrix and rotation matrix for the virtual camera may be denoted by K v and R v (t).
- q p (t) and q v (t) may be used to denote the quaternion representation of R p (t) and R v (t) respectively.
- K v may be a constant (independent of time) so that the same field-of-view (FOV) may be maintained in the stabilized videos, regardless of lens movement.
- the system may obtain K p (t) from the camera lens position, which the system may derive from camera sensor data received from the focal length sensor of the mobile device.
- K v may be predetermined.
- K v may be equal to the intrinsic matrix of the physical camera when the lens is focusing at infinity.
- the system may determine the coordinate of the 2D image point in the virtual camera for any 2D point in physical camera using Equation (5).
- the system may determine an estimate of the physical or real camera orientation R p (t) of the camera in the mobile device for a particular frame or scan line using data obtained from a gyroscope of the mobile device.
- R p (t) the physical or real camera orientation of the camera in the mobile device
- data obtained from a gyroscope of the mobile device it will be appreciated by those of skill in the art that many modern digital cameras use a rolling shutter, which means that each frame of the video is captured by rapidly scanning horizontally or vertically across the scene, as opposed to capturing the entire scene at once. As such, each frame of a video may have a plurality of scan lines that are scanned in succession at slightly different times. Accordingly, if the mobile device is in motion, each individual scan line may be associated with a different physical camera orientation R p (t).
- portions of this disclosure may refer to a physical or virtual camera orientation associated with a frame of the video, it should be understood that each frame may have a number of associated physical and virtual camera orientations that are associated with each scan line of the frame.
- a computing device may continuously receive the gyroscope signal to estimate the most recent real camera orientation.
- the gyroscope signal may be fetched or received by the computing device at a high frequency, such as 200 Hz, for example. Although it should be understood that different frequencies may be used according to different implementations.
- the system may calculate the rotation of the camera relative to the camera orientation at previous gyro timestamp t n-1 .
- the system may first convert the rotational speed (derived by the system from gyroscope data) and time interval (i.e., the difference between current gyroscope timestamp and previous gyroscope timestamp) to a rotation in axis-angle representation.
- the system may then convert this axis angle representation of relative rotation to quaternion, which may be denoted as r p (t n ).
- the system may find the two orientations q p (t 1 ) and q p (t 2 ) with t 1 and t 2 being two timestamps closest to t and t 1 ⁇ t ⁇ t 2 , and perform spherical linear interpolation (SLERP) of q p (t 1 ) and q p (t2).
- SLERP spherical linear interpolation
- the system may determine a corresponding virtual camera orientation.
- FIG. 6 is a flow diagram of a method 600 according to an example implementation of the disclosed technology.
- the method 600 begins with determining 700 an initial virtual camera orientation of a frame or scan line corresponding to a real camera orientation of the same frame or scan line.
- the system may determine an initial virtual camera orientation based on an estimate of the real camera orientation as well as a virtual camera orientation of one or more previous input frames and/or corresponding scan lines of the video.
- the method 600 may further include determining 602 whether an output frame generated using the initial virtual camera orientation would include any undefined pixels.
- determining whether the output frame would include any undefined pixels may include making a determination as to whether the bounds of the output frame would be fully covered by the stabilization mesh if the input image were to be warped to the virtual image using the stabilization mesh. According to some implementations, if an output frame includes undefined pixels, it may be desirable to adjust the initial virtual camera orientation to prevent the inclusion of undefined pixels in the output frame. Accordingly, according to some implementations, the method 600 may further include determining 604 a final virtual camera orientation.
- the system may determine a final virtual camera orientation by adjusting the initial virtual camera orientation by blending the initial virtual camera orientation with the corresponding real camera orientation in response to a determination that use of the initial virtual camera orientation in creating an output image would result in undefined pixels.
- the method may further include outputting 606 data representative of the final virtual camera orientation, which may be utilized in generating an output image from an input image of the video.
- the system may determine a virtual camera orientation R v (t) (or q v (t) for the quaternion representation) that may be used in generating a stabilized output image from an input image obtained at a corresponding real camera orientation.
- the system may use a nonlinear filter to determine the virtual camera orientation.
- the system may use the same q v (t) for the entire frame readout time to remove the rolling shutter effect.
- q v ( t ) q v [n], t ⁇ [t n,0 ,t n,h ], (7) where t n,0 is the timestamp of a first scan line at frame n, and t n,h is the timestamp of a last scan line at frame n. Therefore, the system may use a discrete time representation q v [n] for the virtual camera orientation. According to some implementations, the frequency of the video frames may be different to the frequency of the gyroscope event in Equation (6). Proposing Initial Virtual Camera Orientation
- a method for providing improved video stabilization on a mobile device may include determining, by the system, an initial or proposed virtual camera orientation corresponding to a frame or scan line before generating a final virtual camera orientation corresponding to the frame or scan line, as described below.
- determining an initial virtual orientation may include making a determination if the camera is panning or remaining still and projecting the camera's movement based on historical camera orientation data.
- Historical camera orientation data may include stored data indicative of estimates of physical camera orientations corresponding to previous inputs frames and/or scan lines of the video as well as the corresponding virtual camera orientations.
- the system may set an initial virtual camera orientation corresponding to an input frame to be the same as the virtual camera orientation corresponding to the previous frame of the video.
- FIG. 7 is a flow diagram of a method 700 according to an example implementation of the disclosed technology.
- the method 700 begins with a computing device receiving 702 data representative of a physical camera orientation.
- the computing device may receive gyroscope data from a gyroscope interface of the mobile device and focal length data from a focal length sensor of the mobile device.
- the method 700 further includes determining 704 the dominant panning direction of the camera.
- the computing device may determine the dominant panning direction based on the gyroscope data.
- the method 700 may further include applying 706 a non-linear smoothing filter to data indicative of the physical camera movement to filter the physical camera motion and eliminate noise.
- the system may use a domain transform to filter the physical camera motion.
- use of the domain transform to filter the physical camera motion may make the virtual camera more responsive to the start and stop of physical camera panning.
- the system may use other edge aware filters to filter the physical camera motion.
- the method 700 further includes determining 708 a probability of whether the camera is panning.
- the system may utilize a logistic regression using previous camera orientation data to determine a panning probability S representing the likelihood that the camera is panning.
- the panning probability may be any number from 0 to 1, with 0 representing a stationary camera and 1 representing a panning camera.
- a panning probability might be 0.8.
- the system may use logistic regression to perform panning detection.
- the system may extract gyroscope data from a windowed time frame, obtain the average ⁇ and standard deviation ⁇ of the gyroscope data from this window, and compute a feature
- ⁇ 1 and ⁇ 2 are obtained by training data.
- ⁇ 1 and ⁇ 2 may be selected by a user or system administrator.
- the method 700 may further include determining 710 an initial virtual camera orientation based on the smoothed camera rotation, the panning probability, and the previous virtual camera orientation of the previous frame.
- the computing device may repeatedly store historical data pertaining to real camera orientations and virtual camera orientations in memory and may access this historical data from memory at any time during the execution of the methods described herein.
- it may be necessary for the system to determine if use of a stabilization mesh corresponding to the real camera orientation and the proposed virtual camera orientation may result in undefined pixels in the appearing in the output image. If so, the system may modify the initial virtual camera orientation to create a final virtual camera orientation that eliminates the undefined pixels, thereby preventing undesired black boundaries from showing up in the output image.
- the system may test the output image that results from mapping the real camera image to the proposed virtual camera orientation using the stabilization mesh to determine whether the proposed virtual camera orientation needs to be modified to eliminate undefined pixels that occur after the stabilization mesh is applied.
- the system may adjust the proposed virtual camera orientation by blending the proposed virtual orientation with the real camera orientation to determine a final virtual camera orientation.
- blending of the proposed virtual camera orientation and the real camera orientation may be performed using a binary search.
- the system may verify that the warped image bounding box in the physical camera image covers the entire image area in the virtual camera image, or conversely, that the warped image bounding box in the virtual camera image stays instead the image are of the physical camera image.
- the system may achieve this goal by adjusting the virtual camera orientation to more closely resemble the physical camera orientation using SLERP.
- the minimum SLERP ⁇ 1 [n] coefficient needed to avoid black boundaries may be found by using a binary search.
- FIG. 8 shows a representation of the image boundary 802 of the virtual camera, represented by the outermost box.
- the system may define an outer box 804 and an inner box 806 within the image boundary 802 of the virtual camera.
- the respective sizes of the outer box 804 and inner box 806 may be parameters that may be tuned to achieve different results.
- the system may transform the inner box 806 according to a homography transform from the virtual camera orientation to the physical camera orientation to generate the warped box 808 .
- the system may determine the maximum protrusion 810 outside of the outer box 804 among all vertices of the warped box 808 .
- the system may determine the physical and virtual camera homography from the source and destination quadrilaterals for each horizontal stripe for a physical to virtual stabilization mesh as shown in FIG. 4 , and invert the homography to generate the virtual to physical homography to be used in determining the protrusion.
- the system may normalize the image size to [0, 1] ⁇ [0, 1] before computing the protrusion.
- the system may use ⁇ [n] to denote the protrusion amount at frame n.
- the system may compute a SLERP coefficient ⁇ 2 [n] as:
- ⁇ and ⁇ are predetermined constants.
- the system may determine the final virtual camera orientation as:
- the system may obtain the stabilization mesh to warp the input frame or scan line to achieve video stabilization and rolling shutter correction.
- FIG. 9 shows the results of a video of a typical handheld video where the user is stationary.
- the filtered video generated from the virtual camera orientations shows significant elimination of camera rotation, producing a very stable video image.
- FIG. 10 shows the results of a video where the user is panning with the corresponding calculated probability of panning shown in FIG. 11 .
- the stabilized video of the panning scene shows a significant increase in the smoothness of the video quality.
- FIG. 12 shows the results of a video where the user is walking and panning with the corresponding calculated probability of panning shown in FIG. 13 .
- the method described herein does well at stabilizing this type of video as well.
- the chart in FIG. 13 does show some occasional fast motion, however the parameters of the method may be tuned by, for example, adjusting the outer box 804 margin shown in FIG. 8 to make the video look more natural.
- FIG. 14 shows the results of a video taken while the user was running, which presents drastic camera motion that is challenging to stabilize because there is only a limited amount of cropping that can be used and when the motion is large, it is easy for the crop region to hit the image boundary.
- the methods described herein performed well to reduce the amount of hand shaking from the video.
- These computer-executable program instructions may be loaded onto a general-purpose computer, a special-purpose computer, a processor, or other programmable data processing apparatus to produce a particular machine, such that the instructions that execute on the computer, processor, or other programmable data processing apparatus create means for implementing one or more functions specified in the flow diagram block or blocks.
- These computer program instructions may also be stored in a computer-readable memory that may direct a computer or other programmable data processing apparatus to function in a particular manner, such that the instructions stored in the computer-readable memory produce an article of manufacture including instruction means that implement one or more functions specified in the flow diagram block or blocks.
- implementations of the disclosed technology may provide for a computer program product, comprising a computer-usable medium having a computer-readable program code or program instructions embodied therein, said computer-readable program code adapted to be executed to implement one or more functions specified in the flow diagram block or blocks.
- the computer program instructions may also be loaded onto a computer or other programmable data processing apparatus to cause a series of operational elements or steps to be performed on the computer or other programmable apparatus to produce a computer-implemented process such that the instructions that execute on the computer or other programmable apparatus provide elements or steps for implementing the functions specified in the flow diagram block or blocks.
- blocks of the block diagrams and flow diagrams support combinations of means for performing the specified functions, combinations of elements or steps for performing the specified functions and program instruction means for performing the specified functions. It will also be understood that each block of the block diagrams and flow diagrams, and combinations of blocks in the block diagrams and flow diagrams, may be implemented by special-purpose, hardware-based computer systems that perform the specified functions, elements or steps, or combinations of special-purpose hardware and computer instructions.
Abstract
Description
x=K(t)R(t)X, (1)
x p =K p(t)R p(t)X, (3)
x v =K v R v(t)X (4)
x v =K v R v(t)R p −1(t)K p −1(t)x p (5)
q p(t n)=r p(t n)*q p(t n-1) (6)
q v(t)=q v[n], tε[tn,0 ,t n,h], (7)
where tn,0 is the timestamp of a first scan line at frame n, and tn,h is the timestamp of a last scan line at frame n. Therefore, the system may use a discrete time representation qv[n] for the virtual camera orientation. According to some implementations, the frequency of the video frames may be different to the frequency of the gyroscope event in Equation (6).
Proposing Initial Virtual Camera Orientation
r v[n]=Slerp(r 0,
where r0 represents no rotation or [0, 0, 0, 1] in quaternion representation,
{circumflex over (q)} v[n]=rv[n]*{circumflex over (q)} v[n−1], (9)
where, according to some implementations, the system may initialize the proposed virtual camera orientation at first frame {circumflex over (q)}v[0] to be the physical camera orientation at first frame center centerline:
where ε may be used to avoid numerical instability when σ is very small. In some implementations, the system may use a logistic regression function that takes l as input and generates the panning probability p[n] in Equation (8):
p[n]=1/(1+exp(−(β1 +l·β 2))), (10)
λ3[n]=γ·λ 3[n−1]+(1−γ)·λ2[n], (12)
where γ is a predetermined constant. This smoothing may make videos with drastic motion such as running scenes look more natural and pleasing.
Claims (20)
Priority Applications (6)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/269,447 US9888179B1 (en) | 2016-09-19 | 2016-09-19 | Video stabilization for mobile devices |
CN202110451819.XA CN113382161B (en) | 2016-09-19 | 2017-09-18 | Method, system and medium for providing improved video stability of mobile device |
CN201780041564.1A CN109644231B (en) | 2016-09-19 | 2017-09-18 | Method, system, and medium providing improved video stability for mobile devices |
PCT/US2017/052015 WO2018053400A1 (en) | 2016-09-19 | 2017-09-18 | Improved video stabilization for mobile devices |
EP17778027.7A EP3466050A1 (en) | 2016-09-19 | 2017-09-18 | Improved video stabilization for mobile devices |
US15/848,289 US10200613B2 (en) | 2016-09-19 | 2017-12-20 | Video stabilization for mobile devices |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/269,447 US9888179B1 (en) | 2016-09-19 | 2016-09-19 | Video stabilization for mobile devices |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/848,289 Continuation US10200613B2 (en) | 2016-09-19 | 2017-12-20 | Video stabilization for mobile devices |
Publications (1)
Publication Number | Publication Date |
---|---|
US9888179B1 true US9888179B1 (en) | 2018-02-06 |
Family
ID=60002018
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/269,447 Active US9888179B1 (en) | 2016-09-19 | 2016-09-19 | Video stabilization for mobile devices |
US15/848,289 Active US10200613B2 (en) | 2016-09-19 | 2017-12-20 | Video stabilization for mobile devices |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/848,289 Active US10200613B2 (en) | 2016-09-19 | 2017-12-20 | Video stabilization for mobile devices |
Country Status (4)
Country | Link |
---|---|
US (2) | US9888179B1 (en) |
EP (1) | EP3466050A1 (en) |
CN (2) | CN113382161B (en) |
WO (1) | WO2018053400A1 (en) |
Cited By (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20180084236A1 (en) * | 2016-09-20 | 2018-03-22 | Cyberlink Corp. | Systems and methods for reducing horizontal misalignment in 360-degree video |
US10200613B2 (en) | 2016-09-19 | 2019-02-05 | Google Llc | Video stabilization for mobile devices |
US20190297265A1 (en) * | 2018-03-21 | 2019-09-26 | Sawah Innovations Inc. | User-feedback video stabilization device and method |
US10545215B2 (en) * | 2017-09-13 | 2020-01-28 | Google Llc | 4D camera tracking and optical stabilization |
US10565802B2 (en) * | 2017-08-31 | 2020-02-18 | Disney Enterprises, Inc. | Collaborative multi-modal mixed-reality system and methods leveraging reconfigurable tangible user interfaces for the production of immersive, cinematic, and interactive content |
US10812717B2 (en) * | 2018-05-04 | 2020-10-20 | Google Llc | Stabilizing video by accounting for a location of a feature in a stabilized view of a frame |
US20210088803A1 (en) * | 2019-09-19 | 2021-03-25 | Fotonation Limited | Method for stabilizing a camera frame of a video sequence |
US11064119B2 (en) | 2017-10-03 | 2021-07-13 | Google Llc | Video stabilization |
US20210279843A1 (en) * | 2020-03-04 | 2021-09-09 | Nec Laboratories America, Inc. | Joint rolling shutter image stitching and rectification |
US11190689B1 (en) | 2020-07-29 | 2021-11-30 | Google Llc | Multi-camera video stabilization |
US11477382B2 (en) * | 2016-02-19 | 2022-10-18 | Fotonation Limited | Method of stabilizing a sequence of images |
Families Citing this family (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN112383712B (en) * | 2020-11-12 | 2021-12-14 | 北京环境特性研究所 | Image stabilization processing method and device for airborne video gyroscope |
CN112532883B (en) * | 2020-11-27 | 2022-03-29 | 维沃移动通信有限公司 | Shooting anti-shake method and device, electronic equipment and readable storage medium |
WO2024076363A1 (en) * | 2022-10-04 | 2024-04-11 | Google Llc | Field of view correction techniques for shutterless camera systems |
Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9232138B1 (en) | 2013-06-03 | 2016-01-05 | Amazon Technologies, Inc. | Image stabilization techniques |
Family Cites Families (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2007011306A2 (en) * | 2005-07-20 | 2007-01-25 | Bracco Imaging S.P.A. | A method of and apparatus for mapping a virtual model of an object to the object |
JP2007243250A (en) * | 2006-03-03 | 2007-09-20 | Olympus Imaging Corp | Imaging apparatus and imaging method |
CN101753774B (en) * | 2008-12-16 | 2012-03-14 | 财团法人资讯工业策进会 | Method and system for stabilizing digital images |
WO2012064106A2 (en) * | 2010-11-12 | 2012-05-18 | Samsung Electronics Co., Ltd. | Method and apparatus for video stabilization by compensating for view direction of camera |
GB2492529B (en) * | 2011-05-31 | 2018-01-10 | Skype | Video stabilisation |
US9294676B2 (en) * | 2012-03-06 | 2016-03-22 | Apple Inc. | Choosing optimal correction in video stabilization |
US8773542B2 (en) * | 2012-05-17 | 2014-07-08 | Samsung Electronics Co., Ltd. | Apparatus and method for adaptive camera control method based on predicted trajectory |
JP2014143530A (en) * | 2013-01-23 | 2014-08-07 | Sony Corp | Information processing unit, information processing method, and imaging apparatus |
US20140320592A1 (en) * | 2013-04-30 | 2014-10-30 | Microsoft Corporation | Virtual Video Camera |
TWI502548B (en) * | 2013-06-14 | 2015-10-01 | Vivotek Inc | Real-time image processing method and device therefor |
KR102248161B1 (en) * | 2013-08-09 | 2021-05-04 | 써멀 이미징 레이다 엘엘씨 | Methods for analyzing thermal image data using a plurality of virtual devices and methods for correlating depth values to image pixels |
US9224194B2 (en) * | 2014-01-21 | 2015-12-29 | Adobe Systems Incorporated | Joint video deblurring and stabilization |
US9888179B1 (en) | 2016-09-19 | 2018-02-06 | Google Llc | Video stabilization for mobile devices |
-
2016
- 2016-09-19 US US15/269,447 patent/US9888179B1/en active Active
-
2017
- 2017-09-18 CN CN202110451819.XA patent/CN113382161B/en active Active
- 2017-09-18 CN CN201780041564.1A patent/CN109644231B/en active Active
- 2017-09-18 EP EP17778027.7A patent/EP3466050A1/en active Pending
- 2017-09-18 WO PCT/US2017/052015 patent/WO2018053400A1/en unknown
- 2017-12-20 US US15/848,289 patent/US10200613B2/en active Active
Patent Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9232138B1 (en) | 2013-06-03 | 2016-01-05 | Amazon Technologies, Inc. | Image stabilization techniques |
Non-Patent Citations (1)
Title |
---|
Alexandre Karpenko, David Jacobs, Jongmin Baek, Mark Levoy; Digital Video Stabilization and Rolling Shutter Correction using Gyroscopes; Stanford Tech Report CTSR Mar. 2011; pp. 1-7. |
Cited By (19)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11477382B2 (en) * | 2016-02-19 | 2022-10-18 | Fotonation Limited | Method of stabilizing a sequence of images |
US10200613B2 (en) | 2016-09-19 | 2019-02-05 | Google Llc | Video stabilization for mobile devices |
US10681327B2 (en) * | 2016-09-20 | 2020-06-09 | Cyberlink Corp. | Systems and methods for reducing horizontal misalignment in 360-degree video |
US20180084236A1 (en) * | 2016-09-20 | 2018-03-22 | Cyberlink Corp. | Systems and methods for reducing horizontal misalignment in 360-degree video |
US10565802B2 (en) * | 2017-08-31 | 2020-02-18 | Disney Enterprises, Inc. | Collaborative multi-modal mixed-reality system and methods leveraging reconfigurable tangible user interfaces for the production of immersive, cinematic, and interactive content |
US10545215B2 (en) * | 2017-09-13 | 2020-01-28 | Google Llc | 4D camera tracking and optical stabilization |
US11683586B2 (en) | 2017-10-03 | 2023-06-20 | Google Llc | Video stabilization |
US11064119B2 (en) | 2017-10-03 | 2021-07-13 | Google Llc | Video stabilization |
US20190297265A1 (en) * | 2018-03-21 | 2019-09-26 | Sawah Innovations Inc. | User-feedback video stabilization device and method |
US11227146B2 (en) | 2018-05-04 | 2022-01-18 | Google Llc | Stabilizing video by accounting for a location of a feature in a stabilized view of a frame |
US10812717B2 (en) * | 2018-05-04 | 2020-10-20 | Google Llc | Stabilizing video by accounting for a location of a feature in a stabilized view of a frame |
US10983363B2 (en) * | 2019-09-19 | 2021-04-20 | Fotonation Limited | Method for stabilizing a camera frame of a video sequence |
US20210302755A1 (en) * | 2019-09-19 | 2021-09-30 | Fotonation Limited | Method for stabilizing a camera frame of a video sequence |
US11531211B2 (en) * | 2019-09-19 | 2022-12-20 | Fotonation Limited | Method for stabilizing a camera frame of a video sequence |
US20210088803A1 (en) * | 2019-09-19 | 2021-03-25 | Fotonation Limited | Method for stabilizing a camera frame of a video sequence |
US20210279843A1 (en) * | 2020-03-04 | 2021-09-09 | Nec Laboratories America, Inc. | Joint rolling shutter image stitching and rectification |
US11694311B2 (en) * | 2020-03-04 | 2023-07-04 | Nec Corporation | Joint rolling shutter image stitching and rectification |
US11190689B1 (en) | 2020-07-29 | 2021-11-30 | Google Llc | Multi-camera video stabilization |
US11856295B2 (en) | 2020-07-29 | 2023-12-26 | Google Llc | Multi-camera video stabilization |
Also Published As
Publication number | Publication date |
---|---|
CN113382161A (en) | 2021-09-10 |
WO2018053400A1 (en) | 2018-03-22 |
CN109644231A (en) | 2019-04-16 |
CN113382161B (en) | 2023-05-09 |
EP3466050A1 (en) | 2019-04-10 |
US20180115714A1 (en) | 2018-04-26 |
CN109644231B (en) | 2021-05-04 |
US10200613B2 (en) | 2019-02-05 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10200613B2 (en) | Video stabilization for mobile devices | |
US11140339B2 (en) | Video image processing method, apparatus and terminal | |
US10887519B2 (en) | Method, system and apparatus for stabilising frames of a captured video sequence | |
US10129462B2 (en) | Camera augmented reality based activity history tracking | |
US9860448B2 (en) | Method and electronic device for stabilizing video | |
US9516223B2 (en) | Motion-based image stitching | |
US9589321B2 (en) | Systems and methods for animating a view of a composite image | |
US20170332018A1 (en) | Real-time video stabilization for mobile devices based on on-board motion sensing | |
US9672866B2 (en) | Automated looping video creation | |
US9525821B2 (en) | Video stabilization | |
CN113438511B (en) | Method, medium and system for automatic adjustment of video direction | |
CN107404615B (en) | Image recording method and electronic equipment | |
WO2018223381A1 (en) | Video shake-prevention method and mobile device | |
US10121262B2 (en) | Method, system and apparatus for determining alignment data | |
WO2022116772A1 (en) | Video clipping method and apparatus, storage medium, and electronic device | |
CN115514897B (en) | Method and device for processing image | |
US10911677B1 (en) | Multi-camera video stabilization techniques | |
US20230109047A1 (en) | Methods and apparatus for re-stabilizing video in post-processing | |
JP2010072813A (en) | Image processing device and image processing program | |
US20220279128A1 (en) | Systems and methods for horizon leveling videos | |
JP6103942B2 (en) | Image data processing apparatus and image data processing program | |
Lee | Novel video stabilization for real-time optical character recognition applications | |
CN113660420A (en) | Video frame processing method and video frame processing device | |
KR20170009809A (en) | An imaging photographing device and an imaging photographing method using an video editing | |
CN115278053A (en) | Image shooting method and electronic equipment |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:LIANG, CHIA-KAI;TU, XUE;CHU, LUN-CHENG;AND OTHERS;SIGNING DATES FROM 20160916 TO 20160919;REEL/FRAME:039788/0594 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044567/0001Effective date: 20170929 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |