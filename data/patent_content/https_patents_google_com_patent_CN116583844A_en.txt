CN116583844A - Efficient embedded table storage and lookup - Google Patents
Efficient embedded table storage and lookup Download PDFInfo
- Publication number
- CN116583844A CN116583844A CN202180082619.XA CN202180082619A CN116583844A CN 116583844 A CN116583844 A CN 116583844A CN 202180082619 A CN202180082619 A CN 202180082619A CN 116583844 A CN116583844 A CN 116583844A
- Authority
- CN
- China
- Prior art keywords
- embedding
- machine learning
- embedded
- model
- compression
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/35—Clustering; Classification
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/22—Indexing; Data structures therefor; Storage structures
- G06F16/2282—Tablespace storage structures; Management thereof
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/21—Design, administration or maintenance of databases
- G06F16/215—Improving data quality; Data cleansing, e.g. de-duplication, removing invalid entries or correcting typographical errors
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/22—Indexing; Data structures therefor; Storage structures
- G06F16/2228—Indexing structures
- G06F16/2255—Hash tables
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/3331—Query processing
- G06F16/334—Query execution
- G06F16/3347—Query execution using vector based model
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F30/00—Computer-aided design [CAD]
- G06F30/20—Design optimisation, verification or simulation
- G06F30/27—Design optimisation, verification or simulation using machine learning, e.g. artificial intelligence, neural networks, support vector machines [SVM] or training a model
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/10—File systems; File servers
- G06F16/17—Details of further file system functions
- G06F16/174—Redundancy elimination performed by the file system
- G06F16/1744—Redundancy elimination performed by the file system using compression, e.g. sparse files
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2211/00—Indexing scheme relating to details of data-processing equipment not covered by groups G06F3/00 - G06F13/00
- G06F2211/007—Encryption, En-/decode, En-/decipher, En-/decypher, Scramble, (De-)compress
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2211/00—Indexing scheme relating to details of data-processing equipment not covered by groups G06F3/00 - G06F13/00
- G06F2211/10—Indexing scheme relating to G06F11/10
- G06F2211/1002—Indexing scheme relating to G06F11/1076
- G06F2211/1014—Compression, i.e. RAID systems with parity using compression techniques
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/284—Lexical analysis, e.g. tokenisation or collocates
-
- H—ELECTRICITY
- H03—ELECTRONIC CIRCUITRY
- H03M—CODING; DECODING; CODE CONVERSION IN GENERAL
- H03M7/00—Conversion of a code where information is represented by a given sequence or number of digits to a code where the same, similar or subset of information is represented by a different sequence or number of digits
- H03M7/30—Compression; Expansion; Suppression of unnecessary data, e.g. redundancy reduction
Abstract
The present disclosure provides systems, methods, and computer program products for providing efficient embedded table storage and lookup in a machine learning model. A computer-implemented method may include: obtaining an embedding table comprising a plurality of embeddings respectively associated with corresponding indexes of the embedding table, individually compressing each particular embedment of the embedding table to allow each respective embedment of the embedding table to be decompressed independently of any other embeddings in the embedding table, packaging the embedding table comprising the individually compressed embeddings together with a machine learning model, receiving input for locating the embeddings in the embedding table, determining a lookup value based on the input to search the index of the embedding table, locating the embeddings based on searching the index of the embedding table for the determined lookup value, and decompressing the located embeddings independently of any other embeddings in the embedding table.
Description
Cross Reference to Related Applications
The present application claims priority and benefit from U.S. patent application Ser. No.17/147,844, filed on 1/13/2021. U.S. patent application Ser. No.17/147,844 is incorporated by reference in its entirety.
Technical Field
The present disclosure relates generally to machine learning. More particularly, the present disclosure relates to providing efficient embedded table storage and lookup in a machine learning model.
Background
Embedding generally refers to the manner in which objects are projected from a high-dimensional space to a low-dimensional space. For example, image, video, text, and voice data may be represented in an object space such as a two-dimensional or three-dimensional coordinate system. Each data point in the object space may represent an object, with the proximity between each object indicating a degree of similarity or correlation. In one example, synonyms may be generally more closely positioned together in object space than words that are not associated or similar.
The machine learning model may utilize an embedding table to store and reference the embeddings. The embedded tables become part of the machine learning model and the size of the embedded tables can be quite large. For example, an embedded table of dictionary words from one or more languages may have hundreds of thousands or even millions of entries. In particular, the embedded tables can significantly increase the amount of memory and processing power required to run the machine learning model. Thus, it becomes increasingly difficult to serve large models on a large scale. Furthermore, many types of computing devices, including mobile devices, often lack sufficient computing resources to run large models.
Disclosure of Invention
Aspects and advantages of embodiments of the invention will be set forth in part in the description which follows, or may be learned by practice of the embodiments.
One example aspect of the present disclosure relates to a system that provides efficient embedded table storage and lookup for machine learning models, for example, by: receiving an embedding table comprising a plurality of embeddings respectively associated with corresponding indexes of the embedding table, individually compressing each particular embedment of the embedding table to allow each respective embedment of the embedding table to be decompressed independently of any other embedment in the embedding table, packaging the embedding table comprising the individually compressed embeddings together with a machine learning model, receiving input for locating the embeddings in the embedding table, determining a lookup value based on the input to search the index of the embedding table, locating the embeddings based on searching the index of the embedding table for the determined lookup value, decompressing the located embeddings independently of any other embeddings in the embedding table, and processing the decompressed embeddings in connection with running the machine learning model.
Other aspects of the disclosure relate to various means, non-transitory computer-readable media, computer-implemented methods, user interfaces, and electronic devices. These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate embodiments of the disclosure and together with the description, serve to explain the principles of interest.
Drawings
A detailed discussion of embodiments directed to one of ordinary skill in the art is set forth in the specification, which makes reference to the accompanying drawings, in which:
FIG. 1A depicts a block diagram of an example system for providing efficient embedded table storage and lookup in a machine learning model, according to an example embodiment of the present disclosure.
FIG. 1B depicts a block diagram of an example embedding table for providing efficient embedding table storage and lookup in a machine learning model, according to an example embodiment of the present disclosure.
FIG. 2 depicts a flowchart of an example method for providing efficient embedded table storage for a machine learning model, according to an example embodiment of the present disclosure.
FIG. 3 depicts a block diagram of an example for providing efficient embedded table storage for a machine learning model, according to an example embodiment of the present disclosure.
FIG. 4 depicts a flowchart of an example method for providing efficient embedded table storage and lookup for a machine learning model, according to an example embodiment of the present disclosure.
FIG. 5 depicts a block diagram of an example for providing efficient embedded table storage and lookup for a machine learning model, according to an example embodiment of the present disclosure.
FIG. 6A depicts a block diagram of an example computing system performing efficient embedded table storage and lookup, according to an example embodiment of the present disclosure.
FIG. 6B depicts a block diagram of an example computing device performing efficient embedded table storage and lookup, according to an example embodiment of the present disclosure.
FIG. 6C depicts a block diagram of an example computing device performing efficient embedded table storage and lookup, according to an example embodiment of the present disclosure.
Repeated reference characters across the several figures are intended to identify identical features in the various embodiments.
Detailed Description
SUMMARY
In general, the present disclosure relates to providing efficient embedded table storage and lookup for machine learning models. Examples described in this disclosure are capable of efficiently storing and retrieving embedded tables for use with machine learning models. Such example embodiments provide advantages and improvements over existing methods, including, but not limited to, improved scalability, increased performance, and reduced computational requirements over existing methods.
The embedded tables are used as part of a machine learning model in machine learning. For example, the embedded tables may be used in a machine learning model associated with Natural Language Processing (NLP) or other various embodiments. The embedding table facilitates building abstract relationships between objects (e.g., words, phrases, images, songs, movies, etc.) and includes embedding that projects such objects from a high-dimensional space to a lower-dimensional space. For example, words, phrases, images, songs, movies, or any other type of object may be represented in an object space, such as a two-dimensional or three-dimensional coordinate system, where proximity between objects represents relevance between objects. In various examples, the object space may be any number of dimensions and is not limited to only two or three dimensions. For example, the object space may have more than three dimensions, more than ten dimensions, more than one hundred dimensions, or generally any number of dimensions. Further, for example, the embedded tables can be trained within a machine learning model, and the embedded tables can be reused across different machine learning models.
The embedded tables are stored as part of a machine learning model and can become very large in size. For example, the machine learning model may utilize one or more embedded tables, which may include thousands, millions, or any number of entries. Indeed, the embedded tables may represent a significant portion of the machine learning model size or an overwhelming portion. Furthermore, conventional machine learning platforms and operations require that the computing device load the full-sized embedded table of the machine learning model into memory at a time. As such, servicing large machine learning models on a large scale becomes increasingly complex and expensive because of the large amount of memory and computing resources required. Furthermore, such memory and processing requirements often exceed the limited capabilities of many types of mobile computing devices, which often lack the computer hardware required to process large models.
To address and solve these problems, the present disclosure provides examples of efficient embedded table storage and lookup for machine learning models. In some examples of the present disclosure, a computing system: performing an operation of generating or otherwise obtaining an embedding table comprising a plurality of embeddings respectively associated with corresponding indexes of the embedding table, individually compressing each particular embedment of the embedding table to allow each respective embedment of the embedding table to be decompressed independently of any other embedment in the embedding table, packaging the embedding table comprising the individually compressed embeddings together with a machine learning model, obtaining an input for locating the embeddings in the embedding table, determining a lookup value based on the input of the search index of the embedding table, searching the index of the embedding table for the determined lookup value, locating the embeddings, decompressing the located embeddings independently of any other embeddings in the embedding table, and processing the decompressed embeddings as part of running the machine learning model associated with the embedding table.
The systems, methods, and computer program products described herein provide a number of technical effects and benefits. For example, examples of the present disclosure describe specialized embedded table data structures and associated operations that allow a computing system to access specific entries of an embedded table while running a machine learning model without loading the entire embedded table into memory. As such, the examples described in this disclosure provide an efficient way of storing and retrieving data in embedded tables and associated machine learning models that utilize less computing resources (e.g., less processing power, less memory usage, less power consumption, less memory space, etc.) than conventional operations.
The systems, methods, and computer program products described in examples of the present disclosure are particularly well suited for embedded tables used in machine learning models associated with Natural Language Processing (NLP). Nonetheless, the methodologies described herein can be applied to a variety of technical applications that use embedded tables, including but not limited to: image recognition, image classification, image captions, scene segmentation, object detection, action recognition, semantic segmentation, speech detection, speech translation, speaker identity recognition, language prediction, text classification face recognition, and the like. The input data may include, for example, one or more of any type of image, video, text, voice, audio, sensor, and/or any other type or types of data. Further, examples of the present disclosure are not limited to use with embedded tables of machine learning models, and may generally be used with embedded tables or any other similar type of data structure.
Referring now to the drawings, example embodiments of the present disclosure will be discussed in more detail.
Example System providing efficient Embedded Table storage and lookup
FIG. 1A depicts a block diagram of an example system for providing efficient embedded table storage and lookup in a machine learning model, according to an example embodiment of the present disclosure.
The system 100 includes a computing device 110, a model 120 that utilizes one or more embedding tables 130, a compression operation 140, and a compression model 150 that includes one or more compression embedding tables 160. The system 100 also includes at least one network 170, a computing device 112 including a compression model 152 and a compression embedding table 162, and a computing device 114 including a compression model 154 and a compression embedding table 164.
Computing device 110 may generally be any type of computer device, such as a Personal Computer (PC), a laptop computer, a mobile phone, a tablet computer, a server computer, a wearable computing device, or any other type of computing device. Computing device 110 may run an Operating System (OS) that manages the hardware and software of the corresponding device. Computing device 110 may also run one or more machine learning platforms (not shown) that provide computer software, software development tools, software libraries, and/or Application Programming Interfaces (APIs) to develop, train, test, execute, and/or deploy computer software applications and associated data structures (e.g., machine learning models) configured to perform operations associated with machine learning and artificial intelligence. Examples of machine learning platforms may include, and are not limited to And->A machine learning platform. Computing device 110 may generally include one or more machine learning models, such as model 120.
Model 120 generally refers to a specialized data structure configured and trained to perform one or more types of machine learning tasks, such as prediction, detection, classification, recognition, and the like. In an example, the model 120 may be or otherwise include various machine learning models, such as a neural network (e.g., a deep neural network) or other types of machine learning models, including nonlinear models and/or linear models. The neural network can include a feed forward neural network, a recurrent neural network (e.g., a long and short term memory recurrent neural network), a convolutional neural network, or other form of neural network. Some example machine learning models are able to leverage attention mechanisms, such as self-attention. For example, some example machine learning models can include a multi-headed self-attention model (e.g., a transformer model). A further machine learning model (e.g., model 120) may include and utilize one or more embedded tables 130 for assisting in performing one or more types of machine learning tasks.
The embedded table 130 generally refers to a table or any other type of data structure for indexing and storing embedded collections associated with object groups. An index generally refers to an embedded unique value or key that can be used to find an associated particular input value. In some examples, each embedding may generally represent or otherwise include a set of one or more values, such as a value vector, that projects a particular object onto an object space (e.g., two-dimensional, three-dimensional, or any other type of object space having any dimension).
The embedded tables 130 may generally be used to build and store complex relationships between various types of inputs, such as words, phrases, images, video, text, and voice data. In one example, the embedding table 130 may be used to store the embeddings. For example, each word in the set of words may be associated with a learned representation defined as a numerical value in a vector determined based on the meaning of the word. In another example, each word in the set of words may be associated with a learned representation stored as a numerical group for each word based on the word origin or the language of each respective word. For example, word embedding may generally be associated with natural language processing in which words, phrases, and/or any other feature of word content are mapped to real vectors utilized by a machine learning model.
In general, embedding may be determined and used with respect to any type of input data. As such, embedding is not limited to words and text. Further, the embedding table 130 and associated embeddings can be trained, e.g., separate from and/or as part of a machine learning model. Furthermore, the embedded tables may be reused across different machine learning models.
The size of the embedded table 130 based on the large input set can become very large. For example, the embedded table 130 may account for a significant amount or even nearly the entire amount of the machine learning model size based on words in the vocabulary, such as a dictionary from one language or a collection of dictionaries each in a different language (e.g., 2, 10, 50, 100, or more). For example, some embedded tables 130 may include millions or even billions of entries. As such, the size of the model 120 can grow to hundreds of megabytes and even gigabytes. Moreover, such large models 120 become increasingly difficult to extend and often cannot run on computing devices with limited computing resources. For example, many types of mobile devices, including wearable devices, edge devices, medical devices, and/or other types of portable computing devices, often lack or otherwise require preservation of computer memory and computer hardware resources required to utilize the running model 120 of the large embedded table 130.
In general, the model 120 and the embedded table 130 can be compressed. For example, a machine learning platform may generally provide some platform-supported compression operation that supports compressing the entire model 120 and compressing each table in the associated embedded table 130 as a whole. However, to run such a model, the computer system must include and contribute sufficient memory and processing power to run the entire model 120 and the associated full-sized decompressed embedded tables 130. As such, standard compression does not address reducing computational requirements to help scale models 120 with large embedded tables 130 or run models 120 with large embedded tables 130 on devices with limited, minimal, or unavailable hardware resources.
In one example, one or more compression operations 140 may be performed to provide efficient embedded table 130 storage and lookup for the machine learning model. For example, one or more compression operations 140 may be used to compress individual rows in the embedded table 130 to allow the computing device to run the model 120 without loading and processing the entire decompressed embedded table 130 in memory. In some examples, the model 120 may also be compressed. For example, aspects of model 120 may be pruned, optimized, and/or compressed to generate compression model 150. In some examples, model 120 and/or compression model 150 may be represented, stored, analyzed, updated, and/or run generally as a graph or graph data structure.
The compression operation 140 may be performed independent of any underlying machine learning platform and/or using a compression scheme not available from the underlying machine learning platform. For example, the compression operations 140 are typically not supported by or otherwise provided by the underlying machine learning platform. Further, the associative lookup and decompression operations corresponding to the compression operation 140 may be included within and performed by the model 120 itself without using or referencing the underlying machine learning platform. As such, independent of any underlying machine learning platform, the associative lookup and decompression operations for accessing the individual compressed rows of the associative embedded table 130 may be self-contained and used as part of the run model 120. Thus, the model 120 with the large embedded table 130 may operate using significantly reduced memory and computing resources, thereby improving overall scalability and allowing the model 120 with the large embedded table 130 to operate on devices with limited hardware capabilities.
In one example, one or more compression operations 140 are used to compress the model 120 and/or the associated embedding table 130. For example, the computing system may receive, generate, or otherwise obtain one or more embedded tables 130 associated with the machine learning model 120. The computing system then uses the compression operation 140 to compress each particular embedding of the one or more embedding tables 130 individually to allow each respective embedding of the particular embedding table 130 to be decompressed independently of any other embedding in the particular embedding table 130. The computing system may then use the compression operation 140 to package the embedded one or more compressed embedded tables 160, each with separate compression, with the associated machine learning model. In some examples, compression embedding table 160 is packaged with compression model 150. For example, one or more aspects of model 120 may be pruned, optimized, and/or compressed using compression operations 140 to generate compression model 150. In some examples, the compression embedded table 160 may also be packaged with the model 120 itself that has not been compressed.
In one example, the compression model 150 and the associated compression embedded table 160 are packaged together and provided to one or more computer systems to be run. For example, the compression model 150 and compression embedding table 160 may be stored, deployed, transmitted, or otherwise transferred to one or more other locations that allow one or more computer systems to run the compression model 150 with the compression embedding table 160 and utilize the association operations accordingly. In some examples, compression model 150 and compression embedding table 160 may be stored locally to be run by the same computing device (e.g., computing device 110) that runs compression operation 140. In some examples, the compression model 150 and the compression embedded table 160 are transmitted to one or more other computing devices 112, 114 via a network 170.
The network 170 may be a public network (e.g., the Internet), a private network (e.g., a Local Area Network (LAN) or a Wide Area Network (WAN)), or any combination thereof. In an example, the network 170 may include the internet, one or more intranets, a wired network, a wireless network, and/or other suitable types of communication networks. The network 170 may also include a wireless telecommunication network (e.g., a cellular network) adapted to communicate with other communication networks, such as the internet. Further, the network 170 may include one or more short-range wireless networks.
Computing device 112 and computing device 114 may each be, respectively, any type of computer device, such as a Personal Computer (PC), a laptop computer, a mobile phone, a tablet computer, a server computer, a wearable computing device, or any other type of computing device. Computing device 112 and computing device 114 may each also run an Operating System (OS) and each may run one or more machine learning platforms of the same type or different types.
The computing device 112 includes a compression model 152 having an associated compression embedding table 162. Computing device 114 includes compression model 154 with associated compression embedding table 164. Compression model 152 and compression model 154 may be the same or different. Compression embedding table 162 and compression embedding table 164 may also be the same or different, whether compression model 152 and compression model 154 are the same or not.
In an example, each computing device 110, 112, 114 may run a respective compression model 150, 152, 154, each compression model 150, 152, 154 using a corresponding compression embedding table 160, 162, 164. In an example, a computing device, such as computing device 112, receives, loads, and/or otherwise obtains compressed embedded table 162. For example, the compression embedding table 162 may be associated with a compression model 160 stored on the computing device 112 or otherwise available to the computing device 112. In one example, the compression model 160 itself may be decompressed into a model 120 that utilizes a compression embedding table 162. In some examples, the model 120 or the compression model 160 may be run by one or more applications executed by the computing device 112.
In one example, the model 120 or compression model 150, 152, 154 may provide one or more lookup and decompression operations (not shown) configured to perform lookup and decompression on the corresponding compression embedding table 160, 162, 164 including the embedding of individual compressions. For example, the computing device 112 may run a compression model 152 that receives input for locating an embedding in a corresponding compression embedding table 162. The compression model 152 may then determine a lookup value for searching the index of the corresponding compressed embedding table 162 to locate the embedding associated with the input. For example, the compression model 152 may perform one or more operations on the input, or some derivative thereof, in accordance with determining a lookup value to compress an index in the embedded table 162 to locate based on the mapping of the input.
In one example, the compression model 152 uses the determined lookup value to search the index of the compression embedding table 162 to locate individually compressed embeddings. The compression model 152 then decompresses the individually compressed embeddings independent of any other embeddings in the embedment table to obtain decompressed values associated with the located individually compressed embeddings. As such, the compression model 152 may perform separate compressed embedded lookup and decompression within the corresponding compression embedding table 162 based on specific inputs without decompressing and/or loading into memory any other unrelated embedding or partial decompression of the compression embedding table 162.
In various examples, the compressed embedding table 162 stores separately compressed embeddings and indexes, where the separately compressed embeddings each correspond to one of the indexes. Further, the individually compressed embeddings may generally include or refer to any associated compressed storage unit including, but not limited to, individually compressed records, entries, rows, tuples, and/or any other type of logical or physical storage unit that preserves the individually compressed embeddings in the compressed embedment table 162.
FIG. 1B depicts a block diagram of an example embedding table for providing efficient embedding table storage and lookup in a machine learning model, according to an example embodiment of the present disclosure. The example embedding table 180 includes an embedding table data structure 182, an embedding table index 184, an embedding 186, an input value 188, and a separately decompressed embedded process 190.
In an example, the embedded table data structure 182 may be an embedded table that includes individually compressed embedments 186. For example, the embedded table data structure 182 may include a plurality of different rows or entries, each row or entry having an embedded table index 184 referencing a corresponding, individually compressed, embedded 186. In one example, each row in the embedded table data structure 182 includes an embedded table index 184 (index_a, index_b, index_c, etc.) referencing a corresponding, individually compressed, embedded 186.
In general, the embedded table index 184 may be a unique value or key for looking up an embedded 186 that includes one or more values. In some examples, each of the embeddings 186 includes a set of one or more values, such as a value vector, that projects a particular object into an object space (e.g., two-dimensional, three-dimensional, or any other type of object space having any number of dimensions). For example, each of the embedments 186 in the embedment table data structure 182 can include a set of values, where each value corresponds to a weight associated with a particular dimension (e.g., a measurable attribute). Typically, the embedded values may be set by default, provided from another source, generated as a result of training, updated as a result of training and/or retraining, and so forth.
In one example, each of the embedments 186 in the embedment table data structure 182 includes a numerical weight value associated with an "Animal" dimension of the input value 188, a numerical weight associated with a "Feline (Feline)" dimension of the input value 188, and so forth. As such, in the embedding table data structure 182, the set of values for each of the dimensions ("animal", "feline", "Canine", "complex", and "color") in a particular row generally represent that the embedding 186 is associated with a particular input value 188. For example, row #2 of the embedded table data structure 182 represents an embedded 186 that includes a set of values (-0.7, -0.5, -0.3, 0.9, 0.7), each value being associated with a respective dimension of an input value 188 corresponding to "Houses". In addition, the embedding 186 of row #2 of the embedded table data structure may be located and referenced among the various embedded table indexes 184 using "index_b", e.g., based on a mapping between the input values 188 of "index_b" and "house".
In an example, each of the embedments 186 in each row of the embedment table data structure 182 is compressed separately from any other embedment in any other row. For example, the embeddings 186 in row #1 of the embedment table data structure 182 may be compressed separately and separated from all other embeddings 186 in other rows #2 to #9 of the embedment table data structure 182. Similarly, each of the other embeddings 186 may also be compressed separately from any other embeddings 186 embedded in the table data structure 182. As such, each individual compressed embedding 186 of the embedding table data structure 182 may be accessed individually via the corresponding embedding table index 184 and decompressed independently of the other embeddings 186, e.g., to avoid decompressing, loading, and/or processing the entire embedding table data structure 182 in memory.
In an example, each embedded table index 184 of the embedded table data structure 182 is generated based on an input value 188. For example, an "index_A" embedded in the table data structure 182 may be generated based on an input value 188 of "Birds". In various examples, each input value 188 is deterministically mapped to a corresponding embedded table index 184. For example, one or more hashes and/or other operations are applied to the input values 188 to generate corresponding embedded table indexes 184. As such, each embedding table index 184 may be determined based on a particular input value 188 and used to locate and individually decompress associated embedments 186 in the embedding table data structure 182.
In an example, the embedded table index 184 is used to find the embedded 186 in the embedded table data structure 182 that is associated with separate decompression. In one example, one or more input values 188 are received for processing. For example, input values 188 of "house", "Cat (Cat)", "Kitten", "Dog (Dog)", and "Kitten" may be received. Each of these input values 188 may be processed separately, for example, using hashing and/or one or more other operations to determine a corresponding embedded table index 184. For example, the "house" input value 188 may be processed to determine the embedding table index 184 of "index_b" for use in searching the embedding table index 184 of the embedding table data structure 182 for the embedding 186 associated with "house".
In an example, the embedded table index 184 of "index_b" is found at row #2 of the embedded table data structure 182, where the associated embedded 186 is accessed and decompressed separately from other embedded in the embedded table data structure 182. The input values 188 for "cat", "kitten" and "dog" may be similarly processed to individually locate and individually decompress particular embedments 186 in the embedment table data structure 182. In various examples, the processing 190 of the individually decompressed embeddings may include, but is not limited to, averaging, concatenating, utilizing, returning, forwarding, and/or performing any other operations involving the individually decompressed embeddings 186 obtained from the embedment table data structure 182.
Example method of efficient embedded table storage and lookup in machine learning models
FIG. 2 depicts a flowchart of an example method for providing efficient embedded table storage for a machine learning model, according to an example embodiment of the present disclosure. Although fig. 2 depicts steps performed in a particular order for illustration and discussion, the methods of the present disclosure are not limited to the particular illustrated order or arrangement. The various steps of method 200 may be omitted, rearranged, combined, and/or adjusted in various ways without departing from the scope of the present disclosure.
At 202, a computing system obtains an embedded table associated with a machine learning model. In an example, the computing system 110 obtains one or more embedded tables 130. For example, the computing system 110 may receive the embedded table 130 or a location of the embedded table 130 as input. The computing system 110 may also automatically detect one or more embedded tables 130, for example, based on analyzing the association model 120 or the compression model 150 associated with the embedded tables 130.
In an example, the computing system 110 may receive input, such as a list or set of objects and associated embeddings for generating one or more embedment tables 130. For example, the computing system 110 may receive a list or collection of words in a vocabulary, such as dictionary words for a particular language. The computing system 110 may then use the list or set of words to generate an associated embedding table 130, e.g., based on existing or known embeddings, default or generic embeddings, and/or any type of embeddings that are generally associated with each respective entry of the embedding table 130. In some examples, the computing system 110 receives a plurality of different lists or sets of words, each list or set of words associated with a different language (e.g., 2, 10, 100, or more languages), wherein each list or set of words from a particular language is used to generate a corresponding embedded table 130. In such an example, the list or collection of each word in a particular language will have its own particular embedded table 130.
In an example, the computing system 110 may generate, convert, or otherwise update one or more indexes of the existing embedded table 130. For example, one or more entries of the embedding table 130 obtained or received by the computing system 110 may already have a respective index value associated with the corresponding embedding. In one example, the existing index value of the embedding table 130 associated with the vocabulary of words may be the actual word corresponding to the particular embedding or some derivative thereof (e.g., a tuple, a triplet, etc. of words). As such, the computing system 110 may determine each embedded index value based on existing index values present in the embedded table 130 or existing index values from one or more external sets or lists of values (e.g., words, objects, or any derivatives thereof) that are not present in the embedded table 130.
In an example, the computing system 110 determines one or more index values of the embedding table 130 based on applying a hash operation to each of the one or more values to be associated with a particular embedding from the embedding table 130. As such, the computing system may determine the new index value based on the existing index value from the existing embedded table 130 or based on a list or set of objects used to generate and/or augment the embedded table 130.
In an example, computing system 110 generates, converts, or otherwise updates the index value of embedded table 130 in association with performing compression operation 140. In various examples, one or more operations, including but not limited to a hash operation, are applied to the input values to generate indexes of the embedded table 130, each index being deterministically mapped to a corresponding input. In addition, any relevant hashing operation may use a hash seed. In some examples, one or more operations for generating the embedded table 130 index may map a plurality of different input values to the same index of the embedded table 130, thereby reducing the total number and size of entries of the embedded table 130. In some examples, the one or more operations for generating the embedded table 130 index may generally include one or more operations before and/or after a hash operation performed on any particular value.
At 204, the computing system individually compresses each particular embedding of the embedding table to allow individual decompression of each particular embedding independent of other embeddings in the embedding table. In an example, the computing system 110 uses the compression operation 140 to individually compress each of the one or more embeddings in the embedment table 130. For example, the compression operation 140 may be used to individually compress each respective insert in the insert table 130. In various examples, individual compression of each particular embedding generally allows each embedding or embedded entry to be decompressed and accessed individually, e.g., without decompressing and/or loading any other unrelated embedding from the embedding table 130 into memory.
In an example, the compression operation 140 is performed independent of any underlying machine learning platform. For example, the compression operation 140 may be performed by software separate from and not associated with the underlying machine learning platform. In various examples, such software may be provided, for example, as software applications, scripts, utilities, libraries, and/or tools that provide specialized operations not provided by the machine learning platform. Further, the compression operation 140 may generally provide one or more different compression schemes that are not available from or otherwise supported by the underlying machine learning platform. In some examples, the compression operation 140 may compress the embedding of the embedding table 130 alone, e.g., based on a selection of one or more available compression schemes, which may include, but is not limited to, quantization compression, k-means compression, and/or pruning compression. However, any such type of compression scheme or operation may generally be provided, including custom and/or later known compression operations, to generate the compression embedded table 160 independent of and separate from the underlying machine learning platform.
In an example, the compression operation 140 may compress each of the embeddings of the embedding table 130 individually such that each of the embedded entries of the embedding table is processed independently of the other embeddings of the embedding table 130. As such, the model 120 may perform self-contained lookup and decompression for each particular embedded entry in the embedded table 130 in a single operation without using the underlying machine learning platform. In some examples, the downstream lookup and decompression operations that compress the embedded table 160 are performed automatically by the model 120 itself. In some examples, such downstream lookup and decompression operations of the compression embedded table 160 may also be supported, for example, in whole or in part, by an accompanying lightweight software framework or library associated with the compression operation 140.
In some examples, the model 120 and/or the compression model 150 may be modified or adapted to decompress a particular embedding separately from other embeddings in the compression embedding table 160, for example, when the model 120, 150 is packaged with the compression embedding table 160 with separate compression embeddings. For example, such models 120, 150 may be modified, adapted, configured, or reconfigured (e.g., via compression operation 140) to directly decompress particular individually compressed embeddings as part of corresponding lookup operations performed on the compressed embedment table 160.
In various examples, the underlying machine learning platform may not generally recognize or be aware of the compression embedding table 160 or the particular embedded associative lookup and decompression performed by the model 120 or the compression model 150. As such, individual decompression of a particular embedding can be performed without activating the underlying machine learning platform operation, e.g., an immediate decompression of the entire compression embedding table 160 may otherwise be attempted, whether or not the underlying compression scheme is actually understood or supported by the underlying machine learning platform.
At 206, the computing system packages the embedded table with the individual compression with the machine learning model. In an example, the computing system 110 packages the compression embedding table 160 with the association model 120 or the compression model 150. For example, the computing system 110 may package the compression model 150 and the associated compression embedding table 160 together such that the compression embedding table 160 is combined with or becomes part of the compression model. In various examples, compression model 150 and compression embedding table 160 may be stored, deployed, transmitted, transferred, and/or run on each of one or more other different computing devices. For example, the compression model 150 and the compression embedding table 160 may be stored and run on the computing device 110. Further, one or more instances of the compression model 150 and compression embedding table 160 may be created or replicated and then transferred to run on each of the one or more other computing devices 112, 114.
FIG. 3 depicts a flowchart of an example method for providing efficient embedded table storage for a machine learning model, according to an example embodiment of the present disclosure. Although fig. 3 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particular illustrated order or arrangement. The various steps of method 300 may be omitted, rearranged, combined, and/or adjusted in various ways without departing from the scope of the present disclosure.
At 302, a computing system receives input for generating a set of values for each of one or more embedded features. In an example, the computing system 110 receives or otherwise obtains data for generating one or more embedded tables 130. For example, a corresponding embedding table 130 may be created for each of one or more embedded features extracted from or otherwise associated with text, audio, or visual input data.
In some examples, the embedded features may be determined or extracted directly from the input based on the analysis input. For example, a sequence of consecutive letters or words can be deduced from words in text or speech. In some examples, embedded features may also be provided or otherwise associated with the input. For example, audio or visual input can be provided for one or more categories or classification descriptors. In one example, video inputs may be categorized in one or more ways, including by rating, by genre, era, popularity, studio, and so forth.
In an example, one or more embedded features may be extracted or determined based on the input, wherein each set of values extracted or determined for the embedded features is used to generate a corresponding embedded table 130 for use by the machine learning model. For example, a first set of one or more values for a particular embedded feature, such as a tuple of language models, may be extracted or determined from the input data. A second set of one or more values for the different embedded features, such as a binary set of language models, may then be extracted or determined from the input data. In addition, one or more other value sets, such as triplets, quaternions, quintuples, etc., may be generated for each additional embedded feature. Further, each of the value sets associated with the respective embedded features (e.g., tuples, triples, etc.) may then be used to generate a corresponding embedded table 130 for use with the machine learning model 120. For example, a first embedded table 130 may be generated for a set of values associated with a tuple, a second embedded table 130 may be generated for another set of values associated with a tuple, and so on. In various examples, such embedded tables 130 may be used with one or more types of machine learning models, including, but not limited to, models 120 related to Natural Language Processing (NLP).
At 304, the computing system generates an embedding table 130 for each of one or more embedding features associated with the input. In an example, the computing system 110 may generate the embedding table 130 as a two-dimensional data structure that includes at least a plurality of indexes and a plurality of embeddings, wherein each embedment is referenced by a particular one of the indexes. In some examples, the embedded table 130 may store, reference, or utilize other data and/or features. For example, each row of the embedded table may include one or more fields in addition to the embedded table index and the embedment corresponding to the index.
In various examples, the computing system may generate one or more embedded tables 130 by creating a new embedded table 130 based on input data, based on data determined or extracted from the input data, based on existing embedded tables 130, and so on. In some examples, the computing system 110 may generate the embedded table 130 by creating a new embedded table 130 and inserting associated data accordingly, creating a copy of the existing embedded table 130 and updating the copy accordingly, and/or performing operations directly on the existing embedded table 130 to convert, augment, edit, or otherwise update data in the existing embedded table 130 with new information. In an example, the computing system 110 generates one or more respective embedded tables 130, each embedded table 130 corresponding to a particular embedded feature of the input, e.g., based on example operations performed at 306, 308, and 310, as discussed below.
At 306, the computing system determines an index for each of the embedded tables. In an example, the computing system 110 extracts and/or determines a set of values for each of a plurality of different embedded features, e.g., based on an analysis input. The computing system 110 then refers to the respective sets of values associated with the particular embedded features to generate the corresponding embedded tables 130.
In an example, the computing system 110 processes words from a vocabulary of a particular language. Computing system 110 analyzes the words and extracts or determines one or more embedded features associated with each word. For example, the computing system 110 may utilize n-tuple embedding features to generate an embedding table for a Natural Language Processing (NLP) machine learning model 120. In an example, the computing system 110 generates a set of values for each of the one or more embedded features based on the word input. For example, a set of alphabetic values may be generated for a tuple embedded feature. A second set of alphabetic combinations may be generated for the two-tuple embedded features. Further, a third set of alphabetical combinations may be generated for the triplet embedded feature.
In an example, the computing system 110 determines an index of a first embedding table corresponding to a tuple embedding feature by performing one or more hashes and/or other operations on each value in a set of values associated with the tuple embedding feature to generate an embedding table index that maps to an original tuple value. Continuing with the example, computing system 110 determines an index of a second embedding table corresponding to the binary embedding feature by performing one or more hashes and/or other operations on each value in the set of values associated with the binary embedding feature to generate an embedding table index that maps to the original binary value. Further, computing system 110 determines an index of a third embedding table corresponding to the triplet embedding feature by performing one or more hashes and/or other operations on each value in the set of values associated with the triplet embedding feature to generate an embedding table index that maps to the original triplet value.
In various examples, each determined index of the embedding table 130 is mapped to a value associated with a respective embedded feature. In addition, each of the indices may be used to find the associated embedment in a particular embedment table 130 and/or a compressed embedment table 160. In some examples, multiple different values may be mapped to the same index and associated embedment based on a mapping provided by one or more hashes and/or other operations, thus reducing the total number and size of entries of the embedment table 130.
At 308, the computing system stores the index and corresponding embedments in each respective embedment table. In an example, the computing system 110 stores an index determined from the input and its corresponding embedment in the embedment table 130. For example, computing system 110 may store a particular set of indices and corresponding embeddings associated with one embedded feature in one embedment table while storing a corresponding embedment of another particular set of indices and another embedded feature in a different embedment table 130. Further, one or more such embedded tables 130 may be used alone or in association with any number of machine learning models 120. In various examples, one or more embeddings of the embedding table may be provided, predetermined, default settings, learned as part of training a particular embedding table 130 with the model 120, and/or otherwise determined and updated.
At 310, the computing system compresses each particular row of each embedded table to allow each particular row of the respective embedded table to be decompressed independently of any other row in the respective embedded table. In an example, the computing system 110 compresses each of the embeddings in the embedment table 130 to be separately located and decompressed independently of any other embeddings. For example, the computing system 110 may compress separate embeddings for each of the one or more different embedment tables 130. In an example, each of the plurality of different embedding tables may be stacked and combined into a single unit (e.g., tensor) that may later be unpacked when the association model 120 or the compression model 150 performs an operation (e.g., a lookup operation) involving one or more of the respective corresponding compression embedding tables 160. In one example, such an embedded table stack may be provided by the compression operation 140. Furthermore, de-stacking of the embedded tables may be performed, for example, by the model 120 or the compression model 150.
In an example, the embedding of a particular embedding table 130 may be individually compressed to generate a compressed embedding table 160, whether such compression operations and/or associated compression schemes are available or supported by the underlying machine learning platform of any source computer and/or any target computing machine. As such, generally any type of compression, including custom and/or later known compression operations, may be used for the embedding of the individual compression embedding tables 130 to generate the compression embedding tables 160 that may be used by the model 120 or the compression model 150 independently and separately from any underlying machine learning platform. In some examples, the embedding of the embedding table 130 may be compressed using custom quantization compression, custom pruning compression, custom k-means compression, and/or using any custom or specialized compression that is not available or supported by the machine learning platform.
At 312, the computing system transmits each embedded table including individual compressed rows with associated models to one or more computing devices. In an example, the computing system 110 compresses the model 120 using the compression operation 140 and generates the compression model 150, the compression model 150 being packaged with one or more compression embedding tables 160. In some examples, generating the compression model and/or packaging the compression model with one or more compression embedding tables 160 may involve one or more operations that configure the compression model 150 to perform individual decompression of the embeddings based on the embedded compression scheme used to individually compress the compression embedding tables 160. For example, the compression model 150 may be adapted or otherwise configured or reconfigured to directly decompress particular individual compressed embeddings according to a particular compression scheme not supported by the underlying machine learning platform as part of a corresponding lookup operation performed on the compressed embedment table 160.
In an example, the compression model 150 and the associated compression embedded table 160 are packaged together and provided to one or more computer systems to be run. For example, the compression model 150 and compression embedding table 160 may be transmitted, deployed, or otherwise transferred from one computing device 110 to one or more other computing devices 112, 114 to be operated accordingly.
FIG. 4 depicts a flowchart of an example method for providing efficient embedded table storage and lookup for a machine learning model, according to an example embodiment of the present disclosure. Although fig. 4 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particular illustrated order or arrangement. The various steps of method 400 may be omitted, rearranged, combined, and/or adjusted in various ways without departing from the scope of the present disclosure.
At 402, a computing system obtains an embedded table that includes individually compressed embedments. In an example, a computing system, such as computing system 112 for example, obtains an embedded compression embedding table 162 that includes an index and individual compression. For example, the index of the compressed embedding table 162 may generally reference one of a plurality of individual compressed embeddings that can be decompressed independently of any other embeddings in the compressed embedding table 162. In some examples, computing system 112 may receive one or more compression embedded tables 162 packaged with compression model 152. Similarly, computing system 114 may receive one or more compression embedded tables 164 packaged with compression model 154.
At 404, the computing system receives input for processing according to an embedded table comprising individual compressions. In an example, the compression model 152 running on the computing system 112 receives input for processing using one or more compression embedding tables 162. For example, the compression model 152 may be a neural network model that uses the compression embedding table 162 to process inputs. In one example, the compression model 152 may be directed to one or more aspects of Natural Language Processing (NLP), such as language classification, speech recognition, speech segmentation, and the like. In addition, each of the one or more compressed embedding tables 162 may store word embeddings associated with words of a particular language. For example, different compression embedding tables 162 may each be used to store word embeddings from a variety of different languages.
At 406, the computing system determines a lookup value based on the input to search for an index associated with the embedded table. In an example, the compression model 152 uses the input to determine the lookup value that is used to find the embedding in the one or more compression embedding tables 162. In some examples, the compression model 152 analyzes the input and extracts one or more features from the input to use as lookup values for locating an embedding in one or more compression embedding tables 162.
In an example, the compression model 152 may analyze a word as input and extract one or more lookup values based on the word. For example, the compression model 152 may determine and extract the root of the word to use as a lookup value. In another example, the compression model 152 may determine and extract one or more tuples to use as a lookup value based on the word input. Similarly, the compression model 152 may determine and extract one or more tuples, triples, and/or one or more other clusters of values based on any embedded features to use as a lookup value based on word input. Although the above examples refer to words as example inputs, any type of data that generally represents any kind of information or content (e.g., audio, visual, text, etc.) may be used to determine a lookup value that is used to locate an embedding in the compressed embedding table 162 based on any one or more embedding characteristics.
At 408, the computing system locates the individually compressed embedments in the embedment table based on the determined lookup values. In an example, the compression model 152 locates the embedding in the associated compression embedding table 162 using, for example, a lookup value that has been determined from the input at 406. For example, the compression model 152 may search the index to find a corresponding embedding in the compression embedding table 162 using the determined lookup value or some variation of the determined lookup value based on the mapping associated with the index of the compression embedding table 162.
In an example, the compression model 152 processes the determined lookup value using one or more hashes and/or other operations associated with compressing the index of the embedded table 162. For example, the obtained index of the embedded table 162 may be associated with one or more hashes and/or other operations for mapping the input data to the embedded table index value. In some examples, such mapping may generally map different possible input values to the same index in the compressed embedded table 152 in some instances, thus reducing the total number of entries and reducing the size of the compressed embedded table 152. In some examples, the one or more hashes and/or other operations may include one or more data processing operations, which may be before and/or after any particular hash may be performed.
In an example, the compression model 152 performs one or more hashing operations on the determined lookup value to search the index of the compression embedded table 162. The compression model 152 then performs a lookup operation to search the index of the compression embedded table 162 using the results of one or more hashing operations on the determined lookup value. The compression model then locates one of the indices in the compression embedding table 162 that match the result of the hash operation to obtain the associated embedding.
At 410, the computing system decompresses the individually compressed embeddings independent of any other embeddings in the embedment table. In an example, the compression model 152 locates one of the indices in the compression embedding table 162, where a particular index corresponds to one of the individual compression embeddings of the compression embedding table 162. The compression model 152 decompresses a particular embedding by decompressing the particular embedding separately from any other embedding of the compression embedding table 162. In various examples, the compression model individually decompresses the corresponding entries of the compression embedded table 162 independent of the underlying machine learning platform, e.g., based on compression that is not available from or otherwise supported by the machine learning platform used to run the compression model 152. In some examples, the compression model 152 may be configured to perform embedded lookup of individual compression and individual decompression as part of a single operation. Furthermore, the compression model 152 may be configured such that the separately compressed embedded lookups and the separately decompressed are self-contained and invisible to the underlying machine learning platform or its user.
FIG. 5 depicts a flowchart of an example method for providing efficient embedded table storage and lookup for a machine learning model, according to an example embodiment of the present disclosure. Although fig. 5 depicts steps performed in a particular order for illustration and discussion as an example, the methods of the present disclosure are not limited to the particular illustrated order or arrangement. The various steps of method 500 may be omitted, rearranged, combined, and/or adjusted in various ways without departing from the scope of the present disclosure.
At 502, a computing system receives one or more embedded tables comprising individually compressed rows. In one example, a computing system (e.g., computing system 112) receives one or more compression embedded tables 162 packaged with compression model 152. In various examples, each compressed embedding table 162 stores individually compressed embeddings, each individually compressed embedment being associated with one of the indices of the corresponding compressed embedding table 162. Furthermore, each of the separately compressed embeddings may be decompressed separately from and without decompressing any other embeddings in the compressed embedment table.
In some examples, each of the one or more compressed embedding tables 162 is based on a particular embedding feature of the word embedding. For example, a set of one or more compressed embedding tables 162 may be associated with an n-tuple embedding feature. For example, one of the compressed embedding tables 162 may store an index and an embedding associated with a word tuple. The second one of the compressed embedding tables 162 may store an index and an embedding associated with the word bigram. In addition, a third one of the compressed embedding tables 162 may store an index and an embedding associated with the word triplet. While tuple, and triplet embedding characteristics are generally described in the examples above, the compressed embedding table 162 may be used for any type of embedding characteristics associated with any type of data and is not limited to text data or embedding characteristics of text data.
At 504, the computing system receives input for processing according to one or more embedded tables. In an example, the compression model 152 running on the computing system 112 receives input to process using one or more compression embedding tables 162. For example, the input may include any type of data associated with audio, visual, text, and/or any other type of content. In some examples, the input may be a word or phrase processed by a compression model 152 associated with Natural Language Processing (NLP). Further, each of the one or more compression embedding tables 162 may store word embeddings used by the compression model 152 in processing input.
In some examples, the compression model 152 may receive one or more inputs, such as words in a phrase or sentence, and process each of the words as an input. For example, each word in a sentence may be processed separately using one or more compressed embedding tables 162. For example, a single word, such as "hello," may be processed as an input using a tuple-based compression embedding table 162, and/or a triplet-based compression embedding table 162 (and/or any other type of embedding feature). In some examples, groupings of words that each include one or more words may also be processed as inputs.
At 506, the computing system extracts a set of one or more lookup values based on the input. In an example, the compression model 152 analyzes the input and extracts one or more embedded features from the input to use as a lookup value for locating the embedment in the compression embedding table 162. In one example, the compression model 152 may receive words such as "hello" and extract embedded features such as a tuple of "hello" (e.g., 'h', 'e', 'l', 'o'), "-hello", (e.g., 'he', 'el','ll', 'lo'), "-hello", (e.g., 'hel', 'ell', 'llo'), etc., to process using a corresponding compression embedded table 162 associated with each particular embedded feature. For example, a tuple of "hello" may be processed using a compressed embedded table 162 of a tuple, a tuple of "hello" may be processed using a compressed embedded table of another tuple, and so on. Further, the compression model 152 may generally process any number of inputs (e.g., tokens, entities, objects) based on any number of embedded features corresponding to the at least one compression embedded table 162. Further, any type of input may generally be processed based on any one or more embedded features.
At 508, the computing system determines an embedded table index for each of the lookup values. In an example, the compression model 152 processes each lookup value using one or more hashes and/or other operations to determine an embedded table index to locate in the compressed embedded table 162. For example, compression model 152 may process each particular tuple value determined for "hello" by applying one or more hashes and/or other operations to each tuple value, wherein the one or more hashes and/or other operations map the tuple value to an embedded table index value of a corresponding compressed embedded table 162. The compression model 152 may then use the determined embedding table index to locate an associated tuple embedding in the compression embedding table 162. In other examples, the tuples, triples, and/or any other embedding feature may be similarly processed to obtain one or more embedding table indices of the corresponding compressed embedding table 162.
At 510, the computing system obtains an embedding of each determined embedding table index based on decompressing only a particular corresponding row of the corresponding embedding table. In an example, the compression model 152 searches the index of the compressed embedded table 162 using the embedded table index value determined from the lookup value. For example, the compression model 152 locates one of the indices in the compressed embedding table that matches the embedding table index value determined from the lookup value, and decompresses the particular embedding of the compressed embedding table 162 associated with the located index alone. Specifically, the compression model 152 may decompress a particular individually compressed embedding of the compression embedding table 162 by decompressing the particular individually compressed embedding independently and without decompressing any other embedding of the compression embedding table 162.
In some examples, the compression model 152 locates and individually decompresses one or more embeddings based on the embedded features associated with the input. For example, the compression model 152 may determine an embedding table index as input for each tuple embedding feature value associated with the word "hello". The compression model 152 may then search the corresponding tuple compression embedding table 162 for each of the embedding table indices and decompress each of the located embeddings individually based on the matching compression embedding table 162 indices. The compression model may then process the separately decompressed embeddings from the compressed embedment table 162.
In some examples, the compression model 152 may locate and decompress the embeddings alone from other compression embedment tables 162 associated with other embedment features. For example, the compression model 152 may also determine an embedding table index for each tuple embedding feature value associated with the word "hello," search a different corresponding tuple compression embedding table 162 for each of those embedding table indexes, decompress each of the located embeddings individually based on the matching compression embedding table 162 indexes, and understand the compressed embeddings accordingly. In various examples, the compression model 152 may perform similar operations to locate and individually decompress the embeddings from any compression embedding table 162 corresponding to any embedding characteristics of any type of input.
At 512, the computing system processes the decompressed embedded data obtained from the one or more compressed embedded tables. In an example, the compression model 152 processes one or more groups of individually decompressed embedded data from one or more compression embedding tables 162. In various examples, compression model 152 runs the model using the decompressed embedded data and generates an output. In some examples, compression model 152 averages a plurality of different individually decompressed embeddings obtained from compression embedment table 162. In one example, the compression model 152 may average a plurality of decompressed embeddings, such a plurality of different numerical vectors being obtained for each of a plurality of different lookup values associated with the embedded feature. For example, the compression model 152 may average or calculate an embedded mean for a numerical vector obtained with a lookup value of any embedded feature associated with a tuple as an embedded feature, a triplet as an embedded feature, and/or based on any type of input. In some examples, the compression model 152 may concatenate multiple decompressed embeddings, which may be further processed and/or provided as an output of a particular operation or layer. In some examples, the compressed embedding model 152 may process such data by averaging, concatenating, and/or generally processing separately decompressed embedded data from different embedded features in any manner. Furthermore, the compression model 152 may process any number of individually decompressed embeddings in any manner to generate any type of output.
At 514, the computing system generates an output based on processing the decompressed embedded data obtained from each corresponding embedded table. In an example, the compression model 152 may provide decompressed embedded data as output. In one example, the decompressed embedded data may be provided as an average or calculated average of the decompressed embedded data obtained from one or more compressed embedded tables 162. In one example, such decompressed embedded data may be provided as combined or concatenated data. In some examples, compression model 152 provides predictions, classifications, determinations, and/or other results based on analyzing and processing decompressed embedded data obtained from one or more compressed embedded tables 162.
Example devices and systems
Fig. 6A depicts a block diagram of an example computing system 600 that provides efficient embedded table storage and lookup in a machine learning model, according to an example embodiment of the present disclosure. The example computing system 600 includes a user computing device 602, a server computing system 630, and a training computing system 650 communicatively coupled by a network 680.
The user computing device 602 can be any type of computing device, such as, for example, a personal computing device (e.g., a laptop or desktop), a mobile computing device (e.g., a smart phone or tablet), a game console or controller, a wearable computing device, an embedded computing device, or any other type of computing device.
The user computing device 602 includes one or more processors 612 and memory 614. The one or more processors 612 can be any suitable processing device (e.g., processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.) and can be one processor or multiple processors operatively connected. The memory 614 can include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, and the like, as well as combinations thereof. Memory 614 is capable of storing data 616 and instructions 618 that are executed by processor 612 to cause user computing device 602 to perform operations.
In some examples, the user computing device 602 can store or include one or more machine learning models 620. For example, the machine learning model 620 can be or can otherwise include various machine learning models, such as a neural network (e.g., deep neural network) or other types of machine learning models, including nonlinear models and/or linear models. The neural network can include a feed forward neural network, a recurrent neural network (e.g., a long and short term memory recurrent neural network), a convolutional neural network, or other form of neural network. Some example machine learning models are able to leverage attention mechanisms, such as self-attention. For example, some example machine learning models can include a multi-headed self-attention model (e.g., a transformer model). In some examples, the machine learning model includes and uses one or more embedded tables (not shown).
In some implementations, one or more machine learning models 620 can be received from the server computing system 630 over a network 680, stored in the memory 614, and then used or otherwise implemented by the one or more processors 612. In some implementations, the user computing device 602 is capable of implementing multiple parallel instances of a single machine learning model 620.
Additionally or alternatively, one or more machine learning models 640 can be included in or otherwise stored and implemented by a server computing system 630 in communication with the user computing device 602 in accordance with a client-server relationship. For example, the machine learning model 640 can be implemented by the server computing system 630 as part of a Web service. Accordingly, one or more machine learning models 620 and any associated embedded tables (not shown) can be stored and implemented at the user computing device 602, and/or one or more machine learning models 640 and any associated embedded tables (not shown) can be stored and implemented at the server computing system 630.
The user computing device 602 can also include one or more user input components 622 that receive user input. For example, the user input component 622 can be a touch-sensitive component (e.g., a touch-sensitive display screen or touchpad) that is sensitive to touch by a user input object (e.g., a finger or stylus). The touch sensitive component can be used to implement a virtual keyboard. Other example user input components include a microphone, a conventional keyboard, or other device through which a user can provide user input.
The server computing system 630 includes one or more processors 632 and memory 634. The one or more processors 632 can be any suitable processing device (e.g., a processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.) and can be one processor or multiple processors operatively connected. The memory 634 can include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, and the like, as well as combinations thereof. The memory 634 is capable of storing data 636 and instructions 638 that are executed by the processor 632 to cause the server computing system 630 to perform operations.
In some implementations, the server computing system 630 includes or is otherwise implemented by one or more server computing devices. In instances where the server computing system 630 includes multiple server computing devices, such server computing devices can operate in accordance with a sequential computing architecture, a parallel computing architecture, or some combination thereof.
As described above, the server computing system 630 can store or otherwise include one or more machine learning models 640. For example, the machine learning model 640 can be or can otherwise include various machine learning models. Example machine learning models include neural networks or other multi-layer nonlinear models. Example neural networks include feed forward neural networks, deep neural networks, recurrent neural networks, and convolutional neural networks. Some example machine learning models are able to leverage attention mechanisms, such as self-attention. For example, some example machine learning models can include a multi-headed self-attention model (e.g., a transformer model). In some examples, the machine learning model includes and uses one or more embedded tables (not shown).
The user computing device 602 and/or the server computing system 630 are capable of training the machine learning model 620 and/or the machine learning model 640, and/or any embedded tables (not shown) associated with the machine learning models 620, 640, via interactions with the training computing system 650 communicatively coupled over the network 680. The training computing system 650 can be separate from the server computing system 630 or can be part of the server computing system 630.
The training computing system 650 includes one or more processors 652 and memory 654. The one or more processors 652 can be any suitable processing device (e.g., a processor core, microprocessor, ASIC, FPGA, controller, microcontroller, etc.) and can be one processor or multiple processors that are operably connected. The memory 654 can include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, and the like, as well as combinations thereof. Memory 654 can store data 656 and instructions 658 that are executed by processor 652 to cause training computing system 650 to perform operations. In some implementations, the training computing system 650 includes or is otherwise implemented by one or more server computing devices.
Training computing system 650 can include training machine learning model 620 and/or machine learning model 640 and/or model trainer 660 of any embedded tables (not shown) associated with machine learning models 620, 640, stored at user computing device 602 and/or server computing system 630, using various training or learning techniques, such as, for example, back propagation of errors. For example, the loss function can be counter-propagated through the model to update one or more parameters of the model (e.g., a gradient based on the loss function). Various loss functions can be used, such as mean square error, likelihood loss, cross entropy loss, hinge loss, and/or various other loss functions. Gradient descent techniques can be used to iteratively update parameters over multiple training iterations.
In some implementations, performing back-propagation of the error can include performing truncated back-propagation of the transit time. Model trainer 660 can perform a variety of generalization techniques (e.g., weight decay, dropoff, etc.) to improve the generalization ability of the trained model. Further, model trainer 660 can train machine learning model 620 and/or machine learning model 640, and/or any embedded tables (not shown) associated with machine learning models 620, 640, based on a training data set (e.g., training data set 662).
In some implementations, the training examples can be provided by the user computing device 602 if the user has agreed. Thus, in such an implementation, the machine learning model 620 provided to the user computing device 602 can be trained by the training computing system 650 on user-specific data received from the user computing device 602. In some instances, this process can be referred to as personalizing the model.
Model trainer 660 includes computer logic for providing the desired functionality. Model trainer 660 can be implemented in hardware, firmware, and/or software that controls the processor. For example, in some implementations, model trainer 660 includes program files stored on a storage device, loaded into memory, and executed by one or more processors. In other implementations, model trainer 660 includes one or more sets of computer-executable instructions stored in a tangible computer-readable storage medium such as RAM, a hard disk, or an optical or magnetic medium.
The network 680 can be any type of communication network, such as a local area network (e.g., an intranet), a wide area network (e.g., the internet), or some combination thereof, and can include any number of wired or wireless links. In general, communications over network 680 can be performed using various communication protocols (e.g., TCP/IP, HTTP, SMTP, FTP), coding or formatting (e.g., HTML, XML), and/or protection schemes (e.g., VPN, secure HTTP, SSL) via any type of wired and/or wireless connection.
Examples provided in this disclosure may be used in association with machine learning models used in various tasks, applications, and/or use cases. Further, in accordance with examples of the present disclosure, such machine learning models may utilize one or more embedded tables and enable efficient embedded table storage and lookup.
In some implementations, the input to the machine learning model of the present disclosure can be image data. The machine learning model is capable of processing the image data to generate an output. For example, the machine learning model can process the image data to generate an image recognition output (e.g., recognition of the image data, potential embedding of the image data, encoded representation of the image data, hashing of the image data, etc.). As another example, the machine learning model can process the image data to generate an image segmentation output. As another example, the machine learning model can process image data to generate an image classification output. As another example, the machine learning model can process the image data to generate an image data modification output (e.g., a change in the image data, etc.). As another example, the machine learning model can process the image data to generate an encoded image data output (e.g., an encoded and/or compressed representation of the image data, etc.). As another example, the machine learning model can process image data to generate a scaled-up image data output. As another example, the machine learning model can process the image data to generate a prediction output.
In some implementations, the input of the machine learning model of the present disclosure can be text or natural language data. The machine learning model is capable of processing text or natural language data to generate an output. As an example, the machine learning model can process natural language data to generate a language encoded output. As another example, the machine learning model can process text or natural language data to generate a potential text-embedded output. As another example, the machine learning model can process text or natural language data to generate a translation output. As another example, the machine learning model can process text or natural language data to generate a classification output. As another example, the machine learning model can process text or natural language data to generate a text segmentation output. As another example, the machine learning model can process text or natural language data to generate semantic intent output. As another example, the machine learning model can process text or natural language data to generate scaled-up text or natural language output (e.g., text or natural language data of higher quality than the input text or natural language, etc.). As another example, the machine learning model can process text or natural language data to generate a predictive output.
In some implementations, the input to the machine learning model of the present disclosure can be speech data. The machine learning model is capable of processing speech data to generate an output. As an example, the machine learning model can process speech data to generate a speech recognition output. As another example, the machine learning model can process speech data to generate a speech translation output. As another example, the machine learning model can process the speech data to generate a potential embedded output. As another example, the machine learning model can process the speech data to generate an encoded speech output (e.g., an encoded and/or compressed representation of the speech data, etc.). As another example, the machine learning model can process speech data to generate scaled-up speech output (e.g., speech data of higher quality than the input speech data, etc.). As another example, the machine learning model can process the speech data to generate a textual representation output (e.g., a textual representation of the input speech data, etc.). As another example, the machine learning model can process speech data to generate a prediction output.
In some implementations, the input of the machine learning model of the present disclosure can be potentially encoded data (e.g., a potential spatial representation of the input, etc.). The machine learning model is capable of processing the potentially encoded data to generate an output. As an example, the machine learning model can process the potentially encoded data to generate the recognition output. As another example, the machine learning model may process the potentially encoded data to generate a reconstructed output. As another example, the machine learning model can process the potentially encoded data to generate a search output. As another example, the machine learning model can process the potentially encoded data to generate a reclustering output. As another example, the machine learning model can process the potentially encoded data to generate a prediction output.
In some implementations, the input to the machine learning model of the present disclosure can be statistical data. The machine learning model is capable of processing the statistical data to generate an output. For example, the machine learning model can process the statistical data to generate the recognition output. As another example, the machine learning model can process the statistical data to generate a prediction output. As another example, the machine learning model can process the statistical data to generate a classification output. As another example, the machine learning model may process the statistical data to generate a segmentation output. As another example, the machine learning model can process the statistical data to generate a visual output. As another example, the machine learning model can process the statistical data to generate a diagnostic output.
In some implementations, the input to the machine learning model of the present disclosure can be sensor data. The machine learning model is capable of processing the sensor data to generate an output. As an example, the machine learning model can process the sensor data to generate an identification output. As another example, the machine learning model can process the sensor data to generate a prediction output. As another example, the machine learning model can process the sensor data to generate a classification output. As another example, the machine learning model can process the sensor data to generate a segmented output. As another example, the machine learning model can process the sensor data to generate a visual output. As another example, the machine learning model can process the sensor data to generate a diagnostic output. As another example, the machine learning model can process the sensor data to generate a detection output.
In some cases, the machine learning model can be configured to perform tasks including encoding input data for reliable and/or efficient transmission or storage (and/or corresponding decoding). For example, the task may be an audio compression task. The input may comprise audio data and the output may comprise compressed audio data. In another example, the input includes visual data (e.g., one or more images and/or video), the output includes compressed visual data, and the task is a visual data compression task. In another example, the task may include generating an embedding for input data (e.g., input audio or visual data).
In some cases, the input includes visual data and the task is a computer visual task. In some cases, pixel data including one or more images is input, and the task is an image processing task. For example, the image processing task can be an image classification, wherein the output is a grouping of scores, each score corresponding to a different object class and representing the likelihood that one or more images depict an object belonging to that object class. The image processing task may be object detection, wherein the image processing output identifies one or more regions in the one or more images and, for each region, identifies a likelihood that the region depicts the object of interest. As another example, the image processing task can be image segmentation, wherein the image processing output defines a respective likelihood for each category of a predetermined set of categories for each pixel in the one or more images. For example, the class group can be foreground and background. As another example, the class group can be an object class. As another example, the image processing task can be depth estimation, where the image processing output defines a respective depth value for each pixel in the one or more images. As another example, the image processing task can be motion estimation, wherein the network input includes a plurality of images, and the image processing output defines for each pixel of one of the input images a motion of a scene depicted at a pixel between the images in the network input.
In some cases, the input includes audio data representing a spoken utterance and the task is a speech recognition task. The output may include a text output mapped to the spoken utterance. In some cases, the task includes encrypting or decrypting the input data. In some cases, tasks include microprocessor performance tasks such as branch prediction or memory address translation.
FIG. 6A illustrates one example computing system that can be used to implement the present disclosure. Other computing systems can also be used. For example, in some implementations, the user computing device 602 can include a model trainer 660 and a training data set 662. In such an implementation, the machine learning model 620 can be trained and used locally at the user computing device 602. In some such implementations, the user computing device 602 can implement the model trainer 660 to personalize the machine learning model 620 based on user-specific data.
Fig. 6B depicts a block diagram of an example computing device 682 that performs operations in accordance with an example embodiment of the present disclosure. The computing device 682 can be a user computing device or a server computing device.
Computing device 682 includes a plurality of applications (e.g., applications 1 through N). Each application contains its own machine learning library and machine learning model. For example, each application can include a machine learning model that can include and use one or more embedded tables. Example applications include text messaging applications, email applications, dictation applications, virtual keyboard applications, browser applications, and the like.
As illustrated in fig. 6B, each application is capable of communicating with a plurality of other components of the computing device, such as, for example, one or more sensors, a context manager, a device status component, and/or additional components. In some implementations, each application can communicate with each device component using an Application Programming Interface (API) (e.g., a public API). In some implementations, the APIs used by each application are specific to that application.
Fig. 6C depicts a block diagram of an example computing device 690, performed according to an example embodiment of the present disclosure. Computing device 690 can be a user computing device or a server computing device.
Computing device 690 includes multiple applications (e.g., applications 1 through N). Each application communicates with a central intelligent layer. Example applications include text messaging applications, email applications, dictation applications, virtual keyboard applications, browser applications, and the like. In some implementations, each application can communicate with the central intelligence layer (and models stored therein) using APIs (e.g., generic APIs across all applications).
The central intelligence layer includes a plurality of machine learning models. For example, as illustrated in fig. 6C, a respective machine learning model can be provided for each application and managed by a central intelligent layer. In other implementations, two or more applications can share a single machine learning model. For example, in some embodiments, the central intelligence layer can provide a single model for all applications. In some implementations, the central intelligence layer is included within or otherwise implemented by the operating system of the computing device 690.
The central intelligence layer is capable of communicating with the central device data layer. The central device data layer can be a central data store of computing devices 690. As illustrated in fig. 6C, the central device data layer can communicate with many other components of the computing device, such as, for example, one or more sensors, a context manager, a device status component, and/or additional components. In some implementations, the central device data layer can communicate with each device component using an API (e.g., a proprietary API).
Additional disclosure
The techniques discussed herein make reference to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from such systems. The flexibility inherent in computer-based systems allows for various possible configurations, combinations, and divisions of tasks and functions between and among components. For example, the processes discussed herein can be implemented using a single device or component or multiple devices or components operating in combination. Databases and applications can be implemented on a single system, or distributed across multiple systems. Distributed components can operate in series or in parallel.
While the present subject matter has been described in detail with respect to various specific example embodiments thereof, each example is provided by way of explanation and not limitation of the present disclosure. Modifications, variations and equivalents to these embodiments will readily occur to those skilled in the art upon attaining an understanding of the foregoing. Accordingly, the subject disclosure does not preclude inclusion of modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For instance, features illustrated or described as part of one embodiment, can be used with another embodiment to yield a still further embodiment. Accordingly, the present disclosure is intended to cover such modifications, variations, and equivalents.
Claims (20)
1. A computer-implemented method for performing efficient embedded table storage and lookup in a machine learning model, comprising:
obtaining, by one or more processors, an embedding table associated with a machine learning model, the embedding table comprising a plurality of embeddings respectively associated with corresponding indexes of the embedding table;
compressing, by the one or more processors, each particular embedding of the embedding table individually to allow each respective embedding of the embedding table to be decompressed independently of any other embedding in the embedding table; and
The embedded tables, including individually compressed embedments, are packaged with the machine learning model by the one or more processors.
2. The computer-implemented method of claim 1, further comprising:
corresponding indexes of the embedded table are updated by the one or more processors based on the hash operation.
3. The computer-implemented method of claim 1, further comprising:
the embedded table comprising individually compressed rows packaged with the machine learning model is provided to one or more computing devices by the one or more processors.
4. The computer-implemented method of claim 1, wherein the compressing is performed independent of a machine learning platform.
5. The computer-implemented method of claim 1, wherein the compressing is performed using a compression that is not available from a machine learning platform.
6. The computer-implemented method of claim 1, wherein the respective embedments in the embedment table are associated with an item of a plurality of items included in a vocabulary.
7. The computer-implemented method of claim 1, wherein the machine learning model is associated with an embedded layer of a neural network.
8. A computer-implemented method for performing efficient embedded table storage and lookup in a machine learning model, comprising:
obtaining, by one or more processors, an embedding table associated with a machine learning model, the embedding table comprising a plurality of individual compressed to allow each respective embedding to be decompressed independently of any other embedding in the embedding table;
receiving, by the one or more processors, input to locate an embedding in the embedding table;
determining, by the one or more processors, a lookup value based on the input for searching an index of the embedding table to locate an embedding;
locating, by the one or more processors, the embedding based on searching the index of the embedding table for the determined lookup value; and
decompressing, by the one or more processors, the embeddings independent of any other embeddings in the embedment table according to the positioning.
9. The computer-implemented method of claim 8, further comprising:
decompressed embeddings associated with running the machine learning model are processed by the one or more processors.
10. The computer-implemented method of claim 8, wherein determining the lookup value is based on a hash operation for mapping the input to one of the indexes in the embedded table.
11. The computer-implemented method of claim 8, wherein the respective embedments in the embedment table are associated with an item of a plurality of items included in a vocabulary.
12. The computer-implemented method of claim 8, wherein the decompressing is performed independently of a machine learning platform.
13. The computer-implemented method of claim 9, wherein the decompressing is performed using compression that is not available from a machine learning platform.
14. A computing system for performing efficient embedded table storage and lookup in a machine learning model:
one or more processors; and
one or more non-transitory computer-readable media that collectively store:
a machine learning model configured to receive model inputs and process the model inputs to generate model outputs, wherein the machine learning model comprises an embedding table comprising individually compressed embeddings to allow each respective embedment to be decompressed independently of any other embeddings in the embedding table, and wherein the machine learning model is configured to perform operations comprising:
Obtaining the model input for processing according to the embedded table;
determining a lookup value based on the model input for searching an index of the embedding table to locate the embedding;
locating the embedding based on searching the index of the embedding table for the determined lookup value; and
decompressing, by the one or more processors, the embeddings independent of any other embeddings in the embedment table according to the positioning.
15. The computing system of claim 14, further comprising:
the decompressed embeddings are processed with other corresponding decompressed embeddings from the embedment table.
16. The computing system of claim 14, wherein the operations further comprise:
the model output is generated based on processing the decompressed embedding.
17. The computing system of claim 14, wherein to determine the lookup value is based on a hash operation to map the model input to one of the indices in the embedded table.
18. The computing system of claim 14, wherein the respective embedments in the embedment table are associated with an item of a plurality of items included in a vocabulary.
19. The computing system of claim 14, wherein the decompressing is performed independently of a machine learning platform.
20. The computing system of claim 14, wherein the decompressing is performed using compression that is not available from a machine learning platform.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/147,844 | 2021-01-13 | ||
US17/147,844 US11599518B2 (en) | 2021-01-13 | 2021-01-13 | Efficient embedding table storage and lookup |
PCT/US2021/064355 WO2022154942A1 (en) | 2021-01-13 | 2021-12-20 | Efficient embedding table storage and lookup |
Publications (1)
Publication Number | Publication Date |
---|---|
CN116583844A true CN116583844A (en) | 2023-08-11 |
Family
ID=79730093
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180082619.XA Pending CN116583844A (en) | 2021-01-13 | 2021-12-20 | Efficient embedded table storage and lookup |
Country Status (4)
Country | Link |
---|---|
US (2) | US11599518B2 (en) |
EP (1) | EP4229525A1 (en) |
CN (1) | CN116583844A (en) |
WO (1) | WO2022154942A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN117076720A (en) * | 2023-10-18 | 2023-11-17 | 北京燧原智能科技有限公司 | Embedded table access method and device, electronic equipment and storage medium |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20220398377A1 (en) * | 2021-06-14 | 2022-12-15 | Capital One Services, Llc | Methods for Exact and Approximate Inverses of Neural Embedding Models |
CN117149813B (en) * | 2023-10-30 | 2024-02-09 | 苏州元脑智能科技有限公司 | Vector searching method and device for input features and model operation equipment |
Family Cites Families (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7656561B2 (en) * | 2004-05-31 | 2010-02-02 | Phase One A/S | Image compression for rapid high-quality imaging |
US10929294B2 (en) * | 2017-03-01 | 2021-02-23 | QC Ware Corp. | Using caching techniques to improve graph embedding performance |
CA3046475A1 (en) * | 2018-06-13 | 2019-12-13 | Royal Bank Of Canada | System and method for processing natural language statements |
US10872601B1 (en) * | 2018-09-27 | 2020-12-22 | Amazon Technologies, Inc. | Natural language processing |
US10873782B2 (en) * | 2018-10-02 | 2020-12-22 | Adobe Inc. | Generating user embedding representations that capture a history of changes to user trait data |
US11768874B2 (en) * | 2018-12-19 | 2023-09-26 | Microsoft Technology Licensing, Llc | Compact entity identifier embeddings |
US11354506B2 (en) * | 2019-07-30 | 2022-06-07 | Baidu Usa Llc | Coreference-aware representation learning for neural named entity recognition |
GB201914353D0 (en) * | 2019-10-04 | 2019-11-20 | Myrtle Software Ltd | Hardware Acceleration |
US20210158176A1 (en) * | 2019-11-22 | 2021-05-27 | Bank Of Montreal | Machine learning based database search and knowledge mining |
KR20210098247A (en) * | 2020-01-31 | 2021-08-10 | 삼성전자주식회사 | Electronic device and operating method for the same |
US11704558B2 (en) * | 2020-05-21 | 2023-07-18 | Servicenow Canada Inc. | Method of and system for training machine learning algorithm for object classification |
US11461680B2 (en) * | 2020-05-21 | 2022-10-04 | Sap Se | Identifying attributes in unstructured data files using a machine-learning model |
US11914670B2 (en) * | 2020-09-08 | 2024-02-27 | Huawei Technologies Co., Ltd. | Methods and systems for product quantization-based compression of a matrix |
-
2021
- 2021-01-13 US US17/147,844 patent/US11599518B2/en active Active
- 2021-12-20 CN CN202180082619.XA patent/CN116583844A/en active Pending
- 2021-12-20 EP EP21844867.8A patent/EP4229525A1/en active Pending
- 2021-12-20 WO PCT/US2021/064355 patent/WO2022154942A1/en active Application Filing
-
2023
- 2023-01-30 US US18/161,352 patent/US11892998B2/en active Active
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN117076720A (en) * | 2023-10-18 | 2023-11-17 | 北京燧原智能科技有限公司 | Embedded table access method and device, electronic equipment and storage medium |
CN117076720B (en) * | 2023-10-18 | 2024-02-02 | 北京燧原智能科技有限公司 | Embedded table access method and device, electronic equipment and storage medium |
Also Published As
Publication number | Publication date |
---|---|
US11599518B2 (en) | 2023-03-07 |
US20220222235A1 (en) | 2022-07-14 |
WO2022154942A1 (en) | 2022-07-21 |
US20230169058A1 (en) | 2023-06-01 |
EP4229525A1 (en) | 2023-08-23 |
US11892998B2 (en) | 2024-02-06 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN111368996B (en) | Retraining projection network capable of transmitting natural language representation | |
US11423233B2 (en) | On-device projection neural networks for natural language understanding | |
AU2018214675B2 (en) | Systems and methods for automatic semantic token tagging | |
WO2021042503A1 (en) | Information classification extraction method, apparatus, computer device and storage medium | |
US11892998B2 (en) | Efficient embedding table storage and lookup | |
US11182433B1 (en) | Neural network-based semantic information retrieval | |
CN111753060A (en) | Information retrieval method, device, equipment and computer readable storage medium | |
CN113239700A (en) | Text semantic matching device, system, method and storage medium for improving BERT | |
JP2021528796A (en) | Neural network acceleration / embedded compression system and method using active sparsification | |
Sun et al. | Ensemble softmax regression model for speech emotion recognition | |
JP7417679B2 (en) | Information extraction methods, devices, electronic devices and storage media | |
CN111241828A (en) | Intelligent emotion recognition method and device and computer readable storage medium | |
CN112632258A (en) | Text data processing method and device, computer equipment and storage medium | |
WO2022228127A1 (en) | Element text processing method and apparatus, electronic device, and storage medium | |
CN111368554B (en) | Statement processing method, device, computer equipment and storage medium | |
CN109902162B (en) | Text similarity identification method based on digital fingerprints, storage medium and device | |
CN112307738A (en) | Method and device for processing text | |
CN112749251B (en) | Text processing method, device, computer equipment and storage medium | |
CN114328894A (en) | Document processing method, document processing device, electronic equipment and medium | |
CN114692610A (en) | Keyword determination method and device | |
US11983502B2 (en) | Extracting fine-grained topics from text content | |
US20230161964A1 (en) | Extracting fine-grained topics from text content | |
WO2021237082A1 (en) | Neural network-based semantic information retrieval | |
WO2022240391A1 (en) | Incorporation of decision trees in a neural network | |
Persson | Studies in Semantic Modeling of Real-World Objects using Perceptual Anchoring |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |