JP6511539B2 - Capturing and Rendering Panoramic Virtual Reality Content - Google Patents
Capturing and Rendering Panoramic Virtual Reality Content Download PDFInfo
- Publication number
- JP6511539B2 JP6511539B2 JP2017550745A JP2017550745A JP6511539B2 JP 6511539 B2 JP6511539 B2 JP 6511539B2 JP 2017550745 A JP2017550745 A JP 2017550745A JP 2017550745 A JP2017550745 A JP 2017550745A JP 6511539 B2 JP6511539 B2 JP 6511539B2
- Authority
- JP
- Japan
- Prior art keywords
- camera
- image
- images
- rig
- view
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000009877 rendering Methods 0.000 title description 24
- 238000000034 method Methods 0.000 claims description 92
- 230000015654 memory Effects 0.000 claims description 36
- 238000004590 computer program Methods 0.000 claims description 19
- 230000009471 action Effects 0.000 claims description 15
- 230000008859 change Effects 0.000 claims description 8
- 230000000694 effects Effects 0.000 claims description 7
- 230000004044 response Effects 0.000 claims description 5
- 210000001747 pupil Anatomy 0.000 claims description 3
- 230000003287 optical effect Effects 0.000 description 33
- 238000004891 communication Methods 0.000 description 23
- 230000006870 function Effects 0.000 description 23
- 238000010586 diagram Methods 0.000 description 18
- 230000008569 process Effects 0.000 description 15
- 238000012545 processing Methods 0.000 description 14
- 238000012937 correction Methods 0.000 description 12
- 238000003702 image correction Methods 0.000 description 10
- 238000003384 imaging method Methods 0.000 description 9
- 210000003128 head Anatomy 0.000 description 7
- 238000002156 mixing Methods 0.000 description 5
- 230000009466 transformation Effects 0.000 description 5
- 238000004422 calculation algorithm Methods 0.000 description 4
- 238000004364 calculation method Methods 0.000 description 4
- 238000005070 sampling Methods 0.000 description 4
- 238000005516 engineering process Methods 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 238000012805 post-processing Methods 0.000 description 3
- 230000001360 synchronised effect Effects 0.000 description 3
- 230000001131 transforming effect Effects 0.000 description 3
- 238000004458 analytical method Methods 0.000 description 2
- 230000008901 benefit Effects 0.000 description 2
- 230000001413 cellular effect Effects 0.000 description 2
- 230000007423 decrease Effects 0.000 description 2
- 239000004973 liquid crystal related substance Substances 0.000 description 2
- 238000013507 mapping Methods 0.000 description 2
- 238000005259 measurement Methods 0.000 description 2
- 230000000007 visual effect Effects 0.000 description 2
- 238000013459 approach Methods 0.000 description 1
- 230000003750 conditioning effect Effects 0.000 description 1
- 230000003247 decreasing effect Effects 0.000 description 1
- 238000002594 fluoroscopy Methods 0.000 description 1
- 239000012634 fragment Substances 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 230000007257 malfunction Effects 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 230000003278 mimic effect Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 230000002093 peripheral effect Effects 0.000 description 1
- 238000007781 pre-processing Methods 0.000 description 1
- 230000000750 progressive effect Effects 0.000 description 1
- 230000001179 pupillary effect Effects 0.000 description 1
- 230000002441 reversible effect Effects 0.000 description 1
- 230000011218 segmentation Effects 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 238000000926 separation method Methods 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000006641 stabilisation Effects 0.000 description 1
- 238000011105 stabilization Methods 0.000 description 1
- 230000001502 supplementing effect Effects 0.000 description 1
- 238000010408 sweeping Methods 0.000 description 1
- 239000010409 thin film Substances 0.000 description 1
- 238000000844 transformation Methods 0.000 description 1
- 239000013598 vector Substances 0.000 description 1
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/20—Image signal generators
- H04N13/204—Image signal generators using stereoscopic image cameras
- H04N13/239—Image signal generators using stereoscopic image cameras using two 2D image sensors having a relative position equal to or related to the interocular distance
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/017—Head mounted
- G02B27/0172—Head mounted characterised by optical features
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T19/00—Manipulating 3D models or images for computer graphics
- G06T19/006—Mixed reality
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T3/00—Geometric image transformation in the plane of the image
- G06T3/40—Scaling the whole image or part thereof
- G06T3/4038—Scaling the whole image or part thereof for image mosaicing, i.e. plane images composed of plane sub-images
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/10—Processing, recording or transmission of stereoscopic or multi-view image signals
- H04N13/106—Processing image signals
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/20—Image signal generators
- H04N13/204—Image signal generators using stereoscopic image cameras
- H04N13/243—Image signal generators using stereoscopic image cameras using three or more 2D image sensors
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/30—Image reproducers
- H04N13/332—Displays for viewing with the aid of special glasses or head-mounted displays [HMD]
- H04N13/344—Displays for viewing with the aid of special glasses or head-mounted displays [HMD] with head-mounted left-right displays
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/30—Image reproducers
- H04N13/366—Image reproducers using viewer tracking
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/0101—Head-up displays characterised by optical features
- G02B2027/0138—Head-up displays characterised by optical features comprising image capture systems, e.g. camera
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/0101—Head-up displays characterised by optical features
- G02B2027/014—Head-up displays characterised by optical features comprising information/image processing systems
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/0179—Display position adjusting means not related to the information to be displayed
- G02B2027/0187—Display position adjusting means not related to the information to be displayed slaved to motion of at least a part of the body of the user, e.g. head, eye
Description
関連出願の相互参照
本願は、２０１５年５月２７日に出願され「Capture and Render of Panoramic Virtual Reality Content（パノラマバーチャルリアリティコンテンツのキャプチャおよびレンダリング）」と題された米国特許出願連続番号第１４／７２３，１５１号ならびに２０１５年５月２７日に出願され「Omnistereo Capture and Render of Panoramic Virtual Reality Content（パノラマバーチャルリアリティコンテンツの全方位ステレオキャプチャおよびレンダリング）」と題された米国特許出願連続番号第１４／７２３，１７８号に基づく優先権および利益を主張し、上記両出願の開示を本明細書に引用により援用する。
Cross-Reference to Related Applications This application is related to US Patent Application Serial No. 14/723, filed May 27, 2015, entitled "Capture and Render of Panoramic Virtual Reality Content". No. 151 and US patent application Ser. No. 14/723, filed on May 27, 2015, entitled "Omnistereo Capture and Render of Panoramic Virtual Reality Content". No. 178, and the disclosures of both of the above applications are incorporated herein by reference.
技術分野
本説明は概してパノラマを生成することに関する。特に、本説明は、バーチャルリアリティ（ＶＲ）環境でキャプチャした画像から立体パノラマを生成して表示することに関する。
TECHNICAL FIELD The present description relates generally to generating a panorama. In particular, the present description relates to generating and displaying stereoscopic panoramas from images captured in a virtual reality (VR) environment.
背景
パノラマ写真撮影技術を画像およびビデオに対して用いて、あるシーンの幅広いビューを提供することができる。従来から、パノラマ写真撮影技術および撮像技術を用いて、従来のカメラで撮影された多数の隣接写真からパノラマ画像を得ることができる。当該写真を互いに整列してマウント処理して、パノラマ画像を得ることができる。
BACKGROUND Panoramic photography techniques can be used on images and videos to provide a broad view of a scene. Traditionally, panoramic photography and imaging techniques can be used to obtain panoramic images from a large number of adjacent photos taken with a conventional camera. The photographs can be aligned with one another and mounted to obtain a panoramic image.
概要
１つの一般的な局面において、コンピュータによって実行される方法は、コンピューティングデバイスにおいて、キャプチャされた画像に基づいて１セットの画像を規定することと、コンピューティングデバイスにおいて、バーチャルリアリティ（ＶＲ）ヘッドマウントディスプレイのユーザと関連付けられているビュー方向を受信することと、コンピューティングデバイスにおいて、ビュー方向の変更の指示を受信することとを含み得る。当該方法はさらに、指示を受信したことに応答して、コンピューティングデバイスによって１セットの画像の一部の再投影を構成することを含み得、再投影は、変更されたビュー方向、およびキャプチャされた画像と関連付けられている視野に少なくとも部分的に基づいており、当該方法はさらに、再投影を用いて、一部を球面透視投影から平面透視投影に変換することを含み得る。当該方法はさらに、コンピューティングデバイスによって、ＶＲヘッドマウントディスプレイにおいて表示するために、再投影に基づいて更新されたビューをレンダリングすることを含み得、更新されたビューは、歪みを補正して一部内にステレオ視差を提供するように構成されており、当該方法はさらに、変更されたビュー方向に対応するステレオパノラマシーンを含む更新されたビューをヘッドマウントディスプレイに提供することを含み得る。本局面の他の実施形態は、対応するコンピュータシステム、装置、および１つ以上のコンピュータ記憶装置上に記録されたコンピュータプログラムを含み、その各々が当該方法のアクションを実行するように構成されている。
SUMMARY In one general aspect, a computer-implemented method comprises, at a computing device, defining a set of images based on captured images, and, in the computing device, a virtual reality (VR) head Receiving may include receiving a view orientation associated with the user of the mounted display, and receiving at the computing device an indication of a change in view orientation. The method may further include configuring a reprojection of a portion of the set of images by the computing device in response to receiving the indication, the reprojection being modified the view orientation, and Based at least in part on the field of view associated with the image, the method may further include transforming a portion from spherical perspective projection to planar perspective projection using reprojection. The method may further include rendering the updated view based on the reprojection for display by the computing device on the VR head mounted display, the updated view correcting the distortion to within , And the method may further include providing the head mounted display with an updated view that includes a stereo panoramic scene corresponding to the changed view direction. Other embodiments of this aspect include corresponding computer systems, devices, and computer programs recorded on one or more computer storage devices, each of which is configured to perform an action of the method. .
別の一般的な局面において、装置は、少なくとも１つのプロセッサと、命令を格納しているメモリとを有するシステムを含み、命令は、少なくとも１つのプロセッサによって実行されるとシステムに動作を実行させ、動作は、カメラの少なくとも１つのステレオペアから収集されたキャプチャ済のビデオストリームに基づいて１セットの画像を規定することと、１セットの画像内のオプティカルフローを計算して、１セットの画像の一部ではない画像フレームを補間することと、画像フレームを１セットの画像内にインターリーブし、オプティカルフローに少なくとも部分的に基づいて画像フレームと１セットの画像とを互いにスティッチングすることと、画像フレームおよび１セットの画像を用いて、全方位ステレオパノラマをＶＲヘッドマウントディスプレイにおいて表示するために生成することとを含む。本局面の他の実施形態は、対応するコンピュータシステム、装置、および１つ以上のコンピュータ記憶装置上に記録されたコンピュータプログラムを含み、その各々が当該方法のアクションを実行するように構成されている。 In another general aspect, an apparatus includes a system having at least one processor and a memory storing instructions, the instructions causing the system to perform an action when executed by the at least one processor. The operation comprises defining a set of images based on captured video streams collected from at least one stereo pair of cameras, and calculating an optical flow within the set of images to obtain a set of images Interpolating image frames that are not part of, interleave the image frames into a set of images, stitching together the image frames and the set of images based at least in part on the optical flow, the images VR head with omnidirectional stereo panorama using frame and one set of images And generating for display at the count display. Other embodiments of this aspect include corresponding computer systems, devices, and computer programs recorded on one or more computer storage devices, each of which is configured to perform an action of the method. .
１つの一般的な局面において、方法は、カメラリグ上に配列されたカメラのステレオペアを用いて、カメラリグの周囲のシーンと関連付けられている画像を同時にキャプチャしつつ、カメラリグを第１の方向に回転させることと、ステレオペアを用いて、シーンと関連付けられている付加的な画像を同時にキャプチャしつつ、カメラリグを第２の方向に回転させることとを含み得、カメラリグは円形であり、キャプチャ時に、カメラリグのベースと平行なアーク運動で動くように構成されており、ステレオペアは、カメラリグのベースの中心の両側にオフセットしているビュー方向を有して設置されており、ステレオペアはほぼ人間の瞳孔間距離をおいて配列されている。本局面の他の実施形態は、対応するコンピュータシステム、装置、および１つ以上のコンピュータ記憶装置上に記録されたコンピュータプログラムを含み、その各々が当該方法のアクションを実行するように構成されている。 In one general aspect, a method rotates a camera rig in a first direction while simultaneously capturing an image associated with a scene around the camera rig using a stereo pair of cameras arranged on the camera rig And rotating the camera rig in a second direction while simultaneously capturing additional images associated with the scene using a stereo pair, the camera rig being circular, and at capture time, The camera rig is configured to move in an arc motion parallel to the base of the camera rig, and the stereo pair is installed with the view direction offset to either side of the center of the camera rig base, the stereo pair being approximately human It is arranged at the interpupillary distance. Other embodiments of this aspect include corresponding computer systems, devices, and computer programs recorded on one or more computer storage devices, each of which is configured to perform an action of the method. .
１つの一般的な局面において、コンピュータによって実行される方法は、コンピューティングデバイスにおいて、回転可能なカメラリグからキャプチャされたコンテンツを表わす１セットの画像を受信することと、コンピューティングデバイスを用いて、画像内の画像フレームの部分を選択することとを含み得、画像フレームは、カメラリグのベースの外向きエッジから約半径１メートルから、カメラリグのベースの外向きエッジから約半径５メートルの距離をおいてカメラリグでキャプチャされたコンテンツを含んでおり、当該方法はさらに、画像フレームの部分同士を互いにスティッチングして立体パノラマビューを生成することを含み得、スティッチングは、部分を、部分内の少なくとも１つの他の画像フレームに一致させることに少なくとも部分的に基づいており、スティッチングは、カメラリグの直径に少なくとも部分的に基づいて選択されたスティッチング比を用いて実行され、当該方法はさらに、ビューをヘッドマウントディスプレイに提供することを含み得る。本局面の他の実施形態は、対応するコンピュータシステム、装置、および１つ以上のコンピュータ記憶装置上に記録されたコンピュータプログラムを含み、その各々が当該方法のアクションを実行するように構成されている。 In one general aspect, a computer implemented method comprises receiving, at a computing device, a set of images representing captured content from a rotatable camera rig, and using the computing device And selecting a portion of the image frame within, the image frame being at a distance of approximately 5 meters from the outward edge of the base of the camera rig from the approximately 1 meter radius from the outward edge of the base of the camera rig Camera rig includes captured content, and the method may further include stitching together portions of the image frame to generate a stereoscopic panoramic view, the stitching including: at least one portion within the portion To match two other image frames Based partially on a cloud, stitching is performed using a stitching ratio selected based at least in part on the diameter of the camera rig, and the method further comprises providing a view to the head mounted display May be included. Other embodiments of this aspect include corresponding computer systems, devices, and computer programs recorded on one or more computer storage devices, each of which is configured to perform an action of the method. .
１つの一般的な局面において、コンピュータによって実行される方法は、コンピューティングデバイスにおいて、キャプチャされた画像に基づいて１セットの画像を規定することと、１セットの画像の一部と関連付けられている複数のビュー光線を、曲線経路の周りに配列された複数の視点から１つの視点に再キャストすることによって、コンピューティングデバイスにおいて、１セットの画像の一部を透視画像平面から画像球面上に投影することと、コンピューティングデバイスにおいて、１つの視点に対応する周囲境界を求め、周囲境界の外部の画素を除去することによって更新画像を生成することとを含む。当該方法はさらに、周囲境界の範囲内の更新画像を表示するために提供することを含み得る。本局面の他の実施形態は、対応するコンピュータシステム、装置、および１つ以上のコンピュータ記憶装置上に記録されたコンピュータプログラムを含み、その各々が当該方法のアクションを実行するように構成されている。 In one general aspect, a computer-implemented method comprises, at a computing device, defining a set of images based on captured images and being associated with a portion of the set of images In the computing device, project a portion of a set of images from the perspective image plane onto the image sphere by recasting the view rays from the viewpoints arranged around the curvilinear path to one viewpoint And, at the computing device, determining an ambient boundary corresponding to one viewpoint and generating an updated image by removing pixels outside the ambient boundary. The method may further include providing for displaying the updated image within the perimeter boundary. Other embodiments of this aspect include corresponding computer systems, devices, and computer programs recorded on one or more computer storage devices, each of which is configured to perform an action of the method. .
別の一般的な局面において、撮像システムが含まれており、当該撮像システムはステレオパノラマを生成するように構成されている。当該システムは、規定された１セットの画像から補間された画像を提供し、補間された画像を１セットの画像内にインターリーブして、ステレオパノラマのための付加的なバーチャルコンテンツを生成するように構成された補間モジュールと、１セットの画像を平面透視投影から球面投影に投影するように構成された投影モジュールと、１セットの画像を調整して非円形カメラ軌道を補償するように構成されたキャプチャ補正モジュールとを含み得る。当該システムはさらに、１セットの画像から、および補間された画像から、画像の一部をサンプリングし、画像の一部同士を互いに混合して少なくとも１つの画素値を生成し、画素値を左シーンおよび右シーン内に構成することによってビデオコンテンツを含む三次元立体パノラマを生成するように構成されたスティッチングモジュールを含み得る。撮像システムはさらに、１セットの画像のオプティカルフローを推定して歪みを無くすように構成された画像補正モジュールを含む。本局面の他の実施形態は、対応するコンピュータシステム、装置、および１つ以上のコンピュータ記憶装置上に記録されたコンピュータプログラムを含み、その各々が当該方法のアクションを実行するように構成されている。 In another general aspect, an imaging system is included that is configured to generate a stereo panorama. The system provides an interpolated image from a defined set of images and interleaves the interpolated image into a set of images to generate additional virtual content for stereo panoramas. A configured interpolation module, a projection module configured to project a set of images from planar perspective projection to spherical projection, and a set of images adapted to compensate for non-circular camera trajectory And a capture correction module. The system further samples portions of the image from the set of images and from the interpolated image, mixing portions of the images together to generate at least one pixel value, and the pixel values to the left scene And a stitching module configured to generate a three-dimensional stereoscopic panorama including video content by configuring in the right scene. The imaging system further includes an image correction module configured to estimate the optical flow of the set of images to eliminate distortion. Other embodiments of this aspect include corresponding computer systems, devices, and computer programs recorded on one or more computer storage devices, each of which is configured to perform an action of the method. .
別の一般的な局面において、コンピュータによって実行される方法は、コンピューティングデバイスにおいて、キャプチャされた画像に基づいて１セットの画像を規定することと、コンピューティングデバイスにおいて、（推定されたオプティカルフローを用いて）、１セットの画像を正距円筒図法のビデオストリームにスティッチングすることと、第１のビューおよび第２のビューについてビデオストリームを平面透視から正距円筒図法透視に投影することによって、コンピューティングデバイスにおいて、ビデオストリームを再生するためにレンダリングすることと、コンピューティングデバイスにおいて、歪みが予め規定された閾値より高い境界を求めることとを含み、歪みは、ビデオストリームを投影することに少なくとも部分的に基づいている。当該方法はさらに、境界によって規定される内部の、および当該内部の外の、１セットの画像内の画像コンテンツを除去することによって、コンピューティングデバイスにおいて、更新されたビデオストリームを生成することと、更新されたビデオストリームを表示するために提供することとを含み得る。本局面の他の実施形態は、対応するコンピュータシステム、装置、および１つ以上のコンピュータ記憶装置上に記録されたコンピュータプログラムを含み、その各々が当該方法のアクションを実行するように構成されている。１つの一般的な局面において、コンピュータによって実行される方法は、コンピューティングデバイスにおいて、キャプチャされた画像に基づいて１セットの画像を規定することと、コンピューティングデバイスにおいて、バーチャルリアリティ（ＶＲ）ヘッドマウントディスプレイのユーザと関連付けられているビュー方向を受信することと、コンピューティングデバイスにおいて、ビュー方向の変更の指示を受信することとを含み得る。当該方法はさらに、指示を受信したことに応答して、コンピューティングデバイスによって１セットの画像の一部の再投影を構成することを含み得、再投影は、変更されたビュー方向、およびキャプチャされた画像と関連付けられている視野に少なくとも部分的に基づいており、当該方法はさらに、再投影を用いて、当該一部を球面透視投影から平面透視投影に変換することを含み得る。当該方法はさらに、コンピューティングデバイスによって、ＶＲヘッドマウントディスプレイにおいて表示するために、再投影に基づいて更新されたビューをレンダリングすることを含み得、更新されたビューは、歪みを補正して当該一部内にステレオ視差を提供するように構成されており、当該方法はさらに、変更されたビュー方向に対応するステレオパノラマシーンを含む更新されたビューをヘッドマウントディスプレイに提供することを含み得る。本局面の他の実施形態は、対応するコンピュータシステム、装置、および１つ以上のコンピュータ記憶装置上に記録されたコンピュータプログラムを含み、その各々が当該方法のアクションを実行するように構成されている。 In another general aspect, a computer-implemented method comprises, in a computing device, defining a set of images based on captured images, and in the computing device By stitching one set of images into the equidistant cylindrical projection video stream and projecting the video stream from the planar perspective to the equidistant cylindrical perspective for the first view and the second view, At the computing device, rendering to play the video stream, and at the computing device determining a boundary where the distortion is higher than a predefined threshold, the distortion at least projecting the video stream Partially And Zui. The method further comprises generating an updated video stream at the computing device by removing image content within the set of images defined by the boundaries and outside the boundaries. Providing the updated video stream for display. Other embodiments of this aspect include corresponding computer systems, devices, and computer programs recorded on one or more computer storage devices, each of which is configured to perform an action of the method. . In one general aspect, a computer implemented method comprises defining a set of images in a computing device based on captured images, and a virtual reality (VR) head mount in the computing device Receiving may include receiving a view direction associated with a user of the display, and receiving at the computing device an indication of a change in view direction. The method may further include configuring a reprojection of a portion of the set of images by the computing device in response to receiving the indication, the reprojection being modified the view orientation, and Based at least in part on the field of view associated with the image, the method may further include transforming the portion from spherical perspective projection to planar perspective projection using reprojection. The method may further include rendering the updated view based on the reprojection for display by the computing device on the VR head mounted display, the updated view correcting the distortion to The method is configured to provide stereo parallax in the unit, and the method may further include providing the head mounted display with an updated view including a stereo panoramic scene corresponding to the changed view direction. Other embodiments of this aspect include corresponding computer systems, devices, and computer programs recorded on one or more computer storage devices, each of which is configured to perform an action of the method. .
別の一般的な局面において、装置は、少なくとも１つのプロセッサと、命令を格納しているメモリとを有するシステムを含み、当該命令は、少なくとも１つのプロセッサによって実行されるとシステムに動作を実行させ、当該動作は、カメラの少なくとも１つのステレオペアから収集されたキャプチャ済のビデオストリームに基づいて１セットの画像を規定することと、１セットの画像内のオプティカルフローを計算して、１セットの画像の一部ではない画像フレームを補間することと、画像フレームを１セットの画像内にインターリーブし、オプティカルフローに少なくとも部分的に基づいて画像フレームと１セットの画像とを互いにスティッチングすることと、画像フレームおよび１セットの画像を用いて、ＶＲヘッドマウントディスプレイにおいて表示するために全方位ステレオパノラマを生成することとを含む。本局面の他の実施形態は、対応するコンピュータシステム、装置、および１つ以上のコンピュータ記憶装置上に記録されたコンピュータプログラムを含み、その各々が当該方法のアクションを実行するように構成されている。 In another general aspect, an apparatus includes a system having at least one processor and a memory storing instructions, the instructions causing the system to perform an action when executed by the at least one processor. The operation comprises defining a set of images based on captured video streams collected from at least one stereo pair of cameras, and calculating an optical flow within the set of images, the set of Interpolating image frames that are not part of the image, interleaving the image frames into a set of images, and stitching together the image frames and the set of images based at least in part on the optical flow. VR head mount display using an image frame and a set of images And generating an omnidirectional stereo panorama for display at a. Other embodiments of this aspect include corresponding computer systems, devices, and computer programs recorded on one or more computer storage devices, each of which is configured to perform an action of the method. .
１つ以上の実現例の詳細を添付の図面および以下の説明で述べる。説明および図面から、かつ請求項から他の特徴が明らかになるであろう。 The details of one or more implementations are set forth in the accompanying drawings and the description below. Other features will be apparent from the description and drawings, and from the claims.
さまざまな図面における同様の参照記号は同様の要素を指す。 Like reference symbols in the various drawings indicate like elements.
詳細な説明
パノラマ画像を作成することは一般に、たとえば１つのカメラまたはカメラリグ内の多数のカメラを用いて周囲の三次元（３Ｄ）シーンの画像またはビデオをキャプチャすることを含む。いくつかのカメラを収容するカメラリグを用いる場合、各カメラは、特定の時点で画像をキャプチャするように同期化されて構成され得る。たとえば、各カメラによってキャプチャされる第１のフレームは、第２、第３、および第４のカメラが対応する第１のフレームをキャプチャするのとほぼ同時にキャプチャされ得る。この画像キャプチャは、シーンの一部またはすべてがキャプチャされるまで同時に継続し得る。実現例の多くはカメラに関して説明されるが、実現例は代わりに画像センサに関して、または（画像センサを含み得る）カメラハウジングに関しても説明され得る。
DETAILED DESCRIPTION Creating a panoramic image generally involves capturing an image or video of an ambient three dimensional (3D) scene using, for example, a single camera or multiple cameras in a camera rig. When using a camera rig containing several cameras, each camera may be synchronized and configured to capture an image at a particular point in time. For example, the first frame captured by each camera may be captured at substantially the same time as the second, third, and fourth cameras capture the corresponding first frame. This image capture may continue simultaneously until part or all of the scene is captured. Although many of the implementations are described with respect to a camera, implementations may alternatively be described with respect to an image sensor or with a camera housing (which may include an image sensor).
複数のカメラを収容するカメラリグは、シーンの特定の角度をキャプチャするように構成され得る。たとえば、カメラリグ上に収容されたカメラは特有の角度で方向付けられ得、その角度からキャプチャされたすべての（または少なくとも一部の）コンテンツが処理されて特定のシーンの全パノラマが生成され得る。いくつかの実現例では、カメラの各々は異なる角度に方向付けられてシーンの異なる角度をキャプチャし得る。シーンの一部のみをキャプチャする場合、またはシーンの一部もしくはすべてが歪みを含んでいる場合は、パノラマのうち、いずれかの欠けている、破損している、または歪んでいるコンテンツを補間または構成するために多数のプロセスが実行され得る。以下の開示では、３Ｄパノラマコンテンツを３Ｄバーチャルリアリティ（ＶＲ）環境でヘッドマウントディスプレイ（ＨＭＤ）デバイスにおいて表示する目的でそのようなコンテンツをキャプチャ、処理、補正、およびレンダリングする多数の装置および方法を記載する。 A camera rig containing multiple cameras may be configured to capture a particular angle of the scene. For example, a camera housed on a camera rig may be oriented at a particular angle, and all (or at least some) content captured from that angle may be processed to generate an entire panorama of a particular scene. In some implementations, each of the cameras may be oriented at different angles to capture different angles of the scene. If you want to capture only part of the scene, or if part or all of the scene contains distortion, then either any missing, corrupted or distorted content of the panorama is interpolated or A number of processes may be performed to configure. The following disclosure describes a number of apparatuses and methods for capturing, processing, correcting, and rendering 3D panoramic content for display on a head mounted display (HMD) device in a 3D virtual reality (VR) environment. Do.
図１は、３Ｄバーチャルリアリティ（ＶＲ）環境で立体パノラマをキャプチャしてレンダリングするための例示的なシステム１００のブロック図である。例示的なシステム１００において、カメラリグ１０２は画像をキャプチャし、（たとえば永久もしくはリムーバブル記憶装置に）ローカルに記憶し、および／またはネットワーク１０４上で提供し得るか、あるいは、画像を分析および処理のために画像処理システム１０６に直接提供し得る。システム１００のいくつかの実現例では、モバイルデバイス１０８は、画像をネットワーク１０４を介して提供するためのカメラリグ１０２として機能し得る。画像がキャプチャされると、画像処理システム１０６は画像に対して多数の計算およびプロセスを実行し、たとえば、処理画像をレンダリングのためにネットワーク１０４上でヘッドマウントディスプレイ（ＨＭＤ）デバイス１１０に提供し得る。いくつかの実現例では、画像処理システム１０６はカメラリグ１０２および／またはＨＭＤデバイス１１０に含まれ得る。いくつかの実現例では、画像処理システム１０６はさらに、処理画像をレンダリング、記憶、またはさらなる処理のためにモバイルデバイス１０８に、および／またはコンピューティングデバイス１１２に提供し得る。 FIG. 1 is a block diagram of an example system 100 for capturing and rendering a stereoscopic panorama in a 3D virtual reality (VR) environment. In the exemplary system 100, the camera rig 102 may capture an image, store it locally (eg, in permanent or removable storage), and / or provide it on the network 104, or alternatively, for analysis and processing of the image. Directly to the image processing system 106. In some implementations of system 100, mobile device 108 may function as a camera rig 102 for providing images over network 104. Once the image is captured, image processing system 106 may perform a number of calculations and processes on the image, for example, provide a processed image to head mounted display (HMD) device 110 over network 104 for rendering. . In some implementations, image processing system 106 may be included in camera rig 102 and / or HMD device 110. In some implementations, the image processing system 106 may further provide the processed image to the mobile device 108 and / or the computing device 112 for rendering, storage, or further processing.
ＨＭＤデバイス１１０は、バーチャルリアリティコンテンツを表示可能なバーチャルリアリティヘッドセット、眼鏡、アイピース、または他のウェアラブルデバイスを表わし得る。動作時、ＨＭＤデバイス１１０は、受信したおよび／または処理した画像をユーザに対して再生可能なＶＲアプリケーション（図示せず）を実行し得る。いくつかの実現例では、ＶＲアプリケーションは、図１に示すデバイス１０６，１０８または１１２の１つ以上によってホストされ得る。一例では、ＨＭＤデバイス１１０は、カメラリグ１０２がキャプチャしたシーンのビデオ再生を提供し得る。別の例では、ＨＭＤデバイス１１０は、単一のパノラマシーンにスティッチングされる静止画像の再生を提供し得る。 HMD device 110 may represent a virtual reality headset, glasses, eyepieces, or other wearable device capable of displaying virtual reality content. In operation, the HMD device 110 may execute a VR application (not shown) that can play back the received and / or processed images to the user. In some implementations, a VR application may be hosted by one or more of the devices 106, 108 or 112 shown in FIG. In one example, HMD device 110 may provide video playback of a scene captured by camera rig 102. In another example, the HMD device 110 may provide playback of still images stitched into a single panoramic scene.
カメラリグ１０２は、ＶＲ環境でコンテンツをレンダリングするために画像データを収集するカメラ（キャプチャデバイスとも称され得る）および／または処理デバイスとして用いられるように構成され得る。カメラリグ１０２は本明細書では特定の機能を有して記載されるブロック図として示されているが、リグ１０２は図２〜図６に示す実現例のいずれかの形態を取ることができ、さらに、本開示全体にわたってカメラリグについて記載される機能を有し得る。たとえば、システム１００の機能の記載を簡潔にするために、図１は画像をキャプチャするカメラがリグの周りに配置されていないカメラリグ１０２を示す。カメラリグ１０２の他の実現例は、リグ１０２などの円形カメラリグの周囲に配置され得る任意の数のカメラを含み得る。 The camera rig 102 may be configured to be used as a camera (which may also be referred to as a capture device) and / or a processing device that collects image data to render content in a VR environment. Although the camera rig 102 is shown as a block diagram described herein with particular features, the rig 102 can take any of the forms of implementation shown in FIGS. And may have the functionality described for a camera rig throughout the present disclosure. For example, to simplify the description of the functionality of the system 100, FIG. 1 shows a camera rig 102 in which the camera that captures the image is not located around the rig. Other implementations of camera rig 102 may include any number of cameras that may be placed around a circular camera rig, such as rig 102.
図１に示すように、カメラリグ１０２は多数のカメラ１３９および通信システム１３２を含む。カメラ１３９は単一のスチルカメラまたは単一のビデオカメラを含み得る。いくつかの実現例では、カメラ１３０は、リグ１０２の外周（たとえばリング）に沿って並んで配置された（たとえば着座した）複数のスチルカメラまたは複数のビデオカメラを含み得る。カメラ１３９はビデオカメラ、画像センサ、立体カメラ、赤外線カメラ、および／またはモバイルデバイスであり得る。通信システム１３２を用いて画像、命令、および／または他のカメラ関連のコンテンツがアップロードおよびダウンロードされ得る。当該通信は有線でも無線でもよく、私設網または公衆網上でインターフェイス可能である。 As shown in FIG. 1, the camera rig 102 includes a number of cameras 139 and a communication system 132. The camera 139 may include a single still camera or a single video camera. In some implementations, the camera 130 may include multiple still cameras or multiple video cameras arranged (eg, seated) side by side along the outer periphery (eg, ring) of the rig 102. Camera 139 may be a video camera, an image sensor, a stereo camera, an infrared camera, and / or a mobile device. Images, instructions, and / or other camera related content may be uploaded and downloaded using communication system 132. The communication may be wired or wireless, and can be interfaced on a private network or public network.
カメラリグ１０２は、固定リグまたは回転リグとして機能するように構成され得る。リグ上の各カメラはリグの回転中心からオフセットして配置（たとえば設置）される。カメラリグ１０２は、たとえば、３６０度回転して、シーンの３６０度ビューのすべてまたは一部をスイープしてキャプチャするように構成され得る。いくつかの実現例では、リグ１０２は固定位置で動作するように構成され得、そのような構成では、付加的なカメラをリグに追加してシーンの付加的な外向き角度のビューをキャプチャしてもよい。 The camera rig 102 may be configured to function as a fixed rig or a rotating rig. Each camera on the rig is positioned (eg, installed) offset from the center of rotation of the rig. The camera rig 102 may be configured, for example, to rotate 360 degrees and to sweep and capture all or part of a 360 degree view of the scene. In some implementations, the rig 102 may be configured to operate at a fixed position, and in such a configuration, additional cameras may be added to the rig to capture additional outward angle views of the scene. May be
いくつかの実現例では、カメラリグ１０２は、複数のデジタルビデオカメラを含む。複数のデジタルビデオカメラは、それらのレンズの各々が径方向外向き方向を指し示して周囲のシーンまたは環境の異なる部分を見るように、左右にまたは背中合わせに配置される（たとえば、カメラ３０２Ｂおよび３０２Ｆに関して図３に示される）。いくつかの実現例では、複数のデジタルビデオカメラは、ビュー方向が円形カメラリグ１０２に接している接線構成で配置される。たとえば、カメラリグ１０２は、リグのベースに対して接線方向に配列されつつ、自身のレンズの各々が径方向外向き方向を指し示すように配置される複数のデジタルビデオカメラを含み得る。デジタルビデオカメラは、異なる方向のコンテンツをキャプチャして周囲シーンの異なる角度部分を見るように指し示され得る。 In some implementations, camera rig 102 includes multiple digital video cameras. Multiple digital video cameras are placed side by side or back to back so that each of their lenses point radially outward and look at different parts of the surrounding scene or environment (e.g. with respect to cameras 302B and 302F Shown in Figure 3). In some implementations, multiple digital video cameras are arranged in a tangent configuration where the view direction is tangent to the circular camera rig 102. For example, the camera rig 102 may include a plurality of digital video cameras arranged tangentially to the base of the rig, with each of its lenses pointing radially outward. A digital video camera may be pointed to capture content in different directions and view different angular portions of the surrounding scene.
いくつかの実現例では、カメラからの画像はカメラリグ１０２上の隣接カメラにおいて処理され得る。そのような構成では、隣接カメラの各セットの各第１のカメラはカメラリグベースの円形路に対して接線方向に配置（たとえば設置）され、左方向に（たとえばカメラレンズが左方向を指し示して）整列される。隣接カメラの各セットの各第２のカメラはカメラリグベースの円形路に対して接線方向に配置（たとえば設置）され、（たとえばカメラレンズが）右方向を指し示して整列される。 In some implementations, the images from the cameras may be processed at adjacent cameras on the camera rig 102. In such a configuration, each first camera of each set of adjacent cameras is placed (e.g. installed) tangentially to the circular path of the camera rig base, and towards the left (e.g. ) Aligned. Each second camera of each set of adjacent cameras is arranged (e.g. installed) tangential to the circular path of the camera rig base and aligned pointing towards the right (e.g. camera lens).
カメラリグ１０２上で用いられるカメラについての例示的な設定として、約６０フレーム／秒のプログレッシブスキャンモード（すなわち、ほとんどビデオカメラの標準的な記録モードのように１行おきではなく、各ラスタ線をサンプリングしてビデオの各フレームを生成するモード）が挙げられ得る。また、カメラの各々は同一の（または同様の）設定で構成され得る。各カメラを同一の（または同様の）設定に構成することは、キャプチャ後に所望の態様で互いにスティッチングされ得る画像をキャプチャするという利点をもたらし得る。例示的な設定として、カメラの１つ以上を同じズーム、フォーカス、露光、およびシャッタスピードに設定すること、ならびに安定化機能が相互に関連しているかまたはオフにされた状態でカメラをホワイトバランスするように設定することが挙げられ得る。 As an exemplary setting for the camera used on the camera rig 102, a progressive scan mode of about 60 frames per second (i.e., sampling each raster line rather than every other row as in most video camera standard recording modes) And the mode in which each frame of video is generated. Also, each of the cameras may be configured with the same (or similar) settings. Configuring each camera to the same (or similar) setting may provide the advantage of capturing images that may be stitched together in a desired manner after capture. As an exemplary setting, setting one or more of the cameras to the same zoom, focus, exposure, and shutter speed, and white balance the camera with the stabilization feature interrelated or off It may be mentioned that setting is made.
いくつかの実現例では、カメラリグ１０２は１つ以上の画像またはビデオをキャプチャするために用いられる前に較正され得る。たとえば、カメラリグ１０２上の各カメラはパノラマビデオを撮るように較正および／または構成され得る。設定は、たとえば、リグを３６０度スイープで特定の回転速度で、広視野で、時計回りまたは反時計回り方向に動作させるように構成することを含み得る。いくつかの実現例では、リグ１０２上のカメラは、たとえば、シーンの周りのキャプチャ経路の３６０度スイープの１度毎に１フレームをキャプチャするように構成され得る。そのような実現例では、リグ１０２上のカメラは、たとえば、シーンの周りのキャプチャ経路の３６０度（以下）スイープの１度毎に複数のフレームをキャプチャするように構成され得る。いくつかの実現例では、リグ１０２上のカメラは、たとえば、１度毎に特に測定されたフレームをキャプチャする必要なしにシーンの周りのキャプチャ経路のスイープで複数のフレームをキャプチャするように構成され得る。 In some implementations, the camera rig 102 may be calibrated before being used to capture one or more images or videos. For example, each camera on the camera rig 102 may be calibrated and / or configured to take panoramic video. The setting may include, for example, configuring the rig to operate in a 360 degree sweep at a specific rotational speed, wide field of view, clockwise or counterclockwise. In some implementations, a camera on rig 102 may be configured to capture, for example, one frame per degree of a 360 degree sweep of the capture path around the scene. In such an implementation, the camera on rig 102 may be configured, for example, to capture multiple frames each time a 360 degree (or less) sweep of the capture path around the scene. In some implementations, the camera on rig 102 is configured to capture multiple frames, for example, in a sweep of the capture path around the scene without having to capture frames specifically measured each time. obtain.
いくつかの実現例では、カメラは、同期して機能して、特定の時点でカメラリグ上のカメラからビデオをキャプチャするように構成（たとえばセットアップ）され得る。いくつかの実現例では、カメラは、同期して機能して、ある期間にわたってカメラの１つ以上からビデオの特定の部分をキャプチャするように構成され得る。カメラリグを較正する別の例は、受信画像をどのように記憶するかを構成することを含み得る。たとえば、受信画像は個々のフレームまたはビデオ（たとえば．ａｖｉファイル、．ｍｐｇファイル）として記憶され得、そのように記憶された画像はインターネット、別のサーバもしくはデバイスにアップロードされ得るか、またはカメラリグ１０２上の各カメラを用いてローカルに記憶され得る。いくつかの実現例では、受信画像は符号化ビデオとして記憶され得る。 In some implementations, the cameras may be configured (eg, set up) to function synchronously and to capture video from the camera on the camera rig at a particular point in time. In some implementations, the cameras may be configured to function in synchronization to capture specific portions of the video from one or more of the cameras over a period of time. Another example of calibrating a camera rig may include configuring how to store the received image. For example, received images may be stored as individual frames or videos (eg, .avi files, .mpg files), and such stored images may be uploaded to the Internet, another server or device, or on camera rig 102 It can be stored locally with each camera of. In some implementations, the received image may be stored as encoded video.
画像処理システム１０６は、補間モジュール１１４、キャプチャ補正モジュール１１６、およびスティッチングモジュール１１８を含む。補間モジュール１１６は、たとえば、デジタル画像およびビデオの一部をサンプリングし、カメラリグ１０２からキャプチャされた隣接画像間で起こると考えられる多数の補間された画像を求めるために用いられ得るアルゴリズムを表わす。いくつかの実現例では、補間モジュール１１４は、隣接画像間の補間された画像フラグメント、画像部分、および／または垂直もしくは水平画像ストリップを求めるように構成され得る。いくつかの実現例では、補間モジュール１１４は、隣接画像内の関連画素間のフローフィールド（および／またはフローベクトル）を求めるように構成され得る。フローフィールドを用いて、画像が受けた両変換、および変換を受けた処理画像が補償され得る。たとえば、フローフィールドを用いて、取得された画像の特定の画素格子の変換が補償され得る。いくつかの実現例では、補間モジュール１１４は、周囲画像の補間によって、キャプチャされた画像の一部ではない１つ以上の画像を生成し得、生成された画像をキャプチャされた画像内にインターリーブして、シーンの付加的なバーチャルリアリティコンテンツを生成し得る。 Image processing system 106 includes an interpolation module 114, a capture correction module 116, and a stitching module 118. Interpolation module 116 represents, for example, an algorithm that may be used to sample portions of the digital image and video, and to determine a number of interpolated images that are likely to occur between adjacent images captured from camera rig 102. In some implementations, interpolation module 114 may be configured to determine interpolated image fragments, image portions, and / or vertical or horizontal image strips between adjacent images. In some implementations, interpolation module 114 may be configured to determine flow fields (and / or flow vectors) between relevant pixels in adjacent images. The flow field can be used to compensate for both transformations received by the image, and the processed image that has undergone transformation. For example, flow fields may be used to compensate for the transformation of a particular pixel grid of the acquired image. In some implementations, interpolation module 114 may generate one or more images that are not part of the captured image by interpolation of the surrounding image and interleave the generated image into the captured image Thus, additional virtual reality content of the scene may be generated.
キャプチャ補正モジュール１１６は、非理想的なキャプチャセットアップを補償することによってキャプチャされた画像を補正するように構成され得る。例示的なキャプチャセットアップとして、非限定的な例では、円形カメラ軌道、平行な主（カメラ）軸、カメラ軌道に垂直なビュー方向、カメラ軌道に対して接線方向であるビュー方向、および／または他のキャプチャ条件が挙げられ得る。いくつかの実現例では、キャプチャ補正モジュール１１６は、画像キャプチャ時の非円形カメラ軌道、および／または画像キャプチャ時の非平行主軸の一方または両方を補償するように構成され得る。 The capture correction module 116 may be configured to correct the captured image by compensating for non-ideal capture setups. As an illustrative capture setup, in a non-limiting example, a circular camera trajectory, a parallel main (camera) axis, a view direction perpendicular to the camera trajectory, a view direction tangential to the camera trajectory, and / or other The capture conditions of may be mentioned. In some implementations, the capture correction module 116 may be configured to compensate for one or both of non-circular camera trajectory at the time of image capture and / or non-parallel principal axes at the time of image capture.
キャプチャ補正モジュール１１６は、特定の１セットの画像を調整して、カメラ同士の分離が約３０度より大きい複数のカメラを用いてキャプチャされたコンテンツを補償するように構成され得る。たとえば、カメラ間の距離が４０度の場合、キャプチャ補正モジュール１１６は、付加的なカメラからコンテンツを収集することによって、または欠けているコンテンツを補間することによって、カメラのカバー範囲不足に基づく特定のシーンにおけるいずれかの欠けているコンテンツを説明し得る。 The capture correction module 116 may be configured to adjust a particular set of images to compensate for captured content using multiple cameras with camera separation greater than about 30 degrees. For example, if the distance between the cameras is 40 degrees, the capture correction module 116 may identify the particular camera based on the lack of coverage of the camera by collecting content from additional cameras, or by interpolating missing content. It may explain any missing content in the scene.
いくつかの実現例では、キャプチャ補正モジュール１１６はさらに、１セットの画像を調整して、カメラポーズエラーなどによるカメラの位置合わせ不良を補償するように構成され得る。たとえば、カメラポーズエラー（たとえばカメラの向きおよび位置によるエラー）が画像キャプチャ時に起こると、モジュール１１６はいくつかの画像フレームからの２つ以上の画素列同士を混合して、露光不足（もしくは画像フレーム毎の露光変化）によるおよび／または１つ以上のカメラの位置合わせ不良によるアーティファクトを含むアーティファクトを除去し得る。スティッチングモジュール１１８は、規定された、取得された、および／または補間された画像に基づいて３Ｄ立体画像を生成するように構成され得る。スティッチングモジュール１１８は、複数の画像部分からの画素および／または画像ストリップを混合する／スティッチングするように構成され得る。スティッチングは、たとえば補間モジュール１１４によって求められたフローフィールドに基づき得る。たとえば、スティッチングモジュール１１８は、１セットの画像の一部ではない補間された画像フレームを（補間モジュール１１４から）受信し、画像フレームを１セットの画像内にインターリーブし得る。インターリーブすることは、モジュール１１８が、補間モジュール１１４によって生成されたオプティカルフローに少なくとも部分的に基づいて画像フレームと１セットの画像とを互いにスティッチングすることを含み得る。 In some implementations, the capture correction module 116 may be further configured to adjust a set of images to compensate for camera misalignment, such as due to camera pose errors. For example, if a camera pose error (eg, an error due to camera orientation and position) occurs during image capture, module 116 mixes two or more pixel columns from several image frames together to cause underexposure (or image frame). Artifacts may be removed, including artifacts due to each exposure change) and / or one or more camera misalignments. The stitching module 118 may be configured to generate a 3D stereo image based on the defined, acquired and / or interpolated images. The stitching module 118 may be configured to blend / stitch pixels and / or image strips from multiple image portions. Stitching may be based, for example, on the flow field determined by interpolation module 114. For example, stitching module 118 may receive interpolated image frames that are not part of a set of images (from interpolation module 114) and interleave the image frames into a set of images. The interleaving may include the module 118 stitching together the image frame and the set of images based at least in part on the optical flow generated by the interpolation module 114.
スティッチングされた組合せを用いて、全方位ステレオパノラマがＶＲヘッドマウントディスプレイにおいて表示するために生成され得る。画像フレームは、特定のリグ上に配置されたカメラの多数の隣接ペアから収集された、キャプチャ済のビデオストリームに基づき得る。そのようなリグは、約１２個から約１６個のカメラを含み得る。言い換えると、そのようなリグの他の組合せは、たとえば１２個〜１６個のペアでないまたは個々のカメラを含み得る。いくつかの実現例では、奇数個のカメラがリグに含まれていてもよい。いくつかの実現例では、リグは２セットまたは３セット以上の隣接カメラを含む。いくつかの実現例では、リグは、リグ上に並んで着座し得る同数セットの隣接カメラを含み得る。いくつかの実現例では、スティッチングモジュール１１８は、少なくとも１つの隣接ペアと関連付けられているポーズ情報を用いて、インターリーブを実行する前に１セットの画像の一部を予めスティッチングし得る。カメラリグ上の隣接ペアは、たとえば図３に関連して以下により明確に示されて記載される。 Using stitched combinations, an omnidirectional stereo panorama can be generated for display on a VR head mounted display. Image frames may be based on captured video streams collected from multiple adjacent pairs of cameras placed on a particular rig. Such a rig may include about 12 to about 16 cameras. In other words, other combinations of such rigs may include, for example, 12 to 16 unpaired or individual cameras. In some implementations, an odd number of cameras may be included in the rig. In some implementations, the rig includes two or more sets of adjacent cameras. In some implementations, the rig can include an equal number of adjacent cameras that can sit side by side on the rig. In some implementations, the stitching module 118 may pre-stitch a portion of the set of images prior to performing interleaving, using the pose information associated with the at least one adjacent pair. Adjacent pairs on the camera rig are shown and described more clearly below, for example, in connection with FIG.
いくつかの実現例では、オプティカルフロー技術を用いて画像同士を互いにスティッチングすることは、キャプチャされたビデオコンテンツ同士を互いにスティッチングすることを含み得る。そのようなオプティカルフロー技術を用いて、カメラペアおよび／または単一のカメラを用いて以前にキャプチャされた特定のビデオコンテンツ間の中間ビデオコンテンツが生成され得る。この技術は、画像をキャプチャする円形固定カメラリグ上の一連のカメラをシミュレートする方法として用いられ得る。シミュレートされたカメラは、単一のカメラを円を描くようにスイープして３６０度の画像をキャプチャする方法と同様のコンテンツをキャプチャし得るが、上記の技術では、実際にリグ上に設置されるカメラはより少なく、リグは固定であってもよい。また、一連のカメラをシミュレートする能力は、ビデオ内のフレーム毎にコンテンツ（たとえば１度毎に１画像のキャプチャ間隔で３６０個の画像）をキャプチャすることができるという利点をもたらす。 In some implementations, stitching the images together using optical flow techniques may include stitching the captured video content together. Such optical flow techniques may be used to generate intermediate video content between particular video content previously captured using a camera pair and / or a single camera. This technique can be used as a method to simulate a series of cameras on a circular fixed camera rig that captures images. A simulated camera could capture content similar to the way to capture a 360 degree image by sweeping a single camera into a circle, but with the above techniques it is actually installed on the rig There may be fewer cameras and the rig may be stationary. Also, the ability to simulate a series of cameras provides the advantage of being able to capture content (eg 360 images at one image capture interval per degree) for each frame in the video.
生成された中間ビデオコンテンツは、高密度の１セットの画像（たとえば１度毎に１画像で３６０個の画像）を用いることによってオプティカルフローを用いて実際にキャプチャされたビデオコンテンツにスティッチングされ得るが、実際は、カメラリグがキャプチャした画像は３６０個より少ない。たとえば、円形カメラリグが８ペアのカメラ（すなわち１６個のカメラ）またはペアでない１６個のカメラを含む場合、キャプチャされた画像のカウントは１６個の画像まで低くなり得る。オプティカルフロー技術を用いて１６個の画像間のコンテンツがシミュレートされて３６０度のビデオコンテンツが提供され得る。 The generated intermediate video content can be stitched into the actually captured video content using optical flow by using a high-density set of images (e.g. 360 images, one at a time) But in fact, the camera rig captured less than 360 images. For example, if the circular camera rig includes 8 pairs of cameras (ie 16 cameras) or 16 cameras not paired, the count of captured images may be as low as 16 images. Content between 16 images may be simulated to provide 360 degrees of video content using optical flow techniques.
いくつかの実現例では、オプティカルフロー技術を用いることによって補間効率が向上し得る。たとえば、３６０個の画像を補間する代わりに、カメラの各連続ペア（たとえば［１−２］，［２−３］，［３−４］）間でオプティカルフローが計算され得る。キャプチャされた１６個の画像およびオプティカルフローを仮定して、補間モジュール１１４および／またはキャプチャ補正モジュール１１６は、１６個の画像のうちのいずれかにおいて画像全体を補間する必要なしに任意の中間ビューにおける任意の画素を計算し得る。 In some implementations, interpolation efficiency may be improved by using optical flow techniques. For example, instead of interpolating 360 images, optical flow may be calculated between each successive pair of cameras (e.g. [1-2], [2-3], [3-4]). Assuming the 16 captured images and optical flow, the interpolation module 114 and / or the capture correction module 116 may be used at any intermediate view without having to interpolate the entire image in any of the 16 images. Any pixel can be calculated.
画像処理システム１０６は、投影モジュール１２０および画像補正モジュール１２２をさらに含む。投影モジュール１２０は、画像を透視平面内に投影することによって３Ｄ立体画像を生成するように構成され得る。たとえば、投影モジュール１２０は特定の１セットの画像の投影を取得し得、画像の一部を平面透視投影から球面（すなわち正距円筒図法）透視投影に変換することによって当該１セットの画像の一部の再投影を構成し得る。当該変換は投影モデリング技術を含む。 Image processing system 106 further includes projection module 120 and image correction module 122. The projection module 120 may be configured to generate a 3D stereo image by projecting the image into a perspective plane. For example, projection module 120 may obtain a projection of a particular set of images, and convert one portion of the image from planar perspective projection to spherical (ie, equidistant cylindrical) perspective projection to one of the set of images. It may constitute part reprojection. The transformation includes projection modeling techniques.
投影モデリングは、投影中心および投影面を規定することを含み得る。本開示に記載の例では、投影中心は、予め規定されたｘｙｚ座標系の原点（０，０，０）における光学中心を表わし得る。投影面は投影中心の前に設置され得、カメラはｘｙｚ座標系のｚ軸に沿って画像をキャプチャするように向いている。一般に、投影は、座標（ｘ，ｙ，ｚ）から投影中心までの特定の画像光線の透視平面の交点を用いて計算され得る。投影の変換は、たとえばマトリックス計算を用いて座標系を操作することによって行なわれ得る。 Projection modeling may include defining a projection center and a projection plane. In the example described in the present disclosure, the projection center may represent an optical center at the origin (0, 0, 0) of the predefined xyz coordinate system. The projection plane may be placed in front of the projection center, and the camera is oriented to capture an image along the z-axis of the xyz coordinate system. In general, the projection may be calculated using the intersection of the perspective planes of a particular image ray from the coordinates (x, y, z) to the projection center. The transformation of the projections can be done, for example, by manipulating the coordinate system using matrix calculations.
立体パノラマための投影モデリングは、単一の投影中心を有さないマルチ透視画像を用いることを含み得る。マルチ透視は典型的に円形状（球状）（図１３Ｂ参照）として示される。コンテンツをレンダリングする際、本明細書に記載のシステムは、１つの座標系から別の座標系に変換する際の近似として球面を用い得る。 Projection modeling for stereoscopic panoramas may include using multi-perspective images that do not have a single projection center. Multi fluoroscopy is typically shown as circular (spherical) (see FIG. 13B). In rendering content, the system described herein may use a sphere as an approximation in transforming from one coordinate system to another.
一般に、球面（すなわち正距円筒図法）投影は、球面の中心が投影の中心を等しく囲んでいる球状の平面を提供する。透視投影は、透視平面（たとえば２Ｄ表面）上の３Ｄオブジェクトの画像を提供してユーザの実際の視覚を近似するビューを提供する。一般に、画像は平坦な画像面（たとえばコンピュータモニタ、モバイルデバイスＬＣＤスクリーン）上にレンダリングされ得るので、投影は歪みのないビューを提供するために平面透視で示される。しかし、平面投影では３６０度の視野が可能でない場合があるので、キャプチャされた画像（たとえばビデオ）は正距円筒図法（たとえば球面）透視で記憶され得、レンダリング時に平面透視に再投影され得る。 In general, a spherical (i.e., equidistant cylindrical projection) projection provides a spherical plane in which the centers of the spheres equally surround the center of projection. Perspective projection provides an image of a 3D object on a perspective plane (e.g., a 2D surface) to provide a view that approximates the user's actual vision. In general, as the image can be rendered on a flat image plane (e.g. computer monitor, mobile device LCD screen), the projections are shown in plan view to provide a distortion free view. However, since a 360-degree field of view may not be possible in planar projection, the captured image (eg, video) may be stored in equidistant cylindrical projection (eg, spherical) perspective and re-projected to planar perspective at the time of rendering.
特定の再投影が完了した後、投影モジュール１２０はＨＭＤにおいてレンダリングするために再投影された画像の部分を送信し得る。たとえば、投影モジュール１２０は、再投影の一部をＨＭＤ１１０における左目ディスプレイに、再投影の一部をＨＭＤ１１０における右目ディスプレイに提供し得る。いくつかの実現例では、投影モジュール１２０は、上記の再投影を実行することによって垂直視差を計算して減らすように構成され得る。 After the particular reprojection is complete, projection module 120 may transmit portions of the reprojected image for rendering in the HMD. For example, the projection module 120 may provide part of the reprojection to the left eye display in the HMD 110 and part of the reprojection to the right eye display in the HMD 110. In some implementations, projection module 120 may be configured to calculate and reduce vertical disparity by performing the above reprojection.
画像補正モジュール１２２は、透視歪みを含むがこれに限定されない歪みを補償することによって３Ｄ立体画像を生成するように構成され得る。いくつかの実現例では、画像補正モジュール１２２は、オプティカルフローが３Ｄステレオのために維持される特定の距離を求め得、画像をセグメント化して、そのようなフローが維持されるシーンの一部のみを示し得る。たとえば、画像補正モジュール１２２は、３Ｄステレオ画像のオプティカルフローが、たとえば、円形カメラリグ１０２の外向きエッジから約半径１メートルから、カメラリグ１０２の外向きエッジから約半径５メートルの間に維持されると判断し得る。したがって、画像補正モジュール１２２は、ＨＭＤ１１０のユーザにとって適切な視差を有する適切な３Ｄステレオ効果を提供しつつ、歪みのない投影においてＨＭＤ１１０内でレンダリングするために１メートルから５メートルの間の部分が選択されることを保証し得る。 Image correction module 122 may be configured to generate a 3D stereoscopic image by compensating for distortions including but not limited to perspective distortion. In some implementations, the image correction module 122 may determine a particular distance at which optical flow is maintained for 3D stereo, segmenting the image, and only a portion of the scene where such flow is maintained. Can be shown. For example, the image correction module 122 may maintain the optical flow of the 3D stereo image between, for example, a radius of about 1 meter from the outward edge of the circular camera rig 102 and a radius of about 5 meters from the outward edge of the camera rig 102. It can be judged. Thus, the image correction module 122 selects a portion between 1 meter and 5 meters for rendering in the HMD 110 in distortion free projections while providing an appropriate 3D stereo effect with the appropriate parallax for the HMD 110 user It can be guaranteed to be done.
いくつかの実現例では、画像補正モジュール１２２は、特定の画像を調整することによってオプティカルフローを推定し得る。当該調整は、たとえば、画像の一部を訂正すること、画像の当該一部と関連付けられている推定されたカメラポーズを求めること、および当該一部における画像間のフローを求めることを含み得る。非限定的な例では、画像補正モジュール１２２は、フローが計算されている２つの特定の画像間の回転差を補償し得る。この補正は、回転差（すなわち回転フロー）に起因するフロー成分を除去するように機能し得る。そのような補正によって、並進移動に起因するフロー（たとえば視差フロー）がもたらされ、これは、結果として得られる画像を正確かつロバストにしつつ、フロー推定計算の複雑さを減少させることができる。いくつかの実現例では、画像補正に加えて、レンダリング前に画像に対して処理が行なわれ得る。たとえば、レンダリングが実行される前に、スティッチング、混合、または付加的な補正処理が画像に対して行なわれ得る。 In some implementations, the image correction module 122 may estimate optical flow by adjusting a particular image. The adjustment may include, for example, correcting a portion of the image, determining an estimated camera pose associated with the portion of the image, and determining flow between the images in the portion. In a non-limiting example, the image correction module 122 may compensate for rotational differences between two specific images for which flow is being calculated. This correction may function to remove flow components due to rotational differences (i.e. rotational flow). Such correction results in flows due to translational movement (e.g., parallax flows), which can reduce the complexity of flow estimation calculations while making the resulting image accurate and robust. In some implementations, in addition to image correction, processing may be performed on the image prior to rendering. For example, stitching, blending, or additional correction may be performed on the image before rendering is performed.
いくつかの実現例では、画像補正モジュール１２２は、平面透視投影に基づいていないカメラジオメトリを用いてキャプチャされた画像コンテンツに起因する投影歪みを補正し得る。たとえば、多数の異なるビュー角度からの画像を補間することによって、かつ当該画像と関連付けられているビュー光線を共通の原点から生じていると条件付けることによって、補正が画像に適用され得る。補間された画像はキャプチャされた画像内にインターリーブされて、人間の目にとって無理のないレベルの回転視差を有して人間の目に正確に見えるバーチャルコンテンツが生成され得る。 In some implementations, the image correction module 122 may correct projection distortion due to image content captured using camera geometry that is not based on planar perspective projection. For example, correction may be applied to the image by interpolating the image from a number of different view angles and by conditioning the view rays associated with the image from a common origin. Interpolated images can be interleaved into the captured image to create virtual content that looks accurate to the human eye with a level of rotational parallax that is reasonable for the human eye.
例示的なシステム１００では、デバイス１０６，１０８および１１２はラップトップコンピュータ、デスクトップコンピュータ、モバイルコンピューティングデバイス、またはゲーム機であり得る。いくつかの実現例では、デバイス１０６，１０８および１１２は、ＨＭＤデバイス１１０内に配置され（たとえば設置され／位置し）得るモバイルコンピューティングデバイスであり得る。モバイルコンピューティングデバイスは、たとえば、ＨＭＤデバイス１１０のためのスクリーンとして用いられ得る表示装置を含み得る。デバイス１０６，１０８および１１２は、ＶＲアプリケーションを実行するためのハードウェアおよび／またはソフトウェアを含み得る。さらに、デバイス１０６，１０８および１１２は、これらのデバイスがＨＭＤデバイス１１０の前に設置されるか、またはＨＭＤデバイス１１０に対してある範囲の位置内に保持される場合に、ＨＭＤデバイス１１０の３Ｄ移動を認識、監視、および追跡可能なハードウェアおよび／またはソフトウェアを含み得る。いくつかの実現例では、デバイス１０６，１０８および１１２は、付加的なコンテンツをネットワーク１０４上でＨＭＤデバイス１１０に提供し得る。いくつかの実現例では、デバイス１０２，１０６，１０８，１１０および１１２は、ペアリングされるかまたはネットワーク１０４を介して接続されて、１つ以上と互いに接続／インターフェイスされ得る。当該接続は有線でも無線でもよい。ネットワーク１０４は公衆通信網でも私設通信網でもよい。 In the exemplary system 100, devices 106, 108 and 112 may be laptop computers, desktop computers, mobile computing devices, or game consoles. In some implementations, devices 106, 108 and 112 may be mobile computing devices that may be located (eg, installed / located) within HMD device 110. The mobile computing device may include, for example, a display that may be used as a screen for the HMD device 110. Devices 106, 108 and 112 may include hardware and / or software to execute VR applications. In addition, devices 106, 108 and 112 can be used to 3D move HMD device 110 if they are installed in front of HMD device 110 or held within a range of positions relative to HMD device 110. May include hardware and / or software that can recognize, monitor, and track. In some implementations, devices 106, 108 and 112 may provide additional content to HMD device 110 over network 104. In some implementations, devices 102, 106, 108, 110 and 112 may be paired or connected via network 104 to connect / interface with one or more other devices. The connection may be wired or wireless. The network 104 may be a public communication network or a private communication network.
システム１００は電子記憶装置を含み得る。電子記憶装置はデバイスのうちのいずれか（たとえばカメラリグ１０２、画像処理システム１０６、ＨＭＤデバイス１１０および／またはその他）に含まれ得る。電子記憶装置は、情報を電子的に記憶する非一時的な記憶媒体を含み得る。電子記憶装置は、キャプチャされた画像、取得された画像、前処理された画像、後処理された画像等を記憶するように構成され得る。開示されるカメラリグのいずれかを用いてキャプチャされた画像は処理されて、ビデオの１つ以上のストリームとして記憶され得るか、または個々のフレームとして記憶され得る。いくつかの実現例では、記憶はキャプチャ時に起こり、レンダリングはキャプチャの一部の直後に起こり、キャプチャおよび処理が同時でなかった場合よりも早くパノラマステレオコンテンツへの高速アクセスを可能とし得る。 System 100 may include electronic storage. Electronic storage may be included in any of the devices (eg, camera rig 102, image processing system 106, HMD device 110, and / or the like). Electronic storage may include non-transitory storage media that electronically store information. The electronic storage may be configured to store captured images, captured images, pre-processed images, post-processed images, and the like. Images captured using any of the disclosed camera rigs can be processed and stored as one or more streams of video or stored as individual frames. In some implementations, storage occurs at the time of capture and rendering occurs immediately after a portion of the capture, which may allow for faster access to panoramic stereo content faster than if capture and processing were not simultaneous.
図２は、立体パノラマを生成する際に用いるシーンの画像をキャプチャするように構成された例示的なカメラリグ２００を図示する図である。カメラリグ２００は、リング状の支持ベース（図示せず）に取付けられた第１のカメラ２０２Ａおよび第２のカメラ２０２Ｂを含む。示されるように、カメラ２０２Ａおよび２０２Ｂは、直接外側を（キャプチャすべき画像／シーンに向かって）向いて、かつリグ２００の回転軸（Ａ１）の中心と平行に、環状位置に配置されている。 FIG. 2 is a diagram illustrating an example camera rig 200 configured to capture an image of a scene for use in generating a stereoscopic panorama. The camera rig 200 includes a first camera 202A and a second camera 202B mounted to a ring shaped support base (not shown). As shown, the cameras 202A and 202B are arranged in an annular position, facing outwards (toward the image / scene to be captured) and parallel to the center of the axis of rotation (A1) of the rig 200 .
図示される例では、カメラ２０２Ａおよび２０２Ｂは、ある距離だけ離れて（Ｂ１）マウントプレート２０８上に配置（たとえば設置）される。いくつかの実現例では、カメラリグ２００上の各カメラ間の距離（Ｂ１）は平均的な人間の瞳孔間距離（ＩＰＤ）を表わし得る。カメラをＩＰＤ距離だけ離して設置することによって、人間の目が（矢印２０４によって示すように左右に）回転して矢印２０４によって示されるキャプチャ経路の周りのシーンをスキャンする際にどのように画像を見るかを近似することができる。例示的な平均的な人間のＩＰＤ測定値は、約５センチメートルから約６．５センチメートルであり得る。いくつかの実現例では、標準的なＩＰＤ距離だけ離れて配置された各カメラは１セットの隣接カメラの一部であり得る。 In the illustrated example, cameras 202A and 202B are placed (eg, installed) on mount plate 208 at a distance apart (B1). In some implementations, the distance (B1) between each camera on camera rig 200 may represent an average human pupillary distance (IPD). By placing the camera a distance of IPD, how the human eye rotates (left and right as shown by arrow 204) and scans the scene around the capture path indicated by arrow 204 You can approximate what you see. An exemplary average human IPD measurement may be about 5 centimeters to about 6.5 centimeters. In some implementations, each camera placed a standard IPD distance apart may be part of a set of adjacent cameras.
いくつかの実現例では、カメラリグ２００は標準的な人間の頭部の直径を近似するように構成され得る。たとえば、カメラリグ２００は、約８センチメートルから約１０センチメートルの直径２０６を有して設計され得る。この直径２０６は、人間の頭部がどのように回転して回転中心Ａ１に対して人間の目でシーン画像を見るかをリグ２００が近似するように選択され得る。他の測定も可能であり、リグ２００またはシステム１００は、たとえばより大きい直径を用いる場合は、キャプチャ技術および結果として得られる画像を調整し得る。 In some implementations, the camera rig 200 can be configured to approximate the diameter of a standard human head. For example, camera rig 200 may be designed with a diameter 206 of about 8 centimeters to about 10 centimeters. The diameter 206 may be selected to approximate the rig 200 how the human head rotates and views the scene image with the human eye relative to the center of rotation A1. Other measurements are also possible, and the rig 200 or system 100 may adjust the capture technology and the resulting image, for example when using larger diameters.
非限定的な例では、カメラリグ２００は約８センチメートルから約１０センチメートルの直径２０６を有し得、約６センチメートル離れたＩＰＤ距離に設置されたカメラを収容し得る。多数のリグ配列を以下に記載する。本開示に記載の各配列は、上述のまたは他の直径およびカメラ間距離を有して構成され得る。 In a non-limiting example, the camera rig 200 can have a diameter 206 of about 8 centimeters to about 10 centimeters, and can accommodate cameras installed at IPD distances about 6 centimeters apart. A number of rig sequences are described below. Each arrangement described in the present disclosure may be configured with the above or other diameters and inter-camera distances.
図２に示すように、２つのカメラ２０２Ａ，２０２Ｂは広視野を有して構成され得る。たとえば、カメラは約１５０度から約１８０度の視野をキャプチャし得る。カメラ２０２Ａ，２０２Ｂは、より幅広いビューをキャプチャする魚眼レンズを有していてもよい。いくつかの実現例では、カメラ２０２Ａ，２０２Ｂはカメラの隣接ペアとして機能する。 As shown in FIG. 2, the two cameras 202A, 202B can be configured with a wide field of view. For example, the camera may capture a field of view of about 150 degrees to about 180 degrees. The cameras 202A, 202B may have fisheye lenses that capture a wider view. In some implementations, cameras 202A, 202B function as adjacent pairs of cameras.
動作時、リグ２００は回転中心Ａ１の周りを３６０度回転してパノラマシーンをキャプチャし得る。あるいは、リグは固定のままであってもよく、（図３に示すように）付加的なカメラをカメラリグ２００に追加して３６０度のシーンの付加的な部分をキャプチャしてもよい。 In operation, the rig 200 may rotate 360 degrees around the center of rotation A1 to capture a panoramic scene. Alternatively, the rig may remain fixed, or additional cameras may be added to the camera rig 200 (as shown in FIG. 3) to capture additional portions of the 360 degree scene.
図３は、立体パノラマを生成する際に用いるシーンの画像をキャプチャするように構成された別の例示的なカメラリグ３００を図示する図である。カメラリグ３００は、リング状の支持ベース（図示せず）に取付けられた多数のカメラ３０２Ａ〜３０２Ｈを含む。第１のカメラ３０２Ａは実線で示されており、付加的なカメラ３０２Ｂ〜３０２Ｈはそれらがオプションであることを示すために破線で示されている。カメラリグ２００内に示される平行に装着されたカメラ（カメラ２０２Ａおよび２０２Ｂ参照）とは対照的に、カメラ３０２Ａ〜３０２Ｈは円形カメラリグ３００の外周に対して接線方向に配置される。図３に示すように、カメラ３０２Ａは隣接カメラ３０２Ｂおよび隣接カメラ３０２Ｈを有する。 FIG. 3 is a diagram illustrating another exemplary camera rig 300 configured to capture an image of a scene for use in generating a stereoscopic panorama. The camera rig 300 includes a number of cameras 302A-302H mounted to a ring shaped support base (not shown). The first camera 302A is shown as a solid line, and the additional cameras 302B-302H are shown as a dashed line to indicate that they are optional. In contrast to the parallel mounted cameras (see cameras 202A and 202B) shown in the camera rig 200, the cameras 302A-302H are arranged tangential to the outer periphery of the circular camera rig 300. As shown in FIG. 3, the camera 302A has an adjacent camera 302B and an adjacent camera 302H.
図示される例では、カメラ２０２Ａおよび２０２Ｂは、リグ２００内のカメラと同様に、特定の距離だけ離れて（Ｂ１）配置される。この例では、カメラ３０２Ａおよび３０２Ｂは、以下に詳細に記載するように、隣接ペアとして機能して、中心カメラレンズからそれぞれ左右方向に外れた角度をキャプチャし得る。 In the illustrated example, the cameras 202A and 202B, like the cameras in the rig 200, are placed a certain distance apart (B1). In this example, cameras 302A and 302B may function as an adjacent pair to capture angles that are respectively laterally offset from the central camera lens, as described in detail below.
一例では、カメラリグ３００は、回転可能なまたは固定されたベース（図示せず）およびマウントプレート３０６（支持部とも称され得る）を含む円形リグであり、カメラの隣接ペアは、マウントプレート３０６上に設置され、マウントプレート３０６のエッジに対して接線方向であるビュー方向を指し示すように構成され、左方向を指し示すように配列された第１のカメラ３０２Ａと、第１のカメラと並んでマウントプレート３０６上に設置され、第１のカメラ３０２Ａから瞳孔間距離（または異なる距離（たとえばＩＰＤ距離未満））をおいて設置された第２のカメラ３０２Ｂとを含み、第２のカメラ３０２Ｂは、マウントプレート３０６のエッジに対して接線方向であるビュー方向を指し示すように配列され、右方向を指し示すように配列されている。同様に、隣接ペアはカメラ３０２Ｃおよび３０２Ｄから構成され得、別のペアはカメラ３０２Ｅおよび３０２Ｆから構成され得、さらに別のペアはカメラ３０２Ｇおよび３０２Ｈから構成され得る。いくつかの実現例では、各カメラ（たとえば３０２Ａ）は、リグ上の各カメラがリグ上の別のカメラとペアリングされ得るように、自身に隣接していないが自身の隣接カメラに隣接しているカメラとペアリングされ得る。いくつかの実現例では、各カメラは（いずれかの側の）自身の直接の隣接カメラとペアリングされ得る。 In one example, camera rig 300 is a circular rig that includes a rotatable or fixed base (not shown) and mount plate 306 (which may also be referred to as a support), and adjacent pairs of cameras are mounted on mount plate 306 A first camera 302A that is installed and configured to point at a viewing direction that is tangential to the edge of the mounting plate 306 and is arranged to point leftward, and the mounting plate 306 alongside the first camera. And the second camera 302B includes a second camera 302B mounted on the mounting plate 306, and the second camera 302B includes Are arranged to point in the view direction, which is tangent to the edge of It is column. Similarly, an adjacent pair may be comprised of cameras 302C and 302D, another pair may be comprised of cameras 302E and 302F, and yet another pair may be comprised of cameras 302G and 302H. In some implementations, each camera (eg, 302A) is not adjacent to itself, but adjacent to its own adjacent camera, such that each camera on the rig can be paired with another camera on the rig Can be paired with a camera. In some implementations, each camera may be paired with its own immediate neighbor (on either side).
いくつかの実現例では、補間モジュール１１４によって１つ以上のステレオ画像が生成され得る。たとえば、カメラリグ３００上に示されるステレオカメラに加えて、付加的なステレオカメラが合成ステレオ画像カメラとして生成され得る。特に、キャプチャされた画像からの光線を分析（たとえばレイトレーシング）することによって、３Ｄシーンのシミュレートされたフレームが生成され得る。当該分析は、視点から特定の画像または画像フレームを通ってシーン内に逆方向に光線を追跡することを含み得る。特定の光線がシーン内のオブジェクトに当たると、当該光線が通過する各画像画素は当該オブジェクトと一致する色でペイントされ得る。光線がオブジェクトに当たらなければ、画像画素は背景またはシーン内の他の特徴と一致する色でペイントされ得る。視点およびレイトレーシングを用いて、補間モジュール１１４はシミュレートされたステレオカメラからであるように見える付加的なシーンコンテンツを生成し得る。付加的なコンテンツは、画像効果、欠けている画像コンテンツ、背景コンテンツ、視野外部のコンテンツを含み得る。 In some implementations, one or more stereo images may be generated by interpolation module 114. For example, in addition to the stereo cameras shown on camera rig 300, additional stereo cameras may be generated as synthetic stereo image cameras. In particular, by analyzing (eg, ray tracing) rays from the captured image, simulated frames of the 3D scene may be generated. The analysis may include tracking rays in the reverse direction into the scene from a point of view through a particular image or image frame. When a particular ray hits an object in the scene, each image pixel through which the ray passes may be painted with a color that matches the object. If the ray does not hit the object, the image pixels can be painted in a color that matches the background or other features in the scene. Using viewpoint and ray tracing, interpolation module 114 may generate additional scene content that appears to be from a simulated stereo camera. Additional content may include image effects, missing image content, background content, content outside the field of view.
図３に示すように、カメラ３０２Ａ〜３０２Ｈは、カメラリグ３００の外周に対して接線方向に配置（たとえば設置）されるので、シーンの最大で１８０度のビューをキャプチャし得る。すなわち、カメラは接線的に設置されるので、完全にオクルージョンされていない１８０度の視野がリグ上の各カメラ内にキャプチャされ得る。 As shown in FIG. 3, the cameras 302A-302H may be placed tangential to the perimeter of the camera rig 300, for example, so as to capture up to a 180 degree view of the scene. That is, since the cameras are installed tangentially, a 180 degree field of view that is not completely occluded can be captured in each camera on the rig.
いくつかの実現例では、カメラリグ３００は隣接カメラを含む。たとえば、リグ３００は隣接カメラ３０２Ａおよび３０２Ｂを含み得る。カメラ３０２Ａは、マウントプレート３０４のエッジに対して接線方向であるビュー方向に方向付けられ、かつ左方向を指し示すように配列された関連のレンズを用いて構成され得る。同様に、カメラ３０２Ｂはカメラ３０２Ａと並んでマウントプレート３０４上に配置され、カメラ３０２Ａからほぼ人間の瞳孔間距離をおいて設置され、マウントプレート３０４のエッジに対して接線方向であるビュー方向を指し示すように配列され、右方向を指し示すように配列され得る。 In some implementations, the camera rig 300 includes adjacent cameras. For example, rig 300 may include adjacent cameras 302A and 302B. Camera 302A may be configured with associated lenses oriented in a view direction that is tangential to the edge of mount plate 304 and arranged to point left. Similarly, the camera 302B is disposed on the mount plate 304 alongside the camera 302A, is placed approximately at the distance between human pupils from the camera 302A, and points at a viewing direction that is tangential to the edge of the mount plate 304 And may be arranged to point to the right.
いくつかの実現例では、実際のカメラ３０２Ａ〜Ｈを接線方向に配置するのではなく、カメラ３０２Ａ〜Ｈ上の（またはカメラリグ３００上の）特定のセンサがカメラ３０２Ａ〜Ｈ（またはリグ３００）の外周に対して接線方向に配置されてもよい。このように、カメラ３０２Ａ〜Ｈはユーザ選択に従って設置され得、センサは、リグ３００の場所、スイープ速度に基づいて、またはカメラ構成および設定に基づいて、どのカメラまたはカメラ３０２Ａ〜Ｈが画像をキャプチャ可能であるかを検出し得る。 In some implementations, rather than placing the actual cameras 302A-H tangentially, certain sensors on the cameras 302A-H (or on the camera rig 300) are of the cameras 302A-H (or rig 300). It may be arranged tangential to the outer circumference. In this manner, cameras 302A-H may be installed according to user selection, and sensors may capture images based on the location of rig 300, sweep speed, or based on camera configuration and settings, which camera or cameras 302A-H. It is possible to detect what is possible.
いくつかの実現例では、隣接カメラは、背中合わせにまたは並んで配列されたカメラ３０２Ａおよびカメラ３０２Ｅを含み得る。また、この配列を用いて、それぞれのカメラレンズおよびマウントプレート３０４によって形成される方位角３０８の左右のビュー角度が集められ得る。いくつかの実現例では、カメラは、それぞれカメラレンズおよびマウントプレート３０４によって形成される方位角３０８の左右に傾斜した角度で配列されている。 In some implementations, adjacent cameras may include cameras 302A and 302E arranged back to back or side by side. Also, using this arrangement, the left and right view angles of the azimuth angle 308 formed by the respective camera lenses and the mount plate 304 can be collected. In some implementations, the cameras are arranged at an angle to the left and right of the azimuthal angle 308 formed by the camera lens and the mount plate 304, respectively.
いくつかの実現例では、カメラリグ３００上に設置されるカメラは、画像補間時にその他の隣接カメラとペアリングされ、外向き方向において円形リグの周りに単純に整列され得る。いくつかの実現例では、リグ３００は単一のカメラ（たとえばカメラ３０２Ａ）を含む。カメラ３０２Ａのみがリグ３００に装着される場合は、カメラリグ３００を時計回りに完全に３６０度回転させることによってステレオパノラマ画像がキャプチャされ得る。 In some implementations, a camera located on camera rig 300 may be paired with other adjacent cameras during image interpolation and simply aligned around the circular rig in the outward direction. In some implementations, rig 300 includes a single camera (eg, camera 302A). If only the camera 302A is attached to the rig 300, a stereo panoramic image can be captured by rotating the camera rig 300 completely 360 degrees clockwise.
図４は、立体パノラマを生成する際に用いるシーンの画像をキャプチャするように構成されたモバイルデバイス４０２を図示する図である。この例では、ユーザがモバイルデバイス４０２を操作しており、自身の環境の周りの経路４０４，４０６内で景色を記録することによって、当該デバイスを用いてビデオ画像を撮影することができる。たとえば、ユーザは、モバイルデバイス４０２を外向きにしてデバイス４０２の下端が地面と平行であるように保持し、時計回りまたは反時計回りに回ってモバイルデバイス４０２を経路４０４，４０６を回ってスイープして、ユーザの体の周囲のビデオを記録することができる。別の例では、ユーザはモバイルデバイス４０２を頭上に保持し、デバイス４０２に経路４０４，４０６をスイープさせるように回るか、または経路４０４，４０６を作るようにデバイス４０２を回転させて、同様の画像を得ることができる。さらに別の例では、ユーザはモバイルデバイス４０２をカメラリグまたは三脚に装着し、カメラを部分的なまたは完全な３６０度の円形路を回ってスピンさせることができる。 FIG. 4 is a diagram illustrating a mobile device 402 configured to capture an image of a scene for use in generating a stereoscopic panorama. In this example, the user is operating the mobile device 402, and by recording the scene in paths 404, 406 around his environment, the device can be used to capture video images. For example, the user may turn the mobile device 402 outward so that the lower end of the device 402 is parallel to the ground, and may sweep the mobile device 402 around the paths 404, 406 clockwise or counterclockwise. The video around the user's body can be recorded. In another example, the user holds the mobile device 402 overhead and turns the device 402 to sweep the paths 404, 406 or rotates the device 402 to create the paths 404, 406 and similar images You can get In yet another example, the user can attach the mobile device 402 to a camera rig or tripod and spin the camera around a partial or complete 360 degree circular path.
ユーザが３６０度ビューをどのようにキャプチャするかにかかわらず、ユーザがナビゲートする経路は、デバイス４０２がユーザの体に対して約９０度に保持されるはっきりと制約された弧が形成される経路４０４をたどらない可能性がある。代わりに、ユーザをそのガイドとして有するモバイルデバイス４０２は、横方向および縦方向の両方において衝突および移動が起こり得る、線４０６によって示す経路を横切る可能性の方が高い。そのような移動は、結果として得られるビデオまたは画像に変化をもたらし、当該ビデオまたは画像に対して後処理が行なわれる場合は問題を引起こし得る。したがって、本開示に記載の方法は、ビデオまたは画像コンテンツ内の不完全にキャプチャされた透視ビューを補正して３Ｄ立体パノラマを提供することができる。 Regardless of how the user captures a 360 degree view, the path the user navigates forms a clearly constrained arc where the device 402 is held approximately 90 degrees to the user's body There is a possibility that the path 404 is not followed. Instead, the mobile device 402 having the user as its guide is more likely to cross the path shown by the line 406 where collisions and movements can occur in both the lateral and longitudinal directions. Such movement may cause changes in the resulting video or image and may cause problems if post-processing is performed on the video or image. Thus, the methods described in this disclosure can correct poorly captured perspective views in video or image content to provide a 3D stereoscopic panorama.
図５は、立体パノラマを生成する際に用いるシーンの画像をキャプチャするように構成された別の例示的なカメラリグ５００を図示する図である。ここで、リグ５００は「クアッドコプタ」、すなわちクアッドロータヘリコプタである。カメラリグ５００は、４つのロータによって上昇して推進可能なマルチロータヘリコプタである。カメラリグ５００は、本開示全体にわたって記載されるように、多数のカメラを用いて構成され得る（たとえば多数のカメラが装備され得る）。当該カメラは、シーンの周りの経路のパノラマをキャプチャするように、またはリグ５００の上部もしくは下部の周りに装着されたいくつかのカメラからシーンの経路の周りの単一の画像をキャプチャするように設定され得る。一般に、シーンの周りの経路とは、ユーザもしくはカメラリグの周りの円、ユーザもしくはカメラリグの周りの円の一部、シーンの画像をキャプチャするモバイルデバイスのスイープ動作によって形成される弧、シーンの３６０度のスイープ、歩くか回ることによって自身の周りのコンテンツをキャプチャしているユーザによって行われる制約されていない経路、またはモバイルデバイスもしくはカメラリグを用いて画像コンテンツをキャプチャするために用いられ得る他の経路を指し得る。 FIG. 5 is a diagram illustrating another exemplary camera rig 500 configured to capture an image of a scene for use in generating a stereoscopic panorama. Here, the rig 500 is a "quadcopter", i.e., a quadrotor helicopter. The camera rig 500 is a multi-rotor helicopter that can be raised and propelled by four rotors. Camera rig 500 may be configured with multiple cameras (eg, multiple cameras may be equipped) as described throughout the present disclosure. The cameras capture a panorama of the path around the scene, or capture a single image around the path of the scene from several cameras mounted around the top or bottom of the rig 500 It can be set. Generally, the path around the scene is the circle around the user or camera rig, a part of the circle around the user or camera rig, the arc formed by the sweep action of the mobile device capturing the image of the scene, 360 degrees of the scene Sweeps, unconstrained paths taken by the user capturing content around them by walking or turning, or other paths that may be used to capture image content using a mobile device or camera rig It can point.
図５に示すように、カメラ５０２およびカメラ５０４がクアッドコプタ５００の底部に配置される。たとえば、クアッドコプタ５００が空中で停止して回転しないことになっている場合は、付加的なカメラを追加してパノラマ内の付加的な画像をキャプチャしてもよい。また、１つのカメラを用いるだけで、３Ｄスイープの３６０度ステレオパノラマ画像をキャプチャすることができる。上述のリグ構成のうちの１つ以上に装着されるカメラと同様に、当該カメラは、特定の弧に対して接線方向に、または別の方法で傾いて中心付けられて装着されて、３６０度の円の一部をキャプチャし得る。 As shown in FIG. 5, a camera 502 and a camera 504 are located at the bottom of quadcopter 500. For example, if quadcopter 500 is to stop and rotate in the air, additional cameras may be added to capture additional images in the panorama. Also, using only one camera, it is possible to capture a 360 degree stereo panoramic image of a 3D sweep. Similar to cameras mounted on one or more of the rig configurations described above, the camera is mounted 360 degrees, tangentially or otherwise tilted and centered with respect to a particular arc Can capture part of the circle.
いくつかの実現例では、ユーザはリモートコントロールまたはコンピュータリモートからクアッドコプタ５００を制御可能である。ユーザは、ビデオ、静止フレーム、スイープフレーム、広いビュー、狭いビュー、斜めのビュー等を含むがこれらに限定されない、どのような種類の画像をキャプチャすべきかについての指示をクアッドコプタに与えることができる。いくつかの実現例では、指示または命令は、特定のシーンが確実にキャプチャされるように、クアッドコプタに装着されたＧＰＳユニットに提供され得る。いくつかの実現例では、ユーザは、シーン内の予め定められた要素（たとえばアクター、木、経路、別のクアッドコプタまたはカメラリグ等）を見つける「追跡」モードを提供し得る。この例では、ユーザは、特定の時間にわたって具体的な経路をたどるようにクアッドコプタ５００を設定可能である。いくつかの実現例では、ユーザは、経路を横切りつつ、または特定の目的地に到着すると、特定の速度で回転するようにクアッドコプタ５００を設定可能である。 In some implementations, the user can control the quadcopter 500 from a remote control or computer remote. The user can give the quadcopter instructions on what type of image to capture, including but not limited to video, still frames, sweep frames, wide views, narrow views, oblique views, etc. In some implementations, instructions or instructions may be provided to a GPS unit attached to the quadcopter to ensure that a particular scene is captured. In some implementations, the user may provide a "tracking" mode to find predetermined elements (e.g., actors, trees, paths, another quadcopter or camera rig, etc.) in the scene. In this example, the user can configure quadcopter 500 to follow a specific path over a particular time. In some implementations, the user can configure quadcopter 500 to rotate at a particular speed as it traverses a route or when arriving at a particular destination.
図６は、立体パノラマを生成する際に用いるシーンの画像をキャプチャするように構成された別の例示的なカメラリグ６００を図示する図である。カメラリグ６００はマウントプレート６０２上に装着され得る。マウントプレート６０２は回転可能なベース６０４上に着座し得る。カメラリグ６００は、カメラリグのマウントプレート６０２の周囲領域を埋める多数の並んだカメラ（たとえばカメラ６０６〜６１０）を含む。カメラはリグ６００の周囲を埋めるように設置されてもよく、あるいは、ビュー角度を最大化してリグ自体のキャプチャ部分を最小化するように戦略的に離間していてもよい。 FIG. 6 is a diagram illustrating another exemplary camera rig 600 configured to capture an image of a scene for use in generating a stereoscopic panorama. Camera rig 600 may be mounted on mounting plate 602. The mounting plate 602 can be seated on the rotatable base 604. Camera rig 600 includes a number of side-by-side cameras (e.g., cameras 606-610) that fill the perimeter area of mount plate 602 of the camera rig. The cameras may be positioned to fill the perimeter of the rig 600 or may be strategically spaced to maximize the view angle and minimize the capture portion of the rig itself.
非限定的な例では、円形カメラリグ６００は、回転可能なベース６０４と平行なアーク運動で回転することによって、マウントプレート６０２上に配置された任意の数のカメラを用いて１セットの画像をキャプチャし得る。いくつかの実現例では、カメラリグ６００はマウントプレート６０２上に配置された隣接カメラを含む。隣接カメラは、約１６０度から約１８０度の関連付けられた視野をキャプチャするように同期化され、構成され、位置決めされ得る。他の視野も可能である。いくつかの実現例では、カメラリグ６００は回転可能なベース上に装着されず、固定された態様で機能する。 In a non-limiting example, circular camera rig 600 captures a set of images using any number of cameras disposed on mount plate 602 by rotating in arc motion parallel to rotatable base 604 It can. In some implementations, camera rig 600 includes adjacent cameras disposed on mount plate 602. Adjacent cameras may be synchronized, configured, and positioned to capture an associated field of view of about 160 degrees to about 180 degrees. Other views are also possible. In some implementations, the camera rig 600 is not mounted on a rotatable base and functions in a fixed manner.
リグ動作の一例では、カメラリグ（たとえばリグ２００，３００，４００，５００または６００）は特定のリグを取囲んでいるシーンのキャプチャ時に第１の方向に回転し、続いて当該シーンのキャプチャ時に異なる方向に回転し得る。たとえば、カメラリグ６００は、リグ６００上のカメラの１つ以上がカメラリグ６００のベース６０４の中心の両側にオフセットしているビュー方向を有して設置された状態で、時計回りに回転し得る。同様に、カメラリグ６００は次いで、任意の数のカメラがシーンの付加的なビューをキャプチャするように左または右を向いた状態で、反時計回り方向に回転し得る。リグ６００の１つの例示的な向きでは、他のすべてのカメラは一方向に向けられ得（たとえばカメラレンズが左方向または右方向に角度を付けられ）、中間のカメラは反対（たとえば左向きまたは右向き）方向に向けられる。 In one example of a rig operation, a camera rig (e.g. rig 200, 300, 400, 500 or 600) rotates in a first direction when capturing a scene surrounding a particular rig, and then in a different direction when capturing the scene Can rotate. For example, the camera rig 600 may rotate clockwise with one or more of the cameras on the rig 600 installed with the view direction offset on either side of the center of the base 604 of the camera rig 600. Similarly, camera rig 600 may then rotate in a counterclockwise direction, with any number of cameras facing left or right to capture additional views of the scene. In one exemplary orientation of the rig 600, all other cameras may be oriented in one direction (e.g. the camera lens is angled left or right) and the middle camera in the opposite (e.g. left or right) ) Oriented.
いくつかの実現例では、ベース６０４は固定され得る。たとえば、リグ６００上の各カメラは、スチルモードで機能する任意のスチルカメラまたはビデオカメラであり得る。したがって、カメラは、周囲のシーンの画像フレームを同時にキャプチャするように同期化および／または構成され得る。アスペクト同士が互いにスティッチングされてステレオパノラマビューが形成され得る。 In some implementations, the base 604 can be fixed. For example, each camera on rig 600 may be any still or video camera that functions in still mode. Thus, the cameras may be synchronized and / or configured to simultaneously capture image frames of the surrounding scene. Aspects may be stitched together to form a stereo panoramic view.
いくつかの実現例では、本開示に記載のカメラリグは、円形ハウジング上に装着される任意の数のカメラを含み得る。いくつかの実現例では、カメラは、円形リグの中心から外向きの４方向の各々に一対のカメラがある状態で等距離に装着され得る。この例では、たとえば立体ペアとして構成されるカメラは周囲に沿って外向きに向けられ得、各立体ペアが３６０度の視野の別個の四分円をキャプチャするように、０度、９０度、１８０度、および２７０度に配置され得る。一般に、カメラの選択可能な視野は、立体ペアのカメラビューの重なりの量、ならびにカメラ間および隣接する四分円間の任意の盲点のサイズを決定する。一例のカメラリグは、約１２０度から最大で約１８０度の視野をキャプチャするように構成された１つ以上の立体カメラペアを採用し得る。 In some implementations, a camera rig described in the present disclosure may include any number of cameras mounted on a circular housing. In some implementations, the cameras may be mounted equidistant with a pair of cameras in each of four directions outward from the center of the circular rig. In this example, for example, cameras configured as stereo pairs may be directed outward along the perimeter, such that each stereo pair captures a separate quadrant of a 360 degree field of view, 0 degrees, 90 degrees, etc. It may be placed at 180 degrees and 270 degrees. Generally, the selectable field of view of the cameras determines the amount of overlap of the camera views of the stereo pair, as well as the size of any blind points between the cameras and between adjacent quadrants. An example camera rig may employ one or more stereo camera pairs configured to capture a field of view of about 120 degrees up to about 180 degrees.
いくつかの実現例では、本開示に記載のカメラリグは約５センチメートルから約８センチメートルの直径（たとえば図２の直径２０６）を有して構成され、人間の瞳孔間距離を模倣して、たとえば、ユーザが自身の頭または体を４分の１円、半円、全円、または円の他の部分だけ回した場合に当該ユーザが見ることになるものをキャプチャし得る。いくつかの実現例では、直径は、リグを横切るカメラレンズ間の距離を指し得る。いくつかの実現例では、直径は、リグを横切るカメラセンサ間の距離を指し得る。いくつかの実現例では、直径は単に、リング状プレートを横切るエッジ間のマウントプレート（たとえばマウントプレート２０８）のサイズを指し得る。 In some implementations, the camera rig described in the present disclosure is configured with a diameter of about 5 centimeters to about 8 centimeters (e.g., diameter 206 in FIG. 2) to mimic human interpupillary distance, For example, one may capture what the user would see if he turned his head or body only a quarter circle, a semicircle, a full circle, or another part of a circle. In some implementations, the diameter may refer to the distance between camera lenses across the rig. In some implementations, the diameter may refer to the distance between camera sensors across the rig. In some implementations, the diameter may simply refer to the size of the mounting plate (eg, mounting plate 208) between the edges across the ring-like plate.
いくつかの実現例では、カメラリグは、たとえば付加的なカメラ取付具を収容するように、約８センチメートルから約２５センチメートルに拡大される。いくつかの実現例では、より小さい直径のリグ上により少ないカメラが用いられ得る。そのような例では、本開示に記載のシステムはリグ上のカメラ間のビューを確認または推測して、そのようなビューを実際のキャプチャされたビューでインターリーブし得る。 In some implementations, the camera rig is expanded from about 8 centimeters to about 25 centimeters, for example, to accommodate additional camera attachments. In some implementations, fewer cameras may be used on smaller diameter rigs. In such instances, the system described in the present disclosure may verify or infer the views between cameras on the rig to interleave such views with the actual captured views.
いくつかの実現例では、本開示に記載のカメラリグを用いて、たとえば回転レンズを有するカメラ、または回転カメラを用いて１回の露光でパノラマ全体をキャプチャすることによってパノラマ画像がキャプチャされ得る。上述のカメラおよびカメラリグは、本開示に記載の方法とともに用いられ得る。特に、１つのカメラリグに関して記載される方法は、本明細書に記載の他のカメラリグのいずれかを用いて実行され得る。いくつかの実現例では、カメラリグおよびその後のキャプチャされたコンテンツは、バーチャルコンテンツ、レンダリングされたコンピュータグラフィックス（ＣＧ）コンテンツといった他のコンテンツ、および／または他の得られたもしくは生成された画像と組合され得る。 In some implementations, a panoramic image can be captured by capturing the entire panorama in a single exposure using, for example, a camera with a rotating lens, or a rotating camera, using the camera rig described in the present disclosure. The cameras and camera rigs described above may be used with the methods described in the present disclosure. In particular, the methods described for one camera rig may be performed using any of the other camera rigs described herein. In some implementations, the camera rig and subsequent captured content may be combined with other content such as virtual content, rendered computer graphics (CG) content, and / or other obtained or generated images. It can be done.
図７は、例示的なＶＲデバイス（ＶＲヘッドセット）７０２を示す図である。ユーザは、ゴーグル、サングラス等を被るのと同様にヘッドセット７０２を自身の目に被せることによってＶＲヘッドセット７０２を身に付けることができる。いくつかの実現例では、図１を参照して、ＶＲヘッドセット７０２は、１つ以上の高速有線および／または無線通信プロトコル（たとえば、Ｗｉ−Ｆｉ、ブルートゥース（登録商標）、ブルートゥースＬＥ、ＵＳＢ等）を用いて、またはＨＤＭＩ（登録商標）インターフェイスを用いることによって、コンピューティングデバイス１０６，１０８または１１２の多数のモニタにインターフェイス／接続し得る。当該接続によって、ＶＲヘッドセット７０２に含まれているスクリーン（図示せず）上でユーザに表示するためのバーチャルコンテンツがＶＲヘッドセット７０２に提供され得る。いくつかの実現例では、ＶＲヘッドセット７０２はキャスト対応デバイスであり得る。これらの実現例では、ユーザはコンテンツをＶＲヘッドセット７０２に提供するか「キャストする」（投影する）かを選択し得る。 FIG. 7 is a diagram illustrating an exemplary VR device (VR headset) 702. The user can wear the VR headset 702 by putting the headset 702 on their eyes as well as wearing goggles, sunglasses, etc. In some implementations, referring to FIG. 1, the VR headset 702 may be configured with one or more high speed wired and / or wireless communication protocols (eg, Wi-Fi, Bluetooth®, Bluetooth LE, USB, etc.) Or can be interfaced / connected to multiple monitors of the computing device 106, 108 or 112, or by using an HDMI interface. The connection may provide the VR headset 702 with virtual content for display to a user on a screen (not shown) included in the VR headset 702. In some implementations, VR headset 702 may be a cast enabled device. In these implementations, the user may choose to provide or “cast” (project) the content to the VR headset 702.
さらに、ＶＲヘッドセット７０２は、１つ以上の高速有線および／または無線通信インターフェイスおよびプロトコル（たとえば、Ｗｉ−Ｆｉ、ブルートゥース、ブルートゥースＬＥ、ユニバーサルシリアルバス（ＵＳＢ）等）を用いてコンピューティングデバイス１０４にインターフェイス／接続し得る。コンピューティングデバイス（図１）はＶＲヘッドセット７０２へのインターフェイスを認識し得、これに応答して、バーチャルコンテンツを含むコンピュータで生成された３Ｄ環境（ＶＲ空間）においてユーザおよびコンピューティングデバイスをレンダリングするＶＲアプリケーションを実行し得る。 In addition, the VR headset 702 may be connected to the computing device 104 using one or more high speed wired and / or wireless communication interfaces and protocols (eg, Wi-Fi, Bluetooth, Bluetooth LE, Universal Serial Bus (USB), etc.) Interface / can connect. The computing device (FIG. 1) may be aware of the interface to the VR headset 702 and responsively render the user and computing device in a computer generated 3D environment (VR space) containing virtual content Can run VR application.
いくつかの実現例では、ＶＲヘッドセット７０２はＶＲアプリケーションを実行可能なリムーバブルコンピューティングデバイスを含み得る。リムーバブルコンピューティングデバイスはコンピューティングデバイス１０８または１１２と同様であり得る。リムーバブルコンピューティングデバイスはＶＲヘッドセット（たとえばＶＲヘッドセット７０２）のケーシングまたはフレーム内に組込まれ得、当該ＶＲヘッドセットは次いでＶＲヘッドセット７０２のユーザによって身に付けられ得る。これらの実現例では、リムーバブルコンピューティングデバイスは、ユーザがコンピュータで生成された３Ｄ環境（ＶＲ空間）と対話する際に見るディスプレイまたはスクリーンを提供し得る。上述のように、モバイルコンピューティングデバイス１０４は有線または無線インターフェイスプロトコルを用いてＶＲヘッドセット７０２に接続し得る。モバイルコンピューティングデバイス１０４はＶＲ空間におけるコントローラであり得、ＶＲ空間におけるオブジェクトとして現れ得、ＶＲ空間に入力を提供し得、ＶＲ空間からフィードバック／出力を受信し得る。 In some implementations, VR headset 702 may include a removable computing device capable of executing a VR application. The removable computing device may be similar to computing device 108 or 112. The removable computing device may be incorporated into the casing or frame of the VR headset (eg, VR headset 702), which may then be worn by the user of VR headset 702. In these implementations, the removable computing device may provide a display or screen that the user views as they interact with the computer generated 3D environment (VR space). As mentioned above, the mobile computing device 104 may connect to the VR headset 702 using a wired or wireless interface protocol. Mobile computing device 104 may be a controller in VR space, may appear as an object in VR space, may provide input to VR space, and may receive feedback / output from VR space.
いくつかの実現例では、モバイルコンピューティングデバイス１０８はＶＲアプリケーションを実行し得、ＶＲ空間を形成するためのデータをＶＲヘッドセット７０２に提供し得る。いくつかの実現例では、ＶＲヘッドセット７０２に含まれているスクリーン上でユーザに対して表示されるＶＲ空間のためのコンテンツは、モバイルコンピューティングデバイス１０８に含まれている表示装置上でも表示され得る。これによって、ユーザがＶＲ空間において対話している可能性があるものを他の誰かも見ることができる。 In some implementations, mobile computing device 108 may execute a VR application and may provide data to VR headset 702 to form VR space. In some implementations, content for the VR space displayed to the user on the screen included in VR headset 702 is also displayed on a display included in mobile computing device 108 obtain. This allows anyone else to see what the user may be interacting in the VR space.
ＶＲヘッドセット７０２は、モバイルコンピューティングデバイス１０８の位置および向きを示す情報およびデータを提供し得る。ＶＲアプリケーションは、当該位置および向きのデータをＶＲ空間内のユーザ対話を示すものとして受信して使用し得る。 The VR headset 702 may provide information and data indicating the position and orientation of the mobile computing device 108. The VR application may receive and use the position and orientation data as an indication of user interaction in VR space.
図８は、カメラ（および隣接カメラ）の数をカメラ視野の関数として示す例示的なグラフ８００である。グラフ８００は、立体パノラマを生成するための予め規定された視野についてカメラリグ上に配置され得るカメラの数を求めるために用いられ得る例示的なグラフを表わす。グラフ８００を用いて、特定のステレオパノラマ結果を保証するためのカメラ設定およびカメラ設置が計算され得る。１つの例示的な設定は、特定のカメラリグに取付けるべきカメラの数の選択を含み得る。別の設定は、キャプチャ時、前処理または後処理ステップ時に用いられるアルゴリズムを求めることを含み得る。たとえば、オプティカルフロー補間技術については、完全な３６０度パノラマをスティッチングすることは、すべての光線方向が少なくとも２つのカメラによって見られるべきであることを要求し得る。これによって、完全な３６０度をカバーするために用いられるカメラの最小数がカメラ視界の関数テータ［θ］として制約され得る。オプティカルフロー補間技術は、隣接カメラ（もしくはペア）によってまたは個々のカメラによって実行されて構成され得る。 FIG. 8 is an exemplary graph 800 showing the number of cameras (and adjacent cameras) as a function of camera view. Graph 800 represents an exemplary graph that may be used to determine the number of cameras that can be placed on a camera rig for a predefined field of view to generate a stereoscopic panorama. Using the graph 800, camera settings and camera settings can be calculated to guarantee a particular stereo panoramic result. One exemplary setting may include the selection of the number of cameras to be attached to a particular camera rig. Another setting may include determining an algorithm to be used during capture, pre-processing or post-processing steps. For example, for optical flow interpolation techniques, stitching a full 360 degree panorama may require that all ray directions should be viewed by at least two cameras. This allows the minimum number of cameras used to cover the full 360 degrees to be constrained as a function θ of the camera view. Optical flow interpolation techniques may be implemented and implemented by adjacent cameras (or pairs) or by individual cameras.
図８に示すように、関数８０２を示すグラフが図示されている。関数８０２は、カメラの数［ｎ］８０４をカメラ視野の関数［θ］８０６として表わしている。この例では、約９５度のカメラ視野が線８０８によって示されている。線８０８と関数８０２との交点８１０は、各々が９５度の視野を有する１６個のカメラを用いると所望のパノラマ結果が提供されることを示している。そのような例では、カメラリグは、カメラの隣接セット毎に隣接カメラをインターリーブして、隣接カメラをリグ上に設置する際に発生し得る任意の空間を用いることによって構成され得る。 As shown in FIG. 8, a graph illustrating the function 802 is illustrated. Function 802 represents the number of cameras [n] 804 as a function [θ] 806 of the camera view. In this example, a camera field of view of approximately 95 degrees is indicated by line 808. The intersection 810 of the line 808 with the function 802 indicates that using 16 cameras, each with a 95 degree field of view, provides the desired panoramic results. In such an example, a camera rig may be configured by interleaving adjacent cameras for each adjacent set of cameras to use any space that may occur when installing adjacent cameras on the rig.
隣接カメラをインターリーブすることに加えて、オプティカルフロー要件は、システム１００が同じ種類のカメラ間のオプティカルフローを計算することを要求し得る。すなわち、オプティカルフローは、両方を同時に計算するのではなく、第１のカメラについて、次いで第２のカメラについて計算され得る。一般に、画素におけるフローは、向き（たとえば方向および角度）ならびに大きさ（たとえば速度）として計算され得る。 In addition to interleaving adjacent cameras, optical flow requirements may require that system 100 calculate optical flow between cameras of the same type. That is, the optical flow may be calculated for the first camera and then for the second camera, rather than calculating both simultaneously. In general, flow at a pixel may be calculated as orientation (eg, direction and angle) and magnitude (eg, velocity).
図９は、補間された視野［θ1］９０２をカメラの視野の関数［θ］９０４として示す例示的なグラフ９００である。グラフ９００を用いて、カメラの視野のどの部分がその左右の隣接カメラと共有されているかが求められ得る。ここで、（線９０６によって示される）約９５度のカメラ視野において、補間された視野は、交点９０８によって示すように、約４８度として示されている。 FIG. 9 is an exemplary graph 900 illustrating the interpolated field of view [θ 1 ] 902 as a function of the field of view of the camera [θ] 904. Graph 900 may be used to determine which part of the camera's field of view is shared with its left and right neighbors. Here, at a camera field of view of about 95 degrees (indicated by line 906), the interpolated field of view is shown as about 48 degrees, as indicated by the intersection 908.
２つの連続したカメラは全く同じ視野の画像を典型的にキャプチャしないと仮定して、補間されたカメラの視野は、隣接カメラ同士の視野の交点によって表わされる。補間された視野［θ1］は次いで、カメラ視野の関数［θ］および隣接カメラ間の角度となり得る。（図８に示す方法を用いて）所与のカメラ視野について最小数のカメラが選択された場合は、図９に示すように［θ1］が［θ］の関数として計算され得る。 Interpolated camera views are represented by the intersection of the views of adjacent cameras, assuming that two consecutive cameras do not typically capture an image of exactly the same view. The interpolated field of view [θ 1 ] may then be a function of the camera field of view [θ] and the angle between adjacent cameras. If a minimum number of cameras are selected for a given camera view (using the method shown in FIG. 8), then [θ 1 ] may be calculated as a function of [θ], as shown in FIG.
図１０は、カメラリグについての構成の選択を示す例示的なグラフ１０００である。特に、グラフ１０００は、どれほど大きい特定のカメラリグが設計され得るかを求めるために用いられ得る。グラフ１０００は、リグ直径［センチメートルで表わすＤ］１００４の関数としてスティッチング比［ｄ／Ｄ］１００２のプロットを図示している。快適なバーチャルリアリティパノラマ視聴体験を作り出すために、全方位ステレオスティッチング直径［ｄ］は、本開示の例では、典型的な人間のＩＰＤである約５センチメートルから約６．５センチメートルであるように選択される。いくつかの実現例では、スティッチング直径［ｄ］とほぼ同じキャプチャ直径［Ｄ］を用いて全方位ステレオスティッチングが実行され得る。すなわち、約「１」のスティッチング比を維持することによって、たとえば全方位ステレオ画像の後処理におけるスティッチングが容易になり得る。スティッチングに用いられる光線は実際のカメラキャプチャされた光線と同じであるので、この特定の構成によって歪みが最小化され得る。カメラの選択数が大きい（たとえばリグ毎に１２〜１８個のカメラ）場合、「１」のスティッチング比を得ることは困難であり得る。 FIG. 10 is an exemplary graph 1000 illustrating configuration choices for a camera rig. In particular, the graph 1000 can be used to determine how large a particular camera rig can be designed. Graph 1000 illustrates a plot of stitching ratio [d / D] 1002 as a function of rig diameter [D in centimeters] 1004. In order to create a comfortable virtual reality panoramic viewing experience, the omnidirectional stereo stitching diameter [d] is, in the example of the present disclosure, about 5 centimeters to about 6.5 centimeters that is a typical human IPD To be chosen. In some implementations, omnidirectional stereo stitching may be performed using a capture diameter [D] that is approximately the same as the stitching diameter [d]. That is, maintaining a stitching ratio of about "1" may facilitate stitching, for example, in post-processing of omnidirectional stereo images. This particular configuration can minimize distortion as the rays used for stitching are the same as the actual camera captured rays. When the number of camera choices is large (e.g., 12-18 cameras per rig), it may be difficult to obtain a "1" stitching ratio.
リグ上のカメラが多すぎるという問題を緩和するために、リグサイズは、付加的なカメラを収容してスティッチング比を同じ（または実質的に同じ）ままに維持できるようにより大きいサイズで設計され得る。スティッチングアルゴリズムが、キャプチャ時にレンズの中心近くで撮影された画像内のコンテンツを確実にサンプリングするように、スティッチング比はリグに対するカメラの角度［α］を求めるように固定され得る。たとえば、図１０は、レンズの中心近くでサンプリングすると画像品質が向上し、幾何学的な歪みが最小化されることを示す。特に、角度［α］が小さいほどリグオクルージョン（たとえば、カメラがリグ自体の部品を撮像すること）を回避するのに役立ち得る。 To alleviate the problem of having too many cameras on the rig, the rig size may be designed with a larger size to accommodate additional cameras and keep the stitching ratio the same (or substantially the same) . The stitching ratio may be fixed to determine the angle [α] of the camera relative to the rig, so that the stitching algorithm reliably samples content in the image taken near the center of the lens at the time of capture. For example, FIG. 10 shows that sampling near the center of the lens improves image quality and minimizes geometric distortion. In particular, smaller angles [α] may help to avoid re-occlusion (eg, camera imaging parts of the rig itself).
図１０の１００６に示すように、０．７５のスティッチング比［ｄ／Ｄ］は約６．５センチメートル（すなわち典型的な人間のＩＰＤ）のリグ直径に対応する。スティッチング比［ｄ／Ｄ］を約０．４５まで減少させるとリグ直径を（１００８に示す）約１５センチメートルに増加させることができ、これによって付加的なカメラをリグに追加することができる。カメラリグに対するカメラの角度は、選択されたスティッチング比に基づいて調整され得る。たとえば、カメラ角度を約３０度に調整することは、リグ直径が約１２．５センチメートルまで大きくなり得ることを示す。同様に、カメラ角度を約２５度に調整することは、リグ直径が１５センチメートルまで大きくなり、たとえば、ユーザに対してレンダリングし返される際に適切な視差および視覚効果を依然として維持することができることを示す。 As shown at 1006 in FIG. 10, a stitching ratio [d / D] of 0.75 corresponds to a rig diameter of about 6.5 centimeters (ie a typical human IPD). Reducing the stitching ratio [d / D] to about 0.45 can increase the rig diameter to about 15 centimeters (shown at 1008), which allows additional cameras to be added to the rig . The angle of the camera relative to the camera rig can be adjusted based on the selected stitching ratio. For example, adjusting the camera angle to about 30 degrees indicates that the rig diameter can be increased to about 12.5 centimeters. Similarly, adjusting the camera angle to about 25 degrees can increase the rig diameter to 15 centimeters, for example, and still maintain proper parallax and visual effects when rendered back to the user Indicates
一般に、リグ直径［Ｄ］を仮定して、最適なカメラ角度［α］が計算され得る。［α］から、最大視野［Θu］が計算され得る。最大視野［Θu］は一般に、リグがカメラを部分的にオクルージョンしない視野に対応する。最大視野は、カメラリグがオクルージョンされていないビューを依然として提供しつつどれほど少数のカメラを保持可能であるかを制限し得る。 In general, given the rig diameter [D], the optimal camera angle [α] can be calculated. From [α], the maximum field of view [Θ u ] can be calculated. The maximum field of view [Θ u ] generally corresponds to a field of view where the rig does not partially occlude the camera. The maximum field of view may limit how small a camera rig can hold a small number of cameras while still providing a non-occluded view.
図１１は、予め規定されたリグ直径に従ってカメラの最小数を求めるために用いられ得る例示的な関係を示すグラフ１１００である。ここで、所与のリグ直径［Ｄ］１１０４についてのカメラの最小数［ｎmin］１１０２が示されている。リグ直径［Ｄ］１１０４はオクルージョンされていない最大視野を制限し、これはカメラの最小数を制限するように機能する。グラフの１１０６に示すように、約１０センチメートルのリグ直径について、最小で１６個のカメラがカメラリグ内で用いられてオクルージョンされていないビューが提供され得る。リグ直径を修正することによって、リグ上に設置されるカメラの数を増減させることができる。一例では、リグは、約８センチメートルから約２５センチメートルのリグサイズ上に約１２個から約１６個のカメラを収容し得る。 FIG. 11 is a graph 1100 illustrating an exemplary relationship that may be used to determine the minimum number of cameras according to a predefined rig diameter. Here, the minimum number of cameras [n min ] 1102 for a given rig diameter [D] 1104 is shown. The rig diameter [D] 1104 limits the maximum unoccluded field of view, which serves to limit the minimum number of cameras. As shown at 1106 in the graph, for a rig diameter of about 10 centimeters, a minimum of 16 cameras may be used in the camera rig to provide an unoccluded view. By modifying the rig diameter, the number of cameras installed on the rig can be increased or decreased. In one example, the rig can accommodate about 12 to about 16 cameras on a rig size of about 8 centimeters to about 25 centimeters.
視野および画像キャプチャ設定を微調整する他の方法も利用可能であるので、これらの計算をこれら他の方法と組合せてカメラリグ寸法をさらに改良することができる。たとえば、オプティカルフローアルゴリズムを用いて、全方位ステレオパノラマをスティッチングするために典型的に用いられるカメラの数を変更する（たとえば減らす）ことができる。いくつかの実現例では、本開示に図示される、または本開示に記載のシステムおよび方法から生成されるグラフを組合せて用いて、たとえばＨＭＤデバイスにおいてレンダリングするためのバーチャルコンテンツを生成することができる。 Because other methods of fine-tuning the field of view and image capture settings are also available, these calculations can be combined with these other methods to further refine the camera rig dimensions. For example, an optical flow algorithm can be used to change (e.g., reduce) the number of cameras typically used to stitch an omnidirectional stereo panorama. In some implementations, the graphs illustrated in this disclosure or generated from the systems and methods described in this disclosure can be used in combination to generate, for example, virtual content for rendering on an HMD device .
図１２Ａ〜図１２Ｂは、画像キャプチャ時に起こり得る歪みの線画の例を表わす。特に、ここに示す歪みは、ステレオパノラマをキャプチャする際に起こる影響に対応する。一般に、歪みは、シーンをキャプチャしているカメラに近接して当該シーンがキャプチャされるとより深刻になり得る。図１２Ａは、２メートル×２メートルの、カメラ中心から外向きに１メートルおいて配置されたシーン内の平面を表わす。図１２Ｂは図１２Ａと同じ平面であるが、この図の平面はカメラから外向きに２５センチメートルおいて配置されている。両図とも６．５センチメートルのキャプチャ直径を用いている。図１２Ａは１２０２において中心近くの若干の伸長を示しており、図１２Ｂはより膨張した中心１２０４を示している。この歪みを補正するために多数の技術が採用され得る。以下の段落では、画像コンテンツをキャプチャした近似方法およびシステム（たとえばカメラリグ／キャプチャデバイス）を用いて投影（たとえば球面および平面投影）を分析して歪みを補正することを説明する。 12A-12B represent examples of distortions that may occur during image capture. In particular, the distortion shown here corresponds to the effects that occur when capturing a stereo panorama. In general, distortion can be more severe when the scene is captured close to the camera capturing the scene. FIG. 12A represents a plane in a scene placed one meter outward from the camera center, 2 meters by 2 meters. FIG. 12B is the same plane as FIG. 12A, but the plane of this figure is located 25 centimeters outward from the camera. Both figures use a capture diameter of 6.5 centimeters. FIG. 12A shows some elongation near the center at 1202, and FIG. 12B shows a more expanded center 1204. A number of techniques can be employed to correct this distortion. The following paragraphs describe analyzing projections (eg, spherical and planar projections) to correct for distortion using approximation methods and systems (eg, camera rigs / capture devices) that have captured image content.
図１３Ａ〜図１３Ｂは、パノラマ画像の収集時にキャプチャされた光線の例を図示する。図１３Ａは、キャプチャされた１セットの画像を仮定して、透視画像がキャプチャ経路１３０２上のどこでも左右両目について生成され得ることを示す。ここで、左目についての光線は光線１３０４ａによって示されており、右目についての光線は１３０６ａにおいて示されている。いくつかの実現例では、図示される光線の各々は、カメラセットアップ、動作不良、または単にシーンの不十分なリグセットアップのためにキャプチャされない場合がある。このため、光線１３０４ａおよび１３０６ａの一部が近似され得る。たとえば、シーンが無限に遠く離れている場合は、シーンの１つの測定可能な特徴は、原点から目的地までの光線方向を含む。 13A-13B illustrate examples of rays captured at the time of acquisition of a panoramic image. FIG. 13A illustrates that a fluoroscopic image can be generated for both the left and right eyes anywhere on the capture path 1302, assuming a captured set of images. Here, the ray for the left eye is shown by ray 1304a and the ray for the right eye is shown at 1306a. In some implementations, each of the illustrated rays may not be captured due to camera setup, malfunction, or simply poor scene rig setup. Thus, portions of rays 1304a and 1306a may be approximated. For example, if the scene is far away at infinity, one measurable feature of the scene includes the ray direction from the origin to the destination.
いくつかの実現例では、光線の原点は収集可能でない場合がある。したがって、本開示のシステムは左目および／または右目を近似して光線の原点位置を求め得る。図１３Ｂは、右目について近似された光線方向１３０６ｂから１３０６ｆを示す。この例では、光線が同じ点から生じているのではなく、各光線は円１３０２上の異なる点から生じている。光線１３０６ｂから１３０６ｆは円１３０２をキャプチャするように接線方向に角度を付けて示されており、キャプチャ円１３０２の周囲の特定の領域に配置されている。また、２つの異なる画像センサ、すなわちカメラリグと関連付けられている画像センサ１３−１および画像センサ１３−２（カメラと関連付けられているかカメラに含まれている）の位置がカメラリグ円１３０３上に示されている。図１３Ｂに示すように、カメラリグ円１３０３はキャプチャ円１３０２より大きい。 In some implementations, the ray origin may not be collectable. Thus, the system of the present disclosure may approximate the left eye and / or the right eye to determine the origin of the ray. FIG. 13B shows ray directions 1306 b to 1306 f approximated for the right eye. In this example, the rays do not originate from the same point, but each ray originates from a different point on the circle 1302. Rays 1306 b through 1306 f are shown tangentially angled to capture circle 1302 and are located at specific areas around capture circle 1302. Also, the position of two different image sensors, namely the image sensor 13-1 associated with the camera rig and the image sensor 13-2 (associated with or contained in the camera) are shown on the camera rig circle 1303 ing. As shown in FIG. 13B, camera rig circle 1303 is larger than capture circle 1302.
多数の光線（ならびに各光線と関連付けられている画像の色および強度）は、円から外向きの異なる方向を用いてこのように近似され得る。このように、多くの画像を含む３６０度パノラマビュー全体が左右両目のビューに提供され得る。この技術は範囲中央のオブジェクトの歪みを解決し得るが、場合によっては、近くのオブジェクトを撮像する際に依然として変形を有し得る。簡潔にするために、近似された左目光線方向は図示していない。この例示的な実現例では、いくつかの光線１３０６ｂから１３０６ｆのみを示している。しかし、何千ものそのような光線（およびそれらの光線と関連付けられている画像）が規定され得る。したがって、光線の各々と関連付けられている多くの新たな画像が規定（たとえば補間）され得る。 A large number of rays (as well as the color and intensity of the image associated with each ray) can be so approximated using different directions outward from the circle. In this way, an entire 360 degree panoramic view, including many images, can be provided for the left and right eye views. While this technique can resolve distortion of mid-range objects, in some cases it may still have distortion when imaging nearby objects. For the sake of simplicity, the approximated left eye ray direction is not shown. In this example implementation, only a few rays 1306b to 1306f are shown. However, thousands of such rays (and the images associated with those rays) may be defined. Thus, many new images associated with each of the rays may be defined (eg, interpolated).
図１３Ｂに示すように、光線１３０６ｂは画像センサ１３−１と画像センサ１３−２との間に投影される。画像センサ１３−１は画像センサ１３−２に隣接している。光線は、画像センサ１３−１（たとえば画像センサ１３−１の投影中心）からの距離Ｇ１、および画像センサ１３−２（たとえば画像センサ１３−２の投影中心）からの距離Ｇ２であり得る。距離Ｇ１およびＧ２は、光線１３０６ｂがカメラリグ円１３０３と交差する場所に基づき得る。距離Ｇ１は距離Ｇ２とは異なっていても（たとえば距離Ｇ２より大きくても小さくても）よい。 As shown in FIG. 13B, the light beam 1306b is projected between the image sensor 13-1 and the image sensor 13-2. The image sensor 13-1 is adjacent to the image sensor 13-2. The light ray may be a distance G1 from the image sensor 13-1 (e.g., the projection center of the image sensor 13-1) and a distance G2 from the image sensor 13-2 (e.g., the projection center of the image sensor 13-2). Distances G1 and G2 may be based on where ray 1306b intersects camera rig circle 1303. The distance G1 may be different from the distance G2 (for example, larger or smaller than the distance G2).
光線１３０６ｂと関連付けられている画像（たとえば補間された画像、新たな画像）を規定するために、画像センサ１３−１がキャプチャした第１の画像（図示せず）は、画像センサ１３−２がキャプチャした第２の画像（図示せず）と組合される（たとえば互いにスティッチングされる）。いくつかの実現例では、オプティカルフロー技術を用いて第１の画像と第２の画像とが組合され得る。たとえば、第２の画像からの画素に対応する第１の画像からの画素が識別され得る。 The first image (not shown) captured by the image sensor 13-1 is used by the image sensor 13-2 to define an image (eg, an interpolated image, a new image) associated with the light beam 1306b. Combined with the captured second image (not shown) (eg, stitched together). In some implementations, optical flow techniques may be used to combine the first and second images. For example, pixels from the first image may be identified that correspond to pixels from the second image.
たとえば光線１３０６ｂと関連付けられている画像を規定するために、対応する画素が距離Ｇ１およびＧ２に基づいてシフトされる。画像センサ１３−１，１３−２の解像度、アスペクト比、高度等は、光線１３０６ｂについての画像（たとえば新たな画像）を規定する目的で同じであると仮定され得る。いくつかの実現例では、解像度、アスペクト比、高度等は異なっていてもよい。しかし、そのような実現例では、それらの差異に対応するために補間を修正しなければならなくなる。 Corresponding pixels are shifted based on the distances G1 and G2, for example to define the image associated with the ray 1306b. The resolution, aspect ratio, altitude, etc. of the image sensors 13-1, 13-2 may be assumed to be the same for the purpose of defining an image (e.g. a new image) for the ray 1306b. In some implementations, the resolution, aspect ratio, elevation, etc. may be different. However, in such an implementation, the interpolation would have to be modified to account for those differences.
具体例として、第１の画像内のオブジェクトと関連付けられている第１の画素が、第２の画像内のオブジェクトと関連付けられている第２の画素に対応するとして識別され得る。第１の画像は（カメラリグ円１３０３の周りの第１の場所にある）画像センサ１３−１の透視からキャプチャされ、第２の画像は（カメラリグ円１３０３の周りの第２の場所にある）画像センサ１３−２の透視からキャプチャされるので、オブジェクトは、第２の画像内の位置（たとえばＸ−Ｙ座標位置）と比較して第１の画像内で位置（Ｘ−Ｙ座標位置）がシフトされる。同様に、オブジェクトと関連付けられている第１の画素は、これもオブジェクトと関連付けられている第２の画素に対して位置（たとえばＸ−Ｙ座標位置）がシフトされる。光線１３０６ｂと関連付けられている新たな画像を生成するために、第１の画素および第２の画素（ならびにオブジェクト）に対応する新たな画素が距離Ｇ１とＧ２との比に基づいて規定され得る。具体的には、新たな画素は、距離Ｇ１に基づく（かつ第１の画素の位置と第２の画素の位置との間の距離に基づく分だけスケール変更される）第１の画素、および距離Ｇ２に基づく（かつ第１の画素の位置と第２の画素の位置との間の距離に基づく分だけスケール変更される）第２の画素から位置がシフトされた場所に規定され得る。 As a specific example, a first pixel associated with an object in a first image may be identified as corresponding to a second pixel associated with an object in a second image. The first image is captured from the perspective of image sensor 13-1 (at a first location around camera rig circle 1303) and the second image is at an image (at a second location around camera rig circle 1303) As captured from the perspective of sensor 13-2, the object is shifted in position (X-Y coordinate position) in the first image compared to the position in the second image (e.g., X-Y coordinate position) Be done. Similarly, a first pixel associated with an object is shifted in position (e.g., an XY coordinate location) relative to a second pixel also associated with the object. New pixels corresponding to the first pixel and the second pixel (as well as the object) may be defined based on the ratio of the distances G1 and G2 to generate a new image associated with the ray 1306b. Specifically, the new pixel is based on the distance G1 (and scaled by an amount based on the distance between the position of the first pixel and the position of the second pixel), and the distance A location may be defined where the location is shifted from the second pixel based on G2 (and scaled by an amount based on the distance between the location of the first pixel and the location of the second pixel).
上述の実現例によると、第１の画像および第２の画像と一致している光線１３０６ｂと関連付けられている新たな画像についての視差が規定され得る。具体的には、カメラリグに比較的近いオブジェクトは、カメラリグから比較的遠いオブジェクトより大量にシフトされ得る。この視差は、光線１３０６ｂの距離Ｇ１およびＧ２に基づいて（たとえば第１の画素および第２の画素から）画素をシフトする間も維持され得る。 According to the implementation described above, parallax may be defined for the new image that is associated with the ray 1306b that is consistent with the first and second images. In particular, objects relatively close to the camera rig may be shifted more than objects relatively far from the camera rig. This disparity may also be maintained while shifting pixels (eg, from the first and second pixels) based on the distances G1 and G2 of the light beam 1306b.
このプロセスは、キャプチャ円１３０２の周りのすべての光線（たとえば光線１３０６ｂから１３０６ｆ）について繰返され得る。キャプチャ円１３０２の周りの光線の各々と関連付けられている新たな画像は、カメラリグ円１３０３の周りの光線および画像センサ（たとえば隣接画像センサ、画像センサ１３−１，１３−２）の各々間の距離に基づいて規定され得る。 This process may be repeated for all rays (eg, rays 1306 b through 1306 f) around capture circle 1302. The new image associated with each of the rays around capture circle 1302 is the distance between each of the rays around camera rig circle 1303 and each of the image sensors (eg adjacent image sensors, image sensors 13-1, 13-2) It can be defined on the basis of
図１３Ｂに示すように、カメラリグ円１３０３の直径はキャプチャ円１３０２の直径より大きい。いくつかの実現例では、カメラリグ円１３０３の直径は、キャプチャ円１３０２の直径より１．５倍から８倍大きくてもよい。具体例として、キャプチャ円の直径は６センチメートルであってもよく、カメラリグ円１３０３（たとえば図４Ａに示すカメラ装着リング４１２）の直径は３０センチメートルであってもよい。 As shown in FIG. 13B, the diameter of the camera rig circle 1303 is larger than the diameter of the capture circle 1302. In some implementations, the diameter of camera rig circle 1303 may be 1.5 to 8 times larger than the diameter of capture circle 1302. As a specific example, the diameter of the capture circle may be 6 centimeters, and the diameter of the camera rig circle 1303 (eg, camera mounting ring 412 shown in FIG. 4A) may be 30 centimeters.
図１４Ａ〜図１４Ｂは、図１３Ａ〜図１３Ｂにおいて記載したような、平面透視投影の近似の使用例を示す。図１４Ａは、平面透視光線および投影を近似する前の歪み線を有するパノラマシーンを示す。示されるように、カーテンロッド１４０２ａ、窓枠１４０４ａ、およびドア１４０６ａは曲線特徴を有するオブジェクトとして図示されているが、実際にはそれらは直線特徴のオブジェクトである。直線特徴のオブジェクトとして、曲面を有さないオブジェクト（たとえば平坦なインデックスカード、四角い箱、矩形の窓枠等）が挙げられる。この例では、オブジェクト１４０２ａ，１４０４ａおよび１４０６ａは画像内で歪んでいるために曲線として示されている。図１４Ｂは、平面透視投影が９０度の水平視野で近似された補正画像を示す。ここで、カーテンロッド１４０２ａ、窓枠１４０４ａ、およびドア１４０６ａは、それぞれ補正された直線オブジェクト１４０２ａ，１４０４ｂおよび１４０４ｃとして示されている。 14A-14B illustrate an example use of the planar perspective projection approximation as described in FIGS. 13A-13B. FIG. 14A shows a panoramic scene with planar perspective rays and distortion lines before approximating the projection. As shown, the curtain rod 1402a, the window frame 1404a, and the door 1406a are illustrated as objects with curvilinear features, but in fact they are linear feature objects. Objects of linear features include objects that do not have curved surfaces (eg, flat index cards, square boxes, rectangular window frames, etc.). In this example, objects 1402a, 1404a and 1406a are shown as curves because they are distorted in the image. FIG. 14B shows a corrected image in which planar perspective projection is approximated by a 90 degree horizontal field of view. Here, the curtain rod 1402a, the window frame 1404a, and the door 1406a are shown as corrected straight objects 1402a, 1404b, and 1404c, respectively.
図１５Ａ〜図１５Ｃは、画像の平面に適用される近似された平面透視投影の例を示す。図１５Ａは、本開示に記載の技術を用いてパノラマから撮られた平面透視投影を示す。図示される平面ビュー１５００は、たとえば図１４Ｂの画像内に示される平面のオーバーレイを表わし得る。特に、図１５Ａは、曲線が直線に投影される補正された図１４Ａを表わす。ここで、パノラマの平面１５００は１メートルの距離をおいて（９０度の水平視野で）示されている。線１５０２，１５０４，１５０６および１５０８は直線であるが、（図１４Ａに対応する）以前は、同じ中心線は湾曲して歪んでいた。 15A-15C illustrate examples of approximated planar perspective projection applied to the plane of the image. FIG. 15A shows a planar perspective projection taken from a panorama using the techniques described in this disclosure. The illustrated planar view 1500 may, for example, represent an overlay of the planes shown in the image of FIG. 14B. In particular, FIG. 15A represents corrected FIG. 14A where the curve is projected into a straight line. Here, the panoramic plane 1500 is shown at a distance of 1 meter (with a 90 degree horizontal field of view). The lines 1502, 1504, 1506 and 1508 are straight lines, but before (corresponding to FIG. 14A) the same center line was curved and distorted.
選択された投影スキームに基づいて他の歪みも発生し得る。たとえば、図１５Ｂおよび図１５Ｃは、本開示の技術を用いてパノラマから撮られた平面透視投影を用いて生成された平面（１５１０および１５２０）を表わす。パノラマは２５センチメートルの距離をおいて（９０度の水平視野）キャプチャされた。図１５Ｂは左目キャプチャ１５１０を示し、図１５Ｃは右目キャプチャ１５２０を示す。ここで、平面の底部（１５１２，１５２２）は直線に投影されておらず、垂直視差が導入されている。この特定の変形は、平面透視投影が用いられると発生し得る。 Other distortions may also occur based on the selected projection scheme. For example, FIGS. 15B and 15C represent planes (1510 and 1520) generated using planar perspective projection taken from a panorama using the techniques of this disclosure. The panorama was captured at a distance of 25 cm (90 degree horizontal field of view). FIG. 15B shows left eye capture 1510 and FIG. 15C shows right eye capture 1520. Here, the bottom of the plane (1512, 1522) is not projected as a straight line, but a vertical parallax is introduced. This particular deformation may occur when planar perspective projection is used.
図１６Ａ〜図１６Ｂは、垂直視差を導入する例を示す。図１６Ａは、典型的な全方位ステレオパノラマ技術に従ってキャプチャされている直線１６０２ａを図示する。図示される例では、各光線１６０４ａ〜１６１８ａは円１６２２上の異なる点から生じている。 16A-16B illustrate an example of introducing vertical parallax. FIG. 16A illustrates a straight line 1602a that has been captured according to a typical omnidirectional stereo panorama technique. In the illustrated example, each ray 1604 a-1618 a originates from a different point on the circle 1622.
図１６Ｂは、透視近似技術を用いて見たときの同じ直線を図示する。示されるように、直線１６０２ａは線１６０２ｂとして変形して示されている。光線１６０４ｂ〜１６１８ｂは円１６２２上の単一点から生じている。この変形は、線１６０２ｂの左半分を視聴者に近づけ、当該線の右半分を視聴者から遠ざける効果を有し得る。左目については、反対のことが起こり得、すなわち、線の左半分が遠くに見え、線の右半分が近くに見える。この変形線は、パノラマレンダリング円１６２２の直径１６２４に等しい距離だけ離れている２本の漸近線間で湾曲している。この変形はパノラマキャプチャ半径と同じサイズであるとして示されているので、当該変形は近くのオブジェクト上でしか目立たない場合がある。この変形の形態は、画像を見ているユーザにとっての垂直視差につながり得、これによって、歪み画像に対してスティッチング処理が行なわれる際にフュージングが困難になり得る。 FIG. 16B illustrates the same straight line as viewed using perspective approximation techniques. As shown, straight line 1602a is shown deformed as line 1602b. Rays 1604 b-1618 b originate from a single point on circle 1622. This variation may have the effect of bringing the left half of the line 1602b closer to the viewer and moving the right half of the line away from the viewer. For the left eye, the opposite can occur, that is, the left half of the line appears far and the right half of the line appears near. This deformation line is curved between two asymptotes separated by a distance equal to the diameter 1624 of the panoramic rendering circle 1622. Because this deformation is shown to be the same size as the panoramic capture radius, the deformation may only be noticeable on nearby objects. This form of deformation can lead to vertical parallax for the user viewing the image, which can make fusing difficult when stitching is performed on the distorted image.
図１７Ａ〜図１７Ｂは、３Ｄパノラマにおける点を示すために用いられ得る座標系の例示的な点を図示する。図１７Ａ〜図１７Ｂは、本開示に記載のパノラマ技術によって撮像された点（０，Ｙ，Ｚ）１７０２を図示する。左右パノラマ内へのこの点の投影は、式（１）および（２）において以下に示すようにそれぞれ（−θ，φ）および（θ，φ）によって表わされ得る： 17A-17B illustrate exemplary points of a coordinate system that may be used to indicate points in a 3D panorama. 17A-17B illustrate point (0, Y, Z) 1702 imaged by the panoramic technique described in the present disclosure. The projection of this point into the left and right panoramas can be represented by (−θ, φ) and (θ, φ) respectively as shown below in equations (1) and (2):
式中、ｒ１７０４はパノラマキャプチャの半径である。
図１７Ａは、点（０，Ｙ，Ｚ）１７０２のパノラマ撮像のトップダウン図を図示する。図１７Ｂは、点（０，Ｙ，Ｚ）１７０２のパノラマ撮像の側面図を図示する。示される点は、左パノラマにおいて（−θ，φ）に投影され、右パノラマにおいて（θ，φ）に投影される。これら特定の図はキャプチャされたままの状態であり、まだ別の平面に投影されていない。
Where r 1704 is the radius of the panoramic capture.
FIG. 17A illustrates a top down view of panoramic imaging of point (0, Y, Z) 1702. FIG. 17B illustrates a side view of panoramic imaging of point (0, Y, Z) 1702. The points shown are projected to (−θ, φ) in the left panorama and to (θ, φ) in the right panorama. These particular views are as captured and not yet projected to another plane.
図１８は、図１７Ａ〜図１７Ｂに図示した点の投影図を表わす。ここで、点１７０２の透視ビューは、図１８の１８０２によって示すように、ｙ軸の周りの回転角度［α］を有して水平方向を見るように向けられている。この透視投影は光線方向のみを考慮しているため、パノラマ投影１８０２内の点１７０２を見る光線を透視カメラの基準フレームに変換することによって、点１７０２が沿って投影される光線を見つけることができる。たとえば、点１７０２は以下の表１に示す以下の光線に沿って投影される： FIG. 18 represents a projection of the points illustrated in FIGS. 17A-17B. Here, the perspective view of point 1702 is oriented to look horizontally with a rotation angle [α] around the y axis, as shown by 1802 in FIG. Because this perspective projection only considers ray direction, converting the ray looking at point 1702 in panoramic projection 1802 into the frame of reference of the perspective camera allows the ray projected along point 1702 to be found . For example, point 1702 is projected along the following rays shown in Table 1 below:
透視分割を行なって、以下の表２の式によって示すように、点投影が求められ得る： By performing perspective segmentation, point projections can be determined, as shown by the equations in Table 2 below:
θ＝π／２（無限に遠く離れている元の３Ｄ点１７０２に対応する）である場合は、点１７０２は両透視画像における同一のｙ座標に全体的に投影されるため、垂直視差はないことが理解され得る。しかし、θがπ／２から離れるにつれて（点がカメラに近づくにつれて）、投影されたｙ座標は（点１７０２に向かって見ている透視ビューに対応するα＝０である場合を除いて）左右の目で異なるようになる。 If θ = π / 2 (corresponding to the original 3D point 1702 which is far away infinitely), there is no vertical parallax since point 1702 is projected entirely to the same y coordinate in both fluoroscopic images It can be understood. However, as θ moves away from π / 2 (as the point approaches the camera), the projected y coordinate is left and right (except when α = 0, which corresponds to the perspective view looking towards point 1702) It will be different with your eyes.
いくつかの実現例では、画像およびシーンを特定の態様でキャプチャすることによって歪みを回避することができる。たとえば、カメラの近接場（すなわちカメラまで１メートル未満）内でシーンをキャプチャすると、歪み要素が現れ得る。したがって、外向きに１メートルからシーンまたは画像をキャプチャすることは歪みを最小化させる方法である。 In some implementations, distortion can be avoided by capturing images and scenes in a specific manner. For example, when capturing a scene within the near field of the camera (ie less than 1 meter to the camera), distortion elements may appear. Thus, capturing a scene or image from one meter outward is a way to minimize distortion.
いくつかの実現例では、歪みは深度情報を用いて補正され得る。たとえば、あるシーンについての正確な深度情報を仮定して、歪みを補正することが可能であり得る。すなわち、歪みは現在のビュー方向に依存し得るため、レンダリング前に単一の歪みをパノラマ画像に適用できない場合がある。代わりに、深度情報をパノラマとともに伝えてレンダリング時に用いることができる。 In some implementations, distortion may be corrected using depth information. For example, it may be possible to correct for distortion given accurate depth information for a scene. That is, it may not be possible to apply a single distortion to a panoramic image before rendering, as the distortion may depend on the current view direction. Alternatively, depth information can be conveyed along with the panorama and used during rendering.
図１９は、本開示に記載のパノラマ撮像技術を用いて全方向ステレオ画像内にキャプチャされた光線を示す。この例では、円１９００の周りで時計回り方向を指し示している光線１９０２，１９０４，１９０６は左目についての光線に対応する。同様に、円１９００の周りで反時計回り方向を指し示している光線１９０８，１９１０，１９１２は右目についての光線に対応する。反時計回りの各光線は、同じ方向を見ている円の反対側に、対応する時計回りの光線を有し得る。これによって、単一の画像内に表わされる光線の方向の各々について左／右ビュー光線が提供され得る。 FIG. 19 shows rays captured in an omnidirectional stereo image using the panoramic imaging techniques described in this disclosure. In this example, rays 1902, 1904, 1906 pointing clockwise around circle 1900 correspond to rays for the left eye. Similarly, rays 1908, 1910, 1912 pointing in a counterclockwise direction around circle 1900 correspond to rays for the right eye. Each counterclockwise ray may have a corresponding clockwise ray on the opposite side of the circle looking in the same direction. This may provide left / right viewing rays for each of the ray directions represented in a single image.
本開示に記載のパノラマについて１セットの光線をキャプチャすることは、カメラ（図示せず）を円１９００上で移動させてカメラを円１９００に対して接線方向に整列させる（たとえば、カメラレンズをシーンにおいて外側を向いて円１９００に対して接線方向を指し示すようにする）ことを含み得る。左目については、カメラは右を指し示し得る（たとえば、光線１９０４は中心線１９１４ａの右側にキャプチャされる）。同様に、右目については、カメラは左を指し示し得る（たとえば、光線１９１０は中心線１９１４ａの左側にキャプチャされる）。円１９００の他方側および中心線１９１４ｂの下にあるカメラについては、中心線１９１４ｂを用いて同様の左右領域が規定され得る。全方向ステレオ画像を生成することは、実際のカメラキャプチャに、または以前にレンダリングされたコンピュータグラフィック（ＣＧ）コンテンツに効果的である。ビュー補間を、キャプチャされたカメラコンテンツおよびレンダリングされたＣＧコンテンツの両方とともに用いて、たとえば円１９００上の実際のカメラ間の点のキャプチャがシミュレートされ得る。 Capturing a set of rays for the panorama described in the present disclosure moves the camera (not shown) over the circle 1900 to align the camera tangentially to the circle 1900 (e.g. And pointing tangentially to the circle 1900). For the left eye, the camera may point to the right (eg, ray 1904 is captured to the right of center line 1914a). Similarly, for the right eye, the camera may point to the left (eg, ray 1910 is captured to the left of center line 1914a). For cameras that are on the other side of circle 1900 and below centerline 1914b, centerline 1914b may be used to define similar left and right regions. Generating an omnidirectional stereo image is effective for actual camera capture, or for previously rendered computer graphic (CG) content. View interpolation may be used with both captured camera content and rendered CG content, for example, to simulate capture of points between actual cameras on circle 1900.
１セットの画像をスティッチングすることは、パノラマ画像を記憶するための球面／正距円筒図法の投影を用いることを含み得る。一般に、この方法では各目に１つずつ、２つの画像が存在する。正距円筒図法を用いた画像内の各画素は球面上の方向に対応する。たとえば、ｘ座標は経度に対応し得、ｙ座標は緯度に対応し得る。単一の全方向画像については、画素についてのビュー光線の原点は同じ点であり得る。しかし、ステレオ画像については、各ビュー光線も円１９００上の異なる点から生じ得る。次いで、キャプチャされた画像内の各画素を分析し、投影モデルから理想的なビュー光線を生成し、ビュー光線が理想的な光線と最も一致しているキャプチャされたまたは補間された画像から画素をサンプリングすることによって、キャプチャされた画像からパノラマ画像がスティッチングされ得る。次に、光線値が互いに混合されてパノラマ画素値が生成され得る。 Stitching a set of images may include using a spherical / regular cylindrical projection for storing a panoramic image. In general, there are two images in this method, one for each eye. Each pixel in the image using equidistant cylindrical projection corresponds to the direction on the spherical surface. For example, the x coordinate may correspond to longitude, and the y coordinate may correspond to latitude. For a single omnidirectional image, the view ray origin for a pixel may be the same point. However, for stereo images, each view ray may also originate from a different point on the circle 1900. It then analyzes each pixel in the captured image and generates an ideal view ray from the projection model, and the pixels from the captured or interpolated image where the view ray best matches the ideal ray By sampling, a panoramic image can be stitched from the captured image. The ray values may then be mixed together to generate panoramic pixel values.
いくつかの実現例では、オプティカルフローベースのビュー補間を用いて、円１９００上の１度毎に少なくとも１つの画像が生成され得る。いくつかの実現例では、列内の１つの画素が所与の画像からサンプリングされる場合は、その列内の他の画素はその同じ画像からサンプリングされるであろうと判断され得るので、パノラマ画像の列全体が一度に埋められ得る。 In some implementations, optical flow based view interpolation may be used to generate at least one image per degree on the circle 1900. In some implementations, if one pixel in a column is sampled from a given image, it may be determined that other pixels in that column will be sampled from that same image, so a panoramic image The entire column of can be filled at once.
本開示のキャプチャおよびレンダリング局面で用いられるパノラマフォーマットによって、左右の目が見るオブジェクトの画像座標は水平シフトのみが異なることが保証され得る。この水平シフトは視差として知られている。これは正距円筒図法の投影に当てはまり、この投影では、オブジェクトは非常に歪んでいると見える場合がある。 The panoramic format used in the capture and rendering phase of the present disclosure may ensure that the image coordinates of objects viewed by the left and right eyes differ only in horizontal shift. This horizontal shift is known as parallax. This is true of the equidistant cylindrical projection, where the object may appear to be very distorted.
この歪みの大きさは、カメラまでの距離およびビュー方向に依存し得る。歪みは、線が曲がった歪み、異なる左右の目の歪みを含み得、いくつかの実現例では、視差はもはや水平に見えない場合がある。一般に、１〜２度の（画像球面上の）垂直視差は人間のユーザによって無理なく許容され得る。また、歪みは周囲目線のオブジェクトについては無視され得る。これは、中心のビュー方向からずれた約３０度と相関している。これらの知見に基づいて、不快な歪みを回避するためにオブジェクトが貫通すべきでないカメラの近くの区域を規定する限界が構築され得る。 The magnitude of this distortion may depend on the distance to the camera and the viewing direction. The distortion may include bent lines, different left and right eye distortions, and in some implementations the parallax may no longer appear horizontal. Generally, one to two degrees of vertical parallax (on the image sphere) can be reasonably tolerated by human users. Also, distortion can be ignored for peripheral gaze objects. This correlates approximately 30 degrees off of the central view direction. Based on these findings, a limit may be established that defines the area near the camera that the object should not penetrate in order to avoid unpleasant distortion.
図２０は、３Ｄ空間における点に起因する最大垂直視差を示すグラフ２０００である。特に、グラフ２０００は、３Ｄ空間における点に起因する最大垂直視差（度で表わす）を、それらが画像の中心から３０度に投影されると仮定して図示している。グラフ２０００は、カメラからの水平位置（メートルで表わす）に対するカメラ中心からの垂直位置（メートルで表わす）をプロットしている。この図では、カメラは原点［０，０］における場所である。グラフが原点から離れるにつれて、歪みの程度は軽くなる。たとえば、グラフ上の（垂直方向の）約０から１の２００２、および０から−１の２００４において、歪みは最悪である。これは、（原点に設置された）カメラの真上および真下の画像に対応する。シーンが外側に移動するにつれて歪みは少なくなり、カメラが点２００６および２００８においてシーンを撮像する時までには、０．５度の垂直視差しか発生しない。 FIG. 20 is a graph 2000 illustrating maximum vertical disparity due to points in 3D space. In particular, graph 2000 illustrates maximum vertical parallax (in degrees) attributable to points in 3D space, assuming that they are projected 30 degrees from the center of the image. A graph 2000 plots the vertical position (in meters) from the camera center to the horizontal position (in meters) from the camera. In this figure, the camera is the location at the origin [0,0]. As the graph moves away from the origin, the degree of distortion decreases. For example, at about 0 to 1 2002 (vertical direction) and 0 to -1 2004 on the graph, the distortion is worst. This corresponds to the images directly above and below the camera (located at the origin). The distortion decreases as the scene moves outward, and by the time the camera captures the scene at points 2006 and 2008, only 0.5 degree vertical parallax occurs.
３０度を超える周囲の歪みを無視できる場合は、ビュー方向が両極の３０度以内にあるすべての画素が除去され得る。周囲閾値が１５度に許可される場合は、１５度分の画素が除去され得る。除去された画素は、たとえば、カラーブロック（たとえば黒、白、マゼンタ等）または静止画像（たとえばロゴ、公知の境界、テクスチャ加工された層等）に設定され得、除去された画素の新たな表現が除去された画素の代わりにパノラマに挿入され得る。いくつかの実現例では、除去された画素はぼやけている場合があり、除去された画素のぼやけた表現が除去された画素の代わりにパノラマに挿入され得る。 If ambient distortion beyond 30 degrees can be ignored, then all pixels whose view direction is within 30 degrees of either pole may be removed. If the ambient threshold is allowed at 15 degrees, 15 degrees of pixels may be removed. The removed pixels may, for example, be set to color blocks (eg black, white, magenta etc.) or still images (eg logos, known boundaries, textured layers etc.) and a new representation of the removed pixels May be inserted into the panorama instead of the removed pixel. In some implementations, the removed pixels may be blurred, and the blurred representation of the removed pixels may be inserted into the panorama instead of the removed pixels.
図２１は、ステレオパノラマ画像を生成するプロセス２１００の一実施形態を図示するフローチャートである。図２１に示すように、ブロック２１０２において、システム１００はキャプチャされた画像に基づいて１セットの画像を規定し得る。当該画像は、前処理された画像、後処理された画像、バーチャルコンテンツ、ビデオ、画像フレーム、画像フレームの一部、画素等を含み得る。 FIG. 21 is a flow chart illustrating one embodiment of a process 2100 for generating stereo panoramic images. As shown in FIG. 21, at block 2102, system 100 may define a set of images based on the captured images. The image may include preprocessed images, postprocessed images, virtual content, video, image frames, portions of image frames, pixels, etc.
規定された画像は、たとえば、ヘッドマウントディスプレイ（ＨＭＤ）を用いてコンテンツ（たとえばＶＲコンテンツ）にアクセスしているユーザによってアクセスされ得る。システム１００はユーザが行なう特定のアクションを判断し得る。たとえば、ある点で、システム１００はブロック２１０４において、ＶＲ ＨＭＤのユーザと関連付けられているビュー方向を受信し得る。同様に、ユーザが自身のビュー方向を変更した場合、システムはブロック２１０６において、ユーザのビュー方向の変更の指示を受信し得る。 The defined image may be accessed by a user accessing content (eg, VR content) using, for example, a head mounted display (HMD). System 100 may determine the particular action to be taken by the user. For example, at some point, system 100 may receive at block 2104 a view direction that is associated with a user of the VR HMD. Similarly, if the user changes his view direction, the system may receive an indication of a change in the user's view direction at block 2106.
そのようなビュー方向の変更の指示を受信したことに応答して、システム１００はブロック２１０８に示すように、１セットの画像の一部の再投影を構成し得る。再投影は、変更されたビュー方向、およびキャプチャされた画像と関連付けられている視野に少なくとも部分的に基づき得る。視野は１度から１８０度であり得、シーンのほんのわずかな画像からシーンの全パノラマ画像までを説明し得る。構成された再投影を用いて、１セットの画像の一部が球面透視投影から平面投影に変換され得る。いくつかの実現例では、再投影は、曲線経路の周りに配列された複数の視点からの１セットの画像と関連付けられているビュー光線の一部を球面透視投影から平面透視投影に再キャストすることを含み得る。 In response to receiving such a view orientation change indication, system 100 may configure a reprojection of a portion of the set of images, as indicated at block 2108. Reprojection may be based at least in part on the altered view direction and the field of view associated with the captured image. The field of view can be from 1 degree to 180 degrees and can account for just a few images of the scene to a full panoramic image of the scene. Using the configured reprojection, a portion of a set of images can be converted from spherical perspective projection to planar projection. In some implementations, reprojection recasts a portion of the view ray associated with a set of images from multiple viewpoints arranged around a curvilinear path from spherical perspective projection to planar perspective projection Can be included.
再投影は、球面シーンの表面の一部を平面シーンにマッピングするいずれかのまたはすべてのステップを含み得る。当該ステップは、歪んだシーンコンテンツをレタッチすること、シームにおけるもしくはシームの近くのシーンコンテンツを混合する（たとえばスティッチングする)こと、トーンマッピング、および／またはスケーリングを含み得る。 Reprojection may include any or all steps of mapping a portion of the surface of a spherical scene to a planar scene. The steps may include retouching distorted scene content, mixing (eg, stitching) scene content at or near a seam, tone mapping, and / or scaling.
再投影を完了すると、システム１００はブロック２１１０に示すように、再投影に基づいて更新されたビューをレンダリングし得る。更新されたビューは、歪みを補正してステレオ視差をユーザに提供するように構成され得る。ブロック２１１２において、システム１００は、変更されたビュー方向に対応するステレオパノラマシーンを含む更新されたビューを提供し得る。たとえば、システム１００は、更新されたビューを提供して（再投影前の）元のビュー内の歪みを補正し得、ＶＲヘッドマウントディスプレイのディスプレイにおいてステレオ視差効果を提供し得る。 Upon completing the reprojection, system 100 may render the updated view based on the reprojection, as shown at block 2110. The updated view may be configured to correct the distortion and provide stereo parallax to the user. At block 2112, the system 100 may provide an updated view that includes a stereo panoramic scene that corresponds to the changed view direction. For example, system 100 may provide an updated view to correct distortion in the original view (prior to reprojection) and provide stereo parallax effects in the display of a VR head mounted display.
図２２は、ステレオパノラマ画像をキャプチャするプロセス２２００の一実施形態を図示するフローチャートである。ブロック２２０２において、システム１００は、少なくとも１セットの隣接カメラから収集されたキャプチャ済のビデオストリームに基づいて１セットの画像を規定し得る。たとえば、システム１００は（図２および図５に示すような）隣接カメラ、または（図３および図６に示すような）複数セットの隣接カメラを用い得る。いくつかの実現例では、システム１００は、約１２個から約１６個のカメラから収集されたキャプチャ済のビデオストリームを用いて１セットの画像を規定し得る。いくつかの実現例では、システム１００は一部のまたはすべてのレンダリングされたコンピュータグラフィックス（ＣＧ）コンテンツを用いて１セットの画像を規定し得る。 FIG. 22 is a flow chart illustrating one embodiment of a process 2200 for capturing stereo panoramic images. At block 2202, system 100 may define a set of images based on captured video streams collected from at least one set of adjacent cameras. For example, system 100 may use adjacent cameras (as shown in FIGS. 2 and 5) or multiple sets of adjacent cameras (as shown in FIGS. 3 and 6). In some implementations, system 100 may define a set of images using captured video streams collected from about 12 to about 16 cameras. In some implementations, system 100 may define a set of images using some or all of the rendered computer graphics (CG) content.
ブロック２２０４において、システム１００は１セットの画像内のオプティカルフローを計算し得る。たとえば、１セットの画像内のオプティカルフローを計算することは、上記に詳述したように、１セットの画像と関連付けられている画素列の一部について画像強度フィールドを分析すること、および画素列の当該一部に対してオプティカルフロー技術を実行することを含み得る。 At block 2204, system 100 may calculate optical flow within a set of images. For example, calculating the optical flow in a set of images may, as detailed above, analyze the image intensity field for a portion of the pixel rows associated with the set of images, and May include performing an optical flow technique on the portion of.
いくつかの実現例では、オプティカルフローを用いて、（ブロック２２０６によって示す）上記に詳述したように、１セットの画像の一部ではない画像フレームが補間され得る。システム１００は次いで、オプティカルフローに少なくとも部分的に基づいて画像フレームと１セットの画像とを互いにスティッチングし得る。ブロック２２０８において、システム１００はインターリーブされたフレームおよび１セットの画像を用いて、ＶＲヘッドマウントディスプレイにおいて表示するために全方位ステレオパノラマを生成し得る。いくつかの実現例では、システム１００は少なくとも１セットのステレオ隣接カメラと関連付けられているポーズ情報を用いて画像スティッチングを実行し、たとえば、インターリーブを実行する前に１セットの画像の一部を予めスティッチングし得る。 In some implementations, optical flow may be used to interpolate image frames that are not part of a set of images, as detailed above (shown by block 2206). System 100 may then stitch together the image frame and the set of images based at least in part on the optical flow. At block 2208, the system 100 may use the interleaved frames and the set of images to generate an omnidirectional stereo panorama for display on a VR head mounted display. In some implementations, system 100 performs image stitching using pose information associated with at least one set of stereo adjacent cameras, eg, to perform a portion of a set of images prior to performing interleaving. It can be pre-stitched.
図２３は、ヘッドマウントディスプレイにおいてパノラマ画像をレンダリングするプロセス２３００の一実施形態を図示するフローチャートである。図２３に示すように、ブロック２３０２において、システム１００は１セットの画像を受信し得る。当該画像は、回転可能なカメラリグからキャプチャされたコンテンツを表わし得る。ブロック２３０４において、システム１００は画像内の画像フレームの部分を選択し得る。画像フレームは、カメラリグでキャプチャされたコンテンツを含み得る。システム１００はキャプチャされたコンテンツの任意の部分を用い得る。たとえば、システム１００は、カメラリグのベースの外向きエッジから約半径１メートルから、カメラリグのベースの外向きエッジから約半径５メートルの距離からリグによってキャプチャされたコンテンツを含む画像フレームの一部を選択し得る。いくつかの実現例では、この選択は、ユーザがどれほど遠くまで３Ｄコンテンツを認識し得るかに基づき得る。ここで、カメラから１メートルからカメラから約５メートルの距離は、ユーザが３Ｄコンテンツを見ることができる「区域」を表わし得る。それより短いと３Ｄビューは歪むことがあり、それより長いとユーザは３Ｄ形状を確認できない場合がある。すなわち、シーンは遠くからでは単に２Ｄに見える場合がある。 FIG. 23 is a flow chart illustrating one embodiment of a process 2300 for rendering a panoramic image on a head mounted display. As shown in FIG. 23, at block 2302, system 100 may receive a set of images. The image may represent content captured from a rotatable camera rig. At block 2304, system 100 may select a portion of the image frame in the image. Image frames may include content captured with a camera rig. System 100 may use any portion of the captured content. For example, the system 100 selects a portion of an image frame containing content captured by the rig from a distance of approximately 1 meter from the outward edge of the base of the camera rig and a distance of approximately 5 meters from the outward edge of the base of the camera rig. It can. In some implementations, this selection may be based on how far the user can recognize 3D content. Here, a distance of 1 meter from the camera to about 5 meters from the camera may represent an "area" in which the user can view 3D content. If it is shorter, the 3D view may be distorted and if it is longer, the user may not be able to confirm the 3D shape. That is, the scene may appear to be simply 2D from a distance.
ブロック２３０６において、画像フレームの選択された部分同士が互いにスティッチングされて立体パノラマビューが生成され得る。この例では、スティッチングは、選択された部分を、選択された部分内の少なくとも１つの他の画像フレームに一致させることに少なくとも部分的に基づき得る。ブロック２３０８において、パノラマビューがＨＭＤデバイスなどのディスプレイにおいて提供され得る。いくつかの実現例では、スティッチングは、カメラリグの直径に少なくとも部分的に基づいて選択されたスティッチング比を用いて実行され得る。いくつかの実現例では、スティッチングは、第１の画像フレーム内の第１の画素列を第２の画像フレーム内の第２の画素列に一致させ、第２の画素列を第３の画像フレーム内の第３の画素列に一致させて、まとまりのあるシーン部分を形成する多数のステップを含む。いくつかの実現例では、多くの画素列がこのように一致させられ組合されてフレームが形成され得、それらのフレームが組合されて画像が形成され得る。さらに、それらの画像が組合されてシーンが形成され得る。 At block 2306, selected portions of the image frame may be stitched together to generate a stereoscopic panoramic view. In this example, stitching may be based at least in part on matching the selected portion to at least one other image frame in the selected portion. At block 2308, a panoramic view may be provided at a display, such as an HMD device. In some implementations, stitching may be performed using a stitching ratio selected based at least in part on the diameter of the camera rig. In some implementations, stitching causes the first row of pixels in the first image frame to coincide with the second row of pixels in the second image frame, and the second row of pixels in the third image A number of steps are made to coincide with the third row of pixels in the frame to form a cohesive scene portion. In some implementations, many pixel columns may be matched and combined in this manner to form a frame, and the frames may be combined to form an image. Furthermore, the images may be combined to form a scene.
いくつかの実現例では、方法２３００は、システム１００を用いて、画像フレームの当該部分の一部ではない付加的な画像フレームを補間する補間ステップを含み得る。そのような補間は、たとえば、遠く離れたカメラによってキャプチャされた画像間にフローが確実に起こるように実行され得る。付加的な画像コンテンツの補間が実行されると、システム１００は付加的な画像フレームを画像フレームの当該部分内にインターリーブしてビューのバーチャルコンテンツを生成し得る。このバーチャルコンテンツは、付加的な画像フレームでインターリーブされた画像フレームの部分として互いにスティッチングされ得る。この結果は、たとえば更新されたビューとしてＨＭＤに提供され得る。この更新されたビューは、画像フレームの当該部分および付加的な画像フレームに少なくとも部分的に基づき得る。 In some implementations, method 2300 may include an interpolation step that uses system 100 to interpolate additional image frames that are not part of that portion of the image frame. Such interpolation may be performed, for example, to ensure that flow occurs between images captured by cameras far away. When interpolation of additional image content is performed, system 100 may interleave additional image frames into that portion of the image frame to generate virtual content of the view. This virtual content may be stitched together as part of an image frame interleaved with additional image frames. This result may be provided to the HMD, for example, as an updated view. The updated view may be based at least in part on the portion of the image frame and the additional image frame.
図２４は、画像境界を求めるプロセス２４００の一実施形態を図示するフローチャートである。ブロック２４０２において、システム１００は、少なくとも１セットの隣接カメラから収集されたキャプチャ済のビデオストリームに基づいて１セットの画像を規定し得る。たとえば、システム１００は（図２および図５に示すような）１セットの隣接カメラ、または（図３および図６に示すような）複数セットの隣接カメラを用い得る。いくつかの実現例では、システム１００は、約１２個から約１６個のカメラから収集されたキャプチャ済のビデオストリームを用いて１セットの画像を規定し得る。いくつかの実現例では、システム１００は一部のまたはすべてのレンダリングされたコンピュータグラフィックス（ＣＧ）コンテンツを用いて１セットの画像を規定し得る。いくつかの実現例では、１セットの画像に対応するビデオストリームは符号化ビデオコンテンツを含む。いくつかの実現例では、１セットの画像に対応するビデオストリームは、１８０度の視野を有して構成された少なくとも１セットの隣接カメラで取得されたコンテンツを含み得る。 FIG. 24 is a flow chart illustrating one embodiment of a process 2400 for determining image boundaries. At block 2402, system 100 may define a set of images based on captured video streams collected from at least one set of adjacent cameras. For example, system 100 may use one set of adjacent cameras (as shown in FIGS. 2 and 5) or multiple sets of adjacent cameras (as shown in FIGS. 3 and 6). In some implementations, system 100 may define a set of images using captured video streams collected from about 12 to about 16 cameras. In some implementations, system 100 may define a set of images using some or all of the rendered computer graphics (CG) content. In some implementations, a video stream corresponding to a set of images includes encoded video content. In some implementations, a video stream corresponding to a set of images may include content acquired with at least one set of adjacent cameras configured with a 180 degree field of view.
ブロック２４０４において、システム１００は、１セットの画像の一部と関連付けられているビュー光線を、円形路の一部に配列された複数の視点から１つの視点に再キャストすることによって、１セットの画像の一部を透視画像面から画像球面上に投影し得る。たとえば、１セットの画像は、多数のカメラをホストし得る円形のカメラリグによってキャプチャされ得る。各カメラは視点と関連付けられ得、それらの視点はシーンにおいてカメラリグから外向きに方向付けられている。特に、単一点から生じるのではなく、ビュー光線はリグ上の各カメラから生じている。システム１００は、経路上のさまざまな視点から単一の視点に光線を再キャストし得る。たとえば、システム１００は、カメラがキャプチャしたシーンの各視点を分析し得、補間された単一の視点からのシーンを表わすシーン（またはシーンのセット）を求めるために類似点および相違点を計算し得る。 At block 2404, the system 100 recasts a set of view rays associated with a portion of the set of images from a plurality of views arranged in a portion of the circular path to a view. A portion of the image may be projected from the perspective image plane onto the image sphere. For example, a set of images may be captured by a circular camera rig that may host multiple cameras. Each camera may be associated with viewpoints, which are directed outward from the camera rig in the scene. In particular, rather than originating from a single point, the viewing rays originate from each camera on the rig. System 100 may recast rays from a variety of points on the path to a single point of view. For example, the system 100 may analyze each viewpoint of the scene captured by the camera and calculate similarities and differences to determine a scene (or set of scenes) representing the scene from a single interpolated viewpoint. obtain.
ブロック２４０６において、システム１００は単一の視点に対応する周囲境界を求め、周囲境界外部の画素を除去することによって更新画像を生成し得る。周囲境界は、歪んだ画像コンテンツから明確で簡潔な画像コンテンツの輪郭を描き得る。たとえば、周囲境界は、歪みのある画素から歪みのない画素の輪郭を描き得る。いくつかの実現例では、周囲境界は、ユーザの典型的な周囲ビュー領域外部のビューに関連し得る。そのような画素を除去することによって、歪んだ画像コンテンツがユーザに不必要に提示されないことが保証され得る。画素を除去することは、上記に詳述したように、画素をカラーブロック、静止画像、または画素のぼやけた表現で置換することを含み得る。いくつかの実現例では、周囲境界は、キャプチャされた画像と関連付けられている１つ以上のカメラについて約１５０度の視野に規定される。いくつかの実現例では、周囲境界は、キャプチャされた画像と関連付けられている１つ以上のカメラについて約１２０度の視野に規定される。いくつかの実現例では、周囲境界は、キャプチャされた画像と関連付けられているカメラについてビュー平面より上の約３０度に対応する球状の一部であり、画素を除去することは球面シーンの上部を黒く塗り潰すか除去することを含む。いくつかの実現例では、周囲境界は、キャプチャされた画像と関連付けられているカメラについてビュー平面より下の約３０度に対応する球状の一部であり、画素を除去することは球面シーンの上部を黒く塗り潰すか除去することを含む。ブロック２４０８において、システム１００は、周囲境界の範囲内の更新画像を表示するために提供し得る。 At block 2406, the system 100 may determine a surrounding boundary corresponding to a single viewpoint and generate an updated image by removing pixels outside the surrounding boundary. The perimeter may delineate clear and concise image content from distorted image content. For example, the perimeter may delineate a pixel without distortion from a pixel with distortion. In some implementations, the perimeter may be associated with a view outside the typical surrounding view area of the user. By removing such pixels, it can be ensured that distorted image content is not unnecessarily presented to the user. Removing a pixel may include replacing the pixel with a color block, a still image, or a blurred representation of the pixel, as detailed above. In some implementations, the perimeter is defined at about a 150 degree field of view for one or more cameras associated with the captured image. In some implementations, a perimeter is defined at about 120 degrees of view for one or more cameras associated with the captured image. In some implementations, the perimeter is part of a sphere that corresponds to about 30 degrees above the view plane for the camera associated with the captured image, and removing pixels is the top of the sphere scene Black out or remove. In some implementations, the perimeter is part of a sphere that corresponds to about 30 degrees below the view plane for the camera associated with the captured image, and removing the pixels is the top of the sphere scene Black out or remove. At block 2408, the system 100 may provide for displaying the updated image within the perimeter boundary.
いくつかの実現例では、方法２４００はさらに、１セットの画像内の少なくとも２つのフレーム同士を互いにスティッチングすることを含み得る。スティッチングは、フレームから画素列をサンプリングし、少なくとも２つのサンプリングされた画素列間に、フレーム内にキャプチャされていない付加的な画素列を補間するステップを含み得る。さらに、スティッチングは、サンプリングされた列および付加的な列を互いに混合して画素値を生成するステップを含み得る。いくつかの実現例では、混合することは、キャプチャされた画像を取得するために用いられる円形カメラリグの直径に少なくとも部分的に基づいて選択されたスティッチング比を用いて実行され得る。スティッチングはさらに、画素値を左シーンおよび右シーン内に構成することによって三次元立体パノラマを生成するステップを含み得、当該パノラマはたとえばＨＭＤにおいて表示するために提供され得る。 In some implementations, the method 2400 may further include stitching together at least two frames in the set of images. Stitching may include sampling pixel columns from the frame and interpolating additional pixel columns not captured in the frame between the at least two sampled pixel columns. Additionally, stitching may include mixing the sampled and additional columns with one another to generate pixel values. In some implementations, mixing may be performed using a stitching ratio selected based at least in part on the diameter of the circular camera rig used to acquire the captured image. Stitching may further include the step of generating a three-dimensional stereo panorama by arranging pixel values in the left and right scenes, which may be provided for display in an HMD, for example.
図２５は、ビデオコンテンツを生成するプロセス２５００の一実施形態を図示するフローチャートである。ブロック２５０２において、システム１００は、少なくとも１セットの隣接カメラから収集されたキャプチャ済のビデオストリームに基づいて１セットの画像を規定し得る。たとえば、システム１００は（図２に示すような）ステレオペア、または（たとえば図３および図６に示すような）複数セットの隣接カメラを用い得る。いくつかの実現例では、システム１００は、約１２個から約１６個のカメラから収集されたキャプチャ済のビデオストリームを用いて１セットの画像を規定し得る。いくつかの実現例では、システム１００は一部のまたはすべてのレンダリングされたコンピュータグラフィックス（ＣＧ）コンテンツを用いて１セットの画像を規定し得る。 FIG. 25 is a flow chart illustrating one embodiment of a process 2500 for generating video content. At block 2502, the system 100 may define a set of images based on captured video streams collected from at least one set of neighboring cameras. For example, system 100 may use stereo pairs (as shown in FIG. 2) or sets of adjacent cameras (eg, as shown in FIGS. 3 and 6). In some implementations, system 100 may define a set of images using captured video streams collected from about 12 to about 16 cameras. In some implementations, system 100 may define a set of images using some or all of the rendered computer graphics (CG) content.
ブロック２５０４において、システム１００は１セットの画像を正距円筒図法のビデオストリームにスティッチングし得る。たとえば、スティッチングは、左方向のカメラキャプチャ角度と関連付けられている画像を、右方向を向いているカメラキャプチャ角度と関連付けられている画像と組合せることを含み得る。 At block 2504, the system 100 may stitch a set of images into an equidistant cylindrical projection video stream. For example, stitching may include combining the image associated with the camera capture angle in the left direction with the image associated with the camera capture angle pointing in the right direction.
ブロック２５０６において、システムは、第１のビューおよび第２のビューについてビデオストリームを正距円筒図法から透視投影にすることによって、ビデオストリームを再生するためにレンダリングし得る。第１のビューはヘッドマウントディスプレイの左目のビューに対応し得、第２のビューはヘッドマウントディスプレイの右目のビューに対応し得る。 At block 2506, the system may render the video stream for playback by rendering the video stream from equidistant cylindrical projection for the first view and the second view. The first view may correspond to the left eye view of the head mounted display, and the second view may correspond to the right eye view of the head mounted display.
ブロック２５０８において、システムは、歪みが予め規定された閾値より高い境界を求め得る。予め規定された閾値は、特定の１セットの画像内に許容される視差のレベル、不一致のレベル、および／または誤差のレベルを提供し得る。歪みは、たとえば１つの平面またはビューから別の平面またはビューにビデオストリームを投影する際の投影構成に少なくとも部分的に基づき得る。 At block 2508, the system may seek boundaries where distortion is above a predefined threshold. The predefined thresholds may provide the level of disparity, the level of inconsistencies, and / or the level of error allowed in a particular set of images. The distortion may be based at least in part on the projection configuration, for example when projecting a video stream from one plane or view to another plane or view.
ブロック２５１０において、システムは、上記に詳述したように、境界以上の１セットの画像内の画像コンテンツを除去することによって、更新されたビデオストリームを生成し得る。ビデオストリームを更新すると、更新されたストリームは、たとえばＨＭＤのユーザに表示するために提供され得る。一般に、本開示全体にわたって記載されるシステムおよび方法は、画像をキャプチャし、キャプチャされた画像から歪みを除去し、画像をレンダリングしてＨＭＤデバイスのユーザに３Ｄ立体ビューを提供するように機能し得る。 At block 2510, the system may generate an updated video stream by removing image content in the set of images above the boundary, as detailed above. Upon updating the video stream, the updated stream may be provided, for example, for display to the HMD user. In general, the systems and methods described throughout the present disclosure may function to capture an image, remove distortion from the captured image, and render the image to provide a 3D stereoscopic view to the user of the HMD device. .
図２６は、本明細書に記載の技術とともに用いられ得る汎用コンピュータデバイス２６００および汎用モバイルコンピュータデバイス２６５０の例を示す。コンピューティングデバイス２６００は、ラップトップ、デスクトップ、ワークステーション、携帯情報端末、サーバ、ブレードサーバ、メインフレーム、および他の適切なコンピュータといった、さまざまな形態のデジタルコンピュータを表わすことを意図している。コンピューティングデバイス２６５０は、携帯情報端末、セルラー電話、スマートフォン、および他の同様のコンピューティングデバイスといった、さまざまな形態のモバイルデバイスを表わすことを意図している。ここに示すコンポーネント、それらの接続および関係、ならびにそれらの機能は例示であることが意図されているに過ぎず、本文書に記載のおよび／または請求項に記載の本発明の実現例を限定することを意図していない。 FIG. 26 illustrates an example of a general purpose computing device 2600 and a general purpose mobile computing device 2650 that may be used in conjunction with the techniques described herein. Computing device 2600 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other suitable computers. Computing device 2650 is intended to represent various forms of mobile devices, such as personal digital assistants, cellular phones, smart phones, and other similar computing devices. The components illustrated herein, their connections and relationships, and their functionality are intended to be exemplary only, and limit the practice of the invention as set forth in this document and / or as claimed. Not intended.
コンピューティングデバイス２６００は、プロセッサ２６０２、メモリ２６０４、記憶装置２６０６、メモリ２６０４および高速拡張ポート２６１０に接続している高速インターフェイス２６０８、ならびに低速バス２６１４および記憶装置２６０６に接続している低速インターフェイス２６１２を含む。コンポーネント２６０２，２６０４，２６０６，２６０８，２６１０および２６１２の各々はさまざまなバスを用いて相互に接続されており、共通のマザーボード上にまたは他の態様で適宜搭載され得る。プロセッサ２６０２は、コンピューティングデバイス２６００内で実行される命令を処理可能であり、この命令には、ＧＵＩのためのグラフィカル情報を高速インターフェイス２６０８に結合されているディスプレイ２６１６などの外部入出力デバイス上に表示するためにメモリ２６０４内または記憶装置２６０６上に記憶されている命令が含まれる。他の実現例では、複数のプロセッサおよび／または複数のバスが、複数のメモリおよび複数種類のメモリとともに必要に応じて用いられ得る。また、複数のコンピューティングデバイス２６００が接続され得、各デバイスは（たとえばサーババンク、ブレードサーバのグループ、またはマルチプロセッサシステムとして）必要な動作の一部を提供する。 The computing device 2600 includes a processor 2602, memory 2604, storage 2606, high speed interface 2608 connected to memory 2604 and high speed expansion port 2610, and low speed interface 2612 connected to low speed bus 2614 and storage 2606. . Each of components 2602, 2604, 2606, 2608, 2610 and 2612 are interconnected using various buses and may be suitably mounted on a common motherboard or otherwise. Processor 2602 can process instructions that are executed within computing device 2600, which include graphical information for the GUI on an external input / output device such as display 2616 coupled to high-speed interface 2608. Included are instructions stored in memory 2604 or on storage device 2606 for display. In other implementations, multiple processors and / or multiple buses may be used as needed with multiple memories and multiple types of memories. Also, multiple computing devices 2600 may be connected, with each device providing part of the required operation (eg, as a server bank, a group of blade servers, or a multiprocessor system).
メモリ２６０４は情報をコンピューティングデバイス２６００内に記憶する。一実現例では、メモリ２６０４は１つまたは複数の揮発性メモリユニットである。別の実現例では、メモリ２６０４は１つまたは複数の不揮発性メモリユニットである。また、メモリ２６０４は、磁気ディスクまたは光ディスクといった別の形態のコンピュータ読取可能媒体であってもよい。 Memory 2604 stores information in computing device 2600. In one implementation, memory 2604 is one or more volatile memory units. In another implementation, memory 2604 is one or more non-volatile memory units. Memory 2604 may also be another form of computer readable media, such as a magnetic or optical disk.
記憶装置２６０６は、コンピューティングデバイス２６００に大容量記憶を提供可能である。一実現例では、記憶装置２６０６は、フロッピー（登録商標）ディスクデバイス、ハードディスクデバイス、光ディスクデバイス、またはテープデバイス、フラッシュメモリもしくは他の同様のソリッドステートメモリデバイス、またはストレージエリアネットワークもしくは他の構成におけるデバイスを含む多数のデバイスといった、コンピュータ読取可能媒体であってもよく、または当該コンピュータ読取可能媒体を含んでいてもよい。コンピュータプログラムプロダクトが情報担体内に有形に具体化され得る。また、コンピュータプログラムプロダクトは、実行されると上述のような１つ以上の方法を実行する命令を含み得る。情報担体は、メモリ２６０４、記憶装置２６０６、またはプロセッサ２６０２上のメモリといった、コンピュータ読取可能媒体または機械読取可能媒体である。 Storage device 2606 can provide mass storage to computing device 2600. In one implementation, storage 2606 may be a floppy disk device, hard disk device, optical disk device, or tape device, flash memory or other similar solid state memory device, or device in a storage area network or other configuration. The computer readable medium may be or include any number of devices, including A computer program product may be tangibly embodied in an information carrier. The computer program product may also include instructions that, when executed, perform one or more methods as described above. The information carrier is a computer readable or machine readable medium, such as memory 2604, storage 2606 or memory on processor 2602.
高速コントローラ２６０８はコンピューティングデバイス２６００のための帯域幅集約的な動作を管理するのに対して、低速コントローラ２６１２はより低い帯域幅集約的な動作を管理する。そのような機能の割当ては例示に過ぎない。一実現例では、高速コントローラ２６０８はメモリ２６０４、ディスプレイ２６１６に（たとえばグラフィックスプロセッサまたはアクセラレータを介して）、およびさまざまな拡張カード（図示せず）を受付け得る高速拡張ポート２６１０に結合される。当該実現例では、低速コントローラ２６１２は記憶装置２６０６および低速拡張ポート２６１４に結合される。さまざまな通信ポート（たとえばＵＳＢ、ブルートゥース、イーサネット（登録商標）、無線イーサネット）を含み得る低速拡張ポートは、キーボード、ポインティングデバイス、スキャナ、またはスイッチもしくはルータといったネットワーキングデバイスなどの１つ以上の入出力デバイスに、たとえばネットワークアダプタを介して結合され得る。 High speed controller 2608 manages bandwidth intensive operations for computing device 2600 while low speed controller 2612 manages lower bandwidth intensive operations. The assignment of such functions is for illustration only. In one implementation, high speed controller 2608 is coupled to memory 2604, to display 2616 (eg, via a graphics processor or accelerator), and to a high speed expansion port 2610 that can receive various expansion cards (not shown). In this implementation, low speed controller 2612 is coupled to storage 2606 and low speed expansion port 2614. Low speed expansion ports, which may include various communication ports (eg, USB, Bluetooth, Ethernet, wireless Ethernet), are one or more input / output devices such as keyboards, pointing devices, scanners, or networking devices such as switches or routers , For example, via a network adapter.
コンピューティングデバイス２６００は、図に示すように多数の異なる形態で実現されてもよい。たとえば、コンピューティングデバイス２６００は標準的なサーバ２６２０として、またはそのようなサーバのグループ内で複数回実現されてもよい。また、コンピューティングデバイス２６００はラックサーバシステム２６２４の一部として実現されてもよい。さらに、コンピューティングデバイス２６００はラップトップコンピュータ２６２２などのパーソナルコンピュータにおいて実現されてもよい。あるいは、コンピューティングデバイス２６００からのコンポーネントは、デバイス２６５０などのモバイルデバイス（図示せず）内の他のコンポーネントと組合されてもよい。そのようなデバイスの各々がコンピューティングデバイス２６００，２６５０の１つ以上を含んでいてもよく、システム全体が、互いに通信する複数のコンピューティングデバイス２６００，２６５０で構成されてもよい。 The computing device 2600 may be implemented in many different forms as shown. For example, computing device 2600 may be implemented multiple times as a standard server 2620 or in a group of such servers. Computing device 2600 may also be implemented as part of rack server system 2624. Further, computing device 2600 may be implemented on a personal computer such as laptop computer 2622. Alternatively, components from computing device 2600 may be combined with other components in a mobile device (not shown), such as device 2650. Each such device may include one or more of the computing devices 2600, 2650, and the entire system may be comprised of multiple computing devices 2600, 2650 in communication with one another.
コンピューティングデバイス２６５０は、数あるコンポーネントの中でも特に、プロセッサ２６５２、メモリ２６６４、ディスプレイ２６５４などの入出力デバイス、通信インターフェイス２６６６、およびトランシーバ２６６８を含む。また、デバイス２６５０には、マイクロドライブまたは他のデバイスなどの記憶装置が提供されて付加的なストレージが提供されてもよい。コンポーネント２６５０，２６５２，２６６４，２６５４，２６６６および２６６８の各々はさまざまなバスを用いて相互に接続されており、当該コンポーネントのいくつかは共通のマザーボード上にまたは他の態様で適宜搭載され得る。 Computing device 2650 includes, among other components, processor 2652, memory 2664, input / output devices such as display 2654, communication interface 2666, and transceiver 2668. Device 2650 may also be provided with storage, such as a microdrive or other device, to provide additional storage. Each of the components 2650, 2652, 2664, 2654, 2666 and 2668 are interconnected using various buses, and some of the components may be suitably mounted on a common motherboard or otherwise.
プロセッサ２６５２は、メモリ２６６４に記憶されている命令を含む、コンピューティングデバイス２６５０内の命令を実行可能である。プロセッサは、別個の複数のアナログおよびデジタルプロセッサを含むチップのチップセットとして実現されてもよい。プロセッサは、たとえば、ユーザインターフェイス、デバイス２６５０が実行するアプリケーション、およびデバイス２６５０による無線通信の制御といった、デバイス２６５０の他のコンポーネントの協調を提供し得る。 Processor 2652 can execute instructions in computing device 2650, including instructions stored in memory 2664. The processor may be implemented as a chipset of chips that include separate analog and digital processors. The processor may provide coordination of other components of device 2650, such as, for example, a user interface, an application that device 2650 executes, and control of wireless communications by device 2650.
プロセッサ２６５２は、ディスプレイ２６５４に結合された制御インターフェイス２６５８およびディスプレイインターフェイス２６５６を介してユーザと通信し得る。ディスプレイ２６５４は、たとえば、ＴＦＴ ＬＣＤ（薄膜トランジスタ液晶ディスプレイ）もしくはＯＬＥＤ（有機発光ダイオード）ディスプレイ、または他の適切なディスプレイ技術であり得る。ディスプレイインターフェイス２６５６は、ディスプレイ２６５４を駆動してグラフィカル情報および他の情報をユーザに提示するための適切な回路を含み得る。制御インターフェイス２６５８はユーザからコマンドを受信し、当該コマンドをプロセッサ２６５２に提出するために変換し得る。さらに、外部インターフェイス２６６２が、デバイス２６５０と他のデバイスとの隣接通信を可能にするために、プロセッサ２６５２と通信した状態で提供されてもよい。外部インターフェイス２６６２は、たとえば、ある実現例では有線通信を提供し、他の実現例では無線通信を提供してもよく、また、複数のインターフェイスが用いられてもよい。 Processor 2652 may communicate with the user via control interface 2658 and display interface 2656 coupled to display 2654. The display 2654 may be, for example, a TFT LCD (thin film transistor liquid crystal display) or an OLED (organic light emitting diode) display, or other suitable display technology. Display interface 2656 may include appropriate circuitry to drive display 2654 to present graphical and other information to the user. Control interface 2658 may receive commands from the user and translate the commands for submission to processor 2652. Further, an external interface 2662 may be provided in communication with the processor 2652 to enable adjacent communication between the device 2650 and other devices. External interface 2662 may, for example, provide wired communication in one implementation, may provide wireless communication in another implementation, and multiple interfaces may be used.
メモリ２６６４は情報をコンピューティングデバイス２６５０内に記憶する。メモリ２６６４は、１つもしくは複数のコンピュータ読取可能媒体、１つもしくは複数の揮発性メモリユニット、または１つもしくは複数の不揮発性メモリユニットの１つ以上として実現され得る。さらに、拡張メモリ２６７４が提供され、たとえばＳＩＭＭ（Single In Line Memory Module）カードインターフェイスを含み得る拡張インターフェイス２６７２を介してデバイス２６５０に接続されてもよい。このような拡張メモリ２６７４はデバイス２６５０に余分のストレージスペースを提供し得るか、またはデバイス２６５０のためのアプリケーションもしくは他の情報をさらに記憶し得る。具体的には、拡張メモリ２６７４は上述のプロセスを実行または補足するための命令を含み得、さらにセキュア情報を含み得る。ゆえに、たとえば、拡張メモリ２６７４はデバイス２６５０のためのセキュリティモジュールとして提供されてもよく、デバイス２６５０のセキュアな使用を許可する命令でプログラムされてもよい。さらに、ハッキング不可能なようにＳＩＭＭカード上に識別情報を置くといったように、セキュアなアプリケーションが付加的な情報とともにＳＩＭＭカードを介して提供されてもよい。 Memory 2664 stores information in computing device 2650. Memory 2664 may be implemented as one or more computer readable media, one or more volatile memory units, or one or more non-volatile memory units. Additionally, expansion memory 2674 may be provided and connected to device 2650 via expansion interface 2672, which may include, for example, a Single In Line Memory Module (SIMM) card interface. Such expanded memory 2674 may provide extra storage space for device 2650, or may additionally store applications or other information for device 2650. Specifically, the expanded memory 2674 may include instructions for performing or supplementing the above-described processes, and may further include secure information. Thus, for example, the expanded memory 2674 may be provided as a security module for the device 2650 and may be programmed with instructions to allow secure use of the device 2650. Additionally, a secure application may be provided via the SIMM card along with additional information, such as placing identification information on the SIMM card in a non-hackable manner.
メモリは、以下に記載のように、たとえばフラッシュメモリおよび／またはＮＶＲＡＭメモリを含み得る。一実現例では、コンピュータプログラムプロダクトが情報担体内に有形に具体化される。コンピュータプログラムプロダクトは、実行されると上述のような１つ以上の方法を実行する命令を含む。情報担体は、たとえばトランシーバ２６６８または外部インターフェイス２６６２上で受信され得る、メモリ２６６４、拡張メモリ２６７４、またはプロセッサ２６５２上のメモリといった、コンピュータ読取可能媒体または機械読取可能媒体である。 The memory may include, for example, flash memory and / or NVRAM memory, as described below. In one implementation, a computer program product is tangibly embodied in an information carrier. The computer program product comprises instructions that, when executed, perform one or more methods as described above. The information carrier is a computer readable or machine readable medium, such as, for example, the memory 2664, the expanded memory 2674 or the memory on the processor 2652 which can be received on the transceiver 2668 or the external interface 2662.
デバイス２６５０は、必要に応じてデジタル信号処理回路を含み得る通信インターフェイス２６６６を介して無線通信し得る。通信インターフェイス２６６６は、とりわけ、ＧＳＭ（登録商標）音声通話、ＳＭＳ、ＥＭＳ、またはＭＭＳメッセージング、ＣＤＭＡ、ＴＤＭＡ、ＰＤＣ、ＷＣＤＭＡ（登録商標）、ＣＤＭＡ２０００、またはＧＰＲＳといった、さまざまなモードまたはプロトコル下の通信を提供し得る。そのような通信は、たとえば無線周波数トランシーバ２６６８を介して起こり得る。さらに、ブルートゥース、Ｗｉ−Ｆｉ、または他のそのようなトランシーバ（図示せず）を用いるなどして、短距離通信が起こり得る。さらに、ＧＰＳ（全地球測位システム）レシーバモジュール２６７０が付加的なナビゲーション関連および位置関連の無線データをデバイス２６５０に提供し得、当該データはデバイス２６５０上で実行されるアプリケーションによって適宜用いられ得る。 Device 2650 may communicate wirelessly via communication interface 2666, which may include digital signal processing circuitry as desired. Communication interface 2666 communicates under various modes or protocols such as GSM voice call, SMS, EMS, or MMS messaging, CDMA, TDMA, PDC, WCDMA, CDMA2000, or GPRS, among others. Can be provided. Such communication may occur, for example, via the radio frequency transceiver 2668. In addition, short-range communication may occur, such as using Bluetooth, Wi-Fi, or other such transceivers (not shown). Further, a GPS (Global Positioning System) receiver module 2670 may provide additional navigation-related and position-related wireless data to the device 2650, which may be used as appropriate by the application running on the device 2650.
また、デバイス２６５０は、ユーザから口頭情報を受信して当該情報を使用可能なデジタル情報に変換し得る音声コーデック２６６０を用いて可聴的に通信し得る。音声コーデック２６６０も同様に、たとえばデバイス２６５０のハンドセット内で、スピーカを介すなどしてユーザに可聴音を生成し得る。そのような音は音声通話からの音を含んでいてもよく、録音された音（たとえば音声メッセージ、音楽ファイル等）を含んでいてもよく、さらに、デバイス２６５０上で実行されるアプリケーションが生成する音を含んでいてもよい。 Device 2650 may also communicate audibly using speech codec 2660, which may receive oral information from the user and convert the information to useable digital information. Audio codec 2660 may similarly generate audible sound for the user, such as, for example, within a handset of device 2650, such as through a speaker. Such sounds may include sounds from voice calls, may include recorded sounds (eg, voice messages, music files, etc.), and may be generated by an application running on device 2650. It may contain sounds.
コンピューティングデバイス２６５０は、図に示すように多数の異なる形態で実現されてもよい。たとえば、コンピューティングデバイス２６５０はセルラー電話２６８０として実現されてもよい。また、コンピューティングデバイス２６５０は、スマートフォン２６８２、携帯情報端末、または他の同様のモバイルデバイスの一部として実現されてもよい。 Computing device 2650 may be implemented in a number of different forms as shown. For example, computing device 2650 may be implemented as a cellular telephone 2680. Computing device 2650 may also be implemented as part of a smart phone 2682, a personal digital assistant, or other similar mobile device.
本明細書に記載のシステムおよび技術のさまざまな実現例は、デジタル電子回路、集積回路、特別に設計されたＡＳＩＣ（特定用途向け集積回路）、コンピュータハードウェア、ファームウェア、ソフトウェア、および／またはそれらの組合せで実現され得る。これらのさまざまな実現例は、少なくとも１つのプログラマブルプロセッサを含むプログラマブルシステム上で実行可能および／または解釈可能な１つ以上のコンピュータプログラムにおける実現例を含んでいてもよく、当該プロセッサは専用であっても汎用であってもよく、ストレージシステム、少なくとも１つの入力デバイス、および少なくとも１つの出力デバイスからデータおよび命令を受信するように、かつこれらにデータおよび命令を送信するように結合されている。 Various implementations of the systems and techniques described herein may be digital electronic circuits, integrated circuits, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and / or theirs. It can be realized in combination. These various implementations may include implementations in one or more computer programs executable and / or interpretable on a programmable system including at least one programmable processor, the processor being dedicated and It may also be general purpose, coupled to receive data and instructions from, and transmit data and instructions from a storage system, at least one input device, and at least one output device.
これらのコンピュータプログラム（プログラム、ソフトウェア、ソフトウェアアプリケーションまたはコードとしても公知）はプログラマブルプロセッサのための機械命令を含んでおり、高レベル手続きおよび／もしくはオブジェクト指向プログラミング言語で、ならびに／またはアセンブリ／機械言語で実現され得る。本明細書において使用する「機械読取可能媒体」「コンピュータ読取可能媒体」という用語は、機械命令および／またはデータをプログラマブルプロセッサに提供するために用いられる任意のコンピュータプログラムプロダクト、装置および／またはデバイス（たとえば磁気ディスク、光ディスク、メモリ、プログラマブルロジックデバイス（ＰＬＤ））を指し、機械命令を機械読取可能信号として受信する機械読取可能媒体を含む。「機械読取可能信号」という用語は、機械命令および／またはデータをプログラマブルプロセッサに提供するために用いられる任意の信号を指す。 These computer programs (also known as programs, software, software applications or codes) contain machine instructions for the programmable processor, in high-level procedural and / or object-oriented programming languages, and / or in assembly / machine language It can be realized. The terms "machine-readable medium" and "computer-readable medium" as used herein refer to any computer program product, apparatus and / or device used to provide machine instructions and / or data to a programmable processor. For example, it refers to a magnetic disk, an optical disk, a memory, a programmable logic device (PLD)) and includes a machine readable medium that receives machine instructions as a machine readable signal. The term "machine readable signal" refers to any signal used to provide machine instructions and / or data to a programmable processor.
ユーザとの対話を提供するために、本明細書に記載のシステムおよび技術は、情報をユーザに表示するための表示装置（たとえばＣＲＴ（陰極線管）またはＬＣＤ（液晶ディスプレイ）モニタ）と、ユーザが入力をコンピュータに提供する際に使用可能なキーボードおよびポインティングデバイス（たとえばマウスまたはトラックボール）とを有するコンピュータ上で実現され得る。他の種類のデバイスを用いてユーザとの対話を提供することもでき、たとえば、ユーザに提供されるフィードバックは任意の形態の感覚フィードバック（たとえば視覚フィードバック、聴覚フィードバック、または触覚フィードバック）であり得、ユーザからの入力は、音響、スピーチ、または触覚入力を含む任意の形態で受信され得る。 In order to provide user interaction, the systems and techniques described herein include a display (e.g., a CRT (Cathode Ray Tube) or LCD (Liquid Crystal Display) monitor) for displaying information to the user; It may be implemented on a computer having a keyboard and pointing device (e.g. a mouse or trackball) that can be used in providing input to the computer. Other types of devices may also be used to provide interaction with the user, for example, the feedback provided to the user may be any form of sensory feedback (eg visual feedback, auditory feedback or tactile feedback), Input from the user may be received in any form, including acoustic, speech, or tactile input.
本明細書に記載のシステムおよび技術は、バックエンドコンポーネントを（たとえばデータサーバとして）含む、またはミドルウェアコンポーネントを（たとえばアプリケーションサーバとして）含む、またはフロントエンドコンポーネント（たとえば、ユーザが本明細書に記載のシステムおよび技術の実現例と対話する際に使用可能なグラフィカルユーザインターフェイスもしくはウェブブラウザを有するクライアントコンピュータ）、またはそのようなバックエンド、ミドルウェア、もしくはフロントエンドコンポーネントの任意の組合せを含むコンピューティングシステムにおいて実現され得る。システムのコンポーネントは、任意の形態または媒体のデジタルデータ通信（たとえば通信ネットワーク）によって相互に接続され得る。通信ネットワークの例として、ローカルエリアネットワーク（「ＬＡＮ」）、ワイドエリアネットワーク（「ＷＡＮ」）、およびインターネットが挙げられる。 The systems and techniques described herein include back-end components (eg, as data servers), or include middleware components (eg, as application servers), or front-end components (eg, users described herein) Implemented on a computing system that includes a graphical user interface or client computer with a web browser that can be used to interact with system and technology implementations, or any combination of such back ends, middleware, or front end components It can be done. The components of the system may be interconnected by any form or medium of digital data communication (eg, a communication network). Examples of communication networks include a local area network ("LAN"), a wide area network ("WAN"), and the Internet.
コンピューティングシステムはクライアントおよびサーバを含み得る。クライアントおよびサーバは一般に互いにリモートであり、典型的に通信ネットワークを介して対話する。クライアントとサーバとの関係は、それぞれのコンピュータ上で実行されて互いにクライアント−サーバ関係を有するコンピュータプログラムによって生じる。 The computing system may include clients and servers. The client and server are generally remote from one another and typically interact via a communication network. The relationship of client and server is created by computer programs running on the respective computers and having a client-server relationship to each other.
多数の実施形態を説明した。しかしながら、明細書の精神および範囲から逸脱することなくさまざまな変更がなされ得ることが理解されるであろう。たとえば、以下の各請求項および上記のそのような請求項の例は任意に組合わせられて、付加的な例示的な実施形態が生成され得る。 A number of embodiments have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the specification. For example, each of the following claims and the examples of such claims above may be combined arbitrarily to generate additional exemplary embodiments.
また、図面に示す論理フローは、所望の結果を達成するために、示されている特定の順序、または順番を必要としない。また、記載のフローとは他のステップが提供されてもよく、または当該フローからステップが除去されてもよく、記載のシステムに他のコンポーネントが追加されてもよく、または当該システムからコンポーネントが除去されてもよい。したがって、他の実施形態も以下の請求項の範囲内にある。 Also, the logic flows depicted in the figures do not require the particular order shown, or order, to achieve desirable results. Also, other steps may be provided with the described flow, or steps may be removed from the flow, other components may be added to the described system, or components may be removed from the system It may be done. Accordingly, other embodiments are within the scope of the following claims.
Claims (15)
前記コンピューティングデバイスにおいて、バーチャルリアリティ（ＶＲ）ヘッドマウントディスプレイのユーザと関連付けられているビュー方向を受信することと、
前記コンピューティングデバイスにおいて、前記ビュー方向の変更の指示を受信することと、
前記指示を受信したことに応答して、前記コンピューティングデバイスによって前記１セットの画像の一部の再投影を構成し、前記再投影は、変更された前記ビュー方向、および前記キャプチャされた画像と関連付けられている視野に少なくとも部分的に基づいており、前記再投影を用いて、前記一部を球面透視投影から平面透視投影に変換して前記一部における垂直視差を減少させ、さらに、前記コンピューティングデバイスによって、前記再投影に基づいて更新されたビューを生成することとを備え、前記更新されたビューは、歪みを補正して前記ヘッドマウントディスプレイの前記ユーザのためにステレオ効果を前記一部内に提供するように構成されており、さらに、
変更された前記ビュー方向に対応するステレオパノラマシーンを含む前記更新されたビューを前記ヘッドマウントディスプレイにおいて表示するためにトリガすることを備える、コンピュータによって実行される方法。 Defining at the computing device a set of images based on the captured images;
Receiving at the computing device a view direction associated with a user of a virtual reality (VR) head mounted display;
Receiving an indication of the view direction change at the computing device;
In response to receiving the indication, configuring the reprojection of a portion of the set of images by the computing device, the reprojection is the modified view direction, and the captured image; Based at least in part on the associated field of view, the reprojection is used to convert the portion from spherical perspective projection to planar perspective projection to reduce vertical parallax in the portion, and further to Device to generate an updated view based on the reprojection, wherein the updated view corrects distortion to within a portion of a stereo effect for the user of the head mounted display. And are configured to provide
A computer-implemented method comprising triggering to display on the head mounted display the updated view that includes a stereo panoramic scene that corresponds to the changed view direction.
前記マウントプレート上に設置され、前記マウントプレートのエッジに対して接線方向であるビュー方向を指し示すように構成され、左方向を指し示すように配列された第１のカメラと、
前記第１のカメラと並んで前記マウントプレート上に設置され、前記第１のカメラから瞳孔間距離をおいて設置された第２のカメラとを含み、前記第２のカメラは、前記マウントプレートのエッジに対して接線方向であるビュー方向を指し示すように配列され、右方向を指し示すように配列されている、請求項１または２に記載の方法。 The set of images is captured using a camera rig, the camera rig being circular and configured to move in an arc motion parallel to the rotatable base of the camera rig when captured, the camera rig including The at least one stereo pair comprises a base, a mounting plate, and at least one stereo pair of cameras,
A first camera disposed on the mount plate, configured to point in a view direction that is tangential to the edge of the mount plate, and arranged to point in the left direction;
And a second camera installed on the mount plate in parallel with the first camera and installed at a pupil distance from the first camera, the second camera being The method according to claim 1 or 2, wherein the method is arranged to point to a view direction that is tangential to the edge and to point right.
命令を格納しているメモリとを備える、コンピュータによって実行されるシステムであって、前記命令は、前記少なくとも１つのプロセッサによって実行されると前記システムに動作を実行させ、前記動作は、
コンピューティングデバイスにおいて、キャプチャされた画像に基づいて１セットの画像を規定することと、
前記コンピューティングデバイスにおいて、バーチャルリアリティ（ＶＲ）ヘッドマウントディスプレイのユーザと関連付けられているビュー方向を受信することと、
前記コンピューティングデバイスにおいて、前記ビュー方向の変更の指示を受信することと、
前記指示を受信したことに応答して、前記コンピューティングデバイスによって前記１セットの画像の一部の再投影を構成し、前記再投影は、変更された前記ビュー方向、および前記キャプチャされた画像と関連付けられている視野に少なくとも部分的に基づいており、前記再投影を用いて、前記一部を球面透視投影から平面透視投影に変換して前記一部における垂直視差を減少させ、さらに、前記コンピューティングデバイスによって、前記再投影に基づいて更新されたビューを生成することとを備え、前記更新されたビューは、歪みを補正して前記ヘッドマウントディスプレイの前記ユーザのためにステレオ効果を前記一部内に提供するように構成されており、さらに、
変更された前記ビュー方向に対応するステレオパノラマシーンを含む前記更新されたビューを前記ヘッドマウントディスプレイにおいて表示するためにトリガすることを備えるコンピュータによって実行されるシステム。 At least one processor,
A computer implemented system comprising a memory storing instructions, the instructions causing the system to perform an action when executed by the at least one processor, the action being:
Defining at the computing device a set of images based on the captured images;
Receiving at the computing device a view direction associated with a user of a virtual reality (VR) head mounted display;
Receiving an indication of the view direction change at the computing device;
In response to receiving the indication, configuring the reprojection of a portion of the set of images by the computing device, the reprojection is the modified view direction, and the captured image; Based at least in part on the associated field of view, the reprojection is used to convert the portion from spherical perspective projection to planar perspective projection to reduce vertical parallax in the portion, and further to Device to generate an updated view based on the reprojection, wherein the updated view corrects distortion to within a portion of a stereo effect for the user of the head mounted display. And are configured to provide
A computer-implemented system comprising triggering to display the updated view on the head mounted display comprising a stereo panoramic scene corresponding to the changed view direction.
前記マウントプレート上に設置され、前記マウントプレートのエッジに対して接線方向であるビュー方向を指し示すように構成され、左方向を指し示すように配列された第１のカメラと、
前記第１のカメラと並んで前記マウントプレート上に設置され、前記第１のカメラから瞳孔間距離をおいて設置された第２のカメラとを含み、前記第２のカメラは、前記マウントプレートのエッジに対して接線方向であるビュー方向を指し示すように配列され、右方向を指し示すように配列されている、請求項８または９に記載のシステム。 The set of images is captured using a camera rig, the camera rig being circular and configured to move in an arc motion parallel to the rotatable base of the camera rig when captured, the camera rig including The at least one stereo pair comprises a base, a mounting plate, and at least one stereo pair of cameras,
A first camera disposed on the mount plate, configured to point in a view direction that is tangential to the edge of the mount plate, and arranged to point in the left direction;
And a second camera installed on the mount plate in parallel with the first camera and installed at a pupil distance from the first camera, the second camera being 10. A system according to claim 8 or 9 , arranged to point at a view direction that is tangential to the edge and point to the right.
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/723,151 | 2015-05-27 | ||
US14/723,178 | 2015-05-27 | ||
US14/723,178 US9877016B2 (en) | 2015-05-27 | 2015-05-27 | Omnistereo capture and render of panoramic virtual reality content |
US14/723,151 US10038887B2 (en) | 2015-05-27 | 2015-05-27 | Capture and render of panoramic virtual reality content |
PCT/US2016/034077 WO2016191467A1 (en) | 2015-05-27 | 2016-05-25 | Capture and render of panoramic virtual reality content |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2018522429A JP2018522429A (en) | 2018-08-09 |
JP6511539B2 true JP6511539B2 (en) | 2019-05-15 |
Family
ID=56108721
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2017550745A Active JP6511539B2 (en) | 2015-05-27 | 2016-05-25 | Capturing and Rendering Panoramic Virtual Reality Content |
JP2017550739A Active JP6427688B2 (en) | 2015-05-27 | 2016-05-25 | Omnidirectional stereo capture and rendering of panoramic virtual reality content |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2017550739A Active JP6427688B2 (en) | 2015-05-27 | 2016-05-25 | Omnidirectional stereo capture and rendering of panoramic virtual reality content |
Country Status (5)
Country | Link |
---|---|
EP (3) | EP3304894A1 (en) |
JP (2) | JP6511539B2 (en) |
KR (2) | KR101944050B1 (en) |
CN (2) | CN107431796B (en) |
WO (2) | WO2016191467A1 (en) |
Families Citing this family (46)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9877016B2 (en) | 2015-05-27 | 2018-01-23 | Google Llc | Omnistereo capture and render of panoramic virtual reality content |
US10038887B2 (en) | 2015-05-27 | 2018-07-31 | Google Llc | Capture and render of panoramic virtual reality content |
US10217189B2 (en) * | 2015-09-16 | 2019-02-26 | Google Llc | General spherical capture methods |
CN108696735A (en) * | 2017-03-01 | 2018-10-23 | 中兴通讯股份有限公司 | Virtual reality glasses, intelligent terminal and guide realize and control method |
GB2560185A (en) * | 2017-03-03 | 2018-09-05 | Nokia Technologies Oy | Method and apparatus for a multi-camera unit |
US10579898B2 (en) * | 2017-04-16 | 2020-03-03 | Facebook, Inc. | Systems and methods for provisioning content using barrel projection representation |
US10885711B2 (en) | 2017-05-03 | 2021-01-05 | Microsoft Technology Licensing, Llc | Virtual reality image compositing |
ES2695250A1 (en) * | 2017-06-27 | 2019-01-02 | Broomx Tech S L | Procedure to project immersive audiovisual content (Machine-translation by Google Translate, not legally binding) |
CN107392851A (en) * | 2017-07-04 | 2017-11-24 | 上海小蚁科技有限公司 | Method and apparatus for generating panoramic picture |
KR102336997B1 (en) * | 2017-08-16 | 2021-12-08 | 삼성전자 주식회사 | Server, display apparatus and control method thereof |
CN107396077B (en) * | 2017-08-23 | 2022-04-08 | 深圳看到科技有限公司 | Virtual reality panoramic video stream projection method and equipment |
WO2019066191A1 (en) * | 2017-09-28 | 2019-04-04 | 엘지전자 주식회사 | Method and device for transmitting or receiving 6dof video using stitching and re-projection related metadata |
KR102138413B1 (en) * | 2017-12-18 | 2020-07-27 | 숭실대학교산학협력단 | Frame for standing plurality of laser radars, Laser radar system including the frame, and Method for integrating sensing data coordinates of the plurality of laser radars |
US10585294B2 (en) * | 2018-02-19 | 2020-03-10 | Microsoft Technology Licensing, Llc | Curved display on content in mixed reality |
JP7047095B2 (en) * | 2018-02-27 | 2022-04-04 | エルジー エレクトロニクス インコーポレイティド | A method for transmitting and receiving 360 ° video including camera lens information and its device |
US10827164B2 (en) * | 2018-03-01 | 2020-11-03 | Google Llc | Active LCD shutters for virtual and augmented reality low persistence |
US10582181B2 (en) * | 2018-03-27 | 2020-03-03 | Honeywell International Inc. | Panoramic vision system with parallax mitigation |
US11044456B2 (en) | 2018-05-31 | 2021-06-22 | Electronics And Telecommunications Research Institute | Image processing method and image player using thereof |
US10194114B1 (en) * | 2018-06-22 | 2019-01-29 | Polycom, Inc. | Split screen display without distortion |
KR102073230B1 (en) * | 2018-06-28 | 2020-02-04 | 주식회사 알파서클 | Apparaturs for playing vr video to improve quality of specific area |
KR102491939B1 (en) * | 2018-08-24 | 2023-01-26 | 삼성전자주식회사 | Method and apparatus for processing omnidirectional image |
CN109248428A (en) * | 2018-09-17 | 2019-01-22 | 武汉中奥互联科技有限公司 | A kind of dynamic analysing method of tennis trajectory processing system |
CN109464801A (en) * | 2018-10-29 | 2019-03-15 | 奇想空间（北京）教育科技有限公司 | Game station |
KR102168318B1 (en) * | 2018-12-03 | 2020-10-22 | 동국대학교 산학협력단 | Display device and method for mixed reality |
EP3667414B1 (en) * | 2018-12-14 | 2020-11-25 | Axis AB | A system for panoramic imaging |
EP3691249A1 (en) | 2019-01-29 | 2020-08-05 | Koninklijke Philips N.V. | Image signal representing a scene |
EP3690822A1 (en) | 2019-01-30 | 2020-08-05 | Koninklijke Philips N.V. | Image representation of a scene |
US11194438B2 (en) | 2019-05-09 | 2021-12-07 | Microsoft Technology Licensing, Llc | Capture indicator for a virtual world |
CN112307848B (en) * | 2019-08-01 | 2024-04-30 | 惠普发展公司，有限责任合伙企业 | Detecting spoofed speakers in video conferencing |
KR102135001B1 (en) * | 2019-10-21 | 2020-07-16 | 주식회사 알파서클 | Apparaturs for playing plural vr video in sync or async method |
CN111131767B (en) * | 2019-11-27 | 2021-06-29 | 重庆特斯联智慧科技股份有限公司 | Scene three-dimensional imaging security monitoring system and using method |
CN111314687B (en) * | 2019-11-28 | 2021-06-25 | 歌尔光学科技有限公司 | VR image processing method and device, VR glasses and readable storage medium |
US11150470B2 (en) | 2020-01-07 | 2021-10-19 | Microsoft Technology Licensing, Llc | Inertial measurement unit signal based image reprojection |
US11917119B2 (en) | 2020-01-09 | 2024-02-27 | Jerry Nims | 2D image capture system and display of 3D digital image |
CN111292336B (en) * | 2020-01-21 | 2023-06-06 | 宁波大学 | Omnidirectional image non-reference quality evaluation method based on segmented spherical projection format |
CN111569432B (en) * | 2020-05-19 | 2021-01-15 | 北京中科深智科技有限公司 | System and method for capturing 6DoF scene image from game |
WO2021247405A1 (en) * | 2020-06-03 | 2021-12-09 | Jerry Nims | 2d image capture system & display of 3d digital image |
CN116097167A (en) * | 2020-06-03 | 2023-05-09 | 杰瑞·尼姆斯 | Two-dimensional image capturing system and transmission and display of three-dimensional digital images |
CN111954054B (en) * | 2020-06-05 | 2022-03-04 | 筑觉绘(上海)科技有限公司 | Image processing method, system, storage medium and computer device |
CN114390262A (en) * | 2020-10-21 | 2022-04-22 | 中强光电股份有限公司 | Method and electronic device for splicing three-dimensional spherical panoramic image |
CN112804511B (en) * | 2021-01-04 | 2022-04-01 | 烽火通信科技股份有限公司 | Method and device for dynamically rendering panoramic video |
CN112907447B (en) * | 2021-02-08 | 2022-07-01 | 杭州海康威视数字技术股份有限公司 | Splicing of sky cloud pictures and method for determining installation positions of multiple cameras |
US11516517B2 (en) | 2021-03-19 | 2022-11-29 | Sm Tamjid | Localized dynamic video streaming system |
CN113645462B (en) * | 2021-08-06 | 2024-01-16 | 深圳臻像科技有限公司 | Conversion method and device for 3D light field |
CN113706680B (en) * | 2021-09-02 | 2023-09-19 | 长春理工大学 | VR film picture rendering pixel anti-distortion processing method driven by visual saliency |
CN114500971B (en) * | 2022-02-12 | 2023-07-21 | 北京蜂巢世纪科技有限公司 | Venue 3D panoramic video generation method and device based on data sharing, head-mounted display equipment and medium |
Family Cites Families (24)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5020878A (en) * | 1989-03-20 | 1991-06-04 | Tektronix, Inc. | Method and apparatus for generating a binocular viewing model automatically adapted to a selected image |
GB9124925D0 (en) * | 1991-11-23 | 1992-01-22 | Kew Michael J | Simulated three-dimensional imaging system |
JP3600422B2 (en) * | 1998-01-30 | 2004-12-15 | 株式会社リコー | Stereo image display method and apparatus |
US6801334B1 (en) * | 1998-05-28 | 2004-10-05 | Fuji Photo Film Co., Ltd. | Index print producing method, image processing system, image processing method and image processing device |
US6545702B1 (en) * | 1998-09-08 | 2003-04-08 | Sri International | Method and apparatus for panoramic imaging |
ATE420528T1 (en) * | 1998-09-17 | 2009-01-15 | Yissum Res Dev Co | SYSTEM AND METHOD FOR GENERATING AND DISPLAYING PANORAMIC IMAGES AND FILMS |
US7525567B2 (en) * | 2000-02-16 | 2009-04-28 | Immersive Media Company | Recording a stereoscopic image of a wide field of view |
JP3557168B2 (en) * | 2000-11-27 | 2004-08-25 | 三洋電機株式会社 | Lens distortion coefficient calculation device and calculation method, computer-readable recording medium storing lens distortion coefficient calculation program |
JP2005167638A (en) * | 2003-12-02 | 2005-06-23 | Sharp Corp | Mobile surrounding surveillance apparatus, vehicle, and image transforming method |
JP2005277825A (en) * | 2004-03-25 | 2005-10-06 | Hitachi Ltd | Multi-camera image display device |
US20070024701A1 (en) * | 2005-04-07 | 2007-02-01 | Prechtl Eric F | Stereoscopic wide field of view imaging system |
JP2006350852A (en) * | 2005-06-17 | 2006-12-28 | Nabura:Kk | Image generation system |
US7742046B2 (en) * | 2005-08-31 | 2010-06-22 | Kabushiki Kaisha Toshiba | Method, device, and program for producing elemental image array for three-dimensional image display |
US9270976B2 (en) * | 2005-11-02 | 2016-02-23 | Exelis Inc. | Multi-user stereoscopic 3-D panoramic vision system and method |
JP4987890B2 (en) * | 2009-02-05 | 2012-07-25 | 株式会社東芝 | Stereoscopic image rendering apparatus, stereoscopic image rendering method, stereoscopic image rendering program |
CN102450010A (en) * | 2009-04-20 | 2012-05-09 | 杜比实验室特许公司 | Directed interpolation and data post-processing |
JP2011205573A (en) * | 2010-03-26 | 2011-10-13 | Sony Corp | Control device, camera system, and program |
JP2012160904A (en) * | 2011-01-31 | 2012-08-23 | Sony Corp | Information processor, information processing method, program, and imaging apparatus |
KR101824439B1 (en) * | 2011-02-16 | 2018-02-02 | 주식회사 스테레오피아 | Mobile Stereoscopic Camera Apparatus and Method of Shooting thereof |
US9007430B2 (en) * | 2011-05-27 | 2015-04-14 | Thomas Seidl | System and method for creating a navigable, three-dimensional virtual reality environment having ultra-wide field of view |
US9479697B2 (en) * | 2012-10-23 | 2016-10-25 | Bounce Imaging, Inc. | Systems, methods and media for generating a panoramic view |
CA2938159C (en) * | 2013-02-04 | 2021-07-27 | Valorisation-Recherche, Limited Partnership | Omnistereo imaging |
US9398215B2 (en) * | 2013-04-16 | 2016-07-19 | Eth Zurich | Stereoscopic panoramas |
JP6353214B2 (en) * | 2013-11-11 | 2018-07-04 | 株式会社ソニー・インタラクティブエンタテインメント | Image generating apparatus and image generating method |
-
2016
- 2016-05-25 WO PCT/US2016/034077 patent/WO2016191467A1/en active Application Filing
- 2016-05-25 CN CN201680019971.8A patent/CN107431796B/en active Active
- 2016-05-25 EP EP16727597.3A patent/EP3304894A1/en active Pending
- 2016-05-25 CN CN201680020120.5A patent/CN107431803B/en active Active
- 2016-05-25 JP JP2017550745A patent/JP6511539B2/en active Active
- 2016-05-25 JP JP2017550739A patent/JP6427688B2/en active Active
- 2016-05-25 KR KR1020177027558A patent/KR101944050B1/en active IP Right Grant
- 2016-05-25 KR KR1020177027557A patent/KR101991080B1/en active IP Right Grant
- 2016-05-25 WO PCT/US2016/034072 patent/WO2016191464A1/en active Application Filing
- 2016-05-25 EP EP16727598.1A patent/EP3304897A1/en not_active Ceased
- 2016-05-25 EP EP18185660.0A patent/EP3410388A3/en not_active Withdrawn
Also Published As
Publication number | Publication date |
---|---|
WO2016191464A1 (en) | 2016-12-01 |
WO2016191467A1 (en) | 2016-12-01 |
EP3304894A1 (en) | 2018-04-11 |
CN107431796B (en) | 2019-02-12 |
CN107431803A (en) | 2017-12-01 |
CN107431796A (en) | 2017-12-01 |
JP2018524832A (en) | 2018-08-30 |
EP3304897A1 (en) | 2018-04-11 |
KR101991080B1 (en) | 2019-06-19 |
KR101944050B1 (en) | 2019-04-17 |
EP3410388A2 (en) | 2018-12-05 |
JP6427688B2 (en) | 2018-11-21 |
CN107431803B (en) | 2019-07-26 |
KR20170123328A (en) | 2017-11-07 |
EP3410388A3 (en) | 2019-02-27 |
JP2018522429A (en) | 2018-08-09 |
KR20170123667A (en) | 2017-11-08 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP6511539B2 (en) | Capturing and Rendering Panoramic Virtual Reality Content | |
JP6484349B2 (en) | Camera rig and 3D image capture | |
US10375381B2 (en) | Omnistereo capture and render of panoramic virtual reality content | |
US10038887B2 (en) | Capture and render of panoramic virtual reality content | |
US20170363949A1 (en) | Multi-tier camera rig for stereoscopic image capture | |
US9918011B2 (en) | Omnistereo imaging | |
US10084962B2 (en) | Spherical video stabilization based on accelerometer data | |
JP6596510B2 (en) | Stereo rendering system | |
JP2020525912A (en) | System and method for adaptively stitching digital images | |
WO2018035347A1 (en) | Multi-tier camera rig for stereoscopic image capture | |
US10802390B2 (en) | Spherical omnipolar imaging |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A975 | Report on accelerated examination |
Free format text: JAPANESE INTERMEDIATE CODE: A971005Effective date: 20180620 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20180626 |
|
A601 | Written request for extension of time |
Free format text: JAPANESE INTERMEDIATE CODE: A601Effective date: 20180913 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20181226 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20190312 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20190408 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 6511539Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |