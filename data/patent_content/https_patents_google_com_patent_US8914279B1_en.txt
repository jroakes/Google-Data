US8914279B1 - Efficient parsing with structured prediction cascades - Google Patents
Efficient parsing with structured prediction cascades Download PDFInfo
- Publication number
- US8914279B1 US8914279B1 US13/624,280 US201213624280A US8914279B1 US 8914279 B1 US8914279 B1 US 8914279B1 US 201213624280 A US201213624280 A US 201213624280A US 8914279 B1 US8914279 B1 US 8914279B1
- Authority
- US
- United States
- Prior art keywords
- index set
- specific
- computing device
- pruned
- modifier
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/205—Parsing
- G06F40/216—Parsing using statistical methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/205—Parsing
- G06F40/211—Syntactic parsing, e.g. based on context-free grammar [CFG] or unification grammars
Definitions
- the present disclosure relates to a technique for dependency parsing of text.
- Dependency parsers typically utilize a parsing algorithm to find a most-likely parse for a given text, e.g., a sentence.
- a most-likely parse will identify head-modifier pairs for each word in the text.
- Each word in the text will be identified as a “modifier” for a “head,” which is a different word in the text (or a “null” token such as that identifying a beginning of a sentence).
- Each word in the text does not necessarily operate as a “head” of another word.
- the amount of time necessary to compute the most-likely parse may increase exponentially.
- a text having a length n can have a first-order index set of n 2 elements, where each index may be identified as (h, m) where h has a value of ⁇ 0, . . . n ⁇ , m has a value of ⁇ 0, . . . n ⁇ and h ⁇ m.
- each index may be identified as (h, m) where h has a value of ⁇ 0, . . . n ⁇ , m has a value of ⁇ 0, . . . n ⁇ and h ⁇ m.
- the parsing algorithm analyzes and scores each index individually, the number of operations may be so computationally expensive as to be prohibitive. Additionally, for higher-order dependency parsing, these problems are exacerbated as the number of computations is even larger. An efficient parsing technique that reduces the number of computations necessary while maintaining an acceptable level of accuracy would be desirable.
- a computer-implemented method for dependency parsing can include receiving, at a computing device having one or more processors, a sentence including one or more words.
- the method can further include determining, at the computing device, an index set of possible head-modifier dependencies for the sentence.
- the index set can include inner arcs and outer arcs, where the inners arcs represent possible head-modifier dependency between words in the sentence separated by a distance less than or equal to a first distance threshold and the outer arcs represent possible head-modifier dependency between words in the sentence separated by a distance greater than the first distance threshold.
- the method can include pruning, at the computing device, the outer arcs to exclude arcs representing possible head-modifier dependency between words in the sentence separated by a distance greater than a second distance threshold to obtain a first pruned index set.
- the second distance threshold can be based on a determination of a longest head-modifier dependency distance observed in training data.
- the method can also include pruning, at the computing device, the first pruned index set based on an augmented vine parsing algorithm to obtain a second pruned index set.
- the second pruned index set can include: (i) each specific inner arc when a likelihood that the specific inner arc is appropriate is greater than a first threshold, and (ii) the outer arcs in the first pruned index set when a likelihood that there exists a possible outer arc that is appropriate is greater than the first threshold.
- Each specific inner arc can correspond to a specific index and the likelihood that the specific inner arc is appropriate can be determined based on a max-marginal value of its corresponding specific index.
- the method can further include (i) pruning, at the computing device, the second pruned index set based on a second parsing algorithm to obtain a third pruned index set, the second parsing algorithm being a first-order parsing model, (ii) pruning, at the computing device, the third pruned index set based on a third parsing algorithm to obtain a fourth pruned index set, the third parsing algorithm being a second-order parsing model, and (iii) pruning, at the computing device, the fourth pruned index set based on a fourth parsing algorithm to obtain a fifth pruned index set, the fourth parsing algorithm being a third-order parsing model.
- the method can additionally include determining, at the computing device, a most-likely parse for the sentence from the fifth pruned index set and outputting, from the computing device, the most-likely parse.
- a second computer-implemented method for dependency parsing can include receiving, at a computing device, a sentence including one or more words.
- the method can further include determining, at the computing device, an index set of possible head-modifier dependencies for the sentence.
- the index set can include inner arcs and outer arcs, where the inners arcs represent possible head-modifier dependency between words in the sentence separated by a first distance less than or equal to a distance threshold and the outer arcs represent possible head-modifier dependency between words in the sentence separated by a second distance greater than the distance threshold.
- the method can also include pruning, at the computing device, the index set based on an augmented vine parsing algorithm to obtain a first pruned index set.
- the first pruned index set can include: (i) each specific inner arc when a likelihood that the specific inner arc is appropriate is greater than a first threshold, and (ii) the outer arcs when a likelihood that there exists any possible outer arc that is appropriate is greater than the first threshold. Additionally, the method can include pruning, at the computing device, the first pruned index set based on a second parsing algorithm to obtain a second pruned index set, determining, at the computing device, a most-likely parse for the sentence from the second pruned index set, and outputting, from the computing device, the most-likely parse.
- a computing device can include at least one processor and a non-transitory computer-readable storage medium storing executable computer program code.
- the at least one processor can be configured to execute the executable computer program code to perform operations.
- the operations can include receiving a sentence including one or more words.
- the operations can further include determining an index set of possible head-modifier dependencies for the sentence.
- the index set can include inner arcs and outer arcs, where the inners arcs represent possible head-modifier dependency between words in the sentence separated by a first distance less than or equal to a distance threshold and the outer arcs represent possible head-modifier dependency between words in the sentence separated by a second distance greater than the distance threshold.
- the operations can also include pruning the index set based on an augmented vine parsing algorithm to obtain a first pruned index set.
- the first pruned index set can include: (i) each specific inner arc when a likelihood that the specific inner arc is appropriate is greater than a first threshold, and (ii) the outer arcs when a likelihood that there exists any possible outer arc that is appropriate is greater than the first threshold.
- the operations can include pruning the first pruned index set based on a second parsing algorithm to obtain a second pruned index set, determining a most-likely parse for the sentence from the second pruned index set, and outputting the most-likely parse.
- FIG. 1 is a functional block diagram of a computing network including an example computing device according to some implementations of the present disclosure
- FIG. 2 is a functional block diagram of the example computing device of FIG. 1 ;
- FIG. 3 is an illustration of an example text showing an appropriate parse according to some implementations of the present disclosure
- FIG. 4 is a diagram illustrating an index set for the example text of FIG. 3 according to some implementations of the present disclosure
- FIG. 5 is a diagram illustrating an index set for the example text of FIG. 3 represented in a tree structure according to some implementations of the present disclosure
- FIG. 6 is a flow diagram of an example technique for dependency parsing according to some implementations of the present disclosure.
- FIG. 7 is a flow diagram of an example technique for dependency parsing according to some implementations of the present disclosure.
- a computing network 100 including an example computing device 104 is illustrated.
- the computing device can refer to any suitable computing device including one or more processors (a desktop computer, a laptop computer, a tablet computer, a mobile phone, a server, etc.).
- the computing device 104 can receive input from and/or provide output to a user 108 .
- the computing device 104 can communicate with other computing devices via a network 112 .
- the network 112 can include a local area network (LAN), a wide area network (WAN), e.g., the Internet, or a combination thereof.
- the computing device 104 can communicate with another computing device 200 via the network 112 .
- the other computing device 200 may take the form of a server as illustrated.
- server as used herein can refer to either a single server or two or more servers operating in a parallel or distributed architecture.
- the computing device 200 can be configured to perform the dependency parsing techniques described below.
- the user 108 can provide a user input at the computing device 104 .
- the user input can be, for example, a portion of text (a sentence, a paragraph, a document, etc.) or an identification of a portion of text.
- the computing device 104 can transmit the user input to the other computing device 200 via the network 112 .
- the computing device 104 can then receive a most-likely parse of the text from the other computing device 200 via the network 112 , as described more fully below.
- the computing device 200 can include a communication device 204 , a processor 208 , and a memory 212
- the communication device 204 can control communication between the computing device 200 and other devices via the network 112 .
- the communication device 204 can include any suitable components for communication via the network 112 , e.g., a transceiver.
- the communication device 204 can control communication between the computing device 200 and the computing device 104 via the network 112 . More specifically, the communication device 204 can receive text and output a most-likely parse of the text via the network 112 .
- the processor 208 can control operation of the computing device 200 . It should be appreciated that the term “processor” as used herein can refer to either a single processor or two or more processors operating in a parallel or distributed architecture. For example, the processor 208 can perform functions including, but not limited to loading and executing an operating system of the computing device 200 , controlling information input to and/or output from the computing device 200 , controlling communication over the network 112 via the communication device 204 , and/or controlling read/write operations at the memory 212 .
- the memory 212 can be any suitable storage medium (flash, hard disk, etc.) configured to store information at the computing device 200 .
- the processor 208 can also execute the techniques according to the present disclosure.
- the computing device 200 is configured to perform dependency parsing of a text input.
- Dependency parsing generally refers to the technique of analyzing a text to determine it grammatical structure.
- Dependency parsing can be utilized in machine translation, as well as in other fields.
- the example text 300 is a sentence that reads: “As McGwire neared, fans went wild.”
- a token 302 (represented by “*”) indicating the beginning of the sentence is also shown.
- a plurality of arcs 304 - 1 , 304 - 2 and 304 - 3 (collectively referred to as 304 ) representing head-modifier dependencies is also shown.
- the arcs 304 originate at a head word and point to a modifier of the head word.
- arc 304 - 3 indicates that the word “went” is a head and the word “wild” is a modifier of the head “went.”
- FIG. 4 A diagram illustrating an index set 400 for the text “As McGwire neared, fans went wild” is shown in FIG. 4 .
- the index set can include some or all of the potential head-modifier dependencies for a given text.
- Each index of the index set is represented by a cell in the chart, and an “X” in a cell represents that the head-modifier dependency is an appropriate parse of the text.
- relatively short arcs 304 tend to be more likely to represent an appropriate head-modifier dependency than relatively long arcs 304 .
- index set 500 is shown represented in a tree structure.
- Each index of the index set can include one or more additional sub-indices. This is shown in the illustrated example in which index 510 is shown to include indices 520 - 1 , 520 - 2 . . . 520 - n .
- index 520 - 1 can include indices 531 - 1 , 531 - 2 . . . 531 - p .
- the index 510 can be referred to as a first-order index or an index at the first-order level 550 .
- Indices at a first-order level 550 represent a dependency between a head word and its modifier.
- Indices 520 - 1 , 520 - 2 represent a dependency between a head word and its modifier.
- . . 520 - n can be referred to as second-order indices or indices at the second-order level 552 and can represent, e.g., a dependency between a head word, its modifier and a sibling (a “sibling” can refer to a modifier that previously attached to the head word).
- Indices 531 - 1 , 531 - 2 . . . 531 - p can be referred to as a third-order indices or indices at the third-order level 554 and can represent, e.g., a dependency between a head word, its modifier, a sibling and a grandparent (a “grandparent” can refer to a head word above the current head word).
- a dependency parser can be characterized by the level or order at which it parses a text. For example, a first-order parser will analyze and score indices at a first-order level 550 . Similarly, a second-order parser will analyze and score indices at a second-order level 552 , a third-order parser will analyze and score indices at a third-order level 554 , etc. It should be appreciated that the accuracy of higher-order parsers may be greater than lower-order parsers, e.g., due to the larger amount of features and relationships captured by the model, but at the expense of a higher number of computations.
- the present disclosure describes a structured prediction cascade that utilizes a plurality of pruning or filtering passes before determining a most-likely parse for a given text.
- a coarse-to-fine inference model is utilized where one or more coarse parsers that are less accurate but quicker is utilized to filter out possible indices from an index set before a fine parser that is slower but more accurate is utilized to determine a most-likely parse from the filtered index set.
- a plurality of parsing algorithms may be utilized one after the other to prune the index set to a manageable size upon which the final parsing algorithm is utilized. In this manner, the total time required to determine a most-likely parse for a given text can be reduced when compared to a fine parser analyzing the entire index set, with little to no cost to accuracy.
- an index set of possible head-modifier dependencies for a piece of text is determined. Further, the index set can be characterized to include inner arcs and outer arcs. Inner arcs can be arcs 304 that represent possible head-modifier dependency between words separated by a distance less than or equal to a distance threshold. Conversely, outer arcs can be arcs 304 that represent possible head-modifier dependency between words separated by a distance greater than a distance threshold. It should be appreciated that, as mentioned above, an index set can include one or more additional sub-index sets (see FIG. 5 ). Thus, an arc 304 at a given order (first-order, second-order, third-order, etc.) can also include a number of additional arcs (sub-arcs) representing possible dependency between words in the text.
- the index set may be pruned to reduce the number of indices that the parser will analyze.
- the index set can be pruned to remove all arcs 304 greater than the longest head-modifier dependency distance observed in the training data utilized to train the parser model. For example only, if the longest head-modifier dependency distance observed in training data is fifteen, all arcs representing possible head-modifier dependency between words separated by a distance greater than fifteen can be eliminated from the index set.
- an augmented vine parsing algorithm may be utilized to prune the index set.
- the augmented vine parsing algorithm can be used to analyze the inner arcs individually and the outer arcs as a group and thereby prune the indices for which it is determined are unlikely to provide the most-likely parse.
- the index set includes (h, m) where h is a member of ⁇ 0, . . . n ⁇ , m is a member of ⁇ 1, . . .
- ⁇ b where b indicates a distance threshold can be analyzed and scored by the parser.
- the parser can utilize any information available (e.g., the potential head word for a given modifier) to analyze and score each inner arc.
- the score determined by the parser can indicate the likelihood that the specific inner arc is appropriate.
- the parser can utilize any information available (e.g., the given modifier) to analyze and determine the likelihood that there exists a possible outer arc that is appropriate.
- the likelihood that a specific inner arc is appropriate can be determined based on a max-marginal value of its corresponding specific index, although other algorithms may be utilized.
- Each of the likelihood that the specific inner arc is appropriate and the likelihood that there exists a possible outer arc that is appropriate can be compared to a threshold. For each inner arc, when the likelihood that the specific inner arc is appropriate is greater than the threshold, the specific inner arc is not pruned from the index set. Additionally, when the likelihood that there exists a possible outer arc that is appropriate is greater than the threshold, none of the outer arcs are pruned from the index set.
- the index set will include: (i) inner arcs for which it has been determined have a likelihood greater than a threshold, and (ii) outer arcs for which is has been determined a likelihood that there exists a possible outer arc for a given modifier that is appropriate is greater than the threshold.
- the threshold may be determined based on machine learning from training data, as described more fully below.
- the threshold may be determined based on an analysis of training data. Support vector machines can be utilized for this analysis, although other machine learning techniques could alternatively be used.
- the threshold may be determined based on the equation:
- t ⁇ ⁇ ( w ) ⁇ max y ⁇ Y ⁇ ( y ⁇ w ) + ( 1 - ⁇ ) ⁇ 1 ⁇ I ⁇ ⁇ ⁇ i ⁇ I ⁇ ⁇ m ⁇ ( i ) ⁇ w
- t ⁇ (W) is the threshold
- ⁇ is a model specific parameter having a value between 0 and 1 inclusive
- y is a parse tree of a set of parse trees Y
- w is a weight vector
- i is an index of a set of indices I
- the threshold can be a text-specific (e.g., sentence specific) value.
- the index set may be further pruned by one or more additional pruning passes.
- a first-order parsing algorithm may be utilized to further prune the index set after the augmented vine parsing algorithm
- a second-order parsing algorithm may be utilized to further prune the index set after the first-order parsing algorithm
- a third-order parsing algorithm may be utilized to further prune the index set after the second-order parsing algorithm.
- Each of these first-, second-, and third-order parsing algorithms can score each remaining index of the index set individually and compare the scores, which are indicative of the likelihood that a given index is an appropriate parse, to a threshold.
- Those indices for which the score is greater than or equal to the threshold may be maintained in the index set, while those indices for which the score is less than the threshold may be pruned from the index set.
- Each of the thresholds may be determined based on machine learning from training data.
- a final parsing algorithm can be utilized to determine the most-likely parse from the remaining indices. Because a relatively large number of indices are likely to be removed during the pruning stages, the final parsing algorithm can utilize a more computationally-intensive, higher-order parsing algorithm while maintaining a reasonable operational time.
- the final parsing algorithm can utilize, e.g., a structured perceptron model, a max-margin model, a log-linear model, or a margin infused relaxed algorithm (with or without a hamming-loss margin).
- the text is received by the computing device 200 .
- the text can be, for example, a sentence to be parsed.
- an index set of possible head-modifier dependencies for the text is determined.
- the index set can include inner arcs and outer arcs.
- the inners arcs can represent possible head-modifier dependency between words in the sentence separated by a distance less than or equal to a first distance threshold, and outer arcs can represent possible head-modifier dependency between words in the sentence separated by a distance greater than the first distance threshold.
- the index set can be pruned to exclude arcs representing possible head-modifier dependency between words in the sentence separated by a distance greater than a second distance threshold to obtain a first pruned index set at 612 .
- the second distance threshold can be based on a determination of a longest head-modifier dependency distance observed in training data.
- the technique can further include pruning the first pruned index set based on an augmented vine parsing algorithm to obtain a second pruned index set.
- the second pruned index set can include: (i) each specific inner arc when a likelihood that the specific inner arc is appropriate is greater than a first threshold, and (ii) the outer arcs in the first pruned index set when a likelihood that there exists a possible outer arc that is appropriate is greater than the first threshold.
- each specific inner arc can correspond to a specific index and the likelihood that the specific inner arc is appropriate can be determined based on a max-marginal value of its corresponding specific index.
- the second pruned index set can be further pruned based on a second parsing algorithm to obtain a third pruned index set.
- the second parsing algorithm can be, for example, a first-order parsing model.
- the third pruned index set can be pruned based on a third parsing algorithm to obtain a fourth pruned index set at 624 .
- the third parsing algorithm can be, for example, a second-order parsing model.
- the fourth pruned index set can be pruned based on a fourth parsing algorithm, which can be a third-order parsing model, to obtain a fifth pruned index set at 628 .
- a most-likely parse for the sentence can be determined from the fifth pruned index set at 632 and the computing device 200 can output the most-likely parse at 636 .
- the text is received by the computing device 200 .
- the text can be, for example, a sentence to be parsed.
- an index set of possible head-modifier dependencies for the text is determined.
- the index set can include inner arcs and outer arcs.
- the inners arcs can represent possible head-modifier dependency between words in the sentence separated by a distance less than or equal to a first distance threshold, and outer arcs can represent possible head-modifier dependency between words in the sentence separated by a distance greater than the first distance threshold.
- the index set can be pruned to exclude arcs representing possible head-modifier dependency between words in the sentence separated by a distance greater than a second distance threshold to obtain a first pruned index set at 612 .
- the second distance threshold can be based on a determination of a longest head-modifier dependency distance observed in training data.
- the technique can further include pruning the first pruned index set based on an augmented vine parsing algorithm to obtain a second pruned index set.
- the second pruned index set can include: (i) each specific inner arc when a likelihood that the specific inner arc is appropriate is greater than a first threshold, and (ii) the outer arcs in the first pruned index set when a likelihood that there exists a possible outer arc that is appropriate is greater than the first threshold.
- each specific inner arc can correspond to a specific index and the likelihood that the specific inner arc is appropriate can be determined based on a max-marginal value of its corresponding specific index.
- the second pruned index set can be further pruned based on a second parsing algorithm to obtain a third pruned index set.
- the second parsing algorithm can be, for example, a first-order parsing model.
- the third pruned index set can be pruned based on a third parsing algorithm to obtain a fourth pruned index set at 624 .
- the third parsing algorithm can be, for example, a second-order parsing model.
- the fourth pruned index set can be pruned based on a fourth parsing algorithm, which can be a third-order parsing model, to obtain a fifth pruned index set at 628 .
- a most-likely parse for the sentence can be determined from the fifth pruned index set at 632 and the computing device 200 can output the most-likely parse at 636 .
- the text is received by the computing device 200 .
- the text can be, for example, a sentence to be parsed.
- an index set of possible head-modifier dependencies for the text is determined.
- the index set can include inner arcs and outer arcs.
- the inners arcs can represent possible head-modifier dependency between words in the sentence separated by a distance less than or equal to a first distance threshold, and outer arcs can represent possible head-modifier dependency between words in the sentence separated by a distance greater than the first distance threshold.
- the index set can be pruned to exclude arcs representing possible head-modifier dependency between words in the sentence separated by a distance greater than a second distance threshold to obtain a first pruned index set at 612 .
- the second distance threshold can be based on a determination of a longest head-modifier dependency distance observed in training data.
- the technique can further include pruning the first pruned index set based on an augmented vine parsing algorithm to obtain a second pruned index set.
- the second pruned index set can include: (i) each specific inner arc when a likelihood that the specific inner arc is appropriate is greater than a first threshold, and (ii) the outer arcs in the first pruned index set when a likelihood that there exists a possible outer arc that is appropriate is greater than the first threshold.
- each specific inner arc can correspond to a specific index and the likelihood that the specific inner arc is appropriate can be determined based on a max-marginal value of its corresponding specific index.
- the second pruned index set can be further pruned based on a second parsing algorithm to obtain a third pruned index set.
- the second parsing algorithm can be, for example, a first-order parsing model.
- the third pruned index set can be pruned based on a third parsing algorithm to obtain a fourth pruned index set at 624 .
- the third parsing algorithm can be, for example, a second-order parsing model.
- the fourth pruned index set can be pruned based on a fourth parsing algorithm, which can be a third-order parsing model, to obtain a fifth pruned index set at 628 .
- a most-likely parse for the sentence can be determined from the fifth pruned index set at 632 and the computing device 200 can output the most-likely parse at 636 .
- FIG. 7 a flowchart illustrating an example technique 700 of dependency parsing using structured prediction cascades is shown.
- text is received by the computing device 200 .
- the text can be, for example, a sentence to be parsed.
- an index set of possible head-modifier dependencies for the text is determined.
- the index set can include inner arcs and outer arcs.
- the inners arcs can represent possible head-modifier dependency between words in the sentence separated by a distance less than or equal to a first distance threshold
- outer arcs can represent possible head-modifier dependency between words in the sentence separated by a distance greater than the first distance threshold.
- the technique can further include pruning the index set based on an augmented vine parsing algorithm to obtain a first pruned index set.
- the first pruned index set can include: (i) each specific inner arc when a likelihood that the specific inner arc is appropriate is greater than a first threshold, and (ii) the outer arcs in the first pruned index set when a likelihood that there exists a possible outer arc that is appropriate is greater than the first threshold.
- the first pruned index set can be further pruned based on a second parsing algorithm to obtain a second pruned index set.
- a most-likely parse for the sentence can be determined from the second pruned index set at 720 and the computing device 200 can output the most-likely parse at 724 .
- Example embodiments are provided so that this disclosure will be thorough, and will fully convey the scope to those who are skilled in the art. Numerous specific details are set forth such as examples of specific components, devices, and methods, to provide a thorough understanding of embodiments of the present disclosure. It will be apparent to those skilled in the art that specific details need not be employed, that example embodiments may be embodied in many different forms and that neither should be construed to limit the scope of the disclosure. In some example embodiments, well-known procedures, well-known device structures, and well-known technologies are not described in detail.
- first, second, third, etc. may be used herein to describe various elements, components, regions, layers and/or sections, these elements, components, regions, layers and/or sections should not be limited by these terms. These terms may be only used to distinguish one element, component, region, layer or section from another region, layer or section. Terms such as “first,” “second,” and other numerical terms when used herein do not imply a sequence or order unless clearly indicated by the context. Thus, a first element, component, region, layer or section discussed below could be termed a second element, component, region, layer or section without departing from the teachings of the example embodiments.
- module may refer to, be part of, or include an Application Specific Integrated Circuit (ASIC); an electronic circuit; a combinational logic circuit; a field programmable gate array (FPGA); a processor (shared, dedicated, or group) that executes code, or a process executed by a distributed network of processors and storage in networked clusters or datacenters; other suitable components that provide the described functionality; or a combination of some or all of the above, such as in a system-on-chip.
- the term module may include memory (shared, dedicated, or group) that stores code executed by the one or more processors.
- code may include software, firmware, byte-code and/or microcode, and may refer to programs, routines, functions, classes, and/or objects.
- shared means that some or all code from multiple modules may be executed using a single (shared) processor. In addition, some or all code from multiple modules may be stored by a single (shared) memory.
- group means that some or all code from a single module may be executed using a group of processors. In addition, some or all code from a single module may be stored using a group of memories.
- the techniques described herein may be implemented by one or more computer programs executed by one or more processors.
- the computer programs include processor-executable instructions that are stored on a non-transitory tangible computer readable medium.
- the computer programs may also include stored data.
- Non-limiting examples of the non-transitory tangible computer readable medium are nonvolatile memory, magnetic storage, and optical storage.
- the present disclosure also relates to an apparatus for performing the operations herein.
- This apparatus may be specially constructed for the required purposes, or it may comprise a general-purpose computer selectively activated or reconfigured by a computer program stored on a computer readable medium that can be accessed by the computer.
- a computer program may be stored in a tangible computer readable storage medium, such as, but is not limited to, any type of disk including floppy disks, optical disks, CD-ROMs, magnetic-optical disks, read-only memories (ROMs), random access memories (RAMs), EPROMs, EEPROMs, magnetic or optical cards, application specific integrated circuits (ASICs), or any type of media suitable for storing electronic instructions, and each coupled to a computer system bus.
- the computers referred to in the specification may include a single processor or may be architectures employing multiple processor designs for increased computing capability.
- the present disclosure is well suited to a wide variety of computer network systems over numerous topologies.
- the configuration and management of large networks comprise storage devices and computers that are communicatively coupled to dissimilar computers and storage devices over a network, such as the Internet.
Abstract
Description
where t∝(W) is the threshold, ∝ is a model specific parameter having a value between 0 and 1 inclusive, y is a parse tree of a set of parse trees Y, w is a weight vector, i is an index of a set of indices I, and m(i) is a function of argmaxyεY:y(i)=1y·w. The threshold can be a text-specific (e.g., sentence specific) value. The model specific parameter ∝ can be set to balance the pruning of the index set; ∝=1 will result in a pruning of all indices not in the best parse and ∝=0 will result in a pruning of all indices with a max-marginal value below the mean.
Claims (20)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/624,280 US8914279B1 (en) | 2011-09-23 | 2012-09-21 | Efficient parsing with structured prediction cascades |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201161538539P | 2011-09-23 | 2011-09-23 | |
US13/624,280 US8914279B1 (en) | 2011-09-23 | 2012-09-21 | Efficient parsing with structured prediction cascades |
Publications (1)
Publication Number | Publication Date |
---|---|
US8914279B1 true US8914279B1 (en) | 2014-12-16 |
Family
ID=52015336
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/624,280 Active 2033-02-20 US8914279B1 (en) | 2011-09-23 | 2012-09-21 | Efficient parsing with structured prediction cascades |
Country Status (1)
Country | Link |
---|---|
US (1) | US8914279B1 (en) |
Cited By (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9959271B1 (en) | 2015-09-28 | 2018-05-01 | Amazon Technologies, Inc. | Optimized statistical machine translation system with rapid adaptation capability |
US20180322111A1 (en) * | 2012-07-10 | 2018-11-08 | Robert D. New | Method for parsing natural language text with constituent construction links |
US10185713B1 (en) * | 2015-09-28 | 2019-01-22 | Amazon Technologies, Inc. | Optimized statistical machine translation system with rapid adaptation capability |
US10268684B1 (en) | 2015-09-28 | 2019-04-23 | Amazon Technologies, Inc. | Optimized statistical machine translation system with rapid adaptation capability |
US10437945B2 (en) | 2015-08-05 | 2019-10-08 | Arizona Board Of Regents On Behalf Of Arizona State University | Systems and methods for order-of-magnitude viral cascade prediction in social networks |
US10579739B2 (en) | 2018-01-23 | 2020-03-03 | Wipro Limited | Method and system for identifying places of interest in a natural language input |
WO2021147404A1 (en) * | 2020-07-30 | 2021-07-29 | 平安科技（深圳）有限公司 | Dependency relationship classification method and related device |
US20210406462A1 (en) * | 2020-06-30 | 2021-12-30 | Beijing Xiaomi Pinecone Electronics Co., Ltd. | Method for semantic recognition and electronic device |
US11443102B1 (en) | 2021-08-13 | 2022-09-13 | Pricewaterhousecoopers Llp | Methods and systems for artificial intelligence-assisted document annotation |
US11501065B2 (en) * | 2019-09-11 | 2022-11-15 | Oracle International Corporation | Semantic parser including a coarse semantic parser and a fine semantic parser |
US11645462B2 (en) * | 2021-08-13 | 2023-05-09 | Pricewaterhousecoopers Llp | Continuous machine learning method and system for information extraction |
Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20020046018A1 (en) * | 2000-05-11 | 2002-04-18 | Daniel Marcu | Discourse parsing and summarization |
US6606594B1 (en) * | 1998-09-29 | 2003-08-12 | Scansoft, Inc. | Word boundary acoustic units |
US6901399B1 (en) * | 1997-07-22 | 2005-05-31 | Microsoft Corporation | System for processing textual inputs using natural language processing techniques |
US20070005341A1 (en) * | 2005-06-30 | 2007-01-04 | Microsoft Corporation | Leveraging unlabeled data with a probabilistic graphical model |
US7277850B1 (en) * | 2003-04-02 | 2007-10-02 | At&T Corp. | System and method of word graph matrix decomposition |
US20100076761A1 (en) * | 2008-09-25 | 2010-03-25 | Fritsch Juergen | Decoding-Time Prediction of Non-Verbalized Tokens |
-
2012
- 2012-09-21 US US13/624,280 patent/US8914279B1/en active Active
Patent Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6901399B1 (en) * | 1997-07-22 | 2005-05-31 | Microsoft Corporation | System for processing textual inputs using natural language processing techniques |
US6606594B1 (en) * | 1998-09-29 | 2003-08-12 | Scansoft, Inc. | Word boundary acoustic units |
US20020046018A1 (en) * | 2000-05-11 | 2002-04-18 | Daniel Marcu | Discourse parsing and summarization |
US7277850B1 (en) * | 2003-04-02 | 2007-10-02 | At&T Corp. | System and method of word graph matrix decomposition |
US20070005341A1 (en) * | 2005-06-30 | 2007-01-04 | Microsoft Corporation | Leveraging unlabeled data with a probabilistic graphical model |
US20100076761A1 (en) * | 2008-09-25 | 2010-03-25 | Fritsch Juergen | Decoding-Time Prediction of Non-Verbalized Tokens |
Non-Patent Citations (34)
Title |
---|
Bergsma et al. "Fast and Accurate Arc Filtering for Dependency Parsing". Proceedings of the 23rd International Conference on Computational Linguistics, Beijing, Aug. 2010. * |
Bergsma, S. et al., "Fast and Accurate Arc Filtering for Dependency Parsing," In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), Beijing, pp. 53-61, Aug. 2010. |
Buchholz, S. et al., "CoNLL-X shared task on Multilingual Dependency Parsing," Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), pp. 149-164, New York City, Jun. 2006. |
Carreras, X. et al., "TAG, Dynamic Programming, and the Perceptron for Efficient, Feature-rich Parsing," CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pp. 9-16, Manchester, Aug. 2008. |
Carreras, X., "Experiments with a Higher-Order Projective Dependency Parser," Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, vol. 7, pp. 957-961, Prague, Jun. 2007. |
Charniak, M. et al., "Multilevel Coarse-to-fine PCFG Parsing," Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pp. 168-175, New York, Jun. 2006. |
Collins, M., "Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms," Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics, Philadelphia, Jul. 2002, pp. 1-8. |
Crammer, K. et al., "Online Passive-Aggressive Algorithms," Journal of Machine Learning Research, 7: 551-585 (2006). |
Crammer, K. et al., "Ultraconservative Online Algorithms for Multiclass Problems," Journal of Machine Learning Research 3: 951-991, 2003. |
Eisner, J. et al., "Parsing with Soft and Hard Constraints on Dependency Length," Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pp. 30-41, Vancouver, Oct. 2005. |
Eisner, J., "Bilexical Grammars and Their Cubic-Time Parsing Algorithms," In Harry C. Bunt and Anton Nijholt (eds.), Advances in Probabilistic and Other Parsing Technologies, Chapter 3, pp. 29-62, 2000 Kluwer Academic Publishers. |
Huang, L. et al., "Dynamic Programming for Linear-Time Incremental Parsing," Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pp. 1077-1086, Uppsala, Sweden, Jul. 11-16, 2010. |
Klein, D. et al., "Parsing and Hypergraphs," Computer Science Department, Stanford University, Stanford, CA, 12 pages, 2004. |
Koo, T. et al., "Dual Decomposition for Parsing with Non-Projective Head Automata," Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pp. 1288-1298, MIT, Massachusetts, USA, Oct. 9-11, 2010. |
Koo, T. et al., "Efficient Third-order Dependency Parsers," Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pp. 1-11, Uppsala, Sweden, Jul. 11-16, 2010. |
Kuhlmann, M. et al., "Dynamic Programming Algorithms for Transition-Based Dependency Parsers," Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pp. 673-682, Portland, Oregon, Jun. 19-24, 2011. |
Lafferty, J. et al., "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data," in Proceedings of the 18th International Conference on Machine Learning 2001 (ICML 2001), pp. 282-289. |
McDonald, R. et al., "Online Large-Margin Training of Dependency Parsers," Proceedings of the 43rd Annual Meeting of the ACL, pp. 91-98, Ann Arbor, Jun. 2005. |
McDonald, R. et al., "Online Learning of Approximate Dependency Parsing Algorithms," In Proceedings of EACL, 2006, vol. 6, pp. 81-88. |
Nivre, J. et al., "Memory-Based Dependency Parsing," In Proceedings of CoNLL, 2004, pp. 49-56. |
Nocedal, J. et al., "Numerical Optimization," Springer Series in Operations Research, 1999, 651 pages. |
Pauls, A. et al., "Hierarchical Search for Parsing," Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pp. 557-565, Boulder, Colorado, Jun. 2009. |
Petrov, S. et al., "A Universal Part-of-Speech Tagset," in LREC, pp. 2089-2096 (2012). |
Petrov, S. et al., "Improved Inference for Unlexicalized Parsing," Proceedings of NAACL HLT 2007, pp. 404-411, Rochester, NY, Apr. 2007. |
Petrov, S., "Coarse-to-Fine Natural Language Processing," Ph.D. Thesis, University of California at Berkeley, Berkeley, CA, 2009, 144 pages. |
Roark, B. et al., "Classifying chart cells for quadratic complexity context-free inference," Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pp. 745-752, Manchester, Aug. 2008. |
Rush, A. et al., "Vine Pruning for Efficient Multi-Pass Dependency Parsing," 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 498-507, Montreal, Canada, Jun. 3-8, 2012. |
Shalev-Shwartz, S. et al.,"Pegasos: Primal Estimated sub-gradient solver for SVM," Proceedings of the 24th International Conference on Machine Learning, Corvallis, OR, 2007, pp. 807-814. |
Taskar, B. et al., "Max-Margin Markov Networks," Advances in Neural Information Processing Systems, 16:25-32, 2003. |
Tsochantaridis, I. et al., "Large Margin Methods for Structured and Interdependent Output Variables," Journal of Machine Learning Research 6 (2005) 1453-1484. |
Weiss, D. et al., "Structured Prediction Cascades," Appearing in Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS) 2010, Chia Laguna Resort, Sardinia, Italy. vol. 6 of JMLR: W&CP 6. Copyright 2010, 8 pages. |
Xu, P. et al., "Self-Supervised Discriminative Training of Statistical Language Models," 2009 IEEE Workshop on Automatic Speech Recognition&Understanding, pp. 317-322. |
Yamada, H. et al., "Statistical Dependency Analysis With Support Vector Machines," In Proceedings of IWPT, vol. 3, pp. 195-206, 2003. |
Zhang, Y. et al., "Transition-based Dependency Parsing with Rich Non-local Features," Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pp. 188-193, Portland, Oregon, Jun. 19-24, 2011. |
Cited By (14)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20180322111A1 (en) * | 2012-07-10 | 2018-11-08 | Robert D. New | Method for parsing natural language text with constituent construction links |
US10810368B2 (en) * | 2012-07-10 | 2020-10-20 | Robert D. New | Method for parsing natural language text with constituent construction links |
US10437945B2 (en) | 2015-08-05 | 2019-10-08 | Arizona Board Of Regents On Behalf Of Arizona State University | Systems and methods for order-of-magnitude viral cascade prediction in social networks |
US10268684B1 (en) | 2015-09-28 | 2019-04-23 | Amazon Technologies, Inc. | Optimized statistical machine translation system with rapid adaptation capability |
US9959271B1 (en) | 2015-09-28 | 2018-05-01 | Amazon Technologies, Inc. | Optimized statistical machine translation system with rapid adaptation capability |
US10185713B1 (en) * | 2015-09-28 | 2019-01-22 | Amazon Technologies, Inc. | Optimized statistical machine translation system with rapid adaptation capability |
US10579739B2 (en) | 2018-01-23 | 2020-03-03 | Wipro Limited | Method and system for identifying places of interest in a natural language input |
US11501065B2 (en) * | 2019-09-11 | 2022-11-15 | Oracle International Corporation | Semantic parser including a coarse semantic parser and a fine semantic parser |
US20210406462A1 (en) * | 2020-06-30 | 2021-12-30 | Beijing Xiaomi Pinecone Electronics Co., Ltd. | Method for semantic recognition and electronic device |
US11836448B2 (en) * | 2020-06-30 | 2023-12-05 | Beijing Xiaomi Pinecone Electronics Co., Ltd. | Method for semantic recognition and electronic device |
WO2021147404A1 (en) * | 2020-07-30 | 2021-07-29 | 平安科技（深圳）有限公司 | Dependency relationship classification method and related device |
US11443102B1 (en) | 2021-08-13 | 2022-09-13 | Pricewaterhousecoopers Llp | Methods and systems for artificial intelligence-assisted document annotation |
US11645462B2 (en) * | 2021-08-13 | 2023-05-09 | Pricewaterhousecoopers Llp | Continuous machine learning method and system for information extraction |
US11907650B2 (en) | 2021-08-13 | 2024-02-20 | PwC Product Sales LLC | Methods and systems for artificial intelligence- assisted document annotation |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8914279B1 (en) | Efficient parsing with structured prediction cascades | |
EP3166105B1 (en) | Neural network training apparatus and method | |
US9779087B2 (en) | Cross-lingual discriminative learning of sequence models with posterior regularization | |
US20210319173A1 (en) | Determining syntax parse trees for extracting nested hierarchical structures from text data | |
US10193902B1 (en) | Methods and systems for malware detection | |
US8577155B2 (en) | System and method for duplicate text recognition | |
US9412077B2 (en) | Method and apparatus for classification | |
US20140280256A1 (en) | Automated data parsing | |
US9249287B2 (en) | Document evaluation apparatus, document evaluation method, and computer-readable recording medium using missing patterns | |
US9507852B2 (en) | Techniques for discriminative dependency parsing | |
EP2385471A1 (en) | Measuring document similarity | |
US20130304471A1 (en) | Contextual Voice Query Dilation | |
US10452961B2 (en) | Learning temporal patterns from electronic health records | |
US11163620B2 (en) | Predicting API endpoint descriptions from API documentation | |
CN109948140B (en) | Word vector embedding method and device | |
CN112580346B (en) | Event extraction method and device, computer equipment and storage medium | |
US20200241494A1 (en) | Locking error alarm device and method | |
US20130103441A1 (en) | Generating Predictions for Business Processes Whose Execution is Driven by Data | |
CN108829668B (en) | Text information generation method and device, computer equipment and storage medium | |
US20170091653A1 (en) | Method and system for predicting requirements of a user for resources over a computer network | |
CN113190675A (en) | Text abstract generation method and device, computer equipment and storage medium | |
CN112632000B (en) | Log file clustering method, device, electronic equipment and readable storage medium | |
Negri et al. | Asymptotically distribution free test for parameter change in a diffusion process model | |
CN114093435A (en) | Chemical molecule related water solubility prediction method based on deep learning | |
VanDerwerken et al. | Monitoring joint convergence of MCMC samplers |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:PETROV, SLAV;RUSH, ALEXANDER;REEL/FRAME:029004/0772Effective date: 20120919 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044277/0001Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551)Year of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |