CN118043789A - Pre-inspection of hardware accelerators in a distributed system - Google Patents
Pre-inspection of hardware accelerators in a distributed system Download PDFInfo
- Publication number
- CN118043789A CN118043789A CN202280064293.2A CN202280064293A CN118043789A CN 118043789 A CN118043789 A CN 118043789A CN 202280064293 A CN202280064293 A CN 202280064293A CN 118043789 A CN118043789 A CN 118043789A
- Authority
- CN
- China
- Prior art keywords
- hardware accelerator
- hardware
- subset
- machines
- machine
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000007689 inspection Methods 0.000 title claims abstract description 50
- 230000009471 action Effects 0.000 claims abstract description 126
- 238000000034 method Methods 0.000 claims abstract description 83
- 238000003860 storage Methods 0.000 claims abstract description 17
- 230000004044 response Effects 0.000 claims description 24
- 238000004590 computer program Methods 0.000 abstract description 14
- 238000007726 management method Methods 0.000 description 35
- 230000008569 process Effects 0.000 description 26
- 238000012545 processing Methods 0.000 description 20
- 230000036541 health Effects 0.000 description 16
- 238000010801 machine learning Methods 0.000 description 13
- 239000011159 matrix material Substances 0.000 description 11
- 238000004891 communication Methods 0.000 description 10
- 238000013528 artificial neural network Methods 0.000 description 6
- 238000012544 monitoring process Methods 0.000 description 6
- 238000012360 testing method Methods 0.000 description 6
- 230000032683 aging Effects 0.000 description 4
- 238000004458 analytical method Methods 0.000 description 4
- 230000001934 delay Effects 0.000 description 4
- 230000006870 function Effects 0.000 description 4
- 230000008439 repair process Effects 0.000 description 4
- 238000013473 artificial intelligence Methods 0.000 description 3
- 230000008901 benefit Effects 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 208000024891 symptom Diseases 0.000 description 3
- 238000012795 verification Methods 0.000 description 3
- 230000005540 biological transmission Effects 0.000 description 2
- 238000004364 calculation method Methods 0.000 description 2
- 238000011109 contamination Methods 0.000 description 2
- 230000007547 defect Effects 0.000 description 2
- 230000000977 initiatory effect Effects 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 230000002085 persistent effect Effects 0.000 description 2
- 238000010926 purge Methods 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 229920002803 thermoplastic polyurethane Polymers 0.000 description 2
- 241001091551 Clio Species 0.000 description 1
- XUIMIQQOPSSXEZ-UHFFFAOYSA-N Silicon Chemical compound [Si] XUIMIQQOPSSXEZ-UHFFFAOYSA-N 0.000 description 1
- 230000002159 abnormal effect Effects 0.000 description 1
- 230000002411 adverse Effects 0.000 description 1
- 230000006399 behavior Effects 0.000 description 1
- 230000000903 blocking effect Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 239000003795 chemical substances by application Substances 0.000 description 1
- 238000007796 conventional method Methods 0.000 description 1
- 238000013500 data storage Methods 0.000 description 1
- 230000002950 deficient Effects 0.000 description 1
- 230000018109 developmental process Effects 0.000 description 1
- 238000001152 differential interference contrast microscopy Methods 0.000 description 1
- 239000000428 dust Substances 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000002708 enhancing effect Effects 0.000 description 1
- 230000001815 facial effect Effects 0.000 description 1
- 239000000835 fiber Substances 0.000 description 1
- 230000010354 integration Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 230000006855 networking Effects 0.000 description 1
- 238000003062 neural network model Methods 0.000 description 1
- 230000002093 peripheral effect Effects 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 238000012797 qualification Methods 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
- 230000033764 rhythmic process Effects 0.000 description 1
- 230000011218 segmentation Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 229910052710 silicon Inorganic materials 0.000 description 1
- 239000010703 silicon Substances 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012549 training Methods 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for performing a pre-inspection of a distributed computing system are described. In one aspect, a method includes assigning computing workloads to a first subset of hardware accelerator machines, each having one or more hardware accelerators. A pre-check is performed on the first subset to verify the functionality of each machine in the first subset prior to executing the computing workload. For each hardware accelerator machine of the first subset, installing a package of program code comprising task actions based at least in part on characteristics of the computing workload. Task actions including a sequence of operations are performed on the hardware accelerator machine to determine if the task actions failed. Each time a task fails to operate, the computing workload is reassigned to a second subset of hardware accelerator machines that is different from the first subset.
Description
Technical Field
The specification relates to performing a pre-check (preflight) check on a hardware accelerator in a distributed computing system.
Background
Artificial Intelligence (AI) is the intelligence exhibited by a machine and represents the ability of a computer program or machine to think and learn. One or more computers may be used to perform the artificial intelligence calculations to train the machine to perform the corresponding tasks. The AI computation may include computation represented by one or more machine learning models.
Neural networks belong to the sub-domain of machine learning models. The neural network may employ one or more layers of nodes that represent various operations, such as vector or matrix operations. The one or more computers may be configured to perform operations or computations of the neural network to generate an output, such as classification, prediction, or segmentation of the received input. Some neural networks include one or more hidden layers in addition to the output layer. The output of each hidden layer serves as an input to the next layer in the network, i.e., the next hidden layer or output layer. Each layer of the network generates an output from the received input based on the current values of the respective set of network parameters.
A specially designed hardware accelerator may perform specific functions and operations, including operations or computations specified in a neural network, faster and more efficiently than a general purpose Central Processing Unit (CPU) operates. The hardware accelerator may include a Graphics Processing Unit (GPU), a Tensor Processing Unit (TPU), a Video Processing Unit (VPU), a Field Programmable Gate Array (FPGA), or an Application Specific Integrated Circuit (ASIC).
Disclosure of Invention
According to one aspect, a method for performing a pre-inspection check on one or more hardware accelerators in one or more hardware accelerator machines of a distributed computing system includes: receiving data representing a workload configuration; generating a computing workload representing a set of operations to be performed by the distributed computing system based on the workload configuration; and assigning the computing workload to a first subset of hardware accelerator machines, each hardware accelerator machine comprising one or more hardware accelerators.
The method further includes, prior to executing the computing workload, performing a pre-check on a first subset of the hardware accelerator machines to verify the functionality of the machines. The functionality may include hardware faults, such as hardware component faults and hardware interconnect faults. In order to perform a pre-inspection, the method comprises: for each hardware accelerator machine of the first subset, installing a program code package at the hardware accelerator machine, wherein the program code package includes task actions representing a sequence of operations to be performed by a node manager at the hardware accelerator machine. The task actions are based at least in part on characteristics of the computing workload. The method also includes executing a sequence of operations on the hardware accelerator machines to determine if the task action fails, reassigning the computing workload to a second subset of the hardware accelerator machines each time the corresponding task action fails, and executing the computing workload using the first subset of the hardware accelerator machines each time the task action does not fail.
In some implementations, verifying the functionality of the first subset can be based at least in part on characteristics of one or more hardware accelerators of the first subset and characteristics of the computing workload. The verification process may include checking for at least one or more of arithmetic errors, interconnect bit error rates, topology errors, and inter-accelerator interconnects.
In some implementations, the task actions may include a pre-start task action and a post-completion task action. The pre-start task action may include a first sequence of operations to be performed by a node manager at the hardware accelerator machine prior to executing the computing workload. The post-completion task action may include a second sequence of operations to be performed by a node manager at the hardware accelerator machine after completion of the computing workload.
In some embodiments, the pre-inspection checker binary code (binary) may be implemented by: the pre-check checker binary code is integrated with the computational workload or installed as a machine daemon (daemon) on the hardware accelerator machine. The implementation is determined based at least in part on one or more of a measure of disk space consumption, root privileges of an operating system, or release pace (cadence).
Particular embodiments of the subject matter described herein can be implemented to realize one or more of the following advantages. The techniques described in this specification may improve computational efficiency. Pre-checking the assigned hardware accelerator prior to performing operations specified in the computational workload may improve computational accuracy, prevent contamination of the output by inaccurate results caused by obstructed or failed hardware accelerators or interconnects, and reduce or eliminate job failures or terminations. Although performing a pre-check may increase the delay of scheduling the computational workload, the delay is limited to a few seconds, and correcting errors and/or reassigning accelerators to the workload may take minutes or hours. The pre-inspection is faster than other hardware tests, which typically take hours or days, and this delay is negligible compared to the computational workload, which may require hours or days of runtime. The pre-check is intended to detect and diagnose faults before initiating a computational workload using a selected set of hardware accelerators. By resolving the diagnosed failure before initiating the computational workload using the selected set of hardware accelerators, delays in completing workload operations due to component or interface failures during execution can be avoided, thereby saving significant delay and enhancing user experience, even in view of any slight delay introduced by the pre-inspection checks.
In addition, the techniques described herein may perform satisfactory diagnostics on hardware accelerators or nodes in a cluster. As described above, the described techniques may determine whether one or more hardware accelerators include a failed block (tile) and whether the hardware failure belongs to an accelerator failure or an interconnect failure. The described techniques may provide the machine administrator with the above information for further "health checks" and propose corrective actions to address the failure, such as restarting the node, reinstalling the software package, replacing one or more accelerators in the tray, or replacing cables or interfaces in the tray. This increases the speed at which a failed component or interface is diagnosed, sent for repair, or repaired and re-placed into service within the distributed system, or any combination thereof.
Furthermore, the techniques described herein may robustly configure and reset different hardware accelerators or nodes, which may increase the computational efficiency or speed of the hardware accelerators or nodes. For example, the described techniques may configure different hardware accelerators or nodes prior to running a computing workload based on their respective characteristics using different firmware and runtime parameters. Furthermore, the described techniques may ensure data privacy between different users. For example, the node manager may reset runtime parameters for different hardware accelerators or nodes, or clear memory and storage, or both, after completion of the computational workload, which may avoid potential data leakage and protect the privacy of the user.
The details of one or more embodiments of the subject matter of this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 illustrates an example architecture of a distributed system 100 in which a cluster management system manages the operation of clusters of hardware accelerators.
FIG. 2 illustrates another example architecture of a distributed system 200 having multiple types of accelerator machines.
FIG. 3 is an example process of performing task actions on a hardware accelerator machine before and after executing a computing workload.
FIG. 4 illustrates an example workflow for performing operations specified in application level pre-inspection checker binary code.
FIG. 5 illustrates an example workflow of performing operations specified in a pre-check checker daemon (daemon).
FIG. 6 is an example scenario in which task action failures are avoided when a failed connection associated with an unselected hardware accelerator is detected.
FIG. 7 is a flow chart of an example process of performing a pre-inspection check in a cluster management system.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
Hardware failures are common in large-scale distributed computing systems, which may cause workloads or tasks running on the distributed computing systems to be terminated, failed, or evicted, or to generate inaccurate or erroneous outputs. For example, some example common hardware faults include nodes with faulty hardware accelerators (e.g., hardware accelerator machines), physical interconnect faults in a node or among hardware accelerators that span two or more nodes, and wireless communication faults between following nodes or between following nodes and leading nodes. For brevity, a distributed computing system is also referred to as a distributed system and may include interconnected computing machines, such as nodes, which may include hardware accelerators.
Hardware accelerators may fail for a variety of reasons. For example, hardware accelerators in one or more nodes may fail within months of deployment due to internal defects, even though they may have passed various tests at the manufacturing, integration, or deployment stage. These internal defects may be exposed through extensive use for days or weeks. As another example, a hardware accelerator may be affected by transistor aging, wherein silicon transistors become defective over time. Transistor aging may reduce the performance and reliability of the hardware accelerator and may generate inaccurate and/or unreliable outputs.
Hardware accelerator faults can sometimes be masked and difficult to diagnose. For example, an ASIC in a node may fail, but not generate any symptoms that may be detected by the health check module. This is because the ASIC may still provide output (although inaccurate) and communicate with other ASICs, leading nodes, or following nodes, even if one or more tiles in the ASIC fail and cause the output to be unreliable.
Furthermore, hardware interconnects may fail. For example, large computational workloads or tasks, such as user jobs (jobs) associated with training machine learning models, typically require significant computational resources. Thus, the distributed system is configured to allocate a large amount of computing workload from one or more nodes to a plurality of hardware accelerators to execute jobs. The plurality of hardware accelerators are connected using high speed interfaces such as Peripheral Component Interconnect (PCI) express, inter-chip interconnect (ICI), and/or Ethernet. These high speed interfaces may fail due to cable aging, circuit aging, fiber cavities, and dust contamination.
The term "computing workload" as used throughout this document refers to data representing a plurality of operations (e.g., user jobs (jobs)) performed by a distributed system. The computing workloads may each be generated by a respective workload configuration having a plurality of parameters. For example, the workload configuration parameters may be specified by a user, such as the number and type of hardware accelerators requesting to perform operations, the time period for performing the operations, and the memory address used to obtain calculated input data or store output. The computing workload may include: a long running job that handles short and delay-sensitive requests (e.g., from a few microseconds to hundreds of milliseconds); and batch jobs, which are less sensitive to delays and may take several seconds to days to complete. The computing workloads may each also be associated with a priority according to which the system may allocate computing resources and order in which the jobs are executed. For simplicity, the term "computing workload" is also referred to as a job in the following description.
The distributed system may take action to handle the hardware failure and/or prevent the hardware failure from adversely affecting the computational workload performed by the distributed system. For example, a distributed system may reduce related failures by distributing portions of jobs to different nodes in a cluster when allocating hardware accelerators to perform operations of computing workloads. Or the method may be performed when performing operations specified in a computing workload. For example, the distributed system may generate a copy of the same portion for performing the computing workload, store persistent state in the distributed file system, or apply one or more checkpoints at specific time intervals.
However, the above-described techniques are not capable of diagnosing hardware failures after allocating computing resources to execute a computing workload and before running the computing workload. The pre-inspection checks described herein are capable of detecting and diagnosing such hardware faults. Pre-checking of allocated computing resources (e.g., hardware accelerators and interconnects) is important because pre-checking can verify whether any allocated computing resources are out of order or fail before the distributed system executes the computing workload and encounters a job failure. Such a pre-check may improve throughput and computational efficiency by reassigning computational workload to other suitable computing resources, generating reports indicating obstructed computing resources, and addressing potential hardware failures in advance. By diagnosing a failed or failed hardware accelerator and/or network component prior to executing a workload, future errors may be prevented, which reduces application downtime, errors caused by such downtime, delays in migrating the workload to other hardware accelerators, and inaccurate or erroneous output generated by the distributed system.
In addition, different hardware accelerators may require different firmware, drivers, or configurations, and may be set to different modes. Each mode may require a different set of metrics to determine whether the hardware accelerator is "healthy" (i.e., operating properly).
Further, for security reasons, all hardware accelerators (e.g., ASICs) need to be reset to default settings, and all memory and storage associated with the ASICs need to be purged between two different computing workloads (e.g., by setting the values to zero or other default or pre-workload values).
The technology described in this specification relates to a pre-inspection mechanism for a distributed system that can address at least the above-mentioned problems. More specifically, the described pre-inspection techniques may prevent obstructed hardware accelerators from affecting the process of executing computational workloads in a distributed system.
The described techniques may also diagnose hardware faults, report diagnostic information with actionable information available to take corrective action, and request different corrective actions for the diagnosed hardware accelerator or node before the system begins executing the associated computing workload. For example, the corrective action may be restarting the node, reinstalling a software package on the node, or replacing a hardware component or cable in the tray.
The described techniques may also perform pre-job operations or post-completion operations, or both, on nodes. Pre-job operations may include installing specific firmware or setting parameters to predetermined values appropriate for the hardware accelerator, which may improve computational efficiency. Post-completion operations may include a "cleanup" process to clear memory or storage, which may avoid memory leaks and protect customer data privacy.
The pre-job operation and the post-completion operation may also be used to determine hardware faults. For example, if either a pre-job operation or a post-completion operation fails, a system executing the described techniques may determine whether the failure is due to a hardware failure and generate information for corrective action. One example of pre-job operations may include operations for executing a small portion of a computing workload, e.g., machine learning related workload, using selected hardware accelerators and/or network components, and monitoring execution problems. If the system determines that there is an error or inaccuracy in the results of the calculations on the hardware accelerator or hardware accelerator machine, the system may identify the failed accelerator or machine and generate operational information that may be used to take corrective action. If the system does not find a problem, the system may begin executing the full workload.
The time period for performing pre-job and post-job operations is typically short, e.g., from a few milliseconds to a few seconds, based on the characteristics of the received workload, which enables operations to be performed without introducing significant delay to the process.
FIG. 1 illustrates an example distributed system 100 in which a cluster management system 101 manages the operation of clusters of hardware accelerators 140. Cluster management system 101 is an example management system that may be included in distributed system 100 and implemented on one or more computers (e.g., nodes or machines) located in one or more locations, where the following systems, components, and techniques may be implemented. Some components of cluster management system 101 may be implemented as computer programs configured to run on one or more computers.
The distributed system 100 may include one or more clusters of nodes and each node may have one or more hardware components. The plurality of hardware components may be communicatively connected to each other, either physically or wirelessly, within a node or across different nodes to cooperatively perform operations specified by one or more instructions, e.g., instructions such as a computing workload of a user job. For example, the distributed system 100 may be a cloud computing system, and multiple clusters of the system may be coupled to each other using a high performance network or other suitable network. While the systems and techniques described herein may be applied to various types of computing workloads, the following description is primarily directed to user jobs.
As shown in fig. 1, the cluster 105 may be included in a distributed system 100, the distributed system 100 including one or more other clusters communicatively coupled to the cluster 105. Each cluster in the distributed system 100 may include a plurality of hardware accelerator machines 130 (or equivalently, nodes) communicatively coupled to each other. The hardware accelerator machine 130 may be arranged in a tray connected to the host or connected through a Network Interface Card (NIC). For example, the clusters may include ten, one hundred, ten thousand, or other suitable number of hardware accelerator machines 130.
Each hardware accelerator machine 130 may be managed by a respective node manager 135, and optionally by a machine manager 137. Although the terms "machine" and "node" are generally interchangeable, both may be referred to as a computer server that includes one or more different types of hardware accelerators, and the term "node" may also be defined based on the virtualization of resources. Thus, in some implementations, the hardware accelerator machines 130 may each include one or more nodes. For example, the hardware accelerator machine 130 may include one or more trays, wherein each tray may be considered a node having one or more processors and/or accelerators. As another example, the hardware accelerator machine 130 may include one or more NICs coupled with one or more hardware components, such as an accelerator and/or a processor, wherein the hardware components connected to the NIC may be considered nodes. Note that the above-described hardware machine with one or more nodes 130 may also be managed by node manager 135, and optionally by machine manager 137.
Each accelerator machine 130 may be heterogeneous. In other words, each hardware accelerator machine 130 may include a plurality of hardware accelerators 140 of heterogeneous types and sizes or be communicatively coupled with the plurality of hardware accelerators 140. In general, hardware accelerator 140 may be any type of accelerator suitable for particular computing requirements. For example, the hardware accelerator may include GPU, TPU, VPU, FPGA and/or an ASIC. In another example, the hardware accelerator 140 may be other types of processors, e.g., a general purpose CPU that may not be configured to accelerate machine learning or other types of computations that benefit from the accelerator. Each hardware accelerator machine 130 may have different accelerators coupled directly or indirectly to each other to a Data Center Network (DCN). In some implementations, the hardware accelerators of different hardware accelerator machines may be connected to and in communication with each other and controlled by a topology manager (e.g., the dedicated bag-side network 275 of fig. 2) such as the centralized topology manager 280 of fig. 2. Note that for simplicity, the term "DCN network" is also referred to in the following description as a network (e.g., network 150 of fig. 1 or network 230 of fig. 2), and that the term "bag-side network" throughout this document refers to a network that connects extended computing resources and functionality. More specifically, the term "bag-side network" is configured to provide specific communications among a subset of servers or hosts coupled in a Data Center Network (DCN). The bag-side network may not need to be communicatively coupled to the DCN, ethernet, or any internet protocol. For example, the TPU bag side network may include one or more TPUs that are located in the TPU devices and coupled to each other using the bag side network (e.g., high speed interconnect) such that each of the TPU devices may communicate directly with each other. However, the TPU devices may be coupled with the DCN using any suitable conventional network. Different types of hardware accelerator machines 130 are described in more detail in connection with FIG. 2.
In general, the clusters 105 may receive input data 110. The input data may include a computing workload, such as a user job, that specifies a plurality of operations to be performed by one or more hardware accelerator machines 130 included in the cluster 105. The hardware accelerator machine 130 may generate output data 170, the output data 170 including at least output generated by performing operations of the computing workload. For example, the input data 110 may include data representing a machine learning model (e.g., a trained neural network) that specifies machine learning operations, e.g., inference operations for processing frames of input data (e.g., input images) at time steps to generate inference outputs (e.g., predictions of facial recognition). In another example, the input data 110 may include data to be processed using a machine learning model, e.g., a machine learning model included in the input data 110 or a machine learning model already stored at the cluster 105 or elsewhere in the distributed system 100.
The cluster management system 101 described herein is configured to perform a pre-check on the assigned hardware accelerator 140. The output data 170 may include data indicative of diagnostic information of one or more hardware faults or potential hardware faults predicted to occur in the future. Details of the pre-check mechanism are described below.
The cluster management system 101 may include at least a cluster manager 120 deployed on a server or host and a node manager 135 installed on each hardware accelerator machine 130. In general, cluster manager 120 may assign computing workloads to multiple accelerator machines and instruct node manager 135 on each accelerator machine to perform a pre-check to determine whether the assigned accelerator machine is subject to any hardware failures.
The cluster manager 120 may perform analysis on characteristics or requirements specified in the computing workload included in the input data 110 and allocate a particular set of one or more hardware accelerator machines 130 to perform the computing workload based on the analysis. The analysis may include, for example, determining available hardware accelerators for executing the computing workload and a respective available period for each of the available hardware accelerators. In some implementations, the cluster manager 130 can be coupled with a scheduler (e.g., scheduler 215 of fig. 2) to allocate computing resources for computing workloads. The cooperation of the scheduler and cluster manager is described in more detail in connection with fig. 2.
Node manager 135 is a local agent configured to initiate or terminate execution of an assigned computational workload or a portion of an assigned computational workload on hardware accelerator machine 130. Node manager 135 may also manage local resources, for example, by: manipulating operating system kernel settings, scrolling debug logs, or reporting the status of hardware accelerator machine 130 to cluster manager 120, or managing other monitoring systems (e.g., a distributed system or a plurality of clustered health monitoring systems, which are not depicted in fig. 1), or both.
The data indicating the state of the hardware accelerator machine may include data indicating the number and type of available hardware accelerators included in the hardware accelerator machine for executing the assigned job. For example, when computing a workload requests eight hardware accelerators in cluster 105, node manager 135 located on a first hardware accelerator machine 130 may report to cluster manager 130 that there are three GPUs available for computing the workload, and another node manager may report that there are five TPUs available on another machine. In some implementations, the data indicative of the state of the hardware accelerator machine may also include data indicative of one or more hardware accelerators of the hardware accelerator machine and/or failure of interconnections between hardware accelerators of the hardware accelerator machine (and/or between those hardware accelerators and hardware accelerators of other hardware accelerator machines), and if so, data indicative of a failed hardware accelerator and/or interconnections. The data may also indicate which hardware accelerators and interconnects are operating properly, and the availability of them to be assigned to new computing workloads as described above.
In addition, node manager 135 may initiate the assigned computing workload in the corresponding hardware accelerator machine 130 independently of other machines. Or the node managers of different machines may further coordinate the hardware accelerators with each other in executing the corresponding portions of the computing workload. That is, the node managers may interact in performing the operation of the computing workload to coordinate their respective hardware accelerators.
Cluster manager 120 may be associated with each node manager. For example, the node manager of each hardware accelerator machine 130 in the cluster 130. And (5) communication. More specifically, cluster manager 120 can send management data 160 to node manager 135, where management data 160 includes instructions that instruct node manager 135 to install one or more packages of program code or perform operations corresponding to a computing workload. The installed program code packages may include operations (e.g., task actions) managed by node manager 135 and performed by hardware accelerator machine 130 in advance (i.e., prior to executing the computing workload) to detect whether there are any hardware faults associated with the machine. The hardware failure may include, for example, a hardware accelerator failure or an interconnect failure with one or more hardware accelerators.
The cluster manager 120 may periodically poll (poll) each node manager to receive report data 165 indicating the current state of the hardware accelerator machine (e.g., available computing resources or any hardware failure information). The cluster manager 120 may request report data 165 from the node manager 135 periodically (e.g., every few seconds). For example, if node manager 135 does not respond to several polls, cluster manager 120 may mark node 130 as "off" or "unavailable due to a failure such as a power failure, hardware component failure, or interconnect failure. If so, the cluster manager 120 may reassign the computing workload to another hardware accelerator machine 130. In some other cases, when one or more hardware accelerators 140 in a hardware machine 130 are not responsive to a corresponding node manager 135, the node manager 135 may mark the unresponsive accelerator as "off" or "unavailable" and encode these markers in report data 165. The cluster manager 120 may reassign the computing workload to other responding hardware accelerators 130 in the same machine 130.
As another example, when the hardware accelerator machine 130 appears to be available to the cluster manager 120, the node manager 135 may provide report data 165 to the cluster manager 120 that further includes changes or updates to the hardware accelerator availability, and optionally the cause or cause of the availability change. For example, node manager 135 may provide reporting data 165, which reporting data 165 indicates that there are two hardware accelerators (e.g., computing units coupled to each other in the hardware accelerator to perform operations) that are no longer available due to the block of obstructions. The cluster manager 120 may then reassign the computing workload to other available and functioning accelerators in the same machine or another machine. Details of the reporting data 165 and reporting process are described below.
In some implementations, cluster management system 101 may also include a machine manager 137 disposed on each hardware accelerator machine 130, or may be communicatively coupled, such as wired or wireless, to machine manager 137. The machine manager 137 may be in the form of a daemon that runs continuously as a background process in the system container without direct user control computer readable program code and may be configured to collect telemetry data from all hardware accelerators 140 on the hardware accelerator machine 130. Telemetry data may include data that is statistically measured in the field (e.g., measured while the system is still running), including computing resource usage and/or utilization, data transmission modes, interconnections, overhead, runtime, and power consumption. Machine manager 137 may monitor abnormal system behavior and generate symptom data indicative of machine failure.
Note that machine faults are generally more apparent and more easily identified than other types of hardware faults (e.g., node faults on nodes that maintain good interconnect and runtime characteristics but generate inaccurate or even erroneous outputs). These other types of hardware faults may be identified by the described techniques, but are undetectable by conventional techniques. This is because, for example, a machine failure is typically local to the hardware accelerator machine. Conventional health monitoring systems, such as machine manager 137, typically determine machine anomalies or faults based on collected telemetry data associated with a local hardware machine or machine configuration, rather than based on characteristics or configuration of a particular computing workload (e.g., not based on an application). For example, conventional health monitoring systems do not compare the output from a hardware accelerator machine for a particular computing task to a reference output. In contrast, factors used by conventional health monitoring systems to determine machine failure include, for example, lost responses from hardware accelerators at specific requests, response delays exceeding a threshold, and detected disconnects, if any, in the machine.
FIG. 2 illustrates another example architecture of a distributed system 200 having multiple types of accelerator machines 235 a-d.
As shown in fig. 2, a distributed system 200, which may be similar or identical to the distributed system 100 shown in fig. 1, may include multiple clusters 210, although only one cluster 210 is shown for clarity. Each cluster 210 may include a plurality of hardware accelerator machines, such as accelerator machine A235 a, accelerator machine B235B, accelerator machine C235C, and accelerator machine D235D. Each hardware accelerator machine 235a-d may be of a different type than each other hardware accelerator machine. The details of each type of hardware accelerator machine are as follows.
The cluster 210 may include one or more cluster managers 220 (e.g., cluster manager copies). The distributed system 200 may receive workload configuration data indicating a configuration of a computing workload (e.g., user job) specified by a user or user device. The workload configuration may include, for example, operations specifying the user job, a total run time of the user job, or computing resources required or sufficient to perform the operations of the user job. These operations may include machine learning, e.g., inference, operations, of a machine learning model that may also be included in the workload configuration data. Cluster manager 220 may generate user jobs based on the workload configuration and deploy the user jobs to a subset of the hardware accelerator machines to execute the user jobs based on resource availability. For example, the node managers 255a-d installed on the hardware accelerator machines 235a-d may provide the cluster manager 220 with information indicating the state of the hardware accelerator machine or a hardware accelerator coupled to the machine, as described above. The workload configuration includes, for example, information regarding one or more programs or portions thereof and corresponding inputs to be executed on each of the one or more accelerator machines. Thus, generating user jobs based on workload configuration includes writing and/or compiling such information for each accelerator to enable it to execute the workload.
Each cluster manager 220 may be communicatively coupled with one or more schedulers 215 (e.g., scheduler copies). Upon receiving the computing workload from cluster manager 220, scheduler 215 may first scan all computing resources in distributed system 200, e.g., asynchronously or synchronously, and determine whether sufficient computing resources are currently available to meet the computing requirements specified in the computing workload.
When receiving a plurality of user jobs from different users, scheduler 215 is configured to generate priorities associated with the users or the user jobs or both. In some implementations, the priority may be represented by data stored in the cluster manager or associated with the workload configuration. In addition, scheduler 215 may employ a ranking or queuing scheme (e.g., round-robin scheme) to ensure fairness among different users and to avoid blocking other users due to large user jobs consuming a large portion of the available computing resources of cluster 210.
The scheduler 215 may receive and scan multiple computing workloads according to priority in the queue (e.g., from high priority to low priority) such that computing workloads with high priority are deposited higher in the queue than computing workloads with lower priority. Operations that are higher in the queue typically execute earlier than operations that are lower in the queue.
Distributed system 200 may also include a distributed memory 225, or another suitable type of data storage system or device, communicatively coupled to a network 230 (e.g., a DCN as described above). The distributed memory 225 may be configured to store data used by the various components of the distributed system 200. For example, the data may include configuration data or firmware for the hardware accelerator machine, parameters for performing operations specified in the user job (e.g., weights and hyper-parameters of a trained neural network model), intermediate results or results calculated by the hardware accelerator, or information data indicating the status of the hardware accelerator machine, hardware accelerator, or cluster (e.g., status such as availability, "health," quantity, or type). Other components, such as cluster manager 220, hardware accelerator machines 235a-d, and node managers 255a-d, may access distributed storage 225 over network 230.
In some implementations, the distributed system 200 may also include machine managers 250a-d that are installed on the hardware accelerator machines 235a-235d, respectively. The machine managers 250a-d may be configured to run in the background and measure telemetry data of the machine in the field. Telemetry data may be used, for example, to determine interconnections between accelerators located within one or more hardware accelerator machines.
The distributed system 200 may also include a bag-side network 275. The bag-side network 275 may be used to extend hardware accelerators or other hardware components to the distributed system 200. As described above, the bag-side network may provide specialized high-performance networking for a subset of hosts or servers coupled in the DCN. The bag side network may be independent of the DCN or may not need to be directly coupled to the DCN. The bag-side network may also include a control plane (e.g., a function or process that determines which path to use to send the data packet and/or frame) or a data plane (e.g., a function or process that forwards the data packet and/or frame from one interface to another interface based on control plane logic), or both.
In addition, the distributed system 200 may include a centralized topology manager 280. The centralized topology manager 280 can be configured to route or link hardware accelerators according to the computing workload such that the linked hardware accelerators can execute the assigned workload. The centralized topology manager 280 may also generate the expected topology of the assigned accelerators and provide the expected topology to the node managers 255a-d to determine interconnect faults in the network.
As described above, cluster manager 220 may determine a subset of hardware accelerator machines for executing one or more user jobs. Prior to executing the user job, a cluster management system (e.g., cluster management system 101 shown in fig. 1) may perform a pre-check on the hardware accelerator machine and its corresponding subset of hardware accelerators. The pre-check may be a set of operations performed by the cluster management system 205 to determine hardware faults in the distributed system 200.
To perform the pre-fetch check, the cluster manager 220 may generate instruction data including a package of program code and instruct each node manager 255a-d to install the package of program code on each subset of the hardware accelerator machines. The program code package may include a pre-check checker binary code that specifies a procedure or operation for performing a pre-check.
The pre-check checker binary code is low-level machine-readable program code in the form of 0 and 1. The pre-inspection checker binaries may be deployed as separate machine daemons, similar to the machine manager described above. Or the pre-inspection checker binary code may be integrated as part of the user job, e.g., as additional program code included in the user job, rather than being separately generated by the cluster manager 220. The determination of deploying the pre-inspector binary as a stand-alone inspector daemon or sequence of operations integrated into a user job may be determined based on a tradeoff between persistent disk space consumption, root (root) privileges of an operating system (e.g., linux), and release cadence (e.g., time period or interval between release or update of a machine daemon). In general, when the pre-inspector binary is implemented as a stand-alone inspector daemon, the pre-inspector daemon may be granted root privileges to access more resources of the distributed system, but it may require more disk space and release the cadence slower. Furthermore, the separate inspector daemon may have a slower release pace than the program code integrated into the user job, because the machine daemon is part of the underlying software that supports normal operation of the machine, requires more testing or qualification, and thus releases at a slower rate.
In some implementations, the pre-inspection checker binary code may be encoded in a program code package, installed on a hardware machine, and implemented in a user job, and then the hardware machine may use the same pre-inspection checker binary code for one or more different user jobs. In this way, the system can more flexibly perform pre-inspection with higher performance and safety.
The pre-check of the pre-check checker binary code, whether implemented as a stand-alone checker daemon or integrated into the user job, is performed or conducted prior to executing the user job. The pre-check checker binary code may be executed on the hardware accelerator machines 235a-d immediately before the scheduler 215 initiates the user job on the hardware accelerator machines 235 a-d. In this way, the cluster management system may minimize the time interval between performing the pre-inspection and user jobs, which may increase computational efficiency and enhance user experience. This also provides a real-time current state of the hardware accelerator machines 235a-d at the time the user job is deployed.
The pre-checker binary code may be used to configure a hardware accelerator, a hardware accelerator machine, or other card or tray components included in the distributed system based on the workload configuration. The detailed information of configuring the hardware components is as follows.
The pre-inspection checker binary code may include operations for performing overall health checks on hardware accelerator machines and associated components, such as network components that connect hardware accelerators within the machine and connect the machine (and/or its accelerator) to other machines (and/or its accelerators). The overall health check may include determining whether there is a hardware failure in a subset of the hardware accelerator machines assigned to the user job. Based on the type of hardware accelerator machine and the user's job, the overall health check may include verification of hardware functionality, for example, by: an arithmetic or logical operation is performed to determine an interconnect bit error rate (BER, i.e., number of bit errors per unit time), a topology error, or to check the interconnections of a subset of the hardware accelerator machines. If the cluster management system successfully performs a pre-inspection (or task action) on the hardware accelerator machine, the cluster management system may determine that there is no hardware failure in a subset of the hardware accelerator machines. Otherwise, the cluster management system may determine one or more hardware faults based on the verification type specified in the pre-inspection checker binary code.
For each pre-check of the pre-check checker binary code, when executed by the node manager, it may be determined whether the corresponding accelerator responds with an output within a predetermined period of time, and whether the output is accurate compared to the true value. The operations for generating output may be based on matrix operations or other operations associated with machine learning computations that are typically performed during execution of a computing workload. Whenever the accelerator does not respond or provide an incorrect output, the node manager may determine that the pre-check (or task action) failed, identify a hardware failure, and provide diagnostic information for corrective action. Such identified faults and/or diagnostic information may differ based on whether the accelerator is not responding or whether the output is incorrect.
Similarly, for an interconnect, each pre-check, when performed by the node manager, may determine that the interconnect is faulty, such as a misconnected link, a faulty link, or a poor quality link (e.g., having a BER above a threshold). A pass or failure of the pre-check may indicate whether the hardware accelerator or the hardware accelerator machine is able to successfully perform the operation of computing the workload. The cluster manager may replace the hardware accelerator machine that failed the pre-check with another machine and restart the pre-check on the other machine until no hardware failure is found or a predetermined iteration limit is reached. Additional details of executing the pre-inspection checker binary code are described below.
In some cases where the cluster management system determines one or more hardware failures, the cluster management system may generate diagnostic information indicative of the cause or symptom for further analysis of the hardware failure or provide the information as contextual information to the repair workflow. For example, in response to determining that an output generated by performing a simple arithmetic operation using one or more hardware accelerators is inaccurate compared to a true value, the system may generate information indicative of the failure of the one or more hardware accelerators. The system may then identify the obstructed hardware accelerator and encode the relevant data into diagnostic information. As another example, the system may determine that the inaccurate output may be due to a link failure, and in response, the system may identify the failed link or interconnect and encode the relevant data into the diagnostic information. The system may provide diagnostic information to a node manager (e.g., node manager 255a-d of FIG. 2), a machine manager (e.g., machine manager 250a-d of FIG. 2), or directly to a repair workflow, or any suitable combination thereof. The hardware accelerator, machine, or interface that identified the hardware failure may be handled automatically or by a technician or engineer using a predefined set of actions, such as restarting the hardware accelerator machine, reinstalling a program code package on the machine, or replacing a component (tray) of the machine.
In the following sections, different embodiments of the pre-inspection checker binary code are described in more detail in connection with different types of hardware accelerator machines 235a-d as shown in FIG. 2.
As noted above, hardware accelerator machines may be of different types. For example, and in conjunction with fig. 2, accelerator machine a 235a may include a hardware accelerator a 265a that is not directly connected to network 230 but may nevertheless communicate with network 230 through a host (e.g., machine 235 a). As another example, accelerator machine B235B may include accelerator B265B coupled directly to network 230a through a Network Interface Card (NIC).
In some implementations, a hardware accelerator in the hardware accelerator machine may be connected to a bag-side network. For example, the accelerator machine C235C may include an accelerator C265C connected to the bag-side network 275. As another example, the accelerator machine D235D may include an accelerator D265D connected to the bag-side network 275. The centralized topology manager 280 may be connected to the bag-side network 275 and configured to manage the topology (e.g., connections) of accelerators or machines in the bag-side network 275 due to the configuration of user jobs. The functionality of the centralized topology manager 280 is described in more detail below.
For hardware accelerator machines having accelerators that are not connected to the bag-side network, the cluster management system may implement the pre-inspection checker binary code by integrating the pre-inspection checker binary code into the user job, such pre-inspection checker binary code also being referred to as "application-level pre-inspection checker binary code" because the pre-inspection checker binary code is based at least in part on the characteristics of the corresponding user job. As shown in fig. 2, the pre-check checker binary codes 245a and 245b are integrated into user jobs 1 and 2 of the hardware accelerator machine a 235 a; checker binaries 245c and 245d are integrated into user jobs 3 and 4 of hardware accelerator machine B235B; also, the checker binaries 245g and 245h are integrated into the user jobs 7 and 8 of the hardware accelerator machine D235D.
The pre-checker binary code integrated into the user job on the hardware accelerator machine may specify operations for verifying at least the "health" of the hardware accelerator associated with the hardware accelerator machine. The pre-check checker binary code, when executed, may cause one or more hardware accelerators to acquire and run a small sample workload corresponding to a user job before executing the complete user job. The small sample workload may involve any level of hardware or software components associated with the machine, so the inspector binary code may perform a substantially full stack of pre-inspection health inspections (e.g., a full distributed system from user-oriented front-end to back-end (e.g., databases and architectures) and program code and/or interconnect pre-inspection health inspections connecting the two ends).
For example, the software components may include drivers, accelerator firmware, inter-accelerator network routing, and workflow settings, and the hardware components may include any suitable subcomponents of the accelerator, such as an on-chip cache, an Arithmetic Logic Unit (ALU), a Matrix Multiplication Unit (MMU), a Vector Processing Unit (VPU), an instruction sequencer, off-chip Dynamic Random Access Memory (DRAM), or High Bandwidth Memory (HBM), an accelerator CPU PCI express, or other high-speed interconnect. In some implementations, the pre-check checker binary code may also specify operations for detecting intra-machine connections, i.e., interconnections between accelerators in a hardware accelerator machine.
One example of pre-inspection checking one or more accelerators may include checker binary code. The checker binary code may specify operations to perform simple element-by-element matrix multiplication using one or more accelerators. A simple matrix multiplication may be a user job or a small portion or copy of a predetermined test case of a user job. In this sense, the pre-check (or task action) may be based at least in part on the characteristics of the computing workload. The node manager 255a-d may determine whether the output of the simple matrix multiplication is accurate or has an error limited within a threshold by comparing the output to a true value. If the output is determined to be inaccurate or exceeds a threshold, e.g., different from the reference output, the node manager may determine that at least one hardware accelerator in the hardware accelerator machine failed.
In addition, node manager 255a-d may determine whether hardware accelerator 265a-d is responsive to executing the checker binary code and/or may generate an output after performing an operation specified in the checker binary code. If the hardware accelerator does not respond or provide an output, the node manager 255a-d may determine that at least one of the hardware accelerator machines failed.
In general, if any of the above processes fail, the cluster management system 200 may determine that there is at least one hardware accelerator failure in that hardware accelerator machine and reassign another hardware accelerator machine to execute the assigned computing workload.
As another example, the checker binary file may include the following operations: element-by-element matrix multiplication is performed, followed by a reduction operation on all multiplication results. In this way, the cluster management system may determine whether inter-accelerator communication or interconnection on a machine (e.g., accelerator machine B235B of fig. 2) is functioning properly. As a simple example, assuming there are four accelerators communicatively coupled to each other and each having a Multiplier Accumulator (MAC) unit, the checker binary file may include operations to perform element-by-element multiplication of the first 2x2 matrix and the second 2x2 matrix. Each MAC unit may obtain a product of a respective element in the first matrix and a respective element in the second matrix. The checker binary code may also include an operation to reduce the four products to a sum. If any of the accelerators encounters an inter-accelerator interconnect problem, the sum will be inaccurate compared to the true value.
Referring back to accelerator machine B235B, inter-accelerator interconnect problems may typically occur between the accelerator and the host, between the accelerator and a Network Interface Card (NIC), between the NIC and the network, and between accelerators located on the same tray.
However, inter-accelerator interconnection problems may also occur between accelerators in different trays in one machine or across two or more machines, where the accelerators are coupled to each other through network 230 (e.g., two B-type accelerator machines 235B) or through bag-side network 275 (e.g., two C-type or D-type accelerator machines 235C and 235D, or C-type accelerator machine 235C and D-type accelerator machine 235D).
For hardware accelerator machines with accelerators connected to the bag-side network (e.g., the C-type and D-type accelerator machines 235C, 235D of fig. 2), the cluster management system may implement the pre-inspection checker binary as an inspector daemon installed on the hardware accelerator machine, and may optionally implement the pre-inspection checker by integrating the inspector binary into the user job.
The accelerator machine C235C includes an inspector daemon 270a, and the accelerator machine D235D includes an inspector daemon 270b. For accelerator machine C235C, the pre-inspection inspector binary may be implemented as inspector daemon 270a only, and there is no inspector binary integrated with user jobs 5 and 6. For accelerator machine D235D, the pre-inspector binaries may be implemented as inspector daemon 270b, and optionally as inspector binaries 245g and 245h. The inspector binaries 245g and 245h may be configured to perform health inspections similar to those described above.
The pre-inspection checker binary code implemented as an inspector daemon may specify health inspection operations to be performed as a background process. In this example, the process is not exposed to the user or user application, and the user may access the inspection results through a node manager installed on each machine, for example, via a user interface provided by the node manager.
The operations specified by the inspector daemon 270a or 270b may include operations to inspect inter-accelerator connections. For example, the inter-accelerator connection may include an interconnection with accelerators within a single machine or across one or more machines using a bag-side network 275 (or a general purpose High Performance Network (HPN)), and multiple accelerators of one or more machines may form a two-dimensional or three-dimensional torus or mesh or a folded clios network (i.e., a multi-stage circuit switched network) when connected to the bag-side network 275 controlled by the centralized topology manager 280 according to user jobs.
In general, the centralized topology manager 280 may determine the topology (e.g., interconnections between accelerators in the HPN) based on instructions from the cluster manager 220 according to user jobs. Centralized topology manager 280 includes data representing the blueprints of the overall interconnect, all components connected to the HPN, and any potential extensions to the HPN (e.g., additional accelerators and links). As shown in fig. 6 and in conjunction with fig. 2, the centralized topology manager 280 may maintain data representing the current two-dimensional topology of the accelerators connected to the bag-side network 230. For example, the current topology may include a 2x2 grid with four hardware accelerators 620, 625, 635, and 645 connected to each other through links A, B, C and D. The centralized topology manager 280 can also store data for connecting additional accelerators 630 and 650 using extensible connections 627, 653 and 643.
After cluster manager 220 assigns user jobs to subsets of hardware accelerator machines (e.g., 235c and 235 d) and hardware accelerators (e.g., 265c and 265 d) according to characteristics of the user jobs, centralized topology manager 280 may use the HPNs to determine interconnections between accelerators to execute the user jobs and generate data representing the interconnections as intended topology data. The expected topology data may include adjacency information of the accelerator, a state of each link connecting the accelerator, and a Bit Error Rate (BER).
Centralized topology manager 280 may also communicate with inspector daemons 270a and 270b for new components added to or removed from the HPN and newly formed topologies regarding newly added or removed components. Thus, the centralized topology manager 280 can easily generate updated intended topology data for use in hardware components connected to the HPN.
The checker daemons 270a, 270b may include operations for determining interconnect faults. Interconnect faults may include miswiring or misadjacent information, poor link quality, and faulty interfacing. These operations may include checking whether the observed topology of the accelerator connected to the HPN matches the expected topology controlled by the centralized topology manager 280. For example, the inspector daemon may further include verifying operation of an accelerator adjacent to an accelerator connected according to the intended topology. Node managers 255c and 255d may determine miswired or faulty neighborhood information in response to determining that the observed topology does not match the expected topology. The checker daemon may include an operation to determine link quality by measuring BER in the topology and comparing the measured BER to a predetermined threshold. Node managers 255c and 255d may determine poor link quality based on whether the measured BER exceeds a predetermined threshold.
In some implementations, the checker daemons 270a and 270b can also include operations that enable or disable a particular link that connects the two accelerators in response to detecting a miswiring.
To determine an interconnect failure, the inspector daemons 270a and 270b may receive expected topology data from the centralized topology manager 280 over the network 230. The inspector daemon 270a or 270b may also request telemetry data from the corresponding machine manager 250c or 250 d. As described above, the telemetry data collected by the machine manager may include topology data representing the interconnections between the assigned accelerators, wherein the topology data provided by the machine manager 250c or 250d is used as the detected topology data. The checker daemons 270a and 270b can then compare the detected topology data with expected topology data and determine interconnect faults.
The checker daemon may determine that a failed or problematic connection associated with an unselected or unassigned hardware accelerator will not result in a pre-check failure, which will be described in more detail in connection with fig. 6.
FIG. 3 is an example process 300 of performing task actions on a hardware accelerator machine before and after executing a computing workload. For convenience, the process 300 described above is described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed system, such as cluster management system 101 of FIG. 1, may perform process 300.
After the node manager (e.g., node manager 255a of fig. 2) installs the program code package instructed by the cluster manager (e.g., cluster manager 220 of fig. 2), the node manager may perform task actions including a sequence of operations associated with a computing workload (e.g., a user job). For example, the task actions may include the operation of retrieving checker binary code from an installed package of program code to perform a pre-inspection check.
In some implementations, the task actions may include pre-start task actions to be performed prior to execution of the assigned user job or a portion of the assigned user job, or post-completion task actions to be performed after completion of the assigned user job or a portion of the assigned user job, or both.
As shown in FIG. 3, node manager 255a determines that a portion of the user job assigned to hardware accelerator machine 235a is ready to be started (310), and node manager 255a may first determine if the task action before starting is present in the installed program code package (320).
In response to determining that there is no pre-start task action, node manager 255a may begin performing the operations specified in the portion of the user job (340).
In response to determining that there is a pre-start task action, node manager 255a may perform the pre-start task action (325).
The task action before start may include operations related to "health checks" on the accelerator or interconnect as described above. For example, when a hardware accelerator machine is not connected to an accelerator of the bag-side network, a first type of pre-start task action of the hardware accelerator machine may include an operation to initiate inspector binary code integrated into a user job to determine an accelerator failure (and optionally an in-machine connection failure). Details of the first type of pre-start action are described in connection with fig. 4. As another example, when two or more hardware accelerator machines have accelerators connected to each other via a high speed interface such as ICI, a second type of pre-start task action may include the following operations: the checker daemon is requested or binary code installed on the machine is started to communicate with the checker daemon to determine the interconnect failure. Details of the second type of pre-start action are described in connection with fig. 5.
In some implementations, the pre-start task action may include the following operations: different firmware is loaded or installed to different hardware accelerators, or different hardware frequencies are set, or both, based on the characteristics of the user job. In this way, each hardware accelerator allocated to executing user jobs is executed under custom settings, and thus the system can significantly improve computational efficiency and throughput.
Node manager 255a may determine whether the task action failed before starting (330). In one example, if loading or installing a different firmware or setting a different hardware frequency fails, the task action before starting fails. The pre-start task action may also fail if it identifies the presence of a hardware accelerator failure or an interconnect failure. In response to determining that the task action failed before starting, node manager 255a may report data representing a general failure indication to cluster manager 220. Or node manager 255a may report the identified hardware accelerator fault or interconnect fault to cluster manager 220 (333) so that the identified hardware accelerator or interface may be checked and repaired in a different workflow. In some implementations, the node manager 255a can provide diagnostic information that represents context information for taking corrective action, as described above.
After receiving the fault indication or report, cluster manager 200 may select another machine to execute the portion of the user job. The node manager on the other machine may restart process 300 to perform a pre-check on the newly allocated machine before starting the portion of the user's job.
Next, node manager 255a terminates the portion of executing the user job (337) and eventually stops performing further operations on the hardware accelerator machine (380). The node manager may optionally report to the cluster manager 220 that the current machine is "down" or "out of service" until the hardware failure is resolved.
Node manager 225a may reassign a different hardware accelerator machine to execute the portion of the user job (335) and restart to perform pre-start task action 325.
In response to determining that the task action did not fail prior to start, node manager 255a starts executing the portion of the user job (340). The cluster manager may deploy the user job on the selected hardware accelerator machine that passes the pre-check and monitor the results of executing the user job.
Upon completion of the portion of the user job, node manager 255a determines if there is a post-completion task action in the program code package (360). In response to determining that there is no post-completion task action, node manager 255a provides output data generated from the portion of the user job to cluster manager 220 and stops performing further operations on the hardware accelerator machine (380) until a new portion of the user job is assigned to the machine. If the portion of the user job has multiple operations with multiple outputs, the node manager 255a may provide output data during execution of the portion of the user job, e.g., instead of waiting to end.
In response to determining that the post-completion task action exists, node manager 255a performs the post-completion task action (370). The post-completion task actions may include resetting parameters of an accelerator on the machine or purging memory or operations of memory associated with the machine, which may protect user privacy and minimize data leakage.
Node manager 255a may determine whether the task action failed after completion based on different criteria, i.e., whether the task action was successfully performed after completion, whether the accelerator parameters were successfully reset, or whether memory was "cleaned up" (370).
In response to determining that the task action has not failed after completion (370), node manager 255a may report output data generated from the portion of the user job to cluster manager 220 and cease performing further operations on the hardware accelerator machine (380) until a new portion of the user job is assigned to the machine.
In response to determining that the post-completion task action failed (373), node manager 255a may generate notification or fault information indicating the post-completion task action failure and provide the results to cluster manager 220 after executing the portion of the user job. Node manager 225a stops performing further operations until the unresolved failure information is processed (380).
FIG. 4 illustrates an example workflow 400 for performing operations specified in application-level pre-inspection checker binary code. For convenience, the workflow 400 is described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed system, such as cluster management system 101 of FIG. 1, may execute workflow 400.
As shown in FIG. 4, upon receiving a computing workload (e.g., a user job), cluster manager 410 may generate and provide instruction data 420 to node manager 430 on the assigned hardware accelerator machine 460. In this example, the hardware accelerator machine may not have an accelerator connected to the bag-side network.
Instruction data 420 may include a package of program code 440, where package of program code 440 includes a portion of a user job assigned to machine 460. Node manager 430 may install program code package 440 on machine 460.
As described above, prior to launching the portion of the user job, node manager 430 may obtain program code packages 440 installed on the machine and request checker binary code 450 to perform task actions for testing one or more hardware accelerators 470. Note that the checker binary code 450 is implemented by integrating it into the portion of the user's job, as described above.
Node manager 430 may determine whether a task action failed based on output data 480 returned when the task action was performed. The node manager 430 may then provide report data 490 to the cluster manager 410. Report data 490 may include notifications or reports of failures regarding hardware accelerators and interconnections when task actions fail. The cluster manager may flag the hardware accelerator machine 460 as "down", "unavailable" or "failed," provide a received failure report to help perform corrective actions, e.g., repair for the identified hardware accelerator or interface, and select another machine to perform part of the user job. When the task action has not failed, report data 490 may include results calculated by executing portions of the user job, and cluster manager 410 may aggregate the results received from different portions of the user job and generate an output of the user job.
FIG. 5 illustrates an example workflow 500 for performing operations specified in a pre-checkchecker daemon. For convenience, the workflow 500 is described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed system, such as cluster management system 101 of FIG. 1, may execute workflow 500.
As shown in fig. 5, two or more hardware accelerator machines 560a and 560b may have accelerators 560a and 560b connected to each other via a high speed interface (e.g., link 570) such as ICI. Unlike workflow 400, pre-inspection checks need to check inter-machine connections, and pre-inspection checks may be implemented as inspector daemons 540a and 540b.
In general, when different portions of a computing workload (e.g., user jobs) are assigned to hardware accelerator machines 560a and 560b, cluster manager 510 may generate instruction data 520a and 520b and provide the instruction data to hardware accelerator machines 560a and 560b, respectively. The instruction data may include inspector daemons 540a and 540b to be installed on machines 560a and 560b, respectively.
Cluster manager 510 may also provide job data 590 to centralized topology manager 515. The job data 590 includes data representing the number and type of hardware accelerators required to execute portions of the user job. The centralized topology manager 515 can generate expected topology data based on the job data 590 and provide the expected topology data 580a to the hardware accelerator machine 560a and the expected topology data 580b to the hardware accelerator machine 560b.
Node managers 530a and 530b may perform task actions before executing the respective portions of the user job on each machine 560a, 560 b. Task actions may include requesting checker daemons 540a and 540b to test the interconnection of hardware accelerators 560a and 560 b. More specifically, node managers 530a and 530b may request inspector daemons 540a and 540b to obtain expected topology data 580a and 580b, respectively. The inspector daemons 540a and 540b may also obtain telemetry data representing the observed topology data directly from the hardware accelerators 560a, 560 b. The inspector daemon may use one or more criteria and/or procedures to determine an inter-accelerator or inter-machine interconnect failure. One or more criteria and/or processes are designed to determine all-to-all routability of a subset of hardware components selected for executing a user job. The cluster management system may determine that the interconnect is healthy only if all of the criteria and/or procedures of the interconnect are met or performed. For example, the criteria and/or process may include at least one of: (i) whether the link physical layer state indicates that the link physical layer is operating properly, (ii) determining whether the per-lane BER is less than a threshold, (vi) checking per-lane status flags to ensure that there is no LOSs of lock (LOL) and LOSs of signal (LOS), (iii) checking whether a transceiver cable has been inserted, whether the transceiver is responding to a node manager, and whether the configuration of the transceiver cable is valid, and whether the transceiver is contained in a predefined list, (iv) determining whether the transceiver cable type matches a predefined or required type of the intended topology, (v) checking whether the transceiver transmitter and/or receiver is operating at a predefined power threshold, (vi) determining whether the neighboring node of a given node is the same as represented in the intended topology, (vii) and determining whether all of the above criteria or flows are met for at least a predefined period of time. As a simple example, the cluster management system may determine an interconnection failure by comparing expected topology data and observed topology data on the respective machines.
In addition, inspector daemons 540a and 540b may coordinate with one another to determine inter-machine interconnect faults, e.g., performance of link 570 connecting hardware accelerators 560a and 560 b.
FIG. 6 is an example scenario 600 of avoiding a failure of a task action when a misconnection associated with an unselected hardware accelerator is detected.
As described above, a centralized topology manager (e.g., centralized topology manager 280 of fig. 2 or centralized topology manager 515 of fig. 5) can hold data (e.g., a two-dimensional or three-dimensional grid or torus) representing the overall connection of hardware accelerators in different bag-side network topologies. Upon receiving a computing workload (e.g., a user job), the cluster manager may assign a portion of the user job to a plurality of hardware accelerators. For example, the cluster manager may select a2x 1 grid 610 that includes hardware accelerators 620 and 635 to execute portions of the user's job.
Whether the selected hardware accelerators are located in the same hardware accelerator machine or coupled to each other through HPN or ICI, the cluster management system may perform task actions for pre-inspection checks, including determining interconnect failure of the hardware accelerators. As shown in fig. 6, the cluster management system may determine that link a between selected accelerators 620 and 635 belongs to good connection 621 and that link D connecting selected accelerator 620 and unselected accelerator 625 is problematic connection 623. The cluster management system may further determine that links B and C belong to other good connections 637. Although link D is a failed connection, the system is not able to determine that the task action failed because failed link D does not belong to the interconnection of the 2 x1 grid. In fact, the system may ignore connections that are not used or selected for executing the user job. In this way, the system can perform task actions of the interconnect to examine only links between specified accelerators, not all links among all accelerators, which increases the computational efficiency of the pre-examination on the interconnect.
FIG. 7 is a flow chart of an example process 700 of performing a pre-inspection check in a cluster management system. For convenience, process 700 is described as being performed by a system of one or more computers located at one or more locations. For example, a suitably programmed system, such as cluster management system 101 of FIG. 1, may perform process 700.
The system may receive data representing a workload configuration (710). The data may be provided by a user or user device and received by a cluster manager of the system. As described above, the workload configuration may include data specifying the operations to be performed, the number and types of hardware components required to perform the operations, and the estimated runtime period.
The system may generate a computing workload based on the workload configuration (720). More specifically, a cluster manager of the system may generate a computational workload for performing the operations.
The system may assign computing workload to a first subset of one or more hardware accelerator machines (730). The cluster manager may select the first subset from a set of hardware accelerator machines included in the distributed system. Each machine in the set of hardware accelerator machines includes one or more hardware accelerators. Or the cluster manager may select a first subset of hardware accelerators from all hardware accelerators in the distributed system. The first subset of hardware accelerators may be located in one or more hardware accelerator machines. The hardware accelerator may include one or more GPU, TPU, VPU, FPGA and ASICs.
Prior to executing the computing workload, the system may perform a pre-check on a first subset of the one or more hardware accelerator machines (740). The pre-check may include an operation to verify functionality of the first subset of one or more hardware accelerator machines. As described above, the system may verify the functionality of the hardware accelerator machine based at least in part on the characteristics of the hardware accelerator and the user job. As described above, validating may include checking at least one or more arithmetic errors, interconnect bit error rates, topology errors, or interconnects of the first subset of one or more hardware accelerator machines. For example, the system may determine a hardware accelerator failure or an interconnect failure through a pre-check.
More specifically, for each hardware accelerator machine of the first subset of one or more hardware accelerator machines, the system may install a package of program code at the hardware accelerator machine (750). The program code package may include corresponding task actions representing a sequence of operations to be performed by a node manager at the hardware accelerator. The respective task actions may be based at least in part on characteristics of the computing workload.
The system (or node manager of the system) may then execute the sequence of operations on the hardware accelerator machine to generate an output indicating whether the corresponding task action failed (760). In general, the system may determine that the task action failed based on a determination that at least one of the hardware accelerator or the interconnect failed. Details of the determination process are described in connection with fig. 3.
Each time the corresponding task action fails, the system may reassign the computing workload to another subset of hardware accelerator machines different from the first subset (770). For example, assuming the system determines that a hardware accelerator in a first hardware accelerator machine in the first subset fails, the system may replace the first hardware accelerator machine with another accelerator machine outside of the first set in the distributed system.
The system may execute the computing workload using a first subset of the hardware accelerator machines whenever the corresponding task action has not failed (780).
After completing the computing workload, the system may remove the stored data or reset the current settings to default settings for each hardware accelerator machine in the first subset of one or more hardware accelerator machines (790). More specifically, the system may offload firmware installed on the one or more hardware accelerators, purge memory and storage associated with the one or more hardware accelerators, and reset parameters of the one or more hardware accelerators in the first subset of the one or more hardware accelerator machines to default values.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly embodied computer software or firmware, in computer hardware (including the structures disclosed in this specification and their structural equivalents), or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, e.g., one or more modules of computer program instructions encoded on a tangible, non-transitory program carrier, for execution by, or to control the operation of, data processing apparatus. The computer storage medium may be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or additionally, the program instructions may be encoded on a manually generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus.
The term "data processing apparatus" refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus may also be or further comprise a dedicated logic circuit, for example an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). In addition to hardware, the apparatus may optionally include code that creates an execution environment for the computer program, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program, which may also be referred to or described as a program, software application, app, module, software module, script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a communication network.
A system for one or more computers configured to perform a particular operation or action means that the system has installed thereon software, firmware, hardware, or a combination thereof that in operation causes the system to perform the operation or action. For one or more computer programs configured to perform a particular operation or action, it is meant that the one or more programs include instructions that, when executed by a data processing apparatus, cause the apparatus to perform the operation or action.
As used in this specification, "engine" or "software engine" refers to a software implemented input/output system that provides an output that is different from an input. The engine may be an encoded functional block such as a library, platform, software development kit ("SDK"), or object. Each engine may be implemented on any suitable type of computing device, such as a server, mobile phone, tablet, notebook, music player, electronic book reader, laptop or desktop computer, PDA, smart phone, or other stationary or portable device that includes one or more processors and computer-readable media. In addition, two or more of the engines may be implemented on the same computing device or on different computing devices.
The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, or combinations of, special purpose logic circuitry, e.g., an FPGA or ASIC, and one or more programmed computers.
A computer suitable for executing a computer program may be based on a general-purpose or special-purpose microprocessor or both, or any other kind of central processing unit. Typically, a central processing unit will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a central processing unit for executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory may be supplemented by, or incorporated in, special purpose logic circuitry. Typically, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. But the computer does not require such a device. Furthermore, the computer may be embedded in another device, such as a mobile phone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a Universal Serial Bus (USB) flash drive), to name a few.
Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example: semiconductor memory devices such as EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disk; and CD ROM and DVD-ROM discs.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, or a presence-sensitive display or other surface by which the user can provide input to the computer. Other types of devices may also be used to provide interaction with a user; for example, feedback provided to the user may be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. In addition, the computer may interact with the user by: transmitting and receiving documents to and from devices used by the user; for example, a web page is sent to a web browser on a user device in response to a request received from the web browser. Further, the computer may interact with the user by sending text messages or other forms of messages to a personal device (e.g., a smart phone), running a messaging application, and receiving response messages as feedback from the user.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, such as a data server, or that includes a middleware component, such as an application server, or that includes a front-end component, such as a client computer having a graphical user interface, web browser, or app through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network ("LAN") and a wide area network ("WAN"), such as the Internet.
The computing system may include clients and servers. The client and server are typically remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server sends data (e.g., HTML pages) to the user device, e.g., for purposes of displaying data to and receiving user input from a user interacting with the device as a client. Data generated at the user device, e.g., results of a user interaction, may be received at the server from the device.
In addition to the above-described embodiments, the following embodiments are also innovative:
Embodiment 1 is a method comprising: receiving, by a cluster manager, data representing a workload configuration; generating, by the cluster manager, a computing workload representing a set of operations to be performed based on the workload configuration; assigning, by the cluster manager, the computing workload to a first subset of one or more hardware accelerator machines from among a set of hardware accelerator machines, wherein each of the set of hardware accelerator machines includes one or more hardware accelerators; performing a pre-check on the first subset of one or more hardware accelerator machines to verify functionality of the first subset of one or more hardware accelerator machines prior to executing the computing workload, wherein performing the pre-check includes: for each hardware accelerator machine of the first subset of one or more hardware accelerator machines: installing, at the hardware accelerator machine, a program code package comprising respective task actions representing a sequence of operations to be performed by a node manager at the hardware accelerator machine; executing, by the node manager, the sequence of operations on the hardware accelerator machine to generate an output indicating whether the respective task action failed; reassigning the computing workload to a second subset of hardware accelerator machines different from the first subset whenever the corresponding task action fails; and executing the computing workload using the first subset of hardware accelerator machines whenever the respective task action does not fail.
Embodiment 2 is the method according to embodiment 1, wherein the respective task actions are based at least in part on characteristics of the computing workload, wherein verifying the functionality of the first subset of one or more hardware accelerator machines is based at least in part on characteristics of one or more hardware accelerators of the first subset of one or more hardware accelerator machines and the characteristics of the computing workload, wherein verifying the functionality further comprises checking for at least one or more arithmetic errors, interconnect bit error rates, topology errors, or interconnections of one or more hardware accelerators of the first subset of one or more hardware accelerator machines.
Embodiment 3 is the method according to embodiment 1 or 2, further comprising: after completion of the computing workload, the stored data is removed or the current settings are reset to default settings for each hardware accelerator machine of the first subset of one or more hardware accelerator machines.
Embodiment 4 is the method of any one of embodiments 1-3, further comprising: detecting a failure of the computing workload during execution of the computing workload; and, in response, reassigning, by the cluster manager, the computing workload to a third subset of hardware accelerator machines different from the first subset and the second subset.
Embodiment 5 is the method of any of embodiments 1-4, wherein the respective task actions each comprise a pre-start task action or a post-completion task action, wherein the pre-start task action comprises a first sequence of operations to be performed by the node manager at the hardware accelerator machine before executing the computing workload, wherein the post-completion task comprises a second sequence of operations to be performed by the node manager at the hardware accelerator machine after completing the computing workload.
Embodiment 6 is the method of any one of embodiments 1-5, further comprising: in response to determining that the respective task action failed, generating data indicative of fault information associated with the hardware accelerator machine; and terminating execution of the corresponding task action.
Embodiment 7 is the method of any one of embodiments 1-6, wherein the sequence of operations includes: the pre-checkchecker binary code included in the installed program code package is initiated by the node manager at the hardware accelerator machine.
Embodiment 8 is the method of embodiment 7, wherein the implementation of the pre-checkchecker binary code comprises the pre-checkchecker binary code integrated into the computing workload or installed as a machine daemon on the hardware accelerator machine, wherein the implementation is determined based at least in part on one or more of a metric of disk space consumption, operating system root privileges, or release rhythm.
Embodiment 9 is the method of embodiment 7 or 8, wherein the pre-inspector binary code includes instructions that, when executed by one or more hardware accelerators of the hardware accelerator machine, cause the one or more hardware accelerators of the hardware accelerator machine to execute a portion of the allocated computational workload to generate job output.
Embodiment 10 is the method of embodiment 9, wherein generating the output indicating whether the respective task action failed comprises: comparing the job output with a reference output; and in response to determining that the job output differs from the reference output by a threshold, determining that the corresponding task action failed.
Embodiment 11 is the method of any of embodiments 7-10, wherein the pre-inspection checker binary code is a checker daemon installed on the hardware accelerator machine, the checker daemon when executed configured to determine interconnect faults among one or more hardware accelerators in the first subset of one or more hardware accelerator machines, wherein the determination of the interconnect faults is based on topology data provided by a centralized topology manager, the topology data representing interconnections between hardware accelerators in the first subset of one or more hardware accelerator machines.
Embodiment 12 is the method of embodiment 11, wherein generating the output indicating whether the respective task action failed comprises: obtaining the topology data from the centralized topology manager as expected topology data for the first subset of hardware accelerator machines based on the computing workload; obtaining telemetry data as observed topology data for the first subset of hardware accelerator machines, the telemetry data including at least a representation of observed interconnections between hardware accelerators in the first subset of one or more hardware accelerator machines; and generating, in response to determining an interconnect failure based on the expected topology and the observed topology, the output indicative of failure of the respective task action.
Embodiment 13 is the method of embodiment 12, wherein the determining of the interconnect failure includes checking routing information of neighboring hardware accelerators relative to a given hardware accelerator in the first subset of one or more hardware accelerator machines, or quality of inter-accelerator links connecting one hardware accelerator and another hardware accelerator in the first subset of one or more hardware accelerator machines.
Embodiment 14 is the method of any of embodiments 1-13, wherein the respective task actions further comprise loading different firmware or setting different hardware settings for one or more hardware accelerator machines of the first subset.
Embodiment 15 is the method of any one of embodiments 1-14, wherein the set of hardware accelerator machines includes: a hardware accelerator machine comprising one or more hardware accelerators connected to a network through a host; a hardware accelerator machine comprising one or more hardware accelerators directly connected to a network via a high speed interface; and a hardware accelerator machine comprising one or more hardware accelerators connected to each other through a bag-side network.
Embodiment 16 is a system comprising one or more computers and one or more storage devices storing instructions that when executed by the one or more computers are operable to cause the one or more computers to perform the method of any one of embodiments 1 to 15.
Embodiment 17 is a computer storage medium encoded with a computer program, the program comprising instructions that when executed by data processing apparatus are operable to cause the data processing apparatus to perform the method of any of embodiments 1 to 15.
While this specification contains many specifics of implementations, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the claimed combination and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, although operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Specific embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Reference is made to the claims as claimed.
Claims (20)
1. A method, comprising:
Receiving, by a cluster manager, data representing a workload configuration;
Generating, by the cluster manager, a computing workload representing a set of operations to be performed based on the workload configuration;
Assigning, by the cluster manager, the computing workload to a first subset of one or more hardware accelerator machines from among a set of hardware accelerator machines, wherein each of the set of hardware accelerator machines includes one or more hardware accelerators;
Performing a pre-check on the first subset of one or more hardware accelerator machines to verify functionality of the first subset of one or more hardware accelerator machines prior to executing the computing workload, wherein performing the pre-check includes:
for each hardware accelerator machine of the first subset of one or more hardware accelerator machines:
Installing, at the hardware accelerator machine, a program code package comprising respective task actions representing a sequence of operations to be performed by a node manager in the hardware accelerator machine;
executing, by the node manager, the sequence of operations on the hardware accelerator machine to generate an output indicating whether the respective task action failed;
Reassigning the computing workload to a second subset of hardware accelerator machines different from the first subset whenever the corresponding task action fails; and
The computing workload is executed using the first subset of hardware accelerator machines whenever the respective task action has not failed.
2. The method of claim 1, wherein the respective task actions are based at least in part on characteristics of the computing workload, wherein verifying the functionality of the first subset of one or more hardware accelerator machines is based at least in part on characteristics of one or more hardware accelerators of the first subset of one or more hardware accelerator machines and the characteristics of the computing workload, wherein verifying the functionality further comprises checking for at least one or more arithmetic errors, interconnect bit error rates, topology errors, or interconnects of one or more hardware accelerators of the first subset of one or more hardware accelerator machines.
3. The method of claim 1, further comprising:
After completion of the computing workload, the stored data is removed or the current settings are reset to default settings for each hardware accelerator machine of the first subset of one or more hardware accelerator machines.
4. The method of claim 1, further comprising:
Detecting a failure of the computing workload during execution of the computing workload; and
In response, the computing workload is reassigned by the cluster manager to a third subset of hardware accelerator machines different from the first subset and the second subset.
5. The method of claim 1, wherein the respective task actions each comprise a pre-start task action or a post-completion task action, wherein the pre-start task action comprises a first sequence of operations to be performed by the node manager at the hardware accelerator machine before executing the computing workload, wherein the post-completion task comprises a second sequence of operations to be performed by the node manager at the hardware accelerator machine after completing the computing workload.
6. The method of claim 1, further comprising:
In response to determining that the respective task action failed, generating data indicative of fault information associated with the hardware accelerator machine; and
Terminating execution of the corresponding task action.
7. The method of claim 1, wherein the sequence of operations comprises:
the pre-checkchecker binary code included in the installed program code package is initiated by the node manager at the hardware accelerator machine.
8. The method of claim 7, wherein the implementation of the pre-checkchecker binary comprises the pre-checkchecker binary integrated in the computing workload or installed as a machine daemon on the hardware accelerator machine, wherein the implementation is determined based at least in part on one or more of a measure of disk space consumption, an operating system root privilege, or a release cadence.
9. The method of claim 7, wherein the pre-inspection checker binary code includes instructions that, when executed by one or more hardware accelerators of the hardware accelerator machine, cause the one or more hardware accelerators of the hardware accelerator machine to execute a portion of the allocated computational workload to generate job output.
10. The method of claim 9, wherein generating the output indicating whether the respective task action failed comprises:
comparing the job output with a reference output; and
In response to determining that the job output differs from the reference output by a threshold, determining that the corresponding task action failed.
11. The method of claim 7, wherein the pre-inspection checker binary code is an inspector daemon installed on the hardware accelerator machine, the inspector daemon when executed configured to determine an interconnect fault among one or more hardware accelerators in the first subset of one or more hardware accelerator machines, wherein the determination of the interconnect fault is based on topology data provided by a centralized topology manager, the topology data representing an interconnect between hardware accelerators in the first subset of one or more hardware accelerator machines.
12. The method of claim 11, wherein generating the output indicating whether the respective task action failed comprises:
Obtaining the topology data from the centralized topology manager as expected topology data for the first subset of hardware accelerator machines based on the computing workload;
Obtaining telemetry data as observed topology data for the first subset of hardware accelerator machines, the telemetry data including at least a representation of observed interconnections between hardware accelerators in the first subset of one or more hardware accelerator machines; and
In response to determining an interconnect failure based on the expected topology and the observed topology, the output is generated indicating failure of the respective task action.
13. The method of claim 12, wherein the determination of the interconnect failure includes checking routing information of adjacent hardware accelerators relative to a given hardware accelerator in the first subset of one or more hardware accelerator machines, or quality of an inter-accelerator link connecting one hardware accelerator in the first subset of one or more hardware accelerator machines with another hardware accelerator.
14. The method of claim 1, wherein the respective task actions further comprise loading different firmware or setting different hardware settings for one or more hardware accelerator machines of the first subset.
15. The method of claim 1, wherein the set of hardware accelerator machines comprises: a hardware accelerator machine comprising one or more hardware accelerators connected to a network through a host; a hardware accelerator machine comprising one or more hardware accelerators directly connected to a network via a high speed interface; and a hardware accelerator machine comprising one or more hardware accelerators connected to each other through a bag-side network.
16. One or more computer-readable storage media storing instructions that, when executed by one or more computers, cause the one or more computers to perform respective operations, wherein the respective operations comprise:
Receiving, by a cluster manager, data representing a workload configuration;
Generating, by the cluster manager, a computing workload representing a set of operations to be performed based on the workload configuration;
Assigning, by the cluster manager, the computing workload to a first subset of one or more hardware accelerator machines from among a set of hardware accelerator machines, wherein each of the set of hardware accelerator machines includes one or more hardware accelerators;
Performing a pre-check on the first subset of one or more hardware accelerator machines to verify functionality of the first subset of one or more hardware accelerator machines prior to executing the computing workload, wherein performing the pre-check includes:
for each hardware accelerator machine of the first subset of one or more hardware accelerator machines:
Installing, at the hardware accelerator machine, a program code package comprising respective task actions representing a sequence of operations to be performed by a node manager at the hardware accelerator;
executing, by the node manager, the sequence of operations on the hardware accelerator machine to generate an output indicating whether the respective task action failed;
Reassigning the computing workload to a second subset of hardware accelerator machines different from the first subset whenever the corresponding task action fails; and
The computing workload is executed using the first subset of hardware accelerator machines whenever the respective task action has not failed.
17. The one or more computer-readable storage media of claim 16, wherein the respective task actions are based at least in part on characteristics of the computing workload, wherein verifying the functionality of the first subset of one or more hardware accelerator machines is based at least in part on characteristics of one or more hardware accelerators of the first subset of one or more hardware accelerator machines and the characteristics of the computing workload, wherein verifying the functionality further comprises checking for at least one or more arithmetic errors, interconnect bit error rates, topology errors, or interconnections of one or more hardware accelerators in the first subset of one or more hardware accelerator machines.
18. The one or more computer-readable storage media of claim 16, further comprising:
After completion of the computing workload, the stored data is removed or the current settings are reset to default settings for each hardware accelerator machine of the first subset of one or more hardware accelerator machines.
19. A system comprising one or more computers and one or more storage devices storing instructions that, when executed by the one or more computers, cause the one or more computers to perform respective operations, wherein the respective operations comprise:
Receiving, by a cluster manager, data representing a workload configuration;
Generating, by the cluster manager, a computing workload representing a set of operations to be performed based on the workload configuration;
Assigning, by the cluster manager, the computing workload to a first subset of one or more hardware accelerator machines from among a set of hardware accelerator machines, wherein each of the set of hardware accelerator machines includes one or more hardware accelerators;
Performing a pre-check on the first subset of one or more hardware accelerator machines to verify functionality of the first subset of one or more hardware accelerator machines prior to executing the computing workload, wherein performing the pre-check includes:
for each hardware accelerator machine of the first subset of one or more hardware accelerator machines:
Installing, at the hardware accelerator machine, a program code package comprising respective task actions representing a sequence of operations to be performed by a node manager at the hardware accelerator;
executing, by the node manager, the sequence of operations on the hardware accelerator machine to generate an output indicating whether the respective task action failed;
Reassigning the computing workload to a second subset of hardware accelerator machines different from the first subset whenever the corresponding task action fails; and
The computing workload is executed using the first subset of hardware accelerator machines whenever the respective task action has not failed.
20. The system of claim 19, wherein the respective task actions are based at least in part on characteristics of the computing workload, wherein verifying the functionality of the first subset of one or more hardware accelerator machines is based at least in part on characteristics of one or more hardware accelerators of the first subset of one or more hardware accelerator machines and the characteristics of the computing workload, wherein verifying the functionality further comprises checking for at least one or more arithmetic errors, interconnect bit error rates, topology errors, or interconnections of one or more hardware accelerators of the first subset of one or more hardware accelerator machines.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/540,123 | 2021-12-01 |
Publications (1)
Publication Number | Publication Date |
---|---|
CN118043789A true CN118043789A (en) | 2024-05-14 |
Family
ID=
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10977068B2 (en) | Minimizing impact of migrating virtual services | |
US11249876B2 (en) | System and method for predicting application performance for large data size on big data cluster | |
US20210406079A1 (en) | Persistent Non-Homogeneous Worker Pools | |
US10732957B2 (en) | Determining a stability index associated with a software update | |
US9442791B2 (en) | Building an intelligent, scalable system dump facility | |
US10983887B2 (en) | Validation of multiprocessor hardware component | |
US20170123873A1 (en) | Computing hardware health check | |
US20220188192A1 (en) | Identifying harmful containers | |
US10484300B2 (en) | Admission control based on the end-to-end availability | |
JP2022100301A (en) | Method for determining potential impact on computing device by software upgrade, computer program, and update recommendation computer server (recommendation of stability of software upgrade) | |
US20230239194A1 (en) | Node health prediction based on failure issues experienced prior to deployment in a cloud computing system | |
WO2023003640A1 (en) | Automated cross-service diagnostics for large scale infrastructure cloud service providers | |
CN109062580B (en) | Virtual environment deployment method and deployment device | |
CN118043789A (en) | Pre-inspection of hardware accelerators in a distributed system | |
US20230168919A1 (en) | Preflight checks for hardware accelerators in a distributed system | |
US11755433B2 (en) | Method and system for health rank based virtual machine restoration using a conformal framework | |
US11294804B2 (en) | Test case failure with root cause isolation | |
JP7465045B2 (en) | Increased virtual machine processing power for abnormal events | |
Gao et al. | An Empirical Study on Quality Issues of Deep Learning Platform | |
US20240152421A1 (en) | Apparatus and method for specifying a desired scanning feature | |
US11645142B1 (en) | Use sequential set index for root cause location and problem verification | |
US20230297465A1 (en) | Detecting silent data corruptions within a large scale infrastructure | |
US11556361B2 (en) | Monitoring and managing of complex multi-role applications | |
US20220413887A1 (en) | Recoverable container platform cluster for testing | |
US20240070012A1 (en) | Self-healing in container orchestration systems |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication |