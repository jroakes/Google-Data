US20240114158A1 - Hierarchical Video Encoders - Google Patents
Hierarchical Video Encoders Download PDFInfo
- Publication number
- US20240114158A1 US20240114158A1 US18/529,173 US202318529173A US2024114158A1 US 20240114158 A1 US20240114158 A1 US 20240114158A1 US 202318529173 A US202318529173 A US 202318529173A US 2024114158 A1 US2024114158 A1 US 2024114158A1
- Authority
- US
- United States
- Prior art keywords
- video
- frame
- segment
- representations
- frames
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000034 method Methods 0.000 claims abstract description 83
- 238000012545 processing Methods 0.000 claims abstract description 25
- 230000004807 localization Effects 0.000 claims description 48
- 238000012549 training Methods 0.000 claims description 27
- 230000004044 response Effects 0.000 claims description 10
- 230000002123 temporal effect Effects 0.000 claims description 10
- 230000008569 process Effects 0.000 description 25
- 238000010586 diagram Methods 0.000 description 18
- 230000015654 memory Effects 0.000 description 13
- 238000013528 artificial neural network Methods 0.000 description 12
- 230000000007 visual effect Effects 0.000 description 10
- 239000008186 active pharmaceutical agent Substances 0.000 description 7
- 241000287828 Gallus gallus Species 0.000 description 6
- 238000013459 approach Methods 0.000 description 6
- 230000008901 benefit Effects 0.000 description 4
- 238000004891 communication Methods 0.000 description 4
- 230000006870 function Effects 0.000 description 4
- 230000000699 topical effect Effects 0.000 description 4
- 230000009286 beneficial effect Effects 0.000 description 3
- 230000007246 mechanism Effects 0.000 description 3
- 239000000203 mixture Substances 0.000 description 3
- 235000021116 parmesan Nutrition 0.000 description 3
- 230000000306 recurrent effect Effects 0.000 description 3
- 230000004075 alteration Effects 0.000 description 2
- 238000013527 convolutional neural network Methods 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 238000002372 labelling Methods 0.000 description 2
- 238000013519 translation Methods 0.000 description 2
- 230000009471 action Effects 0.000 description 1
- 238000007792 addition Methods 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 238000007906 compression Methods 0.000 description 1
- 230000006835 compression Effects 0.000 description 1
- 238000013144 data compression Methods 0.000 description 1
- 230000003247 decreasing effect Effects 0.000 description 1
- 230000000694 effects Effects 0.000 description 1
- 230000006872 improvement Effects 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 238000010801 machine learning Methods 0.000 description 1
- 239000003550 marker Substances 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000011218 segmentation Effects 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/30—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using hierarchical techniques, e.g. scalability
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/084—Backpropagation, e.g. using gradient descent
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
- H04N19/136—Incoming video signal characteristics or properties
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/17—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object
- H04N19/172—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object the region being a picture, frame or field
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/177—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being a group of pictures [GOP]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N7/00—Computing arrangements based on specific mathematical models
- G06N7/01—Probabilistic graphical models, e.g. probabilistic networks
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/42—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals characterised by implementation details or hardware specially adapted for video compression or decompression, e.g. dedicated software implementation
- H04N19/436—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals characterised by implementation details or hardware specially adapted for video compression or decompression, e.g. dedicated software implementation using parallelised computational arrangements
Definitions
- the present disclosure relates generally to hierarchical video encoders. More particularly, the present disclosure relates to hierarchical video encoding for moment localization in a corpus of video data.
- Video content can be stored and/or shared online with a variety of users. As an amount of video content available online increases, it can be increasingly desirable to provide systems enabling a user to selectively search for desired video content among online sources. Systems such as search engines may be provided that allow a user to input a search query and retrieve results responsive to the search query from one or more distinct sources. Video content may be included among the retrieved results.
- One example aspect of the present disclosure is directed to a computer-implemented method for generating video representations utilizing a hierarchical video encoder.
- the computer-implemented method can include obtaining, by a computing system including one or more computing devices, a video, wherein the video includes a plurality of frames.
- the computer-implemented method can include processing, by the computing system, each of the plurality of frames with a machine-learned frame-level encoder model to respectively generate a plurality of frame representations for the plurality of frames, the plurality of frame representations respective to the plurality of frames.
- the computer-implemented method can include determining, by the computing system, a plurality of segment representations representative of a plurality of video segments including one or more of the plurality of frames, the plurality of segment representations based at least in part on the plurality of frame representations.
- the computer-implemented method can include processing, by the computing system, the plurality of segment representations with a machine-learned segment-level encoder model to generate a plurality of contextualized segment representations.
- the computer-implemented method can include determining, by the computing system, a video representation based at least in part on the plurality of contextualized segment representations.
- the computer-implemented method can include providing, by the computing system, the video representation as an output.
- the computer-implemented method can include obtaining, by a computing system including one or more computing devices, a user query, the user query including text.
- the computer-implemented method can include identifying, by the computing system, one or more highest likelihood videos of the plurality of videos, each highest likelihood video of the one or more highest likelihood videos identified based at least in part on a video-query compatibility score between the user query and a video representation of the highest likelihood video that is output by a machine-learned hierarchical video encoder model.
- the computer-implemented method can include determining, by the computing system, a moment localization within a matching video of the one or more highest likelihood videos, the moment localization including a moment beginning and a moment end.
- the moment beginning can include a beginning frame of the matching video, the beginning frame having a frame representation that is classified as representing a beginning of a moment described by the user query.
- the moment end can include an end frame of the matching video, the end frame having a frame representation that is classified as representing an end of the moment described by the user query.
- FIG. 1 A depicts a block diagram of an example computing system that performs hierarchical video encoding according to example embodiments of the present disclosure.
- FIG. 1 B depicts a block diagram of an example computing device that performs hierarchical video encoding according to example embodiments of the present disclosure.
- FIG. 1 C depicts a block diagram of an example computing device that performs hierarchical video encoding according to example embodiments of the present disclosure.
- FIG. 2 depicts a block diagram of an example hierarchical video encoder model according to example embodiments of the present disclosure.
- FIG. 3 depicts a block diagram of an example hierarchical video encoder model according to example embodiments of the present disclosure.
- FIG. 4 depicts a block diagram of an example hierarchical video encoder model according to example embodiments of the present disclosure.
- FIG. 5 depicts a block diagram of an example hierarchical video encoder model according to example embodiments of the present disclosure.
- FIG. 6 depicts a flow chart diagram of an example method for generating video representations utilizing a hierarchical video encoder according to example embodiments of the present disclosure.
- FIG. 7 depicts a flow chart diagram of an example method for moment localization in a video corpus including a plurality of videos according to example embodiments of the present disclosure.
- a video can be represented as a sequence of (e.g., fixed length) video segments or “clips” which, intuitively, serve as memory units representing the semantics of one or more frames in the video segment.
- Each video segment can be a nonoverlapping set of one or more frames of a larger video.
- a “frame” with respect to a video refers to audio, visual, and/or captioning/transcript data associated with a (e.g., smallest) temporal slice of the video.
- a video may be composed of at least a (e.g., temporally linear) sequence of frames, where each frame includes an image, a portion of a stream of audio data to be played along with the sequence of images, and/or supplementary text (e.g., captioning) to be displayed along with the sequence of images.
- a (e.g., temporally linear) sequence of frames where each frame includes an image, a portion of a stream of audio data to be played along with the sequence of images, and/or supplementary text (e.g., captioning) to be displayed along with the sequence of images.
- the hierarchical video encoders as described herein include a hierarchy of two (or more) encoder models, such as Transformers (e.g., cross-attentional transformers).
- a lower-level intrasegment encoder (also referred to as a frame-level encoder) encodes frame-level information of video data (e.g., video frames or representations thereof) into frame representations. Segment representations for video segments can be determined based on these frame representations, such as by providing a context token for a given video segment based on the frame representations of frames in that video segment.
- a higher-level intersegment encoder also referred to as a segment-level encoder encodes the segment representations into contextualized segment representations, which can further be used to produce a video representation.
- the hierarchical video encoder model can include a frame-level encoder model configured to receive a plurality of frames of a video as input and provide, in response to receipt of the plurality of frames as input, a plurality of frame representations of the plurality of frames as output.
- the hierarchical video encoder model can include a segment-level encoder model configured to receive a plurality of segment representations as input and provide, in response to receipt of the plurality of segment representations as input, a plurality of contextualized segment representations as output.
- At least one of the frame-level encoder model or the segment-level encoder model can be a multimodal encoder configured to produce a plurality of representations based at least in part on associated text.
- the encoder(s) e.g., the lower-level encoder and/or the higher level encoder
- the encoder(s) can be cross-modal encoders that additionally fuse the video data and/or representations thereof with associated text data, such as, for example, captioning data for the video and/or query data descriptive of a user query representing a user's search for videos and/or, more particularly, content depicted within the videos.
- the input modality pairs can have cross attention, such as visual-caption/transcript, visual-query, and/or transcript-query attention.
- the associated text can be encoded (e.g., by a text encoder model, such as a text transformer).
- a video be denoted by v, and the total number of frames in the video v be noted by N. Additionally, let the (e.g., fixed) length of each video segment be denoted by M. M can be selected based on the task and/or dataset used.
- a query be denoted by h.
- a lower-level cross-attentional encoder ⁇ can receive as input a frame sequence of a video segment u k and the query h and output, in response, contextualized frame-level features x t by ⁇ (x t ; u k , h
- t 1, 2, . . . M) ⁇ for each video segment u k .
- a segment representation ⁇ k of the frames of each video segment can be determined for each video segment based on the frame-level features in the segment.
- V CLS k is a context token (e.g., a visual CLS frame) associated with a video segment u k .
- These segment representations for each video segment can be input (e.g., as a sequence and/or in addition to the query h) to a higher-level cross-attention encoder ⁇ .
- the higher-level encoder ⁇ can output, in response, contextualized segment level features ⁇ ( ⁇ k ; v, h
- k 1, 2, . . .
- the hierarchical video encoder intuitively learns the segment representations using local (intra-segment) self- and/or cross-attention among the frames belonging to the same video segment by the lower-level encoder, while the high-level encoder learns the video representation using global (inter-segment) self- and cross-attention among the video segments of the video.
- the machine-learned frame-level encoder model and the machine-learned segment-level encoder model can include one or more shared parameters.
- the models may be separately utilized but have some or all common parameters between the models such that the models are similar or identical.
- each model can have entirely unique parameters.
- the hierarchical video encoder models described herein can be employed in a computer-implemented method for generating video representations.
- the method can include obtaining (e.g., by a computing system including one or more computing devices) a video, wherein the video includes a plurality of frames.
- Each frame can include visual data (e.g., an image) and/or associated audio data (e.g., a slice of an audio stream).
- the video may be unsegmented, such that no temporal divisions exist in the video.
- the video may be, for example, accessed from a corpus of videos, such as a content sharing website, media provider, database, and/or other suitable corpus.
- the method can include processing (e.g., by the computing system) each of the plurality of frames with a machine-learned frame-level encoder model to respectively generate a plurality of frame representations for the plurality of frames.
- the plurality of frame representations can be respective to the plurality of frames.
- each frame representation can be produced from a respective (e.g., unique) frame of the plurality of frames.
- the frame-level encoder model can be a multimodal encoder model configured to produce the plurality of frame representations based at least in part on associated text (e.g., a user query, captioning for the video, etc.).
- the method can include processing (e.g., by the computing system) the associated text with the machine-learned frame-level encoder model to produce the plurality of frame representations, wherein the plurality of frame representations are based at least in part on the associated text.
- the associated text can be processed concurrently with the plurality of frames.
- the associated text can be encoded.
- the method can include determining (e.g., by the computing system) a plurality of segment representations representative of a plurality of video segments including one or more of the plurality of frames.
- the plurality of video segments can each have about equal length.
- a video may be divided into video segments based at least in part on a fixed segment length.
- the plurality of video segments may be nonoverlapping. For instance, a given frame may be included within only one video segment of the plurality of video segments.
- the plurality of segment representations can be based at least in part on the plurality of frame representations.
- the plurality of segment representations can include a context token.
- the plurality of frame representations can be, can include, or can otherwise be used to generate a contextualized frame representation, such as a context (e.g., CLS) token specific to each frame.
- the context tokens for each frame can be aggregated or otherwise combined to produce a segment representation for a video segment including the frames for which the context tokens are combined.
- the method can include processing (e.g., by the computing system) the plurality of segment representations with a machine-learned segment-level encoder model to generate a plurality of contextualized segment representations.
- the contextualized segment representation can include a context (e.g., CLS) token specific to the respective video segment.
- processing the plurality of segment representations can include processing (e.g., by the computing system) the associated text with the machine-learned segment-level encoder model to produce the plurality of contextualized segment representations.
- the plurality of contextualized segment representations can thus be based at least in part on the associated text.
- the method can include determining (e.g., by the computing system), based at least in part on the plurality of contextualized segment representations, a video representation. For instance, in some implementations, context tokens corresponding to each segment in a video can be aggregated or otherwise combined to produce the video representation. Additionally, the method can include providing (e.g., by the computing system) the video representation as an output (e.g., of the hierarchical video encoder model).
- Hierarchical video encoders as described herein can be useful in a variety of computing tasks.
- One example task relates to identifying and localizing a moment relevant to a user query (e.g., a text query) from a corpus of videos, which may be untrimmed and/or unsegmented.
- a user query may be a single query sentence describing a relatively small portion within a larger video.
- a user searching in response to a user query may wish to see particular moments of a longer video in response to the user query, such as to see only segments of the video depicting content that is relevant to the query.
- a video titled “how to cook chicken parmesan” and depicting steps of making chicken parmesan may include a portion dedicated to a step of butterflying chicken.
- a user searching with a query such as “how to butterfly chicken” may desire to view the video titled “how to cook chicken parmesan” despite the apparent lack of relationship between video title and content.
- the user may be presented with the portion of the video (e.g., the moment) related to butterflying chicken such that the user does not have to manually search for the related content, which may not be immediately apparent to the user.
- This problem can be complicated as the number of channels (e.g., audio, visual, captioning) continues to scale and/or as a number of videos continues to scale. For instance, some existing approaches can experience decreasing performance (e.g., accuracy) as a number of frames irrelevant to a user query in a video increases.
- MLVC Segment localization in video corpus
- video content available online continues to grow, it has become increasingly desirable and increasingly difficult to thoroughly manage and categorize the ever-increasing corpus of video content.
- an intelligent system must understand rich and complex semantic information included in the videos.
- These videos can have a significant variation in factors such as content type, length, appearance, quality, and other factors.
- the MLVC problem can generally require a more fine-grained understanding of video content than, for example, video retrieval. For instance, localizing a moment responsive to a user query can require semantic understanding of many possible segments of videos.
- Some existing approaches to handle this problem pre-segment a larger video into smaller chunks. However, this approach can have issues such as microsegments. Furthermore, existing approaches that depend on a squared average of number of frames can be infeasible for large corpuses of videos and/or having a large number of frames. Furthermore, some existing solutions may pack information of different granularity into a single vector embedding, making it hard to balance the differing demands between retrieving a long video and localizing a short segment.
- a computing system including one or more computing devices can obtain (e.g., from a user) a user query.
- the user query can include text (e.g., text data).
- the user query can be obtained in any suitable manner according to example aspects of the present disclosure.
- the user query can be obtained from a user by providing a user with a text field in which to enter the user query, such as at a search engine service.
- the user query can be obtained from an external computing system or other computing device.
- the user query may be or include only text data, may be or may include speech data (e.g., that is converted into text data) and/or may be or may include any other suitable data.
- the user query can be or can include a short text string (e.g., on the order of fewer than about 20 words) descriptive of a moment within a video.
- a number of highest ranking videos can be selected such that moment localization is performed on the highest ranking videos to identify a moment relevant to the user query.
- a computing system can identify one or more highest likelihood videos of the plurality of videos. This task of identifying the highest ranking video(s) is referred to herein as Video Retrieval, or VR.
- Performing the VR task can primarily be useful in reducing computational requirements by restricting a number of videos that must be searched for moment localization.
- each highest likelihood video of the one or more highest likelihood videos can be identified based at least in part on a video-query compatibility score between the user query and a video representation of the highest likelihood video that is output by a machine-learned hierarchical video encoder model, as described herein.
- the video-query compatibility score can effectively rank the corpus of videos and the K highest scoring video(s) in the corpus, as defined by the video-query compatibility score, can be selected as the highest likelihood video(s).
- the video representation of a highest likelihood video can be based at least in part on a highest scoring segment representation of a plurality of segment representations of the highest likelihood video.
- the hierarchical video encoder may output a plurality of segment representations associated with a plurality of video segments of the highest scoring videos, each of which has an associated compatibility score with the user query.
- the highest score of these compatibility scores can be used as representative of the entire video.
- the one or more highest likelihood videos can be selected based at least in part on a negative log-likelihood of the one or more highest likelihood videos containing the moment described by the user query. For instance, the videos can be selected to minimize the negative log-likelihood.
- a modeling objective for the video retrieval task can select a matching video most likely to have a moment to be localized by employing a contrastive loss that contrasts a compatibility score of positive (e.g., matching) pairs of video representation and query against negative (e.g., not matching) pairs of video representation and query.
- the negative pairs can be randomly sampled.
- One example compatibility score is computed as:
- W VR is a linear regressor
- the representation of a highest likelihood video can include a highest scoring segment representation of a plurality of segment representations of the highest likelihood video. For instance, of a plurality of segments of the video, the score of the highest-scoring segment can be selected as representative of the entire video.
- the one or more highest likelihood videos can be selected based at least in part on a negative log-likelihood of the one or more highest likelihood videos containing the moment described by the user query.
- the VR loss can be computed as:
- v (i) is a ground-truth video for a text query h (i) .
- the hierarchical video encoder model can parameterize the conditional probability P(v
- moment(s) within the videos related to the user query can be localized. For instance, a moment localization can be determined for a moment, where the moment localization specifies a beginning and/or an end of the moment. As one example, the moment localization can be or can include timestamps, frame indices, etc. This task is referred to as Moment Localization in Single Video, or MLSV.
- the hierarchical video encoders as described herein can be jointly trained on both tasks in a multitask learning configuration.
- the hierarchical (e.g., and cross-attentional) encoders as described herein can be beneficial for these tasks, as the two tasks can require understanding semantics of a video at differing temporal resolutions, and the models described herein can model short-range and long-range video semantics.
- the hierarchical video encoders described herein can learn semantic understanding for at least three scales: frame-level, segment-level, and/or video-level.
- segment-level encoders as described herein can provide for capturing both coarse—and fine-grained semantic information in videos.
- one or more classifiers can be applied to identify regions (e.g., frames) corresponding to a beginning and/or an end of a relevant video segment.
- a lower-level classifier e.g., a per-frame classifier
- a higher-level classifier e.g., at the segment level or video level
- the probability of a frame being a start frame can be represented by:
- v , h ) ⁇ k P ⁇ ( t start
- u k , h ) W start T * [ ⁇ ⁇ ( x t ; u k , h ) , ⁇ ⁇ ( T ; u k , h )
- v , h ) U start T [ ⁇ ⁇ ( ⁇ k ; v , h ) , ⁇ ⁇ ( ⁇ k ; v , h ) ]
- W start is a (e.g., linear) frame level classifier
- U start is a (e.g., linear) video level classifier.
- the probability of a frame t being an end frame can be represented by:
- W end is a (e.g., linear) frame-level classifier and U end is a (e.g., linear) video-level classifier.
- Moment localization can thus essentially be treated as a frame classification problem. For instance, each frame can be classified as belonging to one of three labels: a beginning frame, which marks the beginning of a moment localization; an end frame, which marks the end of a moment localization; and an other frame that may or may not be included within a moment localization for a given moment, but is not bordering a moment.
- a loss during training of the hierarchical video encoder model can include a cross-entropy loss between a predicted classification of each frame and a true label of each frame. As one example, the training loss can be expressed as:
- the hierarchical video encoder can provide parameterization of the labeling model P(y t (i)
- the hierarchical video encoders as described herein can perform the two tasks of VR and MLSV at the temporal resolution required for the respective task.
- the user query is a sentence describing some fraction of the video content. Therefore, at the frame level representation, there are a number of frames that are irrelevant to the query, resulting in low signal-to-noise ratio for the VR task.
- the encoders learn a more coarse-grained matching between the video and the query which filters out the noise.
- the MLSV task can benefit from a fine-grained frame-level representation, providing for computing the start and end probabilities of each frame.
- conditional probabilities can be computed at the lower-level (frame).
- the hierarchical video encoding provides for learning the two tasks of VR and MLSV simultaneously in a joint training setup while still learning the respective objectives at the desired temporal resolution.
- the hierarchical video encoders as described herein can be beneficial for video search applications, such as retrieving specific segments of a longer video that are relevant to a given user query.
- the hierarchical video encoders can be useful for learning topical compositions of videos.
- Improved knowledge of topical compositions of videos can be useful for assisting in the placement of anchor points throughout videos that may be useful, for example, for annotation placement, navigability, etc.
- a user can be provided with navigation options based on the topical content.
- the improved knowledge of topical compositions or content of videos can additionally be useful for learning annotations for semantically meaningful video segments for indexing to aid quick retrieval.
- the hierarchical video encoders as described herein can be useful for ad placement within a video. For instance, the content of the video can be understood such that ad content can be strategically placed relative to content of a video.
- systems and methods according to example aspects of the present disclosure can provide for a number of technical effects and benefits, including improvements to computing technology.
- systems and methods according to example aspects of the present disclosure can have improved generality.
- the hierarchical video encoders as described herein can be robust to length of videos, such as by requiring no assumption about any length of the video segments to be localized, which can provide for improved generality to all lengths of videos.
- systems and methods according to example aspects of the present disclosure can have improved scalability.
- the hierarchical video encoders as described herein can be useful as an amount of content available online continues to grow.
- the hierarchical video encoders as described herein can be used to pre-compute representations of lengthy and computationally daunting video content such that the content can be organized and provided to users.
- the use of hierarchical video encoders as described herein can have reduced processing time compared to some existing approaches.
- FIG. 1 A depicts a block diagram of an example computing system 100 that performs hierarchical video encoding according to example embodiments of the present disclosure.
- the system 100 includes a user computing device 102 , a server computing system 130 , and a training computing system 150 that are communicatively coupled over a network 180 .
- the user computing device 102 can be any type of computing device, such as, for example, a personal computing device (e.g., laptop or desktop), a mobile computing device (e.g., smartphone or tablet), a gaming console or controller, a wearable computing device, an embedded computing device, or any other type of computing device.
- a personal computing device e.g., laptop or desktop
- a mobile computing device e.g., smartphone or tablet
- a gaming console or controller e.g., a gaming console or controller
- a wearable computing device e.g., an embedded computing device, or any other type of computing device.
- the user computing device 102 includes one or more processors 112 and a memory 114 .
- the one or more processors 112 can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, an FPGA, a controller, a microcontroller, etc.) and can be one processor or a plurality of processors that are operatively connected.
- the memory 114 can include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof.
- the memory 114 can store data 116 and instructions 118 which are executed by the processor 112 to cause the user computing device 102 to perform operations.
- the user computing device 102 can store or include one or more hierarchical video encoder models 120 .
- the hierarchical video encoder models 120 can be or can otherwise include various machine-learned models such as neural networks (e.g., deep neural networks) or other types of machine-learned models, including non-linear models and/or linear models.
- Neural networks can include feed-forward neural networks, recurrent neural networks (e.g., long short-term memory recurrent neural networks), convolutional neural networks or other forms of neural networks.
- Some example machine-learned models can leverage an attention mechanism such as self-attention.
- some example machine-learned models can include multi-headed self-attention models (e.g., transformer models).
- Example hierarchical video encoder models 120 are discussed with reference to FIGS. 2 through 5 .
- the one or more hierarchical video encoder models 120 can be received from the server computing system 130 over network 180 , stored in the user computing device memory 114 , and then used or otherwise implemented by the one or more processors 112 .
- the user computing device 102 can implement multiple parallel instances of a single hierarchical video encoder model 120 (e.g., to perform parallel hierarchical video encoding across multiple instances of a hierarchical video encoding task).
- the hierarchical video encoder models 120 as described herein include a hierarchy of two (or more) encoder models, such as Transformers (e.g., cross-attentional transformers).
- a lower-level intrasegment encoder (also referred to as a frame-level encoder) encodes frame-level information of video data (e.g., video frames or representations thereof) into frame representations. Segment representations for video segments can be determined based on these frame representations, such as by providing a context token for a given video segment based on the frame representations of frames in that video segment.
- a higher-level intersegment encoder (also referred to as a segment-level encoder) encodes the segment representations into contextualized segment representations, which can further be used to produce a video representation.
- the hierarchical video encoder model 120 can include a frame-level encoder model configured to receive a plurality of frames of a video as input and provide, in response to receipt of the plurality of frames as input, a plurality of frame representations of the plurality of frames as output.
- the hierarchical video encoder model 120 can include a segment-level encoder model configured to receive a plurality of segment representations as input and provide, in response to receipt of the plurality of segment representations as input, a plurality of contextualized segment representations as output.
- one or more hierarchical video encoder models 140 can be included in or otherwise stored and implemented by the server computing system 130 that communicates with the user computing device 102 according to a client-server relationship.
- the hierarchical video encoder models 140 can be implemented by the server computing system 140 as a portion of a web service (e.g., a hierarchical video encoding service, image search service, etc.).
- a web service e.g., a hierarchical video encoding service, image search service, etc.
- one or more models 120 can be stored and implemented at the user computing device 102 and/or one or more models 140 can be stored and implemented at the server computing system 130 .
- the user computing device 102 can also include one or more user input components 122 that receives user input.
- the user input component 122 can be a touch-sensitive component (e.g., a touch-sensitive display screen or a touch pad) that is sensitive to the touch of a user input object (e.g., a finger or a stylus).
- the touch-sensitive component can serve to implement a virtual keyboard.
- Other example user input components include a microphone, a traditional keyboard, or other means by which a user can provide user input.
- the server computing system 130 includes one or more processors 132 and a memory 134 .
- the one or more processors 132 can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, an FPGA, a controller, a microcontroller, etc.) and can be one processor or a plurality of processors that are operatively connected.
- the memory 134 can include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof.
- the memory 134 can store data 136 and instructions 138 which are executed by the processor 132 to cause the server computing system 130 to perform operations.
- the server computing system 130 includes or is otherwise implemented by one or more server computing devices. In instances in which the server computing system 130 includes plural server computing devices, such server computing devices can operate according to sequential computing architectures, parallel computing architectures, or some combination thereof.
- the server computing system 130 can store or otherwise include one or more hierarchical video encoder models 140 .
- the models 140 can be or can otherwise include various machine-learned models.
- Example machine-learned models include neural networks or other multi-layer non-linear models.
- Example neural networks include feed forward neural networks, deep neural networks, recurrent neural networks, and convolutional neural networks.
- Some example machine-learned models can leverage an attention mechanism such as self-attention.
- some example machine-learned models can include multi-headed self-attention models (e.g., transformer models).
- Example models 140 are discussed with reference to FIGS. 2 through 5 .
- the user computing device 102 and/or the server computing system 130 can train the models 120 and/or 140 via interaction with the training computing system 150 that is communicatively coupled over the network 180 .
- the training computing system 150 can be separate from the server computing system 130 or can be a portion of the server computing system 130 .
- the training computing system 150 includes one or more processors 152 and a memory 154 .
- the one or more processors 152 can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, an FPGA, a controller, a microcontroller, etc.) and can be one processor or a plurality of processors that are operatively connected.
- the memory 154 can include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof.
- the memory 154 can store data 156 and instructions 158 which are executed by the processor 152 to cause the training computing system 150 to perform operations.
- the training computing system 150 includes or is otherwise implemented by one or more server computing devices.
- the training computing system 150 can include a model trainer 160 that trains the machine-learned models 120 and/or 140 stored at the user computing device 102 and/or the server computing system 130 using various training or learning techniques, such as, for example, backwards propagation of errors.
- a loss function can be backpropagated through the model(s) to update one or more parameters of the model(s) (e.g., based on a gradient of the loss function).
- Various loss functions can be used such as mean squared error, likelihood loss, cross entropy loss, hinge loss, and/or various other loss functions.
- Gradient descent techniques can be used to iteratively update the parameters over a number of training iterations.
- performing backwards propagation of errors can include performing truncated backpropagation through time.
- the model trainer 160 can perform a number of generalization techniques (e.g., weight decays, dropouts, etc.) to improve the generalization capability of the models being trained.
- the model trainer 160 can train the hierarchical video encoder models 120 and/or 140 based on a set of training data 162 .
- the training data 162 can include, for example, a corpus or dataset of videos, including video data.
- the training data 162 may be labeled with ground truth information such as, for example, ground truth moment localizations (e.g., an actual beginning and/or end of a moment), such as ground truth frame labels.
- the training data can include known positive and/or negative pairs of videos and queries.
- the training examples can be provided by the user computing device 102 .
- the model 120 provided to the user computing device 102 can be trained by the training computing system 150 on user-specific data received from the user computing device 102 . In some instances, this process can be referred to as personalizing the model.
- the model trainer 160 includes computer logic utilized to provide desired functionality.
- the model trainer 160 can be implemented in hardware, firmware, and/or software controlling a general purpose processor.
- the model trainer 160 includes program files stored on a storage device, loaded into a memory and executed by one or more processors.
- the model trainer 160 includes one or more sets of computer-executable instructions that are stored in a tangible computer-readable storage medium such as RAM, hard disk, or optical or magnetic media.
- the network 180 can be any type of communications network, such as a local area network (e.g., intranet), wide area network (e.g., Internet), or some combination thereof and can include any number of wired or wireless links.
- communication over the network 180 can be carried via any type of wired and/or wireless connection, using a wide variety of communication protocols (e.g., TCP/IP, HTTP, SMTP, FTP), encodings or formats (e.g., HTML, XML), and/or protection schemes (e.g., VPN, secure HTTP, SSL).
- the machine-learned models described in this specification may be used in a variety of tasks, applications, and/or use cases, including, for example, VR tasks and/or MLVC tasks.
- the input to the machine-learned model(s) of the present disclosure can be text or natural language data.
- the machine-learned model(s) can process the text or natural language data to generate an output.
- the machine-learned model(s) can process the natural language data to generate a language encoding output.
- the machine-learned model(s) can process the text or natural language data to generate a latent text embedding output.
- the machine-learned model(s) can process the text or natural language data to generate a translation output.
- the machine-learned model(s) can process the text or natural language data to generate a classification output.
- the machine-learned model(s) can process the text or natural language data to generate a textual segmentation output.
- the machine-learned model(s) can process the text or natural language data to generate a semantic intent output.
- the machine-learned model(s) can process the text or natural language data to generate an upscaled text or natural language output (e.g., text or natural language data that is higher quality than the input text or natural language, etc.).
- the machine-learned model(s) can process the text or natural language data to generate a prediction output.
- the input to the machine-learned model(s) of the present disclosure can be speech data.
- the machine-learned model(s) can process the speech data to generate an output.
- the machine-learned model(s) can process the speech data to generate a speech recognition output.
- the machine-learned model(s) can process the speech data to generate a speech translation output.
- the machine-learned model(s) can process the speech data to generate a latent embedding output.
- the machine-learned model(s) can process the speech data to generate an encoded speech output (e.g., an encoded and/or compressed representation of the speech data, etc.).
- an encoded speech output e.g., an encoded and/or compressed representation of the speech data, etc.
- the machine-learned model(s) can process the speech data to generate an upscaled speech output (e.g., speech data that is higher quality than the input speech data, etc.).
- the machine-learned model(s) can process the speech data to generate a textual representation output (e.g., a textual representation of the input speech data, etc.).
- the machine-learned model(s) can process the speech data to generate a prediction output.
- the input to the machine-learned model(s) of the present disclosure can be latent encoding data (e.g., a latent space representation of an input, etc.).
- the machine-learned model(s) can process the latent encoding data to generate an output.
- the machine-learned model(s) can process the latent encoding data to generate a recognition output.
- the machine-learned model(s) can process the latent encoding data to generate a reconstruction output.
- the machine-learned model(s) can process the latent encoding data to generate a search output.
- the machine-learned model(s) can process the latent encoding data to generate a reclustering output.
- the machine-learned model(s) can process the latent encoding data to generate a prediction output.
- the machine-learned model(s) can be configured to perform a task that includes encoding input data for reliable and/or efficient transmission or storage (and/or corresponding decoding).
- the task may be an audio compression task.
- the input may include audio data and the output may comprise compressed audio data.
- the input includes visual data (e.g. one or more images or videos), the output comprises compressed visual data, and the task is a visual data compression task.
- the task may comprise generating an embedding for input data (e.g. input audio or visual data).
- the input includes audio data representing a spoken utterance and the task is a speech recognition task.
- the output may comprise a text output which is mapped to the spoken utterance.
- the task comprises encrypting or decrypting input data.
- FIG. 1 A illustrates one example computing system that can be used to implement the present disclosure.
- the user computing device 102 can include the model trainer 160 and the training dataset 162 .
- the models 120 can be both trained and used locally at the user computing device 102 .
- the user computing device 102 can implement the model trainer 160 to personalize the models 120 based on user-specific data.
- FIG. 1 B depicts a block diagram of an example computing device 10 that performs according to example embodiments of the present disclosure.
- the computing device 10 can be a user computing device or a server computing device.
- the computing device 10 includes a number of applications (e.g., applications 1 through N). Each application contains its own machine learning library and machine-learned model(s). For example, each application can include a machine-learned model.
- Example applications include a text messaging application, an email application, a dictation application, a virtual keyboard application, a browser application, etc.
- each application can communicate with a number of other components of the computing device, such as, for example, one or more sensors, a context manager, a device state component, and/or additional components.
- each application can communicate with each device component using an API (e.g., a public API).
- the API used by each application is specific to that application.
- FIG. 1 C depicts a block diagram of an example computing device 50 that performs according to example embodiments of the present disclosure.
- the computing device 50 can be a user computing device or a server computing device.
- the computing device 50 includes a number of applications (e.g., applications 1 through N). Each application is in communication with a central intelligence layer.
- Example applications include a text messaging application, an email application, a dictation application, a virtual keyboard application, a browser application, etc.
- each application can communicate with the central intelligence layer (and model(s) stored therein) using an API (e.g., a common API across all applications).
- the central intelligence layer includes a number of machine-learned models. For example, as illustrated in FIG. 1 C , a respective machine-learned model can be provided for each application and managed by the central intelligence layer. In other implementations, two or more applications can share a single machine-learned model. For example, in some implementations, the central intelligence layer can provide a single model for all of the applications. In some implementations, the central intelligence layer is included within or otherwise implemented by an operating system of the computing device 50 .
- the central intelligence layer can communicate with a central device data layer.
- the central device data layer can be a centralized repository of data for the computing device 50 . As illustrated in FIG. 1 C , the central device data layer can communicate with a number of other components of the computing device, such as, for example, one or more sensors, a context manager, a device state component, and/or additional components. In some implementations, the central device data layer can communicate with each device component using an API (e.g., a private API).
- an API e.g., a private API
- FIG. 2 depicts a block diagram of an example hierarchical video encoder model 200 according to example embodiments of the present disclosure.
- the hierarchical video encoder model 200 is trained to receive a set of input data 202 descriptive of a video and/or a user query and, as a result of receipt of the input data 202 , provide output data 204 that includes, for example, a plurality of frame representations, a plurality of segment representations, and/or a video representation based on the video.
- a hierarchical structure of encoders can provide semantic understanding in the form of representations at various levels of a video, as described herein.
- FIG. 3 depicts a block diagram of an example hierarchical video encoder model 200 according to example embodiments of the present disclosure.
- the hierarchical video encoder model 200 can further include a frame-level encoder model 210 and a segment-level encoder model 212 .
- the input data 202 can be provided to frame-level encoder model 210 to produce frame representations 214 .
- Segment representations derived from the frame representations 214 can be provided to segment-level encoder model 212 to produce the output data 204 , such as contextualized segment representations and/or a video representation of the output data 204 .
- FIG. 4 depicts a block diagram of an example hierarchical video encoder model 200 according to example embodiments of the present disclosure.
- the hierarchical video encoder model can further be configured to receive associated text data 224 in addition to input data 202 (e.g., video data).
- the associated text data 224 can be encoded by text encoder 222 .
- the text encoder 222 can produce encoded associated text 226 , which can be a representation of associated text data 224 that can be used (e.g., in a cross-attention mechanism) by frame-level encoder model 212 and/or segment-level encoder model 214 .
- predictions from frame-level encoder model 212 and/or segment-level encoder model 214 can be based at least in part on context provided by associated text data 224 (e.g., video captioning, user query, etc.).
- FIG. 5 depicts a block diagram of an example hierarchical video encoder model 500 according to example embodiments of the present disclosure.
- the hierarchical video encoder model can be configured to produce representations of video data 502 .
- Video data 502 can be represented as a sequence of (e.g., fixed length) video segments or “clips” which, intuitively, serve as memory units representing the semantics of one or more frames in the video segment.
- Each video segment can be a nonoverlapping set of one or more frames of a larger video.
- a “frame” with respect to a video refers to audio, visual, and/or captioning/transcript data associated with a (e.g., smallest) temporal slice of the video.
- a video of video data 502 may be composed of at least a (e.g., temporally linear) sequence of frames, where each frame includes an image, a portion of a stream of audio data to be played along with the sequence of images, and/or supplementary text (e.g., captioning) to be displayed along with the sequence of images.
- a (e.g., temporally linear) sequence of frames where each frame includes an image, a portion of a stream of audio data to be played along with the sequence of images, and/or supplementary text (e.g., captioning) to be displayed along with the sequence of images.
- the hierarchical video encoder model 500 includes a hierarchy of two (or more) encoder models.
- the frame-level encoder 510 encodes frame-level information of video data 502 (e.g., video frames or representations thereof) into frame representations 520 .
- Segment representations 525 for video segments can be determined based on these frame representations 520 , such as by providing a context token for a given video segment based on the frame representations 520 of frames in that video segment.
- Segment-level encoder 530 then encodes the segment 525 representations into contextualized segment representations 540 , which can further be used to produce a video representation 550 .
- the frame-level encoder model 510 and the segment-level encoder model 530 can be a multimodal encoder configured to produce a plurality of representations (e.g., 520 , 540 ) based at least in part on associated text 504 (e.g., a user query).
- the encoder(s) 510 , 530 ) can be cross-modal encoders that additionally fuse the video data 502 and/or representations thereof with associated text data 504 , such as, for example, captioning data for the video and/or query data descriptive of a user query representing a user's search for videos and/or, more particularly, content depicted within the videos.
- the input modality pairs can have cross attention, such as visual-caption/transcript, visual-query, and/or transcript-query attention.
- the associated text can be encoded by a text encoder model 516 , 536 , such as a text transformer, prior to being fused with the video data 502 .
- the video data 502 is first encoded by video data encoder 512 .
- associated text data 504 is encoded by text encoder model 516 .
- Cross-modal transformer 514 then produces frame representations 520 based on cross-attention between the encoded video data 502 from video data encoder 512 and encoded associated text data 504 from text encoder model 516 .
- the segment representation(s) 525 are first encoded by video data encoder 532 .
- Associated text data 504 can again be encoded, by a same model as for frame-level encoder 510 (e.g., text encoder 516 ) and/or a second text encoder model 536 .
- Cross-modal transformer 534 then produces contextualized segment representations 540 based on cross-attention between the encoded segment representations 525 from video data encoder 532 and encoded associated text data from text encoder model 536 .
- FIG. 6 depicts a flow chart diagram of an example method 600 for generating video representations utilizing a hierarchical video encoder according to example embodiments of the present disclosure.
- FIG. 6 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particularly illustrated order or arrangement. The various steps of the method 600 can be omitted, rearranged, combined, and/or adapted in various ways without deviating from the scope of the present disclosure.
- the method 600 can include, at 602 , obtaining (e.g., by a computing system including one or more computing devices) a video, wherein the video includes a plurality of frames.
- Each frame can include visual data (e.g., an image) and/or associated audio data (e.g., a slice of an audio stream).
- the video may be unsegmented, such that no temporal divisions exist in the video.
- the video may be, for example, accessed from a corpus of videos, such as a content sharing website, media provider, database, and/or other suitable corpus.
- the method 600 can include, at 604 , processing (e.g., by the computing system) each of the plurality of frames with a machine-learned frame-level encoder model to respectively generate a plurality of frame representations for the plurality of frames.
- the plurality of frame representations can be respective to the plurality of frames.
- each frame representation can be produced from a respective (e.g., unique) frame of the plurality of frames.
- the frame-level encoder model can be a multimodal encoder model configured to produce the plurality of frame representations based at least in part on associated text (e.g., a user query, captioning for the video, etc.).
- the method 600 can include processing (e.g., by the computing system) the associated text with the machine-learned frame-level encoder model to produce the plurality of frame representations, wherein the plurality of frame representations are based at least in part on the associated text.
- the associated text can be processed concurrently with the plurality of frames.
- the associated text can be encoded.
- the method 600 can include, at 606 , determining (e.g., by the computing system) a plurality of segment representations representative of a plurality of video segments including one or more of the plurality of frames.
- the plurality of video segments can each have about equal length.
- a video may be divided into video segments based at least in part on a fixed segment length.
- the plurality of video segments may be nonoverlapping. For instance, a given frame may be included within only one video segment of the plurality of video segments.
- the plurality of segment representations can be based at least in part on the plurality of frame representations.
- the plurality of segment representations can include a context token.
- the plurality of frame representations can be, can include, or can otherwise be used to generate a contextualized frame representation, such as a context (e.g., CLS) token specific to each frame.
- the context tokens for each frame can be aggregated or otherwise combined to produce a segment representation for a video segment including the frames for which the context tokens are combined.
- the method 600 can include, at 608 , processing (e.g., by the computing system) the plurality of segment representations with a machine-learned segment-level encoder model to generate a plurality of contextualized segment representations.
- the contextualized segment representation can include a context (e.g., CLS) token specific to the respective video segment.
- processing the plurality of segment representations can include processing (e.g., by the computing system) the associated text with the machine-learned segment-level encoder model to produce the plurality of contextualized segment representations.
- the plurality of contextualized segment representations can thus be based at least in part on the associated text.
- the method 600 can include, at 610 , determining (e.g., by the computing system), based at least in part on the plurality of contextualized segment representations, a video representation. For instance, in some implementations, context tokens corresponding to each segment in a video can be aggregated or otherwise combined to produce the video representation. Additionally, the method 600 can include, at 612 , providing (e.g., by the computing system) the video representation as an output (e.g., of the hierarchical video encoder model).
- FIG. 7 depicts a flow chart diagram of an example method 700 for moment localization in a video corpus including a plurality of videos according to example embodiments of the present disclosure.
- FIG. 7 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particularly illustrated order or arrangement. The various steps of the method 700 can be omitted, rearranged, combined, and/or adapted in various ways without deviating from the scope of the present disclosure.
- the method 700 can include, at 702 , obtaining (e.g., by a computing system including one or more computing devices) a user query, such as from a user.
- the user query can include text (e.g., text data).
- the user query can be obtained in any suitable manner according to example aspects of the present disclosure.
- the user query can be obtained from a user by providing a user with a text field in which to enter the user query, such as at a search engine service.
- the user query can be obtained from an external computing system or other computing device.
- the user query may be or include only text data, may be or may include speech data (e.g., that is converted into text data) and/or may be or may include any other suitable data.
- the user query can be or can include a short text string (e.g., on the order of fewer than about 20 words) descriptive of a moment within a video.
- the method 700 can include, at 704 , identifying (e.g., by the computing system) one or more highest likelihood videos of the plurality of videos.
- each highest likelihood video of the one or more highest likelihood videos can be identified based at least in part on a video-query compatibility score between the user query and a video representation of the highest likelihood video that is output by a machine-learned hierarchical video encoder model, as described herein.
- the video-query compatibility score can effectively rank the corpus of videos and the K highest scoring video(s) in the corpus, as defined by the video-query compatibility score, can be selected as the highest likelihood video(s).
- the video representation of a highest likelihood video can be based at least in part on a highest scoring segment representation of a plurality of segment representations of the highest likelihood video.
- the hierarchical video encoder may output a plurality of segment representations associated with a plurality of video segments of the highest scoring videos, each of which has an associated compatibility score with the user query. The highest score of these compatibility scores can be used as representative of the entire video.
- the one or more highest likelihood videos can be selected based at least in part on a negative log-likelihood of the one or more highest likelihood videos containing the moment described by the user query. For instance, the videos can be selected to minimize the negative log-likelihood.
- a modeling objective for the video retrieval task can select a matching video most likely to have a moment to be localized by employing a contrastive loss that contrasts a compatibility score of positive (e.g., matching) pairs of video representation and query against negative (e.g., not matching) pairs of video representation and query.
- the negative pairs can be randomly sampled.
- One example compatibility score is computed as:
- W VR is a linear regressor
- the representation of a highest likelihood video can include a highest scoring segment representation of a plurality of segment representations of the highest likelihood video. For instance, of a plurality of segments of the video, the score of the highest-scoring segment can be selected as representative of the entire video.
- the one or more highest likelihood videos can be selected based at least in part on a negative log-likelihood of the one or more highest likelihood videos containing the moment described by the user query.
- the VR loss can be computed as:
- v (i) is a ground-truth video for a text query h (i) .
- the hierarchical video encoder model can parameterize the conditional probability P (v
- the method 700 can include, at 706 , determining a moment localization within a matching video of the one or more highest likelihood videos. For instance, a moment localization can be determined for a moment, where the moment localization specifies a beginning and/or an end of the moment within the matching video. As one example, the moment localization can be or can include timestamps, frame indices, etc. This task is referred to as Moment Localization in Single Video, or MLSV.
- the hierarchical video encoders as described herein can be jointly trained on both tasks in a multitask learning configuration.
- the hierarchical (e.g., and cross-attentional) encoders as described herein can be beneficial for these tasks, as the two tasks can require understanding semantics of a video at differing temporal resolutions, and the models described herein can model short-range and long-range video semantics.
- the hierarchical video encoders described herein can learn semantic understanding for at least three scales: frame-level, segment-level, and/or video-level.
- segment-level encoders as described herein can provide for capturing both coarse- and fine-grained semantic information in videos.
- one or more classifiers can be applied to identify regions (e.g., frames) corresponding to a beginning and/or an end of a relevant video segment.
- a lower-level classifier e.g., a per-frame classifier
- a higher-level classifier e.g., at the segment level or video level
- the probability of a frame being a start frame can be represented by:
- v , h ) ⁇ k P ⁇ ( t start
- u k , h ) W start T * [ ⁇ ⁇ ( x t ; u k , h ) , ⁇ ⁇ ( T ; u k , h )
- v , h ) U start T [ ⁇ ⁇ ( ⁇ k ; v , h ) , ⁇ ⁇ ( ⁇ k ; v , h ) ]
- W start is a (e.g., linear) frame level classifier
- U start is a (e.g., linear) video level classifier.
- the probability of a frame t being an end frame can be represented by:
- W end is a (e.g., linear) frame-level classifier and U end is a (e.g., linear) video-level classifier.
- Moment localization can thus essentially be treated as a frame classification problem. For instance, each frame can be classified as belonging to one of three labels: a beginning frame, which marks the beginning of a moment localization; an end frame, which marks the end of a moment localization; and an other frame that may or may not be included within a moment localization for a given moment, but is not bordering a moment.
- a loss during training of the hierarchical video encoder model can include a cross-entropy loss between a predicted classification of each frame and a true label of each frame. As one example, the training loss can be expressed as:
- the hierarchical video encoder can provide parameterization of the labeling model P(y t (i)
- the method 700 can optionally include, at 708 , providing (e.g., by the computing system) the moment localization for display to a user.
- the matching video can be returned to a user providing the user query, such as in the form of search results, etc.
- the user may additionally be provided with the moment localization, such as the beginning and/or the end.
- a graphical user interface element depicting a progress bar indicative of a user's position within a video e.g., by a marker between a beginning and an end of the video
- the user may be provided with the moment localization in the form of video playback being initiated at the moment beginning (e.g., as opposed to the video beginning) Any other suitable form of providing the moment localization for display to a user can be employed in accordance with example aspects of the present disclosure.
- the technology discussed herein makes reference to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from such systems.
- the inherent flexibility of computer-based systems allows for a great variety of possible configurations, combinations, and divisions of tasks and functionality between and among components.
- processes discussed herein can be implemented using a single device or component or multiple devices or components working in combination.
- Databases and applications can be implemented on a single system or distributed across multiple systems. Distributed components can operate sequentially or in parallel.
Abstract
A computer-implemented method for generating video representations utilizing a hierarchical video encoder includes obtaining a video, wherein the video includes a plurality of frames, processing each of the plurality of frames with a machine-learned frame-level encoder model to respectively generate a plurality of frame representations for the plurality of frames, the plurality of frame representations respective to the plurality of frames determining a plurality of segment representations representative of a plurality of video segments including one or more of the plurality of frames, the plurality of segment representations based at least in part on the plurality of frame representations, processing the plurality of segment representations with a machine-learned segment-level encoder model to generate a plurality of contextualized segment representations, determining a video representation based at least in part on the plurality of contextualized segment representations, and providing the video representation as an output.
Description
- The present disclosure relates generally to hierarchical video encoders. More particularly, the present disclosure relates to hierarchical video encoding for moment localization in a corpus of video data.
- Video content can be stored and/or shared online with a variety of users. As an amount of video content available online increases, it can be increasingly desirable to provide systems enabling a user to selectively search for desired video content among online sources. Systems such as search engines may be provided that allow a user to input a search query and retrieve results responsive to the search query from one or more distinct sources. Video content may be included among the retrieved results.
- Aspects and advantages of embodiments of the present disclosure will be set forth in part in the following description, or can be learned from the description, or can be learned through practice of the embodiments.
- One example aspect of the present disclosure is directed to a computer-implemented method for generating video representations utilizing a hierarchical video encoder. The computer-implemented method can include obtaining, by a computing system including one or more computing devices, a video, wherein the video includes a plurality of frames. The computer-implemented method can include processing, by the computing system, each of the plurality of frames with a machine-learned frame-level encoder model to respectively generate a plurality of frame representations for the plurality of frames, the plurality of frame representations respective to the plurality of frames. The computer-implemented method can include determining, by the computing system, a plurality of segment representations representative of a plurality of video segments including one or more of the plurality of frames, the plurality of segment representations based at least in part on the plurality of frame representations. The computer-implemented method can include processing, by the computing system, the plurality of segment representations with a machine-learned segment-level encoder model to generate a plurality of contextualized segment representations. The computer-implemented method can include determining, by the computing system, a video representation based at least in part on the plurality of contextualized segment representations. The computer-implemented method can include providing, by the computing system, the video representation as an output.
- Another example aspect of the present disclosure is directed to a computer-implemented method for moment localization in a video corpus including a plurality of videos. The computer-implemented method can include obtaining, by a computing system including one or more computing devices, a user query, the user query including text. The computer-implemented method can include identifying, by the computing system, one or more highest likelihood videos of the plurality of videos, each highest likelihood video of the one or more highest likelihood videos identified based at least in part on a video-query compatibility score between the user query and a video representation of the highest likelihood video that is output by a machine-learned hierarchical video encoder model. The computer-implemented method can include determining, by the computing system, a moment localization within a matching video of the one or more highest likelihood videos, the moment localization including a moment beginning and a moment end. The moment beginning can include a beginning frame of the matching video, the beginning frame having a frame representation that is classified as representing a beginning of a moment described by the user query. The moment end can include an end frame of the matching video, the end frame having a frame representation that is classified as representing an end of the moment described by the user query.
- Other aspects of the present disclosure are directed to various systems, apparatuses, non-transitory computer-readable media, user interfaces, and electronic devices.
- These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate example embodiments of the present disclosure and, together with the description, serve to explain the related principles.
- Detailed discussion of embodiments directed to one of ordinary skill in the art is set forth in the specification, which makes reference to the appended figures, in which:
-
FIG. 1A depicts a block diagram of an example computing system that performs hierarchical video encoding according to example embodiments of the present disclosure. -
FIG. 1B depicts a block diagram of an example computing device that performs hierarchical video encoding according to example embodiments of the present disclosure. -
FIG. 1C depicts a block diagram of an example computing device that performs hierarchical video encoding according to example embodiments of the present disclosure. -
FIG. 2 depicts a block diagram of an example hierarchical video encoder model according to example embodiments of the present disclosure. -
FIG. 3 depicts a block diagram of an example hierarchical video encoder model according to example embodiments of the present disclosure. -
FIG. 4 depicts a block diagram of an example hierarchical video encoder model according to example embodiments of the present disclosure. -
FIG. 5 depicts a block diagram of an example hierarchical video encoder model according to example embodiments of the present disclosure. -
FIG. 6 depicts a flow chart diagram of an example method for generating video representations utilizing a hierarchical video encoder according to example embodiments of the present disclosure. -
FIG. 7 depicts a flow chart diagram of an example method for moment localization in a video corpus including a plurality of videos according to example embodiments of the present disclosure. - Reference numerals that are repeated across plural figures are intended to identify the same features in various implementations.
- Generally, the present disclosure is directed to hierarchical video encoders. Additionally and/or alternatively, the present disclosure is directed to systems and methods for moment localization in a video corpus using representations from hierarchical video encoders. Conceptually, a video can be represented as a sequence of (e.g., fixed length) video segments or “clips” which, intuitively, serve as memory units representing the semantics of one or more frames in the video segment. Each video segment can be a nonoverlapping set of one or more frames of a larger video. As used herein, a “frame” with respect to a video refers to audio, visual, and/or captioning/transcript data associated with a (e.g., smallest) temporal slice of the video. For instance, a video may be composed of at least a (e.g., temporally linear) sequence of frames, where each frame includes an image, a portion of a stream of audio data to be played along with the sequence of images, and/or supplementary text (e.g., captioning) to be displayed along with the sequence of images.
- The hierarchical video encoders as described herein include a hierarchy of two (or more) encoder models, such as Transformers (e.g., cross-attentional transformers). A lower-level intrasegment encoder (also referred to as a frame-level encoder) encodes frame-level information of video data (e.g., video frames or representations thereof) into frame representations. Segment representations for video segments can be determined based on these frame representations, such as by providing a context token for a given video segment based on the frame representations of frames in that video segment. A higher-level intersegment encoder (also referred to as a segment-level encoder) encodes the segment representations into contextualized segment representations, which can further be used to produce a video representation. For instance, in some implementations, the hierarchical video encoder model can include a frame-level encoder model configured to receive a plurality of frames of a video as input and provide, in response to receipt of the plurality of frames as input, a plurality of frame representations of the plurality of frames as output. Additionally and/or alternatively, the hierarchical video encoder model can include a segment-level encoder model configured to receive a plurality of segment representations as input and provide, in response to receipt of the plurality of segment representations as input, a plurality of contextualized segment representations as output.
- In some implementations, at least one of the frame-level encoder model or the segment-level encoder model can be a multimodal encoder configured to produce a plurality of representations based at least in part on associated text. For instance, in addition to encoding the video data and/or representations thereof, the encoder(s) (e.g., the lower-level encoder and/or the higher level encoder) can be cross-modal encoders that additionally fuse the video data and/or representations thereof with associated text data, such as, for example, captioning data for the video and/or query data descriptive of a user query representing a user's search for videos and/or, more particularly, content depicted within the videos. For instance, in the encoder(s), the input modality pairs can have cross attention, such as visual-caption/transcript, visual-query, and/or transcript-query attention. In some implementations, the associated text can be encoded (e.g., by a text encoder model, such as a text transformer).
- The following notational conventions will be used herein for the purposes of illustrating example aspects of the present disclosure. It should be understood that the following conventions, although illustrative, are not intended to limit or exclusively construe any example aspects of the present disclosure, and are provided only for the illustration of mathematical and/or other concepts.
- Let a video be denoted by v, and the total number of frames in the video v be noted by N. Additionally, let the (e.g., fixed) length of each video segment be denoted by M. M can be selected based on the task and/or dataset used. Let a query be denoted by h. A lower-level cross-attentional encoder Φ can receive as input a frame sequence of a video segment uk and the query h and output, in response, contextualized frame-level features xt by {Φ(xt; uk, h|t=1, 2, . . . M)} for each video segment uk. A segment representation φk of the frames of each video segment can be determined for each video segment based on the frame-level features in the segment. As one example, the segment representation can be represented by φk=Φ(V CLSk; uk, h) where V CLSk is a context token (e.g., a visual CLS frame) associated with a video segment uk. These segment representations for each video segment can be input (e.g., as a sequence and/or in addition to the query h) to a higher-level cross-attention encoder Ψ. The higher-level encoder Ψ can output, in response, contextualized segment level features {Ψ(φk; v, h|k=1, 2, . . . N/M)}. In this way, the hierarchical video encoder intuitively learns the segment representations using local (intra-segment) self- and/or cross-attention among the frames belonging to the same video segment by the lower-level encoder, while the high-level encoder learns the video representation using global (inter-segment) self- and cross-attention among the video segments of the video.
- In some implementations, the machine-learned frame-level encoder model and the machine-learned segment-level encoder model can include one or more shared parameters. For instance, in some implementations, the models may be separately utilized but have some or all common parameters between the models such that the models are similar or identical. In some implementations, each model can have entirely unique parameters.
- For instance, the hierarchical video encoder models described herein can be employed in a computer-implemented method for generating video representations. The method can include obtaining (e.g., by a computing system including one or more computing devices) a video, wherein the video includes a plurality of frames. Each frame can include visual data (e.g., an image) and/or associated audio data (e.g., a slice of an audio stream). The video may be unsegmented, such that no temporal divisions exist in the video. The video may be, for example, accessed from a corpus of videos, such as a content sharing website, media provider, database, and/or other suitable corpus.
- Additionally, the method can include processing (e.g., by the computing system) each of the plurality of frames with a machine-learned frame-level encoder model to respectively generate a plurality of frame representations for the plurality of frames. The plurality of frame representations can be respective to the plurality of frames. For instance, each frame representation can be produced from a respective (e.g., unique) frame of the plurality of frames.
- In some implementations, the frame-level encoder model can be a multimodal encoder model configured to produce the plurality of frame representations based at least in part on associated text (e.g., a user query, captioning for the video, etc.). For instance, the method can include processing (e.g., by the computing system) the associated text with the machine-learned frame-level encoder model to produce the plurality of frame representations, wherein the plurality of frame representations are based at least in part on the associated text. The associated text can be processed concurrently with the plurality of frames. In some implementations, the associated text can be encoded.
- Additionally, the method can include determining (e.g., by the computing system) a plurality of segment representations representative of a plurality of video segments including one or more of the plurality of frames. In some implementations, the plurality of video segments can each have about equal length. For instance, in some implementations, a video may be divided into video segments based at least in part on a fixed segment length. In some implementations, the plurality of video segments may be nonoverlapping. For instance, a given frame may be included within only one video segment of the plurality of video segments.
- The plurality of segment representations can be based at least in part on the plurality of frame representations. In some implementations, the plurality of segment representations can include a context token. As one example, the plurality of frame representations can be, can include, or can otherwise be used to generate a contextualized frame representation, such as a context (e.g., CLS) token specific to each frame. The context tokens for each frame can be aggregated or otherwise combined to produce a segment representation for a video segment including the frames for which the context tokens are combined.
- Additionally, the method can include processing (e.g., by the computing system) the plurality of segment representations with a machine-learned segment-level encoder model to generate a plurality of contextualized segment representations. The contextualized segment representation can include a context (e.g., CLS) token specific to the respective video segment. In some cases, processing the plurality of segment representations can include processing (e.g., by the computing system) the associated text with the machine-learned segment-level encoder model to produce the plurality of contextualized segment representations. The plurality of contextualized segment representations can thus be based at least in part on the associated text.
- Additionally, the method can include determining (e.g., by the computing system), based at least in part on the plurality of contextualized segment representations, a video representation. For instance, in some implementations, context tokens corresponding to each segment in a video can be aggregated or otherwise combined to produce the video representation. Additionally, the method can include providing (e.g., by the computing system) the video representation as an output (e.g., of the hierarchical video encoder model).
- Hierarchical video encoders as described herein can be useful in a variety of computing tasks. One example task relates to identifying and localizing a moment relevant to a user query (e.g., a text query) from a corpus of videos, which may be untrimmed and/or unsegmented. As one example, in some cases, a user query may be a single query sentence describing a relatively small portion within a larger video. For instance, a user searching in response to a user query may wish to see particular moments of a longer video in response to the user query, such as to see only segments of the video depicting content that is relevant to the query. As one example, a video titled “how to cook chicken parmesan” and depicting steps of making chicken parmesan may include a portion dedicated to a step of butterflying chicken. Thus, a user searching with a query such as “how to butterfly chicken” may desire to view the video titled “how to cook chicken parmesan” despite the apparent lack of relationship between video title and content. Desirably, the user may be presented with the portion of the video (e.g., the moment) related to butterflying chicken such that the user does not have to manually search for the related content, which may not be immediately apparent to the user. This problem can be complicated as the number of channels (e.g., audio, visual, captioning) continues to scale and/or as a number of videos continues to scale. For instance, some existing approaches can experience decreasing performance (e.g., accuracy) as a number of frames irrelevant to a user query in a video increases.
- This problem has been termed “moment localization in video corpus” or MLVC. As video content available online continues to grow, it has become increasingly desirable and increasingly difficult to thoroughly manage and categorize the ever-increasing corpus of video content. For instance, to effectively and efficiently search, browse, or otherwise navigate through a corpus of videos, an intelligent system must understand rich and complex semantic information included in the videos. These videos can have a significant variation in factors such as content type, length, appearance, quality, and other factors. In addition, the MLVC problem can generally require a more fine-grained understanding of video content than, for example, video retrieval. For instance, localizing a moment responsive to a user query can require semantic understanding of many possible segments of videos. Some existing approaches to handle this problem pre-segment a larger video into smaller chunks. However, this approach can have issues such as microsegments. Furthermore, existing approaches that depend on a squared average of number of frames can be infeasible for large corpuses of videos and/or having a large number of frames. Furthermore, some existing solutions may pack information of different granularity into a single vector embedding, making it hard to balance the differing demands between retrieving a long video and localizing a short segment.
- To solve the MLVC problem, systems and methods according to example aspects of the present disclosure can first rank videos in a corpus of videos by relevance to a given user query. For instance, a computing system including one or more computing devices can obtain (e.g., from a user) a user query. The user query can include text (e.g., text data). The user query can be obtained in any suitable manner according to example aspects of the present disclosure. As one example, the user query can be obtained from a user by providing a user with a text field in which to enter the user query, such as at a search engine service. As another example, the user query can be obtained from an external computing system or other computing device. The user query may be or include only text data, may be or may include speech data (e.g., that is converted into text data) and/or may be or may include any other suitable data. In some cases, the user query can be or can include a short text string (e.g., on the order of fewer than about 20 words) descriptive of a moment within a video.
- A number of highest ranking videos (e.g., the K highest ranking videos) can be selected such that moment localization is performed on the highest ranking videos to identify a moment relevant to the user query. For instance, a computing system can identify one or more highest likelihood videos of the plurality of videos. This task of identifying the highest ranking video(s) is referred to herein as Video Retrieval, or VR. Performing the VR task can primarily be useful in reducing computational requirements by restricting a number of videos that must be searched for moment localization.
- In some implementations, each highest likelihood video of the one or more highest likelihood videos can be identified based at least in part on a video-query compatibility score between the user query and a video representation of the highest likelihood video that is output by a machine-learned hierarchical video encoder model, as described herein. For instance, the video-query compatibility score can effectively rank the corpus of videos and the K highest scoring video(s) in the corpus, as defined by the video-query compatibility score, can be selected as the highest likelihood video(s). In some implementations, the video representation of a highest likelihood video can be based at least in part on a highest scoring segment representation of a plurality of segment representations of the highest likelihood video. For instance, the hierarchical video encoder may output a plurality of segment representations associated with a plurality of video segments of the highest scoring videos, each of which has an associated compatibility score with the user query. The highest score of these compatibility scores can be used as representative of the entire video. In some implementations, the one or more highest likelihood videos can be selected based at least in part on a negative log-likelihood of the one or more highest likelihood videos containing the moment described by the user query. For instance, the videos can be selected to minimize the negative log-likelihood.
- A modeling objective for the video retrieval task can select a matching video most likely to have a moment to be localized by employing a contrastive loss that contrasts a compatibility score of positive (e.g., matching) pairs of video representation and query against negative (e.g., not matching) pairs of video representation and query. The negative pairs can be randomly sampled. One example compatibility score is computed as:
-
f(v,h)=maxk(W VR T*Ψ(φk ;v,h)) - where WVR is a linear regressor.
- In some cases, the representation of a highest likelihood video can include a highest scoring segment representation of a plurality of segment representations of the highest likelihood video. For instance, of a plurality of segments of the video, the score of the highest-scoring segment can be selected as representative of the entire video. In some implementations, the one or more highest likelihood videos can be selected based at least in part on a negative log-likelihood of the one or more highest likelihood videos containing the moment described by the user query. As one example, the VR loss can be computed as:
-
- where v(i) is a ground-truth video for a text query h(i). Essentially, the hierarchical video encoder model can parameterize the conditional probability P(v|h) for the video retrieval task by providing a hierarchically-learned video representation for a video v.
- Once the highest ranking video(s) are selected, moment(s) within the videos related to the user query can be localized. For instance, a moment localization can be determined for a moment, where the moment localization specifies a beginning and/or an end of the moment. As one example, the moment localization can be or can include timestamps, frame indices, etc. This task is referred to as Moment Localization in Single Video, or MLSV. The hierarchical video encoders as described herein can be jointly trained on both tasks in a multitask learning configuration. The hierarchical (e.g., and cross-attentional) encoders as described herein can be beneficial for these tasks, as the two tasks can require understanding semantics of a video at differing temporal resolutions, and the models described herein can model short-range and long-range video semantics. For instance, the hierarchical video encoders described herein can learn semantic understanding for at least three scales: frame-level, segment-level, and/or video-level. For example, including segment-level encoders as described herein can provide for capturing both coarse—and fine-grained semantic information in videos.
- Additionally, one or more classifiers can be applied to identify regions (e.g., frames) corresponding to a beginning and/or an end of a relevant video segment. For instance, a lower-level classifier (e.g., a per-frame classifier) can be used to classify a probability of each frame of being a starting frame and/or an ending frame. A higher-level classifier (e.g., at the segment level or video level) can classify a probability of a starting frame and/or an ending frame being located within a segment and/or video. As one example, the probability of a frame being a start frame can be represented by:
-
- where Wstart is a (e.g., linear) frame level classifier and Ustart is a (e.g., linear) video level classifier. Similarly, the probability of a frame t being an end frame can be represented by:
-
- where Wend is a (e.g., linear) frame-level classifier and Uend is a (e.g., linear) video-level classifier.
- Moment localization can thus essentially be treated as a frame classification problem. For instance, each frame can be classified as belonging to one of three labels: a beginning frame, which marks the beginning of a moment localization; an end frame, which marks the end of a moment localization; and an other frame that may or may not be included within a moment localization for a given moment, but is not bordering a moment. Additionally, a loss during training of the hierarchical video encoder model can include a cross-entropy loss between a predicted classification of each frame and a true label of each frame. As one example, the training loss can be expressed as:
-
- where ft (i) is a true label for a frame xt and yt (i) is a predicted label of the frame xt. Essentially, the hierarchical video encoder can provide parameterization of the labeling model P(yt (i)|v(i),h(i)) for the moment localization task.
- The hierarchical video encoders as described herein can perform the two tasks of VR and MLSV at the temporal resolution required for the respective task. For instance, in some cases for the MLVC task, the user query is a sentence describing some fraction of the video content. Therefore, at the frame level representation, there are a number of frames that are irrelevant to the query, resulting in low signal-to-noise ratio for the VR task. By learning segment-level representations, the encoders learn a more coarse-grained matching between the video and the query which filters out the noise. Hence, for the VR task, it may be possible to use the learned representations only at the higher-level (e.g., video segment). The MLSV task can benefit from a fine-grained frame-level representation, providing for computing the start and end probabilities of each frame. Thus, for the MLSV task, conditional probabilities can be computed at the lower-level (frame). The hierarchical video encoding provides for learning the two tasks of VR and MLSV simultaneously in a joint training setup while still learning the respective objectives at the desired temporal resolution.
- The hierarchical video encoders as described herein can be beneficial for video search applications, such as retrieving specific segments of a longer video that are relevant to a given user query. In addition to and/or alternatively to video search applications, the hierarchical video encoders can be useful for learning topical compositions of videos. Improved knowledge of topical compositions of videos can be useful for assisting in the placement of anchor points throughout videos that may be useful, for example, for annotation placement, navigability, etc. As an example, a user can be provided with navigation options based on the topical content. The improved knowledge of topical compositions or content of videos can additionally be useful for learning annotations for semantically meaningful video segments for indexing to aid quick retrieval. As another example, the hierarchical video encoders as described herein can be useful for ad placement within a video. For instance, the content of the video can be understood such that ad content can be strategically placed relative to content of a video.
- Systems and methods according to example aspects of the present disclosure can provide for a number of technical effects and benefits, including improvements to computing technology. As one example, systems and methods according to example aspects of the present disclosure can have improved generality. For instance, in contrast to some existing approaches, the hierarchical video encoders as described herein can be robust to length of videos, such as by requiring no assumption about any length of the video segments to be localized, which can provide for improved generality to all lengths of videos. As another example, systems and methods according to example aspects of the present disclosure can have improved scalability. For instance, the hierarchical video encoders as described herein can be useful as an amount of content available online continues to grow. For example, the hierarchical video encoders as described herein can be used to pre-compute representations of lengthy and computationally daunting video content such that the content can be organized and provided to users. As another example, the use of hierarchical video encoders as described herein can have reduced processing time compared to some existing approaches.
- With reference now to the Figures, example embodiments of the present disclosure will be discussed in further detail.
-
FIG. 1A depicts a block diagram of anexample computing system 100 that performs hierarchical video encoding according to example embodiments of the present disclosure. Thesystem 100 includes auser computing device 102, aserver computing system 130, and atraining computing system 150 that are communicatively coupled over anetwork 180. - The
user computing device 102 can be any type of computing device, such as, for example, a personal computing device (e.g., laptop or desktop), a mobile computing device (e.g., smartphone or tablet), a gaming console or controller, a wearable computing device, an embedded computing device, or any other type of computing device. - The
user computing device 102 includes one ormore processors 112 and amemory 114. The one ormore processors 112 can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, an FPGA, a controller, a microcontroller, etc.) and can be one processor or a plurality of processors that are operatively connected. Thememory 114 can include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof. Thememory 114 can storedata 116 andinstructions 118 which are executed by theprocessor 112 to cause theuser computing device 102 to perform operations. - In some implementations, the
user computing device 102 can store or include one or more hierarchicalvideo encoder models 120. For example, the hierarchicalvideo encoder models 120 can be or can otherwise include various machine-learned models such as neural networks (e.g., deep neural networks) or other types of machine-learned models, including non-linear models and/or linear models. Neural networks can include feed-forward neural networks, recurrent neural networks (e.g., long short-term memory recurrent neural networks), convolutional neural networks or other forms of neural networks. Some example machine-learned models can leverage an attention mechanism such as self-attention. For example, some example machine-learned models can include multi-headed self-attention models (e.g., transformer models). Example hierarchicalvideo encoder models 120 are discussed with reference toFIGS. 2 through 5 . - In some implementations, the one or more hierarchical
video encoder models 120 can be received from theserver computing system 130 overnetwork 180, stored in the usercomputing device memory 114, and then used or otherwise implemented by the one ormore processors 112. In some implementations, theuser computing device 102 can implement multiple parallel instances of a single hierarchical video encoder model 120 (e.g., to perform parallel hierarchical video encoding across multiple instances of a hierarchical video encoding task). - More particularly, the hierarchical
video encoder models 120 as described herein include a hierarchy of two (or more) encoder models, such as Transformers (e.g., cross-attentional transformers). A lower-level intrasegment encoder (also referred to as a frame-level encoder) encodes frame-level information of video data (e.g., video frames or representations thereof) into frame representations. Segment representations for video segments can be determined based on these frame representations, such as by providing a context token for a given video segment based on the frame representations of frames in that video segment. A higher-level intersegment encoder (also referred to as a segment-level encoder) encodes the segment representations into contextualized segment representations, which can further be used to produce a video representation. For instance, in some implementations, the hierarchicalvideo encoder model 120 can include a frame-level encoder model configured to receive a plurality of frames of a video as input and provide, in response to receipt of the plurality of frames as input, a plurality of frame representations of the plurality of frames as output. Additionally and/or alternatively, the hierarchicalvideo encoder model 120 can include a segment-level encoder model configured to receive a plurality of segment representations as input and provide, in response to receipt of the plurality of segment representations as input, a plurality of contextualized segment representations as output. - Additionally or alternatively, one or more hierarchical
video encoder models 140 can be included in or otherwise stored and implemented by theserver computing system 130 that communicates with theuser computing device 102 according to a client-server relationship. For example, the hierarchicalvideo encoder models 140 can be implemented by theserver computing system 140 as a portion of a web service (e.g., a hierarchical video encoding service, image search service, etc.). Thus, one ormore models 120 can be stored and implemented at theuser computing device 102 and/or one ormore models 140 can be stored and implemented at theserver computing system 130. - The
user computing device 102 can also include one or moreuser input components 122 that receives user input. For example, theuser input component 122 can be a touch-sensitive component (e.g., a touch-sensitive display screen or a touch pad) that is sensitive to the touch of a user input object (e.g., a finger or a stylus). The touch-sensitive component can serve to implement a virtual keyboard. Other example user input components include a microphone, a traditional keyboard, or other means by which a user can provide user input. - The
server computing system 130 includes one ormore processors 132 and amemory 134. The one ormore processors 132 can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, an FPGA, a controller, a microcontroller, etc.) and can be one processor or a plurality of processors that are operatively connected. Thememory 134 can include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof. Thememory 134 can storedata 136 andinstructions 138 which are executed by theprocessor 132 to cause theserver computing system 130 to perform operations. - In some implementations, the
server computing system 130 includes or is otherwise implemented by one or more server computing devices. In instances in which theserver computing system 130 includes plural server computing devices, such server computing devices can operate according to sequential computing architectures, parallel computing architectures, or some combination thereof. - As described above, the
server computing system 130 can store or otherwise include one or more hierarchicalvideo encoder models 140. For example, themodels 140 can be or can otherwise include various machine-learned models. Example machine-learned models include neural networks or other multi-layer non-linear models. Example neural networks include feed forward neural networks, deep neural networks, recurrent neural networks, and convolutional neural networks. Some example machine-learned models can leverage an attention mechanism such as self-attention. For example, some example machine-learned models can include multi-headed self-attention models (e.g., transformer models).Example models 140 are discussed with reference toFIGS. 2 through 5 . - The
user computing device 102 and/or theserver computing system 130 can train themodels 120 and/or 140 via interaction with thetraining computing system 150 that is communicatively coupled over thenetwork 180. Thetraining computing system 150 can be separate from theserver computing system 130 or can be a portion of theserver computing system 130. - The
training computing system 150 includes one ormore processors 152 and amemory 154. The one ormore processors 152 can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, an FPGA, a controller, a microcontroller, etc.) and can be one processor or a plurality of processors that are operatively connected. Thememory 154 can include one or more non-transitory computer-readable storage media, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof. Thememory 154 can storedata 156 andinstructions 158 which are executed by theprocessor 152 to cause thetraining computing system 150 to perform operations. In some implementations, thetraining computing system 150 includes or is otherwise implemented by one or more server computing devices. - The
training computing system 150 can include amodel trainer 160 that trains the machine-learnedmodels 120 and/or 140 stored at theuser computing device 102 and/or theserver computing system 130 using various training or learning techniques, such as, for example, backwards propagation of errors. For example, a loss function can be backpropagated through the model(s) to update one or more parameters of the model(s) (e.g., based on a gradient of the loss function). Various loss functions can be used such as mean squared error, likelihood loss, cross entropy loss, hinge loss, and/or various other loss functions. Gradient descent techniques can be used to iteratively update the parameters over a number of training iterations. - In some implementations, performing backwards propagation of errors can include performing truncated backpropagation through time. The
model trainer 160 can perform a number of generalization techniques (e.g., weight decays, dropouts, etc.) to improve the generalization capability of the models being trained. - In particular, the
model trainer 160 can train the hierarchicalvideo encoder models 120 and/or 140 based on a set oftraining data 162. Thetraining data 162 can include, for example, a corpus or dataset of videos, including video data. Thetraining data 162 may be labeled with ground truth information such as, for example, ground truth moment localizations (e.g., an actual beginning and/or end of a moment), such as ground truth frame labels. Additionally and/or alternatively, the training data can include known positive and/or negative pairs of videos and queries. - In some implementations, if the user has provided consent, the training examples can be provided by the
user computing device 102. Thus, in such implementations, themodel 120 provided to theuser computing device 102 can be trained by thetraining computing system 150 on user-specific data received from theuser computing device 102. In some instances, this process can be referred to as personalizing the model. - The
model trainer 160 includes computer logic utilized to provide desired functionality. Themodel trainer 160 can be implemented in hardware, firmware, and/or software controlling a general purpose processor. For example, in some implementations, themodel trainer 160 includes program files stored on a storage device, loaded into a memory and executed by one or more processors. In other implementations, themodel trainer 160 includes one or more sets of computer-executable instructions that are stored in a tangible computer-readable storage medium such as RAM, hard disk, or optical or magnetic media. - The
network 180 can be any type of communications network, such as a local area network (e.g., intranet), wide area network (e.g., Internet), or some combination thereof and can include any number of wired or wireless links. In general, communication over thenetwork 180 can be carried via any type of wired and/or wireless connection, using a wide variety of communication protocols (e.g., TCP/IP, HTTP, SMTP, FTP), encodings or formats (e.g., HTML, XML), and/or protection schemes (e.g., VPN, secure HTTP, SSL). - The machine-learned models described in this specification may be used in a variety of tasks, applications, and/or use cases, including, for example, VR tasks and/or MLVC tasks.
- In some implementations, the input to the machine-learned model(s) of the present disclosure can be text or natural language data. The machine-learned model(s) can process the text or natural language data to generate an output. As an example, the machine-learned model(s) can process the natural language data to generate a language encoding output. As another example, the machine-learned model(s) can process the text or natural language data to generate a latent text embedding output. As another example, the machine-learned model(s) can process the text or natural language data to generate a translation output. As another example, the machine-learned model(s) can process the text or natural language data to generate a classification output. As another example, the machine-learned model(s) can process the text or natural language data to generate a textual segmentation output. As another example, the machine-learned model(s) can process the text or natural language data to generate a semantic intent output. As another example, the machine-learned model(s) can process the text or natural language data to generate an upscaled text or natural language output (e.g., text or natural language data that is higher quality than the input text or natural language, etc.). As another example, the machine-learned model(s) can process the text or natural language data to generate a prediction output.
- In some implementations, the input to the machine-learned model(s) of the present disclosure can be speech data. The machine-learned model(s) can process the speech data to generate an output. As an example, the machine-learned model(s) can process the speech data to generate a speech recognition output. As another example, the machine-learned model(s) can process the speech data to generate a speech translation output. As another example, the machine-learned model(s) can process the speech data to generate a latent embedding output. As another example, the machine-learned model(s) can process the speech data to generate an encoded speech output (e.g., an encoded and/or compressed representation of the speech data, etc.). As another example, the machine-learned model(s) can process the speech data to generate an upscaled speech output (e.g., speech data that is higher quality than the input speech data, etc.). As another example, the machine-learned model(s) can process the speech data to generate a textual representation output (e.g., a textual representation of the input speech data, etc.). As another example, the machine-learned model(s) can process the speech data to generate a prediction output.
- In some implementations, the input to the machine-learned model(s) of the present disclosure can be latent encoding data (e.g., a latent space representation of an input, etc.). The machine-learned model(s) can process the latent encoding data to generate an output. As an example, the machine-learned model(s) can process the latent encoding data to generate a recognition output. As another example, the machine-learned model(s) can process the latent encoding data to generate a reconstruction output. As another example, the machine-learned model(s) can process the latent encoding data to generate a search output. As another example, the machine-learned model(s) can process the latent encoding data to generate a reclustering output. As another example, the machine-learned model(s) can process the latent encoding data to generate a prediction output.
- In some cases, the machine-learned model(s) can be configured to perform a task that includes encoding input data for reliable and/or efficient transmission or storage (and/or corresponding decoding). For example, the task may be an audio compression task. The input may include audio data and the output may comprise compressed audio data. In another example, the input includes visual data (e.g. one or more images or videos), the output comprises compressed visual data, and the task is a visual data compression task. In another example, the task may comprise generating an embedding for input data (e.g. input audio or visual data). In some cases, the input includes audio data representing a spoken utterance and the task is a speech recognition task. The output may comprise a text output which is mapped to the spoken utterance. In some cases, the task comprises encrypting or decrypting input data.
-
FIG. 1A illustrates one example computing system that can be used to implement the present disclosure. Other computing systems can be used as well. For example, in some implementations, theuser computing device 102 can include themodel trainer 160 and thetraining dataset 162. In such implementations, themodels 120 can be both trained and used locally at theuser computing device 102. In some of such implementations, theuser computing device 102 can implement themodel trainer 160 to personalize themodels 120 based on user-specific data. -
FIG. 1B depicts a block diagram of anexample computing device 10 that performs according to example embodiments of the present disclosure. Thecomputing device 10 can be a user computing device or a server computing device. - The
computing device 10 includes a number of applications (e.g.,applications 1 through N). Each application contains its own machine learning library and machine-learned model(s). For example, each application can include a machine-learned model. Example applications include a text messaging application, an email application, a dictation application, a virtual keyboard application, a browser application, etc. - As illustrated in
FIG. 1B , each application can communicate with a number of other components of the computing device, such as, for example, one or more sensors, a context manager, a device state component, and/or additional components. In some implementations, each application can communicate with each device component using an API (e.g., a public API). In some implementations, the API used by each application is specific to that application. -
FIG. 1C depicts a block diagram of anexample computing device 50 that performs according to example embodiments of the present disclosure. Thecomputing device 50 can be a user computing device or a server computing device. - The
computing device 50 includes a number of applications (e.g.,applications 1 through N). Each application is in communication with a central intelligence layer. Example applications include a text messaging application, an email application, a dictation application, a virtual keyboard application, a browser application, etc. In some implementations, each application can communicate with the central intelligence layer (and model(s) stored therein) using an API (e.g., a common API across all applications). - The central intelligence layer includes a number of machine-learned models. For example, as illustrated in
FIG. 1C , a respective machine-learned model can be provided for each application and managed by the central intelligence layer. In other implementations, two or more applications can share a single machine-learned model. For example, in some implementations, the central intelligence layer can provide a single model for all of the applications. In some implementations, the central intelligence layer is included within or otherwise implemented by an operating system of thecomputing device 50. - The central intelligence layer can communicate with a central device data layer. The central device data layer can be a centralized repository of data for the
computing device 50. As illustrated inFIG. 1C , the central device data layer can communicate with a number of other components of the computing device, such as, for example, one or more sensors, a context manager, a device state component, and/or additional components. In some implementations, the central device data layer can communicate with each device component using an API (e.g., a private API). -
FIG. 2 depicts a block diagram of an example hierarchicalvideo encoder model 200 according to example embodiments of the present disclosure. In some implementations, the hierarchicalvideo encoder model 200 is trained to receive a set ofinput data 202 descriptive of a video and/or a user query and, as a result of receipt of theinput data 202, provideoutput data 204 that includes, for example, a plurality of frame representations, a plurality of segment representations, and/or a video representation based on the video. According to example aspects of the present disclosure, a hierarchical structure of encoders can provide semantic understanding in the form of representations at various levels of a video, as described herein. -
FIG. 3 depicts a block diagram of an example hierarchicalvideo encoder model 200 according to example embodiments of the present disclosure. As depicted inFIG. 3 , in some implementations, the hierarchicalvideo encoder model 200 can further include a frame-level encoder model 210 and a segment-level encoder model 212. For instance, theinput data 202 can be provided to frame-level encoder model 210 to produceframe representations 214. Segment representations derived from theframe representations 214 can be provided to segment-level encoder model 212 to produce theoutput data 204, such as contextualized segment representations and/or a video representation of theoutput data 204. -
FIG. 4 depicts a block diagram of an example hierarchicalvideo encoder model 200 according to example embodiments of the present disclosure. As depicted inFIG. 4 , in some implementations, the hierarchical video encoder model can further be configured to receive associatedtext data 224 in addition to input data 202 (e.g., video data). The associatedtext data 224 can be encoded bytext encoder 222. Thetext encoder 222 can produce encoded associatedtext 226, which can be a representation of associatedtext data 224 that can be used (e.g., in a cross-attention mechanism) by frame-level encoder model 212 and/or segment-level encoder model 214. In this way, predictions from frame-level encoder model 212 and/or segment-level encoder model 214 can be based at least in part on context provided by associated text data 224 (e.g., video captioning, user query, etc.). -
FIG. 5 depicts a block diagram of an example hierarchicalvideo encoder model 500 according to example embodiments of the present disclosure. The hierarchical video encoder model can be configured to produce representations ofvideo data 502.Video data 502 can be represented as a sequence of (e.g., fixed length) video segments or “clips” which, intuitively, serve as memory units representing the semantics of one or more frames in the video segment. Each video segment can be a nonoverlapping set of one or more frames of a larger video. As used herein, a “frame” with respect to a video refers to audio, visual, and/or captioning/transcript data associated with a (e.g., smallest) temporal slice of the video. For instance, a video ofvideo data 502 may be composed of at least a (e.g., temporally linear) sequence of frames, where each frame includes an image, a portion of a stream of audio data to be played along with the sequence of images, and/or supplementary text (e.g., captioning) to be displayed along with the sequence of images. - The hierarchical
video encoder model 500 includes a hierarchy of two (or more) encoder models. The frame-level encoder 510 encodes frame-level information of video data 502 (e.g., video frames or representations thereof) intoframe representations 520.Segment representations 525 for video segments can be determined based on theseframe representations 520, such as by providing a context token for a given video segment based on theframe representations 520 of frames in that video segment. Segment-level encoder 530 then encodes thesegment 525 representations into contextualizedsegment representations 540, which can further be used to produce avideo representation 550. - As illustrated in
FIG. 5 , the frame-level encoder model 510 and the segment-level encoder model 530 can be a multimodal encoder configured to produce a plurality of representations (e.g., 520, 540) based at least in part on associated text 504 (e.g., a user query). For instance, in addition to encoding thevideo data 502 and/or representations thereof, the encoder(s) 510, 530) can be cross-modal encoders that additionally fuse thevideo data 502 and/or representations thereof with associatedtext data 504, such as, for example, captioning data for the video and/or query data descriptive of a user query representing a user's search for videos and/or, more particularly, content depicted within the videos. For instance, in the encoder(s) 510.530, the input modality pairs can have cross attention, such as visual-caption/transcript, visual-query, and/or transcript-query attention. In some implementations, the associated text can be encoded by atext encoder model video data 502. - For instance, at frame-
level encoder 510, thevideo data 502 is first encoded byvideo data encoder 512. Additionally, associatedtext data 504 is encoded bytext encoder model 516.Cross-modal transformer 514 then producesframe representations 520 based on cross-attention between the encodedvideo data 502 fromvideo data encoder 512 and encoded associatedtext data 504 fromtext encoder model 516. Similarly, at segment-level encoder 530, the segment representation(s) 525 are first encoded byvideo data encoder 532.Associated text data 504 can again be encoded, by a same model as for frame-level encoder 510 (e.g., text encoder 516) and/or a secondtext encoder model 536.Cross-modal transformer 534 then produces contextualizedsegment representations 540 based on cross-attention between the encodedsegment representations 525 fromvideo data encoder 532 and encoded associated text data fromtext encoder model 536. -
FIG. 6 depicts a flow chart diagram of anexample method 600 for generating video representations utilizing a hierarchical video encoder according to example embodiments of the present disclosure. AlthoughFIG. 6 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particularly illustrated order or arrangement. The various steps of themethod 600 can be omitted, rearranged, combined, and/or adapted in various ways without deviating from the scope of the present disclosure. - The
method 600 can include, at 602, obtaining (e.g., by a computing system including one or more computing devices) a video, wherein the video includes a plurality of frames. Each frame can include visual data (e.g., an image) and/or associated audio data (e.g., a slice of an audio stream). The video may be unsegmented, such that no temporal divisions exist in the video. The video may be, for example, accessed from a corpus of videos, such as a content sharing website, media provider, database, and/or other suitable corpus. - Additionally, the
method 600 can include, at 604, processing (e.g., by the computing system) each of the plurality of frames with a machine-learned frame-level encoder model to respectively generate a plurality of frame representations for the plurality of frames. The plurality of frame representations can be respective to the plurality of frames. For instance, each frame representation can be produced from a respective (e.g., unique) frame of the plurality of frames. - In some implementations, the frame-level encoder model can be a multimodal encoder model configured to produce the plurality of frame representations based at least in part on associated text (e.g., a user query, captioning for the video, etc.). For instance, the
method 600 can include processing (e.g., by the computing system) the associated text with the machine-learned frame-level encoder model to produce the plurality of frame representations, wherein the plurality of frame representations are based at least in part on the associated text. The associated text can be processed concurrently with the plurality of frames. In some implementations, the associated text can be encoded. - Additionally, the
method 600 can include, at 606, determining (e.g., by the computing system) a plurality of segment representations representative of a plurality of video segments including one or more of the plurality of frames. In some implementations, the plurality of video segments can each have about equal length. For instance, in some implementations, a video may be divided into video segments based at least in part on a fixed segment length. In some implementations, the plurality of video segments may be nonoverlapping. For instance, a given frame may be included within only one video segment of the plurality of video segments. - The plurality of segment representations can be based at least in part on the plurality of frame representations. In some implementations, the plurality of segment representations can include a context token. As one example, the plurality of frame representations can be, can include, or can otherwise be used to generate a contextualized frame representation, such as a context (e.g., CLS) token specific to each frame. The context tokens for each frame can be aggregated or otherwise combined to produce a segment representation for a video segment including the frames for which the context tokens are combined.
- Additionally, the
method 600 can include, at 608, processing (e.g., by the computing system) the plurality of segment representations with a machine-learned segment-level encoder model to generate a plurality of contextualized segment representations. The contextualized segment representation can include a context (e.g., CLS) token specific to the respective video segment. In some cases, processing the plurality of segment representations can include processing (e.g., by the computing system) the associated text with the machine-learned segment-level encoder model to produce the plurality of contextualized segment representations. The plurality of contextualized segment representations can thus be based at least in part on the associated text. - Additionally, the
method 600 can include, at 610, determining (e.g., by the computing system), based at least in part on the plurality of contextualized segment representations, a video representation. For instance, in some implementations, context tokens corresponding to each segment in a video can be aggregated or otherwise combined to produce the video representation. Additionally, themethod 600 can include, at 612, providing (e.g., by the computing system) the video representation as an output (e.g., of the hierarchical video encoder model). -
FIG. 7 depicts a flow chart diagram of anexample method 700 for moment localization in a video corpus including a plurality of videos according to example embodiments of the present disclosure. AlthoughFIG. 7 depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particularly illustrated order or arrangement. The various steps of themethod 700 can be omitted, rearranged, combined, and/or adapted in various ways without deviating from the scope of the present disclosure. - The
method 700 can include, at 702, obtaining (e.g., by a computing system including one or more computing devices) a user query, such as from a user. The user query can include text (e.g., text data). The user query can be obtained in any suitable manner according to example aspects of the present disclosure. As one example, the user query can be obtained from a user by providing a user with a text field in which to enter the user query, such as at a search engine service. As another example, the user query can be obtained from an external computing system or other computing device. The user query may be or include only text data, may be or may include speech data (e.g., that is converted into text data) and/or may be or may include any other suitable data. In some cases, the user query can be or can include a short text string (e.g., on the order of fewer than about 20 words) descriptive of a moment within a video. - Additionally, the
method 700 can include, at 704, identifying (e.g., by the computing system) one or more highest likelihood videos of the plurality of videos. In some implementations, each highest likelihood video of the one or more highest likelihood videos can be identified based at least in part on a video-query compatibility score between the user query and a video representation of the highest likelihood video that is output by a machine-learned hierarchical video encoder model, as described herein. For instance, the video-query compatibility score can effectively rank the corpus of videos and the K highest scoring video(s) in the corpus, as defined by the video-query compatibility score, can be selected as the highest likelihood video(s). In some implementations, the video representation of a highest likelihood video can be based at least in part on a highest scoring segment representation of a plurality of segment representations of the highest likelihood video. For instance, the hierarchical video encoder may output a plurality of segment representations associated with a plurality of video segments of the highest scoring videos, each of which has an associated compatibility score with the user query. The highest score of these compatibility scores can be used as representative of the entire video. In some implementations, the one or more highest likelihood videos can be selected based at least in part on a negative log-likelihood of the one or more highest likelihood videos containing the moment described by the user query. For instance, the videos can be selected to minimize the negative log-likelihood. - A modeling objective for the video retrieval task can select a matching video most likely to have a moment to be localized by employing a contrastive loss that contrasts a compatibility score of positive (e.g., matching) pairs of video representation and query against negative (e.g., not matching) pairs of video representation and query. The negative pairs can be randomly sampled. One example compatibility score is computed as:
-
f(v,h)=maxk(W VR T*Ψ(φk ;v,h)) - where WVR is a linear regressor.
- In some cases, the representation of a highest likelihood video can include a highest scoring segment representation of a plurality of segment representations of the highest likelihood video. For instance, of a plurality of segments of the video, the score of the highest-scoring segment can be selected as representative of the entire video. In some implementations, the one or more highest likelihood videos can be selected based at least in part on a negative log-likelihood of the one or more highest likelihood videos containing the moment described by the user query. As one example, the VR loss can be computed as:
-
- where v(i) is a ground-truth video for a text query h(i). Essentially, the hierarchical video encoder model can parameterize the conditional probability P (v|h) for the video retrieval task by providing a hierarchically-learned video representation for a video v.
- Additionally, the
method 700 can include, at 706, determining a moment localization within a matching video of the one or more highest likelihood videos. For instance, a moment localization can be determined for a moment, where the moment localization specifies a beginning and/or an end of the moment within the matching video. As one example, the moment localization can be or can include timestamps, frame indices, etc. This task is referred to as Moment Localization in Single Video, or MLSV. The hierarchical video encoders as described herein can be jointly trained on both tasks in a multitask learning configuration. The hierarchical (e.g., and cross-attentional) encoders as described herein can be beneficial for these tasks, as the two tasks can require understanding semantics of a video at differing temporal resolutions, and the models described herein can model short-range and long-range video semantics. For instance, the hierarchical video encoders described herein can learn semantic understanding for at least three scales: frame-level, segment-level, and/or video-level. For example, including segment-level encoders as described herein can provide for capturing both coarse- and fine-grained semantic information in videos. - Additionally, one or more classifiers can be applied to identify regions (e.g., frames) corresponding to a beginning and/or an end of a relevant video segment. For instance, a lower-level classifier (e.g., a per-frame classifier) can be used to classify a probability of each frame of being a starting frame and/or an ending frame. A higher-level classifier (e.g., at the segment level or video level) can classify a probability of a starting frame and/or an ending frame being located within a segment and/or video. As one example, the probability of a frame being a start frame can be represented by:
-
- where Wstart is a (e.g., linear) frame level classifier and Ustart is a (e.g., linear) video level classifier. Similarly, the probability of a frame t being an end frame can be represented by:
-
- where Wend is a (e.g., linear) frame-level classifier and Uend is a (e.g., linear) video-level classifier.
- Moment localization can thus essentially be treated as a frame classification problem. For instance, each frame can be classified as belonging to one of three labels: a beginning frame, which marks the beginning of a moment localization; an end frame, which marks the end of a moment localization; and an other frame that may or may not be included within a moment localization for a given moment, but is not bordering a moment. Additionally, a loss during training of the hierarchical video encoder model can include a cross-entropy loss between a predicted classification of each frame and a true label of each frame. As one example, the training loss can be expressed as:
-
- where ft (i) is a true label for a frame xt and yt (i) is a predicted label of the frame xt. Essentially, the hierarchical video encoder can provide parameterization of the labeling model P(yt (i)|v(i),h(i)) for the moment localization task.
- Additionally, in some implementations, the
method 700 can optionally include, at 708, providing (e.g., by the computing system) the moment localization for display to a user. As one example, the matching video can be returned to a user providing the user query, such as in the form of search results, etc. In addition to and/or alternatively to displaying the matching video, the user may additionally be provided with the moment localization, such as the beginning and/or the end. As one example, a graphical user interface element depicting a progress bar indicative of a user's position within a video (e.g., by a marker between a beginning and an end of the video) may be highlighted, reduced, or otherwise annotated with a marking element indicative of the location of the moment within the larger video. Additionally and/or alternatively, in some implementations, upon a user selecting the matching video (e.g., from the search results), the user may be provided with the moment localization in the form of video playback being initiated at the moment beginning (e.g., as opposed to the video beginning) Any other suitable form of providing the moment localization for display to a user can be employed in accordance with example aspects of the present disclosure. - The technology discussed herein makes reference to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from such systems. The inherent flexibility of computer-based systems allows for a great variety of possible configurations, combinations, and divisions of tasks and functionality between and among components. For instance, processes discussed herein can be implemented using a single device or component or multiple devices or components working in combination. Databases and applications can be implemented on a single system or distributed across multiple systems. Distributed components can operate sequentially or in parallel.
- While the present subject matter has been described in detail with respect to various specific example embodiments thereof, each example is provided by way of explanation, not limitation of the disclosure. Those skilled in the art, upon attaining an understanding of the foregoing, can readily produce alterations to, variations of, and equivalents to such embodiments. Accordingly, the subject disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For instance, features illustrated or described as part of one embodiment can be used with another embodiment to yield a still further embodiment. Thus, it is intended that the present disclosure cover such alterations, variations, and equivalents.
Claims (21)
1.-20. (canceled)
21. A computer-implemented method, the method comprising:
obtaining, by a computing system comprising one or more processors, a training dataset, wherein the training dataset comprises a video and a plurality of frame labels, wherein the video comprises a plurality of frames, and wherein the plurality of frame labels comprise ground-truth labels for the plurality of frames;
processing, by the computing system, a frame of the plurality of frames with a hierarchical video encoder model to determine a predicted frame classification, where the predicted frame classification is descriptive of a prediction of whether the frame is a border frame of a moment localization;
evaluating, by the computing system, a loss function that evaluates a difference between the predicted frame classification and a respective frame label of the plurality of frame labels, wherein the respective frame label comprises a ground-truth label associated with the frame; and
adjusting, by the computing system, one or more parameters of the hierarchical video encoder model based at least in part on the loss function.
22. The method of claim 21 , wherein the predicted frame classification comprises a beginning frame classification, wherein the beginning frame classification is descriptive of a prediction that the frame marks the beginning of the moment localization.
23. The method of claim 21 , wherein the predicted frame classification comprises an end frame classification, wherein the end frame classification is descriptive of a prediction that the frame marks the end of the moment localization.
24. The method of claim 21 , wherein the predicted frame classification comprises an other frame classification, wherein the other frame classification is descriptive of a prediction that the frame is a non-border frame for a moment of the moment localization.
25. The method of claim 21 , wherein the hierarchical video encoder model comprises a machine-learned frame-level encoder model and a machine-learned segment-level encoder model.
26. The method of claim 21 , wherein the loss function comprises a cross-entropy loss.
27. The method of claim 21 , further comprising:
obtaining, by the computing system, a user query, the user query comprising text;
identifying, by the computing system, one or more highest likelihood videos of the plurality of videos, each highest likelihood video of the one or more highest likelihood videos identified based at least in part on a video-query compatibility score between the user query and a video representation of the highest likelihood video that is output by the hierarchical video encoder model; and
determining, by the computing system, a moment localization within a matching video of the one or more highest likelihood videos, the moment localization comprising a moment beginning and a moment end.
28. The method of claim 27 , wherein the moment beginning comprises a beginning frame of the matching video, the beginning frame having a frame representation that is classified as representing a beginning of a moment described by the user query.
29. The method of claim 27 , wherein the moment end comprises an end frame of the matching video, the end frame having a frame representation that is classified as representing an end of the moment described by the user query.
30. The method of claim 21 , wherein the hierarchical video encoder model comprises:
a frame-level encoder model configured to receive a plurality of frames of a video as input and provide, in response to receipt of the plurality of frames as input, a plurality of frame representations of the plurality of frames as output; and
a segment-level encoder model configured to receive a plurality of segment representations as input and provide, in response to receipt of the plurality of segment representations as input, a plurality of contextualized segment representations as output.
31. A computing system, the system comprising:
one or more processors;
one or more non-transitory computer readable media that collectively store instructions that, when executed by the one or more processors, cause the computing system to perform operations, the operations comprising:
obtaining a training dataset, wherein the training dataset comprises a search query, a ground-truth video, and a negative video-query pair, wherein the ground-truth video is responsive to the search query;
processing the ground-truth video with a machine-learned hierarchical video encoder model to generate a plurality of contextualized segment representations, wherein each contextualized segment representation of the plurality of contextualized segment representations comprise segment-level semantic information for a respective video segment;
determining a first video-query compatibility score based on the search query and the plurality of contextualized segment representations;
determining a second video-query compatibility score based on a respective video representation and respective query of the negative video-query pair;
evaluating a loss function that evaluates a difference between the first video-query compatibility score and the second video-query compatibility score; and
adjusting one or more parameters of the machine-learned hierarchical video encoder model based at least in part on the loss function.
32. The system of claim 31 , wherein the loss function comprises a negative log-likelihood loss.
33. The system of claim 31 , wherein the operations comprise:
obtaining a user query; and
determining a moment localization that is responsive to the user query based on processing the user query and one or more videos with the machine-learned hierarchical video encoder model.
34. The system of claim 33 , wherein the moment localization comprises a plurality of sequential frames from a beginning frame to an end frame, wherein the beginning frame comprises a first temporal slice of a matching video, and wherein the end frame comprises a second temporal slice of the matching video.
35. The system of claim 31 , wherein the machine-learned hierarchical video encoder model comprises one or more cross-attentional transformer models.
36. One or more non-transitory computer readable media that collectively store instructions that, when executed by one or more processors, cause a computing system to perform operations, the operations comprising:
obtaining a training dataset, wherein the training dataset comprises a search query, a around-truth video, and a around-truth moment localization, wherein the ground-truth video and the ground-truth moment localization are responsive to the search query, wherein the ground-truth video comprises a plurality of frames;
processing the plurality of frames with a hierarchical video encoder model to generate a plurality of contextualized segment representations, wherein each contextualized segment representation of the plurality of contextualized segment representations comprise segment-level semantic information for a respective video segment of a plurality of video segments for the ground-truth video;
processing the search query and the plurality of contextualized segment representations to determine a video-query compatibility score and a predicted moment localization, wherein the predicted moment localization comprises a particular video segment from the plurality of video segments;
evaluating a loss function that evaluates a difference between the predicted moment localization and the ground-truth moment localization; and
adjusting one or more parameters of the hierarchical video encoder model based at least in part on the loss function.
37. The one or more non-transitory computer readable media of claim 36 , wherein the hierarchical video encoder model comprises a machine-learned frame-level encoder model and a machine-learned segment-level encoder model.
38. The one or more non-transitory computer readable media of claim 37 , wherein the operations further comprise:
obtaining a video, wherein the video comprises a plurality of frames;
processing each of the plurality of frames with the machine-learned frame-level encoder model to respectively generate a plurality of frame representations for the plurality of frames, the plurality of frame representations respective to the plurality of frames;
determining a plurality of segment representations representative of a plurality of video segments, wherein each of the plurality of video segments comprise a subset of the plurality of frames that comprise a temporally linear sequence of frames;
processing the plurality of segment representations with the machine-learned segment-level encoder model to generate a plurality of contextualized segment representations;
determining a video representation based at least in part on the plurality of contextualized segment representations.
39. The one or more non-transitory computer readable media of claim 38 , wherein the video representation is generated based on one or more segment representations, wherein the one or more segment representations are determined based on a plurality of frames of a segment associated with the one or more segment representations, wherein the one or more segment representations are generated based at least in part on self-attention for the plurality of frames and cross-attention for the plurality of frames, and, wherein each contextualized segment representation of the plurality of contextualized segment representations comprise segment-level semantic information for a respective video segment.
40. The one or more non-transitory computer readable media of claim 36 , wherein the video representation is generated based at least in part on self-attention for a plurality of video segments of the highest likelihood video and cross-attention for the plurality of video segments, and wherein the plurality of segment representations are based at least in part on the plurality of frame representations.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US18/529,173 US20240114158A1 (en) | 2021-01-29 | 2023-12-05 | Hierarchical Video Encoders |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/162,150 US11533495B2 (en) | 2021-01-29 | 2021-01-29 | Hierarchical video encoders |
US18/070,556 US11876986B2 (en) | 2021-01-29 | 2022-11-29 | Hierarchical video encoders |
US18/529,173 US20240114158A1 (en) | 2021-01-29 | 2023-12-05 | Hierarchical Video Encoders |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US18/070,556 Continuation US11876986B2 (en) | 2021-01-29 | 2022-11-29 | Hierarchical video encoders |
Publications (1)
Publication Number | Publication Date |
---|---|
US20240114158A1 true US20240114158A1 (en) | 2024-04-04 |
Family
ID=82704221
Family Applications (3)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/162,150 Active US11533495B2 (en) | 2021-01-29 | 2021-01-29 | Hierarchical video encoders |
US18/070,556 Active US11876986B2 (en) | 2021-01-29 | 2022-11-29 | Hierarchical video encoders |
US18/529,173 Pending US20240114158A1 (en) | 2021-01-29 | 2023-12-05 | Hierarchical Video Encoders |
Family Applications Before (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/162,150 Active US11533495B2 (en) | 2021-01-29 | 2021-01-29 | Hierarchical video encoders |
US18/070,556 Active US11876986B2 (en) | 2021-01-29 | 2022-11-29 | Hierarchical video encoders |
Country Status (1)
Country | Link |
---|---|
US (3) | US11533495B2 (en) |
Families Citing this family (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20230094234A1 (en) * | 2021-09-24 | 2023-03-30 | Apple Inc. | Channel-aware semantic coding |
US20230169110A1 (en) * | 2021-11-30 | 2023-06-01 | Microsoft Technology Licensing, Llc | Method and system of content retrieval for visual data |
CN116385946B (en) * | 2023-06-06 | 2023-08-29 | 山东大学 | Video-oriented target fragment positioning method, system, storage medium and equipment |
Family Cites Families (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11487999B2 (en) * | 2019-12-09 | 2022-11-01 | Salesforce.Com, Inc. | Spatial-temporal reasoning through pretrained language models for video-grounded dialogues |
US11354904B2 (en) * | 2020-07-10 | 2022-06-07 | International Business Machines Corporation | Spatial-temporal graph-to-sequence learning based grounded video descriptions |
-
2021
- 2021-01-29 US US17/162,150 patent/US11533495B2/en active Active
-
2022
- 2022-11-29 US US18/070,556 patent/US11876986B2/en active Active
-
2023
- 2023-12-05 US US18/529,173 patent/US20240114158A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US11533495B2 (en) | 2022-12-20 |
US20220256175A1 (en) | 2022-08-11 |
US11876986B2 (en) | 2024-01-16 |
US20230103148A1 (en) | 2023-03-30 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11816439B2 (en) | Multi-turn dialogue response generation with template generation | |
US11876986B2 (en) | Hierarchical video encoders | |
US11836181B2 (en) | Content summarization leveraging systems and processes for key moment identification and extraction | |
US11657802B2 (en) | Utilizing a dynamic memory network for state tracking | |
JP6361351B2 (en) | Method, program and computing system for ranking spoken words | |
CN113157965B (en) | Audio visual model training and audio visual method, device and equipment | |
US20210133279A1 (en) | Utilizing a neural network to generate label distributions for text emphasis selection | |
CN111723295B (en) | Content distribution method, device and storage medium | |
CN113678149A (en) | Architecture for modeling annotation and editing relationships | |
CN111639176B (en) | Real-time event summarization method based on consistency monitoring | |
US20230274527A1 (en) | Systems and Methods for Training Multi-Class Object Classification Models with Partially Labeled Training Data | |
CN114443899A (en) | Video classification method, device, equipment and medium | |
CN115630145A (en) | Multi-granularity emotion-based conversation recommendation method and system | |
CN116975615A (en) | Task prediction method and device based on video multi-mode information | |
Wang et al. | Data augmentation for internet of things dialog system | |
CN111209468B (en) | Method and equipment for generating keywords | |
CN116977701A (en) | Video classification model training method, video classification method and device | |
US20230237093A1 (en) | Video recommender system by knowledge based multi-modal graph neural networks | |
US11810598B2 (en) | Apparatus and method for automated video record generation | |
US11822893B2 (en) | Machine learning models for detecting topic divergent digital videos | |
CN115186085A (en) | Reply content processing method and interaction method of media content interaction content | |
CN115169472A (en) | Music matching method and device for multimedia data and computer equipment | |
Xie et al. | Enhancing multimodal deep representation learning by fixed model reuse | |
Kim | Multimedia emotion prediction using movie script and spectrogram | |
US20240135187A1 (en) | Method for Training Large Language Models to Perform Query Intent Classification |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:JAIN, VIHAN;LEE, JOONSEOK;ZHAO, MING;AND OTHERS;SIGNING DATES FROM 20210506 TO 20210511;REEL/FRAME:065765/0334 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |