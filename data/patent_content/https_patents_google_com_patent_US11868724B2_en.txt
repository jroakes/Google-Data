US11868724B2 - Generating author vectors - Google Patents
Generating author vectors Download PDFInfo
- Publication number
- US11868724B2 US11868724B2 US17/654,660 US202217654660A US11868724B2 US 11868724 B2 US11868724 B2 US 11868724B2 US 202217654660 A US202217654660 A US 202217654660A US 11868724 B2 US11868724 B2 US 11868724B2
- Authority
- US
- United States
- Prior art keywords
- author
- words
- word
- sequence
- vector
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 239000013598 vector Substances 0.000 title claims abstract description 287
- 238000013528 artificial neural network Methods 0.000 claims abstract description 98
- 238000000034 method Methods 0.000 claims abstract description 77
- 238000012545 processing Methods 0.000 claims description 31
- 230000004044 response Effects 0.000 claims description 26
- 238000004891 communication Methods 0.000 claims description 12
- 230000001143 conditioned effect Effects 0.000 claims description 6
- 230000001419 dependent effect Effects 0.000 claims description 6
- 238000012549 training Methods 0.000 abstract description 53
- 238000004590 computer program Methods 0.000 abstract description 16
- 230000008569 process Effects 0.000 description 57
- 230000026676 system process Effects 0.000 description 13
- 238000010586 diagram Methods 0.000 description 12
- 230000006870 function Effects 0.000 description 5
- 230000015654 memory Effects 0.000 description 5
- 230000003287 optical effect Effects 0.000 description 4
- 230000008901 benefit Effects 0.000 description 3
- 238000010801 machine learning Methods 0.000 description 3
- 230000004913 activation Effects 0.000 description 2
- 238000001994 activation Methods 0.000 description 2
- 230000003750 conditioning effect Effects 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 230000000306 recurrent effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000005540 biological transmission Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000007477 logistic regression Methods 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/289—Phrasal analysis, e.g. finite state techniques or chunking
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/31—Indexing; Data structures therefor; Storage structures
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/35—Clustering; Classification
Definitions
- This specification relates to text classification using data processing systems.
- Text classification systems can classify pieces of electronic text, e.g., electronic documents. For example, text classification systems can classify a piece of text as relating to one or more of a set of predetermined topics. Some text classification systems receive as input features of the piece of text and use the features to generate the classification for the piece of text.
- Neural networks are machine learning models that employ one or more layers of models to generate an output, e.g., a classification, for a received input.
- Some neural networks include one or more hidden layers in addition to an output layer. The output of each hidden layer is used as input to the next layer in the network, i.e., the next hidden layer or the output layer of the network.
- Each layer of the network generates an output from a received input in accordance with current values of a respective set of parameters.
- one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of obtaining a set of sequences of words, the set of sequences of words comprising a plurality of first sequences of words and, for each first sequence of words, a respective second sequence of words that follows the first sequence of words, wherein each first sequence of words and each second sequence of words has been classified as being authored by a first author; and training a neural network system on the first sequences and the second sequences to determine an author vector for the first author, wherein the author vector characterizes the first author.
- inventions of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- a set of word sequences comprising a plurality of sequences of words and, for each sequence of words, a word that follows a last word in the sequence of words, wherein each word sequence in the set has been classified as being authored by a first author; and processing the plurality of sequences of words using a trained neural network system to determine an author vector for the first author, wherein the author vector characterizes the first author.
- inventions of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- a system of one or more computers to be configured to perform particular operations or actions means that the system has installed on it software, firmware, hardware, or a combination of them that in operation cause the system to perform the operations or actions.
- one or more computer programs to be configured to perform particular operations or actions means that the one or more programs include instructions that, when executed by data processing apparatus, cause the apparatus to perform the operations or actions.
- An author vector that effectively characterizes an author can be generated from text written by the author without the text being labeled. Once generated, the author vector can characterize different properties of the author depending on the context of use of the author vector. By clustering the author vectors, clusters of authors that have similar communication styles and, in some implementations, personality types can be effectively be generated. Once generated, the author vectors and, optionally, the clusters can be effectively used for a variety of purposes.
- FIG. 1 shows an example author vector system.
- FIG. 2 shows another example author vector system.
- FIG. 3 shows a flow diagram of an example process for determining an author vector from a set of word sequences.
- FIG. 4 shows a flow diagram of an example process for training a neural network system on a training sequence of words from a particular author.
- FIG. 5 shows a flow diagram of an example process for adjusting the author vector for a set of word sequences.
- FIG. 6 shows a flow diagram of another example process for determining an author vector from a set of word sequences.
- FIG. 7 shows a flow diagram of another example process for training a neural network system on a training sequence of words from a particular author.
- FIG. 8 shows a flow diagram of another example process for adjusting the author vector for a set of word sequences.
- FIG. 1 shows an example author vector system 100 .
- the author vector system 100 is an example of a system implemented as computer programs on one or more computers in one or more locations, in which the systems, components, and techniques described below can be implemented.
- the author vector system 100 generates word scores for sequences of words, e.g., word scores 126 for a word sequence 106 from a set of word sequences 102 that have each been classified as being authored by the same author.
- the word scores 126 for the word sequence 106 include a respective score for each word in a pre-determined set of words, with the word score for a given word representing the predicted likelihood that the word follows the last word in the word sequence 106 .
- the author vector system 100 As part of generating word scores for sequences of words from a given set of word sequences, the author vector system 100 generates an author vector for the author that has been classified as the author of the word sequence, e.g., an author vector 120 for the author of the word sequences 102 .
- the author vector generated by the author vector system 100 for a given author is a vector of numeric values that characterizes the author.
- the author vector can characterize one or more of the communication style of the author, the author's personality type, the author's likelihood of selecting certain content items, and other characteristics of the author.
- the author vectors may be vectors of floating-point values or of quantized floating-point values.
- the author vector system 100 includes a neural network system 110 that, for a given word sequence from a given set of word sequences, receives the word sequence and data identifying the author of the word sequence and processes the word sequence and the data identifying the author to generate the word scores for the word sequence.
- the author vector system 100 can receive an author identifier 104 for the author of the word sequences 102 and the word sequence 106 and generate the word scores 126 for the word sequence 106 .
- the author identifier 104 may be, e.g., pre-assigned to the author or be generated by the author vector system 100 such that the identifier uniquely identifies the author.
- the neural network system 110 includes an embedding layer 112 , a combining layer 114 , and a classifier layer 116 .
- the embedding layer 112 maps the data identifying the author to an author vector, e.g., the author vector 120 for the author of the word sequences 102 , in accordance with current values of a set of author parameters.
- the embedding layer 112 also maps each word in the word sequence to a respective word vector representation, e.g., word vector representations 122 for the words in the word sequence 106 , in accordance with current values of a set of word parameters.
- Each word vector is a vector representation of the corresponding word, e.g., a vector of floating point or quantized floating point values.
- the combining layer 114 receives the author vector and the word vector representations and generates a combined representation from the author vector and the word vector representations, e.g., a combined representation 124 from the word vector representations 122 and the author vector 120 . Generating the combined representation is described in more detail below with reference to FIGS. 3 and 4 .
- the classifier layer 116 receives the combined representation and processes the combined representation to generate the word scores for the word sequence in accordance with current values of a set of classifier parameters. For example, the classifier layer 116 may process the combined representation 124 to generate the word scores 126 for the word sequence 106 .
- the author vector system 100 trains the neural network system 110 on multiple word sequences classified as being authored by various authors in order to determine trained values of the word parameters and the classifier parameters. Training the neural network system is described in more detail below with reference to FIGS. 3 and 4 .
- the author vector system 100 can receive word sequences written by a new author and process the sequences of words using the neural network system 110 to determine an author vector for the new author.
- the author vector system 100 holds the values of the word parameters and the classifier parameters fixed, i.e., fixes the values to be the trained values, while adjusting the values of the author parameters while processing the sequences of words written by the new author.
- the author vector system 100 determines the author vector for the new author using the trained neural network system by iteratively providing each of the sequences classified as being written by the author to the trained neural network system to determine the author vector for the first author using gradient descent. That is, for each sequence of words, the author vector system 100 determines an error between the predicted word generated by the neural network system 110 and the word that actually follows the sequence of words. The author vector system 100 then uses the error to adjust the values of the author parameters. Determining the author vector for a new author is described in more detail below with reference to FIG. 5 .
- FIG. 2 shows an example author vector system 200 .
- the author vector system 200 is an example of a system implemented as computer programs on one or more computers in one or more locations, in which the systems, components, and techniques described below can be implemented.
- the author vector system 200 receives a set of word sequences that have each been classified as being authored by the same author.
- the set of word sequences includes multiple initial word sequences and, for each initial word sequence, a respective next word sequence.
- the next word sequence for a given initial word sequence is a sequence that immediately followed the initial word sequence when authored by the author.
- the set of word sequences can include an initial sequence 202 .
- the author vector system 200 includes an encoder long short-term memory (LSTM) neural network 206 , an embedding layer 208 , a combining subsystem 216 , and a decoder LSTM neural network 220 .
- LSTM long short-term memory
- the author vector system 200 processes the initial sequence using the encoder LSTM neural network 206 to convert the initial sequence to an alternative representation for the initial sequence, e.g., an alternative representation 212 for the initial sequence 102 .
- the encoder LSTM neural network 206 is a recurrent neural network that receives an initial sequence and generates an alternative representation from the initial sequence.
- the encoder LSTM neural network 206 is an LSTM neural network that includes one or more LSTM neural network layers, with each of the LSTM layers including one or more LSTM memory blocks.
- Each LSTM memory block can include one or more cells that each include an input gate, a forget gate, and an output gate that allow the cell to store previous activations generated by the cell, e.g., as a hidden state for use in generating a current activation or to be provided to other components of the LSTM neural network 206 .
- the encoder LSTM neural network 206 has been configured to process each word in a given initial sequence to generate the alternative representation of the initial sequence in accordance with a set of parameters.
- the encoder LSTM neural network 206 is configured to receive each word in the initial sequence in the input order and, for a given word, to update the current hidden state of the encoder LSTM neural network 206 by processing the received word, i.e., to modify the current hidden state of the encoder LSTM neural network 206 that has been generated by processing previous words from the initial sequence by processing the current received word.
- the embedding layer 208 maps an identifier for the author of the initial sequence currently being processed by the author vector system 200 to an author vector, e.g., the author vector 214 for identifier 204 identifying the author of the initial sequence 202 , in accordance with current values of a set of author parameters.
- the combining subsystem 216 combines the alternative representation and the author vector to generate a combined representation for the initial sequence, e.g., a combined representation 218 for the initial sequence 202 .
- the combining subsystem 216 combines the alternative representation and the author vector in a predetermined manner, i.e., the combining subsystem 216 does not include any parameters that have values that are adjusted during training.
- the combining subsystem 216 can concatenate, average, or sum the alternative representation and the author vector.
- the combining subsystem 216 generates the combined representation in accordance with a set of parameters.
- the combining system 216 can concatenate or average the alternative representation and the author vector and then process the result through one or more feedforward neural network layers to generate the combined representation.
- the author vector system 200 can then determine an error between the predicted next sequence and the actual next sequence for the initial word sequence and adjust the values of the parameters of the encoder LSTM neural network 206 , the embedding layer 206 , the decoder LSTM neural network 220 , and, optionally, the combining subsystem 216 using conventional machine learning techniques, e.g., by backpropagating a gradient computed using the error to each of the components of the system.
- the author vector system 200 can process data identifying a given author using the embedding layer 208 to determine a final author vector for the given author.
- the author vector system 200 can adjust the values of the parameters of the components of the system using the new set of sequences of words and then determine the author vector for the new author by processing data identifying the new author using the embedding layer 208 .
- the author vector system 100 or the author vector system 200 can associate the author vector with the identifier for the author in a repository or use the author vector for some immediate purpose.
- an author vector can be used to verify whether or not a set of text has been authored by a particular author.
- the system can receive the set of text and process the sequences in the set of text to determine an author vector for the author of the set of text as described above.
- the system can then compare the determined author vector with an author vector for the particular author to verify whether the new set of text was authored by the particular author.
- the system can determine that the new set of text was authored by the particular author when the determined author vector is sufficiently close to the author vector for the particular author, e.g., when the distance between the determined author vector and the author vector for the particular author is smaller than the distance between the determined author vector and any other author vector by more than a threshold distance.
- an author vector can be used to determine the author of a new set of text.
- the system can receive the new set of text and process the sequences in the new set of text to determine an author vector for the author of the new set of text as described above.
- the system can then compare the determined author vector with other author vectors that have been computed by the system to determine whether the determined author vector is sufficiently close to any other author vector. For example, the system can determine that the determined author vector is sufficiently close to another author vector when the distance between the author vector and the other author vector is smaller than a threshold distance and the distance between the other author vector and any of the other author vectors is larger than a threshold distance. If the determined author vector is sufficiently close to another author vector, the system can determine that the author of the new set of text is the author corresponding to the other author vector.
- an author vector can be used to determine the other authors that are most similar to the author of a new set of text.
- the system can receive the new set of text and process the sequences in the new set of text to determine an author vector for the author of the new set of text as described above.
- the system can then compare the determined author vector with other author vectors that have been computed by the system to determine the other authors that are similar to the author of the new set of text, e.g., other authors that have vectors that are close to the determined author vector.
- the clusters can be used to predict basic personality types, e.g., as characterized by Meyers Briggs assessments, OCEAN personality labels, or other personality classifications.
- the system obtains data classifying the personality type of various authors that have had their author vector clustered and trains a classifier, e.g., a logistic regression classifier, to predict personality types for each cluster using the personality type classifications and the clusters for the corresponding author vectors.
- the system assigns a personality type to each cluster using the classifier.
- the system can then receive or determine a new author vector, assign the new author vector to a cluster, and output the personality type for the assigned cluster as a predicted personality type for the new author.
- the system can condition responses on different personality types, e.g., by conditioning responses on a representative author vector for a cluster that is mapped to a particular personality type.
- a system that ranks content items may rank the content items based in part on which content items users have previously selected, e.g., so that more frequently selected content items are higher in the ranking.
- the system may maintain, for each cluster of author vectors, data identifying selections of content items by users that have author vectors in that cluster.
- the system can use the data identifying content item selections by users having author vectors in that particular cluster to generate a ranking that is specific to users in the particular cluster and then generate the response that is provided to the user based on that ranking.
- the response can be conditioned on the user's author vector or on the cluster to which the user's author vector belongs, even if the user has not already used the system. That is, the author vectors can be estimated in an entirely different domain from the domain in which the system generates responses.
- FIG. 3 is a flow diagram of an example process 300 for determining an author vector from a set of word sequences.
- the process 300 will be described as being performed by a system of one or more computers located in one or more locations.
- an author vector system e.g., the author vector system 100 of FIG. 1 , appropriately programmed, can perform the process 300 .
- the system trains a neural network system, e.g., the neural network system 110 of FIG. 1 , to generate word scores (step 302 ).
- the neural network system is a system that includes an embedding layer, a combining layer, and a classifier layer.
- the embedding layer is configured to receive data identifying an author and a sequence of words authored by a particular author, map the data identifying the author to an author vector representation in accordance with current values of a set of author parameters, and map each word in the sequence of words to a respective word vector representation in accordance with current values of a set of word parameters.
- the combining layer is configured to combine the word vector representations and the author vector representation to generate a combined representation.
- the classifier layer is configured to process the combined representation to generate a set of word scores for the word sequence in accordance with current values of a set of classifier parameters.
- the system performs an instance of a gradient descent training procedure for each of the training sequences.
- the system processes a sequence of words using the neural network system to generate word scores for the sequence and then adjusts the values of the parameters using the word scores and the word that follows the last word in the sequence, e.g., using gradient descent and backpropagation.
- the word that follows the last word in the sequence may be a word that actually follows the last word in the sequence in a longer word sequence authored by a particular author. Adjusting the parameters of the neural network system using a word sequence from a training set of sequences by a particular author is described in more detail below with reference to FIG. 4 .
- the system receives a new set of word sequences by a particular author (step 304 ).
- the new set of word sequence may include, for example, a sentence, a paragraph, a collection of multiple paragraphs, a search query, or another collection of multiple natural language words.
- the word sequences in the new set of word sequences can each be classified as having been authored by a same author, who may be different from authors of the word sequences that were used in training the neural network system.
- the word sequences in the new set of word sequences may be different from the word sequences that were used in training the neural network system.
- the system determines an author vector that characterizes one or more aspects of the author of the new set of word sequences using the trained neural network system (step 306 ).
- the system processes multiple word sequences from the new set of word sequences using the trained neural network system to determine the author vector.
- each of the sequences in the new set of sequences is a fixed length, e.g., includes the same fixed number of words.
- the system can process each word sequence in the new set of sequences using the trained neural network system in order to iteratively determine the author vector for the author of the new set of sequences. That is, the system adjusts the current author vector after each sequence from the new set of word sequences is processed through the trained neural network system to generate word scores for the sequence. Adjusting an author vector using sequences from the new set of word sequences is described in more detail below with reference to FIG. 5 .
- FIG. 4 is a flow diagram of an example process 400 for training a neural network system on a training sequence of words from a particular author.
- the process 400 will be described as being performed by a system of one or more computers located in one or more locations.
- an author vector system e.g., the author vector system 100 of FIG. 1 , appropriately programmed, can perform the process 400 .
- the system maps each of the words in a training word sequence to a respective word vector representation using the embedding layer (step 402 ).
- the system processes each word in the sequence in accordance with current values of the word parameters to determine a respective word vector representation for each of the words in the sequence.
- the system maps data identifying the author of the word sequence to an author vector that, for example, characterizes one or more aspects of the author's writing using the embedding layer (step 404 ).
- the system processes the data identifying the author in accordance with current values of the author parameters to determine an author vector for the author.
- the system generates a combined representation from the word vector representations and the author vector using the combining layer (step 406 ).
- the system processes the word vector representations and the current author vector using the combining layer to generate the combined representation.
- the combining layer may concatenate the word vector representations and the current author vector to generate the combined representation.
- the combining layer may compute a measure of central tendency, e.g., a mean, median, or other average, of the word vector representations and the current author vector to generate the combined representation.
- the system generates word scores from the combined representation using the classifier layer (step 408 ).
- the system processes the combined representation using the classifier layer and in accordance with current values of the parameters of the classifier layer to generate a respective word score for each word in the predetermined set of words.
- the system computes a gradient using the word scores (step 410 ). For example, the system computes an error between the word scores and the desired (target) output for the sequence of words, e.g., a set of word scores that indicates the word that actually follows the last word in the training word sequence, and then computes the gradient of an objective function using the error.
- the system adjusts current values of the parameters of the neural network system using the gradient (step 412 ).
- the system adjusts the current values of the parameters of the classifier layer using the gradient and then adjusts the current values of the parameters of the embedding layer, e.g., the current values of the author parameters and the word parameters, using backpropagation.
- the system can perform the process 400 for each of multiple training word sequences authored by multiple different authors in order to iteratively determine the trained values of the neural network system. For example, for each iteration of the process 400 , the system can randomly select a fixed-length word sequence from a set of multiple word sequences authored by various authors. The system can then perform iterations of the process 400 on each of the selected sequences until each word sequence has been processed or until other termination criteria for the training have been satisfied.
- FIG. 5 is a flow diagram of an example process 500 for adjusting the author vector representation for a set of word sequences.
- the set of word sequences processed in the process 500 can each be classified as having been authored by a same author, so that the resulting author vector characterizes one or more aspects of that particular author's writing, communication style, or personality.
- the process 500 will be described as being performed by a system of one or more computers located in one or more locations.
- an author vector system e.g., the author vector system 100 of FIG. 1 , appropriately programmed, can perform the process 500 .
- the process 500 can be performed using a trained author vector system, e.g., an author vector system that results from the training process 400 of FIG. 4 .
- the system receives a sequence of words from a new set of word sequences that are each classified as having been authored by the same author (step 502 ).
- the sequences of words in the set of word sequences may have a fixed length.
- the new set of word sequences may include word sequences that were not used in training the neural network system of the author vector system, and may be authored by an author that did not author word sequences used in training the neural network system.
- the new set of word sequences may include word sequences that were authored by an author of word sequences that were used in training the neural network system, e.g., to further refine the author vector for that author.
- the system maps each of the words in the sequence to a respective word vector representation (step 504 ). That is, the system processes each of the words in the sequence using the embedding layer to map each word to a word vector in accordance with trained values of the word parameters of the embedding layer.
- system maps data identifying an author of the word sequence to an author vector (step 506 ). That is, the system processes data identifying the author of the word sequence using the embedding layer to map the data identifying the author to an author vector in accordance with current values of the author parameters.
- the system generates a combined representation using the combining layer (step 506 ).
- the system processes the word vector representations and the author vector using the combining layer to generate the combined representation.
- the combining layer may concatenate the word vector representations and the current author vector to generate the combined representation.
- the combining layer may compute a measure of central tendency, e.g., a mean, median, or other average, of the word vector representations and the current author vector to generate the combined representation.
- the system generates word scores from the combined representation using the classifier layer (step 508 ).
- the system processes the combined representation using the classifier layer and in accordance with the trained values of the parameters of the classifier layer to generate a respective word score for each word in the predetermined set of words.
- the system computes a gradient using the word scores (step 510 ). That is, the system computes an error between the word scores, which indicate a predicted word that follows the last word in the sequence, and the desired output for the sequence of words, e.g., a set of word scores that indicates the word that actually follows the last word in the sequence, and then computes the gradient of an objective function using the error.
- the system adjusts the values of the author parameters in the embedding layer using the gradient (step 512 ). For example, the system holds the trained values of the parameters of the classifier layer and the word parameters fixed and updates the current values of the author parameters using backpropagation.
- the current values of the author parameters after each iteration define the values of the author vector for that iteration.
- the system can associate the author vector with the identifier for the author in a repository or use the author vector for some immediate purpose, such as those discussed above in the paragraphs following the discussion of FIG. 2 herein.
- FIG. 6 is a flow diagram of an example process 600 for determining an author vector from a set of word sequences.
- the process 600 will be described as being performed by a system of one or more computers located in one or more locations.
- an author vector system e.g., the author vector system 200 of FIG. 2 , appropriately programmed, can perform the process 600 .
- the system trains an author vector system, e.g., the author vector system 200 of FIG. 2 , to generate predicted next sequences of words that follow from respective initial sequences of words provided as input to the author vector system (step 602 ).
- the respective initial sequences of words from a training set of initial word sequences can each be classified as being authored by the same author.
- the next word sequence for a given initial word sequence is a sequence that immediately followed the initial word sequence when authored by the author.
- the author vector system generates a respective predicted next sequence for each initial sequence.
- the predicted next sequence is an ordered sequence of words that the author vector system has classified as being the sequence that is most likely to immediately follow the initial sequence.
- the combining subsystem is configured to combine the alternative representation and the author vector representation to generate a combined representation of an initial word sequence and the author of the initial word sequence.
- the decoder LSTM neural network is an LSTM neural network that includes one or more LSTM layers and that is configured to generate a predicted next sequence of words that follows the initial sequence of words given that the initial sequence was authored by the identified author.
- the system adjusts the values of the parameters of the encoder LSTM neural network, the embedding layer, the decoder LSTM neural network, and optionally, the combining subsystem, to determine trained values of these parameters.
- the system processes an initial word sequence to generate a predicted next word sequence and then adjusts the values of the parameters of the author vector system based on an error between the true next word sequence and the predicted next word sequence, e.g., using gradient descent and backpropagation.
- the true next word sequence e.g., from the training data, may be a word sequence that actually follows the initial word sequence in a work authored by the author of the initial word sequence. Adjusting the parameters of the neural network system using a word sequence from a training set of sequences by a particular author is described in more detail below with reference to FIG. 7 .
- the system receives a new set of multiple word sequences that have been classified as being authored by the same author (step 604 ).
- the new set of word sequences can include a set of multiple initial word sequences, and for each initial word sequence, a respective true next word sequence that follows the initial word sequence in one or more works of the author.
- the new set of word sequences may include word sequences that were authored by an author of word sequences that were used in training the system, e.g., to further refine the author vector for that author.
- FIG. 7 is a flow diagram of an example process 700 for training an author vector system on a sequence of words from a particular author.
- the process 700 will be described as being performed by a system of one or more computers located in one or more locations.
- an author vector system e.g., the author vector system 200 of FIG. 2 , appropriately programmed, can perform the process 700 .
- the system obtains training data for training the author vector system (step 702 ).
- the training data can include word sequences that have been classified as being authored by multiple different authors, and can include respective sets of multiple word sequences authored by respective ones of multiple different authors.
- the training data can include a set of initial word sequences of the respective author and, for each initial word sequence, a respective next word sequence that follows (e.g., immediately follows) the initial word sequence in a work of the author.
- the initial sequences and the respective next sequences in the training data can be variable-length sequences, e.g., sequences that can contain varying numbers of words.
- the sequences may be sentences or phrases of varying lengths.
- the number of words in a next sequence may be the same as or different from the number of words in the respective initial sequence in the training data.
- the system processes an initial word sequence from the training data using an encoder LSTM neural network of the author vector system (step 704 ).
- the encoder LSTM neural network processes each word in a given initial word sequence to generate an alternative representation of the initial word sequence in accordance with a set of parameters.
- the encoder LSTM neural network 206 can receive each word in the initial word sequence in the order that the words occur in the initial word sequence and, for a given received word, updates the current hidden state of the encoder LSTM neural network by processing the received word, e.g., to modify the current hidden state of the encoder LSTM neural network that has been generated by processing previous words from the initial sequence by processing the current received word.
- the system generates a predicted next sequence of words from the combined representation using a decoder LSTM neural network of the author vector system (step 710 ).
- the values of parameters of the decoder LSTM neural network can be initialized based at least in part on the combined representation.
- the decoder LSTM neural network can then receive a current word in a predicted next sequence and generate a respective output score for each of a set of possible outputs from the current output and in accordance with the current hidden state of the decoder LSTM neural network and current values of a set of parameters.
- the output score for a given output represents the likelihood that the output is the next output in the predicted next sequence, e.g., that the output immediately follows the current output in the predicted next sequence.
- the decoder LSTM neural network also updates the hidden state of the network to generate an updated hidden state.
- the system computes a gradient using the next sequence of words that is predicted to follow the initial sequence of words (step 712 ). For example, the system computes an error between the predicted next sequence generated by the author vector system and the true next sequence indicated by the training data and then computes a gradient of an objective function that is dependent on the error.
- the system processes an initial word sequence from the training data using an encoder LSTM neural network of the author vector system (step 804 ).
- the encoder LSTM neural network processes each word in a given initial word sequence to generate an alternative representation of the initial word sequence in accordance with trained values of a set of parameters of the encoder LSTM neural network.
- the system maps data that identifies the author of the initial word sequence to an author vector using an embedding layer of the author vector system (step 806 ) in accordance with current values of a set of author parameters of the embedding layer.
- the system generates a combined representation reflecting the initial word sequence and the author of the initial word sequence using a combining subsystem of the author vector system (step 808 ).
- the combining subsystem can concatenate, average, or sum the alternative representation and the author vector.
- the combining subsystem can generate the combined representation in accordance with trained values of a set of parameters of the combining subsystem.
- the system generates a predicted next sequence of words from the combined representation using a decoder LSTM neural network of the author vector system (step 810 ).
- the values of parameters of the decoder LSTM neural network can be initialized based at least in part on the combined representation.
- the system computes a gradient using the next sequence of words that is predicted to the follow the initial sequence of words (step 812 ). For example, the system computes an error between the predicted next sequence generated by the author vector system and the true next sequence indicated by the training data and then computes the gradient of an objective function that is dependent on the error.
- the system updates values of the author parameters of the embedding layer by backpropagating the gradients (step 814 ).
- the values of parameters of other components of the author vector system can remain fixed.
- the values of parameters of the other components of the author vector system can be adjusted based on the computed error, e.g., by back-propagating the gradient computed using the error to each of the components of the author vector system.
- the current values of the author parameters after each iteration define the values of the author vector for that iteration.
- the final author vector can be formed from the values of the author parameters after the system has completed processing each of the sequences in the new set of word sequences of the author.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non transitory program carrier for execution by, or to control the operation of, data processing apparatus.
- the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, which is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- the computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.
- data processing apparatus encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- the apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a computer program (which may also be referred to or described as a program, software, a software application, a module, a software module, a script, or code) can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub programs, or portions of code.
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- special purpose logic circuitry e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- Computers suitable for the execution of a computer program include, by way of example, can be based on general or special purpose microprocessors or both, or any other kind of central processing unit.
- a central processing unit will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.
- PDA personal digital assistant
- GPS Global Positioning System
- USB universal serial bus
- Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto optical disks e.g., CD ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Abstract
Description
Claims (17)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/654,660 US11868724B2 (en) | 2015-07-10 | 2022-03-14 | Generating author vectors |
Applications Claiming Priority (5)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201562191120P | 2015-07-10 | 2015-07-10 | |
US15/206,777 US9984062B1 (en) | 2015-07-10 | 2016-07-11 | Generating author vectors |
US15/991,531 US10599770B1 (en) | 2015-07-10 | 2018-05-29 | Generating author vectors |
US16/824,216 US11275895B1 (en) | 2015-07-10 | 2020-03-19 | Generating author vectors |
US17/654,660 US11868724B2 (en) | 2015-07-10 | 2022-03-14 | Generating author vectors |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/824,216 Continuation US11275895B1 (en) | 2015-07-10 | 2020-03-19 | Generating author vectors |
Publications (2)
Publication Number | Publication Date |
---|---|
US20220198145A1 US20220198145A1 (en) | 2022-06-23 |
US11868724B2 true US11868724B2 (en) | 2024-01-09 |
Family
ID=62165966
Family Applications (4)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/206,777 Active US9984062B1 (en) | 2015-07-10 | 2016-07-11 | Generating author vectors |
US15/991,531 Active US10599770B1 (en) | 2015-07-10 | 2018-05-29 | Generating author vectors |
US16/824,216 Active 2036-12-31 US11275895B1 (en) | 2015-07-10 | 2020-03-19 | Generating author vectors |
US17/654,660 Active US11868724B2 (en) | 2015-07-10 | 2022-03-14 | Generating author vectors |
Family Applications Before (3)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/206,777 Active US9984062B1 (en) | 2015-07-10 | 2016-07-11 | Generating author vectors |
US15/991,531 Active US10599770B1 (en) | 2015-07-10 | 2018-05-29 | Generating author vectors |
US16/824,216 Active 2036-12-31 US11275895B1 (en) | 2015-07-10 | 2020-03-19 | Generating author vectors |
Country Status (1)
Country | Link |
---|---|
US (4) | US9984062B1 (en) |
Families Citing this family (16)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11093711B2 (en) * | 2016-09-28 | 2021-08-17 | Microsoft Technology Licensing, Llc | Entity-specific conversational artificial intelligence |
JP6815184B2 (en) * | 2016-12-13 | 2021-01-20 | 株式会社東芝 | Information processing equipment, information processing methods, and information processing programs |
US10380236B1 (en) * | 2017-09-22 | 2019-08-13 | Amazon Technologies, Inc. | Machine learning system for annotating unstructured text |
US10572585B2 (en) * | 2017-11-30 | 2020-02-25 | International Business Machines Coporation | Context-based linguistic analytics in dialogues |
US10606955B2 (en) * | 2018-03-15 | 2020-03-31 | Servicenow, Inc. | Incident matching with vector-based natural language processing |
DE102018210894A1 (en) * | 2018-07-03 | 2020-01-09 | Siemens Aktiengesellschaft | Design and manufacture of a turbomachine blade |
CN109145068B (en) * | 2018-07-12 | 2021-06-04 | 百度在线网络技术（北京）有限公司 | Map updating method and device |
CN109299270B (en) * | 2018-10-30 | 2021-09-28 | 云南电网有限责任公司信息中心 | Text data unsupervised clustering method based on convolutional neural network |
US11144542B2 (en) * | 2018-11-01 | 2021-10-12 | Visa International Service Association | Natural language processing system |
CN110033091B (en) * | 2018-12-13 | 2020-09-01 | 阿里巴巴集团控股有限公司 | Model-based prediction method and device |
US11475277B2 (en) * | 2019-05-16 | 2022-10-18 | Google Llc | Accurate and interpretable classification with hard attention |
US11551006B2 (en) * | 2019-09-09 | 2023-01-10 | International Business Machines Corporation | Removal of personality signatures |
US10783257B1 (en) * | 2019-12-20 | 2020-09-22 | Capital One Services, Llc | Use of word embeddings to locate sensitive text in computer programming scripts |
US11621931B2 (en) * | 2020-03-11 | 2023-04-04 | Prosper Funding LLC | Personality-profiled language modeling for bot |
CN112836482B (en) * | 2021-02-09 | 2024-02-23 | 浙江工商大学 | Method and device for generating problem by sequence generation model based on template |
CN114625876B (en) * | 2022-03-17 | 2024-04-16 | 北京字节跳动网络技术有限公司 | Method for generating author characteristic model, method and device for processing author information |
Citations (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6173262B1 (en) | 1993-10-15 | 2001-01-09 | Lucent Technologies Inc. | Text-to-speech system with automatically trained phrasing rules |
US6505150B2 (en) | 1997-07-02 | 2003-01-07 | Xerox Corporation | Article and method of automatically filtering information retrieval results using test genre |
US20050228236A1 (en) | 2002-10-03 | 2005-10-13 | The University Of Queensland | Method and apparatus for assessing psychiatric or physical disorders |
US20060229882A1 (en) | 2005-03-29 | 2006-10-12 | Pitney Bowes Incorporated | Method and system for modifying printed text to indicate the author's state of mind |
US20070239433A1 (en) | 2006-04-06 | 2007-10-11 | Chaski Carole E | Variables and method for authorship attribution |
US20080281581A1 (en) | 2007-05-07 | 2008-11-13 | Sparta, Inc. | Method of identifying documents with similar properties utilizing principal component analysis |
US20110151416A1 (en) | 1999-11-01 | 2011-06-23 | Kurzweil Cyberart Technologies, Inc., A Delaware Corporation | Poet Personalities |
US8935154B1 (en) | 2012-04-13 | 2015-01-13 | Symantec Corporation | Systems and methods for determining authorship of an unclassified notification message |
US20150046346A1 (en) | 2011-05-06 | 2015-02-12 | Duquesne University Of The Holy Spirit | Authorship Technologies |
US9264387B2 (en) | 2013-02-06 | 2016-02-16 | Msc Intellectual Properties B.V. | System and method for authorship disambiguation and alias resolution in electronic data |
US9519858B2 (en) * | 2013-02-10 | 2016-12-13 | Microsoft Technology Licensing, Llc | Feature-augmented neural networks and applications of same |
-
2016
- 2016-07-11 US US15/206,777 patent/US9984062B1/en active Active
-
2018
- 2018-05-29 US US15/991,531 patent/US10599770B1/en active Active
-
2020
- 2020-03-19 US US16/824,216 patent/US11275895B1/en active Active
-
2022
- 2022-03-14 US US17/654,660 patent/US11868724B2/en active Active
Patent Citations (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6173262B1 (en) | 1993-10-15 | 2001-01-09 | Lucent Technologies Inc. | Text-to-speech system with automatically trained phrasing rules |
US6505150B2 (en) | 1997-07-02 | 2003-01-07 | Xerox Corporation | Article and method of automatically filtering information retrieval results using test genre |
US20110151416A1 (en) | 1999-11-01 | 2011-06-23 | Kurzweil Cyberart Technologies, Inc., A Delaware Corporation | Poet Personalities |
US20050228236A1 (en) | 2002-10-03 | 2005-10-13 | The University Of Queensland | Method and apparatus for assessing psychiatric or physical disorders |
US20060229882A1 (en) | 2005-03-29 | 2006-10-12 | Pitney Bowes Incorporated | Method and system for modifying printed text to indicate the author's state of mind |
US20070239433A1 (en) | 2006-04-06 | 2007-10-11 | Chaski Carole E | Variables and method for authorship attribution |
US20080281581A1 (en) | 2007-05-07 | 2008-11-13 | Sparta, Inc. | Method of identifying documents with similar properties utilizing principal component analysis |
US20150046346A1 (en) | 2011-05-06 | 2015-02-12 | Duquesne University Of The Holy Spirit | Authorship Technologies |
US8935154B1 (en) | 2012-04-13 | 2015-01-13 | Symantec Corporation | Systems and methods for determining authorship of an unclassified notification message |
US9264387B2 (en) | 2013-02-06 | 2016-02-16 | Msc Intellectual Properties B.V. | System and method for authorship disambiguation and alias resolution in electronic data |
US9519858B2 (en) * | 2013-02-10 | 2016-12-13 | Microsoft Technology Licensing, Llc | Feature-augmented neural networks and applications of same |
Non-Patent Citations (23)
Title |
---|
Bengio , et al., "A Neural Probabilistic Language Model", Journal of Machine Learning Research, vol. 3, XP055192871, Jan. 1, 2003, pp. 1137-1155. |
Coyotl-Morales, Rosa Maria , et al., "Authorship attribution using word sequences", Iberoamerican Congress on Pattern Recognition. Springer Berlin Heidelberg, 2006. |
Diab , et al., "A Preliminary Statistical Investigation into the Impace of an N-Gram Analysis Approach Based on World Syntactic Categories Toward Text Author Classification", No. UMIACS-TR-2000-39. Maryland Univ College Park Inst for Advanced Computer Studies, 2000. |
Graves , "Generating sequences with recurrent neural networks", arXiv: 1308.0850v5 [ cs.NE], Jun. 2014, pp. 1-43. |
Hermann , et al., "Multilingual distributed representations without word alignment", ICLR, 2014, Mar. 2014, pp. 1-9. |
Hochreiterand , et al., "Long Short-Term Memory", Neural Computation 9(8), 1997, pp. 1735-1780. |
Hoom, Johan F., et al., "Neural network identification of poets using letter sequences", Literary and Linguistic Computing 14.3, 1999, pp. 311-338. |
Kjell, Bradley , "Authorship attribution of text samples using neural networks and Bayesian classifiers", Systems, Man, and Cybernetics, Humans, Information and Technology. 1994 IEEE International Conference on. vol. 2., 1994, pp. 1660-1664. |
Koppel, Moshe , et al., "Computational methods in authorship attribution", Journal of the American Society for Information Science and Technology 60.1, 2009, pp. 9-26. |
Le , et al., "Distributed Representations of Sentences and Documents", Retrieved from the Internet: URL: http://arxiv.org/abs/1405.4053 [retrieved on Jun. 3, 2015], XP055192720, May 16, 2014, 9 pages. |
Matthews, Robert , et al., "Neural computation in stylometry I: An 22. application to the works of Shakespeare and Fletcher", Literary and Linguistic Computing 8.4, 1993, pp. 203-209. |
Merriam , et al., "Neural computation in stylometry II: An 23. application to the works of Shakespeare and Marlowe", Literary and Linguistic Computing 9 .1, 1994, 6 pages. |
Mikolov , et al., "Distributed Representations of Words and Phrases and their Compositionality", Retrieved from the Internet: URL:http://arxiv.org/abs/1310.4546 [retrievedonJun. 3, 2015], XP055192737, Oct. 16, 2013, 9 pages. |
Mikolov , et al., "Efficient Estimation of Word Representations in Vector Space", Retrieved from the Internet: URL: http://arxiv.org/abs/1301.3781 [retrieved on Jun. 3, 2015], XP055192736, Jan. 16, 2013, 12 pages. |
Mikolov , et al., "Exploiting Similarities among Languages for Machine Translation", Retrieved from the Internet: URL: http://arxiv.org/abs/1309.4168 [retrieved on Jun. 3, 2015], XP055192735, Sep. 16, 2013, 10 pages. |
Mikolov , et al., "Extensions of recurrent neural network language model", ICASSP, May 2011, pp. 5528-5531. |
Mikolov , et al., "Recurrent neural network based language model", Interspeech, Sep. 2010, pp. 1045-1048. |
Peng , et al., "Augmenting naive bayes classifiers with statistical language models", Information Retrieval 7.3, 2004, pp. 317-345. |
Rumelhart , et al., "Learning representations by back-propagating errors", Nature, 323(6088), Oct. 1986, pp. 533-536. |
Schwenk , et al., "Connectionist language modeling for large vocabulary continuous speech recognition", 2002 IEEE International Conference on Acoustics, Speech, and Signal Processing, Proceedings, (ICASSP), May 2002, pp. I-765-I-768. |
Stanczyk, Urszula , et al., "Machine learning approach to authorship attribution of literary texts", International Journal of Applied Mathematics and Informatics 1.4, 2007, pp. 151-158. |
Sutskever , et al., "Sequence to Sequence Learning with Neural Networks", arXiv:1409.3215v3 [cs.CL], Dec. 2014, 9 pages. |
U.S. Appl. No. 16/824,216, filed Mar. 19, 2020, Allowed. |
Also Published As
Publication number | Publication date |
---|---|
US9984062B1 (en) | 2018-05-29 |
US10599770B1 (en) | 2020-03-24 |
US11275895B1 (en) | 2022-03-15 |
US20220198145A1 (en) | 2022-06-23 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11868724B2 (en) | Generating author vectors | |
US11853879B2 (en) | Generating vector representations of documents | |
US11960519B2 (en) | Classifying data objects | |
CN105144164B (en) | Scoring concept terms using a deep network | |
US10642846B2 (en) | Using a generative adversarial network for query-keyword matching | |
US11669744B2 (en) | Regularized neural network architecture search | |
US11550871B1 (en) | Processing structured documents using convolutional neural networks | |
US9449271B2 (en) | Classifying resources using a deep network | |
US20160378863A1 (en) | Selecting representative video frames for videos | |
US11443170B2 (en) | Semi-supervised training of neural networks | |
US20190164084A1 (en) | Method of and system for generating prediction quality parameter for a prediction model executed in a machine learning algorithm | |
US10803380B2 (en) | Generating vector representations of documents | |
US20170228643A1 (en) | Augmenting Neural Networks With Hierarchical External Memory | |
US10460229B1 (en) | Determining word senses using neural networks | |
JP2020512651A (en) | Search method, device, and non-transitory computer-readable storage medium | |
US20230029590A1 (en) | Evaluating output sequences using an auto-regressive language model neural network | |
US20190147365A1 (en) | Deep vector table machine systems |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
FEPP | Fee payment procedure |
Free format text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ENTITY CONVERSION;ASSIGNOR:GOOGLE INC.;REEL/FRAME:059746/0029Effective date: 20170929Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:STROPE, BRIAN PATRICK;LE, QUOC V.;SIGNING DATES FROM 20160826 TO 20160828;REEL/FRAME:059657/0251 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT RECEIVED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: AWAITING TC RESP, ISSUE FEE PAYMENT VERIFIED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |