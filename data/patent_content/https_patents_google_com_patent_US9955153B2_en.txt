US9955153B2 - Devices and methods for sample adaptive offset coding - Google Patents
Devices and methods for sample adaptive offset coding Download PDFInfo
- Publication number
- US9955153B2 US9955153B2 US13/734,778 US201313734778A US9955153B2 US 9955153 B2 US9955153 B2 US 9955153B2 US 201313734778 A US201313734778 A US 201313734778A US 9955153 B2 US9955153 B2 US 9955153B2
- Authority
- US
- United States
- Prior art keywords
- sao
- pixel
- compensation
- sao compensation
- pixels
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- H04N19/00127—
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/80—Details of filtering operations specially adapted for video compression, e.g. for pixel interpolation
- H04N19/82—Details of filtering operations specially adapted for video compression, e.g. for pixel interpolation involving filtering within a prediction loop
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/117—Filters, e.g. for pre-processing or post-processing
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/132—Sampling, masking or truncation of coding units, e.g. adaptive resampling, frame skipping, frame interpolation or high-frequency transform coefficient masking
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
- H04N19/136—Incoming video signal characteristics or properties
- H04N19/14—Coding unit complexity, e.g. amount of activity or edge presence estimation
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/182—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being a pixel
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/189—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the adaptation method, adaptation tool or adaptation type used for the adaptive coding
- H04N19/192—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the adaptation method, adaptation tool or adaptation type used for the adaptive coding the adaptation method, adaptation tool or adaptation type being iterative or recursive
Definitions
- the disclosure relates generally to the field of video coding, and more specifically to systems, devices and methods for sample adaptive offset (SAO) coding.
- SAO sample adaptive offset
- Video compression uses block processing for many operations.
- a block of neighboring pixels is grouped into a coding unit and compression operations treat this group of pixels as one unit to take advantage of correlations among neighboring pixels within the coding unit.
- Block-based processing often includes prediction coding and transform coding.
- Transform coding with quantization is a type of data compression which is commonly “lossy” as the quantization of a transform block taken from a source picture often discards data associated with the transform block in the source picture, thereby lowering its bandwidth requirement but often also resulting in quality loss in reproducing of the original transform block from the source picture.
- MPEG-4 AVC also known as H.264
- H.264 is an established video compression standard that uses transform coding in block processing.
- a picture is divided into macroblocks (MBs) of 16 ⁇ 16 pixels.
- MB macroblocks
- Each MB is often further divided into smaller blocks.
- Blocks equal in size to or smaller than a MB are predicted using intra-/inter-picture prediction, and a spatial transform along with quantization is applied to the prediction residuals.
- the quantized transform coefficients of the residuals are commonly encoded using entropy coding methods (e.g., variable length coding or arithmetic coding).
- Context Adaptive Binary Arithmetic Coding was introduced in H.264 to provide a substantially lossless compression efficiency by combining an adaptive binary arithmetic coding technique with a set of context models.
- Context model selection plays a role in CABAC in providing a degree of adaptation and redundancy reduction.
- H.264 specifies two kinds of scan patterns over 2D blocks. A zigzag scan is used for pictures coded with progressive video compression techniques and an alternative scan is for pictures coded with interlaced video compression techniques.
- HEVC High Efficiency Video Coding
- HD high definition
- FIG. 1A is a video system in which the various embodiments of the disclosure may be used
- FIG. 1B is a computer system on which embodiments of the disclosure may be implemented
- FIGS. 2A, 2B, 3A and 3B illustrate certain video encoding principles according to embodiments of the disclosure
- FIGS. 4A and 4B show possible architectures for an encoder and a decoder according to embodiments of the disclosure
- FIGS. 5A and 5B illustrate further video coding principles according to an embodiments of the disclosure
- FIG. 6 illustrates 5 possible edge offset (EO) sub-classes for sample adaptive offset (SAO) according to embodiments of the disclosure
- FIG. 7 illustrates an example of a segmented line formed by three pixel values or points L, C, and R according to embodiments of the disclosure
- FIG. 8 illustrates an example of a segmented line formed by three pixel values or points L, C, and R according to embodiments of the disclosure
- FIG. 9 illustrates an example band offset specification according to embodiments of the disclosure.
- FIG. 10 illustrates an example band offset specification having a distribution of values according to embodiments of the disclosure
- FIG. 11 illustrates an example band offset specification according to embodiments of the disclosure
- FIG. 12 illustrates an example architecture for coding of offsets according to embodiments of the disclosure
- FIG. 13 illustrates an example band offset specification according to embodiments of the disclosure.
- FIG. 14 illustrates an example block diagram showing a generic multipass SAO process according to embodiments of the disclosure.
- a method for decoding a video bitstream having a plurality of pictures, the bitstream generated by a video coding system with sample adaptive offset (SAO), comprising the steps of: obtaining processed video data from a video bitstream; partitioning the processed video data into blocks, wherein each of the blocks is equal to or smaller than a picture and each block is comprised of a plurality of pixels; applying a first SAO compensation to each of the pixels in a processed video block; and applying a second SAO compensation to each of the pixels in the processed video block.
- the first SAO compensation and second SAO compensation are predetermined.
- the method further comprises: deriving an SAO type from the video bitstream for each of the blocks.
- the first SAO compensation is based on a horizontal edge offset (EO) type and the second SAO compensation is based on a vertical EO type.
- the first SAO compensation and second SAO compensation are orthogonal to each other.
- the method further comprises: determining an SAO sub-class associated with the SAO type for each of the pixels in each of the blocks; and deriving intensity offset from the video bitstream for the sub-class associated with the SAO type; wherein at least one of the first or second SAO compensations is based on the derived intensity offset information.
- the first and second SAO compensations are applied to pixels that have been deblocked.
- at least one of the first and second SAO compensations are applied to pixels that have been deblocked and have previously had SAO compensation applied.
- the pixels that have been deblocked and have previously had SAO applied are used in determining an SAO sub-class associated with the SAO type for each of the pixels in each of the blocks and deriving intensity offset from the video bitstream for the sub-class associated with the SAO type.
- the method further comprises: applying a third SAO compensation to each of the pixels in the processed video block.
- two of the compensations relate to edge offset (EO) types and wherein one of the compensations relate to a band offset (BO) type.
- each color component in a processed video block has its own SAO process for the first and second compensations.
- the method is implemented on a computer having a processor and a memory coupled to said processor, wherein at least some of steps are performed using said processor.
- an apparatus for decoding a video bitstream having a plurality of pictures, the bitstream generated by a video coding system with sample adaptive offset (SAO), comprising a video decoder configured to: obtain processed video data from a video bitstream; partition the processed video data into blocks, wherein each of the blocks is equal to or smaller than a picture and each block is comprised of a plurality of pixels; apply a first SAO compensation to each of the pixels in a processed video block; and apply a second SAO compensation to each of the pixels in the processed video block.
- the apparatus comprises at least one of: an integrated circuit; a microprocessor; and a wireless communication device that includes the video decoder.
- the video decoder is further configured to: derive an SAO type from the video bitstream for each of the blocks. In an embodiment of the second aspect, if the derived SAO type is edge offset, the first SAO compensation and second SAO compensation are orthogonal to each other. In an embodiment of the second aspect, the first and second SAO compensations are applied to pixels that have been deblocked. In an embodiment of the second aspect, at least one of the first and second SAO compensations are applied to pixels that have been deblocked and have previously had SAO compensation applied. In an embodiment of the second aspect, the video decoder is further configured to: apply a third SAO compensation to each of the pixels in the processed video block.
- a method of encoding video data having a plurality of pictures, using sample adaptive offset (SAO), comprising the steps of: partitioning video data into blocks, wherein each of the blocks is equal to or smaller than a picture and each block is comprised of a plurality of pixels; applying a first SAO compensation to each of the pixels in a processed video block; and applying a second SAO compensation to each of the pixels in the processed video block.
- the method further comprises: selecting an SAO type for the video data for each of the blocks; selecting an SAO sub-class associated with the SAO type for each of the pixels in each of the blocks; and deriving intensity offset for the video data for the sub-class associated with the SAO type.
- At least one of the first or second SAO compensations is based on the derived intensity offset information.
- the first and second SAO compensations are applied to pixels that have been deblocked.
- at least one of the first and second SAO compensations is applied to pixels that have been deblocked and have previously had SAO compensation applied.
- the method further comprises: applying a third SAO compensation to each of the pixels in the processed video block.
- the method is implemented on a computer having a processor and a memory coupled to said processor, wherein at least some of steps are performed using said processor.
- an apparatus for encoding a video data having a plurality of pictures, using sample adaptive offset (SAO), comprising a video encoder configured to: partition video data into blocks, wherein each of the blocks is equal to or smaller than a picture and each block is comprised of a plurality of pixels; apply a first SAO compensation to each of the pixels in a processed video block; and apply a second SAO compensation to each of the pixels in the processed video block.
- SAO sample adaptive offset
- coding refers to encoding that occurs at the encoder or decoding that occurs at the decoder.
- coder refers to an encoder, a decoder, or a combined encoder/decoder (CODEC).
- CODEC combined encoder/decoder
- a video system may include a head end 100 of a cable television network.
- the head end 100 may be configured to deliver video content to neighborhoods 129 , 130 and 131 .
- the head end 100 may operate within a hierarchy of head ends, with the head ends higher in the hierarchy generally having greater functionality.
- the head end 100 may be communicatively linked to a satellite dish 112 and receive video signals for non-local programming from it.
- the head end 100 may also be communicatively linked to a local station 114 that delivers local programming to the head end 100 .
- the head end 100 may include a decoder 104 that decodes the video signals received from the satellite dish 112 , an off-air receiver 106 that receives the local programming from the local station 114 , a switcher 102 that routes data traffic among the various components of the head end 100 , encoders 116 that encode video signals for delivery to customers, modulators 118 that modulate signals for delivery to customers, and a combiner 120 that combines the various signals into a single, multi-channel transmission.
- a decoder 104 that decodes the video signals received from the satellite dish 112
- an off-air receiver 106 that receives the local programming from the local station 114
- a switcher 102 that routes data traffic among the various components of the head end 100
- encoders 116 that encode video signals for delivery to customers
- modulators 118 that modulate signals for delivery to customers
- a combiner 120 that combines the various signals into a single, multi-channel transmission.
- the head end 100 may also be communicatively linked to a hybrid fiber cable (HFC) network 122 .
- the HFC network 122 may be communicatively linked to a plurality of nodes 124 , 126 , and 128 .
- Each of the nodes 124 , 126 , and 128 may be linked by coaxial cable to one of the neighborhoods 129 , 130 and 131 and deliver cable television signals to that neighborhood.
- One of the neighborhoods 130 of FIG. 1A is shown in more detail.
- the neighborhood 130 may include a number of residences, including a home 132 shown in FIG. 1A . Within the home 132 may be a set-top box 134 communicatively linked to a video display 136 .
- the set-top box 134 may include a first decoder 138 and a second decoder 140 .
- the first and second decoders 138 and 140 may be communicatively linked to a user interface 142 and a mass storage device 144 .
- the user interface 142 may be communicatively linked to the video display 136 .
- head end 100 may receive local and nonlocal programming video signals from the satellite dish 112 and the local station 114 .
- the nonlocal programming video signals may be received in the form of a digital video stream, while the local programming video signals may be received as an analog video stream.
- local programming may also be received as a digital video stream.
- the digital video stream may be decoded by the decoder 104 and sent to the switcher 102 in response to customer requests.
- the head end 100 may also include a server 108 communicatively linked to a mass storage device 110 .
- the mass storage device 110 may store various types of video content, including video on demand (VOD), which the server 108 may retrieve and provide to the switcher 102 .
- VOD video on demand
- the switcher 102 may route local programming directly to the modulators 118 , which modulate the local programming, and route the non-local programming (including any VOD) to the encoders 116 .
- the encoders 116 may digitally encode the non-local programming.
- the encoded non-local programming may then be transmitted to the modulators 118 .
- the combiner 120 may be configured to receive the modulated analog video data and the modulated digital video data, combine the video data and transmit it via multiple radio frequency (RF) channels to the HFC network 122 .
- RF radio frequency
- the HFC network 122 may transmit the combined video data to the nodes 124 , 126 and 128 , which may retransmit the data to their respective neighborhoods 129 , 130 and 131 .
- the home 132 may receive this video data at the set-top box 134 , more specifically at the first decoder 138 and the second decoder 140 .
- the first and second decoders 138 and 140 may decode the digital portion of the video data and provide the decoded data to the user interface 142 , which then may provide the decoded data to the video display 136 .
- the encoders 116 and the decoders 138 and 140 of FIG. 1A may be implemented as computer code comprising computer readable instructions stored on a computer readable storage device, such as memory or another type of storage device.
- the computer code may be executed on a computer system by a processor, such as an application-specific integrated circuit (ASIC), or other type of circuit.
- ASIC application-specific integrated circuit
- computer code for implementing the encoders 116 may be executed on a computer system (such as a server) residing in the headend 100 .
- Computer code for the decoders 138 and 140 may be executed on the set-top box 134 , which constitutes a type of computer system.
- the code may exist as software programs comprised of program instructions in source code, object code, executable code or other formats. It should be appreciated that the computer code for the various components shown in FIG. 1A may reside anywhere in system 10 or elsewhere (such as in a cloud network), that is determined to be desirable or advantageous. Furthermore, the computer code may be located in one or more components, provided the instructions may be effectively performed by the one or more components.
- FIG. 1B shows an example of a computer system on which computer code for the encoders 116 and the decoders 138 and 140 may be executed.
- the computer system generally labeled 400 , includes a processor 401 , or processing circuitry, that may implement or execute software instructions performing some or all of the methods, functions and other steps described herein. Commands and data from processor 401 may be communicated over a communication bus 403 , for example.
- Computer system 400 may also include a computer readable storage device 402 , such as random access memory (RAM), where the software and data for processor 401 may reside during runtime. Storage device 402 may also include non-volatile data storage.
- Computer system 400 may include a network interface 404 for connecting to a network.
- the computer system 400 may reside in the headend 100 and execute the encoders 116 , and may also be embodied in the set-top box 134 to execute the decoders 138 and 140 . Additionally, the computer system 400 may reside in places other than the headend 100 and the set-top box 134 , and may be miniaturized so as to be integrated into a smartphone or tablet computer.
- Video encoding systems achieve compression by removing redundancy in the video data, e.g., by removing those elements that can be discarded without adversely affecting reproduction fidelity. Because video signals take place in time and space, most video encoding systems exploit both temporal and spatial redundancy present in these signals. Typically, there is high temporal correlation between successive frames. This is also true in the spatial domain for pixels which are close to each other. Thus, high compression gains are achieved by carefully exploiting these spatio-temporal correlations.
- HEVC High Efficiency Video Coding
- LCUs largest coding units
- CTUs coding tree units
- An LCU can be divided into four square blocks, called CUs (coding units), which are a quarter of the size of the LCU.
- CUs coding units
- Each CU can be further split into four smaller CUs, which are a quarter of the size of the original CU. The splitting process can be repeated until certain criteria are met.
- FIG. 3A shows an example of LCU partitioned into CUs.
- a flag is set to “1” if the node is further split into sub-nodes. Otherwise, the flag is unset at “0.”
- the LCU partition of FIG. 3A can be represented by the quadtree of FIG. 3B .
- These “split flags” may be jointly coded with other flags in the video bitstream, including a skip mode flag, a merge mode flag, and a predictive unit (PU) mode flag, and the like.
- the split flags 10100 could be coded as overhead along with the other flags. Syntax information for a given CU may be defined recursively, and may depend on whether the CU is split into sub-CUs.
- a CU that is not split may include one or more prediction units (PUs).
- PUs prediction units
- a PU represents all or a portion of the corresponding CU, and includes data for retrieving a reference sample for the PU for purposes of performing prediction for the CU.
- a final CU of 2N ⁇ 2N can possess one of four possible patterns (N ⁇ N, N ⁇ 2N, 2N ⁇ N and 2N ⁇ 2N), as shown in FIG. 2B . While shown for a 2N ⁇ 2N CU, other PUs having different dimensions and corresponding patterns (e.g., square or rectangular) may be used.
- a CU can be either spatially or temporally predictive coded. If a CU is coded in intra mode, each PU of the CU can have its own spatial prediction direction. If a CU is coded in inter mode, each PU of the CU can have its own motion vector(s) and associated reference picture(s).
- the data defining the motion vector may describe, for example, a horizontal component of the motion vector, a vertical component of the motion vector, a resolution for the motion vector (e.g., one-quarter pixel precision or one-eighth pixel precision), a reference frame to which the motion vector points, and/or a reference list (e.g., list 0 or list 1 ) for the motion vector.
- Data for the CU defining the one or more PUs of the CU may also describe, for example, partitioning of the CU into the one or more PUs. Partitioning modes may differ between whether the CU is uncoded, intra-prediction mode encoded, or inter-prediction mode encoded.
- the prediction can be formed by a weighted average of the previously encoded samples, located above and to the left of the current block.
- the encoder may select the mode that minimizes the difference or cost between the original and the prediction and signals this selection in the control data.
- inter-prediction encoding video sequences have high temporal correlation between frames, enabling a block in the current frame to be accurately described by a region in the previous coded frames, which are known as reference frames.
- Inter-prediction utilizes previously encoded and reconstructed reference frames to develop a prediction using a block-based motion estimation and compensation technique.
- Quantization generally refers to a process in which transform coefficients are quantized to possibly reduce the amount of data used to represent the coefficients, e.g., by converting high precision transform coefficients into a finite number of possible values.
- Each CU can also be divided into transform units (TUs) by application of a block transform operation.
- a block transform operation tends to decorrelate the pixels within the block and compact the block energy into the low order coefficients of the transform block.
- one transform of 8 ⁇ 8 or 4 ⁇ 4 may be applied.
- a set of block transforms of different sizes may be applied to a CU, as shown in FIG. 5A where the left block is a CU partitioned into PUs and the right block is the associated set of transform units (TUs).
- the size and location of each block transform within a CU is described by a separate quadtree, called RQT.
- FIG. 5B shows the quadtree representation of TUs for the CU in the example of FIG. 5A . In this example, 11000 is coded and transmitted as part of the overhead.
- the TUs and PUs of any given CU may be used for different purposes.
- TUs are typically used for transformation, quantizing and coding operations
- PUs are typically used for spatial and temporal prediction. There is not necessarily a direct relationship between the number of PUs and the number of TUs for a given CU.
- Video blocks may comprise blocks of pixel data in the pixel domain, or blocks of transform coefficients in the transform domain, e.g., following application of a transform, such as a discrete cosine transform (DCT), an integer transform, a wavelet transform, or a conceptually similar transform to residual data for a given video block, wherein the residual data represents pixel differences between video data for the block and predictive data generated for the block.
- video blocks may comprise blocks of quantized transform coefficients in the transform domain, wherein, following application of a transform to residual data for a given video block, the resulting transform coefficients are also quantized.
- quantization is the step that introduces loss, so that a balance between bitrate and reconstruction quality can be established.
- Block partitioning serves an important purpose in block-based video coding techniques. Using smaller blocks to code video data may result in better prediction of the data for locations of a video frame that include high levels of detail, and may therefore reduce the resulting error (e.g., deviation of the prediction data from source video data), represented as residual data.
- prediction exploits the spatial or temporal redundancy in a video sequence by modeling the correlation between sample blocks of various dimensions, such that only a small difference between the actual and the predicted signal needs to be encoded. A prediction for the current block is created from the samples which have already been encoded. While potentially reducing the residual data, such techniques may, however, require additional syntax information to indicate how the smaller blocks are partitioned relative to a video frame, and may result in an increased coded video bitrate. Accordingly, in some techniques, block partitioning may depend on balancing the desirable reduction in residual data against the resulting increase in bitrate of the coded video data due to the additional syntax information.
- blocks and the various partitions thereof may be considered video blocks.
- a slice may be considered to be a plurality of video blocks (e.g., macroblocks, or coding units), and/or sub-blocks (partitions of macroblocks, or sub-coding units).
- Each slice may be an independently decodable unit of a video frame.
- frames themselves may be decodable units, or other portions of a frame may be defined as decodable units.
- a GOP also referred to as a group of pictures, may be defined as a decodable unit.
- the encoders 116 may be, according to an embodiment of the disclosure, composed of several functional modules as shown in FIG. 4A . These modules may be implemented as hardware, software, or any combination of the two. Given a current PU, x, a prediction PU, x′, may first be obtained through either spatial prediction or temporal prediction. This spatial or temporal prediction may be performed by a spatial prediction module 129 or a temporal prediction module 130 respectively.
- the spatial prediction module 129 can perform per PU, including horizontal, vertical, 45-degree diagonal, 135-degree diagonal, DC, Planar, etc.
- an additional mode called IntraFromLuma
- a syntax indicates the spatial prediction direction per PU.
- the encoder 116 may perform temporal prediction through motion estimation operation. Specifically, the temporal prediction module 130 ( FIG. 4A ) may search for a best match prediction for the current PU over reference pictures.
- the best match prediction may be described by motion vector (MV) and associated reference picture (refIdx).
- MV motion vector
- refIdx reference picture
- a PU in B pictures can have up to two MVs. Both MV and refIdx may be part of the syntax in the bitstream.
- the prediction PU may then be subtracted from the current PU, resulting in the residual PU, e.
- the residual PU, e may then be transformed by a transform module 117 , one transform unit (TU) at a time, resulting in the residual PU in the transform domain, E.
- the transform module 117 may use e.g., either a square or a non-square block transform.
- the transform coefficients E may then be quantized by a quantizer module 118 , converting the high precision transform coefficients into a finite number of possible values.
- the quantization process may reduce the bit depth associated with some or all of the coefficients. For example, an n-bit value may be rounded down to an m-bit value during quantization, where n is greater than m.
- external boundary conditions are used to produce modified one or more transform coefficients. For example, a lower range or value may be used in determining if a transform coefficient is given a nonzero value or just zeroed out.
- quantization is a lossy operation and the loss by quantization generally cannot be recovered.
- the quantized coefficients may then be entropy coded by an entropy coding module 120 , resulting in the final compression bits.
- the specific steps performed by the entropy coding module 120 will be discussed below in more detail.
- the encoder 116 may also take the quantized transform coefficients E and dequantize them with a dequantizer module 122 resulting in the dequantized transform coefficients E′.
- the dequantized transform coefficients are then inverse transformed by an inverse transform module 124 , resulting in the reconstructed residual PU, e′.
- the reconstructed residual PU, e′ is then added to the corresponding prediction, x′, either spatial or temporal, to form a reconstructed PU, x′′.
- a deblocking filter (DBF) operation may be performed on the reconstructed PU, x′′, first to reduce blocking artifacts.
- a sample adaptive offset (SAO) process may be conditionally performed after the completion of the deblocking filter process for the decoded picture, which compensates the pixel value offset between reconstructed pixels and original pixels.
- both the DBF operation and SAO process are implemented by adaptive loop filter functions, which may be performed conditionally by a loop filter module 126 over the reconstructed PU.
- the adaptive loop filter functions minimize the coding distortion between the input and output pictures.
- loop filter module 126 operates during an inter-picture prediction loop. If the reconstructed pictures are reference pictures, they may be stored in a reference buffer 128 for future temporal prediction.
- HEVC specifies two loop filters that are applied in order with the de-blocking filter (DBF) applied first and the sample adaptive offset (SAO) filter applied afterwards.
- the DBF is similar to the one used by H.264/MPEG-4 AVC but with a simpler design and better support for parallel processing.
- the DBF only applies to an 8 ⁇ 8 sample grid while with H.264/MPEG-4 AVC the DBF applies to a 4 ⁇ 4 sample grid.
- DBF uses an 8 ⁇ 8 sample grid since it causes no noticeable degradation and significantly improves parallel processing because the DBF no longer causes cascading interactions with other operations.
- Another change is that HEVC only allows for three DBF strengths of 0 to 2.
- HEVC also requires that the DBF first apply horizontal filtering for vertical edges to the picture and only after that does it apply vertical filtering for horizontal edges to the picture. This allows for multiple parallel threads to be used for the DBF.
- the SAO filter process is applied after the DBF and is made to allow for better reconstruction of the original signal amplitudes by using e.g., a look up table that includes some parameters that are based on a histogram analysis made by the encoder.
- the SAO filter has two basic types which are the edge offset (EO) type and the band offset (BO) type.
- One of the SAO types can be applied per coding tree block (CTB).
- the edge offset (EO) type has four sub-types corresponding to processing along four possible directions (e.g., horizontal, vertical, 135 degree, and 45 degree). For a given EO sub-type, the edge offset (EO) processing operates by comparing the value of a pixel to two of its neighbors using one of four different gradient patterns.
- An offset is applied to pixels in each of the four gradient patterns. For pixel values that are not in one of the gradient patterns, no offset is applied.
- the band offset (BO) processing is based directly on the sample amplitude which is split into 32 bands.
- An offset is applied to pixels in 16 of the 32 bands, where a group of 16 bands corresponds to a BO sub-type.
- the SAO filter process was designed to reduce distortion compared to the original signal by adding an offset to sample values. It can increase edge sharpness and reduce ringing and impulse artifacts. Further detail on the SAO process will be discussed below with reference to FIGS. 6-14 .
- intra pictures such as an I picture
- inter pictures such as P pictures or B pictures
- An intra picture may be coded without referring to other pictures.
- spatial prediction may be used for a CU/PU inside an intra picture.
- An intra picture provides a possible point where decoding can begin.
- an inter picture generally aims for high compression.
- Inter picture supports both intra and inter prediction.
- a CU/PU in inter picture is either spatially or temporally predictive coded. Temporal references are the previously coded intra or inter pictures.
- the entropy coding module 120 takes the quantized matrix of coefficients received from the quantizer module 118 and uses it to generate a sign matrix that represents the signs of all of the quantized coefficients and to generate a significance map.
- a significance map may be a matrix in which each element specifies the position(s) of the non-zero quantized coefficient(s) within the quantized coefficient matrix. Specifically, given a quantized 2D transformed matrix, if the value of a quantized coefficient at a position (y, x) is non-zero, it may be considered as significant and a “1” is assigned for the position (y, x) in the associated significance map. Otherwise, a “0” is assigned to the position (y, x) in the significance map.
- the entropy coding module 120 may code the significance map. In one embodiment, this is accomplished by using a context-based adaptive binary arithmetic coding (CABAC) technique. In doing so, the entropy coding module 120 scans the significance map along a scanning line and, for each entry in the significance map, the coding module chooses a context model for that entry. The entropy coding module 120 then codes the entry based on the chosen context model. That is, each entry is assigned a probability based on the context model (the mathematical probability model) being used. The probabilities are accumulated until the entire significance map has been encoded.
- CABAC context-based adaptive binary arithmetic coding
- the value output by the entropy coding module 120 as well as the entropy encoded signs, significance map and non-zero coefficients may be inserted into the bitstream by the encoder 116 ( FIG. 1A ). This bitstream may be sent to the decoders 138 and 140 over the HFC network 122 .
- prediction, transform, and quantization described above may be performed for any block of video data, e.g., to a PU and/or TU of a CU, or to a macroblock, depending on the specified coding standard.
- An entropy decoding module 146 of the decoder 145 may decode the sign values, significance map and non-zero coefficients to recreate the quantized and transformed coefficients.
- the entropy decoding module 146 may perform the reverse of the procedure described in conjunction with the entropy coding module 120 —decoding the significance map along a scanning pattern made up of scanning lines.
- the entropy decoding module 146 then may provide the coefficients to a dequantizer module 147 , which dequantizes the matrix of coefficients, resulting in E′.
- the dequantizer module 147 may provide the dequantized coefficients to an inverse transform module 149 .
- the inverse transform module 149 may perform an inverse transform operation on the coefficients resulting in e′. Filtering and spatial prediction may be applied in a manner described in conjunction with FIG. 4A .
- an encoder categorizes the pixels into one of six possible types (both types and sub-types are collectively referred to as types here): four edges offset (EO) types E 0 , E 1 , E 2 , E 3 and two band offset (BO) types B 0 , B 1 .
- EO edges offset
- BO band offset
- the pixels are further sub-categorized into one of five possible sub-classes based upon local behavior along the EO type direction. These five sub-classes are described in further detail below.
- the pixels are further sub-categorized into one of sixteen possible sub-classes based upon intensity.
- the same offset is applied for a given sub-class of pixels within an SAO type. For example, if the offset for sub-class i is o i , then the SAO output corresponding to an input of p i will be p i +o i .
- the encoder typically selects the SAO type per sub-class to minimize a cost function.
- the encoder may signal to the decoder the SAO type per partition and the corresponding offsets per sub-class, and the decoder may perform the classification for the SAO type and applies the offsets per sub-class to each pixel.
- the SAO type can be signaled per color component, or a given type can be signaled and used for more than one color component. In some embodiments, it is also possible for the encoder to not use or turn off SAO, and this can also be signaled to the decoder.
- HE high efficiency
- LC low complexity
- VLCs variable length codewords
- CABAC context-based adaptive binary arithmetic coding
- an encoder may signal the SAO type using a unary code, for example (0's and 1's can be interchanged) as shown in Table 1:
- units or digits within a codeword may be referred to as “bits” for LC and “bins” for HE.
- bits for LC
- bins for HE.
- the difference in terminology is a result of applying CABAC to the codeword in the HE method.
- units includes both bins and bits in codewords.
- EO type or class refers to the direction along where pixels will be processed
- sub-class refers to the categorization of pixel values according to the gradient pattern along the EO type or class direction.
- the 5 possible EO sub-classes per type e.g., sub-class 0 - 4
- FIG. 6 The 5 possible EO sub-classes per type (e.g., sub-class 0 - 4 ) are illustrated in FIG. 6 , where the 3 pixels indicate whether neighboring pixels are greater than, less than, or equal to the current pixel.
- the specific numbering of sub-classes in FIG. 6 is shown for illustration purposes only and other numberings are also contemplated.
- Line 1 is defined by three points or pixel values: left neighbor pixel value (L), current pixel value (C) and right neighbor pixel value (R).
- Line 1 therefore illustrates a current pixel value C with a left neighboring pixel L having a smaller value and a right neighboring pixel R having a greater value than C.
- Line 2 illustrates a current pixel value C with a left neighboring pixel L having a higher value and a right neighboring pixel R having a smaller value than C.
- Line 3 illustrates a current pixel value C with a left neighboring pixel L having an equal value and a right neighboring pixel R having an equal value to C.
- the current pixel value C is generally lower than its neighboring pixels L and R, so a positive offset value to increase the value of C may be applied.
- the current pixel value C is generally greater than its neighboring pixels L and R, so a negative offset value to decrease the value of C may be applied.
- a predicted or interpolated current pixel value (I) may be generated using a weighted combination of its two neighbors, L and R. For example, if 2*C>(L+R), then the interpolated pixel value (I) can be computed as (L+R+1)>>1 (round to +infinity). If 2*C ⁇ (L+R), then the interpolated pixel value (I) can be computed as (L+R)>>1 (round to ⁇ infinity).
- the interpolated current pixel value I is an average of its two neighbors, but in general it can be a weighted combination or function of one or more neighboring values.
- FIG. 7 illustrates an example of a segmented line formed by L, C, and R and the relationship of I to the segmented line.
- any function that takes into account one or more neighboring pixel values to generate I may be used. Additionally, the neighboring pixel values need not be the immediate neighboring pixels.
- using the interpolated current pixel value (I) serves as a smoothing function because it operates as a limit defined by a function (e.g., average) of its neighbors. This, in turn, allows the offset (O) to serve efficiently both as a noise threshold to reduce variations caused by noise and as a signal enhancer to restore signal values.
- offsets (O) may be allowed to be applied both towards and away from the segmented line formed by (L, I, R or L, C, R), such that the offset can be defined to be a signed value, where the sign can indicate the direction towards or away from the line, and the magnitude indicates the amount of movement.
- offsets are only allowed to be applied in the direction towards the segmented line formed by (L, I, R), thus allowing a single sign classification (e.g., defining the offset as non-negative). In such instances, only the offset magnitude needs to be transmitted.
- the current pixel value (C) may not be available or is hard to retrieve e.g., LCU boundary pixels.
- the predicted or interpolated pixel value (I) can be derived from the available pixels e.g., along the same direction defined by EO type.
- the EO offset direction can be defined according to the predicted or interpolated pixel value.
- the predicted pixel or interpolated value can be used in classifying the current pixel value, or a combination of the predicted pixel value together with the neighboring pixels can be used in the classification.
- the interpolated pixel value (I) can be a weighted combination of its two neighbors, more generally I can be computed based on different neighboring values as well as on C and other parameters or functions.
- These parameters can be signaled to the decoder per partition, LCU, slice, picture, group of pictures, or sequence, or be known to both encoder and decoder without further signaling.
- the offset tends to be negative (towards the line).
- the offset tends to be positive (towards the line). Since in both cases, the offset is applied towards the line, the offset may be defined to be positive if it is in the direction of the line, and negative if away from the line (and zero for no change). With such a definition, entropy coding of the offset can take advantage of the higher probability that the offset will be positive (or non-negative). Consequently, shorter codewords may be assigned to the positive offset values, thereby achieving bit rate savings.
- the negative offsets are ignored or discarded, thus simplifying the coding of offsets. Ignoring the negative offsets may have some advantages for e.g., subjective quality of reconstructed frames. If negative offsets are not used or allowed, then the entropy coding can be designed appropriately. For example, in some embodiments, when negative offsets are not allowed, the most frequent offset in not zero, but rather a positive offset (e.g., 1). In this instance, by assigning the shortest codeword to this positive offset, bit rate savings can be achieved. In other words, assigning the shortest codeword to a most probable non-negative offset may result in greater efficiencies.
- a segmented line may be formed by L, R, and C. If the current sub-class 0 pixel value is below the line (e.g., C has a lower value than the average value of its neighbors L, R), the pixel is re-categorized or placed into sub-class 1 or 2 or a new sub-class, e.g., sub-class 5 . If the current sub-class 0 pixel value is above the line (e.g., C has a greater value than the average value of its neighbors L, R), the pixel is re-categorized or placed into sub-class 3 or 4 or a new sub-class, e.g., sub-class 6 . If the current sub-class 0 pixel is on the line, then it remains as sub-class 0 where no offset is applied. It should be appreciated that using this offset application technique, offsets may be applied to more pixels to improve performance without transmitting additional offsets.
- additional offset classes or sub-classes can be defined using e.g., distance thresholds, where a different offset is applied based on distances between the current pixel value, the predicted or interpolated pixel value, and/or the neighboring pixel values.
- FIG. 8 illustrates an example segmented line formed by L, C, and R and the relationship of I to the segmented line.
- the current pixel value C is a very large distance from the interpolated pixel value I.
- C would be classified as belonging to sub-class 1 , with a standard or predetermined offset O being applied.
- C is approximately the distance of three offsets O from I. Consequently, applying an offset that is larger than the predetermined offset O may be beneficial. In other words, different offsets may be applied depending on the distance between C and I.
- additional sub-classes can be defined based upon the distance of C from I, and additional offsets can be signaled for each sub-class.
- a scaled offset can be applied, where the scale factor depends on the distance between C and I.
- an offset that moves in either direction e.g., a signed offset
- a new EO sub-class EN may be defined as provided herein.
- the current pixel value C may be classified into sub-class EN if
- the entropy coding e.g., VLC or binarization and CABAC
- the statistics of the offset values may at least partially depend on or relate to the value of the closeness threshold (T) chosen.
- the number of conventional sub-classes may be reduced.
- sub-class 1 and 2 pixels may be combined into a new sub-class 1 ′ and sub-class 3 and 4 pixels may be combined into a new sub-class 2 ′, resulting in two offsets per EO type.
- the new sub-class 1 ′ pixels are such that the two neighbors are both greater than or equal to the current pixel
- the new sub-class 2 ′ pixels are such that the two neighbors are both less than or equal to the current pixel.
- a generalization of defining sub-classes to reduce the overall number of sub-classes and inclusion of pixels in current sub-class 0 can be achieved by defining two (three total) sub-classes of pixels: One group has pixel values higher than interpolated value I, and a second group has pixel values lower than interpolated value I (and the third sub-class has pixels that are equal to interpolated value I).
- One group has pixel values higher than interpolated value I
- a second group has pixel values lower than interpolated value I (and the third sub-class has pixels that are equal to interpolated value I).
- the first category may include all the pixels with the same value as the interpolated value I.
- the second category may include all the other pixels.
- two offsets (one for each of the two sub-classes) may be signaled, or one offset may be signaled for the second category.
- the signaled offset value for the first category if signaled, may be a signed value while the signaled offset value for the second category may be unsigned and the sign of actual offset, to be applied to each pixel, may be derived from the relative position of that pixel relative to the interpolated value I.
- EO type or class refers to the direction along where pixels will be processed
- sub-class refers to the categorization of pixel values according to the gradient pattern along the EO type or class direction.
- the number of EO sub-classes may be extended to a total of nine sub-classes, where each pixel is classified depending on whether it is smaller, equal, or larger than the two neighboring pixels along the direction indicated by EO type or class.
- any suitable increased number e.g., greater than five
- more offsets may need to be sent to the decoder.
- more offsets may need to be sent for the additional sub-classes, the reduction in distortion may improve performance.
- one or more of the above EO modifications can be combined to improve overall performance.
- SAO (EO) sub-classes and offsets described herein can be signaled at a partition, LCU, slice, picture, group of pictures, or sequence level.
- the SAO (EO) sub-classes and offsets can also be combined with band offset types and offsets signaled at the partition, LCU, slice, picture, group of pictures, or sequence level.
- SAO uses two fixed band types, B 0 and B 1 , covering the entire intensity range, with each band further dividing the respective intensity range into 16 equal sub-classes.
- An offset can be signaled for each of the sub-classes. Because the statistics of a given picture may not fall nicely into one of the two existing band types, B 0 and B 1 , it may be preferable to combine or merge the bands.
- one band type can be used, where the range of values to apply an offset can be specified, and a number of sub-classes for the range can be specified, e.g., using a uniform sub-partitioning. An example of such partitioning using a single band type is illustrated in FIG. 9 .
- the range of values define the one or more sub-classes.
- the range of values where the offset is applied can be determined based on the data and on rate-distortion considerations.
- the offsets may generally be applied to values where the distortion can be reduced.
- SAO selection type need not be performed, such as when there is a single band type and no other SAO type. In such instances, the single band type is used without the additional steps associated with SAO selection.
- the start of the band is specified by b s , and N s sub-classes of width w s can be used.
- four offsets can be signaled to the decoder for the four sub-classes.
- the last sub-class exceeds the maximum intensity range, the last sub-class can end at the maximum value or wrap around to zero. Additional discussion on BO sub-classes can be found in U.S.
- b s is transmitted from the encoder to the decoder.
- N s is transmitted from the encoder to the decoder.
- w s is transmitted from the encoder to the decoder.
- a fixed set of values of b s , N s and/or w s can be specified and agreed upon at the encoder and/or decoder.
- only some parameters e.g., the unspecified values
- these parameters can be signaled to the decoder and can be determined for e.g., per partition, LCU, slice (or other unit), picture, group of pictures, sequence, etc.
- a unit refers to data that both the encoder and decoder have and are configured to derive SAO parameters from.
- a unit may refer to an LCU, slice, picture, etc.
- the unit may be specified implicitly (e.g., fixed, slice-dependent, prediction list-based, etc.) or explicitly (e.g., in sequence or slice header, etc.).
- the range of reconstructed values can be determined after the unit is encoded or decoded, and the BO type may be derived from this range.
- the unit can be signaled from the encoder to the decoder, or derived from other coding parameters.
- Examples of the unit can include an LCU, slice, picture, or group of pictures.
- the set of values for which SAO is applied can be determined from the unit or a portion of the unit.
- the set of values can be determined from a subset of samples in the unit, e.g. alternating samples, the first quarter LCU pixel samples (e.g., top left quarter of the unit), the four corner and center samples of the unit, pixel samples that are not affected by the deblocking filter, etc.
- an SAO unit size may be selected that is suitable for typical data, which yields good performance given the number of offsets applied, without requiring too much buffer, delay, etc.
- an LCU unit in the range of 16 ⁇ 16 to 128 ⁇ 128 may be generally suitable for one or more processes described herein.
- the encoder and decoder can decode an LCU and determine the range of values [min, max] in the LCU.
- a BO type (or types) can then be defined based on the range of values [min, max]. That is, the range of values [b s , e s ] in FIG. 9 for a BO type can be derived based on [min, max], with b s denoting the start of the BO type and e s denoting the end of the BO type.
- the band can be divided into N s sub-classes (note that N s can be different for different BO types), for example, N s sub-classes of equal width w s .
- the number of sub-classes N s or width w s may be agreed upon at the encoder and decoder, as described above, or it can be transmitted for the LCU, at LCU level, slice level picture level, etc. for one or more color planes.
- Four BO types are defined by B 0 , B 1 , B 2 , and B 3 (in general BO types can cover overlapping or non-overlapping pixel values.).
- N i sub-classes can be defined by uniform or non-uniform partitioning of the B i range.
- the sub-classes can be distributed among the two intervals shown.
- the additional information about the BO types and sub-classes shown in FIG. 10 can either be transmitted to the decoder or agreed upon the encoder and decoder. Alternatively, they can be derived (and not transmitted) based upon statistical properties of the values, such as the mean, variance, etc.
- the b s and e s values can be obtained from the minimum and maximum values in the subset of samples.
- the b s and e s values may be determined based on a function or transformation of the samples.
- a transformation such as a DCT can be used, and b s and e s can be derived from the transformed values, e.g. DC coefficient, AC coefficient, etc.
- w i and v i can be, for example, DCT coefficients.
- the mean of the values may be used to specify the location of sub-class intervals for a BO type.
- the mean may be computed on a subset of values within the unit (e.g., LCU) and the subset of values can be in a certain region in the unit, alternating samples (or every n th sample horizontally and mth sample vertically), etc.
- a benefit of computing the mean value is that only one addition per pixel is performed in the subset of unit (e.g., LCU) and a division operation, or if the number of samples is 2 n , a shift of the final total by n bits. For example, if the number of samples over which the mean (average) value is computed is a power of two, then the division by that number can be achieved by a bit shift, which can require less computation than a general division operation.
- a BO type and/or set of sub-classes can then be derived based on the mean value. For example, a number of sub-classes can be specified relative to the mean value. The sub-classes can be centered or shifted about the mean and can cover a limited range or extended to cover the entire range (e.g., by extending the range of the first and last sub-classes as explained in further detail below).
- FIG. 11 shows an example where a BO type is specified using the mean value (m) and a width (w s ) for a case of 4 sub-classes with extended first and last sub-classes.
- the mean can be the center (or approximately the center) of that span.
- the mean can indicate the center (or approximately the center) of all sub-classes.
- the entire range may be divided into fixed intervals.
- the sub-class can be defined as the interval in which the mean value is located.
- adjacent intervals can be chosen, for example, centered about the interval with the mean value.
- This mapping may be achieved using a look-up table or by other means (e.g., binary-shift of pixels values).
- each pixel value may be shifted by 3 binary positions to the right to get the band-index.
- the band-index is useful is determining which offset should be applied to each pixel.
- a combination of pixel value shift and merging offsets may be used to map pixel values in non-uniform and/or discontinuous situations.
- the finest granularity of offset-intervals may be defined by shift and they may then be merged based on min-max or some other conditions to include only N contiguous offset.
- 4 offsets are transmitted, to be applied to 4 regions which are not necessarily uniform or contiguous.
- the BO type parameters need not be transmitted, thereby saving on overhead bits.
- the BO parameters for a current unit can be determined based on a unit previously available to the encoder and decoder. Using previously available unit parameters may reduce latency so that SAO processing can begin on the first sample in the unit without needing to process the unit to determine the BO type parameters.
- a collocated unit or motion-compensated unit from a previously coded picture can be used to determine the BO parameters for a current unit.
- sub-classes with no pixels in the respective intensity range there may be many sub-classes with no pixels in the respective intensity range (e.g., also known as empty sub-classes).
- these sub-classes with a zero offset in some embodiments, only the offset values for those sub-classes that have pixel intensity values are encoded and signaled.
- Such encoding of sub-classes that have pixel intensity values may be achieved by additionally encoding an escape code or end-of-offset code to signal no more offset values. This escape code can be, for example, a value that is larger than the maximum offset value used.
- This approach can be beneficial when there are many empty sub-classes; however, in cases where there are not many empty sub-classes, a combination of only encoding sub-classes having intensity pixel values and encoding sub-classes with a zero offset may be implemented.
- the approach can be used for signaling of offsets in both band offset and edge offset types. For the case of edge offset types, an empty sub-class corresponds to the case where there are no pixels with the respective gradient pattern. Additional discussion on offset for empty sub-classes can be found in U.S. patent application Ser. Nos. 13/672,476 and 13/672,484, previously incorporated by reference in their entirety.
- the decoder receives information on a band offset specification type such as shown in FIG. 9 .
- the decoder classifies the reconstructed pixel values into the sub-classes according to their intensities.
- the decoder receives the sequences of offset values, it can assign the offsets to each sub-class according to where pixel intensities exist in the sub-class.
- FIG. 12 illustrates this as an example.
- FIG. 12 shows an example of BO with eight sub-classes 0 - 7 .
- the locations of the eight sub-classes or range of pixel amplitudes can be signaled to the decoder using methods previously described.
- there are only pixel intensities in sub-classes 1 and 6 while there are no pixel intensities in sub-classes 0 , 2 , 3 , 4 , 5 , and 7 .
- the latter sub-classes are empty and so no offsets need to be signaled.
- the offset value of 2 for sub-class 1 and value of ⁇ 1 for sub-class 6 can be signaled, followed by an optional escape value signaling no more offset values. If the escape value is not signaled, then it is assumed that the decoder performs pixel classification into sub-classes prior to parsing the offset values. After the decoder receives the information specifying the BO sub-classes using methods such as previously described, it can classify the pixel intensities. After classifying the pixel intensities, the decoder assigns the first offset value of 2 to the first non-empty sub-class of 1 and the second offset value of ⁇ 1 to the second non-empty sub-class of 6 .
- the interval can then be partitioned into sub-classes.
- FIG. 9 shows the sub-classes of equal intervals, although this need not be the case.
- no offset is transmitted or applied for values outside the range from b s to e s .
- values may occur outside the range if b s and e s are determined from a subset of samples in the unit, while the offset is applied to all the samples in the unit or to samples outside the subset. In such instances, it may be beneficial to apply an offset (e.g., non-zero) to these values outside the range.
- FIG. 11 illustrates a method for applying an offset to values outside the b s and e s ranges, where the first and last sub-classes are extended to the smallest and largest possible values (e.g. 0 and 255, respectively). This is one example of non-uniform interval BO sub-classes, and other examples are possible.
- the offsets for each sub-class can be determined based on e.g., RD optimization of a subset of samples or for all the samples in the unit.
- the complexity of SAO BO can be reduced.
- the additional out of range checks are not needed.
- the first and last sub-classes in a BO type can be extended to cover more pixel values with the potential to increase coding efficiency and/or reduce complexity.
- the “first” boundary (after 0) can be specified as b_s+w_s
- the “last” boundary (before 255) can be specified as e_s ⁇ w_s, with N_s ⁇ 2 intervals in between.
- the modifications described herein may allow for less overhead, computation, and latency for specification and processing using SAO BO.
- the BO can be specified and applied to any or all color components, and the number of offsets can be different for each color component. For example, luma BO may have 4 offsets and chroma BO may have 2 offsets.
- SAO band types and offsets described herein can be signaled at a partition, LCU, slice, picture, group of pictures, or sequence level. They can also be combined with EO types and offsets signaled at the partition, LCU, slice, picture, group of pictures, or sequence level.
- a deblocking filter (DBF) operation may be performed on the reconstructed PU, x′′, first to reduce blocking artifacts.
- a sample adaptive offset (SAO) process may be conditionally performed after the completion of the deblocking filter process for the decoded picture, which compensates the pixel value offset between reconstructed pixels and original pixels.
- FIG. 14 illustrates a general block diagram of an SAO process 1400 , with the incoming data from the deblocking filter shown in block 1410 and a first data set after a first SAO process 1420 is applied shown in block 1430 . There is optionally shown a second set of data after a second SAO process 1440 is applied shown in block 1450 .
- the incoming data in block 1410 contains signal and noise distortion relative to the original data.
- the distortion has been reduced in the first data set in block 1430 from applying offsets from the SAO process 1420 .
- the classification e.g., edge type or band type
- the classification is usually determined using a rate-distortion process where the different types are applied and the one with the smallest cost is selected. Thereafter, the selected SAO type and offset information can be signaled to the decoder and applied to the data in block 1410 .
- a set of offsets may be assumed for more than one SAO type and applied in a e.g., sequential manner.
- more than one offset e.g., a set of offsets
- the process may be referred to as “multipass.”
- a signal can be sent to specify either EO type or BO type.
- EO type if EO type is selected, then two sets of offsets can be sent, where the first set of offsets is applied along horizontal classification and the second set of offsets is applied based on vertical direction classification.
- the first set of offsets is applied along horizontal classification and the second set of offsets is applied based on vertical direction classification.
- no classification type is signaled, but three sets of offsets are signaled for three sequential SAO type operations.
- the pixels are categorized according to the SAO type and sub-class, and the offsets are applied.
- the first set of offsets may be applied using EO based on horizontal classification.
- the second set of offsets may be applied in conjunction with vertical-edge classification, and the resulting pixels may then be used for BO classification, where the third set of offsets may be applied accordingly.
- the first SAO classification 1420 and the second SAO classification 1440 use pixel values that have been decoded and deblocked (e.g., raw data in block 1410 ).
- pixel values that have been decoded, deblocked and SAO has been applied e.g., filtered data in block 1430
- these decoded+deblocked+SAO values can occur on the left and upper boundaries of an LCU.
- a second SAO process is applied after a first SAO process.
- the decoded+deblocked+SAO values from the current LCU in the first pass may be used for classification in the second SAO pass.
- the SAO offsets can be specified to decrease distortion, or keep it the same for the case of a zero offset or SAO off, performance may improve (or not degrade) with additional applications of SAO. In some embodiments, if overall rate-distortion performance is not improved, additional SAO passes can be disabled.
- two or more SAO passes can be applied, and a combination of edge offset and/or band offset can be applied.
- EO can be applied to pixels in the first pass, and then BO can be applied to the resulting pixels from the EO pass.
- color is constructed from one luma component and two chroma components. Accordingly, each color component can have a different number of SAO passes and the SAO type for each pass per component can be different.
- multipass SAO can be applied starting with decoded pixel values, or after decoded+other_processing, where other processing can include deblocking, loop filtering, etc. Benefits of multipass SAO may include improved visual quality and coding efficiency.
- SAO classification is predetermined, the classification type need not be transmitted, thus resulting in bit savings.
- predetermined SAO classification refers to classifications that have been agreed upon between the encoder and decoder (e.g., perform two EO passes in two agreed upon directions).
- Multipass may be applied using at least two or more passes with associated SAO types.
- the SAO types are of the same type (e.g., horizontal EO type).
- the SAO types are of a similar type but different sub-type (e.g., first is horizontal EO type and second is vertical EO type).
- the SAO types are of a different type (e.g., first offset is EO and second offset is BO). With the number of SAO types and sub-types in fluctuation, there is a number of different possible combinations for desirable multipass processes.
- any of the EO and BO modifications may be applied as part of the multipass process.
- a two-pass EO process can be applied using a new sub-class EN with one value of T used for EO classification and processing first along the horizontal direction, followed by EO classification and processing along the vertical direction with a different value of T, where T is a threshold relating a current pixel value C and an interpolated pixel value I.
Abstract
Description
TABLE 1 | |
SAO | Code |
Off | |
0 | |
E0 | 10 |
|
110 |
E2 | 1110 |
E3 | 11110 |
B0 | 111110 |
B1 | 1111110 |
if (p < a0) | ||
apply offset o0 | ||
else if (p < a1) | ||
apply offset o1 | ||
else if (p < a2) | ||
apply offset o2 | ||
else | ||
apply offset o3 | ||
Claims (28)
Priority Applications (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2013/020388 WO2013103893A1 (en) | 2012-01-05 | 2013-01-04 | Devices and methods for multipass sample adaptive offset coding |
US13/734,778 US9955153B2 (en) | 2012-01-05 | 2013-01-04 | Devices and methods for sample adaptive offset coding |
Applications Claiming Priority (8)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201261583555P | 2012-01-05 | 2012-01-05 | |
US201261589298P | 2012-01-21 | 2012-01-21 | |
US201261589297P | 2012-01-21 | 2012-01-21 | |
US201261597041P | 2012-02-09 | 2012-02-09 | |
US201261616373P | 2012-03-27 | 2012-03-27 | |
US201261619916P | 2012-04-03 | 2012-04-03 | |
US201261639998P | 2012-04-30 | 2012-04-30 | |
US13/734,778 US9955153B2 (en) | 2012-01-05 | 2013-01-04 | Devices and methods for sample adaptive offset coding |
Publications (2)
Publication Number | Publication Date |
---|---|
US20130177068A1 US20130177068A1 (en) | 2013-07-11 |
US9955153B2 true US9955153B2 (en) | 2018-04-24 |
Family
ID=48743906
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/734,778 Active 2036-05-24 US9955153B2 (en) | 2012-01-05 | 2013-01-04 | Devices and methods for sample adaptive offset coding |
Country Status (2)
Country | Link |
---|---|
US (1) | US9955153B2 (en) |
WO (1) | WO2013103893A1 (en) |
Families Citing this family (33)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP2777272B1 (en) | 2011-11-08 | 2019-01-09 | Google Technology Holdings LLC | Devices and methods for sample adaptive offset coding and/or signaling |
WO2013103892A1 (en) | 2012-01-05 | 2013-07-11 | General Instrument Corporation | Devices and methods for sample adaptive offset coding and/or selection of edge offset parameters |
CN110234012B (en) * | 2012-01-17 | 2021-12-31 | 华为技术有限公司 | Apparatus for in-loop filtering of lossless coding modes in high performance video coding |
WO2013109879A1 (en) * | 2012-01-19 | 2013-07-25 | Cisco Technology Inc. | Digital video compression system, method and computer readable medium |
KR20160118365A (en) | 2012-02-06 | 2016-10-11 | 노키아 테크놀로지스 오와이 | Method for coding and an apparatus |
US9282328B2 (en) * | 2012-02-10 | 2016-03-08 | Broadcom Corporation | Sample adaptive offset (SAO) in accordance with video coding |
US10587898B2 (en) * | 2012-03-12 | 2020-03-10 | Sun Patent Trust | Image coding method, and image decoding method |
ES2637166T3 (en) * | 2012-04-05 | 2017-10-11 | Telefonaktiebolaget Lm Ericsson (Publ) | Adaptive sample filtering with slides |
WO2013152356A1 (en) | 2012-04-06 | 2013-10-10 | Motorola Mobility Llc | Devices and methods for signaling sample adaptive offset (sao) parameters |
US10694214B2 (en) * | 2012-12-21 | 2020-06-23 | Qualcomm Incorporated | Multi-type parallelized sample adaptive offset in video coding |
US20140254659A1 (en) * | 2013-03-11 | 2014-09-11 | Mediatek Inc. | Video coding method using at least evaluated visual quality and related video coding apparatus |
KR102164698B1 (en) * | 2013-03-25 | 2020-10-14 | 광운대학교 산학협력단 | Apparatus of processing sample adaptive offset of reusing input buffer and method of the same |
US10334253B2 (en) * | 2013-04-08 | 2019-06-25 | Qualcomm Incorporated | Sample adaptive offset scaling based on bit-depth |
FI3005696T3 (en) * | 2013-05-30 | 2023-08-08 | Huawei Tech Co Ltd | Offset dynamic range constraints for edge offset sao filtering |
WO2015007200A1 (en) * | 2013-07-15 | 2015-01-22 | Mediatek Inc. | Method of sample adaptive offset processing for video coding |
CN103442229B (en) * | 2013-08-27 | 2018-04-03 | 复旦大学 | The bit rate estimation method of SAO mode adjudgings suitable for the encoder of HEVC standard |
CN103442238B (en) * | 2013-08-29 | 2018-02-23 | 复旦大学 | SAO hardware processing method a kind of encoder suitable for HEVC standard |
TWI496456B (en) | 2013-11-19 | 2015-08-11 | Ind Tech Res Inst | Method and apparatus for inter-picture cost computation |
US9872022B2 (en) * | 2014-01-03 | 2018-01-16 | Mediatek Inc. | Method and apparatus for sample adaptive offset processing |
JP2015185897A (en) * | 2014-03-20 | 2015-10-22 | パナソニックＩｐマネジメント株式会社 | Image encoding method and device |
US9716884B2 (en) * | 2014-03-20 | 2017-07-25 | Hfi Innovation Inc. | Method of signaling for mode selection in 3D and multi-view video coding |
KR102298599B1 (en) | 2014-04-29 | 2021-09-03 | 마이크로소프트 테크놀로지 라이센싱, 엘엘씨 | Encoder-side decisions for sample adaptive offset filtering |
US9877024B2 (en) * | 2015-03-06 | 2018-01-23 | Qualcomm Incorporated | Low complexity sample adaptive offset (SAO) coding |
WO2016204479A1 (en) * | 2015-06-16 | 2016-12-22 | 엘지전자(주) | Method for encoding/decoding image, and device therefor |
WO2017078450A1 (en) * | 2015-11-04 | 2017-05-11 | 엘지전자 주식회사 | Image decoding method and apparatus in image coding system |
KR20170058869A (en) * | 2015-11-18 | 2017-05-29 | 한국전자통신연구원 | Method for decoding a video using an inloop filter and apparatus therefor |
EP3291553A1 (en) * | 2016-08-30 | 2018-03-07 | Thomson Licensing | Method and apparatus for video coding with sample adaptive offset |
EP3291554A1 (en) * | 2016-08-30 | 2018-03-07 | Thomson Licensing | Method and apparatus for video coding with sample adaptive offset |
WO2018066849A1 (en) * | 2016-10-04 | 2018-04-12 | 한국전자통신연구원 | Method and device for encoding/decoding image, and recording medium storing bit stream |
US10432972B2 (en) * | 2016-10-19 | 2019-10-01 | Google Llc | Guided offset correction for loop restoration in video coding |
EP3349459A1 (en) | 2017-01-11 | 2018-07-18 | Thomson Licensing | A method and a device for image encoding and decoding |
GB2580173B (en) * | 2018-12-21 | 2022-07-27 | Canon Kk | A filter |
JP7410619B2 (en) * | 2019-10-31 | 2024-01-10 | キヤノン株式会社 | Image processing device, image processing method and program |
Citations (20)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5446806A (en) * | 1993-11-15 | 1995-08-29 | National Semiconductor Corporation | Quadtree-structured Walsh transform video/image coding |
US5859979A (en) * | 1993-11-24 | 1999-01-12 | Intel Corporation | System for negotiating conferencing capabilities by selecting a subset of a non-unique set of conferencing capabilities to specify a unique set of conferencing capabilities |
US7782722B2 (en) * | 2006-08-31 | 2010-08-24 | Canon Kabushiki Kaisha | Method of adjusting spherical aberration and focus offset and information recording/reproduction apparatus using the same |
WO2011127961A1 (en) | 2010-04-13 | 2011-10-20 | Fraunhofer-Gesellschaft zur Förderung der angewandten Forschung e. V. | Adaptive image filtering method and apparatus |
US20110305274A1 (en) | 2010-06-15 | 2011-12-15 | Mediatek Inc. | Apparatus and method of adaptive offset for video coding |
US20120082232A1 (en) | 2010-10-01 | 2012-04-05 | Qualcomm Incorporated | Entropy coding coefficients using a joint context model |
US20120207227A1 (en) | 2011-02-16 | 2012-08-16 | Mediatek Inc. | Method and Apparatus for Slice Common Information Sharing |
US8259808B2 (en) * | 2010-03-25 | 2012-09-04 | Mediatek Inc. | Low complexity video decoder |
US20120287988A1 (en) | 2011-05-10 | 2012-11-15 | Qualcomm Incorporated | Offset type and coefficients signaling method for sample adaptive offset |
US20120294353A1 (en) | 2011-05-16 | 2012-11-22 | Mediatek Inc. | Apparatus and Method of Sample Adaptive Offset for Luma and Chroma Components |
US20130003829A1 (en) | 2011-07-01 | 2013-01-03 | Kiran Misra | System for initializing an arithmetic coder |
US8351310B2 (en) * | 2010-08-20 | 2013-01-08 | Sunplus Technology Co., Ltd. | Method and apparatus for determining an optimal focus bias and spherical aberration compensating value in an optical disc drive |
US20130022103A1 (en) | 2011-06-22 | 2013-01-24 | Texas Instruments Incorporated | Method and apparatus for sample adaptive offset parameter estimationfor image and video coding |
US20130114674A1 (en) | 2011-11-04 | 2013-05-09 | Qualcomm Incorporated | Adaptive center band offset filter for video coding |
US20130114678A1 (en) | 2011-11-08 | 2013-05-09 | General Instrument Corporation | Devices and methods for sample adaptive offset coding and/or signaling |
US20130151486A1 (en) | 2011-12-08 | 2013-06-13 | General Instrument Corporation | Method and apparatus that collect and uploads implicit analytic data |
WO2013103892A1 (en) | 2012-01-05 | 2013-07-11 | General Instrument Corporation | Devices and methods for sample adaptive offset coding and/or selection of edge offset parameters |
US20130188741A1 (en) | 2012-01-21 | 2013-07-25 | General Instrument Corporation | Devices and methods for sample adaptive offset coding and/or selection of band offset parameters |
US20130266058A1 (en) | 2012-04-06 | 2013-10-10 | General Instrument Corporation | Devices and methods for signaling sample adaptive offset (sao) parameters |
US8861617B2 (en) * | 2010-10-05 | 2014-10-14 | Mediatek Inc | Method and apparatus of region-based adaptive loop filtering |
-
2013
- 2013-01-04 WO PCT/US2013/020388 patent/WO2013103893A1/en active Application Filing
- 2013-01-04 US US13/734,778 patent/US9955153B2/en active Active
Patent Citations (25)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5446806A (en) * | 1993-11-15 | 1995-08-29 | National Semiconductor Corporation | Quadtree-structured Walsh transform video/image coding |
US5859979A (en) * | 1993-11-24 | 1999-01-12 | Intel Corporation | System for negotiating conferencing capabilities by selecting a subset of a non-unique set of conferencing capabilities to specify a unique set of conferencing capabilities |
US7782722B2 (en) * | 2006-08-31 | 2010-08-24 | Canon Kabushiki Kaisha | Method of adjusting spherical aberration and focus offset and information recording/reproduction apparatus using the same |
US8259808B2 (en) * | 2010-03-25 | 2012-09-04 | Mediatek Inc. | Low complexity video decoder |
WO2011127961A1 (en) | 2010-04-13 | 2011-10-20 | Fraunhofer-Gesellschaft zur Förderung der angewandten Forschung e. V. | Adaptive image filtering method and apparatus |
US20110305274A1 (en) | 2010-06-15 | 2011-12-15 | Mediatek Inc. | Apparatus and method of adaptive offset for video coding |
US8660174B2 (en) * | 2010-06-15 | 2014-02-25 | Mediatek Inc. | Apparatus and method of adaptive offset for video coding |
US8351310B2 (en) * | 2010-08-20 | 2013-01-08 | Sunplus Technology Co., Ltd. | Method and apparatus for determining an optimal focus bias and spherical aberration compensating value in an optical disc drive |
US20120082232A1 (en) | 2010-10-01 | 2012-04-05 | Qualcomm Incorporated | Entropy coding coefficients using a joint context model |
US8861617B2 (en) * | 2010-10-05 | 2014-10-14 | Mediatek Inc | Method and apparatus of region-based adaptive loop filtering |
US20120207227A1 (en) | 2011-02-16 | 2012-08-16 | Mediatek Inc. | Method and Apparatus for Slice Common Information Sharing |
US9001883B2 (en) * | 2011-02-16 | 2015-04-07 | Mediatek Inc | Method and apparatus for slice common information sharing |
US9008170B2 (en) * | 2011-05-10 | 2015-04-14 | Qualcomm Incorporated | Offset type and coefficients signaling method for sample adaptive offset |
US20120287988A1 (en) | 2011-05-10 | 2012-11-15 | Qualcomm Incorporated | Offset type and coefficients signaling method for sample adaptive offset |
US20120294353A1 (en) | 2011-05-16 | 2012-11-22 | Mediatek Inc. | Apparatus and Method of Sample Adaptive Offset for Luma and Chroma Components |
US20130022103A1 (en) | 2011-06-22 | 2013-01-24 | Texas Instruments Incorporated | Method and apparatus for sample adaptive offset parameter estimationfor image and video coding |
US20130003829A1 (en) | 2011-07-01 | 2013-01-03 | Kiran Misra | System for initializing an arithmetic coder |
US20130114674A1 (en) | 2011-11-04 | 2013-05-09 | Qualcomm Incorporated | Adaptive center band offset filter for video coding |
US20130114677A1 (en) | 2011-11-08 | 2013-05-09 | General Instrument Corporation | Devices and methods for sample adaptive offset coding and/or signaling |
US20130114678A1 (en) | 2011-11-08 | 2013-05-09 | General Instrument Corporation | Devices and methods for sample adaptive offset coding and/or signaling |
US20130151486A1 (en) | 2011-12-08 | 2013-06-13 | General Instrument Corporation | Method and apparatus that collect and uploads implicit analytic data |
WO2013103892A1 (en) | 2012-01-05 | 2013-07-11 | General Instrument Corporation | Devices and methods for sample adaptive offset coding and/or selection of edge offset parameters |
US20130177067A1 (en) | 2012-01-05 | 2013-07-11 | General Instrument Corporation | Devices and methods for sample adaptive offset coding and/or selection of edge offset parameters |
US20130188741A1 (en) | 2012-01-21 | 2013-07-25 | General Instrument Corporation | Devices and methods for sample adaptive offset coding and/or selection of band offset parameters |
US20130266058A1 (en) | 2012-04-06 | 2013-10-10 | General Instrument Corporation | Devices and methods for signaling sample adaptive offset (sao) parameters |
Non-Patent Citations (42)
Title |
---|
Bankoski et al. "Technical Overview of VP8, an Open Source Video Codec for the Web". Dated Jul. 11, 2011. |
Bankoski et al. "VP8 Data Format and Decoding Guide" Independent Submission. RFC 6389, Dated Nov. 2011. |
Bankoski et al. "VP8 Data Format and Decoding Guide; draft-bankoski-vp8-bitstream-02" Network Working Group. Internet-Draft, May 18, 2011, 288 pp. |
Bross et al., "High-Efficiency Video Coding", Joint Collaborative Team on Video Coding (JCT-VC) JCTVCF803 of ITU-T SG16 WP3 and ISO/IEC JTC1/SC29/WG11 6th Meeting: Torino, IT, Jul. 14-22, 2011. |
Chih-Ming Fu, et al., "TE 10 Subest 3: Quadtree-based Adaptive Offset," JCTVC-C147, pp. 1-6, 3rd Meeting: Guangzhou, China, Oct. 7-15, 2010. |
C-M Fu et al.: "CE8 Subtest3: Picture Quadtree Adaptive Offset", 4, JCT-VC Meeting; Jan. 20, 2011-Jan. 1, 2011; Daegu; (Joint Collaborative Team on Video Coding of ISO/IEC JTC1/SC29/WG11 and ITU-T SG.16); URL: http://wftp3.itu.int/av-arch/jctvc-site/, No. JTCVC-D122, Jan. 15, 2011 (Jan. 15, 2011), pp. 1-10. |
C-M Fu et al.: "CE8.c.2: Single-source SAO and ALF virtual boundary processing", 98, MPEG Meeting; Nov. 28, 2011-Dec. 2, 2011; Geneva; (Motion Picture Expert Group or ISO/IEC JTC1/SC29/WG11) No. m21763, 21, JCTVC-G204 Nov. 2011, all pages. |
C-M Fu et al.: "Sample Adaptive Offset for Chroma", 6, JCT-VC Meeting; 97, MPEG Meeting Jul. 14, 2011-Jul. 22, 2011, Torino; (Joint Collaborative Team on Video Coding of ISO/IEC JTC1/SC29/WG11 and ITU-T SG16) URL: http://wftp3.itu.int/av-arch/jctvc-site/, No. JCTC-F057, Jul. 20, 2011, all pages. |
C-M Fu et al.:"TE10 Subtest 3: Quadtree-based adaptive offset", 94 MPEG Meeting; Oct. 11, 2010-Oct. 15, 2010; Guangzhou; (Motion Picture Expert Group or ISO/IEC JTC1/SC29/WG11),JCTVCC147, No. M18173, Oct. 28, 2010, all pages. |
C-Y Chen et al.: "Non-CE8: Sample Adaptive Offset with LCU-based Syntax". JCT-VC Meeting; 98. MPEG Meeting; Nov. 21, 2011-Nov. 30, 2011; Geneva; (Joint Collaborative Team on Video Coding of ISO/IEC JTC1/SC29/WG11 and ITU-T Sg.16); URL: http://wftp3.itu.int/av-arch/jtvc-site/,. No. JCTVC-G831, Nov. 9, 2011 (Nov. 9, 2011), all pages. |
Fu et al., "Sample adaptive offset for HEVC" Multimedia Signal Processing (MMSP); 2011 IEEE 13th International Workshop on; Oct. 17, 2011; 5 pages. |
Implementors' Guide; Series H: Audiovisual and Multimedia Systems; Coding of moving video: Implementors Guide for H.264: Advanced video coding for generic audiovisual services. H.264. International Telecommunication Union. Version 12. Dated Jul. 30, 2010. |
Kim, "Non-CE8: Reduced number of band offsets in SAO" Joint Collaborative Team on Video Coding (JCT-VC) of ITU-T SG16 WP3 and ISO/IEC JTC/WG11; JCTVCG682 7th Meeting: Geneva, CH; Nov. 21-30, 2011; 6 pages. |
Korean Office Action dated Jul. 21, 2015 in related matter. 5 pages. 10-2014-7014200. |
Laroche (Canon) G et al.: "On additional SAO Band Offset classifications", 7 JCT-VC Meeting; 98. MPEG Meeting; Nov. 21, 2011-Nov. 30, 2011; Geneva; (Joint Collaborative Team on Video Coding of ISO/IEC JTC1/SC29/WG11 and ITU-T-SG.16); URL:http://wftp3.itu.int/av-arch/jctvc-site/,, No. JCTV-G246, Nov. 8, 2011, all pages. |
Marta Mrak et al.: "CE6: Report and evaluation of new combined intra prediction settings", 4 JCT-VC Meeting; 95, MPEG Meeting; Jan. 20, 2011-Jan. 28, 2011; Daegu; (Joing Collaborative Team on Video Coding of ISO/IEC JTC1/SC29/WG11 and ITU-T SG. 16); URL: http://wftp3.itu.int/av-arch/jctvc-site/ No. JCTVC-D191, Jan. 15, 2011, all pages. |
McCann et al., "HM4: HEVC Test Model 4 Encoder Description" Joint Collaborative Team on Video Coding (JCT-VC) of ITU-T SG16 WP3 and ISO/IEC JTC1/SC29/WG11; 6th Meeting: Torino, IT; Jul. 14-22, 2011; 36 pages. |
McCann et al., "Samsung's Respons to the Call for Proposals on Video Compression Technology" Joint Collaborative Team on Video Coding (JCT-VC) of ITU-T SG16 WP3 and ISO/IEC JTC1/SC29/WG11; 1st Meeting: Dresden, DE; Apr. 15-23, 2010; 42 pages. |
McCann K et al.: HM4: HEVC Test Model 4 Encoder Description, 6. JCT-VC Meeting: 97, MPEG Meeting Jul. 14, 2011-Jul. 22, 2011; Torino; (Joint Collaborative Team on Video Coding of ISO/IEC JTC1/SC29/WG11 and ITU-T SG16) URL: http://wftp3.itu.int/av-arch/jctvc-site/, No. JCTC-F802, Oct. 4, 2011, all pages. |
Overview; VP7 Data Format and Decoder. Version 1.5. On2 Technologies, IxNC. Dated Mar. 28, 2005. |
Segall (Sharp) A et al.: Video Coding technology proposal by Sharp, 1. JCT-VC Meeting; Apr. 15, 2010-Apr. 23, 2010; Dresden; (Joint Collaborative Team on Video Coding of ISO/IEC TTC1/SC29/WG11 and ITU-T SG16); URL: http://wftp3.itu.int/av-arch/jctvc-site/, No. JCTVC-A105, Apr. 22, 2010 (Apr. 22, 2010, all pageS. |
Segall A et al.: "Unifed Deblocking and SAO", 7, JCT-VC Meeting; 98, MPEG Meeting; Nov. Nov. 21, 2011-Nov. 20, 2011; Geneva; (Joint Collaborative Team on Video Coding of ISO/IEC JTC1/SC29/WG11 and ITU-T SG.16); URL:http://wftp3.itu.int/av-arch/jctvc-site/,, No. JCTVC-6608, Nov. 9, 2011, all pages. |
Series H: Audiovisual and Multimedia Systems; Infrastructure of audiovisual services-Coding of moving video. H.264. Advanced video coding for generic audiovisual services. International Telecommunication Union Version 12. Dated Mar. 2010. |
Series H: Audiovisual and Multimedia Systems; Infrastructure of audiovisual services—Coding of moving video. H.264. Advanced video coding for generic audiovisual services. International Telecommunication Union Version 12. Dated Mar. 2010. |
Series H: Audiovisual and Multimedia Systems; Infrastructure of audiovisual services-Coding of moving video. H.264. Advanced video coding for generic audiovisual services. International Telecommunication Union. Version 11. Dated Mar. 2009. |
Series H: Audiovisual and Multimedia Systems; Infrastructure of audiovisual services—Coding of moving video. H.264. Advanced video coding for generic audiovisual services. International Telecommunication Union. Version 11. Dated Mar. 2009. |
Series H: Audiovisual and Multimedia Systems; Infrastructure of audiovisual services-Coding of moving video. H.264. Advanced video coding for generic audiovisual services. Version 8. International Telecommunication Union. Dated Nov. 1, 2007. |
Series H: Audiovisual and Multimedia Systems; Infrastructure of audiovisual services—Coding of moving video. H.264. Advanced video coding for generic audiovisual services. Version 8. International Telecommunication Union. Dated Nov. 1, 2007. |
Series H: Audiovisual and Multimedia Systems; Infrastructure of audiovisual services-Coding of moving video. H.264. Amendment 2: New profiles for professional applications. International Telecommunication Union. Dated Apr. 2007. |
Series H: Audiovisual and Multimedia Systems; Infrastructure of audiovisual services—Coding of moving video. H.264. Amendment 2: New profiles for professional applications. International Telecommunication Union. Dated Apr. 2007. |
Series H: Audiovisual and Multimedia Systems; Infrastructure of audiovisual services-Coding of moving video; Advanced video coding for generic audiovisual services. H.264. Amendment 1: Support of additional colour spaces and removal of the High 4:4:4 Profile. International Telecommunication Union. Dated Jun. 2006. |
Series H: Audiovisual and Multimedia Systems; Infrastructure of audiovisual services—Coding of moving video; Advanced video coding for generic audiovisual services. H.264. Amendment 1: Support of additional colour spaces and removal of the High 4:4:4 Profile. International Telecommunication Union. Dated Jun. 2006. |
Series H: Audiovisual and Multimedia Systems; Infrastructure of audiovisual services-Coding of moving video; Advanced video coding for generic audiovisual services. H.264. Version 1. International Telecommunication Union. Dated May 2003. |
Series H: Audiovisual and Multimedia Systems; Infrastructure of audiovisual services—Coding of moving video; Advanced video coding for generic audiovisual services. H.264. Version 1. International Telecommunication Union. Dated May 2003. |
Series H: Audiovisual and Multimedia Systems; Infrastructure of audiovisual services-Coding of moving video; Advanced video coding for generic audiovisual services. H.264. Version 3. International Telecommunication Union. Dated Mar. 2005. |
Series H: Audiovisual and Multimedia Systems; Infrastructure of audiovisual services—Coding of moving video; Advanced video coding for generic audiovisual services. H.264. Version 3. International Telecommunication Union. Dated Mar. 2005. |
VP6 Bitstream & Decoder Specification. Version 1.02. On2 Technologies, IxNC. Dated Aug. 17, 2006. |
VP6 Bitstream & Decoder Specification. Version 1.03. On2 Technologies, IxNC. Dated Oct. 29, 2007. |
VP8 Data Format and Decoding Guide. WebM Project. Google On2. Dated: Dec. 1, 2010. |
Wang Lai et al., "CE8 Subtest 1: Block-based filter adaptation with features on subset of pixels" Joint Collaborative Team on video Coding of ITU-T SG16 WP3 and ISO/IEC JTC1/SC29/WG11; 6th meeting: Torino, Italy: Jul. 14-22, 2011; JCTVCF301; 6 pages. |
W-S Kim et al.: "Non-CE8: Method of visual coding artifact removal for SAO"> 7. JCT-VC Meeting; 98. MPEG Meeting; Nov. 21, 2011-Nov. 30, 2011; Geneva; (Joint Collaborative Team on Video Coding of ISO-IEC JTC1/SC29/WG11 and ITU-T SG.16) URL: http://wftp3.itu.int/av-arch/jctvc-site/ No. JCTVC-G680, Nov. 9, 2011, all pages. |
Yamakage et al., "Description of Core Experiment 8 (CE8): Non-deblocking loop filtering" Joint Collaborative Team on Video Coding (JCT-VC) of ITU-T SG16 WP3 and ISO/IEC JTC1/SC29/WG11; 6th Meeting: Torino, IT; Jul. 14-22, 2011; 12 pages. |
Also Published As
Publication number | Publication date |
---|---|
WO2013103893A1 (en) | 2013-07-11 |
US20130177068A1 (en) | 2013-07-11 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9955153B2 (en) | Devices and methods for sample adaptive offset coding | |
US9888249B2 (en) | Devices and methods for sample adaptive offset coding and/or selection of edge offset parameters | |
US9774853B2 (en) | Devices and methods for sample adaptive offset coding and/or signaling | |
US11758149B2 (en) | In-loop filtering method and apparatus for same | |
US20130188741A1 (en) | Devices and methods for sample adaptive offset coding and/or selection of band offset parameters | |
US9565435B2 (en) | Devices and methods for context reduction in last significant coefficient position coding | |
US9872034B2 (en) | Devices and methods for signaling sample adaptive offset (SAO) parameters | |
US9813701B2 (en) | Devices and methods for context reduction in last significant coefficient position coding | |
US20140029670A1 (en) | Devices and methods for processing of partition mode in high efficiency video coding | |
US20140146894A1 (en) | Devices and methods for modifications of syntax related to transform skip for high efficiency video coding (hevc) | |
US20140140406A1 (en) | Devices and methods for processing of non-idr related syntax for high efficiency video coding (hevc) | |
US20140092975A1 (en) | Devices and methods for using base layer motion vector for enhancement layer motion vector prediction | |
US11039166B2 (en) | Devices and methods for using base layer intra prediction mode for enhancement layer intra mode prediction | |
EP4205400A1 (en) | Residual and coefficients coding for video coding | |
WO2013109419A1 (en) | Devices and methods for sample adaptive offset coding and/or selection of band offset parameters |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GENERAL INSTRUMENT CORPORATION, PENNSYLVANIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:MINOO, KOOHYAR;BAYLON, DAVID;YU, YUE;AND OTHERS;SIGNING DATES FROM 20130221 TO 20130225;REEL/FRAME:029871/0352 |
|
AS | Assignment |
Owner name: MOTOROLA MOBILITY LLC, ILLINOISFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:GENERAL INSTRUMENT HOLDINGS, INC.;REEL/FRAME:033776/0586Effective date: 20140915Owner name: GENERAL INSTRUMENT HOLDINGS, INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:GENERAL INSTRUMENT CORPORATION;REEL/FRAME:033776/0553Effective date: 20140915 |
|
AS | Assignment |
Owner name: GOOGLE TECHNOLOGY HOLDINGS LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:MOTOROLA MOBILITY LLC;REEL/FRAME:034274/0290Effective date: 20141028 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |