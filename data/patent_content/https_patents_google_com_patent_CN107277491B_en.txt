CN107277491B - Generate the method and corresponding medium of the depth map of image - Google Patents
Generate the method and corresponding medium of the depth map of image Download PDFInfo
- Publication number
- CN107277491B CN107277491B CN201710367433.4A CN201710367433A CN107277491B CN 107277491 B CN107277491 B CN 107277491B CN 201710367433 A CN201710367433 A CN 201710367433A CN 107277491 B CN107277491 B CN 107277491B
- Authority
- CN
- China
- Prior art keywords
- depth
- image
- weight
- color
- pixel
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/50—Depth or shape recovery
- G06T7/55—Depth or shape recovery from multiple images
- G06T7/593—Depth or shape recovery from multiple images from stereo images
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N13/00—Stereoscopic video systems; Multi-view video systems; Details thereof
- H04N13/20—Image signal generators
- H04N13/261—Image signal generators with monoscopic-to-stereoscopic image conversion
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/20—Analysis of motion
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/50—Depth or shape recovery
- G06T7/55—Depth or shape recovery from multiple images
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10016—Video; Image sequence
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/10—Image acquisition modality
- G06T2207/10024—Color image
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2207/00—Indexing scheme for image analysis or image enhancement
- G06T2207/20—Special algorithmic details
- G06T2207/20212—Image combination
Abstract
The method and corresponding medium for generating the depth map of image are provided.This method comprises: access includes the image of multiple pixels, each pixel has color and position in image；Color pixel-based determines the color depth figure of image；The historical depth information of the pixel of analogous location determines the spatial depth figure of image in position pixel-based and other multiple images；The movement depth map of image is determined based on the pixel motion in image；Color depth figure weight, spatial depth figure weight and movement depth map weight are determined, based on the spatial depth figure weight of the difference of the determining description historical depth information of historical depth covariance information of the pixel in analogous location in other multiple images；With the combined depth figure for generating image from the combination of the color depth figure by color depth figure Weight, the spatial depth figure by spatial depth figure Weight and the movement depth map by movement depth map Weight.
Description
This case be the applying date be on October 28th, 2013, application No. is 201380055544.1, it is entitled " based on group
The depth cue of conjunction from monoscopic image generate depth map " application for a patent for invention divisional application.
Technical field
This disclosure relates to which video is handled, and the conversion in particular it relates to monoscopic image to three-dimensional 3D rendering.
Background technique
Three-dimensional or " 3D " video passes through the illusion of artificial stereo vision enhancement depth perception, thus the emulation for passing through parallax
Create the illusion of depth.However, delay three-dimensional video-frequency generally use be on one side stereo format video it is available
Property.Conventionally, generate three-dimensional video-frequency main method be using from different points of view setting angle two different cameras into
Row stereoscopic shooting is to capture depth information.Due to difficulty associated with stereoscopic shooting and cost, stereopsis fewer so far
Frequency has been generated.
In addition, although at present be possible to from monoscopic image create three-dimensional video-frequency, some prior arts dependent on pair
It is then approximate to determine depth of the object relative to the plane of delineation as dividing to identify the object in image.Object Segmentation may
The boundary for erroneously determining that object, causes the depth assignment of mistake, and the depth assignment of mistake makes viewer be difficult to distinguish image
In what object is prominent and what object recess.Therefore, the prior art generally can not be in a manner of consistent and be accurate from retouching
The monoscopic image of the depth for the object drawn as in creates stereo-picture.
Summary of the invention
Monoscopic figure is directed to based on the weighted array for the color depth figure of image, spatial depth figure and movement depth map
Depth map as generating combination, wherein each figure in figure describes depth of each pixel relative to the plane of delineation in image
Degree.In one embodiment, each depth map in single depth map is associated with the weight for calculating combined depth map.
Weight can adaptively illustrate the variation between different monoscopic images.In some cases, depth map can be with weight sets
Associated, each weight corresponds to a part of image, and weight sets includes the weight of each individual pixel or pixel group.
Color depth figure color pixel-based describes the depth of each pixel in image.Based on determining with similar
The pixel of color is likely to have similar depth to generate color depth figure, color depth figure provide by the color of pixel with
The associated color depth function of the determination of its depth.In one embodiment, it is used for based on the distribution of color in image to determine
The weight of color depth figure.Color depth figure weight is scaled according to color contrast, which indicates for being based on
Color carrys out the confidence level of quantisation depth.
It is averaged by the depth to the pixel at each position across numerous typical monoscopic images to generate space
Depth map.In generating spatial depth figure, indicate that the variogram of the variance of the pixel depth of each pixel position can also be by
It generates.Spatial depth figure weight is determined based on the variance indicated by variogram.For each location of pixels to be analyzed, access
The variogram and variance being in reverse at each position carrys out scale space depth map weight.
Movement depth map use with the pixel that comparatively fast moves closer to image prospect judgement based on their part
Move the depth to determine pixel.Local fortune is calculated by subtracting camera motion from the overall movement of the pixel between two frames
It is dynamic.Move depth function local motion calculated is associated with the figure of pixel depth.Based on the amount of exercise in image come really
Determine the weight for moving depth map.The percentage of the pixel with local motion in image is determined, and moves depth
Figure weight is increased or decreased according to the function of the percentage of mobile pixel.
Specifically, according to the present invention, a kind of method for generating the depth map of image is provided, which comprises visit
Ask that described image, described image include multiple pixels, wherein each pixel has color and position in described image；Based on figure
The color of pixel as in determines the color depth figure of image；Position pixel-based and for similar in other multiple images
The historical depth information of the pixel of position determines the spatial depth figure of image；Image is determined based on the pixel motion in image
Move depth map；It determines color depth figure weight, spatial depth figure weight and movement depth map weight, is based on other multiple images
The historical depth covariance information of pixel in middle analogous location determines spatial depth figure weight, and the description of historical depth covariance information is gone through
The difference of history depth information；With from by color depth figure Weight color depth figure, by spatial depth figure Weight
Spatial depth figure and the combined depth figure that image is generated by the combination of the movement depth map of movement depth map Weight.
According to the present invention, a kind of nonvolatile stored for generating the computer program instructions of the depth map of image is also provided
Computer readable storage medium, the computer program instructions are executable to execute the step of including the following steps: described in access
Image, described image include multiple pixels, wherein each pixel has color and position in described image；Based in image
The color of pixel determines the color depth figure of image；Position pixel-based and for analogous location in other multiple images
The historical depth information of pixel determines the spatial depth figure of image；Determine that the movement of image is deep based on the pixel motion in image
Degree figure；Color depth figure weight, spatial depth figure weight and movement depth map weight are determined, based on similar in other multiple images
The historical depth covariance information of pixel in position determines spatial depth figure weight, and historical depth covariance information describes historical depth
The difference of information；With from the color depth figure, deep by the space of spatial depth figure Weight by color depth figure Weight
Degree figure and the combined depth figure that image is generated by the combination of the movement depth map of movement depth map Weight.
According to the present invention, a kind of method for generating the depth map of image is also provided, which comprises described in access
Image, described image include multiple pixels, wherein each pixel has color and position in described image；Based in image
The color of pixel determines the color depth figure of image；Position pixel-based and for analogous location in other multiple images
The historical depth information of pixel determines the spatial depth figure of image；Determine that the movement of image is deep based on the pixel motion in image
Degree figure；Color depth figure weight, spatial depth figure weight and movement depth map weight are determined, based on the image with local motion
In pixel percentage determine movement depth map weight；With from by color depth figure Weight color depth figure, by sky
Between depth map Weight spatial depth figure and figure is generated by the combination of the movement depth map of movement depth map Weight
The combined depth figure of picture.
The content of present invention and it is described in detail below described in feature and advantage be not comprising all.In view of attached drawing,
Specification and its claim, many additional feature and advantage will become readily apparent to those skilled in the art.
Detailed description of the invention
Fig. 1 illustrates according to one embodiment generation image combined depth map overview.
Fig. 2 is the block diagram according to the depth map generation module of one embodiment.
Fig. 3 is the flow chart for illustrating the process for generating movement depth map according to one embodiment.
Fig. 4 is the flow chart for illustrating the process of the combined depth map for generating image according to one embodiment.
Attached drawing describes each embodiment of the invention exclusively for the purposes of illustration.Those skilled in the art are from following discussion
In will readily appreciate that structures and methods illustrated herein alternative embodiment can without departing from described herein send out
It is used in the case where bright principle.
Specific embodiment
Overview
Fig. 1 illustrates the overviews of the process of the combined depth map for generating image.Video frame 102 is monoscopic figure
Picture, the monoscopic image are by the frame of the video of monoscopic camera shooting in one embodiment.Video frame 102 has multiple pictures
Element, and one or more object can be described.Because video frame 102 is captured by monoscopic camera, video frame 102
For pixel on same level, which is referred to herein as the plane of delineation.Pixel is not explicitly described by video frame 102
The original depth relationship of the object of description.
However, indicating for the original depth relationship of the pixel of video frame 102 can be by generating for each of video frame 102
Depth map is planted to create.Color depth Figure 104 uses the color of pixel as the index of their depth to determine video frame 102
In pixel depth.Object based on positions certain in image will be with certain depth it is assumed that spatial depth Figure 106 is used
The position of pixel in image determines depth.Depth map 108 is moved to use between two frames between such as frame I-1 and frame I
Movement is to determine pixel depth.Each depth map in color depth Figure 104, spatial depth Figure 106 and movement depth map 108 mentions
For depth value pixel-by-pixel, pixel depth value description indicates pixel perpendicular to the amount that the plane of video frame 102 is prominent or is recessed.?
In one embodiment, biggish depth value instruction pixel is close to the back side of frame, and small or negative depth instruction pixel is close
Before plane.
It can be by combining several depth maps with the improved depth map for determining pixel depth using the multifrequency nature of image
To generate.Combined depth map 110 is color depth Figure 104, spatial depth Figure 106 and the linear combination for moving depth map 108.
In one embodiment, combined depth map 110 calculates based on pixel-by-pixel.For example, given referred to by color depth Figure 104
The depth Dcolor shown, the depth Dspatial indicated by spatial depth Figure 106 and the depth indicated by movement depth map 108
Dmotion, the depth of the pixel at position (x, y) in each dramatic video frame 102, combined depth map D (x, y)
It can indicate are as follows:
D (x, y)=w1*Dcolor (x, y)+w2*Dspatial (x, y)+w3*Dmotion (x, y) (1)
Wherein w1 is color depth figure weight, and w2 is spatial depth figure weight, and w3 is movement depth map weight.Another
In one embodiment, the depth map 110 of combination is determined for the pixel group of image.Each picture to video frame 102 can be used
The same or different weight of element, the depth map 110 that combination is generated using the different characteristic of the different piece of image, with most smart
Really determine the depth at each part.
Combined depth Figure 110 can be used for generating perspective view from monoscopic image.In one embodiment, it is based on depth map
The drafting (DIBR) of picture can be used for generating frames identical with video frame 102 but with offset pixels.For example, if video
Frame 102 is used as left frame, then DIBR is based on the depth described in combined depth map 110 by creating from left vertical shift pixel
Build right frame.
Fig. 2 is the depth map generation module 200 for being configured to generate combined depth map 110 according to one embodiment
Block diagram.Depth map generation module 200 includes color depth diagram generator 202, spatial depth diagram generator 204, movement depth map
Generator 206 and combined depth map module 208.The alternative embodiment of depth map generation module 200 has is retouched with this paper
The embodiment difference stated and/or additional module.Similarly, function can be to be distributed from different mode described herein
Between the modules.
Depth map generation module 200 is configured to communicate with video database 212.In one embodiment, depth map is raw
It is communicated by the network of such as internet etc with video database 212 at module 200.In other embodiments, depth map is raw
It is communicated by hardware or the exclusive data communication technology with video database 212 at module 202.Video database 212 storage from
The monoscopic and three-dimensional video-frequency that each provenance obtains.Video database 212 can additionally or alternatively store individual image.
Perhaps image can be obtained from user for example by user to video warehouse or video support for video in video database 212
Pipe network station uploaded videos.Video in video database 212 includes multiple frames, and each frame has two-dimensional pixel array.Pixel
Particular color can be defined in color space, such as RGB YCbCr color space.
The processing video frame of depth generation module 200 is to generate one or more depth map, one or more depth
Figure describes the pixel depth relative to the plane of delineation in each frame.In one embodiment, if depth generation module 200 generates
Dry depth map, and depth map is combined into the single expression of pixel depth, wherein each depth map use it is different in frame
Depth cue is created.Color depth diagram generator 202, spatial depth diagram generator 204 and the movement of depth generation module 200
Depth map generator 206 generates depth map using different depth cues, which is combined by depth map module 208.
Color depth diagram generator 202 receives video frame 102 as input, and is generated using color tips for frame
Depth map to determine the depth of pixel.Generally speaking, color depth diagram generator 202 is based on associated pixel color and depth
The rule of heuristic definition is associated with different depth by different colours (or range of color).In one embodiment, this
The rule of sample is defined by the analysis of historical depth data.Color depth diagram generator 202 is analyzed in video database 212
The sample set of image captured with lens stereoscope and that there is the known depth information for being directed to each pixel color.Pixel face
Color can be specified by the triple (triplet) of the intensity of each primary colors in instruction pixel.For example, in RGB color,
White can be indicated by (100%, 100%, 100%), (255,255,255) or #FFFFFF, indicate red, green and blue component
Maximum intensity.Based on the history color depth data, color depth diagram generator 202 is directed to each color or color gamut
Pixel determine mean depth (or other quality factor).Mean depth can be integrated into color depth priori, such as
By each color triplet look-up table associated with depth value.For example, deep by the color that color depth diagram generator 202 generates
Degree priori can indicate small depth value associated with the pixels with more red (that is, closer to frame before), and
Greater depths value (that is, closer to back side of frame) associated with the pixel with more blues.Such relationship can be derived from
The object of such as sky that is typically found in image background or tree (mainly there is blue) etc, and pair of such as people etc
As (mainly having red) is conventionally positioned in prospect.
In another embodiment, color depth diagram generator 202 can red and blue component based on pixel color
Relative intensity is using look-up table (or equivalent functions) come by lower depth value (that is, closer to frame before) and red pixel phase
Association and will more advanced angle value (that is, closer to back side of frame) it is associated with blue pixel.For example, in YCbCr color space
In, look-up table (or equivalent functions) can be by the linear combination of the blue (Cb) of pixel and red (Cr) difference component and determination
Pixel depth association.It is usually associated it is assumed that color depth function can with close to the object at the back side of frame based on blue pixel
To be weighted so that biggish Cb component leads to biggish pixel depth, and biggish Cr ingredient leads to smaller or negative pixel
Depth.For example, pixel depth Dcolor can be by the color depth function representation with following form:
Dcolor=α (Cb)+(1- α) (β-Cr) (2)
Wherein, α and β is exported from pixel.Value β indicates the size of the range of the probable value of Cb and Cr.For example, if Cb and Cr
It can have any value between 0 and 255, then β is equal to 255.
In one embodiment, color depth diagram generator 202 determines α by executing principal component analysis, the principal component
Analysis determine between the difference component Cb and Cr of the pixel of analyzed image or multiple image in image (or across several
Image) direction of largest extension.After the RGB of the color of pixel expression is converted to YCbCr expression, if applicable, face
Color depth diagram generator 202 determines a and b for the pixel value of each analysis, wherein a=Cr-128 and b=Cb-128.Three
Different desired values is calculated: Sa=E (a2), Sb=E (b2) and sab=E (ab), wherein expectation E (z) is analyzed all
Pixel z average value.Desired value Sa、SbAnd SabFor creating Matrix C, Matrix C is defined as:
Principal component analysis determines the characteristic value and feature vector of C, and selects the larger value corresponded in two characteristic values
Feature vector v.When being scaled so that its element summation is 1, v has element α and 1- α.Color depth diagram generator 202 makes
Color depth Figure 104 for video frame 102 is generated with the color depth function of equation (2).
In one embodiment, color depth diagram generator 202 passes through according to description outdoor scene or indoor scene pair
Image is classified to improve color depth figure.The classification of image can include indoor and outdoor and background image by collecting
Training set of images determines that each image is marked using its classification.From training image extraction feature, such as each image
The color of pixel.Color depth diagram generator 202 is based on image mark using the classifier of such as support vector machines (SVM) etc
Label construct the model for classifying to image according to extracted feature.Different face can be generated for each classification
Color depth priori.When receiving new non-classified image, color depth diagram generator 202 extracts identical spy from new images
It levies and the model of application training is to determine the classification of new images.Then, it is determined from the color depth priori for image classification
The depth of pixel in image.
Spatial depth diagram generator 204 is generated based on the mean pixel depth at each position in frame for video frame
102 another depth map.In order to determine mean pixel depth, spatial depth diagram generator 204 is analyzed in video database 212
The sample set of the image of the known depth information for each location of pixels is captured and had with lens stereoscope.Location of pixels
(x, y) can be indicated according to actual coordinate, or according to the relative position of the percentage based on the offset with image origin
It indicates, such as (x%, y%), wherein x% is the percentage for the total picture traverse of given pixel.Therefore, 640 × 480
The pixel at (320,240) in image is at position (0.5,0.5).By to across a large amount of 3D renderings in pre-position
Known pixels depth is averaged, and space diagram generator 204, which generates spatial depth priori, (indicates the pixel depth at each position
Assembly average) and variance priori (variance for indicating the pixel depth at each position).Spatial depth priori can be matched
It is set to location of pixels and the associated look-up table of depth.Similarly, variance priori can be configured as location of pixels and depth
The associated look-up table of variance.
Due to being generally positioned in the prospect of image close to the center of frame and the object of bottom, by space diagram generator
The 204 spatial depth priori generated can indicate small depth associated with the pixel at the center and bottom that are positioned close to image
The big depth value of angle value and the pixel for close top and two sides.In one embodiment, spatial depth diagram generator
204 determine several spatial depth priori, determine a space for each of several possible scene classifications scene classification
Depth priori.For example, spatial depth diagram generator 204 can be generated by support vector cassification as described above for room
Outer and indoor scene isolated spatial depth priori.In one embodiment, when spatial depth diagram generator 204 receives haplopia
When field video frame 102 is as input, the pixel for image is arranged by the position according to pixel in picture depth priori for it
Depth value generate spatial depth Figure 106；The determination is made for each pixel (or pixel group) in image.Another
In one embodiment, spatial depth diagram generator 204 can scale the value specified by spatial depth priori to generate for pixel
Depth value.For example, the average value in spatial depth priori can be scaled to become the image for falling into " outdoor " classification
It is bigger, illustrate the potentially bigger depth of field in outdoor scene.
The movement of the pixel of video frame 102 of the depth map generator 206 based on the movement relative to camera is moved, generates and uses
In the depth map of video frame 102.Movement depth map generator 206 is generally proximal to before frame using the object with largest motion
Hypothesis determine depth.Fig. 3, which is illustrated, to be used by movement depth map generator 206 to calculate movement and the base between two frames
The process of depth is determined in movement.
In order to calculate movement, movement depth map generator 206 receives two or more video frame as input, such as regards
Frame in frequency frame 102 and video sequence before frame 102.It is calculated using such as being detected for feature well known by persons skilled in the art
Method extracts (302) feature from frame.These features may include any characteristics of image in multiple images feature, and such as color is special
Levy (for example, tone and saturation degree in hsv color space), textural characteristics (for example, coming from gal cypress (Gabor) small echo), edge
Feature those of (for example, detected by Tuscany (Canny) edge finder), line feature are (for example, by probability Hough (Hough)
Change detection to those of) or such as SIFT (Scale invariant features transform), GLOH (gradient locations towards histogram), LESH
The feature of (shape histogram based on local energy) or SURF (accelerating robust feature) etc.In one embodiment, high
This-Laplce's (Laplacian-of-Gaussian) filter is used to detect point of interest in a frame, and passes through calculating
118 Wei Jiabai (Gabor) small echos of textural characteristics on regional area determines local feature.In one embodiment, it moves
Depth map generator 206 is 103Magnitude on from each frame extract feature.
After extracting feature, movement depth map generator 206 is by calculating the extracted characteristic point between input frame
Movement determine the global motions of 304 images.The movement of global motion expression camera itself.For example, if camera is from left-hand
The right side is panned with fixed rate while capturing video, then video will have the global motion for corresponding to the fixed rate.In order to true
Determine global flow, it is assumed that the object with local motion in video by include each frame pixel small subset；Most of pictures
Element is likely to have identical movement between two frames.It is the global motion of image by the movement that most of pixels are shared.
In one embodiment, random sampling consistency (RANSAC) algorithm is determined for the robust fit of stream, ignores with office
The peripheral pixels of portion's movement are to determine global flow.Pixel without local motion is determined as inlier by RANSAC algorithm, interior
Perithallium is the data point that its distribution can be explained by global flow.RANSAC is described in Martin A.Fischler and Robert
C.Bolles (in June, 1981), " Random Sample Consensus:A Paradigm for Model Fitting
With Application to Image Analysis and Automated Cartography ", Comm.of the ACM
24 (6): it in 381-395, is incorporated herein by reference.
The position of pixel in one frame is mapped to the homography square of its position in the next frame by the output of RANSAC algorithm
Battle array A.For example, to frame I is scheduled on0In position (x0, y0) at and in frame I1In (x1, y1) at pixel, RANSAC determined 3
× 3 homography matrix A is to minimize transformation mistake
For being determined as all pixels of inlier, it is assumed that λ is scalar value.After determining homography matrix, movement is deep
Spend the determinant M of 206 calculating matrix A of diagram generator, the global motion of the pixel of determinant M quantitation video frame 102.
It moves depth map generator 206 and generates (306) total motion vector also for each pixel in image.In a reality
It applies in example, total motion vector is by determining optical flow algorithm well known by persons skilled in the art.For example, optical flow algorithm by
Berthold K.P.Horn and Brian G.Schunck (1981), " Determining Optical Flow ",
Artificial Intelligence 17:185-203 is described.The optical flow algorithm used by movement depth map generator 206
The speed of the pixel between the frame in video, the space of image pixel intensities are measured based on the derivative of the room and time of image pixel intensities
It is solved to time-derivative by related etc the method for such as Block- matching, phase or several calculus of variations.
Movement depth map generator 206 is counted by subtracting the global motion M of (308) frame from the motion vector of single pixel
Calculate the local motion of each pixel.Specifically, local motion is the amplitude of total motion vector and the determinant M of homography matrix A
Between difference.Hypothesis of the very fast mobile object in the prospect of frame be may then based on to determine pixel depth (310).One
In a embodiment, movement depth map generator 206 by threshold application in each pixel local motion with by each pixel classifications
For with movement or without movement.It is determined as that there is those of movement pixel can be endowed the depth value for 0 (by them
It is placed in prospect), and be determined as (they being placed in background without the depth value that the pixel of movement can be endowed 255
In).
Depth map module 208 is by calculating color depth figure, spatial depth figure and the weighted array next life for moving depth map
At combined depth map.Color depth figure weight w1, spatial depth figure weight w2 and movement depth map weight w3 make depth map
Module 208 can generate combined depth map 110 from each depth map in single depth map.In one embodiment, weight
W1, w2 and w3 all have between zero and one and including 0 and 1 value, and summation is 1 together.
In one embodiment, depth map module 208 heuristically determines weight w1, w2 and w3.In another embodiment,
Feature of the weight based on frame is adaptive, and across frame is changed according to the feature of position.
Adaptive color depth map weight
In one embodiment, depth map module 208 determines the color for image based on the distribution of color in image
The adaptive weighting of depth map.Adaptive color depth figure weight w1 expression is able to use color tips to generate depth map
Confidence level.If image has narrow distribution of color, all pixels in image are by color having the same or similar
Color, regardless of their depth in the picture.Therefore, when distribution of color is narrow, such as spatial cues or fortune are more relied on
The alternative depth cue of prompt etc is moved to determine that depth is advantageous.On the other hand, when image has broader distribution of color
When, color depth module 208 will determine more accurate color depth, it is meant that increase color depth when distribution of color is wide
Figure weight is advantageous.
In one embodiment, depth map module 208 is by calculating the color contrast of image come the distribution of quantized color.
For example, depth map module 208 can calculate root mean square (RMS) figure according to following expression based on the intensity of the pixel in image
Image contrast c
For the image of m × n size, IijBe the intensity of the pixel at position (i, j) andIt is the flat of pixel in image
Equal intensity.The value of c is normalized in range [0,1].The upper limit w1_max and lower limit w1_ of color depth figure weight are given respectively
Min, color depth figure weight w1 are determined based on contrast c according to the following formula
W1=w1_min+c (w1_max-w1_min) (4)
In another embodiment, depth map module calculates the face for image based on the discrete entropy calculated for histogram
Color distribution.For example, in YCbCr color space, depth map module 208 is available, and that B (for example, 255) is quantified as in x-axis is a
Color histogram hist_y, hist_cb and hist_cr of vertical bar (bin).Histogram is logical for each color in color space
Road indicates the number of pixels in frame in each color vertical bar.Depth map module 208 calculate each histogram entropy H (x) and
The entropy of uniform histogram with B vertical bar.Indicate the equally distributed uniform histogram tool of all colours in each channel
There is maximum possible entropy H (unif).Calculating H (hist_y), the H for respectively indicating the entropy of the histogram in the channel Y, Cb and Cr
(hist_cb) and after H (hist_cr), depth map module 208 by the ratio to histogram and H (unif) be averaged come
Determine color depth figure weight w1:
In equation (5), w1_max is the upper limit of the heuristic selection to the value of w1.
Adaptive motion depth map weight
In one embodiment, the part of the pixel between two frames or more multiframe of the depth map module 208 based on video
The amount of movement determines the adaptive weighting for moving depth map.If the pixel of image hardly has local motion,
Pixel with similar local motion will be likely to different depth.As a result, adaptive motion depth map weight w2 table
Show using movement and determines the confidence level of depth.
Depth map module 208 calculates adaptive movement depth based on the percentage of the pixel in frame with local motion
Figure weight.In one embodiment, single pixel is endowed binary system motion value, which is in fortune
In dynamic or it is not in movement.Distance threshold can be applied to the difference vector calculated by movement depth map generator 206
Amplitude, so that the pixel for possessing the difference vector with the amplitude higher than threshold value, which is determined to be in movement, (and is endowed fortune
Dynamic value " 1 "), and the pixel for possessing the difference vector with the amplitude lower than threshold value is confirmed as static (and being endowed movement
It is worth " 0 ").After distance threshold is applied to difference vector, depth map module 208 has determined the pixel in frame with local motion
Percentage p, i.e. p=(MV_1/N), wherein MV_1 is the number with the pixel of motion value=1, and N is the pixel in image
Number.
Movement depth map weight w2 is adjusted according to the function of percentage p.In one embodiment, depth estimation module
208 are applied to movement threshold the percentage of the pixel with local movement.If percentage p is higher than movement threshold, by w2
Increase small amount from preset value.If percentage p is lower than movement threshold, w2 is reduced to small amount.Specifically, given threshold value ε
With percentage p, depth estimation module 208 can be by by w2iMultiplied by the value close to 1.0 relative to w2i-1To determine w2iValue,
w2iCorresponding to the movement depth map weight of the pixel in frame i, w2i-1Movement depth map corresponding to the same pixel in frame i-1
Weight.For example, depth estimation module 208 can determine w2 according to the following formulai
Multiplier value (1.02 in this example and 0.98) can be determined heuristically, and any desired value can be by
Depth map module 208 uses.Depth map module 208, which can also define constrained motion depth map weight, can deviate the amount of preset value
W2 bound.
Adaptive space depth map weight
In one embodiment, depth map module 208 determines the sky for image based on the variance of spatial depth priori
Between depth map adaptive weighting.As specified by spatial depth priori, low variance indicates the average depth in pixel position
The high probability of the angle value accurately depth of prediction pixel.The variance priori generated by spatial depth diagram generator 204 describes
In the depth variance of each pixel position.In order to generate the adaptive space depth map for the pixel at position (x, y)
Weight w3, depth map module 208 find the variance at (x, y) in variance priori.If variance is small, depth map module 208
Increase the value of w3, and if variance is big, reduce w3.In one embodiment, depth map module 208 by with by equation (6)
The method of description similar method determines w3, by w3 multiplied by predetermined value if variance is higher or lower than preset threshold.
Generate combined depth map
If adaptive weighting be used to generate the combined depth map for being used for image, depth map module 208 can be
Three weight summations are that one or two adaptive weightings are determined using method as described above simultaneously under 1.0 constraint
And Weighted residue is calculated based on identified weight.For example, if one adaptive weighting of generation of depth map module 208 is (all
Such as adaptive w1), remaining two weights can be defined as to be made with fixed ratio cc
α=w2/w3 (7)
Then, the value of w2 and w3 can be determined by following formula
And
Alternatively, if depth map module 208 generates two adaptive weightings, third weight can be by from 1.0
Binding occurrence subtracts two generated weights to determine.
Fig. 4 is the flow chart for illustrating the process of the combined depth map for generating monoscopic image.The step of process, can
To be executed by depth map generation module 200.Other embodiments can have additional or less step, and can be with not
With sequence execute step.
Depth map generation module 200 accesses the monoscopic image that (402) have multiple pixels.In one embodiment, scheme
It seem the frame of video, such as video frame 102.Determine that (404) are used for by using the color of pixel to determine their depth
The color depth figure of image.The hypothesis with similar depth is generated to color depth figure based on the pixel with Similar color.
In one embodiment, the colouring information of the pixel in 200 access images of depth map generation module and based on historical depth believe
Breath or color depth function calculate color depth figure.
Depth map generation module 200 also determines that (406) are used for by using the position of pixel to determine their depth
The spatial depth figure of image.By to from got in a large amount of 3D renderings at various locations from known pixels depth be averaged
The spatial depth priori that value calculates provides the position of pixel in image and the correlation between the figure of its depth.Implement at one
In example, spatial depth priori is by the position of pixel and the associated look-up table of its depth.
Depth map generation module 200 is determined by using the movement of the pixel between two frames with determining their depth
(408) it is used for the movement depth map of image.Pixel motion by from the pixel between identical two frame it is total movement subtract two frames it
Between global motion determine.
Color depth figure weight, spatial depth figure weight and movement depth map weight are also determined (410).Weight be between
Value between 0 and 1, and summation is 1.0 together.In one embodiment, weight is between images and across each image
Adaptive, illustrate each depth drawing method of the different characteristic in image and the depth for accurate quantification different characteristic can
By property.
Finally, depth map generation module 200 generates the depth map of (412) combination.Combined depth map is by color depth
The color depth figure of figure Weight adds by the spatial depth figure of spatial depth figure Weight and by movement depth map weight
The linear combination of the movement depth map of power.By generating combined depth map, in the offer image of depth map generation module 200 with
The figure for the pixel depth being provided separately by single figure compares the figure of more accurate pixel depth.
Additional configurations consider
The foregoing description of the embodiment of the present invention is presented for illustrative purposes；It is not intended to be exhaustive or incite somebody to action this
Invention is limited to disclosed precise forms.Those skilled in the relevant arts can be appreciated that according to many modifications disclosed above and change
Change is possible.
The some parts of this description describe implementation of the invention in terms of the algorithm of the operation to information and symbol expression
Example.These algorithm descriptions and expression are usually used to others skilled in the art effectively by data processing field technical staff
The essence for conveying them to work.When these operations by functionally, calculate ground or when logically describing, should be appreciated that as by counting
Calculation machine program or equivalent circuit, microcode etc. are realized.In addition, in the case where without loss of generality, it has also been demonstrated that sometimes will
It is convenient that the arrangement of these operations, which is known as module,.Described operation and their associated modules can be embodied in soft
In part, firmware, hardware or any combination thereof.
Any step, operation or processing described herein can use one or more hardware or software mould
Block individually or with other equipment is performed or realizes in combination.In one embodiment, it includes containing that software module, which utilizes,
There is the computer program product of the computer-readable medium of computer program code to realize, computer program code can by with
It is executed in the step of executing description, operation or any or whole computer processor in the process.
The embodiment of the present invention alsos relate to a kind of apparatus for performing the operations herein.The device can be for need
The purpose wanted and specially construct and/or it may include by store computer program selective activation in a computer or
The universal computing device reconfigured.Such computer program can be stored in non-transient, tangible computer-readable deposit
Storage media or suitable for storage e-command any kind of medium in, it is total that medium can be coupled to computer system
Line.In addition, any computing system being related in the present specification may include uniprocessor or can be using in order to increase
Calculated performance and the framework of multiprocessor that designs.
The embodiment of the present invention alsos relate to the product produced by calculating process described herein.Such product can
To include the information derived from calculating process, wherein information is stored on non-transient, tangible computer readable storage medium simultaneously
It and may include any embodiment of computer program product or other data described herein combination.
Finally, the language used in the present specification selects mainly for readable and introduction purpose, and
It may not be chosen so as to describe or limit subject matter.Therefore, the scope of the present invention is intended to not limited by the detailed description
System, but any claim by being issued application based on this is limited.Therefore, the disclosure of the embodiment of the present invention is intended to
Bright property is not intended to limit the scope of the invention, and the scope of the present invention is set forth in following claims.
Claims (18)
1. a kind of method for generating the depth map of image, which comprises
Described image is accessed, described image includes multiple pixels, wherein each pixel has color and position in described image；
The color depth figure of image is determined based on the color of the pixel in image；
Position pixel-based and image is determined for the historical depth information of the pixel of analogous location in other multiple images
Spatial depth figure；
The movement depth map of image is determined based on the pixel motion in image；
Color depth figure weight, spatial depth figure weight and movement depth map weight are determined, based on similar in other multiple images
The historical depth covariance information of pixel in position determines spatial depth figure weight, and historical depth covariance information describes historical depth
The difference of information；With
From by color depth figure Weight color depth figure, by the spatial depth figure of spatial depth figure Weight and by transporting
The movement depth map of dynamic depth map Weight combines to generate the combined depth figure of image.
2. according to the method described in claim 1, wherein determining that the color depth figure weight includes:
Determine the histogram of the distribution of color of description pixel；With
Color depth figure weight is determined based on the distribution of the color described by histogram.
3. according to the method described in claim 2, wherein being determined based on the distribution of the color described by the histogram described
Color depth figure weight includes:
Determine that entropy associated with histogram, the entropy are based on distribution of color；
Determine that the ratio of the entropy and maximum entropy associated with described image, the ratio describe opposite point of the color
Cloth；With
Determine that color depth figure weight, color depth figure weight are directly in direct ratio with the ratio according to ratio.
4. according to the method described in claim 1, wherein determining that the movement depth map weight includes determining to have local motion
Image in pixel percentage, the movement percentage of the depth map weight based on the pixel with local motion.
5. according to the method described in claim 4, further include:
The movement depth map weight of the second image is determined based on the percentage of the pixel in the second image with local motion,
Before the first image of second image in the video sequence；
Determine that the first multiplier, first multiplier have the value greater than 1；
Determine that the second multiplier, second multiplier have the value less than 1；With
The percentage of pixel in the first image with local motion is compared with movement threshold；
Wherein the movement depth map weight of determining the first image includes:
Have the high percentage of the pixel in the first image of local motion in movement threshold in response to determination, by the second image
Depth map weight is moved multiplied by the first multiplier；With
In response to determining that the percentage of the pixel in first image with local motion is lower than movement threshold, by the second image
Depth map weight is moved multiplied by the second multiplier.
6. according to the method described in claim 1, wherein determining the spatial depth figure based on the historical depth covariance information
Weight includes:
Retrieve historical depth covariance information associated with the position in image；
Determine that the first multiplier, first multiplier have the value greater than 1；
Determine that the second multiplier, second multiplier have the value less than 1；
Historical depth covariance information associated with position is compared with variance threshold values；
Wherein determine that the spatial depth figure weight includes:
It is in response to determining that historical depth covariance information associated with the position is higher than the variance threshold values, the space is deep
Figure weight is spent multiplied by first multiplier；With
It is in response to determining that historical depth covariance information associated with the position is lower than the variance threshold values, the space is deep
Figure weight is spent multiplied by second multiplier.
7. a kind of nonvolatile computer readable storage medium stored for generating the computer program instructions of the depth map of image,
The computer program instructions are executed to execute the step of including the following steps:
Described image is accessed, described image includes multiple pixels, wherein each pixel has color and position in described image；
The color depth figure of image is determined based on the color of the pixel in image；
Position pixel-based and image is determined for the historical depth information of the pixel of analogous location in other multiple images
Spatial depth figure；
The movement depth map of image is determined based on the pixel motion in image；
Color depth figure weight, spatial depth figure weight and movement depth map weight are determined, based on similar in other multiple images
The historical depth covariance information of pixel in position determines spatial depth figure weight, and historical depth covariance information describes historical depth
The difference of information；With
From by color depth figure Weight color depth figure, by the spatial depth figure of spatial depth figure Weight and by transporting
The movement depth map of dynamic depth map Weight combines to generate the combined depth figure of image.
8. nonvolatile computer readable storage medium according to claim 7, wherein determining the color depth figure weight
Include:
Determine the histogram of the distribution of color of description pixel；With
Color depth figure weight is determined based on the distribution of the color described by histogram.
9. nonvolatile computer readable storage medium according to claim 8, wherein based on what is described by the histogram
The distribution of color includes: to determine the color depth figure weight
Determine that entropy associated with histogram, the entropy are based on distribution of color；
Determine that the ratio of the entropy and maximum entropy associated with described image, the ratio describe opposite point of the color
Cloth；With
Determine that color depth figure weight, color depth figure weight are directly proportional to the ratio according to ratio.
10. nonvolatile computer readable storage medium according to claim 7, wherein determining the movement depth map weight
Percentage including determining the pixel in the image with local motion, the movement depth map weight are based on having local motion
Pixel percentage.
11. nonvolatile computer readable storage medium according to claim 10, further includes:
The movement depth map weight of the second image is determined based on the percentage of the pixel in the second image with local motion,
Before the first image of second image in the video sequence；
Determine that the first multiplier, first multiplier have the value greater than 1；
Determine that the second multiplier, second multiplier have the value less than 1；With
The percentage of pixel in the first image with local motion is compared with movement threshold；
Wherein the movement depth map weight of determining the first image includes:
Have the high percentage of the pixel in the first image of local motion in movement threshold in response to determination, by the second image
Depth map weight is moved multiplied by the first multiplier；With
In response to determining that the percentage of the pixel in first image with local motion is lower than movement threshold, by the second image
Depth map weight is moved multiplied by the second multiplier.
12. nonvolatile computer readable storage medium according to claim 7, wherein being believed based on the historical depth variance
Breath determines that the spatial depth figure weight includes:
Retrieve historical depth covariance information associated with the position in image；
Determine that the first multiplier, first multiplier have the value greater than 1；
Determine that the second multiplier, second multiplier have the value less than 1；
Historical depth covariance information associated with position is compared with variance threshold values；
Wherein determine that the spatial depth figure weight includes:
It is in response to determining that historical depth covariance information associated with the position is higher than the variance threshold values, the space is deep
Figure weight is spent multiplied by first multiplier；With
It is in response to determining that historical depth covariance information associated with the position is lower than the variance threshold values, the space is deep
Figure weight is spent multiplied by second multiplier.
13. a method of for generating the depth map of image, which comprises
Described image is accessed, described image includes multiple pixels, wherein each pixel has color and position in described image；
The color depth figure of image is determined based on the color of the pixel in image；
Position pixel-based and image is determined for the historical depth information of the pixel of analogous location in other multiple images
Spatial depth figure；
The movement depth map of image is determined based on the pixel motion in image；
Color depth figure weight, spatial depth figure weight and movement depth map weight are determined, based on the image with local motion
In pixel percentage determine movement depth map weight；With
From by color depth figure Weight color depth figure, by the spatial depth figure of spatial depth figure Weight and by transporting
The movement depth map of dynamic depth map Weight combines to generate the combined depth figure of image.
14. according to the method for claim 13, wherein determining that the color depth figure weight includes:
Determine the histogram of the distribution of color of description pixel；With
Color depth figure weight is determined based on the distribution of the color described by histogram.
15. according to the method for claim 14, wherein based on the distribution of the color described by the histogram to determine
Stating color depth figure weight includes:
Determine that entropy associated with histogram, the entropy are based on distribution of color；
Determine that the ratio of the entropy and maximum entropy associated with described image, the ratio describe opposite point of the color
Cloth；With
Determine that color depth figure weight, color depth figure weight are directly proportional to the ratio according to the ratio.
16. according to the method for claim 13, further includes:
The movement depth map weight of the second image is determined based on the percentage of the pixel in the second image with local motion,
Before the first image of second image in the video sequence；
Determine that the first multiplier, first multiplier have the value greater than 1；
Determine that the second multiplier, second multiplier have the value less than 1；With
The percentage of pixel in the first image with local motion is compared with movement threshold；
Wherein the movement depth map weight of determining the first image includes:
Have the high percentage of the pixel in the first image of local motion in movement threshold in response to determination, by the second image
Depth map weight is moved multiplied by the first multiplier；With
In response to determining that the percentage of the pixel in first image with local motion is lower than movement threshold, by the second image
Depth map weight is moved multiplied by the second multiplier.
17. according to the method for claim 13, wherein determining that the spatial depth figure weight includes:
Determine that the historical depth covariance information of the pixel of analogous location in other multiple images, the historical depth covariance information are retouched
State the difference of the historical depth information；With
Spatial depth figure weight is determined based on historical depth covariance information.
18. according to the method for claim 17, wherein determining that the space is deep based on the historical depth covariance information
Degree figure weight includes:
Retrieve the historical depth covariance information associated with the position in image；
Determine that the first multiplier, first multiplier have the value greater than 1；
Determine that the second multiplier, second multiplier have the value less than 1；
Historical depth covariance information associated with position is compared with variance threshold values；
Wherein determine that the spatial depth figure weight includes:
It is in response to determining that historical depth covariance information associated with the position is higher than the variance threshold values, the space is deep
Figure weight is spent multiplied by first multiplier；With
It is in response to determining that historical depth covariance information associated with the position is lower than the variance threshold values, the space is deep
Figure weight is spent multiplied by second multiplier.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/666,566 | 2012-11-01 | ||
US13/666,566 US9098911B2 (en) | 2012-11-01 | 2012-11-01 | Depth map generation from a monoscopic image based on combined depth cues |
CN201380055544.1A CN104756491B (en) | 2012-11-01 | 2013-10-28 | Depth cue based on combination generates depth map from monoscopic image |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201380055544.1A Division CN104756491B (en) | 2012-11-01 | 2013-10-28 | Depth cue based on combination generates depth map from monoscopic image |
Publications (2)
Publication Number | Publication Date |
---|---|
CN107277491A CN107277491A (en) | 2017-10-20 |
CN107277491B true CN107277491B (en) | 2019-04-30 |
Family
ID=50546717
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201710367433.4A Active CN107277491B (en) | 2012-11-01 | 2013-10-28 | Generate the method and corresponding medium of the depth map of image |
CN201380055544.1A Active CN104756491B (en) | 2012-11-01 | 2013-10-28 | Depth cue based on combination generates depth map from monoscopic image |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201380055544.1A Active CN104756491B (en) | 2012-11-01 | 2013-10-28 | Depth cue based on combination generates depth map from monoscopic image |
Country Status (6)
Country | Link |
---|---|
US (2) | US9098911B2 (en) |
EP (1) | EP2915333B8 (en) |
JP (1) | JP6438403B2 (en) |
KR (1) | KR102138950B1 (en) |
CN (2) | CN107277491B (en) |
WO (1) | WO2014068472A1 (en) |
Families Citing this family (64)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9483836B2 (en) * | 2011-02-28 | 2016-11-01 | Sony Corporation | Method and apparatus for real-time conversion of 2-dimensional content to 3-dimensional content |
WO2012174090A2 (en) * | 2011-06-13 | 2012-12-20 | University Of Florida Research Foundation, Inc. | Systems and methods for estimating the structure and motion of an object |
US8995755B2 (en) * | 2011-09-30 | 2015-03-31 | Cyberlink Corp. | Two-dimensional to stereoscopic conversion systems and methods |
JP2013172190A (en) * | 2012-02-17 | 2013-09-02 | Sony Corp | Image processing device and image processing method and program |
JP6140935B2 (en) | 2012-05-17 | 2017-06-07 | キヤノン株式会社 | Image processing apparatus, image processing method, image processing program, and imaging apparatus |
US10298898B2 (en) | 2013-08-31 | 2019-05-21 | Ml Netherlands C.V. | User feedback for real-time checking and improving quality of scanned image |
US9619884B2 (en) * | 2013-10-03 | 2017-04-11 | Amlogic Co., Limited | 2D to 3D image conversion device and method |
US10311595B2 (en) * | 2013-11-19 | 2019-06-04 | Canon Kabushiki Kaisha | Image processing device and its control method, imaging apparatus, and storage medium |
AU2013263760A1 (en) * | 2013-11-28 | 2015-06-11 | Canon Kabushiki Kaisha | Method, system and apparatus for determining a depth value of a pixel |
EP3089102B1 (en) | 2013-12-03 | 2019-02-20 | ML Netherlands C.V. | User feedback for real-time checking and improving quality of scanned image |
EP3092603B1 (en) | 2014-01-07 | 2022-05-11 | ML Netherlands C.V. | Dynamic updating of composite images |
WO2015104236A1 (en) | 2014-01-07 | 2015-07-16 | Dacuda Ag | Adaptive camera control for reducing motion blur during real-time image capture |
WO2015173173A1 (en) * | 2014-05-12 | 2015-11-19 | Dacuda Ag | Method and apparatus for scanning and printing a 3d object |
WO2015175907A1 (en) * | 2014-05-15 | 2015-11-19 | Indiana University Research And Technology Corp. | Three dimensional moving pictures with a single imager and microfluidic lens |
US9940541B2 (en) | 2015-07-15 | 2018-04-10 | Fyusion, Inc. | Artificially rendering images using interpolation of tracked control points |
US10176592B2 (en) | 2014-10-31 | 2019-01-08 | Fyusion, Inc. | Multi-directional structured image array capture on a 2D graph |
US10262426B2 (en) | 2014-10-31 | 2019-04-16 | Fyusion, Inc. | System and method for infinite smoothing of image sequences |
US10275935B2 (en) | 2014-10-31 | 2019-04-30 | Fyusion, Inc. | System and method for infinite synthetic image generation from multi-directional structured image array |
US10726593B2 (en) | 2015-09-22 | 2020-07-28 | Fyusion, Inc. | Artificially rendering images using viewpoint interpolation and extrapolation |
US9292926B1 (en) * | 2014-11-24 | 2016-03-22 | Adobe Systems Incorporated | Depth map generation |
US9679387B2 (en) * | 2015-02-12 | 2017-06-13 | Mitsubishi Electric Research Laboratories, Inc. | Depth-weighted group-wise principal component analysis for video foreground/background separation |
JP2018506797A (en) * | 2015-02-12 | 2018-03-08 | ネクストブイアール・インコーポレイテッド | Method and apparatus for making environmental measurements and / or for using such measurements |
PL412832A1 (en) | 2015-06-24 | 2017-01-02 | Politechnika Poznańska | Method of rendering based on the image of depth and the system for rendering based on the image of depth |
US10147211B2 (en) * | 2015-07-15 | 2018-12-04 | Fyusion, Inc. | Artificially rendering images using viewpoint interpolation and extrapolation |
US10242474B2 (en) | 2015-07-15 | 2019-03-26 | Fyusion, Inc. | Artificially rendering images using viewpoint interpolation and extrapolation |
US10222932B2 (en) | 2015-07-15 | 2019-03-05 | Fyusion, Inc. | Virtual reality environment based manipulation of multilayered multi-view interactive digital media representations |
US10852902B2 (en) | 2015-07-15 | 2020-12-01 | Fyusion, Inc. | Automatic tagging of objects on a multi-view interactive digital media representation of a dynamic entity |
US11095869B2 (en) | 2015-09-22 | 2021-08-17 | Fyusion, Inc. | System and method for generating combined embedded multi-view interactive digital media representations |
US11006095B2 (en) | 2015-07-15 | 2021-05-11 | Fyusion, Inc. | Drone based capture of a multi-view interactive digital media |
CN108141606B (en) * | 2015-07-31 | 2022-03-01 | 港大科桥有限公司 | Method and system for global motion estimation and compensation |
US11783864B2 (en) | 2015-09-22 | 2023-10-10 | Fyusion, Inc. | Integration of audio into a multi-view interactive digital media representation |
US10033926B2 (en) * | 2015-11-06 | 2018-07-24 | Google Llc | Depth camera based image stabilization |
US10372968B2 (en) * | 2016-01-22 | 2019-08-06 | Qualcomm Incorporated | Object-focused active three-dimensional reconstruction |
DE102016208056A1 (en) * | 2016-05-11 | 2017-11-16 | Robert Bosch Gmbh | Method and device for processing image data and driver assistance system for a vehicle |
CN109154499A (en) * | 2016-08-18 | 2019-01-04 | 深圳市大疆创新科技有限公司 | System and method for enhancing stereoscopic display |
US11202017B2 (en) | 2016-10-06 | 2021-12-14 | Fyusion, Inc. | Live style transfer on a mobile device |
CN106981079A (en) * | 2016-10-26 | 2017-07-25 | 李应樵 | A kind of method adjusted based on weight adaptive three-dimensional depth |
CN106447719B (en) * | 2016-10-31 | 2019-02-12 | 成都通甲优博科技有限责任公司 | A kind of method that monocular-camera obtains depth map |
CN106504289B (en) * | 2016-11-02 | 2019-12-17 | 深圳乐动机器人有限公司 | indoor target detection method and device |
US10437879B2 (en) | 2017-01-18 | 2019-10-08 | Fyusion, Inc. | Visual search using multi-view interactive digital media representations |
CN107133982B (en) * | 2017-04-28 | 2020-05-15 | Oppo广东移动通信有限公司 | Depth map construction method and device, shooting equipment and terminal equipment |
US10313651B2 (en) | 2017-05-22 | 2019-06-04 | Fyusion, Inc. | Snapshots at predefined intervals or angles |
EP3418975A1 (en) * | 2017-06-23 | 2018-12-26 | Koninklijke Philips N.V. | Depth estimation for an image |
US11069147B2 (en) | 2017-06-26 | 2021-07-20 | Fyusion, Inc. | Modification of multi-view interactive digital media representation |
US10776992B2 (en) * | 2017-07-05 | 2020-09-15 | Qualcomm Incorporated | Asynchronous time warp with depth data |
CN109842789A (en) * | 2017-11-28 | 2019-06-04 | 奇景光电股份有限公司 | Depth sensing device and depth sensing method |
US11062479B2 (en) * | 2017-12-06 | 2021-07-13 | Axalta Coating Systems Ip Co., Llc | Systems and methods for matching color and appearance of target coatings |
CN110349196B (en) * | 2018-04-03 | 2024-03-29 | 联发科技股份有限公司 | Depth fusion method and device |
CN108537836A (en) * | 2018-04-12 | 2018-09-14 | 维沃移动通信有限公司 | A kind of depth data acquisition methods and mobile terminal |
US11853713B2 (en) * | 2018-04-17 | 2023-12-26 | International Business Machines Corporation | Graph similarity analytics |
US10592747B2 (en) | 2018-04-26 | 2020-03-17 | Fyusion, Inc. | Method and apparatus for 3-D auto tagging |
CN110889851B (en) * | 2018-09-11 | 2023-08-01 | 苹果公司 | Robust use of semantic segmentation for depth and disparity estimation |
CN110084826B (en) * | 2018-11-30 | 2023-09-12 | 叠境数字科技（上海）有限公司 | Hair segmentation method based on TOF camera |
US11164326B2 (en) * | 2018-12-18 | 2021-11-02 | Samsung Electronics Co., Ltd. | Method and apparatus for calculating depth map |
CN111340922A (en) * | 2018-12-18 | 2020-06-26 | 北京三星通信技术研究有限公司 | Positioning and mapping method and electronic equipment |
US11449769B2 (en) * | 2019-04-11 | 2022-09-20 | International Business Machines Corporation | Cognitive analytics for graphical legacy documents |
JP7257272B2 (en) * | 2019-06-24 | 2023-04-13 | 日本放送協会 | DEPTH MAP GENERATION DEVICE AND PROGRAM THEREOF, AND 3D IMAGE GENERATION DEVICE |
CN110400344B (en) * | 2019-07-11 | 2021-06-18 | Oppo广东移动通信有限公司 | Depth map processing method and device |
US11470299B2 (en) * | 2019-09-27 | 2022-10-11 | Nevermind Capital Llc | Methods and apparatus for encoding frames captured using fish-eye lenses |
KR102262832B1 (en) * | 2019-11-29 | 2021-06-08 | 연세대학교 산학협력단 | Device and Method for Estimating Depth of Monocular Video Image |
US11450018B1 (en) | 2019-12-24 | 2022-09-20 | X Development Llc | Fusing multiple depth sensing modalities |
CN111260597B (en) * | 2020-01-10 | 2021-12-03 | 大连理工大学 | Parallax image fusion method of multiband stereo camera |
CN112785575B (en) * | 2021-01-25 | 2022-11-18 | 清华大学 | Image processing method, device and storage medium |
JP7303930B1 (en) | 2022-07-27 | 2023-07-05 | 維沃移動通信有限公司 | Image processing method, device, electronic device and readable storage medium |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN102263979A (en) * | 2011-08-05 | 2011-11-30 | 清华大学 | Depth map generation method and device for plane video three-dimensional conversion |
CN102360489A (en) * | 2011-09-26 | 2012-02-22 | 盛乐信息技术（上海）有限公司 | Method and device for realizing conversion from two-dimensional image to three-dimensional image |
Family Cites Families (35)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP3949796B2 (en) | 1997-11-06 | 2007-07-25 | 株式会社ブリヂストン | Tire shape determination device |
US7352386B1 (en) | 1999-06-22 | 2008-04-01 | Microsoft Corporation | Method and apparatus for recovering a three-dimensional scene from two-dimensional images |
JP2001320731A (en) * | 1999-11-26 | 2001-11-16 | Sanyo Electric Co Ltd | Device for converting two-dimensional image into there dimensional image and its method |
CA2472272A1 (en) | 2001-11-24 | 2003-06-05 | Tdv Technologies Corp. | Generation of a stereo image sequence from a 2d image sequence |
JP4214976B2 (en) * | 2003-09-24 | 2009-01-28 | 日本ビクター株式会社 | Pseudo-stereoscopic image creation apparatus, pseudo-stereoscopic image creation method, and pseudo-stereoscopic image display system |
JP4770154B2 (en) | 2004-11-08 | 2011-09-14 | ソニー株式会社 | Image processing apparatus, image processing method, and computer program |
CA2553473A1 (en) | 2005-07-26 | 2007-01-26 | Wa James Tam | Generating a depth map from a tw0-dimensional source image for stereoscopic and multiview imaging |
CA2653815C (en) | 2006-06-23 | 2016-10-04 | Imax Corporation | Methods and systems for converting 2d motion pictures for stereoscopic 3d exhibition |
US8330801B2 (en) | 2006-12-22 | 2012-12-11 | Qualcomm Incorporated | Complexity-adaptive 2D-to-3D video sequence conversion |
CA2627999C (en) | 2007-04-03 | 2011-11-15 | Her Majesty The Queen In Right Of Canada, As Represented By The Minister Of Industry Through The Communications Research Centre Canada | Generation of a depth map from a monoscopic color image for rendering stereoscopic still and video images |
ATE507542T1 (en) * | 2007-07-03 | 2011-05-15 | Koninkl Philips Electronics Nv | CALCULATION OF A DEPTH MAP |
WO2009011492A1 (en) * | 2007-07-13 | 2009-01-22 | Samsung Electronics Co., Ltd. | Method and apparatus for encoding and decoding stereoscopic image format including both information of base view image and information of additional view image |
US8463019B2 (en) | 2007-07-19 | 2013-06-11 | JVC Kenwood Corporation | Pseudo 3D image generation device, image encoding device, image encoding method, image transmission method, image decoding device, and image decoding method |
JP4886898B2 (en) * | 2007-07-26 | 2012-02-29 | コーニンクレッカ フィリップス エレクトロニクス エヌ ヴィ | Method and apparatus for communicating depth related information |
KR101506926B1 (en) | 2008-12-04 | 2015-03-30 | 삼성전자주식회사 | Method and appratus for estimating depth, and method and apparatus for converting 2d video to 3d video |
US8553972B2 (en) * | 2009-07-06 | 2013-10-08 | Samsung Electronics Co., Ltd. | Apparatus, method and computer-readable medium generating depth map |
CN101945295B (en) * | 2009-07-06 | 2014-12-24 | 三星电子株式会社 | Method and device for generating depth maps |
US8644624B2 (en) | 2009-07-28 | 2014-02-04 | Samsung Electronics Co., Ltd. | System and method for indoor-outdoor scene classification |
KR101699920B1 (en) * | 2009-10-07 | 2017-01-25 | 삼성전자주식회사 | Apparatus and method for controling depth |
US8537200B2 (en) | 2009-10-23 | 2013-09-17 | Qualcomm Incorporated | Depth map generation techniques for conversion of 2D video data to 3D video data |
US20110122225A1 (en) | 2009-11-23 | 2011-05-26 | General Instrument Corporation | Depth Coding as an Additional Channel to Video Sequence |
US9042636B2 (en) | 2009-12-31 | 2015-05-26 | Disney Enterprises, Inc. | Apparatus and method for indicating depth of one or more pixels of a stereoscopic 3-D image comprised from a plurality of 2-D layers |
JP5227993B2 (en) * | 2010-03-31 | 2013-07-03 | 株式会社東芝 | Parallax image generation apparatus and method thereof |
KR20110124473A (en) | 2010-05-11 | 2011-11-17 | 삼성전자주식회사 | 3-dimensional image generation apparatus and method for multi-view image |
US8432392B2 (en) * | 2010-09-02 | 2013-04-30 | Samsung Electronics Co., Ltd. | Display system with image conversion mechanism and method of operation thereof |
US8836765B2 (en) | 2010-11-05 | 2014-09-16 | Chung-Ang University Industry-Academy Cooperation Foundation | Apparatus and method for generating a fully focused image by using a camera equipped with a multi-color filter aperture |
US9171372B2 (en) * | 2010-11-23 | 2015-10-27 | Qualcomm Incorporated | Depth estimation based on global motion |
JP5963422B2 (en) | 2010-12-17 | 2016-08-03 | キヤノン株式会社 | Imaging apparatus, display apparatus, computer program, and stereoscopic image display system |
JP5242667B2 (en) * | 2010-12-22 | 2013-07-24 | 株式会社東芝 | Map conversion method, map conversion apparatus, and map conversion program |
EP2509324A1 (en) * | 2011-04-08 | 2012-10-10 | Thomson Licensing | Method and apparatus for analyzing stereoscopic or multi-view images |
JP5291755B2 (en) | 2011-04-21 | 2013-09-18 | 株式会社エム・ソフト | Stereoscopic image generation method and stereoscopic image generation system |
CN102196292B (en) * | 2011-06-24 | 2013-03-06 | 清华大学 | Human-computer-interaction-based video depth map sequence generation method and system |
US8817073B2 (en) * | 2011-08-12 | 2014-08-26 | Himax Technologies Limited | System and method of processing 3D stereoscopic image |
US9501834B2 (en) * | 2011-08-18 | 2016-11-22 | Qualcomm Technologies, Inc. | Image capture for later refocusing or focus-manipulation |
US20140098100A1 (en) * | 2012-10-05 | 2014-04-10 | Qualcomm Incorporated | Multiview synthesis and processing systems and methods |
-
2012
- 2012-11-01 US US13/666,566 patent/US9098911B2/en active Active
-
2013
- 2013-10-28 EP EP13851154.8A patent/EP2915333B8/en active Active
- 2013-10-28 CN CN201710367433.4A patent/CN107277491B/en active Active
- 2013-10-28 WO PCT/IB2013/059733 patent/WO2014068472A1/en active Application Filing
- 2013-10-28 KR KR1020157007981A patent/KR102138950B1/en active IP Right Grant
- 2013-10-28 CN CN201380055544.1A patent/CN104756491B/en active Active
- 2013-10-28 JP JP2015538620A patent/JP6438403B2/en active Active
-
2015
- 2015-06-29 US US14/754,365 patent/US9426449B2/en active Active
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN102263979A (en) * | 2011-08-05 | 2011-11-30 | 清华大学 | Depth map generation method and device for plane video three-dimensional conversion |
CN102360489A (en) * | 2011-09-26 | 2012-02-22 | 盛乐信息技术（上海）有限公司 | Method and device for realizing conversion from two-dimensional image to three-dimensional image |
Also Published As
Publication number | Publication date |
---|---|
CN104756491A (en) | 2015-07-01 |
KR102138950B1 (en) | 2020-07-28 |
EP2915333B8 (en) | 2017-11-22 |
US9098911B2 (en) | 2015-08-04 |
KR20150079576A (en) | 2015-07-08 |
US20140118494A1 (en) | 2014-05-01 |
US9426449B2 (en) | 2016-08-23 |
EP2915333B1 (en) | 2017-10-11 |
JP6438403B2 (en) | 2018-12-12 |
EP2915333A1 (en) | 2015-09-09 |
WO2014068472A1 (en) | 2014-05-08 |
JP2016500975A (en) | 2016-01-14 |
CN104756491B (en) | 2017-07-25 |
CN107277491A (en) | 2017-10-20 |
EP2915333A4 (en) | 2015-11-04 |
US20150304630A1 (en) | 2015-10-22 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN107277491B (en) | Generate the method and corresponding medium of the depth map of image | |
KR102319177B1 (en) | Method and apparatus, equipment, and storage medium for determining object pose in an image | |
CN102741879B (en) | Method for generating depth maps from monocular images and systems using the same | |
CN109360235A (en) | A kind of interacting depth estimation method based on light field data | |
CN111988593B (en) | Three-dimensional image color correction method and system based on depth residual optimization | |
Li et al. | HDRFusion: HDR SLAM using a low-cost auto-exposure RGB-D sensor | |
Kuo et al. | Depth estimation from a monocular view of the outdoors | |
KR101125061B1 (en) | A Method For Transforming 2D Video To 3D Video By Using LDI Method | |
Fan et al. | Vivid-DIBR based 2D–3D image conversion system for 3D display | |
Zhang et al. | Interactive stereoscopic video conversion | |
JP2013005025A (en) | Stereoscopic image generating device, stereoscopic image generating method, program, and recording medium | |
AU2016273979A1 (en) | System and method for adjusting perceived depth of an image | |
Lee et al. | Estimating scene-oriented pseudo depth with pictorial depth cues | |
Yang et al. | A depth map generation algorithm based on saliency detection for 2D to 3D conversion | |
GB2585197A (en) | Method and system for obtaining depth data | |
CN102708570A (en) | Method and device for obtaining depth map | |
CN108900825A (en) | A kind of conversion method of 2D image to 3D rendering | |
Cheng et al. | A novel saliency model for stereoscopic images | |
Cheung et al. | Spatio-temporal disocclusion filling using novel sprite cells | |
Schwarz et al. | Improved edge detection for EWOC depth upscaling | |
Shinde et al. | Image object saliency detection using center surround contrast | |
Rogalska et al. | A model of saliency-based visual attention for movie retrospection | |
CN108062741A (en) | Binocular image processing method, imaging device and electronic equipment | |
CN117315152B (en) | Binocular stereoscopic imaging method and binocular stereoscopic imaging system | |
Helgason et al. | Multiscale framework for adaptive and robust enhancement of depth in multi-view imagery |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |