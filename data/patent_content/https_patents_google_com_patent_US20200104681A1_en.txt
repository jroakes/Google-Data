US20200104681A1 - Neural Networks with Area Attention - Google Patents
Neural Networks with Area Attention Download PDFInfo
- Publication number
- US20200104681A1 US20200104681A1 US16/586,813 US201916586813A US2020104681A1 US 20200104681 A1 US20200104681 A1 US 20200104681A1 US 201916586813 A US201916586813 A US 201916586813A US 2020104681 A1 US2020104681 A1 US 2020104681A1
- Authority
- US
- United States
- Prior art keywords
- area
- items
- attention
- memory
- key
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000013528 artificial neural network Methods 0.000 title claims abstract description 59
- 238000003860 storage Methods 0.000 claims abstract description 12
- 238000012545 processing Methods 0.000 claims description 19
- 230000007246 mechanism Effects 0.000 claims description 14
- 230000009466 transformation Effects 0.000 claims description 2
- 238000000844 transformation Methods 0.000 claims description 2
- 238000004590 computer program Methods 0.000 abstract description 13
- 238000000034 method Methods 0.000 abstract description 9
- 238000003062 neural network model Methods 0.000 abstract 1
- 238000010801 machine learning Methods 0.000 description 7
- 238000013519 translation Methods 0.000 description 7
- 230000008569 process Effects 0.000 description 6
- 239000013598 vector Substances 0.000 description 6
- 238000004364 calculation method Methods 0.000 description 5
- 238000004891 communication Methods 0.000 description 5
- 230000003287 optical effect Effects 0.000 description 4
- 238000012549 training Methods 0.000 description 4
- 230000003993 interaction Effects 0.000 description 3
- 238000011176 pooling Methods 0.000 description 3
- 230000008901 benefit Effects 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 230000001537 neural effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 238000010361 transduction Methods 0.000 description 2
- 230000026683 transduction Effects 0.000 description 2
- 238000011282 treatment Methods 0.000 description 2
- 241000009334 Singa Species 0.000 description 1
- 230000009286 beneficial effect Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000001149 cognitive effect Effects 0.000 description 1
- 230000001143 conditioned effect Effects 0.000 description 1
- 238000013135 deep learning Methods 0.000 description 1
- 238000003745 diagnosis Methods 0.000 description 1
- 238000009826 distribution Methods 0.000 description 1
- 238000012268 genome sequencing Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 238000003058 natural language processing Methods 0.000 description 1
- 238000005457 optimization Methods 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 230000000306 recurrent effect Effects 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000013518 transcription Methods 0.000 description 1
- 230000035897 transcription Effects 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G06N3/0454—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/903—Querying
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/10—Interfaces, programming languages or software development kits, e.g. for simulating neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
Definitions
- This specification relates to processing inputs using neural networks.
- Neural networks are machine learning models that employ one or more layers of nonlinear units to predict an output for a received input.
- Some neural networks include one or more hidden layers in addition to an output layer. The output of each hidden layer is used as input to another layer in the network, i.e., a hidden layer or an output layer.
- Each layer of the network generates an output from a received input in accordance with current values of a respective set of parameters.
- Attentional mechanisms have been used to boost the accuracy on a variety of deep learning tasks. They allow a model to focus selectively on specific pieces of information; a piece of information can be, for example, a word in a sentence for neural machine translation or a region of pixels in image captioning.
- An attentional mechanism typically follows a memory-query paradigm, where the memory M contains a collection of items of information from a source modality, e.g., the embeddings of an image or the hidden states of encoding an input sentence, and the query q comes from a target modality, e.g., the hidden state of a decoder model.
- a source modality e.g., the embeddings of an image or the hidden states of encoding an input sentence
- the query q comes from a target modality, e.g., the hidden state of a decoder model.
- self-attention involves queries and memory from the same modality for either encoder or decoder.
- Each item in the memory has a key-value pair, (k i , v i ), where the key is used to compute the probability a i regarding how well the query matches the item, as expressed in equation 1.
- Typical choices for f att include dot products qk i and a multilayer perceptron.
- the output O q M from querying the memory M with q is then calculated as the sum of all the values in the memory weighted by their probabilities, as expressed in equation 2, which can be fed to other parts of the model for further calculation.
- the model learns to attend to specific pieces of information given a query. For example, it can associate a word in the target sentence with a word in the source sentence for translation tasks.
- Attention mechanisms are typically designed to focus on a predetermined granularity of individual items in the entire memory, where each item defines the granularity of what the model can attend to.
- each item can be a character for a character-level translation model, a word for a word-level model, a grid cell for an image-based model, or a hidden state in a latent space.
- This specification describes a system implemented as computer programs on one or more computers in one or more locations that generates a machine learning model output from a machine learning model input.
- this specification describes a system that uses a self-attention neural network to generate model output from a model input, with at least one of the self-attention layers in the neural network employing an area attention mechanism.
- the output can be a target sequence that includes a respective output at each of multiple positions in an output order
- the input can be an input sequence that includes a respective input at each of multiple positions in an input order
- both the input and output can be a sequence, i.e., the system transduces an input sequence into an target sequence.
- the system may be a neural machine translation system. That is, if the input sequence is a sequence of words or characters in an original language, e.g., a sentence or phrase, the target sequence may be a translation of the input sequence into a target language, i.e., a sequence of words or characters in the target language that represents the sequence of words in the original language.
- the system may be a speech recognition system. That is, if the input sequence is a sequence of audio data representing a spoken utterance, the target sequence may be a sequence of graphemes, characters, or words that represents the utterance, i.e., is a transcription of the input sequence.
- the system may be a natural language processing system.
- the input sequence is a sequence of words in an original language, e.g., a sentence or phrase
- the target sequence may be a summary of the input sequence in the original language, i.e., a sequence that has fewer words than the input sequence but that retains the essential meaning of the input sequence.
- the target sequence can be a sequence of words that form an answer to the question.
- the target sequence may be a sequence that defines a parse tree, e.g., a dependency parse or a constituency parse, of the input sequence.
- the output of the neural network can be a natural language understanding output, e.g., an output for an entailment task, a paraphrase task, a textual similarity task, a sentiment task, a grammaticality task, and so on.
- the system may be part of a computer-assisted medical diagnosis system.
- the input sequence can be a sequence of data from an electronic medical record and the output can be a sequence of predicted treatments or a single predicted treatment to be performed after the last input in the input sequence.
- the system may be an image generation system that generates images conditioned on a particular type of input, e.g., a smaller image, an object category, or a natural language text sequence.
- the system may receive the input and then generate the output image as a sequence of color values, i.e., of color channel values for the pixels of the output image, or as a two-dimensional structure of color values.
- the system may be part of an image captioning system. That is, the system can receive an image, e.g., as a sequence of the color values of the image or a 3D tensor, or an embedding of an image, and then generate a sequence of text that captions the image, i.e., that is a natural language description of the image.
- an image e.g., as a sequence of the color values of the image or a 3D tensor, or an embedding of an image
- a sequence of text that captions the image i.e., that is a natural language description of the image.
- the system may be a system that receives as input one or more video frames.
- the system can receive as input a sequence of video frames and then predict the next frame, e.g. as a sequence of color values or as a two-dimensional structure of color values.
- the system can receive as input a sequence of video frames from a video and then generate a natural language text sequence that describes the video.
- the system may be a system that processes sequences of biological data, e.g., genome sequencing reads.
- the neural network includes one or more area attention layers.
- Each area attention layer is configured to, during the processing of each neural network input by the neural network, receive data specifying a memory including a plurality of items and, for each item, a respective key and a respective value.
- the area attention layer determines a plurality of areas within the memory, wherein (i) each area includes one or more items in the memory, and (ii) one or more of the areas include multiple adjacent items in the memory.
- the area attention layer determines, for each of the plurality of areas, a respective area key and a respective area value from at least the keys and values of the items in the area.
- the area attention layer then receives an attention query and applies an attention mechanism between the attention query and the area keys for each area to generate a respective attention weight for each area.
- the area attention layer then generates an area attention layer output by combining the area values for each area in accordance with the attention weights.
- An area attention layer can be used in a neural network in place of some or all of the conventional attention layers in the neural network.
- some neural networks include an encoder neural network, a decoder neural network, and an attention layer.
- the encoder neural network generates a respective encoded representation for each of multiple portions of the neural network input and provides the encoded representation to the attention layer.
- the encoder neural network can generate a respective encoded representation of each element in the sequence.
- the encoder neural network can generate a respective encoded representation of multiple spatial regions within the image or can receive an embedding of the image and generate the respective encoded representation for each of the spatial regions from the embedding.
- the decoder neural network generates the neural network output by querying the attention layer, i.e., by providing hidden states of certain components to the decoder neural network layer and using received attention layer outputs to generate the network output.
- the area attention neural network layer can be used in place of the attention layer, with the items in the memory corresponding to the multiple portions, the keys and values both being the encoded representations, and the attention query being the query from the decoder.
- neural networks employ an attention-based architecture that are made up of multiple layers that employ multi-head attention.
- An example of such a neural network architecture is the Transformer architecture described in Vaswani, et al., Attention is all you need, http://arxiv.org/abs/1706.03762.
- the area attention layer can be employed in place of some or all of the attention heads in the multi-head attention layers.
- the area attention layer is an attention head in a self-attention layer
- the queries, keys, and values are all derived from the same input, i.e., by the multi-head attention layer before being provided to the area attention layer.
- the area attention layer is an attention head in an encoder-decoder attention layer
- the queries are derived from the decoder input to the encoder-decoder attention layer
- the keys and values are derived from the encoded representations of the neural network input.
- the described area attention layer also generates attention weights for areas of items, i.e., groups of multiple items within a memory. This allows the area attention layer to capture rich alignment distributions by focusing on multiple items when beneficial for processing the current input instead of being limited to focusing on a single item.
- the described systems can achieve high quality and even state-of-the-art results on a variety of tasks, e.g., machine translation and image captioning
- a sequence transduction neural network can transduce sequences more accurately than existing networks that are based on convolutional layers or recurrent layers. Additionally, the use of an attention-based architecture allows the sequence transduction neural network to transduce sequences quicker, to be trained faster, or both, e.g., because the operation of the network can be more easily parallelized.
- FIG. 1 illustrates example areas of attention in relation to a one-dimensional memory.
- FIG. 2 illustrates example areas of attention in relation to a two-dimensional memory.
- FIG. 3 illustrates a system of one or more computers implementing a neural network system with an area attention layer.
- FIG. 4 and FIG. 5 show pseudocode for performing area attention calculations.
- area attention is a general mechanism for a model to attend to a group of items in the memory that are structurally adjacent.
- a memory contains a collection of individual items with a predefined, fixed granularity, e.g., a word token or an image grid.
- each unit for attention calculation is an area that can contain or represent one or more than one item of the original memory. Because each of the areas can aggregate a varying number of items, in area attention, the granularity of attention is learned from training data rather than being predetermined.
- Area attention can be applied to both single or multi-head attention mechanisms. In multi-head attention, with each head using area attention, a model can attend to multiple areas in the memory.
- Area attention can be applied by any or all of the attention layers in neural network. That is, the area attention mechanism can replace a conventional attention mechanism employed by any or all of the attention layers in a neural network.
- FIG. 1 illustrates example areas of attention in relation to a one-dimensional memory, i.e., a memory that contains a one-dimensional structure of items.
- An area is a group of structurally adjacent items in the memory, e.g., spatially for a two-dimensional memory arrangement, e.g., for images, or temporally for a one-dimensional memory arrangement, e.g., for natural language sentences.
- the memory consists of a sequence of items, i.e., has a one-dimensional structure
- an area is a range of items that are sequentially or temporally adjacent; and the number of items in the area can be one or more than one.
- Many language-related tasks e.g., machine translation or sequence prediction tasks, fall in the one-dimensional case.
- FIG. 1 shows an example in which the original memory 100 is a 4-item sequence of items 11 , 12 , 13 , 14 .
- area memory 110 is formed where each item 21 - 29 in the area memory is a combination of multiple adjacent items in the original memory. Items 21 - 24 are combinations of one item; items 25 - 27 are combinations of two; and items 28 - 29 are combinations of three.
- the maximum area size a system will consider for a task can be limited. In the example of FIG. 1 , the maximum area size is three items.
- FIG. 2 illustrates example areas of attention in relation to a two-dimensional memory 200 , i.e., a memory that contains a two-dimensional structure of items 31 - 39 .
- a two-dimensional memory 200 i.e., a memory that contains a two-dimensional structure of items 31 - 39 .
- an area can be any rectangular region in the grid.
- the maximum area size the system will consider for a task can be limited in the two-dimensional case as well. For a two-dimensional area, this can be done by setting the maximum height and width for each area.
- the original memory is a 3 ⁇ 3 grid of items and the maximum height and width allowed for each area is two items.
- the area memory 210 thus contains areas of dimension 1 ⁇ 1 with area items 41 - 49 ; areas of dimension 1 ⁇ 2 with area items 51 - 56 , each of dimension 1 ⁇ 2; areas of dimension 2 ⁇ 1 with area items 61 - 66 , each of dimension 2 ⁇ 1; and areas of dimensions 2 ⁇ 2 with area items 71 - 74 , each of dimension 2 ⁇ 2.
- area item 47 is a 1 ⁇ 1 area and therefore includes only item 37
- area item 56 is a 1 ⁇ 2 area and includes items 33 and 36
- area item 66 is a 2 ⁇ 1 area and includes items 38 and 39
- area item 71 is a 2 ⁇ 2 area and includes items 32 , 33 , 36 , and 36 .
- the remaining components of the neural network can learn which levels of granularity are important for a particular task during training of the neural network system.
- a key and value are defined for each area that contains or represents one or multiple items in the original memory.
- the system can then receive an attention query and apply an attention mechanism using the area keys and values.
- FIG. 3 illustrates a system 300 of one or more computers 310 implementing a neural network system 320 with an area attention layer 330 .
- the neural network system is configured to receive a neural network input 322 and to generate a neural network output 324 .
- the area attention layer is configured to perform operations to implement area attention during the processing of the neural network input.
- the attention layer is configured to receive data specifying a memory 332 that contains multiple items and, for each item, a respective key and a respective value, and to determine multiple areas within the memory 324 .
- Each area includes one or more items in the memory, and one or more of the areas include multiple adjacent items in the memory.
- the attention layer is further configured to determine, for each of the areas, a respective area key and a respective area value 326 from at least the keys and values of the items in the area; receive an attention query 328 ; apply an attention mechanism between the attention query and the area keys for each area to generate a respective attention weight for each area 330 ; and generate an area attention layer output 332 by combining the area values for each area in accordance with the attention weights.
- the area attention layer is further configured to provide the area attention layer output 334 to another component of the neural network system.
- the memory is arranged as a sequence of items, and determining multiple areas includes identifying, as different areas, each combination of adjacent items that includes no more than a maximum number of items.
- the memory is arranged as a two-dimensional grid of items, and determining multiple areas includes identifying, as different areas, each rectangular region of items within the two-dimensional grid that has no more than a maximum height and no more than a maximum width.
- the maximums can be predetermined as hyperparameter values or determined using a hyperparameter search technique.
- the area attention layer is configured to determine, for each of the areas, a respective area key and a respective area value. In some implementations, the layer does so by determining the area value to be a sum of the values of the items in the area or a mean of the keys of the items in the area, or by determining a plurality of features of the items in the area and combining the features to generate the area key of the area. This combining of features can be done by summing or concatenating the features to generate a combined feature and applying one or more learned non-linear transformations to the combined feature to generate the area key.
- the features that can be combined include one or more of an embedding corresponding to a number of items in the area, a mean of the keys of the items in the area, or a variance of the keys of the items in the area.
- the area key and area value for the area are the key and value for the single item.
- each item corresponds to a portion of the neural network input
- the value for each memory item is an encoded representation of the neural network input
- other components of the neural network system provide as input to the attention layer the values and keys for the items in the memory and the attention query.
- the key is the same as the value the item in the memory. In other cases, the key and the value are different for each item in the memory.
- the key of an area is defined simply as the mean vector of the key of each item in the area, as expressed in equation 3.
- the value of an area is defined as the sum of all value vectors of the items in the area, as expressed in equation 4.
- equations 3 and 4 use average and sum pooling over an area of vectors. It is possible to use other pooling methods, e.g., max pooling, to compute the key and value vector for each area.
- Richer representations of each area can also be derived by using features other than the mean of the key vectors of the area. For example, a system can consider the standard deviation of the key vectors within each area, shown in equation 5, below.
- the system can also consider other features, e.g., the height and width of each area.
- the system can process the features, i.e., the mean, the standard deviation and, if used, features identifying the height and width of the area, using a multi-layer perceptron neural network that is trained jointly with the neural network system to generate the key for the area.
- a system can implement an optimization technique known as a summed area table.
- the summed area table is based on an integral image, I, which can be efficiently computed in a single pass of the memory. With the integral image, the system can calculate the key and value of each area in constant time.
- FIG. 4 and FIG. 5 show pseudocode for performing area attention calculations. In particular, they perform the calculations of equations 3, 4 and 5 for all areas that have less than the maximum size given an input memory grid as well as determine the shape size of each area.
- the operations in the pseudocode can be used to efficiently compute the area keys and area values for all of the areas for a given two-dimensional memory grid.
- the operations in the pseudocode are based on the tensor operations of the TensorFlow open-source software library.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non transitory storage medium for execution by, or to control the operation of, data processing apparatus.
- the computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.
- the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- data processing apparatus refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can also be, or further include, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- the apparatus can optionally include, in addition to hardware, code that creates an execution environment for computer programs, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a computer program which may also be referred to or described as a program, software, a software application, an app, a module, a software module, a script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it can be deployed in any form, including as a stand alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub programs, or portions of code.
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
- the term “database” is used broadly to refer to any collection of data: the data does not need to be structured in any particular way, or structured at all, and it can be stored on storage devices in one or more locations.
- the index database can include multiple collections of data, each of which may be organized and accessed differently.
- engine is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more specific functions.
- an engine will be implemented as one or more software modules or components, installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines can be installed and running on the same computer or computers.
- the processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA or an ASIC, or by a combination of special purpose logic circuitry and one or more programmed computers.
- Computers suitable for the execution of a computer program can be based on general or special purpose microprocessors or both, or any other kind of central processing unit.
- a central processing unit will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data.
- the central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.
- PDA personal digital assistant
- GPS Global Positioning System
- USB universal serial bus
- Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto optical disks e.g., CD ROM and DVD-ROM disks.
- embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's device in response to requests received from the web browser.
- a computer can interact with a user by sending text messages or other forms of message to a personal device, e.g., a smartphone that is running a messaging application, and receiving responsive messages from the user in return.
- Data processing apparatus for implementing machine learning models can also include, for example, special-purpose hardware accelerator units for processing common and compute-intensive parts of machine learning training or production, i.e., inference, workloads.
- Machine learning models can be implemented and deployed using a machine learning framework, e.g., a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework.
- a machine learning framework e.g., a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework.
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (LAN) and a wide area network (WAN), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- a server transmits data, e.g., an HTML page, to a user device, e.g., for purposes of displaying data to and receiving user input from a user interacting with the device, which acts as a client.
- Data generated at the user device e.g., a result of the user interaction, can be received at the server from the device.
Abstract
Description
- This application claims the benefit under 35 U.S.C. § 119(e) of the filing date of U.S. Patent Application No. 62/737,913, for “Neural Networks with Area Attention,” which was filed on Sep. 27, 2018, and which is incorporated here by reference.
- This specification relates to processing inputs using neural networks.
- Neural networks are machine learning models that employ one or more layers of nonlinear units to predict an output for a received input. Some neural networks include one or more hidden layers in addition to an output layer. The output of each hidden layer is used as input to another layer in the network, i.e., a hidden layer or an output layer. Each layer of the network generates an output from a received input in accordance with current values of a respective set of parameters.
- Attentional mechanisms have been used to boost the accuracy on a variety of deep learning tasks. They allow a model to focus selectively on specific pieces of information; a piece of information can be, for example, a word in a sentence for neural machine translation or a region of pixels in image captioning.
- An attentional mechanism typically follows a memory-query paradigm, where the memory M contains a collection of items of information from a source modality, e.g., the embeddings of an image or the hidden states of encoding an input sentence, and the query q comes from a target modality, e.g., the hidden state of a decoder model. In architectures such as Transformer, self-attention involves queries and memory from the same modality for either encoder or decoder. Each item in the memory has a key-value pair, (ki, vi), where the key is used to compute the probability ai regarding how well the query matches the item, as expressed in
equation 1. -
- Typical choices for fatt include dot products qki and a multilayer perceptron. The output Oq M from querying the memory M with q is then calculated as the sum of all the values in the memory weighted by their probabilities, as expressed in
equation 2, which can be fed to other parts of the model for further calculation. -
- During training, the model learns to attend to specific pieces of information given a query. For example, it can associate a word in the target sentence with a word in the source sentence for translation tasks.
- Attention mechanisms are typically designed to focus on a predetermined granularity of individual items in the entire memory, where each item defines the granularity of what the model can attend to. For example, it can be a character for a character-level translation model, a word for a word-level model, a grid cell for an image-based model, or a hidden state in a latent space.
- This specification describes a system implemented as computer programs on one or more computers in one or more locations that generates a machine learning model output from a machine learning model input.
- In particular, this specification describes a system that uses a self-attention neural network to generate model output from a model input, with at least one of the self-attention layers in the neural network employing an area attention mechanism.
- Generally, at least one of the input and output is a sequence. For example, the output can be a target sequence that includes a respective output at each of multiple positions in an output order, the input can be an input sequence that includes a respective input at each of multiple positions in an input order, or both the input and output can be a sequence, i.e., the system transduces an input sequence into an target sequence.
- For example, the system may be a neural machine translation system. That is, if the input sequence is a sequence of words or characters in an original language, e.g., a sentence or phrase, the target sequence may be a translation of the input sequence into a target language, i.e., a sequence of words or characters in the target language that represents the sequence of words in the original language.
- As another example, the system may be a speech recognition system. That is, if the input sequence is a sequence of audio data representing a spoken utterance, the target sequence may be a sequence of graphemes, characters, or words that represents the utterance, i.e., is a transcription of the input sequence.
- As another example, the system may be a natural language processing system. For example, if the input sequence is a sequence of words in an original language, e.g., a sentence or phrase, the target sequence may be a summary of the input sequence in the original language, i.e., a sequence that has fewer words than the input sequence but that retains the essential meaning of the input sequence. As another example, if the input sequence is a sequence of words that form a question, the target sequence can be a sequence of words that form an answer to the question. As yet another example, if the input sequence is a sequence of words, the target sequence may be a sequence that defines a parse tree, e.g., a dependency parse or a constituency parse, of the input sequence. As yet another example, if the input sequence is a sequence of words, the output of the neural network can be a natural language understanding output, e.g., an output for an entailment task, a paraphrase task, a textual similarity task, a sentiment task, a grammaticality task, and so on.
- As another example, the system may be part of a computer-assisted medical diagnosis system. For example, the input sequence can be a sequence of data from an electronic medical record and the output can be a sequence of predicted treatments or a single predicted treatment to be performed after the last input in the input sequence.
- As another example, the system may be an image generation system that generates images conditioned on a particular type of input, e.g., a smaller image, an object category, or a natural language text sequence. In these examples, the system may receive the input and then generate the output image as a sequence of color values, i.e., of color channel values for the pixels of the output image, or as a two-dimensional structure of color values.
- As another example, the system may be part of an image captioning system. That is, the system can receive an image, e.g., as a sequence of the color values of the image or a 3D tensor, or an embedding of an image, and then generate a sequence of text that captions the image, i.e., that is a natural language description of the image.
- As another example, the system may be a system that receives as input one or more video frames. For example, the system can receive as input a sequence of video frames and then predict the next frame, e.g. as a sequence of color values or as a two-dimensional structure of color values. As another example, the system can receive as input a sequence of video frames from a video and then generate a natural language text sequence that describes the video.
- As another example, the system may be a system that processes sequences of biological data, e.g., genome sequencing reads.
- In particular, the neural network includes one or more area attention layers.
- Each area attention layer is configured to, during the processing of each neural network input by the neural network, receive data specifying a memory including a plurality of items and, for each item, a respective key and a respective value.
- The area attention layer then determines a plurality of areas within the memory, wherein (i) each area includes one or more items in the memory, and (ii) one or more of the areas include multiple adjacent items in the memory. The area attention layer determines, for each of the plurality of areas, a respective area key and a respective area value from at least the keys and values of the items in the area.
- The area attention layer then receives an attention query and applies an attention mechanism between the attention query and the area keys for each area to generate a respective attention weight for each area.
- The area attention layer then generates an area attention layer output by combining the area values for each area in accordance with the attention weights.
- An area attention layer can be used in a neural network in place of some or all of the conventional attention layers in the neural network.
- For example, some neural networks include an encoder neural network, a decoder neural network, and an attention layer. The encoder neural network generates a respective encoded representation for each of multiple portions of the neural network input and provides the encoded representation to the attention layer. For example, when the network input is a sequence, the encoder neural network can generate a respective encoded representation of each element in the sequence. When the network input is an image, the encoder neural network can generate a respective encoded representation of multiple spatial regions within the image or can receive an embedding of the image and generate the respective encoded representation for each of the spatial regions from the embedding. The decoder neural network generates the neural network output by querying the attention layer, i.e., by providing hidden states of certain components to the decoder neural network layer and using received attention layer outputs to generate the network output.
- In this example, the area attention neural network layer can be used in place of the attention layer, with the items in the memory corresponding to the multiple portions, the keys and values both being the encoded representations, and the attention query being the query from the decoder.
- As another example, some neural networks employ an attention-based architecture that are made up of multiple layers that employ multi-head attention. An example of such a neural network architecture is the Transformer architecture described in Vaswani, et al., Attention is all you need, http://arxiv.org/abs/1706.03762.
- In these cases the area attention layer can be employed in place of some or all of the attention heads in the multi-head attention layers. When the area attention layer is an attention head in a self-attention layer, the queries, keys, and values are all derived from the same input, i.e., by the multi-head attention layer before being provided to the area attention layer. When the area attention layer is an attention head in an encoder-decoder attention layer, the queries are derived from the decoder input to the encoder-decoder attention layer, and the keys and values are derived from the encoded representations of the neural network input.
- The subject matter described in this specification can be implemented in particular embodiments so as to realize one or more of the following advantages.
- Conventional attention layers within a neural network generate attention weights for only individual items within a memory. The described area attention layer, on the other hand, also generates attention weights for areas of items, i.e., groups of multiple items within a memory. This allows the area attention layer to capture rich alignment distributions by focusing on multiple items when beneficial for processing the current input instead of being limited to focusing on a single item.
- By using an area attention layer within a neural network to replace some or all of the convention attention layers in the neural network, the described systems can achieve high quality and even state-of-the-art results on a variety of tasks, e.g., machine translation and image captioning
- With an encoder and decoder that are area attention-based, a sequence transduction neural network can transduce sequences more accurately than existing networks that are based on convolutional layers or recurrent layers. Additionally, the use of an attention-based architecture allows the sequence transduction neural network to transduce sequences quicker, to be trained faster, or both, e.g., because the operation of the network can be more easily parallelized.
-
FIG. 1 illustrates example areas of attention in relation to a one-dimensional memory. -
FIG. 2 illustrates example areas of attention in relation to a two-dimensional memory. -
FIG. 3 illustrates a system of one or more computers implementing a neural network system with an area attention layer. -
FIG. 4 andFIG. 5 show pseudocode for performing area attention calculations. - Like reference numbers and designations in the various drawings indicate like elements.
- This specification describes an attention mechanism that will be referred to as area attention, which is a general mechanism for a model to attend to a group of items in the memory that are structurally adjacent. In general, in this context, a memory contains a collection of individual items with a predefined, fixed granularity, e.g., a word token or an image grid. In area attention, each unit for attention calculation is an area that can contain or represent one or more than one item of the original memory. Because each of the areas can aggregate a varying number of items, in area attention, the granularity of attention is learned from training data rather than being predetermined. Area attention can be applied to both single or multi-head attention mechanisms. In multi-head attention, with each head using area attention, a model can attend to multiple areas in the memory.
- Area attention can be applied by any or all of the attention layers in neural network. That is, the area attention mechanism can replace a conventional attention mechanism employed by any or all of the attention layers in a neural network.
-
FIG. 1 illustrates example areas of attention in relation to a one-dimensional memory, i.e., a memory that contains a one-dimensional structure of items. An area is a group of structurally adjacent items in the memory, e.g., spatially for a two-dimensional memory arrangement, e.g., for images, or temporally for a one-dimensional memory arrangement, e.g., for natural language sentences. When the memory consists of a sequence of items, i.e., has a one-dimensional structure, an area is a range of items that are sequentially or temporally adjacent; and the number of items in the area can be one or more than one. Many language-related tasks, e.g., machine translation or sequence prediction tasks, fall in the one-dimensional case.FIG. 1 shows an example in which theoriginal memory 100 is a 4-item sequence ofitems area memory 110 is formed where each item 21-29 in the area memory is a combination of multiple adjacent items in the original memory. Items 21-24 are combinations of one item; items 25-27 are combinations of two; and items 28-29 are combinations of three. The maximum area size a system will consider for a task can be limited. In the example ofFIG. 1 , the maximum area size is three items. -
FIG. 2 illustrates example areas of attention in relation to a two-dimensional memory 200, i.e., a memory that contains a two-dimensional structure of items 31-39. When the memory contains a grid of items, a two-dimensional structure, an area can be any rectangular region in the grid. The maximum area size the system will consider for a task can be limited in the two-dimensional case as well. For a two-dimensional area, this can be done by setting the maximum height and width for each area. In the example ofFIG. 2 , the original memory is a 3×3 grid of items and the maximum height and width allowed for each area is two items. Thearea memory 210 thus contains areas ofdimension 1×1 with area items 41-49; areas ofdimension 1×2 with area items 51-56, each ofdimension 1×2; areas ofdimension 2×1 with area items 61-66, each ofdimension 2×1; and areas ofdimensions 2×2 with area items 71-74, each ofdimension 2×2. - As particular examples,
area item 47 is a 1×1 area and therefore includes onlyitem 37, whilearea item 56 is a 1×2 area and includesitems area item 66 is a 2×1 area and includesitems area item 71 is a 2×2 area and includesitems - Because the output of area attention includes attended outputs over information with varying granularity, the remaining components of the neural network can learn which levels of granularity are important for a particular task during training of the neural network system.
- For a model be able to attend to each area as a unit of attention, a key and value are defined for each area that contains or represents one or multiple items in the original memory. The system can then receive an attention query and apply an attention mechanism using the area keys and values.
-
FIG. 3 illustrates asystem 300 of one ormore computers 310 implementing aneural network system 320 with anarea attention layer 330. The neural network system is configured to receive aneural network input 322 and to generate aneural network output 324. - The area attention layer is configured to perform operations to implement area attention during the processing of the neural network input. The attention layer is configured to receive data specifying a
memory 332 that contains multiple items and, for each item, a respective key and a respective value, and to determine multiple areas within thememory 324. Each area includes one or more items in the memory, and one or more of the areas include multiple adjacent items in the memory. The attention layer is further configured to determine, for each of the areas, a respective area key and arespective area value 326 from at least the keys and values of the items in the area; receive anattention query 328; apply an attention mechanism between the attention query and the area keys for each area to generate a respective attention weight for eacharea 330; and generate an areaattention layer output 332 by combining the area values for each area in accordance with the attention weights. The area attention layer is further configured to provide the areaattention layer output 334 to another component of the neural network system. - In some cases, the memory is arranged as a sequence of items, and determining multiple areas includes identifying, as different areas, each combination of adjacent items that includes no more than a maximum number of items. In other cases, the memory is arranged as a two-dimensional grid of items, and determining multiple areas includes identifying, as different areas, each rectangular region of items within the two-dimensional grid that has no more than a maximum height and no more than a maximum width. The maximums can be predetermined as hyperparameter values or determined using a hyperparameter search technique.
- The area attention layer is configured to determine, for each of the areas, a respective area key and a respective area value. In some implementations, the layer does so by determining the area value to be a sum of the values of the items in the area or a mean of the keys of the items in the area, or by determining a plurality of features of the items in the area and combining the features to generate the area key of the area. This combining of features can be done by summing or concatenating the features to generate a combined feature and applying one or more learned non-linear transformations to the combined feature to generate the area key. The features that can be combined include one or more of an embedding corresponding to a number of items in the area, a mean of the keys of the items in the area, or a variance of the keys of the items in the area.
- For areas that include only respective a single item from the memory, the area key and area value for the area are the key and value for the single item.
- In some cases, each item corresponds to a portion of the neural network input, and the value for each memory item is an encoded representation of the neural network input.
- In some implementations, other components of the neural network system provide as input to the attention layer the values and keys for the items in the memory and the attention query.
- In some cases, the key is the same as the value the item in the memory. In other cases, the key and the value are different for each item in the memory.
- In some implementations, the key of an area, μi, is defined simply as the mean vector of the key of each item in the area, as expressed in
equation 3. -
- Here |ri| is the size of the area ri.
- In these implementations, the value of an area is defined as the sum of all value vectors of the items in the area, as expressed in equation 4.
-
- With the keys and values defined, a system can use the standard way for calculating attention as described above in reference to
equations equations 3 and 4, is parameter-free; that is, it does not introduce any parameters to be learned. Essentially,equation 3 and 4 use average and sum pooling over an area of vectors. It is possible to use other pooling methods, e.g., max pooling, to compute the key and value vector for each area. - Richer representations of each area can also be derived by using features other than the mean of the key vectors of the area. For example, a system can consider the standard deviation of the key vectors within each area, shown in equation 5, below.
-
- The system can also consider other features, e.g., the height and width of each area.
- When considering these other features, the system can process the features, i.e., the mean, the standard deviation and, if used, features identifying the height and width of the area, using a multi-layer perceptron neural network that is trained jointly with the neural network system to generate the key for the area.
- To efficiently compute μi, σi, and νi r
i , a system can implement an optimization technique known as a summed area table. The summed area table is based on an integral image, I, which can be efficiently computed in a single pass of the memory. With the integral image, the system can calculate the key and value of each area in constant time. -
FIG. 4 andFIG. 5 show pseudocode for performing area attention calculations. In particular, they perform the calculations ofequations 3, 4 and 5 for all areas that have less than the maximum size given an input memory grid as well as determine the shape size of each area. The operations in the pseudocode can be used to efficiently compute the area keys and area values for all of the areas for a given two-dimensional memory grid. The operations in the pseudocode are based on the tensor operations of the TensorFlow open-source software library. - This specification uses the term “configured” in connection with systems and computer program components. For a system of one or more computers to be configured to perform particular operations or actions means that the system has installed on it software, firmware, hardware, or a combination of them that in operation cause the system to perform the operations or actions. For one or more computer programs to be configured to perform particular operations or actions means that the one or more programs include instructions that, when executed by data processing apparatus, cause the apparatus to perform the operations or actions.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or in addition, the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- The term “data processing apparatus” refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can also be, or further include, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). The apparatus can optionally include, in addition to hardware, code that creates an execution environment for computer programs, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- A computer program, which may also be referred to or described as a program, software, a software application, an app, a module, a software module, a script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it can be deployed in any form, including as a stand alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
- In this specification, the term “database” is used broadly to refer to any collection of data: the data does not need to be structured in any particular way, or structured at all, and it can be stored on storage devices in one or more locations. Thus, for example, the index database can include multiple collections of data, each of which may be organized and accessed differently.
- Similarly, in this specification the term “engine” is used broadly to refer to a software-based system, subsystem, or process that is programmed to perform one or more specific functions. Generally, an engine will be implemented as one or more software modules or components, installed on one or more computers in one or more locations. In some cases, one or more computers will be dedicated to a particular engine; in other cases, multiple engines can be installed and running on the same computer or computers.
- The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA or an ASIC, or by a combination of special purpose logic circuitry and one or more programmed computers.
- Computers suitable for the execution of a computer program can be based on general or special purpose microprocessors or both, or any other kind of central processing unit. Generally, a central processing unit will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.
- Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's device in response to requests received from the web browser. Also, a computer can interact with a user by sending text messages or other forms of message to a personal device, e.g., a smartphone that is running a messaging application, and receiving responsive messages from the user in return.
- Data processing apparatus for implementing machine learning models can also include, for example, special-purpose hardware accelerator units for processing common and compute-intensive parts of machine learning training or production, i.e., inference, workloads.
- Machine learning models can be implemented and deployed using a machine learning framework, e.g., a TensorFlow framework, a Microsoft Cognitive Toolkit framework, an Apache Singa framework, or an Apache MXNet framework.
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (LAN) and a wide area network (WAN), e.g., the Internet.
- The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, a server transmits data, e.g., an HTML page, to a user device, e.g., for purposes of displaying data to and receiving user input from a user interacting with the device, which acts as a client. Data generated at the user device, e.g., a result of the user interaction, can be received at the server from the device.
- While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or on the scope of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially be claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
- Similarly, while operations are depicted in the drawings and recited in the claims in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
- Particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (17)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/586,813 US20200104681A1 (en) | 2018-09-27 | 2019-09-27 | Neural Networks with Area Attention |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201862737913P | 2018-09-27 | 2018-09-27 | |
US16/586,813 US20200104681A1 (en) | 2018-09-27 | 2019-09-27 | Neural Networks with Area Attention |
Publications (1)
Publication Number | Publication Date |
---|---|
US20200104681A1 true US20200104681A1 (en) | 2020-04-02 |
Family
ID=69945909
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/586,813 Pending US20200104681A1 (en) | 2018-09-27 | 2019-09-27 | Neural Networks with Area Attention |
Country Status (1)
Country | Link |
---|---|
US (1) | US20200104681A1 (en) |
Cited By (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN113378973A (en) * | 2021-06-29 | 2021-09-10 | 沈阳雅译网络技术有限公司 | Image classification method based on self-attention mechanism |
WO2022035522A1 (en) * | 2020-08-14 | 2022-02-17 | Micron Technology, Inc. | Transformer network in memory |
US20220261711A1 (en) * | 2021-02-12 | 2022-08-18 | Accenture Global Solutions Limited | System and method for intelligent contract guidance |
US11983619B2 (en) | 2020-08-14 | 2024-05-14 | Micron Technology, Inc. | Transformer neural network in memory |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20190005090A1 (en) * | 2017-06-29 | 2019-01-03 | FutureWel Technologies, Inc. | Dynamic semantic networks for language understanding and question answering |
US20190130273A1 (en) * | 2017-10-27 | 2019-05-02 | Salesforce.Com, Inc. | Sequence-to-sequence prediction using a neural network model |
US20190251952A1 (en) * | 2018-02-09 | 2019-08-15 | Baidu Usa Llc | Systems and methods for neural voice cloning with a few samples |
-
2019
- 2019-09-27 US US16/586,813 patent/US20200104681A1/en active Pending
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20190005090A1 (en) * | 2017-06-29 | 2019-01-03 | FutureWel Technologies, Inc. | Dynamic semantic networks for language understanding and question answering |
US20190130273A1 (en) * | 2017-10-27 | 2019-05-02 | Salesforce.Com, Inc. | Sequence-to-sequence prediction using a neural network model |
US20190251952A1 (en) * | 2018-02-09 | 2019-08-15 | Baidu Usa Llc | Systems and methods for neural voice cloning with a few samples |
Non-Patent Citations (7)
Title |
---|
Jain AK, Agarwalla A, Agrawal KK, Mitra P. Recurrent Memory Addressing for Describing Videos. InCVPR Workshops 2017 Jul 1 (Vol. 7, pp. 1-8). (Year: 2017) * |
Kaiser Ł, Nachum O, Roy A, Bengio S. Learning to remember rare events. arXiv preprint arXiv:1703.03129. 2017 Mar 9. (Year: 2017) * |
Miller A, Fisch A, Dodge J, Karimi AH, Bordes A, Weston J. Key-value memory networks for directly reading documents. arXiv preprint arXiv:1606.03126. 2016 Jun 9. (Year: 2016) * |
Povey D, Hadian H, Ghahremani P, Li K, Khudanpur S. A time-restricted self-attention layer for ASR. In2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2018 Apr 15 (pp. 5874-5878). IEEE. (Year: 2018) * |
Sukhbaatar S, Weston J, Fergus R. End-to-end memory networks. Advances in neural information processing systems. 2015;28. (Year: 2015) * |
Tan Z, Wang M, Xie J, Chen Y, Shi X. Deep semantic role labeling with self-attention. InProceedings of the AAAI conference on artificial intelligence 2018 Apr 26 (Vol. 32, No. 1). (Year: 2018) * |
Yang B, Tu Z, Wong DF, Meng F, Chao LS, Zhang T. Modeling localness for self-attention networks. arXiv preprint arXiv:1810.10182. 2018 Oct 24. (Year: 2018) * |
Cited By (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2022035522A1 (en) * | 2020-08-14 | 2022-02-17 | Micron Technology, Inc. | Transformer network in memory |
US11983619B2 (en) | 2020-08-14 | 2024-05-14 | Micron Technology, Inc. | Transformer neural network in memory |
US20220261711A1 (en) * | 2021-02-12 | 2022-08-18 | Accenture Global Solutions Limited | System and method for intelligent contract guidance |
US11755973B2 (en) * | 2021-02-12 | 2023-09-12 | Accenture Global Solutions Limited | System and method for intelligent contract guidance |
CN113378973A (en) * | 2021-06-29 | 2021-09-10 | 沈阳雅译网络技术有限公司 | Image classification method based on self-attention mechanism |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
AU2022221389B2 (en) | Attention-based sequence transduction neural networks | |
US11886998B2 (en) | Attention-based decoder-only sequence transduction neural networks | |
US10997503B2 (en) | Computationally efficient neural network architecture search | |
US20200104681A1 (en) | Neural Networks with Area Attention | |
US20210248473A1 (en) | Attention neural networks with linear units | |
US20230316055A1 (en) | Attention neural networks with parallel attention and feed-forward layers | |
US20240119261A1 (en) | Discrete token processing using diffusion models | |
US20240005131A1 (en) | Attention neural networks with tree attention mechanisms | |
US20240078379A1 (en) | Attention neural networks with n-grammer layers | |
US20230206030A1 (en) | Hyperparameter neural network ensembles |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:LI, YANG;KAISER, LUKASZ MIECZYSLAW;BENGIO, SAMUEL;AND OTHERS;SIGNING DATES FROM 20191014 TO 20191025;REEL/FRAME:050837/0842 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: FINAL REJECTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |