CN113382250A - Alpha channel prediction - Google Patents
Alpha channel prediction Download PDFInfo
- Publication number
- CN113382250A CN113382250A CN202110625992.7A CN202110625992A CN113382250A CN 113382250 A CN113382250 A CN 113382250A CN 202110625992 A CN202110625992 A CN 202110625992A CN 113382250 A CN113382250 A CN 113382250A
- Authority
- CN
- China
- Prior art keywords
- value
- channel
- alpha
- color
- obtaining
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000034 method Methods 0.000 claims description 36
- 238000006243 chemical reaction Methods 0.000 claims description 12
- 238000004891 communication Methods 0.000 description 123
- 238000005192 partition Methods 0.000 description 34
- 238000000638 solvent extraction Methods 0.000 description 21
- 208000037170 Delayed Emergence from Anesthesia Diseases 0.000 description 18
- 238000001914 filtration Methods 0.000 description 17
- 238000010586 diagram Methods 0.000 description 13
- 238000013500 data storage Methods 0.000 description 10
- 238000012545 processing Methods 0.000 description 7
- 239000013598 vector Substances 0.000 description 7
- 239000011159 matrix material Substances 0.000 description 6
- 230000006835 compression Effects 0.000 description 5
- 238000007906 compression Methods 0.000 description 5
- 238000004590 computer program Methods 0.000 description 5
- 230000003287 optical effect Effects 0.000 description 5
- 238000013139 quantization Methods 0.000 description 5
- 230000002123 temporal effect Effects 0.000 description 5
- 230000006870 function Effects 0.000 description 4
- 230000001131 transforming effect Effects 0.000 description 4
- 241000023320 Luma <angiosperm> Species 0.000 description 3
- 230000005540 biological transmission Effects 0.000 description 3
- OSWPMRLSEDHDFF-UHFFFAOYSA-N methyl salicylate Chemical compound COC(=O)C1=CC=CC=C1O OSWPMRLSEDHDFF-UHFFFAOYSA-N 0.000 description 3
- 238000003491 array Methods 0.000 description 2
- 230000001413 cellular effect Effects 0.000 description 2
- 238000013461 design Methods 0.000 description 2
- 229910001416 lithium ion Inorganic materials 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- QELJHCBNGDEXLD-UHFFFAOYSA-N nickel zinc Chemical compound [Ni].[Zn] QELJHCBNGDEXLD-UHFFFAOYSA-N 0.000 description 2
- 238000010606 normalization Methods 0.000 description 2
- 238000005457 optimization Methods 0.000 description 2
- HBBGRARXTFLTSG-UHFFFAOYSA-N Lithium ion Chemical compound [Li+] HBBGRARXTFLTSG-UHFFFAOYSA-N 0.000 description 1
- 230000000903 blocking effect Effects 0.000 description 1
- OJIJEKBXJYRIBZ-UHFFFAOYSA-N cadmium nickel Chemical compound [Ni].[Cd] OJIJEKBXJYRIBZ-UHFFFAOYSA-N 0.000 description 1
- 229910052804 chromium Inorganic materials 0.000 description 1
- 238000003066 decision tree Methods 0.000 description 1
- 238000000354 decomposition reaction Methods 0.000 description 1
- 239000000835 fiber Substances 0.000 description 1
- 239000000446 fuel Substances 0.000 description 1
- 229910052987 metal hydride Inorganic materials 0.000 description 1
- 229910052759 nickel Inorganic materials 0.000 description 1
- PXHVJJICTQNCMI-UHFFFAOYSA-N nickel Substances [Ni] PXHVJJICTQNCMI-UHFFFAOYSA-N 0.000 description 1
- -1 nickel metal hydride Chemical class 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000003595 spectral effect Effects 0.000 description 1
- 238000001228 spectrum Methods 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 238000013519 translation Methods 0.000 description 1
- 229910052720 vanadium Inorganic materials 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/20—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using video object coding
- H04N19/21—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using video object coding with binary alpha-plane coding for video objects, e.g. context-based arithmetic encoding [CAE]
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/50—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding
- H04N19/503—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using predictive coding involving temporal prediction
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/17—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object
- H04N19/176—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being an image region, e.g. an object the region being a block, e.g. a macroblock
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T9/00—Image coding
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/103—Selection of coding mode or of prediction mode
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/103—Selection of coding mode or of prediction mode
- H04N19/105—Selection of the reference unit for prediction within a chosen coding or prediction mode, e.g. adaptive choice of position and number of pixels used for prediction
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/12—Selection from among a plurality of transforms or standards, e.g. selection between discrete cosine transform [DCT] and sub-band transform or selection between H.263 and H.264
- H04N19/122—Selection of transform size, e.g. 8x8 or 2x4x8 DCT; Selection of sub-band transforms of varying structure or type
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/124—Quantisation
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/102—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or selection affected or controlled by the adaptive coding
- H04N19/129—Scanning of coding units, e.g. zig-zag scan of transform coefficients or flexible macroblock ordering [FMO]
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
- H04N19/136—Incoming video signal characteristics or properties
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/134—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the element, parameter or criterion affecting or controlling the adaptive coding
- H04N19/146—Data rate or code amount at the encoder output
- H04N19/147—Data rate or code amount at the encoder output according to rate distortion criteria
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/10—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding
- H04N19/169—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding
- H04N19/186—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using adaptive coding characterised by the coding unit, i.e. the structural portion or semantic portion of the video signal being the object or the subject of the adaptive coding the unit being a colour or a chrominance component
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/20—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using video object coding
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/40—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using video transcoding, i.e. partial or full decoding of a coded input stream followed by re-encoding of the decoded output stream
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/44—Decoders specially adapted therefor, e.g. video decoders which are asymmetric with respect to the encoder
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/60—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using transform coding
- H04N19/61—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using transform coding in combination with predictive coding
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/60—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using transform coding
- H04N19/625—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using transform coding using discrete cosine transform [DCT]
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/85—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using pre-processing or post-processing specially adapted for video compression
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/90—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using coding techniques not provided for in groups H04N19/10-H04N19/85, e.g. fractals
- H04N19/91—Entropy coding, e.g. variable length coding [VLC] or arithmetic coding
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N19/00—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals
- H04N19/90—Methods or arrangements for coding, decoding, compressing or decompressing digital video signals using coding techniques not provided for in groups H04N19/10-H04N19/85, e.g. fractals
- H04N19/96—Tree coding, e.g. quad-tree coding
Abstract
The present disclosure relates to alpha channel prediction. Transcoding the image using alpha channel prediction may include generating a reconstructed image using alpha channel prediction and outputting the reconstructed image. Generating the reconstructed image by using alpha channel prediction may include: decoding a reconstructed color channel value of a current pixel represented with reference to a first color space; obtaining a color space converted color channel value of the current pixel by converting the reconstructed color channel value to a second color space; obtaining an alpha channel lower limit for an alpha channel value of the current pixel by converting the color channel value using a color space; generating a candidate predicted alpha value of the current pixel; obtaining an adjusted predicted alpha value for the current pixel by using the candidate predicted alpha value and an alpha channel lower limit; generating a reconstructed pixel for the current pixel by using the adjusted predicted alpha value; and including the reconstructed pixel in the reconstructed image.
Description
Technical Field
The present disclosure relates to alpha channel prediction.
Background
Digital images and videos may be used, for example, on the internet for remote commercial conferencing via video conferencing, high definition video entertainment, video advertising, or sharing of user-generated content. High performance compression may be advantageous for transmission and storage due to the large amount of data involved in transmitting and processing image and video data. It would therefore be advantageous to provide high resolution images and video sent over communication channels with limited bandwidth (e.g., by transcoding images and video using alpha channel prediction).
Disclosure of Invention
The present application relates to encoding and decoding image data, video stream data, or both for transmission or storage. Systems, methods, and apparatuses for encoding and decoding using alpha channel prediction are disclosed herein.
One aspect is a method for image transcoding by using alpha channel prediction. Transcoding the image using alpha channel prediction may include generating a reconstructed image using alpha channel prediction and outputting the reconstructed image. Generating the reconstructed image by using alpha channel prediction includes: obtaining a reconstructed color channel value for a current pixel of the current image represented by reference to a first color space; obtaining a color space converted color channel value for the current pixel by converting the reconstructed color channel value to a second color space; obtaining an alpha channel lower limit for an alpha channel value of the current pixel by converting a color channel value using the color space; generating a candidate predicted alpha value for the current pixel; obtaining an adjusted predicted alpha value for the current pixel by using the candidate predicted alpha value and the alpha channel lower limit; generating a reconstructed pixel for the current pixel by using the adjusted predicted alpha value; and including the reconstructed pixel in the reconstructed image.
Another aspect is a method for image transcoding by using alpha channel prediction. Transcoding an image using alpha channel prediction may include generating an encoded image using alpha channel prediction and outputting an output bitstream. Generating the encoded image by using the alpha channel prediction includes: identifying a current pixel from an input image, wherein the current pixel comprises input color channel values, wherein the input color channel values are represented with reference to a first color space, and wherein the input color channel values comprise input alpha channel values; obtaining a pre-multiplied color channel value for the pixel by using the input color channel value; obtaining a reconstructed color value for the pixel by using the pre-multiplied color channel value, wherein the reconstructed color channel value is represented with reference to a second color space; obtaining a color space conversion color channel value for the current pixel by color space converting the reconstructed color channel value to the first color space; obtaining an alpha channel lower limit for a reconstructed alpha channel value of the current pixel by converting a color channel value using the color space; generating a candidate predicted alpha value for the current pixel; obtaining an adjusted predicted alpha value for the current pixel by using the candidate predicted alpha value and the alpha channel lower limit; obtaining a residual alpha value as a difference of subtracting the adjusted predicted alpha value from the input alpha channel value; and including the residual alpha values in an output bitstream.
Another aspect is an apparatus for image transcoding using alpha channel prediction. The apparatus may include a processor configured to generate a reconstructed image by using alpha channel prediction and output the reconstructed image. The processor may be configured to generate the reconstructed image using alpha channel prediction by: obtaining a reconstructed color channel value for a current pixel of the current image referenced to a first color space representation; obtaining a color space converted color channel value for the current pixel by converting the reconstructed color channel value to a second color space; obtaining an alpha channel lower limit for an alpha channel value of the current pixel by converting a color channel value using the color space; generating a candidate predicted alpha value for the current pixel; obtaining an adjusted predicted alpha value for the current pixel by using the candidate predicted alpha value and the alpha channel lower limit; generating a reconstructed pixel for the current pixel by using the adjusted predicted alpha value; and including the reconstructed pixel in the reconstructed image.
Another aspect is an apparatus for image transcoding using alpha channel prediction. The apparatus may include a processor configured to generate an encoded image by using alpha channel prediction and output the encoded image in an output bitstream. The processor may be configured to generate the encoded image using the alpha channel prediction by: identifying a current pixel from an input image, wherein the current pixel comprises input color channel values, wherein the input color channel values are represented with reference to a first color space, and wherein the input color channel values comprise input alpha channel values; obtaining a pre-multiplied color channel value for the pixel by using the input color channel value; obtaining a reconstructed color value for the pixel by using the pre-multiplied color channel value, wherein the reconstructed color channel value is represented with reference to a second color space; obtaining a color space conversion color channel value for the current pixel by color space converting the reconstructed color channel value to the first color space; obtaining an alpha channel lower limit for a reconstructed alpha channel value of the current pixel by converting a color channel value using the color space; generating a candidate predicted alpha value for the current pixel; obtaining an adjusted predicted alpha value for the current pixel by using the candidate predicted alpha value and the alpha channel lower limit; obtaining a residual alpha value as a difference of subtracting the adjusted predicted alpha value from the input alpha channel value; and including the residual alpha values in an output bitstream.
Variations in these and other aspects will be described in more detail below.
Drawings
The description herein makes reference to the accompanying drawings wherein like reference numerals refer to like parts throughout the several views unless otherwise indicated or otherwise clear from the context.
FIG. 1 is a diagram of a computing device according to an embodiment of the present disclosure.
Fig. 2 is a diagram of a computing and communication system according to an embodiment of the present disclosure.
Fig. 3 is a diagram of a video stream for encoding and decoding according to an embodiment of the present invention.
Fig. 4 is a block diagram of an encoder according to an embodiment of the present disclosure.
Fig. 5 is a block diagram of a decoder according to an embodiment of the present invention.
Fig. 6 is a block diagram of a representation of a portion of a frame according to an embodiment of the present disclosure.
Fig. 7 is a flowchart of an example of decoding by using alpha channel prediction according to an embodiment of the present disclosure.
Fig. 8 is a flowchart of an example of encoding by using alpha channel prediction according to an embodiment of the present disclosure.
Detailed Description
Image and video compression schemes may include decomposing an image or frame into smaller portions, such as blocks, and generating an output bitstream using techniques that minimize the bandwidth utilization of the information included for each block in the output. In some embodiments, the information included for each block in the output may be limited by reducing spatial redundancy, reducing temporal redundancy, or a combination thereof. For example, temporal or spatial redundancy may be reduced by predicting a frame or a portion thereof based on information available to both the encoder and decoder, and including information in the encoded bitstream representing the difference or residue between the predicted frame and the original frame. The residual information may be further compressed by transforming the residual information into transform coefficients, quantizing the transform coefficients, and entropy encoding the quantized transform coefficients. Other transcoding information, such as motion information, may be included in the encoded bitstream, which may include sending differential information based on predictions of the encoded information, which may be entropy encoded to further reduce corresponding bandwidth utilization. The encoded bitstream may be decoded to reconstruct the block and source images from limited information. In some implementations, the accuracy, efficiency, or both of encoding blocks using inter-prediction or intra-prediction may be limited.
Embodiments of transcoding (e.g., encoding or decoding) using alpha channel prediction may include using previously reconstructed alpha pre-multiplied RGB color values to obtain an alpha channel lower bound, and using the alpha channel lower bound to improve the accuracy of the predictor of the alpha channel.
Fig. 1 is a diagram of a computing device 100, in accordance with an embodiment of the present disclosure. Computing device 100 is shown including memory 110, processor 120, User Interface (UI)130, electronic communication unit 140, sensors 150, power supply 160, and bus 170. As used herein, the term "computing device" includes any unit, or combination of units, capable of performing any of the methods disclosed herein, or any one or more portions thereof.
Although shown as a single unit, memory 110 may comprise multiple physical units, e.g., one or more primary memory units, such as random access memory units; one or more secondary data storage units, such as a magnetic disk; or a combination thereof. For example, data 112 or a portion thereof, instructions 114 or a portion thereof, or both, may be stored in a secondary storage unit and may be loaded into or otherwise transferred to a primary storage unit in connection with processing the respective data 112, executing the respective instructions 114, or both. In some implementations, the memory 110, or a portion thereof, may be removable memory.
Although illustrated as being included in memory 110, in some embodiments, the instructions 114, or portions thereof, may be implemented as a special purpose processor or circuitry that may include dedicated hardware for performing any one of the methods, algorithms, aspects, or combinations thereof, as described herein. Portions of the instructions 114 may be distributed across multiple processors on the same machine or on different machines, or across a network such as a local area network, a wide area network, the internet, or a combination thereof.
The user interface 130 may include any unit capable of interfacing with a user, such as a virtual or physical keypad, a touchpad, a display, a touch display, a speaker, a microphone, a camera, a sensor, or any combination thereof. For example, user interface 130 may be an audio-video display device, and computing device 100 may render audio, e.g., decoded audio, by using the audio-video display device of user interface 130, e.g., in conjunction with displaying video, e.g., decoded video. Although shown as a single unit, the user interface 130 may comprise one or more physical units. For example, the user interface 130 may include an audio interface for performing audio communication with a user, and a touch display for performing visual and touch-based communication with the user.
Although electronic communication interface 142 is illustrated in fig. 1 as a wireless antenna, electronic communication interface 142 may be a wireless antenna as shown, a wired communication port such as an ethernet port, an infrared port, a serial port, or any other wired or wireless unit capable of interfacing with a wired or wireless electronic communication medium 180. Although fig. 1 shows a single electronic communication unit 140 and a single electronic communication interface 142, any number of electronic communication units and any number of electronic communication interfaces may be used.
The sensor 150 may include, for example, an audio sensing device, a visible light sensing device, a motion sensing device, or a combination thereof. For example, the sensor 150 may include a sound sensing device, such as a microphone, or any other sound sensing device now known or later developed that may sense sounds, such as speech or other speech, emitted by a user operating the computing device 100 in the vicinity of the computing device 100. In another example, the sensor 150 may include a camera, or any other image sensing device now known or later developed, that may sense an image, such as an image of a user operating the computing device. Although a single sensor 150 is shown, the computing device 100 may include multiple sensors 150. For example, computing device 100 may include a first camera oriented with a field of view directed toward a user of computing device 100, and a second camera oriented with a field of view away from the user of computing device 100.
The power supply 160 may be any suitable device for powering the computing device 100. For example, the power supply 160 may include a wired external power interface; one or more dry cell batteries, for example, nickel cadmium (NiCd), nickel zinc (NiZn), nickel metal hydride (NiMH), lithium ion (Li-ion); a solar cell; a fuel cell; or any other device capable of powering computing device 100. Although a single power supply 160 is shown in fig. 1, the computing device 100 may include multiple power supplies 160, for example, a battery and a wired external power interface.
Although shown as separate units, electronic communication unit 140, electronic communication interface 142, user interface 130, power supply 160, or a portion thereof, may be configured as a combined unit. For example, electronic communication unit 140, electronic communication interface 142, user interface 130, and power supply 160 may be implemented as communication ports capable of interfacing with an external display device to provide communication, power, or both.
One or more of the memory 110, processor 120, user interface 130, electronic communication unit 140, sensor 150, or power supply 160 may be operatively coupled via a bus 170. Although a single bus 170 is shown in FIG. 1, computing device 100 may include multiple buses. For example, memory 110, processor 120, user interface 130, electronic communication unit 140, sensor 150, and bus 170 may receive power from power supply 160 via bus 170. In another example, memory 110, processor 120, user interface 130, electronic communication unit 140, sensors 150, power supply 160, or a combination thereof may communicate data, such as sending and receiving electronic signals, via bus 170.
Although not separately shown in fig. 1, one or more of the processor 120, the user interface 130, the electronic communication unit 140, the sensor 150, or the power supply 160 may include an internal memory, such as an internal buffer or register. For example, processor 120 may include an internal memory (not shown), and may read data 112 from memory 110 into the internal memory (not shown) for processing.
Although shown as separate elements, memory 110, processor 120, user interface 130, electronic communication unit 140, sensor 150, power supply 160, and bus 170, or any combination thereof, may be integrated in one or more electronic units, circuits, or chips.
Fig. 2 is a diagram of a computing and communication system 200 in accordance with an embodiment of the present disclosure. The illustrated computing and communication system 200 includes computing and communication devices 100A, 100B, 100C, access points 210A, 210B, and a network 220. For example, computing and communication system 200 may be a multiple-access system that provides communications (e.g., voice, audio, data, video, messaging, broadcast, or a combination thereof) to one or more wired or wireless communication devices, such as computing and communication devices 100A, 100B, 100C. Although fig. 2 shows three computing and communication devices 100A, 100B, 100C, two access points 210A, 210B, and one network 220 for simplicity, any number of computing and communication devices, access points, and networks may be used.
The computing and communication devices 100A, 100B, 100C may be, for example, computing devices such as computing device 100 shown in FIG. 1. For example, the computing and communication devices 100A, 100B may be user devices, such as mobile computing devices, laptop computers, thin clients (thin clients), or smart phones, and the computing and communication device 100C may be a server, such as a mainframe or cluster. Although the computing and communication device 100A and the computing and communication device 100B are described as user devices and the computing and communication device 100C is described as a server, any computing and communication device may perform some or all of the functions of a server, some or all of the functions of a user device, or some or all of the functions of a server and a user device. For example, the server computing and communication device 100C may receive, encode, process, store, transmit audio data, or a combination of the operations described; and one or both of the computing and communication device 100A and the computing and communication device 100B may receive, decode, process, store, render audio data, or a combination thereof.
Each computing and communication device 100A, 100B, 100C may be configured to perform wired or wireless communications (e.g., via network 220), which may include a User Equipment (UE), a mobile station, a fixed or mobile subscriber unit, a cellular telephone, a personal computer, a tablet computer, a server, consumer electronics, or any similar device. For example, the computing and communication devices 100A, 100B, 100C may be configured to transmit or receive wired or wireless communication signals. Although each computing and communication device 100A, 100B, 100C is shown as a single unit, the computing and communication devices may include any number of interconnected elements.
Each access point 210A, 210B may be any type of device configured to communicate with the computing and communication device 100A, 100B, 100C, the network 220, or both, via a wired or wireless communication link 180A, 180B, 180C. For example, the access points 210A, 210B may include base stations, Base Transceiver Stations (BTSs), node-Bs, enhanced node-Bs (eNode-Bs), home node-Bs (HNode-Bs), wireless routers, wired routers, hubs, repeaters, switches, or any similar wired or wireless devices. Although each access point 210A, 210B is shown as a single unit, the access points may include any number of interconnected elements.
The network 220 may be any type of network configured to provide services such as voice, data, applications, voice over internet protocol (VoIP), or any other communication protocol or combination of communication protocols over wired or wireless communication links. For example, the network 220 may be a Local Area Network (LAN), a Wide Area Network (WAN), a Virtual Private Network (VPN), a mobile or cellular telephone network, the Internet, or any other means of electronic communication. The network may use a communication protocol such as Transmission Control Protocol (TCP), User Datagram Protocol (UDP), Internet Protocol (IP), real-time transport protocol (RTP), hypertext transfer protocol (HTTP), or a combination thereof.
The computing and communication devices 100A, 100B, 100C may communicate with each other via the network 220 using one or more wired or wireless communication links, or via a combination of wired and wireless communication links. For example, as shown, computing and communication devices 100A, 100B may communicate via wireless communication links 180A, 180B, and computing and communication device 100C may communicate via wired communication link 180C. Any of the computing and communication devices 100A, 100B, 100C may communicate using any one or more wired or wireless communication links. For example, a first computing and communication device 100A may communicate via a first access point 210A using a first type of communication link, a second computing and communication device 100B may communicate via a second access point 210B using a second type of communication link, and a third computing and communication device 100C may communicate via a third access point (not shown) using a third type of communication link. Similarly, the access points 210A, 210B may communicate with the network 220 via one or more types of wired or wireless communication links 230A, 230B. Although fig. 2 illustrates the computing and communication devices 100A, 100B, 100C communicating via the network 220, the computing and communication devices 100A, 100B, 100C may communicate with each other via any number of communication links (e.g., direct wired or wireless communication links).
In some implementations, communication between one or more of the computing and communication devices 100A, 100B, 100C may omit communication via the network 220, and may include transferring data via another medium (not shown), such as a data storage device. For example, the server computing and communication device 100C may store audio data, such as encoded audio data, in a data storage device, such as a portable data storage unit, and one or both of the computing and communication device 100A or the computing and communication device 100B may access, read, or retrieve the stored audio data from the data storage unit, such as by physically disconnecting the data storage device from the server computing and communication device 100C and physically connecting the data storage device to the computing and communication device 100A or the computing and communication device 100B.
Other implementations of the computing and communication system 200 are possible. For example, in an embodiment, the network 220 may be an ad hoc network, and one or more of the access points 210A, 210B may be omitted. Computing and communication system 200 may include devices, units, or elements not shown in fig. 2. For example, the computing and communication system 200 may include many more communication devices, networks, and access points.
Fig. 3 is a diagram of a video stream 300 for encoding and decoding, according to an embodiment of the present disclosure. A video stream 300, such as a video stream captured by a camera or a video stream generated by a computing device, may include a video sequence 310. Video sequence 310 may include a sequence of adjacent frames 320. Although three adjacent frames 320 are shown, the video sequence 310 may include any number of adjacent frames 320.
Each frame 330 from adjacent frames 320 may represent a single image from the video stream. Although not shown in fig. 3, frame 330 may include one or more segments, tiles, or planes that may be independently (e.g., in parallel) transcoded or otherwise processed. The frame 330 may include one or more tiles 340. Each tile 340 may be a rectangular region of a frame that can be independently transcoded. Each tile 340 may include a respective tile 350. Although not shown in fig. 3, a block may include pixels. For example, a block may include 16 × 16 pixel groups, 8 × 8 pixel groups, 8 × 16 pixel groups, or any other pixel groups. Unless otherwise indicated herein, the term "block" may include a super-block, a macroblock, a slice, or any other portion of a frame. A frame, block, pixel, or combination thereof may include display information, such as luminance information, chrominance information, or any other information that may be used to store, modify, convey, or display the video stream or a portion thereof.
Fig. 4 is a block diagram of an encoder 400 according to an embodiment of the present disclosure. The encoder 400 may be implemented in a device, such as the computing device 100 shown in fig. 1 or the computing and communication devices 100A, 100B, 100C shown in fig. 2, as a computer software program, for example, stored in a data storage unit, such as the memory 110 shown in fig. 1. The computer software program may include machine instructions that are executable by a processor (e.g., processor 120 shown in fig. 1) and that cause the apparatus to encode video data as described herein. The encoder 400 may be implemented as dedicated hardware included in the computing device 100, for example.
The encoder 400 may encode an input video stream 402, such as the video stream 300 shown in fig. 3, to generate an encoded (compressed) bitstream 404. In some embodiments, the encoder 400 may include a forward path for generating the compressed bitstream 404. The forward path may include an intra/inter prediction unit 410, a transform unit 420, a quantization unit 430, an entropy coding unit 440, or any combination thereof. In some embodiments, encoder 400 may include a reconstruction path (indicated by the broken connecting lines) for reconstructing the frame to encode other blocks. The reconstruction path may include a dequantization unit 450, an inverse transform unit 460, a reconstruction unit 470, a filtering unit 480, or any combination thereof. Other structural variations of the encoder 400 may be used to encode the video stream 402.
To encode the video stream 402, each frame within the video stream 402 may be processed in units of blocks. Accordingly, the current block may be identified from the blocks in the frame, and the current block may be encoded.
At the intra/inter prediction unit 410, the current block may be encoded by using intra prediction, which may be within a single frame, or inter prediction, which may be from frame to frame. Intra-prediction may include generating a prediction block from samples in the current frame that have been previously encoded and reconstructed. Inter-prediction may include generating a prediction block from samples in one or more previously constructed reference frames. Generating the prediction block for the current block in the current frame may include performing motion estimation to generate a motion vector indicating an appropriate reference portion of the reference frame.
The intra/inter prediction unit 410 may subtract the prediction block from the current block (original block) to generate a residual block. Transform unit 420 may perform a block-based transform, which may include transforming the residual block into transform coefficients, e.g., in the frequency domain. Examples of block-based transforms include Karhunen-loeve transform (KLT), Discrete Cosine Transform (DCT), singular value decomposition transform (SVD), and Asymmetric Discrete Sine Transform (ADST). In one example, DCT may include transforming the blocks into the frequency domain. DCT may include using spatial frequency based transform coefficient values with the lowest frequency (i.e., DC) coefficient in the upper left corner of the matrix and the highest frequency coefficient in the lower right corner of the matrix.
The quantization unit 430 may convert the transform coefficients into discrete quantized values, which may be referred to as quantized transform coefficients or quantization levels. The quantized transform coefficients may be entropy encoded by entropy encoding unit 440 to produce entropy encoded coefficients. Entropy encoding may include using a probability distribution metric. The entropy coded coefficients and information used to decode the block (which may include the type of prediction used, the motion vector, and the quantizer value) may be output to the compressed bitstream 404. The compressed bitstream 404 may be formatted by using various techniques, for example, as Run Length Encoding (RLE) and zero run transcoding.
The reconstruction path may be used to maintain reference frame synchronization between the encoder 400 and a corresponding decoder (e.g., the decoder 500 shown in fig. 5). The reconstruction path may be similar to the decoding process discussed below, and may include decoding the encoded frame or a portion thereof, which may include decoding the encoded block, which may include dequantizing the quantized transform coefficients at a dequantization unit 450 and inverse transforming the dequantized transform coefficients at an inverse transform unit 460 to produce a derived residual block. The reconstruction unit 470 may add the prediction block generated by the intra/inter prediction unit 410 to the derived residual block to create a decoded block. A filtering unit 480 may be applied to the decoded block to generate a reconstructed block, which may reduce distortions such as block artifacts. Although one filtering unit 480 is shown in fig. 4, filtering the decoded block may include loop filtering, deblocking filtering, or other types of filtering, or combinations of types of filtering. The reconstructed block, which may be a portion of a reference frame used to encode another portion of the current frame, another frame, or both, may be stored or otherwise accessible as a reconstructed block, as indicated by the dashed line at 482. As indicated by the dashed line at 484, transcoding information (e.g., a deblocking threshold index value) for the frame may be encoded, included in the compressed bitstream 404, or both.
Other variations of the encoder 400 may be used to encode the compressed bitstream 404. For example, the non-transform based encoder 400 may quantize the residual block directly without the transform unit 420. In some embodiments, the quantization unit 430 and the dequantization unit 450 may be combined into a single unit.
Fig. 5 is a block diagram of a decoder 500 according to an embodiment of the present disclosure. In a device such as the computing device 100 shown in fig. 1 or the computing and communication devices 100A, 100B, 100C shown in fig. 2, the decoder 500 may be implemented as a computer software program, for example, stored in a data storage unit such as the memory 110 shown in fig. 1. The computer software program may include machine-readable instructions that may be executed by a processor (e.g., processor 120 shown in fig. 1) and may cause the apparatus to decode video data as described herein. The decoder 500 may be implemented as dedicated hardware included in the computing device 100, for example.
The decoder 500 may receive a compressed bitstream 502, such as the compressed bitstreams 40, 4 shown in fig. 4, and may decode the compressed bitstream 502 to generate an output video stream 504. The decoder 500 may include an entropy decoding unit 510, a dequantization unit 520, an inverse transform unit 530, an intra/inter prediction unit 540, a reconstruction unit 550, a filtering unit 560, or any combination thereof. Other structural variations of the decoder 500 may be used to decode the compressed bitstream 502.
The entropy decoding unit 510 may decode data elements within the compressed bitstream 502 by using, for example, context-adaptive binary arithmetic decoding to produce a set of quantized transform coefficients. The dequantization unit 520 may dequantize the quantized transform coefficients, and the inverse transform unit 530 may inverse transform the dequantized transform coefficients to generate a derived residual block, which may correspond to the derived residual block generated by the inverse transform unit 460 shown in fig. 4. The intra/inter prediction unit 540 may generate a prediction block corresponding to the prediction block created in the encoder 400 by using header information decoded from the compressed bitstream 502. At the reconstruction unit 550, the prediction block may be added to the derived residual block to create a decoded block. The filtering unit 560 may be applied to the decoding block to reduce artifacts such as blocking artifacts, which may include loop filtering, deblocking filtering, or other types of filtering, or combinations of filtering types, and which may include generating a reconstruction block that may be output as the output video stream 504.
Other variations of the decoder 500 may be used to decode the compressed bitstream 502. For example, the decoder 500 may generate the output video stream 504 without a deblocking filtering unit.
Fig. 6 is a block diagram of a representation of a portion 600 of a frame, such as frame 330 shown in fig. 3, according to an embodiment of the present disclosure. As shown, the portion 600 of the box includes four 64 x 64 blocks 610 in two rows and two columns in a matrix or cartesian plane. In some embodiments, a 64 x 64 block may be the largest transcoding unit, with N being 64. Each 64 x 64 block may include four 32 x 32 blocks 620. Each 32 x 32 block 620 may include four 16 x 16 blocks 630. Each 16 x 16 block 630 may include four 8 x 8 blocks 640. Each 8 x 8 block 640 may include four 4 x 4 blocks 650. Each 4 x 4 block 650 may include 16 pixels, which may be represented in four rows and four columns in each respective block in a cartesian plane or matrix. The pixels may include information representing the image captured in the frame, such as luminance information, color information, and position information. In some implementations, a block, such as a 16 × 16 pixel block as shown, may include: a luminance block 660, which may include luminance pixels 662; and two chroma blocks 670, 680, such as a U or Cb chroma block 670 and a V or Cr chroma block 680. The chroma blocks 670, 680 may include chroma pixels 690. For example, as shown, luma block 660 may include 16 × 16 luma pixels 662 and each chroma block 670, 680 may include 8 × 8 chroma pixels 690. Although one arrangement of blocks is shown, any arrangement may be used. Although fig. 6 shows an N × N block, in some embodiments, an N × M block may be used. For example, 32 × 64 blocks, 64 × 32 blocks, 16 × 32 blocks, 32 × 16 blocks, or any other size block may be used. In some embodiments, an N × 2N block, a 2N × N block, or a combination thereof may be used.
In some implementations, video transcoding can include ordered block level transcoding. Sequential block-level transcoding may include transcoding blocks of a frame in an order, such as a raster scan order, where the blocks may be identified and processed as follows: starting with a block in the upper left corner of a frame or portion of a frame and proceeding from left to right along the rows and from the top row to the bottom row, each block is identified in turn for processing. For example, the 64 x 64 block in the top row and left column of the frame may be the first block transcoded, and the 64 x 64 block immediately to the right of the first block may be the second block transcoded. The second row from the top may be the transcoded second row such that the 64 x 64 blocks in the left column of the second row may be transcoded after the 64 x 64 blocks in the rightmost column of the first row.
In some implementations, transcoding the block can include: using quadtree transcoding, which may include transcoding smaller block units in a block in raster scan order. For example, the 64 x 64 blocks shown in the lower left corner of the portion of the frame shown in fig. 6 may be transcoded using quad-tree transcoding, where the 32 x 32 blocks in the upper left corner may be transcoded, then the 32 x 32 blocks in the upper right corner may be transcoded, then the 32 x 32 blocks in the lower left corner may be transcoded, and then the 32 x 32 blocks in the lower right corner may be transcoded. Each 32 x 32 block may be transcoded using quad-tree transcoding, where the top left 16 x 16 block may be transcoded, the top right 16 x 16 block may be transcoded, the bottom left 16 x 16 block may be transcoded, and the bottom right 16 x 16 block may be transcoded. Each 16 x 16 block may be transcoded using quad-tree transcoding, where the 8 x 8 block in the upper left corner may be transcoded, the 8 x 8 block in the upper right corner may be transcoded, the 8 x 8 block in the lower left corner may be transcoded, and the 8 x 8 block in the lower right corner may be transcoded. Each 8 x 8 block may be transcoded using quad-tree transcoding, where the upper left 4 x 4 block may be transcoded, the upper right 4 x 4 block may be transcoded, the lower left 4 x 4 block may be transcoded, and the lower right 4 x 4 block may be transcoded. In some implementations, for a 16 x 16 block, 8 x 8 blocks may be omitted and the 16 x 16 block may be transcoded using quad-tree transcoding, where the upper left 4 x 4 block may be transcoded and the other 4 x 4 blocks in the 16 x 16 block may then be transcoded in raster scan order.
In some implementations, video transcoding can include: the information included in the original or input frame is compressed by omitting some of the information in the original frame from the corresponding encoded frame. For example, transcoding may include: reducing the frequency spectrum redundancy; spatial redundancy is reduced; the time redundancy is reduced; or a combination thereof.
In some embodiments, reducing spectral redundancy may include: a color model based on a luminance component (Y) and two chrominance components (U and V or Cb and Cr) is used, which may be referred to as a YUV or YCbCr color model or color space. Using the YUV color model may include: a relatively large amount of information is used to represent the luminance component of a portion of a frame and a relatively small amount of information is used to represent each corresponding chrominance component of the portion of the frame. For example, a portion of a frame may be represented by a high resolution luma component (which may comprise a 16 × 16 block of pixels) and may be represented by two lower resolution chroma components, each of which represents the portion of the frame as an 8 × 8 block of pixels. A pixel may indicate a value (e.g., a value in the range of 0 to 255) and may be stored or transmitted using, for example, eight bits. Although the present disclosure is described with reference to a YUV color model, any color model may be used.
In some embodiments, reducing spatial redundancy may comprise: the blocks are transformed into the frequency domain by using, for example, a Discrete Cosine Transform (DCT). For example, a unit of an encoder (such as transform unit 420 of fig. 4) may perform DCT by using spatial frequency-based transform coefficient values.
In some embodiments, reducing temporal redundancy may include: similarities between frames are used to encode frames using a relatively small amount of data based on one or more reference frames, which may be previously encoded, decoded, and reconstructed frames of a video stream. For example, a block or pixel of the current frame may be similar to a spatially corresponding block or pixel of the reference frame. In some embodiments, the blocks or pixels of the current frame may be similar to blocks or pixels of the reference frame at different spatial locations, and reducing temporal redundancy may comprise: motion information indicating spatial differences is generated, or a translation between the position of a block or pixel in the current frame and the corresponding position of a block or pixel in the reference frame.
In some embodiments, reducing temporal redundancy may include: a portion of the reference frame corresponding to a current block or pixel of the current frame is identified. For example, a reference frame or portion of a reference frame, which may be stored in memory, may be searched to identify a portion for generating a prediction for encoding a current block or pixel of a current frame with maximum efficiency. For example, the search may identify a portion of the reference frame for which a difference in pixel values between the current block and a prediction block generated based on the portion of the reference frame is minimized, and may be referred to as a motion search. In some implementations, the portion of the searched reference frame may be limited. For example, the portion of the reference frame searched (which may be referred to as a search area) may include a limited number of rows of the reference frame. In an example, identifying the portion of the reference frame used to generate the prediction may include: a cost function, such as a Sum of Absolute Differences (SAD), is calculated between pixels of the portion of the search area and pixels of the current block.
In some embodiments, the spatial difference between the position of the portion of the reference frame used to generate the prediction in the reference frame and the current block in the current frame may be represented as a motion vector. The difference in pixel values between the prediction block and the current block may be referred to as differential data, residual data, a prediction error, or a residual block. In some embodiments, generating motion vectors may be referred to as motion estimation and may be based on using cartesian coordinates (e.g., f)x,y) Indicating the pixels of the current block. Similarly, it may be based on using Cartesian coordinates (e.g., r)x,y) Indicating the pixels of the search area of the reference frame. The Motion Vector (MV) for the current block may be determined based on, for example, the SAD between the pixels of the current frame and the corresponding pixels of the reference frame.
Although described herein with reference to a matrix or cartesian representation of a frame for clarity, frames may be stored, transmitted, processed, or any combination thereof in any data structure so that pixel values may be efficiently represented for a frame or image. For example, frames may be stored, transmitted, processed, or any combination thereof, in a two-dimensional data structure (such as a matrix as shown) or in a one-dimensional data structure (such as a vector array). In one embodiment, a representation of a frame (such as the two-dimensional representation shown) may correspond to a physical location when the frame is rendered as an image. For example, a position in the upper left corner of a block in the upper left corner of a frame may correspond to a physical position in the upper left corner where the frame is rendered as an image.
In some embodiments, block-based transcoding efficiency may be improved by partitioning an input block into one or more predictive partitions, which may be rectangular (including square) partitions used for predictive transcoding. In some implementations, video transcoding using predictive partitioning may include selecting a predictive partitioning scheme from among a plurality of candidate predictive partitioning schemes. For example, in some implementations, a candidate prediction partition scheme for a 64 × 64 transcoding unit may include rectangular-sized prediction partitions that vary in size from 4 × 4 to 64 × 64 (such as 4 × 4, 4 × 8, 8 × 4, 8 × 8, 8 × 16, 16 × 8, 16 × 16, 16 × 32, 32 × 16, 32 × 32, 32 × 64, 64 × 32, or 64 × 64). In some implementations, video transcoding using predictive partitioning may include a complete predictive partitioning search, which may include selecting a predictive partitioning scheme by encoding a transcoding unit using each available candidate predictive partitioning scheme and selecting the best scheme (e.g., the scheme that yields the smallest rate distortion error).
In some implementations, encoding a video frame can include identifying a predictive partitioning scheme for encoding a current block (such as block 610). In some implementations, identifying the prediction partitioning scheme may include determining whether to encode the block into a single prediction partition of a maximum transcoding unit size (which may be 64 × 64 as illustrated), or to partition the block into multiple prediction partitions, which may correspond to sub-blocks, such as 32 × 32 blocks 620, 16 × 16 blocks 630, or 8 × 8 blocks 640 as illustrated, and may include determining whether to partition into one or more smaller prediction partitions. For example, a 64 × 64 block may be partitioned into four 32 × 32 prediction partitions. Three of the four 32 x 32 prediction partitions may be encoded as 32 x 32 prediction partitions, and the fourth 32 x 32 prediction partition may be further partitioned into four 16 x 16 prediction partitions. Three of the four 16 x 16 prediction partitions may be encoded as 16 x 16 prediction partitions, the fourth 16 x 16 prediction partition may be further divided into four 8 x 8 prediction partitions, and each prediction partition may be encoded as an 8 x 8 prediction partition. In some implementations, identifying a predictive partitioning scheme can include using a predictive partitioning decision tree.
In some embodiments, video transcoding for a current block may include identifying an optimal predictive transcoding mode from a plurality of candidate predictive transcoding modes, which may provide flexibility in processing video signals having various statistical properties and may improve compression efficiency. For example, the video transcoder may evaluate each candidate predictive transcoding mode to identify an optimal predictive transcoding mode, which may be, for example, a predictive transcoding mode that minimizes an error metric (e.g., rate distortion cost) for the current block. In some embodiments, the complexity of searching for candidate predictive transcoding modes may be reduced by limiting the set of available candidate predictive transcoding modes based on the similarity between the current block and the corresponding prediction block. In some implementations, the complexity of searching each candidate predictive transcoding mode may be reduced by performing a directed refinement mode search. For example, metrics may be generated for a limited set of candidate block sizes (e.g., 16 × 16, 8 × 8, and 4 × 4), error metrics associated with each block size may be sorted in descending order, and additional candidate block sizes (such as 4 × 8 and 8 × 4 block sizes) may be evaluated.
In some embodiments, block-based transcoding efficiency may be improved by partitioning the current residual block into one or more transform partitions, which may be rectangular (including square) partitions for transform transcoding. In some implementations, video transcoding using transform partitions may include selecting a unified transform partition scheme. For example, a current residual block (such as block 610) may be a 64 × 64 block and may be transformed without partitioning by using a 64 × 64 transform.
Although not explicitly shown in fig. 6, the residual block may be transform partitioned by using a unified transform partitioning scheme. For example, a 64 × 64 residual block may be transform partitioned by using a unified transform partitioning scheme including four 32 × 32 transform blocks, using a unified transform partitioning scheme including 16 × 16 transform blocks, using a unified transform partitioning scheme including 64 8 × 8 transform blocks, or using a unified transform partitioning scheme including 256 4 × 4 transform blocks.
In some embodiments, video transcoding using transform partitions may include identifying a plurality of transform block sizes of residual blocks using multi-form transform partition transcoding. In some embodiments, the multi-form transform partition transcoding may include: it is recursively determined whether to transform the current block using a current block size transform or to transcode each partition by partitioning the current block and a multi-form transform partition. For example, the lower left block 610 shown in fig. 6 may be a 64 × 64 residual block, and the multi-form transform partition transcoding may include determining whether to transcode the current 64 × 64 residual block using a 64 × 64 transform, or to transcode the 64 × 64 residual block by partitioning the 64 × 64 residual block into packets such as four 32 × 32 blocks 620 and the multi-form transform partition transcoding each partition. In some implementations, determining whether to transform the current block may be based on comparing a cost for encoding the current block using a current block size transform to a sum of costs for encoding each partition using a partition size transform.
Fig. 7 is a flowchart of an example of decoding 700 using alpha channel prediction according to an embodiment of the present invention. Decoding using alpha channel prediction 700 may be implemented in a decoder, such as decoder 500 shown in fig. 5.
As shown in fig. 7, decoding 700 using alpha channel prediction includes: the current pixel is identified at 710, a reconstructed color value is obtained at 720, a color space converted color value is obtained at 730, an alpha channel lower limit is obtained at 740, a predicted alpha channel value is generated at 750, an adjusted predicted alpha channel value is obtained at 760, the reconstructed pixel is generated at 770, and output at 780.
Although not explicitly shown in fig. 7, decoding 700 using alpha channel prediction may include obtaining (such as being received via a wired or wireless electronic communication medium, such as network 220 shown in fig. 2, or being read from an electronic data storage medium, such as memory 110 shown in fig. 1) at least a portion of an encoded bitstream. Decoding 700 using alpha channel prediction may include generating a reconstructed image. Generating the reconstructed image may include generating the reconstructed image by using alpha channel prediction.
A current pixel may be identified at 710. The current pixel may be a pixel of a current block of a current picture, such as block 610 shown in fig. 6.
A reconstructed color channel value for the current pixel may be obtained at 720. For example, obtaining a reconstructed color channel value for the current pixel may include obtaining a reconstructed color channel value for the current pixel represented in a color model (which may be referred to as a YUV or YCbCr color model or color space) based on a luminance component (Y) and two chrominance components (U and V or Cb and Cr). Obtaining the reconstructed color channel values may include reconstructing the reconstructed color channel values by using data decoded from the encoded bitstream, such as residual color channel values. The data decoded from the encoded bitstream may be lossless transcoded data or lossy transcoded data.
A color space converted color value for the pixel may be obtained at 730. Obtaining the color space conversion color value for the pixel may include color space conversion of the reconstructed color channel value, such as from a YUV color space to another color space, such as an RGB color space, which may include a red channel (R), a green channel (G), and a blue channel (B).
An alpha channel lower limit may be obtained at 740. In some implementations, the alpha channel lower bound may be an approximate alpha channel lower bound, such as obtained by using lossy transcoding data. Obtaining the alpha channel lower limit may include obtaining a normalized red channel value by dividing the red channel value (R) by a defined maximum value (maxR) for the red channel. Obtaining the alpha channel lower limit may include obtaining a normalized green channel value by dividing the green channel value (G) by a defined maximum value for the green channel (maxG). Obtaining the alpha channel lower limit may include obtaining a normalized blue channel value by dividing the blue channel value (B) by a defined maximum value (maxB) for the blue channel. Obtaining the alpha channel lower bound may include: a maximum value among the normalized red, green, and blue channel values is identified, and a product of the maximum value among the normalized red, green, and blue channel values multiplied by a defined maximum value (maxA) for the alpha channel is identified as an approximate lower limit. The lower alpha channel limit (minA) may be obtained as follows:
in some embodiments, normalization may be omitted and the resulting alpha channel lower limit (minA) may be denoted as minA ═ max (R, G, B).
Candidate predicted alpha channel values may be generated at 750. Generating candidate predicted alpha channel values (rpredA) may include using alpha values of one or more reconstructed context pixels, such as pixels from a block above the current block, pixels of a block to the left of the current block, and pixels of blocks above and to the left of the current block. For example, the candidate predicted alpha channel value may be an average of alpha channel values of the context pixels.
An adjusted alpha channel predictor may be obtained at 760 by using the candidate predicted alpha channel value identified at 750 and the alpha channel lower limit identified at 740. The adjusted predicted alpha channel value (predA) may be obtained as predA ═ max (rpredA). In some embodiments, obtaining the adjusted predicted alpha channel value (predA) may include using a lossy compression-based alpha channel lower limit adjustment (m), e.g., an adjustment based on data lost in lossy compression, such as based on a quantization level, and obtaining the adjusted predicted alpha channel value (predA) may be denoted as predA ═ max (rpredA, minA-m).
Reconstructed pixels may be generated at 770. Generating the reconstructed pixel may include generating a reconstructed alpha channel value for the pixel. Generating the reconstructed alpha channel value may comprise obtaining a sum of the adjusted predicted alpha channel value and the decoded residual alpha channel value for the pixel as the reconstructed alpha channel value.
The reconstructed block pixels may be output at 780. For example, the reconstructed pixels may be included in a reconstructed image, and the reconstructed image may be output, e.g., via the output stream 504 shown in fig. 5, for example, for presentation to a user. Although not explicitly shown in fig. 7, generating the reconstructed block or reconstructed image may include filtering, such as the filtering shown at 560 in fig. 5.
Fig. 8 is a flow diagram of an example of encoding 800 using alpha channel prediction according to an embodiment of the present disclosure. Encoding 800 using alpha channel prediction may be implemented in an encoder, such as encoder 400 shown in fig. 4. Encoding 800 using alpha channel prediction may be similar to decoding 700 using alpha channel prediction as shown in fig. 7, except as described herein or otherwise clear from the context.
As shown in fig. 8, encoding 800 using alpha channel prediction includes: the current pixel is identified at 810, a pre-multiplied color value is obtained at 820, a reconstructed color value is obtained at 830, a color space converted color value is obtained at 840, an alpha channel lower limit is obtained at 850, a predicted alpha channel value is generated at 860, an adjusted predicted alpha channel value is obtained at 870, a reconstructed pixel is generated, and output at 880.
Although not explicitly shown in fig. 8, encoding 800 using alpha channel prediction may include obtaining a current image, which may be an input image.
A current pixel may be identified at 810. The current pixel may be a pixel of a current block of a current picture, such as block 610 shown in fig. 6.
A pre-multiplied color value for the pixel may be obtained at 820. For example, a pixel may be represented in an input image by using an RGB color space, which may include a red channel (R), a green channel (G), and a blue channel (B), and an alpha channel (a). Obtaining the pre-multiplied color values may include: the product of the red channel value multiplied by the alpha channel value is identified as a pre-multiplied red channel value, the product of the green channel value multiplied by the alpha channel value is identified as a pre-multiplied green channel value, and the product of the blue channel value multiplied by the alpha channel value is identified as a pre-multiplied blue channel value. Obtaining the pre-multiplied color values may include: obtaining a normalized alpha channel value; and obtaining the pre-multiplied color channel value using the normalized alpha channel value. Obtaining the normalized alpha channel value may include obtaining a result of dividing the input alpha channel value by the maximum alpha channel value. In some implementations, the input color channel value for the pixel can be a pre-multiplied color value.
A reconstructed color channel value for the current pixel may be obtained at 830. Although not otherwise shown in fig. 8, the pre-multiplied color values may be encoded, and the reconstructed color channel values may be obtained by decoding the encoded data for the pre-multiplied color values. The encoded pre-multiplied color values may include a color space conversion such as from an RGB color space to pre-multiplied color values based on a color model of a luminance component (Y) and two chrominance components (U and V or Cb and Cr), which may be referred to as a YUV or YCbCr color model or color space. The encoded data may be lossless transcoded data or lossy transcoded data.
Color space transform reconstructed color values for the pixel may be obtained at 840. Obtaining a color space conversion reconstructed color value for a pixel may include a color space conversion such as a reconstructed color channel value from a YUV color space to another color space (such as an RGB color space).
A lower limit of the alpha channel, which may be an approximate lower limit, may be obtained at 850. Obtaining the alpha channel lower limit may include defining the red spatially converted reconstructed color value (R) as a maximum value of the red channel (maxR). Obtaining the alpha channel lower limit may include obtaining a normalized green channel value by dividing the green space transform reconstructed color value (G) by a defined maximum value (maxG) of the green channel. Obtaining the alpha channel lower limit may include obtaining a normalized blue channel value by dividing the blue spatial transform reconstructed color value (B) by a defined maximum value (maxB) of the blue channel. Obtaining the alpha channel lower bound may include: a maximum value among the normalized red, green, and blue channel values is identified, and a product of the maximum value among the normalized red, green, and blue channel values multiplied by a defined maximum value (maxA) of the alpha channel is identified as an approximate lower limit. The lower alpha channel limit (minA) may be obtained as follows:
in some embodiments, normalization may be omitted and the resulting alpha channel lower limit (minA) may be denoted as minA ═ max (R, G, B).
Predicted alpha channel values may be generated at 860. Generating the predicted alpha channel values (rpredA) may include using alpha values of one or more reconstructed context pixels, such as pixels from a block above the current block, pixels of a block to the left of the current block, and pixels of blocks above and to the left of the current block. For example, the candidate predicted alpha channel value may be an average of alpha channel values of the context pixels.
An adjusted alpha channel predictor may be obtained at 870 by using the candidate predicted alpha channel value identified at 860 and the alpha channel lower limit identified at 850. The adjusted predicted alpha channel value (predA) may be obtained as predA ═ max (rpredA).
The encoded image data may be output at 880. Although not otherwise shown in fig. 8, the adjusted predicted alpha channel values obtained at 870 may be subtracted from the input pixel alpha values to obtain residual alpha values, and may be included in the output bitstream.
In some implementations, the residual alpha value can be a limited alpha channel lower limit residual alpha value, wherein for residual alpha values less than the alpha channel lower limit, the alpha channel lower limit can be used as the residual alpha value. In some embodiments, the lower alpha channel limit, an approximation of the lower alpha channel limit, is obtained, for example, by using lossy transcoding data, wherein for residual alpha values that are smaller than the approximation of the lower alpha channel limit, the approximation of the lower alpha channel limit may be used as the residual alpha value. The approximation of the lower alpha channel limit may be the product of the lower alpha channel limit multiplied by a defined approximation parameter, such as 0.9.
The terms "optimal," "optimized," "optimization," or other forms thereof, as used herein, are relative to the corresponding context and do not indicate absolute theoretical optimization unless explicitly indicated herein.
The term "set" as used herein indicates a distinguishable set or grouping of zero or more different elements or members that can be represented as a one-dimensional array or vector, unless explicitly described herein or clearly evident from the context.
The word "example" or "exemplary" is used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as "exemplary" or "exemplary" is not necessarily to be construed as preferred or advantageous over other aspects or designs. Rather, use of the word "example" or "exemplary" is intended to present concepts in a concrete fashion. As used in this application, the term "or" is intended to mean an inclusive "or" rather than an exclusive "or". That is, unless specified otherwise or clearly indicated otherwise by context, the statement that "X comprises a or B" is intended to mean any of the natural inclusive permutations. That is, if X comprises A; x comprises B; or X includes both a and B, then "X includes a or B" is satisfied in any of the above examples. In addition, the articles "a" and "an" as used in this application and the appended claims should generally be construed to mean "one or more" unless specified otherwise or indicated by context to be directed to a singular form. Furthermore, use of the term "embodiment" or the term "one embodiment" throughout this disclosure is not intended to mean the same embodiment, unless so described. As used herein, the terms "determine" and "identify," or any variation thereof, include selecting, determining, calculating, looking up, receiving, determining, establishing, obtaining, or otherwise identifying or determining in any way using one or more of the devices shown in fig. 1.
Moreover, for simplicity of explanation, while the figures and descriptions herein may include a sequence or series of steps or stages, elements of the methods disclosed herein may occur in various orders and/or concurrently. Additionally, elements of the methods disclosed herein may appear with other elements not explicitly shown and described herein. Furthermore, in accordance with the disclosed subject matter, one or more elements of the methods described herein may be omitted from embodiments of the methods.
Embodiments of the sending computing and communication device 100A or the receiving computing and communication device 100B (as well as algorithms, methods, instructions, etc. stored thereon and/or executed thereby) may be implemented in hardware, software, or any combination thereof. The hardware may include, for example, a computer, an Intellectual Property (IP) core, an Application Specific Integrated Circuit (ASIC), a programmable logic array, an optical processor, a programmable logic controller, microcode, microcontrollers, servers, microprocessors, digital signal processors, or any other suitable circuit. In the claims, the term "processor" should be understood to encompass any of the above hardware, alone or in combination. The terms "signal" and "data" are used interchangeably. In addition, the respective parts of the transmitting computing and communication device 100A or the receiving computing and communication device 100B do not necessarily have to be implemented in the same way.
Further, in one implementation, for example, the transmitting computing and communication device 100A or the receiving computing and communication device 100B may be implemented using a computer program that, when executed, performs any of the various methods, algorithms, and/or instructions described herein. Additionally or alternatively, for example, a special purpose computer/processor may be utilized that may contain specialized hardware for performing any of the methods, algorithms, or instructions described herein.
The sending computing and communication device 100A and the receiving computing and communication device 100B may be implemented on computers in a real-time video system, for example. Alternatively, the sending computing and communication device 100A may be implemented on a server, while the receiving computing and communication device 100B may be implemented on a device separate from the server (e.g., a handheld communication device). In this case, the transmitting computing and communication device 100A may encode the content into an encoded video signal using the encoder 400 and transmit the encoded video signal to the communication device. The communication device may then decode the encoded video signal using the decoder 500. Alternatively, the communication device may decode content stored locally on the communication device (such as content not transmitted by the transmitting computing and communication device 100A). Other suitable embodiments of the sending computing and communication device 100A and the receiving computing and communication device 100B are available. For example, the receiving computing and communication device 100B may be a generally stationary personal computer rather than a portable communication device, and/or a device including the encoder 400 may also include the decoder 500.
Furthermore, some or all of the implementations of the present disclosure may take the form of a computer program product accessible from a computer-usable or computer-readable medium, for example. A computer-usable or computer-readable medium may be, for example, any device that can tangibly contain, store, communicate, or transport the program for use by or in connection with any processor. The medium may be, for example, an electronic, magnetic, optical, electromagnetic, or semiconductor device. Other suitable media are also available.
It will be appreciated that aspects may be implemented in any convenient form. For example, aspects may be implemented by suitable computer programs, which may be carried on suitable carrier media, which may be tangible carrier media (e.g., magnetic disks) or intangible carrier media (e.g., communications signals). Aspects may also be implemented using suitable apparatus, which may take the form of a programmable computer running a computer program arranged to implement the methods and/or techniques disclosed herein. Various aspects may be combined such that features described in the context of one aspect may be implemented in another aspect.
The above embodiments have been described in order to facilitate understanding of the present application and do not limit the present application. On the contrary, the application is intended to cover various modifications and equivalent arrangements included within the scope of the appended claims, which scope is to be accorded the broadest interpretation allowed under the law so as to encompass all such modifications and equivalent structures as is permitted under the law.
Claims (15)
1. A method, comprising:
generating the reconstructed image by using alpha channel prediction, wherein generating the reconstructed image by using alpha channel prediction comprises:
obtaining a reconstructed color channel value for a current pixel of a current image represented by a reference first color space;
obtaining a color space converted color channel value for the current pixel by converting the reconstructed color channel value to a second color space;
obtaining an alpha channel lower limit for an alpha channel value of the current pixel by converting a color channel value using the color space;
generating a candidate predicted alpha value for the current pixel;
obtaining an adjusted predicted alpha value for the current pixel by using the candidate predicted alpha value and the alpha channel lower limit;
generating a reconstructed pixel for the current pixel by using the adjusted predicted alpha value; and
including the reconstructed pixel in the reconstructed image; and
and outputting the reconstructed image.
2. The method of claim 1, wherein:
the first color space is a YUV color space;
the reconstructed color channel value comprises a luminance channel value, a first chrominance channel value and a second chrominance channel value; and
obtaining the reconstructed color channel values comprises decoding residual color channel values from an encoded bitstream.
3. The method of claim 1, wherein the second color space is an RGB color space and the color space conversion color channel values include red, green, and blue channel values.
4. The method of claim 3, wherein obtaining the alpha channel lower bound comprises:
obtaining a normalized red channel value by dividing the red channel value by a defined maximum value for the red channel;
obtaining a normalized green channel value by dividing the green channel value by a defined maximum value for the green channel;
obtaining a normalized blue channel value by dividing the blue channel value by a defined maximum value for the blue channel;
identifying a maximum value among the normalized red channel value, the normalized green channel value, and the normalized blue channel value; and
identifying the product of the maximum value multiplied by a defined maximum value for the alpha channel as the alpha channel lower limit.
5. The method of claim 1, wherein generating the candidate predicted alpha values comprises:
identifying previously reconstructed context pixels for predicting the candidate predicted alpha value; and
obtaining the candidate alpha prediction value by using the previously reconstructed context pixel.
6. The method of claim 1, wherein obtaining the adjusted predicted alpha value comprises:
identifying a maximum of the candidate alpha predictor value and the alpha channel lower limit as the adjusted alpha predictor value.
7. A method, comprising:
generating an encoded image using alpha channel prediction, wherein generating the encoded image using the alpha channel prediction comprises:
identifying a current pixel from an input image, wherein the current pixel comprises input color channel values, wherein the input color channel values are represented with reference to a first color space, and wherein the input color channel values comprise input alpha channel values;
obtaining a pre-multiplied color channel value for the pixel by using the input color channel value;
obtaining a reconstructed color channel value for the pixel by using the pre-multiplied color channel value, wherein the reconstructed color channel value is represented with reference to a second color space;
obtaining a color space converted color channel value for the current pixel by converting the reconstructed color channel value color space to the first color space;
obtaining an alpha channel lower limit for a reconstructed alpha channel value of the current pixel by converting a color channel value using the color space;
generating a candidate predicted alpha value for the current pixel;
obtaining an adjusted predicted alpha value for the current pixel by using the candidate predicted alpha value and the alpha channel lower limit;
obtaining a difference of the adjusted predicted alpha value subtracted from the input alpha channel value as a residual alpha value; and
including the residual alpha values in an output bitstream; and
outputting the output bitstream.
8. The method of claim 7, wherein the second color space is a YUV color space and the reconstructed color channel values include a luminance channel value, a first chrominance channel value, and a second chrominance channel value.
9. The method of claim 7, wherein:
the first color space is an RGB color space;
the input color channel values include an input red channel value, an input green channel value, and an input blue channel value;
the pre-multiplied color channel values comprise a pre-multiplied red channel value, a pre-multiplied green channel value and a pre-multiplied blue channel value; and
the color space conversion color channel values include a color space conversion red channel value, a color space conversion green channel value, and a color space conversion blue channel value.
10. The method of claim 9, wherein obtaining the pre-multiplied color channel value comprises:
obtaining a product of multiplying the input red channel value by the input alpha channel value as the pre-multiplied red channel value;
obtaining a product of multiplying the input green channel value by the input alpha channel value as the pre-multiplied green channel value; and
obtaining a product of multiplying the input blue channel value by the input alpha channel value as the pre-multiplied blue channel value.
11. The method of claim 9, wherein obtaining the alpha channel lower bound comprises:
obtaining a normalized red channel value by dividing the color space converted red channel value by a defined maximum value for the red channel;
obtaining a normalized green channel value by dividing the color space converted green channel value by a defined maximum value for the green channel;
obtaining a normalized blue channel value by dividing the color space converted blue channel value by a defined maximum value for the blue channel;
identifying a maximum value among the normalized red channel value, the normalized green channel value, and the normalized blue channel value; and
identifying the product of the maximum value multiplied by a defined maximum value for the alpha channel as the alpha channel lower limit.
12. The method of claim 7, wherein obtaining the reconstructed color channel value comprises:
converting the pre-multiplied color channel value color space to the second color space;
obtaining respective predicted color channel values for the pre-multiplied color channel values;
obtaining respective residual color channel values as respective differences of subtracting the predicted color channel values from the corresponding pre-multiplied color channel values;
lossy encoding the residual color channel values to obtain encoded residual color channel values; and
obtaining, as the reconstructed color channel value, a respective sum of the encoded residual color channel value and the corresponding predicted color channel value.
13. The method of claim 7, wherein generating the candidate predicted alpha values comprises:
identifying previously reconstructed context pixels for predicting the candidate predicted alpha value; and
obtaining the candidate alpha prediction value by using the previously reconstructed context pixel.
14. The method of claim 7, wherein obtaining the adjusted predicted alpha value comprises:
identifying a maximum value among the candidate predicted alpha value and the alpha channel lower limit as the adjusted predicted alpha value.
15. An apparatus comprising a processor configured to perform the method of any one of claims 1-14.
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
EP20305773.2 | 2020-07-07 | ||
EP20305773.2A EP3937487A1 (en) | 2020-07-07 | 2020-07-07 | Alpha channel prediction |
Publications (1)
Publication Number | Publication Date |
---|---|
CN113382250A true CN113382250A (en) | 2021-09-10 |
Family
ID=71944034
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202110625992.7A Pending CN113382250A (en) | 2020-07-07 | 2021-06-04 | Alpha channel prediction |
Country Status (3)
Country | Link |
---|---|
US (3) | US11528498B2 (en) |
EP (1) | EP3937487A1 (en) |
CN (1) | CN113382250A (en) |
Families Citing this family (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20220337865A1 (en) * | 2019-09-23 | 2022-10-20 | Sony Group Corporation | Image processing device and image processing method |
US20220286709A1 (en) * | 2021-02-26 | 2022-09-08 | Lemon Inc. | Methods of coding images/videos with alpha channels |
US20220417537A1 (en) * | 2021-06-23 | 2022-12-29 | Black Sesame International Holding Limited | Unprocessed image coding and decoding |
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2003087572A (en) * | 2001-09-14 | 2003-03-20 | Akuseru:Kk | Method and device for compressing image, image compression program and image processor |
US20070053423A1 (en) * | 2005-09-02 | 2007-03-08 | Tinic Uro | System and method for decompressing video data and alpha channel data using a single stream |
CN109413430A (en) * | 2016-08-15 | 2019-03-01 | 联发科技股份有限公司 | Video coding-decoding method and its device |
Family Cites Families (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5748789A (en) * | 1996-10-31 | 1998-05-05 | Microsoft Corporation | Transparent block skipping in object-based video coding systems |
US6707459B1 (en) * | 2001-02-01 | 2004-03-16 | Apple Computer, Inc. | Adjustment of color values for optimized image processing |
-
2020
- 2020-07-07 EP EP20305773.2A patent/EP3937487A1/en active Pending
-
2021
- 2021-06-04 CN CN202110625992.7A patent/CN113382250A/en active Pending
- 2021-06-22 US US17/354,562 patent/US11528498B2/en active Active
-
2022
- 2022-11-28 US US17/994,593 patent/US11765377B2/en active Active
-
2023
- 2023-08-11 US US18/448,561 patent/US20230388534A1/en active Pending
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2003087572A (en) * | 2001-09-14 | 2003-03-20 | Akuseru:Kk | Method and device for compressing image, image compression program and image processor |
US20070053423A1 (en) * | 2005-09-02 | 2007-03-08 | Tinic Uro | System and method for decompressing video data and alpha channel data using a single stream |
CN109413430A (en) * | 2016-08-15 | 2019-03-01 | 联发科技股份有限公司 | Video coding-decoding method and its device |
Also Published As
Publication number | Publication date |
---|---|
US20220014773A1 (en) | 2022-01-13 |
US11765377B2 (en) | 2023-09-19 |
EP3937487A1 (en) | 2022-01-12 |
US11528498B2 (en) | 2022-12-13 |
US20230090481A1 (en) | 2023-03-23 |
US20230388534A1 (en) | 2023-11-30 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10694180B2 (en) | Entropy coding transform partitioning information | |
CN110741645B (en) | Blockiness reduction | |
GB2546888B (en) | Tile copying for video compression | |
CN115134602A (en) | Low latency two-pass video coding | |
US11528498B2 (en) | Alpha channel prediction | |
US20230308679A1 (en) | Motion prediction coding with coframe motion vectors | |
US11153588B2 (en) | Dual deblocking filter thresholds | |
US11849113B2 (en) | Quantization constrained neural image coding | |
CN107079156B (en) | Method for alternate block constrained decision mode coding | |
US10951921B2 (en) | Adjustable per-symbol entropy coding probability updating for image and video coding | |
US11012714B1 (en) | Image coding using lexicographic coding order with floating block-partitioning | |
US20230291925A1 (en) | Inter-Intra Prediction With Implicit Models | |
US20240089433A1 (en) | Chroma Transform Type Determination | |
WO2024005777A1 (en) | Circular-shift transformation for image and video coding |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |