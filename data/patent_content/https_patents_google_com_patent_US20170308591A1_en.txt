US20170308591A1 - Graphical keyboard with integrated search features - Google Patents
Graphical keyboard with integrated search features Download PDFInfo
- Publication number
- US20170308591A1 US20170308591A1 US15/332,409 US201615332409A US2017308591A1 US 20170308591 A1 US20170308591 A1 US 20170308591A1 US 201615332409 A US201615332409 A US 201615332409A US 2017308591 A1 US2017308591 A1 US 2017308591A1
- Authority
- US
- United States
- Prior art keywords
- search
- search results
- visual representation
- computing device
- keyboard
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
Images
Classifications
-
- G06F17/30554—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/248—Presentation of query results
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/24—Querying
- G06F16/245—Query processing
- G06F16/2455—Query execution
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/25—Integrating or interfacing systems involving database management systems
- G06F16/252—Integrating or interfacing systems involving database management systems between a Database Management System and a front-end application
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/903—Querying
- G06F16/9032—Query formulation
- G06F16/90324—Query formulation using system suggestions
- G06F16/90328—Query formulation using system suggestions using search space presentation or visualization, e.g. category or range presentation and selection
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/951—Indexing; Web crawling techniques
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/0482—Interaction with lists of selectable items, e.g. menus
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/04842—Selection of displayed objects or displayed text elements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
- G06F3/04883—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures for inputting data by handwriting, e.g. gesture or text
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
- G06F3/04886—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures by partitioning the display area of the touch-screen or the surface of the digitising tablet into independently controllable areas, e.g. virtual keyboards or menus
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/10—Text processing
- G06F40/166—Editing, e.g. inserting or deleting
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T13/00—Animation
- G06T13/80—2D [Two Dimensional] animation, e.g. using sprites
Definitions
- GUI graphical user interface
- a user of a computing device may explicitly or implicitly instruct the computing device to switch between different application GUIs. For example, a user of a computing device may cease entering text in a messaging application and provide input to cause the device to switch to a search application to search for information, then switch back to the search application and enter further text regarding the searched-for information, resulting in an inelegant and inefficient user experience.
- a method includes outputting, by a computing device, for display, a graphical keyboard including a plurality of keys and a suggestion region.
- the method also may include determining, by the computing device, based on a selection of the suggestion region or one or more keys from the plurality of keys, a search query; and retrieving, by the computing device, one or more search results determined based on the search query; and outputting, by the computing device, in place of at least a portion of the graphical keyboard, a visual representation of a particular search result of the one or more search results.
- a computing device in another example, includes a presence-sensitive display, at least one processor, and a memory storing instructions that, when executed by the at least one processor, cause the at least one processor to output, for display at the presence-sensitive display, a graphical keyboard including a plurality of keys and a suggestion region.
- the memory also stores instructions that, when executed by the at least one processor, cause the at least one processor to determine, based on a selection of the suggestion region or one or more keys from the plurality of keys, a search query; and retrieve one or more search results determined based on the search query; and output, in place of at least a portion of the graphical keyboard, a visual representation of a particular search result of the one or more search results.
- a computer-readable storage medium encoded with instructions that, when executed by at least one processor of a computing device, cause the at least one processor to output, for display at a presence-sensitive display, a graphical keyboard comprising a plurality of keys and a suggestion region.
- the computer-readable storage medium is also encoded with instructions that, when executed by at least one processor of a computing device, cause the at least one processor to determine, based on a selection of the suggestion region or one or more keys from the plurality of keys, a search query; and retrieve one or more search results determined based on the search query; and output, in place of at least a portion of the graphical keyboard, a visual representation of a particular search result of the one or more search results.
- FIG. 1 is a conceptual diagram illustrating an example computing device that is configured to present a graphical keyboard with integrated search features, in accordance with one or more aspects of the present disclosure.
- FIG. 2 is a block diagram illustrating an example computing device that is configured to present a graphical keyboard with integrated search features, in accordance with one or more aspects of the present disclosure.
- FIG. 3 is a block diagram illustrating an example computing device that outputs graphical content for display at a remote device, in accordance with one or more techniques of the present disclosure.
- FIGS. 4A and 4B are conceptual diagrams illustrating example graphical user interfaces of an example computing device that is configured to present a graphical keyboard with integrated search features, in accordance with one or more aspects of the present disclosure.
- FIG. 5 is a flowchart illustrating example operations of a computing device that is configured to present a graphical keyboard with integrated search features, in accordance with one or more aspects of the present disclosure.
- this disclosure is directed to techniques for enabling a computing device to output, for display, a visual representation of one or more search results within a graphical keyboard.
- the computing device may enable a user to perform a search query for desired information and view information received in response to the search query without switching between applications.
- the computing device may enable the user to input the search query and view the one or more search results within a graphical user interface that includes the graphical keyboard.
- the computing device may also determine, based on user input, selection of a portion of the visual representation of a particular search result of the one or more search results.
- the computing device may be configured to, automatically, without further user input, insert information related to the search result in a text edit region displayed adjacent to the graphical keyboard.
- the computing device may enable insertion of information from the search result in the application for which the graphical keyboard is providing input.
- the computing device may allow the user to input information with fewer inputs than a typical, multi-gesture, copy and paste action. In this way, the computing device described herein may enable information retrieval using a search query and insertion of the retrieved information in an application without application switching and with few user inputs.
- FIG. 1 is a conceptual diagram illustrating an example computing device 110 that is configured to output a graphical keyboard with integrated search features, in accordance with one or more aspects of the present disclosure.
- computing device 110 may represent a mobile computing device, such as a smart phone, a tablet computer, a laptop computer, computerized watch, computerized eyewear, computerized gloves, or any other type of portable computing device.
- computing device 110 includes desktop computers, televisions, personal digital assistants (PDA), portable gaming systems, media players, e-book readers, mobile television platforms, automobile navigation and entertainment systems, vehicle (e.g., automobile, aircraft, or other vehicle) cockpit displays, kiosks, or any other types of wearable and non-wearable, mobile or non-mobile computing devices that may output a graphical keyboard for display.
- PDA personal digital assistants
- portable gaming systems media players, e-book readers
- mobile television platforms e.g., automobile, aircraft, or other vehicle
- vehicle e.g., automobile, aircraft, or other vehicle cockpit displays, kiosks, or any other types of wearable and non-wearable, mobile or non-mobile computing devices that may output a graphical keyboard for display.
- vehicle e.g., automobile, aircraft, or other vehicle cockpit displays, kiosks, or any other types of wearable and non-wearable, mobile or non-mobile computing devices that may output a graphical keyboard for display.
- Computing device 110 includes a presence-sensitive display (PSD) 112 , a user interface (UI) module 120 , and a keyboard module 122 .
- Modules 120 and 122 may perform operations described using software, hardware, firmware, or a mixture of hardware, software, and firmware residing in and/or executing at computing device 110 .
- one or more processors of computing device 110 may execute instructions that are stored at a memory or other non-transitory storage medium of computing device 110 to perform the operations of modules 120 and 122 .
- Computing device 110 may execute modules 120 and 122 as virtual machines executing on underlying hardware.
- modules 120 and 122 may execute as one or more services of an operating system or computing platform, or modules 120 and 122 may execute as one or more executable programs at an application layer of a computing platform.
- PSD 112 of computing device 110 may function as respective input and/or output devices for computing device 110 .
- PSD 112 may be implemented using various technologies. For instance, PSD 112 may function as an input devices using presence-sensitive input screens, such as resistive touchscreens, surface acoustic wave touchscreens, capacitive touchscreens, projective capacitance touchscreens, pressure sensitive screens, acoustic pulse recognition touchscreens, or another presence-sensitive display technology.
- presence-sensitive input screens such as resistive touchscreens, surface acoustic wave touchscreens, capacitive touchscreens, projective capacitance touchscreens, pressure sensitive screens, acoustic pulse recognition touchscreens, or another presence-sensitive display technology.
- PSD 112 may also function as output (e.g., display) devices using any one or more display devices, such as liquid crystal displays (LCD), dot matrix displays, light emitting diode (LED) displays, organic light-emitting diode (OLED) displays, e-ink, or similar monochrome or color displays capable of outputting visible information to a user of computing device 110 .
- display devices such as liquid crystal displays (LCD), dot matrix displays, light emitting diode (LED) displays, organic light-emitting diode (OLED) displays, e-ink, or similar monochrome or color displays capable of outputting visible information to a user of computing device 110 .
- PSD 112 may detect input (e.g., touch and non-touch input) from a user of computing device 110 .
- PSD 112 may detect input by detecting one or more gestures from a user (e.g., the user touching, pointing, and/or swiping at or near one or more locations of PSD 112 with a finger or a stylus pen).
- PSD 112 may output information to a user in the form of a user interface (e.g., user interface 114 ), which may be associated with functionality provided by computing device 110 .
- PSD 112 may present user interface 114 which, as shown in FIG. 1 , is a graphical user interface of a chat application executing at computing device 110 and includes various graphical elements displayed at various locations of PSD 112 .
- user interface 114 may be any graphical user interface that includes a graphical keyboard with integrated search features.
- user interface 114 includes an output region 116 A, a graphical keyboard 116 B, and a text edit region 116 C.
- a user of computing device 110 may provide input at graphical keyboard 116 B to produce textual characters within edit region 116 C that form the content of the electronic messages displayed within output region 116 A.
- the messages displayed within output region 116 A form a chat conversation between a user of computing device 110 and a user of a different computing device.
- UI module 120 manages user interactions with PSD 112 and other components of computing device 110 .
- UI module 120 may act as an intermediary between various components of computing device 110 to make determinations based on user input detected by PSD 112 and generate output at PSD 112 , e.g., in response to the user input.
- UI module 120 may receive instructions from an application, service, platform, or other module of computing device 110 to cause PSD 112 to output a user interface (e.g., user interface 114 ).
- UI module 120 may manage inputs received by computing device 110 as a user views and interacts with the user interface presented at PSD 112 and update the user interface in response to receiving additional instructions from the application, service, platform, or other module of computing device 110 that is processing the user input.
- Keyboard module 122 represents an application, service, or component executing at or accessible to computing device 110 that provides computing device 110 with a graphical keyboard having integrated search features. Keyboard module 122 may switch between operating in text-entry mode in which keyboard module 122 functions similar to a traditional graphical keyboard, or search mode in which keyboard module 122 performs various integrated search functions, including, for example, receiving search queries or outputting one or more search results for display instead of at least part of graphical keyboard 116 B (e.g., one or more keys of plurality of graphical keys 118 A).
- keyboard module 122 may be a stand-alone application, service, or module executing at computing device 110 . In other examples, keyboard module 122 may be a sub-component of an application, service, or module executing at computing device 110 . For example, keyboard module 122 may be integrated into a chat or messaging application executing at computing device 110 . As another example, keyboard module 122 may be a stand-alone application or subroutine that is invoked by an application or operating platform of computing device 110 any time an application or operating platform requires graphical keyboard input functionality. In some examples, computing device 110 may download and install keyboard module 122 from an application repository of a service provider (e.g., via the Internet). In other examples, keyboard module 122 may be preloaded during production of computing device 110 .
- a service provider e.g., via the Internet
- keyboard module 122 of computing device 110 may perform traditional, graphical keyboard operations used for text-entry, such as: generating a graphical keyboard layout including plurality of graphical keys 118 A for display at PSD 112 , mapping detected inputs at PSD 112 to selections of graphical keys 118 A, determining characters based on selected graphical keys 118 A, and predicting or autocorrecting words and/or phrases based on the characters determined from selected graphical keys 118 A.
- Graphical keyboard 116 B includes graphical elements displayed as graphical keys 118 A.
- Keyboard module 122 may output information to UI module 120 that specifies the layout of graphical keyboard 116 B within user interface 114 .
- the information may include instructions that specify locations, sizes, colors, characters, and other characteristics of graphical keys 118 A.
- UI module 120 may cause PSD 112 display graphical keyboard 116 B as part of user interface 114 .
- Each key of graphical keys 118 A may be associated with a respective character (e.g., a letter, number, punctuation, or other character) displayed within the key.
- a user of computing device 110 may provide input at locations of PSD 112 at which one or more of graphical keys 118 A are displayed to input content (e.g., characters, search results, etc.) into text edit region 116 C (e.g., for composing messages that are sent and displayed within output region 116 A or for inputting a search query that computing device 110 executes from within graphical keyboard 116 B).
- Keyboard module 122 may receive information from UI module 120 indicating locations associated with input detected by PSD 112 that are relative to the locations of each of the graphical keys. Using a spatial and/or language model, keyboard module 122 may translate the inputs to selections of keys and characters, words, and/or phrases.
- PSD 112 may detect user input as a user of computing device 110 provides inputs at or near a location of PSD 112 where PSD 112 presents graphical keys 118 A.
- UI module 120 may receive, from PSD 112 , an indication of the user input detected by PSD 112 and output, to keyboard module 122 , information about the user input.
- Information about the user input may include an indication of one or more touch events (e.g., locations and other information about the input) detected by PSD 112 .
- keyboard module 122 may map detected user inputs at PSD 112 to selections of graphical keys 118 A, determine characters based on selected graphical keys 118 A, and predict or autocorrect words and/or phrases determined based on the characters associated with the selected graphical keys 118 A.
- keyboard module 122 may include a spatial model that may determine, based on the locations of keys 118 A and the information about the input, the most likely one or more graphical keys 118 A being selected. Responsive to determining the most likely one or more graphical keys 118 A being selected, keyboard module 122 may determine one or more characters, words, and/or phrases.
- each of the one or more graphical keys 118 A being selected from a user input at PSD 112 may represent an individual character or a keyboard operation.
- Keyboard module 122 may determine a sequence of characters selected based on the one or more selected graphical keys 118 A.
- keyboard module 122 may apply a language model to the sequence of characters to determine one or more likely candidate letters, morphemes, words, and/or phrases that a user is trying to input based on the selection of graphical keys 118 A.
- Keyboard module 122 may send the sequence of characters and/or candidate words and phrases to UI module 120 and UI module 120 may cause PSD 112 to present the characters and/or candidate words determined from a selection of one or more graphical keys 118 A as text within edit region 116 C.
- keyboard module 122 may cause UI module 120 to display the candidate words and/or phrases as one or more selectable spelling corrections and/or selectable word or phrase suggestions within suggestion region 118 B.
- keyboard module 122 of computing device 110 also provides integrated search capability. That is, rather than requiring a user of computing device 110 to navigate away from user interface 114 which provides graphical keyboard 116 B and output region 116 A (e.g., to a different application or service executing at or accessible from computing device 110 ), keyboard module 122 may operate in search mode in which keyboard module 122 may execute search operations and present one or more search results within the same region of PSD 112 at which graphical keyboard 116 B is displayed.
- keyboard module 122 may execute as a stand-alone application, service, or module executing at computing device 110 or as a single, integrated sub-component thereof. Therefore, if keyboard module 122 forms part of a chat or messaging application executing at computing device 110 , keyboard module 122 may provide the chat or messaging application with text-entry capability as well as search capability. Similarly, if keyboard module 122 is a stand-alone application or subroutine that is invoked by an application or operating platform of computing device 110 any time an application or operating platform requires graphical keyboard input functionality, keyboard module 122 may provide the invoking application or operating platform with text-entry capability as well as search capability.
- Keyboard module 122 may further operate in search mode, as opposed to text entry mode.
- keyboard module 122 may cause graphical keyboard 116 B to include search element 118 C.
- Search element 118 C represents a selectable element of graphical keyboard 116 B for invoking one or more of the various search features of graphical keyboard 116 B, such as invoking the search mode.
- search element 118 C e.g., by tapping or gesturing at a location or within a region of PSD 112 at which search element 118 C is displayed
- computing device 110 By selecting search element 118 C (e.g., by tapping or gesturing at a location or within a region of PSD 112 at which search element 118 C is displayed), a user can cause computing device 110 to invoke the various integrated search features without having to navigate to a separate application, service, or other feature executing at or accessible from computing device 110 .
- UI module 120 may output information to keyboard module 122 indicating that a user of computing device 110 may have selected search element 118 C. Responsive to determining that search element 118 C was selected, keyboard module 122 may transition to operating in search mode. While operating in search mode, keyboard module 122 may reconfigure graphical keyboard 116 B to execute search features as opposed to operations that are primarily attributed to text entry to text edit region 116 C. In some examples, keyboard module 122 may still enable text entry via graphical keys 118 A.
- keyboard module 122 may configure suggestion region 118 B to present suggested content (e.g., predicted search queries, predicted emoticons or so called “emojis”, or other suggested content) as selectable elements within suggestion region 118 B instead of predicted characters, words or phrases or other primarily linguistic information that keyboard module 122 derives from a language model, lexicon, or dictionary.
- suggested content e.g., predicted search queries, predicted emoticons or so called “emojis”, or other suggested content
- computing device 110 may present, within suggestion region 118 B, suggested search related content that computing device 110 determines may assist a user in providing input related to electronic communications.
- keyboard module 122 may configure suggestion region 118 B to present search queries input using graphical keys 118 A instead of predicted characters, words or phrases or other primarily linguistic information that keyboard module 122 derives from a language model, lexicon, or dictionary.
- computing device 110 may present, within suggestion region 118 B, search queries input using graphical keys 118 A.
- computing device 110 may determine, based on user input, a search query.
- the user input may include selection of one or more graphical keys 118 A of graphical keyboard 116 B.
- keyboard module 122 may receive, from UI module 120 , indications of user input at locations of PSD 112 corresponding to graphical keyboard 116 B indicating a search query.
- keyboard module 122 may receive, from UI module 120 , an indication of a user input selecting search element 118 C.
- keyboard module 122 may invoke the search mode, and may configure suggestion region 118 B to present search queries input using one or more graphical keys 118 A.
- keyboard module 122 may configure suggestion region 118 B to display search queries with a different format than predicted characters, words or phrases so that a user may visually distinguish search queries from predicted characters, words or phrases.
- keyboard module 122 may configure suggestion region 118 B to display search queries with a predetermined text color, font, font size, etc., or adjacent to or within a distinguishing user interface element, such as a text box, to visually differentiate a search query from predicted characters, words or phrases.
- keyboard module 122 may receive, from UI module 120 , an indication of a user input selecting or attempting to select one or more graphical keys 118 A.
- Keyboard module 122 may include a spatial model, a language model, or both, which may determine, based on locations of keys 118 A, information about the input, and a sequence of characters, the most likely one or more letters, morphemes, words, and/or phrases that a user is trying to input based on the selection of keys 118 A.
- Keyboard module 122 may cause UI module 120 to output the candidate letters, morphemes, words, and/or phrases within suggestion region 118 B as the search query.
- keyboard module 122 may cause UI module 120 to output multiple candidate letters, morphemes, words, and/or phrases, e.g., by expanding suggestion region 118 B to present multiple candidate letters, morphemes, words, and/or phrases as individual lines of text.
- Keyboard module 122 may receive, from UI module 120 , an indication of a user input indicating completion of the search query, which may correspond to a user selecting a user interface element displayed at PSD 112 , such as one of graphical keys 118 A (e.g., the RETURN/SEARCH key illustrated in FIG. 1 ); one of the candidate letters, morphemes, words, and/or phrases displayed at suggestion region 118 B; etc.
- a user interface element displayed at PSD 112 such as one of graphical keys 118 A (e.g., the RETURN/SEARCH key illustrated in FIG. 1 ); one of the candidate letters, morphemes, words, and/or phrases displayed at suggestion region 118 B; etc.
- keyboard module 122 may determine the search query, e.g., as the letters, morphemes, words, and/or phrases displayed at suggestion region 118 B at the time of the user input indicating completion of the search query or the selected one or the multiple candidate letters, morphemes, words, and/or phrases displayed in an expanded suggestion region 118 B.
- keyboard module 122 may determine a predicted search query based on content of output region 116 A, text edit region 116 C, or both.
- output region 116 A is an application such as a messaging or chat application, and illustrates communications from the user of computing device 110 (under “YOU”) and communications from a user of another computing device (under “FRIEND”).
- output region 116 A may be another application user interface, such as a notes application, a web browser, a search application, a maps application, an email application, a text editor application, or any other application for which graphical keyboard 116 B provides text input.
- keyboard module 122 may determine a predicted query based on content of the communication from the user of computing device 110 .
- FIG. 1 illustrates the content of the communication from the user of computing device 110 as being “Dinner tonight?” Based on the content of this communication, keyboard module 122 has determined a predicted search query “restaurants”. Keyboard module 122 then outputs an indication of the predicted search query to UI module 120 , which causes the predicted search query “restaurants” to be output at suggestion region 118 B.
- keyboard module 122 may analyze only content of communications from the user of computing device 110 to determine a predicted search query. In other words, in some examples, keyboard module 122 may refrain from analyzing content of communications from a user of another computing device to determine a predicted search query. In other examples, keyboard module 122 may analyze content of communications from a user of another computing device (and received by computing device 110 ) to determine a predicted search query.
- Computing device 110 may determine a predicted search query based on content of the communication from the user of computing device 110 , content of the communication from the user of the other computing device, or both, only if the computing device receives permission from the user of the respective computing device to analyze the information. For example, before computing device 110 can collect or may make use of information associated with a user of computing device 110 or a user of another computing device, the user may be provided with an opportunity to provide input to control whether programs or features of computing device 110 can collect and make use of user information (e.g., information about a user's current location, current speed, text conversations, etc.), or to dictate whether and/or how to computing device 110 may receive content that may be relevant to the user.
- user information e.g., information about a user's current location, current speed, text conversations, etc.
- certain data may be treated in one or more ways before it is stored or used by computing device 110 , so that personally-identifiable information is removed.
- a user's identity may be treated so that no personally identifiable information can be determined about the user, or a user's geographic location may be generalized where location information is obtained (such as to a city, ZIP code, or state level), so that a particular location of a user cannot be determined.
- location information such as to a city, ZIP code, or state level
- keyboard module 122 may transmit instructions to UI module 120 to cause the predicted search query to be displayed with a different visual appearance than predicted characters, words or phrases.
- keyboard module 122 transmit instructions to UI module 120 to cause the predicted search query to be displayed with a predetermined text color, font, font size, etc., or adjacent to or within a distinguishing user interface element, such as a text box, an underline, an icon or picture, etc. to visually differentiate a search query from predicted characters, words or phrases. This may allow a user of computing device 110 to easily distinguish between a predicted search query and predicted characters, words or phrases.
- the user may select suggestion region 118 B or the predicted search query displayed at suggestion region 118 B.
- Keyboard module 122 may receive an indication of the selection of suggestion region 118 B or the predicted search query displayed at suggestion region 118 B from UI module 120 , and may determine that the search query is the predicted search query displayed at suggestion region 118 B.
- Computing device 110 then may retrieve one or more search results based on the search query.
- computing device 110 may retrieve the one or more search results by transmitting the search query to a data service.
- the data service may be, for example, a search system or a cloud-based computing service remote from computing device 110 .
- the data service may perform a search.
- the search may include, for example, a search of webpages; a search of a semantic network including objects, facts, and relationships between the objects; an image search; a video search; a search localized to a location of the user of computing device 110 ; etc.
- the data service then may transmit at least one search result to computing device 110 , and computing device 110 may receive the at least one search result.
- the one or more search results may include textual information, images, videos, hyperlinks, etc.
- computing device 110 may retrieve the one or more search results by performing a search based on content of the search query.
- the search may include, for example, a search of webpages; a search of a semantic network including objects, facts, and relationships between the objects; an image search; a video search; a search localized to a location of the user of computing device 110 ; etc.
- Keyboard module 122 of computing device 110 may retrieve the one or more search results and cause UI module 120 to output a visual representation of at least a portion of the one or more search results in place of at least a portion of graphical keyboard 116 B.
- keyboard module 122 may cause UI module 120 to output the visual representation of at least a portion of the one or more search results in place of at least some keys of graphical keys 118 A.
- keyboard module 122 of computing device 110 may cause UI module 120 to output the visual representation as a card-based user interface element.
- the card-based user interface element may appear similar to a notecard, and may include a representation of a particular search result of the one or more search results.
- the representation of the particular search result may include, for example, at least one of text, a picture, an icon, or the like.
- Keyboard module 122 may generate a respective card-based user interface element for each respective search result of the one or more search results. Keyboard module 122 may cause UI module 120 to output at least one card-based user interface element for display at a time, and UI module 120 may be configured to switch between respective card-based user interface elements in response to determining a user gesture, e.g., a swipe. In this way, keyboard module 122 may enable a user to switch among one or more search results within graphical keyboard 116 B.
- a user gesture e.g., a swipe.
- keyboard module 122 may be configured to determine based on an indication of user input received from UI module 120 , selection of a predetermined portion of the visual representation of a search result. In response to determining the selection, keyboard module 122 may be configured to automatically, without further user input, output an indication of information related to the search result to UI module 120 , which is configured to insert the information related to the search result in text edit region 116 C.
- the information related to the search result may include, for example, text, a hyperlink, an image, an icon, etc.
- the selection of the predetermined portion of the visual representation of the search result may include a tap gesture in which the user taps the predetermined portion of the visual representation of the search result.
- keyboard module 122 may be configured to insert a textual representation of the search result in text edit region 116 C or output region 116 A.
- the selection of the predetermined portion of the visual representation of the search result may include a drag gesture in which the user is dragging the visual representation of the search result to text edit region 116 C or output region 116 A.
- keyboard module 122 may be configured to insert an image of the search result or a portion of the search result in text edit region 116 C or output region 116 A.
- computing device 110 is configured to allow one-gesture input of information from the search result to the text edit region 116 C, enabling easy and efficient entry of information from the search result to the application for which graphical keyboard 116 B is being used to input text.
- the techniques of the disclosure may enable a computing device to provide a graphical keyboard with integrated search features that include display of a visual representation of at least a portion of one or more search results in place of at least a portion of the graphical keyboard.
- the visual representation may include a card-based user interface element
- computing device 110 may be configured to output different card-based user interface elements, corresponding to different search results determined based on the search query, in response to receiving an indication of a user input, such as a swipe gesture.
- computing device 110 may be configured to determine, based on user input, selection of a predetermined portion of the visual representation of the at least a portion (e.g., a search result) of the one or more search results and, automatically, without further user input, insert, in text edit region 116 C displayed adjacent to graphical keyboard 116 B, information related to the at least a portion (e.g., a search result) of the one or more search results.
- a predetermined portion of the visual representation of the at least a portion e.g., a search result
- computing device 110 may be configured to determine, based on user input, selection of a predetermined portion of the visual representation of the at least a portion (e.g., a search result) of the one or more search results and, automatically, without further user input, insert, in text edit region 116 C displayed adjacent to graphical keyboard 116 B, information related to the at least a portion (e.g., a search result) of the one or more search results.
- computing device 110 may facilitate insertion of information from the search result in the application for which graphical keyboard 116 B is providing input. Further, by inserting the information from the search result in text edit region 116 C with a single user input, computing device 110 may allow the user to input information with fewer inputs than a typical, multi-gesture copy and paste action. In this way, computing device 110 may facilitate information retrieval using a search query and insertion of the retrieved information in an application without application switching and with few user inputs.
- FIG. 2 is a block diagram illustrating computing device 210 as an example computing device that is configured to present a graphical keyboard with integrated search features, in accordance with one or more aspects of the present disclosure.
- Computing device 210 of FIG. 2 is described below as an example of computing device 110 of FIG. 1 .
- FIG. 2 illustrates only one particular example of computing device 210 , and many other examples of computing device 210 may be used in other instances and may include a subset of the components included in example computing device 210 or may include additional components not shown in FIG. 2 .
- computing device 210 includes presence-sensitive display (PSD 212 ), one or more processors 240 , one or more communication units 242 , one or more input components 244 , one or more output components 246 , and one or more storage components 248 .
- PSD 212 includes display component 202 and presence-sensitive input component 204 .
- Storage components 248 of computing device 210 include UI module 220 , keyboard module 222 , and one or more application modules 224 .
- Keyboard module 222 may include spatial model (“SM”) module 226 , language model (“LM”) module 228 , and search module 230 .
- SM spatial model
- LM language model
- Communication channels 250 may interconnect each of the components 212 , 240 , 242 , 244 , 246 , and 248 for inter-component communications (physically, communicatively, and/or operatively).
- communication channels 250 may include a system bus, a network connection, an inter-process communication data structure, or any other method for communicating data.
- One or more communication units 242 of computing device 210 may communicate with external devices via one or more wired and/or wireless networks by transmitting and/or receiving network signals on the one or more networks.
- Examples of communication units 242 include a network interface card (e.g. such as an Ethernet card), an optical transceiver, a radio frequency transceiver, a GPS receiver, or any other type of device that can send and/or receive information.
- Other examples of communication units 242 may include short wave radios, cellular data radios, wireless network radios, as well as universal serial bus (USB) controllers.
- USB universal serial bus
- One or more input components 244 of computing device 210 may receive input. Examples of input are tactile, audio, and video input.
- Input components 242 of computing device 210 includes a presence-sensitive input device (e.g., a touch sensitive screen, a PSD), mouse, keyboard, voice responsive system, video camera, microphone or any other type of device for detecting input from a human or machine.
- a presence-sensitive input device e.g., a touch sensitive screen, a PSD
- mouse e.g., keyboard, voice responsive system, video camera, microphone or any other type of device for detecting input from a human or machine.
- input components 242 may include one or more sensor components one or more location sensors (GPS components, Wi-Fi components, cellular components), one or more temperature sensors, one or more movement sensors (e.g., accelerometers, gyros), one or more pressure sensors (e.g., barometer), one or more ambient light sensors, and one or more other sensors (e.g., microphone, camera, infrared proximity sensor, hygrometer, and the like).
- Other sensors may include a heart rate sensor, magnetometer, glucose sensor, hygrometer sensor, olfactory sensor, compass sensor, step counter sensor, to name a few other non-limiting examples.
- One or more output components 246 of computing device 110 may generate output. Examples of output are tactile, audio, and video output.
- Output components 246 of computing device 210 includes a PSD, sound card, video graphics adapter card, speaker, cathode ray tube (CRT) monitor, liquid crystal display (LCD), or any other type of device for generating output to a human or machine.
- PSD 212 of computing device 210 may be similar to PSD 112 of computing device 110 and includes display component 202 and presence-sensitive input component 204 .
- Display component 202 may be a screen at which information is displayed by PSD 212 and presence-sensitive input component 204 may detect an object at and/or near display component 202 .
- presence-sensitive input component 204 may detect an object, such as a finger or stylus, that is within two inches or less of display component 202 .
- Presence-sensitive input component 204 may determine a location (e.g., an [x, y] coordinate) of display component 202 at which the object was detected.
- presence-sensitive input component 204 may detect an object six inches or less from display component 202 . Other ranges are also possible.
- Presence-sensitive input component 204 may determine the location of display component 202 selected by a user's finger using capacitive, inductive, and/or optical recognition techniques. In some examples, presence-sensitive input component 204 also provides output to a user using tactile, audio, or video stimuli as described with respect to display component 202 . In the example of FIG. 2 , PSD 212 may present a user interface (such as graphical user interface 114 of FIG. 1 ).
- PSD 212 may also represent an external component that shares a data path with computing device 210 for transmitting and/or receiving input and output.
- PSD 212 represents a built-in component of computing device 210 located within and physically connected to the external packaging of computing device 210 (e.g., a screen on a mobile phone).
- PSD 212 represents an external component of computing device 210 located outside and physically separated from the packaging or housing of computing device 210 (e.g., a monitor, a projector, etc. that shares a wired and/or wireless data path with computing device 210 ).
- PSD 212 of computing device 210 may detect two-dimensional and/or three-dimensional gestures as input from a user of computing device 210 .
- a sensor of PSD 212 may detect a user's movement (e.g., moving a hand, an arm, a pen, a stylus, etc.) within a threshold distance of the sensor of PSD 212 .
- PSD 212 may determine a two or three dimensional vector representation of the movement and correlate the vector representation to a gesture input (e.g., a hand-wave, a pinch, a clap, a pen stroke, etc.) that has multiple dimensions.
- a gesture input e.g., a hand-wave, a pinch, a clap, a pen stroke, etc.
- PSD 212 can detect a multi-dimension gesture without requiring the user to gesture at or near a screen or surface at which PSD 212 outputs information for display. Instead, PSD 212 can detect a multi-dimensional gesture performed at or near a sensor which may or may not be located near the screen or surface at which PSD 212 outputs information for display.
- processors 240 may implement functionality and/or execute instructions associated with computing device 210 .
- Examples of processors 240 include application processors, display controllers, auxiliary processors, one or more sensor hubs, and any other hardware configure to function as a processor, a processing unit, or a processing device.
- Modules 220 , 222 , 224 , 226 , 228 , and 230 may be operable by processors 240 to perform various actions, operations, or functions of computing device 210 .
- processors 240 of computing device 210 may retrieve and execute instructions stored by storage components 248 that cause processors 240 to perform the operations of modules 220 , 222 , 224 , 226 , 228 , and 230 .
- the instructions when executed by processors 240 , may cause computing device 210 to store information within storage components 248 .
- One or more storage components 248 within computing device 210 may store information for processing during operation of computing device 210 (e.g., computing device 210 may store data accessed by modules 220 , 222 , 224 , 226 , 228 , and 230 during execution at computing device 210 ).
- storage component 248 is a temporary memory, meaning that a primary purpose of storage component 248 is not long-term storage.
- Storage components 248 on computing device 210 may be configured for short-term storage of information as volatile memory and therefore not retain stored contents if powered off. Examples of volatile memories include random access memories (RAM), dynamic random access memories (DRAM), static random access memories (SRAM), and other forms of volatile memories known in the art.
- Storage components 248 also include one or more computer-readable storage media.
- Storage components 248 in some examples include one or more non-transitory computer-readable storage mediums.
- Storage components 248 may be configured to store larger amounts of information than typically stored by volatile memory.
- Storage components 248 may further be configured for long-term storage of information as non-volatile memory space and retain information after power on/off cycles. Examples of non-volatile memories include magnetic hard discs, optical discs, floppy discs, flash memories, or forms of electrically programmable memories (EPROM) or electrically erasable and programmable (EEPROM) memories.
- EPROM electrically programmable memories
- EEPROM electrically erasable and programmable
- Storage components 248 may store program instructions and/or information (e.g., data) associated with modules 220 , 222 , 224 , 226 , 228 , and 230 .
- Storage components 248 may include a memory configured to store data or other information associated with modules 220 , 222 , 224 , 226 , 228 , and 230 .
- UI module 220 may include all functionality of UI module 120 of computing device 110 of FIG. 1 and may perform similar operations as UI module 120 for managing a user interface (e.g., user interface 114 ) that computing device 210 provides at PSD 212 for handling input from a user.
- UI module 220 of computing device 210 may query keyboard module 222 for a keyboard layout (e.g., an English language QWERTY keyboard, etc.).
- UI module 220 may transmit a request for a keyboard layout over communication channels 250 to keyboard module 222 .
- Keyboard module 222 may receive the request and reply to UI module 220 with data associated with the keyboard layout.
- UI module 220 may receive the keyboard layout data over communication channels 250 and use the data to generate a user interface.
- UI module 220 may transmit a display command and data over communication channels 250 to cause PSD 212 to present the user interface at PSD 212 .
- UI module 220 may receive an indication of one or more user inputs detected at PSD 212 and may output information about the user inputs to keyboard module 222 .
- PSD 212 may detect a user input and send data about the user input to UI module 220 over communications channels 250 .
- UI module 220 may generate one or more touch events based on the detected input.
- a touch event may include information that characterizes user input, such as a location component (e.g., [x,y] coordinates) of the user input, a time component (e.g., when the user input was received), a force component (e.g., an amount of pressure applied by the user input), or other data (e.g., speed, acceleration, direction, density, etc.) about the user input.
- UI module 220 may determine that the detected user input is associated the graphical keyboard. UI module 220 may send an indication of the one or more touch events to keyboard module 222 for further interpretation. Keyboard module 222 may determine, based on the touch events received from UI module 220 , that the detected user input represents an initial selection of one or more keys of the graphical keyboard.
- Application modules 224 represent all the various individual applications and services executing at and accessible from computing device 210 that may rely on a graphical keyboard having integrated search features.
- a user of computing device 210 may interact with a graphical user interface associated with one or more application modules 224 to cause computing device 210 to perform a function.
- Numerous examples of application modules 224 may exist and include, a fitness application, a calendar application, a personal assistant or prediction engine, a search application, a map or navigation application, a transportation service application (e.g., a bus or train tracking application), a social media application, a game application, an e-mail application, a chat or messaging application, an Internet browser application, or any and all other applications that may execute at computing device 210 .
- Keyboard module 222 may include all functionality of keyboard module 122 of computing device 110 of FIG. 1 and may perform similar operations as keyboard module 122 for providing a graphical keyboard having integrated search features. Keyboard module 222 may include various submodules, such as spatial model (SM) module 226 , language model (LM) module 228 , and search module 230 , which may perform the functionality of keyboard module 222 .
- SM spatial model
- LM language model
- search module 230 search module 230
- SM module 226 may receive one or more touch events as input, and output a character or sequence of characters that likely represents the one or more touch events, along with a degree of certainty or spatial model score indicative of how likely or with what accuracy the one or more characters define the touch events. In other words, SM module 226 may infer touch events as a selection of one or more keys of a keyboard and may output, based on the selection of the one or more keys, a character or sequence of characters.
- LM module 228 may receive a character or sequence of characters as input, and output one or more candidate characters, words, or phrases that LM module 228 identifies from a lexicon as being potential replacements for a sequence of characters that LM module 228 receives as input for a given language context (e.g., a sentence in a written language).
- Keyboard module 222 may cause UI module 220 to present one or more of the candidate words or phrases at suggestion regions 118 C of user interface 114 .
- the lexicon of computing device 210 may include a list of words within a written language vocabulary (e.g., a dictionary).
- the lexicon may include a database of words (e.g., words in a standard dictionary and/or words added to a dictionary by a user or computing device 210 ).
- LM module 228 may perform a lookup in the lexicon of a character string to identify one or more letters, words, and/or phrases that include parts or all of the characters of the character string.
- LM module 228 may assign a language model probability or a similarity coefficient (e.g., a Jaccard similarity coefficient) to one or more candidate words located at a lexicon of computing device 210 that include at least some of the same characters as the inputted character or sequence of characters.
- the language model probability assigned to each of the one or more candidate words indicates a degree of certainty or a degree of likelihood that the candidate word is typically found positioned subsequent to, prior to, and/or within, a sequence of words (e.g., a sentence) generated from text input detected by presence-sensitive input component 204 prior to and/or subsequent to receiving the current sequence of characters being analyzed by LM module 228 .
- LM module 228 may output the one or more candidate words from lexicon data stores 260 A that have the highest similarity coefficients.
- Search module 230 of keyboard module 222 may perform integrated search functions on behalf of keyboard module 222 . That is, when invoked (e.g., in response to a user of computing device 210 selecting selectable element 218 C of user interface 114 ), keyboard module 222 may operate in search mode where keyboard module 222 enables computing device 210 to perform search functions from within graphical keyboard 118 A.
- search module 230 may determine, based on user input, a search query.
- the user input may include selection of one or more graphical keys 118 A of graphical keyboard 116 A ( FIG. 1 ).
- search module 230 may receive, from UI module 220 , indications of user input at locations of PSD 212 corresponding to graphical keyboard 116 B indicating a search query.
- search module 230 may receive, from UI module 220 , an indication of a user input selecting search element 118 C ( FIG. 1 ).
- keyboard module 222 may invoke the search mode, and may configure suggestion region 118 B ( FIG. 1 ) to present search queries input using one or more graphical keys 118 A.
- keyboard module 222 may configure suggestion region 118 B to display search queries with a different format than predicted characters, words or phrases so that a user may visually distinguish search queries from predicted characters, words or phrases.
- keyboard module 222 may configure suggestion region 118 B to display search queries with a predetermined text color, font, font size, etc., or adjacent to or within a distinguishing user interface element, such as a text box, to visually differentiate a search query from predicted characters, words or phrases.
- search module 230 may receive, from UI module 220 , an indication of a user input selecting or attempting to select one or more graphical keys 118 A (e.g., a touch event).
- SM module 226 may determine a character or sequence of characters that likely represents the one or more touch events.
- LM module 228 may receive the character or sequence of characters that likely represents the one or more touch events from SM module 226 , and may determine the most likely one or more letters, morphemes, words, and/or phrases that a user is trying to input.
- LM module 228 of keyboard module 222 may cause UI module 220 to output the candidate letters, morphemes, words, and/or phrases within suggestion region 118 B as the search query.
- keyboard module 222 may cause UI module 220 to output multiple candidate letters, morphemes, words, and/or phrases, e.g., by expanding suggestion region 118 B to present multiple candidate letters, morphemes, words, and/or phrases as individual lines of text.
- Search module 230 may receive, from UI module 220 , an indication of a user input indicating completion of the search query, which may correspond to a user selecting a user interface element displayed at PSD 212 , such as one of graphical keys 118 A (e.g., the RETURN/SEARCH key illustrated in FIG. 1 ); selecting one of the candidate letters, morphemes, words, and/or phrases displayed at suggestion region 118 B; etc.
- graphical keys 118 A e.g., the RETURN/SEARCH key illustrated in FIG. 1
- selecting one of the candidate letters, morphemes, words, and/or phrases displayed at suggestion region 118 B etc.
- search module 230 may determine the search query, e.g., as the letters, morphemes, words, and/or phrases displayed at suggestion region 118 B at the time of the user input indicating completion of the search query or the selected one or the multiple candidate letters, morphemes, words, and/or phrases displayed in an expanded suggestion region 118 B.
- search module 230 may determine a predicted search query based on content of output region 116 A, text edit region 116 C, or both. For example, search module 230 may determine a predicted query based on content of a communication input using graphical keyboard 116 B by the user of computing device 210 . Keyboard module 222 then outputs an indication of the predicted search query to UI module 220 , which causes the predicted search query to be output at suggestion region 118 B.
- search module 230 may analyze only content of communications from the user of computing device 210 to determine a predicted search query. In other words, in some examples, search module 230 may refrain from analyzing content of communications from a user of another computing device to determine a predicted search query. In other examples, search module 230 may analyze content of communications from a user of another computing device (and received by computing device 210 ) to determine a predicted search query.
- Search module 230 may determine a predicted search query based on content of the communication from the user of computing device 210 , content of the communication from the user of the other computing device, or both, only if the computing device receives permission from the user of the respective computing device to analyze the information. For example, before computing device 210 can collect or may make use of information associated with a user of computing device 210 or a user of another computing device, the user may be provided with an opportunity to provide input to control whether programs or features of computing device 210 can collect and make use of user information (e.g., information about a user's current location, current speed, text conversations, etc.), or to dictate whether and/or how to computing device 210 may receive content that may be relevant to the user.
- user information e.g., information about a user's current location, current speed, text conversations, etc.
- certain data may be treated in one or more ways before it is stored or used by computing device 210 , so that personally-identifiable information is removed.
- a user's identity may be treated so that no personally identifiable information can be determined about the user, or a user's geographic location may be generalized where location information is obtained (such as to a city, ZIP code, or state level), so that a particular location of a user cannot be determined.
- location information such as to a city, ZIP code, or state level
- search module 230 may receive an indication of the selection of suggestion region 118 B or the predicted search query displayed at suggestion region 118 B from UI module 220 , and may determine that the search query is the predicted search query displayed at suggestion region 118 B.
- Search module 230 then may retrieve one or more search results based on the search query.
- search module 230 may retrieve the one or more search results by causing communication units 242 to transmit the search query to a data service.
- the data service may include, for example, a search system, a cloud-based computing environment, etc.
- the data service may perform a search.
- the search may include, for example, a search of webpages, a relational or hierarchical knowledge collection search, an image search, a video search, a search localized to a location of computing device 210 , etc.
- the data service then may transmit at least one search result to computing device 210 , and communication units 242 may receive the at least one search result.
- the one or more search results may include textual information, images, videos, hyperlinks, etc.
- search module 230 may retrieve one or more search results based on the search query by performing a search based on content of the search query.
- the search may include, for example, a search of webpages, a relational or hierarchical knowledge collection search, an image search, a video search, a search localized to a location of computing device 210 , etc.
- the one or more search results may include textual information, images, videos, hyperlinks, etc.
- Search module 230 may retrieve the at least one search result from communication units 242 .
- Search module 230 may cause UI module 220 to output a visual representation of at least a portion of the one or more search results in place of at least a portion of graphical keyboard 116 B.
- search module 230 may cause UI module 220 to output the visual representation of at least a portion of the one or more search results in place of at least some keys of graphical keys 118 A.
- search module 230 may cause UI module 220 to output the visual representation as a card-based user interface element.
- the card-based user interface element may appear similar to a notecard, and may include a representation of a particular search result of the one or more search results.
- the representation of the particular search result may include, for example, at least one of text, an image, an icon, etc.
- Search module 230 may generate a respective card-based user interface element for each respective search result of the one or more search results.
- Search module 230 may cause UI module 220 to output at least one card-based user interface element for display at a time, and UI module 220 may be configured to switch between respective card-based user interface elements in response to determining a user gesture, e.g., a swipe. In this way, search module 230 and UI module 220 may enable a user to switch among search results within graphical keyboard 116 B.
- the visual representation may be divided into multiple, different portions.
- a first portion of the visual representation may include a representation of content of the particular search result, such as text, an image, an icon associated with the particular search result, etc.
- a second portion may be associated with an action performed by an application other than the graphical keyboard 116 B.
- search module 230 may be configured to determine, based on an indication of user input at PSD 212 received from UI module 220 (e.g., an indication of a touch event), selection of the first portion of the visual representation of a search result. In response to determining the selection of the first portion of the visual representation, search module 230 may be configured to automatically, without further user input, output an indication of information related to the search result to UI module 220 and cause UI module 220 to insert the information related to the search result in text edit region 116 C ( FIG. 1 ).
- the information related to the search result may include, for example, text, a hyperlink, an image, a video, an icon, etc.
- the selection of the first portion of the visual representation of the search result may include a tap gesture in which the user taps the first portion of the visual representation of the search result.
- keyboard module 122 may be configured to insert a textual representation of the search result in text edit region 116 C or output region 116 A.
- the selection of the first portion of the visual representation of the search result may include a drag gesture in which the user is dragging the visual representation of the search result to text edit region 116 C or output region 116 A.
- keyboard module 122 may be configured to insert an image of the search result or a portion of the search result in text edit region 116 C or output region 116 A.
- search module 230 is configured to allow one-gesture input of information related to the search result to the text edit region 116 C, enabling easy and efficient entry of information from the search result to the application for which graphical keyboard 116 B is being used to input text.
- the second portion of the visual representation may be associated with an action performed by an application other than the graphical keyboard 116 B.
- the second portion may include at least one user interface element (e.g., icon, image, text etc.) associated with the action, which, when selected by a user, causes UI module 220 to output the touch event to another application module of applications module 224 to perform the indicated action.
- the user interface element may include an element associated with applications including a web browser, a dedicated search application, a phone application, a maps application, etc.
- the user interface element may include a phone icon, with or without accompanying text (such as “Call”); a hyperlink; an icon representative of directions, with or without accompanying text (such as “Navigate”); a play button icon, with or without accompanying text (such as “Play”); etc.
- UI module 220 may determine selection of the second portion of the visual representation of the search result (or a particular user interface element of the second portion), and may transmit an indication of the selection of the second portion of the visual representation to the associated application, which may perform the action associated with the second portion or the user interface element of the second portion.
- search module 230 may present a user with shortcuts to actions performed by other applications.
- keyboard module 222 computing device 210 may provide a graphical keyboard with integrated search features that include display of a visual representation of at least a portion of one or more search results in place of at least a portion of the graphical keyboard.
- the visual representation may include a card-based user interface element, and computing device 210 may be configured to output different card-based user interface elements, corresponding to different search results determined based on the search query, in response to receiving an indication of a user input, such as a swipe gesture.
- computing device 210 may be configured to determine, based on user input, selection of a predetermined portion of the visual representation of the at least a portion (e.g., a search result) of the one or more search results and, automatically, without further user input, insert, in text edit region 116 C displayed adjacent to graphical keyboard 116 B, information related of the at least a portion (e.g., a search result) of the one or more search results.
- a predetermined portion of the visual representation of the at least a portion e.g., a search result
- computing device 210 may be configured to determine, based on user input, selection of a predetermined portion of the visual representation of the at least a portion (e.g., a search result) of the one or more search results and, automatically, without further user input, insert, in text edit region 116 C displayed adjacent to graphical keyboard 116 B, information related of the at least a portion (e.g., a search result) of the one or more search results.
- FIG. 3 is a block diagram illustrating an example computing device that outputs graphical content for display at a remote device, in accordance with one or more techniques of the present disclosure.
- Graphical content generally, may include any visual information that may be output for display, such as text, images, a group of moving images, to name only a few examples.
- the example shown in FIG. 3 includes a computing device 310 , a PSD 312 , communication unit 342 , projector 380 , projector screen 382 , mobile device 386 , and visual display component 390 .
- PSD 312 may be a presence-sensitive display as described in FIGS. 1-2 . Although shown for purposes of example in FIGS.
- a computing device such as computing device 310 may, generally, be any component or system that includes a processor or other suitable computing environment for executing software instructions and, for example, need not include a presence-sensitive display.
- computing device 310 may be a processor that includes functionality as described with respect to processors 240 in FIG. 2 .
- computing device 310 may be operatively coupled to PSD 312 by a communication channel 362 A, which may be a system bus or other suitable connection.
- Computing device 310 may also be operatively coupled to communication unit 342 , further described below, by a communication channel 362 B, which may also be a system bus or other suitable connection.
- communication channel 362 B may also be a system bus or other suitable connection.
- computing device 310 may be operatively coupled to PSD 312 and communication unit 342 by any number of one or more communication channels.
- a computing device may refer to a portable or mobile device such as mobile phones (including smart phones), laptop computers, etc.
- a computing device may be a desktop computer, tablet computer, a smart television platform, a camera, a personal digital assistant (PDA), an automobile navigation or entertainment device, a kiosk, a server, a mainframe, etc.
- PDA personal digital assistant
- PSD 312 may include display component 302 and presence-sensitive input component 304 .
- Display component 302 may, for example, receive data from computing device 310 and display the graphical content.
- presence-sensitive input component 304 may determine one or more user inputs (e.g., continuous gestures, multi-touch gestures, single-touch gestures) at PSD 312 using capacitive, inductive, and/or optical recognition techniques and send indications of such user input to computing device 310 using communication channel 362 A.
- user inputs e.g., continuous gestures, multi-touch gestures, single-touch gestures
- presence-sensitive input component 304 may be physically positioned on top of display component 302 such that, when a user positions an input unit over a graphical element displayed by display component 302 , the location at which presence-sensitive input component 304 corresponds to the location of display component 302 at which the graphical element is displayed.
- computing device 310 may also include and/or be operatively coupled with communication unit 342 .
- Communication unit 342 may include functionality of communication unit 242 as described in FIG. 2 .
- Examples of communication unit 342 may include a network interface card, an Ethernet card, an optical transceiver, a radio frequency transceiver, or any other type of device that can send and receive information.
- Other examples of such communication units may include Bluetooth, 3G, and WiFi radios, Universal Serial Bus (USB) interfaces, etc.
- Computing device 310 may also include and/or be operatively coupled with one or more other devices (e.g., input devices, output components, memory, storage devices) that are not shown in FIG. 3 for purposes of brevity and illustration.
- FIG. 3 also illustrates a projector 380 and projector screen 382 .
- projection devices may include electronic whiteboards, holographic display components, and any other suitable devices for displaying graphical content.
- Projector 380 and projector screen 382 may include one or more communication units that enable the respective devices to communicate with computing device 310 . In some examples, the one or more communication units may enable communication between projector 380 and projector screen 382 .
- Projector 380 may receive data from computing device 310 that includes graphical content. Projector 380 , in response to receiving the data, may project the graphical content onto projector screen 382 .
- projector 380 may determine one or more user inputs (e.g., continuous gestures, multi-touch gestures, single-touch gestures) at projector screen using optical recognition or other suitable techniques and send indications of such user input using one or more communication units to computing device 310 .
- projector screen 382 may be unnecessary, and projector 380 may project graphical content on any suitable medium and detect one or more user inputs using optical recognition or other such suitable techniques.
- Projector screen 382 may include a presence-sensitive display 384 .
- Presence-sensitive display 384 may include a subset of functionality or all of the functionality of presence-sensitive display 112 and/or 312 as described in this disclosure.
- presence-sensitive display 384 may include additional functionality.
- Projector screen 382 (e.g., an electronic whiteboard), may receive data from computing device 310 and display the graphical content.
- presence-sensitive display 384 may determine one or more user inputs (e.g., continuous gestures, multi-touch gestures, single-touch gestures) at projector screen 382 using capacitive, inductive, and/or optical recognition techniques and send indications of such user input using one or more communication units to computing device 310 .
- FIG. 3 also illustrates mobile device 386 and visual display component 390 .
- Mobile device 386 and visual display component 390 may each include computing and connectivity capabilities. Examples of mobile device 386 may include e-reader devices, convertible notebook devices, hybrid slate devices, etc. Examples of visual display component 390 may include other devices such as televisions, computer monitors, etc.
- visual display component 390 may be a vehicle cockpit display or navigation display (e.g., in an automobile, aircraft, or some other vehicle). In some examples, visual display component 390 may be a home automation display or some other type of display that is separate from computing device 310 .
- mobile device 386 may include a presence-sensitive display 388 .
- Visual display component 390 may include a presence-sensitive display 392 .
- Presence-sensitive displays 388 , 392 may include a subset of functionality or all of the functionality of presence-sensitive display 112 , 212 , and/or 312 as described in this disclosure.
- presence-sensitive displays 388 , 392 may include additional functionality.
- presence-sensitive display 392 may receive data from computing device 310 and display the graphical content.
- presence-sensitive display 392 may determine one or more user inputs (e.g., continuous gestures, multi-touch gestures, single-touch gestures) at projector screen using capacitive, inductive, and/or optical recognition techniques and send indications of such user input using one or more communication units to computing device 310 .
- user inputs e.g., continuous gestures, multi-touch gestures, single-touch gestures
- computing device 310 may output graphical content for display at PSD 312 that is coupled to computing device 310 by a system bus or other suitable communication channel.
- Computing device 310 may also output graphical content for display at one or more remote devices, such as projector 380 , projector screen 382 , mobile device 386 , and visual display component 390 .
- computing device 310 may execute one or more instructions to generate and/or modify graphical content in accordance with techniques of the present disclosure.
- Computing device 310 may output the data that includes the graphical content to a communication unit of computing device 310 , such as communication unit 342 .
- Communication unit 342 may send the data to one or more of the remote devices, such as projector 380 , projector screen 382 , mobile device 386 , and/or visual display component 390 .
- computing device 310 may output the graphical content for display at one or more of the remote devices.
- one or more of the remote devices may output the graphical content at a presence-sensitive display that is included in and/or operatively coupled to the respective remote devices.
- computing device 310 may not output graphical content at PSD 312 that is operatively coupled to computing device 310 .
- computing device 310 may output graphical content for display at both a PSD 312 that is coupled to computing device 310 by communication channel 362 A, and at one or more remote devices.
- the graphical content may be displayed substantially contemporaneously at each respective device. For instance, some delay may be introduced by the communication latency to send the data that includes the graphical content to the remote device.
- graphical content generated by computing device 310 and output for display at PSD 312 may be different than graphical content display output for display at one or more remote devices.
- Computing device 310 may send and receive data using any suitable communication techniques.
- computing device 310 may be operatively coupled to external network 374 using network link 373 A.
- Each of the remote devices illustrated in FIG. 3 may be operatively coupled to network external network 374 by one of respective network links 373 B, 373 C, or 373 D.
- External network 374 may include network hubs, network switches, network routers, etc., that are operatively inter-coupled thereby providing for the exchange of information between computing device 310 and the remote devices illustrated in FIG. 3 .
- network links 373 A- 373 D may be Ethernet, ATM or other network connections. Such connections may be wireless and/or wired connections.
- computing device 310 may be operatively coupled to one or more of the remote devices included in FIG. 3 using direct device communication 378 .
- Direct device communication 378 may include communications through which computing device 310 sends and receives data directly with a remote device, using wired or wireless communication. That is, in some examples of direct device communication 378 , data sent by computing device 310 may not be forwarded by one or more additional devices before being received at the remote device, and vice-versa. Examples of direct device communication 378 may include Bluetooth, Near-Field Communication, Universal Serial Bus, WiFi, infrared, etc.
- One or more of the remote devices illustrated in FIG. 3 may be operatively coupled with computing device 310 by communication links 376 A- 376 D.
- communication links 376 A- 376 D may be connections using Bluetooth, Near-Field Communication, Universal Serial Bus, infrared, etc. Such connections may be wireless and/or wired connections.
- computing device 310 may be operatively coupled to visual display component 390 using external network 374 .
- Computing device 310 may output a graphical keyboard for display at PSD 312 .
- computing device 310 may send data that includes a representation of the graphical keyboard to communication unit 342 .
- Communication unit 342 may send the data that includes the representation of the graphical keyboard to visual display component 390 using external network 374 .
- Visual display component 390 in response to receiving the data using external network 374 , may cause PSD 312 to output the graphical keyboard.
- visual display device 130 may send an indication of the user input to computing device 310 using external network 374 .
- Communication unit 342 of may receive the indication of the user input, and send the indication to computing device 310 .
- Computing device 310 may select, based on the user input, one or more keys or user interface elements (e.g., a suggested or predicted search query). Computing device 310 may determine, based on the selection of one or more keys or user interface elements, a search query. In some examples, computing device 310 may retrieve one or more search result based on the search query. Computing device 310 may output a representation of an updated graphical user interface including an updated graphical keyboard in which at least a portion of the one or more search results (e.g., a search result) is displayed instead of at least a portion of the graphical keyboard (e.g., at least one graphical key of the graphical keyboard).
- search results e.g., a search result
- Communication unit 342 may receive the representation of the updated graphical user interface and may send the send the representation to visual display component 390 , such that visual display component 390 may cause PSD 312 to output the updated graphical keyboard, including at least a portion of the one or more search results (e.g., a search result) displayed instead of at least a portion of the graphical keyboard (e.g., at least one graphical key of the graphical keyboard).
- search results e.g., a search result
- computing device 310 may be configured to determine, based on an indication of user input received at PSD 312 , selection of a predetermined portion of the visual representation of a search result. In response to determining the selection, computing device 310 may be configured to automatically, without further user input, output a representation of an updated graphical user interface including an updated graphical keyboard in which information related to the search result is displayed in a text edit region adjacent to the graphical keyboard. In this way, computing device 310 is configured to allow one-gesture input of information from the search result to the text edit region, enabling easy and efficient entry of information from the search result to the application for which the graphical keyboard is being used to input text.
- FIGS. 4A and 4B are conceptual diagrams illustrating example graphical user interfaces of an example computing device that is configured to present a graphical keyboard with integrated search features, in accordance with one or more aspects of the present disclosure.
- FIGS. 4A and 4B illustrate, respectively, example graphical user interfaces 414 A and 414 B (collectively, user interfaces 414 ). However, many other examples of graphical user interfaces may be displayed in other instances.
- Each of graphical user interfaces 414 may correspond to a graphical user interface displayed by computing devices 110 , 210 , or 310 of FIGS. 1-3 , respectively.
- FIGS. 4A and 4B are described below in the context of computing device 110 for ease of description only, and may be displayed by any computing device.
- Graphical user interface 414 A of FIG. 4A includes a graphical keyboard 416 B includes an output region 416 A, a graphical keyboard 416 B, and a text edit region 416 C.
- Output region 416 A is a graphical user interface of an application.
- output region 416 A is a graphical user interface of a messaging or chat application, and illustrates communications from the user of the computing device displaying graphical user interface 414 A (under “YOU”) and communications from a user of another computing device (under “FRIEND”).
- output region may be another application user interface, such as a notes application, a web browser, a search application, a maps application, an email application, a text editor application, or any other application for which graphical keyboard 116 B provides text input.
- Text edit region 416 C is the region of graphical user interface 414 A at which text is displayed after being input using graphical keyboard 416 B prior to being committed to output region 416 A.
- Graphical keyboard 416 B may include multiple, different views. One view may be similar to or substantially the same as graphical keyboard 116 B illustrated in FIG. 1 . Other views may include a plurality of symbol keys, a plurality of extended symbol keys, a plurality of keys corresponding to letters or phonemes of a particular language, etc.
- FIG. 4A illustrates a view of graphical keyboard 416 B in which at least a portion of the plurality of keys (in this example, all of the keys) has been replaced by at least a portion of one or more search results returned in response to a search query.
- Graphical keyboard 416 B includes suggestion region 418 B, search element 418 C, and search results region 418 D.
- Suggestion region 418 B may be similar to or substantially the same as suggestion region 118 B illustrated in FIG. 1 .
- suggestion region 418 B may alternately be configured to display predicted letters, words, or phrases; predicted search queries; input search queries; etc.
- suggestion region 418 B is configured to display an input search query, “How old is President Obama?”
- Search element 418 C may be similar to or substantially the same as search element 118 C of FIG. 1 .
- Search element 118 C represents a selectable element of graphical keyboard 116 B for invoking one or more of the various search features of graphical keyboard 116 B, such as invoking the search mode.
- keyboard module 122 may configure suggestion region 418 B to present search queries input using graphical keys 118 A ( FIG. 1 ) instead of predicted characters, words or phrases or other primarily linguistic information that keyboard module 122 derives from a language model, lexicon, or dictionary.
- FIG. 4A An example of this is illustrated in FIG. 4A , in which the search query “How old is President Obama?” is displayed at suggestion region 418 B.
- Search results region 418 D displays a visual representation of one or more search result retrieved by computing device 110 in response to the search query.
- the visual representation of the at least one search result may be a card-based user interface element 420 A.
- Card-based user interface element 420 A may appear similar to a notecard, and may include a visual representation of a particular search result of the one or more search results.
- the representation of the particular search result may include, for example, at least one of text, a picture, an icon, or the like.
- card-based user interface element 420 A includes text indicating an answer to the search query (“54 years” and “Barack Obama/Age”) and a picture related to the search query (such as a picture of President Obama).
- card-based user interface element 420 A includes a plurality of predetermined portions. Each predetermined portion of the plurality of predetermined portions may be associated with a different functionality provided by card-based user interface element 420 A.
- card-based user interface element 420 A includes a first predetermined portion 422 A and a second predetermined portion 422 B.
- the first predetermined portion 422 A of card-based user interface element 420 A may include a representation of content of the particular search result, such as text, an image associated with the particular search result, an icon associated with the particular search result, etc.
- the first predetermined portion 422 A includes text indicating an answer to the search query (“54 years” and “Barack Obama/Age”) and an image related to the search query (such as a picture of President Obama).
- keyboard module 122 may be configured to determine, based on an indication of user input at PSD 112 received from UI module 120 (e.g., an indication of a touch event), selection of first predetermined portion 422 A. In response to determining the selection of first predetermined portion 422 A, keyboard module 122 may be configured to automatically, without further user input, output an indication of information related to of the search result to UI module 120 and cause UI module 120 to insert the information related to the search result in text edit region 416 C.
- the information related to the search result may include, for example, text, a hyperlink, an image, a video, an icon, etc. In this way, keyboard module 122 is configured to allow one-gesture input of information from the search result to the text edit region 416 C, enabling easy and efficient entry of information from the search result to the application for which graphical keyboard 416 B is being used to input text.
- the information related to the search result may include, for example, textual information related to the selected search result.
- the information related to the selected search result includes a text answer to the search query (“President Obama is 54 years old.”).
- the text answer may be formatted as a natural language response to the search query.
- the information related to the selected search result also includes a hyperlink to a website related to the selected search result, which may allow the user of computing device 110 or the user of the other computing device (e.g., “FRIEND” in FIG. 4A ) to select the hyperlink to access more information related to the search result once the content in text edit region 416 C is committed to output region 416 A.
- selection of first portion 422 A of card-based user interface element 420 A may include a tap gesture in which a user taps first portion 422 A of card-based user interface element 420 A, and in response, keyboard module 122 may be configured to insert a textual representation of the search result in text edit region 416 C or output region 416 A.
- the information related to the selected search result also may include an image (such as an image of President Obama, an image of card-based user interface element 420 A, or an image of a portion of card-based user interface element 420 A), an icon, etc.
- the selection of the first portion 422 A of card-based user interface element 420 A may include a drag gesture in which the user is dragging card-based user interface element 420 A to text edit region 416 C or output region 416 A.
- keyboard module 122 may be configured to insert an image of card-based user interface element 420 A or a portion of card-based user interface element 420 A in text edit region 416 C or output region 416 A.
- Card-based user interface element 420 A also includes second predetermined portion 422 B, which is associated with an action performed by an application other than graphical keyboard 416 B.
- second predetermined portion 422 B includes at least one user interface element (e.g., icon, image, text etc.) associated with the action, which, when selected by a user, causes UI module 120 to output the touch event to another application module to perform the indicated action.
- the user interface element may include an element associated with applications including a web browser, a dedicated search application, a phone application, a maps application, etc.
- the user interface element may include a phone icon, with or without accompanying text (such as “Call”); a hyperlink; an icon representative of directions, with or without accompanying text (such as “Navigate”); a play button icon, with or without accompanying text (such as “Play”); etc.
- the user interface element includes go-to icon 424 A.
- Go-to icon 424 A is associated with an action of opening a new application related to the search result displayed in card-based user interface element 420 A, such as a search application or a website from which the information in the search result was retrieved.
- UI module 120 may determine selection of the second predetermined portion 422 B or go-to icon 424 A, and may transmit an indication of the selection of second predetermined portion 422 B to the associated application, which may perform the action associated with the second predetermined portion 422 B or go-to icon 424 A.
- keyboard module 122 may present a user with shortcuts to actions performed by other applications.
- computing device 110 may retrieve more than one search result in response to a search query.
- Keyboard module 122 may generate a respective card-based user interface element for each respective search result of the one or more search results.
- keyboard module 122 may cause search results region 418 D to display more than one search result (e.g., more than one card-based user interface element 420 A.
- search results region 418 D may display first card-based user interface element 420 A and a portion of a second card-based user interface element 420 B.
- the display of the portion of second card-based user interface element 420 B may visually indicate to a user of computing device 110 that additional search results exist and may be viewed by navigating to the second card-based user interface element 420 B.
- keyboard module 122 may cause UI module 120 to switch from card-based user interface element 420 A to second card-based user interface element 420 B in response to determining a user gesture based on touch events, e.g., a swipe from right to left in FIG. 4A .
- keyboard module 122 may enable a user to switch among search results within search results region 418 D of graphical keyboard 116 B.
- keyboard module 122 may enable a user to switch among card-based user interface elements using swipes to different directions (e.g., right and left), and the card-based user interface elements may be conceptually arranged in a carousel.
- FIG. 4B illustrates an example graphical user interface 414 B that includes a view of graphical keyboard 416 B in which at least a portion of the plurality of keys (in this example, all of the keys) has been replaced by at least a portion of one or more search results returned in response to a search query. Similar to graphical user interface 414 A, graphical user interface 414 B includes output region 416 A, text edit region 416 C, and graphical keyboard 416 B.
- graphical keyboard 416 B includes suggestion region 418 B, search element 418 C, and search results region 418 D.
- suggestion region 418 B is configured to display an input search query, “Sandwich place”.
- Search results region 418 D displays a visual representation of one or more search result retrieved by computing device 110 in response to the search query.
- the visual representation of the at least one search result may be a card-based user interface element 420 C.
- Card-based user interface element 420 C may appear similar to a notecard, and may include a visual representation of a particular search result of the one or more search results.
- the representation of the particular search result may include, for example, at least one of text, a picture, an icon, or the like.
- card-based user interface element 420 C includes text indicating an answer to the search query. In FIG.
- the text indicating an answer to the search query includes a restaurant name (“Sandwich Place”), restaurant rating (5 stars), a number of reviews (“1,821”), an address (“123 Address Way”), a restaurant type (“Sandwiches”), and an image (such as a picture of Sandwich Place).
- card-based user interface element 420 C includes a first predetermined portion 422 C and a second predetermined portion 422 D.
- the first predetermined portion 422 C of card-based user interface element 420 C (the visual representation of a first search result) may include a representation of content of the particular search result, such as text, an image associated with the particular search result, an icon associated with the particular search result, etc.
- the text indicating an answer to the search query includes a restaurant name (“Sandwich Place”), restaurant rating (5 stars), a number of reviews (“1,821”), an address (“123 Address Way”), a restaurant type (“Sandwiches”), and an image (such as a picture of Sandwich Place).
- keyboard module 122 may be configured to determine, based on an indication of user input at PSD 112 received from UI module 120 (e.g., an indication of a touch event), selection of first predetermined portion 422 C. In response to determining the selection of first predetermined portion 422 C, keyboard module 122 may be configured to automatically, without further user input, output an indication of information related to of the search result to UI module 120 and cause UI module 120 to insert the information related to the search result in text edit region 416 C. In this way, keyboard module 122 is configured to allow one-gesture input of information from the search result to the text edit region 416 C, enabling easy and efficient entry of information from the search result to the application for which graphical keyboard 416 B is being used to input text.
- the information related to the search result may include, for example, image or graphical information related to the selected search result.
- the information related to the selected search result includes a visual representation 426 of card-based user interface element 420 C.
- the selection of the first portion 422 C of card-based user interface element 420 C may include a drag gesture 428 in which the user is dragging card-based user interface element 420 C to text edit region 416 C or output region 416 A.
- keyboard module 122 may be configured to insert visual representation 426 of card-based user interface element 420 C or a visual representation of a portion of card-based user interface element 420 C in text edit region 416 C or output region 416 A.
- the information related to the search result may include, for example, textual information related to the selected search result.
- the information related to the selected search result may include a text answer to the search query (“Sandwich Place 123 Address Way”).
- the information related to the selected search result also could include a hyperlink to a website that provides directions to Sandwich Place, which may allow the user of computing device 110 or the user of the other computing device (e.g., “FRIEND” in FIG. 4B ) to select the hyperlink to access directions to Sandwich Place once the content in text edit region 416 C is committed to output region 416 A.
- selection of first portion 422 C of card-based user interface element 420 C may include a tap gesture in which a user taps first portion 422 C of card-based user interface element 420 C, and in response, keyboard module 122 may be configured to insert a textual representation of the search result in text edit region 416 C or output region 416 A.
- Card-based user interface element 420 C also includes second predetermined portion 422 D, which is associated with one or more actions performed by an application other than graphical keyboard 416 B.
- second predetermined portion 422 D includes multiple user interface elements (e.g., icons, images, text etc.) associated with respective actions, which, when selected by a user, causes UI module 120 to output the touch event to another application module to perform the indicated respective action.
- the user interface elements include phone icon 424 B, navigation icon 424 C, and go-to icon 424 D.
- Phone icon 424 B is associated with an action of opening a phone application and calling Sandwich Place (i.e., the establishment associated with the search result).
- Navigation icon 424 C is associated with an action of opening a navigation or maps application and retrieving directions to Sandwich Place (i.e., the establishment associated with the search result).
- Go-to icon 424 D is associated with an action of opening a new application related to the search result displayed in card-based user interface element 420 C, such as a search application or a website from which the information in the search result was retrieved.
- UI module 120 or keyboard module 122 may determine selection of one of icons 424 B- 424 D in the second predetermined portion 422 D, and may transmit an indication of the selection of the one of icons 424 B- 424 D to the associated application, which may perform the action associated with the selected one of icons 424 B- 424 D.
- keyboard module 122 may present a user with shortcuts to actions performed by other applications.
- computing device 110 may retrieve more than one search result in response to a search query.
- Keyboard module 122 may generate a respective card-based user interface element for each respective search result of the one or more search results.
- keyboard module 122 may cause search results region 418 D to display more than one search result (e.g., more than one card-based user interface element 420 C). For example, as shown in FIG. 4B , keyboard module 122 may cause search results region 418 D to display first card-based user interface element 420 C and a portion of a second card-based user interface element 420 D.
- the display of the portion of second card-based user interface element 420 D may visually indicate to a user of computing device 110 that additional search results exist and may be viewed by navigating to the second card-based user interface element 420 D.
- keyboard module 122 may cause UI module 120 to switch from card-based user interface element 420 C to second card-based user interface element 420 D in response to determining a user gesture based on touch events, e.g., a swipe from right to left in FIG. 4B .
- keyboard module 122 may enable a user to switch among search results within search results region 418 D of graphical keyboard 116 B.
- keyboard module 122 may enable a user to switch among card-based user interface elements using swipes to different directions (e.g., right and left), and the card-based user interface elements may be conceptually arranged in a carousel.
- FIG. 5 is a flowchart illustrating example operations of a computing device that is configured to present a graphical keyboard with integrated search features, in accordance with one or more aspects of the present disclosure.
- the operations of FIG. 5 may be performed by one or more processors of a computing device, such as computing devices 110 of FIG. 1 or computing device 210 of FIG. 2 .
- FIG. 5 is described below within the context of computing devices 110 of FIG. 1 .
- the technique of FIG. 5 includes outputting, by computing device 110 , for display (e.g., at PSD 112 ), a graphical keyboard 116 B comprising a plurality of keys 118 A and a suggestion region 118 B ( 502 ).
- keyboard module 122 may output an indication of the graphical keyboard 116 B to UI module 120 , which may cause UI module 120 to control PSD 112 to output a graphical user interface including graphical keyboard 116 B.
- Graphical keyboard 116 B may include a search element 118 C in addition to plurality of keys 118 A and suggestion region 118 B.
- the technique of FIG. 5 also includes determining, by computing device 110 , based on a selection of suggestion region 118 B or one or more keys from the plurality of keys 118 A, a search query ( 504 ).
- keyboard module 122 may cause suggestion region 118 B to display a predicted or suggested search query.
- keyboard module 122 may analyze a previous entry (e.g., message or sentence) made by a user using graphical keyboard 116 B and determine a predicted or suggested search query based on the previous entry. Keyboard module 122 then may output an indication of the predicted or suggested search query to UI module 120 , which may cause UI module 120 to control PSD 112 to output a graphical user interface including the predicted or suggested search query in suggestion region 118 B.
- keyboard module 122 may receive, from UI module 120 , an indication of a touch event at PSD 112 selecting search element 116 C. In response, keyboard module 122 may output an message to UI module 120 that causes UI module 120 to control PSD 112 to output a graphical user interface in which suggestion region 118 B displays text entered by the user (made by selecting keys of plurality of graphical keys 118 A) as a search query.
- Keyboard module 122 may receive from UI module 120 an indication of a touch event indicating selection of a search query (e.g., the predicted or suggested search query in suggestion region 118 B or the search query entered by user in suggestion region 118 B), and may determine the search query based on this user input (touch event).
- a touch event indicating selection of a search query (e.g., the predicted or suggested search query in suggestion region 118 B or the search query entered by user in suggestion region 118 B), and may determine the search query based on this user input (touch event).
- the technique of FIG. 5 may further include retrieving, by computing device 110 , one or more search results determined based on the search query ( 506 ).
- retrieving, by computing device 110 , one or more search results determined based on the search query ( 506 ) may include performing, by computing device 110 , a search based on the search query.
- retrieving, by computing device 110 , one or more search results determined based on the search query ( 506 ) may include transmitting, by computing device 110 , the search query to a data service, and receiving, by computing device 110 , from the data service, one or more search results based on the search query.
- the technique of FIG. 5 includes outputting, by computing device 110 , in place of at least a portion of graphical keyboard 116 B, a visual representation of a search result of the one or more search results ( 508 ).
- keyboard module 122 may output an indication of a visual representation of the one or more search results that include one or more card-based user interface element, each card-based user interface element being associated with a respective search result of the one or more search results.
- Keyboard module 122 may output an indication of the visual representation to UI module 120 , which causes UI module 120 to cause PSD to display a user interface in which the visual representation is displayed in place at least a portion of graphical keyboard 116 B, such as in place of at least one of graphical keys 118 A (e.g., in place of all of graphical keys 118 A).
- the technique of FIG. 5 optionally includes determining, by computing device 110 , based on user input (e.g., at PSD 112 ), selection of a predetermined portion of the visual representation of the search result ( 510 ).
- keyboard module 122 may receive, from UI module 120 , an indication of user input (e.g., a touch event) selecting a first portion of the visual representation of the search result.
- the first portion of the visual representation of the search result may include content of the search result, such as, for example, text, an image, an icon, a video, or the like.
- the 5 also optionally may include automatically, without further user input, inserting, by computing device 110 , in a text edit region 116 C displayed adjacent to graphical keyboard 116 B, information related to the search result ( 512 ).
- the information related to the search result may include, for example, text, an image, an icon, a video, or the like.
- the information related to the search result may be substantially similar to the information represented in the first portion of the visual representation of the search result, although formatting of the information may be different when inserted in the text edit region 116 C than when displayed as part of the first predetermined portion of the visual representation of the search result.
- the techniques of the disclosure may enable a computing device 110 to provide a graphical keyboard 116 B with integrated search features that include display of a visual representation of at least a portion of one or more search results in place of at least a portion of the graphical keyboard 116 B.
- the visual representation may include a card-based user interface element, and computing device 110 may be configured to output different card-based user interface elements, corresponding to different search results determined based on the search query, in response to receiving an indication of a user input, such as a swipe gesture.
- computing device 110 may be configured to determine, based on user input, selection of a predetermined portion of the visual representation of the at least a portion (e.g., a search result) of the one or more search results and, automatically, without further user input, insert, in text edit region 116 C displayed adjacent to graphical keyboard 116 B, information related to the at least a portion (e.g., a search result) of the one or more search results.
- a predetermined portion of the visual representation of the at least a portion e.g., a search result
- computing device 110 may be configured to determine, based on user input, selection of a predetermined portion of the visual representation of the at least a portion (e.g., a search result) of the one or more search results and, automatically, without further user input, insert, in text edit region 116 C displayed adjacent to graphical keyboard 116 B, information related to the at least a portion (e.g., a search result) of the one or more search results.
- computing device 110 may facilitate insertion of information from the search result in the application for which graphical keyboard 116 B is providing input. Further, by inserting the information from the search result in text edit region 116 C with a single user input, computing device 110 may allow the user to input information with fewer inputs than a typical, multi-gesture copy and paste action. In this way, computing device 110 may facilitate information retrieval using a search query and insertion of the retrieved information in an application without application switching and with few user inputs.
- a method comprising: outputting, by a computing device, for display, a graphical keyboard comprising a plurality of keys and a suggestion region; determining, by the computing device, based on a selection of the suggestion region or one or more keys from the plurality of keys, a search query; retrieving, by the computing device, one or more search results determined based on the search query; and outputting, by the computing device, in place of at least a portion of the graphical keyboard, a visual representation of a particular search result of the one or more search results.
- retrieving one or more search results determined based on the search query comprises: transmitting, by the computing device and to a data service, the search query; and receiving, by the computing device, from the data service, one or more search results determined based on the search query.
- Clause 3 The method of clause 1, wherein retrieving one or more search results determined based on the search query comprises performing, by the computing device, a search based on the search query to determine the one or more search results.
- Clause 4 The method of clause 1, further comprising: determining, by the computing device, based on user input, selection of a predetermined portion of the visual representation of the particular search result; and automatically, without further user input, inserting, by the computing device, in a text edit region displayed adjacent to the graphical keyboard, information related to the particular search result.
- Clause 5 The method of clause 4, wherein the predetermined portion comprises a first predetermined portion and the user input comprises a first user input, further comprising: determining, by the computing device, based on a second user input, selection of a second predetermined portion of the visual representation of the particular search result; and automatically, without further user input, performing, by the computing device, a predetermined action associated with the second predetermined portion.
- Clause 6 The method of any one of clauses 1 to 5, wherein: the particular search result comprises a first particular search result; the visual representation comprises a first visual representation; and the first visual representation comprises a first card-based user interface elements; the method further comprising: receiving, by the computing device, an indication of a swipe user input gesture; and responsive to receiving the indication of the swipe user input gesture, outputting, by the computing device, for display, an animation switching from the first card-based user interface element to a second card-based user interface element that is a second visual representation of a second particular search result of the one or more search results, wherein each of the first card-based user interface element and the second card-based user interface element are displayed in place of at least a portion of the graphical keyboard.
- Clause 7 The method of any one of clauses 1 to 6, wherein outputting the visual representation of the particular search result in place of at least a portion of the graphical keyboard comprising outputting a visual representation of the particular search result that replaces at least some keys of the plurality of keys.
- Clause 8 The method of clause 4 or 5, wherein inserting, in the text edit region displayed adjacent to the graphical keyboard, the information related to the particular search result of the at least a portion of the one or more search results comprises inserting, in the text edit region displayed adjacent to the graphical keyboard, a natural language response to the search query.
- a computing device comprising: a presence-sensitive display; at least one processor; and a memory storing instructions that, when executed by the at least one processor, cause the at least one processor to: output, for display at the presence-sensitive display, a graphical keyboard comprising a plurality of keys and a suggestion region; determine, based on a selection of the suggestion region or one or more keys from the plurality of keys, a search query; retrieve one or more search results determined based on the search query; and output, in place of at least a portion of the graphical keyboard, a visual representation of a particular search result of the one or more search results.
- Clause 10 The computing device of clause 9, wherein the instructions that, when executed by the at least one processor, cause the at least one processor to retrieve the one or more search results determined based on the search query cause the at least one processor to: transmit, to a data service, the search query; and receive, from the data service, one or more search results determined based on the search query.
- Clause 11 The computing device of claim 9 , wherein the instructions that, when executed by the at least one processor, cause the at least one processor to retrieve the one or more search results determined based on the search query cause the at least one processor to perform a search based on the search query to determine the one or more search results.
- Clause 12 The computing device of clause 9, wherein the memory further comprises instructions that, when executed by the at least one processor, cause the at least one processor to: determine, based on user input, selection of a predetermined portion of the visual representation of the particular search result; and automatically, without further user input, insert, in a text edit region displayed adjacent to the graphical keyboard, information related to the particular search result.
- Clause 13 The computing device of clause 12, wherein: the predetermined portion comprises a first predetermined portion and the user input comprises a first user input; and the memory further comprises instructions that, when executed by the at least one processor, cause the at least one processor to: determine, based on a second user input, selection of a second predetermined portion of the visual representation of the particular search result; and automatically, without further user input, perform a predetermined action associated with the second predetermined portion.
- Clause 14 The computing device of any one of clauses 9 to 13, wherein: the particular search result comprises a first particular search result; the visual representation comprises a first visual representation; the first visual representation comprises a first card-based user interface elements; and the memory further comprises instructions that, when executed by the at least one processor, cause the at least one processor to: receive, from the presence-sensitive display, an indication of a swipe user input gesture; and responsive to receiving the indication of the swipe user input gesture, output, for display at the presence-sensitive display, an animation switching from the first card-based user interface element to a second card-based user interface element that is a second visual representation of a second particular search result of the one or more search results, wherein each of the first card-based user interface element and the second card-based user interface element are displayed in place of at least a portion of the graphical keyboard.
- Clause 15 The computing device of any one of clauses 9 to 14, wherein the visual representation of the particular search result replaces at least some keys of the plurality of keys.
- Clause 16 The computing device of clause 12 or 13, wherein the information related to the particular search result comprises a natural language response to the search query.
- a computer-readable storage medium encoded with instructions that, when executed by at least one processor of a computing device, cause the at least one processor to: output, for display at a presence-sensitive display, a graphical keyboard comprising a plurality of keys and a suggestion region; determine, based on a selection of the suggestion region or one or more keys from the plurality of keys, a search query; retrieve one or more search results determined based on the search query; and output, in place of at least a portion of the graphical keyboard, a visual representation of a particular search result of the one or more search results.
- Clause 18 The computer-readable storage medium of clause 17, further comprising instructions that, when executed by the at least one processor, cause the at least one processor to: determine, based on user input, selection of a predetermined portion of the visual representation of the particular search result; and automatically, without further user input, insert, in a text edit region displayed adjacent to the graphical keyboard, information related to the particular search result.
- Clause 19 The computer-readable storage medium of clause 17 or 18, wherein: the predetermined portion comprises a first predetermined portion and the user input comprises a first user input; and the memory further comprises instructions that, when executed by the at least one processor, cause the at least one processor to: determine, based on a second user input, selection of a second predetermined portion of the visual representation of the particular search result; and automatically, without further user input, perform a predetermined action associated with the second predetermined portion.
- Clause 20 The computer-readable storage medium of any one of clauses 17 to 19, wherein: the particular search result comprises a first particular search result; the visual representation comprises a first visual representation; the first visual representation comprises a first card-based user interface elements; and further comprising instructions that, when executed by the at least one processor, cause the at least one processor to: receive, from the presence-sensitive display, an indication of a swipe user input gesture; and responsive to receiving the indication of the swipe user input gesture, output, for display at the presence-sensitive display, an animation switching from the first card-based user interface element to a second card-based user interface element that is a second visual representation of a second particular search result of the one or more search results, wherein each of the first card-based user interface element and the second card-based user interface element are displayed in place of at least a portion of the graphical keyboard.
- Clause 21 The computer-readable storage medium of any one of clauses 17 to 20, wherein the visual representation of the particular search result replaces at least some keys of the plurality of keys.
- Clause 22 The computer-readable storage medium of any one of clauses 18 to 21, wherein the information related to the particular search result comprises a natural language response to the search query.
- Computer-readable media may include computer-readable storage media, which corresponds to a tangible medium such as data storage media, or communication media including any medium that facilitates transfer of a computer program from one place to another, e.g., according to a communication protocol.
- computer-readable media generally may correspond to (1) tangible computer-readable storage media, which is non-transitory or (2) a communication medium such as a signal or carrier wave.
- Data storage media may be any available media that can be accessed by one or more computers or one or more processors to retrieve instructions, code and/or data structures for implementation of the techniques described in this disclosure.
- a computer program product may include a computer-readable medium.
- such computer-readable storage media can comprise RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage, or other magnetic storage devices, flash memory, or any other medium that can be used to store desired program code in the form of instructions or data structures and that can be accessed by a computer.
- any connection is properly termed a computer-readable medium.
- a computer-readable medium For example, if instructions are transmitted from a website, server, or other remote source using a coaxial cable, fiber optic cable, twisted pair, digital subscriber line (DSL), or wireless technologies such as infrared, radio, and microwave, then the coaxial cable, fiber optic cable, twisted pair, DSL, or wireless technologies such as infrared, radio, and microwave are included in the definition of medium.
- DSL digital subscriber line
- Disk and disc includes compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), floppy disk and Blu-ray disc, where disks usually reproduce data magnetically, while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer-readable media.
- processors such as one or more digital signal processors (DSPs), general purpose microprocessors, application specific integrated circuits (ASICs), field programmable logic arrays (FPGAs), or other equivalent integrated or discrete logic circuitry.
- DSPs digital signal processors
- ASICs application specific integrated circuits
- FPGAs field programmable logic arrays
- processors may refer to any of the foregoing structure or any other structure suitable for implementation of the techniques described.
- the functionality described may be provided within dedicated hardware and/or software modules. Also, the techniques could be fully implemented in one or more circuits or logic elements.
- the techniques of this disclosure may be implemented in a wide variety of devices or apparatuses, including a wireless handset, an integrated circuit (IC) or a set of ICs (e.g., a chip set).
- IC integrated circuit
- a set of ICs e.g., a chip set.
- Various components, modules, or units are described in this disclosure to emphasize functional aspects of devices configured to perform the disclosed techniques, but do not necessarily require realization by different hardware units. Rather, as described above, various units may be combined in a hardware unit or provided by a collection of interoperative hardware units, including one or more processors as described above, in conjunction with suitable software and/or firmware.
Abstract
Description
- This application is a continuation of U.S. application Ser. No. 15/134,243, filed Apr. 20, 2016, the entire contents of which are hereby incorporated by reference.
- Despite being able to simultaneously execute several applications, some computing devices can only present a graphical user interface (GUI) of a single application at a time. To interact with multiple applications at once, a user of a computing device may explicitly or implicitly instruct the computing device to switch between different application GUIs. For example, a user of a computing device may cease entering text in a messaging application and provide input to cause the device to switch to a search application to search for information, then switch back to the search application and enter further text regarding the searched-for information, resulting in an inelegant and inefficient user experience.
- In one example, a method includes outputting, by a computing device, for display, a graphical keyboard including a plurality of keys and a suggestion region. The method also may include determining, by the computing device, based on a selection of the suggestion region or one or more keys from the plurality of keys, a search query; and retrieving, by the computing device, one or more search results determined based on the search query; and outputting, by the computing device, in place of at least a portion of the graphical keyboard, a visual representation of a particular search result of the one or more search results.
- In another example, a computing device includes a presence-sensitive display, at least one processor, and a memory storing instructions that, when executed by the at least one processor, cause the at least one processor to output, for display at the presence-sensitive display, a graphical keyboard including a plurality of keys and a suggestion region. The memory also stores instructions that, when executed by the at least one processor, cause the at least one processor to determine, based on a selection of the suggestion region or one or more keys from the plurality of keys, a search query; and retrieve one or more search results determined based on the search query; and output, in place of at least a portion of the graphical keyboard, a visual representation of a particular search result of the one or more search results.
- In another example, a computer-readable storage medium encoded with instructions that, when executed by at least one processor of a computing device, cause the at least one processor to output, for display at a presence-sensitive display, a graphical keyboard comprising a plurality of keys and a suggestion region. The computer-readable storage medium is also encoded with instructions that, when executed by at least one processor of a computing device, cause the at least one processor to determine, based on a selection of the suggestion region or one or more keys from the plurality of keys, a search query; and retrieve one or more search results determined based on the search query; and output, in place of at least a portion of the graphical keyboard, a visual representation of a particular search result of the one or more search results.
- The details of one or more examples are set forth in the accompanying drawings and the description below. Other features, objects, and advantages of the disclosure will be apparent from the description and drawings, and from the claims.
-
FIG. 1 is a conceptual diagram illustrating an example computing device that is configured to present a graphical keyboard with integrated search features, in accordance with one or more aspects of the present disclosure. -
FIG. 2 is a block diagram illustrating an example computing device that is configured to present a graphical keyboard with integrated search features, in accordance with one or more aspects of the present disclosure. -
FIG. 3 is a block diagram illustrating an example computing device that outputs graphical content for display at a remote device, in accordance with one or more techniques of the present disclosure. -
FIGS. 4A and 4B are conceptual diagrams illustrating example graphical user interfaces of an example computing device that is configured to present a graphical keyboard with integrated search features, in accordance with one or more aspects of the present disclosure. -
FIG. 5 is a flowchart illustrating example operations of a computing device that is configured to present a graphical keyboard with integrated search features, in accordance with one or more aspects of the present disclosure. - In general, this disclosure is directed to techniques for enabling a computing device to output, for display, a visual representation of one or more search results within a graphical keyboard. By including the visual representation of the one or more search results within the graphical keyboard, the computing device may enable a user to perform a search query for desired information and view information received in response to the search query without switching between applications. In particular, the computing device may enable the user to input the search query and view the one or more search results within a graphical user interface that includes the graphical keyboard.
- In some examples, the computing device may also determine, based on user input, selection of a portion of the visual representation of a particular search result of the one or more search results. In response, the computing device may be configured to, automatically, without further user input, insert information related to the search result in a text edit region displayed adjacent to the graphical keyboard. In this way, the computing device may enable insertion of information from the search result in the application for which the graphical keyboard is providing input. Further, by inserting the information from the search result in the text edit region with a single user input, the computing device may allow the user to input information with fewer inputs than a typical, multi-gesture, copy and paste action. In this way, the computing device described herein may enable information retrieval using a search query and insertion of the retrieved information in an application without application switching and with few user inputs.
-
FIG. 1 is a conceptual diagram illustrating anexample computing device 110 that is configured to output a graphical keyboard with integrated search features, in accordance with one or more aspects of the present disclosure. In some examples,computing device 110 may represent a mobile computing device, such as a smart phone, a tablet computer, a laptop computer, computerized watch, computerized eyewear, computerized gloves, or any other type of portable computing device. Additional examples ofcomputing device 110 include desktop computers, televisions, personal digital assistants (PDA), portable gaming systems, media players, e-book readers, mobile television platforms, automobile navigation and entertainment systems, vehicle (e.g., automobile, aircraft, or other vehicle) cockpit displays, kiosks, or any other types of wearable and non-wearable, mobile or non-mobile computing devices that may output a graphical keyboard for display. -
Computing device 110 includes a presence-sensitive display (PSD) 112, a user interface (UI)module 120, and akeyboard module 122.Modules computing device 110. For example, one or more processors ofcomputing device 110 may execute instructions that are stored at a memory or other non-transitory storage medium ofcomputing device 110 to perform the operations ofmodules Computing device 110 may executemodules modules modules -
PSD 112 ofcomputing device 110 may function as respective input and/or output devices forcomputing device 110. PSD 112 may be implemented using various technologies. For instance, PSD 112 may function as an input devices using presence-sensitive input screens, such as resistive touchscreens, surface acoustic wave touchscreens, capacitive touchscreens, projective capacitance touchscreens, pressure sensitive screens, acoustic pulse recognition touchscreens, or another presence-sensitive display technology. PSD 112 may also function as output (e.g., display) devices using any one or more display devices, such as liquid crystal displays (LCD), dot matrix displays, light emitting diode (LED) displays, organic light-emitting diode (OLED) displays, e-ink, or similar monochrome or color displays capable of outputting visible information to a user ofcomputing device 110. -
PSD 112 may detect input (e.g., touch and non-touch input) from a user ofcomputing device 110. PSD 112 may detect input by detecting one or more gestures from a user (e.g., the user touching, pointing, and/or swiping at or near one or more locations ofPSD 112 with a finger or a stylus pen). PSD 112 may output information to a user in the form of a user interface (e.g., user interface 114), which may be associated with functionality provided bycomputing device 110. Such user interfaces may be associated with computing platforms, operating systems, applications, and/or services executing at or accessible from computing device 110 (e.g., electronic message applications, chat applications, Internet browser applications, mobile or desktop operating systems, social media applications, electronic games, and other types of applications). For example, PSD 112 maypresent user interface 114 which, as shown inFIG. 1 , is a graphical user interface of a chat application executing atcomputing device 110 and includes various graphical elements displayed at various locations ofPSD 112. - Although shown in
FIG. 1 as a chat user interface,user interface 114 may be any graphical user interface that includes a graphical keyboard with integrated search features. In the example ofFIG. 1 ,user interface 114 includes anoutput region 116A, agraphical keyboard 116B, and atext edit region 116C. A user ofcomputing device 110 may provide input atgraphical keyboard 116B to produce textual characters withinedit region 116C that form the content of the electronic messages displayed withinoutput region 116A. The messages displayed withinoutput region 116A form a chat conversation between a user ofcomputing device 110 and a user of a different computing device. -
UI module 120 manages user interactions withPSD 112 and other components ofcomputing device 110. In other words,UI module 120 may act as an intermediary between various components ofcomputing device 110 to make determinations based on user input detected by PSD 112 and generate output atPSD 112, e.g., in response to the user input.UI module 120 may receive instructions from an application, service, platform, or other module ofcomputing device 110 to causePSD 112 to output a user interface (e.g., user interface 114).UI module 120 may manage inputs received bycomputing device 110 as a user views and interacts with the user interface presented at PSD 112 and update the user interface in response to receiving additional instructions from the application, service, platform, or other module ofcomputing device 110 that is processing the user input. -
Keyboard module 122 represents an application, service, or component executing at or accessible to computingdevice 110 that providescomputing device 110 with a graphical keyboard having integrated search features.Keyboard module 122 may switch between operating in text-entry mode in whichkeyboard module 122 functions similar to a traditional graphical keyboard, or search mode in whichkeyboard module 122 performs various integrated search functions, including, for example, receiving search queries or outputting one or more search results for display instead of at least part ofgraphical keyboard 116B (e.g., one or more keys of plurality ofgraphical keys 118A). - In some examples,
keyboard module 122 may be a stand-alone application, service, or module executing atcomputing device 110. In other examples,keyboard module 122 may be a sub-component of an application, service, or module executing atcomputing device 110. For example,keyboard module 122 may be integrated into a chat or messaging application executing atcomputing device 110. As another example,keyboard module 122 may be a stand-alone application or subroutine that is invoked by an application or operating platform ofcomputing device 110 any time an application or operating platform requires graphical keyboard input functionality. In some examples,computing device 110 may download and installkeyboard module 122 from an application repository of a service provider (e.g., via the Internet). In other examples,keyboard module 122 may be preloaded during production ofcomputing device 110. - When operating in text-entry mode,
keyboard module 122 ofcomputing device 110 may perform traditional, graphical keyboard operations used for text-entry, such as: generating a graphical keyboard layout including plurality ofgraphical keys 118A for display atPSD 112, mapping detected inputs atPSD 112 to selections ofgraphical keys 118A, determining characters based on selectedgraphical keys 118A, and predicting or autocorrecting words and/or phrases based on the characters determined from selectedgraphical keys 118A. -
Graphical keyboard 116B includes graphical elements displayed asgraphical keys 118A.Keyboard module 122 may output information toUI module 120 that specifies the layout ofgraphical keyboard 116B withinuser interface 114. For example, the information may include instructions that specify locations, sizes, colors, characters, and other characteristics ofgraphical keys 118A. Based on the information received fromkeyboard module 122,UI module 120 may causePSD 112 displaygraphical keyboard 116B as part ofuser interface 114. - Each key of
graphical keys 118A may be associated with a respective character (e.g., a letter, number, punctuation, or other character) displayed within the key. A user ofcomputing device 110 may provide input at locations ofPSD 112 at which one or more ofgraphical keys 118A are displayed to input content (e.g., characters, search results, etc.) intotext edit region 116C (e.g., for composing messages that are sent and displayed withinoutput region 116A or for inputting a search query thatcomputing device 110 executes from withingraphical keyboard 116B).Keyboard module 122 may receive information fromUI module 120 indicating locations associated with input detected byPSD 112 that are relative to the locations of each of the graphical keys. Using a spatial and/or language model,keyboard module 122 may translate the inputs to selections of keys and characters, words, and/or phrases. - For example,
PSD 112 may detect user input as a user ofcomputing device 110 provides inputs at or near a location ofPSD 112 wherePSD 112 presentsgraphical keys 118A.UI module 120 may receive, fromPSD 112, an indication of the user input detected byPSD 112 and output, tokeyboard module 122, information about the user input. Information about the user input may include an indication of one or more touch events (e.g., locations and other information about the input) detected byPSD 112. - Based on the information received from
UI module 120,keyboard module 122 may map detected user inputs atPSD 112 to selections ofgraphical keys 118A, determine characters based on selectedgraphical keys 118A, and predict or autocorrect words and/or phrases determined based on the characters associated with the selectedgraphical keys 118A. For example,keyboard module 122 may include a spatial model that may determine, based on the locations ofkeys 118A and the information about the input, the most likely one or moregraphical keys 118A being selected. Responsive to determining the most likely one or moregraphical keys 118A being selected,keyboard module 122 may determine one or more characters, words, and/or phrases. For example, each of the one or moregraphical keys 118A being selected from a user input atPSD 112 may represent an individual character or a keyboard operation.Keyboard module 122 may determine a sequence of characters selected based on the one or more selectedgraphical keys 118A. In some examples,keyboard module 122 may apply a language model to the sequence of characters to determine one or more likely candidate letters, morphemes, words, and/or phrases that a user is trying to input based on the selection ofgraphical keys 118A. -
Keyboard module 122 may send the sequence of characters and/or candidate words and phrases toUI module 120 andUI module 120 may causePSD 112 to present the characters and/or candidate words determined from a selection of one or moregraphical keys 118A as text withinedit region 116C. In some examples, when functioning as a traditional keyboard for performing text-entry operations, and in response to receiving a user input atgraphical keys 118A (e.g., as a user is typing atgraphical keyboard 116B to enter text withinedit region 116C),keyboard module 122 may causeUI module 120 to display the candidate words and/or phrases as one or more selectable spelling corrections and/or selectable word or phrase suggestions withinsuggestion region 118B. - In addition to performing traditional, graphical keyboard operations used for text-entry,
keyboard module 122 ofcomputing device 110 also provides integrated search capability. That is, rather than requiring a user ofcomputing device 110 to navigate away fromuser interface 114 which providesgraphical keyboard 116B andoutput region 116A (e.g., to a different application or service executing at or accessible from computing device 110),keyboard module 122 may operate in search mode in whichkeyboard module 122 may execute search operations and present one or more search results within the same region ofPSD 112 at whichgraphical keyboard 116B is displayed. - As indicated above,
keyboard module 122 may execute as a stand-alone application, service, or module executing atcomputing device 110 or as a single, integrated sub-component thereof. Therefore, ifkeyboard module 122 forms part of a chat or messaging application executing atcomputing device 110,keyboard module 122 may provide the chat or messaging application with text-entry capability as well as search capability. Similarly, ifkeyboard module 122 is a stand-alone application or subroutine that is invoked by an application or operating platform ofcomputing device 110 any time an application or operating platform requires graphical keyboard input functionality,keyboard module 122 may provide the invoking application or operating platform with text-entry capability as well as search capability. -
Keyboard module 122 may further operate in search mode, as opposed to text entry mode. In some examples, when operating in text entry mode,keyboard module 122 may causegraphical keyboard 116B to includesearch element 118C.Search element 118C represents a selectable element ofgraphical keyboard 116B for invoking one or more of the various search features ofgraphical keyboard 116B, such as invoking the search mode. By selectingsearch element 118C (e.g., by tapping or gesturing at a location or within a region ofPSD 112 at whichsearch element 118C is displayed), a user can causecomputing device 110 to invoke the various integrated search features without having to navigate to a separate application, service, or other feature executing at or accessible fromcomputing device 110. - For example,
UI module 120 may output information tokeyboard module 122 indicating that a user ofcomputing device 110 may have selectedsearch element 118C. Responsive to determining thatsearch element 118C was selected,keyboard module 122 may transition to operating in search mode. While operating in search mode,keyboard module 122 may reconfiguregraphical keyboard 116B to execute search features as opposed to operations that are primarily attributed to text entry to textedit region 116C. In some examples,keyboard module 122 may still enable text entry viagraphical keys 118A. - For example,
keyboard module 122 may configuresuggestion region 118B to present suggested content (e.g., predicted search queries, predicted emoticons or so called “emojis”, or other suggested content) as selectable elements withinsuggestion region 118B instead of predicted characters, words or phrases or other primarily linguistic information thatkeyboard module 122 derives from a language model, lexicon, or dictionary. In other words, rather than providing spelling or word suggestions from a dictionary withinsuggestion region 118B,computing device 110 may present, withinsuggestion region 118B, suggested search related content thatcomputing device 110 determines may assist a user in providing input related to electronic communications. - As another example, in response to receiving an indication that a user may have selected
search element 118C,keyboard module 122 may configuresuggestion region 118B to present search queries input usinggraphical keys 118A instead of predicted characters, words or phrases or other primarily linguistic information thatkeyboard module 122 derives from a language model, lexicon, or dictionary. In other words, rather than providing spelling or word suggestions from a dictionary withinsuggestion region 118B,computing device 110 may present, withinsuggestion region 118B, search queries input usinggraphical keys 118A. - In operation,
computing device 110 may determine, based on user input, a search query. In some examples, the user input may include selection of one or moregraphical keys 118A ofgraphical keyboard 116B. For example,keyboard module 122 may receive, fromUI module 120, indications of user input at locations ofPSD 112 corresponding tographical keyboard 116B indicating a search query. In some examples,keyboard module 122 may receive, fromUI module 120, an indication of a user input selectingsearch element 118C. In response,keyboard module 122 may invoke the search mode, and may configuresuggestion region 118B to present search queries input using one or moregraphical keys 118A. - In some examples,
keyboard module 122 may configuresuggestion region 118B to display search queries with a different format than predicted characters, words or phrases so that a user may visually distinguish search queries from predicted characters, words or phrases. For example,keyboard module 122 may configuresuggestion region 118B to display search queries with a predetermined text color, font, font size, etc., or adjacent to or within a distinguishing user interface element, such as a text box, to visually differentiate a search query from predicted characters, words or phrases. - In the search mode,
keyboard module 122 may receive, fromUI module 120, an indication of a user input selecting or attempting to select one or moregraphical keys 118A.Keyboard module 122 may include a spatial model, a language model, or both, which may determine, based on locations ofkeys 118A, information about the input, and a sequence of characters, the most likely one or more letters, morphemes, words, and/or phrases that a user is trying to input based on the selection ofkeys 118A.Keyboard module 122 may causeUI module 120 to output the candidate letters, morphemes, words, and/or phrases withinsuggestion region 118B as the search query. In some examples,keyboard module 122 may causeUI module 120 to output multiple candidate letters, morphemes, words, and/or phrases, e.g., by expandingsuggestion region 118B to present multiple candidate letters, morphemes, words, and/or phrases as individual lines of text. -
Keyboard module 122 may receive, fromUI module 120, an indication of a user input indicating completion of the search query, which may correspond to a user selecting a user interface element displayed atPSD 112, such as one ofgraphical keys 118A (e.g., the RETURN/SEARCH key illustrated inFIG. 1 ); one of the candidate letters, morphemes, words, and/or phrases displayed atsuggestion region 118B; etc. In response,keyboard module 122 may determine the search query, e.g., as the letters, morphemes, words, and/or phrases displayed atsuggestion region 118B at the time of the user input indicating completion of the search query or the selected one or the multiple candidate letters, morphemes, words, and/or phrases displayed in an expandedsuggestion region 118B. - In other examples, rather than receiving, from
UI module 120, an indication of a user input selectingsearch element 118C to invoke the search mode then receiving, fromUI module 120, an indication of a user input selecting or attempting to select one or moregraphical keys 118A,keyboard module 122 may determine a predicted search query based on content ofoutput region 116A,text edit region 116C, or both. For example, as illustrated inFIG. 1 ,output region 116A is an application such as a messaging or chat application, and illustrates communications from the user of computing device 110 (under “YOU”) and communications from a user of another computing device (under “FRIEND”). In other examples,output region 116A may be another application user interface, such as a notes application, a web browser, a search application, a maps application, an email application, a text editor application, or any other application for whichgraphical keyboard 116B provides text input. - In some examples,
keyboard module 122 may determine a predicted query based on content of the communication from the user ofcomputing device 110. For example,FIG. 1 illustrates the content of the communication from the user ofcomputing device 110 as being “Dinner tonight?” Based on the content of this communication,keyboard module 122 has determined a predicted search query “restaurants”.Keyboard module 122 then outputs an indication of the predicted search query toUI module 120, which causes the predicted search query “restaurants” to be output atsuggestion region 118B. - In some examples,
keyboard module 122 may analyze only content of communications from the user ofcomputing device 110 to determine a predicted search query. In other words, in some examples,keyboard module 122 may refrain from analyzing content of communications from a user of another computing device to determine a predicted search query. In other examples,keyboard module 122 may analyze content of communications from a user of another computing device (and received by computing device 110) to determine a predicted search query. -
Computing device 110 may determine a predicted search query based on content of the communication from the user ofcomputing device 110, content of the communication from the user of the other computing device, or both, only if the computing device receives permission from the user of the respective computing device to analyze the information. For example, before computingdevice 110 can collect or may make use of information associated with a user ofcomputing device 110 or a user of another computing device, the user may be provided with an opportunity to provide input to control whether programs or features ofcomputing device 110 can collect and make use of user information (e.g., information about a user's current location, current speed, text conversations, etc.), or to dictate whether and/or how tocomputing device 110 may receive content that may be relevant to the user. In addition, certain data may be treated in one or more ways before it is stored or used by computingdevice 110, so that personally-identifiable information is removed. For example, a user's identity may be treated so that no personally identifiable information can be determined about the user, or a user's geographic location may be generalized where location information is obtained (such as to a city, ZIP code, or state level), so that a particular location of a user cannot be determined. Thus, the user may have control over how information is collected about the user and used by computingdevice 110. - In some examples,
keyboard module 122 may transmit instructions toUI module 120 to cause the predicted search query to be displayed with a different visual appearance than predicted characters, words or phrases. For example,keyboard module 122 transmit instructions toUI module 120 to cause the predicted search query to be displayed with a predetermined text color, font, font size, etc., or adjacent to or within a distinguishing user interface element, such as a text box, an underline, an icon or picture, etc. to visually differentiate a search query from predicted characters, words or phrases. This may allow a user ofcomputing device 110 to easily distinguish between a predicted search query and predicted characters, words or phrases. - If the user decides to perform a search, the user may select
suggestion region 118B or the predicted search query displayed atsuggestion region 118B.Keyboard module 122 may receive an indication of the selection ofsuggestion region 118B or the predicted search query displayed atsuggestion region 118B fromUI module 120, and may determine that the search query is the predicted search query displayed atsuggestion region 118B. -
Computing device 110 then may retrieve one or more search results based on the search query. In some examples,computing device 110 may retrieve the one or more search results by transmitting the search query to a data service. In some examples, the data service may be, for example, a search system or a cloud-based computing service remote fromcomputing device 110. In response to receiving the search query, and based on content of the search query, the data service may perform a search. The search may include, for example, a search of webpages; a search of a semantic network including objects, facts, and relationships between the objects; an image search; a video search; a search localized to a location of the user ofcomputing device 110; etc. The data service then may transmit at least one search result tocomputing device 110, andcomputing device 110 may receive the at least one search result. The one or more search results may include textual information, images, videos, hyperlinks, etc. - In some examples,
computing device 110 may retrieve the one or more search results by performing a search based on content of the search query. The search may include, for example, a search of webpages; a search of a semantic network including objects, facts, and relationships between the objects; an image search; a video search; a search localized to a location of the user ofcomputing device 110; etc. -
Keyboard module 122 ofcomputing device 110 may retrieve the one or more search results and causeUI module 120 to output a visual representation of at least a portion of the one or more search results in place of at least a portion ofgraphical keyboard 116B. For example,keyboard module 122 may causeUI module 120 to output the visual representation of at least a portion of the one or more search results in place of at least some keys ofgraphical keys 118A. - In some examples,
keyboard module 122 ofcomputing device 110 may causeUI module 120 to output the visual representation as a card-based user interface element. The card-based user interface element may appear similar to a notecard, and may include a representation of a particular search result of the one or more search results. The representation of the particular search result may include, for example, at least one of text, a picture, an icon, or the like. -
Keyboard module 122 may generate a respective card-based user interface element for each respective search result of the one or more search results.Keyboard module 122 may causeUI module 120 to output at least one card-based user interface element for display at a time, andUI module 120 may be configured to switch between respective card-based user interface elements in response to determining a user gesture, e.g., a swipe. In this way,keyboard module 122 may enable a user to switch among one or more search results withingraphical keyboard 116B. - In some examples,
keyboard module 122 may be configured to determine based on an indication of user input received fromUI module 120, selection of a predetermined portion of the visual representation of a search result. In response to determining the selection,keyboard module 122 may be configured to automatically, without further user input, output an indication of information related to the search result toUI module 120, which is configured to insert the information related to the search result intext edit region 116C. The information related to the search result may include, for example, text, a hyperlink, an image, an icon, etc. In some examples, the selection of the predetermined portion of the visual representation of the search result may include a tap gesture in which the user taps the predetermined portion of the visual representation of the search result. Inresponse keyboard module 122 may be configured to insert a textual representation of the search result intext edit region 116C oroutput region 116A. In some examples, the selection of the predetermined portion of the visual representation of the search result may include a drag gesture in which the user is dragging the visual representation of the search result totext edit region 116C oroutput region 116A. In response,keyboard module 122 may be configured to insert an image of the search result or a portion of the search result intext edit region 116C oroutput region 116A. In this way,computing device 110 is configured to allow one-gesture input of information from the search result to thetext edit region 116C, enabling easy and efficient entry of information from the search result to the application for whichgraphical keyboard 116B is being used to input text. - In this way, the techniques of the disclosure may enable a computing device to provide a graphical keyboard with integrated search features that include display of a visual representation of at least a portion of one or more search results in place of at least a portion of the graphical keyboard. Further, in some examples, the visual representation may include a card-based user interface element, and
computing device 110 may be configured to output different card-based user interface elements, corresponding to different search results determined based on the search query, in response to receiving an indication of a user input, such as a swipe gesture. Additionally,computing device 110 may be configured to determine, based on user input, selection of a predetermined portion of the visual representation of the at least a portion (e.g., a search result) of the one or more search results and, automatically, without further user input, insert, intext edit region 116C displayed adjacent tographical keyboard 116B, information related to the at least a portion (e.g., a search result) of the one or more search results. - In this way,
computing device 110 may facilitate insertion of information from the search result in the application for whichgraphical keyboard 116B is providing input. Further, by inserting the information from the search result intext edit region 116C with a single user input,computing device 110 may allow the user to input information with fewer inputs than a typical, multi-gesture copy and paste action. In this way,computing device 110 may facilitate information retrieval using a search query and insertion of the retrieved information in an application without application switching and with few user inputs. -
FIG. 2 is a block diagram illustratingcomputing device 210 as an example computing device that is configured to present a graphical keyboard with integrated search features, in accordance with one or more aspects of the present disclosure.Computing device 210 ofFIG. 2 is described below as an example ofcomputing device 110 ofFIG. 1 .FIG. 2 illustrates only one particular example ofcomputing device 210, and many other examples ofcomputing device 210 may be used in other instances and may include a subset of the components included inexample computing device 210 or may include additional components not shown inFIG. 2 . - As shown in the example of
FIG. 2 ,computing device 210 includes presence-sensitive display (PSD 212), one ormore processors 240, one ormore communication units 242, one ormore input components 244, one ormore output components 246, and one ormore storage components 248.PSD 212 includesdisplay component 202 and presence-sensitive input component 204.Storage components 248 ofcomputing device 210 includeUI module 220,keyboard module 222, and one ormore application modules 224.Keyboard module 222 may include spatial model (“SM”)module 226, language model (“LM”)module 228, andsearch module 230.Communication channels 250 may interconnect each of thecomponents communication channels 250 may include a system bus, a network connection, an inter-process communication data structure, or any other method for communicating data. - One or
more communication units 242 ofcomputing device 210 may communicate with external devices via one or more wired and/or wireless networks by transmitting and/or receiving network signals on the one or more networks. Examples ofcommunication units 242 include a network interface card (e.g. such as an Ethernet card), an optical transceiver, a radio frequency transceiver, a GPS receiver, or any other type of device that can send and/or receive information. Other examples ofcommunication units 242 may include short wave radios, cellular data radios, wireless network radios, as well as universal serial bus (USB) controllers. - One or
more input components 244 ofcomputing device 210 may receive input. Examples of input are tactile, audio, and video input.Input components 242 ofcomputing device 210, in one example, includes a presence-sensitive input device (e.g., a touch sensitive screen, a PSD), mouse, keyboard, voice responsive system, video camera, microphone or any other type of device for detecting input from a human or machine. In some examples,input components 242 may include one or more sensor components one or more location sensors (GPS components, Wi-Fi components, cellular components), one or more temperature sensors, one or more movement sensors (e.g., accelerometers, gyros), one or more pressure sensors (e.g., barometer), one or more ambient light sensors, and one or more other sensors (e.g., microphone, camera, infrared proximity sensor, hygrometer, and the like). Other sensors may include a heart rate sensor, magnetometer, glucose sensor, hygrometer sensor, olfactory sensor, compass sensor, step counter sensor, to name a few other non-limiting examples. - One or
more output components 246 ofcomputing device 110 may generate output. Examples of output are tactile, audio, and video output.Output components 246 ofcomputing device 210, in one example, includes a PSD, sound card, video graphics adapter card, speaker, cathode ray tube (CRT) monitor, liquid crystal display (LCD), or any other type of device for generating output to a human or machine. -
PSD 212 ofcomputing device 210 may be similar toPSD 112 ofcomputing device 110 and includesdisplay component 202 and presence-sensitive input component 204.Display component 202 may be a screen at which information is displayed byPSD 212 and presence-sensitive input component 204 may detect an object at and/ornear display component 202. As one example range, presence-sensitive input component 204 may detect an object, such as a finger or stylus, that is within two inches or less ofdisplay component 202. Presence-sensitive input component 204 may determine a location (e.g., an [x, y] coordinate) ofdisplay component 202 at which the object was detected. In another example range, presence-sensitive input component 204 may detect an object six inches or less fromdisplay component 202. Other ranges are also possible. Presence-sensitive input component 204 may determine the location ofdisplay component 202 selected by a user's finger using capacitive, inductive, and/or optical recognition techniques. In some examples, presence-sensitive input component 204 also provides output to a user using tactile, audio, or video stimuli as described with respect to displaycomponent 202. In the example ofFIG. 2 ,PSD 212 may present a user interface (such asgraphical user interface 114 ofFIG. 1 ). - While illustrated as an internal component of
computing device 210,PSD 212 may also represent an external component that shares a data path withcomputing device 210 for transmitting and/or receiving input and output. For instance, in one example,PSD 212 represents a built-in component ofcomputing device 210 located within and physically connected to the external packaging of computing device 210 (e.g., a screen on a mobile phone). In another example,PSD 212 represents an external component ofcomputing device 210 located outside and physically separated from the packaging or housing of computing device 210 (e.g., a monitor, a projector, etc. that shares a wired and/or wireless data path with computing device 210). -
PSD 212 ofcomputing device 210 may detect two-dimensional and/or three-dimensional gestures as input from a user ofcomputing device 210. For instance, a sensor ofPSD 212 may detect a user's movement (e.g., moving a hand, an arm, a pen, a stylus, etc.) within a threshold distance of the sensor ofPSD 212.PSD 212 may determine a two or three dimensional vector representation of the movement and correlate the vector representation to a gesture input (e.g., a hand-wave, a pinch, a clap, a pen stroke, etc.) that has multiple dimensions. In other words,PSD 212 can detect a multi-dimension gesture without requiring the user to gesture at or near a screen or surface at whichPSD 212 outputs information for display. Instead,PSD 212 can detect a multi-dimensional gesture performed at or near a sensor which may or may not be located near the screen or surface at whichPSD 212 outputs information for display. - One or
more processors 240 may implement functionality and/or execute instructions associated withcomputing device 210. Examples ofprocessors 240 include application processors, display controllers, auxiliary processors, one or more sensor hubs, and any other hardware configure to function as a processor, a processing unit, or a processing device.Modules processors 240 to perform various actions, operations, or functions ofcomputing device 210. For example,processors 240 ofcomputing device 210 may retrieve and execute instructions stored bystorage components 248 that causeprocessors 240 to perform the operations ofmodules processors 240, may causecomputing device 210 to store information withinstorage components 248. - One or
more storage components 248 withincomputing device 210 may store information for processing during operation of computing device 210 (e.g.,computing device 210 may store data accessed bymodules storage component 248 is a temporary memory, meaning that a primary purpose ofstorage component 248 is not long-term storage.Storage components 248 oncomputing device 210 may be configured for short-term storage of information as volatile memory and therefore not retain stored contents if powered off. Examples of volatile memories include random access memories (RAM), dynamic random access memories (DRAM), static random access memories (SRAM), and other forms of volatile memories known in the art. -
Storage components 248, in some examples, also include one or more computer-readable storage media.Storage components 248 in some examples include one or more non-transitory computer-readable storage mediums.Storage components 248 may be configured to store larger amounts of information than typically stored by volatile memory.Storage components 248 may further be configured for long-term storage of information as non-volatile memory space and retain information after power on/off cycles. Examples of non-volatile memories include magnetic hard discs, optical discs, floppy discs, flash memories, or forms of electrically programmable memories (EPROM) or electrically erasable and programmable (EEPROM) memories.Storage components 248 may store program instructions and/or information (e.g., data) associated withmodules Storage components 248 may include a memory configured to store data or other information associated withmodules -
UI module 220 may include all functionality ofUI module 120 ofcomputing device 110 ofFIG. 1 and may perform similar operations asUI module 120 for managing a user interface (e.g., user interface 114) thatcomputing device 210 provides atPSD 212 for handling input from a user. For example,UI module 220 ofcomputing device 210 may querykeyboard module 222 for a keyboard layout (e.g., an English language QWERTY keyboard, etc.).UI module 220 may transmit a request for a keyboard layout overcommunication channels 250 tokeyboard module 222.Keyboard module 222 may receive the request and reply toUI module 220 with data associated with the keyboard layout.UI module 220 may receive the keyboard layout data overcommunication channels 250 and use the data to generate a user interface.UI module 220 may transmit a display command and data overcommunication channels 250 to causePSD 212 to present the user interface atPSD 212. - In some examples,
UI module 220 may receive an indication of one or more user inputs detected atPSD 212 and may output information about the user inputs tokeyboard module 222. For example,PSD 212 may detect a user input and send data about the user input toUI module 220 overcommunications channels 250.UI module 220 may generate one or more touch events based on the detected input. A touch event may include information that characterizes user input, such as a location component (e.g., [x,y] coordinates) of the user input, a time component (e.g., when the user input was received), a force component (e.g., an amount of pressure applied by the user input), or other data (e.g., speed, acceleration, direction, density, etc.) about the user input. - Based on location information of the touch events generated from the user input,
UI module 220 may determine that the detected user input is associated the graphical keyboard.UI module 220 may send an indication of the one or more touch events tokeyboard module 222 for further interpretation.Keyboard module 222 may determine, based on the touch events received fromUI module 220, that the detected user input represents an initial selection of one or more keys of the graphical keyboard. -
Application modules 224 represent all the various individual applications and services executing at and accessible fromcomputing device 210 that may rely on a graphical keyboard having integrated search features. A user ofcomputing device 210 may interact with a graphical user interface associated with one ormore application modules 224 to causecomputing device 210 to perform a function. Numerous examples ofapplication modules 224 may exist and include, a fitness application, a calendar application, a personal assistant or prediction engine, a search application, a map or navigation application, a transportation service application (e.g., a bus or train tracking application), a social media application, a game application, an e-mail application, a chat or messaging application, an Internet browser application, or any and all other applications that may execute atcomputing device 210. -
Keyboard module 222 may include all functionality ofkeyboard module 122 ofcomputing device 110 ofFIG. 1 and may perform similar operations askeyboard module 122 for providing a graphical keyboard having integrated search features.Keyboard module 222 may include various submodules, such as spatial model (SM)module 226, language model (LM)module 228, andsearch module 230, which may perform the functionality ofkeyboard module 222. -
SM module 226 may receive one or more touch events as input, and output a character or sequence of characters that likely represents the one or more touch events, along with a degree of certainty or spatial model score indicative of how likely or with what accuracy the one or more characters define the touch events. In other words,SM module 226 may infer touch events as a selection of one or more keys of a keyboard and may output, based on the selection of the one or more keys, a character or sequence of characters. - When
keyboard module 222 operates in text-entry mode,LM module 228 may receive a character or sequence of characters as input, and output one or more candidate characters, words, or phrases thatLM module 228 identifies from a lexicon as being potential replacements for a sequence of characters thatLM module 228 receives as input for a given language context (e.g., a sentence in a written language).Keyboard module 222 may causeUI module 220 to present one or more of the candidate words or phrases atsuggestion regions 118C ofuser interface 114. - The lexicon of
computing device 210 may include a list of words within a written language vocabulary (e.g., a dictionary). For instance, the lexicon may include a database of words (e.g., words in a standard dictionary and/or words added to a dictionary by a user or computing device 210).LM module 228 may perform a lookup in the lexicon of a character string to identify one or more letters, words, and/or phrases that include parts or all of the characters of the character string. For example,LM module 228 may assign a language model probability or a similarity coefficient (e.g., a Jaccard similarity coefficient) to one or more candidate words located at a lexicon ofcomputing device 210 that include at least some of the same characters as the inputted character or sequence of characters. The language model probability assigned to each of the one or more candidate words indicates a degree of certainty or a degree of likelihood that the candidate word is typically found positioned subsequent to, prior to, and/or within, a sequence of words (e.g., a sentence) generated from text input detected by presence-sensitive input component 204 prior to and/or subsequent to receiving the current sequence of characters being analyzed byLM module 228. In response to determining the one or more candidate words,LM module 228 may output the one or more candidate words from lexicon data stores 260A that have the highest similarity coefficients. -
Search module 230 ofkeyboard module 222 may perform integrated search functions on behalf ofkeyboard module 222. That is, when invoked (e.g., in response to a user ofcomputing device 210 selecting selectable element 218C of user interface 114),keyboard module 222 may operate in search mode wherekeyboard module 222 enablescomputing device 210 to perform search functions from withingraphical keyboard 118A. - In operation,
search module 230 may determine, based on user input, a search query. In some examples, the user input may include selection of one or moregraphical keys 118A ofgraphical keyboard 116A (FIG. 1 ). For example,search module 230 may receive, fromUI module 220, indications of user input at locations ofPSD 212 corresponding tographical keyboard 116B indicating a search query. In some examples,search module 230 may receive, fromUI module 220, an indication of a user input selectingsearch element 118C (FIG. 1 ). In response,keyboard module 222 may invoke the search mode, and may configuresuggestion region 118B (FIG. 1 ) to present search queries input using one or moregraphical keys 118A. - In some examples,
keyboard module 222 may configuresuggestion region 118B to display search queries with a different format than predicted characters, words or phrases so that a user may visually distinguish search queries from predicted characters, words or phrases. For example,keyboard module 222 may configuresuggestion region 118B to display search queries with a predetermined text color, font, font size, etc., or adjacent to or within a distinguishing user interface element, such as a text box, to visually differentiate a search query from predicted characters, words or phrases. - In the search mode,
search module 230 may receive, fromUI module 220, an indication of a user input selecting or attempting to select one or moregraphical keys 118A (e.g., a touch event).SM module 226 may determine a character or sequence of characters that likely represents the one or more touch events.LM module 228 may receive the character or sequence of characters that likely represents the one or more touch events fromSM module 226, and may determine the most likely one or more letters, morphemes, words, and/or phrases that a user is trying to input.LM module 228 ofkeyboard module 222 may causeUI module 220 to output the candidate letters, morphemes, words, and/or phrases withinsuggestion region 118B as the search query. In some examples,keyboard module 222 may causeUI module 220 to output multiple candidate letters, morphemes, words, and/or phrases, e.g., by expandingsuggestion region 118B to present multiple candidate letters, morphemes, words, and/or phrases as individual lines of text. -
Search module 230 may receive, fromUI module 220, an indication of a user input indicating completion of the search query, which may correspond to a user selecting a user interface element displayed atPSD 212, such as one ofgraphical keys 118A (e.g., the RETURN/SEARCH key illustrated inFIG. 1 ); selecting one of the candidate letters, morphemes, words, and/or phrases displayed atsuggestion region 118B; etc. In response,search module 230 may determine the search query, e.g., as the letters, morphemes, words, and/or phrases displayed atsuggestion region 118B at the time of the user input indicating completion of the search query or the selected one or the multiple candidate letters, morphemes, words, and/or phrases displayed in an expandedsuggestion region 118B. - In other examples, rather than receiving, from
UI module 220, an indication of a user input selectingsearch element 118C to invoke the search mode then receiving, fromUI module 220, an indication of a user input selecting or attempting to select one or moregraphical keys 118A,search module 230 may determine a predicted search query based on content ofoutput region 116A,text edit region 116C, or both. For example,search module 230 may determine a predicted query based on content of a communication input usinggraphical keyboard 116B by the user ofcomputing device 210.Keyboard module 222 then outputs an indication of the predicted search query toUI module 220, which causes the predicted search query to be output atsuggestion region 118B. - In some examples,
search module 230 may analyze only content of communications from the user ofcomputing device 210 to determine a predicted search query. In other words, in some examples,search module 230 may refrain from analyzing content of communications from a user of another computing device to determine a predicted search query. In other examples,search module 230 may analyze content of communications from a user of another computing device (and received by computing device 210) to determine a predicted search query. -
Search module 230 may determine a predicted search query based on content of the communication from the user ofcomputing device 210, content of the communication from the user of the other computing device, or both, only if the computing device receives permission from the user of the respective computing device to analyze the information. For example, before computingdevice 210 can collect or may make use of information associated with a user ofcomputing device 210 or a user of another computing device, the user may be provided with an opportunity to provide input to control whether programs or features ofcomputing device 210 can collect and make use of user information (e.g., information about a user's current location, current speed, text conversations, etc.), or to dictate whether and/or how tocomputing device 210 may receive content that may be relevant to the user. In addition, certain data may be treated in one or more ways before it is stored or used by computingdevice 210, so that personally-identifiable information is removed. For example, a user's identity may be treated so that no personally identifiable information can be determined about the user, or a user's geographic location may be generalized where location information is obtained (such as to a city, ZIP code, or state level), so that a particular location of a user cannot be determined. Thus, the user may have control over how information is collected about the user and used by computingdevice 210. - If the user decides to perform a search, the user may select
suggestion region 118B or the predicted search query displayed atsuggestion region 118B.Search module 230 may receive an indication of the selection ofsuggestion region 118B or the predicted search query displayed atsuggestion region 118B fromUI module 220, and may determine that the search query is the predicted search query displayed atsuggestion region 118B. -
Search module 230 then may retrieve one or more search results based on the search query. In some examples,search module 230 may retrieve the one or more search results by causingcommunication units 242 to transmit the search query to a data service. The data service may include, for example, a search system, a cloud-based computing environment, etc. In response to receiving the search query, and based on content of the search query, the data service may perform a search. The search may include, for example, a search of webpages, a relational or hierarchical knowledge collection search, an image search, a video search, a search localized to a location of computingdevice 210, etc. The data service then may transmit at least one search result tocomputing device 210, andcommunication units 242 may receive the at least one search result. The one or more search results may include textual information, images, videos, hyperlinks, etc. - In some examples,
search module 230 may retrieve one or more search results based on the search query by performing a search based on content of the search query. The search may include, for example, a search of webpages, a relational or hierarchical knowledge collection search, an image search, a video search, a search localized to a location of computingdevice 210, etc. The one or more search results may include textual information, images, videos, hyperlinks, etc. -
Search module 230 may retrieve the at least one search result fromcommunication units 242.Search module 230 may causeUI module 220 to output a visual representation of at least a portion of the one or more search results in place of at least a portion ofgraphical keyboard 116B. For example,search module 230 may causeUI module 220 to output the visual representation of at least a portion of the one or more search results in place of at least some keys ofgraphical keys 118A. - In some examples,
search module 230 may causeUI module 220 to output the visual representation as a card-based user interface element. The card-based user interface element may appear similar to a notecard, and may include a representation of a particular search result of the one or more search results. The representation of the particular search result may include, for example, at least one of text, an image, an icon, etc. -
Search module 230 may generate a respective card-based user interface element for each respective search result of the one or more search results.Search module 230 may causeUI module 220 to output at least one card-based user interface element for display at a time, andUI module 220 may be configured to switch between respective card-based user interface elements in response to determining a user gesture, e.g., a swipe. In this way,search module 230 andUI module 220 may enable a user to switch among search results withingraphical keyboard 116B. - In some examples, the visual representation may be divided into multiple, different portions. For example, a first portion of the visual representation may include a representation of content of the particular search result, such as text, an image, an icon associated with the particular search result, etc., and a second portion may be associated with an action performed by an application other than the
graphical keyboard 116B. - In some examples,
search module 230 may be configured to determine, based on an indication of user input atPSD 212 received from UI module 220 (e.g., an indication of a touch event), selection of the first portion of the visual representation of a search result. In response to determining the selection of the first portion of the visual representation,search module 230 may be configured to automatically, without further user input, output an indication of information related to the search result toUI module 220 and causeUI module 220 to insert the information related to the search result intext edit region 116C (FIG. 1 ). The information related to the search result may include, for example, text, a hyperlink, an image, a video, an icon, etc. In some examples, the selection of the first portion of the visual representation of the search result may include a tap gesture in which the user taps the first portion of the visual representation of the search result. Inresponse keyboard module 122 may be configured to insert a textual representation of the search result intext edit region 116C oroutput region 116A. In some examples, the selection of the first portion of the visual representation of the search result may include a drag gesture in which the user is dragging the visual representation of the search result totext edit region 116C oroutput region 116A. In response,keyboard module 122 may be configured to insert an image of the search result or a portion of the search result intext edit region 116C oroutput region 116A. In this way,search module 230 is configured to allow one-gesture input of information related to the search result to thetext edit region 116C, enabling easy and efficient entry of information from the search result to the application for whichgraphical keyboard 116B is being used to input text. - The second portion of the visual representation may be associated with an action performed by an application other than the
graphical keyboard 116B. For example, the second portion may include at least one user interface element (e.g., icon, image, text etc.) associated with the action, which, when selected by a user, causesUI module 220 to output the touch event to another application module ofapplications module 224 to perform the indicated action. The user interface element may include an element associated with applications including a web browser, a dedicated search application, a phone application, a maps application, etc. As a particular example, the user interface element may include a phone icon, with or without accompanying text (such as “Call”); a hyperlink; an icon representative of directions, with or without accompanying text (such as “Navigate”); a play button icon, with or without accompanying text (such as “Play”); etc. In response to receiving, fromPSD 212, a touch event,UI module 220 may determine selection of the second portion of the visual representation of the search result (or a particular user interface element of the second portion), and may transmit an indication of the selection of the second portion of the visual representation to the associated application, which may perform the action associated with the second portion or the user interface element of the second portion. In this way, in addition to allowing insertion of information from the search result into the application for which the graphical keyboard is being used to input text,search module 230 may present a user with shortcuts to actions performed by other applications. - In this way, the techniques of the disclosure may enable
keyboard module 222computing device 210 to provide a graphical keyboard with integrated search features that include display of a visual representation of at least a portion of one or more search results in place of at least a portion of the graphical keyboard. Further, in some examples, the visual representation may include a card-based user interface element, andcomputing device 210 may be configured to output different card-based user interface elements, corresponding to different search results determined based on the search query, in response to receiving an indication of a user input, such as a swipe gesture. Additionally,computing device 210 may be configured to determine, based on user input, selection of a predetermined portion of the visual representation of the at least a portion (e.g., a search result) of the one or more search results and, automatically, without further user input, insert, intext edit region 116C displayed adjacent tographical keyboard 116B, information related of the at least a portion (e.g., a search result) of the one or more search results. -
FIG. 3 is a block diagram illustrating an example computing device that outputs graphical content for display at a remote device, in accordance with one or more techniques of the present disclosure. Graphical content, generally, may include any visual information that may be output for display, such as text, images, a group of moving images, to name only a few examples. The example shown inFIG. 3 includes a computing device 310, a PSD 312, communication unit 342, projector 380, projector screen 382, mobile device 386, and visual display component 390. In some examples, PSD 312 may be a presence-sensitive display as described inFIGS. 1-2 . Although shown for purposes of example inFIGS. 1 and 2 as a stand-alone computing device 110, a computing device such as computing device 310 may, generally, be any component or system that includes a processor or other suitable computing environment for executing software instructions and, for example, need not include a presence-sensitive display. - As shown in the example of
FIG. 3 , computing device 310 may be a processor that includes functionality as described with respect toprocessors 240 inFIG. 2 . In such examples, computing device 310 may be operatively coupled to PSD 312 by a communication channel 362A, which may be a system bus or other suitable connection. Computing device 310 may also be operatively coupled to communication unit 342, further described below, by a communication channel 362B, which may also be a system bus or other suitable connection. Although shown separately as an example inFIG. 3 , computing device 310 may be operatively coupled to PSD 312 and communication unit 342 by any number of one or more communication channels. - In other examples, such as illustrated previously by computing
device 110 inFIGS. 1-2 , a computing device may refer to a portable or mobile device such as mobile phones (including smart phones), laptop computers, etc. In some examples, a computing device may be a desktop computer, tablet computer, a smart television platform, a camera, a personal digital assistant (PDA), an automobile navigation or entertainment device, a kiosk, a server, a mainframe, etc. - PSD 312 may include display component 302 and presence-sensitive input component 304. Display component 302 may, for example, receive data from computing device 310 and display the graphical content. In some examples, presence-sensitive input component 304 may determine one or more user inputs (e.g., continuous gestures, multi-touch gestures, single-touch gestures) at PSD 312 using capacitive, inductive, and/or optical recognition techniques and send indications of such user input to computing device 310 using communication channel 362A. In some examples, presence-sensitive input component 304 may be physically positioned on top of display component 302 such that, when a user positions an input unit over a graphical element displayed by display component 302, the location at which presence-sensitive input component 304 corresponds to the location of display component 302 at which the graphical element is displayed.
- As shown in
FIG. 3 , computing device 310 may also include and/or be operatively coupled with communication unit 342. Communication unit 342 may include functionality ofcommunication unit 242 as described inFIG. 2 . Examples of communication unit 342 may include a network interface card, an Ethernet card, an optical transceiver, a radio frequency transceiver, or any other type of device that can send and receive information. Other examples of such communication units may include Bluetooth, 3G, and WiFi radios, Universal Serial Bus (USB) interfaces, etc. Computing device 310 may also include and/or be operatively coupled with one or more other devices (e.g., input devices, output components, memory, storage devices) that are not shown inFIG. 3 for purposes of brevity and illustration. -
FIG. 3 also illustrates a projector 380 and projector screen 382. Other such examples of projection devices may include electronic whiteboards, holographic display components, and any other suitable devices for displaying graphical content. Projector 380 and projector screen 382 may include one or more communication units that enable the respective devices to communicate with computing device 310. In some examples, the one or more communication units may enable communication between projector 380 and projector screen 382. Projector 380 may receive data from computing device 310 that includes graphical content. Projector 380, in response to receiving the data, may project the graphical content onto projector screen 382. In some examples, projector 380 may determine one or more user inputs (e.g., continuous gestures, multi-touch gestures, single-touch gestures) at projector screen using optical recognition or other suitable techniques and send indications of such user input using one or more communication units to computing device 310. In such examples, projector screen 382 may be unnecessary, and projector 380 may project graphical content on any suitable medium and detect one or more user inputs using optical recognition or other such suitable techniques. - Projector screen 382, in some examples, may include a presence-sensitive display 384. Presence-sensitive display 384 may include a subset of functionality or all of the functionality of presence-
sensitive display 112 and/or 312 as described in this disclosure. In some examples, presence-sensitive display 384 may include additional functionality. Projector screen 382 (e.g., an electronic whiteboard), may receive data from computing device 310 and display the graphical content. In some examples, presence-sensitive display 384 may determine one or more user inputs (e.g., continuous gestures, multi-touch gestures, single-touch gestures) at projector screen 382 using capacitive, inductive, and/or optical recognition techniques and send indications of such user input using one or more communication units to computing device 310. -
FIG. 3 also illustrates mobile device 386 and visual display component 390. Mobile device 386 and visual display component 390 may each include computing and connectivity capabilities. Examples of mobile device 386 may include e-reader devices, convertible notebook devices, hybrid slate devices, etc. Examples of visual display component 390 may include other devices such as televisions, computer monitors, etc. In some examples, visual display component 390 may be a vehicle cockpit display or navigation display (e.g., in an automobile, aircraft, or some other vehicle). In some examples, visual display component 390 may be a home automation display or some other type of display that is separate from computing device 310. - As shown in
FIG. 3 , mobile device 386 may include a presence-sensitive display 388. Visual display component 390 may include a presence-sensitive display 392. Presence-sensitive displays 388, 392 may include a subset of functionality or all of the functionality of presence-sensitive display - As described above, in some examples, computing device 310 may output graphical content for display at PSD 312 that is coupled to computing device 310 by a system bus or other suitable communication channel. Computing device 310 may also output graphical content for display at one or more remote devices, such as projector 380, projector screen 382, mobile device 386, and visual display component 390. For instance, computing device 310 may execute one or more instructions to generate and/or modify graphical content in accordance with techniques of the present disclosure. Computing device 310 may output the data that includes the graphical content to a communication unit of computing device 310, such as communication unit 342. Communication unit 342 may send the data to one or more of the remote devices, such as projector 380, projector screen 382, mobile device 386, and/or visual display component 390. In this way, computing device 310 may output the graphical content for display at one or more of the remote devices. In some examples, one or more of the remote devices may output the graphical content at a presence-sensitive display that is included in and/or operatively coupled to the respective remote devices.
- In some examples, computing device 310 may not output graphical content at PSD 312 that is operatively coupled to computing device 310. In other examples, computing device 310 may output graphical content for display at both a PSD 312 that is coupled to computing device 310 by communication channel 362A, and at one or more remote devices. In such examples, the graphical content may be displayed substantially contemporaneously at each respective device. For instance, some delay may be introduced by the communication latency to send the data that includes the graphical content to the remote device. In some examples, graphical content generated by computing device 310 and output for display at PSD 312 may be different than graphical content display output for display at one or more remote devices.
- Computing device 310 may send and receive data using any suitable communication techniques. For example, computing device 310 may be operatively coupled to external network 374 using network link 373A. Each of the remote devices illustrated in
FIG. 3 may be operatively coupled to network external network 374 by one of respective network links 373B, 373C, or 373D. External network 374 may include network hubs, network switches, network routers, etc., that are operatively inter-coupled thereby providing for the exchange of information between computing device 310 and the remote devices illustrated inFIG. 3 . In some examples, network links 373A-373D may be Ethernet, ATM or other network connections. Such connections may be wireless and/or wired connections. - In some examples, computing device 310 may be operatively coupled to one or more of the remote devices included in
FIG. 3 using direct device communication 378. Direct device communication 378 may include communications through which computing device 310 sends and receives data directly with a remote device, using wired or wireless communication. That is, in some examples of direct device communication 378, data sent by computing device 310 may not be forwarded by one or more additional devices before being received at the remote device, and vice-versa. Examples of direct device communication 378 may include Bluetooth, Near-Field Communication, Universal Serial Bus, WiFi, infrared, etc. One or more of the remote devices illustrated inFIG. 3 may be operatively coupled with computing device 310 by communication links 376A-376D. In some examples, communication links 376A-376D may be connections using Bluetooth, Near-Field Communication, Universal Serial Bus, infrared, etc. Such connections may be wireless and/or wired connections. - In accordance with techniques of the disclosure, computing device 310 may be operatively coupled to visual display component 390 using external network 374. Computing device 310 may output a graphical keyboard for display at PSD 312. For instance, computing device 310 may send data that includes a representation of the graphical keyboard to communication unit 342. Communication unit 342 may send the data that includes the representation of the graphical keyboard to visual display component 390 using external network 374. Visual display component 390, in response to receiving the data using external network 374, may cause PSD 312 to output the graphical keyboard. In response to receiving a user input at PSD 312 to select one or more keys of the keyboard, visual display device 130 may send an indication of the user input to computing device 310 using external network 374. Communication unit 342 of may receive the indication of the user input, and send the indication to computing device 310.
- Computing device 310 may select, based on the user input, one or more keys or user interface elements (e.g., a suggested or predicted search query). Computing device 310 may determine, based on the selection of one or more keys or user interface elements, a search query. In some examples, computing device 310 may retrieve one or more search result based on the search query. Computing device 310 may output a representation of an updated graphical user interface including an updated graphical keyboard in which at least a portion of the one or more search results (e.g., a search result) is displayed instead of at least a portion of the graphical keyboard (e.g., at least one graphical key of the graphical keyboard). Communication unit 342 may receive the representation of the updated graphical user interface and may send the send the representation to visual display component 390, such that visual display component 390 may cause PSD 312 to output the updated graphical keyboard, including at least a portion of the one or more search results (e.g., a search result) displayed instead of at least a portion of the graphical keyboard (e.g., at least one graphical key of the graphical keyboard).
- In some examples, computing device 310 may be configured to determine, based on an indication of user input received at PSD 312, selection of a predetermined portion of the visual representation of a search result. In response to determining the selection, computing device 310 may be configured to automatically, without further user input, output a representation of an updated graphical user interface including an updated graphical keyboard in which information related to the search result is displayed in a text edit region adjacent to the graphical keyboard. In this way, computing device 310 is configured to allow one-gesture input of information from the search result to the text edit region, enabling easy and efficient entry of information from the search result to the application for which the graphical keyboard is being used to input text.
-
FIGS. 4A and 4B are conceptual diagrams illustrating example graphical user interfaces of an example computing device that is configured to present a graphical keyboard with integrated search features, in accordance with one or more aspects of the present disclosure.FIGS. 4A and 4B illustrate, respectively, examplegraphical user interfaces devices FIGS. 1-3 , respectively.FIGS. 4A and 4B are described below in the context ofcomputing device 110 for ease of description only, and may be displayed by any computing device. -
Graphical user interface 414A ofFIG. 4A includes agraphical keyboard 416B includes anoutput region 416A, agraphical keyboard 416B, and atext edit region 416C.Output region 416A is a graphical user interface of an application. For example, as illustrated inFIG. 4A ,output region 416A is a graphical user interface of a messaging or chat application, and illustrates communications from the user of the computing device displayinggraphical user interface 414A (under “YOU”) and communications from a user of another computing device (under “FRIEND”). In other examples, output region may be another application user interface, such as a notes application, a web browser, a search application, a maps application, an email application, a text editor application, or any other application for whichgraphical keyboard 116B provides text input.Text edit region 416C is the region ofgraphical user interface 414A at which text is displayed after being input usinggraphical keyboard 416B prior to being committed tooutput region 416A. -
Graphical keyboard 416B may include multiple, different views. One view may be similar to or substantially the same asgraphical keyboard 116B illustrated inFIG. 1 . Other views may include a plurality of symbol keys, a plurality of extended symbol keys, a plurality of keys corresponding to letters or phonemes of a particular language, etc.FIG. 4A illustrates a view ofgraphical keyboard 416B in which at least a portion of the plurality of keys (in this example, all of the keys) has been replaced by at least a portion of one or more search results returned in response to a search query. -
Graphical keyboard 416B includessuggestion region 418B,search element 418C, andsearch results region 418D.Suggestion region 418B may be similar to or substantially the same assuggestion region 118B illustrated inFIG. 1 . For example,suggestion region 418B may alternately be configured to display predicted letters, words, or phrases; predicted search queries; input search queries; etc. In the view illustrated inFIG. 4A ,suggestion region 418B is configured to display an input search query, “How old is President Obama?” -
Search element 418C may be similar to or substantially the same assearch element 118C ofFIG. 1 .Search element 118C represents a selectable element ofgraphical keyboard 116B for invoking one or more of the various search features ofgraphical keyboard 116B, such as invoking the search mode. For example, in response to receiving an indication that a user may have selectedsearch element 418C,keyboard module 122 may configuresuggestion region 418B to present search queries input usinggraphical keys 118A (FIG. 1 ) instead of predicted characters, words or phrases or other primarily linguistic information thatkeyboard module 122 derives from a language model, lexicon, or dictionary. An example of this is illustrated inFIG. 4A , in which the search query “How old is President Obama?” is displayed atsuggestion region 418B. - Search results
region 418D displays a visual representation of one or more search result retrieved by computingdevice 110 in response to the search query. In some examples, the visual representation of the at least one search result may be a card-baseduser interface element 420A. Card-baseduser interface element 420A may appear similar to a notecard, and may include a visual representation of a particular search result of the one or more search results. The representation of the particular search result may include, for example, at least one of text, a picture, an icon, or the like. For example, card-baseduser interface element 420A includes text indicating an answer to the search query (“54 years” and “Barack Obama/Age”) and a picture related to the search query (such as a picture of President Obama). - In some examples, card-based
user interface element 420A includes a plurality of predetermined portions. Each predetermined portion of the plurality of predetermined portions may be associated with a different functionality provided by card-baseduser interface element 420A. For example, card-baseduser interface element 420A includes a firstpredetermined portion 422A and a secondpredetermined portion 422B. - The first
predetermined portion 422A of card-baseduser interface element 420A (the visual representation of a first search result) may include a representation of content of the particular search result, such as text, an image associated with the particular search result, an icon associated with the particular search result, etc. In the example ofFIG. 4A , the firstpredetermined portion 422A includes text indicating an answer to the search query (“54 years” and “Barack Obama/Age”) and an image related to the search query (such as a picture of President Obama). - In some examples,
keyboard module 122 may be configured to determine, based on an indication of user input atPSD 112 received from UI module 120 (e.g., an indication of a touch event), selection of firstpredetermined portion 422A. In response to determining the selection of firstpredetermined portion 422A,keyboard module 122 may be configured to automatically, without further user input, output an indication of information related to of the search result toUI module 120 and causeUI module 120 to insert the information related to the search result intext edit region 416C. The information related to the search result may include, for example, text, a hyperlink, an image, a video, an icon, etc. In this way,keyboard module 122 is configured to allow one-gesture input of information from the search result to thetext edit region 416C, enabling easy and efficient entry of information from the search result to the application for whichgraphical keyboard 416B is being used to input text. - The information related to the search result may include, for example, textual information related to the selected search result. For example, as shown in
FIG. 4A , the information related to the selected search result includes a text answer to the search query (“President Obama is 54 years old.”). In some examples, the text answer may be formatted as a natural language response to the search query. In the example ofFIG. 4A , the information related to the selected search result also includes a hyperlink to a website related to the selected search result, which may allow the user ofcomputing device 110 or the user of the other computing device (e.g., “FRIEND” inFIG. 4A ) to select the hyperlink to access more information related to the search result once the content intext edit region 416C is committed tooutput region 416A. In some examples, selection offirst portion 422A of card-baseduser interface element 420A may include a tap gesture in which a user tapsfirst portion 422A of card-baseduser interface element 420A, and in response,keyboard module 122 may be configured to insert a textual representation of the search result intext edit region 416C oroutput region 416A. - The information related to the selected search result also may include an image (such as an image of President Obama, an image of card-based
user interface element 420A, or an image of a portion of card-baseduser interface element 420A), an icon, etc. In some examples, the selection of thefirst portion 422A of card-baseduser interface element 420A may include a drag gesture in which the user is dragging card-baseduser interface element 420A to textedit region 416C oroutput region 416A. In response,keyboard module 122 may be configured to insert an image of card-baseduser interface element 420A or a portion of card-baseduser interface element 420A intext edit region 416C oroutput region 416A. - Card-based
user interface element 420A also includes secondpredetermined portion 422B, which is associated with an action performed by an application other thangraphical keyboard 416B. For example, secondpredetermined portion 422B includes at least one user interface element (e.g., icon, image, text etc.) associated with the action, which, when selected by a user, causesUI module 120 to output the touch event to another application module to perform the indicated action. The user interface element may include an element associated with applications including a web browser, a dedicated search application, a phone application, a maps application, etc. As a particular example, the user interface element may include a phone icon, with or without accompanying text (such as “Call”); a hyperlink; an icon representative of directions, with or without accompanying text (such as “Navigate”); a play button icon, with or without accompanying text (such as “Play”); etc. - In the example of
FIG. 4A , the user interface element includes go-toicon 424A. Go-toicon 424A is associated with an action of opening a new application related to the search result displayed in card-baseduser interface element 420A, such as a search application or a website from which the information in the search result was retrieved. In response to receiving, fromPSD 112, a touch event,UI module 120 may determine selection of the secondpredetermined portion 422B or go-toicon 424A, and may transmit an indication of the selection of secondpredetermined portion 422B to the associated application, which may perform the action associated with the secondpredetermined portion 422B or go-toicon 424A. In this way, in addition to allowing insertion of information related to the search result into the application for which the graphical keyboard is being used to input text,keyboard module 122 may present a user with shortcuts to actions performed by other applications. - In some examples,
computing device 110 may retrieve more than one search result in response to a search query.Keyboard module 122 may generate a respective card-based user interface element for each respective search result of the one or more search results. In some examples,keyboard module 122 may causesearch results region 418D to display more than one search result (e.g., more than one card-baseduser interface element 420A. For example, as shown inFIG. 4A ,keyboard module 122 may causesearch results region 418D to display first card-baseduser interface element 420A and a portion of a second card-baseduser interface element 420B. The display of the portion of second card-baseduser interface element 420B may visually indicate to a user ofcomputing device 110 that additional search results exist and may be viewed by navigating to the second card-baseduser interface element 420B. For example,keyboard module 122 may causeUI module 120 to switch from card-baseduser interface element 420A to second card-baseduser interface element 420B in response to determining a user gesture based on touch events, e.g., a swipe from right to left inFIG. 4A . In this way,keyboard module 122 may enable a user to switch among search results withinsearch results region 418D ofgraphical keyboard 116B. In some examples,keyboard module 122 may enable a user to switch among card-based user interface elements using swipes to different directions (e.g., right and left), and the card-based user interface elements may be conceptually arranged in a carousel. -
FIG. 4B illustrates an examplegraphical user interface 414B that includes a view ofgraphical keyboard 416B in which at least a portion of the plurality of keys (in this example, all of the keys) has been replaced by at least a portion of one or more search results returned in response to a search query. Similar tographical user interface 414A,graphical user interface 414B includesoutput region 416A,text edit region 416C, andgraphical keyboard 416B. - As in
FIG. 4A ,graphical keyboard 416B includessuggestion region 418B,search element 418C, andsearch results region 418D. In the view illustrated inFIG. 4B ,suggestion region 418B is configured to display an input search query, “Sandwich place”. - Search results
region 418D displays a visual representation of one or more search result retrieved by computingdevice 110 in response to the search query. In some examples, the visual representation of the at least one search result may be a card-baseduser interface element 420C. Card-baseduser interface element 420C may appear similar to a notecard, and may include a visual representation of a particular search result of the one or more search results. The representation of the particular search result may include, for example, at least one of text, a picture, an icon, or the like. For example, card-baseduser interface element 420C includes text indicating an answer to the search query. InFIG. 4B , the text indicating an answer to the search query includes a restaurant name (“Sandwich Place”), restaurant rating (5 stars), a number of reviews (“1,821”), an address (“123 Address Way”), a restaurant type (“Sandwiches”), and an image (such as a picture of Sandwich Place). - Similar to card-based
user interface element 420A, card-baseduser interface element 420C includes a firstpredetermined portion 422C and a secondpredetermined portion 422D. The firstpredetermined portion 422C of card-baseduser interface element 420C (the visual representation of a first search result) may include a representation of content of the particular search result, such as text, an image associated with the particular search result, an icon associated with the particular search result, etc. InFIG. 4B , the text indicating an answer to the search query includes a restaurant name (“Sandwich Place”), restaurant rating (5 stars), a number of reviews (“1,821”), an address (“123 Address Way”), a restaurant type (“Sandwiches”), and an image (such as a picture of Sandwich Place). - In some examples,
keyboard module 122 may be configured to determine, based on an indication of user input atPSD 112 received from UI module 120 (e.g., an indication of a touch event), selection of firstpredetermined portion 422C. In response to determining the selection of firstpredetermined portion 422C,keyboard module 122 may be configured to automatically, without further user input, output an indication of information related to of the search result toUI module 120 and causeUI module 120 to insert the information related to the search result intext edit region 416C. In this way,keyboard module 122 is configured to allow one-gesture input of information from the search result to thetext edit region 416C, enabling easy and efficient entry of information from the search result to the application for whichgraphical keyboard 416B is being used to input text. - The information related to the search result may include, for example, image or graphical information related to the selected search result. For example, as shown in
FIG. 4B , the information related to the selected search result includes avisual representation 426 of card-baseduser interface element 420C. In some examples, the selection of thefirst portion 422C of card-baseduser interface element 420C may include adrag gesture 428 in which the user is dragging card-baseduser interface element 420C to textedit region 416C oroutput region 416A. In response,keyboard module 122 may be configured to insertvisual representation 426 of card-baseduser interface element 420C or a visual representation of a portion of card-baseduser interface element 420C intext edit region 416C oroutput region 416A. - The information related to the search result may include, for example, textual information related to the selected search result. For example, the information related to the selected search result may include a text answer to the search query (“
Sandwich Place 123 Address Way”). In the example ofFIG. 4B , the information related to the selected search result also could include a hyperlink to a website that provides directions to Sandwich Place, which may allow the user ofcomputing device 110 or the user of the other computing device (e.g., “FRIEND” inFIG. 4B ) to select the hyperlink to access directions to Sandwich Place once the content intext edit region 416C is committed tooutput region 416A. In some examples, selection offirst portion 422C of card-baseduser interface element 420C may include a tap gesture in which a user tapsfirst portion 422C of card-baseduser interface element 420C, and in response,keyboard module 122 may be configured to insert a textual representation of the search result intext edit region 416C oroutput region 416A. - Card-based
user interface element 420C also includes secondpredetermined portion 422D, which is associated with one or more actions performed by an application other thangraphical keyboard 416B. For example, secondpredetermined portion 422D includes multiple user interface elements (e.g., icons, images, text etc.) associated with respective actions, which, when selected by a user, causesUI module 120 to output the touch event to another application module to perform the indicated respective action. - In the example of
FIG. 4B , the user interface elements includephone icon 424B,navigation icon 424C, and go-to icon 424D.Phone icon 424B is associated with an action of opening a phone application and calling Sandwich Place (i.e., the establishment associated with the search result).Navigation icon 424C is associated with an action of opening a navigation or maps application and retrieving directions to Sandwich Place (i.e., the establishment associated with the search result). Go-toicon 424D is associated with an action of opening a new application related to the search result displayed in card-baseduser interface element 420C, such as a search application or a website from which the information in the search result was retrieved. - In response to receiving, from
PSD 112, a touch event,UI module 120 orkeyboard module 122 may determine selection of one oficons 424B-424D in the secondpredetermined portion 422D, and may transmit an indication of the selection of the one oficons 424B-424D to the associated application, which may perform the action associated with the selected one oficons 424B-424D. In this way, in addition to allowing insertion of information related to the search result into the application for whichgraphical keyboard 416B is being used to input text,keyboard module 122 may present a user with shortcuts to actions performed by other applications. - In some examples,
computing device 110 may retrieve more than one search result in response to a search query.Keyboard module 122 may generate a respective card-based user interface element for each respective search result of the one or more search results. In some examples,keyboard module 122 may causesearch results region 418D to display more than one search result (e.g., more than one card-baseduser interface element 420C). For example, as shown inFIG. 4B ,keyboard module 122 may causesearch results region 418D to display first card-baseduser interface element 420C and a portion of a second card-baseduser interface element 420D. The display of the portion of second card-baseduser interface element 420D may visually indicate to a user ofcomputing device 110 that additional search results exist and may be viewed by navigating to the second card-baseduser interface element 420D. For example,keyboard module 122 may causeUI module 120 to switch from card-baseduser interface element 420C to second card-baseduser interface element 420D in response to determining a user gesture based on touch events, e.g., a swipe from right to left inFIG. 4B . In this way,keyboard module 122 may enable a user to switch among search results withinsearch results region 418D ofgraphical keyboard 116B. In some examples,keyboard module 122 may enable a user to switch among card-based user interface elements using swipes to different directions (e.g., right and left), and the card-based user interface elements may be conceptually arranged in a carousel. -
FIG. 5 is a flowchart illustrating example operations of a computing device that is configured to present a graphical keyboard with integrated search features, in accordance with one or more aspects of the present disclosure. The operations ofFIG. 5 may be performed by one or more processors of a computing device, such ascomputing devices 110 ofFIG. 1 orcomputing device 210 ofFIG. 2 . For purposes of illustration only,FIG. 5 is described below within the context ofcomputing devices 110 ofFIG. 1 . - The technique of
FIG. 5 includes outputting, by computingdevice 110, for display (e.g., at PSD 112), agraphical keyboard 116B comprising a plurality ofkeys 118A and asuggestion region 118B (502). For example,keyboard module 122 may output an indication of thegraphical keyboard 116B toUI module 120, which may causeUI module 120 to controlPSD 112 to output a graphical user interface includinggraphical keyboard 116B.Graphical keyboard 116B may include asearch element 118C in addition to plurality ofkeys 118A andsuggestion region 118B. - The technique of
FIG. 5 also includes determining, by computingdevice 110, based on a selection ofsuggestion region 118B or one or more keys from the plurality ofkeys 118A, a search query (504). In some examples,keyboard module 122 may causesuggestion region 118B to display a predicted or suggested search query. For example,keyboard module 122 may analyze a previous entry (e.g., message or sentence) made by a user usinggraphical keyboard 116B and determine a predicted or suggested search query based on the previous entry.Keyboard module 122 then may output an indication of the predicted or suggested search query toUI module 120, which may causeUI module 120 to controlPSD 112 to output a graphical user interface including the predicted or suggested search query insuggestion region 118B. - Alternatively,
keyboard module 122 may receive, fromUI module 120, an indication of a touch event atPSD 112 selectingsearch element 116C. In response,keyboard module 122 may output an message toUI module 120 that causesUI module 120 to controlPSD 112 to output a graphical user interface in whichsuggestion region 118B displays text entered by the user (made by selecting keys of plurality ofgraphical keys 118A) as a search query. -
Keyboard module 122 may receive fromUI module 120 an indication of a touch event indicating selection of a search query (e.g., the predicted or suggested search query insuggestion region 118B or the search query entered by user insuggestion region 118B), and may determine the search query based on this user input (touch event). - The technique of
FIG. 5 may further include retrieving, by computingdevice 110, one or more search results determined based on the search query (506). In some examples, retrieving, by computingdevice 110, one or more search results determined based on the search query (506) may include performing, by computingdevice 110, a search based on the search query. In some examples, retrieving, by computingdevice 110, one or more search results determined based on the search query (506) may include transmitting, by computingdevice 110, the search query to a data service, and receiving, by computingdevice 110, from the data service, one or more search results based on the search query. - Further, the technique of
FIG. 5 includes outputting, by computingdevice 110, in place of at least a portion ofgraphical keyboard 116B, a visual representation of a search result of the one or more search results (508). For example,keyboard module 122 may output an indication of a visual representation of the one or more search results that include one or more card-based user interface element, each card-based user interface element being associated with a respective search result of the one or more search results.Keyboard module 122 may output an indication of the visual representation toUI module 120, which causesUI module 120 to cause PSD to display a user interface in which the visual representation is displayed in place at least a portion ofgraphical keyboard 116B, such as in place of at least one ofgraphical keys 118A (e.g., in place of all ofgraphical keys 118A). - In some examples, the technique of
FIG. 5 optionally includes determining, by computingdevice 110, based on user input (e.g., at PSD 112), selection of a predetermined portion of the visual representation of the search result (510). For example,keyboard module 122 may receive, fromUI module 120, an indication of user input (e.g., a touch event) selecting a first portion of the visual representation of the search result. The first portion of the visual representation of the search result may include content of the search result, such as, for example, text, an image, an icon, a video, or the like. The technique ofFIG. 5 also optionally may include automatically, without further user input, inserting, by computingdevice 110, in atext edit region 116C displayed adjacent tographical keyboard 116B, information related to the search result (512). The information related to the search result may include, for example, text, an image, an icon, a video, or the like. In some examples, the information related to the search result may be substantially similar to the information represented in the first portion of the visual representation of the search result, although formatting of the information may be different when inserted in thetext edit region 116C than when displayed as part of the first predetermined portion of the visual representation of the search result. - In this way, the techniques of the disclosure may enable a
computing device 110 to provide agraphical keyboard 116B with integrated search features that include display of a visual representation of at least a portion of one or more search results in place of at least a portion of thegraphical keyboard 116B. Further, in some examples, the visual representation may include a card-based user interface element, andcomputing device 110 may be configured to output different card-based user interface elements, corresponding to different search results determined based on the search query, in response to receiving an indication of a user input, such as a swipe gesture. Additionally,computing device 110 may be configured to determine, based on user input, selection of a predetermined portion of the visual representation of the at least a portion (e.g., a search result) of the one or more search results and, automatically, without further user input, insert, intext edit region 116C displayed adjacent tographical keyboard 116B, information related to the at least a portion (e.g., a search result) of the one or more search results. - In this way,
computing device 110 may facilitate insertion of information from the search result in the application for whichgraphical keyboard 116B is providing input. Further, by inserting the information from the search result intext edit region 116C with a single user input,computing device 110 may allow the user to input information with fewer inputs than a typical, multi-gesture copy and paste action. In this way,computing device 110 may facilitate information retrieval using a search query and insertion of the retrieved information in an application without application switching and with few user inputs. - The following numbered clauses may illustrate one or more aspects of the disclosure:
-
Clause 1. A method comprising: outputting, by a computing device, for display, a graphical keyboard comprising a plurality of keys and a suggestion region; determining, by the computing device, based on a selection of the suggestion region or one or more keys from the plurality of keys, a search query; retrieving, by the computing device, one or more search results determined based on the search query; and outputting, by the computing device, in place of at least a portion of the graphical keyboard, a visual representation of a particular search result of the one or more search results. - Clause 2. The method of
clause 1, retrieving one or more search results determined based on the search query comprises: transmitting, by the computing device and to a data service, the search query; and receiving, by the computing device, from the data service, one or more search results determined based on the search query. - Clause 3. The method of
clause 1, wherein retrieving one or more search results determined based on the search query comprises performing, by the computing device, a search based on the search query to determine the one or more search results. - Clause 4. The method of
clause 1, further comprising: determining, by the computing device, based on user input, selection of a predetermined portion of the visual representation of the particular search result; and automatically, without further user input, inserting, by the computing device, in a text edit region displayed adjacent to the graphical keyboard, information related to the particular search result. - Clause 5. The method of clause 4, wherein the predetermined portion comprises a first predetermined portion and the user input comprises a first user input, further comprising: determining, by the computing device, based on a second user input, selection of a second predetermined portion of the visual representation of the particular search result; and automatically, without further user input, performing, by the computing device, a predetermined action associated with the second predetermined portion.
- Clause 6. The method of any one of
clauses 1 to 5, wherein: the particular search result comprises a first particular search result; the visual representation comprises a first visual representation; and the first visual representation comprises a first card-based user interface elements; the method further comprising: receiving, by the computing device, an indication of a swipe user input gesture; and responsive to receiving the indication of the swipe user input gesture, outputting, by the computing device, for display, an animation switching from the first card-based user interface element to a second card-based user interface element that is a second visual representation of a second particular search result of the one or more search results, wherein each of the first card-based user interface element and the second card-based user interface element are displayed in place of at least a portion of the graphical keyboard. - Clause 7. The method of any one of
clauses 1 to 6, wherein outputting the visual representation of the particular search result in place of at least a portion of the graphical keyboard comprising outputting a visual representation of the particular search result that replaces at least some keys of the plurality of keys. - Clause 8. The method of clause 4 or 5, wherein inserting, in the text edit region displayed adjacent to the graphical keyboard, the information related to the particular search result of the at least a portion of the one or more search results comprises inserting, in the text edit region displayed adjacent to the graphical keyboard, a natural language response to the search query.
- Clause 9. A computing device comprising: a presence-sensitive display; at least one processor; and a memory storing instructions that, when executed by the at least one processor, cause the at least one processor to: output, for display at the presence-sensitive display, a graphical keyboard comprising a plurality of keys and a suggestion region; determine, based on a selection of the suggestion region or one or more keys from the plurality of keys, a search query; retrieve one or more search results determined based on the search query; and output, in place of at least a portion of the graphical keyboard, a visual representation of a particular search result of the one or more search results.
- Clause 10. The computing device of clause 9, wherein the instructions that, when executed by the at least one processor, cause the at least one processor to retrieve the one or more search results determined based on the search query cause the at least one processor to: transmit, to a data service, the search query; and receive, from the data service, one or more search results determined based on the search query.
- Clause 11. The computing device of claim 9, wherein the instructions that, when executed by the at least one processor, cause the at least one processor to retrieve the one or more search results determined based on the search query cause the at least one processor to perform a search based on the search query to determine the one or more search results.
- Clause 12. The computing device of clause 9, wherein the memory further comprises instructions that, when executed by the at least one processor, cause the at least one processor to: determine, based on user input, selection of a predetermined portion of the visual representation of the particular search result; and automatically, without further user input, insert, in a text edit region displayed adjacent to the graphical keyboard, information related to the particular search result.
- Clause 13. The computing device of clause 12, wherein: the predetermined portion comprises a first predetermined portion and the user input comprises a first user input; and the memory further comprises instructions that, when executed by the at least one processor, cause the at least one processor to: determine, based on a second user input, selection of a second predetermined portion of the visual representation of the particular search result; and automatically, without further user input, perform a predetermined action associated with the second predetermined portion.
- Clause 14. The computing device of any one of clauses 9 to 13, wherein: the particular search result comprises a first particular search result; the visual representation comprises a first visual representation; the first visual representation comprises a first card-based user interface elements; and the memory further comprises instructions that, when executed by the at least one processor, cause the at least one processor to: receive, from the presence-sensitive display, an indication of a swipe user input gesture; and responsive to receiving the indication of the swipe user input gesture, output, for display at the presence-sensitive display, an animation switching from the first card-based user interface element to a second card-based user interface element that is a second visual representation of a second particular search result of the one or more search results, wherein each of the first card-based user interface element and the second card-based user interface element are displayed in place of at least a portion of the graphical keyboard.
- Clause 15. The computing device of any one of clauses 9 to 14, wherein the visual representation of the particular search result replaces at least some keys of the plurality of keys.
- Clause 16. The computing device of clause 12 or 13, wherein the information related to the particular search result comprises a natural language response to the search query.
- Clause 17. A computer-readable storage medium encoded with instructions that, when executed by at least one processor of a computing device, cause the at least one processor to: output, for display at a presence-sensitive display, a graphical keyboard comprising a plurality of keys and a suggestion region; determine, based on a selection of the suggestion region or one or more keys from the plurality of keys, a search query; retrieve one or more search results determined based on the search query; and output, in place of at least a portion of the graphical keyboard, a visual representation of a particular search result of the one or more search results.
- Clause 18. The computer-readable storage medium of clause 17, further comprising instructions that, when executed by the at least one processor, cause the at least one processor to: determine, based on user input, selection of a predetermined portion of the visual representation of the particular search result; and automatically, without further user input, insert, in a text edit region displayed adjacent to the graphical keyboard, information related to the particular search result.
- Clause 19. The computer-readable storage medium of clause 17 or 18, wherein: the predetermined portion comprises a first predetermined portion and the user input comprises a first user input; and the memory further comprises instructions that, when executed by the at least one processor, cause the at least one processor to: determine, based on a second user input, selection of a second predetermined portion of the visual representation of the particular search result; and automatically, without further user input, perform a predetermined action associated with the second predetermined portion.
- Clause 20. The computer-readable storage medium of any one of clauses 17 to 19, wherein: the particular search result comprises a first particular search result; the visual representation comprises a first visual representation; the first visual representation comprises a first card-based user interface elements; and further comprising instructions that, when executed by the at least one processor, cause the at least one processor to: receive, from the presence-sensitive display, an indication of a swipe user input gesture; and responsive to receiving the indication of the swipe user input gesture, output, for display at the presence-sensitive display, an animation switching from the first card-based user interface element to a second card-based user interface element that is a second visual representation of a second particular search result of the one or more search results, wherein each of the first card-based user interface element and the second card-based user interface element are displayed in place of at least a portion of the graphical keyboard.
- Clause 21. The computer-readable storage medium of any one of clauses 17 to 20, wherein the visual representation of the particular search result replaces at least some keys of the plurality of keys.
- Clause 22. The computer-readable storage medium of any one of clauses 18 to 21, wherein the information related to the particular search result comprises a natural language response to the search query.
- In one or more examples, the functions described may be implemented in hardware, software, firmware, or any combination thereof. If implemented in software, the functions may be stored on or transmitted over, as one or more instructions or code, a computer-readable medium and executed by a hardware-based processing unit. Computer-readable media may include computer-readable storage media, which corresponds to a tangible medium such as data storage media, or communication media including any medium that facilitates transfer of a computer program from one place to another, e.g., according to a communication protocol. In this manner, computer-readable media generally may correspond to (1) tangible computer-readable storage media, which is non-transitory or (2) a communication medium such as a signal or carrier wave. Data storage media may be any available media that can be accessed by one or more computers or one or more processors to retrieve instructions, code and/or data structures for implementation of the techniques described in this disclosure. A computer program product may include a computer-readable medium.
- By way of example, and not limitation, such computer-readable storage media can comprise RAM, ROM, EEPROM, CD-ROM or other optical disk storage, magnetic disk storage, or other magnetic storage devices, flash memory, or any other medium that can be used to store desired program code in the form of instructions or data structures and that can be accessed by a computer. Also, any connection is properly termed a computer-readable medium. For example, if instructions are transmitted from a website, server, or other remote source using a coaxial cable, fiber optic cable, twisted pair, digital subscriber line (DSL), or wireless technologies such as infrared, radio, and microwave, then the coaxial cable, fiber optic cable, twisted pair, DSL, or wireless technologies such as infrared, radio, and microwave are included in the definition of medium. It should be understood, however, that computer-readable storage media and data storage media do not include connections, carrier waves, signals, or other transient media, but are instead directed to non-transient, tangible storage media. Disk and disc, as used, includes compact disc (CD), laser disc, optical disc, digital versatile disc (DVD), floppy disk and Blu-ray disc, where disks usually reproduce data magnetically, while discs reproduce data optically with lasers. Combinations of the above should also be included within the scope of computer-readable media.
- Instructions may be executed by one or more processors, such as one or more digital signal processors (DSPs), general purpose microprocessors, application specific integrated circuits (ASICs), field programmable logic arrays (FPGAs), or other equivalent integrated or discrete logic circuitry. Accordingly, the term “processor,” as used may refer to any of the foregoing structure or any other structure suitable for implementation of the techniques described. In addition, in some aspects, the functionality described may be provided within dedicated hardware and/or software modules. Also, the techniques could be fully implemented in one or more circuits or logic elements.
- The techniques of this disclosure may be implemented in a wide variety of devices or apparatuses, including a wireless handset, an integrated circuit (IC) or a set of ICs (e.g., a chip set). Various components, modules, or units are described in this disclosure to emphasize functional aspects of devices configured to perform the disclosed techniques, but do not necessarily require realization by different hardware units. Rather, as described above, various units may be combined in a hardware unit or provided by a collection of interoperative hardware units, including one or more processors as described above, in conjunction with suitable software and/or firmware.
- Various examples have been described. These and other examples are within the scope of the following claims.
Claims (23)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/332,409 US9946773B2 (en) | 2016-04-20 | 2016-10-24 | Graphical keyboard with integrated search features |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/134,243 US9965530B2 (en) | 2016-04-20 | 2016-04-20 | Graphical keyboard with integrated search features |
US15/332,409 US9946773B2 (en) | 2016-04-20 | 2016-10-24 | Graphical keyboard with integrated search features |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/134,243 Continuation US9965530B2 (en) | 2016-04-20 | 2016-04-20 | Graphical keyboard with integrated search features |
Publications (2)
Publication Number | Publication Date |
---|---|
US20170308591A1 true US20170308591A1 (en) | 2017-10-26 |
US9946773B2 US9946773B2 (en) | 2018-04-17 |
Family
ID=57799935
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/134,243 Active 2037-01-11 US9965530B2 (en) | 2016-04-20 | 2016-04-20 | Graphical keyboard with integrated search features |
US15/332,409 Active US9946773B2 (en) | 2016-04-20 | 2016-10-24 | Graphical keyboard with integrated search features |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/134,243 Active 2037-01-11 US9965530B2 (en) | 2016-04-20 | 2016-04-20 | Graphical keyboard with integrated search features |
Country Status (2)
Country | Link |
---|---|
US (2) | US9965530B2 (en) |
WO (1) | WO2017184219A1 (en) |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11157145B2 (en) * | 2016-12-02 | 2021-10-26 | International Business Machines Corporation | Dynamic web actions palette |
US11435883B2 (en) * | 2018-07-10 | 2022-09-06 | Samsung Electronics Co., Ltd. | Electronic device, and method for controlling electronic device |
US20230153077A1 (en) * | 2021-11-12 | 2023-05-18 | Citrix Systems, Inc. | Keyboard accessible web page embedded interactive element |
Families Citing this family (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9684398B1 (en) | 2012-08-06 | 2017-06-20 | Google Inc. | Executing a default action on a touchscreen device |
US10489768B2 (en) * | 2015-12-30 | 2019-11-26 | Visa International Service Association | Keyboard application with third party engagement selectable items |
US10140017B2 (en) | 2016-04-20 | 2018-11-27 | Google Llc | Graphical keyboard application with integrated search |
US10305828B2 (en) | 2016-04-20 | 2019-05-28 | Google Llc | Search query predictions by a keyboard |
US9965530B2 (en) | 2016-04-20 | 2018-05-08 | Google Llc | Graphical keyboard with integrated search features |
US10078673B2 (en) | 2016-04-20 | 2018-09-18 | Google Llc | Determining graphical elements associated with text |
US10222957B2 (en) | 2016-04-20 | 2019-03-05 | Google Llc | Keyboard with a suggested search query region |
US10664157B2 (en) | 2016-08-03 | 2020-05-26 | Google Llc | Image search query predictions by a keyboard |
US20200057541A1 (en) * | 2017-12-22 | 2020-02-20 | Google Llc | Dynamically generated task shortcuts for user interactions with operating system user interface elements |
US10956507B2 (en) * | 2018-08-16 | 2021-03-23 | Rovi Guides, Inc. | Reaction compensated result selection |
Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20170102871A1 (en) * | 2015-10-12 | 2017-04-13 | Microsoft Technology Licensing, Llc | Multi-window keyboard |
Family Cites Families (70)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6104397A (en) | 1997-06-30 | 2000-08-15 | Sun Microsystems, Inc. | Method and system for generating improved progress indicators |
US7599847B2 (en) * | 2000-06-09 | 2009-10-06 | Airport America | Automated internet based interactive travel planning and management system |
US20090006543A1 (en) * | 2001-08-20 | 2009-01-01 | Masterobjects | System and method for asynchronous retrieval of information based on incremental user input |
US8028250B2 (en) * | 2004-08-31 | 2011-09-27 | Microsoft Corporation | User interface having a carousel view for representing structured data |
US7788248B2 (en) | 2005-03-08 | 2010-08-31 | Apple Inc. | Immediate search feedback |
US7590699B2 (en) | 2005-06-23 | 2009-09-15 | Microsoft Corporation | Instant messaging with built-in search |
US7676517B2 (en) | 2005-10-14 | 2010-03-09 | Microsoft Corporation | Search results injected into client applications |
US7925716B2 (en) | 2005-12-05 | 2011-04-12 | Yahoo! Inc. | Facilitating retrieval of information within a messaging environment |
US20070300177A1 (en) | 2006-06-23 | 2007-12-27 | Microsoft Corporation | User interface for specifying multi-valued properties |
WO2008011454A2 (en) * | 2006-07-18 | 2008-01-24 | Chacha Search, Inc. | Anonymous search system using human searchers |
US8564544B2 (en) * | 2006-09-06 | 2013-10-22 | Apple Inc. | Touch screen device, method, and graphical user interface for customizing display of content category icons |
US20080201434A1 (en) | 2007-02-16 | 2008-08-21 | Microsoft Corporation | Context-Sensitive Searches and Functionality for Instant Messaging Applications |
US8745168B1 (en) * | 2008-07-10 | 2014-06-03 | Google Inc. | Buffering user interaction data |
US10705692B2 (en) * | 2009-05-21 | 2020-07-07 | Sony Interactive Entertainment Inc. | Continuous and dynamic scene decomposition for user interface |
US9116615B2 (en) | 2009-10-13 | 2015-08-25 | Blackberry Limited | User interface for a touchscreen display |
US20110112824A1 (en) | 2009-11-06 | 2011-05-12 | Craig Peter Sayers | Determining at least one category path for identifying input text |
US20110191321A1 (en) | 2010-02-01 | 2011-08-04 | Microsoft Corporation | Contextual display advertisements for a webpage |
US9129012B2 (en) | 2010-02-03 | 2015-09-08 | Google Inc. | Information search system with real-time feedback |
US8650210B1 (en) | 2010-02-09 | 2014-02-11 | Google Inc. | Identifying non-search actions based on a search query |
US8782556B2 (en) * | 2010-02-12 | 2014-07-15 | Microsoft Corporation | User-centric soft keyboard predictive technologies |
US8621379B2 (en) * | 2010-03-12 | 2013-12-31 | Apple Inc. | Device, method, and graphical user interface for creating and using duplicate virtual keys |
US9483175B2 (en) * | 2010-07-26 | 2016-11-01 | Apple Inc. | Device, method, and graphical user interface for navigating through a hierarchy |
US8918734B2 (en) | 2010-07-28 | 2014-12-23 | Nuance Communications, Inc. | Reduced keyboard with prediction solutions when input is a partial sliding trajectory |
AU2011291544B2 (en) | 2010-08-19 | 2015-03-26 | Google Llc | Predictive query completion and predictive search results |
CN103348353B (en) * | 2010-10-06 | 2016-07-06 | 西里克斯系统公司 | Resource access is carried out intermediary's adjustment by the physical location based on mobile equipment |
US10346479B2 (en) | 2010-11-16 | 2019-07-09 | Microsoft Technology Licensing, Llc | Facilitating interaction with system level search user interface |
US8515984B2 (en) | 2010-11-16 | 2013-08-20 | Microsoft Corporation | Extensible search term suggestion engine |
EP2641145A4 (en) | 2010-11-20 | 2017-05-03 | Nuance Communications, Inc. | Systems and methods for using entered text to access and process contextual information |
US9104992B2 (en) * | 2010-12-17 | 2015-08-11 | Microsoft Technology Licensing, Llc | Business application publication |
US9111238B2 (en) * | 2010-12-17 | 2015-08-18 | Microsoft Technology Licensing, Llc | Data feed having customizable analytic and visual behavior |
US10642934B2 (en) | 2011-03-31 | 2020-05-05 | Microsoft Technology Licensing, Llc | Augmented conversational understanding architecture |
US20120256840A1 (en) * | 2011-04-10 | 2012-10-11 | Mahmoud Razzaghi | Virtual keyboard |
US9176944B1 (en) | 2011-08-23 | 2015-11-03 | Google Inc. | Selectively processing user input |
CN102955658B (en) * | 2011-08-25 | 2017-03-22 | 腾讯科技（深圳）有限公司 | Device and method for page switching in interaction interface |
WO2013054957A1 (en) | 2011-10-13 | 2013-04-18 | Lg Electronics Inc. | Input interface controlling apparatus and method thereof |
US9652448B2 (en) | 2011-11-10 | 2017-05-16 | Blackberry Limited | Methods and systems for removing or replacing on-keyboard prediction candidates |
US8914451B2 (en) * | 2012-02-17 | 2014-12-16 | Blackberry Limited | Electronic device configured with messaging composition interface |
US9310888B2 (en) | 2012-03-16 | 2016-04-12 | Microsoft Technology Licensing, Llc | Multimodal layout and rendering |
US9685160B2 (en) | 2012-04-16 | 2017-06-20 | Htc Corporation | Method for offering suggestion during conversation, electronic device using the same, and non-transitory storage medium |
US20130285916A1 (en) | 2012-04-30 | 2013-10-31 | Research In Motion Limited | Touchscreen keyboard providing word predictions at locations in association with candidate letters |
US9460237B2 (en) | 2012-05-08 | 2016-10-04 | 24/7 Customer, Inc. | Predictive 411 |
US8484573B1 (en) | 2012-05-23 | 2013-07-09 | Google Inc. | Predictive virtual keyboard |
US9582146B2 (en) | 2012-05-29 | 2017-02-28 | Nokia Technologies Oy | Causing display of search results |
US9116552B2 (en) | 2012-06-27 | 2015-08-25 | Blackberry Limited | Touchscreen keyboard providing selection of word predictions in partitions of the touchscreen keyboard |
US20140115070A1 (en) | 2012-10-22 | 2014-04-24 | Nokia Corporation | Apparatus and associated methods |
US9305114B2 (en) | 2012-12-17 | 2016-04-05 | Microsoft Technology Licensing, Llc | Building long search queries |
US10228819B2 (en) * | 2013-02-04 | 2019-03-12 | 602531 British Cilumbia Ltd. | Method, system, and apparatus for executing an action related to user selection |
WO2014139120A1 (en) | 2013-03-14 | 2014-09-18 | Microsoft Corporation | Search intent preview, disambiguation, and refinement |
US20140282203A1 (en) | 2013-03-15 | 2014-09-18 | Research In Motion Limited | System and method for predictive text input |
US9529856B2 (en) * | 2013-06-03 | 2016-12-27 | Google Inc. | Query suggestion templates |
US9449079B2 (en) | 2013-06-28 | 2016-09-20 | Yandex Europe Ag | Method of and system for displaying a plurality of user-selectable refinements to a search query |
US9461945B2 (en) | 2013-10-18 | 2016-10-04 | Jeffrey P. Phillips | Automated messaging response |
KR102157264B1 (en) | 2013-10-30 | 2020-09-17 | 삼성전자주식회사 | Display apparatus and UI providing method thereof |
KR20150081181A (en) | 2014-01-03 | 2015-07-13 | 삼성전자주식회사 | Display apparatus and Method for providing recommendation characters thereof |
KR102225031B1 (en) * | 2014-01-14 | 2021-03-09 | 엘지전자 주식회사 | Terminal and operating method thereof |
US20150242086A1 (en) * | 2014-02-21 | 2015-08-27 | Markport Limited | Drag and drop event system and method |
US9471570B2 (en) * | 2014-04-30 | 2016-10-18 | Excalibur Ip, Llc | Method and system for user selection of query suggestions |
KR20150126213A (en) | 2014-05-02 | 2015-11-11 | 삼성전자주식회사 | System and method for searching infomation |
KR102177607B1 (en) * | 2014-05-16 | 2020-11-11 | 엘지전자 주식회사 | Mobile terminal and method for controlling the same |
US9671956B2 (en) | 2014-06-18 | 2017-06-06 | Lenovo Enterprise Solutions (Singapore) Pte. Ltd. | Presenting search term suggestions on graphical user interfaces |
US9930167B2 (en) * | 2014-07-07 | 2018-03-27 | Verizon Patent And Licensing Inc. | Messaging application with in-application search functionality |
US20160034977A1 (en) | 2014-08-01 | 2016-02-04 | Yahoo! Inc. | System and method for embedded search within messaging applications |
US10275152B2 (en) | 2014-10-28 | 2019-04-30 | Idelan, Inc. | Advanced methods and systems for text input error correction |
US20160224524A1 (en) | 2015-02-03 | 2016-08-04 | Nuance Communications, Inc. | User generated short phrases for auto-filling, automatically collected during normal text use |
US10547571B2 (en) * | 2015-05-06 | 2020-01-28 | Kakao Corp. | Message service providing method for message service linked to search service and message server and user terminal to perform the method |
US10222957B2 (en) | 2016-04-20 | 2019-03-05 | Google Llc | Keyboard with a suggested search query region |
US10305828B2 (en) | 2016-04-20 | 2019-05-28 | Google Llc | Search query predictions by a keyboard |
US9965530B2 (en) | 2016-04-20 | 2018-05-08 | Google Llc | Graphical keyboard with integrated search features |
US10140017B2 (en) | 2016-04-20 | 2018-11-27 | Google Llc | Graphical keyboard application with integrated search |
US10078673B2 (en) | 2016-04-20 | 2018-09-18 | Google Llc | Determining graphical elements associated with text |
-
2016
- 2016-04-20 US US15/134,243 patent/US9965530B2/en active Active
- 2016-10-24 US US15/332,409 patent/US9946773B2/en active Active
- 2016-12-29 WO PCT/US2016/069170 patent/WO2017184219A1/en active Application Filing
Patent Citations (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20170102871A1 (en) * | 2015-10-12 | 2017-04-13 | Microsoft Technology Licensing, Llc | Multi-window keyboard |
Non-Patent Citations (2)
Title |
---|
MS Hub (02/23/2016): https://blogs.microsoft.com/firehose/2016/02/23/hub-keyboard-app-from-microsoft-garage-makes-it-easy-to-multitask-from-one-mobile-screen/#sm.0003gjoqg1af9fnhzei1w2hq0ii8r "Hub Keyboard, a Microsoft Garage Project" (Page 1) * |
StackExchange (2013): https://ux.stackexchange.com/questions/40104/how-to-provide-autocomplete-and-autosuggest-on-the-same-search-box-at-the-same-t; "How to provide autocomplete and autosuggest on the same search box at the same time" (pages 1-2). * |
Cited By (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11157145B2 (en) * | 2016-12-02 | 2021-10-26 | International Business Machines Corporation | Dynamic web actions palette |
US11435883B2 (en) * | 2018-07-10 | 2022-09-06 | Samsung Electronics Co., Ltd. | Electronic device, and method for controlling electronic device |
US20230153077A1 (en) * | 2021-11-12 | 2023-05-18 | Citrix Systems, Inc. | Keyboard accessible web page embedded interactive element |
US11720331B2 (en) * | 2021-11-12 | 2023-08-08 | Citrix Systems, Inc. | Keyboard accessible web page embedded interactive element |
Also Published As
Publication number | Publication date |
---|---|
US9946773B2 (en) | 2018-04-17 |
WO2017184219A1 (en) | 2017-10-26 |
US20170308586A1 (en) | 2017-10-26 |
US9965530B2 (en) | 2018-05-08 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10140017B2 (en) | Graphical keyboard application with integrated search | |
US9977595B2 (en) | Keyboard with a suggested search query region | |
EP3479213B1 (en) | Image search query predictions by a keyboard | |
US9946773B2 (en) | Graphical keyboard with integrated search features | |
US9720955B1 (en) | Search query predictions by a keyboard | |
CN108700951B (en) | Iconic symbol search within a graphical keyboard | |
EP3400539B1 (en) | Determining graphical elements associated with text | |
US20170308290A1 (en) | Iconographic suggestions within a keyboard | |
US20180173692A1 (en) | Iconographic symbol predictions for a conversation | |
US20190034080A1 (en) | Automatic translations by a keyboard |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:MOHSIN, MUHAMMAD;GUPTA, PRANAY;BURKS, MICHAEL;SIGNING DATES FROM 20160420 TO 20160504;REEL/FRAME:040103/0807 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044567/0001Effective date: 20170929 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |