CN117581231A - Dataset refinement using machine translation quality prediction - Google Patents
Dataset refinement using machine translation quality prediction Download PDFInfo
- Publication number
- CN117581231A CN117581231A CN202180100202.1A CN202180100202A CN117581231A CN 117581231 A CN117581231 A CN 117581231A CN 202180100202 A CN202180100202 A CN 202180100202A CN 117581231 A CN117581231 A CN 117581231A
- Authority
- CN
- China
- Prior art keywords
- translation
- machine translation
- model
- output
- feature
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000013519 translation Methods 0.000 title claims abstract description 278
- 239000013598 vector Substances 0.000 claims abstract description 32
- 238000013528 artificial neural network Methods 0.000 claims abstract description 12
- 238000000605 extraction Methods 0.000 claims abstract description 6
- 238000000034 method Methods 0.000 claims description 55
- 238000012549 training Methods 0.000 claims description 34
- 230000015654 memory Effects 0.000 claims description 16
- 230000001537 neural effect Effects 0.000 claims description 14
- 238000001914 filtration Methods 0.000 claims description 7
- 239000003550 marker Substances 0.000 claims description 7
- 230000000052 comparative effect Effects 0.000 claims description 5
- 238000005516 engineering process Methods 0.000 abstract description 9
- 230000014616 translation Effects 0.000 description 216
- 238000013459 approach Methods 0.000 description 10
- 238000011156 evaluation Methods 0.000 description 7
- 230000009286 beneficial effect Effects 0.000 description 6
- 230000001419 dependent effect Effects 0.000 description 5
- 230000008569 process Effects 0.000 description 5
- 238000004891 communication Methods 0.000 description 3
- PCHJSUWPFVWCPO-UHFFFAOYSA-N gold Chemical compound [Au] PCHJSUWPFVWCPO-UHFFFAOYSA-N 0.000 description 3
- 239000010931 gold Substances 0.000 description 2
- 229910052737 gold Inorganic materials 0.000 description 2
- 238000010801 machine learning Methods 0.000 description 2
- 238000012545 processing Methods 0.000 description 2
- 238000001303 quality assessment method Methods 0.000 description 2
- 230000002441 reversible effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 238000003646 Spearman's rank correlation coefficient Methods 0.000 description 1
- 230000004931 aggregating effect Effects 0.000 description 1
- 230000003190 augmentative effect Effects 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 229910002056 binary alloy Inorganic materials 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 238000010586 diagram Methods 0.000 description 1
- 238000011478 gradient descent method Methods 0.000 description 1
- 238000005065 mining Methods 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 238000012805 post-processing Methods 0.000 description 1
- 238000007781 pre-processing Methods 0.000 description 1
- 238000013442 quality metrics Methods 0.000 description 1
- 230000000306 recurrent effect Effects 0.000 description 1
- 238000007670 refining Methods 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 230000000717 retained effect Effects 0.000 description 1
- 238000012552 review Methods 0.000 description 1
- 230000035945 sensitivity Effects 0.000 description 1
- 230000011664 signaling Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000008685 targeting Effects 0.000 description 1
- 238000012360 testing method Methods 0.000 description 1
- 238000012795 verification Methods 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/58—Use of machine translation, e.g. for multi-lingual retrieval, for server-side translation for client devices or for real-time translation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/51—Translation evaluation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/205—Parsing
- G06F40/211—Syntactic parsing, e.g. based on context-free grammar [CFG] or unification grammars
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/284—Lexical analysis, e.g. tokenisation or collocates
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/42—Data-driven translation
- G06F40/44—Statistical methods, e.g. probability models
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
- G06N3/0442—Recurrent networks, e.g. Hopfield networks characterised by memory or gating, e.g. long short-term memory [LSTM] or gated recurrent units [GRU]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
- G06N3/0455—Auto-encoder networks; Encoder-decoder networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/0499—Feedforward networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
- G06N3/0895—Weakly supervised learning, e.g. semi-supervised or self-supervised learning
Abstract
Various aspects of the technology employ a Machine Translation Quality Prediction (MTQP) model to refine the dataset used to train the machine translation system. This includes receiving a source sentence and a pair of sentences of the translation output by a machine translation quality prediction model (802). Feature extraction is then performed on the sentence pairs using a set of two or more feature extractors, wherein each feature extractor generates a corresponding feature vector (804). Corresponding feature vectors from the feature extractor set are concatenated together (806). And the concatenated feature vector is applied to a feedforward neural network, wherein the feedforward neural network generates a machine translation quality prediction score for the translation output (808).
Description
Background
Machine-based translation may be used to translate text from one language to another. Machine translation quality estimation or prediction involves evaluating the output of a machine translation system without accessing a "golden" tag sequence. The machine translation model may be trained with large parallel data sets, such as millions (or more) of sentence pairs. However, the real world data set may contain a large amount of noise data. The use of such data by machine translation models can produce poor training results, which in turn can lead to poor quality translations.
Disclosure of Invention
Aspects of the technology employ a Machine Translation Quality Prediction (MTQP) model to refine a dataset for training a machine translation system. The MTQP model is configured to provide an indication of the quality of sentence pairs. Given a large dataset containing sentence pairs (e.g., hundreds of thousands, millions, or billions of sentence pairs) from a real-world dataset, the MTQP model assigns a score to each sentence. The model marks low scoring pairs below a selected threshold. The resulting high quality dataset pairs can then be used to train various types of machine translation models, such as Neural Machine Translation (NMT) models. Thus, example implementations are directed to particular technology implementations of a machine translation training system that filters training data using an MTQP model, and then trains a machine translation model using the filtered training data.
According to one aspect of the technology, a computer-implemented method includes: receiving a source sentence and a sentence pair output by translation through a machine translation quality prediction model; performing feature extraction on the sentence pair using a set of two or more feature extractors, each feature extractor generating a corresponding feature vector; cascading corresponding feature vectors from the feature extractor set together; and applying the concatenated feature vector to a feedforward neural network that generates a machine translation quality prediction score for the translation output.
In one example, the method further includes storing the machine translation quality prediction score in a database in association with the translation output. In another example, the method further includes sending the machine translation quality prediction score to a user. In either case, the set of two or more feature extractors may include at least two of: a quasi-machine translation (quasi-MT) feature extractor, a neural machine translation feature extractor, a language model extractor, and a LogPr feature extractor. The quasi-MT feature extractor may use the internal score of a quasi-MT model that is trained by using information in both the source sentence and the golden sentence to attempt to predict each marker in the golden sentence. The neural machine translation feature extractor may use an internal score from at least one decoder of the neural machine translation model. The language model extractor may use internal scores from both language models. Here, a first one of the language models is trained on a selected corpus of source language, and a second one of the language models is a comparative language model that is first trained on the selected corpus and then incrementally trained on the corpus formed by source sentences in the training sentence pair set.
In a further example, the method further includes determining whether the machine translation quality prediction score exceeds a quality threshold, and filtering the translation output when the machine translation quality prediction score does not exceed the quality threshold. Filtering the translation output may include storing a flag with the translation output to indicate that the machine translation quality prediction score does not exceed a quality threshold. Filtering the translation output includes removing the translation output from the corpus of translation output sentences.
In another example, the method further includes determining whether the machine translation quality prediction score exceeds a quality threshold, and adding the translation output to a corpus of translation output sentences when the machine translation quality prediction score exceeds the quality threshold. In yet another example, the method further includes training a machine translation model using the translation output when the machine translation quality prediction score exceeds a quality threshold.
In another example, the method further includes creating a culled dataset of source sentences and corresponding translation outputs, wherein each translation output exceeds a quality threshold, and then training a machine translation model using the culled dataset. The trained machine translation model may be a neural machine translation model.
According to another aspect of the technology, a system is provided that includes a memory configured to store machine translation quality prediction information and one or more processors operatively coupled to the memory. The one or more processors are configured to implement a machine translation quality prediction model by: receiving a source sentence and a sentence pair output by translation; performing feature extraction on the sentence pairs using a set of two or more feature extractors, each feature extractor generating a corresponding feature vector; performing concatenation of corresponding feature vectors from the set of feature extractors; and applying the concatenated feature vector to a feedforward neural network, wherein the feedforward neural network is configured to generate a machine translation quality prediction score for the translation output.
In one example, the set of two or more feature extractors includes at least two of: a quasi-machine translation (quasi-MT) feature extractor, a neural machine translation feature extractor, a language model extractor, and a LogPr feature extractor.
In another example, the one or more processors are further configured to: a determination is made as to whether the machine translation quality prediction score exceeds a quality threshold, and the translation output is filtered when the machine translation quality prediction score does not exceed the quality threshold. The one or more processors are further configured to filter the translation output by storing a flag with the translation output to indicate that the machine translation quality prediction score does not exceed the quality threshold. The one or more processors are further configured to: determining whether the machine translation quality prediction score exceeds a quality threshold; and adding the translation output to the corpus of translation output sentences when the machine translation quality prediction score exceeds the quality threshold. The one or more processors are further configured to train the machine translation model using the translation output when the machine translation quality prediction score exceeds a quality threshold. And the one or more processors may be further configured to: creating a carefully selected dataset of source sentences and corresponding translation outputs, wherein each translation output exceeds a quality threshold; storing the beneficiated data set in a memory; and training a machine translation model using the beneficiated data set.
Drawings
FIG. 1 illustrates an example set of scenarios of machine translation configurations in accordance with aspects of the present technique.
FIG. 2A illustrates an example quasi-machine translation model in accordance with aspects of the present technique.
Fig. 2B illustrates an example quality estimation architecture in accordance with aspects of the present technique.
FIG. 3 illustrates a general model approach in accordance with aspects of the present technique.
FIG. 4 illustrates an example method for projecting source sentences and translation output onto feature vectors in accordance with aspects of the present technique.
FIG. 5 illustrates an example model structure for generating a predictive quality score in accordance with aspects of the present technique.
FIG. 6 illustrates a model workflow in accordance with aspects of the present technique.
Fig. 7A-7B illustrate a system for use with aspects of the technology.
Fig. 8 illustrates a method in accordance with aspects of the present technique.
Detailed Description
SUMMARY
Machine Translation Quality Prediction (MTQP), also known as Machine Translation Quality Estimation (MTQE), is intended to evaluate the output of a machine translation system without reference translation. For example, given a source sentence (sentence in the source language) and a translation output (sentence generated by the machine translation system), it is beneficial to be able to predict the quality score of the translation even if the machine translation system or the golden sentence (e.g., a human generated reference translation sentence) is not known. In particular, the MTQP predicts whether the translation output matches the meaning of the source sentence and whether the destination sentence is fluent.
Different metrics may be used to evaluate the quality of machine translation. For example, a BLEU score based on n-gram accuracy may be employed. Here, the BLEU score may be calculated between the translation output and the gold sentence. Parallel corpora containing source sentences and corresponding gold-labeled sentences can be used to evaluate translation quality. For example, the BLEU index may be averaged over a corpus to provide an indication of how well the machine translation system is trained. MTQP may be used to evaluate the quality of a particular translation output for a given source sentence. Calculating the average MTQP for the entire corpus may not be meaningful compared to BLEU, as MTQP is advantageous for marking low quality translation outputs before the next post-editing.
For example, it may be beneficial for an application service provider or other customer to receive a confidence score (quality assessment) along with the translation. This score can be used to determine whether the machine translation can be used directly without or how much post-editing is required. Since manual post editing may be the most important cost in a local workflow, confidence score features are important to reduce cost and provide additional information about translation quality.
For situations where post editing may be required, MTQP allows the expert to focus on translations that are estimated to be low quality, further reducing post editing costs. For example, a service provider may translate 1000 tens of thousands of sentences using a given machine translation system while also desiring to ensure that all translations are good, e.g., have at least some threshold quality. For example, the quality threshold may be reached with only the first 30-40% translations (or more or less). In terms of manpower and/or computing resources, there may be a significant cost factor to examine all 1000 tens of thousands of sentences and make post-editing. However, if a quality assessment (QE) score is provided with the translation, a threshold may be set for the translation to be reviewed. Here, for example, the service provider may select only the lowest 10000 sentences (or set the threshold QE score) and send those sentences below the threshold to the expert for post-editing. In this case, the costs associated with post-editing can be reduced by 99.9% compared to post-editing evaluation of the entire translation set.
It is also possible that no post editing is required, but a fast turnaround time is required. In this case, it may be particularly beneficial for the service provider to only publish translations of high quality. Here, lower quality translations below the threshold may be maintained as the source language. In this case, the MTQP score may provide a reliable indicator for picking (or selecting) only high quality translated sentences.
FIG. 1 illustrates a set of examples 100 of different scenarios with different methods of processing translations. For example, while the system may employ only Quality Estimation (QE), it may alternatively use machine translation plus QE, as shown in block 102. Here, for each source sentence provided (e.g., received from an application service provider or other customer), the system generates a translation sentence in the selected language, as well as a QE score.
As shown at the level below block 102, the system may use generic or custom QEs, as well as generic or custom machine translations. For generic QEs, pairs of source sentences and translation outputs are provided, and a generic QE model (e.g., trained by some generic data of the system markup) is used to predict the quality scores of each sentence pair. For custom QEs, a dataset is provided that includes [ source sentence, translation output, quality tag ] pairs. Here, the system trims or retrains the QE model based on the [ source sentence, translation output ]. In this case, a custom QE model can be used to predict the quality score for each sentence pair. For general machine translation, a Neural Machine Translation (NMT) model uses received source sentences to generate translated sentences. And for custom machine translation, the system employs parallel corpora. Here, the system fine-tunes the NMT model to derive a custom machine translation model. The source sentence is applied to a custom machine translation model to generate a translated sentence.
Block 104 illustrates a configuration for general machine translation using general QEs. Block 106 shows a configuration for custom machine translation using generic QEs. Block 108 illustrates a configuration for universal machine translation using custom QEs. And block 110 illustrates a configuration for custom machine translation using custom QEs. Each of these configurations may be adapted to the needs of different customers, e.g. depending on whether the customers have the ability to provide their own data quality markers and/or their own data sets.
Block 112 shows the option of tagging data by a user (e.g., a client) and block 114 shows the option of tagging data by a system pipeline. For the user tagged data of block 112, the user is responsible for selecting which sentences in the dataset to tag, and for tagging the QE score. This approach allows users to design their own tagging rules and/or follow the guidelines of the MTQP system. For tagging according to the system pipeline as in block 114, the pipeline may be used to tag not only the generic QE data to train the generic QE model, but also the translated output of the user data. Furthermore, custom QEs may not be required for satisfactory applications of the generic QE method. In contrast, custom qe+ custom machine translation methods may be most appropriate in applications where data may be field-specific, such as for movies or other videos.
Fig. 2A shows an example 200 of a quasi-machine translation (quasi-MT) model of a converter for an encoder 202 and a decoder 204. As shown, a source sentence (e.g., "He driven to eat") is input to an embedded box 206, which embedded box 206 is fed to the encoder 202. Training may be accomplished using parallel data sets. The encoder 202 operates on the data received from the embedding block 206 and adds the outputs together (along with the output of the null block from the decoder 204) at 208. The embedding block 206 embeds each word into a vector (e.g., 1024-dimensional vector) in the embedding space. The empty block may act as a placeholder or starting symbol because there are no previous words that can be used when the model attempts to predict the first word in the translation. The added output at 208 is fed to the decoder 204 and the output from the decoder 204 is fed to a soft maximum box (softmax) 210. The soft max box 210 is configured to assign a fractional (e.g., decimal) probability to a possible output. The probability must be 1.0 in addition.
For example, the quasi-MT model 200 is trained to predict each tag based on the source sentence from the translation output. Given the source sentence [ a, B, C ] and the translated sentence [ a, B, C, D ], the quasi-MT model 200 predicts each tag in the translated sentence based on the "bi-directional information in the source sentence + the translated sentence". More specifically, in this example, the model attempts to predict "A" based on [ a, B, C ] and [ B, C, D ]; predicting "B" based on [ a, B, C ] and [ A, C, D ]; predicting "C" based on [ a, B, C ] and [ A, B, D ]; and predicting "D" based on [ a, B, C ] and [ A, B, C ]. These predictions were made in parallel (independent of each other).
FIG. 2B illustrates an example 200 of a Bi-directional LSTM (Bi-LSTM) type Recurrent Neural Network (RNN) for a QE model, in which features generated by the quasi-machine translation module 200 are applied. These features may be generated by other feature extractors, such as an NMT feature extractor (see 504b in fig. 5). Each feature extractor has its own LSTM to process the internal score and produce a fixed length feature vector. The feature vectors are then concatenated. For example, the QE data from the soft max box 210 is applied to the hidden layer of the bi-directional LSTM for final prediction. The bi-directional LSTM may be trained with a feed forward neural network (see 508 in fig. 5) and may also fine tune the feature extractor.
The data used to train the quasi-MT model may be a large parallel corpus that is a [ source sentence, golden sentence ] pair. For training the QE model, the data may be MTQP data, i.e., a [ source sentence, translation output, quality tag ] pair.
Training data set
For any machine learning problem with sentence pairs as input, such as text implications, semantic similarity, etc., the MTQP method discussed herein may be used as the feature score for the input. Notably, the effectiveness of machine translation systems can be limited by the quality of the data used to train the machine translation model. In particular, for models trained on sentence pairs, the performance of a given model may be highly dependent on the quality of the dataset. A large number of sentence pairs (e.g., hundreds of thousands, millions, or more) may be collected, all sentence pairs may then be scored using the MTQP service, and only high quality sentence pairs may be retained in the dataset. However, in some cases, it is beneficial to avoid using the same MTQP service on the trained machine translation model to avoid bias.
MTQP model structure
As discussed herein, the MTQP model takes sentence pairs as input and returns (predicted) scores as output. Fig. 3 shows a general overview 300 of the model process. Sentence pairs are provided, as shown in block 302. The sentence pair may come from pre-existing (legacy) data, such as data previously collected for the application. For example, sentence pairs may be sampled from a mix of translation and network source data, and if sentence pairs have consistent meanings, then a tag is applied. The sentence pairs may also be from mined data, such as sentence pairs mined from a network. Here, the system may crawl the network to obtain translation pairs. In this case, the different languages may come from different parts of the same text. At block 304, sentence pairs are input to the MTQP model. And at block 306, the MTQP model generates a predicted MTQP score. This includes aggregating any internal scores. A higher score indicates better (more faithful) translation quality from the source language to the translation language. The threshold is used to cull translations of lower quality (less faithful) so that the translation by the threshold can be used to train a new translation model.
According to one aspect of the present technique, the predicted MTQP scores may have different levels. For example, in a level 1 scenario, the neural machine translation model may be fixed (static), meaning that the model does not need to be trained (or it has been trained offline before). Here, the supported language pairs depend on a neural machine translation model. The level 1 scene may be used to obtain a forced decoding score that is calculated by summing the cross entropy (e.g., logarithmic probability) of each token, and then normalizing (dividing by) the sentence length.
For example, the input sentence pair may be ("a b c d", "e f g"). The following steps may be performed to calculate a Forced Decoding Score (FDS). First, a translation model is run on this sentence pair, and at each tag in the second sentence, the model will produce a probability distribution. Let the distribution generated at the first mark be { a:0.5, b:0.3, e:0.1, g:0.1}. The log probability of "e" at the first marker is log (0.1). Similarly, a logarithmic probability of "f" may be obtained at the second marker and a logarithmic probability of "g" may be obtained at the third marker. Finally, the system may add all log probabilities and divide by the number of tokens (e.g., sentence length), which in this example is 3. This score is the forced decoding score. The benefit of this approach is that it does not require additional training data. It also applies to language pairs supported by machine translation.
Other level scores, such as level 2 and level 3 scores, may be generated by the same model structure, but they are trained statically (level 2) or dynamically (level 3) on different data sets. Unlike the tier 1 approach, these other tiers require training. For example, the level 2 approach may be particularly beneficial for users (or applications) that require very high quality MTQP scores. In this case, a large training set may be collected for some popular language pairs, and the system may train a generic model for each of these language pairs. Here, the model is static (trained offline). Each entry in the dataset contains a source sentence, a translated sentence generated by the machine translation system, and a tag indicating whether the translation is good enough. In this way, the MTQP model is able to learn to distinguish between good and bad translations. However, manual tagging of large data sets is expensive (considering that the annotators need to be bilingual to be able to determine if the translation is good), especially for low-resource languages. The level 2 approach may use approximately 100000 samples (e.g., approximately 80000-120000 samples) to train and validate the generic MTQP model.
In one example of a level 3 (dynamic training) method, a user may provide custom training data to train a custom MTQP model. For example, this may involve about 15000 (e.g., about 10000-20000) samples to train the custom MTQP model from scratch. The data size for fine-tuning from the generic MTQP model may be much smaller, such as a 1/3 size (e.g., 5,000 samples). Here, since custom training data can be custom for a specific application (e.g., subtitles of a movie), it can produce very efficient results.
As shown in view 400 of fig. 4, for the level 2 or level 3 approach, source sentence 402 and translation output 404 are projected to feature vector 406. Feature vector 406 is used to find the closest embedded sentence in the other language and then MTQP model is used to score the (source, translation) pair. Classification and/or regression may be applied to the generated feature vectors 406. For example, the stream of vector values is fed as input to a classifier that builds the MTQP model. As discussed further below, these levels may be significantly better than the level 1 approach.
Fig. 5 illustrates an example technical implementation 500 of a model structure that may be used with the level 2 or level 3 methods to generate predicted MTQP scores. Sentence pair [ source sentence, translation output ]502 are fed to one or more feature extractors 504. For each sentence pair, each feature extractor generates a feature vector. As shown in this example, the feature extractor includes a quasi-MT feature extractor 504 a NMT feature extractor 504 b Language model feature extractor 504 c And LogPr feature extractor 504 d . At block 506, the feature vectors generated by the feature extractor 504 are concatenated together. At block 508, the concatenated feature vector is applied to the feed-forward neural network to project the concatenated features to the predicted score 510. Once trained, the model can be used for filtering. For example, the model may be used to discard or flag low scoring pairs below a selected threshold. For example, the threshold may be selected based on the type of sentence being translated, the application (e.g., subtitles for a movie), historical translation information, manual tagging of a given dataset, and so forth. In one scenario, for different language pairs, a subset of the data may be marked to identify what percentage is satisfactory (e.g., 30%, 70%, or some other threshold).
For a classification setting, the predicted scores are n-dimensional vectors, where n is the number of categories, and each score represents the probability of that category. For regression settings, the predicted score is a single value. When in training mode, the loss is calculated and the gradient can be counter-propagated to update the parameters in the MTQP model. For example, in training the MTQP model, a gradient descent method that finds local minima may be used. For a classification setting, the loss is a cross entropy loss. For regression settings, the loss is Mean Square Error (MSE). In one scenario, the predicted score 510 may be normalized, e.g., such that the distribution of the predicted scores is similar between different languages.
quasi-MT feature extractor 504 a The internal scores of the quasi-MT models trained on a large parallel corpus of sentences are used. The quasi-MT model is trained by using information in both the source sentence and the golden sentence to attempt to predict each marker in the golden sentence. For example, in view of the discussion above regarding the quasi-MT model 200, assume the active sentence [ a, b, c ]]And the gold mark [ A, B, C, D ]]. In this case, use is made of [ a, b, c ]]And [ B, C, D ]]To predict a. By [ a, b, c ]]And [ A, C, D ]]To predict B. By [ a, b, c ]]And [ A, B, D ]]To predict C. And using [ a, b, c ]]And [ A, B, C]To predict D. Note that for a conventional MT model, beam searching is required when the model generates translations during the inference time, during which it processes one tag at a time (in a sequential fashion). However, since the quasi-MT processes all the tags simultaneously, no beam search is required.
NMT feature extractor 504 b The internal scores of the encoder and decoder from the NMT translation model are used. Here, the use of encoder scores is optional. In addition to these internal scores, the feature extractor may also use non-matching features and Monte Carlo drop out word-level confidence features. To use the inner fraction in the decoder, it uses the output of the decoder before the soft maximum layer (204 in fig. 2A). After feeding those fractions into the LSTM, a feature vector of fixed length is obtained. The internal fraction in the encoder is similar to the output of 202 in fig. 2A. All features are optional except the decoder score. In order to attempt to characterize the MT model uncertainty on a given input, monte carlo is employed, wherein the system can run the underlying MT model several times for the feature extractor, each time using a different exit mask for the same exit probability value. Here, the mean and variance of each of the log-objective probability pairs is concatenated to other target-side MT-derived features before LSTM encoding into a fixed dimension.
Language model feature extractor 504 c Internal scores from both language models are used. The first is a language model trained on a large corpus of source language. The second is a comparative language model that is first trained on a large corpus and then trained on sentencesIncremental training is carried out on a corpus composed of centered source sentences. In addition to the internal scores from the two language models, the feature extractor also has mismatch and entropy features in the NMT feature extractor. Entropy (H) k ) Can be obtained from predictor P according to the following:wherein t is k Representing the running flag at position k. For example, P (t) k ) May be from an NMT model or a language model.
Source side language model feature extractor 504 c The expansion may be by using a comparative language model, wherein a second (adapted) language model is trained incrementally from a previous language model on confidence estimation training data. The purpose of the second language model is to capture the differences between the domain in which the machine translation and confidence estimation model is to be employed and the domain in which the machine translation model is trained. Here, the same features as the basic language model are used for the adapted language model. In the case of using a contrast language model feature extractor, a concatenation of feature sequences from two language models can be augmented with two different features and sent to the LSTM layer encoded into fixed dimension features, where:
arg maxP base (s k )＝＝arg maxP adapted (s k ) (binary system)
The LogPr feature extractor 504d calculates log P (target|source)/len (target) as a single feature from the NMT model based on the target (translated) sentence and the source sentence. Len (target) is the length of the target. Target sentence t= [ T ] by NMT model 1 ,...,t k ,...t {length(T)} ]Log P (t) generated at each position k in (3) k ) Sum over all k=1..length (T):
ideally, the calculated value corresponds to the forced decoding score.
Various adjustments may be made to the model structure. For example, to evaluate confidence in the model, it may run multiple times at different exit rates, or generate different top n candidates during decoding. The more diverse the results, the lower the confidence of the model and the lower the MTQP score should be generated. Exit involves randomly exiting the node during training. For example, the n values of the first n candidates may be 5, or more or less.
The reverse translation forced decode score may also be used to evaluate system performance. For example, since the forced decoding score for each sentence pair can be calculated directly, the system can switch sentence pairs and calculate the forced decoding score again, which is a reverse translation forced decoding score. Using these two forced decoding scores, the system can then combine them (e.g., average only) to see if the performance is better. This would involve adding FDS and reverse-translated FDS to the features. Just as the mismatch feature, the system can add any FDS feature, which can make the MTQP model better, as there are more features overall.
In another scenario, the NMT decoder may generate a posterior probability grid. In this case, the posterior probability of each token at the target side can be used for confidence scoring. The functionality is also applicable in other fields, e.g. generating alternatives for a given tag/phrase at the target side.
Evaluation index and test
It may be beneficial to control the amount of noise introduced in the downstream machine translation pipeline. To assist in evaluating performance, different metrics may be employed. For example, to evaluate noise (or whether the translation data is accurate enough to be used by the translation system), the primary performance index R@P =t may be used. Here, R represents the recall (or sensitivity) which corresponds to the percentage of relevant instances retrieved by the system. P represents the precision (or positive predictive value) corresponding to the percentage of the relevant instances in the total number of instances retrieved. In this evaluation, precision maximization recall is constrained by precision above a threshold t.
Setting t to a high value may control the amount of noise introduced in the downstream pipeline. For example, a t value on the order of 0.9 (e.g., +/-10%) provides sufficient accuracy for most machine translation cases. t=0.9 means that when the user directly uses those translations with high MTQP scores, 90% of them are truly good translations (no post-editing is needed). In other examples, t may be above or below 0.9. The parameters may be adjustable, for example, based on the type of information being translated, the type of application (e.g., video captioning, scientific paper translation, etc.), or other factors.
In a classification setting, where the data markers are binary, an evaluation index of the area under the curve (AUC) of the precision-recall curve or AUC of the receiver operating characteristics curve may also be used. And in a regression setting, where the data tag may have a value between 0 and 1, one or more of the following indices may be employed: mean Square Error (MSE), mean Absolute Error (MAE), pearson correlation coefficient (Pearson), spearman rank correlation coefficient (Spearman), or Kendall rank correlation coefficient (Kendall). For the case where the data set is provided by the user, the user may set the operation criteria in consideration of the index information. For example, precision-recall curve information may be used to determine an operating point (e.g., to set t).
Merely as an example of a level 3 score using a custom trained MTQP model, the primary performance metrics may be: 0.2r@p=0.9, which means that at least 0.2 recall is achieved when the precision is 0.9. Both the recall value and the precision value may vary, for example, 5-15%, or more or less. For the level 1 and level 2 scores, the target value t may be relaxed, for example, to between 0.75-0.85.
The following is an evaluation example of parallel sentence mining comparing the forced decoding score of level 1 and the MTQP score of level 3. In this example, the data source may include legacy data, such as sampled sentence pairs from a mix of translation and network data, that are marked on sentence pairs that have a consistent meaning. The data sources may also include mined data, in which case sentence pairs are mined from a large corpus, such as from natural data on a network. Here, the mined data may not be translation data. According to one example, the system may segment all sentences that appear on the network into de-duplicated mono-lingual sentences and filter the high quality portions using the sentence quality scores. Thus, sentence pairs can be mined directly from a single-lingual sentence using language agnostic embedding. In one scenario, legacy data has about 30000 sentence pairs and mined data has about 10000 sentence pairs. The language pairs evaluated include: english (En) -Chinese (Zh), english (En) -Russian (Ru), english (En) -Indonesia (Hi), english (En) -French (Fr), english (En) -Spanish (Es), and English (En) -Portuguese (Pt).
Table 1 shows the score in the case where the main performance evaluation index is R@P =0.9
Score of | En:Zh | En:Ru | En:Hi | En:Fr | En:Es | En:Pt |
Forced decoding score (level 1) | 0.137 | 0.049 | 0.059 | 0.260 | 0.256 | 0.229 |
MTQP score (level 3) | 0.315 | 0.250 | 0.167 | 0.215 | 0.353 | 0.335 |
Table 1: R@P =0.9
Table 2 shows the score in the case where the main performance evaluation index is R@P =0.8
Score of | En:Zh | En:Ru | En:Hi | En:Fr | En:Es | En:Pt |
Forced decoding score (level 1) | 0.310 | 0.492 | 0.222 | 0.488 | 0.502 | 0.412 |
MTQP score (level 3) | 0.467 | 0.505 | 0.358 | 0.483 | 0.621 | 0.517 |
Table 2: R@P =0.8
It can be seen that the MTQP score is better (i.e., higher) than the mandatory decoding score for each language translation, except En: fr, and is 50% or higher than the mandatory decoding score for certain languages.
Another index may be used to show how the forced decoding score model and MTQP model behave when considering the top level translation samples. The index may be calculated by first ordering the samples according to the predicted score (forced decoding score or MTQP score). Then, the first X percentiles (e.g., 10%, 15%, 20%, 25%, and 30%) are selected. For each of the first X percentiles, how many samples in the set provide satisfactory translation (e.g., do not require any post-editing), and how many samples provide unsatisfactory translation (e.g., may require a large amount of post-editing). Note that some translations may be between satisfactory and unsatisfactory, as they may require a minimum amount of post-editing. Based on such criteria, table 3 below shows the metrics for En:zh machine translation, where X is estimated to be between 10% and 30%.
Table 3: evaluating top level translation samples
Tables 4 and 5 show examples of other metrics that are applied to training of classification or regression with the level 3MTQP method, as compared to the level 1 forced decoding score method. Table 4 shows the results of english to french translations and table 5 shows the results of english to russian translations. In these examples, R@P =0.9. For MSE or MAE, lower values indicate higher quality machine translations, while for pearson, spearman, kendel and R@P, larger values indicate higher quality machine translations.
Training strategy | MSE | MAE | Pearson | Spirman' s | Kendell type | R@P＝0.9 |
Level 1 | 0.138 | 0.309 | 0.344 | 0.334 | 0.246 | 0.058 |
Level 3: classification | 0.111 | 0.259 | 0.503 | 0.576 | 0.414 | 0.317 |
Level 3: regression | 0.059 | 0.164 | 0.535 | 0.551 | 0.411 | 0.256 |
Table 4: training strategy (En Fr)
Training strategy | MSE | MAE | Pearson | Spirman' s | Kendell type | R@P＝0.9 |
Level 1 | 0.142 | 0.317 | 0.533 | 0.522 | 0.397 | 0.378 |
Level 3: classification | 0.106 | 0.203 | 0.618 | 0.649 | 0.489 | 0.657 |
Level 3: regression | 0.052 | 0.144 | 0.675 | 0.653 | 0.501 | 0.667 |
Table 5: training strategy (En Ru)
It can be seen that both classification and regression training strategies perform quite well on a variety of criteria. The actual performance depends on the particular language pair. However, in some cases, classification methods may be more appropriate, for example, when the translation memory can be easily converted into training data. Furthermore, the classification settings may be compatible with regression data, but not vice versa, because the regression marks may be converted to binary marks by setting a threshold.
System architecture
Fig. 6 illustrates an MTQP model workflow 600, for example, where MTQP services are used in an online manner, such as with a translation Application Programming Interface (API). As shown, the system may have several parts including one or more users 602, a translation API 604, an MTQP service 606, and a dependent service 608. For example, when the user invokes the translate API 604 for a source sentence, a flag may be specified where the translate API 604 will return a translate sentence along with an MTQP score. The user 602 may be an end user or other client, whether external (third party client) or internal. In one example, the user 602 may be an external application service provider or an internal service that provides a video stream with subtitles. In other examples, the user 602 may use the predicted quality score in various ways, such as to: determining whether machine translation can be used without post-editing, selecting the best translation from multiple sources, providing higher quality accurate machine verification, providing more cost effective manual quality review (e.g., by targeting specific translations), signaling as an improved machine learning model, refining descriptions of different languages based on quality scores, ordering video (or audio) content with good local subtitles, or descriptions, and so forth. The affiliated service 608 can maintain a training model and the system can send Remote Procedure Calls (RPCs) to evaluate sentence pairs.
As indicated by arrow 610, user 602 may send a request to translation API 604. Here, the request includes one or more source sentences. As indicated by arrow 612, the translation API 604 sends a request to the MTQP service 606 that includes the received one or more source sentences and one or more translation sentences. As shown by arrow 614, the MTQP service 606 requests the dependent service 608 to perform model inference, and the dependent service 608 returns a predicted score according to arrow 616. For example, when sentence pairs arrive at MTQP service 606, preprocessing may be performed to convert those sentence pairs into tensors that can be consumed by the MTQP model. The tensor is then passed to the dependent service where the MTQP model is serviced. After retrieving the output tensor (arrow 616), the MTQP service 606 performs post-processing to convert the tensor into a predicted MTQP score. Based on the predicted score, MTQP service 606 returns an MTQP score to translation API 604, as indicated by arrow 618. The translation API 604 returns one or more translation sentences having one or more MTQP scores, as indicated by arrow 620. Alternatively, as indicated by arrow 622, the user 604 may send the request with the source sentence and the translated sentence directly to the MTQP service 606. Here, in response, MTQP service 606 provides MTQP scores directly to user 602 (after performing model inference and receiving predicted scores). Based on the MTQP score for the translated sentence, the system may tag the translated sentence below a quality threshold and modify the translation database based thereon. Alternatively, translated sentences that meet the quality threshold may be tagged and the database updated accordingly. One or more users may access high quality translations and use them in various applications. Conversely, translations marked as not meeting the quality threshold may be post-edited by adjusting so that they meet the quality threshold. Thus, according to one aspect of the present technique, nothing is discarded even if the translation is below the quality threshold. By providing MTQP scores to one or more users, this leaves the user or users with a choice of how to handle those scores.
In accordance with features disclosed herein, the MTQP model method may be implemented using a TPU, CPU, or other computing architecture. An example computing architecture is shown in fig. 7A and 7B. In particular, fig. 7A and 7B are schematic and functional diagrams, respectively, of an example system 700, the example system 700 including a plurality of computing devices and databases connected via a network. For example, one or more computing devices 702 may be a cloud-based server system. Databases 704, 706, and 708 may store, for example, a corpus of source sentences, a corpus of translation outputs, and different feature extractors (such as a quasi-MT feature extractor, an NMT feature extractor, a language model feature extractor, and/or a LogPr feature extractor), respectively. The database may be accessed by the server system via network 710. One or more user devices or systems may include a computing system 712 and a desktop computer 714, for example, to provide a parallel corpus and/or other information to one or more computing devices 702.
As shown in FIG. 7B, each of computing devices 702 and 712-714 may include one or more processors, memory, data, and instructions. The memory stores information accessible by the one or more processors, including instructions and data (e.g., machine translation models, parallel corpus information, feature extractors, etc.) that are executable or otherwise used by the one or more processors. The memory may be of any type capable of storing information accessible by one or more processors, including computing device readable media. The memory is a non-transitory medium such as a hard disk drive, memory card, optical disk, solid state, etc. The system may include different combinations of the foregoing, whereby different portions of the instructions and data are stored on different types of media. The instructions may be any set of instructions that are executed directly (such as machine code) or indirectly (such as scripts) by one or more processors. For example, the instructions may be stored as computing device code on a computing device readable medium. In this regard, the terms "instructions," "modules," and "programs" may be used interchangeably herein. The instructions may be stored in object code format for direct processing by the processor, or in any other computing device language, including scripts or collections of individual source code modules that are interpreted or precompiled as needed.
The processor may be any conventional processor such as a commercially available CPU, TPU, or the like. Alternatively, each processor may be a dedicated device, such as an ASIC or other hardware-based processor. While fig. 7B functionally shows the processors, memory, and other elements of a given computing device within the same block, such a device may in fact comprise multiple processors, computing devices, or memories, which may or may not be stored within the same physical housing. Similarly, the memory may be a hard disk drive or other storage medium located in a housing other than that of the one or more processors, such as in a cloud computing system of server 702. Thus, references to a processor or computing device will be understood to include references to a collection of processors or computing devices or memories that may or may not operate in parallel.
Input data, such as source sentences or translation output, may be manipulated by the MTQP module to generate one or more predicted scores and related information. The predicted score may be used to filter the translation results such that only results that exceed a threshold (e.g., the first 10-40%) are provided to or otherwise utilized by the user. The user device may utilize this information in various applications or other programs to provide accurate, high quality translations in accordance with the various applications discussed herein. For example, this may include using the score as a filter to identify high quality sentence pairs for use as training data for better translation models. In accordance with one aspect of the present technique, the data (sentence pairs) of the MTQP analysis is used to perform NMT training data filtering. For example, quality predictions from the MTQP model are used for "nominated" sentence pairs, e.g., as marker data. This enables the system to manage a more suitable dataset (dataset where the quality predictions meet some quality metrics) that is then used to train a machine translation model (e.g., NMT model).
The computing device may include all of the components typically used in conjunction with computing devices, such as the processors and memory described above, as well as a user interface subsystem for receiving input from a user and presenting information (e.g., text, images, and/or other graphical elements) to the user. The user interface subsystem may include one or more user inputs (e.g., at least one front-facing (user) camera, mouse, keyboard, touch screen, and/or microphone) and one or more display devices (e.g., a monitor with a screen or any other electrical device operable to display information (e.g., text, images, and/or other graphical elements)). Other output devices, such as one or more speakers, may also provide information to the user.
The user-related computing devices (e.g., 712-714) may communicate with the back-end computing system (e.g., server 702) via one or more networks, such as network 710. The network 710 and intermediate nodes may include various configurations and protocols including short-range communication protocols such as bluetooth TM Bluetooth LE TM The internet, the world wide web, an intranet, a virtual private network, a wide area network, a local network, a private network using one or more corporate proprietary communication protocols, ethernet, wiFi, and HTTP, as well as various combinations of the foregoing. Such communication may be facilitated by any device capable of transmitting data to and from other computing devices, such as modems and wireless interfaces.
In one example, computing device 702 may include one or more server computing devices having multiple computing devices, such as a load balancing server farm or cloud computing system, that exchange information with different nodes of a network to receive, process, and send data to and from other computing devices. For example, computing device 702 may include one or more server computing devices capable of communicating with any of computing devices 712-714 via network 710.
FIG. 8 illustrates a method 800 that involves receiving source sentence and sentence pairs of translation output by a machine translation quality prediction model at block 802, in accordance with aspects of the present technique. At block 804, the method involves performing feature extraction on sentence pairs using a set of two or more feature extractors, each feature extractor generating a corresponding feature vector. Then, at block 806, the corresponding feature vectors from the feature extractor set are concatenated together. And at block 808, the method includes applying the concatenated feature vector to a feed-forward neural network. The feed forward neural network generates a machine translation quality prediction score for the translation output.
In accordance with aspects of the present technique, given a pair of sentences [ source sentence and translation output ], the MTQP service returns a score to indicate the quality of the translation. The MTQP score may be used by an application, service, or other user in a variety of ways. For example, the score may be used to estimate the post-editing workload of each sentence. Alternatively, for high quality translation output exceeding the selected threshold, post editing may be omitted. Here, the user can directly use the translation of the high MTQP score and send other translations for post-editing (automatic or manual post-editing), thereby saving the post-editing cost. In this case, an upper threshold may be configured for translation that does not require post editing. This approach may also improve post-editing efficiency by separating scores into different queues so that translators focus on similar types of work.
In another scenario, the system may ensure that low quality translations below a selected threshold are post-edited. Here, the user may not have strict requirements on the quality of the translations, meaning that most machine translations are acceptable, and it may only be necessary to pick out translations of very low quality (e.g., 10% of the bedding, or more or less) and post-edit these translations. In this case, a lower threshold may be configured for translations requiring post-editing. Yet another scenario may involve direct manual translation over poor machine translation. Here, the user may send the source sentence directly for manual translation and discard the corresponding machine translation with a very low MTQP score, as bad translations may be misleading and the post-editor may need some time to read the translation. This approach eliminates the burden of poor machine translation, but instead uses the translator to directly perform manual translation.
Although the technology herein has been described with reference to particular embodiments, it is to be understood that these embodiments are merely illustrative of the principles and applications of the present technology. It is therefore to be understood that numerous modifications may be made to the illustrative embodiments and that other arrangements may be devised without departing from the spirit and scope of the present technology as defined by the appended claims.
Claims (21)
1. A computer-implemented method, comprising:
receiving a source sentence and a sentence pair output by translation through a machine translation quality prediction model;
performing feature extraction on the sentence pairs using a set of two or more feature extractors, each feature extractor generating a corresponding feature vector;
concatenating the corresponding feature vectors from the set of feature extractors together; and
the concatenated feature vector is applied to a feedforward neural network that generates a machine translation quality prediction score for the translation output.
2. The method of claim 1, further comprising storing the machine translation quality prediction score in a database in association with the translation output.
3. The method of claim 1, further comprising sending the machine translation quality prediction score to a user.
4. The method of any of the preceding claims, wherein the set of two or more feature extractors comprises at least two of: a quasi-machine translation (quasi-MT) feature extractor, a neural machine translation feature extractor, a language model extractor, and a LogPr feature extractor.
5. The method of claim 4, wherein the quasi-MT feature extractor uses an internal score of a quasi-MT model trained by attempting to predict each marker in a golden sentence using information in both the source sentence and the golden sentence.
6. The method of claim 4 or claim 5, wherein the neural machine translation feature extractor uses an internal score from at least one decoder of a neural machine translation model.
7. The method of any of claims 4 to 6, wherein the language model extractor uses internal scores from two language models, a first one of the language models being trained on a selected corpus of source language and a second one of the language models being a comparative language model, the comparative language model being trained first on the selected corpus and then incrementally trained on the corpus formed by source sentences in the training sentence pair set.
8. The method of any of the preceding claims, further comprising:
determining, by one or more processors, whether the machine translation quality prediction score exceeds a quality threshold; and
the translation output is filtered when the machine translation quality prediction score does not exceed the quality threshold.
9. The method of claim 8, wherein filtering the translation output comprises storing a flag with the translation output to indicate that the machine translation quality prediction score does not exceed the quality threshold.
10. The method of claim 8, wherein filtering the translation output comprises removing the translation output from a corpus of translation output sentences.
11. The method of any of the preceding claims, further comprising:
determining, by one or more processors, whether the machine translation quality prediction score exceeds a quality threshold; and
and adding the translation output to a corpus of translation output sentences when the machine translation quality prediction score exceeds the quality threshold.
12. The method of any of the preceding claims, further comprising:
the translation output is used to train a machine translation model when the machine translation quality prediction score exceeds a quality threshold.
13. The method of any of the preceding claims, further comprising:
creating a carefully selected dataset of source sentences and corresponding translation outputs, wherein each translation output exceeds a quality threshold; and
the selected dataset is used to train a machine translation model.
14. The method of claim 13, wherein the trained machine translation model is a neural machine translation model.
15. A system, comprising:
a memory configured to store machine translation quality prediction information; and
One or more processors operatively coupled to the memory, the one or more processors configured to implement a machine translation quality prediction model by:
receiving a source sentence and a sentence pair output by translation;
performing feature extraction on the sentence pairs using a set of two or more feature extractors, each feature extractor generating a corresponding feature vector;
performing a concatenation of the corresponding feature vectors from the set of feature extractors; and
the concatenated feature vector is applied to a feed-forward neural network configured to generate a machine translation quality prediction score for the translation output.
16. The system of claim 15, wherein the set of two or more feature extractors comprises at least two of: a quasi-machine translation (quasi-MT) feature extractor, a neural machine translation feature extractor, a language model extractor, and a LogPr feature extractor.
17. The system of claim 15 or claim 16, wherein the one or more processors are further configured to:
determining whether the machine translation quality prediction score exceeds a quality threshold; and
The translation output is filtered when the machine translation quality prediction score does not exceed the quality threshold.
18. The system of any of claims 15 to 17, wherein the one or more processors are configured to filter the translation output by storing a flag with the translation output to indicate that the machine translation quality prediction score does not exceed the quality threshold.
19. The system of any of claims 15 to 18, wherein the one or more processors are further configured to:
determining whether the machine translation quality prediction score exceeds a quality threshold; and
and adding the translation output to a corpus of translation output sentences when the machine translation quality prediction score exceeds the quality threshold.
20. The system of any of claims 15 to 19, wherein the one or more processors are further configured to train a machine translation model using the translation output when the machine translation quality prediction score exceeds a quality threshold.
21. The system of any of claims 15 to 20, wherein the one or more processors are further configured to:
creating a carefully selected dataset of source sentences and corresponding translation outputs, wherein each translation output exceeds a quality threshold;
Storing the beneficiated data set in the memory; and
the selected dataset is used to train a machine translation model.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2021/040492 WO2023282887A1 (en) | 2021-07-06 | 2021-07-06 | Dataset refining with machine translation quality prediction |
Publications (1)
Publication Number | Publication Date |
---|---|
CN117581231A true CN117581231A (en) | 2024-02-20 |
Family
ID=77168435
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180100202.1A Pending CN117581231A (en) | 2021-07-06 | 2021-07-06 | Dataset refinement using machine translation quality prediction |
Country Status (5)
Country | Link |
---|---|
US (1) | US20230025739A1 (en) |
EP (1) | EP4341846A1 (en) |
KR (1) | KR20240008930A (en) |
CN (1) | CN117581231A (en) |
WO (1) | WO2023282887A1 (en) |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN117910482A (en) * | 2024-03-19 | 2024-04-19 | 江西师范大学 | Automatic machine translation evaluation method based on depth difference characteristics |
-
2021
- 2021-07-06 EP EP21749437.6A patent/EP4341846A1/en active Pending
- 2021-07-06 KR KR1020237043409A patent/KR20240008930A/en active Search and Examination
- 2021-07-06 WO PCT/US2021/040492 patent/WO2023282887A1/en active Application Filing
- 2021-07-06 CN CN202180100202.1A patent/CN117581231A/en active Pending
-
2022
- 2022-06-29 US US17/852,863 patent/US20230025739A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
WO2023282887A1 (en) | 2023-01-12 |
KR20240008930A (en) | 2024-01-19 |
US20230025739A1 (en) | 2023-01-26 |
EP4341846A1 (en) | 2024-03-27 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
WO2021143800A1 (en) | System and method for semantic analysis of multimedia data using attention-based fusion network | |
US11093707B2 (en) | Adversarial training data augmentation data for text classifiers | |
JP5346279B2 (en) | Annotation by search | |
Dashtipour et al. | Exploiting deep learning for Persian sentiment analysis | |
CN112270379A (en) | Training method of classification model, sample classification method, device and equipment | |
CN108304373B (en) | Semantic dictionary construction method and device, storage medium and electronic device | |
US20200227030A1 (en) | Adversarial Training Data Augmentation for Generating Related Responses | |
CN111898379B (en) | Slot filling model training method, electronic equipment and storage medium | |
US11538481B2 (en) | Speech segmentation based on combination of pause detection and speaker diarization | |
CN113961666B (en) | Keyword recognition method, apparatus, device, medium, and computer program product | |
CN111488455A (en) | Model training method, text classification method, system, device and medium | |
CN112818670A (en) | Split syntax and semantics in a decomposable variational auto-encoder sentence representation | |
US20230025739A1 (en) | Dataset Refining with Machine Translation Quality Prediction | |
CN113190696A (en) | Training method of user screening model, user pushing method and related devices | |
US20230386473A1 (en) | Method for Configuring and Using a Numeric-to-Alphabetic Expression Machine Learning Model | |
CN111460224B (en) | Comment data quality labeling method, comment data quality labeling device, comment data quality labeling equipment and storage medium | |
CN111651660A (en) | Method for cross-media retrieval of difficult samples | |
CN111814496B (en) | Text processing method, device, equipment and storage medium | |
CN114398489A (en) | Entity relation joint extraction method, medium and system based on Transformer | |
CN114330701A (en) | Model training method, device, computer equipment, storage medium and program product | |
JP2024518458A (en) | System and method for automatic topic detection in text | |
JP6979909B2 (en) | Information processing equipment, information processing methods, and programs | |
Sheikh et al. | Improved neural bag-of-words model to retrieve out-of-vocabulary words in speech recognition | |
US11810572B2 (en) | Multi-threaded speaker identification | |
US11922947B2 (en) | Systems and methods for configuring and using an audio transcript correction machine learning model |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |