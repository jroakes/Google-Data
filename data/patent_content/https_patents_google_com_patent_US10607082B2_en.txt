US10607082B2 - Systems, methods, and apparatus for image-responsive automated assistants - Google Patents
Systems, methods, and apparatus for image-responsive automated assistants Download PDFInfo
- Publication number
- US10607082B2 US10607082B2 US15/700,106 US201715700106A US10607082B2 US 10607082 B2 US10607082 B2 US 10607082B2 US 201715700106 A US201715700106 A US 201715700106A US 10607082 B2 US10607082 B2 US 10607082B2
- Authority
- US
- United States
- Prior art keywords
- data
- image
- camera
- interface
- user
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G06K9/00671—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/40—Information retrieval; Database structures therefor; File system structures therefor of multimedia data, e.g. slideshows comprising image and additional audio data
- G06F16/48—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/487—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using geographical or spatial information, e.g. location
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/903—Querying
- G06F16/9032—Query formulation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
- G06F3/014—Hand-worn input/output arrangements, e.g. data gloves
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/0482—Interaction with lists of selectable items, e.g. menus
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0487—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser
- G06F3/0488—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures
- G06F3/04886—Interaction techniques based on graphical user interfaces [GUI] using specific features provided by the input device, e.g. functions controlled by the rotation of a mouse with dual sensing arrangements, or of the nature of the input device, e.g. tap gestures based on pressure sensed by a digitiser using a touch-screen or digitiser, e.g. input of commands through traced gestures by partitioning the display area of the touch-screen or the surface of the digitising tablet into independently controllable areas, e.g. virtual keyboards or menus
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q30/00—Commerce
- G06Q30/02—Marketing; Price estimation or determination; Fundraising
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q30/00—Commerce
- G06Q30/02—Marketing; Price estimation or determination; Fundraising
- G06Q30/0281—Customer communication at a business location, e.g. providing product or service information, consulting
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/20—Scenes; Scene-specific elements in augmented reality scenes
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/60—Type of objects
- G06V20/68—Food, e.g. fruit or vegetables
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/63—Control of cameras or camera modules by using electronic viewfinders
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/667—Camera operation mode switching, e.g. between still and video, sport and normal or high- and low-resolution modes
-
- H04N5/23245—
-
- H04N5/23293—
-
- G06K2209/17—
-
- G06K2209/21—
-
- G06K2209/23—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V2201/00—Indexing scheme relating to image or video recognition or understanding
- G06V2201/07—Target detection
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V2201/00—Indexing scheme relating to image or video recognition or understanding
- G06V2201/08—Detecting or categorising vehicles
Definitions
- Humans may engage in human-to-computer dialogs with interactive software applications referred to herein as “automated assistants” (also referred to as “digital agents,” “chatbots,” “assistant applications,” “interactive personal assistants,” “intelligent personal assistants,” “conversational agents,” etc.).
- automated assistants also referred to as “digital agents,” “chatbots,” “assistant applications,” “interactive personal assistants,” “intelligent personal assistants,” “conversational agents,” etc.
- humans (which when they interact with automated assistants may be referred to as “users”) may provide commands and/or requests using (i) spoken natural language input (i.e. utterances), which may in some cases be converted into text and then processed, and/or (ii) by providing textual (e.g., typed) natural language input.
- Certain automated assistants can provide information (e.g., movie times, store hours, etc.) in response to voice commands from a user, and/or control peripheral devices according to the voice commands. Although such features are convenient, there may be ways of providing more elaborate commands, providing commands with less arduous inputs, providing commands that protect the privacy of a corresponding user, and/or providing commands with additional or alternative benefits.
- Implementations disclosed herein relate to generating output that is tailored to attribute(s) of objects captured in image(s), from a camera of a client device, and causing the tailored output to be rendered (e.g., audibly and/or graphically) at the client device, optionally along with (e.g., graphically overlaid on) a presentation of a real-time image feed from the camera.
- at least one image captured by the camera is processed to determine one or more attributes of objects captured by the image.
- a subgroup of one or more conversation modes is selected, from a group of a plurality of available conversation modes, based on one or more of the determined attributes.
- Selectable element(s) that correspond to the conversation mode(s) of the subgroup are then caused to be displayed, at an interface of the client device, as initial output.
- the initial output includes selectable element(s) that correspond to conversation mode(s) that are tailored to determined attribute(s) of an object in the processed image(s).
- further output is caused to be displayed, where the further output includes object data that is tailored to the conversation mode of the selected element, and that is tailored to the object in the processed image(s).
- the object data can be identified based on content that is responsive to a query formulated based on the conversation mode and based on one or more of the determined attribute(s) of the object (including, and/or in addition to, the determined attributes utilized to select the subgroup of conversation modes).
- the query can be issued to retrieve the responsive content in response to the selection of the selectable element, or can be issued prior to selection of the selectable element.
- the object data that is based on the responsive content can be audibly or graphically rendered at the client device, as further output, in response to the selection of the selectable element.
- the object data can be graphically presented along with a rendered real-time image feed from the camera.
- the further output includes object data that is tailored to a selected conversation mode and to determined attribute(s) of an object in the processed image(s).
- one or more contextual features can additionally be utilized in selecting the conversation mode(s), determining a presentation prominence for selectable elements for multiple conversation mode(s), and/or in determining the object data.
- the contextual features can include, for example, a location of the computing device, a time of day, a day of the week, features of object(s) recently detected in images from the camera, etc.
- a “pricing” conversation mode may be selected based on an object identifier of “food item” if a current location of the computing device is at a “grocery store”, whereas the “pricing” conversation mode may not be selected (or a corresponding selectable element presented less prominently) if the current location is instead a “home” location for a user of the computing device.
- a query that is issued to determine the object data can be further generated based on the contextual features (e.g., include term(s) that are based on the contextual data).
- a camera of a client device can capture an image.
- the image can capture a large Red Delicious apple, and can be processed to determine attributes of “food”, “apple”, and “Red Delicious”.
- a “calorie” conversation mode can be selected, from a group of a plurality of available conversation modes, based on the “calorie” conversation mode being defined, in one or more computer readable media, as having an association to the “food” attribute.
- a selectable element that corresponds to the “calorie” conversation mode can then be displayed, at an interface of the client device, as initial output. In response to selection of the “calorie” selectable element, further output can be displayed that is tailored to the “calorie” conversation mode, and that is further tailored to the “Red Delicious” and “apple” attributes.
- a query of “calories in Red Delicious apple” can be transmitted to a search engine, a response of “72 Calories” received, and “72 Calories” displayed at the client device.
- the object data can be graphically presented along with a rendered real-time image feed from the camera.
- the user can thereafter direct the camera at different food objects and receive calorie information for those objects.
- an additional image captured by the camera while still in the “calorie” mode can capture a banana, and can be processed to determine a “banana” attribute. Based on the “banana” attribute being determined in the additional image, a query of “calories in banana” can be transmitted to a search engine, a response of “105 Calories” received, and “105 Calories” displayed at the client device.
- the above and other techniques described herein enable a user to interact with an automated assistant and obtain relevant output from the automated assistant without requiring arduous typed input to be provided by the user and/or without requiring the user to provide spoken input that could cause privacy concerns (e.g., if other individuals are nearby). Further, various implementations can reduce the number of inputs required to obtain relevant input relative to other techniques, which may conserve client device computational resources and/or assist users with speech and/or dexterity issues. Additionally, various implementations disclosed herein perform processing of images locally at a client device to determine attribute(s) of object(s) contained in the images.
- any selection of a conversation mode and/or determining of object data that occurs on a remote device can occur based on the determined attributes, without any reference to the images themselves. In this manner, the images can be maintained at the client device, without requiring the images to be transmitted from the device to select a conversation mode and/or obtain responsive object data—thereby enhancing the security of those images.
- a method implemented by one or more processors can include steps such as generating an object identifier for an object graphically represented in a real-time image feed from a camera of a computing device.
- the real-time image feed can be displayed at an interface of the computing device and generating the object identifier can include processing one or more images from the real-time image feed.
- the method can also include selecting, based on the generated object identifier, a conversation mode from a plurality of conversation modes for interacting with an assistant application via the camera of the computing device. Additionally, the method can include causing, in response to the selection of the conversation mode, a selectable element that corresponds to the selected conversation mode to be displayed at the interface of the computing device.
- the method can further include receiving, at the interface, a selection of the selectable element, causing a query to be transmitted for retrieving data associated with the object identified by the object identifier, and causing, in response to receiving the selection, the data to be displayed at the interface.
- the method can include, when the real-time image feed is displayed at the interface, receiving a selection of a graphical representation of the object at the interface. Additionally, the step of generating the object identifier can be in response to receiving the selection of the graphical representation of the object.
- the data can be displayed simultaneous to the interface displaying the graphical representation of the object.
- the selectable element can identify the image conversation mode in which the assistant application provides the data.
- the method can include determining contextual data associated with the image data. In this way, selecting the image conversation mode can be further based on the contextual data.
- the contextual data can include geolocation data that identifies a location of the computing device.
- the textual data can include a time at which the real-time image feed is being generated by the camera.
- a system is set forth as including a camera, a display device, one or more processors in communication with the display device and the camera, and memory.
- the memory can be configured to store instructions that, when executed by the one or more processors, cause the one or more processors to perform steps that include receiving image data from the camera.
- the image data can include or capture an object being present in a field of view of the camera.
- the steps can also include determining, based on processing the image data, an object identifier for the object, and causing a selectable element to be graphically represented at the display device.
- the selectable element can identify a conversation mode for interacting with an assistant application using the camera.
- the steps can further include receiving a selection of the selectable element, and transmitting the object identifier and a selection identifier, corresponding to the received selection, to one or more remote devices configured to provide object data based on the object identifier and the selection identifier.
- the steps can also include receiving the object data from the one or more remote devices, and causing the object data to be graphically represented at the display device simultaneous to the object being present in the field of view of the camera.
- the object can be associated with different types of object data available at the one or more remote devices, and the conversation mode can be associated with at least one type of object data to be represented at the display device by the assistant application.
- the steps can include, in response to a different object being presented in the field of view of the camera, causing different object data to be graphically represented at the display device.
- the different object data can correspond to the at least one type of object data associated with the conversation mode.
- the steps can include, in response to a different object being presented in the field of view of the camera, causing a different selectable element to be graphically represented at the display device.
- the different selectable element can identify a different conversation mode in which to interact with the assistant application using the camera.
- a non-transitory computer readable medium can store instructions that, when executed by one or more processors, cause the one or more processors to perform steps that include operating an assistant application in an image conversation mode in which the assistant application is responsive to a real-time image feed provided by a camera of a computing device.
- the steps can also include causing the assistant application to provide object data at an interface of the computing device where the image feed is displayed.
- the object data can correspond to a first object graphically represented in the image feed and a type of data associated with the image conversation mode.
- the steps can further include receiving, at the interface, a selection of a graphical representation of a second object at which the camera is directed, and causing the assistant application to provide different object data at the interface of the computing device.
- the different object data can correspond to the type of data associated with the image conversation mode.
- the steps can include generating contextual data associated with the image feed, and selecting the type of data according to the contextual data.
- the type of data can include: pecuniary data, nutritional data, and/or factual data.
- the contextual data can include: geolocation data associated with the computing device, and/or time data associated with the image feed.
- the steps can include causing the assistant application to query a third-party agent application for the different object data in response to receiving the selection of the graphical representation of the second object.
- causing the assistant application to query the third-party agent application can include causing an object identifier, corresponding to the different object, to be transmitted to a remote device that hosts the third-party agent application.
- the interface can be a touch screen display, and the selection of the graphical representation of the second object can be a touch input at the touch screen display.
- the steps can also include causing the assistant application to provide a selectable element at the interface of the computing device. The selectable element can identify a different image conversation mode available through the assistant application.
- some implementations include one or more processors of one or more computing devices, where the one or more processors are operable to execute instructions stored in associated memory, and where the instructions are configured to cause performance one or more methods described herein.
- the processors may include one or more graphics processing units (GPUs), central processing units (CPUs), and/or tensor processing units (TPUs).
- GPUs graphics processing units
- CPUs central processing units
- TPUs tensor processing units
- Some implementations include one or more non-transitory computer readable storage media storing computer instructions executable by one or more processors to perform one or more methods described herein.
- FIG. 1 illustrates a system for providing an automated assistant application that is operable in a variety of image conversation modes.
- FIG. 2A illustrates a view of a computing device that includes or accesses an automated assistant application capable of operating in multiple different image conversation modes.
- FIG. 2B illustrates a view of the computing device operating in an image conversation mode.
- FIG. 3A illustrates a view of a computing device operating an automated assistant application capable of using context data and image data to provide data and/or execute certain controls.
- FIG. 3B illustrates a view of a computing device operating an automated assistant in a pricing image conversation mode in which the pricings of objects is presented at the interface when the user directs the camera at the objects.
- FIG. 4 illustrates a method for interacting with an automated assistant application using image data from a real-time image feed provided by a camera of a computing device.
- FIG. 5 illustrates a method for providing object data at an interface of a computing device based on an object at which a camera of the computing device is directed.
- FIG. 6 is a block diagram of an example computer system.
- an automated assistant can cause particular information to be presented (e.g., audibly and/or graphically) at a client device in response to particular objects being provided in a viewable range of a camera of the client device.
- the automated assistant can leverage the computational resources of a remote device (e.g., a remote server) to process images from the camera to identify particular objects in the images.
- the remote device or the automated assistant can suggest functions and/or provide information related to the identified objects. For example, when a certain object is identified in an image, the automated assistant can respond according to a predetermined process established for similar objects (e.g., a process established for any objects that conform to classification(s) of the certain object).
- a user can create, through a verbal command or other interface input, a preference for the automated assistant to provide certain information when the camera is directed at an object having one or more particular attributes (e.g., “Assistant, could you please show me competing pricing options for a car when I point the camera at the car” (where the particular attribute is a classification of “car”)).
- an automated assistant can be preconfigured by a party that created the automated assistant and/or using configurations created by other users. For instance, if one or more users create a preference for their automated assistants to respond in a certain manner to certain objects viewed by their cameras, the preference can be preconfigured in, or otherwise shared with, other automated assistants.
- the manufacturer, or a third party can configure the automated assistant to operate according to image conversation modes in which the automated assistant is responsive to particular objects being present in a viewable range of a computing device camera.
- the computing device that employs the automated assistant can be a wearable device, a cellular device, a tablet computer, and/or any other device capable of hosting an automated assistant.
- the computing device can include at least a microphone, a display (e.g., a touch display), and a camera for collecting images to be processed by the remote device.
- the user can provoke the automated assistant through a camera application, an automated assistant application, and/or any application capable of processing an image captured by the camera. For example, while operating a camera application, the user can point the camera at a food item (e.g., an apple), thereby causing an image of the food item to be graphically presented at the display of the computing device.
- a food item e.g., an apple
- the image can be processed at the computing device and/or transmitted to a remote device for remote processing, to identify feature(s) of object(s) contained in the image.
- an object identifier can be transmitted back to the computing device (when the image is processed remotely), or generated at the computing device (when the image is processed locally), and used by the automated assistant to provide suggestions of conversation modes for assisting the user.
- an object identifier identified based on processing of an image can indicate that an object in the image is food, and the suggestions can identify conversation modes related to the food.
- the touch display can present multiple selectable elements, with each selectable element including corresponding text and/or other indicia that identifies a conversation mode (e.g., calorie mode, nutrition mode, price mode, etc.) that is selected based on the “food” object identifier.
- the automated assistant can cause content to be provided that is based on the selected conversation mode. For example, if the user selects the calorie mode, the automated assistant can cause calorie information to be presented at the display and/or rendered audibly.
- the automated assistant can cause “There are 95 calories in an apple” to be displayed and/or audibly presented, where the calorie information is provided based on the selected “calorie mode”, and is tailored to caloric content of an “apple” based on an additional object identifier for the object that indicates the object is an “apple”.
- the additional object identifier can also be generated, locally or remotely, through processing of the image and/or an additional image.
- the calorie information can be rendered by the computing device simultaneous to a real-time image feed from the camera also being displayed at the computing device. In these and other manners, the user can direct the camera at different food items in order to cause the automated assistant to present calorie information about different foods, without having to verbally query the automated assistant.
- the user can point the camera at an apple to cause calorie information about the apple to be presented, and then re-direct the camera at a banana to cause calorie information about the banana to be presented.
- the calorie information about the banana can be presented based on the selected “calorie mode”, and can be tailored to caloric content of a “banana” based on an additional image being captured while the camera is directed at the banana, and an object identifier of “banana” being generated based on the additional image.
- conversation modes corresponding to different objects in a field of view of the camera can be presented.
- the user can be presented with suggestion elements associated with the different objects, despite the objects being categorically different.
- the user can point the camera of their computing device toward a street that includes restaurants, parked cars, and a skyline.
- An image that includes the aforementioned objects can be processed by the computing device, or a remote device, for providing object identifier(s) for each of the objects.
- the automated assistant can query the user to identify the object they are interested in (e.g., “Tap on what you're interested in.”).
- the user can tap the touch display at an area where an object is graphically represented, or otherwise indicate to the automated assistant that they are interested in a particular object (e.g., the restaurants).
- a location where the user tapped can be mapped to object identifier(s) of an object at the location, and, in response, the automated assistant can cause suggestion elements to be presented at the touch display for initializing a conversation mode associated with the object identifier(s) of the selected location. For instance, if the selected location is mapped to a “restaurant” object identifier, the suggestion elements presented to the user at the touch display can correspond to a restaurant review mode, a social event mode, a fun facts mode, and/or any other mode that is associated with a “restaurant” object identifier. When the user selects the restaurant review mode, the automated assistant can cause restaurant review(s) to be presented at the touch display according to which restaurant the user is directing their camera.
- a particular restaurant currently captured in an image can be identified based on processing of the image (e.g., based on recognition of text, in the image, that includes the restaurant's name and/or logo) and/or based on other contextual cues (e.g., based on a current location of the computing device).
- one or more reviews related to the particular restaurant can be gleaned from various sources (e.g., restaurant review websites and applications), and snippet(s) of the review(s) provided for display at the touch display simultaneous to the camera being directed at the restaurant with which the reviews are associated.
- the conversation modes available to the user can include a random fact mode, a price comparison mode, a review mode, a nutrition mode, a specifications mode, and/or any other mode where details of an object in an image can be presented.
- the random fact mode can be generated by the automated assistant using a web query that is initialized by the automated assistant as a background process relative to a camera application that is controlling the camera.
- the background process can include determining object identifier(s) for an object in the image, generating a query based on the object identifier(s) and optionally based on the selected conversation mode, and identifying a random fact about the object based on search result(s) that are responsive to the query.
- the random fact can then be presented at an interface with the camera application.
- the results include web search results and the random fact is generated based on one of the web search results (e.g., a snippet from one of the web search results).
- the query is issued against a knowledge graph or other database that defines a plurality of entities and, for each of the entities, properties of the entity and/or relationship(s) of the entities to other entities.
- the query can define a particular entity determined through processing of the image, a node of the knowledge graph identified that corresponds to the particular entity, and a random fact generated based on a “result” that is a property assigned to the entity in the knowledge graph.
- the random fact can be a “first opened date” for the restaurant, as defined by a “first opened date” property node that is mapped (e.g., via an “edge”) to a node, for the particular restaurant, in the knowledge graph.
- the user can direct the camera of their computing device toward an object to cause the automated assistant to present one or more prices for the object.
- the automated assistant can transmit an image that includes the object to a remote device for processing and identifying the object.
- An identifier for the object can be generated at the remote device and transmitted back to the computing device, and the automated assistant can use the object identifier to perform a price query to identify prices for the object.
- the automated assistant can transmit the image with a price query to cause the remote device to generate an identifier for the object, to use the identifier to identify prices for the object, and to transmit the prices back to the computing device.
- the automated assistant can use the prices resulting from the price query for presentation at an interface of the camera application.
- the object e.g., a car
- multiple prices for the object can be presented adjacent to the graphical representation of the object.
- the user can shift the camera to other objects in order to cause the automated assistant to provide, in real-time, prices for the other objects that are in the viewable range of the camera.
- a context of the camera usage can additionally or alternatively be used as a basis for the conversation mode.
- the context can be a location of the user (e.g., as indicated by a location of the user's computing device), a time of the camera usage, data related to persons near the user, other actions being performed by the user, and/or any other context in which a user could be interacting with an automated assistant.
- a location of the user can be used to select the conversation mode(s) suggested to the user when the user is directing their camera at an object. For instance, when the user is at a restaurant and has received their meal, the user can direct the camera of their computing device at their meal.
- the automated assistant can use an image of the meal to identify object(s) in the image (e.g., the food items in their meal) and use the object(s) and a location of the user to select conversation modes to suggest to the user.
- the automated assistant can, for example, filter out or otherwise not select a price mode because the user is at a restaurant and has already received food, therefore the user would likely not be interested in seeing the price of their food.
- the user may be interested in the nutrition mode and/or the calorie mode, in order to appreciate the meal and, perhaps, track their caloric consumption.
- the resulting selected conversation modes can be suggested to the user while the user is directing their camera at their meal.
- the automated assistant can direct the user to tap on an object in their meal (e.g., “Please tap on a food item you are interested in seeing nutrition for?”). For instance, the user can tap on a graphical representation of a baked potato that is on their plate and, in response, the automated assistant can provide nutritional information about the baked potato (e.g., “Iron 6%, Zinc 20%, . . . ”). In some implementations, the automated assistant can determine a size of the portion (e.g., a small baked potato, a medium baked potato, a large baked potato, etc.) and provide the nutritional information and/or calorie information according to the size of the portion.
- a size of the portion e.g., a small baked potato, a medium baked potato, a large baked potato, etc.
- a context in which the user is operating the camera can be used by the automated assistant to determine or filter the sources of information for a conversation mode. For instance, a user that is traveling internationally can direct their camera at landmarks of interest in order to cause the automated assistant to provide interesting facts while operating in a fact conversation mode of the automated assistant.
- the automated assistant can determine that the user is away from their home on vacation (e.g., using geolocation data and/or calendar data), and select the vacation location as the context of the fact conversation mode.
- facts presented to the user when the user is directing their camera at an object can be from a source associated with the vacation location. For instance, a user that is traveling to a national park for vacation can direct their camera at a landmark within the national park.
- the landmark can be, for example, a very large mountain within the national park.
- the automated assistant can use the context of the image (e.g., the user being on vacation at a national park) to identify more contextually relevant sources of facts about the mountain. For example, a user directing their camera at Mount Rainer, while on vacation in Mount Rainer National Park, can receive information on activities to do near Mount Rainer (e.g., “Wonderland Trail is a 93 mile trail that circumnavigates the peak of Mount Rainer.”), as opposed to receiving non-activity related information (e.g., “Mount Rainer has an elevation of 14,411 feet.”). In this way, the user can be within the park and direct their camera at various landmarks in the park to discover activities to do in the park, instead of having to provide specific verbal or textual gestures to discover such activities.
- FIG. 1 illustrates a system 100 for providing an automated assistant application that is operable in a variety of image conversation modes.
- the system 100 can include a computing device 102 , which can be a cellular device, a tablet device, a wearable device, a personal computer, and/or any other device capable of employing a camera to capture an image.
- the computing device 102 can include a camera 106 , which can capture photos and/or video for processing by the computing device 102 or a remote device 112 .
- the computing device 102 can further include a display device 108 , which can provide a real-time image feed based on image data provided by the camera 106 .
- the display device 108 is a touch display, in that it is touch-sensitive and provides a touch interface for a user to interact with various applications on the computing device 102 .
- the computing device 102 can include an assistant interface 110 , which can be an application interface associated with an assistant application 118 on the computing device 102 and/or the server device 112 .
- an assistant application 118 can be implemented on the computing device 102 .
- aspects of the assistant application 118 are implemented via a local assistant application of the computing device 102 and interface with the server device 112 that implements other aspects of the assistant.
- the server device 112 can optionally serve a plurality of users and their associated assistant applications via multiple threads.
- the local assistant application can be an application that is separate from an operating system of the computing device 102 (e.g., installed “on top” of the operating system)—or can alternatively be implemented directly by the operating system of the computing device 102 (e.g., considered an application of, but integral with, the operating system).
- the assistant application 118 is an automated assistant application capable of receiving verbal and/or textual commands via the assistant interface 110 .
- the assistant application 118 can provide data, perform an application function, communicate with a third party agent, control a peripheral device, and/or otherwise perform any command suitable for execution by a computing device.
- the assistant interface 110 can be a microphone for receiving verbal commands, which can be converted to audio data and processed to determine the appropriate response.
- the audio data can be processed at the computing device 102 or the server device 112 .
- the server device 112 can include a voice to text engine 116 for processing audio data received from the computing device 102 .
- the voice to text engine 116 can operate to receive the audio data, identify speech within the audio data, and output the speech in textual form so that other applications, such as the assistant application 118 , can use the speech text.
- the assistant application 118 can operate according to one or more image conversation modes 120 .
- An image conversation mode 120 corresponds to an operating mode in which the assistant application 118 is responsive to image data from the camera 106 of the computing device 102 .
- the assistant application 118 can cause dynamic changes to the assistant interface 110 (e.g., a graphical user interface provided at the display device 108 ), allowing a user to interact with the assistant application 118 while image data is being provided by camera 106 .
- the assistant interface 110 can be displayed with a real-time image feed from the camera 106 .
- the assistant interface 110 can include one or more selectable elements with text and/or other indicia that invites the user to initialize an image conversation mode (e.g., “Nutrition Mode,” “Price Mode,” “Fact Mode,” “What are you interested in?,” etc.).
- an image conversation mode e.g., “Nutrition Mode,” “Price Mode,” “Fact Mode,” “What are you interested in?,” etc.
- data that is relevant to the image conversation mode of the selectable element, and that is relevant to an object in the image can be graphically and/or audibly rendered via the assistant interface 110 for presentation to a user of the computing device 102 .
- the selectable element(s) presented to a user can be selected based on object identifier(s) of an object captured in image(s) captured by the camera 106 and the rendered data can be determined based on object identifier(s) of the object (the same object identifier(s) used in selecting the selectable element(s) and/or additional object identifier(s)).
- the object identifier(s) of an image can be identified based on processing of the image by the image processing engine 114 , which can be provided at the computing device 102 or the server device 112 .
- the image processing engine 114 can receive the image from the camera 106 and process the image to identify object identifier(s), of object(s) within the image, which can be associated with an image conversation mode 120 .
- the image processing engine 114 can employ one or more image processing techniques for determining object identifiers that correspond to objects in the image captured by the camera 106 .
- the image processing engine 114 can employ a computer vision algorithm to identify a tangible object that is graphically represented in the image and generate an object identifier that corresponds to the tangible object.
- the image processing engine 114 can utilize one or more machine learning models, such as a deep neural network model that accepts an image as input, and that utilizes learned parameters to generate, as output based on the image, measure(s) that indicate which of a plurality of corresponding attributes are present in an image. If a measure indicates that a particular attribute is present in an image (e.g., if the measure satisfies a threshold), that attribute can be considered “resolved” for the image (i.e., that attribute can be considered to be present in the image).
- An object identifier can correspond to one or more of the resolved attributes. For example, a resolved attribute can be a “car” classification, and the object identifier can be the “car” classification.
- the image processing engine 114 can employ additional and/or alternative image processing techniques in generating object identifier(s), such as optical character recognition (“OCR”), image similarity techniques (e.g., to identify an object identifier based on a “label” for a reference image determined to be most similar to an image under consideration), etc.
- OCR optical character recognition
- image similarity techniques e.g., to identify an object identifier based on a “label” for a reference image determined to be most similar to an image under consideration
- One or more object identifiers can be generated by the image processing engine 114 and provided to the assistant application 118 .
- the assistant application 118 can use the object identifiers to provide suggestions regarding the image conversation modes 120 that will be suggested to the user via the display device 108 .
- the assistant application 118 can store or access an index or table that correlates object identifiers to image conversation modes 120 (e.g., nutrition mode, price mode, fact mode, etc.) available to the assistant application 118 .
- object identifiers e.g., nutrition mode, price mode, fact mode, etc.
- objects that are typically for sale, such as cars and food can, for example, be correlated to a price mode.
- food can additionally be correlated to a nutrition mode, whereas cars would not be correlated to the nutrition mode.
- the display device 108 can be a touch display capable of receiving touch inputs for selecting objects that appear in the real-time image feed provided by the camera 106 .
- a user can select objects presented in the real-time image feed in order to identify items of interest. If an object selected corresponds to an object identifier generated by the image processing engine 114 or the assistant application 118 , the assistant application 118 can, in response, provide corresponding selectable elements.
- the corresponding selectable elements can identify image conversation modes 120 in which the user can interact with the assistant application using the camera 106 and the display device 108 .
- the assistant application 118 can present selectable elements for activating an image conversation mode 120 .
- an image conversation mode 120 For instance, a user can be walking down a street and direct their camera 106 at buildings that are facing the street.
- the assistant application 118 can simultaneously present options for activating particular image conversation modes 120 , such as a price mode, a translate mode, a facts mode, a review mode, and/or any other mode that can be associated with images captured by the camera 106 . If the user selects a mode (e.g., a price mode), additional modes and/or data can be presented at the display device 108 , in place of any previously represented modes.
- a mode e.g., a price mode
- prices of objects in the street can be presented at the display device.
- the objects can be identified using the image processing engine 114 and their prices can be identified through a separate first network device 126 , which can communicate with the server device 112 over a network 122 , such as the internet.
- a separate first network device 126 can communicate with the server device 112 over a network 122 , such as the internet.
- the user can direct the camera 106 in different directions to cause the prices of different objects to be displayed.
- selectable elements for other modes relevant to those object(s) can be displayed.
- the user can be operating the camera 106 and directing the camera 106 at a document or other textual medium that includes a language that is not the primary dialect of the user.
- An image captured from the camera 106 can be processed for identifying one or more languages of text that are in the captured image.
- the text can then be translated by a translator application that is available to the assistant application 118 , and the translated text can be presented by the assistant application 118 at the display device 108 of the computing device 102 .
- the translation application can provide an indication of the language of the text for comparing with a primary dialect setting of the assistant application 118 . If the language indicated is the same as the primary dialect setting, the assistant application 118 can bypass providing any translated text at the display device 108 . However, if the language indicated is different than the primary dialect setting, the assistant application 118 can provide the translated text at the display device 108 .
- a prompt can be provided to the user by the automated assistant 118 regarding whether to enter the translate mode.
- the assistant application 118 can bypass prompting the user regarding whether to present the translated text and automatically provide the translated text at the display device 108 when the user directs the camera 106 at a particular object.
- the assistant application 118 can present updated translated text in response to the user redirecting the camera 106 at an object that includes foreign text.
- other conversation modes related to the translated text can be presented to the user at the display device 108 by the automated assistant 118 .
- the automated assistant 118 can cause the display device 108 to present translated text from the menu. Furthermore, the automated assistant 118 can process the translated text to determine a context of the text and/or other properties of the text. For instance, the automated assistant 118 can determine that the translated text relates to food and provide a selectable element related to entering a calorie mode.
- a foreign language e.g., German
- the translated text e.g., “baked potato and steak”
- the translated text can be presented at the display device 108 along with calorie or nutrition data (e.g., “800 calories”) for the food identified by the translated text.
- a particular mode e.g., a nutrition mode
- data related to the particular mode can be presented about objects in the viewing area of the camera 106 .
- other modes and/or data can be presented.
- the assistant application 118 in the nutrition mode, in which nutritional data is presented for food represented in the real-time image feed, the user can direct the camera 106 at a type of food they typically get at the grocery.
- the assistant application 118 can suggest another mode for continuing an image conversation with the assistant application 118 .
- the other mode can be, for example, a price mode, in which prices from competing vendors of the food are displayed at the display device 108 .
- the other mode can be identified in a selectable element that is presented at the display device 108 simultaneous to the grocery item being presented at the display device 108 .
- the assistant application 118 can determine over time that the user prefers to enter the nutrition mode when directing the camera 106 at food. As a result, the assistant application 118 can bypass presenting the user with a selectable element for entering the nutrition mode and, instead, automatically provide nutrition data about the food the camera 106 is directed at. In this way, the user does not need to continually make a manual selection for entering an image conversation mode, but rather, can rely on the assistant application 118 learning the image data the user prefers to view under certain circumstances.
- the assistant application 118 can access data for the image conversation modes through the network 122 .
- the assistant application 118 can be connected to one or more remote devices (e.g., a first remote device 126 that includes first data 132 , a second remote device 128 that includes second data 134 , and an Nth remote device 130 that includes Nth data 136 ).
- Each of the remote devices can include data associated with object identifiers generated by the image processing engine 114 and/or the assistant application 118 .
- the assistant application 118 can access one or more of the remote devices (e.g., 126 , 128 , 130 ) to retrieve price data.
- the price data can be associated with competing prices for an object at which the camera 106 is directed.
- the remote devices can host websites, application data, and/or any other data that can be accessed over a network and associated with an object.
- the suggestion of a particular image conversation mode can be based on one or more machine learning models that can receive one or more inputs and output probabilities for image conversation modes to be suggested.
- the assistant application 118 can include or access a machine learning model that can receive an image from the camera 106 and process the image 106 to determine suitable image conversation modes to suggest.
- the machine learning model can be a deep neural network model, which can be trained to enable generation of probabilities based on input that includes an image of an object and/or object identifier(s) generated based on an image.
- the probabilities can be correlated to image conversation modes in which the assistant application 118 can operate.
- the machine learning model when an image that includes an automobile is applied to the machine learning model, the machine learning model can be utilized to generate a higher probability for a price image conversation mode than a probability for a nutrition image conversation mode. Furthermore, when an image that includes multiple food items is applied to the machine learning model, the machine learning model can be utilized to generate a higher probability for a nutrition image conversation mode than a fact image conversation mode.
- the assistant application 118 can include or access multiple machine learning models that can accept different inputs. For instance, the assistant application 118 can access a first machine learning model that receives an image as an input and a second machine learning model that receives image context as an input. The image context can be input to the second machine learning model using one or more context identifiers.
- a context identifier can include data that identifies a context of an image, such as a location, a time, an event, an environment, and/or any other feature that can be indicative of context. For instance, the context identifier can identify a location of where an image was captured using geolocation data provided by a global positioning system (GPS) transmitter of the computing device 102 .
- GPS global positioning system
- the geolocation data can be used by the assistant application 118 to identify the location of where the image was captured.
- the assistant application 118 can access a map application over the network 122 to determine that a user is positioned inside of a particular restaurant.
- the restaurant name or the location can be provided to the second machine learning model, and the second machine learning model can output a higher probability for a review image conversation mode than fact image conversation mode.
- an image of food from the restaurant can be provided to the first machine learning model and, in response, the first machine learning model can provide equal probabilities for the review image conversation mode and the fact image conversation mode.
- the second machine learning model has provided a higher probability for the review image conversation mode, the review image conversation mode can be suggested to the user at the display device 108 .
- the computing device 102 can include one or more memory devices capable of storing images, documents, and/or any other media capable of being stored and/or edited by a computing device.
- the assistant application 118 can be responsive to the media being accessed by the user. For instance, the user can be viewing an image of food and the assistant application 118 can process the image to determine suggestions for image conversation modes 120 to present to the user for selection.
- the assistant application 118 can suggest a nutrition image conversation mode and a calorie image conversation mode when the user is viewing images that are stored at the computing device 102 or otherwise being accessed at the computing device 102 .
- the user can be viewing a document in a portable document format (PDF) and the assistant application 118 can process the PDF in order to identify content of the PDF that is associated with an available image conversation mode.
- PDF portable document format
- the assistant application 118 can present the user with a selectable element for entering a fact image conversation mode, which can provide facts for assisting the user with understanding the journal article.
- the assistant application 118 can automatically enter the fact image conversation mode and present facts at the display device 108 .
- the facts can be based on content provided in the PDF and the presented facts can change dynamically as the user is scrolling through the PDF or opening different PDFs.
- FIG. 2A illustrates a view 200 of a computing device 210 that includes or accesses an automated assistant application capable of operating in multiple different image conversation modes.
- An image conversation mode is an operating mode in which the automated assistant provides data in response to a camera of the computing device 210 being directed at one or more objects.
- the image conversation modes can include a calorie mode and a nutrition mode. While operating in the calorie mode, a user of the computing device 210 can direct a camera (e.g., a camera located on a back surface of the computing device 210 , facing away from an interface 204 ) at an object, such as an apple 202 , and receive calorie data for the apple 202 .
- a camera e.g., a camera located on a back surface of the computing device 210 , facing away from an interface 204
- an object such as an apple 202
- the calorie data (e.g., “An apple has 95 calories”) can be presented at the interface 204 of the computing device 210 simultaneous to the camera providing the image of the apple 202 in a real-time image feed.
- the user of the computing device 210 can direct the camera at an object, such as the apple 202 , and receive nutrition data for the apple 202 .
- the nutrition data (e.g., “Potassium 195 mg, dietary fiber 4.4 g, . . . ”) can also be presented at the interface 204 simultaneous to the camera providing the image of the apple 202 in the real-time image feed.
- FIG. 2A can illustrate the interface 204 a user sees when operating a camera of the computing device 210 to initialize an image conversation mode (e.g., a calorie mode and/or a nutrition mode 208 ).
- an image conversation mode e.g., a calorie mode and/or a nutrition mode 208
- images from the real-time image feed can be processed to identify objects within the images.
- an image can be transmitted from the computing device 210 to a remote device (e.g., a server device) for processing, or the image can be processed at the computing device 210 .
- Processing the image can include executing a computer vision algorithm for identifying and classifying objects within the image.
- one or more object identifiers or classifications can be generated or identified, and used by the assistant application to recommend image conversation modes. For example, when the camera is directed at the apple 202 , the assistant application can determine that the camera is directed at an apple 202 or food and identify suitable image conversation modes to suggest to the user.
- the image conversation modes can be correlated to various object identifiers, object types, object classifications, and/or any other descriptor for an object. Correlations for the objects and the conversation modes can be provided by an index accessible to the automated assistant, or inferred from one or more machine learning models, as discussed herein.
- the image conversation modes identified by the automated assistant can be selectable at the interface 204 as a first selectable element 206 and a second selectable element 208 .
- the first selectable element 206 can identify a first image conversation mode (e.g., “CALORIE MODE”) and the second selectable element 208 can identify a second image conversation mode (e.g., “NUTRITION MODE”).
- the automated assistant can receive the selection and generate a query for providing data corresponding to the image conversation mode.
- the assistant application can access the data when the data is available at the computing device 210 .
- the query can be generated based on the object identifier for the object (e.g., the apple 202 ) in the image, and the image conversation mode selected by the user (e.g., the calorie mode).
- a query template of “calories in [most granular classification for object]” can be defined for a “calorie” conversation mode.
- the placeholder “[most granular classification for object]” can be filled in with an alias for the most granular classification for the object in the image. For instance, assume an image includes a large Red Delicious apple. If the most granular classification determined based on processing of the image is “apple”, the query can be “calories in apple”.
- a non-natural language query can be generated based on the selected conversation mode and an identifier of an object. For instance, for a “calorie” conversation mode, a query template of “[calorie parameter] for [node for most granular classification for object]” can be defined. The query template defines that the data responsive to the query is a calorie parameter defined, in an entity database, for a node of the entity database that corresponds to the most granular classification of the object.
- the query can be transmitted to a remote device that hosts the assistant application, a search engine, a knowledge graph system, and/or other system(s) that are responsive to queries.
- the remote device can use the query, that is based on the object identifier (e.g., “apple”) and the image conversation mode selected, to provide the assistant application with data (e.g., calorie content of the apple) for presenting at the interface 204 .
- a search engine can provide an “authoritative answer” (if any) to the query as the data, or a most pertinent snippet from the most highly ranked document responsive to the query as the data.
- a knowledge graph system can identify particular node(s) of a knowledge graph based on the query, and provide information from the node(s) (or corresponding to the nodes) as the data.
- the remote device can store the data or access the data from a separate server that includes application data or web data satisfying the query.
- the assistant application receives the data, the data can be presented at the interface 204 while the camera is directed at the apple 202 .
- the data e.g., calorie content
- the data can be presented along with other suggestions for image conversation modes.
- a query can be transmitted to a remote device for identifying the data that will satisfy the query.
- the remote device which can host the assistant application, can also identify other conversation modes that might be useful to a user that is interested in the calorie content of food.
- the assistant application can include a health tracker image conversation mode that allows the user to direct the camera at different foods in order to see how they fit in the diet or daily caloric intake for the user.
- the automated assistant can compare the nutritional value or caloric content of food the user previously ate to the food that the camera is being directed to.
- the automated assistant can cause the interface 204 to present the user with notifications when a particular food item is outside of the diet, or cause their desired daily caloric intake to be surpassed for the day.
- the health tracker image conversation mode can be represented as an additional selectable element in response to the user selecting the calorie mode or nutrition mode.
- FIG. 2B illustrates a view 212 of the computing device 210 operating in an image conversation mode.
- FIG. 2B illustrates an automated assistant application causing the interface 204 to provide calorie data 214 at the interface while the camera is directed at a different object 218 (a banana).
- the automated assistant can also cause the interface 204 to provide additional suggestions for image conversation modes while operating an image conversation mode. For instance, the user can select the calorie mode from FIG. 2A and redirect the camera from object 202 of FIG. 2A to object 218 of FIG. 2B .
- the real-time image feed provided at the interface 204 can update with the object 218 and an image from the real-time image feed can be processed to identify objects in the image.
- the image can be processed and an object identifier can be generated from the image.
- the object identifier can then be used by the automated assistant to generate a query for identifying caloric data associated with the object 218 .
- the caloric data e.g., “120 CALORIES”
- the caloric data can then be presented at the interface 204 as a graphical element 214 while the camera is directed at the object 218 .
- the automated assistant can cause the interface 204 to present other suggestions for image conversation modes in response to the camera being redirected from one object (e.g., object 202 ) to another object (e.g., object 218 ).
- a machine learning model can determine probabilities for the image conversation modes to be suggested to the user.
- the image conversation modes can be ranked according to their corresponding probabilities and the top N (e.g., 1, 2, 3, etc.) image conversation modes can be suggested to the user while the user is directing the camera at an object.
- N e.g., 1, 2, 3, etc.
- a price image conversation mode can be presented to the user in response to the user previously selecting a different image conversation mode (e.g., the calorie image conversation mode) and redirecting the camera to a different object (e.g., object 218 ). If the user selects the price image conversation mode (e.g., by selecting the additional selectable element 216 ), a price for the object 218 can be presented at the interface 204 with, or in place of, the graphical element 214 (that displays the data responsive to the calorie mode).
- a different image conversation mode e.g., the calorie image conversation mode
- object 218 e.g., object 218
- FIG. 3A illustrates a view 300 of a computing device 312 operating an automated assistant application capable of using context data and image data to provide data and/or execute certain controls.
- the automated assistant can provide suggestions regarding image conversation modes to operate in according to the context data and the image data.
- a user can be operating a camera application of the computing device 312 and directing a camera of the computing device 312 at an object 302 (e.g., a car).
- An image provided by the camera can be processed by the automated assistant or a separate application for identifying one or more objects in the image.
- the automated assistant can receive or generate an object identifier (e.g., a data object that identifies the car), which can be used by the automated assistant to select one or more image conversation modes to present at the interface 304 .
- an object identifier e.g., a data object that identifies the car
- a context identifier e.g., a data object that identifies a context of the image
- a user can be walking through a car lot shopping for a car and using the camera of the computing device 312 to capture images of the cars in the car lot.
- the automated assistant can receive an image of a car, such as the image presented at the interface 304 of FIG. 3A , and use the image to determine that the camera is directed at the car.
- the automated assistant can determine image conversation modes to suggest to the user according to the objects identified in the image. For instance, because the image includes a car (i.e., object 302 ), the automated assistant can identify image conversation modes associated with cars, such as, for example, a price mode, a fact mode, and/or a directions mode. Furthermore, the automated assistant can determine a context of the image to select the image conversation modes to suggest to the user.
- the context can be based on a time at which the image was captured, a location at which the image was captured, an event associated with the image, and/or any other contextual descriptor that can be embodied in data.
- the location at which the image was captured can be a car lot for buying cars, thereby indicating the user would be interested in learning about the car rather than receiving directions for going somewhere with the car. Therefore, the object identifier (e.g., “car”) and a context identifier (e.g., “a car lot”) can be used in combination by the automated assistant to select the image conversation modes to be suggested to the user. For example, the price mode and the fact mode can be suggested to the user, while the directions mode can be omitted from the suggestions.
- an index or table may define the price mode and the fact mode as being more relevant to the combination of an object identifier of “car” and a context identifier of “car lot”, than is a “directions mode” to the combination.
- the suggestions can be identified in a first selectable element 306 and a second selectable element 308 at the interface 304 , as provided in FIG. 3A .
- each of the object identifier and the context identifier can be input into one or more machine learning models (e.g., a deep learning model) for correlating probabilities to each of the image conversation modes available to the automated assistant.
- the object identifier for object 302 can be provided to a first learning model for determining the probabilities or ranks for image conversation modes to suggest to the user.
- the context identifier that identifies the context of the object can be provided to a second machine learning model for determining the probabilities or ranks for image conversation modes to suggest to the user. Probabilities from the first learning model and the second learning model can be combined according to their respective image conversation modes, and the image conversation modes corresponding to the highest probabilities can be presented to the user.
- the price mode and the fact mode can be associated with higher probabilities than a directions mode when the object identifier is a car and the context identifier is a car lot.
- the assistant application can provide a graphical element 310 that advises the user to tap on different objects, or different portions of an object, presented at the interface in order receive data or suggestions of image conversation modes associated with the selected objects.
- An image provided by the camera can be processed by the assistant application or separate application to identify the parts of the image that correspond to objects.
- Each of the objects can be correlated to object identifiers such that, when the user taps on a portion of the interface, the object identifier can be processed to provide suggestions for image conversation modes.
- the automated assistant can generate object identifiers for the car and a tire 314 from the image at the interface 304 .
- the automated assistant can generate different object identifiers from an image using one or more techniques that can classify an image on a pixel-by-pixel basis, or a pixel group-by-pixel group basis. For example, each N ⁇ N group of pixels of the image can be associated with one or more corresponding object identifiers (and optionally corresponding probabilities for each of multiple object identifiers). For instance, a group of pixels that correspond to the tires of the car can be associated most strongly with a “tire” object classification, whereas groups of pixels that correspond to other portions of the car are associated most strongly with a “car” classification.
- image conversation mode(s) most relevant to a “tire” classification can be presented. If, on the other hand, the user selects one or more of the “car” pixels, image conversation mode(s) most relevant to a “car” classification can be presented.
- FIG. 3B illustrates a view 316 of a computing device 312 operating an automated assistant in a price image conversation mode in which prices of objects are presented at the interface 304 when the user directs the camera at the objects.
- a user can select the price mode (e.g., selectable element 306 ) presented in FIG. 3A to cause a conversation element 318 to be presented at the interface 304 .
- the conversation element 318 can include data related to the object selected by the user (e.g., the car).
- the automated assistant can cause a query to be generated for identifying a price of the car, as well as any other data that can assist the user in reflecting on the price of the car (e.g., the model and the year).
- the suggestions for image conversation modes at the interface 304 can be modified. For instance, a selectable element 320 can be presented in place of selectable element 306 , in order to provide the user with the option of entering a review image conversation mode. Furthermore, the unselected selectable element 308 can remain at the interface 304 or be replaced by a different selectable element that identifies a different image conversation mode.
- the review image conversation mode can cause the automated assistant to provide web reviews at the interface 304 for objects at which the camera is directed.
- the web reviews can be provided by one or more remote devices that host web data or application data associated with user-submitted reviews of particular objects (e.g., the car).
- the fact mode when selected can cause the automated assistant to provide facts associated with one or more objects in the image.
- the fact data can also be provided by one or more remote device that host web data and/or application data associated the object.
- the fact data and/or the web reviews can be presented in an interface element (e.g., similar to the conversation element 318 ) when the assistant application is operating in the fact image conversation mode or the review image conversation mode and the camera is directed at an object (e.g., the car).
- FIG. 4 illustrates a method 400 for interacting with an automated assistant application using an image from a real-time image feed provided by a camera of a computing device.
- the method 400 can be performed by a computing device, a server device, and/or any other apparatus suitable for causing data to be presented at a graphical user interface.
- the method 400 can include a block 402 of receiving image data from a camera of a computing device.
- the image data being generated by the camera in response to an object being present in a field of view of the camera.
- the object can be any object that can be captured in an image by a camera. For instance, the object can be a street in a city.
- the method 400 at block 404 can include determining, based on the image data, an object identifier for the object.
- the object identifier can correspond to the object itself (e.g., a street), a component of, or a separate object within the object.
- the object identifier can identify a restaurant located at the street that is in the field of view of the camera.
- the object identifier can be generated by the automated assistant or a remote device that has received the image data from the camera.
- the generation of the object identifier can include an optical character recognition (OCR) algorithm that can identify text within the image data (e.g., the name of the restaurant). Additional and/or alternative image processing techniques can be utilized to generate the object identifier.
- OCR optical character recognition
- the method 400 can further include a block 406 of causing a selectable element to be graphically represented at the display device.
- the selectable element can identify one or more image conversation modes in which the automated assistant can operate.
- block 406 includes selecting an image conversation mode based on the object identifier identified at block 404 , and causing the selectable element to be graphically represented based on it corresponding to the image conversation mode.
- the image conversation mode can be selected from a plurality of available image conversation modes. Each image conversation mode can use image data from the camera to elicit data from and/or execute functions of the automated assistant.
- the selectable element can include text that identifies a review image conversation mode in which the automated assistant provides web reviews for objects at which the camera of the computing device is directed. In this way, the user does not necessarily have to provide textual or verbal commands to the automated assistant, but rather, can simply direct the camera at different objects to elicit a response.
- the method 400 can also include a block 408 of receiving a selection of the selectable element.
- the selection can be made at a touch interface of the display device, through a verbal command (e.g., “Assistant, please start the review mode”), or a textual command.
- a verbal command e.g., “Assistant, please start the review mode”
- a textual command e.g., “Assistant, please start the review mode”
- object data can be identified using at least one object identifier for an object in the image, and the conversation mode that corresponds to the selectable element selected by the selection of block 408 .
- the object identifier for the object can be the same object identifier utilized in block 406 and/or can include an additional object identifier (e.g., an object identifier that identifies the object more granularly than the object identifier utilized in block 406 ).
- the automated assistant can identify the object data at the computing device using data available at a memory device of the computing device. Alternatively, the automated assistant can provide one or more queries to one or more remote devices for gathering object data for presenting at the display device.
- a query can be a search engine query provided to a server that hosts the search engine.
- Results from the query can include reviews of the object as submitted by other people, and the object data can be identified based on one or more of the results. For instance, when the object is a restaurant, and the automated assistant is operating in the review image conversation mode, the object data can include portions of one or more of the reviews identified by the search engine.
- the method 400 at block 412 can include causing the object data to be graphically represented at the display device simultaneous to the object being graphically represented at the display device.
- the object data can be presented over the real-time image feed.
- a separate object identifier for the different restaurant can be generated by the automated assistant or a remote device. The separate object identifier can be used to retrieve reviews or other object data related to the other restaurant, and the reviews or other object data can be presented at the display device automatically.
- FIG. 5 illustrates a method 500 for providing object data at an interface of a computing device based on an object at which a camera of the computing device is directed.
- the method 500 can be performed by a computing device, a server device, and/or any other apparatus capable of processing image-related data.
- the method 500 can include a block 502 of operating an assistant application in an image conversation mode in which the assistant application is responsive to a real-time image feed provided by a camera of a computing device.
- the assistant application can be provided at the computing device or otherwise accessible to the computing device.
- the image conversation mode can be an operating mode that causes the assistant application to provide graphical, audible, or other output in response to the camera being directed at one or more particular objects.
- the image conversation mode can be a fact mode in which the assistant application provides facts about an object when a user directs the camera at the object. This can be helpful when the user is on vacation and is interested in their surroundings but may not be comfortable providing multiple verbal or textual queries to their assistant application to receive information about their surroundings.
- the method 500 at block 504 can include causing the assistant application to provide object data at an interface of the computing device where the image feed is displayed.
- the object data can correspond to a first object graphically represented in the image feed and a type of data associated with the image conversation mode. For instance, when the image conversation mode is a fact image conversation mode and the first object is a monument, the assistant application can provide a historical fact about the first object as the object data (e.g., “Construction of the Washington Monument began in 1848.”).
- the object data can be obtained from a remote server that hosts data related to the first object.
- An identifier for the object can be generated by the assistant application, or a remote device that employs one or more image processing techniques for identifying objects in images.
- the image of the first object can be transmitted to an image-based search engine that can receive images and provide web search results from the images.
- the web search results can include an object identifier (e.g., “the Washington monument”), which can be used by the assistant application to retrieve additional information about the object.
- the method 500 at block 506 can include receiving, at the interface of the computing device, a selection of a graphical representation of a second object at which the camera is directed.
- the selection of the graphical representation can include a tap gesture at a location on the interface where the second object is represented.
- the selection of the graphical representation can be the user directing the camera at the second object.
- the user can seamlessly maneuver the camera to different objects at a location in order to gather data about the different objects. For instance, while the assistant application is operating in the fact image conversation mode, the user can direct the camera at the first object (e.g., the Washington monument) and then redirect the camera toward a second object (e.g., the White House).
- the second object appears in the real-time image feed provided by the camera, the second object can be considered selected, at least with respect to the image conversation mode.
- the method 500 at block 508 can include causing the assistant application to provide different object data at the interface of the computing device.
- the different object data can correspond to the type of data associated with the image conversation mode. For instance, when the assistant application is operating in the fact conversation mode and the second object is the White House, the different object data can be a fact about the White House (e.g., “the first Oval Office in the White House was built in 1909.”).
- contextual data related to the camera being direct at the second object can be used as a basis for the object data that is presented to the user. For instance, the contextual data can identify the user as a visitor to the location of the second object, and therefore the assistant application can provide details about what to do at the location while the user is operating the image conversation mode.
- the contextual data can include a time when the second object is being viewed, and the assistant application can use the time to identify activities to do at or near the second object. For example, when the second object is the White House, and the time is the Monday after Easter, the second object data can include a message such as “Today is the Easter Egg Roll at the White House.”
- FIG. 6 is a block diagram of an example computer system 610 .
- Computer system 610 typically includes at least one processor 614 which communicates with a number of peripheral devices via bus subsystem 612 .
- peripheral devices may include a storage subsystem 624 , including, for example, a memory 625 and a file storage subsystem 626 , user interface output devices 620 , user interface input devices 622 , and a network interface subsystem 616 .
- the input and output devices allow user interaction with computer system 610 .
- Network interface subsystem 616 provides an interface to outside networks and is coupled to corresponding interface devices in other computer systems.
- User interface input devices 622 may include a keyboard, pointing devices such as a mouse, trackball, touchpad, or graphics tablet, a scanner, a touchscreen incorporated into the display, audio input devices such as voice recognition systems, microphones, and/or other types of input devices.
- pointing devices such as a mouse, trackball, touchpad, or graphics tablet
- audio input devices such as voice recognition systems, microphones, and/or other types of input devices.
- use of the term “input device” is intended to include all possible types of devices and ways to input information into computer system 610 or onto a communication network.
- User interface output devices 620 may include a display subsystem, a printer, a fax machine, or non-visual displays such as audio output devices.
- the display subsystem may include a cathode ray tube (CRT), a flat-panel device such as a liquid crystal display (LCD), a projection device, or some other mechanism for creating a visible image.
- the display subsystem may also provide non-visual display such as via audio output devices.
- output device is intended to include all possible types of devices and ways to output information from computer system 610 to the user or to another machine or computer system.
- Storage subsystem 624 stores programming and data constructs that provide the functionality of some or all of the modules described herein.
- the storage subsystem 624 may include the logic to perform selected aspects of method 400 , method 500 , and/or to implement one or more of the computing device 102 , the server device 112 , the assistant application 118 , the remote device 112 , and/or any other application or device discussed herein.
- Memory 625 used in the storage subsystem 624 can include a number of memories including a main random access memory (RAM) 630 for storage of instructions and data during program execution and a read only memory (ROM) 632 in which fixed instructions are stored.
- a file storage subsystem 626 can provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges.
- the modules implementing the functionality of certain implementations may be stored by file storage subsystem 626 in the storage subsystem 624 , or in other machines accessible by the processor(s) 614 .
- Bus subsystem 612 provides a mechanism for letting the various components and subsystems of computer system 610 communicate with each other as intended. Although bus subsystem 612 is shown schematically as a single bus, alternative implementations of the bus subsystem may use multiple busses.
- Computer system 610 can be of varying types including a workstation, server, computing cluster, blade server, server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computer system 610 depicted in FIG. 6 is intended only as a specific example for purposes of illustrating some implementations. Many other configurations of computer system 610 are possible having more or fewer components than the computer system depicted in FIG. 6 .
- the systems described herein collect personal information about users (or as often referred to herein, “participants”), or may make use of personal information
- the users may be provided with an opportunity to control whether programs or features collect user information (e.g., information about a user's social network, social actions or activities, profession, a user's preferences, or a user's current geographic location), or to control whether and/or how to receive content from the content server that may be more relevant to the user.
- user information e.g., information about a user's social network, social actions or activities, profession, a user's preferences, or a user's current geographic location
- certain data may be treated in one or more ways before it is stored or used, so that personal identifiable information is removed.
- a user's identity may be treated so that no personal identifiable information can be determined for the user, or a user's geographic location may be generalized where geographic location information is obtained (such as to a city, ZIP code, or state level), so that a particular geographic location of a user cannot be determined.
- geographic location information such as to a city, ZIP code, or state level
- the user may have control over how information is collected about the user and/or used.
Abstract
Description
Claims (17)
Priority Applications (11)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/700,106 US10607082B2 (en) | 2017-09-09 | 2017-09-09 | Systems, methods, and apparatus for image-responsive automated assistants |
PCT/US2018/050047 WO2019051293A1 (en) | 2017-09-09 | 2018-09-07 | Systems, methods, and apparatus for image-responsive automated assistants |
KR1020217027310A KR102421662B1 (en) | 2017-09-09 | 2018-09-07 | Systems, methods, and apparatus for image-responsive automated assistants |
JP2019568378A JP2020530604A (en) | 2017-09-09 | 2018-09-07 | Systems, methods, and equipment for image response automatic assistants |
KR1020197036386A KR102297392B1 (en) | 2017-09-09 | 2018-09-07 | System, method and apparatus for image responsive automated assistant |
EP18786452.5A EP3532940A1 (en) | 2017-09-09 | 2018-09-07 | Systems, methods, and apparatus for image-responsive automated assistants |
CN202311553420.8A CN117666896A (en) | 2017-09-09 | 2018-09-07 | Systems, methods, and apparatus for image response automatic assistant |
CN201880038695.9A CN110741331B (en) | 2017-09-09 | 2018-09-07 | Systems, methods, and apparatus for image response automatic assistant |
US16/806,505 US11417092B2 (en) | 2017-09-09 | 2020-03-02 | Systems, methods, and apparatus for image-responsive automated assistants |
JP2022074840A JP7461405B2 (en) | 2017-09-09 | 2022-04-28 | Systems, methods, and apparatus for image-responsive automated assistants - Patents.com |
US17/888,163 US20220392216A1 (en) | 2017-09-09 | 2022-08-15 | Systems, methods, and apparatus for image-responsive automated assistants |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/700,106 US10607082B2 (en) | 2017-09-09 | 2017-09-09 | Systems, methods, and apparatus for image-responsive automated assistants |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/806,505 Continuation US11417092B2 (en) | 2017-09-09 | 2020-03-02 | Systems, methods, and apparatus for image-responsive automated assistants |
Publications (2)
Publication Number | Publication Date |
---|---|
US20190080169A1 US20190080169A1 (en) | 2019-03-14 |
US10607082B2 true US10607082B2 (en) | 2020-03-31 |
Family
ID=63858031
Family Applications (3)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/700,106 Active 2038-04-04 US10607082B2 (en) | 2017-09-09 | 2017-09-09 | Systems, methods, and apparatus for image-responsive automated assistants |
US16/806,505 Active 2038-01-04 US11417092B2 (en) | 2017-09-09 | 2020-03-02 | Systems, methods, and apparatus for image-responsive automated assistants |
US17/888,163 Pending US20220392216A1 (en) | 2017-09-09 | 2022-08-15 | Systems, methods, and apparatus for image-responsive automated assistants |
Family Applications After (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/806,505 Active 2038-01-04 US11417092B2 (en) | 2017-09-09 | 2020-03-02 | Systems, methods, and apparatus for image-responsive automated assistants |
US17/888,163 Pending US20220392216A1 (en) | 2017-09-09 | 2022-08-15 | Systems, methods, and apparatus for image-responsive automated assistants |
Country Status (6)
Country | Link |
---|---|
US (3) | US10607082B2 (en) |
EP (1) | EP3532940A1 (en) |
JP (2) | JP2020530604A (en) |
KR (2) | KR102297392B1 (en) |
CN (2) | CN117666896A (en) |
WO (1) | WO2019051293A1 (en) |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20210319492A1 (en) * | 2018-08-08 | 2021-10-14 | Samsung Electronics Co., Ltd. | Electronic device for providing keywords related to product information included in image |
US11423253B2 (en) * | 2019-08-07 | 2022-08-23 | Capital One Services, Llc | Systems and methods for generating graphical user interfaces |
Families Citing this family (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10206014B2 (en) * | 2014-06-20 | 2019-02-12 | Google Llc | Clarifying audible verbal information in video content |
US9805125B2 (en) | 2014-06-20 | 2017-10-31 | Google Inc. | Displaying a summary of media content items |
US10607082B2 (en) * | 2017-09-09 | 2020-03-31 | Google Llc | Systems, methods, and apparatus for image-responsive automated assistants |
US11270168B1 (en) * | 2018-03-02 | 2022-03-08 | Autodata Solutions, Inc. | Method and system for vehicle image classification |
US10984503B1 (en) | 2018-03-02 | 2021-04-20 | Autodata Solutions, Inc. | Method and system for vehicle image repositioning using machine learning |
WO2020103001A1 (en) * | 2018-11-20 | 2020-05-28 | 华为技术有限公司 | Method for estimating object parameters and electronic device |
KR20210017087A (en) * | 2019-08-06 | 2021-02-17 | 삼성전자주식회사 | Method for recognizing voice and an electronic device supporting the same |
US11803887B2 (en) * | 2019-10-02 | 2023-10-31 | Microsoft Technology Licensing, Llc | Agent selection using real environment interaction |
US11553407B2 (en) * | 2020-12-14 | 2023-01-10 | Capital One Services, Llc | Methods and systems for signal interpretation via image analysis |
US11709653B1 (en) | 2022-04-11 | 2023-07-25 | Google Llc | Contextual assistant using mouse pointing or touch cues |
Citations (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20130044132A1 (en) * | 2007-10-18 | 2013-02-21 | Yahoo! Inc. | User augmented reality for camera-enabled mobile devices |
EP2595070A1 (en) | 2011-11-17 | 2013-05-22 | Acer Incorporated | Object data search systems and methods |
US20140152882A1 (en) * | 2012-12-04 | 2014-06-05 | Hand Held Products, Inc. | Mobile device having object-identification interface |
US9208177B2 (en) * | 2009-08-07 | 2015-12-08 | Google Inc. | Facial recognition with social network aiding |
US20160283564A1 (en) * | 2015-03-26 | 2016-09-29 | Dejavuto Corp. | Predictive visual search enginge |
US20160283595A1 (en) * | 2013-05-01 | 2016-09-29 | Camfind, Inc. | Image directed search |
US20170311053A1 (en) * | 2016-04-22 | 2017-10-26 | Microsoft Technology Licensing, Llc | Identifying entities based on sensor data |
US10185898B1 (en) * | 2013-05-01 | 2019-01-22 | Cloudsight, Inc. | Image processing including streaming image output |
US20190026939A1 (en) * | 2017-07-21 | 2019-01-24 | David M. Frankel | Systems and methods for blind and visually impaired person environment navigation assistance |
Family Cites Families (17)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2006146454A (en) * | 2004-11-18 | 2006-06-08 | Sony Corp | Information conversion device and method |
JP2009301248A (en) * | 2008-06-12 | 2009-12-24 | Canon Inc | Language processor |
JP5328810B2 (en) * | 2008-12-25 | 2013-10-30 | パナソニック株式会社 | Information display device and information display method |
US9977496B2 (en) * | 2010-07-23 | 2018-05-22 | Telepatheye Inc. | Eye-wearable device user interface and augmented reality method |
KR101337555B1 (en) | 2010-09-09 | 2013-12-16 | 주식회사 팬택 | Method and Apparatus for Providing Augmented Reality using Relation between Objects |
KR101923929B1 (en) * | 2012-06-06 | 2018-11-30 | 삼성전자주식회사 | Mobile communication terminal device for providing augmented reality service and method for changing to display for augmented reality service |
JP2014116852A (en) | 2012-12-11 | 2014-06-26 | Canon Inc | Monitoring system |
KR102098058B1 (en) | 2013-06-07 | 2020-04-07 | 삼성전자 주식회사 | Method and apparatus for providing information in a view mode |
US9690370B2 (en) * | 2014-05-05 | 2017-06-27 | Immersion Corporation | Systems and methods for viewport-based augmented reality haptic effects |
CN106937531B (en) * | 2014-06-14 | 2020-11-06 | 奇跃公司 | Method and system for generating virtual and augmented reality |
JP6705124B2 (en) * | 2015-04-23 | 2020-06-03 | セイコーエプソン株式会社 | Head-mounted display device, information system, head-mounted display device control method, and computer program |
US9798143B2 (en) | 2014-08-11 | 2017-10-24 | Seiko Epson Corporation | Head mounted display, information system, control method for head mounted display, and computer program |
US10725533B2 (en) * | 2014-09-26 | 2020-07-28 | Intel Corporation | Systems, apparatuses, and methods for gesture recognition and interaction |
TWI540522B (en) * | 2015-02-26 | 2016-07-01 | 宅妝股份有限公司 | Virtual shopping system and method utilizing virtual reality and augmented reality technology |
US10768772B2 (en) * | 2015-11-19 | 2020-09-08 | Microsoft Technology Licensing, Llc | Context-aware recommendations of relevant presentation content displayed in mixed environments |
US10607082B2 (en) * | 2017-09-09 | 2020-03-31 | Google Llc | Systems, methods, and apparatus for image-responsive automated assistants |
WO2019160194A1 (en) * | 2018-02-14 | 2019-08-22 | 엘지전자 주식회사 | Mobile terminal and control method therefor |
-
2017
- 2017-09-09 US US15/700,106 patent/US10607082B2/en active Active
-
2018
- 2018-09-07 EP EP18786452.5A patent/EP3532940A1/en not_active Ceased
- 2018-09-07 CN CN202311553420.8A patent/CN117666896A/en active Pending
- 2018-09-07 KR KR1020197036386A patent/KR102297392B1/en active IP Right Grant
- 2018-09-07 KR KR1020217027310A patent/KR102421662B1/en active IP Right Grant
- 2018-09-07 CN CN201880038695.9A patent/CN110741331B/en active Active
- 2018-09-07 JP JP2019568378A patent/JP2020530604A/en active Pending
- 2018-09-07 WO PCT/US2018/050047 patent/WO2019051293A1/en unknown
-
2020
- 2020-03-02 US US16/806,505 patent/US11417092B2/en active Active
-
2022
- 2022-04-28 JP JP2022074840A patent/JP7461405B2/en active Active
- 2022-08-15 US US17/888,163 patent/US20220392216A1/en active Pending
Patent Citations (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20130044132A1 (en) * | 2007-10-18 | 2013-02-21 | Yahoo! Inc. | User augmented reality for camera-enabled mobile devices |
US9208177B2 (en) * | 2009-08-07 | 2015-12-08 | Google Inc. | Facial recognition with social network aiding |
EP2595070A1 (en) | 2011-11-17 | 2013-05-22 | Acer Incorporated | Object data search systems and methods |
US20140152882A1 (en) * | 2012-12-04 | 2014-06-05 | Hand Held Products, Inc. | Mobile device having object-identification interface |
US20160283595A1 (en) * | 2013-05-01 | 2016-09-29 | Camfind, Inc. | Image directed search |
US10185898B1 (en) * | 2013-05-01 | 2019-01-22 | Cloudsight, Inc. | Image processing including streaming image output |
US20160283564A1 (en) * | 2015-03-26 | 2016-09-29 | Dejavuto Corp. | Predictive visual search enginge |
US20170311053A1 (en) * | 2016-04-22 | 2017-10-26 | Microsoft Technology Licensing, Llc | Identifying entities based on sensor data |
US20190026939A1 (en) * | 2017-07-21 | 2019-01-24 | David M. Frankel | Systems and methods for blind and visually impaired person environment navigation assistance |
Non-Patent Citations (2)
Title |
---|
European Patent Office, International Search Report and Written Opinion of PCT Ser. No. PCT/US2018/050047; 15 pages; dated Nov. 26, 2018. |
Google Developers; "Google I/O (Google I/O'17)", YouTube; Retrieved from Internet: URL:https://www.youtube.com/watch?v=Y2VF8tmLFHw [retrieved on Nov. 12, 2018] especially sections regarding Google Lens, e.g. from 24mn20s to 27mn04; 4 pages; May 17, 2017. |
Cited By (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20210319492A1 (en) * | 2018-08-08 | 2021-10-14 | Samsung Electronics Co., Ltd. | Electronic device for providing keywords related to product information included in image |
US11636529B2 (en) * | 2018-08-08 | 2023-04-25 | Samsung Electronics Co., Ltd. | Method and device for providing keywords related to product information included in image |
US11423253B2 (en) * | 2019-08-07 | 2022-08-23 | Capital One Services, Llc | Systems and methods for generating graphical user interfaces |
US20220351006A1 (en) * | 2019-08-07 | 2022-11-03 | Capital One Services, Llc | Systems and methods for generating graphical user interfaces |
US11748070B2 (en) * | 2019-08-07 | 2023-09-05 | Capital One Services, Llc | Systems and methods for generating graphical user interfaces |
Also Published As
Publication number | Publication date |
---|---|
CN117666896A (en) | 2024-03-08 |
US11417092B2 (en) | 2022-08-16 |
US20200202130A1 (en) | 2020-06-25 |
KR20210109052A (en) | 2021-09-03 |
CN110741331B (en) | 2023-12-08 |
JP2020530604A (en) | 2020-10-22 |
KR20200006563A (en) | 2020-01-20 |
KR102421662B1 (en) | 2022-07-14 |
KR102297392B1 (en) | 2021-09-03 |
US20220392216A1 (en) | 2022-12-08 |
JP7461405B2 (en) | 2024-04-03 |
CN110741331A (en) | 2020-01-31 |
US20190080169A1 (en) | 2019-03-14 |
WO2019051293A1 (en) | 2019-03-14 |
EP3532940A1 (en) | 2019-09-04 |
JP2022115933A (en) | 2022-08-09 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11417092B2 (en) | Systems, methods, and apparatus for image-responsive automated assistants | |
US11908187B2 (en) | Systems, methods, and apparatus for providing image shortcuts for an assistant application | |
US20230053873A1 (en) | Invoking automated assistant function(s) based on detected gesture and gaze | |
US9619046B2 (en) | Determining phrase objects based on received user input context information | |
CN110637295A (en) | Storing metadata relating to captured images | |
US20160224591A1 (en) | Method and Device for Searching for Image | |
US20200410049A1 (en) | Personalizing online feed presentation using machine learning | |
US20190318450A1 (en) | Dynamic adaptation of device interfaces in a voice-based system | |
JP7471371B2 (en) | Selecting content to render on the assistant device's display | |
CN110688011A (en) | Dynamic list composition based on modalities of multimodal client devices | |
US20240061694A1 (en) | Interactive application widgets rendered with assistant content | |
CN117099095A (en) | Intelligent advice for image zoom regions |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:NOWAK-PRZYGODZKI, MARCIN;BAKIR, GOEKHAN;SIGNING DATES FROM 20170905 TO 20170908;REEL/FRAME:043539/0967 |
|
FEPP | Fee payment procedure |
Free format text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044567/0001Effective date: 20170929 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PRE-INTERVIEW COMMUNICATION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NOTICE OF ALLOWANCE MAILED -- APPLICATION RECEIVED IN OFFICE OF PUBLICATIONS |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT RECEIVED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: PUBLICATIONS -- ISSUE FEE PAYMENT VERIFIED |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |