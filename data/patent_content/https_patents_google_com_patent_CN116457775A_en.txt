CN116457775A - Video clustering and analysis - Google Patents
Video clustering and analysis Download PDFInfo
- Publication number
- CN116457775A CN116457775A CN202280007124.5A CN202280007124A CN116457775A CN 116457775 A CN116457775 A CN 116457775A CN 202280007124 A CN202280007124 A CN 202280007124A CN 116457775 A CN116457775 A CN 116457775A
- Authority
- CN
- China
- Prior art keywords
- video
- videos
- given
- knowledge graph
- pair
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/41—Higher-level, semantic clustering, classification or understanding of video scenes, e.g. detection, labelling or Markovian modelling of sport events or news items
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/70—Information retrieval; Database structures therefor; File system structures therefor of video data
- G06F16/75—Clustering; Classification
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/901—Indexing; Data structures therefor; Storage structures
- G06F16/9024—Graphs; Linked lists
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/762—Arrangements for image or video recognition or understanding using pattern recognition or machine learning using clustering, e.g. of similar faces in social networks
- G06V10/7635—Arrangements for image or video recognition or understanding using pattern recognition or machine learning using clustering, e.g. of similar faces in social networks based on graphs, e.g. graph cuts or spectral clustering
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/23—Processing of content or additional data; Elementary server operations; Server middleware
- H04N21/234—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs
- H04N21/23418—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs involving operations for analysing video streams, e.g. detecting features or characteristics
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/27—Server based end-user applications
- H04N21/274—Storing end-user multimedia data in response to end-user request, e.g. network recorder
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for improved content analysis. A method comprising: a video uploaded by a video publisher is obtained. A conceptual index is generated for each given video. The concept index is generated based at least on (i) concepts conveyed by one or more objects depicted in the video and (ii) the level of saliency of the concepts in the given video. Based on the conceptual index of the video, a plurality of video packets are created. Each given video packet includes two or more different videos, each of the two or more different videos having a specified level of similarity to other videos in the given video packet. Based on the data obtained through the feedback loop, insight is generated regarding the plurality of video packets. The distribution of the at least one video is modified based on insight into a given video packet comprising the at least one video.
Description
Cross Reference to Related Applications
The present application claims priority from the israel application serial No. 287859 filed on 5, 11, 2021, which is incorporated herein by reference in its entirety.
Background
The present description relates to data processing and analysis of video. The amount of online video grows year by year, and everyone now having a computer is able to upload video content. Because the manner in which videos can be used to convey similar concepts is different, it is difficult to identify groupings of similar videos to analyze.
Disclosure of Invention
In general, one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of: obtaining, by a service device, a video uploaded by a video publisher; generating, by the service device, a concept index for each given video, wherein the concept index is generated based at least on (i) concepts conveyed by one or more objects depicted in the video and (ii) a level of saliency of the concepts in the given video; creating, by the service device, a plurality of video packets based on the conceptual index of the video, wherein each given video packet of the plurality of video packets is created to include two or more different videos, each of the two or more different videos having a specified level of similarity to other videos in the given video packet; generating, by the service device, insight regarding the plurality of video packets based upon data obtained through the feedback loop; and modifying, by the service device, a manner of distributing the at least one video over the network based on the insight regarding the given video packet including the at least one video.
Other embodiments of this aspect include corresponding systems, apparatus, and computer programs configured to perform the actions of the methods and encoded on computer storage devices.
These and other embodiments can optionally include one or more of the following features. Generating a conceptual index for each given video may include: for each given one of the videos: obtaining one or more knowledge graphs of portions of a given video, wherein each of the knowledge graphs represents one or more concepts conveyed by the given video; and determining, for each given knowledge graph, a presence share indicative of a level of saliency of the concept represented by the given knowledge graph, wherein the concept index is generated based at least in part on a number of instances of the given knowledge graph in the given video and a total presence share of the given knowledge graph over a length of the video.
Generating a conceptual index for each given video may include: for each given knowledge graph obtained for a given video: summing the presence shares of the given knowledge graph over the length of the given video; determining a number of portions of a given video described by a given knowledge graph; and generating a conceptual index for the given video based on the ratio of the summed presence shares relative to the number of portions.
The method may include: for each given knowledge graph obtained for a plurality of videos uploaded by a video publisher: generating an inverse document frequency metric for the given knowledge graph based on a total number of the plurality of videos represented by the given knowledge graph; and for each video of the plurality of videos, applying the generated inverse document frequency to the total existing share of the given knowledge graph.
The method may include: selecting a pair of videos from the plurality of videos; generating a count of shared knowledge maps between the pair of videos, wherein each shared knowledge map is a given knowledge map of the knowledge maps representing each video of the pair of videos; for each particular shared knowledge graph between the pair of videos: a shared similarity score for a particular shared knowledge graph is derived based on a minimum existing share of the knowledge graph for any of the pair of videos, and a likely similarity score for the particular shared knowledge graph is derived based on a maximum existing share of the knowledge graph for any of the pair of videos.
The method may include: generating a dissimilarity count based on a number of dissimilarity knowledge maps that represent only one of the videos in the pair of videos; calculating a dissimilarity score for each of the pair of videos based on the conceptual index of the dissimilarity knowledge graph for the pair of videos; and generating one or more cluster factors for the pair of videos based on the dissimilarity score, the potential similarity score, and the shared similarity score.
Creating the plurality of video packets may include: comparing, for each pair of videos, the clustering factor of the pair of videos with a specified similarity level; including in the same grouping a first pair of videos for which the clustering factor meets a specified level; and excluding from the same group a third video for which the clustering factor with respect to the first pair of videos does not satisfy the specified level.
Particular embodiments of the subject matter described in this specification can be implemented to realize one or more of the following advantages. Unlike conventional similarity techniques, the techniques discussed herein enable a computer to take substantially different videos, determine conceptual similarities between the different videos, and cluster the different videos together to improve the ability of the computer to evaluate the video groupings together. This results in a more efficient analysis system because instead of having to evaluate each video in a cluster (also referred to as a group), a subset of the representative videos and/or aggregate information of the cluster can be evaluated at run-time, enabling the computer to provide real-time responses to queries, which would not be possible if all videos had to be evaluated to provide responses to queries. Furthermore, when two videos may have very few (e.g., 1 or 2) features in common, but the similarity distance between these features is very small, the techniques discussed herein prevent errors in similarity determination that occur using conventional similarity techniques. In these cases, the similarity of a small number of common features may weigh more than all dissimilarities between videos, resulting in false determinations of video similarity.
The techniques discussed herein also reduce the amount of time required for a distribution policy of the computer Xi Shipin (or other content). Conventional computer systems require a significant learning period to determine the situation where delivering content to a client device would be an efficient use of server-side resources (e.g., network bandwidth, server core, etc.) required to deliver the content and/or an efficient use of client-side resources (e.g., battery consumption, processor cycles, data plan use, etc.) required to render the content. The learning period may be one week or more, and during the learning period, delivery of content tends to result in wasted server-side resources (e.g., server core usage) and wasted client-side resources (e.g., battery consumption) as the computer system learns the difference between the good time of delivering the content and the bad time of delivering the content. However, using the techniques described herein, the learning period, if eliminated, can be significantly reduced, thereby enabling the computer system to more efficiently utilize both server-side resources and client-side resources relative to using traditional learning period methods. This achieves this improvement in resource utilization by initially determining the cluster to which the new video (or other content) belongs and using the aggregate information from the videos in that cluster to initiate the delivery of the new video without the need for a long learning phase.
The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Drawings
FIG. 1 is a block diagram of an example online environment.
Fig. 2 is a block diagram illustrating concept extraction.
Fig. 3 is a block diagram illustrating video similarity between a pair of videos and video clustering by conceptual similarity.
FIG. 4 is a flow chart of an example process for using video similarity to provide insight (insight) into video groupings and to modify the manner in which videos are distributed.
FIG. 5 is a block diagram of an example computer system.
Like reference numbers and designations in the various drawings indicate like elements.
Detailed Description
This specification describes techniques for clustering videos based on overall concepts conveyed by different multimedia content (e.g., a combination of images, video, and/or audio), although there are differences between the different multimedia content that would otherwise prevent the different multimedia content from being identified as similar. Using an online video as an example, the resolution, coloring, spoken language, order of elements, and other aspects of the two videos may be significantly different, but the two videos may still convey similar concepts such that the two videos should be clustered together for analysis of the videos conveying the same/similar concepts. However, using conventional techniques, it is possible that the two videos may not be clustered together for analysis purposes, because conventionally, a computer has not been able to determine concept similarity in videos (or other multimedia content) when the videos (or other multimedia content) differ in their presentation properties and/or the manner in which the presentation of the different videos (or other multimedia content) conveys concepts.
Unlike conventional similarity techniques, the techniques discussed herein enable a computer to take substantially different videos, determine conceptual similarities between the different videos, and cluster the different videos together to improve the ability of the computer to evaluate the video packets together. As discussed throughout this specification, the evaluation of video groupings can be used to provide insight as to the concepts for which new videos should be developed, determine representative videos and/or aggregate information for each cluster (e.g., set of concepts), which can be used to reduce subsequent processing of videos in the groupings, and speed up optimization of content distribution rules to efficiently utilize server-side and client-side computing resources.
As discussed in more detail, the techniques generally involve extracting conceptual information from a video (or other content) and clustering the video based on conceptual similarity, regardless of how the conceptual information is conveyed (e.g., by different people, in different languages, or in different orders). Once the videos are clustered according to concepts, the videos may be analyzed on a per concept basis to provide insight into the different concepts. Visual mappings of videos may also be created to visually convey information about closely related concepts and closely related video groupings. By selecting a representative video for each video packet and de-duplicating substantially the same video except for differences in resolution, color scale, compression techniques, etc., visual mapping may be de-cluttered, thereby enhancing the usefulness of the mapping in limited display areas. The representative video may be selected in a variety of ways (e.g., similarity to other videos, feedback data, etc.). The visualization of the video map may be output to the user, and the map may include interaction features that enable the user to obtain information about each video packet through interaction with the map. For example, a user may click on a representative image of a video packet and be provided with information about the video in the packet. Similarly, a user may interact with the map (e.g., zoom in/out) to go deep into the video groupings, which may reveal additional sub-groupings of video, or view wider grouping level information.
Throughout this specification, video is used as an example of multimedia content to which the techniques discussed may be applied to provide more specific examples and specific use cases. However, the techniques discussed herein may be applied to any form of content such that the use of video for example purposes should not be considered limiting. In other words, throughout the specification, the phrase "video" may be generally replaced with the term "content".
Fig. 1 is a block diagram of an example online environment 100. The example environment 100 includes a network 102, such as a Local Area Network (LAN), wide Area Network (WAN), the Internet, or a combination thereof. Network 102 connects electronic document server 104, user device 106, digital component server 108, and service 110. The example environment 100 may include many different electronic document servers 104, user devices 106, digital component servers 108, and service devices 110.
Client device 106 is an electronic device capable of requesting and receiving resources over network 102. Example client devices 106 include personal computers, mobile communication devices, digital assistance devices, and other devices capable of sending and receiving data over the network 102. Client device 106 typically includes a user application (e.g., a web browser) to facilitate sending and receiving data over network 102, although a native application executed by client device 106 may also facilitate sending and receiving data over network 102.
Digital auxiliary devices include devices having a microphone and a speaker. Digital auxiliary devices are typically able to receive input through speech and respond with content using auditory feedback, and may present other auditory information. In some cases, the digital auxiliary device also includes or communicates with a visual display (e.g., through a wireless or wired connection). Feedback or other information may also be provided visually when a visual display is present. In some cases, the digital auxiliary device may also control other devices, such as lights, locks, cameras, climate control devices, alarm systems, and other devices registered with the digital auxiliary device.
An electronic document is data that presents a set of content at a client device 106. Examples of electronic documents include web pages, word processing documents, portable Document Format (PDF) documents, images, videos, search result pages, and feed sources. A local application (e.g., an "app") such as an application installed on a mobile, tablet, or desktop computing device is also an example of an electronic document. The electronic document may be provided to the client device 106 by the electronic document server 104 ("electronic file server (Electronic Doc Server)"). For example, the electronic document server 104 may include a server hosting a publisher website. In this example, the client device 106 may initiate a request for a given publisher web page, and the electronic server 104 hosting the given publisher web page may respond to the request by sending machine-executable instructions that initiate rendering of the given web page at the client device 106.
In another example, the electronic document server 104 may include a video server from which the client device 106 may download video (e.g., user-created video or other video). In this example, the client device 106 may download files needed to play the video in a web browser or local application configured to play the video.
The electronic document may include various contents. For example, the electronic document may include static content (e.g., text or other specified content) within the electronic document itself and/or that does not change over time. The electronic document may also include dynamic content that may change over time or on a per request basis. For example, a publisher of a given electronic document may maintain a data source that populates portions of the electronic document. In this example, the given electronic document may include a script that causes the client device 106 to request content from a data source when the given electronic document is processed (e.g., rendered or executed) by the client device 106. The client device 106 integrates the content obtained from the data source into a given electronic document to create a composite electronic document that includes the content obtained from the data source.
In some cases, a given electronic document may include a digital component script that references service device 110 or a particular service provided by service device 110. In these cases, when the client device 106 processes a given electronic document, the client device 106 executes the digital component script. Execution of the digital component script configures client device 106 to generate a request for digital component 112 (referred to as a "component request") that is transmitted to digital component distribution system 110 over network 102. For example, the digital component script may enable the client device 106 to generate a packet data request that includes a header and payload data. Component request 112 can include event data specifying characteristics such as the name (or network location) of the server from which the digital component is being requested, the name (or network location) of the requesting device (e.g., client device 106), and/or information that service 110 can use to select one or more digital components, or other content provided in response to the request. The component request 112 is transmitted by the client device 106 to a server of the service apparatus 110 over the network 102 (e.g., a telecommunications network).
Component request 112 can include event data specifying other event characteristics, such as characteristics of the electronic document being requested and the location of the electronic document at which the digital component can be presented. For example, the digital component distribution system 110 may be provided with event data specifying a reference (e.g., a URL) to an electronic document (e.g., a web page) in which the digital component is to be presented, an available location of the electronic document available for presentation of the digital component, a size of the available location (e.g., a portion of a page or duration within a video), and/or a media type that is eligible for presentation at the location. Similarly, event data specifying keywords associated with an electronic document ("document keywords") or entities referenced by the electronic document (e.g., people, places, or things) may also be included in the component request 112 (e.g., as payload data) and provided to the service device 110 to facilitate identifying digital components that are eligible for presentation with the electronic document. The event data may also include search queries submitted from the client device 106 to obtain search results pages (e.g., search results pages that present general search results or video search results).
Component request 112 can also include event data related to other information, such as information that a user of the client device has provided, geographic information indicating the state or region from which the component request was submitted, language settings of the client device, or other information that provides context about the environment in which the digital component will be displayed (e.g., time of day of the component request, day of the week of the component request, device type such as a mobile device or tablet device in which the digital component will be displayed). The component request 112 may be transmitted, for example, over a packet network, and the component request 112 itself may be formatted as packet data with a header and payload data. The header may specify the destination of the packet and the payload data may include any of the information described above.
Service device 110 selects digital components (e.g., video files, audio files, images, text, and combinations thereof, all of which may take the form of advertising content or non-advertising content) to be presented with a given electronic document in response to receiving component request 112 and/or using information included in component request 112. In some implementations, the digital components are selected in less than one second to avoid errors that may be caused by delayed selection of the digital components. For example, providing a delay in the digital component in response to the component request 112 may result in a page load error at the client device 106, or result in portions of the electronic document remaining unfilled even after other portions of the electronic document are presented at the client device 106. Further, as the delay in providing the digital components to the client device 106 increases, it is likely that the electronic document will no longer be presented at the client device 106 when the digital components are delivered to the client device 106, thereby negatively affecting the user experience with respect to the electronic document. Further, for example, if the electronic document is no longer presented at the client device 106 at the time the digital component is provided, delays in providing the digital component may result in delivery failure of the digital component.
In some implementations, the service apparatus 110 is implemented in a distributed computing system that includes, for example, a server and a set 114 of multiple computing devices, the server and the set 114 of multiple computing devices being interconnected and identifying and distributing digital components in response to the request 112. The set 114 of multiple computing devices operate together to identify a set of digital components from a corpus of millions of available digital components (DC 1-x) that qualify for presentation in an electronic document. For example, millions of available digital components in the digital component database 116 may be indexed. Each digital component index entry may reference a corresponding digital component and/or include a distribution parameter (DP 1-DPx) that facilitates (e.g., triggers, regulates, or limits) the distribution/transmission of the corresponding digital component. For example, the distribution parameters may facilitate (e.g., trigger) the transfer of the digital component by requiring the component request to include at least one criterion that matches (e.g., exactly matches or has some pre-specified level of similarity) one of the distribution parameters of the digital component.
The identification of qualified digital components may be partitioned into a plurality of tasks 117a-117c, which are then distributed among computing devices within the collection 114 of computing devices. For example, different computing devices in the collection 114 may each analyze different portions of the digital component database 116 to identify various digital components having distribution parameters that match the information included in the component request 112. In some implementations, each given computing device in the collection 114 may analyze a different data dimension (or set of dimensions) and communicate (e.g., transfer) the results of the analysis (Res 1-Res 3) 118a-118c back to the digital component distribution system 110. For example, the results 118a-118c provided by each computing device in the collection 114 may identify a subset of digital components that qualify for distribution in response to component requests and/or a subset of digital components having particular distribution parameters. Identifying the subset of digital components may include: for example, the event data is compared to the distribution parameters and a subset of digital components having distribution parameters that match at least some features of the event data are identified.
Service 110 aggregates results 118a-118c received from the collection 114 of computing devices and uses information associated with the aggregated results to select one or more digital components to be provided in response to the request 112. For example, service device 110 may select a set of highest ranked digital component(s) based on the results of one or more content evaluation processes. Further, the service apparatus 110 may generate and transmit reply data 120 (e.g., digital data representing a reply) over the network 102, the reply data 120 enabling the client device 106 to integrate the set of highest ranked digital components into the given electronic document such that the set of highest ranked digital components are presented with the content of the electronic document at the display of the client device 106.
In some implementations, the client device 106 executes instructions included in the reply data 120 that configure and enable the client device 106 to obtain the set of highest ranked digital components from one or more digital component servers. For example, the instructions in reply data 120 may include a network location (e.g., a Uniform Resource Locator (URL)) and a script that causes client device 106 to transmit a Server Request (SR) 121 to digital component server 108 to obtain a given highest ranked digital component from digital component server 108. In response to the request, the digital component server 108 will identify the given highest ranked digital component specified in the server request 121 (e.g., within a database storing a plurality of digital components) and transmit digital component data (DC data) 122 to the client device 106, the digital component data 122 presenting the given highest ranked digital component in the electronic document at the client device 106.
The service device 110 may utilize various techniques to evaluate the qualifications of various different digital components that may be used to communicate in response to a given component request (e.g., a separate component request). For example, the service apparatus 110 may compare the qualification scores of the various different digital components and select one or more digital components with the highest qualification score as the digital components to be transmitted to the client device 106 in response to a given component request. In some cases, the initial qualification score may be determined based on one or more factors. For example, one provider (P1) of a video clip (VC 1) may provide a distribution standard X for VC1, while a different provider (P2) of a different video clip (VC 2) may provide a different distribution standard Y. For purposes of this example, assume that a component request requests only one digital component to be presented with a particular web page or particular video. To select which of the two video clips will be provided, service apparatus 110 may rank VC1 and VC2 based on their respective qualification scores, which may be determined based on a comparison of the distribution criteria provided by P1 and P2 with the information included in request 112. In some implementations, the set of distribution criteria most similar to the information in request 112 will have the highest qualification score, and therefore the highest ranking. Service 110 may select the highest ranked video clip to transmit to the client device in response to component request 112.
In some cases, the qualification score is enhanced (or changed) based on one or more other factors. For example, the service 110 may generate an adjusted qualification score for a digital component based on an initial qualification score for the digital component and a quality factor for the digital component.
The quality factor of a given digital component may quantify the likelihood that the given digital component is the appropriate digital component to be provided in response to a given component request. In some implementations, the quality factor is determined based on one or more features specified by the event data. More specifically, the service device 110 may input one or more characteristics from the event data (e.g., geographic information and/or terms from the electronic document) into a machine learning system that outputs a predicted distribution result that may be used as a quality factor. The predicted distribution result may be represented, for example, as a predicted interaction rate (e.g., click through rate, play completion rate, or another measure of interaction with the digital component) of the digital component in the context of the current component request.
Once the quality factor is obtained, it may be applied to the initial qualification score to arrive at an adjusted qualification score. For example, the adjusted qualification score (adjusted eligibility score, AES) may be a product of the initial qualification score and the quality factor (e.g., AES = quality factor the initial qualification score). In some cases, the adjusted qualification scores for the various different digital components may be used to rank the digital components (e.g., from highest to lowest), and one or more highest ranked digital components may be selected to be transmitted in response to the component request.
The effectiveness of the distributed content (e.g., video) may be assessed based on the user's response to the content delivered to the client device, which may also be used as an indicator of whether the use of server-side and/or client-side computing resources is reasonable. While tracking user responses to various portions of content (e.g., various videos) may provide insight as to which portions of content result in efficient use of computing resources required to deliver the content, such evaluation does not provide insight as to why some portions of content are more suitable for distribution (e.g., better utilization of limited server-side and/or client-side computer resources and/or elicitation of more aggressive user responses). In order to provide this type of deeper insight as to why some portions of content are more suitable for distribution, a collective analysis of multiple similar portions of content needs to be performed, which entails that the portions of content need to be grouped together in a meaningful way.
One way to group portions of content is by their conceptual similarity. As used throughout this document, conceptual similarity refers to similarity between concepts conveyed by two or more portions of content. As described in more detail below, the conceptual similarity between two portions of content may be determined based on a level of match between concepts conveyed by the two portions of content, which may be determined independent of the manner in which those concepts are conveyed.
For example, assume video 1 (V1) is a 30 second animated video in which 25 seconds in the video depict a cat chasing a mouse, while the remaining 5 seconds of the video present the text "dial 555-555-5555 call pest killer X". Suppose also that video 2 (V2) is a 30 second live video that does not include any text presented, but depicts a cat chasing a mouse, which jumps to a safe place. In this example, based on the fact that a majority of each video depicts a cat chasing a mouse, the two videos may be considered very similar to the concept of a cat chasing a mouse, although there are significant differences in how to convey the concept (e.g., animation versus live). If these two videos are grouped with other videos that point to the concept of cat pursuing mice, then the effectiveness of the video that points to the concept of cat pursuing mice can be effectively assessed using the aggregated information (e.g., user response information), regardless of the differences in how those concepts are conveyed.
As discussed in detail below, the effectiveness of the video directed to each concept may be drawn on a per user group basis such that insight regarding concepts that should be used in the video (or other content) may be generated based on the effectiveness of the target audience and the different concepts when presented to the target audience. Furthermore, grouping and evaluating videos in this manner may enable a computer system to learn optimal distribution parameters for new videos more quickly based on insight determined for a set of pre-existing videos belonging to the same cluster (or video grouping) as the new video, as described below. The ability to learn optimal distribution parameters more quickly reduces the waste of resources that would result from using a traditional learning period (which may take days or weeks), and also reduces the computational resources required by a computer to learn the distribution parameters.
Service device 110 includes a concept evaluation device ("CEA") 150 that includes one or more data processors. CEA 150 is configured to cluster videos (and other content) based on concept similarity and evaluate the resulting video packets (or other content packets) to determine the validity of different video concepts. The assessment may be used to present insight regarding the use of some concepts according to the target audience (e.g., the group of users to which the content is to be directed). As described below, these insights may be visually conveyed in a tabular format, a chart format, or other format. Further, these insights can be used to recommend concepts of new videos, provide concept feedback about groupings of existing videos, and initialize distribution rules of new videos based on the concepts to which the new videos are directed.
Fig. 2 is a block diagram 200 illustrating concepts extracted from video. Block diagram 200 includes a video player 202 in which video is presented. The video player 202 is shown as part of a web page, but the video player 202 may be implemented in a local application (e.g., on a mobile or tablet device) or in another suitable device (e.g., a streaming video player plugged into a television or embedded into a television). Furthermore, the concept extraction discussed below may be performed independently of the video player 202, so the video player 202 is included merely to aid in describing the concept extraction. Further, concept extraction is discussed as being performed by service device 110, but concept extraction may be performed on a portion of the service device (e.g., CEA 150) or another suitable device.
The video available for playback by the video player 202 has a playback duration, also referred to as the length of the video. For example, the playback timer 204 in the video player 202 shows that the playback duration of the currently loaded video is one minute. To evaluate the context of the video, the service device 110 (e.g., through CEA 150) may determine the context of the video across the length of the video. For example, the service device 110 may select different time stamps for the context of the extracted video over the length of the video. In this example, the video is partitioned into 4 different segments ("quartiles") by selecting time stamps at 0, 15, 30, 45, and 60 seconds, respectively identified by reference numerals 206, 208, 210, 212, and 214. This quartile division is used to illustrate the concept of timestamp selection while preventing drawing confusion. In practice, however, the service device 110 may select the time stamp finely or coarsely as desired. In some implementations, the timestamp is selected at every second of the video duration, and in some implementations, the service device 110 may select a sub-second timestamp (e.g., 1/6 second).
Each of the selected timestamps is an indication of a portion of the video that is to be evaluated to determine the concept being conveyed in the video at that point. In some implementations, the service device 110 examines the video frames presented at each timestamp to determine the context of the video at that timestamp. Returning to the previous example, the service device 110 may evaluate the video at the beginning of the video (e.g., 0 seconds), at 15 seconds, at 30 seconds, at 45 seconds, and at the end of the video (e.g., at 1:00).
The evaluation of the video at each timestamp may include identifying any objects presented by the video, attributes of those objects, and/or relationships between objects presented by the video. For example, the video presented in video player 202 includes person 216, chair 218, table 220, and two more persons. In this example, the service device 110 may identify the objects, their relative locations, and attributes such as color, pattern, or other visual attributes using object detection techniques known in the art.
Once the service device 110 obtains information about the objects depicted by the video at a given timestamp, the service device 110 may generate a knowledge graph representing concepts conveyed by the video at the given timestamp. As used in this document, a knowledge graph is a representation of the relationships between unique entities, and the knowledge graph may be stored in one or more data stores. Each node may represent a different unique entity, and the nodes may be connected by graph edges (e.g., logical links) that represent relationships between the entities. The knowledge graph may be implemented, for example, in a graphical form or in a data structure including data representing each node and data representing relationships between each node.
Returning to the example above, the knowledge graph of the video depicted in video player 202 may include separate nodes for person 216, chair 218, table 220, and each of the other two persons. Each of these nodes may be connected by graph edges representing relationships between entities. For example, a node representing chair 218 may be linked by an edge labeled "sitting" to a node representing a person sitting in the chair to represent the fact that the video depicts a person sitting in chair 218. Similarly, the node representing the table 220 may also be linked to the node representing the person sitting in the chair by an edge labeled "hand on" to represent the fact that the person sitting in the chair 218 has their hand on the table 220. Other attributes of the video may be similarly represented by knowledge graphs, such as color, text, or other attributes. Of course, the edges between nodes need not include any labels, but may simply indicate some relationship between nodes.
For a given video, the service device 110 may create multiple knowledge maps. For example, the service device 110 may create a different knowledge graph for each timestamp selected for a given video. In the above example, the service device 110 will generate five different knowledge maps 222, 224, 226, 228, and 230. The service 110 stores each knowledge graph in the knowledge graph database 232 for further processing. For example, as described below, the service apparatus 110 may evaluate the conceptual similarities between videos using the knowledge graph and cluster the videos based on the conceptual similarities.
Fig. 3 is a block diagram 300 illustrating video similarity between a pair of videos and video clustering by conceptual similarity. Block 300 shows that the service device 110 receives as input a set of two knowledge maps for processing. More specifically, the service device 110 is receiving a video 1 knowledge graph (V1 KG) 302 and a video 2 knowledge graph (V2 KG) 304. V1KG 302 includes a plurality of knowledge graphs created based on objects presented at different selected time stamps within video 1, and V2KG includes a plurality of knowledge graphs created based on objects presented at different selected time stamps within video 2. Knowledge graph 306 is a visualization of an example knowledge graph that may be included in V1KG 302, and knowledge graph 308 is a visualization of an example knowledge graph that may be included in V2KG 304. Each of knowledge maps 306 and 308 may represent a portion of video that presents content similar to frames of video presented in video player 202 of fig. 2.
For example, knowledge graph 306 includes nodes 310 representing people, nodes 312 representing chairs, and nodes 314 representing tables. These nodes 310, 312, and 314 may represent, for example, the chair 218, the table 220, and a person sitting in the chair 218, as shown in fig. 2. Similarly, knowledge graph 308 includes nodes 316, 318, and 320, which represent a person, a chair, and a table, respectively. As discussed further below, the persons, chairs, and tables represented by nodes 316, 318, and 320 may be different persons, tables, and chairs than those depicted in video player 202 of fig. 2, or they may represent the same persons, tables, and chairs.
Knowledge graph 306 also includes nodes 322 representing famous actor "celebrity X" and nodes 324 and 326 representing brown and green, respectively. The knowledge graph also includes edges that connect nodes and specifies the type of relationship between the nodes. For example, knowledge graph 306 includes an edge 328 between nodes 310 and 312 that indicates that a relationship exists between a person and a chair. In this example, the edge 328 has indicia indicating a "sitting" relationship with an arrow in the direction of the node 312 representing the chair. Thus, the edge indicates that the person and the chair are related by the fact that the person sits on the chair, as shown in fig. 2. Similarly, knowledge graph 206 includes linked nodes 310 and 314 and has an edge 330 labeled "hand on". Thus, edge 330 represents the fact that a person places their hand on a table, as shown in FIG. 2. Knowledge graph 306 also includes an edge 332 between nodes 301 and 322, the edge 332 indicating that the person represented by node 310 is celebrity X, which is an actor. Edge 334 is between node 312 and node 326 indicating that the chair is green, and edge 336 is between node 314 and node 324 indicating that the table is brown.
Like knowledge graph 306, knowledge graph 308 also includes nodes 338 that represent brown. However, knowledge graph 308 does not include nodes representing celebrities X or green. Knowledge graph 308 also includes edges that link nodes of knowledge graph 308 and represent relationships between the nodes. For example, edge 340 of link nodes 316 and 318 represents the fact that a person sits in a chair, while edge 342 of link nodes 316 and 320 instructs a person to place their feet on a table. In addition, edges 344 and 346 indicate that both the table and chair are brown.
The service 110 processes the sets of knowledge graphs (e.g., V1KG 302 and V2KG 304) to determine conceptual similarities between videos, and clusters the videos based on the similarity of knowledge graphs in each set, for example.
As part of the processing, service device 110 generates conceptual index 1 for video 1 (348), and also generates conceptual index 2 for video 2 (350). The concept index of each video represents the importance of each concept depicted by each video. The concept index for a given video may be generated based on concepts conveyed by objects depicted in the given video and the level of saliency of each concept over the length of the video. In some implementations, the conceptual index for a given video may be generated, for example, using a set of knowledge maps obtained for the given video. For example, referring to fig. 3, service device 110 may use set 304 of knowledge maps for video 2 to generate concept index 2 (350).
More specifically, the service 110 may examine all knowledge graphs in the collection 304 to identify, for each knowledge graph, the total existing share of that knowledge graph over the length of the video 2. Because each knowledge graph represents concepts conveyed by a given video, the total present share of a particular knowledge graph may represent the total present share of the corresponding concepts conveyed over the length of the video. In some implementations, the total present share of a particular knowledge graph over the length of the video may be determined by aggregating the present shares of each knowledge graph at each timestamp of the video being evaluated.
Assuming, for example, that five timestamps are selected, as discussed with reference to fig. 2, the total present share of a particular knowledge graph in this example may be the sum of the present shares of the particular knowledge graph at each of the five timestamps (e.g.,where ts denotes a timestamp). The output obtained by summing the presence shares over the video length (e.g., at each selected timestamp) will typically be a digital value that can be combined with other information to create a conceptual index, as discussed further below. Referring to fig. 3, the service 110 may determine the total present share of the knowledge graph 308 by summing the present shares of the knowledge graph 308 for each time stamp evaluated within the video 2.
The presence share of a particular knowledge graph may be based on a number of factors, such as the portion of the frame occupied by the object represented by the knowledge graph, the location of the object in the frame, or other factors corresponding to the saliency of the object in the frame. In general, the presence share of a knowledge graph representing an object will increase as the saliency of the object in the frame increases. For example, when an object is larger or occupies a larger portion of a frame, the presence share of the knowledge graph representing the object will increase. The presence share assigned to each knowledge graph at each timestamp may be a number between 0 (least significant) and 1 (most significant), although other suitable scales (e.g., 0-10 or 0-100) may be used.
As part of the concept index generation, the service 110 may also determine a total number of timestamps for each knowledge graph representing each concept detected there within a given video. In some implementations, the service device 110 may determine whether a particular knowledge graph has been collected for each timestamp by searching the knowledge graph obtained at that timestamp, and increment a counter for each instance of the particular knowledge graph detected at the timestamp, thereby determining the total number of timestamps for the video at which the particular knowledge graph was detected. For example, if five timestamps are selected, as discussed with reference to fig. 2, the service device may search the knowledge graphs collected at each timestamp to determine if any of the collected knowledge graphs match a particular knowledge graph, and increment a counter for each timestamp having a knowledge graph that matches a particular knowledge graph. The value of the counter after all the time stamps for a given video have been searched represents the total number of time stamps (also referred to as the number of parts) for the given video described by a particular knowledge graph.
For a given video, the service 110 may combine the total existing share of the particular knowledge graph over the length of the given video and the total number of time stamps of the given video where the particular knowledge graph was detected to derive a concept value corresponding to the level of saliency of the concept represented by the particular knowledge graph over the length of the given video. In some implementations, the concept value may be calculated by taking a ratio of the summed presence shares of a particular knowledge graph representing the concept to the total number of timestamps of a given video at which the particular knowledge graph was detected. The generalized ratio may take the form expressed in relation (1).
Wherein:
the concept value KGi is a concept value of a concept represented by the i-th knowledge graph;
the summed present share KGi is the summed present share of the ith knowledge graph on the given video;
the total instance KGi is the total number of time stamps (or portions) at which a given video of the ith knowledge graph was detected.
In some implementations, each conceptual value generated for a given video may be considered a conceptual index. In some implementations, the concept index for a given video includes a concept value for each of a plurality of different concepts conveyed by the video and represented by different knowledge graphs. For example, when a concept index of a video includes a plurality of different concept values for different concepts, each value of the concept index may correspond to a different concept and a knowledge graph representing the concept.
The service 110 generates knowledge graph reverse document frequency (knowledge graph inverse document frequency, "KGIDF") metrics (352) based on the occurrence of different knowledge graphs across all (or a subset) of the videos to be clustered. KGIDF can be used to adjust the inverse document frequency of a single knowledge graph in each video. For example, the relationship (2) may be used to generate KGIDFs for all videos.
Wherein, | { v ε number of videos: kGi e v is the number of videos in which KGi and KGi is the ith knowledge graph.
Service device 110 applies KGIDF to set of knowledge graphs 302 for video 1 (354) and also applies set of knowledge graphs 304 for video 2 (356). And applying KGIDF to the set of knowledge graphs to obtain the adjusted reverse document frequency of each knowledge graph. The tuned KGIDF prevents errors that occur when two videos have very few common knowledge graphs. For example, suppose two videos share only one common attribute (e.g., purple), but the common attributes are very similar (e.g., have a small similarity distance). In this case, some similarity techniques will result in very high similarity, despite the fact that videos may be very dissimilar. In other words, the similarity of common attributes (e.g., similarity=1/sqrt (distance)) using some similarity techniques may result in one attribute being more similar than the other, resulting in erroneous similarity determinations.
In some implementations, KGIDF is applied to the set of knowledge graphs according to relationship (3).
adj value {videoj|KGi} ：＝value {videoj|KGi} *id KGi (3)
Wherein:
adj value {videoj|KGi} is the adjustment value of the ith knowledge graph in the jth video;
value {videoj|KGi} is the summed presence share of the ith knowledge graph over the length of the jth video; and is also provided with
idf KGi Is KGIDF of the ith knowledge graph.
For example, the service device 110 may generate an adjustment value for the knowledge graph 308, which may also be referred to as an adjusted presence share, by multiplying the total presence share of the knowledge graph 308 in video 2 by the KGIDF of the knowledge graph 308 across all videos that the service device 110 is analyzing.
The service device 110 derives (e.g., determines or calculates) a similarity score for the shared knowledge graph between the pair of videos (358). Referring to fig. 3, service device 110 may use set of knowledge maps 302 and set of knowledge maps 304 to determine a similarity score for a video pair comprising video 1 and video 2. For example, the service device 110 may derive a shared similarity score for the pair of videos and a likely similarity score for the pair of videos. In some implementations, a separate shared similarity score is derived for each knowledge graph detected in both videos in the pair (e.g., video 1 and video 2). Similarly, the service device may derive a separate potential similarity score for each knowledge graph detected in both videos in the pair.
The shared similarity score refers to a similarity score based on a minimum amount of conceptual salience in the pair of videos. For example, the shared similarity score may represent the minimum total share of the particular knowledge graph in any video in the pair. In other words, the service device 110 may determine which video (e.g., video 1 or video 2) of the pair has a lower adjustment value for a particular knowledge graph, e.g., an adjustment value determined using relationship (3), and select the lower adjustment value for the particular knowledge graph as the shared similarity score for the pair of videos. This determination may be performed, for example, using relationship (4).
SS-KGi (video 1, video 2) ＝min(adj value { video 1|KGi } ，adj value { video 2|KGi } ) (4)
Wherein:
SS_KGi (video 1, video 2) Is the sharing similarity of the ith knowledge graph between video 1 and video 2;
adj value { video 1|KGi } Is the adjustment value of the ith knowledge graph in video 1 (see relationship 3); and is also provided with
adj value { video 2|KGi } Is the adjustment value of the ith knowledge graph in video 2 (see relationship 3).
The likely similarity score refers to the similarity score based on the largest amount of conceptual salience in the pair of videos. For example, the likely similarity score may represent the maximum total share of the particular knowledge graph in any video in the pair. In other words, the service device 110 may determine which video (e.g., video 1 or video 2) of the pair has a higher adjustment value for a particular knowledge graph, e.g., an adjustment value determined using relationship (3), and select the higher adjustment value for the particular knowledge graph as the likely similarity score for the pair of videos. This determination may be performed, for example, using relationship (5).
PS_KGi (video 1, video 2) ＝max(adj value { video 1|KGi } ，αdj value { video 2|KGi } ) (5)
Wherein:
PS_KGi (video 1, video 2) Is the possible similarity of the ith knowledge graph between video 1 and video 2;
adj value { video 1|KGi } Is the adjustment value of the ith knowledge graph in video 1 (see relationship 3); and is also provided with
adj value { video 2|KGi } Is the adjustment value of the ith knowledge graph in video 2 (see relationship 3).
The service 110 derives (e.g., determines or calculates) a dissimilarity score for the dissimilarity knowledge graph in each of the pair of videos (358). Referring to fig. 3, service device 110 may use set of knowledge maps 302 and set of knowledge maps 304 to determine a dissimilarity score for a video pair that includes video 1 and video 2. In some implementations, the service 110 may generate the dissimilarity score for each video in the pair by summing the conceptual values of the dissimilarity knowledge graphs in the pair. For example, the service device 110 may identify a knowledge graph of video 1 that is not included in video 2 and a knowledge graph of video 2 that is not included in video 1. In this example, the service device may sum the conceptual indexes of the dissimilar knowledge maps of video 1 and video 2 and output the sum as the dissimilarity score of the pair of videos including video 1 and video 2.
Another dissimilarity measure that the service unit 110 may determine is the total number of distinct knowledge maps in each of the pair of videos, which is referred to as a dissimilarity count. This may be determined by incrementing a counter each time a knowledge graph identified by the service apparatus 110 for one video in the pair does not find a match in the other video in the pair.
The service device clusters the videos based on the similarity score and the dissimilarity score (362). The cluster creates a plurality of video packets, each video packet containing a video having at least a specified level of similarity. For example, a group created by clustering will include multiple videos, where each video has a specified level of similarity to one or more other videos in the group. Details of how the similarity and dissimilarity metrics are used to create the packet are discussed in more detail with reference to fig. 4.
FIG. 4 is a flow chart of an example process 400 for using video similarity to provide insight into the grouping of videos and to modify the manner in which videos are distributed. The operations of process 400 may be implemented, for example, by service device 110 (and/or CEA 150) of fig. 1. The operations of process 400 may also be implemented using instructions encoded on one or more computer-readable media, which may be non-transitory. The instructions, when executed by one or more data processing apparatus (e.g., one or more computing devices), cause the one or more data processing apparatus to perform the operations of process 400.
Video is obtained by a service device (402). In some implementations, the obtained video is obtained when the video publisher uploads the video. The video publisher may be a user that uploads video to a video sharing website or an entity that creates video for presentation with other content. For example, as discussed with reference to fig. 1, the obtained video may be in the form of digital components provided for distribution with other content.
One or more conceptual indexes are generated for each given video obtained (404). The one or more concept indexes may include a separate concept index generated for each given knowledge graph obtained for a given video. In some implementations, the concept index for the given video is generated based at least on (i) concepts conveyed by one or more objects depicted in the given video and (ii) a level of prominence of the concepts in the given video. As discussed above with reference to fig. 3, one or more concepts conveyed by one or more objects depicted in a given video may be represented by one or more knowledge maps obtained for multiple portions of the given video (e.g., at multiple different timestamps). The one or more knowledge maps may be obtained, for example, by evaluation of the video and/or from a database storing previously generated knowledge maps, as discussed with reference to fig. 2.
In some implementations, the concept index may be generated by evaluating the presence of knowledge graphs in the video. For example, a particular concept index may be generated for concepts represented by a particular knowledge graph based on the number of times the knowledge graph is detected in the video and the total existing share of the knowledge graph over the length of the video, as discussed in detail with reference to fig. 3. More specifically, the concept index of a particular knowledge graph may be determined as a ratio of (i) the total existing share of the particular knowledge graph over the length of the video and (ii) the total number of instances of the particular knowledge graph in the given video.
As described above, the presence share at a given timestamp within the video indicates a level of saliency of one or more concepts represented by a given knowledge graph at that timestamp of the video, which also indicates a level of saliency of one or more concepts conveyed by the video. In some implementations, the total present share of a given knowledge graph (and corresponding concepts) is determined by summing the present shares of the given knowledge graph over the length of the video. Meanwhile, the number of portions of video described by a given knowledge graph may be determined based on the number of time stamps (or portions) of the video at which a particular knowledge graph was detected.
Reverse document frequencies are generated and applied (406) for each given knowledge graph. For example, the inverse document frequency of a given knowledge graph is generated by determining the frequency of the given knowledge graph in all knowledge graphs obtained from the video being evaluated. In some implementations, the inverse document frequency for a given knowledge graph can be generated based on a total number of the plurality of videos represented by the given knowledge graph. In other words, the inverse document frequency for a given knowledge graph may be based on how many videos have a set of knowledge graphs that include the given knowledge graph. In some implementations, the inverse document frequency for a given knowledge graph may be determined using relationship (2), as discussed with reference to fig. 3.
For example, the inverse document frequency for a given knowledge graph may be applied by multiplying the inverse document frequency by the total existing share of the given knowledge graph in a particular video. For example, relationship (3) discussed with reference to FIG. 3 may be used to apply the reverse document frequency. In some implementations, a separate inverse document frequency is generated for each different knowledge graph, and the inverse document frequency generated for a given knowledge graph may be used to adjust the total existing share of each given knowledge graph in each video. In other words, for each of a plurality of videos having a timestamp represented by a given knowledge graph, the generated inverse document frequency is applied to the total existing share of the given knowledge graph.
A pair of videos is selected from the plurality of acquired videos (408). In some implementations, the pair of videos may be selected randomly (or pseudo-randomly).
A similarity measure is derived for the pair of videos (410). In some implementations, deriving the similarity measure includes: obtaining a shared similarity score of a shared knowledge graph between the pair of videos; and deriving a likely similarity score for the shared knowledge graph. A similarity score may be generated for each particular shared knowledge graph between the pair of videos. In some implementations, the shared knowledge graph is a knowledge graph found in each video of the pair of videos. For example, a particular knowledge graph identified for video 1 may be considered a shared knowledge graph if a matching knowledge graph is also identified for video 2. Note that the matching knowledge graph may be, but need not be, an exact match. In other words, the matching knowledge graph may be a knowledge graph having one or more identical nodes representing one or more objects. The level of matching required to view the two knowledge maps as a match may vary depending on the application.
The shared similarity score may be derived based on a minimum existing share of the shared knowledge graph in any of the pair of videos. For example, as discussed above with reference to fig. 3, the shared similarity score for a particular shared knowledge graph may be the smallest adjusted existing share (e.g., adjustment value) of the particular shared knowledge graph in any of the pair of videos, as shown by relationship (4).
The likely similarity score may be derived based on the maximum existing share of the shared knowledge graph in any of the pair of videos. For example, as discussed above with reference to fig. 3, the likely similarity score for a particular shared knowledge graph may be the largest adjusted existing share (e.g., adjustment value) of the particular shared knowledge graph in any of the pair of videos, as shown by relationship (5).
Deriving the similarity measure may further comprise: a count of the shared knowledge graph between the pair of videos is generated. Each shared knowledge graph is a given knowledge graph in the knowledge graphs representing each video of the pair of videos. As described above, the shared knowledge graph may be identified by finding a matching knowledge graph in one of the videos that has been identified for a given knowledge graph of the other video in the pair. The count of shared knowledge graphs may be generated, for example, by incrementing a counter (or otherwise counting) each time a given knowledge graph of one video in the pair is considered to match a knowledge graph in the other video. In some cases, the counter may be incremented only once for all matches for a given knowledge graph. In other cases, a counter may be incremented for each instance of a match between a given knowledge graph in one video and multiple instances of a matching knowledge graph in another video.
One or more dissimilarity measures are derived for the pair of videos (412). In some implementations, deriving the dissimilarity measure includes: a dissimilarity count is generated, and a dissimilarity score is calculated. For example, a dissimilarity measure may be derived using a dissimilarity knowledge graph from each video in the pair. For example, while the similarity metrics discussed above were generated using the matching knowledge graphs of the pair of videos, the dissimilarity metrics were generated using those knowledge graphs of each video in the pair for which no matching knowledge graph was found in the other video. It should be appreciated that regardless of the conditions used to determine a match, there will be a set of matching knowledge graphs that are used to derive a similarity measure and a set of dissimilar knowledge graphs (or unmatched knowledge graphs) that will be used to determine a dissimilarity measure.
The generation of the dissimilarity count may be based on a number of different knowledge maps that identify the pair of videos. The number of distinct knowledge graphs may be, for example, the total number of knowledge graphs of two videos for which no matching knowledge graph is identified in the other video of the pair. The number of distinct knowledge graphs may be generated, for example, by incrementing a counter each time a match of the knowledge graphs in one video is not found in the other video, and the number of distinct knowledge graphs may be the value of the counter after all knowledge graphs of both videos have been analyzed to find a match. In other words, the dissimilarity count may be a number of distinct knowledge maps representing only one video of the pair of videos.
The dissimilarity score for each of the pair of videos may be calculated based on the conceptual index of the dissimilarity knowledge graph for the pair of videos. For example, a concept index may be identified for each distinct knowledge graph of each video in the pair. These conceptual indices for each of the dissimilar knowledge maps may then be summed to obtain a dissimilarity score for the two videos in the pair.
A plurality of video packets is created based on the similarity measure and the dissimilarity measure (414). In some implementations, each given video packet will be created to include only those videos that have at least a specified level of similarity to other videos in the given packet. For example, for inclusion in a given packet, a given video may be required to satisfy a set of similarity conditions with respect to each other video in the packet. The set of similarity conditions may be related to the similarity and dissimilarity metrics discussed above, and may be used to ensure that all videos are conceptually related to similar concepts.
Creating the packets may be based on, for example, a conceptual index of the video (either directly or indirectly). For example, dissimilarity measures are derived directly based on the concept index of the dissimilar knowledge graph, while similarity measures are derived based on the presence shares, which are also used to determine the concept index.
In some implementations, creating the packet may depend on one or more of the following factors:
1) The number of shared knowledge graphs, denoted by n_similarity, refers to the number of matching knowledge graphs between a pair of graphs.
2) The number of distinct knowledge graphs, denoted m_dissimilarity, refers to the number of knowledge graphs in each video of the pair that do not match in the other video of the pair.
3) The sum of possible similarities, denoted total_likelihood_similarity, refers to the mathematical sum of possible similarity measures for all shared knowledge graphs of the pair of videos.
4) The sum of the shared similarities, represented by shared_similarity_mass, refers to the mathematical sum of the shared similarity metrics for all shared knowledge graphs of the pair.
5) The sum of dissimilarity scores, represented by dissimilarity_mass, is the sum of dissimilarity scores of all the dissimilarity knowledge maps of the pair of videos.
6) Dissimilarity share, expressed as dissimilarity_share, where dissimilarity_share=dissimilarity_mass/shared_similarity_mass.
7) Total, expressed in total_mass, where total_mass=dissimilarity_mass+total_porous_similarity.
8) Share of dissimilarity, expressed as share of dissimilarity, where share of dissimilarity = dissimilarity share/total mass.
9) The share of the realized similarity is expressed by share_of_related_similarity, wherein share_of_related_similarity=shared_similarity_mass/total_mass.
10 Share of possible similarity, expressed in share of similarity, where share of similarity=total similarity/total mass.
11 Comparative score, represented by compacton score = shared similarity mass/dissimilarity mass.
12 Creative method similarity score, represented by the creation_application_similarity_score, where creation_application_similarity_score=total_potential_similarity/similarity_mass.
In some implementations, one or more of these factors are used to describe a relationship (e.g., similarity) between each pair of videos being evaluated, such that each pair of videos obtained is characterized by these factors. Factors describing the relationship between video pairs may be input to a clustering algorithm that groups videos together based on these factors.
In some implementations, the video packets may be visually presented in a graphical user interface. To reduce clutter of the graphical user interface and improve processing time, the video packets may be de-duplicated to remove the same video from the representation. The deduplication process may identify, for example, those video pairs within a given group that have more than a threshold share similarity and/or less than a specified level of dissimilarity (which would be an indication that the videos are substantially identical).
Insight is generated regarding a plurality of video packets (416). In some implementations, the insight of each given video packet is determined based on data obtained through a feedback loop. For example, a user response to the presented video may be obtained through a feedback loop and recorded with reference to the presented video and/or packets containing the video. The reactions may be aggregated on a per-packet basis, which may be used to provide insight into video packets, concepts conveyed by the video packets, and other attributes of the video packets.
The feedback loop may be implemented, for example, as a script that is initiated when a user interacts with the video (e.g., clicks on the video), a ping (pins) that is automatically generated during playback of the video to report the viewing time, or using other mechanisms. With these feedback loops, the user does not need to report their reactions on the video presented to the user separately. Instead, the reaction (e.g., positive or negative) may be inferred based on data collected using various feedback loops.
The insight generated based on the aggregated data of the plurality of different video packets may include, for example, identification of video packets for which higher levels of positive feedback data are obtained relative to other video packets for which lower levels of positive feedback data are obtained. Once the packets with higher level of positive feedback data are identified, the video publishers can be provided with information about the video type that received the higher level of positive feedback data so that they can incorporate similar features in the new video. Similarly, the video publisher may be provided with information about video packets that received lower levels of positive feedback data so that the video publisher may omit similar features from the new video.
In some implementations, insight may be created on a per audience type basis. For example, the feedback data may be partitioned based on any type of audience feature (e.g., device type, interest group, time of day, geographic area, etc.), and insight may be generated for each audience type or audience segment. When the data is partitioned in this manner, the video publisher may be provided with insight (e.g., as described above) based on the type of audience for which the video publisher created the video.
The generated insight may extend beyond the insight used by the video publisher to create videos that will get a higher level of positive feedback. In some implementations, the insight can be used to significantly reduce the amount of time required to train a computer to efficiently and effectively distribute the uploaded new video. For example, when a new video is uploaded, it may be evaluated and clustered as discussed throughout this document. Once a new video is assigned to a cluster, insight into videos in the cluster can be used to generate an initial distribution criteria for the new video based on known similarities of the new video to other videos in the cluster. By generating initial distribution criteria in this manner and distributing new videos using initial distribution criteria generated based on insight into other videos in the cluster, the training period (which may exceed one week) required by existing systems is significantly reduced or eliminated so that the computer system can more quickly adjust the distribution criteria of the new videos to reach an optimal set of distribution criteria.
The manner in which the at least one video is distributed is modified based on the insight (418). In some implementations, the modification of the manner in which the at least one video is distributed may include: the distribution criteria of the video are adjusted. For example, assume that the initial intended audience for a given video in a cluster is a football fan in atlanta, georgia, but that the video packets containing the given video have low feedback data for that packet, but high feedback data for hockey fans in atlanta, georgia. In this example, the distribution criteria for a given video may be adjusted to increase the likelihood that the given video will be presented to the hockey fan audience of atlanta, georgia. In another example, the time at which the video is distributed or the content of the video is distributed with each packet may be adjusted based on the insights generated for that packet. In addition, the properties of the video itself may be modified to increase the likelihood that the received feedback data is more aggressive.
FIG. 5 is a block diagram of an example computer system 500 that may be used to perform the operations described above. The system 500 includes a processor 510, a memory 520, a storage device 530, and an input/output device 540. Each of the components 510, 520, 530, and 540 may be interconnected, for example, using a system bus 550. Processor 510 is capable of processing instructions for execution within system 500. In one implementation, the processor 510 is a single-threaded processor. In another implementation, the processor 510 is a multi-threaded processor. The processor 510 is capable of processing instructions stored in the memory 520 or on the storage device 530.
Memory 520 stores information within system 500. In one implementation, the memory 520 is a computer-readable medium. In one implementation, the memory 520 is a volatile memory unit. In another implementation, the memory 520 is a non-volatile memory unit.
The storage device 530 is capable of providing mass storage for the system 500. In one implementation, the storage device 530 is a computer-readable medium. In various different implementations, storage device 530 may include, for example, a hard disk device, an optical disk device, a storage device shared by multiple computing devices over a network (e.g., a cloud storage device), or some other mass storage device.
Input/output device 540 provides input/output operations for system 400. In one implementation, the input/output device 540 may include one or more network interface devices such as an Ethernet card, a serial communication device (e.g., RS-232 port), and/or a wireless interface device such as an 802.11 card. In another implementation, the input/output devices may include driver devices, such as keyboards, printers, and display devices, configured to receive input data and send output data to the peripheral devices 560. However, other implementations may be used, such as mobile computing devices, mobile communication devices, set-top box television client devices, and the like.
Although an example processing system has been described in FIG. 5, implementations of the subject matter and functional operations described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
This document relates to a service device 110. As used herein, a service device 110 is one or more data processing devices that perform operations to facilitate distribution of content over a network. Service device 110 is depicted in a block diagram as a single block. However, while the service apparatus 110 may be a single device or a single group of devices, the present disclosure contemplates that the service apparatus 110 may also be a group of devices, or even a plurality of different systems that communicate to provide various content to the client device 106. For example, the service device 110 may include one or more of a search system, a video streaming service, an audio streaming service, an email service, a navigation service, an advertising service, or any other service.
An electronic document (for brevity, will be referred to simply as a document) may, but need not, correspond to a file. Documents may be stored in a portion of a file that holds other documents, in a single file dedicated to the document in question, or in multiple collaborative files.
Embodiments of the subject matter and the operations described in this specification can be implemented as digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on a computer storage medium for execution by, or to control the operation of, data processing apparatus. Alternatively or additionally, the program instructions may be encoded on a manually generated propagated signal (e.g., a machine-generated electrical, optical, or electromagnetic signal) that is generated to encode information for transmission to suitable receiver apparatus for execution by data processing apparatus. The computer storage medium may be or be included in a computer readable storage device, a computer readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. Furthermore, while the computer storage medium is not a propagated signal, the computer storage medium may be a source or destination of computer program instructions encoded in an artificially generated propagated signal. Computer storage media may also be or be included in one or more separate physical components or media (e.g., a plurality of CDs, discs, or other storage devices).
The operations described in this specification may be implemented as operations performed by a data processing apparatus on data stored on one or more computer readable storage devices or received from other sources.
The term "data processing apparatus" encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system-on-a-chip, or a plurality or combination of the foregoing. The apparatus may comprise a dedicated logic circuit, such as an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). In addition to hardware, the apparatus may include code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus and execution environment may implement a variety of different computing model infrastructures, such as web services, distributed computing, and grid computing infrastructures.
A computer program (also known as a program, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. The computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a communication network.
The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for performing actions in accordance with instructions and one or more memory devices for storing instructions and data. Typically, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, the computer need not have such devices. Further, the computer may be embedded in another device, such as a mobile phone, a Personal Digital Assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver or a portable storage device (e.g., a Universal Serial Bus (USB) flash drive), etc. Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and storage devices including, for example: semiconductor memory devices such as EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disk; CDROM and DVD-ROM discs. The processor and the memory may be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having: a display device for displaying information to a user, such as a CRT (cathode ray tube) or LCD (liquid crystal display) monitor; and a keyboard and pointing device, such as a mouse or trackball, by which a user can provide input to a computer. Other kinds of devices may also be used to provide for interaction with a user; for example, feedback provided to the user may be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input. Further, the computer may interact with the user by sending and receiving documents to and from the device used by the user; for example, by sending a web page to a web browser on a user's client device in response to a request received from the web browser.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include local area networks ("LANs") and wide area networks ("WANs"), the internet (e.g., the internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
The computing system may include clients and servers. The client and server are typically remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, the server communicates data (e.g., HTML pages) to the client device (e.g., to display data to and receive user input from a user interacting with the client device). Data generated at the client device (e.g., results of the user interaction) may be received at the server from the client device.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Furthermore, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, although operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Thus, particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. In some cases, the actions recited in the claims can be performed in a different order and still achieve desirable results. Moreover, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some implementations, multitasking and parallel processing may be advantageous.
Claims (20)
1. A method performed by a data processing apparatus, the method comprising:
obtaining, by a service device, a video uploaded by a video publisher;
Generating, by the service device, a concept index for each given video, wherein the concept index is generated based at least on (i) concepts conveyed by one or more objects depicted in the video and (ii) a level of saliency of the concepts in the given video;
creating, by the service device, a plurality of video packets based on the conceptual index of the video, wherein each given video packet of the plurality of video packets is created to include two or more different videos, each of the two or more different videos having a specified level of similarity to other videos in the given video packet;
generating, by the service device, insight regarding the plurality of video packets based upon data obtained through the feedback loop; and
the manner in which the at least one video is distributed over the network is modified by the service device based on insight regarding a given video packet comprising the at least one video.
2. The method of claim 1, wherein generating a conceptual index for each given video comprises:
for each given one of the videos:
obtaining one or more knowledge graphs of portions of a given video, wherein each of the knowledge graphs represents one or more concepts conveyed by the given video; and
For each given knowledge graph, determining a presence share indicative of a level of saliency of the concept represented by the given knowledge graph, wherein a concept index is generated based at least in part on a number of instances of the given knowledge graph in the given video and a total presence share of the given knowledge graph over a length of the video.
3. The method of claim 2, wherein generating a conceptual index for each given video comprises:
for each given knowledge graph obtained for a given video:
summing the presence shares of the given knowledge graph over the length of the given video;
determining a number of portions of a given video described by a given knowledge graph; and
a conceptual index for a given video is generated based on the ratio of the summed presence shares relative to the number of portions.
4. A method according to claim 3, further comprising:
for each given knowledge graph obtained for a plurality of videos uploaded by a video publisher:
generating an inverse document frequency metric for the given knowledge graph based on a total number of the plurality of videos represented by the given knowledge graph; and
for each of the plurality of videos, applying the generated inverse document frequency to the total existing share of the given knowledge graph.
5. The method of claim 4, further comprising:
selecting a pair of videos from the plurality of videos;
generating a count of shared knowledge maps between the pair of videos, wherein each shared knowledge map is a given knowledge map of the knowledge maps representing each video of the pair of videos;
for each particular shared knowledge graph between the pair of videos:
based on the minimum existing share of the knowledge graph of any video in the pair of videos, obtaining a sharing similarity score of the specific sharing knowledge graph; and
based on the maximum existing share of the knowledge graph for any of the pair of videos, a likely similarity score for the particular shared knowledge graph is derived.
6. The method of claim 5, further comprising:
generating a dissimilarity count based on a number of dissimilarity knowledge maps that represent only one of the videos in the pair of videos;
calculating a dissimilarity score for each of the pair of videos based on the conceptual index of the dissimilarity knowledge graph for the pair of videos; and
one or more clustering factors are generated for the pair of videos based on the dissimilarity score, the likely similarity score, and the shared similarity score.
7. The method of claim 6, wherein creating a plurality of video packets comprises:
Comparing, for each pair of videos, the clustering factor of the pair of videos with a specified similarity level;
including in the same grouping a first pair of videos for which the clustering factor meets a specified level; and
the third video whose clustering factor does not satisfy the specified level with respect to the first pair of videos is excluded from the same group.
8. A system, comprising:
a memory device; and
one or more processors configured to interact with the memory device and execute instructions that cause the one or more processors to perform operations comprising:
obtaining a video uploaded by a video publisher;
generating a concept index for each given video, wherein the concept index is generated based at least on (i) concepts conveyed by one or more objects depicted in the video and (ii) a level of saliency of the concepts in the given video;
creating a plurality of video packets based on the conceptual index of the video, wherein each given video packet of the plurality of video packets is created to include two or more different videos, each of the two or more different videos having a specified level of similarity to other videos in the given video packet;
Generating insight about the plurality of video packets based upon data obtained through a feedback loop; and
the manner in which the at least one video is distributed over the network is modified based on insight regarding a given video packet comprising the at least one video.
9. The system of claim 8, wherein generating a conceptual index for each given video comprises:
for each given one of the videos:
obtaining one or more knowledge graphs of portions of a given video, wherein each of the knowledge graphs represents one or more concepts conveyed by the given video; and
for each given knowledge graph, determining a presence share indicative of a level of saliency of the concept represented by the given knowledge graph, wherein a concept index is generated based at least in part on a number of instances of the given knowledge graph in the given video and a total presence share of the given knowledge graph over a length of the video.
10. The system of claim 9, wherein generating a conceptual index for each given video comprises:
for each given knowledge graph obtained for a given video:
summing the presence shares of the given knowledge graph over the length of the given video;
determining a number of portions of a given video described by a given knowledge graph; and
A conceptual index for a given video is generated based on the ratio of the summed presence shares relative to the number of portions.
11. The system of claim 10, wherein the instructions cause the one or more processors to perform operations further comprising:
for each given knowledge graph obtained for a plurality of videos uploaded by a video publisher:
generating an inverse document frequency metric for the given knowledge graph based on a total number of the plurality of videos represented by the given knowledge graph; and
for each of the plurality of videos, applying the generated inverse document frequency to the total existing share of the given knowledge graph.
12. The system of claim 11, wherein the instructions cause the one or more processors to perform operations further comprising:
selecting a pair of videos from the plurality of videos;
generating a count of shared knowledge maps between the pair of videos, wherein each shared knowledge map is a given knowledge map of the knowledge maps representing each video of the pair of videos;
for each particular shared knowledge graph between the pair of videos:
based on the minimum existing share of the knowledge graph of any video in the pair of videos, obtaining a sharing similarity score of the specific sharing knowledge graph; and
Based on the maximum existing share of the knowledge graph for any of the pair of videos, a likely similarity score for the particular shared knowledge graph is derived.
13. The system of claim 12, wherein the instructions cause the one or more processors to perform operations further comprising:
generating a dissimilarity count based on a number of dissimilarity knowledge maps that represent only one of the videos in the pair of videos;
calculating a dissimilarity score for each of the pair of videos based on the conceptual index of the dissimilarity knowledge graph for the pair of videos; and
one or more clustering factors are generated for the pair of videos based on the dissimilarity score, the likely similarity score, and the shared similarity score.
14. The system of claim 13, wherein creating a plurality of video packets comprises:
comparing, for each pair of videos, the clustering factor of the pair of videos with a specified similarity level;
including in the same grouping a first pair of videos for which the clustering factor meets a specified level; and
the third video whose clustering factor does not satisfy the specified level with respect to the first pair of videos is excluded from the same group.
15. One or more non-transitory computer-readable media storing instructions that, when executed by one or more data processing apparatus, cause the one or more data processing apparatus to perform operations comprising:
Obtaining a video uploaded by a video publisher;
generating a concept index for each given video, wherein the concept index is generated based at least on (i) concepts conveyed by one or more objects depicted in the video and (ii) a level of saliency of the concepts in the given video;
creating a plurality of video packets based on the conceptual index of the video, wherein each given video packet of the plurality of video packets is created to include two or more different videos, each of the two or more different videos having a specified level of similarity to other videos in the given video packet;
generating insight about the plurality of video packets based upon data obtained through a feedback loop; and
the manner in which the at least one video is distributed over the network is modified based on insight regarding a given video packet comprising the at least one video.
16. The non-transitory computer-readable medium of claim 15, wherein generating a conceptual index for each given video comprises:
for each given one of the videos:
obtaining one or more knowledge graphs of portions of a given video, wherein each of the knowledge graphs represents one or more concepts conveyed by the given video; and
For each given knowledge graph, determining a presence share indicative of a level of saliency of the concept represented by the given knowledge graph, wherein a concept index is generated based at least in part on a number of instances of the given knowledge graph in the given video and a total presence share of the given knowledge graph over a length of the video.
17. The non-transitory computer-readable medium of claim 16, wherein generating a conceptual index for each given video comprises:
for each given knowledge graph obtained for a given video:
summing the presence shares of the given knowledge graph over the length of the given video;
determining a number of portions of a given video described by a given knowledge graph; and
a conceptual index for a given video is generated based on the ratio of the summed presence shares relative to the number of portions.
18. The non-transitory computer-readable medium of claim 17, wherein the instructions cause the one or more processors to perform operations further comprising:
for each given knowledge graph obtained for a plurality of videos uploaded by a video publisher:
generating an inverse document frequency metric for the given knowledge graph based on a total number of the plurality of videos represented by the given knowledge graph; and
For each of the plurality of videos, applying the generated inverse document frequency to the total existing share of the given knowledge graph.
19. The non-transitory computer-readable medium of claim 18, wherein the instructions cause the one or more processors to perform operations further comprising:
selecting a pair of videos from the plurality of videos;
generating a count of shared knowledge maps between the pair of videos, wherein each shared knowledge map is a given knowledge map of the knowledge maps representing each video of the pair of videos;
for each particular shared knowledge graph between the pair of videos:
based on the minimum existing share of the knowledge graph of any video in the pair of videos, obtaining a sharing similarity score of the specific sharing knowledge graph; and
based on the maximum existing share of the knowledge graph for any of the pair of videos, a likely similarity score for the particular shared knowledge graph is derived.
20. The non-transitory computer-readable medium of claim 19, wherein the instructions cause the one or more processors to perform operations further comprising:
generating a dissimilarity count based on a number of dissimilarity knowledge maps that represent only one of the videos in the pair of videos;
Calculating a dissimilarity score for each of the pair of videos based on the conceptual index of the dissimilarity knowledge graph for the pair of videos; and
one or more clustering factors are generated for the pair of videos based on the dissimilarity score, the likely similarity score, and the shared similarity score.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
IL287859A IL287859A (en) | 2021-11-05 | 2021-11-05 | Video clustering and analysis |
IL287859 | 2021-11-05 | ||
PCT/US2022/046006 WO2023080990A1 (en) | 2021-11-05 | 2022-10-07 | Video clustering and analysis |
Publications (1)
Publication Number | Publication Date |
---|---|
CN116457775A true CN116457775A (en) | 2023-07-18 |
Family
ID=84329566
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202280007124.5A Pending CN116457775A (en) | 2021-11-05 | 2022-10-07 | Video clustering and analysis |
Country Status (7)
Country | Link |
---|---|
EP (1) | EP4275134A1 (en) |
JP (1) | JP2023554219A (en) |
KR (1) | KR20230066430A (en) |
CN (1) | CN116457775A (en) |
CA (1) | CA3196587A1 (en) |
IL (1) | IL287859A (en) |
WO (1) | WO2023080990A1 (en) |
Family Cites Families (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20180101540A1 (en) * | 2016-10-10 | 2018-04-12 | Facebook, Inc. | Diversifying Media Search Results on Online Social Networks |
-
2021
- 2021-11-05 IL IL287859A patent/IL287859A/en unknown
-
2022
- 2022-10-07 JP JP2023524774A patent/JP2023554219A/en active Pending
- 2022-10-07 CA CA3196587A patent/CA3196587A1/en active Pending
- 2022-10-07 EP EP22800920.5A patent/EP4275134A1/en active Pending
- 2022-10-07 KR KR1020237012230A patent/KR20230066430A/en unknown
- 2022-10-07 WO PCT/US2022/046006 patent/WO2023080990A1/en active Application Filing
- 2022-10-07 CN CN202280007124.5A patent/CN116457775A/en active Pending
Also Published As
Publication number | Publication date |
---|---|
KR20230066430A (en) | 2023-05-15 |
WO2023080990A1 (en) | 2023-05-11 |
IL287859A (en) | 2023-06-01 |
CA3196587A1 (en) | 2024-04-07 |
JP2023554219A (en) | 2023-12-27 |
EP4275134A1 (en) | 2023-11-15 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP6449351B2 (en) | Data mining to identify online user response to broadcast messages | |
US11250087B2 (en) | Content item audience selection | |
US10387915B2 (en) | Digital magazine recommendations by topic | |
US9256688B2 (en) | Ranking content items using predicted performance | |
US20170140397A1 (en) | Measuring influence propagation within networks | |
WO2011031433A2 (en) | Audience segment estimation | |
CN107623862B (en) | Multimedia information push control method and device and server | |
US20190303995A1 (en) | Training and utilizing item-level importance sampling models for offline evaluation and execution of digital content selection policies | |
US20160189202A1 (en) | Systems and methods for measuring complex online strategy effectiveness | |
US9846746B2 (en) | Querying groups of users based on user attributes for social analytics | |
US20140258400A1 (en) | Content item audience selection | |
US8234265B1 (en) | Content selection data expansion | |
Su et al. | Link prediction in recommender systems with confidence measures | |
US20180191837A1 (en) | Pattern based optimization of digital component transmission | |
KR20220137943A (en) | pattern-based classification | |
CN116457775A (en) | Video clustering and analysis | |
US9251171B2 (en) | Propagating image signals to images | |
US20230222377A1 (en) | Robust model performance across disparate sub-groups within a same group | |
US20230177543A1 (en) | Privacy preserving machine learning expansion models | |
US20220318644A1 (en) | Privacy preserving machine learning predictions | |
US20230205754A1 (en) | Data integrity optimization | |
US20190171955A1 (en) | System and method for inferring anonymized publishers | |
Papini et al. | Strategies for mining user preferences in a data stream setting | |
CN117916727A (en) | Privacy sensitive estimation of digital resource access frequency |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |