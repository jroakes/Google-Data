CN116888668A - User interface and tools for facilitating interactions with video content - Google Patents
User interface and tools for facilitating interactions with video content Download PDFInfo
- Publication number
- CN116888668A CN116888668A CN202280017301.8A CN202280017301A CN116888668A CN 116888668 A CN116888668 A CN 116888668A CN 202280017301 A CN202280017301 A CN 202280017301A CN 116888668 A CN116888668 A CN 116888668A
- Authority
- CN
- China
- Prior art keywords
- content
- video
- video stream
- annotation
- presenter
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000003993 interaction Effects 0.000 title description 12
- 238000000034 method Methods 0.000 claims abstract description 101
- 238000013518 transcription Methods 0.000 claims description 85
- 230000035897 transcription Effects 0.000 claims description 85
- 230000015654 memory Effects 0.000 claims description 51
- 238000013519 translation Methods 0.000 claims description 46
- 230000004044 response Effects 0.000 claims description 44
- 238000003860 storage Methods 0.000 claims description 37
- 238000009877 rendering Methods 0.000 claims description 36
- 239000003550 marker Substances 0.000 claims description 12
- 230000006870 function Effects 0.000 claims description 11
- 230000014616 translation Effects 0.000 description 45
- 230000008569 process Effects 0.000 description 38
- 238000004891 communication Methods 0.000 description 27
- 238000004590 computer program Methods 0.000 description 15
- 238000012545 processing Methods 0.000 description 13
- 230000009471 action Effects 0.000 description 9
- 238000012552 review Methods 0.000 description 7
- 230000000007 visual effect Effects 0.000 description 6
- 230000008901 benefit Effects 0.000 description 5
- 230000000694 effects Effects 0.000 description 5
- 238000010586 diagram Methods 0.000 description 4
- 238000005516 engineering process Methods 0.000 description 4
- 230000002452 interceptive effect Effects 0.000 description 4
- 238000012986 modification Methods 0.000 description 4
- 230000004048 modification Effects 0.000 description 4
- 230000001960 triggered effect Effects 0.000 description 4
- 238000004422 calculation algorithm Methods 0.000 description 3
- 230000001413 cellular effect Effects 0.000 description 3
- 230000033001 locomotion Effects 0.000 description 3
- 238000010801 machine learning Methods 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 238000004458 analytical method Methods 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 239000003086 colorant Substances 0.000 description 2
- 239000004973 liquid crystal related substance Substances 0.000 description 2
- 238000004519 manufacturing process Methods 0.000 description 2
- 239000000463 material Substances 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 239000010813 municipal solid waste Substances 0.000 description 2
- 239000004065 semiconductor Substances 0.000 description 2
- 230000011273 social behavior Effects 0.000 description 2
- 238000000638 solvent extraction Methods 0.000 description 2
- 230000001360 synchronised effect Effects 0.000 description 2
- 238000012360 testing method Methods 0.000 description 2
- 206010016035 Face presentation Diseases 0.000 description 1
- 230000000981 bystander Effects 0.000 description 1
- 238000005266 casting Methods 0.000 description 1
- 210000004027 cell Anatomy 0.000 description 1
- 210000003850 cellular structure Anatomy 0.000 description 1
- 238000012790 confirmation Methods 0.000 description 1
- 238000007796 conventional method Methods 0.000 description 1
- 230000002708 enhancing effect Effects 0.000 description 1
- 230000010354 integration Effects 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 230000003278 mimic effect Effects 0.000 description 1
- 238000012805 post-processing Methods 0.000 description 1
- 230000000750 progressive effect Effects 0.000 description 1
- 230000001902 propagating effect Effects 0.000 description 1
- 210000003705 ribosome Anatomy 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 238000006467 substitution reaction Methods 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 230000001502 supplementing effect Effects 0.000 description 1
- 239000010409 thin film Substances 0.000 description 1
- 230000003442 weekly effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/10—Text processing
- G06F40/166—Editing, e.g. inserting or deleting
- G06F40/169—Annotation, e.g. comment data or footnotes
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/10—Indexing; Addressing; Timing or synchronising; Measuring tape travel
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/955—Retrieval from the web using information identifiers, e.g. uniform resource locators [URL]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/0485—Scrolling or panning
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/02—Editing, e.g. varying the order of information signals recorded on, or reproduced from, record carriers
- G11B27/031—Electronic editing of digitised analogue information signals, e.g. audio or video signals
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/10—Indexing; Addressing; Timing or synchronising; Measuring tape travel
- G11B27/19—Indexing; Addressing; Timing or synchronising; Measuring tape travel by using information detectable on the record carrier
- G11B27/28—Indexing; Addressing; Timing or synchronising; Measuring tape travel by using information detectable on the record carrier by using information signals recorded by the same method as the main recording
- G11B27/32—Indexing; Addressing; Timing or synchronising; Measuring tape travel by using information detectable on the record carrier by using information signals recorded by the same method as the main recording on separate auxiliary tracks of the same or an auxiliary record carrier
- G11B27/327—Table of contents
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/10—Indexing; Addressing; Timing or synchronising; Measuring tape travel
- G11B27/34—Indicating arrangements
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L65/00—Network arrangements, protocols or services for supporting real-time applications in data packet communication
- H04L65/40—Support for services or applications
- H04L65/401—Support for services or applications wherein the services involve a main real-time session and one or more additional parallel real-time or time sensitive sessions, e.g. white board sharing or spawning of a subconference
- H04L65/4015—Support for services or applications wherein the services involve a main real-time session and one or more additional parallel real-time or time sensitive sessions, e.g. white board sharing or spawning of a subconference where at least one of the additional parallel sessions is real time or time sensitive, e.g. white board sharing, collaboration or spawning of a subconference
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L65/00—Network arrangements, protocols or services for supporting real-time applications in data packet communication
- H04L65/60—Network streaming of media packets
- H04L65/75—Media network packet handling
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N5/00—Details of television systems
- H04N5/76—Television signal recording
- H04N5/765—Interface circuits between an apparatus for recording and another apparatus
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N9/00—Details of colour television systems
- H04N9/79—Processing of colour television signals in connection with recording
- H04N9/80—Transformation of the television signal for recording, e.g. modulation, frequency changing; Inverse transformation for playback
- H04N9/82—Transformation of the television signal for recording, e.g. modulation, frequency changing; Inverse transformation for playback the individual colour picture signal components being recorded simultaneously only
- H04N9/8205—Transformation of the television signal for recording, e.g. modulation, frequency changing; Inverse transformation for playback the individual colour picture signal components being recorded simultaneously only involving the multiplexing of an additional signal and the colour video signal
-
- G—PHYSICS
- G09—EDUCATION; CRYPTOGRAPHY; DISPLAY; ADVERTISING; SEALS
- G09B—EDUCATIONAL OR DEMONSTRATION APPLIANCES; APPLIANCES FOR TEACHING, OR COMMUNICATING WITH, THE BLIND, DEAF OR MUTE; MODELS; PLANETARIA; GLOBES; MAPS; DIAGRAMS
- G09B5/00—Electrically-operated educational appliances
- G09B5/06—Electrically-operated educational appliances with both visual and audible presentation of the material to be studied
- G09B5/062—Combinations of audio and printed presentations, e.g. magnetically striped cards, talking books, magnetic tapes with printed texts thereon
Abstract
Systems and methods are described that include causing a recording to begin capturing video content. The video content may include presenter video streams, screen projection video streams, and annotation video streams. The system and method may include generating, based on the video content and during capture of the video content, a metadata record representing timing information for synchronizing at least one portion of the video content with input received in at least one of the presenter video stream, the screen projection video stream, or the annotation video stream.
Description
Cross Reference to Related Applications
The present application is a continuation of and claims to U.S. application Ser. No.17/303,075 filed 5/19 at 2021, the disclosure of which is incorporated herein by reference in its entirety.
Background
When a presentation is made, the presenter often must repeat instructions and information to explain concepts to a group of users. In turn, each user will typically make notes about the concept so that the notes can be further reviewed at a later time. If a record is generated from a presentation, the presenter may repeat the concept less often. However, conventionally recorded video may not provide a user with a simple way to find specific content within the video without viewing and/or scanning the entire video. That is, when a user looks for a concept in a video, the user will have to watch or scroll through the entire recording to locate the concept.
Disclosure of Invention
The systems and methods described herein may provide a plurality of User Interfaces (UIs) and/or presentation tools to facilitate interactions with video content. For example, these tools may facilitate recording, sharing, viewing, searching, and projecting video content. The video content may be instructional, demonstrative, and/or otherwise based on information and input provided by any number of demonstrators and consumed by any number of users. The systems and methods described herein may provide, execute, and/or control UI and presentation tools based on commands received from applications (e.g., browser, web application, native application, etc.) and/or commands received from an operating system (O/S) of a computing device. In some implementations, the UI and presentation tools described herein may be provided in a hybrid combination of information from both the application and the O/S. For example, portions of tools, UIs, and related instructional content (e.g., video content, files, notes, etc.) may be provided by different application-triggered or O/S-triggered sources.
The systems and methods described herein may present a presentation tool that includes at least an interactive toolbar having a plurality of selectable tools (e.g., screen projection, recording screen projection, presenter camera (e.g., forward (i.e., self-portrait) camera), real-time transcription, real-time translation, laser pointer tools, annotation tools, and magnifier tools). The toolbar may be configured for easy presentation, recording, projection by a presenter with a single input. Additionally, the toolbar may provide options for switching presentations, recordings, and/or projections. For example, a particular tool and/or screen content may be configured to switch between on and off during recording. In some implementations, the viewer of the recording may also be provided with specific tools (in real-time or post-recording) for switching the toolbar, screen content, and/or video stream associated with the video. For example, particular elements of the recording (e.g., forward camera stream, transcript stream, translation stream, annotation stream, etc.) may be switched on or off during the recording and/or during user review of the recording.
The systems and methods described herein are configured to enable presentation tools to trigger sharing of content from one or more computer displays. The presentation tools may allow a presenter and/or user to annotate (i.e., annotate) shared content in an efficient manner. The annotations may be stored so that they may be later retrieved and aligned with the time stamp and video content in order to be accurately placed on the shared content. For example, the content may be annotated during video recording and/or projection of the content. The annotations may be layered onto the content (e.g., underlying application content) and stored in metadata such that when a window event is detected (i.e., the annotations move when the window scrolls, resizes, or moves across the UI), the annotations can be removed or adapted to be properly positioned to move with the content. For example, if the presenter switches to another document (or scrolls within a document) during recording, the annotation layer is saved using metadata to trigger the appropriate annotations to be overlaid on the appropriate content, for example, when the presenter switches between documents throughout the recording. This may allow concepts to be depicted using multiple sources, and may allow a presenter to place marker annotations on content in the overlay (i.e., rather than in word processing editing) to allow the overlay to be removed and reapplied when the presenter or user requests that the overlay be removed or reapplied.
The systems and methods described herein may store annotations so that a presenter or user may switch between multiple documents, applications, or other recorded content (accessed while recording occurs) while annotating such content, and the annotations may be retrieved and provided as overlays, with the annotations properly located, as performed during video recording. The screen content, presenter camera captured content, transcript content, translation content, and annotation content may be configured to switch between on and off during and after recording (i.e., during presenter viewing and user viewing).
In some implementations, the presentation tools described herein include annotation tools configured to allow a presenter or user to use one or more marking tools to indicate chapters within content, key ideas within content during recording. The marking tool may include any number of input mechanisms including text input, laser pointer (and/or cursor, controller input, etc.), pen input, highlighting input, shape input, etc.
In some implementations, the systems and methods described herein may generate and display real-time transcription and/or translation of audio content and video content. Transcription and/or translation may be depicted on the screen along with other teaching content. In some embodiments, transcripts and/or translations may be generated and then collated for later viewing. For example, the transcript may be formatted for review and formatted for receiving annotations from a presenter or user, wherein the annotations may indicate particular concepts of the content as important concepts to learn.
The systems and methods described herein may include tools for performing, formatting, and displaying translations and/or transcriptions of video content. When viewing video (during or after recording), a user may scroll (e.g., video scroll) content (e.g., web pages, documents, etc.), and in response, the transcription section may automatically scroll in synchronization with the video scroll. Such synchronization between video and text content can facilitate an effective and resource-efficient search of content contained within the video, as corresponding text can be used for the search.
In some implementations, annotations and transcriptions may be used to automatically generate summarized (e.g., summarized) video representing portions of recorded video content. The systems and methods described herein may configure annotations and transcribed audio to be searchable (and/or indexed) for presentation with applications (e.g., a browser) and/or search offerings in an O/S computing device accessing recorded video content.
In some implementations, the presentation tools described herein may include an amplifier tool that allows for a single input based zoom-in or zoom-out mode. The magnifier tool may be used without manually resizing the window or web page. In addition, the amplifier tool may be used in conjunction with an annotation tool. When the user exits the zoom-in or zoom-out mode, the annotation may be automatically resized with the video content to match the content of the annotation. Such resizing enables annotations to be stored via metadata, which can be later retrieved and applied as an overlay to the content, without the annotated or scaled content being unsuitable in size when reviewing the video content after the end of recording.
A system of one or more computers can be configured to perform particular operations or actions by way of software, firmware, hardware, or combinations thereof installed on the system that in operation cause the system to perform the actions. The one or more computer programs can be configured to perform particular operations or actions by virtue of comprising instructions that, when executed by the data processing apparatus, cause the apparatus to perform the actions.
In a first general aspect, a computer-implemented method is described that includes causing a recording to begin capturing video content, the video content including a presenter video stream, a screen projection video stream, and an annotation video stream, and generating, based on the video content and during the capturing of the video content, a metadata record representing timing information for synchronizing at least one portion of the video content with an input received in at least one of the presenter video stream, the screen projection video stream, or the annotation video stream.
Implementations can include any or all of the following features. In some implementations, in response to termination of the recording, the method can include generating a representation of the video content based on the metadata recording, the representation including portions of the video content annotated by a user associated with the presenter video stream. In some implementations, the timing information corresponds to a plurality of timestamps associated with respective ones of the received inputs and at least one location in a document associated with the video content, and synchronizing the inputs includes: at least one timestamp of the plurality of timestamps is matched to at least one location in the document for the respective input.
In some implementations, the video content further includes a transcript video stream, and the transcript video stream includes real-time transcribed audio data from the presenter video stream, the real-time transcribed audio data being generated as modifiable transcript data configured for display with the screen projection video stream during recording of the video content. In some implementations, the transcript video stream further includes real-time translated audio data from the presenter video stream, the real-time translated audio data being generated as text data configured for display with the screen projected video stream and the transcribed audio data during recording of the video content. In some implementations, transcription of the real-time transcribed audio data is performed by at least one speech-to-text application, wherein the at least one speech-to-text application is selected from a plurality of speech-to-text applications determined to be accessible by the transcribed video stream, and the modifiable transcription data and text data are stored in the metadata record according to the timestamp and are configured to be searchable.
In some implementations, the input includes an annotation input associated with the annotated video stream, where the annotation input includes video marker data and video annotator data generated by a user associated with the presenter video stream. In some implementations, the presenter video stream, the screen projection video stream, and the annotation video stream are configured to switch between on and off during recording, wherein switching between on and off triggers display of or removes from display the respective presenter video stream, the respective screen projection video stream, or the respective annotation video stream.
In a second general aspect, a system is described that includes a memory and at least one processor coupled to the memory, wherein the at least one processor is configured to generate a collaborative online user interface configured to receive commands from: a renderer configured to render audio and video content associated with accessing a plurality of applications from within the user interface; an annotation generator tool configured to receive annotation input in the user interface and generate a plurality of annotation data records for the received annotation input during rendering of the audio and video content, the annotation generator tool comprising at least one control for receiving the annotation input; a transcription generator tool configured to transcribe the audio content during rendering of the audio and video content and display the transcribed audio content in a user interface; a content generator tool configured to generate representations of audio and video content in response to detecting termination of rendering. The representation may be based on annotation input, video content, and transcribed audio content, where the representation includes portions of the rendered audio and video marked with annotation input.
Implementations can include any or all of the following features. In some implementations, the content generator tool is further configured to generate URL links to representations of the audio and video content and index the representations to implement a search function for finding at least a portion of the audio and video content in the web browser application. In some implementations, the plurality of annotation data records includes an indication that at least one of the plurality of applications received the annotation input, and machine readable instructions for overlaying the annotation input onto at least one image frame depicting a portion of the rendered video content of the indicated at least one application according to the respective time stamp.
In some implementations, overlaying the annotation input onto the at least one image frame includes retrieving at least one annotation data record of the plurality of annotation data records, executing machine-readable instructions, and generating a document that enables a user to scroll the at least one image frame, wherein the annotation input is overlaid onto the at least one image frame in accordance with the at least one annotation data record. In some implementations, the annotation generator tool is further configured to cause recording of the rendered audio and video content to begin, the rendered video content including data associated with a first application of the plurality of applications in which a first set of annotations during a first segment of the recorded video content is received, the first set of annotations is stored according to a respective timestamp associated with the first segment, a second set of annotations during a second segment of the recorded video content is received in the second application, and the second set of annotations is stored according to a respective timestamp associated with the second segment.
In response to detecting that the cursor focus has been switched from the first application to the second application, the annotation generator tool is further configured to retrieve the second set of annotations and data associated with the second application, match a timestamp associated with the second segment with the second set of annotations, and cause the retrieved second set of annotations to be displayed on the second application in accordance with the respective timestamp associated with the second segment.
In some implementations, the first and second sets of annotations are generated by an annotation tool that enables the first and second sets of annotations to be marked, stored, and scrolled while preserving, for each annotation in the first and second sets of annotations, an initial location on data associated with the first application or data associated with the second application. In some implementations, the annotation generator tool is further configured to, in response to detecting that cursor focus has been switched from the second application to the first application, retrieve the first set of annotations and data associated with the first application, match a timestamp associated with the first segment with the first set of annotations, and cause the retrieved first set of annotations to be displayed on the first application in accordance with the respective timestamp associated with the first segment.
In some implementations, the annotation generator tool is further configured to receive additional annotations in the second application, wherein the additional annotations are associated with respective timestamps, and generate a document from the second set of annotations and the additional annotations in response to detecting completion of the recording, wherein the document comprises: a second set of annotations and additional annotations overlaid onto data associated with the second application and transcription of the recorded audio content associated with the second segment according to respective time stamps associated with the second segment and respective time stamps associated with the additional annotations.
In a third general aspect, a non-transitory computer-readable storage medium includes instructions stored thereon that, when executed by at least one processor, are configured to cause a computing system to execute instructions comprising: causing the recording to begin capturing video content including a presenter video stream, a screen projection video stream, a transcription video stream, and an annotation video stream; and generating, based on the video content and during capture of the video content, a metadata record representing timing information for synchronizing at least one portion of the video content with input received in at least one of the presenter video stream, the screen projection video stream, the transcription video stream, or the annotation video stream.
Implementations may include any or all of the following features. In some implementations, the instructions further include: in response to termination of the recording, summary video of the video content is generated based on the metadata recording, the summary video including portions of the video content annotated by a user associated with the presenter video stream.
In some implementations, the timing information corresponds to a plurality of timestamps associated with respective ones of the received inputs and at least one location in a document associated with the video content, and synchronizing the inputs includes: at least one timestamp of the plurality of timestamps is matched to at least one location in the document for the respective input.
In some implementations, transcribing the video stream includes: real-time transcribed audio data from the presenter video stream, the real-time transcribed audio data being generated as text data configured for display with the screen projected video stream during recording of the video content; and real-time translated audio data from the presenter video stream, the real-time translated audio data being generated as text data configured for display with the screen projected video stream and the transcribed audio data during recording of the video content. In some implementations, the real-time transcribed audio data is generated as modifiable transcription data configured for display with the screen projected video stream during recording of the video content, and transcription of the real-time transcribed audio data is performed by at least one speech-to-text application selected from a plurality of speech-to-text applications determined to be accessible by the transcription video stream, and the modifiable transcription data and text data are stored in the metadata record according to the time stamp and configured to be searchable.
In some implementations, the input includes annotation input associated with the annotated video stream, the annotation input including video marker data and video annotator data generated by a user associated with the presenter video stream. In some implementations, the presenter video stream, the screen projection video stream, the transcript video stream, and the annotation video stream are configured to switch between on and off during recording, the switching between on and off triggering display of or removal from display of the respective presenter video stream, the respective screen projection video stream, the respective transcript video stream, or the respective annotation video stream.
In a fourth general aspect, a non-transitory computer-readable storage medium includes instructions stored thereon that, when executed by at least one processor, are configured to cause a computing system to execute instructions comprising: causing the recording to begin capturing audio content and video content, the video content including at least a presenter video stream, a screen cast video stream, a transcription video stream, and an annotation video stream, causing rendering of the audio content and video content associated with accessing a plurality of applications from within the user interface, receiving annotation input in the user interface during rendering of the audio content and video content, the annotation input being recorded in the annotation video stream, transcribing the audio content during rendering of the audio content and video content, the transcribed audio content being recorded in the transcription video stream, translating the transcribed audio content during rendering of the audio content and video content, and causing rendering of the transcribed audio content and translation of the transcribed audio content in the user interface with the rendered audio content and video content.
Implementations may include any or all of the following features. In some implementations, the computer-executable instructions are further configured to cause the online presentation system to generate content representative of at least a portion of the audio content and the video content in response to detecting the termination of the rendering of the video content and the termination of the rendering of the video content. The representative content may be based on annotation input, video content, transcribed audio content, and translated audio content, where the representative content includes portions of the rendered audio and video marked with annotation input. In some implementations, the annotation input is caused to be rendered as an overlay on the video content, the annotation input being configured to move with the video content in response to detecting a window event or cursor event that triggers a switch to other video content accessed during recording.
In a fifth general aspect, a computer-implemented method is described that includes: at least one video stream is received, metadata representing timing information associated with detected inputs in the at least one video stream is received, wherein the timing information is configured to synchronize detected inputs provided in the at least one video stream with portions of the at least one video stream. In response to receiving a request to view at least one video stream, the computer-implemented method may include: generating a portion of the at least one video stream, wherein the generating is based on the metadata and the detected user indication requesting to view a representation of the at least one video stream, and causing rendering of the portion of the at least one video stream.
Implementations may include any or all of the following features. In some implementations, the timing information corresponds to a plurality of timestamps associated with respective inputs detected in the at least one video stream and at least one location in the content associated with the at least one video stream, and synchronizing the detected inputs includes: for a respective input, at least one timestamp is matched with at least one location in a document associated with at least one video stream. In some implementations, the at least one video stream includes a presenter video stream, a screen projection video stream, a transcription video stream, and an annotation video stream. In some implementations, the representation of the at least one video stream is based on the detected input and includes rendered portions of the at least one video stream annotated with the input.
The systems, methods, computer-readable storage media, and aspects described above may be configured to perform any combination of the aspects described above, each of which may be implemented with any suitable combination of the features and aspects listed above.
Implementations of the described technology may include hardware, methods or processes, or computer software on a computer-accessible medium. The details of one or more implementations are set forth in the accompanying drawings and the description below. Other features will be apparent from the description and drawings, and from the claims.
Drawings
Fig. 1 is a block diagram illustrating an example of a real-time presentation system according to embodiments described herein.
Fig. 2A-2B are block diagrams illustrating an example computing system configured to generate and operate a real-time online presentation system according to embodiments described herein.
Fig. 3A-3C are screen shots illustrating an example User Interface (UI) of a real-time presentation system and switching between annotated content according to embodiments described herein.
FIG. 4 is a screen shot illustrating an example toolbar provided by a real-time presentation system in accordance with an embodiment described herein.
Fig. 5A-5C illustrate screen shots of examples of sharing screens in an example UI of a real-time presentation system according to embodiments described herein.
Fig. 6A and 6B illustrate screen shots of an example toolbar provided by a real-time presentation system in accordance with embodiments described herein.
FIG. 7 illustrates a screenshot of an example use of a toolbar provided by a real-time presentation system according to an embodiment described herein.
Fig. 8 illustrates a flowchart of an example of using a real-time presentation system according to embodiments described herein.
FIG. 9 is a screen shot illustrating an example of a transcription generated by a real-time presentation system according to embodiments described herein.
FIG. 10 is a screen shot illustrating an example of presenting recorded content to a user of a real-time presentation system according to an embodiment described herein.
FIG. 11 is a screen shot illustrating another example of presenting recorded content to a user of a real-time presentation system according to embodiments described herein.
FIG. 12 is a screen shot illustrating an example of critical ideas and content marked during recording of a session generated by a real-time presentation system according to an embodiment described herein.
Fig. 13A-13G illustrate screen shots depicting markup content configured by a user accessing a real-time presentation system according to embodiments described herein.
Fig. 14 is a screen shot illustrating translated text shown in real-time during recording of a session generated by a real-time presentation system according to embodiments described herein.
FIG. 15 illustrates a flowchart of an example process of generating and recording screen shots according to embodiments described herein.
FIG. 16 illustrates a flowchart of an example process of generating metadata records associated with multiple video streams according to embodiments described herein.
Fig. 17 is a flowchart of an example process for generating and recording a video presentation in a real-time presentation system according to embodiments described herein.
Fig. 18 is a flowchart of an example process for presenting a video presentation in a real-time presentation system according to embodiments described herein.
Fig. 19 illustrates an example of a computer device and a mobile computer device that can be used to implement the techniques described herein.
The use of similar or identical reference numbers in the various figures is intended to indicate the presence of similar or identical elements or features.
Detailed Description
This document describes User Interfaces (UIs) and/or presentation tools for facilitating recording, sharing, viewing, interacting with, searching, and projecting video content. The UI and presentation tools may be provided in a presentation system that may present content online and in real-time. Presentation tools may be used to interact with presented (e.g., shared, projected, etc.) content. The systems and methods described herein may provide, execute, and/or control UIs and presentation tools based on commands received from applications (e.g., browsers, web applications, extensions, native applications, etc.) and/or commands received from an operating system (O/S) of a computing device. Accordingly, the systems and methods described herein may provide an online real-time presentation system with a set of user interfaces provided for applications or O/S.
In some implementations, the systems and methods described herein may be used to generate teaching content to be presented with a presentation tool. The content may all be transcribed, translated, and annotated in real time to distinguish important instructional content. The annotations may be used to generate additional related content (e.g., tutorial content, study guidelines, representative (e.g., summaries, clips) videos and related content, video clips, screenshots, image frames, etc.). For example, an application can automatically generate summary video based on annotations provided to content during recording of the video (e.g., one or more presentations, curriculums, seminars, etc.). The annotations may be provided by a presenter and/or a user. In operation, a presenter and/or user may provide input to generate annotation tags in text form, importance indicators of presenter tags (or user tags), and/or transcribed audio content tags, where the input is generated as tags or overlays onto content recorded in a video.
Conventional online teaching videos may not provide a convenient way for a user to find specific content within a specific video without viewing and/or scanning the entire video. Once the video is recorded, conventional techniques may generate transcripts that may be searched later, but may not provide a real-time side-by-side view of portions of the video related to the transcribed content. There is a need for a solution to provide live transcription and/or translation while recording video. The systems and methods described herein provide a solution that enables side-by-side visual display of transcribed and/or translated (e.g., translation of transcribed audio content) alongside real-time annotated video content and/or screen sharing/screen projected content. This may provide the advantage of enhancing learning and understanding of the content of the video. The technical solutions provided by the systems and methods described herein may enable video content (tutorials, notes, presenter-indicated elements, transcripts, translations, etc.) to be quickly indexed and made searchable by users. For example, the systems and methods described herein may provide a native application (or web application) configured to generate presentation (e.g., screen projection) functionality having features and tools to record and interact with content being presented.
The techniques described herein provide the technical effect of enabling a single input command that triggers the start of a screen shot (or screen share) presentation, the recording of the screen shot, and the transcription/translation of the content being screen shot at the same time. Several layers of recorded content (e.g., document, website, nested video content layer, picture-in-picture layer, annotation layer, presenter camera (e.g., self-timer) layer, participant (e.g., user) layer, transcription layer, and translation layer) may be captured separately to enable a presenter (i.e., recorder) or user (i.e., participant or viewer) to switch between opening and closing these layers. This can provide a more flexible recording method and one recording can be more computationally efficient than, for example, recording different layers separately or having to post-process the video to obtain a transcription. Further, the content of the recorded screen shots may be indexed to enable the search task to retrieve and present the content while interacting with the recorded screen shots or while the recorded screen shots are determined to have been recently accessed. This can provide integration of video content (not just file names) into the OS level search function in an efficient manner to avoid lengthy post-processing of video. By implementing the techniques described herein at the OS level, the method can be more versatile than application-specific annotation methods because such OS-level methods can receive and utilize signals from executing applications to adjust annotations (e.g., such as window events).
The systems and methods herein may address the technical challenges (e.g., problems) of finding the most recent educational video content for a particular user. This can be helpful when traditional learning-based class/lectures are replaced by home or "virtual" learning. For example, when learning or performing a homework task related to the teaching content of a professor in video content for an examination, the user may not know where or how to retrieve previously captured video content. Typically, a user may have to use many previously recorded videos to learn for an examination. Conventional systems may allow a user to completely review, scan, and/or view each video. However, users may benefit from the key ideas and concepts of each video. Accordingly, the systems and methods described herein provide a solution for automatically generating representative videos that are annotated during the recording of one or more original videos to indicate key ideas and concepts. For example, the systems and methods may allow for the generation of one or more collated, searchable video content (e.g., summaries, clips) (e.g., collections thereof) that are deemed important by a presenter or by a user (e.g., a presentation participant). The generation of these representative videos is facilitated by the stream-based method of capturing content described herein.
The systems and methods described herein provide a solution to this technical problem by using an underlying O/S to generate a repository of content (e.g., metadata, video content, etc.) and a UI that can be used to present video clips. The technical solutions described herein may provide the technical effects of improved content management, improved content access, and improved UI interaction. For example, the systems and methods described herein may generate a representative video that provides interactive interpretation, presenter comments, notes, etc. of a portion of video content. In addition, conventional file searching or web browser applications may be used to search for these segments.
Fig. 1 is a block diagram illustrating an example of a real-time presentation system 100 according to the description herein. The system 100 may be provided by one or more applications 102 or operating system O/S104. In some implementations, the system 100 can access and/or receive content from online services, online drivers, online libraries, and the like. The content may be depicted in one or more User Interfaces (UIs) 106.
The real-time presentation system 100 may provide controls to the user to enable the user to select whether and when the systems, operating systems, applications (e.g., programs), and/or other features described herein may enable user information (e.g., information about the user's social network, social behavior or activity, profession, user preferences, and/or the user's current location) to be collected and whether to send content or communications from a server to the user. Additionally, the system 100 may ensure that certain data is processed in one or more ways before it is stored or used so that personally identifiable information is removed. For example, the identity of the user may be processed such that personally identifiable information of the user cannot be determined, or the geographic location of the user may be generalized (such as to a city, zip code, or state level) where location information is obtained such that a specific location of the user cannot be determined. Thus, the user can control what information is collected about the user, how that information is used, and what information is provided to the user.
The system 100 can generate any number of UIs (e.g., the UI 107) that can be screen projected, screen shared, and/or recorded and uploaded in real-time or after recording to an online resource. The UI 106 may include, present, or otherwise have access to a toolbar 108, a video stream and an audio stream 110, representative content 112, notes 114, and a repository 116. For example, the system 100 may be an online real-time presentation system (e.g., application, UI, O/S based portal) in which a user can present content using the toolbar 108, notes 114, and repository 116. The user may also use the system 100 to generate video content and audio content 110 depicting annotations 114 provided by the user and/or presenter. The presentation may be recorded, screen projected, shared, and modified to provide specific representative content 112 that may include portions of the presentation. In some implementations, the representative content 112 is summary content (e.g., audio and/or video content with or without annotations) that summarizes all or a portion of particular video content. In some implementations, the representative content 112 includes portions of video and/or audio content associated with a particular topic or category. In some implementations, the representative content 112 includes video and/or audio content that includes chapter information or title information for a particular video. In some implementations, the representative content 112 includes portions of the video that include tags (e.g., annotations), and such portions may include associated audio and/or metadata.
In general, toolbar 108 may include an interactive toolbar with multiple selectable tools (e.g., screen shots, recording screen shots, presenter cameras (e.g., forward (i.e., self-timer) cameras), real-time transcription, real-time translation, laser pointer tools, annotation tools, magnifier tools, etc.). The toolbar may be configured for easy presentation, recording, projection by a presenter with a single input. Additionally, the toolbar may provide options for switching presentations, recordings, and/or sharing. An example toolbar is shown at toolbar 118 of fig. 1. Toolbar 118 includes recording tools, laser pointer tools, pen tools (for generating annotation 114), eraser tools, amplifier tools, self-timer cameras or other capturing tools, and real-time transcription and translation tools, among others.
In some implementations, the toolbar 108 may include an annotation generator tool 108a configured to receive annotation input (e.g., annotations 120) in the UI 107. The annotation generator tool 108a (e.g., selected from the toolbar 118) may generate annotation data records (e.g., records 214) of the received annotation input 120 during rendering of audio and video content (and as shown in the UI 107). In some implementations, the annotation generator tool 108a can include at least one control (e.g., a software or hardware-based input control) to receive the annotation input 120 and trigger the storage of a timestamp of the received annotation input. For example, the system 100 may receive the annotation 114 (e.g., the annotation 120) and, in response, may store metadata (e.g., the annotation data record 214) that includes one or more time stamps indicating when the input 120 was received and in which application the input 120 was received. The metadata may then be used to generate video clips and/or representative content 112 based on when the input was received, what the input indicates and/or the importance level of the input and/or content related to the input. In some implementations, for example, any number of tools on the toolbar 118 may be part of the annotation generator tool 108a, as the user may select any number of tools to generate annotations to the content.
In some implementations, presentation system 100 may also generate and modify video stream and audio stream 110. For example, the system 100 can be used to present content using the various libraries 116 and accessed applications, images, or other resources. The toolbar 118 may be used to record content. The recorded content can be accessed by a presenter or other user. The recorded content can be used by the system 100 to automatically generate representative content 112.
In some implementations, a forward-facing camera tool (e.g., a self-timer camera) may be included on the computing device host system 100. The self-timer camera may be used to generate a presenter video stream, as shown by example presenter video stream 122. A consumer or presenter (shown in stream 122) of the content depicted in UI 107 on system 100 may switch between opening or closing the view of stream 122. For example, if stream 122 overlaps content 124, a presenter or consumer of the content depicted in UI 107 may remove stream 122 from view to ensure that more is a view of content 124. Similarly, participant video stream 126 may be depicted in UI 107. The participant video stream 126 may also be switched on or off by any participant or by a presenter.
In operation, a presenter (e.g., a user shown in stream 122) may access system 100 to be presented, for example, with UI 107 and toolbar 118. The presenter may use the toolbar 118 to share, screen cast, or otherwise share any or all of the content in the UI 107 in order to present content, annotate content, record content and/or annotations, upload content and/or annotations for future review. In this example, the presenter is accessing the system 100 via a browser application and has selected to share (e.g., cast) the entire browser application including the presentation 101, tab 128, stream 122, stream 126, and previously entered annotations 120. Toolbar 118 is also presented in the shared content and may switch into and out of view.
Fig. 2A-2B are block diagrams illustrating an example computing system 200 configured to generate and operate the real-time online presentation system 100 according to embodiments described herein. The system 100 may operate on any of the computing systems described herein in a desktop operating system, a mobile operating system, an application extension, or other software. The system 200 may be used to configure computing devices (e.g., computing system 201, computing system 202, and server computing system 204) and/or other devices (not shown in fig. 2A) to operate the system 100 (and corresponding UIs). For example, system 200 may generate multiple UIs to allow a presenter to share, annotate, and record audio and video using system 100.
As shown in FIG. 2A, computing system 202 includes an operating system (O/S) 216. In general, the O/S216 may be used to execute and/or control applications, UI interactions, accessed services, and/or device communications, not shown. For example, the O/S216 may execute and/or otherwise manage the applications 218 and the UI generator 220. In some implementations, the O/S216 may also execute and/or otherwise manage the real-time presentation system 100. In some implementations, one or more applications 218 may execute and/or otherwise manage the real-time presentation system 100. In some implementations, the browser 222 may execute and/or otherwise manage the real-time presentation system 100.
The application 218 may be any type of computer program capable of being executed/delivered by the computing system 202 (or the server computing system 204 or via an external service). The applications 218 may provide a user interface (e.g., application window, menu, video stream, toolbar, etc.) to allow a user to interact with the functionality of the respective application 218. The application window of a particular application 218 may display application data as well as any type of control, such as menu(s), icons, toolbars, widgets, and the like. The application 218 may include or have access to application information 224 and session data 226, both of which may be used to generate content and/or data and provide such content and/or data to the user and/or O/S216 via the device interface. Application information 224 may correspond to information executed or otherwise accessed by a particular application 218. For example, the application information 224 may include text, images, video content, metadata (e.g., metadata 228), control signals associated with inputs, outputs, or interactions with the application 218. In some implementations, the application information 224 may include data downloaded from a cloud server, server 204, service, or other storage resource. In some implementations, the application information 224 can include data associated with the particular application 218, including but not limited to metadata, tags, time stamp data, URL data, and the like. In some implementations, the applications 218 can include a browser 222. The system 100 can utilize the browser 222 to configure content for presentation, projection, and/or otherwise sharing.
Session data 226 may relate to user sessions 230 with applications 218. For example, the user may access the user account 232 via a user profile 234 on the computing system 202 or associated with the computing system 202, or alternatively via the server computing system 204. Accessing the user account 232 may include providing a user name/password or other type of authentication credentials and/or licensing data 236. A login screen may be displayed to permit the user to provide user credentials that, when authenticated, allow the user to access functions of the computing system 202. The session may begin in response to user account 232 being determined to be accessed or when one or more User Interfaces (UIs) of computing system 202 are displayed. In some implementations, the computing system 202 may be used to authenticate and access sessions and user accounts without communicating with the server computing system 204.
In some implementations, the user profile 234 can include multiple profiles for a single user. For example, a user may have a business user profile and a personal user profile. Both profiles may utilize the real-time presentation system 100 to use and access content items stored from both user profiles. Thus, if the user has a browser session opened with a professional profile and an online file or application opened with a personal user profile, the system 100 can access content on both profiles.
During the session (and if authorized by the user), session data 226 is generated. Session data 226 includes information about session items used/enabled by the user during a particular computing session 230. The conversation item can include clipboard content, browser tabs/windows, documents, online documents, applications (e.g., web applications, native applications), virtual desktops, display states (or modes) (e.g., split screen, picture-in-picture, full screen modes, self-timer modes, etc.), and/or other graphical control elements (e.g., files, windows, control screens, etc.).
When a user initiates, enables, and/or manipulates these session items on the user interface, session data 226 is generated. Session data 226 may include an identification of which session item (e.g., document, browser tab, etc.) has been launched, configured, or enabled. Session data 226 may also include any or all of window orientation, window size, whether the session item is positioned in foreground or background, whether the session item is in focus or out of focus, time of use (or last use) of the session item, and/or recency or last order of appearance of the session item, and/or metadata defining such details of the session. In some examples, session data 226 may include recorded content of the session, such as audio stream record 110a and video stream record 110b. Such records may be stored on a server (such as server 204 or cloud server), locally (e.g., on device 201 or 202), or in a particular library 116 configured to store recorded content and metadata of system 100.
In some examples, session data 226 is transmitted to server computing system 204 over network 240, where the data may be stored in memory 242 in association with user account 232 according to user permissions data 236 for a user at server computing system 204. For example, when a user initiates and/or manipulates a conversation item on a user interface on computing system 202 (e.g., of system 100), conversation data 226 about the conversation item may be transmitted to server computing system 204. In some implementations, the session data 226 is instead (or as well) stored within a memory device 244 on the computing system 202.
The UI generator 220 may generate content items and toolbar representations for rendering in UIs associated with the system 100 and/or provided by the system 100. The UI generator 220 may perform searches, content item analysis, browser process launching, and other processing activities to ensure accurate and efficient rendering of content items within a particular region or sequence in the UI associated with the system 100. For example, the generator 220 may determine how to render a particular content item in a UI associated with the system 100. In some implementations, the generator 220 can add formatting to the content items depicted by the system 100. In some implementations, the generator 220 can remove formatting from the content items depicted by the system 100.
As shown in FIG. 2A, the O/S216 may include or have access to services (not shown), a communication module 248, a camera 250, a memory 244, and a CPU/GPU 252. The computing system 202 may also include or have access to metadata 228, preferences 256. Additionally, computing system 202 may also include or have access to input device 258 and/or output device 260.
Services (not shown) to which system 200 may have access may include online storage, content item access, account session or profile access, permission data access, and the like. In some implementations, a service may be used in place of the server computing system 204, where user information and accounts 232 are accessed via the service. Similarly, the real-time presentation system 100 may be accessed via one or more services.
The camera 250 may include one or more image sensors (not shown) that may detect changes in background data associated with camera capture (and video capture) performed by the computing system 202 (or another device in communication with the computing system 202). The camera 250 may include a backward capture mode and a forward capture mode.
The computing system 202 may generate and/or distribute specific policies, permissions and preferences 256. Policies and permissions, as well as preferences 256, may be configured by computing system 202, the device manufacturer of system 100, and/or by a user accessing system 202. Policies and preferences 256 may include routines (i.e., a set of actions) triggered based on audio commands, visual commands, schedule-based commands, or other configurable commands. For example, the user may set a particular UI to be displayed and begin recording interactions with the UI in response to the particular action. In response to detecting such an action, the system 202 may display a UI and trigger recording. Other policies and preferences 256 may be configured to modify and/or control content associated with the system 202 configured with policies and permissions and/or preferences 256.
The input device 258 may provide to the system 202, for example, data received via a touch input device capable of receiving tactile user input, a keyboard, a mouse, a hand controller, a wearable controller, a mobile device (or other portable electronic device), via a microphone capable of receiving audible user input, and the like. The output device 260 may include, for example, a device that generates displayed content for visual output, one or more speakers for audio output, and the like.
In some implementations, the computing system 202 may store the particular application and/or O/S data in a repository. For example, the annotation 114, data record 214, metadata 228, audio stream record 110a, and video stream record 110b may be stored for later searching and/or retrieval. Similarly, screen capture and annotation video streams may also be stored and retrieved from such a repository.
The server computing system 204 may include any number of computing devices in the form of a number of different devices, such as a standard server, a group of such servers, or a rack server system. In some examples, server computing system 204 may be a single system sharing components such as processor 262 and memory 242. For example, user account 232 may be associated with system 204 and session 230 configuration and/or profile 234 configuration according to user permissions data 236 and may be provided to system 202 upon request by a user of user account 232.
Network 240 may include the internet and/or other types of data networks, such as a Local Area Network (LAN), a Wide Area Network (WAN), a cellular network, a satellite network, or other types of data networks. Network 240 may also include any number of computing devices (e.g., computers, servers, routers, network switches, etc.) configured to receive and/or transmit data within network 240. Network 240 may also include any number of hardwired and/or wireless connections.
The server computing system 204 may include one or more processors 262, an operating system (not shown), and one or more memory devices 242 formed in a substrate. Memory device 242 may represent any kind (or kinds) of memory (e.g., RAM, flash memory, cache, disk, tape, etc.). In some examples (not shown), the memory device 242 may include external storage, such as memory that is physically remote from the server computing system 204 but accessible by the server computing system 204. The server computing system 204 may include one or more modules or engines that represent specially programmed software.
In general, computing systems 100, 201, 202, and 204 may communicate with each other via communication module 248 and/or wirelessly transmit data between each other via network 240 using the systems and techniques described herein. In some implementations, each system 100, 201, 202, and 204 may be configured in system 200 to communicate with other devices associated with system 200.
Fig. 2B illustrates an example architecture 263 for recording video and audio and storing the resulting recorded content (e.g., audio stream record 110a, video stream record 110B, recorded annotations 114, and other recorded video streams) and associated metadata 228. In this example, the real-time presentation system 100 is accessed via a native application of the O/S and uses a logging tool associated with the native application. The recordings (e.g., video and audio streams) may be uploaded to the online drive in real-time.
As shown in FIG. 2B, the O/S216 may include or have access to the real-time presentation system 100 and any number of applications 218. For example, the application 218 may also include a browser 222. Browser 222 represents a web browser configured to access information on the internet. Browser 222 can launch one or more browser processes 264 to generate browser content or other browser-based operations. Browser 222 may also launch browser tab 266 in the context of one or more browser windows 268.
The applications 218 may include a web application 270.Web application 270 represents an application program stored on a remote server (e.g., a Web server) and delivered over network 240, for example, through browser tab 266. In some implementations, the web application 270 is a progressive web application that may be saved on a device and used offline. The applications 218 may also include non-web applications, which may be programs stored at least partially (e.g., locally) on the computing system 202. In some examples, non-web applications may be executed by (or run on) O/S216.
Applications 218 may also include a native application 272. Native application 272 represents a software program developed for use on a particular platform or device. In some examples, the native application 272 is a software program developed for multiple platforms or devices. In some examples, the native application 272 is a software program developed for use on a mobile platform and also configured to execute on a desktop or laptop computer.
In some implementations, the real-time presentation system 100 may be executed as an application. In some implementations, the system 100 can execute within a video conferencing application. In some implementations, the real-time presentation system 100 may execute as a native application. In general, the system 100 can be configured to support selection, modification, and recording of audio data or text, HTML, images, objects, tables, or other content items within the application 218.
The presentation system 100 shown in fig. 2B includes a record 273, a real-time transcription 274, a real-time translation 275, a drawing 276, and key ideological metadata 278. Each element 273-278 may be recorded during a session of system 100. The recorded elements 273-278 may represent video and/or audio streams that may be annotated by a first user (e.g., presenter) during a session and provided (shared, projected, streamed, etc.) to any number of other users (data consumers, participants, etc.).
In some implementations, the streams of records associated with elements 273-278 may be generated using one or more tools associated with system 100. The system 100 may include and/or have access to a memory and at least one processor coupled to the memory, wherein the at least one processor is configured to generate a collaborative online user interface (e.g., the system 100). The user interface is configured to receive commands from the renderers and tools/toolbars 108 (e.g., annotation generator tool 108a, transcription generator tool 108b, video content generator tool 108 c). Each tool/toolbar 108 may be accessible via a UI or toolbar presented by system 100.
A renderer (e.g., UI generator 220) may be configured to render audio and video content associated with accessing one or more of the plurality of applications from within a user interface of system 100. For example, the renderer may utilize the UI generator 220 to render applications, annotations, cursors, inputs, video streams, or other UI content within the system 100 or associated with the computing system 202.
The annotation generator tool 108a (e.g., on the toolbar 118) may be configured to receive annotation input (e.g., annotation input 120) in a user interface. The annotation generator tool 108a may then use the input to generate any number of annotation data records for the received annotation input(s) during rendering of the audio and video content. The annotation generator tool 108a may include at least one control to receive annotation inputs and cause the storage of timestamps of the respective received annotation inputs. The time stamp may be used to match the video content with annotations, transcriptions, translations, and/or other data associated with the system 100.
In some implementations, the annotation data record 211 (e.g., generated from the annotations 114 and/or metadata 228) can include an indication that at least one application being accessed is receiving annotation input. The annotation data record 211 may further comprise machine readable instructions for overlaying annotation input (according to the respective timestamp) onto at least one image frame depicting a portion of the rendered video content of the indicated application. For example, annotation data record 211 may utilize any number of video streams, metadata, and annotation inputs to determine which particular application is receiving annotations and when to receive annotations in order to determine, for example, the correct positioning of overlays (e.g., video stream overlays) for particular frames of one or more other video streams depicting an application. These image frames and annotation overlays can be used to generate representative content 112 to allow a user to quickly review the annotated concepts, which can allow the user to avoid reviewing the entire video stream.
Overlaying the annotation input onto the at least one image frame may include retrieving at least one of a plurality of annotation data records, executing machine readable instructions for performing the overlaying. The system 100 can then generate a document (e.g., an online document, video clip, transcript clip, image, etc.), wherein the document enables a user to scroll through the at least one image frame with annotation input overlaid over the at least one image frame (based on the annotation data record(s) indicative of the timestamp, annotation, etc.).
The transcription generator tool 108b may be configured to transcribe audio content captured during rendering of the audio and video content and may display the transcribed audio content in a user interface associated with the system 100. In some implementations, the transcription generator tool 108b can also provide a marker, highlighting, or other indicator overlaid on the transcribed text to indicate to the user viewing the presentation a particular location in the transcription that corresponds to the audio speech being rendered by the system 100 and spoken by the presenter. In some implementations, additional indicators may be provided with or on the transcribed text to indicate important concepts or languages. A user accessing a record at a later time can utilize such an indicator to quickly find important concepts or languages. In addition, the system 100 may use such indicators as triggers to obtain audio content, video content, transcript content, translation content, and/or annotation content that appear within a time threshold associated with the marking of a particular indicator. Such indicators may be used to generate summary content and/or other representations of video streams (e.g., audio and video content).
For example, the summary generator tool 108c may be configured to retrieve such indicators (and/or annotations) in response to detecting termination of rendering of audio and/or video in order to generate the representative content 112. Representative content may be based on annotation input, video content, and transcribed audio content. In some implementations, summary content may include portions of rendered audio and video marked with annotation input (or other indicators). In some implementations, the video content generator tool 108c is further configured to generate URL links to the representative content 112. For example, the system 100 may trigger uploading specially assembled, collated or otherwise combined portions of video and/or audio content of one or more video streams to a website or online storage memory to allow for convenient and later access to those portions. In some implementations, the tool 108c can also index the representative content 112 to implement a search function for locating at least a portion of the representative content 112 using, for example, the web browser application 222.
In operation, a first user (e.g., presenter-computing system 279) may trigger a session of the real-time presentation system (e.g., via an application trigger or an O/S trigger). The system may be operated by a presenter of the system 279 to present and record content. For example, system 279 may trigger recording 273 to generate video and/or audio content in the form of recorded presenter video streams (e.g., content captured from a self-timer camera), screen projection video streams (e.g., drawing 276 and screen projection 277 content), annotation video streams (annotation data records 214 and/or key idea markers and corresponding metadata 278), transcript video streams (e.g., real-time transcript 274), and/or translation video streams (e.g., real-time translation 275). The presenter may turn on/off any of these streams during recording. In some implementations, metadata 228 may be captured and stored during recording. Metadata 228 may relate to any number of video streams. Each video stream may also include audio data and/or annotation data. However, in some implementations, annotation data may be recorded separately as a video layer.
Upon triggering recording and beginning to present and/or annotate content, system 100 can trigger projection application 280 to project a presentation and/or annotation onto a separate device (e.g., conference room television 281 or other device). The system 100 may also trigger transcription of the video/audio content 282, which may be generated in real-time and provided to an online store 283. The content may be formatted for presentation in real-time within the system 100 by the formatting application 284, which formatting application 284 may also provide such transcribed (and/or translated data) to, for example, the application 285 (or other application accessible to a user using the computing system 286). In some implementations, the user may not request that translations and transcriptions be provided in the view of the UI of system 100. In this case, the presenter-computing system 279 may provide the recorded content directly to the formatting application 284 in real time and then (and in some examples via the application 285) to the user computing system 286.
In some implementations, the system 100 can cause the recording 273 to begin capturing video content (and/or audio content). The video content (and/or audio content) may be represented as a presenter video stream, a screen projection video stream, a transcription video stream, a translation video stream, an audio stream, and/or an annotation video stream. Any suitable combination of these streams may form video content, and if a presenter chooses to turn one or more of the streams off or on during recording 273, the streams within the video content may change. This ability to select different streams in a simple manner provides a flexible way of recording content and generating additional representative content from the recorded content. The system 100 may generate at least one metadata record based on the video content (and/or audio content) and during capture of the video content (and/or audio content). Each metadata record may represent timing information for synchronizing at least a portion of the video content with inputs (e.g., notes 114/records 214, key ideas metadata 278) received in at least one of the recorded video streams. In other words, the timing information can be used to synchronize input received in at least one of the presenter video stream, the screen projection video stream, or the annotation video stream (or in any other stream) with the video content. The timing information may be used at a later time to generate a learning guide (e.g., representative content 112), annotate overlays on a segment of video content, searchable video content, and so forth.
Fig. 3A-3C are screen shots illustrating an example User Interface (UI) of a real-time presentation system and switching between annotated content according to embodiments described herein. In this example, the presenter (shown in presenter video stream 122) may trigger a presentation (e.g., screen projection, screen sharing, video conference, etc.) to begin presenting and recording content for consumption by the user shown in participant stream 126. In some implementations, the system 100 is configured to trigger the start of recording of particular audio and video content rendered by the system 100. For example, a presenter may utilize a single control to indicate the start of sharing content from system 100, which may trigger automatic recording of such content.
As shown in fig. 3A, a presenter in stream 122 is presenting a first application 302 and a second application 304. The first application 302 is annotated at annotation 306 and annotation 308. For example, a presenter in stream 122 may actively annotate using a cursor 310a using a pen tool 312 from an annotator generator tool (e.g., toolbar 314). In operation, the rendered video content may include data (map and annotations 306 and 308) associated with the first application 302 from any number of open or available applications accessible by the system 100. The rendered video content may also include data (e.g., geographic concepts) associated with the second application 304.
Because a presenter (or a consumer of presented content) may annotate on any number of applications, documents, content items, or display portion(s) presented by the system 100, the system 100 is configured to track which of the above items received the annotation. Tracking annotations to annotated items may allow annotations to be captured as a layer of video content (e.g., a stream) such that the layer may be later covered or removed from view when a user accesses the recorded content at a later time. Such switching of the overlay may ensure that the user is able to properly view the application content and annotations of the appropriate application content. Additionally, the user can use a scroll control (e.g., control 316) associated with an application (e.g., application 304). The presenter may scroll through the content and cause the annotations to scroll (e.g., move) with the content in a particular application having a cursor focus for scrolling through the content. Thus, a set of overlay annotations may be captured and scrolled with the application content to ensure that the annotated application content is preserved.
As shown in fig. 3B, the presenter (shown in presenter stream 122) is presenting application content in application 304. In this example, the presenter uses toolbar 314 to annotate content in application 304, as shown by annotation 318, annotation 320, and annotation 322. Although the annotations 318-322 are depicted as text written with a selected pen tool, any number of annotations and annotation types may be entered using a marking tool and/or selections within the application content. For example, the content may be highlighted, drawn, modified, marked, etc. In some implementations, the particular content can include an indicator for marking the content. For example, some content may be related to paragraphs of text. In such an example, the entire paragraph may be marked by selecting an indicator presented on or near the paragraph in the application content. Each annotation 318-322 may be associated with one or more timestamps representing the time at which the corresponding annotation was entered by the user in the recorded video. The timestamp may indicate the manner in which the system 100 tracked and searched for particular content including annotations.
For example, tracking annotations may allow system 100 to receive a first set of annotations (e.g., annotations 306 and 308) in real-time during recording of a first segment of video content in a first application and store the first set of annotations (e.g., annotations 114 and or annotation data records 214) according to respective timestamps associated with the first segment. The system 100 may also receive a second set of annotations (e.g., annotations 318, 320, and 322) in real-time in a second application (e.g., application 304) during the recording of the second segment of video content, and may store the second set of annotations according to respective timestamps associated with the second segment. At some point, the system 100 may detect that cursor focus has switched between applications. For example, the system 100 may determine that the presenter has switched from using the application 302 with the cursor 310a in focus to the application 304 where the cursor 310b is instead in focus. Because annotations may be provided as a layer on the application content, annotations may be applied and removed in response to changes in cursor focus to avoid having annotation content that no longer applies to applications or application content for which cursor focus has been recently received.
In response to detecting that cursor focus has been switched from first application 302 to second application 304, system 100 can retrieve a second set of annotations 318, 320, and 322, and can retrieve data associated with the second application (e.g., application content, metadata, or other settings of the content). The system 100 may then match the timestamp associated with the second segment with the second set of annotations 318, 320, and 322. To properly display the annotations received at the previous timestamp, the system 100 matches the content being viewed at the time of the timestamp (e.g., screen shots, etc.) and overlays the annotations (e.g., annotations 318, 320, and 322). The system 100 may then cause the retrieved second set of annotations (e.g., annotations 318, 320, and 322) to be displayed on the second application 304 according to the respective time stamps associated with the second segment. Additionally, the system 100 may remove annotations applied to different applications associated with the system 100. For example, when the presenter switches cursor focus to application 304, system 100 may remove annotations associated with application 302. If the user were to switch back to application 302, as shown in FIG. 3A, system 100 may remove annotations 318, 320, and 322 and instead retrieve and render annotations 306 and 308 to ensure that application 302 depicts accurate annotations, e.g., from previous tags. In examples where the applications 302, 304 are arranged side-by-side (i.e., do not overlap) within the UI, the annotations 306, 308 may be displayed on the application 302 and the annotations 318, 320, 322 may be displayed on the application 304 at the same time. In this way, the user can see all annotations of the content being displayed at the same time.
In some implementations, a presenter using the system 100 can trigger generation of a first set of annotations (e.g., annotations 306 and 308) and a second set of annotations (e.g., annotations 318, 320, 322) via an annotation tool (e.g., one or more tools from the toolbar 314 or another toolbar). The annotation tool may enable tagging, storing, and scrolling of a first set of annotations (e.g., annotations 306 and 308) and a second set of annotations (annotations 318, 320, and 322) while preserving an initial location on data associated with the first application or data associated with the second application for each of the first set of annotations and the second set of annotations. That is, the annotation tool can store metadata for each annotation that indicates where (i.e., where) on the data content presented by a particular application to locate the corresponding annotation. In this way, the system 100 is able to generate an overlay of annotations that can be restored on the data content when, for example, summary content (or other representative content) is generated. In another example, the system 100 can generate such an overlay of annotations in appropriate locations on the data content as the presenter scrolls the data content and/or switches between applications.
In some implementations, additional annotations (e.g., annotation 324) can be received in the second application 304. In this example, the presenter has added library code, resource links, and notes about office time changes. Additional annotations (e.g., annotations 324) may also be associated with respective timestamps corresponding to when the annotations 324 were added to content in the application 304 during recording. In response to detecting completion of the recording, the system 100 may generate a document 328, as shown in FIG. 3C. The document 328 may be generated from the second set of annotations (e.g., annotations 318, 320, and 322) and additional annotations (e.g., annotation 324). The document may include a second set of annotations 318-322 and additional annotations 324 overlaid onto data associated with the second application 304 according to respective time stamps associated with the second segment and respective time stamps associated with the additional annotations. In some implementations, one or more still frames or video clips 330 may be generated to execute within the document 328 or may be provided as links or search results associated with the document 328. By matching the time stamps to corresponding locations in the document 328 associated with the video content, the inputs (such as the annotations 318-322 and the additional annotations 324) can be synchronized with the video content (i.e., overlaid at the correct locations on the data from the application 304).
In some implementations, the system 100 can also generate a transcription 332 of the recorded audio content associated with the second segment. In general, the document 328 may be configured to be modified at any point in time. For example, the presenter may later make changes to the recorded presentation, such as modified audio, additional markers or notes, and/or other changes. Such changes may be configured to trigger document 328 to be regenerated to include the changes. The document 328 can also be referred to as a summary content document or a representative content document.
Fig. 4 is a screen shot illustrating an example presenter toolbar 400 provided by a real-time presentation system according to an embodiment described herein. The presenter toolbar 400 includes at least a laser pointer tool 402, a pen tool 404, an amplifier tool 406, an eraser tool 408, a recording screen projection tool 410, a create chapter tool 412, a self-timer (e.g., presenter) camera tool 414, a closed caption tool 416, a transcription tool 418, and a marking tool 420. Each tool 402-420 in the toolbar 400 may be part of the annotation generator tool 108 a. For example, each tool may be used to annotate the content being presented.
The laser pointer tool 402 may be used to configure a cursor as a laser pointer during presentation with the system 100. The laser pointer tool 402 may provide a visual focus for a consumer of the presentation provided by the system 100. The pen tool 404 can provide annotation functionality for any content or portion of a presented screen (e.g., window, application, full screen, etc.). Pen tool 404 may include any number of selectable pens, color content, size, shape, etc. of content and/or text. The magnifier tool 406 may provide zoom functionality to all small text and graphics magnified by the presenter during the presentation. Erasing tool 408 can provide a delete and erase function similar to a manual eraser to correct errors or remove annotations, for example, to make room to generate more annotations.
Recording screen projection tool 410 may provide a recording function to begin recording such recorded content locally and upload it to a cloud server or other selected location. In some implementations, the recording screen projection tool 410 triggers screen projection, screen sharing or other presentation modes, and triggers recording. For example, if the presenter selects tool 410, the presentation and recording may begin simultaneously. This may provide the advantage of being easy for a user (e.g., a presenter) to present and record because the user can select a single control input to quickly begin presenting content while recording content and/or related audio content.
In general, the screen or window to be shared when selecting tool 410 may be the last screen to share settings or usage that was last detected prior to selecting tool 410. That is, the presenter's recording range may match a previously selected display range (e.g., tab, window, full screen, etc.). In some implementations, a confirmation UI may be presented upon selection of tool 410 to allow the presenter to select which display scope to share and/or record. In some implementations, the presenter may stop presentation by reselecting the tool 410. However, this action may not stop recording. This may conveniently allow the presenter to add further notes, audio or additional content that the viewer may wish to have when accessing the recording at another time.
To terminate the recording, the presenter may select another tool or command (not shown). Terminating (e.g., stopping) a recording in the system 100 may cause the toolbar 400 to be removed from view. Additionally, upon detecting an indication to stop a recording, the system 100 may automatically trigger the uploading, sending, or otherwise completing of the recording. Because records are typically uploaded when a record occurs, not when a record is completed, delay may be minimal for upload completion. In some implementations, the system 100 can be offline, and in such cases, a local copy of the record can be generated instead.
The create chapter tool 412 can be used by a presenter to record video with respect to time annotations. For example, the presenter may select the tool 412 to generate chapters for recording video at any time during the presentation. In some implementations, the create chapter tool 412 (or post-recording tool) can be used to create chapters for recording after recording is complete (e.g., post-recording). Thus, a presenter may wish to further annotate a presentation with chapters to facilitate a user's search for and review of content from the presentation at a future time. Chapters represent segments of video. The chapter may provide a preview image frame to assist the user in identifying the chapter content. The chapter may also include metadata, title data, or user-added or system-added identification data. The video divided in chapters may be presented in a timeline view so that a user may select on a previously configured chapter indicator presented in the timeline. Conventional systems that provide chapter generation provide such features after recording. That is, conventional systems do not provide the option of generating chapters in real-time (e.g., instantaneously) while recording video.
The self-timer (e.g., presenter) camera tool 414 may trigger execution of the forward-facing camera functions on the computing device (e.g., device 202) of the real-time presentation system 100. The tools 414 may be switched on and off by a presenter and/or user (e.g., consumer) of the presented content. The video stream captured by tool 414 may be used by closed caption tool 416 and/or transcription tool 418 to generate subtitles, transcriptions, and translations of audio data presented from the video/audio stream (e.g., stream 122) captured by tool 414 (e.g., via camera 250).
Transcription tool 418 represents transcription generator tool 108b, as described herein. The presenter of system 100 may switch the real-time transcription of audio between on and off. In some implementations, the transcription tool 418 can trigger a live transcription with complete translation by using the closed caption tool 416 in conjunction with the transcription generator tool 108 b. For example, transcription tool 418 may work with UI generator 220 to generate specially formatted transcriptions for rendering with content presented via a screen sharing presentation from system 100.
The marking tool 420 may be selected by a presenter, for example, to mark particular content, ideas, slides, notes, or other presented portions of a screen as critical ideas. The key ideas may represent important learning guidance materials that the presenter deems useful and/or elements that the presenter deems selectable for the representative content 112. If the presenter selects the marking tool 420, other indications (e.g., highlighting, notes, etc.) can be made of the presented content for storage as key ideas in the system 100. In some implementations, the marking tool 420 may provide user feedback in the form of a backlight or other indication on the tool 420 to provide the presenter with an understanding that the tool 420 is active. Other feedback options are also possible.
Toolbar 400 may also include a close menu control (not shown) that may be used to close or minimize the toolbar. Toolbar 400 may be moved and/or rotated for any presentation provided by system 100. In some implementations, the toolbar 400 may be hidden if a cursor is dragged over the toolbar, such as when a mouse hover event occurs over the toolbar. This may provide the advantage of ensuring that the presenter and viewer (e.g., user) of the presentation can view the content without having to manually move the toolbar 400.
Fig. 5A-5C illustrate screen shots of examples of sharing screens in an example UI of a real-time presentation system according to embodiments described herein. Fig. 5A depicts a browser 500 in which a user is accessing a presentation 101 (e.g., P101) home page. The user is also accessing content in browser tab 502 and browser tab 504. The user may decide to present the content to one or more other users. For example, the user may be a presenter who is planning to provide a presentation to a plurality of users.
The presenter may access a menu UI 506 provided by the computing system 202 (e.g., via the O/S216 or an application 218 hosting the real-time presentation system 100). UI 506 may be presented from a quick setup UI. From the UI 506, the presenter may select the presentation control 508 with the cursor 510 to be provided with additional screens to configure screen shots and/or screen shares for presenting content from the presentation 101.
Fig. 5B depicts a presentation UI 512 in which a presenter may choose to project 514 content or share content via a video conference 516. For example, a presenter may choose to present presentation 101 to a conference room television (e.g., television 281) via screen projection. Alternatively, the presenter may choose to present the presentation 101 via a video conferencing application (e.g., by means of a native application or browser application). In this example, the presenter selects to project presentation 101, as shown by cursor 518.
Fig. 5C depicts a projection UI 520 in which the presenter may select which display focus to project. Because the user is selecting shared content, the system 100 may populate the toolbar 522 to indicate that a presentation tool is available. UI 520 includes options to share the screen. These options include at least a built-in display option 524 and an external display option 526. In this example, the presenter selects the built-in display 524 as shown by cursor 528. The presenter may also be provided with the option to share a screen range. The example options depicted include an entire screen option 530, a browser tab option 532, and an application window 534. Other options are possible and based on the content in the cursor focus behind the UI 520. The presenter may be provided with an option 536 to share (or not share) audio content. An option 538 may also be provided to the presenter to render (or not render) the presenter tool. The presenter may select an option and save the selected option using save control 540.
Fig. 6A and 6B illustrate screen shots of an example toolbar provided by the real-time presentation system 100 in accordance with embodiments described herein. FIG. 6A depicts a shared presentation of a browser tab 600 with a rendered toolbar 602. The presenter can access tools on toolbar 602, similar to toolbar 400. In this example, the presenter has selected pen tool 604. In response, the system 100 has provided a sub-panel 606 for the pen tool 604 to allow the presenter to select the pen's option. The sub-panel 606 also includes a trash can option 609 for removing the selected annotation.
As shown in fig. 6A, the presenter has provided annotation inputs such as a drawing 610, text 612, and a drawing (e.g., a circle with a line 614). The presenter has also drawn additional marks 616 that appear to be erroneous or extra strokes. In this case, the user may select the mark 616 and then select option 609 to remove the mark 616.
Annotations from toolbar 602 may be generated on content within a shared window or screen. If the presenter begins drawing or annotating outside of this range, the system 100 can trigger an indication that the annotation is outside of the view. Additionally, the annotations may be scrollable and may be configured to remain with the content annotated during the recording/projection session. An annotated video stream with corresponding metadata may be captured to match content to annotations, thereby enabling access to recorded content and annotations after recording/projection. In some implementations, the system 100 can be configured to capture annotations in the annotation stream, but if a scroll event is detected, the annotations can be removed from view during recording/projection. In some implementations, for example, the system 100 may allow each user to manually clear annotations after recording.
In some implementations, when switching from one window or application to another window or application, the window switch can trigger the annotation to be removed (e.g., hidden). The annotation may then be replaced (e.g., unhidden) when switching back to the window or application associated with the annotation. In addition, the size of the annotation may be adjusted according to the resized window. In some implementations, the annotations can remain visible (i.e., rendered and displayed for viewing) as long as the underlying application content is visible to the user. In other words, the annotation may be visible even if the associated application is overlapped by another window or application, or otherwise not in the foreground.
FIG. 6B depicts an example toolbar 602 with another example pane 620. In this example, toolbar 602 includes a garbage can option 622 for deleting a particular annotation, a redo/undo button for redo or undo annotation input, a static pen 626, a temporary pen 628, a highlighter 630, and any number of selectable colors 632, 634, and 636, just to name a few examples. For example, other sub-panels may be provided for display to allow the presenter to select colors, fonts, line patterns, or other options associated with the pen tool 604.
Fig. 7 illustrates a screenshot of an example use of the toolbar 108 provided by the real-time presentation system 100, according to an embodiment described herein. UI 700 depicts a partial map of the united states. The presenter can interact with the UI 700 and the depicted content of the UI 700 using the toolbar 702. In this example, the presenter has selected during the recording of the presentation to create a chapter tool 704 to generate chapters, as indicated by an indicator message 708 that the presenter has generated two chapters.
The create chapter tool 702 can be used by a presenter to record video with respect to time annotations. For example, a presenter may select tool 702 at any time during a presentation to generate chapters for recording video. Chapters represent segments of video. The chapter may provide a preview image frame to assist the user in identifying the chapter content. The chapter may also include metadata, title data, or user-added or system-added identification data (or trigger storage thereof). The video divided in chapters may be presented in a timeline view so that a user may select on a previously configured chapter indicator presented in the timeline.
As shown in fig. 7, a self-timer camera stream (e.g., presenter video stream) may be used to generate a pass-through view 706 for provision in any portion of the presentation UI space. The presenter may be a presenter or a presenter of video and audio content. For example, a presenter video stream may be automatically positioned to a location on the screen throughout the recording to ensure that the stream does not obstruct the view of the content being annotated. In some implementations, the presenter can drag the presenter video stream of view 706 within the presented UI content. In some implementations, the presenter may zoom out or zoom in on the view 706. In some implementations, the presenter can crop the view 706. In some implementations, the presenter may hide the view 706.
Fig. 8 illustrates a flowchart of an example of using a real-time presentation system according to embodiments described herein. In this example, a presenter may use the system 100 to present ideas or content. In operation, a user may access system 100 via a quick setup UI (such as UI 506 or UI 512). The user may select 804 a destination for the presentation. For example, the user may be presented via projection or via a video conference. The user may then select (806) a range of screens to share. For example, the user may choose to share one or more screens, one or more browser tabs, one or more applications, one or more windows, and so forth.
In some implementations, the user may wish to record a screen projection of the presentation and this may be done by selecting (808) to also record the presentation. Screen projection may then begin. In some implementations, the quick setup UI may provide the option to project, share, and record with a single input command. The user may then execute the presentation and may generate 810 notes, chapters, and other data. The user may select 812 to stop rendering by selecting a stop rendering control. If the user selects to record the presentation (e.g., screen shot), the user may end the presentation by stopping the recording, which may trigger 814 the system 100 to complete the recording and complete uploading the recording to the repository.
Fig. 9 is a screen shot 900 illustrating an example of a transcript 902 generated by a real-time presentation system according to an embodiment described herein. The view of screenshot 900 may be provided after the recording of the presentation/screen projection. The system 100 may have generated the transcription 902 in real time as the recording occurs. In addition, the presenter may have annotated during the recording to mark the key ideas 904 and 906. The presenter may perform post-recording annotations and marks to make the video content useful to other users. For example, the presenter may decide to generate additional annotations and/or key ideas indicia, such as key ideas 908 and 910, and may do so after recording. New key ideas and/or annotations may be part of the video stream that may be added to the recorded data. Similarly, the presenter may add more audio data by recording additional content. Transcription 902 may be updated with new audio data. In addition, transcript 902 may be otherwise modified after recording to add or delete content.
In some implementations, the system 100 can automatically highlight particular content being accessed after recording. The highlighted content may indicate some kind of error or mistake to the presenter. Highlighting draws attention to errors or mistakes so that the presenter can correct the errors, for example, before propagating additional information (e.g., representative content 112, video stream, etc.) with the recording. In some implementations, the system 100 can indicate an area in which additional information is to be provided. For example, the presenter may add titles, labels, etc. to the key ideas.
In some implementations, the system 100 may utilize machine learning techniques to learn and correct particular errors. In some implementations, the system 100 can utilize machine learning techniques to learn which content is presented to a presenter in order to provide a list of items to update and/or correct. In some implementations, the system 100 can utilize machine learning techniques to automatically generate titles and additional content from the records to allow a presenter to pick and select which updates to apply or add to the records.
The presenter may also add closed caption content and/or translated content as shown in UI 912. In some implementations, the user can select one or more languages using control 914 to provide transcribed content, closed captioning content, and/or translated content in as many languages as the presenter determines to provide.
FIG. 10 is a screen shot illustrating an example of presenting recorded content to a user of a real-time presentation system according to an embodiment described herein. In this example, the presenter may have completed a recording, a portion of which is shown in screenshot 1000. In response, the system 100 can analyze and index the recorded content (e.g., any or all of the video streams, annotations, transcriptions, translations, audio, presentation content, or resources accessed during presentation, etc.). The analysis may also include determining which content in the recording to use to generate portions of the video content (e.g., representative or summary video or clips, learning guides, audio tracks, etc.). Such content can be generated based on metadata records and can include portions of video content annotated by the presenter (or by a user associated with the presenter video stream). In some implementations, the summarized video may also include other portions of the video content that are not annotated but instead selected for inclusion in the representative content.
As depicted in fig. 10, system 100 generates a video segment 1002 that discusses translation and transcription, which involves ribosomes in cells. The presenter may provide for indicators, titles, and/or messages to be presented with the video clip 1002 as shown by the presented item 1004. The item may be presented based on annotations generated by the presenter. A user receiving the presented item 1004 may select a link, video, or other information to obtain information presented by the item 1004 and/or to respond or comment on the item.
The user may also use control 1006 to search for content in the record, metadata, or other streams associated with the record. In this example, the user has entered a search query for the term "cellular structure". In response, the system 100 can provide the presented item 1004 as a search result and highlight a portion that includes a transcription (or translation) of the search term, as shown by highlighting 1008. In addition, the system 100 can highlight additional transcribed or translated content 1010 that may be relevant to the search query.
FIG. 11 is a screen shot illustrating another example of presenting recorded content to a user of a real-time presentation system according to embodiments described herein. In this example, web browser application 1102 executing system 100 depicts the instructional content, for example, in window 1104. The system 100 can generate representative content 112 as shown by menu 1106 and UI 1108. Representative content of the menu 1106 may include an example menu 1106 accessed by a user viewing content in the window 1104. Menu 1106 includes available video clips 1110 that are related to the subject matter presented in window 1104. In some implementations, the video clip 1110 can include clips or image frames of content presented on a particular topic or date. In some implementations, any number of video clips and/or links may be embedded in menu 1106 to provide quick answers and content to the user. Thus, instead of presenting results from the Internet, the system 100 can present search results from previously accessed content locally accessed, in an online library, in an online drive, and/or from another repository. In some implementations, the system 100 can prioritize recently accessed or viewed key idea clips (e.g., video clips). Menu 1106 may be provided at a time that is useful for the user to access the menu. In addition, related searches may be presented as options in menu 1106. For example, a user accessing menu 1106 is provided with a search for the term "ribosome" 1112 based on topics discussed in the content of window 1104.
The system 100 may otherwise present the recorded content to the user. For example, the O/S provided menu 1114 may present additional content associated with the window 1104 or with record (S) corresponding to content provided in the window 1104. In this example, the O/S presents the search results in UI 1108. In some implementations, the system 100 can present content in the UI 1108 based on the search query 1120 entered by the user. For example, the input search query 1120 may be matched with key ideas from video recordings associated with window 1104 and may be presented as O/S generated search results.
As shown, the UI 1108 includes video and timeline 1116, which are key ideas for top search results. The user may select on any event listed in timeline 1116 to be directed to the video portion that includes such content in window 1104 or the new window. In addition, the UI 1108 also includes one or more related videos 1118 related to the content accessed in the window 1104.
In some implementations, the menu 1106 and/or content presented in a UI, such as UI 1108, can also be retrieved from sources other than the particular recorded video accessed in window 1104. For example, the system 100 may retrieve content for populating the menu 1106 and/or UI 1108 from another presenter or another presentation similar to the presentation being accessed in the window 1104 (or similar to the content in the presentation). Thus, the system 100 can utilize content from other demonstrators, enterprises, users, and/or one or more authoritative sources or resources regarding topics determined to be related to content accessed in window 1104.
FIG. 12 is a screen shot illustrating an example of critical ideas and content marked during recording of a session generated by a real-time presentation system according to an embodiment described herein. In this example, the user may use an extension, application, or O/S that provides and initiates screen projection. For example, the browser window 1200 may be shared using the system 100. The shared content includes at least time lines 1202 with key ideas 1204, 1206, and 1208, each corresponding to a respective time stamp 1210, 1212, and 1214. The timeline 1202 may be generated by a presenter of the content 1216, for example during a presentation. The presenter may instead generate the key ideas and timeline 1202 after completion of the video recording. It can be seen that the transcripts are synchronized with the timeline 1202 such that scrolling of one of the content 1216 or the transcripts causes corresponding scrolling of the other.
Fig. 13A-13G illustrate screen shots depicting markup content configured by a user accessing the real-time presentation system 100 according to embodiments described herein. In this example, the user may use an extension, application, or O/S that provides and initiates screen projection. Toolbar 1302 is depicted with browser window 1304 projected by online real-time presentation system 100. Toolbar 1302 may be launched at the beginning of the projection of browser window 1304, which may enable a presenter to select a tool to begin video annotation (e.g., annotate moving or still video content). In some implementations, the toolbar described herein may be bypassed if, for example, a presenter uses a stylus, smart pen, or other such tool to provide input in the content of the presentation.
Referring to fig. 13A, toolbar 1302 includes pointer tools, temporary pen tools, closed captioning tools, mute tools, and key ideas marking tools 1306. Marking tool 1306 may represent controls that may be selected by a presenter, for example, to mark particular content, ideas, slides, notes, or other rendered portions of a screen as critical ideas. The key ideas may represent important learning guidance materials that the presenter deems useful and/or elements that the presenter deems selectable for the representative content 112. In general, key ideas may be organized by date, time stamp, and/or topic.
In this example, the presenter has used a pen tool to enter text 1308 and/or highlight 1310 and 1312. The presenter may then have selected the marking tool 1306 and then marked the text 1308 and highlighting the annotations of 1310 and 1312 to indicate such content as a key idea. In response, the system 100 may provide an indicator message 1314 to provide feedback to the presenter regarding the ideas marked as critical ideas. In some implementations, the marking tool 1306 can also be used to generate chapters (e.g., generate video marks of marking data, generate chapter marks of marking data, etc.), which can be provided as annotation input along with video marker data (i.e., highlighting 1310 and/or text 1308). The presenter may mark such annotation inputs with video annotation and key ideas in real-time and during recording using marking tool 1306 and/or other toolbar tools. For example, a presenter may interactively mark chapters, notes, key ideas, and the like while presented. The interactive derived annotations can be used by the system 100 to generate learning guidelines, representative content 112, video clips, and searchable content to enable users (e.g., presentation participants) to easily access key ideas and/or annotated summary videos.
Referring to fig. 13B, the browser window 1304 is shown with additional transcription segments 1316. Transcription segment 1316 may be generated in real-time while the presenter is speaking and presenting content in window 1304 using system 100. Dubbing segment 1316 may represent a currently recorded dubbing video stream. Transcription segment 1316 may highlight the sentence currently being spoken, as shown by highlighting 1318. In the case where the user is accessing the recorded video after completion of recording, the sentence that is currently being spoken may be highlighted and continue to be updated as speech (e.g., audio) is provided throughout the video. This may provide the advantage of allowing a user to follow transcription segment 1316. As the audio progresses, the update is highlighted to illustrate the particular audio being spoken.
In some implementations, the presenter or user can access the record after completion and can navigate through the transcript to cause the content in window 1320 to be updated according to the transcript selected in segment 1316. For example, the user may select a paragraph in the transcription to navigate to the beginning of the paragraph and trigger matching content in window 1320. In addition, the user may access search control 1322 to search for transcribed content. The browser window 1304 also depicts a share option 1324 to allow a presenter or user to share a particular complete recording, a portion of a transcript, a portion of window 1320, or other portion of a video recording.
Referring to FIG. 13C, a browser window 1304 is shown and includes additional options. For example, a marking tool 1326 is provided on a transcription paragraph to enable a user to mark (or de-mark) a particular portion of a transcription (and the resulting video portion associated with the transcription) as a key idea. For example, the user has marked a paragraph as a key idea 1328 by the selection marking tool 1326. The user may mark or unmark paragraphs in the script of the entire video. The marked portions may be accessed by the system 100 to generate representative content 112. The mark-up transcript may be used to automatically select the relevant video streams at the same time stamp (or multiple time stamps). Thus, if a particular transcribed paragraph is marked as a key idea, other content in or around the same timestamp may also be marked as a key idea. That is, marking one video stream may be used to mark other video streams with key ideas including, but not limited to, annotation (e.g., via annotating a video stream), translation (e.g., via translating a video stream), screen content (e.g., projecting a video stream via a screen), camera view (e.g., via a presenter video stream).
Referring to fig. 13D, the browser window 1304 is again shown, and the key ideas marker shown in fig. 13D is depicted in a timeline 1330, with key ideas 1328 marked at a timestamp 1332 within the video. Indicator 1334 depicts a portion of transcript 1316. The indicator may be a video clip or image frame to assist the user in identifying the content at key idea timestamp 1332. In some implementations, the user can use the timeline 1330 to mark, unmark, or otherwise modify key ideas of the mark.
Referring to FIG. 13E, the browser window 1304 is again shown and additional key ideas have been labeled. For example, the partial sequence key ideas 1336 and the no title key ideas 1338 have been labeled by a user using the system 100. Corresponding timestamps 1340 and 1342 are also generated for the timeline 1330. In one example, the user selects paragraph 1344 to trigger concept 336. In addition, when a user selects a particular translated paragraph (or other content that the user uses to generate key ideas), an editing tool 1346 may be provided. Editing tools 1346 may be used to edit any transcribed section. In some implementations, editing tools 1346 can be used to combine and/or divide transcript portions, triggering possible changes to key ideas.
Referring to fig. 13F, the user selects an editing tool 1346 to edit the transcript 1344, which may trigger editing of the key ideas 1336 in the timeline 1330. In response to the editing tool on the selection portion 1344, the system 100 can present a UI 1348. The UI 1348 may provide an entry for modifying the key idea title using the control 1350 and any portion of the actual transcription shown in the control 1352. In addition, UI 1348 may provide controls for combining or partitioning the transcribed parts, which may trigger the combining or partitioning of key ideas. Such changes to the key ideas may change the underlying video frames, text, and context of the key ideas.
Referring to FIG. 13G, a plurality of search results 1354, 1356, and 1358 are presented in response to a user entering a search 1360. Such search results may be generated by the system 100. For example, after a presenter (or other user) generates key ideas and annotations for a video provided by the system 100, the system 100 may configure the video (as well as the underlying video stream and associated metadata) to be searchable. If the user searches (in a search engine) for content associated with the video, the search engine may return search results (e.g., text, video, images, etc.) that include portions of the video and/or the associated content.
As shown in fig. 13G, the search includes a set and subset of search terms. Search results 1354-1358 may be provided because system 100 is capable of performing or triggering indexing of portions of representative video content (e.g., key ideas, transcriptions, annotations, inputs, etc.) to implement search functionality for using a web browser application to find at least a portion of the representative content. A specific URL link may be generated to direct the user to a portion of the video or text that includes representative content. In some implementations, video search results may be provided that may be selected to direct a user to a location in the video (e.g., a timestamp) that correlates a search term with a matching key idea. Each search result may be configured to include a video thumbnail and timestamp, a title, a transcription highlighting (e.g., highlighting 1362, 1364, and 1366), a user name, and an uploaded video timestamp.
Fig. 14 is a screen shot illustrating translated text shown in real-time during recording of a session generated by the real-time presentation system 100 according to embodiments described herein. For example, in addition to the closed caption version 1402 of the audio being recorded and/or presented, the system 100 may also generate and render a real-time translation 275 shown as text 1404. The user may use control 1406 to select a language to view a particular translation. In some examples, the translation in the selected language can form part of the transcript video stream or can be provided as a separate translation stream.
Tools 1408 on the toolbar 1410 can be used to switch between opening or closing closed captions. Providing closed caption content 1402 may make it easier for a user to follow during a presentation. Real-time translation content 1404 enables a user who is learning the presenter language to follow during the presentation. In some implementations, a user can access a previously recorded video that includes translations in a first language, and can select a second language to view translations in the second language. This can help the user who is requesting help from the parent or other users who do not speak the language of the presentation.
Fig. 15 illustrates a flowchart of an example process 1500 of generating and recording screen shots in accordance with embodiments described herein. The presenter may configure the computing system 202 to, for example, generate screen shots starting from one or more libraries 116 associated with the real-time presentation system 100. The library may include content associated with the presenter, which may be stored on a local storage drive, an online storage drive, a server computing system 204, or another location accessible to computing system 201 and/or computing system 202. The presenter may enter the library 116 and select 1502 to begin recording screen shots. The presenter may then select (1504) a range of content (e.g., window, tab, full screen, etc.) to record. The system 100 may engage a screen projection/screen sharing tool to trigger a UI to select a scope. Although the user is recording screen shots, the user may choose not to share the screen, for example, if the screen shot record is for the user to view at a later time.
Next, the system 100 can begin recording according to the selected scope and can present one or more toolbars (e.g., toolbar 108). The presenter may annotate the content using (1506) a screen projection tool (e.g., toolbar 108). The presenter may choose to end the recording at a certain point in time. Once the recording is complete, the system 100 may automatically upload the video (and any corresponding video streams and metadata) to the library 116 as a new available file. In some implementations, the system 100 configures the video to be viewed by and shared with other people.
Fig. 16 illustrates a flowchart of an example process 1600 of generating metadata records associated with multiple video streams according to embodiments described herein. In general, the process 1600 utilizes the systems and algorithms described herein to generate metadata records for use by the real-time presentation system 100. Process 1600 may utilize one or more computing systems having at least one processing device and memory storing instructions that, when executed, cause the processing device(s) to perform the various operations and computer-implemented steps described in the claims. In general, system 100, system 200, system 263, and/or system 1900 may be used in the description and execution of process 1600.
At block 1602, the process 1600 includes causing a recording to begin capturing video content. The video content may include any or all of a presenter video stream, a screen projection video stream, a transcription video stream, and/or an annotation video stream. For example, the system 100 may be accessed by a user (e.g., a presenter) to begin capturing a recording of video content. Such video content may include presenter video streams (e.g., content captured from a self-timer camera), screen cast video streams (e.g., drawing 276 and screen cast 277 content), annotation video streams (annotation data records 214 and/or key idea markers and corresponding metadata 278), transcript video streams (e.g., real-time transcript 274), and/or translation video streams (e.g., real-time translation 275).
At block 1604, process 1600 includes generating, based on the video content and during capture of the video content, a metadata record representing timing information. The timing information may be used to synchronize input received in at least one of the presenter video stream, the screen projection video stream, the transcription video stream, or the annotation video stream with portions of the video content. In some implementations, the input includes an annotation input associated with annotating the video stream. In some implementations, the annotations can include drawings 276, text, audio input, reference links, and the like. In some implementations, the annotation input includes video marker data and/or video annotator data generated by a user associated with the presenter video stream. For example, a presenter may use a video annotator to enter annotations to enter drawings, text, etc. as overlays on the video content. Similarly, the presenter may use a marking tool to mark chapters during recording. These chapters may be stored as videomark data that may be used to generate chapters of video content.
In some implementations, each metadata record represents timestamp data for synchronizing inputs (e.g., notes 114/records 214, key ideas metadata 278) received in at least one of the recorded video streams. In some implementations, metadata 228 may be captured and stored during recording. The metadata 228 may relate to any number of video streams and annotations received during or after recording of the video streams. Each video stream may also include audio data. In some implementations, the video stream can store annotation data as metadata. However, in some implementations, annotation data may be recorded separately as a video layer, and thus metadata 228 may be obtained from the video layer.
In some implementations, the process 1600 includes generating content representing portions of video and/or audio content based on metadata records. For example, representative content may include portions of video content annotated by a user associated with a presenter video stream (e.g., a presenter) in response to termination of a recording. The video content may include the representative content 112 and may be generated based on timing information, metadata 228, and/or other video content or annotations of the video content. The generation may be automated in response to termination of the recording, or may be initiated by a user or otherwise in response to user input at the termination of the recording. In some implementations, the representative video content may include overlay image frames depicting annotations on the rendered video content and/or screen content. In some examples, the representative content may also include one or more portions of the video content immediately preceding and/or following the corresponding portion of the video content annotated by the user.
In some implementations, the timing information corresponds to a plurality of timestamps associated with respective ones of the received inputs. For example, the timing information may correspond to annotations received during recording and/or screen projection (e.g., provided by a presenter). The received annotations may be provided at one or more particular time stamps. The timing information may also correspond to at least one location in the content or document associated with the presenter video stream, the screen projection video stream, or the annotation video stream at which the input was received (or in other words, in the content or document associated with the video content). For example, the timing of the creation of the annotation also corresponds to the (spatial) position within the screen/video/content where the annotation is placed during the time period comprising the timestamp. In some implementations, the synchronization input includes: at least one timestamp of the plurality of timestamps is matched to at least one location in the content or document for the respective input. For example, the system 100 may perform a matching process to match annotation or marker inputs to locations in the video content and times associated with receiving annotation or marker inputs during recording of the video content.
In some implementations, the video content includes a transcript video stream in addition to the other plurality of video streams. The transcript video stream may include real-time transcribed audio data from the presenter video stream. The real-time transcribed audio may be generated as modifiable transcription data (e.g., text data) configured for display with the screen projected video stream during recording of the video content. That is, the transcript may be generated and rendered in real-time or near real-time as the presenter records and presents the content. In some implementations, the audio data from the real-time translation of the presenter video stream is generated as text data configured for display with the screen projected video stream and the transcribed audio data during recording of the video content. For example, the transcript may be rendered during recording and with other video streaming content from the screen projection. In some implementations, the system 100 can also utilize text data of the transcribed video stream to perform and render translations of the transcription. Thus, text (transcription) data may be rendered with or without translation.
In some implementations, transcription of the real-time transcribed audio data is performed by at least one speech-to-text application. The at least one speech-to-text application may be selected from any number of speech-to-text applications determined to be accessible by the transcribed video stream. For example, the system 100 can determine which speech-to-text application can provide accurate and convenient transcription of audio content. Such decisions may be made based on the audio content, the language of the audio content, demographic data provided by the user presenting or accessing the video stream, and so forth. The modifiable transcript data and text data may be stored in a metadata record according to a timestamp and may be configured to be searchable. This can facilitate searching for content within a video stream in an effective and resource efficient manner.
In some implementations, the presenter video stream, the screen projection video stream, and the annotation video stream are configured to switch between on and off during recording. Switching between on and off may trigger the display (or remove it from the display) of the respective presenter video stream, the respective screen projection video stream, or the respective annotation video stream.
Fig. 17 is a flowchart of an example process for generating and recording a video presentation in a real-time presentation system according to embodiments described herein. In general, the process 1700 utilizes the systems and algorithms described herein to generate metadata records for use by the real-time presentation system 100. Process 1700 may utilize one or more computing systems having at least one processing device and memory storing instructions that, when executed, cause the processing device(s) to perform the various operations and computer-implemented steps described in the claims. In general, system 100, system 200, system 263, and/or system 1900 may be used in the description and execution of process 1700.
The real-time online presentation system 100 may be a system that includes at least one camera, at least one microphone, at least one speaker, at least one display screen, and one or more user interfaces configured to be displayed on the at least one display screen. The system 100 may execute the instructions of the process 1700 using at least one processor and one or more computer-readable hardware storage devices having computer-executable instructions stored thereon that are executable by the at least one processor.
At block 1702, the process 1700 includes causing a recording to begin capturing audio content and video content. For example, a presenter may access the system 100 to trigger a presentation and/or recording to begin capturing audio content and video content being presented, which ultimately may generate the recordings 110, 110b and/or annotations 114. The video content may include at least a presenter video stream, a screen projection video stream, a transcription video stream, and an annotation video stream, as described throughout this disclosure. In some implementations, metadata records may be generated based on video content, as discussed with reference to fig. 16.
At block 1704, the process 1700 includes causing rendering of audio content and video content associated with accessing a plurality of applications from within a user interface. For example, during presentation and recording of audio and video content, the system 100 may trigger content sharing (e.g., screen sharing, video conference sharing, screen casting, etc.). Video data may be rendered via screens that provide various UIs, and audio content may be rendered via speakers. In some implementations, the audio content is also rendered as transcribed and/or translated text near or within a threshold distance of the remaining content presented by the system 100.
At block 1706, the process 1700 includes receiving annotation input in a user interface during rendering of audio content and video content. The annotation input may be recorded in an annotated video stream. For example, when a user annotates video content (e.g., annotations 306, 308 of fig. 3A), system 100 can record the annotations in separate streams, which can be represented as overlays that can be located on content from other video streams captured by system 100. In some implementations, the annotation input is caused to be rendered as an overlay on the video content. The annotation input may also be configured to move with the video content in response to detecting a window event or cursor event that triggers a switch to other video content (e.g., application, window, browser tab, etc.) accessed during recording. For example, a window event or other signal indicating scrolling of a window may be received, and the annotation input may be configured to scroll with the content of the underlying application such that the annotation remains at a fixed location relative to the application content of the underlying annotation.
At block 1708, process 1700 includes transcribing the audio content during rendering of the audio content and the video content. For example, the audio content is transcribed in real time. The transcribed audio content may be recorded in a transcribed video stream and may be rendered and tagged by the system 100 in real-time. For example, a presenter (or user viewing a presentation) may mark, annotate, modify, or otherwise interact with transcript data presented in a UI provided by the system 100.
At block 1710, the process 1700 optionally includes translating the audio content during rendering of the audio content and the video content. For example, the translation may be performed in real time. In addition to translating audio information that appears during the presentation, translation may also include translating text presented in screen shots (or other sharing mechanisms).
At block 1712, the process 1700 includes causing the transcribed audio content (and optionally the translated audio content) to be rendered in real-time in a user interface with the rendered audio content and video content. For example, the tutorial/presentation content, transcribed content, and optionally translated content can be depicted in a single UI such that the presenter and the user viewing the presentation have access to the presented video stream in one view. In some implementations, additional video streams are added to such views, such as presenter video streams, annotation video streams, participant video streams, and the like.
In some implementations, the process 1700 can also include causing the online presentation system 100 to generate summary content in response to detecting termination of rendering of the video content and the audio content. The summary content may be, for example, representative content 112, and the content 112 may be based on annotation input, video content, transcribed audio content, and translated audio content (i.e., the content 112 can include portions of video content selected or determined based on annotation input, transcribed audio content, etc.). Summary content may be generated based on the generated metadata record. In some implementations, the summary content includes portions of the rendered audio and video marked with annotation input.
Fig. 18 is a flowchart of an example process 1800 for presenting a video presentation in a real-time presentation system according to an embodiment described herein. In general, process 1800 utilizes the systems and algorithms described herein to generate metadata records for use by real-time presentation system 100. Process 1800 may utilize one or more computing systems having at least one processing device and memory storing instructions that, when executed, cause the processing device(s) to perform the various operations and computer-implemented steps described in the claims. In general, system 100, system 200, system 263, and/or system 1900 may be used in the description and execution of process 1800.
At step 1802, the process 1800 includes receiving at least one video stream. For example, a user may access the system 100 to view presentation content (e.g., video and audio content). The user may select a recording to view or may view the recording live using the system 100. In response to indicating which recording to view, system 100 may trigger system 202, for example, to receive one or more of the plurality of video streams. The video streams may include, but are not limited to, at least presenter video streams, screen projection video streams, transcription video streams, and annotation video streams, as described throughout this disclosure.
At step 1804, process 1800 includes receiving metadata representing timing information associated with an input detected in at least one video stream. For example, the system 100 may trigger the system 202 to receive metadata 228 representing timing information. The timing information may be configured to synchronize detected inputs provided in the at least one video stream with content (e.g., video, audio, data, metadata, etc.) of the at least one video stream. For example, the timing information may include information and/or instructions configured to synchronize the detected input (e.g., annotation, marker, etc.) with at least one of the plurality of video streams.
At step 1806, process 1800 includes generating a portion of at least one video stream based on the metadata. These portions may be generated in response to receiving a request to view any or all of the at least one video stream. For example, a user may request to view content associated with a video stream. In response, the system 100 may generate summary video, or other representative video (and/or audio) as a compilation or other combination of video stream portions based on the metadata.
In some implementations, the system 100 can generate and present the UI 302 with the annotations 306 and 308 retrieved from the metadata depicted as overlays onto the content shown in the UI 302. UI 302 may be depicted with annotations 306 and 308 overlaid onto content within UI 302 at timestamps indicated in metadata in response to detected user indications requesting viewing of assembled content (e.g., summarized content, and/or other representative content) associated with multiple video streams. The generated portions may include video and/or audio content representing annotated content, video content, or other user-requested and/or system 100-provided content. In some implementations, the generated portion includes content based on the detected input and includes rendered portions of the video stream annotated with the input.
In some implementations, in response to detecting a request to view assembled or otherwise consolidated content, the entire screen shot shown in FIG. 3A may be provided as an image frame because the frame includes the annotated content. The annotated content may be an indicator that the information in the image frame includes key data, as indicated by a presenter associated with the content of the at least one video stream.
At step 1808, process 1800 includes causing rendering of the portion of the at least one video stream in at least one user interface. For example, the UI generator 220 uses a renderer to format and display the portions indicated as assembled (e.g., summarized) content. Other portions of the video stream may also or alternatively be displayed in response to a request to view a compilation or other combination of content. For example, video and/or audio content may also be depicted, such as video and/or audio content associated with a presenter video stream, a translation video stream, a transcription video stream, another annotation video stream, and/or other video streams generated by the system 100.
In some implementations, the timing information corresponds to a plurality of time stamps associated with respective inputs detected in one or more of the video streams and at least one location in (i.e., in) content or a document associated with at least one of the one or more video streams. In some implementations, synchronizing the detected inputs includes: at least one timestamp is matched with at least one location in the document for the respective input.
In some implementations, the recorded video can be opened in a native application of the device (e.g., desktop, tablet, mobile device, wearable device, etc.). The native application may provide additional tools to allow the user to read a transcription of the video recording, navigate the video recording by selecting the transcription, skip/browse between key ideas, search within and across videos, and/or view key ideas across a series of videos (e.g., show me all "This will be on the test (which will be under test)" moments from preparing a presentation of an employee taking an examination). In some implementations, the recorded video and system 100 may be provided as an application extension rather than a native application.
In operation of the system 100, the presenter may be provided with options to mark key ideas, draw on records in real-time, and store such annotations and records online as any number of separate video streams in order to facilitate the generation of content 112 for the records. At the end of the recording, the presenter can review the recording and upload the recording to an online drive for sharing with one or more applications and/or directly with the user. The system 100 enables a presenter to create a bystander screen projection for later viewing, asynchronously recording and sharing presentations and related content, performing face-to-face presentations, and preparing for remote presentations via video conferencing software and related applications.
The systems and methods described herein may provide a screen sharing scope selection tool (e.g., presentation system 100). The tools of system 100 may provide the user with an option to select a presentation mode (e.g., an extended display or mirrored display mode, etc.) while connected to an external display (e.g., television or projector hardware) that also includes access to the presenter toolbar. The presenter toolbar may include a projection destination tool, a screen sharing panel, a record screen sharing tool, a stop screen sharing tool, a video annotation tool, a laser pointer tool, a closed caption tool, a camera tool, a marking tool, and any number of annotation tools (e.g., pen, highlighter, shape, etc.). The video annotation tool may enable a user to annotate video anywhere on the screen. Instead, the presenter toolbar is bypassed and the annotation is made directly using a stylus. The closed caption tool option provides live captioning and translation on the device over the highlighted text, for example, using input from a microphone associated with the system 100. The translated language may be selected by a user and may be provided in a text format. In some examples, the translated text may be synthesized and output to a user as audio data.
When the user selects the record option from the presenter toolbar or screen sharing panel, the current screen sharing scope is enabled and the tool confirms to the user whether to record and upload to the cloud server. When a record is triggered via the screen capture tool, the toolbar may provide the first user with an option to move to the screen sharing scope selection tool to clip and publish the record. The marking option (i.e., the asterisk option in toolbar 400) may enable a user to mark important/critical ideas presented on the screen and may display indicator text to confirm the marking.
The toolbar may automatically transcribe captured records and may highlight text for the user to check for accuracy, and may require the user to provide titles of key ideas before uploading to the repository to share records with the system 100 user.
The system 100 may allow another user to search for transcripts, navigate using transcripts and/or key ideas, or view a summary (e.g., summary, representative portion) video of all key ideas on a predetermined time basis (e.g., daily, weekly, monthly, quarterly, yearly, etc.) as the key ideas are organized by date and subject matter via a search bar provided when the user accesses the recording. The system 100 may highlight the current sentence in the transcript (being read) and may enable the user to edit the title, transcribe, and mark the paragraph key ideas. When the user's query matches the recorded key ideas, the system may display the recorded clip as a search result or a quick answer in the browser.
In some embodiments, system 100 may provide a side-by-side reading aid UI. For example, the system 100 may utilize side-by-side electronic books to provide reference assistance to preserve context for reading and reference content while reading. The user can select any text from within the system 100 to upload the text. The system 100 can use the uploaded text to actively suggest helpful learning moments. For example, like vocabulary style related content, the system 100 may provide key concepts to present articles and videos about those concepts. In some implementations, the system 100 can adjust the particular textA level. For example, the system 100 may replace particularly advanced words in text with simpler terms to customize content for users with, for example, smaller vocabulary. In some implementations, the system 100 can replace particular content with less advanced content to assist the reader in understanding paragraphs of content. The system 100 may then switch to the original content to provide further understanding of vocabulary usage in the text.
In some implementations, the system 100 can also provide context learning moments. For example, the system 100 may build-in paragraph translations for users having a first learning language that is different from the language of the text. The system 100 may also provide quick links for vocabulary searches and/or answer searches.
In some implementations, the system 100 may provide access to accessibility functionality, such as reading through speed, tone, and accent adjustments. In some implementations, the system 100 may provide fonts to assist readers with reading paragraphs that are difficult to read, and may also highlight sentences and/or words that are being audibly read by the system 100. The system 100 may perform other highlighting, annotating, and compositing of data to assist a user in learning presented concepts.
Fig. 19 illustrates an example of a computer device 1900 and a mobile computer device 1950 that can be used with the techniques described herein. Computing device 1900 is intended to represent various forms of digital computers, such as laptops, desktops, tablets, workstations, personal digital assistants, smart devices, appliances, electronic sensor-based devices, televisions, servers, blade servers, mainframes, and other appropriate computing devices. Computing device 1950 is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smartphones, and other similar computing devices. The components shown herein, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the invention described and/or claimed in this document.
The computing device 1900 includes a processor 1902, a memory 1904, a storage device 1906, a high-speed interface 1908 coupled to the memory 1904 and to the high-speed expansion port 1910, and a low-speed interface 1912 coupled to the low-speed bus 1914 and to the storage device 1906. The processor 1902 can be a semiconductor-based processor. The memory 1904 can be a semiconductor-based memory. Each of the components 1902, 1904, 1906, 1908, 1910, and 1912 are interconnected using various buses, and may be mounted on a common motherboard or in other manners as appropriate. The processor 1902 is capable of processing instructions for execution within the computing device 1900, including instructions stored in the memory 1904 or on the storage device 1906, to display graphical information for a GUI on an external input/output device, such as the display 1916 coupled to the high-speed interface 1908. In other embodiments, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and memory types. In addition, multiple computing devices 1900 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multiprocessor system).
Memory 1904 stores information within computing device 1900. In one implementation, the memory 1904 is one or more volatile memory units. In another implementation, the memory 1904 is one or more non-volatile memory units. Memory 1904 may also be another form of computer-readable medium, such as a magnetic or optical disk. Generally, a computer readable medium may be a non-transitory computer readable medium.
The storage device 1906 is capable of providing mass storage for the computing device 1900. In one implementation, storage device 1906 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. The computer program product can be tangibly embodied in an information carrier. The computer program product may also contain instructions that, when executed, perform one or more methods and/or computer-implemented methods, such as those described above. The information carrier is a computer-or machine-readable medium, such as the memory 1904, storage device 1906, or memory on processor 1902.
The high speed controller 1908 manages bandwidth-intensive operations for the computing device 1900, while the low speed controller 1912 manages lower bandwidth-intensive operations. Such allocation of functions is merely exemplary. In one embodiment, high-speed controller 1908 is coupled to memory 1904, display 1916 (e.g., via a graphics processor or accelerator), and high-speed expansion port 1910, which high-speed expansion port 1910 may accept various expansion cards (not shown). In this embodiment, a low speed controller 1912 is coupled to storage device 1906 and low speed expansion port 1914. The low-speed expansion port, which may include various communication ports (e.g., USB, bluetooth, ethernet, wireless ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a network device, such as a switch or router, for example, through a network adapter.
Computing device 1900 may be implemented in a number of different forms, as shown. For example, it may be implemented as a standard server 1920, or multiple times in a group of such servers. It may also be implemented as part of a rack server system 1924. In addition, it may be implemented in a computer such as a laptop 1922. Alternatively, components from computing device 1900 may be combined with other components in a mobile device (not shown), such as device 1950. Each of such devices may contain one or more of the computing devices 1900, 1950, and the entire system may be made up of multiple computing devices 1900, 1950 communicating with each other.
The computing device 1950 includes a processor 1952, memory 1964, input/output devices such as a display 1954, a communication interface 1966, and a transceiver 1968, among other components. The device 1950 may also be provided with a storage device, such as a microdrive or other device, to provide additional storage. Each of the components 1950, 1952, 1964, 1954, 1966, and 1968 are interconnected using various buses, and several of the components may be mounted on a common motherboard or in other manners as appropriate.
Processor 1952 may execute instructions within computing device 1950, including instructions stored in memory 1964. The processor may be implemented as a chipset of chips that include separate multiple analog and digital processors. The processor may provide, for example, for coordination of the other components of the device 1950, such as control of user interfaces, applications run by device 1950, and wireless communication of device 1950.
The processor 1952 may communicate with a user through a control interface 1958 and a display interface 1956 coupled to a display 1954. The display 1954 may be, for example, a TFT LCD (thin film transistor liquid crystal display) or OLED (organic light emitting diode) display or other suitable display technology. Display interface 1956 may include appropriate circuitry for driving display 1954 to present graphical and other information to a user. The control interface 1958 may receive commands from a user and convert them for submission to the processor 1952. In addition, an external interface 1962 may be provided in communication with the processor 1952 to enable near area communication of the device 1950 with other devices. The external interface 1962 may provide, for example, for wired communication in some embodiments, or for wireless communication in other embodiments, and multiple interfaces may also be used.
Memory 1964 stores information within computing device 1950. Memory 1964 can be implemented as one or more of one or more computer-readable media, one or more volatile memory units, or one or more non-volatile memory units. Expansion memory 1974 may also be provided and connected to device 1950 through expansion interface 1972, which expansion interface 1972 may include, for example, a SIMM (single in-line memory module) card interface. Such expansion memory 1974 may provide additional storage space for device 1950 or may also store applications or other information for device 1950. Specifically, expansion memory 1974 may include instructions for performing or supplementing the processes described above, and may also include secure information. Thus, for example, expansion memory 1974 may be provided as a security module for device 1950 and may be programmed with instructions to permit secure use of device 1950. In addition, secure applications may be provided via the SIMM cards, as well as additional information, such as placing identifying information on the SIMM cards in an indestructible manner.
The memory may include, for example, flash memory and/or NVRAM memory, as discussed below. In one implementation, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer-or machine-readable medium, such as the memory 1964, expansion memory 1974, or memory on processor 1952, which may be received, for example, over transceiver 1968 or external interface 1962.
The device 1950 may communicate wirelessly through a communication interface 1966, which communication interface 1966 may include digital signal processing circuitry as necessary. Communication interface 1966 may provide for communication under various modes or protocols, such as GSM voice calls, SMS, EMS, or MMS messaging, CDMA, TDMA, PDC, WCDMA, CDMA2000, or GPRS, among others. Such communication may occur, for example, through radio frequency transceiver 1968. In addition, short-range communications may occur, such as using Bluetooth, wi-Fi, or other such transceivers (not shown). In addition, a GPS (Global positioning System) receiver module 1970 may provide additional navigation-and location-related wireless data to the device 1950, which may be suitably used by applications running on the device 1950.
The device 1950 may also communicate audibly using an audio codec 1960, and the audio codec 1960 may receive voice information from a user and convert it to usable digital information. The audio codec 1960 may likewise generate audible sound for a user, such as through a speaker in a handset of the device 1950. Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on device 1950.
Computing device 1950 may be implemented in a number of different forms, as shown. For example, it may be implemented as a cellular phone 1980. It may also be implemented as part of a smart phone 1982, personal digital assistant, or other similar mobile device.
Various implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various embodiments can include embodiments in one or more computer programs executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
These computer programs (also known as modules, programs, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms "machine-readable medium," computer-readable medium "and/or" computer program product, apparatus and/or device (e.g., magnetic discs, optical disks, memory, programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term "machine-readable signal" refers to any signal used to provide machine instructions and/or data to a programmable processor.
To provide for interaction with a user, the systems and techniques described herein can be implemented on a computer having: a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, or LED (light emitting diode)) for displaying information to a user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other kinds of devices can also be used to provide for interaction with a user. For example, feedback provided to the user can be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback), and can receive input from the user in any form, including acoustic, speech, or tactile input.
The systems and techniques described herein can be implemented in such a computing system: including a back-end component (e.g., as a data server), or including a middleware component (e.g., an application server), or including a front-end component (e.g., a client computer having a graphical user interface or a web browser through which a user can interact with an implementation of the systems and techniques described here), or any combination of such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication (e.g., a communication network). Examples of communication networks include a local area network ("LAN"), a wide area network ("WAN"), and the Internet.
The computing system can include clients and servers. The client and server are typically remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
In some embodiments, the computing device depicted in fig. 19 can include a sensor that interfaces with a virtual reality or headset (VR headset/AR headset/HMD device 1990). For example, one or more sensors included on the computing device 1950 or other computing device depicted in fig. 19 can provide input to the AR/VR headset 1990, or generally to the AR/VR space. The sensors can include, but are not limited to, touch screens, accelerometers, gyroscopes, pressure sensors, biometric sensors, temperature sensors, humidity sensors, and ambient light sensors. The computing device 1950 can use the sensors to determine an absolute position of the computing device in the AR/VR space and/or a detected rotation, which can then be used as input to the AR/VR space. For example, the computing device 1950 may be incorporated into the AR/VR space as a virtual object such as a controller, laser pointer, keyboard, tool, etc. Positioning of the computing device/virtual object by the user when incorporated into the AR/VR space can allow the user to position the computing device to view the virtual object in the AR/VR space in some manner.
In some embodiments, one or more input devices included on computing device 1950 or connected to computing device 1950 can be used as input to the AR/VR space. The input device can include, but is not limited to, a touch screen, a keyboard, one or more buttons, a track pad, a touch pad, a pointing device, a mouse, a track ball, a joystick, a camera, a microphone, an in-ear earphone or ear bud with input capabilities, a game controller, or other connectable input devices. When the computing device 1950 is incorporated in the AR/VR space, a user interacting with input devices included on the computing device 1950 can cause certain actions to occur in the AR/VR space.
In some embodiments, one or more output devices included on the computing device 1950 can provide output and/or feedback to a user of the AR/VR headset 1990 in the AR/VR space. The output and feedback can be visual, tactile or audio. The output and/or feedback can include, but is not limited to, rendering an AR/VR space or virtual environment, vibrating, turning on and off one or more lights or flashing lights flashing and/or flashing, alerting, playing a ring tone, playing a song, and playing an audio file. The output devices can include, but are not limited to, vibration motors, vibration coils, piezoelectric devices, electrostatic devices, light Emitting Diodes (LEDs), flashlights, and speakers.
In some embodiments, the computing device 1950 can be placed within the AR/VR headset 1990 to create an AR/VR system. The AR/VR headset 1990 can include one or more positioning elements that allow a computing device 1950, such as a smart phone 1982, to be placed in place within the AR/VR headset 1990. In such an embodiment, the display of smart phone 1982 is capable of rendering stereoscopic images representing an AR/VR space or virtual environment.
In some embodiments, computing device 1950 may appear as another object in the computer-generated 3D environment. Interactions with the computing device 1950 by a user (e.g., rotating, shaking, touching the touch screen, sliding a finger across the touch screen) can be interpreted as interactions with objects in the AR/VR space. As an example, the computing device can be a laser pointer. In such examples, computing device 1950 appears as a virtual laser pointer in a computer-generated 3D environment. As the user manipulates the computing device 1950, the user in the AR/VR space sees the movement of the laser pointer. The user receives feedback from interactions with the computing device 1950 in an AR/VR environment, either on the computing device 1950 or on the AR/VR headset 1990.
In some embodiments, computing device 1950 may include a touch screen. For example, a user can interact with the touch screen and also interact with the touch screen in a particular manner that can mimic what happens on the touch screen with what happens in the AR/VR space. For example, a user may use a pinch-type motion to zoom in on content displayed on a touch screen. Such pinch-type movements on the touch screen can enable information provided in the AR/VR space to be scaled. In another example, the computing device may be rendered as a virtual book in a computer-generated 3D environment. In the AR/VR space, pages of the book can be displayed in the AR/VR space, and sliding of the user's finger across the touch screen can be interpreted as flipping/flipping pages of the virtual book. As each page is flipped/flipped, the user may be provided with audio feedback, such as the sound of flipping a page in the book, in addition to seeing the page content changes.
In some embodiments, one or more input devices can be rendered in addition to a computing device (e.g., mouse, keyboard) in a computer-generated 3D environment. The rendered input device (e.g., rendered mouse, rendered keyboard) can be used as rendered in the AR/VR space to control objects in the AR/VR space.
A number of embodiments have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the invention.
Furthermore, the logic flows depicted in the figures do not require the particular order shown, or sequential order, to achieve desirable results. In addition, other steps may be provided, or steps may be eliminated, from the described flows, and other components may be added to, or removed from, the described systems. Accordingly, other embodiments are within the scope of the following claims.
In addition to the above description, the user is provided with a control that allows the user to make selections regarding: whether and when a system, program, device, network, or feature described herein may enable collection of user information (e.g., information about a user's social network, social behavior or activity, profession, user's preferences, or user's current location), and whether to send content or communications from a server to the user. In addition, some data may be processed in one or more ways before it is stored or used so that user information is removed. For example, the identity of a user may be processed such that user information cannot be determined for the user, or the geographic location of the user may be generalized (e.g., to a city, zip code, or state level) where location information is obtained such that a particular location of the user cannot be determined. Thus, the user can control what information is collected about the user, how that information is used, and what information is provided to the user.
The computer system (e.g., computing device) may be configured to communicate wirelessly with the network server over the network via a communication link established with the network server using any known wireless communication technology and protocol, including Radio Frequency (RF), microwave frequency (MWF), and/or infrared frequency (IRF) wireless communication technologies and protocols suitable for communicating over the network.
In accordance with aspects of the present disclosure, implementations of the various techniques described herein may be implemented in digital electronic circuitry, or in computer hardware, firmware, software, or in combinations of them. Embodiments may be implemented as a computer program product (e.g., a computer program tangibly embodied in an information carrier, a machine-readable storage device, a computer-readable medium, a tangible computer-readable medium) for processing by or for controlling the operation of data processing apparatus (e.g., a programmable processor, a computer, or multiple computers). In some implementations, a tangible computer-readable storage medium may be configured to store instructions that, when executed, cause a processor to perform a process. A computer program, such as the computer program(s) described above, can be written in any form of programming language, including compiled or interpreted languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A computer program can be deployed to be processed on one computer or on one site or on multiple computers that are distributed across multiple sites and interconnected by a communication network.
Specific structural and functional details disclosed herein are merely representative for purposes of describing example embodiments. However, the example embodiments may be embodied in many alternate forms and should not be construed as limited to only the embodiments set forth herein.
The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of embodiments. As used herein, the singular forms "a", "an" and "the" are intended to include the plural forms as well, unless the context clearly indicates otherwise. It will be further understood that the terms "comprises," "comprising," "includes" and/or "including," when used in this specification, specify the presence of stated features, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, steps, operations, elements, components, and/or groups thereof.
It will be understood that when an element is referred to as being "coupled," connected "or" responsive "to" another element, it can be directly coupled, connected or responsive to the other element or "on" the other element or intervening elements may also be present. In contrast, when an element is referred to as being "directly coupled," directly connected "or" directly responsive "to" or "directly on" another element, there are no intervening elements present. As used herein, the term "and/or" includes any and all combinations of one or more of the associated listed items.
Spatially relative terms, such as "below … …," "below … …," "lower," "above … …," "upper," and the like, may be used herein for ease of description to describe one element or feature's relationship to other elements or features as illustrated in the figures. It will be understood that spatially relative terms are intended to encompass different orientations of the device in use or operation in addition to the orientation depicted in the figures. For example, if the device in the figures is turned over, elements described as "below" or "beneath" other elements or features would then be oriented "above" the other elements or features. Thus, the term "under … …" can encompass both an orientation of above and below. The device may be otherwise oriented (rotated 70 degrees or at other orientations) and the spatially relative descriptors used herein interpreted accordingly.
Example embodiments of the concepts are described herein with reference to cross-sectional illustrations that are schematic illustrations of idealized embodiments (and intermediate structures) of the example embodiments. Thus, variations from the shapes of the illustrations as a result, for example, of manufacturing techniques and/or tolerances, are to be expected. Thus, example embodiments of the described concepts should not be construed as limited to the particular shapes of regions illustrated herein but are to include deviations in shapes that result, for example, from manufacturing. Thus, the regions illustrated in the figures are schematic in nature and their shapes are not intended to illustrate the actual shape of a region of a device and are not intended to limit the scope of example embodiments.
It will be understood that, although the terms "first," "second," etc. may be used herein to describe various elements, these elements should not be limited by these terms. These terms are only used to distinguish one element from another element. Thus, a "first" element may be termed a "second" element without departing from the teachings of the present embodiment.
Unless otherwise defined, terms (including technical and scientific terms) used herein have the same meaning as commonly understood by one of ordinary skill in the art to which these concepts belong. It will be further understood that terms, such as those defined in commonly used dictionaries, should be interpreted as having a meaning that is consistent with their meaning in the context of the relevant art and/or the present specification and will not be interpreted in an idealized or overly formal sense unless expressly so defined herein.
While certain features of the described embodiments have been illustrated as described herein, many modifications, substitutions, changes, and equivalents will now occur to those skilled in the art. It is, therefore, to be understood that the appended claims are intended to cover such modifications and changes as fall within the scope of the embodiments. It is to be understood that they have been presented by way of example only, and not limitation, and various changes in form and details may be made. Any portion of the apparatus and/or methods described herein may be combined in any combination other than mutually exclusive combinations. The embodiments described herein can include various combinations and/or sub-combinations of the functions, components, and/or features of the different embodiments described.
Claims (29)
1. A computer-implemented method, comprising:
causing the recording to begin capturing video content, the video content including a presenter video stream, a screen projection video stream, and an annotation video stream; and
a metadata record is generated based on the video content and during capture of the video content, the metadata record representing timing information for synchronizing at least one portion of the video content with input received in at least one of the presenter video stream, the screen cast video stream, or the annotation video stream.
2. The computer-implemented method of claim 1, further comprising:
in response to termination of the recording, a representation of the video content is generated based on the metadata recording, the representation including portions of the video content annotated by a user associated with the presenter video stream.
3. The computer-implemented method of claim 1 or 2, wherein:
the timing information corresponds to a plurality of time stamps associated with the input and at least one location in a document associated with the video content; and
synchronizing the input includes matching at least one timestamp of the plurality of timestamps with the at least one location in the document.
4. The computer-implemented method of any of claims 1-3, wherein the video content further comprises a transcript video stream comprising:
real-time transcribed audio data from the presenter video stream, the real-time transcribed audio data being generated as modifiable transcription data configured for display with the screen projected video stream during the recording of the video content; and
real-time translated audio data from the presenter video stream, the real-time translated audio data being generated as text data configured for display with the screen projected video stream and the real-time transcribed audio data during the recording of the video content.
5. The computer-implemented method of claim 4, wherein:
transcription of the real-time transcribed audio data is performed by at least one speech-to-text application selected from a plurality of speech-to-text applications determined to be accessible by the transcribed video stream; and
the modifiable transcript data and the text data are stored in the metadata record according to a timestamp and are configured to be searchable.
6. The computer-implemented method of any of claims 1 to 5, wherein the input comprises an annotation input associated with the annotated video stream, the annotation input comprising video marker data and video annotator data generated by a user associated with the presenter video stream.
7. The computer-implemented method of any of claims 1-6, wherein the presenter video stream, the screen cast video stream, and the annotation video stream are configured to switch between on and off during the recording, the switching between on and off triggering display of the presenter video stream, the screen cast video stream, or the annotation video stream or removal of the presenter video stream, the screen cast video stream, or the annotation video stream from display.
8. A system, comprising:
a memory; and
at least one processor coupled to the memory, the at least one processor configured to generate a user interface configured to receive commands from:
a renderer configured to render audio and video content associated with accessing a plurality of applications from within the user interface;
An annotation generator tool configured to receive annotation input in the user interface and generate a plurality of annotation data records for the received annotation input during rendering of the audio and video content, the annotation generator tool comprising at least one control for receiving the annotation input;
a transcription generator tool configured to transcribe the audio content during the rendering of the audio and video content and display the transcribed audio content in the user interface; and
a content generator tool configured to generate a representation of the audio and video content in response to detecting termination of the rendering, the representation being based on the annotation input, the video content, and the transcribed audio content, wherein the representation includes portions of the rendered audio and video content marked with the annotation input.
9. The system of claim 8, wherein the content generator tool is further configured to:
generating URL links to the representations of the audio and video content; and
The representations are indexed to implement a search function for finding at least a portion of the audio and video content in a web browser application.
10. The system of claim 8 or 9, wherein the plurality of annotation data records comprises:
at least one application of the plurality of applications receives an indication of the annotation input; and
machine-readable instructions for overlaying the annotation input onto at least one image frame depicting a portion of the rendered video content of the indicated at least one application according to the respective time stamp.
11. The system of claim 10, wherein overlaying the annotation input onto the at least one image frame comprises:
retrieving at least one annotation data record of the plurality of annotation data records,
executing the machine-readable instructions; and
a document is generated that enables a user to scroll through the at least one image frame, wherein the annotation input is overlaid onto the at least one image frame according to the at least one annotation data record.
12. The system of any of claims 8 to 11, wherein the annotation generator tool is further configured to:
Causing a recording of the rendered audio and video content to begin, the rendered video content including data associated with a first application of the plurality of applications and data associated with a second application of the plurality of applications;
in the first application, receiving a first set of annotations during a first segment of recorded video content;
storing the first set of annotations according to respective timestamps associated with the first segment;
in the second application, receiving a second set of annotations during a second segment of recorded video content;
storing the second set of annotations according to respective timestamps associated with the second segment;
in response to detecting that cursor focus has been switched from the first application to the second application,
retrieving the second set of annotations and data associated with the second application;
matching the timestamp associated with the second segment to the second set of annotations; and
causing the retrieved second set of annotations to be displayed on the second application in accordance with the respective timestamp associated with the second segment.
13. The system of claim 12, wherein the first and second sets of annotations are generated by the annotation tool, the annotation tool enabling marking, storing, and scrolling of the first and second sets of annotations while preserving, for each annotation in the first and second sets of annotations, an initial location on data associated with the first application or data associated with the second application.
14. The system of claim 12, wherein the annotation generator tool is further configured to:
in response to detecting that the cursor focus has been switched from the second application to the first application,
retrieving the first set of annotations and data associated with the first application;
matching the timestamp associated with the first segment to the first set of annotations; and
causing the retrieved first set of annotations to be displayed on the first application in accordance with the respective timestamp associated with the first segment.
15. The system of claim 12, wherein the annotation generator tool is further configured to:
receiving additional annotations in the second application, the additional annotations being associated with respective timestamps; and
in response to detecting completion of the recording, generating a document from the second set of annotations and the additional annotation, the document comprising:
the second set of annotations and the additional annotations overlaid onto data associated with the second application according to respective time stamps associated with the second segment and respective time stamps associated with the additional annotations; and
Transcription of the recorded audio content associated with the second segment.
16. A non-transitory computer-readable storage medium comprising instructions stored thereon that, when executed by at least one processor, are configured to cause a computing system to perform instructions comprising:
causing the recording to begin capturing video content including a presenter video stream, a screen projection video stream, a transcription video stream, and an annotation video stream; and
a metadata record is generated based on the video content and during capture of the video content, the metadata record representing timing information for synchronizing at least one portion of the video content with input received in at least one of the presenter video stream, the screen projection video stream, the transcription video stream, or the annotation video stream.
17. The non-transitory computer-readable storage medium of claim 16, wherein the instructions further comprise:
in response to termination of the recording, a representation of the video content is generated based on the metadata recording, the representation including portions of the video content annotated by a user associated with the presenter video stream.
18. The non-transitory computer readable storage medium of claim 16 or 17, wherein:
the timing information corresponds to a plurality of time stamps associated with the received input and at least one location in a document associated with the video content; and
synchronizing the input includes matching at least one timestamp of the plurality of timestamps with the at least one location in the document.
19. The non-transitory computer readable storage medium of any of claims 16 to 18, wherein the transcript video stream comprises:
real-time transcribed audio data from the presenter video stream, the real-time transcribed audio data being generated as text data configured for display with the screen projected video stream during the recording of the video content; and
real-time translated audio data from the presenter video stream, the real-time translated audio data being generated as text data configured for display with the screen projected video stream and the transcribed audio data during the recording of the video content.
20. The non-transitory computer-readable storage medium of claim 19, wherein:
the real-time transcribed audio data is generated as modifiable transcription data configured for display with the screen projected video stream during the recording of the video content;
transcription of the real-time transcribed audio data is performed by at least one speech-to-text application selected from a plurality of speech-to-text applications determined to be accessible by the transcribed video stream; and
the modifiable transcript data and the text data are stored in a metadata record according to a timestamp and are configured to be searchable.
21. The non-transitory computer-readable storage medium of any of claims 16-20, wherein the input comprises an annotation input associated with the annotated video stream, the annotation input comprising video marker data and video annotator data generated by a user associated with the presenter video stream.
22. The non-transitory computer-readable storage medium of any one of claims 16 to 21, wherein the presenter video stream, the screen cast video stream, the transcript video stream, and the annotation video stream are configured to switch between on and off during the recording, the switching between on and off triggering a display of or removing from the presenter video stream, the screen cast video stream, the transcript video stream, or the annotation video stream.
23. A non-transitory computer-readable storage medium comprising instructions stored thereon that, when executed by at least one processor, are configured to cause a computing system to perform instructions comprising:
causing the recording to begin capturing audio content and video content, the video content including at least a presenter video stream, a screen projection video stream, a transcription video stream, and an annotation video stream;
causing rendering of the audio content and the video content associated with accessing a plurality of applications from within a user interface;
receiving annotation input in the user interface during the rendering of the audio content and the video content, the annotation input being recorded in the annotated video stream;
transcribing the audio content during the rendering of the audio content and the video content, the transcribed audio content being recorded in the transcribed video stream;
translating the transcribed audio content during the rendering of the audio content and the video content; and
such that the transcribed audio content and a translation of the transcribed audio content are rendered in the user interface with the rendered audio content and video content.
24. The non-transitory computer-readable medium of claim 23, wherein the instructions further comprise:
in response to detecting termination of the rendering of the video content and the audio content, generating content representative of at least a portion of the audio content and the video content, the representative content being based on the annotation input, the video content, the transcribed audio content, and the translated audio content, wherein the representative content includes portions of the rendered audio and video marked with the annotation input.
25. The non-transitory computer-readable medium of claim 23 or 24, wherein the annotation input is caused to be rendered as an overlay on the video content, the annotation input configured to move with the video content in response to detecting a window event or cursor event that triggers a switch to other video content accessed during the recording.
26. A computer-implemented method, comprising:
receiving at least one video stream;
receiving metadata representing timing information associated with detected inputs in the at least one video stream, the timing information configured to synchronize detected inputs provided in the at least one video stream with content depicted in the at least one video stream;
Generating a portion of the at least one video stream in response to receiving a request to view the at least one video stream, the generating being based on the metadata and a detected user indication requesting to view a representation of the at least one video stream; and
such that the portion of the at least one video stream is rendered.
27. The computer-implemented method of claim 26, wherein the timing information corresponds to at least one location in a plurality of timestamps associated with respective inputs detected in the at least one video stream and content associated with the at least one video stream; and
synchronizing the detected inputs includes: for a respective input, at least one timestamp is matched with the at least one location in a document associated with the at least one video stream.
28. The computer-implemented method of claim 26 or 27, wherein the at least one video stream is selected from a presenter video stream, a screen projection video stream, a transcription video stream, and an annotation video stream.
29. The computer-implemented method of any of claims 26 to 28, wherein the representation of the at least one video stream is based on a detected input and includes a rendered portion of the at least one video stream annotated with the input.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/303,075 US20220374585A1 (en) | 2021-05-19 | 2021-05-19 | User interfaces and tools for facilitating interactions with video content |
US17/303,075 | 2021-05-19 | ||
PCT/US2022/072434 WO2022246450A1 (en) | 2021-05-19 | 2022-05-19 | User interfaces and tools for facilitating interactions with video content |
Publications (1)
Publication Number | Publication Date |
---|---|
CN116888668A true CN116888668A (en) | 2023-10-13 |
Family
ID=82320057
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202280017301.8A Pending CN116888668A (en) | 2021-05-19 | 2022-05-19 | User interface and tools for facilitating interactions with video content |
Country Status (5)
Country | Link |
---|---|
US (1) | US20220374585A1 (en) |
EP (1) | EP4272211A1 (en) |
KR (1) | KR20230172004A (en) |
CN (1) | CN116888668A (en) |
WO (1) | WO2022246450A1 (en) |
Families Citing this family (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN113448475A (en) * | 2021-06-30 | 2021-09-28 | 广州博冠信息科技有限公司 | Interaction control method and device for virtual live broadcast room, storage medium and electronic equipment |
US11968476B2 (en) * | 2021-10-31 | 2024-04-23 | Zoom Video Communications, Inc. | Virtual environment streaming to a video communications platform |
US11880644B1 (en) * | 2021-11-12 | 2024-01-23 | Grammarly, Inc. | Inferred event detection and text processing using transparent windows |
US11854267B2 (en) * | 2021-12-09 | 2023-12-26 | Motorola Solutions, Inc. | System and method for witness report assistant |
US20230244857A1 (en) * | 2022-01-31 | 2023-08-03 | Slack Technologies, Llc | Communication platform interactive transcripts |
Family Cites Families (44)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US4972274A (en) * | 1988-03-04 | 1990-11-20 | Chyron Corporation | Synchronizing video edits with film edits |
US7689898B2 (en) * | 1998-05-07 | 2010-03-30 | Astute Technology, Llc | Enhanced capture, management and distribution of live presentations |
US6430357B1 (en) * | 1998-09-22 | 2002-08-06 | Ati International Srl | Text data extraction system for interleaved video data streams |
US7330875B1 (en) * | 1999-06-15 | 2008-02-12 | Microsoft Corporation | System and method for recording a presentation for on-demand viewing over a computer network |
US7216266B2 (en) * | 2003-03-12 | 2007-05-08 | Thomson Licensing | Change request form annotation |
FR2858087B1 (en) * | 2003-07-25 | 2006-01-21 | Eastman Kodak Co | METHOD FOR DIGITIGLY SIMULATING AN IMAGE SUPPORT RENDER |
WO2007052285A2 (en) * | 2005-07-22 | 2007-05-10 | Yogesh Chunilal Rathod | Universal knowledge management and desktop search system |
US8437409B2 (en) * | 2006-12-06 | 2013-05-07 | Carnagie Mellon University | System and method for capturing, editing, searching, and delivering multi-media content |
US9665529B1 (en) * | 2007-03-29 | 2017-05-30 | Amazon Technologies, Inc. | Relative progress and event indicators |
US10872322B2 (en) * | 2008-03-21 | 2020-12-22 | Dressbot, Inc. | System and method for collaborative shopping, business and entertainment |
US9330069B2 (en) * | 2009-10-14 | 2016-05-03 | Chi Fai Ho | Layout of E-book content in screens of varying sizes |
US9508387B2 (en) * | 2009-12-31 | 2016-11-29 | Flick Intelligence, LLC | Flick intel annotation methods and systems |
WO2021220058A1 (en) * | 2020-05-01 | 2021-11-04 | Monday.com Ltd. | Digital processing systems and methods for enhanced collaborative workflow and networking systems, methods, and devices |
US8903798B2 (en) * | 2010-05-28 | 2014-12-02 | Microsoft Corporation | Real-time annotation and enrichment of captured video |
US9183560B2 (en) * | 2010-05-28 | 2015-11-10 | Daniel H. Abelow | Reality alternate |
US20120236201A1 (en) * | 2011-01-27 | 2012-09-20 | In The Telling, Inc. | Digital asset management, authoring, and presentation techniques |
US20160148517A1 (en) * | 2011-04-11 | 2016-05-26 | Ali Mohammad Bujsaim | Talking notebook with projection |
US20150003595A1 (en) * | 2011-04-25 | 2015-01-01 | Transparency Sciences, Llc | System, Method and Computer Program Product for a Universal Call Capture Device |
US20130110565A1 (en) * | 2011-04-25 | 2013-05-02 | Transparency Sciences, Llc | System, Method and Computer Program Product for Distributed User Activity Management |
WO2012150602A1 (en) * | 2011-05-03 | 2012-11-08 | Yogesh Chunilal Rathod | A system and method for dynamically monitoring, recording, processing, attaching dynamic, contextual & accessible active links & presenting of physical or digital activities, actions, locations, logs, life stream, behavior & status |
US8798598B2 (en) * | 2012-09-13 | 2014-08-05 | Alain Rossmann | Method and system for screencasting Smartphone video game software to online social networks |
US20140222462A1 (en) * | 2013-02-07 | 2014-08-07 | Ian Shakil | System and Method for Augmenting Healthcare Provider Performance |
US9268756B2 (en) * | 2013-04-23 | 2016-02-23 | International Business Machines Corporation | Display of user comments to timed presentation |
EP3448006B1 (en) * | 2013-07-02 | 2023-03-15 | Family Systems, Limited | System for improving audio conferencing services |
US10891428B2 (en) * | 2013-07-25 | 2021-01-12 | Autodesk, Inc. | Adapting video annotations to playback speed |
US20150234571A1 (en) * | 2014-02-17 | 2015-08-20 | Microsoft Corporation | Re-performing demonstrations during live presentations |
US10033825B2 (en) * | 2014-02-21 | 2018-07-24 | Knowledgevision Systems Incorporated | Slice-and-stitch approach to editing media (video or audio) for multimedia online presentations |
US10431259B2 (en) * | 2014-04-23 | 2019-10-01 | Sony Corporation | Systems and methods for reviewing video content |
US9924240B2 (en) * | 2015-05-01 | 2018-03-20 | Google Llc | Systems and methods for interactive video generation and rendering |
US11036458B2 (en) * | 2015-10-14 | 2021-06-15 | Google Llc | User interface for screencast applications |
US9812175B2 (en) * | 2016-02-04 | 2017-11-07 | Gopro, Inc. | Systems and methods for annotating a video |
CN108323239B (en) * | 2016-11-29 | 2020-04-28 | 华为技术有限公司 | Screen recording and playing method, screen recording terminal and screen playing terminal |
CN107920280A (en) * | 2017-03-23 | 2018-04-17 | 广州思涵信息科技有限公司 | The accurate matched method and system of video, teaching materials PPT and voice content |
US10762284B2 (en) * | 2017-08-21 | 2020-09-01 | International Business Machines Corporation | Automated summarization of digital content for delivery to mobile devices |
US11259075B2 (en) * | 2017-12-22 | 2022-02-22 | Hillel Felman | Systems and methods for annotating video media with shared, time-synchronized, personal comments |
CN108459836B (en) * | 2018-01-19 | 2019-05-31 | 广州视源电子科技股份有限公司 | Annotate display methods, device, equipment and storage medium |
US11030796B2 (en) * | 2018-10-17 | 2021-06-08 | Adobe Inc. | Interfaces and techniques to retarget 2D screencast videos into 3D tutorials in virtual reality |
US10805651B2 (en) * | 2018-10-26 | 2020-10-13 | International Business Machines Corporation | Adaptive synchronization with live media stream |
US11437072B2 (en) * | 2019-02-07 | 2022-09-06 | Moxtra, Inc. | Recording presentations using layered keyframes |
US11170782B2 (en) * | 2019-04-08 | 2021-11-09 | Speech Cloud, Inc | Real-time audio transcription, video conferencing, and online collaboration system and methods |
US20220013127A1 (en) * | 2020-03-08 | 2022-01-13 | Certified Electronic Reporting Transcription Systems, Inc. | Electronic Speech to Text Court Reporting System For Generating Quick and Accurate Transcripts |
US11128636B1 (en) * | 2020-05-13 | 2021-09-21 | Science House LLC | Systems, methods, and apparatus for enhanced headsets |
US11665284B2 (en) * | 2020-06-20 | 2023-05-30 | Science House LLC | Systems, methods, and apparatus for virtual meetings |
US11606220B2 (en) * | 2020-06-20 | 2023-03-14 | Science House LLC | Systems, methods, and apparatus for meeting management |
-
2021
- 2021-05-19 US US17/303,075 patent/US20220374585A1/en active Pending
-
2022
- 2022-05-19 EP EP22735278.8A patent/EP4272211A1/en active Pending
- 2022-05-19 CN CN202280017301.8A patent/CN116888668A/en active Pending
- 2022-05-19 KR KR1020237039449A patent/KR20230172004A/en unknown
- 2022-05-19 WO PCT/US2022/072434 patent/WO2022246450A1/en active Application Filing
Also Published As
Publication number | Publication date |
---|---|
US20220374585A1 (en) | 2022-11-24 |
KR20230172004A (en) | 2023-12-21 |
EP4272211A1 (en) | 2023-11-08 |
WO2022246450A1 (en) | 2022-11-24 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11849196B2 (en) | Automatic data extraction and conversion of video/images/sound information from a slide presentation into an editable notetaking resource with optional overlay of the presenter | |
US20220374585A1 (en) | User interfaces and tools for facilitating interactions with video content | |
US20210056251A1 (en) | Automatic Data Extraction and Conversion of Video/Images/Sound Information from a Board-Presented Lecture into an Editable Notetaking Resource | |
CN101930779B (en) | Video commenting method and video player | |
US10275203B2 (en) | Methods and apparatus for enhancing electronic presentations with location information | |
US8358309B2 (en) | Animation of audio ink | |
US20160179225A1 (en) | Paper Strip Presentation of Grouped Content | |
US20150121189A1 (en) | Systems and Methods for Creating and Displaying Multi-Slide Presentations | |
WO2012103267A2 (en) | Digital asset management, authoring, and presentation techniques | |
US9335838B2 (en) | Tagging of written notes captured by a smart pen | |
US20170300746A1 (en) | Organizing Written Notes Using Contextual Data | |
JP2015533002A (en) | Correlation between written notes and digital content | |
US10965743B2 (en) | Synchronized annotations in fixed digital documents | |
US11694371B2 (en) | Controlling interactivity of digital content overlaid onto displayed data via graphics processing circuitry using a frame buffer | |
Cabral et al. | A creation-tool for contemporary dance using multimodal video annotation | |
US20170004859A1 (en) | User created textbook | |
KR20150135056A (en) | Method and device for replaying content | |
Denoue et al. | ProjectorBox: Seamless presentation capture for classrooms | |
CN115437736A (en) | Method and device for recording notes | |
US20240078751A1 (en) | Systems and methods for educating in virtual reality environments | |
KR20190142761A (en) | The creating of anew content by the extracting multimedia core | |
CN115052192A (en) | Video processing method and device | |
Cabral | Video Interaction using Pen-Based Technology |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |