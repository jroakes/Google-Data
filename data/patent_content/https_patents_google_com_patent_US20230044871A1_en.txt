US20230044871A1 - Search Results With Result-Relevant Highlighting - Google Patents
Search Results With Result-Relevant Highlighting Download PDFInfo
- Publication number
- US20230044871A1 US20230044871A1 US17/622,067 US202017622067A US2023044871A1 US 20230044871 A1 US20230044871 A1 US 20230044871A1 US 202017622067 A US202017622067 A US 202017622067A US 2023044871 A1 US2023044871 A1 US 2023044871A1
- Authority
- US
- United States
- Prior art keywords
- metadata
- image
- visually
- query
- verifiable
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000004044 response Effects 0.000 claims abstract description 40
- 238000000034 method Methods 0.000 claims description 22
- 238000012795 verification Methods 0.000 claims description 10
- 238000005516 engineering process Methods 0.000 abstract description 9
- 230000015654 memory Effects 0.000 description 16
- 230000000007 visual effect Effects 0.000 description 8
- 238000010586 diagram Methods 0.000 description 7
- 238000012545 processing Methods 0.000 description 7
- 238000004422 calculation algorithm Methods 0.000 description 4
- 230000006870 function Effects 0.000 description 4
- 230000008447 perception Effects 0.000 description 4
- 230000008569 process Effects 0.000 description 4
- 238000012015 optical character recognition Methods 0.000 description 3
- 238000001514 detection method Methods 0.000 description 2
- 238000010801 machine learning Methods 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 230000011218 segmentation Effects 0.000 description 2
- 230000004931 aggregating effect Effects 0.000 description 1
- 238000004873 anchoring Methods 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 238000004891 communication Methods 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 238000013518 transcription Methods 0.000 description 1
- 230000035897 transcription Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/20—Information retrieval; Database structures therefor; File system structures therefor of structured data, e.g. relational data
- G06F16/29—Geographical information databases
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/53—Querying
- G06F16/538—Presentation of query results
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/34—Route searching; Route guidance
- G01C21/36—Input/output arrangements for on-board computers
- G01C21/3679—Retrieval, searching and output of POI information, e.g. hotels, restaurants, shops, filling stations, parking facilities
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/587—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using geographical or spatial information, e.g. location
Definitions
- Map applications have historically provided representations of geographic areas and directions between locations. Modern map applications provide additional information and services beyond those historically provided, such as satellite imagery, street level imagery, user-provided imagery, virtual tours of locations, business listings, business information, three-dimensional models of locations, real-time navigation, and real-time traffic conditions amongst other information and services. Map applications, and other applications that incorporate map application features, such as virtual assistants, may provide any of this information and services to users when requested. However, it may be difficult for users to trust that the information provided to them is accurate without first understanding where the information came from. Users may try to verify the information before relying on it, such as by looking at street level imagery which can be disorienting to users if they have never visited a particular location. Moreover, the information provided to the users may be difficult and time-consuming to locate within the provided imagery
- One aspect of the disclosure is directed to a method for providing metadata in an application.
- the method includes sending, by one or more processors, a query from the application; receiving, by the one or more processors, in response to the query, visually-verifiable metadata corresponding to one or more points of interest relevant to the query and an image associated with the visually-verifiable metadata; and displaying, by the one or more processors in the application, the image associated with the visually-verifiable metadata, wherein the visually-verifiable metadata is annotated within the image.
- the one or more processors are configured to send a query from an application; receive, in response to the query, visually-verifiable metadata corresponding to one or more points of interest relevant to the query and an image associated with the visually-verifiable metadata; and display, in the application, the image associated with the visually-verifiable metadata, wherein the visually-verifiable metadata is annotated within the image.
- Another aspect of the disclosure is directed to a non-transitory computer-readable storage medium storing instructions executable by one or more processors for performing a method, comprising sending a query from an application; receiving, in response to the query, visually-verifiable metadata corresponding to one or more points of interest relevant to the query and an image associated with the visually-verifiable metadata; and displaying, in the application, the image associated with the visually-verifiable metadata, wherein the visually-verifiable metadata is annotated within the image.
- the image includes imagery of the one or more points of interest.
- a request for verification of the visually-verifiable metadata is received, wherein the image is displayed in response to the request.
- the application includes a selectable input, wherein the request for verification is received through the selectable input.
- annotated visually-verifiable metadata is highlighted, circled, identified, outlined, or enlarged within the image.
- the application is a navigation application and the query is a destination.
- the application includes a search interface, wherein the query is received through the search interface and the image and visually-verifiable metadata is displayed within the search interface.
- FIG. 1 is a functional diagram of an example system in accordance with aspects of the disclosure.
- FIG. 2 is a pictorial diagram of the example system of FIG. 1 .
- FIG. 3 is an example of a database stored in a storage system in accordance with aspects of the disclosure.
- FIG. 4 is a flow diagram illustrating the creation of a database in accordance with aspects of the disclosure.
- FIG. 5 is an example image with identified objects and text in accordance with aspects of the disclosure.
- FIG. 6 is a flow diagram illustrating the identification and providing of metadata in response to a query in accordance with aspects of the disclosure.
- FIG. 7 is an image illustrating an example use case of the technology within a map application in accordance with aspects of the disclosure.
- FIG. 8 is an image illustrating another example use case of the technology in accordance with aspects of the disclosure.
- FIG. 9 is an image illustrating an example use case of the technology within a navigation application in accordance with aspects of the disclosure.
- FIG. 10 is an image illustrating an example use case of the technology providing visually-verifiable metadata in response to a query for a POI that satisfies a category in accordance with aspects of the disclosure.
- FIGS. 11 A and 11 B illustrate an example use case of the technology within a search interface in accordance with aspects of the disclosure.
- the technology described herein provides visually-verifiable metadata about points of interest.
- the application may return data that includes relevant metadata about one or more points of interest in response to the query.
- Such applications may be referred to as map applications, applications, or apps herein.
- visual verification of the metadata may be provided or otherwise made available to the user, such as by providing an image that includes the metadata.
- the visually-verifiable metadata may be provided in one or more images of the one or more points of interest and the metadata may corroborate the information returned in response to the query.
- metadata that includes information associated with the business may be provided to the user.
- This metadata may include information such as the street number.
- an image of the business showing the street number on a wall of the business may also be provided.
- the street number within the image is visually-verifiable metadata that provides visual evidence to the user that the street number in the metadata is accurate.
- the metadata provided in response to a query may include the operating hours of the business.
- the operating hours metadata may be visually-verifiable within an image having the operating hours of the business printed on a door of the business.
- the portions of the images that are relied upon as verifying the metadata may be highlighted, circled, or otherwise identified to emphasize the information contained in the metadata and guide the user to the relevant portion of the image.
- FIGS. 1 and 2 illustrate an example system 100 in which the features described above may be implemented. It should not be considered as limiting the scope of the disclosure or usefulness of the features described herein.
- system 100 can include computing devices 110 , 120 , 130 , and 140 as well as storage system 150 .
- Each of the computing devices 110 - 140 can contain one or more processors 112 , memory 114 and other components typically present in general-purpose computing devices.
- Memory 114 of each of computing device 110 - 140 can store information accessible by the one or more processors 112 , including instructions 116 that can be executed by the one or more processors 112 .
- Memory can also include data 118 that can be retrieved, manipulated or stored by the processor.
- the memory can be of any type, optionally a non-transitory type, capable of storing information accessible by the processor, such as a hard-drive, memory card, ROM, RAM, DVD, CD-ROM, write-capable, and read-only memories.
- the instructions 116 can be any set of instructions to be executed directly, such as machine code, or indirectly, such as scripts, by the one or more processors.
- the terms “instructions,” “application,” “steps,” and “programs” can be used interchangeably herein.
- the instructions can be stored in object code format for direct processing by a processor, or in any other computing device language including scripts or collections of independent source code modules that are interpreted on demand or compiled in advance. Functions, methods, and routines of the instructions are explained in more detail below.
- Data 118 may be retrieved, stored or modified by the one or more processors 112 in accordance with the instructions 116 .
- the data can be stored in computer registers, in a relational database as a table having many different fields and records, or XML documents.
- the data can also be formatted in any computing device-readable format such as, but not limited to, binary values, ASCII or Unicode.
- the data can comprise any information sufficient to identify the relevant information, such as numbers, descriptive text, proprietary codes, pointers, references to data stored in other memories such as at other network locations, or information that is used by a function to calculate the relevant data.
- the one or more processors 112 can be any conventional processors, such as a commercially available CPU. Alternatively, the processors can be dedicated components such as an application specific integrated circuit (“ASIC”) or other hardware-based processor. Although not necessary, one or more of computing devices 110 - 140 may include specialized hardware components to perform specific computing processes and functions, such as decoding video, matching video frames with images, distorting videos, encoding distorted videos, machine learning, machine perception, logo recognition, visual text transcription, semantic segmentation, and other such processes and machine learning faster or more efficiently.
- ASIC application specific integrated circuit
- FIG. 1 functionally illustrates the processor, memory, and other elements of computing devices 110 and 120 as being within the same block, respectively, the processor, memory, and other elements can actually comprise multiple processors, memories, or other elements that may or may not be stored within the same physical housing.
- the memory 114 of server computing device 110 can be a hard drive or other storage media, such as RAM, located in one or more housings different from that of the other components of server computing device 110 , such as processors 112 .
- references to a processor, memory, or other elements of the computing devices will be understood to include references to a collection of processors, memories, or elements that may or may not operate in parallel.
- each computing device may be comprised of multiple computing devices.
- each server computing device 110 may include multiple server computing devices operating as a load-balanced server farm, distributed system, etc.
- server computing devices 110 may include multiple server computing devices operating as a load-balanced server farm, distributed system, etc.
- server computing devices operating as a load-balanced server farm, distributed system, etc.
- Each of the computing devices 110 , 120 , 130 , and 140 can be at different nodes of a network 160 and capable of directly and indirectly communicating with other nodes of network 160 .
- computing devices 110 , 120 , 130 , and 140 are depicted in FIGS. 1 - 2 , it should be appreciated that fewer or more computing devices may be possible.
- a typical system can include a large number of connected computing devices, with each different computing device being at a different node of the network 160 .
- the network 160 and intervening nodes described herein can be interconnected using various protocols and systems, such that the network can be part of the Internet, World Wide Web, intranets, wide area networks, or local networks.
- the network can utilize standard communications protocols and systems, such as Ethernet, Wi-Fi and HTTP, protocols that are proprietary to one or more companies, and various combinations of the foregoing.
- each server computing device 110 may include one or more servers capable of communicating with storage system 150 as well as computing devices 120 , 130 , and 140 via the network.
- one or more of server computing devices 110 may use network 160 to transmit and present information to a user, such as user 220 , 230 , or 240 , on a display, such as displays 122 , 132 , or 142 of computing devices 120 , 130 , or 140 .
- computing devices 120 , 130 , and 140 may be considered client computing devices and may perform all or some of the features described herein.
- Each of the client computing devices 120 , 130 , and 140 may be configured similarly to the server computing devices 110 , with one or more processors, memory and instructions as described above.
- Each client computing device 120 , 130 , or 140 may be a personal computing device intended for use by a user 220 , 230 , 240 , and have all of the components normally used in connection with a personal computing device such as a central processing unit (CPU), memory (e.g., RAM and internal hard drives) storing data and instructions, a display such as displays 122 , 132 , or 142 (e.g., a monitor having a screen, a touch-screen, a projector, a television, or other device that is operable to display information), and user input device 124 (e.g., a mouse, keyboard, touch-screen, or microphone).
- CPU central processing unit
- memory e.g., RAM and internal hard drives
- a display such as displays 122 , 132 , or 142 (e.g.,
- the client computing device may also include a camera for recording video streams and/or capturing images, speakers, a network interface device, and all of the components used for connecting these elements to one another.
- Server computing device 110 may also include some or all of the components normally used in connection with a personal computing device.
- client computing devices 120 , 130 , and 140 may each comprise a full-sized personal computing device, they may alternatively comprise mobile computing devices capable of wirelessly exchanging data with servers over a network such as the Internet.
- client computing devices 120 , 130 may be a mobile phone and client computing device may be a laptop.
- client computing device 120 , 130 , and 140 may be a device such as a wireless-enabled PDA, a tablet PC, a netbook, a head-mounted computing systems, or any other such computing device.
- the user may input information using a small keyboard, a keypad, microphone, using visual signals with a camera, or a touch screen.
- storage system 150 can be of any type of computerized storage capable of storing information accessible by the server computing devices 110 , such as a hard-drive, memory card, ROM, RAM, DVD, CD-ROM, write-capable, and read-only memories.
- storage system 150 may include a distributed storage system where data is stored on a plurality of different storage devices which may be physically located at the same or different geographic locations.
- Storage system 150 may be connected to the computing devices via the network 160 as shown in FIG. 1 and/or may be directly connected to any of the computing devices 110 , 120 , 130 , and 140 (not shown).
- Storage system 150 may store data related to points of interest (POI) for retrieval in response to queries for information regarding points of interests as described herein.
- points of interests may include any place or location, or objects at places or locations.
- POI include businesses, such as restaurants, shops, or service providers such as mechanic auto shops, hotels, gas stations, and electric vehicle charging stations. Additional examples include natural landmarks, parks, public service providers such as police stations or post offices, and civil service locations, such as a municipal building.
- Other examples of POIs include signage of any kind, including road and traffic signs, addresses, street names, parking indications, retail signs, banners etc.
- POIs can include any real-world entity with a physical address, including non-business structures such as residences.
- a POI can be associated with more than a singular location, such as latitude/longitude coordinates.
- POIs may be associated with many locations or, in some instances, 3D geometry.
- POIs may not be points, but may include many locations, such as, for example, a business occupying a building, part of a building, or entire grounds.
- POIs can be parts of larger, aggregating POIs. For example, each store in a mall may constitute a respective POI, and the mall may be a POI that aggregates each store in the mall.
- Storage system 150 includes database 301 .
- database 301 is shown as being stored in storage system 150 , the database 301 , or other such storage structures may be stored in any computing device, including anyone of computing devices 110 - 140 .
- the database 301 may be stored across multiple computing devices or storage systems. In this regard, portions of database 301 may be stored on different storage devices and/or computing devices.
- Database 301 may be formed from two or more smaller databases.
- Database 301 stores images and metadata that may be provided by an application, such as a mapping application in response to a query.
- Metadata may include any data that may be relevant to one or more points of interest or other such data that contains information that may be provided by a map application or other such application that provides map information or services, in response to a query.
- metadata may include image data, such as satellite imagery, street level imagery, user-provided imagery, as well as other data such as virtual tours of locations, business listings, business information, three-dimensional models of locations, real-time navigation, and real-time traffic conditions amongst other information and services.
- image data may be considered metadata, but for purposes of clarity, image data is referred to as being distinct from other types of metadata herein.
- Database 301 includes entries for three points of interest including POI 1 , 311 , POI 2 313 , and POI 3 315 .
- Each entry for a POI may include sub-entries in which image data and/or metadata associated with, or otherwise relevant to, the POI entry may be stored.
- the entry for POI 1 311 includes sub-entries 321 , 331 , and 341 .
- Sub-entries 321 and 331 store image data including Image A and Image B, respectively.
- the entry for POI 2 313 includes sub-entries 323 , 333 , and 343 .
- Sub-entries 323 and 333 store image data including Image C 323 and Image D 333 , respectively.
- the entry for POI 3 315 includes sub-entries 325 , 335 , 345 , and 355 .
- Sub-entries 325 , 335 , and 345 store image data including Image E, Image F, and a collection of images, Images G-M, respectively.
- Sub-entries 341 , 343 , and 355 store other metadata that is not defined or derived from an image, as detailed herein.
- each POI entry 311 , 313 , 315 includes a single metadata sub-entry 341 , 343 , 355 , respectively, any number of metadata sub-entries entries may be included in each POI entry.
- any number of image data sub-entries may be included in each POI entry.
- Database 301 illustrates discrete POI entries for each individual POI, although sets of POIs may be grouped together in a single POI entry or a hierarchy of POI entries.
- a POI may contain many other POIs.
- a park POI may include other POIs within it, such as a walking path, statue, dog park, etc. In some instances, some or all of the POIs within the park may be grouped together as a single POI entry for the park.
- POI entries may be stored hierarchically.
- a park POI may contain other POIs, such as a splash park and dog park located within the park.
- the splash park may contain additional POIs such as slides and fountains and the dog park may additional POIs such as an obstacle course and benches.
- the POIs can be organized hierarchically.
- the park POI entry may be the top-level of the hierarchy, with the splash park POI entry and dog park POI entry being mid-level POI entries, under the park POI entry.
- Each slide POI entry and fountain POI entry may be stored under the splash park POI entry.
- the obstacle course POI entry and bench POI entry may be stored under the dog park POI entry.
- the hierarchy of entries is described from the top down, the hierarchy may be reversed.
- the park POI entry may be the lowest layer of the hierarchy.
- the hierarchical relationship between POI entries may be defined through other data.
- the relationship between POI entries may be stored in a separate portion of the database 301 or in another database entirely.
- the POI entries may be stored at the same level in the database, with the hierarchical relationship between the POI entries being described through other data.
- Each entry for a POI may include sub-entries in which image data and/or metadata associated with, or otherwise relevant to, the POI entry may be stored.
- Image data within a sub-entry may include one or more images of the POI associated with the sub-entry.
- image data within a sub-entry may include images that contain imagery associated with the POI for which the image data-subentry is stored.
- the image data for a POI may include a photo of an object and/or individual captured at, or near the POI, or any other images which may have some association or relation with the POI.
- images may include photographs, screenshots, videos, including video clips and/or video stills/frames, or any other such visual data including actively-lit or passive non-visible band imagery such as LiDAR, radar, or infrared, taken from any perspective, such as ground level, aerial, satellite, etc.
- images may be captured by cameras, video cameras, sensors, such as LiDAR, radar, and infrared sensors, or other such devices.
- the metadata may include information about the image data within each sub-entry.
- the metadata within each sub-entry may be based on data corresponding to the image data within the sub-entry.
- image data may include data such as the time and location the image or images within the image data were captured. The time and location may be stored as metadata within the sub-entry.
- Metadata based on data from the image data may be considered defined metadata.
- the metadata within each sub-entry may be derived or otherwise generated from the image data within the sub-entry. Metadata generated from the image data may be considered derived metadata. Metadata may also include data that is associated with a POI, but not derived or defined from image data.
- Metadata may be stored within sub-entries in the database 301 .
- sub-entry 321 within the entry for POI 1 311 includes metadata associated with Image A.
- metadata associated with a POI, but not associated with image data may be stored in separate sub-entries, such as sub-entries 341 , 343 , 355 , as further shown in FIG. 3 .
- FIG. 3 illustrates each entry as including unique sub-entries
- a sub-entry may be associated with two or more entries and/or the same sub-entries may be stored in two or more entries.
- a photo that captures imagery of POI 1 and POI 2 may be stored in a sub-entry within the entry for POI 1 311 and again as a sub-entry within the entry for POI 2 313 .
- a photo that captures imagery of POI 1 and POI 2 may be stored as a sub-entry within one of the POI entries.
- a link or association to the sub-entry may be stored in the other POI entry.
- a sub-entry may be associated with both entries, including the entry for POI 1 311 and the entry for POI 2 .
- sub-entries may be stored outside of entries or in a separate database. By using a link or association, a sub-entry may be stored once, thereby negating the need to store multiple copies of the same sub-entry.
- Database 301 is one example of a storage structure capable of storing data related to points of interest and is described herein for reference.
- any storage structure which allows for image data and metadata to be stored in association with a POI entry may be used.
- image data and metadata may be stored in separate databases than POI entries.
- the image data and metadata may include, or otherwise be associated with, pointers that link the image data and metadata to particular POIs.
- defined and/or derived metadata may be stored in separate databases than the image data.
- the defined and/or derived metadata may include, or otherwise be associated with, pointers that link each piece of defined and/or derived metadata to the image data, image, and/or images from which it was gathered or otherwise generated from.
- FIG. 4 is a flow diagram 400 that illustrates the process for gathering and indexing image data by a server computing device, such as server computing device 110 , to provide to users in response to queries, such as those submitted to a map application.
- image data may be gathered from sources, as shown in block 401 . Gathering image data may be done automatically or manually. For instance, a user may upload image data from a computing device, such as one of computing devices 120 - 140 , to server computing device 110 or storage system 150 . In another example, image data collected in the field, such as with a navigation car fitted with cameras, may be uploaded to server computing device 110 or storage system 150 .
- automatic collection of image data may be performed, such as by a web crawler installed on server computing device 110 .
- the creators or owners of image data may provide copies of the image data content directly to the server computing device. Permission to retrieve image data, manually or automatically, may be provided by the owners or handlers of the image data prior to retrieval of the image data.
- Each piece of gathered image data may be stored as sub-entry of a POI entry for which the image data has an association or relation as shown in block 403 .
- “Image A” may contain imagery of POI 1 .
- Image A is stored as a sub-entry 321 of entry 311 representative of POI 1 .
- Image A may also contain imagery of POI 4 (not shown).
- Image A may be stored as sub-entry 321 of POI 1 and a sub-entry of POI 4 (not shown).
- Image A may be stored once, such as a sub-entry 321 of POI 1 and linked to POI 4 , to avoid the need for duplicative storage.
- images may not be stored as sub-entries, but rather in another portion of the index or database, or in an entirely different database. The images may then be linked to entries in the database.
- an image may be segmented into parts with each part being stored as its own image.
- image recognition models such as those described further herein, may identify objects in an image and separate the image data associated with an identified object or objects into its own image.
- the gathered image data may be analyzed by the server computing device 110 to identify whether it includes any defined metadata, as shown in block 405 of FIG. 4 .
- the defined metadata may be stored in association with the image data from which it was retrieved, as shown in block 407 of FIG. 4 .
- the image data which was used to create sub-entry 321 of Image A may include defined metadata such as the file name of Image A and the location where Image A was captured. This defined metadata may be retrieved and stored in association with Image A in sub-entry 321 , as illustrated by the “defined” designation of the metadata.
- Metadata may be derived from the image data, as shown in block 409 of FIG. 4 .
- image data may be processed to identify content within an image, such as objects and texts within the image data.
- the identified objects and text can then be used to generate derived metadata for use in responding to queries.
- image 501 shown in FIG. 5 may be processed and objects within the image 501 may be identified.
- objects include houses 503 and 505 , driveway 507 , and road 509 .
- Metadata indicating image 501 includes the identified objects 503 , 505 , 507 , and/or 509 may be generated. Additional metadata, such as data identifying the locations of the identified objects in the image 501 may also be derived.
- the derived metadata may be stored in association with the image data, as shown in block 411 .
- Metadata may be derived from Image B in sub-entry 331 .
- the metadata associated with Image B may be stored in association with Image B and include a “derived” designation.
- FIG. 3 identifies the metadata in sub-entries 321 and 331 as being “defined” and/or “derived,” in practice the metadata may not be identified as being defined or derived.
- image recognition models may include machine perception, logo recognition, semantic segmentation etc., and other such image and text processing algorithms.
- image recognition models may include 3D reconstruction and Structure-from-Motion (SfM), which are capable of fusing semantics across multiple images.
- 3D reconstruction and SfM may be used to reconcile the physical positions of detected objects within images with one another, correlating detections from different sources.
- 3D reconstruction and/or SfM may be used to position a sign object in images captured from different angles and perspectives, as well as at different times. SfM can also be used to more precisely localize photos in the world, by anchoring to previously-captured image data at the location.
- Text processing may include any variant of optical character recognition (OCR).
- FIG. 5 illustrates the content identified after using a machine perception algorithm, such as building detection and 3D reconstruction by fusing semantics across multiple imagery sets, including on image 501 .
- a machine perception algorithm such as building detection and 3D reconstruction by fusing semantics across multiple imagery sets, including on image 501 .
- objects including houses 503 and 505 , a driveway 507 , and a road 509 .
- Machine perception may be used to identify each of these objects.
- an image contains a logo or other marking associated with a business, individual, etc.
- logo recognition algorithms may be used to identify the logo in the image.
- derived metadata indicating image 501 includes houses, a road, and a driveway may be generated.
- the location of the identified objects in the real world, relative to other, nearby features or objects and/or in global coordinate frames relative to a datum, such as latitude/longitude/altitude or earth-centered, earth-fixed (ECEF), may be stored in the metadata or other such database.
- the image recognition model may also identify textual content within the image data. For example, each house 503 and 505 has a house number 521 and 523 , respectively. Moreover, street sign 511 has the street name 525 .
- the image recognition model may use a text recognition and processing algorithm such as OCR and determine the house number 521 of house 503 is 208 , the house number 523 of house 505 is 210 , and the street name 525 is “Main St.” Based on this information, metadata may be generated that identifies road 509 as “Main St.” and the houses 503 and 505 as having addresses of 208 Main St. and 210 Main St., respectively. This additional metadata may be stored in the index in association with image 501 .
- the location of textual data within images may also be stored as metadata.
- the location of the text in the real world, relative to features around it and/or in global coordinate frames relative to a datum, such as latitude/longitude/altitude or ECEF, may be stored in the metadata or other such database.
- defined and other derived metadata may be used to derive additional metadata.
- the image data associated with image 501 may include a location of capture of the image in latitude/longitude coordinates.
- the location of capture information may be used to determine, based on other metadata within a database, such as database 301 , that the location the image was captured was in Manhattan.
- the system may determine that houses 503 and 505 are both located in New York City in the State of NY.
- the system may determine that house 503 's address is 208 Main St. New York, NY and house 505 's address is 210 Main St. New York, NY.
- Metadata indicating the addresses of the houses may be generated and stored in association with image 501 .
- Any metadata that was derived or defined within the image data, and which can be shown within the image data from which it was derived or defined can be considered visually-verifiable metadata.
- Such visually-verifiable metadata may be marked as such in the database 301 .
- the addresses of houses 503 and 505 may be considered visually-verifiable metadata, as the house numbers and street name are included in the image 501 from which the addresses of houses 521 and 523 were derived.
- the metadata including the addresses of houses 503 and 505 may be marked as being visually-verifiable
- FIG. 6 is a flow diagram 600 that illustrates the process for providing visually-verifiable information about points of interest.
- a query for information about a POI may be received by a server computing device, such as server computing device 110 , as shown in block 601 .
- the query may be submitted via a client computing device.
- user 220 may submit a query to a map application executing on client computing 120 .
- an application may automatically submit a query for information about a point of interest.
- a virtual assistant may submit a query for information about a location where an upcoming appointment for a user is located.
- a query may be received by the server computing device from another server computing device.
- the server computing device may retrieve relevant metadata, as shown in block 603 .
- the relevant metadata may be retrieved from a database, such as database 301 that stores data regarding to one or more POIs or other such information that may be provided by a map application in response to a query.
- Relevant metadata may be considered any type of data that includes information that may be responsive or otherwise material to the query.
- the server computing device 110 may retrieve relevant data from locations other than database 301 . For instance, the server computing device 110 may visit a website of a POI related to the query and collect relevant information about the POI from the website.
- the server computing device 110 may determine whether the relevant metadata includes visually-verifiable metadata relevant to the retrieved data is available, as shown in block 605 . Identification of visually-verifiable metadata may include analyzing the relevant metadata to see if any of the relevant metadata is marked as being visually-verifiable.
- the server computing system may provide the retrieved relevant metadata in response to the query, as shown in block 609 .
- the provided relevant data may include some or all of the retrieved relevant metadata.
- the visually-verifiable metadata may be prioritized, as shown in block 607 .
- a query may result in many relevant metadata items being retrieved. However, only some of the retrieved metadata items may be identified as visually-verifiable metadata.
- the visually-verifiable metadata items may be prioritized over relevant metadata items that are not visually-verifiable, as visually-verifiable metadata may boost the confidence of the information corresponding to the metadata being correct.
- Visually-verifiable metadata that is old relative to the rate of change of a place may not be prioritized as much as newer visually-verifiable metadata
- Other factors, such as relevance to a query, may also be used to prioritize the metadata for returning in response to the query.
- relevant metadata may be visually-verifiable
- a piece of metadata that is not visually-verifiable may be prioritized for being more relevant to the query.
- the relevant metadata including the visually-verifiable metadata may be provided in response to the query, as shown in block 611 .
- the image or images associated with the visually-verifiable metadata may also be provided with the metadata, or may be provided in response to a subsequent request from the user or application.
- the visually-verifiable metadata may be annotated to bring a user's focus to the metadata.
- Annotation of the visually-verifiable metadata may include highlighting, circling, outlining, enlarging, or otherwise bringing attention and focus to the visually-verifiable metadata.
- the map application may return metadata that includes relevant information about one or more points of interest in response to the query.
- This metadata can be easily viewed and verified by a user. As a result, users may gain confidence that the map application, and the data it provides in response to queries, is accurate.
- any computing device including a client computing device, may execute the steps.
- FIG. 7 illustrates an example use case in accordance with aspects of the disclosure.
- a user may enter an address query, “ 1103 Main St.,” into a map application on a user device, such as computing device 120 .
- the map application may forward the request to a server computing device, such as server computing device 110 .
- the server computing device 110 may determine that a database, such as database 301 , includes visually-verifiable metadata corresponding to the number “ 1103 ” from an image 701 including the building located at 1103 Main St.
- the server computing device 110 may return the image along with the location where the visually-verifiable metadata was derived.
- the client computing device may output the image 701 with the location 721 , corresponding to the street number on the side of the building, being annotated with a circle and arrow, as shown in FIG. 7 .
- the server computing device 110 may return an annotated image with the visually-verifiable metadata being identified or otherwise highlighted to the client computing device.
- FIG. 8 illustrates another example use case where visually-verifiable metadata is outlined in response to a query.
- image data including an image 801 with visually-verifiable metadata 809 is identified in response to a query for the address corresponding to a house.
- the visually-verifiable metadata 809 includes imagery of the house within the image 801 .
- the visually verifiable metadata 809 is outlined 810 and displayed or otherwise returned in response to the query.
- FIG. 9 illustrates an example use case where visually-verifiable metadata is displayed in a navigation application interface.
- the navigation application interface 901 includes a destination 902 and an image of the destination 903 .
- the image of the destination 903 may include visually-verifiable metadata including a house 909 located at the address of the destination 902 and a house number 919 of the house. Both pieces of visually-verifiable metadata, including the house 909 and house number 919 are annotated.
- the house 909 is outlined 910 and the house number 920 is enlarged for easier viewing by the user.
- the image 903 may be processed in real-time.
- the image 903 may be a frame of a video feed from a camera system of a vehicle.
- An image recognition model may identify content and textual content within the frame in real-time and highlight some or all of the visually-verifiable metadata within the frame, such as the house 909 and house number 919 , as shown in FIG. 9 . Processing may occur for each frame received from the video feed or at some predetermined time or number of frames.
- FIG. 10 illustrates another example use case where visually-verifiable metadata is provided in response to a query for a POI that satisfies a category.
- the query may be “Thai restaurant with delivery,” where the query includes a request for POIs (restaurants) that satisfy the categories of “Thai” and “delivery”.
- the system may identify a point of interest entry in the database that includes metadata that satisfies the categories of “Thai” restaurants and “delivery”.
- the POI entry may include an image 1001 with visually-verifiable metadata that shows the restaurant 1002 in the image 1001 is a Thai restaurant and that it offers free delivery.
- the visually-verifiable metadata 1010 may be annotated 1012 and returned in response to the query.
- FIGS. 11 A and 11 B illustrate an example use case of the technology used within a search interface 1101 .
- a user may enter a query for an address “123 ABC St. NY, N.Y.” 1103 in a search bar 1102 .
- the search interface 1101 may provide an image 1110 of the POI 1120 located at the address 1103 searched in the query as well as the location of the address on a map 1180 , as indicated by pin 1181 .
- the search interface may provide selectable input, including 1111 and 1112 .
- selectable inputs 1111 , 1112 when selected, highlight the visually-verifiable metadata that confirms what is shown in the image and map is accurate.
- Selectable inputs 1111 and 1112 are merely for illustration purposes, any type of input may be provided in the search interface or other such interface.
- the visually-verifiable metadata may be highlighted. For example, and as shown in FIG. 11 B , upon a user selecting selectable icon 1111 or 1112 , the search interface 1101 may annotate the visually-verifiable metadata, including the house number 1190 for POI 1120 in the image 1110 . In this example, the house number of POI 1120 is enlarged to make it easier for the user to see and to confirm that the POI 1120 is located at the address 1103 entered in the query.
- a map application executing on a client computing device may include a database, such as database 301 .
- the map application may access the database directly, without requiring the use of a server computing system.
- the technology described herein is advantageous because it provides map applications with the ability to source and provides users with content that can be verified visually. By doing such, users are assured that the metadata provided in response to a query is accurate without the need for a user to manually confirm such results. Furthermore, this is done either automatically as part of a response to a single search query (e.g. as in FIGS. 7 - 10 ) or through selectable inputs which allow the user to cause additional visual information to be provided on demand at the same time as viewing search results (e.g. through selection of the selectable icon 1111 or 1112 in FIGS. 11 A- 11 B , which may be selectable with e.g. a single touch or click). This makes it quick and easy for the user to interact with the application to cause relevant additional visual information about a point of interest to be provided.
- the presence of visual verification, and its use in ranking results, may also incentivize users of the map application, as well as owners of POIs, such as merchants, to provide image data for POIs, to assure correct, and visually-verifiable information is provided for POIs. This is particularly true when that evidence must be gathered in places that are difficult to access or not open to the general public, such as indoor spaces belonging to a business.
Abstract
Description
- Map applications have historically provided representations of geographic areas and directions between locations. Modern map applications provide additional information and services beyond those historically provided, such as satellite imagery, street level imagery, user-provided imagery, virtual tours of locations, business listings, business information, three-dimensional models of locations, real-time navigation, and real-time traffic conditions amongst other information and services. Map applications, and other applications that incorporate map application features, such as virtual assistants, may provide any of this information and services to users when requested. However, it may be difficult for users to trust that the information provided to them is accurate without first understanding where the information came from. Users may try to verify the information before relying on it, such as by looking at street level imagery which can be disorienting to users if they have never visited a particular location. Moreover, the information provided to the users may be difficult and time-consuming to locate within the provided imagery
- Aspects of this disclosure provide visually-verifiable metadata about points of interest. One aspect of the disclosure is directed to a method for providing metadata in an application. The method includes sending, by one or more processors, a query from the application; receiving, by the one or more processors, in response to the query, visually-verifiable metadata corresponding to one or more points of interest relevant to the query and an image associated with the visually-verifiable metadata; and displaying, by the one or more processors in the application, the image associated with the visually-verifiable metadata, wherein the visually-verifiable metadata is annotated within the image.
- Another aspect of the disclosure is directed to a system comprising one or more processors. The one or more processors are configured to send a query from an application; receive, in response to the query, visually-verifiable metadata corresponding to one or more points of interest relevant to the query and an image associated with the visually-verifiable metadata; and display, in the application, the image associated with the visually-verifiable metadata, wherein the visually-verifiable metadata is annotated within the image.
- Another aspect of the disclosure is directed to a non-transitory computer-readable storage medium storing instructions executable by one or more processors for performing a method, comprising sending a query from an application; receiving, in response to the query, visually-verifiable metadata corresponding to one or more points of interest relevant to the query and an image associated with the visually-verifiable metadata; and displaying, in the application, the image associated with the visually-verifiable metadata, wherein the visually-verifiable metadata is annotated within the image.
- In some instances the image includes imagery of the one or more points of interest.
- In some instances, prior to displaying the image, a request for verification of the visually-verifiable metadata is received, wherein the image is displayed in response to the request.
- In some instances, the application includes a selectable input, wherein the request for verification is received through the selectable input.
- In some instances, annotated visually-verifiable metadata is highlighted, circled, identified, outlined, or enlarged within the image.
- In some instances, the application is a navigation application and the query is a destination. In some examples, the application includes a search interface, wherein the query is received through the search interface and the image and visually-verifiable metadata is displayed within the search interface.
-
FIG. 1 is a functional diagram of an example system in accordance with aspects of the disclosure. -
FIG. 2 is a pictorial diagram of the example system ofFIG. 1 . -
FIG. 3 is an example of a database stored in a storage system in accordance with aspects of the disclosure. -
FIG. 4 is a flow diagram illustrating the creation of a database in accordance with aspects of the disclosure. -
FIG. 5 is an example image with identified objects and text in accordance with aspects of the disclosure. -
FIG. 6 is a flow diagram illustrating the identification and providing of metadata in response to a query in accordance with aspects of the disclosure. -
FIG. 7 is an image illustrating an example use case of the technology within a map application in accordance with aspects of the disclosure. -
FIG. 8 is an image illustrating another example use case of the technology in accordance with aspects of the disclosure. -
FIG. 9 is an image illustrating an example use case of the technology within a navigation application in accordance with aspects of the disclosure. -
FIG. 10 is an image illustrating an example use case of the technology providing visually-verifiable metadata in response to a query for a POI that satisfies a category in accordance with aspects of the disclosure. -
FIGS. 11A and 11B illustrate an example use case of the technology within a search interface in accordance with aspects of the disclosure. - The technology described herein provides visually-verifiable metadata about points of interest. In this regard, when a query is made to a map application or other such application that provides map information or services, the application may return data that includes relevant metadata about one or more points of interest in response to the query. Such applications may be referred to as map applications, applications, or apps herein. To increase a user's confidence that the metadata provided in response to a query is accurate, visual verification of the metadata may be provided or otherwise made available to the user, such as by providing an image that includes the metadata.
- The visually-verifiable metadata may be provided in one or more images of the one or more points of interest and the metadata may corroborate the information returned in response to the query. For instance, in response to a query for a business, metadata that includes information associated with the business may be provided to the user. This metadata may include information such as the street number. In addition to the metadata provided to the user, an image of the business showing the street number on a wall of the business may also be provided. The street number within the image is visually-verifiable metadata that provides visual evidence to the user that the street number in the metadata is accurate. In another example, the metadata provided in response to a query may include the operating hours of the business. The operating hours metadata may be visually-verifiable within an image having the operating hours of the business printed on a door of the business. The portions of the images that are relied upon as verifying the metadata may be highlighted, circled, or otherwise identified to emphasize the information contained in the metadata and guide the user to the relevant portion of the image.
-
FIGS. 1 and 2 illustrate anexample system 100 in which the features described above may be implemented. It should not be considered as limiting the scope of the disclosure or usefulness of the features described herein. In this example,system 100 can includecomputing devices storage system 150. Each of the computing devices 110-140 can contain one or more processors 112,memory 114 and other components typically present in general-purpose computing devices.Memory 114 of each of computing device 110-140 can store information accessible by the one or more processors 112, includinginstructions 116 that can be executed by the one or more processors 112. - Memory can also include
data 118 that can be retrieved, manipulated or stored by the processor. The memory can be of any type, optionally a non-transitory type, capable of storing information accessible by the processor, such as a hard-drive, memory card, ROM, RAM, DVD, CD-ROM, write-capable, and read-only memories. - The
instructions 116 can be any set of instructions to be executed directly, such as machine code, or indirectly, such as scripts, by the one or more processors. In that regard, the terms “instructions,” “application,” “steps,” and “programs” can be used interchangeably herein. The instructions can be stored in object code format for direct processing by a processor, or in any other computing device language including scripts or collections of independent source code modules that are interpreted on demand or compiled in advance. Functions, methods, and routines of the instructions are explained in more detail below. -
Data 118 may be retrieved, stored or modified by the one or more processors 112 in accordance with theinstructions 116. For instance, although the subject matter described herein is not limited by any particular data structure, the data can be stored in computer registers, in a relational database as a table having many different fields and records, or XML documents. The data can also be formatted in any computing device-readable format such as, but not limited to, binary values, ASCII or Unicode. Moreover, the data can comprise any information sufficient to identify the relevant information, such as numbers, descriptive text, proprietary codes, pointers, references to data stored in other memories such as at other network locations, or information that is used by a function to calculate the relevant data. - The one or more processors 112 can be any conventional processors, such as a commercially available CPU. Alternatively, the processors can be dedicated components such as an application specific integrated circuit (“ASIC”) or other hardware-based processor. Although not necessary, one or more of computing devices 110-140 may include specialized hardware components to perform specific computing processes and functions, such as decoding video, matching video frames with images, distorting videos, encoding distorted videos, machine learning, machine perception, logo recognition, visual text transcription, semantic segmentation, and other such processes and machine learning faster or more efficiently.
- Although
FIG. 1 functionally illustrates the processor, memory, and other elements ofcomputing devices memory 114 ofserver computing device 110 can be a hard drive or other storage media, such as RAM, located in one or more housings different from that of the other components ofserver computing device 110, such as processors 112. Accordingly, references to a processor, memory, or other elements of the computing devices will be understood to include references to a collection of processors, memories, or elements that may or may not operate in parallel. Moreover, each computing device may be comprised of multiple computing devices. For example, eachserver computing device 110 may include multiple server computing devices operating as a load-balanced server farm, distributed system, etc. Yet further, although some functions described below are indicated as taking place on a single computing device having a single processor, various aspects of the subject matter described herein can be implemented by a plurality of computing devices, for example, communicating information overnetwork 160, as described herein. - Each of the
computing devices network 160 and capable of directly and indirectly communicating with other nodes ofnetwork 160. Although only computingdevices FIGS. 1-2 , it should be appreciated that fewer or more computing devices may be possible. In this regard, a typical system can include a large number of connected computing devices, with each different computing device being at a different node of thenetwork 160. - The
network 160 and intervening nodes described herein can be interconnected using various protocols and systems, such that the network can be part of the Internet, World Wide Web, intranets, wide area networks, or local networks. The network can utilize standard communications protocols and systems, such as Ethernet, Wi-Fi and HTTP, protocols that are proprietary to one or more companies, and various combinations of the foregoing. Although certain advantages are obtained when information is transmitted or received as noted above, other aspects of the subject matter described herein are not limited to any particular manner of transmission of information. - As an example, each
server computing device 110 may include one or more servers capable of communicating withstorage system 150 as well ascomputing devices server computing devices 110 may usenetwork 160 to transmit and present information to a user, such asuser displays computing devices computing devices - Each of the
client computing devices server computing devices 110, with one or more processors, memory and instructions as described above. Eachclient computing device user displays Server computing device 110 may also include some or all of the components normally used in connection with a personal computing device. - Although the
client computing devices client computing devices client computing device - As with
memory 114,storage system 150 can be of any type of computerized storage capable of storing information accessible by theserver computing devices 110, such as a hard-drive, memory card, ROM, RAM, DVD, CD-ROM, write-capable, and read-only memories. In addition,storage system 150 may include a distributed storage system where data is stored on a plurality of different storage devices which may be physically located at the same or different geographic locations.Storage system 150 may be connected to the computing devices via thenetwork 160 as shown inFIG. 1 and/or may be directly connected to any of thecomputing devices -
Storage system 150 may store data related to points of interest (POI) for retrieval in response to queries for information regarding points of interests as described herein. As used herein, points of interests may include any place or location, or objects at places or locations. Examples of a POI include businesses, such as restaurants, shops, or service providers such as mechanic auto shops, hotels, gas stations, and electric vehicle charging stations. Additional examples include natural landmarks, parks, public service providers such as police stations or post offices, and civil service locations, such as a municipal building. Other examples of POIs include signage of any kind, including road and traffic signs, addresses, street names, parking indications, retail signs, banners etc. Moreover, POIs can include any real-world entity with a physical address, including non-business structures such as residences. - In some instances, a POI can be associated with more than a singular location, such as latitude/longitude coordinates. In this regard, POIs may be associated with many locations or, in some instances, 3D geometry. In this regard, POIs may not be points, but may include many locations, such as, for example, a business occupying a building, part of a building, or entire grounds. Further, POIs can be parts of larger, aggregating POIs. For example, each store in a mall may constitute a respective POI, and the mall may be a POI that aggregates each store in the mall.
- An example of data stored by
storage system 150 is shown inFIG. 3 .Storage system 150 includesdatabase 301. Althoughdatabase 301 is shown as being stored instorage system 150, thedatabase 301, or other such storage structures may be stored in any computing device, including anyone of computing devices 110-140. Alternatively or additionally, thedatabase 301, other such storage structures, may be stored across multiple computing devices or storage systems. In this regard, portions ofdatabase 301 may be stored on different storage devices and/or computing devices.Database 301 may be formed from two or more smaller databases. -
Database 301 stores images and metadata that may be provided by an application, such as a mapping application in response to a query. Metadata may include any data that may be relevant to one or more points of interest or other such data that contains information that may be provided by a map application or other such application that provides map information or services, in response to a query. For example, metadata may include image data, such as satellite imagery, street level imagery, user-provided imagery, as well as other data such as virtual tours of locations, business listings, business information, three-dimensional models of locations, real-time navigation, and real-time traffic conditions amongst other information and services. Thus, image data may be considered metadata, but for purposes of clarity, image data is referred to as being distinct from other types of metadata herein. -
Database 301 includes entries for three points of interest including POI1, 311,POI2 313, andPOI3 315. Each entry for a POI may include sub-entries in which image data and/or metadata associated with, or otherwise relevant to, the POI entry may be stored. For instance, the entry forPOI1 311 includes sub-entries 321, 331, and 341.Sub-entries POI2 313 includes sub-entries 323, 333, and 343.Sub-entries Image C 323 andImage D 333, respectively. The entry forPOI3 315 includes sub-entries 325, 335, 345, and 355.Sub-entries Sub-entries POI entry single metadata sub-entry -
Database 301 illustrates discrete POI entries for each individual POI, although sets of POIs may be grouped together in a single POI entry or a hierarchy of POI entries. For example, a POI may contain many other POIs. For example, a park POI may include other POIs within it, such as a walking path, statue, dog park, etc. In some instances, some or all of the POIs within the park may be grouped together as a single POI entry for the park. - In some instances, POI entries may be stored hierarchically. For example, a park POI may contain other POIs, such as a splash park and dog park located within the park. The splash park may contain additional POIs such as slides and fountains and the dog park may additional POIs such as an obstacle course and benches. To capture the relationship of the different areas within the park, the POIs can be organized hierarchically. In this regard, the park POI entry may be the top-level of the hierarchy, with the splash park POI entry and dog park POI entry being mid-level POI entries, under the park POI entry. Each slide POI entry and fountain POI entry may be stored under the splash park POI entry. Similarly, the obstacle course POI entry and bench POI entry may be stored under the dog park POI entry. Although the hierarchy of entries is described from the top down, the hierarchy may be reversed. For example, the park POI entry may be the lowest layer of the hierarchy.
- In some instances, the hierarchical relationship between POI entries may be defined through other data. In this regard, the relationship between POI entries may be stored in a separate portion of the
database 301 or in another database entirely. In this regard, the POI entries may be stored at the same level in the database, with the hierarchical relationship between the POI entries being described through other data. - Each entry for a POI may include sub-entries in which image data and/or metadata associated with, or otherwise relevant to, the POI entry may be stored. Image data within a sub-entry may include one or more images of the POI associated with the sub-entry. In some instances, image data within a sub-entry may include images that contain imagery associated with the POI for which the image data-subentry is stored. For instance, the image data for a POI may include a photo of an object and/or individual captured at, or near the POI, or any other images which may have some association or relation with the POI. As used herein, images may include photographs, screenshots, videos, including video clips and/or video stills/frames, or any other such visual data including actively-lit or passive non-visible band imagery such as LiDAR, radar, or infrared, taken from any perspective, such as ground level, aerial, satellite, etc. In this regard, images may be captured by cameras, video cameras, sensors, such as LiDAR, radar, and infrared sensors, or other such devices.
- The metadata may include information about the image data within each sub-entry. The metadata within each sub-entry may be based on data corresponding to the image data within the sub-entry. For example, image data may include data such as the time and location the image or images within the image data were captured. The time and location may be stored as metadata within the sub-entry. Metadata based on data from the image data may be considered defined metadata. In some instances, and as further described herein, the metadata within each sub-entry may be derived or otherwise generated from the image data within the sub-entry. Metadata generated from the image data may be considered derived metadata. Metadata may also include data that is associated with a POI, but not derived or defined from image data.
- Metadata may be stored within sub-entries in the
database 301. For instance, and referring toFIG. 3 , sub-entry 321 within the entry forPOI1 311 includes metadata associated with Image A. In some instances, metadata associated with a POI, but not associated with image data may be stored in separate sub-entries, such assub-entries FIG. 3 . - Although
FIG. 3 illustrates each entry as including unique sub-entries, in some instances a sub-entry may be associated with two or more entries and/or the same sub-entries may be stored in two or more entries. For instance, a photo that captures imagery of POI1 and POI2 may be stored in a sub-entry within the entry forPOI1 311 and again as a sub-entry within the entry forPOI2 313. - Alternatively, a photo that captures imagery of POI1 and POI2 may be stored as a sub-entry within one of the POI entries. A link or association to the sub-entry may be stored in the other POI entry. In another example, a sub-entry may be associated with both entries, including the entry for
POI1 311 and the entry for POI2. In this regard, sub-entries may be stored outside of entries or in a separate database. By using a link or association, a sub-entry may be stored once, thereby negating the need to store multiple copies of the same sub-entry. -
Database 301 is one example of a storage structure capable of storing data related to points of interest and is described herein for reference. In operation, any storage structure which allows for image data and metadata to be stored in association with a POI entry may be used. For example, image data and metadata may be stored in separate databases than POI entries. The image data and metadata may include, or otherwise be associated with, pointers that link the image data and metadata to particular POIs. In some instances, defined and/or derived metadata may be stored in separate databases than the image data. The defined and/or derived metadata may include, or otherwise be associated with, pointers that link each piece of defined and/or derived metadata to the image data, image, and/or images from which it was gathered or otherwise generated from. -
FIG. 4 is a flow diagram 400 that illustrates the process for gathering and indexing image data by a server computing device, such asserver computing device 110, to provide to users in response to queries, such as those submitted to a map application. In this regard, image data may be gathered from sources, as shown inblock 401. Gathering image data may be done automatically or manually. For instance, a user may upload image data from a computing device, such as one of computing devices 120-140, toserver computing device 110 orstorage system 150. In another example, image data collected in the field, such as with a navigation car fitted with cameras, may be uploaded toserver computing device 110 orstorage system 150. Alternatively, or in combination with the manual collection of image data, automatic collection of image data may be performed, such as by a web crawler installed onserver computing device 110. In some instances, the creators or owners of image data may provide copies of the image data content directly to the server computing device. Permission to retrieve image data, manually or automatically, may be provided by the owners or handlers of the image data prior to retrieval of the image data. - Each piece of gathered image data may be stored as sub-entry of a POI entry for which the image data has an association or relation as shown in
block 403. For example, and referring toFIG. 3 , “Image A” may contain imagery of POI1. As such, Image A is stored as a sub-entry 321 ofentry 311 representative of POI1. In another example, Image A may also contain imagery of POI4 (not shown). In this case, Image A may be stored assub-entry 321 of POI1 and a sub-entry of POI4 (not shown). Alternatively, Image A may be stored once, such as a sub-entry 321 of POI1 and linked to POI4, to avoid the need for duplicative storage. In yet another example, images may not be stored as sub-entries, but rather in another portion of the index or database, or in an entirely different database. The images may then be linked to entries in the database. - In some instances, an image may be segmented into parts with each part being stored as its own image. For instance, image recognition models, such as those described further herein, may identify objects in an image and separate the image data associated with an identified object or objects into its own image.
- The gathered image data may be analyzed by the
server computing device 110 to identify whether it includes any defined metadata, as shown inblock 405 ofFIG. 4 . In the event the image data includes defined metadata, the defined metadata may be stored in association with the image data from which it was retrieved, as shown inblock 407 ofFIG. 4 . For instance, and again referring toFIG. 3 , the image data which was used to create sub-entry 321 of Image A may include defined metadata such as the file name of Image A and the location where Image A was captured. This defined metadata may be retrieved and stored in association with Image A insub-entry 321, as illustrated by the “defined” designation of the metadata. - Metadata may be derived from the image data, as shown in
block 409 ofFIG. 4 . In this regard, image data may be processed to identify content within an image, such as objects and texts within the image data. The identified objects and text can then be used to generate derived metadata for use in responding to queries. For example,image 501 shown inFIG. 5 , may be processed and objects within theimage 501 may be identified. Such objects includehouses driveway 507, androad 509.Metadata indicating image 501 includes the identifiedobjects image 501 may also be derived. The derived metadata may be stored in association with the image data, as shown inblock 411. - Referring to
FIG. 3 , metadata may be derived from Image B insub-entry 331. The metadata associated with Image B may be stored in association with Image B and include a “derived” designation. AlthoughFIG. 3 identifies the metadata insub-entries - Processing of image data to derive metadata may be performed using one or more image recognition models to identify the contents within the image data. The image recognition models may include machine perception, logo recognition, semantic segmentation etc., and other such image and text processing algorithms. For example, image recognition models may include 3D reconstruction and Structure-from-Motion (SfM), which are capable of fusing semantics across multiple images. 3D reconstruction and SfM may be used to reconcile the physical positions of detected objects within images with one another, correlating detections from different sources. For instance, 3D reconstruction and/or SfM may be used to position a sign object in images captured from different angles and perspectives, as well as at different times. SfM can also be used to more precisely localize photos in the world, by anchoring to previously-captured image data at the location. Text processing may include any variant of optical character recognition (OCR).
- For example,
FIG. 5 illustrates the content identified after using a machine perception algorithm, such as building detection and 3D reconstruction by fusing semantics across multiple imagery sets, including onimage 501. Withinimage 501 areobjects including houses driveway 507, and aroad 509. Machine perception may be used to identify each of these objects. Similarly, if an image contains a logo or other marking associated with a business, individual, etc., logo recognition algorithms may be used to identify the logo in the image. - Some or all of the identified objects in an image may be used to derive metadata descriptive of the content of the image For instance, derived
metadata indicating image 501 includes houses, a road, and a driveway may be generated. The location of the identified objects in the real world, relative to other, nearby features or objects and/or in global coordinate frames relative to a datum, such as latitude/longitude/altitude or earth-centered, earth-fixed (ECEF), may be stored in the metadata or other such database. - The image recognition model may also identify textual content within the image data. For example, each
house house number street sign 511 has thestreet name 525. The image recognition model may use a text recognition and processing algorithm such as OCR and determine thehouse number 521 ofhouse 503 is 208, thehouse number 523 ofhouse 505 is 210, and thestreet name 525 is “Main St.” Based on this information, metadata may be generated that identifiesroad 509 as “Main St.” and thehouses image 501. The location of textual data within images may also be stored as metadata. The location of the text in the real world, relative to features around it and/or in global coordinate frames relative to a datum, such as latitude/longitude/altitude or ECEF, may be stored in the metadata or other such database. - In some instances, defined and other derived metadata may be used to derive additional metadata. For example, and again referring to
FIG. 5 , the image data associated withimage 501 may include a location of capture of the image in latitude/longitude coordinates. The location of capture information may be used to determine, based on other metadata within a database, such asdatabase 301, that the location the image was captured was in Manhattan. Based on this additional information, the system may determine thathouses house 503's address is 208 Main St. New York, NY andhouse 505's address is 210 Main St. New York, NY. Metadata indicating the addresses of the houses may be generated and stored in association withimage 501. - Any metadata that was derived or defined within the image data, and which can be shown within the image data from which it was derived or defined can be considered visually-verifiable metadata. Such visually-verifiable metadata may be marked as such in the
database 301. For instance, and continuing the above example, the addresses ofhouses image 501 from which the addresses ofhouses houses -
FIG. 6 is a flow diagram 600 that illustrates the process for providing visually-verifiable information about points of interest. In this regard, a query for information about a POI may be received by a server computing device, such asserver computing device 110, as shown inblock 601. The query may be submitted via a client computing device. For example,user 220 may submit a query to a map application executing onclient computing 120. In some instances, an application may automatically submit a query for information about a point of interest. For example, a virtual assistant may submit a query for information about a location where an upcoming appointment for a user is located. In another example, a query may be received by the server computing device from another server computing device. - In response to the query, the server computing device may retrieve relevant metadata, as shown in
block 603. The relevant metadata may be retrieved from a database, such asdatabase 301 that stores data regarding to one or more POIs or other such information that may be provided by a map application in response to a query. Relevant metadata may be considered any type of data that includes information that may be responsive or otherwise material to the query. In some instances, theserver computing device 110 may retrieve relevant data from locations other thandatabase 301. For instance, theserver computing device 110 may visit a website of a POI related to the query and collect relevant information about the POI from the website. - The
server computing device 110 may determine whether the relevant metadata includes visually-verifiable metadata relevant to the retrieved data is available, as shown inblock 605. Identification of visually-verifiable metadata may include analyzing the relevant metadata to see if any of the relevant metadata is marked as being visually-verifiable. - In the event no visually-verifiable metadata is identified, the server computing system may provide the retrieved relevant metadata in response to the query, as shown in
block 609. The provided relevant data may include some or all of the retrieved relevant metadata. - In the event visually-verifiable metadata is identified within the retrieved relevant metadata, the visually-verifiable metadata may be prioritized, as shown in
block 607. For instance, a query may result in many relevant metadata items being retrieved. However, only some of the retrieved metadata items may be identified as visually-verifiable metadata. The visually-verifiable metadata items may be prioritized over relevant metadata items that are not visually-verifiable, as visually-verifiable metadata may boost the confidence of the information corresponding to the metadata being correct. Visually-verifiable metadata that is old relative to the rate of change of a place may not be prioritized as much as newer visually-verifiable metadata Other factors, such as relevance to a query, may also be used to prioritize the metadata for returning in response to the query. In this regard, although some relevant metadata may be visually-verifiable, a piece of metadata that is not visually-verifiable may be prioritized for being more relevant to the query. - The relevant metadata, including the visually-verifiable metadata may be provided in response to the query, as shown in
block 611. In some instances, the image or images associated with the visually-verifiable metadata may also be provided with the metadata, or may be provided in response to a subsequent request from the user or application. The visually-verifiable metadata may be annotated to bring a user's focus to the metadata. Annotation of the visually-verifiable metadata may include highlighting, circling, outlining, enlarging, or otherwise bringing attention and focus to the visually-verifiable metadata. - By providing the visually-verifiable metadata, the map application may return metadata that includes relevant information about one or more points of interest in response to the query. This metadata can be easily viewed and verified by a user. As a result, users may gain confidence that the map application, and the data it provides in response to queries, is accurate.
- Although the steps of flow diagram 600 are described herein as being performed by a server computing device, any computing device, including a client computing device, may execute the steps.
-
FIG. 7 illustrates an example use case in accordance with aspects of the disclosure. A user may enter an address query, “1103 Main St.,” into a map application on a user device, such ascomputing device 120. The map application may forward the request to a server computing device, such asserver computing device 110. Theserver computing device 110 may determine that a database, such asdatabase 301, includes visually-verifiable metadata corresponding to the number “1103” from animage 701 including the building located at 1103 Main St. Theserver computing device 110 may return the image along with the location where the visually-verifiable metadata was derived. The client computing device may output theimage 701 with thelocation 721, corresponding to the street number on the side of the building, being annotated with a circle and arrow, as shown inFIG. 7 . In some instances, theserver computing device 110 may return an annotated image with the visually-verifiable metadata being identified or otherwise highlighted to the client computing device. -
FIG. 8 illustrates another example use case where visually-verifiable metadata is outlined in response to a query. InFIG. 8 , image data including animage 801 with visually-verifiable metadata 809 is identified in response to a query for the address corresponding to a house. The visually-verifiable metadata 809 includes imagery of the house within theimage 801. The visuallyverifiable metadata 809 is outlined 810 and displayed or otherwise returned in response to the query. -
FIG. 9 illustrates an example use case where visually-verifiable metadata is displayed in a navigation application interface. Thenavigation application interface 901 includes adestination 902 and an image of thedestination 903. The image of thedestination 903 may include visually-verifiable metadata including ahouse 909 located at the address of thedestination 902 and ahouse number 919 of the house. Both pieces of visually-verifiable metadata, including thehouse 909 andhouse number 919 are annotated. In this regard, thehouse 909 is outlined 910 and thehouse number 920 is enlarged for easier viewing by the user. - The
image 903 may be processed in real-time. For example, theimage 903 may be a frame of a video feed from a camera system of a vehicle. An image recognition model may identify content and textual content within the frame in real-time and highlight some or all of the visually-verifiable metadata within the frame, such as thehouse 909 andhouse number 919, as shown inFIG. 9 . Processing may occur for each frame received from the video feed or at some predetermined time or number of frames. -
FIG. 10 illustrates another example use case where visually-verifiable metadata is provided in response to a query for a POI that satisfies a category. The query may be “Thai restaurant with delivery,” where the query includes a request for POIs (restaurants) that satisfy the categories of “Thai” and “delivery”. In response to the query, the system may identify a point of interest entry in the database that includes metadata that satisfies the categories of “Thai” restaurants and “delivery”. The POI entry may include animage 1001 with visually-verifiable metadata that shows therestaurant 1002 in theimage 1001 is a Thai restaurant and that it offers free delivery. The visually-verifiable metadata 1010 may be annotated 1012 and returned in response to the query. -
FIGS. 11A and 11B illustrate an example use case of the technology used within asearch interface 1101. As shown inFIG. 11A , a user may enter a query for an address “123 ABC St. NY, N.Y.” 1103 in asearch bar 1102. In response to the query, thesearch interface 1101 may provide animage 1110 of thePOI 1120 located at theaddress 1103 searched in the query as well as the location of the address on amap 1180, as indicated bypin 1181. - To provide confidence to the user that the
image 1110 includes thePOI 1120, and that the location of the address indicated on the map is accurate, the search interface may provide selectable input, including 1111 and 1112. Theseselectable inputs Selectable inputs - Upon receiving a selection of one of the selectable icons, the visually-verifiable metadata may be highlighted. For example, and as shown in
FIG. 11B , upon a user selectingselectable icon search interface 1101 may annotate the visually-verifiable metadata, including thehouse number 1190 forPOI 1120 in theimage 1110. In this example, the house number ofPOI 1120 is enlarged to make it easier for the user to see and to confirm that thePOI 1120 is located at theaddress 1103 entered in the query. - Although the foregoing examples illustrated in
FIGS. 7-12 describe the identification of visual-verifiable metadata as being performed by a server computing device, any computing device may perform these steps. For example, a map application executing on a client computing device, such asclient computing device 120, may include a database, such asdatabase 301. As such, the map application may access the database directly, without requiring the use of a server computing system. - The technology described herein is advantageous because it provides map applications with the ability to source and provides users with content that can be verified visually. By doing such, users are assured that the metadata provided in response to a query is accurate without the need for a user to manually confirm such results. Furthermore, this is done either automatically as part of a response to a single search query (e.g. as in
FIGS. 7-10 ) or through selectable inputs which allow the user to cause additional visual information to be provided on demand at the same time as viewing search results (e.g. through selection of theselectable icon FIGS. 11A-11B , which may be selectable with e.g. a single touch or click). This makes it quick and easy for the user to interact with the application to cause relevant additional visual information about a point of interest to be provided. - The presence of visual verification, and its use in ranking results, may also incentivize users of the map application, as well as owners of POIs, such as merchants, to provide image data for POIs, to assure correct, and visually-verifiable information is provided for POIs. This is particularly true when that evidence must be gathered in places that are difficult to access or not open to the general public, such as indoor spaces belonging to a business.
- Most of the foregoing alternative examples are not mutually exclusive, but may be implemented in various combinations to achieve unique advantages. As these and other variations and combinations of the features discussed above can be utilized without departing from the subject matter defined by the claims, the foregoing description of the embodiments should be taken by way of illustration rather than by way of limitation of the subject matter defined by the claims. As an example, the preceding operations do not have to be performed in the precise order described above. Rather, various steps can be handled in a different order, such as reversed, or simultaneously. Steps can also be omitted unless otherwise stated. In addition, the provision of the examples described herein, as well as clauses phrased as “such as,” “including” and the like, should not be interpreted as limiting the subject matter of the claims to the specific examples; rather, the examples are intended to illustrate only one of many possible embodiments. Further, the same reference numbers in different drawings can identify the same or similar elements.
Claims (20)
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2020/067300 WO2022146418A1 (en) | 2020-12-29 | 2020-12-29 | Method, system and computer-readable medum for providing search results with visually-verifiable metadata |
Publications (1)
Publication Number | Publication Date |
---|---|
US20230044871A1 true US20230044871A1 (en) | 2023-02-09 |
Family
ID=74195199
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/622,067 Pending US20230044871A1 (en) | 2020-12-29 | 2020-12-29 | Search Results With Result-Relevant Highlighting |
Country Status (2)
Country | Link |
---|---|
US (1) | US20230044871A1 (en) |
WO (1) | WO2022146418A1 (en) |
Citations (12)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20060041375A1 (en) * | 2004-08-19 | 2006-02-23 | Geographic Data Technology, Inc. | Automated georeferencing of digitized map images |
US20080268876A1 (en) * | 2007-04-24 | 2008-10-30 | Natasha Gelfand | Method, Device, Mobile Terminal, and Computer Program Product for a Point of Interest Based Scheme for Improving Mobile Visual Searching Functionalities |
US20100225665A1 (en) * | 2009-03-03 | 2010-09-09 | Microsoft Corporation | Map aggregation |
US8265400B2 (en) * | 2010-06-18 | 2012-09-11 | Google Inc. | Identifying establishments in images |
US8566325B1 (en) * | 2010-12-23 | 2013-10-22 | Google Inc. | Building search by contents |
US20140280230A1 (en) * | 2013-03-13 | 2014-09-18 | Qualcomm Incorporated | Hierarchical orchestration of data providers for the retrieval of point of interest metadata |
US20150186414A1 (en) * | 2012-07-18 | 2015-07-02 | Jonah Jones | Automatic meta-neighborhood and annotation generation for maps |
US20160019704A1 (en) * | 2014-07-17 | 2016-01-21 | Baidu Online Network Technology (Beijing) Co., Ltd | Method and apparatus for displaying point of interest |
US20160366240A1 (en) * | 2015-06-10 | 2016-12-15 | Ricoh Company, Ltd. | Offline mobile capture |
US20180300341A1 (en) * | 2017-04-18 | 2018-10-18 | International Business Machines Corporation | Systems and methods for identification of establishments captured in street-level images |
US20180307707A1 (en) * | 2013-04-25 | 2018-10-25 | Google Inc. | System and method for presenting condition-specific geographic imagery |
US20200300656A1 (en) * | 2019-03-24 | 2020-09-24 | Apple Inc. | Systems and methods for resolving points of interest on maps |
Family Cites Families (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN104121910A (en) * | 2013-04-28 | 2014-10-29 | 腾讯科技（深圳）有限公司 | Navigation method, device, terminal, server and system |
-
2020
- 2020-12-29 US US17/622,067 patent/US20230044871A1/en active Pending
- 2020-12-29 WO PCT/US2020/067300 patent/WO2022146418A1/en active Application Filing
Patent Citations (14)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20060041375A1 (en) * | 2004-08-19 | 2006-02-23 | Geographic Data Technology, Inc. | Automated georeferencing of digitized map images |
US20080268876A1 (en) * | 2007-04-24 | 2008-10-30 | Natasha Gelfand | Method, Device, Mobile Terminal, and Computer Program Product for a Point of Interest Based Scheme for Improving Mobile Visual Searching Functionalities |
US20100225665A1 (en) * | 2009-03-03 | 2010-09-09 | Microsoft Corporation | Map aggregation |
US8265400B2 (en) * | 2010-06-18 | 2012-09-11 | Google Inc. | Identifying establishments in images |
US8566325B1 (en) * | 2010-12-23 | 2013-10-22 | Google Inc. | Building search by contents |
US9171011B1 (en) * | 2010-12-23 | 2015-10-27 | Google Inc. | Building search by contents |
US20150186414A1 (en) * | 2012-07-18 | 2015-07-02 | Jonah Jones | Automatic meta-neighborhood and annotation generation for maps |
US20140280230A1 (en) * | 2013-03-13 | 2014-09-18 | Qualcomm Incorporated | Hierarchical orchestration of data providers for the retrieval of point of interest metadata |
US20180307707A1 (en) * | 2013-04-25 | 2018-10-25 | Google Inc. | System and method for presenting condition-specific geographic imagery |
US20160019704A1 (en) * | 2014-07-17 | 2016-01-21 | Baidu Online Network Technology (Beijing) Co., Ltd | Method and apparatus for displaying point of interest |
US9710946B2 (en) * | 2014-07-17 | 2017-07-18 | Baidu Online Network Technology (Beijing) Co., Ltd | Method and apparatus for displaying point of interest |
US20160366240A1 (en) * | 2015-06-10 | 2016-12-15 | Ricoh Company, Ltd. | Offline mobile capture |
US20180300341A1 (en) * | 2017-04-18 | 2018-10-18 | International Business Machines Corporation | Systems and methods for identification of establishments captured in street-level images |
US20200300656A1 (en) * | 2019-03-24 | 2020-09-24 | Apple Inc. | Systems and methods for resolving points of interest on maps |
Also Published As
Publication number | Publication date |
---|---|
WO2022146418A1 (en) | 2022-07-07 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8873857B2 (en) | Mobile image search and indexing system and method | |
US8447792B2 (en) | System and method for presenting user generated geo-located objects | |
US7904483B2 (en) | System and method for presenting geo-located objects | |
US20170286493A1 (en) | Hybrid Use of Location Sensor Data And Visual Query to Return Local Listings For Visual Query | |
US9497581B2 (en) | Incident reporting | |
US9171011B1 (en) | Building search by contents | |
US8051089B2 (en) | Systems and methods for location-based real estate service | |
US8266132B2 (en) | Map aggregation | |
US20110276556A1 (en) | Computer-implemented method for providing location related content to a mobile device | |
US10606824B1 (en) | Update service in a distributed environment | |
JP5608680B2 (en) | Mobile image retrieval and indexing system and method | |
US9437004B2 (en) | Surfacing notable changes occurring at locations over time | |
Zhang et al. | Terrafly geocloud: an online spatial data analysis and visualization system | |
US9208171B1 (en) | Geographically locating and posing images in a large-scale image repository and processing framework | |
EP4254222A2 (en) | Visual menu | |
US20230044871A1 (en) | Search Results With Result-Relevant Highlighting | |
US9230366B1 (en) | Identification of dynamic objects based on depth data | |
US20150379040A1 (en) | Generating automated tours of geographic-location related features | |
US20230417569A1 (en) | Updating Map Data With User-Generated Image Data | |
Deeksha et al. | A spatial clustering approach for efficient landmark discovery using geo-tagged photos | |
WO2024010569A1 (en) | Cluster search on maps applications | |
US11526569B2 (en) | Generating directions and routes with personalized landmarks | |
Skjønsberg | Ranking Mechanisms for Image Retrieval based on Coordinates, Perspective, and Area | |
Christodoulakis et al. | Semantic maps and mobile context capturing for picture content visualization and management of picture databases | |
EID et al. | Image Retrieval based on Reverse Geocoding |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:BREWINGTON, BRIAN EDMOND;STAAF, CARL;REEL/FRAME:058468/0186Effective date: 20201230 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: FINAL REJECTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE AFTER FINAL ACTION FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: ADVISORY ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: FINAL REJECTION MAILED |
|
STCV | Information on status: appeal procedure |
Free format text: NOTICE OF APPEAL FILED |