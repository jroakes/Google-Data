CN115696049A - Micro video system, format and generation method - Google Patents
Micro video system, format and generation method Download PDFInfo
- Publication number
- CN115696049A CN115696049A CN202211391220.2A CN202211391220A CN115696049A CN 115696049 A CN115696049 A CN 115696049A CN 202211391220 A CN202211391220 A CN 202211391220A CN 115696049 A CN115696049 A CN 115696049A
- Authority
- CN
- China
- Prior art keywords
- video
- image
- micro
- frame
- motion
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/681—Motion detection
- H04N23/6811—Motion detection based on the image signal
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/02—Editing, e.g. varying the order of information signals recorded on, or reproduced from, record carriers
- G11B27/031—Electronic editing of digitised analogue information signals, e.g. audio or video signals
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/10—Indexing; Addressing; Timing or synchronising; Measuring tape travel
- G11B27/19—Indexing; Addressing; Timing or synchronising; Measuring tape travel by using information detectable on the record carrier
- G11B27/28—Indexing; Addressing; Timing or synchronising; Measuring tape travel by using information detectable on the record carrier by using information signals recorded by the same method as the main recording
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N1/00—Scanning, transmission or reproduction of documents or the like, e.g. facsimile transmission; Details thereof
- H04N1/21—Intermediate information storage
- H04N1/2104—Intermediate information storage for one or a few pictures
- H04N1/2112—Intermediate information storage for one or a few pictures using still video cameras
- H04N1/212—Motion video recording combined with still video recording
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N1/00—Scanning, transmission or reproduction of documents or the like, e.g. facsimile transmission; Details thereof
- H04N1/32—Circuits or arrangements for control or supervision between transmitter and receiver or between image input and image output device, e.g. between a still-image camera and its memory or between a still-image camera and a printer device
- H04N1/32101—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title
- H04N1/32128—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title attached to the image data, e.g. file header, transmitted message header, information on the same page or in the same computer file as the image
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/667—Camera operation mode switching, e.g. between still and video, sport and normal or high- and low-resolution modes
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/681—Motion detection
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/681—Motion detection
- H04N23/6812—Motion detection based on additional sensors, e.g. acceleration sensors
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/681—Motion detection
- H04N23/6815—Motion detection by distinguishing pan or tilt from motion
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/68—Control of cameras or camera modules for stable pick-up of the scene, e.g. compensating for camera body vibrations
- H04N23/682—Vibration or motion blur correction
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/60—Control of cameras or camera modules
- H04N23/69—Control of means for changing angle of the field of view, e.g. optical zoom objectives or electronic zooming
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/70—Circuitry for compensating brightness variation in the scene
- H04N23/73—Circuitry for compensating brightness variation in the scene by influencing the exposure time
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N5/00—Details of television systems
- H04N5/76—Television signal recording
- H04N5/765—Interface circuits between an apparatus for recording and another apparatus
- H04N5/77—Interface circuits between an apparatus for recording and another apparatus between a recording apparatus and a television camera
- H04N5/772—Interface circuits between an apparatus for recording and another apparatus between a recording apparatus and a television camera the recording apparatus and the television camera being placed in the same enclosure
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N2201/00—Indexing scheme relating to scanning, transmission or reproduction of documents or the like, and to details thereof
- H04N2201/32—Circuits or arrangements for control or supervision between transmitter and receiver or between image input and image output device, e.g. between a still-image camera and its memory or between a still-image camera and a printer device
- H04N2201/3201—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title
- H04N2201/3261—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title of multimedia information, e.g. a sound signal
- H04N2201/3264—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title of multimedia information, e.g. a sound signal of sound signals
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N2201/00—Indexing scheme relating to scanning, transmission or reproduction of documents or the like, and to details thereof
- H04N2201/32—Circuits or arrangements for control or supervision between transmitter and receiver or between image input and image output device, e.g. between a still-image camera and its memory or between a still-image camera and a printer device
- H04N2201/3201—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title
- H04N2201/3261—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title of multimedia information, e.g. a sound signal
- H04N2201/3267—Display, printing, storage or transmission of additional information, e.g. ID code, date and time or title of multimedia information, e.g. a sound signal of motion picture signals, e.g. video clip
Abstract
The present disclosure provides systems and methods for using and/or generating image files according to novel micro video image formats. For example, a micro-video may be a file containing both still images and short videos. The micro-video may include multiple tracks, such as, for example, a single video track, an audio track, and/or one or more metadata tracks. As one example track, a micro-video may include a motion data track that stores motion data that may be used (e.g., while a file is running) to stabilize video frames. A micro video generation system included in the image capture device may determine the cropping of the video on the fly as the image capture device captures the micro video.
Description
The application is a divisional application of a patent application with the application date of 2018, 07, 12 and the application number of 201880044253.5 and the invented name of 'micro video system, format and generation method'.
Technical Field
The present disclosure relates generally to the capture and representation (e.g., playback) of imagery including video. More particularly, the present disclosure relates to systems and methods for using and/or generating image files according to novel micro video image formats, and also to techniques for cropping micro videos.
Background
An image capture device is a device capable of capturing imagery (e.g., in the form of image frames). The image capture device includes a camera, recorder, sensor, and/or other device. In some cases, the image capture device may have a primary purpose other than capturing imagery. For example, the image capture device may comprise a "camera-enabled" device or a device having an image capture system embedded within the device, such as, for example, certain smart phones, laptops, smart appliances, smart speakers, home manager devices, security systems, and so forth. In some cases, the image capture device may be a mobile image capture device that is capable of movement and/or a wearable image capture device. Furthermore, computing systems that do not explicitly capture imagery themselves may still be used to view, edit, or display imagery captured by the image capture device.
Some existing image capture devices may include the following functions: the device is enabled to capture moving image clips (i.e., "movies") when the user operates the device to capture images. In particular, a short movie may include images captured within a predetermined amount of time (e.g., 1.5 seconds) before and after a user operates the device to capture the images (e.g., by selecting a physical or virtual button to instruct the device to capture the images). However, the short movie generally includes an extraneous image that degrades the quality of the moving image. For example, movies may include imagery corresponding to out-of-pocket motion (e.g., motion captured when a user removes a device from their pocket), sudden brightness changes, and/or undesirable camera zooming and rotation.
Another challenge faced in the design and use of certain forms of image capture devices (e.g., mobile image capture devices and/or wearable image capture devices) is the limited environmental resources in which they operate. In particular, the design and use of image capture devices is typically limited by: limited storage capacity for storing images over a significant period of time; the processing power or capacity for continuously processing images is limited; limited energy available for operation over long periods of time; and/or the thermal energy that may be dissipated is limited (i.e. the device temperature should not be exceeded to prevent the device from overheating or causing discomfort to the user wearing the mobile image capturing device in certain situations).
Disclosure of Invention
Aspects and advantages of embodiments of the present disclosure will be set forth in part in the description which follows, or may be learned by practice of the embodiments.
Example aspects of the present disclosure are directed to systems and methods for using and/or generating image files according to novel micro video image formats. For example, a micro-video may be a file containing both still images and short videos. The micro-video may include multiple tracks, such as, for example, a single video track, an audio track, and/or one or more metadata tracks. As one example track, a micro-video may include a motion data track that stores motion data that may be used (e.g., while a file is running) to stabilize video frames. A micro video generation system included in an image capture device may determine a crop of video on the fly as the image capture device captures the micro video.
These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate exemplary embodiments of the disclosure and together with the description, serve to explain the relevant principles.
Drawings
A detailed discussion of embodiments directed to one of ordinary skill in the art is set forth in the specification, which makes reference to the appended drawings, in which:
fig. 1 depicts a block diagram of an example image capture device, according to an example embodiment of the present disclosure.
Fig. 2 depicts a block diagram of an example micro video format, according to an example embodiment of the disclosure.
Fig. 3 depicts a diagram of a homography matrix between a pair of image frames according to an example embodiment of the present disclosure.
FIG. 4 depicts a diagram of an example file track, according to an example embodiment of the present disclosure.
Fig. 5 depicts a block diagram of an example micro video generation system, according to an example embodiment of the present disclosure.
Fig. 6 depicts a diagram of an example micro video cropping process, according to an example embodiment of the present disclosure.
Fig. 7 depicts a graph of an example relationship between camera speed and motion threshold, according to an example embodiment of the disclosure.
Fig. 8 depicts a block diagram of an example computing system, according to an example embodiment of the present disclosure.
Detailed Description
In general, the present disclosure is directed to systems and methods for using and/or generating image files according to novel micro video image formats. For example, a micro-video may be a file containing both still images and short videos. The image capture device may include a micro video generation system that generates a micro video file according to a micro video image format. Likewise, various types of computing systems, including those that do not themselves capture imagery, may include one or more image viewers that may open or otherwise access the micro video files to display the micro videos to a user.
Thus, according to one aspect, the present disclosure provides a novel micro video image file format that enables a single micro video file to contain both still images and brief video. For example, the video information may be concatenated with or otherwise appended to the still image information. In some implementations, the video portion can be included in a container (e.g., a JPEG container) associated with the still image portion.
A viewer supporting a still image format or a video format may open or otherwise access the micro video file. As such, the micro video format may enable a user to view still images or video (e.g., in a looping manner). As one example, the still image may be a JPEG image and the video may be MP4 format video. However, other formats may be used.
According to another aspect, the micro-video may include multiple tracks, such as, for example, a single video track, an audio track, and/or one or more metadata tracks. As one example track, a micro-video may include a motion data track that stores motion data that may be used (e.g., while a file is running) to stabilize video frames. For example, the motion data track may store one or more homography matrices (e.g., one or more matrices per frame) that may be used to stabilize the video.
In some implementations, each homography matrix may be calculated or otherwise derived from motion data received from one or more sensors, image sensors, and/or Image Signal Processors (ISPs) (e.g., when generating a micro-video). For example, the one or more sensors may include one or more gyroscopes, accelerometers, inertial Measurement Units (IMUs), or other motion sensing sensors that provide sensor data indicative of motion of the image sensor and/or the image capture device as a whole. As another example source of motion data, the image sensor and/or ISP may provide statistical or other data about the imagery, including exposure information, brightness information, zoom information, focus information, optical Image Stabilization (OIS) lens position, or other data describing image frames.
According to another aspect of the present disclosure, a micro video generation system included in an image capture device may determine a crop of a video on the fly as the image capture device captures the micro video. In particular, the micro video generation system may determine the best start and end frames and improve video quality. For example, by cropping the video, the system may remove imagery corresponding to scooping motion (e.g., motion captured when the user removes the device from their pocket), sudden brightness changes, undesirable camera scaling and rotation, blurred images, and/or other undesirable imagery, thereby improving video quality.
More specifically, in some implementations, a micro video generation system may include a buffer that stores a rolling window of image frames and motion data captured by a device. In some implementations, upon receiving a user control input (e.g., a user activating a "shutter" control), the micro video generation system may begin searching for a starting frame within the image frames stored in the buffer. For example, the micro-video generation system may search backwards from a "shutter frame" captured when a user control input is received.
Alternatively or additionally, in some embodiments, the micro video generation system may be operable to generate micro videos in an automated manner. For example, a shutter frame may be automatically selected when a particular trigger event occurs. For example, the presence of a particular object (e.g., detected by an object detector); a certain amount of motion within a scene; or other event that may be detected by the image capture device and trigger the automatic generation of a micro-video.
In some implementations, the micro video generation system can identify the starting frame by applying one or more criteria. As one example, the one or more criteria may include a motion criterion that determines whether a motion distance between the current frame under examination and the shutter frame is greater than a motion threshold. For example, the motion threshold may be dynamically determined based on the speed of motion of the image capture device, which enables the micro video generation system to handle cropping of panning shots (e.g., panoramas) in a manner different from still shots.
As another example, the one or more criteria may include a total sensitivity change criterion that determines whether a total sensitivity change between the current frame and the shutter frame exceeds a sensitivity threshold. For example, the total sensitivity may be a function of the sensor sensitivity and the exposure time, which may be retrieved from an ISP, for example. As yet another example, the one or more criteria may include a zoom criterion that determines whether the change in zoom is greater than a zoom threshold.
As another example, the one or more criteria may include a motion blur criterion that analyzes an amount of motion blur associated with the current frame. The motion blur criteria may be used to remove or otherwise begin or end the micro-video at or adjacent to a blurred frame caused by camera motion. As yet another example, the one or more criteria may include a focus criterion that analyzes autofocus data associated with the current frame. The focus criteria may be used to remove or otherwise begin or end the micro-video at or adjacent to the blurred frame caused by the lens being out of focus.
In some implementations, the micro-video generation system may select the first frame that meets a certain number (e.g., one, two, all, etc.) of one or more criteria (e.g., the three example criteria described above) as the starting frame. In some embodiments, if none of the frames in the buffer meet the certain number of criteria, the earliest frame in the buffer may be selected as the starting frame.
In some implementations, once the starting frame is determined, the micro video generation system may instruct or operate the encoder to encode the micro video starting from the starting frame. Before or after such encoding begins, the micro video generation system may begin searching for an end frame.
In some implementations, the micro video generation system may analyze each new frame as it is captured or otherwise available to the micro video generation system for analysis to determine whether such frame should be designated as an end frame. For example, to identify the end frame, the micro video generation system may apply the same or similar standard procedure as described above with respect to the selection of the starting frame. The same criteria may be used or different criteria may be used. The same or different thresholds may be used if the same criteria are used. In some implementations, a new frame a predetermined period of time after the shutter frame can be selected as the end frame if the new frame captured within a predetermined period of time (e.g., 1.5 seconds) after the time of the user-controlled event does not meet a certain number of criteria.
The encoder may encode the image frame until the end frame is reached. Thus, the duration of the micro-video may be from the start frame to the end frame.
In some implementations, the micro video generation system may check the micro video duration against a minimum duration threshold (e.g., 1 second) because the user may not expect the micro video to be very short and not very harmonious (jarring). In some implementations, the system can cancel the micro-video if the micro-video duration is less than a minimum duration threshold. For example, still photographs may be stored without storing micro-videos. In other embodiments, micro-videos with durations less than a minimum duration threshold may still be saved. In other embodiments, additional frames may be added to the beginning and/or end of the micro-video until a minimum duration threshold is reached. In some implementations, a user may be provided with control over how the micro-video generation system processes micro-videos having a duration less than a minimum duration threshold.
In some implementations, the shutter frame can be used as a still photograph of the micro video. In other embodiments, various algorithms may be used to identify the highest quality, most representative of the entire video, most desirable and/or most unique image frames from the video as still photographs of the micro-video.
In some implementations, the micro video generation system may generate multiple micro video files in parallel. For example, multiple parallel instances of a micro video generation system may be implemented in response to a user providing user control input in a quick, repetitive manner (such as collecting a series of images). In at least some implementations, since the start and end cropping decisions are based on the characteristics of the shutter frames, multiple parallel instances of these micro-video cropping and generation can result in micro-videos of different durations, different start frames, different end frames, and thus different content.
The system and method of the present disclosure provide a number of technical effects and benefits. For example, the micro video image format may allow only a single file to be stored from which still images and video may be viewed independently, whereas in a conventional file format, two files may be required to achieve this. Storing a single file that allows viewing of both still images and video may require less storage capacity than storing two separate files. Note that the still image may not be viewed as a thumbnail (or one of the other compressed version of the video frame), but rather at the same or higher resolution as the video frame.
As another example, the micro video format can store motion data in a separate track for later use to stabilize the video (e.g., at runtime). This is in contrast to alternative techniques that use motion data to stabilize the video at the time of initial processing and storage. This allows the overall processing requirements to be reduced because many users do not view every image/video they capture (or even most of the images/videos they capture). Thus, the cost of performing stabilization techniques is avoided for each micro-video that the user does not ultimately view.
Another example technical effect and benefit derived from a single motion data track is particularly applicable to image capture devices having resource constrained operating environments. For example, a user may capture a micro video using an image capture device, but typically view the micro video on a different device with fewer resource constraints. In this case, the resource requirements for performing the stabilization process on the micro video may be shifted from the image capturing device to another device with fewer resource constraints, thereby improving the performance of the image capturing device.
In addition to the above technical effects and benefits, in an image capture device having a resource constrained operating environment, shifting stabilization processing requirements from an initial capture time to a later point in time enables more resources to be dedicated to the image capture components of the device at the capture time, which typically requires a large amount of resources, such as power. Furthermore, the device can simply stream image data to the encoder, and does not need to present (surface) data or convert data between different formats while the image capture component is operating.
As another example of technical effects and benefits, maintaining a separate motion trajectory can preserve motion data for later use in cases where enhanced stabilization techniques are developed.
As another example technical effect and benefit, because raw motion data (e.g., raw sensor data) may be specific to a particular device and/or sensor type, converting raw motion data into a homography matrix improves consistency of the motion data and provides a standardized motion metric that enables general processing. Furthermore, converting raw motion data into homography matrices may improve user privacy.
As another example technical effect and benefit, in some implementations, the micro video cropping techniques described herein do not require a full video to begin the cropping process. Instead, the micro video generation system may operate on-the-fly. In particular, the system only requires past buffered frames and can start immediately after receiving user control input. As a result, the micro video generation system may increase the speed of the computing process and generate micro videos within a reduced time frame. As another related result, the micro video generation system does not require a large buffer to buffer the entire video, but only the portion before the user control input. Thus, the micro video pruning techniques described herein reduce the storage requirements of the device.
As another example technical effect and benefit, in some embodiments, the micro video cropping techniques described herein use sensor and/or ISP data rather than performing analysis on the image itself. As a result, the pruning technique has relatively low computational complexity, thereby reducing the need for processing resources and increasing the speed at which the technique can be performed. In particular, the low computational complexity due to the use of sensor and/or ISP data enables the process to run much faster than methods using image analysis. In fact, the complexity is low enough that the system can achieve real-time performance.
As another example technical effect and benefit, in some embodiments, the micro video generation system crops differently for still shots and pan shots. For example, micro-video may retain more content in panning situations and crop more strictly static scenes. The system can handle both panning situations and static scene situations and can perform smooth operational transitions between the two situations. Therefore, the responsiveness of the image capturing apparatus to the environment of image capturing can be improved. In addition, the system can discard (drop) the micro-video that is too short after cropping or extend it to a minimum length.
As a further example, experimental testing of an example micro video generation system provides the following example results: the trimmer clips the micro video average duration from 3s to 2.4s, which saves 20% storage and power. In addition, the micro-video that is too short after cropping may be discarded. Log records show that the system discards 33% of the short micro-videos after pruning, thereby saving storage space by 18% again.
Referring now to the drawings, example embodiments of the disclosure will be discussed in further detail.
Fig. 1 depicts a block diagram of an example image capture device 102, according to an example embodiment of the present disclosure. Image capture device 102 may include an image sensor 120 and an image signal processor 122 that operate together to generate frames of imagery. The image sensor 120 is operable to capture raw image data. The image signal processor 122 may process the raw image data to form an image frame. In some embodiments, the image signal processor 122 may comprise a dedicated digital signal processor.
In some implementations, the image signal processor 122 may include one or more hardware blocks operable to facilitate formation of image frames. Example hardware blocks that may be included in the image signal processor 122 include: an original filter block; a bad pixel correction block; an original denoising block; a black level (flash) correction block; a lens shading correction block; a white balance block; a demosaicer block; a Bayer conversion block; removing mosaic blocks; a color correction block; a gamma correction block; a tone mapping block; and/or a color space conversion block. Image signal processor 122 may also include various other components, blocks, and/or subsystems that facilitate forming image(s) based on raw image data captured by image sensor 120.
The image capture device 102 may also include one or more sensors 128. As an example, the sensor(s) 128 may include one or more gyroscopes, accelerometers, magnetometers, inertial Measurement Units (IMUs), or other motion sensing sensors that provide sensor data indicative of motion of the image sensor 128 and/or of the image capture device 102 as a whole. The sensor 128 may be an electronic device that measures and reports one or more of velocity, orientation, and gravity applied to the image capture device 102 or otherwise associated with the image capture device 102. In some embodiments, data from one or more sensors 128 or motion data derived therefrom (e.g., homography matrix) may be appended to images captured when the sensors 128 generate such data (e.g., in separate motion data tracks) or may be used to crop micro-videos, as will be discussed further below. Such information may also be used by downstream processes, for example, when selecting (or rejecting) a particular image due to blurring, motion, or other undesirable attributes due to accidental movement, jostling, or other physical disturbances of the image capture device 102 (e.g., physical disturbances caused by a user's gait). Thus, the image capture timing may be based on data from the sensor 128.
In some implementations, aspects of the present disclosure may also use near-free signals generated from the sensor 128, the image sensor 120, and/or the image signal processor 122. In particular, these near-free signals (such as hardware-generated statistics) are typically generated anyway for performing auto-exposure, auto-white balance and/or auto-focus during the image forming stage.
An example near-free signal that may be used by the image capture device 102 includes metadata or settings that are freely available from image capture parameters. Example metadata includes processing pipeline metadata and sensor metadata. Example settings include exposure time, analog gain, and digital gain. Additional near-free signals include sensor signals, such as Inertial Motion Unit (IMU) signals (e.g., angular velocity described by one or more IMUs).
Additional example hardware-generated statistics include thumbnails, color histograms, luminance histograms, and/or high frequency maps, which are readily available with only a small amount of marginal energy. The auto-exposure, auto-white balance, and/or auto-focus statistics may include auto-focus spot filter sums, auto-white balance spot pixel sums, and the like.
The image capturing apparatus may further include a shutter controller 130. Shutter controller 130 may control when image data is collected, stored, and/or processed. For example, shutter controller 130 may operate to control image capture device 102 in response to a user control input. For example, the user control input may include selecting a physical or virtual shutter control button.
According to aspects of the present disclosure, the image capture device may also include a micro video generation system 124 that generates micro video. The micro video generation system 124 may generate image files according to the novel micro video image formats described herein. For example, a micro-video may be a file containing both still images and short videos. One example micro video generation system 124 will be discussed in more detail with reference to fig. 5. An example micro video format is described with reference to fig. 2-4.
The micro-video generated by the micro-video generation system 124 may be stored in one or more storage devices 126. The storage device 126 can include one or more of a variety of different types of memory, including volatile and/or non-volatile memory. Example storage devices 126 include RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof.
Fig. 2 depicts a block diagram of an example micro video format, according to an example embodiment of the disclosure. In some embodiments, the micro-video may have the format < jpeg _ bytes > < mp4_ bytes >, with an extension of.jpg. In some embodiments, the mp4 byte may be appended directly to the end of the compressed jpeg. In some embodiments, the XMP tag and filename formats described below may be used.
In some embodiments, the XMP tag may include some or all of the following example fields:
in some implementations, the micro-video may follow a particular file name format to be recognized as a micro-video by some systems. For example, a photo storage application may quickly recognize a micro-video and display the appropriate markers. In some cases, if the filename format does not match the specification, the client may not attempt to parse the XMP from the file, but rather treat it as simple JPEG. One example file name format is as follows: MVIMG _ [ a-zA-Z0-9_ + (JPG | JPG | JPEG | JPEG).
In some embodiments, the micro-video may include a video track, and optionally an audio track. Optionally, the MPEG4 data may also include two additional metadata tracks specifying how to stabilize playback of the video frames. For example, these metadata tracks may be mett tracks (e.g., MPEG4 atom oov. Trak. Mdia. Hdlr. Handlertype = "mett").
The track providing the conversion information on how to render the frames may have a MIME type "application/micro video-metadata-stream". This may be specified via the TextMetaDataSampleEntry defined in section 12.3.3.2 of ISOBMFF to signal the mime format of the metadata. Each frame in the track may correspond to a video frame and may contain a serialization Protocol Buffer specified by the following example Protocol Buffer definition:
each homography in data homographic data may represent a conversion from a previous frame to a current frame. For pixel (x 1, y 1) on the current frame, the matched pixel (x 0, y 0) on the previous frame may satisfy the homographic transformation [ x1, y1,1 ]] T ＝H*[x0,y0,1] T 。
In some embodiments, each frame is divided into a plurality of portions. For example, the portions may be bands (bands) along the scan line direction. In one example, 12 bands are defined for each frame. Each portion (e.g., band) may have a homographic transformation. Therefore, in the exemplary embodiment using 12 bands along the scan line direction, there are 12 homography matrices H1 to H12, as shown in fig. 3. In particular, fig. 3 depicts a graph of an example homography matrix between a pair of image frames in accordance with an example embodiment of the present disclosure.
FIG. 4 depicts a diagram of an example file track, according to an example embodiment of the present disclosure.
In some implementations, although a video track may have unordered frames (e.g., B-frames), the metadata track must be ordered and have timestamps in increasing order. For each timestamp in a video track, there may be frames in the metadata track that have the same timestamp.
In some embodiments, there is an is _ key _ frame field in proto to indicate whether the frame itself is a key frame. Key frames may also be referred to as "I frames" (or "intra frames"). The information may come from the encoder (e.g., bufferinfo. FLAGs may set BUFFER _ FLAG _ KEY _ FRAME for the KEY FRAME if coded using MediaCodec). In some embodiments, this information may be used to assist in pruning. For example, if it is desired to crop an initial frame without transcoding, the clip may be limited to proceeding on a key frame.
Another example metadata track may contain information about whether the video should be stabilized at the back end, and a timestamp of the frame at which the video should begin playback of the animation before looping back to the beginning and continuing the video loop. The track may have mime-type "application/micro video-image-metadata" and may contain a single frame with a presentation timestamp (e.g., the timestamp of the first video frame) corresponding to any video frame. In some embodiments, the only limitation is that the timestamps must have a value between the first and last video frame timestamps (including the first and last video frame timestamps). The frame data may contain a serialization protocol buffer, for example, as shown in the following example:
as one example, when a micro video file is uploaded to the photo application backend, it may be subject to additional image stabilization. The do _ not _ stable field allows the developer to specify that the video content should not undergo any additional stabilization processing.
Fig. 5 depicts a block diagram of an example micro video generation system 500, according to an example embodiment of the present disclosure. The micro video generation system 500 may include a circular buffer 502, a trimmer 504, and an encoder 506. The micro video generation system 500 may generate a micro video 508.
More specifically, in some embodiments, it may be assumed that the micro-video is captured at most M seconds before the shutter and at most N seconds after the shutter. In some embodiments, as one example, M =1.5 and n =1.5. Other values (e.g., 2 seconds, 3 seconds, etc.) may also be used, and M need not be equal to N. Fig. 5 is a high level diagram of a pruning method, which in some embodiments includes the following steps. The video frames and sensor/ISP data from sensor 120, processor 122 and sensor(s) 128 may first be fed into a ring buffer 502. The trimmer 504 may be triggered by the shutter control 130 (e.g., in response to a shutter press). The pruner 504 may determine the start/end frames of the micro video and send their timestamps to the encoder/multiplexer 506 to create the micro video 508 within that range.
In some implementations, the video frames and sensor/ISP data may be buffered in the ring buffer 502 for M seconds at all times during the preview. Buffer 502 may include or be implemented in one or more different types of computer-readable storage devices, including, for example, RAM.
Whenever shutter controller 130 indicates that a user control input (e.g., a shutter button press) is received, the current frame may be selected as a shutter frame, which may be denoted as F _ shutter.
As shown in fig. 6, all frames and sensor/ISP data within M seconds before the shutter are already in the ring buffer so that they can be accessed immediately (e.g., by the trimmer 504). All frames and sensor/ISP data after the user control event are not yet available.
In some embodiments, the micro video generation system 500 may start a new instance of the trimmer 504 immediately after the shutter controller 130 indicates that user input is received to determine the best starting frame. Multiple trimmers 504 can be run simultaneously (e.g., if the shutter button is clicked multiple times during a short period of time, such as in the case of button shuffling). The start frame F _ start should always be above or before the shutter frame (F _ start < = F _ shuter).
In some implementations, the clipper 504 can move the current frame backward from the shutter frame to find the best starting frame for the micro video 508. If the current frame, denoted F _ current, satisfies some number (e.g., one, two, all, etc.) of one or more criteria, it may be set as the starting frame. Three examples of criteria that may be selected for use are as follows.
Example criteria 1: the criterion checks whether the camera motion distance from the current frame to the shutter frame is greater than a threshold T _ motion. This criterion can be used to trim out unwanted camera motion, such as taking the phone out of a pocket or putting it back. A large translational or rotational movement results in a large movement distance and thus is trimmed.
Example criterion 1 may be evaluated according to the following equation: max _ i | (x _ i (F _ current), y _ i (F _ current)) - (x _ i (F _ shutter), y _ i (F _ shutter)) | > T _ motion, where | | | | | is the L2 distance. (x _ i (F _ shutterer), y _ i (F _ shutterer)), i =1.. 4, representing 4 corners of the shutter frame. They may be (0,0), (width-1,0), (height-1,0), (width-1, height-1). (x _ i (F _ current), y _ i (F _ current)), i =1.. 4, representing 4 corners of the current frame in the coordinates of the shutter frame. They may be calculated by the conversion between the current frame and the shutter frame, which may be derived from the sensor data and the ISP data.
In some implementations, the threshold T _ motion can be adaptively determined based on the camera motion speed V near the shutter frame. The speed V may be calculated from the camera movement distance described in the above expression divided by the time between the current frame and the shutter frame. In some implementations, the velocities can be averaged for the K frames closest to the shutter frame. As an example, K =6.
In some embodiments, the adaptive threshold T _ motion may be a piecewise linear function of the camera motion speed V, as shown in fig. 7. Thus, as an example, if V < V1, T _ motion = T _ small; if V > V2, T _ motion = T _ large; if T _ motion = T _ small + (V-V1) (T _ large-T _ small)/(V2-V1). This piecewise linear function may lead to the following results: if the camera movement speed is slow, it is likely to be still photography, and the threshold T _ motion may be set to a small value. Since the threshold T _ motion is small, the clipper 504 will only hold frames that move a little from the shutter frame to produce good still micro video. In contrast, if the camera movement speed is fast, panning is likely, and the threshold T _ motion may be set to a large value. Since the threshold T _ motion is large, the trimmer will hold the frame even if the camera moves far enough away from the shutter frame to produce good panned micro video. There is also a smooth transition between these two cases.
The piecewise linear function is provided as an example only. Other relationships between the camera motion speed V and the motion threshold T _ motion may also be used, including linear, logarithmic, and/or exponential relationships.
Example standard 2: the criterion checks whether the total sensitivity variation between the current frame and the shutter frame is greater than a threshold. This criterion can be used to eliminate sudden brightness changes in the micro-video.
Example criterion 2 may be evaluated according to the following equation: abs (total _ sensitivity _ current-total _ sensitivity _ shtter) > T _ total _ sensitivity. The total sensitivity may be defined as total _ sensitivity = sensor _ sensitivity _ exposure _ time. Both sensor _ sensitivity and exposure _ time are available from ISP data that can be retrieved directly from the camera hardware.
Example criteria 3: the criteria checks whether the camera zoom factor change between the current frame and the shutter frame is greater than a threshold. This criterion can be used to remove unwanted scaling in the micro-video.
Example criterion 3 may be evaluated according to the following equation: abs (zoom _ current-zoom _ shot) > T _ zoom.
Example standard 4: the one or more criteria may include a motion blur criterion that analyzes an amount of motion blur associated with the current frame. The motion blur criteria may be used to remove or otherwise begin or end the micro-video at or adjacent to a blurred frame caused by camera motion.
Example criterion 4 may be evaluated according to the following equation: motion _ blu > T _ motion _ blu, where T _ motion _ blu is a motion blur threshold. motion _ blu can be defined as: motion _ blu = V _ current _ exposure _ time, where V _ current is the current speed of camera motion of the current frame. V current may be derived from, for example, gyroscope sensor data and/or ISP data.
Example criteria 5: the one or more criteria may include a focus criterion that analyzes autofocus data associated with the current frame. The focus criteria may be used to remove or otherwise begin or end the micro-video at or adjacent to the blurred frame caused by the lens being out of focus. The autofocus data for each frame may be obtained from the ISP. For example, the autofocus data may include an autofocus state. In some embodiments, this criterion may be met when the autofocus status of the current frame is "unfocused".
In some embodiments, if the trimmer 504 arrives M seconds before the shutter frame while not meeting some number or any of the one or more criteria, the start frame may be set to M seconds before the shutter frame, i.e., F _ start = F _ cut-M.
In some implementations, once the best start frame is determined, the encoder 506 will encode the micro video 508 starting from the start frame. The pruner 504 then needs to determine the best end frame to stop the encoder 506.
In some implementations, for each new frame that comes after shutter controller 130 indicates a user input, trimmer 504 can check whether the micro video should end at that frame. The pruner 504 may set the new frame as the current frame and use the same or similar process as used to determine the starting frame. For example, the same or similar criteria may be used. The same or different thresholds may be used if the same criteria are used. Some number of criteria that must be met may be the same or different. The pruner 504 may set the current frame as the end frame if the current frame meets a certain number of the one or more criteria.
In some embodiments, if the trimmer 504 reaches a frame N seconds after the shutter frame while not meeting some number or any of the criteria, the trimmer 504 may set the frame as the end frame, i.e., F _ end = F _ cutout + N
In some implementations, once the end frame is determined, the system 500 can stop the encoder 506 at the end frame. The duration of the micro-video 508 will be from the start frame to the end frame, i.e., duration = F _ end-F _ start.
Often users do not want to see very short micro-videos because they look less harmonious. The system 500 may check the micro-video duration against a minimum duration threshold T _ duration. As one example, the T _ duration may be 1 second. If duration > = T _ duration, the system 500 may store the micro video 508. However, if the duration < T _ duration, the system 500 may continue with a number of different options. In a first option, the micro-video may be cancelled: no micro-video is saved and only still photos are saved. In the second option, F _ end may be set equal to F _ start + T _ duration. The micro-video with T _ duration can be saved even if cropping is not optimal. In a third option, frames may be added to the beginning and/or end until the T _ duration threshold is met. The user may be provided with control to decide which option to use (no micro-video or micro-video with non-optimal cropping) according to their preferences.
FIG. 8 depicts an example image capture, management (curing) and editing system 100 according to an example embodiment of the present disclosure. The system 100 is provided as only one example system. The systems, methods, techniques, and principles of the present disclosure may be applied and applied to many different systems in addition to or in lieu of the exemplary system 100. For example, in some implementations, the image capture device 102 does not join the network 180 of the user computing device 135 and/or the server computing device 150.
The example system 100 includes at least one image capture device 102 and a user computing device 135. In some implementations, the image capture device 102 may be mobile and/or wearable. For example, the image capture device 102 may be a smartphone, tablet computer, or other similar device. As another example, the image capture device 102 can be an embedded device and/or a smart device (e.g., a smart appliance, a smart speaker, a home manager device, an auxiliary device, a security system, etc.). The system 100 may also include one or more additional image capture devices and/or server computing devices 150.
Example image capture device 102 may also include a lens cover 116, one or more lenses 118, and an image sensor 120. Image sensor 120 may be a sensor that detects incident light or other electromagnetic radiation and outputs data sufficient to generate an image of a scene. For example, the image sensor 120 may be a CMOS sensor or a CCD sensor. In some implementations, the one or more lenses 118 may include a wide-angle lens such that the image produced by the data output by the image sensor 120 is a wide-angle image.
As described above, the image sensor 120 may output data sufficient to generate an image of a scene viewed by the image sensor 120. Image capture device 102 may include various additional components for processing such data from image sensor 120 to generate such images. As one example, the image capture device 102 may include an image signal processor 122. The image signal processor 122 may include one or more image processing components operable to process raw data from the image sensor 120 to form image frames.
As described with reference to fig. 1, the image capture device 102 also includes a micro-video generation system 124, one or more storage devices 126, one or more sensors 128, and a shutter controller 130.
The image capture device 102 also includes a network interface 132. The network interface 132 may include any number of components to provide networked communication (e.g., transceivers, antennas, controllers, cards, etc.). In some implementations, the image capture device 102 includes a first network interface operable to communicate using a short-range wireless protocol (such as, for example, bluetooth and/or bluetooth low energy), and also includes a second network interface operable to communicate using other wireless network protocols (such as, for example, wi-Fi). In one example, the image capture devices are operable to communicate with each other using a short-range wireless protocol (e.g., bluetooth low energy). Further, the image capture device 102 may be operable to communicate with the user computing device 135 using a short-range wireless protocol (e.g., sending captured visualizations) or Wi-Fi or other more robust network protocol (e.g., for transmitting captured images).
The image capture device 102 may also include a power supply 134. As one example, power source 134 may be an onboard battery, such as an onboard lithium ion battery. The image capture device 102 may also be electrically connected (e.g., via a micro-USB port or other electrical port and/or data connection port) to a wall outlet or other utility or other source of appropriate power rating. Plugging the image capture device 102 into a wall outlet may recharge the onboard battery. In some implementations, the image capture device 102 is configured to transmit images to the user computing device 135 or perform other high-energy-consuming tasks over the network 180 only when the image capture device 102 is connected to an external power source (such as a wall outlet).
The system 100 may also include a user computing device 135 and a server computing device 150. In some implementations, the image capture device 102 is communicatively connected to the user computing device 135 through a local area network portion of the network 180, and the user computing device 135 is communicatively connected to the server computing device 150 through a wide area network portion of the network 180. In other implementations, the image capture device 102 is directly communicatively connected to the server computing device 150 through a wide area network.
In some implementations, the user computing device 135 can perform image management and allow the user to edit the images. In particular, in some implementations, when connected to the image capture device 102 (e.g., after a capture session has been completed), the user computing device 135 may select certain images stored in the image capture device for transfer to the user computing device 135. The user computing device 135 may also provide a user interface that enables a user to selectively edit the images to be transferred. In particular, the user interface may enable various advanced image editing techniques, such as calculating photography, camera repositioning, and the like.
The user computing device 135 may be, for example, a computing device having a processor 136 and a memory 137, such as a wireless mobile device, a Personal Digital Assistant (PDA), a smartphone, a tablet, a laptop, a desktop computer, a computing-enabled watch, computing-enabled glasses, or other such device/system. Briefly, the user computing device 135 may be any computer, device, or system capable of interacting with the image capture device 102 and (e.g., by sending and receiving data) to implement the present disclosure.
The processor 136 of the user computing device 135 may be any suitable processing device and may be an operably connected processor or processors. Memory 137 may include any number of computer readable instructions 139 or other stored data 138. In particular, the instructions 139 stored in the memory 137 may include one or more applications. When implemented by the processor 136, the one or more applications may respectively cause or instruct the processor 136 to perform operations consistent with the present disclosure, such as, for example, executing image storage, management, editing, and sharing applications. The memory 137 may also store any number of images captured by the image capture device 102 and/or the user computing device 135.
The user computing device 135 may also include a display. The display may be any of a number of different technologies for displaying information to a user, including touch-sensitive display technologies.
In some implementations, the user computing device 135 also includes a transfer controller 140. In particular, in some implementations, transfer controller 140 may select certain images stored in image capture device 102 for transfer to user computing device 135 when connected to image capture device 102 (e.g., after a capture session has been completed). For example, such selection may be guided by various image attributes reflected in various metadata annotations provided by image capture device 120 for the stored images, respectively.
In some implementations, transfer controller 140 may execute an optimization algorithm to select which images stored in memory 114 of image capture device 102 are requested to be transferred. The optimization algorithm may have a number of associated goals including, for example, at least a variety of settings and a variety of depicted people, while still requesting that the most desirable images be delivered. In some embodiments, transfer controller 140 may implement sub-module functionality to select which images are requested to be transferred. In some implementations, transport controller 140 may consider one or more metrics generated for the image from hardware generated statistics.
In some implementations, the user computing device 135 also includes a best shot selector 142. The best shot selector 142 may be implemented to select at least one highlight image from a set of images (e.g., at least one highlight image for each time instant within the map). For example, the best shot selector 142 may select the highlight images based on the annotations or other metadata included with each image. In some embodiments, best shot selector 142 normalizes each image in a micro-video relative to the peer images within such micro-video prior to performing a highlight selection. Thus, selecting a highlighted image for each micro video may be based at least in part on comparing the annotation of each image included at the moment in time relative to the annotation of at least one other image included at the moment in time. In some embodiments, best shot selector 142 may consider one or more metrics generated for the image from hardware generated statistics.
The user computing device 135 also includes a user interface controller 144. The user interface controller 144 may be implemented to provide a user interface that enables a user to explore images within the map in time and/or space. In particular, the user interface controller 144 may be implemented to detect and respond to certain user inputs with appropriate user interface interactions.
Each of the transfer controller 140, the best shot selector 142, and the user interface controller 144 may include computer logic for providing desired functionality. Accordingly, each of the transfer controller 140, the best shot selector 142, and the user interface controller 144 may be implemented in hardware, firmware, and/or software that controls a general purpose processor. In some embodiments, each of the transfer controller 140, the best shot selector 142, and the user interface controller 144 includes program code files stored on a storage device, loaded into the memory 137 and executed by the processor 136, or may be provided from a computer program product, such as computer executable instructions 139 stored in a tangible computer readable storage medium (such as a RAM disk or card or other computer readable optical or magnetic medium).
The server computing device 150 may be implemented using one or more server computing devices and may include a processor 152 and memory 154. Where the server computing device 150 is comprised of multiple server devices, such server devices may operate according to any computing architecture, including a parallel computing architecture, a distributed computing architecture, or a combination thereof.
The processor 152 may be any suitable processing device and may be an operatively connected processor or processors. The memory 154 may store instructions 158 that cause the processor 152 to perform operations embodying the present disclosure.
The network 180 may be any type of communication network, such as a local area network (e.g., an intranet), a wide area network (e.g., the internet), or some combination thereof, and may include any number of wired or wireless links. In general, communications between the server computing device 150 and the user computing device 135 may be conducted via any type of wired and/or wireless connection, using various communication protocols (e.g., TCP/IP, HTTP, SMTP, FTP), encodings or formats (e.g., HTML, XML), and/or protection schemes (e.g., VPN, secure HTTP, SSL). The server computing device 150 may communicate with the user computing device 135 over the network 180 by sending and receiving data.
Further, any process, operation, program, application, or instructions described as being stored at server computing device 150 or executed by server computing device 150 may be stored in whole or in part at user computing device 135 or executed by user computing device 135, and vice versa. In particular, in some implementations, the image capture device 102 may be directly communicatively connected to the server computing device 150, and the server computing device 150 may perform image management, editing, storage, and sharing functions attributed to the user computing device 135 elsewhere in the disclosure (e.g., via a web application). Likewise, any process, operation, program, application, or instructions described as being stored at the image capture device 102 or executed by the image capture device 102 may be stored in whole or in part at the user computing device 135 or executed by the user computing device 135, and vice versa. For example, in some implementations, transfer controller 140 may be located at image capture device 102 instead of user computing device 135.
The technology discussed herein relates to servers, databases, software applications, and other computer-based systems, and the actions taken and information sent to and received from these systems. The inherent flexibility of computer-based systems allows for a variety of possible configurations, combinations, and divisions of tasks and functions between and among components. For example, the processes discussed herein may be implemented using a single device or component or a plurality of devices or components working in concert. The database and applications may be implemented on a single system or may be distributed across multiple systems. The distributed components may operate sequentially or in parallel.
While the present subject matter has been described in detail with respect to various specific example embodiments thereof, each example is provided by way of explanation, not limitation, of the disclosure. Alterations, permutations, and equivalents of these embodiments may be readily produced by those skilled in the art having the benefit of the foregoing description. Accordingly, the subject disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For instance, features illustrated or described as part of one embodiment, can be used with another embodiment to yield a still further embodiment. Thus, the present disclosure is intended to cover such alternatives, modifications, and equivalents.
Any acts, operations, components, methods, techniques, or other aspects described herein as being included in or performed by an image capture device may also be included in or performed by a device or system that does not capture an image, but simply processes or makes decisions based on the image.
Although the figures depict the steps or processes as being performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particular depicted order or arrangement. Various steps of the methods described herein may be omitted, rearranged, combined, and/or adapted in various ways without departing from the scope of the present disclosure.
Each of the micro video generation system 124, shutter control 130, trimmer 504, and encoder 506 may include computer logic for providing the desired functionality. Each of the micro video generation system 124, shutter controller 130, trimmer 504, and encoder 506 may be implemented in hardware, firmware, and/or software that controls a general purpose processor. For example, in some embodiments, each of the micro video generation system 124, shutter controller 130, trimmer 504, and encoder 506 comprises a program file stored on a storage device, loaded into memory, and executed by one or more processors. In other implementations, each of the micro video generation system 124, shutter controller 130, trimmer 504, and encoder 506 includes one or more sets of computer executable instructions stored in a tangible computer readable storage medium, such as a RAM hard disk or an optical or magnetic medium.
Claims (20)
1. An image capture device comprising:
an image sensor operative to capture raw image data;
an image signal processor configured to process raw image data to form a plurality of image frames, wherein the image signal processor comprises one or more hardware blocks;
one or more motion sensing sensors providing sensor data indicative of motion of the image sensor or the image capture device;
one or more processors; and
a micro video generation system implemented by the one or more processors, wherein the micro video generation system is configured to generate micro videos that include both still images and videos in the same data file,
wherein the micro video generation system executes an image selection algorithm to select the still image, an
Wherein the video is an input to the image selection algorithm to select the still image.
2. The image capture device of claim 1, wherein the still image comprises a JPEG image and the video comprises MP4 video.
3. The image capture device of claim 1, wherein the video is located within a container associated with the still image.
4. The image capture device of claim 1, wherein the micro video comprises a video track and a motion data track, wherein the motion data track stores motion data usable to stabilize the micro video.
5. The image capture device of claim 4, wherein the motion data comprises a plurality of homography matrices.
6. The image capture device of claim 1, wherein to generate the micro video, the micro video generation system crops the plurality of image frames available for inclusion in the micro video.
7. The image capture device of claim 1, wherein to generate the micro video, the micro video generation system:
identifying a shutter frame based at least in part on a time associated with the user input;
searching the image frames stored in the buffer to identify a start frame;
identifying an end frame; and
generating the micro video comprising image frames spanning from the start frame to the end frame.
8. The image capture device of claim 7, wherein to identify the starting frame, the micro video generation system searches backwards in time the image frames stored in the buffer.
9. The image capture device of claim 7, wherein to identify one or both of the start frame or the end frame, the micro video generation system applies one or more criteria.
10. The image capture device of claim 9, wherein the one or more criteria include a motion criterion that analyzes an amount of motion between the current frame being evaluated and the shutter frame.
11. The image capture device of claim 8, wherein the motion criterion analyzes a maximum amount of motion between any pair of four pairs of frame angles between the current frame and the shutter frame being evaluated.
12. The image capturing device of claim 10, wherein:
the motion criteria compares the amount of motion to a motion threshold;
the motion threshold is dynamically determined based at least in part on a camera motion speed; and is
The camera motion speed is determined based at least in part on an amount of motion between the current frame and the shutter frame being evaluated.
13. The image capture device of claim 9, wherein the one or more criteria include a sensitivity criterion that analyzes an amount of change in sensitivity between the current frame being evaluated and the shutter frame.
14. The image capture device of claim 9, wherein the one or more criteria include a scaling criterion that analyzes an amount of change in a zoom level between the current frame and the shutter frame being evaluated.
15. The image capture device of claim 9, wherein the one or more criteria include a motion blur criterion that analyzes an amount of motion blur associated with the current frame.
16. The image capture device of claim 9, wherein the one or more criteria include a focus criteria that analyzes autofocus data associated with the current frame.
17. The image capture device of claim 17, wherein the micro video generation system starts encoding from the start frame before proceeding to identify the end frame.
18. A computer-implemented method, comprising:
generating, by a computing system comprising one or more processors, a micro-video comprising both still images and video in the same data file;
executing, by the computing system, an image selection algorithm to select the still image; and
inputting, by the computing system, the video to the image selection algorithm to select the still image.
19. The computer-implemented method of claim 18, wherein the still image comprises a JPEG image and the video comprises MP4 video.
20. One or more non-transitory computer-readable media storing instructions that, when executed by one or more processors of a computing system, cause the computing system to perform operations comprising:
generating, by a computing system comprising one or more processors, a micro-video comprising both still images and video in the same data file;
executing, by the computing system, an image selection algorithm to select the still image; and
inputting, by the computing system, the video to the image selection algorithm to select the still image.
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201762567373P | 2017-10-03 | 2017-10-03 | |
US62/567,373 | 2017-10-03 | ||
CN201880044253.5A CN110809797B (en) | 2017-10-03 | 2018-07-12 | Micro video system, format and generation method |
PCT/US2018/041748 WO2019070325A1 (en) | 2017-10-03 | 2018-07-12 | Microvideo system, format, and method of generation |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880044253.5A Division CN110809797B (en) | 2017-10-03 | 2018-07-12 | Micro video system, format and generation method |
Publications (1)
Publication Number | Publication Date |
---|---|
CN115696049A true CN115696049A (en) | 2023-02-03 |
Family
ID=63244947
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202211391220.2A Pending CN115696049A (en) | 2017-10-03 | 2018-07-12 | Micro video system, format and generation method |
CN201880044253.5A Active CN110809797B (en) | 2017-10-03 | 2018-07-12 | Micro video system, format and generation method |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201880044253.5A Active CN110809797B (en) | 2017-10-03 | 2018-07-12 | Micro video system, format and generation method |
Country Status (4)
Country | Link |
---|---|
US (3) | US11343429B2 (en) |
EP (1) | EP3622513A1 (en) |
CN (2) | CN115696049A (en) |
WO (1) | WO2019070325A1 (en) |
Families Citing this family (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2019070325A1 (en) * | 2017-10-03 | 2019-04-11 | Google Llc | Microvideo system, format, and method of generation |
CN112102449B (en) * | 2020-09-14 | 2024-05-03 | 北京百度网讯科技有限公司 | Virtual character generation method, virtual character display device, virtual character display equipment and virtual character display medium |
DE102022112743B4 (en) * | 2022-05-20 | 2024-02-01 | Audi Aktiengesellschaft | Method for improving the quality of an audio and/or video recording and control device for a mobile terminal |
Family Cites Families (22)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7986339B2 (en) * | 2003-06-12 | 2011-07-26 | Redflex Traffic Systems Pty Ltd | Automated traffic violation monitoring and reporting system with combined video and still-image data |
CN1963930A (en) * | 2005-11-09 | 2007-05-16 | 上海乐金广电电子有限公司 | Rewriteable medium, managing/playing method and apparatus for constitution of play data of static image |
US20110102616A1 (en) | 2009-08-28 | 2011-05-05 | Nikon Corporation | Data structure for still image file, image file generation device, image reproduction device, and electronic camera |
CN101646006A (en) * | 2009-09-11 | 2010-02-10 | 北京中星微电子有限公司 | Image combination device and portable terminal containing device |
US9501495B2 (en) | 2010-04-22 | 2016-11-22 | Apple Inc. | Location metadata in a media file |
US8885952B1 (en) * | 2012-02-29 | 2014-11-11 | Google Inc. | Method and system for presenting similar photos based on homographies |
EP2763077B1 (en) * | 2013-01-30 | 2023-11-15 | Nokia Technologies Oy | Method and apparatus for sensor aided extraction of spatio-temporal features |
US9159371B2 (en) | 2013-08-14 | 2015-10-13 | Digital Ally, Inc. | Forensic video recording with presence detection |
US9407823B2 (en) * | 2013-12-09 | 2016-08-02 | Microsoft Technology Licensing, Llc | Handling video frames compromised by camera motion |
JP6142793B2 (en) * | 2013-12-20 | 2017-06-07 | Tdk株式会社 | Rare earth magnets |
KR102153435B1 (en) * | 2013-12-20 | 2020-09-08 | 엘지전자 주식회사 | The mobile terminal and the control method thereof |
US10664687B2 (en) * | 2014-06-12 | 2020-05-26 | Microsoft Technology Licensing, Llc | Rule-based video importance analysis |
CN104113698A (en) * | 2014-08-06 | 2014-10-22 | 北京北纬通信科技股份有限公司 | Blurred image processing method and system applied to image capturing device |
US9754416B2 (en) * | 2014-12-23 | 2017-09-05 | Intel Corporation | Systems and methods for contextually augmented video creation and sharing |
WO2016174524A2 (en) | 2015-04-29 | 2016-11-03 | Tomtom International B.V. | Data processing systems |
US10154196B2 (en) * | 2015-05-26 | 2018-12-11 | Microsoft Technology Licensing, Llc | Adjusting length of living images |
KR102445699B1 (en) * | 2016-02-18 | 2022-09-21 | 삼성전자주식회사 | Electronic device and operating method thereof |
US10000931B1 (en) * | 2016-12-13 | 2018-06-19 | Daniel T. Daugherty | Apparatus and method for moss remediation |
WO2019070325A1 (en) * | 2017-10-03 | 2019-04-11 | Google Llc | Microvideo system, format, and method of generation |
US10762716B1 (en) * | 2019-05-06 | 2020-09-01 | Apple Inc. | Devices, methods, and graphical user interfaces for displaying objects in 3D contexts |
GB2599441B (en) * | 2020-10-02 | 2024-02-28 | Emotional Perception Ai Ltd | System and method for recommending semantically relevant content |
US11343420B1 (en) * | 2021-03-30 | 2022-05-24 | Tectus Corporation | Systems and methods for eye-based external camera selection and control |
-
2018
- 2018-07-12 WO PCT/US2018/041748 patent/WO2019070325A1/en unknown
- 2018-07-12 EP EP18755946.3A patent/EP3622513A1/en active Pending
- 2018-07-12 CN CN202211391220.2A patent/CN115696049A/en active Pending
- 2018-07-12 US US16/624,952 patent/US11343429B2/en active Active
- 2018-07-12 CN CN201880044253.5A patent/CN110809797B/en active Active
-
2022
- 2022-05-23 US US17/751,274 patent/US11595580B2/en active Active
-
2023
- 2023-02-27 US US18/175,152 patent/US20230292005A1/en active Pending
Also Published As
Publication number | Publication date |
---|---|
US20200228708A1 (en) | 2020-07-16 |
US20220286610A1 (en) | 2022-09-08 |
US11343429B2 (en) | 2022-05-24 |
CN110809797B (en) | 2022-11-15 |
US11595580B2 (en) | 2023-02-28 |
CN110809797A (en) | 2020-02-18 |
WO2019070325A1 (en) | 2019-04-11 |
EP3622513A1 (en) | 2020-03-18 |
US20230292005A1 (en) | 2023-09-14 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11159763B2 (en) | Low power framework for controlling image sensor mode in a mobile image capture device | |
US10732809B2 (en) | Systems and methods for selective retention and editing of images captured by mobile image capture device | |
US9836484B1 (en) | Systems and methods that leverage deep learning to selectively store images at a mobile image capture device | |
US10372991B1 (en) | Systems and methods that leverage deep learning to selectively store audiovisual content | |
US11403509B2 (en) | Systems and methods for providing feedback for artificial intelligence-based image capture devices | |
US9838641B1 (en) | Low power framework for processing, compressing, and transmitting images at a mobile image capture device | |
US11595580B2 (en) | Microvideo system, format, and method of generation | |
US9836819B1 (en) | Systems and methods for selective retention and editing of images captured by mobile image capture device | |
US9251765B2 (en) | Image processing device, image processing method, and program for generating composite image | |
CN103685940A (en) | Method for recognizing shot photos by facial expressions | |
US9692963B2 (en) | Method and electronic apparatus for sharing photographing setting values, and sharing system | |
US20120188396A1 (en) | Digital photographing apparatuses, methods of controlling the same, and computer-readable storage media | |
CN112825543B (en) | Shooting method and equipment | |
JP2015177262A (en) | Imaging apparatus, subject tracking method, and program | |
US9013603B2 (en) | Image processing apparatus, image capture device, server and computer program, where captured movie image is written in the storage medium based on the shooting condition | |
KR20150083491A (en) | Methed and system for synchronizing usage information between device and server | |
CN112136309A (en) | System and method for performing rewind operations with a mobile image capture device | |
JP7182893B2 (en) | Image processing device, imaging device, image processing method, and program | |
JP5368614B2 (en) | Image search apparatus and image search method | |
CN115942113A (en) | Image shooting method, application processing chip and electronic equipment | |
CN117177046A (en) | Shooting processing method, processor, storage medium and electronic equipment | |
JP5613304B2 (en) | Image search apparatus and image search method | |
TWI531987B (en) | Image capturing apparatus and image capturing method thereof | |
JP2017139702A (en) | Image processing apparatus, image processing method and program | |
JP2015109614A (en) | Camera system, control device, and control method of the same |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |