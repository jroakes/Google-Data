US6711651B1 - Method and apparatus for history-based movement of shared-data in coherent cache memories of a multiprocessor system using push prefetching - Google Patents
Method and apparatus for history-based movement of shared-data in coherent cache memories of a multiprocessor system using push prefetching Download PDFInfo
- Publication number
- US6711651B1 US6711651B1 US09/655,642 US65564200A US6711651B1 US 6711651 B1 US6711651 B1 US 6711651B1 US 65564200 A US65564200 A US 65564200A US 6711651 B1 US6711651 B1 US 6711651B1
- Authority
- US
- United States
- Prior art keywords
- entry
- cache
- storage element
- processing node
- caches
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Expired - Lifetime, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/0802—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches
- G06F12/0862—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches with prefetch
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F12/00—Accessing, addressing or allocating within memory systems or architectures
- G06F12/02—Addressing or allocation; Relocation
- G06F12/08—Addressing or allocation; Relocation in hierarchically structured memory systems, e.g. virtual memory systems
- G06F12/0802—Addressing of a memory level in which the access to the desired data or data block requires associative addressing means, e.g. caches
- G06F12/0806—Multiuser, multiprocessor or multiprocessing cache systems
- G06F12/0815—Cache consistency protocols
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F2212/00—Indexing scheme relating to accessing, addressing or allocation within memory systems or architectures
- G06F2212/60—Details of cache memory
- G06F2212/6024—History based prefetching
Definitions
- the present invention relates generally to memories in computer processing systems and, in particular, to a method and apparatus for history-based movement of shared-data in coherent cache memories.
- the prior art uses history information to help prefetch invalidated cache lines in a node/processor before the invalidated cache lines are used again.
- the Cosmos “coherence message predictor” and the “memory sharing predictor” predict the source and type of coherence messages for a cache line in a multiprocessor (MP) computer system using a complex prediction logic.
- the Cosmos coherence message predictor is described by Hill et al., in “Using Prediction to Accelerate Coherence Protocols”, Proceedings of the 25th Annual International Symposium on Computer Architecture (ISCA), Barcelona, Spain, June 27 through Jul. 2, 1998, pp. 179-90.
- the memory sharing predictor is described by Falsafi et. al, in “Memory Sharing Predictor: The Key to a Speculative Coherent DSM”, Proceedings of the 26th International Symposium Computer. Architecture (ISCA), Atlanta, Ga., May 2-4, 1999, pp. 172-83.
- the goal of the two preceding predictors is to predict incoming messages that affect memory blocks and to timely execute the incoming messages.
- the two approaches are basically similar in that they are both based on the ability to predict likely messages in sequence and speculatively execute the messages.
- the two approaches suffer from a long learning overhead which is required to accurately predict “follow-on” messages.
- the invention reduces data misses and access latencies in coherent cache memories in a multiprocessor (MP) computer system.
- the invention employs mechanisms for causing the pushing of data from one node/processor to other nodes/processors in an MP computer system based on memory sharing patterns.
- the concept is to monitor and keep a history of which processors in the system consume a particular datum or data (hereinafter “data”) that is produced by another node/processor, and push the produced data (e.g., cache line(s)) to the consuming nodes/processors.
- Cache lines may be pushed from one cache to one or more other caches at other nodes/processors.
- the invention can reduce the latency associated with lateral interventions.
- the invention is not concerned with the sequence in which messages arrive, and does not affect the way messages get executed.
- the invention simply identifies nodes/processors that are likely to use newly produced data, and attempts to timely move copies of the data closer to the consuming nodes/processors to reduce possible cache misses that must be intervened by other nodes/processors in the system.
- the invention uses a simple history gathering technique, and does not require a complex prediction mechanism to predict likely messages that must follow in series.
- the invention employs a data-centric approach that actively and aggressively moves data closer to the consuming processor(s), and incurs only a minimal learning overhead, in contrast to the prior art.
- a method for moving at least one of instructions and operand data throughout a plurality of caches included in a computer system wherein each of the plurality of caches is included in one of a plurality of processing nodes of the system.
- the method includes the step of storing a plurality of entries in a table attached to each of the plurality of caches, wherein each of the entries is associated with a plurality of storage elements in one of the plurality of caches and includes information of prior usage of the plurality of storage elements by each of the plurality of processing nodes.
- a method for moving at least one of instructions and operand data throughout a plurality of caches included in a computer system wherein each of the plurality of caches is included in one of a plurality of processing nodes of the system.
- the method includes the step of storing a plurality of entries in a table attached to each of the plurality of caches, wherein each of the entries is associated with a plurality of storage elements in one of the plurality of caches and includes information of prior usage of the plurality of storage elements by each of the plurality of processing nodes.
- any given storage elements that caused the miss are transferred to the given cache from one of main memory and another cache.
- a given entry that is associated with the given storage elements is created in the table.
- the method further includes the step of displacing at least one existing storage element in the given cache to make room for the given storage elements.
- the method further includes the step invalidating an entry associated with the displaced at least one existing storage element, if the entry exists.
- the method upon a processing node performing a store operation that updates at least one storage element in a cache included in the processing node, the method further includes the step of searching the table for an entry associated with the at least one storage element. If the entry is found, the at least one storage element is requested to be transmitted to any processing nodes identified in the entry based upon the information stored in the entry.
- the method upon a request by a processing node for a storage element in a cache included in another processing node, the method further includes the step of searching the table for an entry corresponding to the requested storage element. If the entry is found, the information stored in the entry is updated to indicate that the requested storage element has been sent to the other processing node. If the entry is not found, a new entry is created in the table for the requested storage element.
- the creating step includes the step of identifying a usage of the requested storage element by the other processing node.
- the method upon a receipt of a storage element by a processing node from another processing node that recently updated the storage element, the method further includes the step of determining whether the received storage element is to be stored in the cache included in the processing node, based upon a current content of the cache.
- FIG. 1 is a diagram illustrating a snapshot of a multiprocessor bus-based memory address trace grouped into cache lines, according to an illustrative embodiment of the invention
- FIG. 2 is a diagram of a multiprocessor (MP) computer system having the mechanisms for implementing history-based movement of shared data, according to an illustrative embodiment of the invention
- FIG. 3 is a flow diagram illustrating a method for history-based movement of shared-data in coherent cache memories of a multiprocessor computer system, according to an illustrative embodiment of the invention.
- FIG. 4 is a flow diagram illustrating a method for updating the consume after produce (CAP) table with CAP information, according to an illustrative embodiment of the invention.
- the present invention is directed to a method and apparatus for history-based movement of shared-data in coherent cache memories. It is to be understood that the present invention may be implemented in various forms of hardware, software, firmware, special purpose processors, or a combination thereof. In one embodiment, the present invention may be implemented in software as an application program tangibly embodied on a program storage device.
- the application program may be uploaded to, and executed by, a machine comprising any suitable architecture.
- the machine is implemented on a computer platform having hardware such as one or more central processing units (CPUs), a random access memory (RAM), and input/output (I/O) interface(s).
- the computer platform may also include an operating system and/or micro instruction code.
- various processes and functions described herein may either be part of the micro instruction code or part of the application program (or a combination thereof) which is executed via the operating system.
- various other peripheral devices may be connected to the computer platform such as an additional data storage device and a printing device.
- the invention reduces the data/instruction access latency in modern multiprocessor (MP) computer systems. Moreover, the invention leverages the inherent producer/consumer memory sharing patterns among nodes/processors running parallel workloads. According to the invention, the memory space is logically divided into contiguous blocks for which this producer/consumer information will be kept. Typically, this division of memory will correspond to the lines or blocks of a cache, and, thus, all such memory blocks are referred to herein as “cache lines”.
- the writing processor For each cache line that is written, the writing processor creates and maintains a “consume after produce” (CAP) entry at its coherent cache controller (where the various nodes or processors communicate via shared memory).
- This CAP entry records the set of other nodes/processors within the system that access the corresponding cache line after it is written by the producing node/processor, and before a subsequent processor writes to the same cache line.
- the CAP entry is used to predict the consumers of future writes.
- the CAP entry for that block is examined (if one exists).
- the cache block is pushed to the nodes/processors on the CAP list.
- the pushing of the cache block is a transaction, where the data is sent to the expected consumers as if they had executed a prefetch for that cache block.
- the number of nodes/processors to which the block is scheduled to be sent depends on the bit pattern of the CAP entry, which records the prior consumers of that block.
- cache block # 16 was stored (written) to 34 times by processor 0 and there were three distinct patterns of consumers between those writes. There are 30 occurrences of the pattern where every processor in the system consumed the data produced by producer 0 . There are 3 occurrences of the pattern where every processor except processor 5 consumed the data. There is only 1 occurrence of the pattern where every processor except processors 4 and 5 consumed the data. Thus, there are 3 distinct CAP patterns for block # 16 on processor 0 . However, the mechanism for generating a CAP entry unifies these patterns into a single pattern that subsumes the constituent patterns by continuously updating consumer information of written data. This allows the use of a single CAP entry per cache block, and avoids difficulties in deciding between sets of CAP patterns.
- the push semantics are similar to those of a prefetch (i.e., a processor to which the data is pushed can ignore the push, or save the pushed data in a specialized prefetch buffer), the cost of executing extra pushes is memory bus traffic.
- FIG. 2 is a diagram of a multiprocessor (MP) computer system 200 having the mechanisms for implementing history-based movement of shared data, according to an illustrative embodiment of the invention.
- each of the various nodes/processors e.g., Pi 202 , Pj 252
- has its own local cache memory e.g., Li 204 , Lj 254 , respectively
- an intervening medium e.g., a bus, a ring, and so forth.
- the intervening network is illustrated by request and response lines.
- Blocks CCi 206 and CCj 256 represent the cache controllers for the Li 204 and Lj 254 local caches, respectively.
- the invention is described in detail with respect to processor Pj 252 and its accompanying cache Lj 254 and coherent cache controller CCj 256 .
- the invention employs a CAP table 282 and a push prefetching engine 284 (also collectively referred to herein as CAP engine 290 ) at the coherent cache controller CCj 256 of processor Pj.
- the CAP table 282 is effectively a cache of CAP entries, where each CAP entry corresponds to a specific cache block, and records the set of nodes/processors that consume the data produced by the current node/processor in that cache block.
- the structure of the CAP table 282 needs to be large enough to ensure history availability while still being small enough to be relatively insignificant in terms of the overall local cache (e.g., local cache Lj 254 ) space.
- CAP entry The most basic form of a CAP entry is simply a bit vector that associates a bit with each processor in the multiprocessor system. For the CAP entries of FIG. 1, this would mean that a 12-bit CAP entry is used for each cache block to represent the 12 processors that can potentially consume data written into each of the blocks.
- CAP entries could be used to denote groups of processors (e.g. multiprocessor nodes, or other more arbitrary divisions of the processors into groups).
- CAP table structure given the computer processing system in which the invention is to be implemented, as well as various ways in which to represent a CAP entry, all while maintaining the spirit and scope of the invention.
- a processor or node can be represented in a CAP entry by more than one bit.
- the additional bit (s) may be used to provide information regarding the cache block usage patterns and coherence activities at the consuming node/processor. For example, if the consuming processor or node of a cache line always consumes and writes immediately, thereby requesting the block in an exclusive mode, then the consuming processor or node will speed up processing to provide the data to the requesting processor in exclusive mode. An additional bit representation could therefore indicate whether the data is needed in shared or exclusive mode by the consuming processor. Given the teachings of the invention provided herein, one of ordinary skill in the related art will contemplate these and various other multi-bit implementations for gathering and/or representing the CAP data, while maintaining the spirit and scope of the invention.
- data pushing employs two distinct actions.
- the push prefetching engine 284 should be able to detect requests by other processors for a given memory block.
- the cache controller CCj 256 of the processor Pj 256 must notify the push prefetching 284 engine when the cache controller CCj 256 does a store operation and to which cache block address.
- the push prefetching engine 284 should be aware of memory access requests in the system 200 .
- the processor first needs to acquire exclusive access to that memory block. While the processor holds exclusive access, no other processor may have a valid copy of the data.
- the processor that wrote the memory block must give up its exclusivity.
- the processor that wrote a memory block must be notified that some other processor has requested the data.
- the processor must be notified not only hat some other processor has requested the data but also which processor made the request.
- all processors need to be aware of all (shared memory) data access requests by all other processors.
- the cache controller can simply send the relevant information (i.e. the memory block address being read and the processor doing the reading) to the push prefetching engine 284 .
- the push prefetching engine 284 would access the CAP table 282 to see whether the requested memory block has a corresponding CAP entry in the CAP table 282 and, if so, then set the bit corresponding to the requesting processor.
- the remaining consideration corresponds to when the push prefetching engine is triggered to generate the memory block data pushes.
- the mechanism to trigger data pushing can be invoked through the detection of a completed store operation to a given memory block.
- a processor could acquire exclusive access to the memory block, conduct one thousand separate stores to various bytes of the memory block, and only after the last of these stores would any other processor request the data (or a portion thereof) in the memory block. While the push prefetching engine 284 could push the data after each separate store operation, this would result in one thousand times more push traffic than necessary, which would result in an undesirable increase in the multiprocessor communication traffic. Hence the push prefetching engine 284 should attempt to determine when a full set of stores to a given memory block has completed before pushing the data to other processors. Detecting the completion of a memory block write can be implemented using several different methods. However, according to a preferred embodiment of the invention, a unified detection of store completion mechanism is adopted herein by combining a conservative approach with a somewhat more aggressive approach in an attempt to balance the reduction in memory bus traffic and the reduction in other processor demand fetch misses.
- One approach is not to generate any data pushes until a request to read the data has been received from another processor, e.g., until a demand fetch is received for a memory block that this processor currently holds in exclusive mode. Once the demand fetch is detected, the demanding processor would be sent the data (via the normal demand fetch serving mechanism of the multiprocessor) and the push prefetching engine 284 would generate a sequence of data push messages to the remaining processors indicated in the CAP entry of the associated memory block.
- the advantage of this approach is that the write operation of the current processor is guaranteed to be complete when a demand request for that memory block is received.
- the potential disadvantage is that the data is not pushed until the data is already being requested, so there is less time between the push of the data and use of the data.
- a second approach is to leverage the observation that a processor's use of a memory block generally comes in sequential bursts of activity, and thus a sequence of writes to a memory block is likely to be completed when the processor moves out of that memory block and onto another memory block.
- the processor will write data to the first memory block of the array, and proceed to the next memory block, and then subsequent memory blocks.
- the write operation on the first memory block can be assumed to be completed, and the data can be pushed out to the consumer processors listed in the CAP entry.
- the store completion detection mechanism for triggering push prefetching monitors for either: (1) a processor writing to a given memory block moves off the given memory block to perform a write activity on another memory block; or (2) a read request for a memory block currently being held at a given processor in an exclusive mode is received from another processor.
- FIG. 3 is a flow diagram illustrating a method for history-based movement of shared-data in coherent cache memories of a multiprocessor computer system, according to an illustrative embodiment of the invention.
- processor Pj 252 completes writing to memory block X ( 304 ), or that cache controller CCj 256 receives an external request for memory block X (which is held in exclusive mode in processor Pj) and notifies the push prefetching engine 284 of the same ( 305 ), the push prefetching engine 284 then accesses the CAP table 282 using the address of memory block X (step 306 ) and determines whether there is a CAP entry for memory block X therein (step 310 ). Such an entry would include an encoding of the previous consumers of the data in memory block X.
- CAP table 282 for memory block X If there is no entry in the CAP table 282 for memory block X, then a new CAP entry is created in the CAP table 282 for memory block X (box 320 ). In contrast, if there is an entry in the CAP table 282 for memory block X (and the CAP entry is a bit-vector where one bit is associated with each processor of the system), then the push prefetching engine 284 will read out the CAP entry, and for each bit that is set in the CAP entry bit vector, a data push message is generated to send memory block X to the processor associated with that bit (step 350 ).
- bit zero would correspond to consumption by processor zero, and if bit zero is set, then a data push message is generated to send the data to processor zero at step 350 .
- the message is forwarded to the cache controller CCj 256 , which sends the data out on the multiprocessor communication network to the target processor.
- the cache controller can filter the stream of data push messages to remove redundant pushes (e.g., to the processor that performed a demand fetch of block X). After all of the data push messages have been generated, the push prefetching engine 284 is considered to have completed the writing of memory block X.
- steps 304 and 305 of FIG. 3 may be implemented as follows. Presume that processor Pj 252 begins writing to memory block X.
- the cache controller CCj notifies the push prefetching engine 284 that the processor is writing to block X, and makes note of the same.
- the cache controller CCj 256 notifies the push prefetching engine 284 . If the processor then moves on to write to memory block Y (which is distinct from X), then the processor again notifies the push prefetching engine 284 that it is writing to a memory block, this time to memory block Y.
- the push prefetching engine 284 notes that this is a change in the target of the memory block writes and, thus, assumes that the writes to memory block X are complete.
- the cache controller CCj 256 receives an external request for any data block, e.g., X in this case, then the cache controller CCj 256 must notify the push prefetching engine 284 . Any of these two actions can initiate push prefetching on data block X.
- FIG. 4 is a flow diagram illustrating a method for updating the CAP table 282 with CAP information, according to an illustrative embodiment of the invention.
- the cache controller CCj 256 can either forward all such snooped requests or filter the requests in an appropriate manner to reduce message traffic, if needed.
- the cache controller CCj 256 snooped a read request (remote access) by processor Pi 252 for some memory block Z ( 410 ).
- the push prefetching engine 284 receives the information that processor Pi 252 requested the data for block Z, and the push prefetching engine 284 accesses the CAP table 282 using the address of memory block Z (step 412 ) and determines whether there is a CAP entry in the CAP table 282 for memory block Z (step 414 ). If there is no CAP entry in the CAP table 282 for memory block Z, then the push prefetching engine 284 ignores the read request information, and the method is terminated. In contrast, if there is a CAP entry for memory bloc,k Z then the push prefetching engine 284 updates the CAP entry to include processor Pi 252 as a consumer of memory block Z (step 414 ), and the method is terminated.
- push prefetching engine 284 is assumed to generate a special category of data communication message, i.e., the data push message.
- data push messages operate similar to data prefetch request responses, with at least one distinction being that the processor that receives the data push message never generated a prefetch request.
- push prefetching is implemented in a system where the multiprocessors and cache controllers are designed using a priority scheduling algorithm or mechanism that helps to ensure that demand requests have preferred priority over prefetch requests or data push messages and, thus, push prefetching should not unduly impact the operation of the demand requests.
- the data push messages (and any prefetch requests) are received by the target processors into special memories and, thus, need not pollute their caches or overly impact the normal cache behavior. It is to be appreciated that push prefetching does not require any particular implementation of a priority scheduler or of special buffering for data push or prefetch messages; the use of such schemes would, however, generally provide better local processor (and system) performance.
Abstract
Description
Claims (22)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US09/655,642 US6711651B1 (en) | 2000-09-05 | 2000-09-05 | Method and apparatus for history-based movement of shared-data in coherent cache memories of a multiprocessor system using push prefetching |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US09/655,642 US6711651B1 (en) | 2000-09-05 | 2000-09-05 | Method and apparatus for history-based movement of shared-data in coherent cache memories of a multiprocessor system using push prefetching |
Publications (1)
Publication Number | Publication Date |
---|---|
US6711651B1 true US6711651B1 (en) | 2004-03-23 |
Family
ID=31978971
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US09/655,642 Expired - Lifetime US6711651B1 (en) | 2000-09-05 | 2000-09-05 | Method and apparatus for history-based movement of shared-data in coherent cache memories of a multiprocessor system using push prefetching |
Country Status (1)
Country | Link |
---|---|
US (1) | US6711651B1 (en) |
Cited By (19)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20020184450A1 (en) * | 2001-05-23 | 2002-12-05 | Shirish Gadre | Multifunctional I/O organizer unit for multiprocessor multimedia chips |
US20030154351A1 (en) * | 2001-11-16 | 2003-08-14 | Jim Nilsson | Coherence message prediction mechanism and multiprocessing computer system employing the same |
US20040075664A1 (en) * | 2002-10-22 | 2004-04-22 | Patrick Law | Hardware assisted format change mechanism in a display controller |
US20060004965A1 (en) * | 2004-06-30 | 2006-01-05 | Tu Steven J | Direct processor cache access within a system having a coherent multi-processor protocol |
US20060085602A1 (en) * | 2004-10-15 | 2006-04-20 | Ramakrishna Huggahalli | Method and apparatus for initiating CPU data prefetches by an external agent |
US20060095679A1 (en) * | 2004-10-28 | 2006-05-04 | Edirisooriya Samantha J | Method and apparatus for pushing data into a processor cache |
US20060155933A1 (en) * | 2005-01-13 | 2006-07-13 | International Business Machines Corporation | Cost-conscious pre-emptive cache line displacement and relocation mechanisms |
US20070255908A1 (en) * | 2006-04-28 | 2007-11-01 | Sun Microsystems, Inc. | Speculative directory lookup for sharing classification |
US20080104325A1 (en) * | 2006-10-26 | 2008-05-01 | Charles Narad | Temporally relevant data placement |
US20080229009A1 (en) * | 2007-03-14 | 2008-09-18 | Gaither Blaine D | Systems and methods for pushing data |
US20090157979A1 (en) * | 2007-12-18 | 2009-06-18 | International Business Machines Corporation | Target computer processor unit (cpu) determination during cache injection using input/output (i/o) hub/chipset resources |
US20090157978A1 (en) * | 2007-12-18 | 2009-06-18 | International Business Machines Corporation | Target computer processor unit (cpu) determination during cache injection using input/output (i/o) adapter resources |
US20090157961A1 (en) * | 2007-12-18 | 2009-06-18 | International Business Machines Corporation | Two-sided, dynamic cache injection control |
US20090157977A1 (en) * | 2007-12-18 | 2009-06-18 | International Business Machines Corporation | Data transfer to memory over an input/output (i/o) interconnect |
US20130097387A1 (en) * | 2011-10-14 | 2013-04-18 | The Board Of Trustees Of The Leland Stanford Junior University | Memory-based apparatus and method |
US20130179637A1 (en) * | 2012-01-11 | 2013-07-11 | International Business Machines Corporation | Data storage backup with lessened cache pollution |
US20150331797A1 (en) * | 2014-05-17 | 2015-11-19 | International Business Machines Corporation | Memory access tracing method |
US10331560B2 (en) * | 2014-01-31 | 2019-06-25 | Hewlett Packard Enterprise Development Lp | Cache coherence in multi-compute-engine systems |
US11256623B2 (en) * | 2017-02-08 | 2022-02-22 | Arm Limited | Cache content management |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5214766A (en) * | 1989-04-28 | 1993-05-25 | International Business Machines Corporation | Data prefetching based on store information in multi-processor caches |
US5829023A (en) * | 1995-07-17 | 1998-10-27 | Cirrus Logic, Inc. | Method and apparatus for encoding history of file access to support automatic file caching on portable and desktop computers |
US6038644A (en) * | 1996-03-19 | 2000-03-14 | Hitachi, Ltd. | Multiprocessor system with partial broadcast capability of a cache coherent processing request |
US6065058A (en) * | 1997-05-09 | 2000-05-16 | International Business Machines Corp. | Dynamic push filtering based on information exchanged among nodes in a proxy hierarchy |
US6311187B1 (en) * | 1998-12-29 | 2001-10-30 | Sun Microsystems, Inc. | Propogating updates efficiently in hierarchically structured data under a push model |
-
2000
- 2000-09-05 US US09/655,642 patent/US6711651B1/en not_active Expired - Lifetime
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5214766A (en) * | 1989-04-28 | 1993-05-25 | International Business Machines Corporation | Data prefetching based on store information in multi-processor caches |
US5829023A (en) * | 1995-07-17 | 1998-10-27 | Cirrus Logic, Inc. | Method and apparatus for encoding history of file access to support automatic file caching on portable and desktop computers |
US6038644A (en) * | 1996-03-19 | 2000-03-14 | Hitachi, Ltd. | Multiprocessor system with partial broadcast capability of a cache coherent processing request |
US6065058A (en) * | 1997-05-09 | 2000-05-16 | International Business Machines Corp. | Dynamic push filtering based on information exchanged among nodes in a proxy hierarchy |
US6311187B1 (en) * | 1998-12-29 | 2001-10-30 | Sun Microsystems, Inc. | Propogating updates efficiently in hierarchically structured data under a push model |
Non-Patent Citations (10)
Title |
---|
Abdel-Shafi et al, "An Evaluation of Fine-Grain Producer Initiated Communication in Cache Coherent Multiprocessors," 1997, Int'l. Sympoon High Performance Computer Architecture, Feb. 1-5, 1997, pp. 204-215.* * |
Falsafi et al., "Memory Sharing Predictor: The Key to a Speculative Coherent DSM", Proceedings of the 26<th >International Symposium Computer Architecture (ISCA), Atlanta, GA, May 2-4, 1999, pp. 172-183. |
Falsafi et al., "Memory Sharing Predictor: The Key to a Speculative Coherent DSM", Proceedings of the 26th International Symposium Computer Architecture (ISCA), Atlanta, GA, May 2-4, 1999, pp. 172-183. |
Hauswirth et al, "A Component and Communication Model For Push Systems," ACM Sigsoft Software Engineering Notes, vol. 24, No. 6, Nov. 1999, pp. 20-38.* * |
Hill et al., "Using Prediction to Accelerate Coherence Protocols", Proceedings of the 25<th >Annual International Symposium on Computer Architecture (ISCA), Barcelona, Spain, Jun. 27 through Jul. 2, 1998, pp. 179-190. |
Hill et al., "Using Prediction to Accelerate Coherence Protocols", Proceedings of the 25th Annual International Symposium on Computer Architecture (ISCA), Barcelona, Spain, Jun. 27 through Jul. 2, 1998, pp. 179-190. |
Koufaty et al, "Comparing Data Forwarding and Prefetching for Communication-Induced Misses in Shared-Memory MPs," Proceedings of the 12<th >International Conference on Supercomputing, 1998, pp. 53-60.* * |
Koufaty et al, "Data Forwarding In Scalable Shared-Memory Multiprocessors," IEEE Trans. on Parallel and Distributed Sys., vol. 7, No. 12, Dec. 1996, pp. 1250-1264.* * |
Koufaty et al, "Comparing Data Forwarding and Prefetching for Communication-Induced Misses in Shared-Memory MPs," Proceedings of the 12th International Conference on Supercomputing, 1998, pp. 53-60.* |
Yoon et al, "COPEN: A CoRbtA-Based Intelligent Push Engine," Proceedings 1998 Asia Pacific Software Engineering Conf., Dec. 2-4, 1998, pp. 330-337.* * |
Cited By (39)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7165128B2 (en) * | 2001-05-23 | 2007-01-16 | Sony Corporation | Multifunctional I/O organizer unit for multiprocessor multimedia chips |
US20020184450A1 (en) * | 2001-05-23 | 2002-12-05 | Shirish Gadre | Multifunctional I/O organizer unit for multiprocessor multimedia chips |
US20030154351A1 (en) * | 2001-11-16 | 2003-08-14 | Jim Nilsson | Coherence message prediction mechanism and multiprocessing computer system employing the same |
US6973547B2 (en) * | 2001-11-16 | 2005-12-06 | Sun Microsystems, Inc. | Coherence message prediction mechanism and multiprocessing computer system employing the same |
US20040075664A1 (en) * | 2002-10-22 | 2004-04-22 | Patrick Law | Hardware assisted format change mechanism in a display controller |
US9377987B2 (en) * | 2002-10-22 | 2016-06-28 | Broadcom Corporation | Hardware assisted format change mechanism in a display controller |
US20060004965A1 (en) * | 2004-06-30 | 2006-01-05 | Tu Steven J | Direct processor cache access within a system having a coherent multi-processor protocol |
US7360027B2 (en) | 2004-10-15 | 2008-04-15 | Intel Corporation | Method and apparatus for initiating CPU data prefetches by an external agent |
US20060085602A1 (en) * | 2004-10-15 | 2006-04-20 | Ramakrishna Huggahalli | Method and apparatus for initiating CPU data prefetches by an external agent |
US20060095679A1 (en) * | 2004-10-28 | 2006-05-04 | Edirisooriya Samantha J | Method and apparatus for pushing data into a processor cache |
US20060155933A1 (en) * | 2005-01-13 | 2006-07-13 | International Business Machines Corporation | Cost-conscious pre-emptive cache line displacement and relocation mechanisms |
US7454573B2 (en) | 2005-01-13 | 2008-11-18 | International Business Machines Corporation | Cost-conscious pre-emptive cache line displacement and relocation mechanisms |
US20090083492A1 (en) * | 2005-01-13 | 2009-03-26 | International Business Machines Corporation | Cost-conscious pre-emptive cache line displacement and relocation mechanisms |
US20070255908A1 (en) * | 2006-04-28 | 2007-11-01 | Sun Microsystems, Inc. | Speculative directory lookup for sharing classification |
US7373461B2 (en) | 2006-04-28 | 2008-05-13 | Sun Microsystems, Inc. | Speculative directory lookup for sharing classification |
US20080104325A1 (en) * | 2006-10-26 | 2008-05-01 | Charles Narad | Temporally relevant data placement |
US7761666B2 (en) * | 2006-10-26 | 2010-07-20 | Intel Corporation | Temporally relevant data placement |
US20080229009A1 (en) * | 2007-03-14 | 2008-09-18 | Gaither Blaine D | Systems and methods for pushing data |
US8051250B2 (en) * | 2007-03-14 | 2011-11-01 | Hewlett-Packard Development Company, L.P. | Systems and methods for pushing data |
US20090157961A1 (en) * | 2007-12-18 | 2009-06-18 | International Business Machines Corporation | Two-sided, dynamic cache injection control |
US7865668B2 (en) * | 2007-12-18 | 2011-01-04 | International Business Machines Corporation | Two-sided, dynamic cache injection control |
US7958314B2 (en) * | 2007-12-18 | 2011-06-07 | International Business Machines Corporation | Target computer processor unit (CPU) determination during cache injection using input/output I/O) hub/chipset resources |
US7958313B2 (en) * | 2007-12-18 | 2011-06-07 | International Business Machines Corporation | Target computer processor unit (CPU) determination during cache injection using input/output (I/O) adapter resources |
US20090157978A1 (en) * | 2007-12-18 | 2009-06-18 | International Business Machines Corporation | Target computer processor unit (cpu) determination during cache injection using input/output (i/o) adapter resources |
US8510509B2 (en) * | 2007-12-18 | 2013-08-13 | International Business Machines Corporation | Data transfer to memory over an input/output (I/O) interconnect |
US20090157977A1 (en) * | 2007-12-18 | 2009-06-18 | International Business Machines Corporation | Data transfer to memory over an input/output (i/o) interconnect |
US20090157979A1 (en) * | 2007-12-18 | 2009-06-18 | International Business Machines Corporation | Target computer processor unit (cpu) determination during cache injection using input/output (i/o) hub/chipset resources |
US20130097387A1 (en) * | 2011-10-14 | 2013-04-18 | The Board Of Trustees Of The Leland Stanford Junior University | Memory-based apparatus and method |
US9519549B2 (en) * | 2012-01-11 | 2016-12-13 | International Business Machines Corporation | Data storage backup with lessened cache pollution |
US20130179637A1 (en) * | 2012-01-11 | 2013-07-11 | International Business Machines Corporation | Data storage backup with lessened cache pollution |
US10331560B2 (en) * | 2014-01-31 | 2019-06-25 | Hewlett Packard Enterprise Development Lp | Cache coherence in multi-compute-engine systems |
US20150331797A1 (en) * | 2014-05-17 | 2015-11-19 | International Business Machines Corporation | Memory access tracing method |
US9928175B2 (en) * | 2014-05-17 | 2018-03-27 | International Business Machines Corporation | Identification of a computing device accessing a shared memory |
US9940237B2 (en) * | 2014-05-17 | 2018-04-10 | International Business Machines Corporation | Identification of a computing device accessing a shared memory |
US10169237B2 (en) | 2014-05-17 | 2019-01-01 | International Business Machines Corporation | Identification of a computing device accessing a shared memory |
US10241917B2 (en) | 2014-05-17 | 2019-03-26 | International Business Machines Corporation | Identification of a computing device accessing a shared memory |
US20150331795A1 (en) * | 2014-05-17 | 2015-11-19 | International Business Machines Corporation | Memory access tracing method |
US11163681B2 (en) | 2014-05-17 | 2021-11-02 | International Business Machines Corporation | Identification of a computing device accessing a shared memory |
US11256623B2 (en) * | 2017-02-08 | 2022-02-22 | Arm Limited | Cache content management |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US6711651B1 (en) | Method and apparatus for history-based movement of shared-data in coherent cache memories of a multiprocessor system using push prefetching | |
US8244988B2 (en) | Predictive ownership control of shared memory computing system data | |
CN110865968B (en) | Multi-core processing device and data transmission method between cores thereof | |
US7539823B2 (en) | Multiprocessing apparatus having reduced cache miss occurrences | |
EP1388065B1 (en) | Method and system for speculatively invalidating lines in a cache | |
US8041897B2 (en) | Cache management within a data processing apparatus | |
US7363435B1 (en) | System and method for coherence prediction | |
US6272602B1 (en) | Multiprocessing system employing pending tags to maintain cache coherence | |
US7930504B2 (en) | Handling of address conflicts during asynchronous memory move operations | |
EP3547146B1 (en) | System, method, and apparatus for detecting repetitive data accesses and automatically loading data into local cache | |
US6601144B1 (en) | Dynamic cache management in a symmetric multiprocessor system via snoop operation sequence analysis | |
US20130080709A1 (en) | System and Method for Performing Memory Operations In A Computing System | |
CN108268385B (en) | Optimized caching agent with integrated directory cache | |
KR100266314B1 (en) | Self-invalidation apparatus and method for reducing coherence overheads in a bus-based shared-memory multiprocessor | |
US20070239940A1 (en) | Adaptive prefetching | |
US11520589B2 (en) | Data structure-aware prefetching method and device on graphics processing unit | |
US8595443B2 (en) | Varying a data prefetch size based upon data usage | |
US7194586B2 (en) | Method and apparatus for implementing cache state as history of read/write shared data | |
CN1673980A (en) | Method to provide cache management commands for a DMA controller | |
US20090106498A1 (en) | Coherent dram prefetcher | |
KR20060102565A (en) | System and method for canceling write back operation during simultaneous snoop push or snoop kill operation in write back caches | |
US7464227B2 (en) | Method and apparatus for supporting opportunistic sharing in coherent multiprocessors | |
CN114020656A (en) | Non-blocking L1 Cache in multi-core SOC | |
US9558119B2 (en) | Main memory operations in a symmetric multiprocessing computer | |
CN101013399A (en) | Method for writing a data line into an l2 cache |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: INTERNATIONAL BUSINESS MACHINES CORPORATION, NEW YFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:MORENO, JAIME H.;RIVERS, JUDE A.;WELLMAN, JOHN-DAVID;REEL/FRAME:011068/0511Effective date: 20000829 |
|
FEPP | Fee payment procedure |
Free format text: PAYOR NUMBER ASSIGNED (ORIGINAL EVENT CODE: ASPN); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
FPAY | Fee payment |
Year of fee payment: 4 |
|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:INTERNATIONAL BUSINESS MACHINES CORPORATION;REEL/FRAME:026894/0001Effective date: 20110817 |
|
FPAY | Fee payment |
Year of fee payment: 8 |
|
FPAY | Fee payment |
Year of fee payment: 12 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044127/0735Effective date: 20170929 |