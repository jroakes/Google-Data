US20130346347A1 - Method to Predict a Communicative Action that is Most Likely to be Executed Given a Context - Google Patents
Method to Predict a Communicative Action that is Most Likely to be Executed Given a Context Download PDFInfo
- Publication number
- US20130346347A1 US20130346347A1 US13/568,957 US201213568957A US2013346347A1 US 20130346347 A1 US20130346347 A1 US 20130346347A1 US 201213568957 A US201213568957 A US 201213568957A US 2013346347 A1 US2013346347 A1 US 2013346347A1
- Authority
- US
- United States
- Prior art keywords
- mobile platform
- context
- location
- application
- learning
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Abandoned
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M1/00—Substation equipment, e.g. for use by subscribers
- H04M1/72—Mobile telephones; Cordless telephones, i.e. devices for establishing wireless links to base stations without route selection
- H04M1/724—User interfaces specially adapted for cordless or mobile telephones
- H04M1/72448—User interfaces specially adapted for cordless or mobile telephones with means for adapting the functionality of the device according to specific conditions
- H04M1/72454—User interfaces specially adapted for cordless or mobile telephones with means for adapting the functionality of the device according to specific conditions according to context-related or environment-related conditions
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M1/00—Substation equipment, e.g. for use by subscribers
- H04M1/60—Substation equipment, e.g. for use by subscribers including speech amplifiers
- H04M1/6008—Substation equipment, e.g. for use by subscribers including speech amplifiers in the transmitter circuit
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M1/00—Substation equipment, e.g. for use by subscribers
- H04M1/72—Mobile telephones; Cordless telephones, i.e. devices for establishing wireless links to base stations without route selection
- H04M1/724—User interfaces specially adapted for cordless or mobile telephones
- H04M1/72448—User interfaces specially adapted for cordless or mobile telephones with means for adapting the functionality of the device according to specific conditions
- H04M1/72451—User interfaces specially adapted for cordless or mobile telephones with means for adapting the functionality of the device according to specific conditions according to schedules, e.g. using calendar applications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M1/00—Substation equipment, e.g. for use by subscribers
- H04M1/72—Mobile telephones; Cordless telephones, i.e. devices for establishing wireless links to base stations without route selection
- H04M1/724—User interfaces specially adapted for cordless or mobile telephones
- H04M1/72448—User interfaces specially adapted for cordless or mobile telephones with means for adapting the functionality of the device according to specific conditions
- H04M1/72457—User interfaces specially adapted for cordless or mobile telephones with means for adapting the functionality of the device according to specific conditions according to geographic location
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04M—TELEPHONIC COMMUNICATION
- H04M2250/00—Details of telephonic subscriber devices
- H04M2250/06—Details of telephonic subscriber devices including a wireless LAN interface
Definitions
- Mobile devices are ubiquitous in modern communication networks. Many of these mobile devices are “smart phones”, capable of running one or more applications while acting as a communication device.
- the applications and/or the smart phone itself can have a number of settings subject to user control, such as volume settings, network addresses/names, contact data, and calendar information.
- a smart phone user can change some or all of these settings based on their context, such as location and activity. For example, the user can turn down a ringing volume and/or mute a ringer, prior to watching a movie at a movie theater. After the movie theater completes, the user can the turn up the ringing volume and/or un-mute the ringer.
- a machine-learning service executing on a mobile platform receives data related to a plurality of features.
- the machine-learning service determines at least one feature in the plurality of features based on the received data.
- the machine-learning service generates an output by performing a machine-learning operation on the at least one feature of the plurality of features.
- the machine-learning operation is selected from among: an operation of ranking the at least one feature, an operation of classifying the at least one feature, an operation of predicting the at least one feature, and an operation of clustering the at least one feature.
- the machine-learning service sends the output.
- a machine-learning service executing on a mobile platform receives feature-related data related to a plurality of features.
- the feature-related data includes data related to a first plurality of features received from an application executing on the mobile platform and data related to a second plurality of features received from the mobile platform.
- the first plurality of features and the second plurality of features differ.
- the machine-learning service determines at least one feature among the first plurality of features and the second plurality of feature based on the feature-related data.
- the machine-learning service generates an output by performing a machine-learning operation on the at least one feature.
- the machine-learning service sends the output to the application.
- an article of manufacture includes a non-transitory computer-readable storage medium having instructions stored thereon that, when executed on by a processor, cause the processor to perform functions.
- the functions include: (a) receiving data related to a plurality of features, (b) determining at least one feature in the plurality of features based on the received data, (c) generating an output by performing a machine-learning operation on the at least one feature of the plurality of features, where the machine-learning operation is selected from among: an operation of ranking the at least one feature, an operation of classifying the at least one feature, an operation of predicting the at least one feature, and an operation of clustering the at least one feature, and (d) sending the output.
- a mobile platform includes a processor and a non-transitory computer-readable storage medium, configured to store instructions that, when executed by the processor, cause the mobile platform to perform functions.
- the functions include: (a) receiving data related to a plurality of features, (b) determining at least one feature in the plurality of features based on the received data, (c) generating an output by performing a machine-learning operation on the at least one feature of the plurality of features, where the machine-learning operation is selected from among: an operation of ranking the at least one feature, an operation of classifying the at least one feature, an operation of predicting the at least one feature, and an operation of clustering the at least one feature, and (d) sending the output.
- a context-identification system executing on a mobile platform receives data comprising: context-related data associated with the mobile platform and application-related data received from the mobile platform.
- the context-identification system identifies at least one context using the context-related data associated with the mobile platform and/or the application-related data received from the mobile platform.
- the context-identification system predicts a communicative action associated with the mobile platform by performing a machine-learning operation on the received data.
- An instruction is received to execute the communicative action associated with the mobile platform.
- an article of manufacture includes a non-transitory computer-readable storage medium having instructions stored thereon that, when executed on by a processor, cause the processor to perform functions.
- the functions include: (a) receiving data at a context-identification system executing on a mobile platform, where the received data includes context-related data associated with the mobile platform and application-related data received from the mobile platform, (b) identifying a context using the context-related data associated with the mobile platform and/or the application-related data received from the mobile platform, (c) based on at least one context identified, predicting a communicative action associated with the mobile platform by performing a machine-learning operation on the data received; and (d) receiving an instruction to execute the communicative action associated with the mobile platform.
- a mobile platform includes a processor and a non-transitory computer-readable storage medium, configured to store instructions that, when executed by the processor, cause the mobile platform to perform functions. (a) receiving data at a context-identification system executing on a mobile platform, where the received data includes context-related data associated with the mobile platform and application-related data received from the mobile platform, (b) identifying a context using the context-related data associated with the mobile platform and/or the application-related data received from the mobile platform, (c) based on at least one context identified, predicting a communicative action associated with the mobile platform by performing a machine-learning operation on the data received; and (d) receiving an instruction to execute the communicative action associated with the mobile platform.
- a machine-learning service executing on a mobile platform receives feature-related data.
- the feature-related data includes image-related data related to one or more images received from an application executing on the mobile platform and platform-related data received from the mobile platform.
- the image-related data and the platform-related data differ.
- the machine-learning service generates a title related to the one or more images by performing a machine-learning operation on the feature-related data.
- the machine-learning service sends the title related to the one or more images to the application.
- an article of manufacture includes a non-transitory computer-readable storage medium having instructions stored thereon that, when executed on by a processor, cause the processor to perform functions.
- the functions include: (i) receiving feature-related data that includes image-related data related to one or more images and platform-related data, where the image-related data and the platform-related data differ, (ii) generating a title related to the one or more images by performing a machine-learning operation on the feature-related data, and (iii) sending the title related to the one or more images.
- a mobile platform includes a processor and a non-transitory computer-readable storage medium, configured to store instructions that, when executed by the processor, cause the mobile platform to perform functions.
- the functions include: (i) receiving feature-related data that includes image-related data related to one or more images and platform-related data, where the image-related data and the platform-related data differ, (ii) generating a title related to the one or more images by performing a machine-learning operation on the feature-related data, and (iii) sending the title related to the one or more images.
- a machine-learning service executing on a mobile platform receives feature-related data.
- the feature-related data includes communications-related data related to one or more searches for establishing electronic communications received from an application executing on the mobile platform and platform-related data received from the mobile platform.
- the communications-related data and the platform-related data differ.
- the machine-learning service determines whether the machine-learning service is trained to perform machine-learning operations related to predicting outcomes of searches for establishing electronic communications.
- the machine-learning service In response to determining that the machine-learning service is trained: (a) the machine-learning service receives a request for a predicted outcome of a search for establishing an electronic communication, (b) the machine-learning service generates the predicted outcome by performing a machine-learning operation on the feature-related data, and (c) the predicted outcome is sent to the application.
- an article of manufacture includes a non-transitory computer-readable storage medium having instructions stored thereon that, when executed on by a processor, cause the processor to perform functions.
- the functions include: (a) receiving feature-related data at a machine-learning service, where the feature-related data includes communications-related data related to one or more searches for establishing electronic communications and platform-related data received from the mobile platform, and where the communications-related data and the platform-related data differ, (b) determining whether the machine-learning service is trained to perform machine-learning operations related to predicting outcomes of searches for establishing electronic communications, (c) in response to determining that the machine-learning service is trained: (i) receiving, at the machine-learning service, a request for a predicted outcome of a search for establishing an electronic communication, (ii) generating the predicted outcome by the machine-learning service performing a machine-learning operation on the feature-related data, and (iii) sending the predicted outcome.
- a mobile platform includes a processor and a non-transitory computer-readable storage medium, configured to store instructions that, when executed by the processor, cause the mobile platform to perform functions.
- the functions include: (a) receiving feature-related data at a machine-learning service, where the feature-related data includes communications-related data related to one or more searches for establishing electronic communications and platform-related data, and where the communications-related data and the platform-related data differ, (b) determining whether or not outcomes of searches for establishing electronic communications can be predicted, (c) in response to determining that the outcomes of searches for establishing electronic communications can be predicted: (i) receiving a request for a predicted outcome of a search for establishing an electronic communication, (ii) generating the predicted outcome by performing a machine-learning operation on the feature-related data, and (iii) sending the predicted outcome.
- a machine-learning service executing on a mobile platform receives feature-related data.
- the feature-related data includes usage-related data about one or more time spans that the mobile platform is activated and platform-related data received from the mobile platform.
- the usage-related data and the platform-related data differ.
- the machine-learning service determines whether the machine-learning service is trained to perform machine-learning operations related to predicting a time span that the mobile platform will be activated.
- the machine-learning service In response to determining that the machine-learning service is trained: (a) the machine-learning service receives a request for a predicted time span that the mobile platform will be activated, (b) the machine-learning service determines the predicted time span by performing a machine-learning operation on the feature-related data, and (c) the predicted time span is sent to the application.
- an article of manufacture includes a non-transitory computer-readable storage medium having instructions stored thereon that, when executed on by a processor, cause the processor to perform functions.
- the functions include: (a) receiving feature-related data at a machine-learning service, where the feature-related data includes usage-related data about one or more time spans and platform-related data received from the mobile platform, and where the usage-related data and the platform-related data differ, (b) determining whether the machine-learning service is trained to perform machine-learning operations related to predicting a time span, (c) in response to determining that the machine-learning service is trained: (i) receiving, at the machine-learning service, a request for a predicted time span, (ii) determining the predicted time span by the machine-learning service performing a machine-learning operation on the feature-related data, and (iii) sending the predicted time span.
- a mobile platform includes a processor and a non-transitory computer-readable storage medium, configured to store instructions that, when executed by the processor, cause the mobile platform to perform functions.
- the functions include: (a) receiving feature-related data at a machine-learning service, where the feature-related data includes usage-related data about one or more time spans that the mobile platform is activated and platform-related data, and where the usage-related data and the platform-related data differ, (b) determining whether the machine-learning service is trained to perform machine-learning operations related to predicting a time span that the mobile platform will be activated, (c) in response to determining that the machine-learning service is trained: (i) receiving a request for a predicted time span that the mobile platform will be activated at the machine-learning service, (ii) determining the predicted time span by the machine-learning service performing a machine-learning operation on the feature-related data, and (iii) sending the predicted time span.
- a machine-learning service executing on a mobile platform receives feature-related data.
- the feature-related data includes volume-related data about one or more volume-related settings for the mobile platform and platform-related data received from the mobile platform.
- the volume-related data and the platform-related data differ.
- the machine-learning service determines whether the machine-learning service is trained to perform machine-learning operations related to predicting a change in the one or more volume-related settings for the mobile platform.
- the machine-learning service In response to determining that the machine-learning service is trained: (a) the machine-learning service receives a request for predicting the change in the one or more volume-related settings, (b) the machine-learning service determining the predicted change in the one or more volume-related settings by performing a machine-learning operation on the feature-related data, and (c) sending the predicted change in the one or more volume-related settings.
- an article of manufacture includes a non-transitory computer-readable storage medium having instructions stored thereon that, when executed on by a processor, cause the processor to perform functions.
- the functions include: (a) receiving feature-related data at a machine-learning service executing on the processor, where the feature-related data includes volume-related data about one or more volume-related settings and platform-related data, and where the usage-related data and the platform-related data differ, (b) determining whether the machine-learning service is trained to perform machine-learning operations related to predicting a change in the one or more volume-related settings, (c) in response to determining that the machine-learning service is trained: (i) receiving, at the machine-learning service, a request for predicting the change in the one or more volume-related settings, (ii) determining the predicted change in the one or more volume-related settings by the machine-learning service performing a machine-learning operation on the feature-related data, and (iii) sending the predicted change in the one or more volume-related settings
- a mobile platform includes a processor and a non-transitory computer-readable storage medium, configured to store instructions that, when executed by the processor, cause the mobile platform to perform functions.
- the functions include: (a) receiving feature-related data at a machine-learning service executing on the processor, where the feature-related data includes volume-related data about one or more volume-related settings and platform-related data received from the mobile platform, and where the usage-related data and the platform-related data differ, (b) determining whether the machine-learning service is trained to perform machine-learning operations related to predicting a change in the one or more volume-related settings for the mobile platform, (c) in response to determining that the machine-learning service is trained: (i) receiving, at the machine-learning service, a request for predicting the change in the one or more volume-related settings, (ii) determining the predicted change in the one or more volume-related settings by the machine-learning service performing a machine-learning operation on the feature-related data, and (iii) sending
- FIG. 1A is a flow chart of a method, in accordance with an example embodiment.
- FIG. 1B is a flow chart of another method, in accordance with an example embodiment.
- FIG. 2 is a block diagram of a mobile platform, in accordance with an example embodiment.
- FIG. 3 is a block diagram of a machine learning and adaptation service, in accordance with an example embodiment.
- FIG. 4 depicts programming interfaces for the machine learning and adaptation service, in accordance with an example embodiment.
- FIG. 5A shows an example user interface used in a first scenario, in accordance with an example embodiment.
- FIG. 5B shows example communications used in the first scenario, in accordance with an example embodiment.
- FIG. 6A shows an example user interface used in a second scenario, in accordance with an example embodiment.
- FIG. 6B shows example communications for the second scenario, in accordance with an example embodiment.
- FIG. 7 is a flow chart of a method, in accordance with an example embodiment.
- FIG. 8A shows an example user interface used in a third scenario, in accordance with an example embodiment.
- FIG. 8B shows an example data record, in accordance with an example embodiment.
- FIG. 9A shows an example context identification system, in accordance with an example embodiment.
- FIG. 9B shows an example collective context identification system, in accordance with an example embodiment.
- FIG. 9C shows an example context identification system adaptable to feedback, in accordance with an example embodiment.
- FIG. 10 is a flow chart of a method, in accordance with an example embodiment.
- FIG. 11A shows an example image identification system, in accordance with an example embodiment.
- FIG. 11B shows an example collective image identification system, in accordance with an example embodiment.
- FIG. 11C shows an example image identification system adaptable to feedback, in accordance with an example embodiment.
- FIG. 12A shows an example user interface used in a fourth scenario, in accordance with an example embodiment.
- FIG. 12B shows example communications for the fourth scenario, in accordance with an example embodiment.
- FIG. 13 is a flow chart of a method, in accordance with an example embodiment.
- FIG. 14 shows a scenario for establishing wireless communications with a number of access points, in accordance with an example embodiment.
- FIG. 15A shows an example user interface used in a fifth scenario, in accordance with an example embodiment.
- FIG. 15B shows example communications for the fifth scenario, in accordance with an example embodiment.
- FIG. 16 is a flow chart of a method, in accordance with an example embodiment.
- FIG. 17A shows a usage log of activations of a mobile platform, in accordance with an example embodiment.
- FIG. 17B shows a usage log of activations of a mobile platform at a particular location, in accordance with an example embodiment.
- FIG. 17C is a graph of a linear regression model based on the data in the usage log shown in FIG. 17B , in accordance with an example embodiment.
- FIG. 18A is a classification diagram based on the data in the log shown in FIG. 17A , in accordance with an example embodiment.
- FIG. 18B is an example user interface for a jukebox application used in a sixth scenario, in accordance with an example embodiment.
- FIG. 19 is a flow chart of a method, in accordance with an example embodiment.
- FIG. 20A shows an example user interface used in a seventh scenario, in accordance with an example embodiment.
- FIG. 20B shows example communications for the seventh scenario, in accordance with an example embodiment.
- FIG. 21 depicts a distributed computing architecture, in accordance with an example embodiment.
- FIG. 22A is a block diagram of a computing device, in accordance with an example embodiment.
- FIG. 22B depicts a cloud-based server system, in accordance with an example embodiment.
- a “machine-learning service” or machine-learning and adaptation service can support automatic adaptation of preferences of person(s) using the mobile platform.
- the machine-learning service is software running on the mobile platform that provides the necessary functionality for software applications to learn from interactions of person(s) using the mobile platform.
- the machine-learning service can communicate with software applications via an Application Program Interface (API).
- API Application Program Interface
- the API provides access to several commonly-used machine adaptation techniques.
- the API can provide access to interfaces for ranking, clustering, classifying, and prediction techniques.
- a software application can provide one or more inputs to the machine-learning service.
- a software application controlling a volume setting of a speaker can provide volume setting values as an input to the machine-learning service.
- the API can also utilize data that the software application does not and perhaps cannot access.
- a volume-setting application may not have access to location data for the mobile platform.
- the volume-setting application could request the machine-learning service to predict the volume setting of the speaker based on location.
- the machine-learning service need only provide the predictions, not the locations, to the volume-setting application.
- the machine-learning service can encapsulate the use of sensitive data.
- the machine-learning service can include a data aggregation and representation engine (DARE) that constantly receives and stores input data, perhaps from multiple sources.
- DARE data aggregation and representation engine
- the stored input data can be aggregated to discover features within the data; such as location labels based on actions and times; e.g. Home, Work, School, etc.
- the machine adaptation techniques used by the machine-learning service can be implemented to work best within the processing, memory, and other resource constraints of a mobile platform.
- the machine adaptation techniques can use incremental learning algorithms that require limited or no historical information for training, and thus may reduce the total amount of memory needed by the machine-learning-service.
- the machine-learning service can utilize network support functionality to access non-private and/or anonymized data aggregated across multiple mobile platforms.
- the aggregated data can be stored in one or more servers, or other devices other than the mobile platform, and downloaded as needed.
- aggregated data can be used to train and/or set initial values for the machine adaptation techniques used by the machine-learning service.
- Example uses of the machine-learning service can include:
- Predicting a phone number to be dialed at the onset of utilizing a phone dialing application based on location, time, calendar entries, prior behavior, etc.
- Predicting speaker and/or mute settings for the mobile platform based on location, time, calendar entries, prior behavior, etc.
- the machine-learning service can make mobile platforms easier to use, more efficient from a user's point of view, and save users time and effort in utilizing the variety of applications available on the mobile platform.
- FIG. 1A is a flow chart of method 100 , in accordance with an example embodiment.
- part or all of method 100 can be executed using one or more mobile platforms; e.g., mobile platform 200 , 502 , 602 , 802 , 1202 , 1502 , 1852 , 2002 and/or one or more computing devices; e.g., computing device 2200 .
- Method 100 begins at block 110 , where a machine-learning service executing on a mobile platform can receive data related to a plurality of features.
- the received data can include data aggregated from a plurality of mobile platforms.
- the machine-learning service can determine at least one feature in the plurality of features based on the received data.
- the machine-learning service can generate an output by the performing a machine-learning operation on the at least one feature of the plurality of features.
- the machine-learning operation can be selected from among: an operation of ranking the at least one feature, an operation of classifying the at least one feature, an operation of predicting the at least one feature, and an operation of clustering the at least one feature.
- generating the output by the machine-learning service can include selecting a machine-learning algorithm to perform the machine-learning operation.
- selecting the machine-learning algorithm to perform the machine-learning operation can include the machine-learning service selecting the machine-learning algorithm.
- selecting the machine-learning algorithm to perform the machine-learning operation can include receiving a selection related to the machine-learning algorithm from an application executing on the mobile platform, where the application differs from the machine-learning service.
- receiving a selection related to the machine-learning algorithm from the application can include receiving a selection related to the machine-learning algorithm from the application via an Application Programming Interface (API) of the machine-learning service.
- API Application Programming Interface
- the machine-learning service can send the output.
- the received data can include data related to a plurality of features related to communication signals and locations, where the mobile platform is located at a first location, and where the output includes an indication of whether to search for the communications signal at the first location.
- the received data can include data related to a plurality of features related to durations of communication sessions and the output can include a prediction of a duration of a new communication session.
- the received data can include data related to features of telephone calls originated by the mobile platform and the output can include a called party of a telephone call to be originated by the mobile platform.
- the received data can include data related to volume and mute settings of the mobile platform and the output can include a prediction of a volume setting and/or a mute setting of the mobile platform.
- the received data can include data related to one or more images, and the output can include a name related to at least one image of the one or more images.
- method 100 can further include: (i) storing the at least one feature and the output for use by the machine-learning service, (ii) receiving second data related to a second plurality of features at the machine-learning service, (iii) determining at least a second feature in the second plurality of features based on the second data, (iv) generating a second output by the machine-learning service performing the machine-learning operation on the at least the second feature and the stored at least one feature and output, and (v) sending the second output from the machine-learning service.
- FIG. 1B is a flow chart of method 150 , in accordance with an example embodiment.
- part or all of method 150 can be executed using one or more mobile platforms; e.g., mobile platform 200 , 502 , 602 , 802 , 1202 , 1502 , 1852 , 2002 and/or one or more computing devices; e.g., computing device 2200 .
- Method 150 begins at block 160 , where a machine-learning service executing on a mobile platform can receive feature-related data.
- the feature-related data can include data related to a first plurality of features received from an application executing on the mobile platform and data related to a second plurality of features received from the mobile platform.
- the first plurality of features and the second plurality of features can differ.
- the first plurality of features can include a feature selected from among a time, a location, a duration, a signal strength, a power level, a party of a telephone call, an e-mail address, a contact, and a calendar entry.
- the second plurality of features can include a feature selected from among an image-related feature and a speech-related feature.
- the machine-learning service can determine at least one feature from among the first plurality of features and the second plurality of features based on the feature-related data.
- the machine-learning service can generate an output by performing a machine-learning operation on the at least one feature of the plurality of features.
- the machine-learning service can send the output to the application.
- FIG. 2 is a block diagram of mobile platform 200 , in accordance with an example embodiment.
- Mobile platform 200 includes mobile platform system 210 , machine learning and adaptation service 220 , system application(s) 230 , and user application(s) 240 .
- FIG. 2 shows that mobile platform system 210 includes mobile platform hardware 212 and operating system software 214 .
- Mobile platform hardware 212 can include hardware of a computing device, such as computing device 2200 described below.
- Operating system software 214 can include, but is not limited to software configured to utilize mobile platform hardware 212 , provide user services such as user interfaces for the mobile platform, allocate resources, and schedule processes and/or threads.
- An example of operating system software 214 is the Android operating system developed by Google Inc. of Mountain View, Calif. Additional hardware and/or software can be used by mobile platform system 210 as well.
- System application 230 can be an application provided by a provider of part or all of mobile platform system 210 .
- User application 240 can be an application provided by an entity other than a provider of part or all of mobile platform system 210
- Machine learning and adaptation service 220 can be configured to provide the functionality of a machine-learning service.
- FIG. 2 shows that machine learning and adaptation service 220 can communicate with both mobile platform system 210 and applications 230 , 240 via feature-related data (FRD) and machine-learning operation output (MLOO).
- FFD feature-related data
- MLOO machine-learning operation output
- Feature-related data can be “built-in” or provided by mobile platform system 210 ; e.g., as feature-related data 222 .
- Built-in feature related data can include, but are not limited to, times, dates, locations, settings for mobile platform hardware 212 (ringing volume, mute settings, ring tones, brightness levels, power/battery data, etc.), calling data (calling party, called party, dialed digits, calling state, etc.), power/battery data, communication network data (addresses, networks, login/password information, signal strengths, etc.), current application(s) being executed, and user preference information.
- Machine learning and adaptation service 220 can determine one or more features from feature-related data 222 .
- feature-related data 222 includes the text of a Short Message Service (SMS) message stating “I'm outta here.”
- Machine learning and adaptation service 220 can generate features represented as feature vectors.
- An example feature vector for SMS messages can be a ⁇ word, frequency> pair.
- a set of corresponding example feature vectors can be ⁇ I'm, 1>, ⁇ outta, 1>, ⁇ here, 1> ⁇ .
- a set of corresponding example feature vectors can be ⁇ Scott-, 1>, ⁇ Either, 1>, ⁇ we, 2>, ⁇ go, 2>, ⁇ now, 1>, ⁇ or, 1>, ⁇ after, 1>, ⁇ John's, 1>, ⁇ talk, 1> ⁇ .
- the list of suggested labels can be provided as features to machine learning and adaptation service 220 .
- An example feature can be a ⁇ higher ranked feature, lower ranked feature> feature vector, and as applied to the list of suggested labels, the following feature vectors can be determined ⁇ W4, W1>, ⁇ W4, W2>, ⁇ W4, W3>, ⁇ W4, W5> . . . ⁇ W4, Wn>.
- Many other examples of features, feature vectors, and feature-related data are possible as well.
- feature-related data can include commands to machine learning and adaptation service 220 , such as a command to “train” or learn about input data and perform one or more machine learning operations on the learned input data.
- machine learning and adaptation service 220 can receive a command to cluster a series of locations, which can be provided as latitude/longitude pairs, perhaps using cluster labels such as “Work”, “Home”, “School”, and “Gym”. Then, once trained, machine learning and adaptation service 220 can output one or more machine-learning operation outputs 224 .
- the corresponding machine-learning operation output 224 can be a cluster label.
- Machine learning and adaptation service 220 can also train with and utilize feature-related data provided by system application 230 as feature-related data 232 and/or feature-related data provided by user application 240 as feature-related data 242 . Then, upon receiving input data, machine learning and adaptation service 220 can provide machine-learning operation output(s) to system application 230 as machine-learning operation output 234 and/or provide machine-learning operation output(s) to user application 240 as machine-learning operation output 244 .
- machine learning and adaptation service 220 need not have detailed information about feature-related data 222 , 232 , 242 a priori; rather machine learning and adaptation service 220 can utilize incremental learning techniques to generate a model of feature-related data 222 , 232 , 242 , and update the model as needed based on feature-related data 222 , 232 , 242 .
- machine learning and adaptation service 220 need not have detailed information about applications 230 , 240 a priori. Rather, applications 230 , 240 can use interface(s) to machine learning and adaptation service 220 to permit use of machine learning and adaptation service 220 as a toolkit of machine-learning techniques.
- FIG. 3 is a block diagram of machine learning and adaptation service 220 , in accordance with an example embodiment.
- Machine learning and adaptation service 220 includes machine learning and adaptation service API 310 , machine learning and adaptation engine (MLAE) 312 , data aggregation and representation engine (DARE) 314 , service manager 316 , and machine learning and adaptation service (MLAS) network support 320 .
- MLAE machine learning and adaptation engine
- DARE data aggregation and representation engine
- MLAS machine learning and adaptation service
- Machine learning and adaptation service API 310 provides interfaces to access a number of machine learning and adaptation techniques and to exchange data with machine learning and adaptation service 220 .
- models for machine learning and adaptation can be saved and loaded via machine learning and adaptation service API 310 .
- Machine learning and adaptation engine 312 performs the machine learning and adaptation techniques of machine learning and adaptation service 220 .
- machine learning and adaptation engine 312 can learn data from one or more sources, and then classify, cluster, rank, and/or predict data from given input data.
- Classifying data involves putting data with a number N of possible values into one of C1 pre-determined categories, where C1 is finite.
- C1 is finite.
- a mobile platform can be classified, for each value T of a set of times, into one of two categories: either “powered up” or “not powered up”.
- a location specified as a latitude/longitude pair (or via another technique) can be classified into one of three categories: “having a strong accessible Wi-Fi signal”, “having an adequate accessible Wi-Fi signal”, or “having little or no accessible Wi-Fi signal.”
- Many other examples are possible as well.
- Classification can be performed using one or more statistical classification techniques, such as, but not limited to, linear classifiers, support vector machines, quadratic classifiers, kernel estimation, decision trees, neural networks, Bayesian techniques and/or networks, hidden Markov models, binary classifiers, and/or multi-class classifiers.
- statistical classification techniques such as, but not limited to, linear classifiers, support vector machines, quadratic classifiers, kernel estimation, decision trees, neural networks, Bayesian techniques and/or networks, hidden Markov models, binary classifiers, and/or multi-class classifiers.
- Clustering data involves putting data with a number N of possible values into one of C2 clusters, where C2 is finite, and where the clusters are not necessarily pre-determined.
- each data item in a given cluster is more similar to each other than to data item(s) in other cluster(s).
- a mobile platform can track its location throughout the day to find clusters of locations where the mobile platform can be commonly found, such as work location(s), home location(s), shopping location(s), entertainment location(s), and other location(s).
- Location clusters can vary from person to person; an attorney who works at a law office and then frequents a restaurant may consider the law office as a “work location” and the restaurant as an “entertainment location”, but chef may consider the law office as an “other location” and the restaurant as a “work location.”
- Clustering can be performed using one or more clustering algorithms, such as, but not limited to, connectivity-based clustering, hierarchical clustering, centroid-based clustering, distribution-based clustering, density-based clustering, and partitioning algorithms.
- clustering algorithms such as, but not limited to, connectivity-based clustering, hierarchical clustering, centroid-based clustering, distribution-based clustering, density-based clustering, and partitioning algorithms.
- Ranking a set of data items of size S involves applying a ranking function RF to the S data items and returning the highest (or lowest) N ranked data items, where 0 ⁇ N ⁇ S. For example, suppose a co-ed volleyball team had the following statistics for points scored:
- Another ranking example is ranking documents based on a query of one or more keywords.
- the ranking of a document D out of a total of N documents can express the relative relevance of document D to the query.
- rankings can have two or more partitions or levels. For example, given a query with keywords K1, K2, . . . Kn, documents can first be ranked as relevant or irrelevant based on keywords K1-Kn. Then, a second ranking function can rank the subset of relevant documents, perhaps in more detail.
- Ranking can be performed using one or more ranking algorithms, such as, but not limited to, instance ranking algorithms, label ranking algorithms, subset ranking algorithms, rank aggregation algorithms, bipartite/k-partite ranking algorithms, and learning-to-rank algorithms.
- instance ranking algorithms such as, but not limited to, instance ranking algorithms, label ranking algorithms, subset ranking algorithms, rank aggregation algorithms, bipartite/k-partite ranking algorithms, and learning-to-rank algorithms.
- Prediction can be performed using one or more prediction algorithms, such as, but not limited to, minimax, decision tree algorithms, classification tree algorithms, linear regression, polynomial regression, statistical regression, curve fitting, interpolation, and maximum likelihood estimation algorithms.
- Data aggregation and representation engine 314 aggregates, or combines, data from applications and the mobile platform system and stores the aggregated data in a model accessible to machine learning and adaptation engine 312 .
- aggregation of data enables discovery of new inferences based on the combined data that were difficult to observe in the raw data. For example, suppose a mobile platform is located in the labeled locations listed in Table 2 below:
- these data indicate that the mobile platform is at work, home, or traveling. That is, the mobile platform spends 80% of the time at work and home and more than 90% of the time at work, home, or (likely) in a vehicle. Based on this information, the mobile platform likely has easy access to commercial electrical power at least 80% of the time, and so battery life may not be as important to this mobile platform, as compared to, say, being able to recharge a battery of the mobile phone from commercial electrical power and while driving.
- Service manager 316 can manage learning session(s), or instances of one or more machine learning and adaptation models, for the application(s) utilizing machine learning and adaptation service 220 .
- Service manager 316 can also coordinate permissions for access and/or display of data.
- service manager 316 can prioritize learning sessions and/or resources used by machine learning and adaptation service 220
- Machine learning and adaptation service network support 320 can provide “back end” support to access non-private and/or anonymized data that has been authorized to be gathered from a plurality of mobile platforms.
- the non-private and/or anonymized data can be resident on mobile platform 200 , while in other scenarios, part or all of the non-private and/or anonymized data can reside on other devices, such as servers and/or other mobile platforms.
- mobile platform 200 may temporarily or permanently store the non-private and/or anonymized data, perhaps depending on the access to the non-private and/or anonymized data provided to mobile platform 200 and/or machine learning and adaptation service 220 .
- the non-private and/or anonymized data can be used by machine learning and adaptation service 220 , but not shared or “hidden” from applications.
- a built-in manager can enable access to built-in features for machine learning and adaptation service 220 while hiding some or all built-in features from applications.
- FIG. 4 depicts programming interfaces 400 related to machine learning and adaptation service 200 , in accordance with an example embodiment.
- Applications 402 a , 402 b , 402 c communicate with service manager 316 to manage learning sessions.
- Each learning session is shown in FIG. 4 as a learning session thread.
- learning session threads 432 a , 432 b , 432 c respectively communicate with applications 402 a , 402 b , and 402 c to conduct a learning session.
- an application can conduct zero learning sessions or conduct multiple learning sessions simultaneously.
- FIG. 4 shows that service manager 316 includes built-in features 410 , learning application programming interface (API) 412 , and session information 420 .
- built-in features 410 , learning API 412 , and/or session information 420 can be components of other portions of machine learning and adaptation service 220 .
- built-in features 410 , learning API 412 , and/or session information 420 can be separate components of machine learning and adaptation service 220 .
- built-in features 410 can provide various types of data to service manager 316 and, more generally, to machine learning and adaptation service 220 .
- FIG. 4 shows examples of data provided by built-in features 410 , such as location, location labels, application statistics, people-related features, calling information, battery/power data, network information, and historical/usage information.
- Locations can be specified in terms of latitude/longitude pairs, latitude/longitude/altitude triples, street addresses, intersections, and/or using other location-specifying data.
- Location labels can include names such as the “Locations” shown in Table 2 above; e.g., work, home, stores, etc.
- Application statistics can include a time T an application App was last executed, a location loc an application was last executed, a probability P(App
- the above-mentioned application statistics can be generated by statistical software operating on data regarding application execution times and locations. Other statistics can be determined as application statistics as well or instead.
- Information about people can be determined from contact information, calendar information, social networking accounts and pages, and from other sources.
- information such as, but not limited to a person's name, address, phone number(s), e-mail address(es), social networking account information can be provided to machine learning and adaptation service 220 .
- Calling information can include per-call information such as calling party, dialed digits/called party information, call duration, call start time, and call end time.
- calling information can also or instead include contact information, such as a name and/or “Caller-ID” data associated with a called party, and/or historical calling information for a number of previous calls NPC, where NPC>0.
- other types of communication information can be provided as part of built-in features 410 .
- Examples of other types of communication information include, but are not limited to, e-mail information, SMS information, blog information, and social networking messaging information.
- Battery/power information can include, but is not limited to, information about batteries and/or power for the mobile platform, such as battery levels, battery status (e.g., operating, disconnected, charging), commercial power status (e.g., connected, charging, disconnected), time(s) of connection(s) to and/or disconnection(s) from commercial power.
- Network information can include, but is not limited to, information about telecommunications networks, local area networks, other networks, network addresses, connection time(s), disconnection time(s), and network status (e.g., connecting, connected, acquiring address(es), communicating data, idle, disconnecting, disconnected).
- Historical/usage information can include, but is not limited to, information about previous usage of machine learning and adaptation service 220 .
- Example information includes a number of features “pushed” or provided to machine learning and adaptation service 220 , a number of items “pulled” or requested from machine learning and adaptation service 220 , a number of applications currently using machine learning and adaptation service 220 , a number of applications that have used machine learning and adaptation service 220 in a unit period of time (e.g., a number of application using machine learning and adaptation service 220 per hour or per day), and maximum and/or minimum numbers of applications currently using machine learning and adaptation service 220 .
- Learning API 412 can include at least the functions shown in FIG. 4 : getService( ), RankerInterface( ), PredictionInterface( ), ClassificationInterface( ), and ClusteringInterface( ). In some embodiments, more or fewer functions can be part of learning API 412 .
- An application can call the getService( ) function to initialize and start a learning session; e.g., initialize and create a learning session thread.
- the getService( ) function can return a learning session key that can specify a specific learning session.
- FIG. 4 shows learning API 412 with four example learning interfaces: RankerInterface( ), PredictionInterface( ), ClassificationInterface( ), and ClusteringInterface( ). In some embodiments, more or fewer learning interfaces can be part of learning API 412 .
- An application can call the RankerInterface( ) function to initialize and generate a ranking model of machine learning and adaptation service 220 .
- the ranking model can be used to order or rank a set of data items of size S by applying a ranking function RF to the S data items and returning the highest (or lowest) N ranked data items, such as discussed above in the context of FIG. 3 .
- An application can call the PredictionInterface( ) function to initialize and generate a prediction model of machine learning and adaptation service 220 .
- the prediction model can be used to determine a predicted value given a previous pattern of values, such as discussed above in the context of FIG. 3 .
- An application can call the ClassificationInterface( ) function to initialize and generate a classification model of machine learning and adaptation service 220 .
- the classification model can be used to put data with a number N of possible values into one of C1 pre-determined categories, where C1 is finite, such as discussed above in the context of FIG. 3 .
- An application can call the ClusteringInterface( ) function to initialize and generate a clustering model of machine learning and adaptation service 220 .
- the clustering model can be used to put data with a number N of possible values into one of C2 clusters, where C2 is finite, and where the clusters are not necessarily pre-determined, such as discussed above in the context of FIG. 3 .
- an application can use only one learning interface per learning session, while in other embodiments, the application can use multiple learning interfaces per learning session.
- Each of the learning interfaces shown in learning API 412 can include at least the functions of example learning interface 440 .
- FIG. 4 shows learning interface 440 with five functions: Push( ), Pull( ), Notify( ), Save( ), and Load( ). In some embodiments, more or fewer functions can be part of learning interface 440 .
- the Push( ) function can enable an application to provide data to a learning session.
- the Pull( ) function can enable an application to request and receive information, such as inference(s) and/or prediction(s), from a learning session.
- the Notify( ) function can enable an application to receive information, such as inference(s) and/or prediction(s), from a learning session without requesting the information. For example, suppose a learning session is informed about an event of interest to the application and consequently uses its learning model to generate a prediction for use by the application. In this example, the application has not requested the prediction, i.e., the application did not call the Pull( ) function. Therefore, the learning session can provide the prediction to the application using the Notify( ) function.
- information such as inference(s) and/or prediction(s
- the Save( ) function can save learning-model data needed to start/restart a learning model, and provide a copy of the learning-model data to the application.
- the Load( ) function can take a copy of learning-model data from the application and to start/restart a learning model with the learning-model data.
- Session information 420 can include information about each learning session conducted by machine learning and adaptation service 220 .
- Example session information 420 can include, but is not limited to, a learning session key for the learning session, information about the application such as an application name, application addressing information, type(s) of learning interface(s) used in the learning session (e.g., RankerInterface( ), PredictionInterface( )), usage information for the learning interface(s), and perhaps data stored by the learning model for the learning session, such as a learning model cache.
- service manager 316 can manage learning session(s) for the application(s) utilizing machine learning and adaptation service 220 .
- service manager can enable communications between applications 402 a - 402 c , data aggregation and representation engines 314 a , 314 b and learning session threads 432 a - 432 c .
- service manager 316 can ensure that information from a given application is directed to the corresponding learning session thread, and not routed to another learning session thread, and vice versa.
- session information 420 can include a mapping between session keys, applications, and learning session threads.
- Each of data aggregation and representation engines 314 a , 314 b aggregates, or combines, data from applications 402 a - 402 c and the mobile platform system and stores the aggregated data in a model accessible to appropriate learning session thread(s) 432 a - 432 c , such as discussed above in the context of FIG. 3 .
- Each of learning session threads 432 a , 432 b , and 432 c implements a learning model.
- FIG. 4 shows that learning session threads 432 a , 432 b , and 432 c each respectively include: machine learning and adaptation engine (MLAE) 312 a , 312 b , 312 c , feature buffer 434 a . 434 b , 434 c , and context 436 a , 436 b , 436 c .
- MLAE machine learning and adaptation engine
- Each of machine learning and adaptation engines 312 a , 312 b , 312 c performs the machine learning and adaptation technique discussed above in the context of FIG. 3 and this figure, FIG. 4 .
- Each of feature buffers 434 a , 434 b , 434 c stores feature(s) used to train and/or perform functions for the corresponding machine learning and adaptation engines 312 a , 312 b , 312 c.
- Each of contexts 436 a , 436 b , and 436 c stores a learning model that embodies classifications, predictions, clusterings, or rankings learned by the corresponding machine learning and adaptation engines 312 a , 312 b , 312 c .
- a respective context can be saved by the application using the Save( ) function of learning interface 440 , and the respective context can be loaded using the Load( ) function of learning interface 440 .
- FIG. 5A shows example user interface 510 used in example scenario 500 , in accordance with an example embodiment.
- Scenario 500 includes user interface 510 executing on mobile platform 502 .
- user interface 510 includes an interface for dialer application 520 that includes microphone dialog 522 .
- Microphone dialog 522 includes manual microphone setting 524 , smart microphone setting 526 , an OK button to save current settings before exiting dialog 522 , and a Cancel button to exit dialog 522 without saving current settings.
- Manual microphone setting 524 includes a slider bar that enables a user to set a microphone output setting between a minimum setting of 0, which effectively mutes the microphone, and a maximum setting of 100, which provides the maximum output volume (a.k.a. maximum gain) for a given input.
- FIG. 5A shows manual microphone setting set to 53, or approximately halfway between the minimum and maximum settings.
- smart microphone setting 526 can learn and set microphone volume based on called party and can be set to either enabled or disabled.
- dialer application 520 can instruct a microphone application (not shown in FIG. 5A ) to use manual microphone setting 524 to determine output volume for a microphone of mobile platform 502 .
- dialer application 520 can use a learning service of machine learning and adaptation service 220 to perform a machine-learning operation to provide setting values for the microphone setting. Then, upon receiving a setting value, dialer application 520 can provide the setting value to the microphone application, which can then determine output volume for the microphone of mobile platform 502 using the setting value.
- smart microphone setting 526 is set to enabled.
- FIG. 5B shows example communications for scenario 500 , in accordance with an example embodiment.
- learning model 530 is partially trained based on data stored in a “DialerModel” variable and then loaded into learning model 530 via a Load( ) function call.
- communications between dialer application 520 and learning model 530 of machine learning and adaptation service 220 indicate that dialer application 520 had access to calling party information.
- microphone settings can be requested by dialer application 520 and provided by learning model 530 .
- learning model data is stored back in the DialerModel variable to preserve information garnered during scenario 500 for later use and loading into learning model 530 .
- Scenario 500 continues with dialer application 520 calling the Load( ) function via communication 556 to provide stored learning model data, e.g., a context, stored in a DialerModel variable passed as a parameter of the Load( ) function to learning model 530 .
- learning model 530 can load a context and perhaps other data stored in the DialerModel variable into a learning session thread that can be utilized by learning model 530 . In some scenarios not shown in FIG.
- learning model 530 can provide a response to communication 556 , such as a return value to the Load( ) function and/or a communication providing a status of the Load( ) function; e.g., LoadOK or LoadFail.
- Dialer application 520 can instruct learning model 530 , via communication 558 , to set up a prediction interface.
- communication 558 includes a PredictionInterface( ) call with three parameters: session key S1, an input-source reference of CalledParty, and a requested output prediction of microphone volumes, as shown in FIG. 5B , using the predefined MIC_VOL value.
- learning model 530 sends communication 560 to built-in features 540 to request a built-in feature (ReqBI) of microphone values, as shown in FIG. 5B by passing the predefined MIC_VOL value to built-in features 540 .
- Built-in features 540 can include at least the functionality of built-in features 410 discussed above.
- built-in features 540 returns a built-in session key BI1 via communication 562 .
- Built-in session key BI1 is included with subsequent communications between learning model 530 and built-in features 540 to permit addressing the subsequent built-in related communications to the correct learning model.
- FIG. 5B continues with dialer application 520 sending communication 564 to learning model 530 with a Push( ) function to inform learning model 530 that a call is being made to a called party “Grandma.” Additionally, built-in features 540 sends communication 566 , with a Push( ) function to inform learning model 530 that the microphone volume is being set to a setting of 80.
- FIG. 5B shows a number of communications 568 - 586 for five subsequent calls made with the calling parties indicated by dialer application 520 and microphone volume indicated by built-in features 540 to learning model 530 with the values listed in Table 3 below.
- Scenario 500 continues with dialer application 520 sending communications 588 and 590 a to learning model 530 .
- Communication 588 uses the Push( ) function to inform learning model 530 that a call is being made to a called party “Boss.”
- Communication 590 a requests, via the Pull( ) function, a predicted value for the MIC_VOL (microphone volume) setting.
- learning model 530 predicts a microphone volume setting of 37 based on data (a) stored in the DialerModel variable and loaded as a consequence of communication 556 and (b) learned from communications 564 - 586 , particularly communications 568 , 570 , 576 , and 578 which relate to calls to a called party of “Boss.” Subsequently, learning model 530 can send a Pull Response (PullResp) as communication 590 b to dialer application 520 providing the predicted microphone volume setting of 37. In some embodiments, the Pull Response can be a return value of the Pull( ) function of communication 590 a . In response to communication 590 b , dialer application 520 instructs microphone application 550 to set the microphone volume to 37 via a SetMicVol( ) function communicated in communication 590 c.
- PullResp Pull Response
- dialer application 520 instructs microphone application 550 to set the microphone volume to 37 via a SetMicVol( ) function communicated in communication 590
- learning model 530 can communicate to an application, such as dialer application 520 , once the learning model has been trained and is ready to perform machine-learning operations, such as predicting microphone volume levels.
- learning model 530 is not trained sufficiently to provide a prediction in response to communication 590 a .
- learning model 530 can inform the application that the learning model is insufficiently trained to perform machine-learning operations, such as predicting microphone volume settings.
- microphone application 550 can provide a response, such as a function return value or communication (e.g., “SetMicVolOK” or “SetMicVolFail”) to the SetMicVol( ) command.
- FIG. 5B shows scenario 500 continuing by dialer application 520 sending communications 592 and 594 a to learning model 530 .
- Communication 592 informs learning model 530 that a call is being made to a called party “Grandpa” and communication 594 a requests a predicted value for the microphone volume setting.
- learning model 530 predicts a microphone volume setting of 81 based on data stored in the DialerModel variable and learned from communications 564 - 586 , particularly communications 584 and 586 which relate to a call to a called party of “Grandpa.”
- learning model 530 can send a Pull Response as communication 594 b to provide the predicted microphone volume setting of 81 to dialer application 520 .
- dialer application 520 can instruct microphone application via communication 594 c to set the microphone volume to 81.
- Scenario 500 concludes with dialer application 520 call Save( ) function via communication 596 to request that learning model 530 save a context, and perhaps other data, of a learning session thread in the DialerModel variable.
- learning model 530 can provide a response to communication 596 , such as a return value to the Save( ) function and/or a communication providing a status of the Save( ) function; e.g., SaveOK or SaveFail.
- FIG. 6A shows example user interface 610 used in scenario 600 , in accordance with an example embodiment.
- Scenario 600 includes user interface 610 executing on mobile platform 602 .
- user interface 610 includes microphone dialog 622 as an interface for microphone application 620 .
- Microphone dialog 622 includes manual microphone setting 624 , smart microphone setting 626 , an OK button to save current settings before exiting dialog 622 , and a Cancel button to exit dialog 622 without saving current settings.
- Manual microphone setting 624 includes a slider bar that enables a user to set a microphone output setting to between a minimum setting 0, which effectively mutes the microphone, and a maximum setting of 100, which provides the maximum output volume (a.k.a. maximum gain) for a given input.
- FIG. 6A shows manual microphone setting 624 set to 48, or approximately halfway between the minimum and maximum settings.
- smart microphone setting 626 can be set to either enabled or disabled and used to learn and set microphone volume based on called party.
- microphone application 620 can use manual microphone setting 624 to determine an output volume for a microphone of mobile platform 602 .
- microphone application 620 can use a learning service of machine learning and adaptation service 220 to perform a machine-learning operation to provide setting values.
- microphone application 620 can determine output volume for the microphone of mobile platform 602 .
- smart microphone setting 626 is set to enabled.
- FIG. 6B shows example communications for scenario 600 , in accordance with an example embodiment.
- Scenario 600 has similar communications to those of scenario 500 shown in FIG. 5B , but there are two substantive differences.
- dialer application 520 has access to calling party information.
- microphone application 620 does not have access to calling party information during scenario 600 ; thus, calling party information is hidden from microphone application 620 by learning model 630 .
- the second substantive difference is that in scenario 500 , learning model 530 was trained in part using data stored in a DialerModel variable. In scenario 600 , no stored model data is loaded into learning model 630 .
- communications in scenario 600 begin with microphone application 620 calling the getService( ) function of learning model 630 via communication 652 .
- learning model 630 provides a session key S2 via communication 654 to microphone application 620 .
- Session key S2 is included for subsequent communications between microphone application 620 and learning model 630 to permit addressing the subsequent communications to the correct learning model, e.g., learning model 630 , and the correct application, e.g., microphone application 620 , for a learning session keyed by S2.
- Microphone application 620 can instruct learning model 630 , via communication 656 , to set up a prediction interface.
- communication 658 includes a PredictionInterface( ) call with three parameters: session key 51 , an input-source reference MicVol, and a requested output prediction of microphone volumes, as shown in FIG. 6B , using the predefined MIC_VOL value.
- the requested output prediction of microphone volumes is based on called parties, as indicated by the fourth parameter CALLED_PARTY to the PredictionInterface( ) function.
- the value of CALLED_PARTY can be predefined.
- the output prediction can be based on multiple values as requested via the PredictionInterface( ) function.
- learning model 630 sends communication 658 to built-in features (BIFs) 640 to request built-in feature (ReqBI) of called party values, as shown in FIG. 6B by passing the predefined CALLED_PARTY value to built-in features 640 .
- Built-in features 640 can include at least the functionality of built-in features 410 discussed above.
- built-in features 640 returns a built-in session key BI2 to learning model 630 via communication 660 .
- Built-in session key BI2 is included with subsequent communications between learning model 630 and built-in features 640 to permit addressing the subsequent built-in-feature related communications to the correct learning model.
- FIG. 6B continues with microphone application 620 sending communication 662 to learning model 630 , via the Push( ) function, to indicate that the microphone volume is being set to a setting of 80. Additionally, built-in features 640 can send communication 664 , via the Push( ) function, to learning model 630 to indicate that a call is being made to a called party “Grandma.”
- Scenario 600 continues with learning model 630 receiving communication 680 from built-in features 640 .
- Communication 680 informs learning model 630 that a call is being made to a called party “Boss.”
- learning model 630 predicts a microphone volume setting of 40 based on data learned from communications 662 - 678 , particularly communications 666 , 668 , and 672 which relate to calls to a called party of “Boss.” Subsequently, learning model 630 can send communication 682 including the Notify( ) function to indicate the predicted microphone volume setting of 40 to microphone application 620 . In response to communication 682 , microphone application 620 can perform function 684 to set the microphone volume to 40.
- Scenario 600 continues with learning model 630 receiving communication 686 from built-in features 640 , which informs learning model 630 that a call is being made to a called party “Grandpa.”
- learning model 630 predicts a microphone volume setting of 82 based on data learned from communications 662 - 678 , particularly communications 674 and 678 which relate to a call to a called party of “Grandpa.”
- learning model 630 can send communication 688 including the Notify( ) function to indicate the predicted microphone volume setting of 82 to microphone application 620 .
- microphone application 620 can perform function 690 to set the microphone volume to 40.
- FIG. 7 is a flow chart of method 700 , in accordance with an example embodiment.
- part or all of method 700 can be executed using one or more mobile platforms; e.g., mobile platform 200 , 502 , 602 , 802 , 1202 , 1502 , 1852 , 2002 and/or one or more computing devices; e.g., computing device 2200 .
- Method 700 begins at block 710 , where a context-identification system executing on a mobile platform can receive data.
- the received data can include context-related data associated with the mobile platform and application-related data received from the mobile platform.
- the context-related data and the application-related data can differ.
- context-related data can be received from a network.
- the network can include a server, a cloud-based server, and/or a database with context-related data stored dynamically, among other possibilities to provide context-related data.
- a mobile platform can connect to such a network to receive context-related data continuously and/or periodically, possibly by executing a network connection application.
- context-related data can be received by a network with multiple programmable devices sitting on different nodes of the network, possibly as described with respect to FIG. 21 . In such instances, context-related data can be received through the network from multiple other programmable devices communicating with the network.
- the context-related data can be received and/or generated by a context-identification system (CIS) executing on an exemplary mobile platform.
- the CIS can be configured to extract context-related data from information streaming to the mobile platform, possibly from a network as described above.
- the CIS may be initiated by a context-related application executed on the mobile platform.
- An exemplary context-identification system is further described with respect to FIG. 9A .
- a context-related application may be executed to cause the CIS to stream context-related data associated with mobile platform and identify a context. For example, the mobile platform may be turned off before a flight to Chicago and then turned back on after the flight arriving in Chicago. After turning on the mobile platform, the context-related application may cause the CIS to receive and/or generate context-related data. Further certain context signals associated with the context-related data may be extracted such as: a) the location of the mobile platform (i.e. Chicago O'Hare International Airport), b) the time zone based on the location of the mobile platform (i.e., Central Time zone), and/or c) the weather of the area near the mobile platform (e.g., rainy at 58° F.), among other possibilities. In some instances, such context signals may be used to determine that the user has just arrived at Chicago O'Hare International airport via an airplane flight.
- context-related data can be directly received and/or generated by sensors associated with the mobile platform and/or components provided inside the mobile platform.
- sensors can be external to the mobile platform and capable of receiving and/or generating context-related data.
- sensors could be any one or more of a motion detector (e.g., a gyroscope, an accelerometer, and/or a shock sensor), a camera, a proximity sensor, an impact sensor, a contact sensor (e.g., capacitive sensing device), a location determination device (e.g., a GPS device), a magnetometer, and/or an orientation sensor (e.g., a theodolite), among other possibilities.
- a motion detector e.g., a gyroscope, an accelerometer, and/or a shock sensor
- a camera e.g., a camera, a proximity sensor, an impact sensor, a contact sensor (e.g., capacitive sensing device), a location determination device (e.g.,
- application-related data can include data related to an exemplary mobile platform.
- application-related data can include data associated with operating the mobile platform.
- application-related data can include a “dialing indication” for opening a phone dialing application and/or a “received call indication” indicating a new call has been received at the mobile platform.
- a user can enter one or more digits of a phone number to be dialed using the phone dialing application (e.g., entering the digit “5” for the number “555-555-5555”).
- received call indications can include digits of a phone number from another phone calling the mobile platform and/or a phone number from another phone that has called the mobile platform in the past.
- an indication can include a digit of the mobile platform's phone number.
- a user can vocalize a command while using voice recognition to dial the phone number (e.g., verbally reciting “call my wife” in the vicinity of the mobile platform).
- application-related data can include a “messaging indication” for opening a messaging application, a short message service (SMS) text messaging application for example.
- SMS short message service
- a user may enter one or more text characters of a message to be sent using the messaging application (e.g., entering “h” or “hey” for “hey, you want to play golf today?”).
- the messaging application may receive a messaging indication for an incoming text message with incoming text (e.g., “Golf sounds great—which course?”).
- a user can vocalize a command and/or part of a command while using voice recognition to send a message using the phone messaging application (e.g., verbally reciting part of “hey, you want to play golf today?”)
- a “contact indication” for opening a contact may be provided through a user interface of the mobile platform.
- an “application indication” for executing an application on the mobile platform may be provided through the user interface of the mobile platform.
- application-related data may include how an application is executed, opened, and/or interacted with. For example, in some instances, an application may be opened quickly, in parallel with other applications (e.g., a phone dialing application), and in some instances, an application may be opened sequentially after performing other operations (e.g., opening other applications) with the mobile platform. Yet further, in some instances, an application may be executed remotely from a mobile platform, perhaps using a laptop and/or a tablet computer. Other examples are possible as well.
- the context-identification system may identify at least one context using the context-related data associated with the mobile platform and/or the application-related data received from the mobile platform.
- context-related data may include one or more “context signals.”
- an exemplary mobile platform may be configured to identify a context by determining various context signals and/or acquiring context signals from other sources, such as the external sensors and/or networks, perhaps as mentioned above.
- a context signal may be any signal that provides a measurement of or otherwise provides information pertaining to the state or the environment associated with a certain subject (e.g., with a certain person, device, event, etc.). Many types of information from many different sources may be used to establish context signals and/or provide information from which context signals may be determined. In some instances a context-identification system (CIS) may be used to receive and/or generate one or more context signals.
- CIS context-identification system
- context signals may include: (a) the current time, (b) the current date, (c) the current day of the week, (d) the current month, (e) the current season, (f) a time of a future event or future user-context, (g) a date of a future event or future user-based context, (h) a day of the week of a future event or future context, (i) a month of a future event or future user-context, (j) a season of a future event or future user-based context, (k) a time of a past event or past user-based context, (l) a date of a past event or past user-based context, (m) a day of the week of a past event or past user-based context, (n) a month of a past event or past user-based context, (o) a season of a past event or past user-based context, ambient temperature near the user (or near a monitoring device associated with a user), (p)
- context signals may take the form of discrete measurements. For example, a temperature measurement or a current GPS location may be received as a context signal.
- context signals may also be received over time, or may even be a continuous signal stream.
- an exemplary mobile platform may use the current volume of a continuous audio feed from an ambient microphone as one context signal, and the volume of a continuous audio feed from a directional microphone as another context signal.
- a context may be identified by extracting and/or interpreting context signals from other types of data (e.g., weather forecast data, satellite data, GPS coordinates, etc.). Further, in some embodiments, a machine-learning operation may cluster the one or more context signals by grouping one or more context signals. In addition, a machine-learning operation may categorize or classify the context by assigning a label to one or more context-signals, possibly after grouping them. For example, one or more context signals may take the form of data indicative of the environment or state information.
- context signals may be categorized or classified as “at home,” “at work,” “outside,” “in a car,” “outdoors,” “indoors,” “inside,” “outside,” “free,” and/or “in a meeting,” among other possibilities.
- a context may be a qualitative or quantitative indication that is based on one or more context signals.
- a context may be identified by extracting specific context signals from an aggregation of context signals. For example, context signals may indicate a change in time to 6:30 AM on a weekday (possibly setting off an alarm set by a mobile platform). The user may be located at their home and such information may be used to categorize or classify contexts such that the user went from “sleeping” to “awake.” In some instances, a context may simply be reflected in a context-related database as “getting ready for work.”
- a context may be identified by a mobile platform. For example, a mobile platform may determine, based on GPS location signals, that it has changed its location from one city to another different city. As such, the mobile platform may determine the context to be changing locations from “in Los Angeles” to “in Chicago.” Many other examples are also possible.
- a context may include data indicating changes to the environment or state information such as moving from “home” to “at work,” from “outside” to “in a car,” from “outdoors” to “indoors,” from “inside” to “outside,” and/or from “free” to “in a meeting,” among other possibilities. Further, in some instances, a context may indicate “going to work,” “getting in the car,” “going inside,” “going outside,” and/or “going to a meeting,” among other possibilities.
- the context-identification system may identify a context based on application-related data. For example, a user operating an exemplary mobile platform may provide an indication of a current context. In particular, after receiving a text message via the mobile platform's text messaging application, the user may respond in a text message reciting, “Can't talk, in a meeting.” In a further example, the user may update their status on a social networking site by stating, “I am so excited about this all-day meeting.” In addition, the user may have accepted a meeting invite to the all-day meeting set in the user's calendar such that the context-identification can identify the context of “attending a meeting.”
- a context may be identified by a user indication.
- a dialing indication may be generated by opening up a phone dialing application on the mobile platform.
- a dialing indication can be received by the phone dialing application, e.g., a dialing indication for an incoming phone call.
- the dialing indication may be used to identify a context for “calling co-worker.” Further, entering the first digit of the co-worker's phone number may further provide an indication for the “calling co-worker” context.
- a messaging indication may be generated by opening up the messaging application on the mobile platform.
- a messaging indication can be received by the messaging application, e.g., a messaging indication for an incoming text message.
- the messaging indication may be used to identify a context for “messaging co-worker.” Further, selecting and/or opening up the messaging co-worker's contact may further provide an indication for such contexts directed to communicating with the messaging co-worker. Other examples are possible as well.
- the context-identification system can predict at least one communicative action associated with the mobile platform by performing a machine-learning operation on the received data.
- the user may manually provide the context by entering the text “going to office” in a context field of the mobile platform's user interface.
- a context may be classified by assigning a label to one or more context-signals.
- a machine-learning operation may be performed to assign such labels.
- the mobile platform may determine the context from “subway” to “office” when the user leaves the subway station and arrives to their office.
- a “communicative action” may include any of the following: dialing a phone number using the mobile platform, ending a current call using the mobile platform, sending a message using the mobile platform, opening a contact on the mobile platform, executing an application on the mobile platform, updating a status on social networking site, and/or sending communication via email, among other possibilities.
- identifying a context may help predict at least one communicative action.
- an exemplary mobile platform may store context signals associated with “leaving for work,” “driving to work,” “driving home from work,” and “arriving home.” Further, the mobile platform may track previously-called phone numbers associated with such contexts, possibly storing the previously-called phone numbers with their respective contexts in a context-related database. For example, if the mobile platform identifies the “leaving for work” context, the mobile platform may display (e.g., on its user interface) a phone-number for the “leaving for work” context taken from a list of all previously-stored phone numbers called associated with the “leaving for work” context. Other possibilities may also exist.
- the mobile platform may have stored three phone numbers associated with the “driving home from work” context: 1) a son's phone number, 2) a daughter's phone number, and 3) a wife's phone number.
- the mobile platform may be able to determine its velocity, possibly consistent with the movement of the user's car. For example, the mobile platform may be able to detect if it is moving toward the son's location, the daughter's location, and/or the wife's location.
- the mobile platform may change the rankings of the three phone numbers stored.
- the mobile platform's location and velocity may indicate the mobile platform is traveling toward the son's location.
- the mobile platform may suggest the user call the son, perhaps by displaying the son's phone number as a “Suggested Call” and/or “Suggested Number” on the phone dialing application's user interface.
- identifying changes in context may allow a mobile platform to intelligently predict communicative actions (e.g., a phone number to dial).
- the mobile platform may determine that a current context matches a context that corresponds to a previously-dialed phone number. In such instances, the mobile platform may suggest the user to call the previously-dialed phone number. This aspect is discussed in greater detail below with reference to FIG. 8B . Other examples are possible as well.
- an instruction can be received to execute at least one communicative action associated with the mobile platform.
- the instruction may be provided from a user of the mobile platform.
- the mobile platform may provide a prompt on its user interface with a suggested number to call.
- the suggested number to call may be provided based on a prediction using the machine-learning operation.
- the user may use an instruction input on the mobile platform's user interface to initiate the call to the suggested number.
- the mobile platform may identify the “leaving work and going home” context as the user is driving their car from the workplace after a day at work.
- the mobile platform may prompt the user with a computerized voice communication stating, “Would you like to text your wife that you are ‘leaving work and going home’?”
- the user may respond, “Yes,” and a text message may be sent to the user's wife indicating that the user is “leaving work and going home.”
- the user is able to send text messages without making physical contact with the phone and/or making visual contact with the phone, possibly to refrain from distracting the user from driving.
- the instruction may be provided from the context-identification system (CIS).
- the CIS may identify a context associated with “arriving to the office” and provide an instruction to dial into a conference call as done regularly upon identifying the “arriving to the office” context.
- previously-stored data may indicate a pre-determined instruction to dial into the conference call upon identifying the “arriving to the office” context.
- pre-determined instructions can enable the CIS to automatically originate other phone calls, generate and send text messages, and/or other communicative actions, Many other examples are possible as well.
- FIG. 8A shows an example user interface 804 used in scenario 800 , in accordance with an example embodiment.
- Scenario 800 includes user interface 804 executing on mobile platform 802 .
- User interface 804 includes an interface for context identification 806 .
- user interface 804 includes an interface for suggested contact 808 to be viewed, possibly based on context identification 806 .
- user interface 804 includes an interface for suggested phone number 810 to be dialed, also possibly based on context identification 806 .
- user interface 804 includes instruction input 812 .
- FIG. 8A shows context identification 806 , perhaps based on receiving context related-data.
- mobile platform 802 may receive multiple context signals such as the date, time, temperature, and/or weather, as shown by context-related data 814 .
- Context identification 806 may illustrate the identified context, “Mar. 12, 2012 8:05 AM Chicago O'Hare Airport Arrival,” by accessing a pre-stored entry in the user's calendar indicating when the user is scheduled to land in Chicago. Yet, in some instances, mobile platform 802 may detect a recent operation for turning on the power of mobile platform 802 and use a GPS locator to detect the location of mobile platform 802 . In such instances, mobile platform 802 may make a prediction that the user has arrived at Chicago O'Hare Airport, perhaps after a flight. In other instances, mobile platform 802 may access the user's itinerary stored in mobile platform 802 's memory to identify that the user has arrived at Chicago O'Hare Airport after a flight. Other possibilities may exist.
- FIG. 8A shows context identification 806 with a “Change” button to change and/or correct the context identified, a “Save” button to save the context identified, and a “Dismiss” button to dismiss the context identified without saving the context identified.
- the “Change” button may allow the user to correct the context identified to provide further context-related data, possibly to predict communicative actions.
- the “Save” button may be used for identifying contexts in the future, and perhaps storing the context signals associated with the context identified.
- FIG. 8A shows suggested contact 808 as “Limo Driver.”
- the user may press the “Limo Driver” button to see further information regarding the Limo Driver and possibly view other options for text messaging the Limo Driver.
- multiple suggestions can be provided, and the user can press the circle enclosing a downward-facing triangle within suggested contact 808 to view additional suggestions.
- pressing the circle may provide a list of other contacts, possibly including local taxi services, rental car services, airport bus services, and/or other contacts that the user may want view and/or call based on the context identified. Other possibilities may also exist.
- FIG. 8A shows suggested phone number 810 as “555-555-5555” as the phone number to call the Limo Driver.
- the user may provide an instruction through instruction input 812 .
- Instruction input 812 includes a Wait button to possibly delay the phone call to the Limo Driver such that the call may be made later upon a timed prompt (not shown in FIG. 8A ) for suggested phone number 810 .
- Instruction input 812 also provides a Dial button to call the Limo Driver immediately and a Cancel button to exit user interface 804 without calling the Limo Driver.
- airport arrival data record 820 may include context-related data 822 corresponding to the date, time, temperature, and/or weather, as shown by context-related data 814 in FIG. 8A .
- context-related data 822 may be updated to include additional data corresponding to a recent operation for turning on the power of mobile platform 802 and the GPS location data of mobile platform 802 indicative of the arrival at Chicago O'Hare Airport after a flight.
- FIG. 8B further illustrates limo driver data 824 with its corresponding phone number data 826 , contact data 828 , and context-related data 830 .
- user's son data 832 may include its corresponding phone number data 834 , contact data 836 , and context-related data 838 .
- user's daughter data 840 may include its corresponding phone number data 842 , contact data 844 , and context-related data 846 .
- user's wife data 848 may include its corresponding phone number data 850 , contact data 852 , and context-related data 854 .
- context-related data 822 , 830 , 838 , 846 , and 854 may be stored in airport arrival data record 820 to predict a communicative action by a user of mobile platform 802 .
- context-related data may include mobile platform 802 's call history (e.g., contacts called most frequently), information corresponding to the user's relationships with other contacts (e.g., people that the user has spoken to the most over a given period in time), and/or browsing history of an internet browser application indicative of the user's interests. Other possibilities may also exist.
- phone number data 826 , 834 , 842 , and 850 may include information regarding each respective person's phone number. Further, phone number data 826 , 834 , 842 , and 850 may also include more than one phone number for a given person if the person has more than one phone number (e.g., cell phone number, office number, and/or pager number, among other possibilities). For example, phone number data 826 may correspond to the limo driver's phone number, 555-555-5555, shown as suggested phone number 810 in FIG. 8A .
- contact data 828 , 836 , 844 , and 852 may correspond to each person's availability, possibly incorporating work schedules, historical data indicative of when each person has answered (or has not answered) after calling them during certain times, and/or other possible alternative contacts to reach the person, among other possibilities.
- context-related data 830 , 838 , 846 , and 854 may correspond to context signals detected when each respective person has been called by mobile platform 802 and/or when each person has called mobile platform 802 .
- context-related data 830 may correspond to context signals indicating an operation for turning on the power of mobile platform 802 and the GPS location data of mobile platform 802 indicative of an arrival at Chicago O'Hare Airport after a flight.
- Context-related data 838 , 846 , and 854 may also correspond to context signals indicative of the GPS location at Chicago O'Hare Airport.
- context-related data 838 , 846 , and 854 may not include data suggestive of a recent operation turning on the power of mobile platform 802 indicative of an after-flight arrival. For example, it may be possible that the son, the daughter, and the wife were previously called just prior to the user's departure from Chicago O'Hare Airport.
- a current context identified with mobile platform 802 may closely match context-related data 822 . Further, context-related data 822 may match most closely with context-related data 830 as compared to context-related data 838 , 846 , and 854 . As such, limo driver data 824 may be ranked higher than user's son data 832 , user's daughter data 840 , and user's wife data 848 , indicating that the user is more likely to call the Limo Driver. Thus, a prediction can be made that the user will want to contact the Limo Driver using phone number data 826 .
- contact data 828 , 836 , 844 , and 852 may establish a weighting factor to further predict who the user may want to call.
- contact data 828 may indicate that the limo service does not open until 9:00 AM on Mar. 12, 2012.
- context-related data 822 may match most closely with context-related data 830
- limo driver data 824 may fall in the rankings since the limo driver is probably not available.
- user's son data 832 may be next in line to prompt the user with a suggested phone number corresponding to phone number data 834 .
- application-related data corresponding to the user's indications on the mobile platform may adjust the weighting factor.
- instruction input 812 provides a Cancel button to not call the Limo Driver at all.
- a weighting factor may be applied to lower limo driver data 824 in the rankings, perhaps eliminating limo driver data 824 from the rankings and/or eliminating limo driver data 824 from airport arrival data record 820 .
- Other examples are possible as well.
- FIG. 9A shows example context identification system (CIS) 900 , in accordance with an example embodiment.
- CIS 900 receives context signals 910 .
- context signals 910 can be stored as a data string in a mobile platform, a context-related database, a cloud, and/or a network, among other possibilities.
- Context identification 912 of CIS 900 can receive context signals 910 , such as, but not limited to (a) a current time, (b) a current date, (c) a current day of the week, (d) a current month, (e) a current season, (f) a time of a future event or future context, (g) a date of a future event or future context, (h) a day of the week of a future event or future context, (i) a month of a future event or future user-context, (j) a season of a future event or future context, (k) a time of a past event or past context, (l) a date of a past event or past context, (m) a day of the week of a past event or past context, (n) a month of a past event or past context, (o) a season of a past event or past context, (p) ambient temperature, (q) a current, future, or past weather forecast at a current location, (r
- context identification 912 can copy and/or extract data from context signals 910 to identify one or more contexts.
- the context identified can be output from context identification 912 as a vector.
- a classification indication can include the classification of a context, possibly using a predetermined classification scheme. For example, suppose classifier function 914 determines a recurring context indicative of the user going on their lunch break at noon. In this example, classifier function 914 can output a classification indication assigning a label “going out to lunch.” Many other possible classification indications are possible as well.
- Recognition function 916 can receive the classification indications and recognize certain functions. For example, upon receiving the classification indication of the mobile platform “moving towards the pizza parlor,” recognition function 916 can recognize previous phone calls made in the context of “moving towards the pizza parlor.” For example, in some prior instances, perhaps the user made a call to the pizza parlor to make reservations. Then, recognition function 916 can recognize these previous phone calls and generate a predicted communication (e.g., phone call, e-mail or text message) to the pizza parlor.
- a predicted communication e.g., phone call, e-mail or text message
- Recognition function 916 can be performed using software and data solely resident and executing on a mobile platform, while in other embodiments, recognition function 916 can be performed using software and data both resident and executing on the mobile platform and on other computing devices, such as one or more servers configured to recognize contexts.
- Application-related data 918 can include data generated by and/or related to an application executed on the mobile platform. Examples of application-related data 818 can include, but are not limited to interacting with applications, initiating programs on the mobile platform, and/or executing communicative actions such as dialing phone numbers, among other possibilities. Additional examples of application-related data are discussed above in the context of FIG. 7 , among other figures as well. Further, application-related data 918 can differ from context-related data, context vector(s) extracted by context identification 912 , classification indications from classifier function 914 , and recognized contexts from recognition function 916 .
- user model 920 can take recognized contexts from recognition function 916 and application-related data 918 to predict and/or suggest communicative actions, such as possible phone numbers to dial. For example, suppose that the mobile platform recognizes a context such that the user is on their lunch break and is at their office continuing to work. Further, application-related data may indicate that the user is initiating a phone dialing application and/or a restaurant browsing application. The mobile phone may predict that the user may want to order food and provide a phone number to a local restaurant, possibly a previously-called restaurant for making such a food order.
- FIG. 9B shows an example collective context identification system (CCIS) 902 , in accordance with an example embodiment.
- CCIS 902 can be configured to receive context-related data and application-related data from a number N of mobile platforms.
- each of the N mobile platforms can be configured to determine whether or not to provide some or all context-related data and/or application-related data to CCIS 902 .
- some or all of the context-related data and/or application-related data used CCIS 902 can be anonymously used.
- context-related data including context signals 922 a , 922 b , 922 c and recognized contexts 924 a , 924 b , 924 c , and application-related data 918 a - 918 c can be provided to collective learner 928 .
- Collective model 930 can take recognized contexts provided by collective learner 928 to suggest communicative actions to execute, such as possible phone numbers to dial and/or addresses for text messages and/or emails to be sent.
- FIG. 9C shows an example feedback-adaptable context identification system (FACIS) 904 , in accordance with an example embodiment.
- FACIS 904 takes application-related data 918 and context-related data 922 as inputs, and provides the inputs to both user model 920 and collective model 930 , which respectively generate user-specific communication addresses 940 and general communication addresses 942 .
- user-specific communication addresses 940 may include phone numbers, e-mail addresses, Instant Messaging (IM) addresses, IP addresses, Uniform Resource Locators (URL)s, and/or other communication addresses to restaurants previously contacted by a user
- general communication addresses 942 may include phone numbers, e-mail addresses, Instant Messaging (IM) addresses, IP addresses, Uniform Resource Locators (URL)s, and/or other communication addresses to common restaurants, and perhaps other establishments, in proximity to the mobile platform's current location.
- User-specific communication addresses 940 and general communication addresses 942 can be provided to communication address suggestion user interface (UI) 944 for presentation and possible selection as a communication address for contacting one or more entities.
- UI communication address suggestion user interface
- a user using communication address suggestion UI 944 can view one or more suggested user-specific communication addresses 940 , and perhaps provide additional feedback, such as dismissing the suggestion (i.e. refusing to use the suggested communication address).
- Information about the suggested communication address and any additional feedback can be respectively provided as feedback 946 , 948 to user model 920 and collective model 930 .
- user model 920 and collective model 930 can adapt to the feedback such as by increasing probabilities that the next communication address suggested by the other model will be accepted by the user.
- user model 920 can examine feedback 946 , facilitate collective model 930 's prediction for another communication address to be used, and increase the probability that the next communication address provided to communication address suggestion communication address UI) will be accepted going forward.
- collective model 930 can examine feedback 948 , facilitate user model 920 's prediction for another communication address to be used, and increase the probability that the next communication address provided to communication address suggestion user UI 944 will be accepted going forward.
- Mobile platforms such as mobile phones, can include one or more cameras to capture images. Frequently, the images are named, perhaps with a brief description of the image. For example, an image named “Sunset 18 Mar. 2012” can be a picture of a sunset captured on Mar. 18, 2012. Labeling a large number of pictures can take a great deal of time in reviewing the image, determining a title for the image, and then naming the image with the determined title.
- An image identification system based on the learning models described above, can suggest image and album titles based on image content and other data.
- the other data can include social signals such as social-networking messages, postings, comments, tags, e-mail and other messages, other persons' social signals, and social status indicators, calendar data, locations, and times.
- the IIS can learn a collective model that represents the collective data from at least one person and their images.
- the collective model can be learned, stored, and executed on the mobile platform alone or, in some embodiments, on the mobile platform and one or more other computing devices 2200 , such as servers.
- the IIS can also be configured with image feature recognition and detection capabilities, such as facial recognition modules, object recognition modules, landmark recognition models, and modules to recognize features in images such as lines, edges, patches of color, shapes, object areas/volumes, and other features.
- image feature recognition and detection capabilities such as facial recognition modules, object recognition modules, landmark recognition models, and modules to recognize features in images such as lines, edges, patches of color, shapes, object areas/volumes, and other features.
- the IIS can detect features in one image, a sequence of images, and/or a video file or stream for a sequence of one or more images.
- the IIS and/or collective model can include or otherwise be associated with a personalized model, perhaps using data stored solely on the mobile platform.
- the personalized model can be equipped to adapt to feedback from a user of the mobile platform. For example, for a given image IMG1, the personalized model can suggest three titles: T1, T2, and T3. Then, if the user decides to entitle IMG1 with title T2, the personalized model can increase the probability that title T2 and/or titles similar to title T2 are suggested in the future. In response to selection of title T2 in this example, the personalized model can decrease the probabilities that unselected titles T1 and T3 and/or titles similar to title T2 are suggested in the future.
- FIG. 10 is a flow chart of method 1000 , in accordance with an example embodiment.
- part or all of method 1000 can be executed using one or more mobile platforms; e.g., mobile platform 200 , 502 , 602 , 802 , 1202 , 1502 , 1852 , 2002 and/or one or more computing devices; e.g., computing device 2200 .
- Method 1000 begins at block 1010 , where a machine-learning service executing on a mobile platform can receive feature-related data.
- the feature-related data can include image-related data related to one or more images received from an application executing on the mobile platform and platform-related data received from the mobile platform.
- the image-related data and the platform-related data can differ.
- the image-related data can be generated by an image-identification system (IIS) executing on the mobile platform.
- IIS image-identification system
- the IIS can be configured to extract image-related features from the one or more images.
- the IIS can be configured to classify at least one image-related feature of the image-related features.
- the IIS can be configured to recognize the at least one classified image-related feature and/or object(s) and/or scene(s) that include the at least one classified image-related feature.
- the at least one classified image-related feature can include a feature related to a face of a person, and the IIS can be configured to recognize the face of the person.
- the at least one classified image-related feature can include a feature related to a landmark, and the IIS can be configured to recognize the landmark.
- the at least one classified image-related feature can include a feature related to an object, and the IIS can be configured to recognize the object.
- the machine-learning service can generate a title related to the one or more images.
- the title can be generated by the machine-learning service performing a machine-learning operation on the feature-related data.
- the title related to the one or more images can include at least one datum from the image-related data and at least one datum from the platform-related data.
- the at least one datum from the image-related data can include at least one datum selected from among a name of a person, a name of a landmark, and/or a name of an object and the at least one datum from the platform-related data includes at least one datum selected from among a date, a time, a calendar entry, a social-networking message, and a location.
- the machine-learning service can send the title related to the one or more images from the machine-learning service to the application.
- FIG. 11A shows example image identification system (IIS) 1100 , in accordance with an example embodiment.
- IIS 1100 receives an input image 1110 .
- Image 1110 can be an electronically-storable image perhaps stored in a well-known image format, e.g., JPG, MPEG, GIF, BMP, etc.
- Feature extraction function 1112 of IIS 1100 can detect features from image 1110 , such as, but not limited to, lines/edges, corners, points of interest, contours, patches of color, regions of interest. For some or all of the detected features, feature extraction function 1112 can copy and/or extract a “local image patch” from image 1110 that includes the detected feature.
- the detected feature(s) and/or corresponding local image patch(es) can be output from feature extraction function 1112 as a feature vector.
- a classification indication can include the classification of a recognized feature, and an indication of a range of pixels that include the classified feature. For example, suppose classifier function 1114 determines an image of a face is depicted by a rectangle of pixels whose upper-left-hand corner has pixel coordinates (10, 10), and whose lower-right-hand corner has pixel coordinates (90, 100). In this example, classifier function 1114 can output a classification indication of ⁇ face, (10, 10), (90, 100)>, to indicate an image of a face can be located within the rectangle having pixel coordinates from (10, 10) to (90, 100).
- a classification indication of ⁇ landmark, (1,100), (400, 400)> can indicate an image of a landmark can be found within the rectangle having pixel coordinates from (1, 100) to (400, 400). Many other possible classification indications are possible as well.
- Recognition function 1116 can receive the classification indications and recognize the feature in the classification indication. For example, upon receiving the classification indication ⁇ face, (10, 10), (90, 100)>, recognition function 1116 can attempt to recognize the face in the image. Recognition function 1116 can be performed using software and data solely resident and executing on a mobile platform, while in other embodiments, recognition function 1116 can be performed using software and data both resident and executing on the mobile platform and on other computing devices, such as one or more servers configured to recognize image features.
- Platform-related data 1118 can include data generated by and/or related to a mobile platform. Examples of platform-related data 1118 include, but are not limited to times, dates, locations, calendar entries, social-networking messages and other messages. Additional examples of platform-related data are discussed above as built-in feature-related data in the context of at least FIG. 2 . Other examples are possible as well.
- Feature-related data 1118 can differ from image-related data that includes image 1110 , feature vector(s) extracted by feature extraction function 1112 , classification indications from classifier function 1114 , and recognized features from recognition function 1116 .
- User model 1120 can take recognized features from recognition function 1116 and platform-related data 1118 to generate titles for images and/or collections of images (a.k.a. albums). For example, suppose an image I1 had three recognized features: recognized faces of Alice and Bob and a recognized object of a bottle, and that the platform-related data indicated that I1 was taken at 9 AM on Mar. 18, 2012, at Alice and Bob's home. Platform-related data 1118 , authorized by Bob to examine his calendar entries, can include a calendar entry of “St. Pats Party 2012” that was held starting at 7 PM on Mar. 17, 2012. Based on this information, user model 1120 can generate example titles for image I1, such as:
- images I2, I3, and I4 are taken in succession, all of which include images of Alice, Bob, and the bottle
- the above-generated example titles can be used for a photo album that includes images I1, I2, I3, and I4.
- the above-generated example titles could be used for a video clip of Bob, Alice, and the bottle taken at 9 AM on Mar. 18, 2012 that includes images I1, I2, I3, and I4.
- Images I1-I4 can reside on a mobile platform; e.g., Alice's mobile platform, and/or on a server configured to store data, such as images I1-I4.
- FIG. 11B shows an example collective image identification system (CIIS) 1102 , in accordance with an example embodiment.
- CIIS 1102 can be configured to receive image-related data and platform-related data from a number N of mobile platforms.
- each of the N mobile platforms can be configured to determine whether or not to provide some or all image-related data and/or platform-related data to CIIS 1102 .
- some or all of the image-related data and/or platform-related data used CIIS 1102 can be anonymized.
- image-related data including images 1122 a , 1122 b , 1122 c and recognized features 1124 a , 1124 b , 1124 c , and platform-related data 1118 a - 1118 c can be provided to collective learner 1128 .
- Collective model 1130 can take recognized features provided by collective learner 1128 to generate titles for images and/or collections of images (a.k.a. albums).
- FIG. 11C shows an example feedback-adaptable image identification system (FAIIS) 1104 , in accordance with an example embodiment.
- FAIIS 1104 takes platform-related data 1118 and image-related data 1122 , 1124 as inputs, provides the inputs to both user model 1120 and collective model 1130 , which respectively generate user-specific titles 1140 and general titles 1142 .
- Image-related data 1122 , 1124 can include images 1122 a - c and/or recognized features 1124 a - c .
- User-specific titles 1140 and general titles 1142 can be provided to image/album title suggestion user interface (UI) 1144 for presentation and possible selection as an image, video, or album title.
- UI image/album title suggestion user interface
- a user using image/album title suggestion UI 1144 can select one of user-specific titles 1140 , and perhaps provide additional feedback, such as a rating of the title. Information about the selected title and any additional feedback can be respectively provided as feedback 1146 , 1148 to user model 1120 and collective model 1130 . Based on this feedback, user model 1120 and collective model 1130 can adapt to the feedback such as by increasing probabilities of titles generated by the other model. Continuing this example, user model 1120 can examine feedback 1146 , determine that collective model 1130 generated the selected title, and increase the probability that titles similar to the selected title are provided to image/album title suggestion UI 1144 going forward. Additionally or instead, collective model 1130 can examine feedback 1148 , determine that user model 1120 generated the selected title, and increase the probability that titles similar to the selected title are provided to image/album title suggestion UI 1144 going forward.
- FIG. 12A shows an example user interface 1210 used in scenario 1200 , in accordance with an example embodiment.
- Scenario 1200 includes user interface 1210 executing on mobile platform 1202 .
- User interface 1210 includes an interface for camera application 1220 and photo album/title application 1222 .
- FIG. 12A shows photo album/title application 1222 operating in the foreground and utilizing suggested album/title dialog 1222 a .
- Suggested album/title dialog 1222 a includes album suggestion 1226 and title suggestion 1228 .
- FIG. 12A shows that album suggestion 1226 includes an album name suggestion of “Chicago_May — 2012”, a Suggest button to request another album name suggestion, a Save button to create an album named as suggested, e.g., Chicago_ May — 2012, an Edit button to modify the album name, and a Cancel button to exit the photo album/title application 1222 without creating an album title.
- album suggestion 1226 includes an album name suggestion of “Chicago_May — 2012”, a Suggest button to request another album name suggestion, a Save button to create an album named as suggested, e.g., Chicago_ May — 2012, an Edit button to modify the album name, and a Cancel button to exit the photo album/title application 1222 without creating an album title.
- FIG. 12A shows that album suggestion 1226 includes an album name suggestion of “Chicago_ May — 2012”, a Suggest button to request another album name suggestion, a Save button to create an album named as suggested, e.g., Chicago_ May — 2012, an Edit
- 12A also shows title suggestion 1228 with a title suggestion of “AnnJohn_ChicagoTheater — 23 May 2012.jpg” for a particular image, a Suggest button to request another title suggestion, a Save button to save an image named as suggested, e.g., AnnJohn_ChicagoTheater — 23 May 2012.jpg, an Edit button to modify the album name, and a Cancel button to exit the photo album/title application 1222 without using the suggested title.
- use of the Edit button to change album suggestion 1228 and/or title suggestion 1228 can generate feedback 1146 to user model 1120 and/or feedback 1148 to collective model 1130 .
- Feedback 1146 , 1148 can include the originally-suggested title and the title as edited.
- Feedback 1146 , 1148 can be provided to the user model 1120 and/or collective model 1130 in response to use of the Save button to save an edited album suggestion 1226 and/or title suggestion 1228 .
- image 1224 has recently been captured.
- Image 1224 is an image of two people, Ann and John, pictured beneath the marquee of the landmark Chicago Theater.
- Image 1224 has been provided to photo album/title application 1222 , which in turn used an image identification system, such as IIS 800 , CIIS 802 , or FAIIS 804 , to generate the suggested image title and image album names discussed above and shown in FIG. 12A .
- image identification system such as IIS 800 , CIIS 802 , or FAIIS 804
- photo album/title application 1222 can operate in the background if so authorized; e.g., photo album/title application 1222 operates without displaying suggested album/title dialog 1222 a . Then, when an image is captured, such as image 1224 , album/title application 1222 can provide the captured image to photo album/title application 1222 and generate suggested image title and image album names, perhaps after enough images have been captured and/or named to train learning model 1230 . Once the suggested image title and image album names are generated, album/title application 1222 can generate a notification, such as a pop-up dialog, that presents the suggested image title and/or image album names.
- a notification such as a pop-up dialog
- Timing and/or display of the notifications can depend on user response.
- album/title application 1222 can use threshold times to determine “relatively quick” and “relatively slow” response. For example, if a user responds to a notification within a relatively-quick threshold period of time; e.g., within five seconds of notification display, album/title application 1222 can continue to generate notifications as soon as suggested image title and/or image album names are available.
- album/title application 1222 can either generate notifications less frequently and/or stop generating notifications in favor of waiting for album/title application 1222 to be returned to the foreground, and then provide suggested image title and/or image album names via suggested album/title dialog 1222 a .
- Other values for threshold periods of time are possible.
- FIG. 12B shows communications for the example scenario 1200 , in accordance with an example embodiment. Communications shown in FIG. 12B involve photo album/title application 1222 requesting ranked lists of photo titles and photo album names from learning model 1230 .
- Learning model 1230 gets image-related data from IIS 1250 and platform-related data from built-in features (BIFs) 1240 .
- Built-in features 1240 can include at least the functionality of built-in features 410 discussed above.
- photo album/title application 1222 provides an image to learning model 1230
- learning model 1230 generates a list of a photo album names and photo titles.
- Photo album/title application 1222 initiates communications in scenario 1200 by calling the getService( ) function for learning model 1230 via communication 1252 .
- learning model 1230 provides a session key S3 via communication 1254 to photo album/title application 1222 .
- Session key S3 is included for subsequent communications between photo album/title application 1222 and learning model 1230 to permit addressing the subsequent communications to the correct learning model, e.g., learning model 1230 , and to the correct application, e.g., photo album/title application 1222 , for a learning session keyed by S3.
- Photo album/title application 1222 can instruct learning model 1230 , via communication 1256 , to set up a ranking interface.
- communication 1256 includes a RankingInterface( ) call with three parameters: session key S3 and requests for ranked lists of photo titles and album names, as shown in FIG. 12B , using the predefined PHOTO_TITLE and PHOTO_ALBUM values.
- learning model 1230 sends communication 1258 to IIS 1250 to request identification of image features, such as faces, objects, and landmarks.
- image features such as faces, objects, and landmarks.
- learning model 1230 requests identifications using the RankingImageProc( ) function with three parameters: requests to identify faces, objects, and landmarks using the respective pre-defined values of FACES, OBJS, and LANDMARKS.
- IIS 850 sends an image-identification session key of IP1 via communication 1260 .
- Scenario 1200 continues with learning model 1230 sending communication 1262 to built-in features 1250 to request retrieval one or more images stored on a mobile platform, as shown in FIG. 12B by passing the predefined STORED_IMAGES parameter in a ReqBI( ) function call to built-in features 1240 .
- built-in features 1240 returns a built-in session key BI3 via communication 1264 to learning model 1230 .
- built-in features 1244 can provide an image stored on the mobile platform to learning model 1230 via communication 1266 .
- Communication 1266 includes a Push( ) function call with three parameters: built-in session key BI3, identification of a stored image using the predefined value STORED_IMAGES, and a reference I1 to the stored image.
- learning model 1230 Upon receiving the stored image, learning model 1230 provides the stored image to IIS 1250 via communication 1268 to determine image-related features related to the stored image.
- Communication 1268 includes a Push( ) function call with three parameters: image-identification session key IP1, identification of a stored image using the predefined value STORED_IMAGES, and a reference I1 to the stored image.
- Scenario 1200 continues with IIS 1250 determining image-related data for the image referenced by I1 and returning the image-related data to learning model 1230 via communication 1270 .
- Communication 1270 includes a Push( ) function call with five parameters: image-identification session key IP1, a reference I1 to the stored image, image-related data for faces in F1, image-related data for objects in O1, and image-related data for landmarks in L1.
- Scenario 1200 continues with built-in features 1240 continuing to provide images stored on the mobile platform to learning model 1230 , which in turn provides each stored image to IIS 1250 to generate image-related data for faces, objects, and landmarks for each stored image.
- Learning model 1230 requests platform-related data from built-in features 1240 via communication 1272 .
- Communication 1272 includes a ReqBI( ) function call with five parameters: built-in session key BI3, a predefined value of SOCIAL_MEDIA requesting all authorized social networking messages stored on the mobile platform, a predefined value of NAMES requesting all authorized contact names stored on the mobile platform, a predefined value of EVENTS requesting all authorized calendar entries and/or events stored on the mobile platform, and IMAGE_COMMENTS requesting all authorized comments on the STORED_IMAGES previously requested.
- more or less information can be requested by learning model 1230 as platform-related data.
- built-in features 1240 deliver a name referred to as Name1 via communication 1274 to learning model 1230 .
- Built-in features 1240 can use additional Push( ) function calls to provide additional requested and authorized feature-related data to learning model 1230 .
- Scenario 1230 continues with photo album/title application 1222 providing an image Image1 to learning model via communication 1276 .
- Photo album/title application 1222 then requests a list of three suggested photo album names related to Image1 from learning model 1230 via communication 1278 .
- learning model 1230 In response to communication 1278 , learning model 1230 generates a list of three suggested photo album names related to Image1 and provides the requested list as the “Album[3]” parameter of the Pull response shown in communication 1280 .
- FIG. 12B shows that photo album/title application 1222 then requests a list of five suggested photo titles related to Image1 from learning model 1230 via communication 1282 .
- learning model 1230 In response to communication 1282 , learning model 1230 generates a list of five suggested photo titles related to Image1 and provides the requested list as the “Title[5]” parameter of the Pull response shown as part of communication 1284 .
- scenario 1200 then concludes.
- Some mobile platforms can search to connect with available communication networks.
- the mobile platform can turn on a transmitter to send one or more probe request messages requesting connection information, and/or turn on a receiver to listen for a beacon signal. If a network is available, the mobile platform either receives a probe response signal to a sent probe message, or receives a listened-for beacon signal. The mobile platform can establish a Wi-Fi connection with the network after performing authenticated and associated procedures. If no network is available, the mobile station does not receive a signal; e.g., either a probe request signal or beacon signal, and so can determine that no network is available.
- the mobile platform has to turn on a radio receiver to listen for signals and, if sending probe messages, perhaps also turn on a radio transmitter to send probe messages.
- mobile platforms can continue searching until an available network is found and a Wi-Fi connection is established. This searching procedure can take a relatively large amount of power, thereby reducing power available for other mobile platform tasks, such as voice and/or data communication.
- the mobile platform can train and use a machine-learning service to predict whether or not a search to connect with available communication networks will succeed.
- the machine-learning service can be trained using a number of features, including, but not limited to: mobile platform location, mobile platform velocity, time, signal characteristics such as signal strength, device usage, time since a last search for wireless communication service was performed, and outcome of the last search for wireless communication service. By predicting search outcomes rather than performing actual searches, the mobile platform can save power and reduce the amount of signaling used in attempting to connect with communication networks.
- the mobile platform can carry out a number of searches to connect with available communication networks. At least while training, the mobile platform can store the above-mentioned features, along with an outcome of each search to connect, e.g., success or failure. The outcome of each search can be treated as a fact or “ground truth.”
- the machine-learning service can include a binary classifier that predicts a likelihood of a finding an available communication network, a.k.a. predicts a result of a search to connect with available communication networks.
- the machine-learning service can include a tri-nary classifier that predicts a likelihood of a finding an available communication network or indicates that the service cannot predict the outcome of a search. For example, if the machine-learning service were trained to look for communications networks in north-eastern Illinois only and the mobile platform was moved to central California, the machine-learning service can output that it cannot predict the outcome of a search to connect with available communication networks, as the current set of features, especially location, do not match any features used to train the machine-learning service.
- the machine-learning service can get features and communication search outcomes from other mobile platforms, either via direct communication with other mobile platforms or via communication with one or more intermediate computing devices, such as servers storing feature and outcome data.
- the mobile platform can download feature and outcome data from mobile platforms in central California, and use the California-based data to predict search outcomes.
- the machine-learning service can learn about and generate predictions regarding connections with communication networks along with or instead of Wi-Fi networks, such as, but not limited to, Wi-Max, GSM, CDMA, TDMA, 3G, 4G, Bluetooth, and Zigbee networks.
- FIG. 13 is a flow chart of a method 1300 , in accordance with an example embodiment.
- part or all of method 1300 can be executed using one or more mobile platforms; e.g., mobile platform 200 , 502 , 602 , 802 , 1202 , 1502 , 1852 , 2002 and/or one or more computing devices; e.g., computing device 2200 .
- Method 1300 begins at block 1310 , where a machine-learning service executing on a mobile platform can receive feature-related data.
- the feature-related data can include communications-related data related to one or more searches for establishing electronic communications received from an application executing on the mobile platform and platform-related data received from the mobile platform.
- the communications-related data and the platform-related data can differ.
- the machine-learning service can determine whether or not the machine-learning service is trained to perform machine-learning operations related to predicting outcomes of searches for establishing electronic communications.
- the searches for establishing electronic communications can include a search to establish a wireless local-area network connection.
- the search to establish the wireless local-area network connection can include a search to establish the wireless local-area network connection based on an Institute of Electrical and Electronics Engineers (IEEE) 802.11 standard.
- IEEE Institute of Electrical and Electronics Engineers
- additional feature-related data can be received at the machine-learning service, where the additional feature-related data can include additional communications-related data related to one or more searches for establishing electronic communications received from the application and additional platform-related data from the mobile platform, and where the communications-related data, additional communications-related data, platform-related data, and additional platform-related data all differ.
- the machine-learning service in response to determining that the machine-learning service is trained: (i) the machine-learning service can receive a request for a predicted outcome of a search for establishing an electronic communication, (ii) the machine-learning service can generate the predicted outcome by performing a machine-learning operation on the feature-related data, and (iii) the predicted outcome can be sent to the application, perhaps by the machine-learning service.
- the machine-learning service can generate the predicted outcome by performing a machine-learning operation on the feature-related data without receiving the request for the predicted outcome of the search; i.e., the machine-learning service can generate the predicted outcome of the search without being prompted by the request.
- method 1300 can further include: in response to determining that the machine-learning service is trained, a communication indicating that the machine-learning service is trained can be sent to the application, perhaps by the machine-learning service.
- method 1300 can further include: in response to the predicted outcome indicating a search for establishing electronic communications would be successful, a wireless interface of the mobile platform can be activated. In still other embodiments, in response to the predicted outcome indicating a search for establishing electronic communications would not be successful, deferring activation of a radio of the mobile platform. In particular of the still other embodiments, after receiving the predicted outcome indicating the search for establishing electronic communications would be not successful, the application can start a periodic request for the predicted outcome of the search for establishing the electronic communication.
- the application starting the periodic request for the predicted outcome of the search for establishing an electronic communication can include: sending the request for the predicted outcome of the search for establishing the electronic communication to the machine-learning service, receiving the predicted outcome from the machine-learning service, in response to the predicted outcome indicating a search for establishing electronic communications would not be successful, waiting at least a pre-determined amount of time, and in response to the predicted outcome indicating a search for establishing electronic communications would be successful: stopping the periodic request for the predicted outcome and attempting establishment of the electronic communications.
- method 1300 can further include: in response to the predicted outcome indicating failure to predict a search for establishing electronic communications: conducting the search to establish electronic communications, determining an outcome of the search, and sending an indication of the outcome of the search to the machine-learning service.
- FIG. 14 shows a scenario 1400 for establishing wireless communications with a number of access points, in accordance with an example embodiment.
- a mobile platform travels to a number of locations and attempts to establish wireless electronic communications at each of the locations. Some of the locations are served by one or more wireless networks, and so some of the attempts to establish wireless electronic communications are successful while other attempts are unsuccessful. Records of the attempts to establish wireless electronic communications scenario during scenario 1400 are stored in log 1460 , which can be stored on the mobile platform.
- FIG. 14 depicts a twelve-block map.
- the four north/south streets shown in FIG. 14 are numbered from 1 st St., on the left or west side, 2 nd St., just to the east of 1 st St., 3 rd St. to east of 2 nd St. and 4 th St., which is the right-most or east-most north/south street.
- the three east/west avenues shown in FIG. 14 going from north to south are: Apple Ave, Berry Ave., and Cherry Ave.
- transmitters 1420 , 1422 , 1424 , 1426 , 1428 , 1430 have wireless communications transmitters, specifically shown as transmitters 1420 , 1422 , 1424 , 1426 , 1428 , 1430 .
- Transmitter 1420 generates network “4thApple” with a range shown using circle 1440 .
- transmitters 1422 , 1424 , 1426 , 1428 , 1430 respectively generate networks “ApBer”, “FanSar Books”, “Zone3”, “C2”, and “BearClaw”, with respective ranges shown using circles 1442 , 1444 , 1446 , 1448 , 1450 .
- the mobile platform travels from location A, shown in FIG. 14 as an octagon surrounding the letter “A” that is just north of Apple Ave near 2 nd St, through locations B, C, D, E, F, and G.
- a wireless network such as wireless network conducted using an IEEE 802.11 standard (a.k.a. a Wi-Fi network) or perhaps using another communications protocol.
- the mobile platform stores data related to the attempt in a log, shown in FIG. 14 as log 1460 .
- log 1460 stores the following information:
- more or fewer data items can be collected and stored in log 1460 .
- log 1460 shows that the mobile platform was at location A at time 17:00 on Tuesday and attempted to connect to a wireless network. The outcome of the attempt was unsuccessful. At 17:00 on Tuesday, the mobile platform had been used for two minutes, and it had been five minutes since the last attempt to connect to a wireless network.
- log 1460 shows that the mobile platform was at location B (just south of the intersection 2 nd St. and Apple Ave.) at time 17:10 on Tuesday and attempted to connect to a wireless network. The outcome of the attempt was successful.
- the mobile platform connected to the ApBer network with a signal strength of 40 out of 100.
- the mobile platform had been used for twelve minutes, and it had been ten minutes since the last attempt to connect to a wireless network.
- FIG. 15A shows example user interface 1510 used in scenario 1500 , in accordance with an example embodiment.
- Scenario 1500 includes user interface 1510 executing on mobile platform 1502 .
- user interface 1510 includes Wi-Fi application 1520 with Wi-Fi Predictor Setting 1522 .
- FIG. 15A shows example user interface 1510 used in scenario 1500 , in accordance with an example embodiment.
- Scenario 1500 includes user interface 1510 executing on mobile platform 1502 .
- user interface 1510 includes Wi-Fi application 1520 with Wi-Fi Predictor Setting 1522 .
- FIG. 15A shows example user interface 1510 used in scenario 1500 , in accordance with an example embodiment.
- Scenario 1500 includes user interface 1510 executing on mobile platform 1502 .
- user interface 1510 includes Wi-Fi application 1520 with Wi-Fi Predictor Setting 1522 .
- FIG. 15A shows example user interface 1510 used in scenario 1500 , in accordance with an example embodiment.
- Scenario 1500 includes user interface 1510 executing on mobile platform 150
- Wi-Fi Predictor Setting 1522 is set to active and that a Wi-Fi predictor is “Trained to Predict Wi-Fi Connections.”
- the Wi-Fi predictor has a Wi-Fi Predictor Dialog 1524 that includes three predictions 1526 a , 1526 b , 1526 c , an Refresh button to re-execute the Wi-Fi predictor and display new predictions before exiting dialog 1522 , and an close button to close dialog 1524 .
- Each of predictions 1526 a , 1526 b , 1526 c shown in FIG. 15A indicates a prediction for connecting to a wireless network.
- prediction 1526 a indicates that mobile platform 1502 is likely to connect to the “ApBer” Network
- prediction 1526 b indicates that the Wi-Fi Predictor indicates that mobile platform 1502 is unlikely to connect to the “C2”Network
- prediction 1526 c indicates that mobile platform 1502 is unlikely to connect to the “FanSarBooks” Network
- Each of predictions 1526 a , 1526 b , 1526 c shown in FIG. 15A additionally enables a user of Wi-Fi Predictor Dialog 1524 to attempt to connect with the network mentioned in the prediction. For example, by clicking on the check-mark box underneath the word “Connect?” on the right side of prediction 1526 a , mobile platform 1502 can attempt to connect to the “ApBer” network.
- FIG. 15B shows example communications for scenario 1500 , in accordance with an example embodiment.
- Scenario 1500 is related to Scenario 1400 , as scenario 1500 starts with the same attempts to communicate wirelessly as shown in log 1460 .
- the logged data is provided as training data to a classifier learning model 1530 .
- learning model 1530 is then requested to predict communications at three additional locations H, I, and J, which are shown in the map depicted in FIG. 14 , using the location's letter surrounded by an octagon.
- Location H is just southwest of location A
- location I is near the northernmost point of 4 th St. depicted in FIG. 14
- location J is just south of location B.
- Wi-Fi application 1520 requests service from learning model 1530 using the getService( ) function in communication 1552 .
- learning model 1530 sends a learning model session key of “S10” to Wi-Fi application 1520 using communication 1554 .
- Session key S10 is included for subsequent communications between Wi-Fi application 1520 and learning model 1530 to permit addressing the subsequent communications to the correct learning model, e.g., learning model 1530 and correct application, e.g., Wi-Fi application 1520 for a learning session keyed by S10.
- Wi-Fi application 1520 can instruct learning model 1530 via communication 1520 to set up a classification interface.
- communication 1556 includes a ClassificationInterface( ) call with five parameters: session key S10, an input-source reference of Conns, and a requested outputs of Wi-Fi signal strengths, Wi-Fi Network names, and Wi-Fi search outcomes, as shown in FIG. 15B , using the predefined WIFI_SIG_STR, WIFI_NTWK, and WIFI_OUTCOME values.
- an application can request a similar prediction for a non-Wi-Fi network, such as, but not limited to, a Wi-Max, GSM, CDMA, TDMA, 3G, 4G, Bluetooth, or Zigbee network.
- a non-Wi-Fi network such as, but not limited to, a Wi-Max, GSM, CDMA, TDMA, 3G, 4G, Bluetooth, or Zigbee network.
- learning model 1530 requests platform-related data from built-in features 1540 via communication 1578 .
- Communication 1578 includes a ReqBI( ) function call with five parameters: a predefined value of LOC requesting a current location of the mobile platform, a predefined value of VEL requesting a current velocity of the mobile platform, a predefined value of CLOCK_TIME requesting a current clock time, a predefined value of USAGE requesting a current usage value, such as uptime, of the mobile platform a, and a predefined value of LAST_WIFI_SEARCH_TIME requesting a time of a last search for a Wi-Fi network.
- built in features 1540 returns a built-in session key BI10 via communication 1560 .
- FIG. 15B shows Wi-Fi Application 1520 providing information about searching for Wi-Fi networks at location A via communication 1562 , which includes a Push( ) function call with five parameters.
- the five parameters are a session key S10, an input-source reference of Conns, a Wi-Fi signal strength value of 0, a Wi-Fi Network name of 0 to indicate no network was found, and a Wi-Fi search outcome of NO to indicate the search was unsuccessful.
- learning model 1530 can send communication 1564 with a Push( ) function to built-in features 1540 to request platform-related data.
- the Push( ) function sent via communication 1564 has six parameters: a built-in session key of BI10, a location parameter L1, a velocity parameter V1, a clock-time parameter CT1, a usage parameter U1, and a last Wi-Fi search time parameter LWST1.
- built-in features 1540 can send a Push( ) function via communication 1566 with values for the parameters requested using communication 1564 . As shown in FIG.
- communication 1566 includes a Push( ) function with a built-in session key of BI10, a location value of “A” indicating location A as shown in FIG. 14 , a velocity value of 2 MPH, a clock-time value of 1700, a usage value of 2 minutes, and a last Wi-Fi search time value of 5 minutes.
- learning model 1530 and/or built-in features 1540 can log part or all of the information provided by Wi-Fi Application 1520 and/or built-in features 1540 , such as shown in log 1460 of FIG. 14 .
- Scenario 1500 continues with the mobile platform moving to location B and searching for Wi-Fi service.
- the mobile platform is able to connect to the “ApBer” network.
- FIG. 15B shows Wi-Fi Application 1520 providing information about searching for Wi-Fi networks at location B via communication 1568 , which includes a Push( ) function call with five parameters.
- the five parameters are a session key S10, an input-source reference of Conns, a Wi-Fi signal strength value of 40, a Wi-Fi Network name of ApBer to indicate that the ApBer network was found, and a Wi-Fi search outcome of YES to indicate the search was successful.
- learning model 1530 can send communication 1570 with a Push( ) function to built-in features 1540 to request platform-related data.
- the Push( ) function sent via communication 1570 has six parameters: a built-in session key of BI10, a location parameter L1, a velocity parameter V1, a clock-time parameter CT1, a usage parameter U1, and a last Wi-Fi search time parameter LWST1.
- built-in features 1540 can send a Push( ) function via communication 1572 with values for the parameters requested using communication 1570 . As shown in FIG.
- communication 1572 include a Push( ) function with a built-in session key of BI10, a location value of “B” indicating location B as shown in FIG. 14 , a velocity value of 3 MPH, a clock-time value of 1710, a usage value of 12 minutes, and a last Wi-Fi search time value of 10 minutes.
- learning model 1530 and/or built-in features 1540 can log part or all of the information provided by Wi-Fi Application 1520 and/or built-in features 1540 , such as shown in log 1460 of FIG. 14 .
- Scenario 1500 continues with the mobile platform traveling to locations C, D, E, and F as shown in FIG. 14 .
- the mobile platform searches to find one or more Wi-Fi networks.
- the resulting communication search data and platform-related data are shown in log 1460 of FIG. 14 , but are not shown in FIG. 15B to save space.
- FIG. 15B shows Wi-Fi Application 1520 providing information about searching for Wi-Fi networks at location G via communication 1574 , which includes a Push( ) function call with five parameters.
- the five parameters are a session key S10, an input-source reference of Conns, a Wi-Fi signal strength value of 12, a Wi-Fi Network name of ApBer to indicate that the ApBer network was found, and a Wi-Fi search outcome of YES to indicate the search was successful.
- learning model 1530 can send communication 1576 with a Push( ) function to built-in features 1540 to request platform-related data.
- the Push( ) function sent via communication 1576 has six parameters: a built-in session key of BI10, a location parameter L1, a velocity parameter V1, a clock-time parameter CT1, a usage parameter U1, and a last Wi-Fi search time parameter LWST1.
- built-in features 1540 can send a Push( ) function via communication 1578 with values for the parameters requested using communication 1576 . As shown in FIG.
- communication 1578 include a Push( ) function with a built-in session key of BI10, a location value of “G” indicating location G as shown in FIG. 14 , a velocity value of 2 MPH, a clock-time value of 2002, a usage value of 122 minutes, and a last Wi-Fi search time value of 17 minutes.
- learning model 1530 and/or built-in features 1540 can log part or all of the information provided by Wi-Fi Application 1520 and/or built-in features 1540 , such as shown in log 1460 of FIG. 14 .
- learning model 1530 is trained after receiving the data from communications 1576 and 1578 .
- FIG. 15B shows that learning model 1530 indicates it is trained by sending communication 1580 to Wi-Fi Application 1520 with a Notify( ) function call including two parameters: a session key S10, and a predefined value of LM_TRAINED to indicate that learning model 1530 is now trained.
- Scenario 1500 continues with the mobile platform traveling to location H and requesting a prediction of a Wi-Fi search outcome.
- Wi-Fi Application 1520 can send communication 1582 with a Pull( ) function call.
- the Pull( ) function call has two parameters: a session key S10, and a pre-defined value WIFI_SEARCH, indicating that Wi-Fi Application 1520 is requesting a prediction of an outcome of search for Wi-Fi networks from learning model 1530 .
- the Pull( ) function call as sent in communication 1582 can have more than two parameters.
- a third parameter specifying a network name e.g., Zone3
- a third parameter specifying a location e.g., location D
- learning model 1530 can be interpreted by learning model 1530 as a request for a prediction for a search of wireless communications at the specified location.
- parameters specifying both a network name and a location can be interpreted by learning model 1530 as a request for a prediction for a search of wireless communications at the specified location for the specified network. Many other possible requests for prediction of searches for wireless communication are possible as well.
- learning model 1530 can send communication 1584 a with a Push( ) function to built-in features 1540 to request platform-related data.
- the Push( ) function sent via communication 1584 a has six parameters: a built-in session key of BI10, a location parameter L1, a velocity parameter V1, a clock-time parameter CT1, a usage parameter U1, and a last Wi-Fi search time parameter LWST1.
- built-in features 1540 can send a Push( ) function via communication 1584 b with values for the parameters requested using communication 1584 a . As shown in FIG.
- communication 1584 b includes a Push( ) function with a built-in session key of BI10, a location value of “H” indicating location H as shown in FIG. 14 , a velocity value of 0 MPH, a clock-time value of 2005, a usage value of 125 minutes, and a last Wi-Fi search time value of 3 minutes.
- Learning model 1530 can then classify the data received from built-in features 1540 into one of two possible predicted outcomes: a prediction that a search for a Wi-Fi network will be successful, and a prediction that a search for a Wi-Fi network will be unsuccessful.
- a third possible outcome that the learning model is unable to predict the outcome, perhaps because the learning model is not trained to generate a prediction given the current data, is possible as well.
- other predictions are possible as well.
- learning model 1530 can predict that a search for a Wi-Fi connection at location H is unlikely to succeed, perhaps based on recent data from nearby-location A.
- Learning model 1530 can send communication 1586 a to inform Wi-Fi Application 1520 of the prediction.
- FIG. 15B shows that communication 1586 a includes a PullResp( ) function call with five parameters: a session key S10, a input-source reference Conns, a predicted Wi-Fi signal strength of 0, a predicted Wi-Fi network of none indicated using the predefined value of NO_NTWK, and a predicted Wi-Fi search outcome of no network found indicated using the predefined value of PRED_NO.
- Wi-Fi Application 1520 can determine that the wireless interface (WI) activation to search for and/or connect to a Wi-Fi network can be deferred, as shown in box 1586 b of FIG. 15B , as learning model 1530 has predicted the search and/or attempt to connect would be unsuccessful. Wi-Fi Application 1520 can save power for the mobile platform by deferring wireless interface activation and making fewer unsuccessful searches for Wi-Fi connections.
- WI wireless interface
- Wi-Fi Application 1520 in response to the unsuccessful prediction, can make one or more additional requests for predictions of searches for Wi-Fi connections. For example, Wi-Fi Application 1520 can periodically request predictions of searches for Wi-Fi connections, such as requesting one prediction every N seconds, where N>0. In other embodiments, Wi-Fi Application 1520 can request another prediction of a search for Wi-Fi connections when the mobile platform has moved more than a threshold amount from a location of an unsuccessful search. For example, if the threshold amount is 100 yards, then Wi-Fi Application 1520 can request another predicted search when mobile platform moves more than 100 yards from location H. Other responses to unsuccessful predictions are possible as well.
- Scenario 1500 continues with the mobile platform traveling to location I and requesting a prediction of a Wi-Fi search outcome.
- Wi-Fi Application 1520 can send communication 1588 with a Pull( ) function call.
- the Pull( ) function call has two parameters: a session key S10, and a pre-defined value WIFI_SEARCH, indicating that Wi-Fi Application 1520 is requesting a prediction of an outcome of search for Wi-Fi networks from learning model 1530 .
- learning model 1530 can send communication 1590 a with a Push( ) function to built-in features 1540 to request platform-related data.
- the Push( ) function sent via communication 1590 a has six parameters: a built-in session key of BI10, a location parameter L1, a velocity parameter V1, a clock-time parameter CT1, a usage parameter U1, and a last Wi-Fi search time parameter LWST1.
- built-in features 1540 can send a Push( ) function via communication 1590 b with values for the parameters requested using communication 1590 a . As shown in FIG.
- communication 1590 b includes a Push( ) function with a built-in session key of BI10, a location value of “I” indicating location I as shown in FIG. 14 , a velocity value of 3 MPH, a clock-time value of 2015, a usage value of 135 minutes, and a last Wi-Fi search time value of 13 minutes.
- learning model 1530 can determine that learning model 1530 is unable to predict an outcome of a search for a Wi-Fi connection at location I. For example, learning model 1530 can determine that training data does not apply to locations, velocities, clock times, usage values, and/or last search times as provided in communication 1590 b , and so determine that learning model 1530 is unable to predict an outcome of a search for a Wi-Fi connection at location I moving at 3 MPH at 20:15 with 135 minutes of usage and with 13 minutes since a last Wi-Fi search for a Wi-Fi connection.
- Learning model 1530 can send communication 1592 to inform Wi-Fi Application 1520 of the inability of learning model 1530 to make a prediction.
- FIG. 15B shows that communication 1592 includes a PullResp( ) function call with five parameters: a session key S10, a input-source reference Conns, a predicted Wi-Fi signal strength of 0, a predicted Wi-Fi network of none indicated using the predefined value of NO_NTWK, and a predicted Wi-Fi search outcome of an unknown outcome as indicated using the predefined value of PRED_UNK.
- Wi-Fi Application 1520 can then determine that learning model 1530 to unable make a prediction.
- Wi-Fi Application 1520 can determine that activating the wireless interface to search for and/or connect to a Wi-Fi network can be deferred, as learning model 1530 has not predicted the search and/or attempt to connect would be successful.
- Wi-Fi Application 1520 in response to the unknown prediction, can activate the wireless interface, search for Wi-Fi connections, and provide search result data to learning model 1530 , such as performed via communications 1562 , 1568 , and 1574 .
- Other responses to prediction unavailability are possible as well.
- Scenario 1500 continues with the mobile platform traveling to location J and requesting a prediction of a Wi-Fi search outcome.
- Wi-Fi Application 1520 can send communication 1594 with a Pull( ) function call.
- the Pull( ) function call has two parameters: a session key S10, and a pre-defined value WIFI_SEARCH, indicating that Wi-Fi Application 1520 is requesting a prediction of an outcome of search for Wi-Fi networks from learning model 1530 .
- learning model 1530 can send communication 1596 a with a Push( ) function to built-in features 1540 to request platform-related data.
- the Push( ) function sent via communication 1596 a has six parameters: a built-in session key of BI10, a location parameter L1, a velocity parameter V1, a clock-time parameter CT1, a usage parameter U1, and a last Wi-Fi search time parameter LWST1.
- built-in features 1540 can send a Push( ) function via communication 1596 b with values for the parameters requested using communication 1596 a . As shown in FIG.
- communication 1596 b includes a Push( ) function with a built-in session key of BI10, a location value of “J” indicating location J as shown in FIG. 14 , a velocity value of 18 MPH, a clock-time value of 2022, a usage value of 142 minutes, and a last Wi-Fi search time value of 20 minutes.
- learning model 1530 can predict that a search for a Wi-Fi connection at location J is likely to succeed, perhaps based on recent data from nearby-location B.
- Learning model 1530 can send communication 1598 a to inform Wi-Fi Application 1520 of the prediction.
- FIG. 15B shows that communication 1598 a includes a PullResp( ) function call with five parameters: a session key S10, a input-source reference Conns, a predicted Wi-Fi signal strength of 36, a predicted Wi-Fi network name of “ApBer”, and a predicted Wi-Fi search outcome a network found indicated using the predefined value of PRED_YES.
- Wi-Fi Application 1520 of mobile platform 1502 can activate the wireless interface, as shown in box 1598 of FIG. 15B . Once the wireless interface is activated, mobile platform 1502 can search for and/or connect to a Wi-Fi network, as learning model 1530 has predicted the search and/or attempt to connect would be successful. Other responses to successful predictions are possible as well.
- a “usage session” for a mobile platform is the span of time between when the mobile platform is awakened or perhaps powered up and when the mobile platform is put to sleep or perhaps powered down.
- the time span or duration of the usage session is the amount of time between the awakening of the mobile platform and putting the mobile platform to sleep. For example, if a mobile platform is powered up at 8:00 AM and powered down at 8:05 AM, the time span of the usage session starting at 8:00 AM is five minutes.
- the machine-learning service can predict a time span of a usage session either numerically or via classification.
- An example numerical prediction would be that a usage session will be six minutes long.
- An example classification of the duration of the usage session can be: short (e.g., less than five minutes), medium (e.g., between five and ten minutes), or long (greater than five minutes). In an example utilizing these classifications, the machine-learning service can predict a usage session will be either short, medium, or long.
- a software application can use a predicted session duration or time span to guide the application's behavior. For example, suppose a user powers up a mobile platform and then starts a jukebox application to play random audio and/or video files.
- the jukebox application can send a request to the machine-learning service to predict a time span for the current usage session. In this example, the machine-learning service predicts a “medium” (5-10 minute) time span for the usage session.
- the jukebox application can initially select only audio, video, and/or audio-video files that cumulatively take ten minutes or less to play based on the predicted medium time span.
- the jukebox application first plays an audio, video, and/or audio-video file that takes three minutes. Then, in selecting a second audio, video, and/or audio-video file, the jukebox application can select files that take no more than seven minutes long, based on a ten-minute long maximum duration of the usage session, with three minutes already consumed by playing the first file. Many other examples of application requests and using predicted session durations and/or time spans are possible as well.
- FIG. 16 is a flow chart of a method 1600 , in accordance with an example embodiment.
- part or all of method 1600 can be executed using one or more mobile platforms; e.g., mobile platform 200 , 502 , 602 , 802 , 1202 , 1502 , 1852 , 2002 and/or one or more computing devices; e.g., computing device 2200 .
- feature-related data can be received at a machine-learning service executing on a mobile platform.
- the feature-related data can include usage-related data and platform-related data.
- the usage-related data can include data about one or more time spans that the mobile platform is activated.
- the platform-related data can be received from the mobile platform.
- the usage-related data and the platform-related data can differ.
- the machine-learning service can determine whether the machine-learning service is trained to perform machine-learning operations related to predicting a time span that the mobile platform will be activated.
- the machine-learning operations can include a classification operation.
- the machine-learning operations can include a regression operation.
- the regression operation can include a linear regression operation.
- the predicted time span can include a predicted amount for the predicted time span.
- the machine-learning service in response to determining that the machine-learning service is trained, can: receive a request for a predicted time span that the mobile platform will be activated, determine the predicted time span by performing a machine-learning operation on the feature-related data, and send the predicted time span.
- the machine-learning service can determine the predicted time span by performing a machine-learning operation on the feature-related data without receiving the request for the predicted time span; i.e., the machine-learning service can generate the predicted time span without being prompted by the request.
- the predicted time span can include a predicted classification of the time span.
- the predicted classification of the time span can be selected from among a short time span, a medium time span, and a long time span.
- method 1600 can include selecting one or more media files based on the predicted time span and presenting the selected one or more media files using the mobile platform, such as discussed above in the context of the example jukebox application.
- FIG. 17A shows usage log 1700 for activations of a mobile platform, in accordance with an example embodiment.
- Usage log 1700 shows, for each activation of the mobile platform, a time span (TS), a starting time shown using military-time notation, a location number, a location label (Loc Label), and a time-span label (TS Label).
- TS time span
- Loc Label location label
- TS Label time-span label
- Semantic labels can be part of usage log 1700 .
- the first row of usage log 1700 corresponds to an eight-minute long usage session starting at 8:08 AM (0808) at location 1, also known as “Home”, and the time span of “medium” duration.
- a usage session with a time span of zero to five minutes is labeled “short”
- a usage session with a time span of five to ten minutes is labeled “medium”
- a usage session with a 10+ minute time span is labeled “long.”
- more, fewer, and/or other semantic labels for time spans can be used.
- other ranges of times can be used for short, medium, and/or long time spans.
- locations can have semantically labels. As shown in FIG. 17A , seven separate locations are listed, both numerically using the numbers one through seven, and with semantic labels.
- FIG. 17A shows location 1 with a semantic location label of “Home”, location 2 with a semantic location label of “Work”, location 3 with a semantic location label of “Random1”, location 4 with a semantic location label of “Groceries”, location 5 with a semantic location label of “Lunch Rest.” abbreviating Lunch Restaurant, location 6 with a semantic location label of “Movie” for a movie theater, and location 7 with a semantic location label of “Random2.”
- FIG. 17B shows a usage log 1710 of activations of a mobile platform at a particular location, in accordance with an example embodiment.
- usage log 1710 shows activations of the mobile platform logged in usage log 1700 at location four, which has a semantic location label of “Groceries” as shown in FIG. 17A .
- the time-span data in usage log 1710 ranges from a minimum time-span of 15 minutes in the last entry of usage log 1710 to a maximum time-span of 22 minutes in the second entry of usage log 1710 .
- Usage log 1710 also includes a “Start” column with starting times for usage sessions shown in military time notation and a “Mins” column with starting times for usage sessions shown in terms of the number of minutes since midnight.
- FIG. 17C shows graph 1720 of a linear regression model based on the data in usage log 1710 , in accordance with an example embodiment.
- a learning model can use regression techniques, such as, but not limited to linear regression, multiple regression, multivariate linear regression, and non-linear regression techniques.
- Mins column data were used instead of “Start” column data, as the Mins column data covers a single range of integers from 0 (corresponding to 0000 military time of 0000) to 1439 (corresponding to 2359 military time), while the set of military time values involve 24 separate ranges of integers (0000-0059, 0100-0159, . . . 2300-2359).
- the linear regression model can predict numerical values for time spans. For example, suppose the machine-learning model was requested to predict a time span of a usage session for the mobile platform at location “Groceries” at 6 PM or 1800 military time. The machine-learning service can use the linear regression model shown in FIG. 17C to predict a time span of 18.85 minutes for the usage session starting at 1800.
- FIG. 18A shows classification diagram 1800 , which based on the data in usage log 1700 shown in FIG. 17A , in accordance with an example embodiment.
- Classification diagram 1810 depicts a result of a possible analysis of the data in usage log 1700 that can be used by the machine-learning service to predict time spans of usage sessions.
- Classification diagram 1800 begins at node 1810 , where a location is determined.
- FIG. 18A shows locations using the some of the semantic labels shown in FIG. 17A , such as Home used in node 1820 , Work used in node 1822 , Groceries used in node 1824 , and Lunch Rest. used in node 1826 .
- Node 1828 uses a label of “Single-Session Locs.” to group all of the locations involved in one usage session as recorded in usage log 1700 .
- Node 1828 include data generated for locations with semantic labels “Movie”, “Random”, and “Random2” in usage log 1700 .
- numeric locations can replace the use of semantic location labels in classification diagram 1800 .
- a short time span can range from zero to five minutes
- a medium time span can range from six to ten minutes
- a long time span ranges longer than eleven minutes.
- node 1820 shows an analysis of usage-session time spans from usage log 1700 at location Home, indicating that at location Home, six of sixteen of total sessions or 37.5% had a short time span, seven of sixteen total sessions or 43.8% had a medium time span, and three of sixteen total sessions or 18.8% had a long time span.
- more, fewer, and/or other semantic labels for time spans can be used.
- other ranges of times can be used for short, medium, and/or long time spans.
- Node 1822 shows an analysis of usage-session time spans at location Work, indicating that at location Work, ten of twelve total sessions or 83.3% had a short time span, two of twelve total sessions or 16.7% had a medium time span, and zero sessions had a long time span.
- nodes 1824 , 1826 , and 1828 Only one semantic time label is shown as being utilized for each of nodes 1824 , 1826 , and 1828 .
- node 1824 representing location Groceries
- node 1826 regarding location Lunch Rest.
- Node 1828 reflecting data from the Single-Session Locations, indicates that all three usage sessions at those locations had short time spans.
- node 1830 shown connected to node 1820 in classification diagram 1800 , indicates a result of further analysis of usage sessions for location “Home” based on both usage-session time spans and time of day.
- times of day are classified as either before work (BW), during work (DW), or after work (AW).
- BW before work
- DW during work
- AW after work
- the BW times of day can range from midnight to 8:59 AM
- the DW times of day can range from 9:00 AM to 5:00 PM
- the AW times of day can range from 5:01 PM to 11:59 PM.
- times of day can be used.
- other ranges of times can be used for the BW, DW, and AW time spans.
- times of day can be used without classifications.
- Node 1830 of FIG. 18A shows that: (a) three of seven usage sessions before work at location Home have short time spans and four usage sessions at location Home have medium time spans, (b) no usage sessions were made at location Home during work, and that (c) equal numbers of short, medium, and long duration usage sessions were made at location Home after work.
- a prediction is requested while the mobile platform is at location Home, one prediction is that the call is slightly more likely to be of medium duration than short duration before work, and that all durations are equally likely either during work or after work.
- additional analysis may be performed to better refine the after work call estimate that “all durations are equally likely”; e.g., use additional time span and/or time-of-day classifications for after-work calls.
- times of day are classified as either “AM” for morning work hours; e.g., between 9:01 AM and 11:59 AM, “Meal” or during a meal, a.k.a. lunch or dinner; e.g., between 12:00 PM and 12:59 PM, and “PM” for post-meal afternoon work hours; e.g., between 1:00 PM and 4:59 PM.
- AM for morning work hours
- Meal or during a meal, a.k.a. lunch or dinner
- PM for post-meal afternoon work hours
- more, fewer, and/or other semantic labels for times of day can be used.
- other ranges of times can be used for the AM, Meal, and PM times of day.
- times of day can be used without classifications.
- 18A shows that: (a) all AM-time usage sessions have short time spans, (b) all Meal-time usage sessions have short time spans, and (c) it is somewhat more likely that short time-span usage sessions will be made during the PM times of day than medium time-span usage sessions.
- Classifications and classification diagram 1800 can be used to generate numerically-valued predictions. For example, using the data in node 1820 , if a usage session is begun at location Home, 37.5% of all usage sessions are short, 43.8% of all usage sessions are medium, and 18.8% of all usage session are long. To generate a numerical prediction, one set of assumptions that can be used is that the mid-point value of a given range is chosen to represent usage-sessions in the range, and a long time-span usage session is arbitrarily chosen to have a 20 minute long time span.
- a predicted time span PT for a usage session at location Home can be determined as:
- FIG. 18B shows example user interface 1854 for jukebox application 1860 used in scenario 1850 , in accordance with an example embodiment.
- user interface 1854 shows feature-related data display 1856 and jukebox application display 1860 .
- Feature-related data display shows a current time and date of “12:23 Sat.”, a current location with a semantic label of “Home”, and current weather conditions of “58° F.” and “Cloudy.”
- a jukebox application can be configured to request the machine-learning service to predict a time span of a usage session and then select media, including but not limited to audio, video, and audio/video files, to play during the predicted duration.
- the jukebox application generates jukebox application display 1860 , which includes prediction dialog 1862 , media list dialog 1864 , play list button 1870 , and close button 1872 .
- Prediction dialog 1862 shows the usage session time span predicted by the machine-learning service of 10 minutes, perhaps based on the feature-related data shown in feature-related display 1856 , such as the displayed location, date, and time. Prediction dialog 1862 includes two buttons: a “Change” button to permit the user to change the usage session time span from the predicted value and a “Dismiss” button to close prediction dialog 1862 .
- media list dialog 1864 displays a media list selected by the jukebox application.
- the media list includes three media files to be played: “Two-Minute News” with a duration of 2:03, a “Favorite Song 1” with a duration of 4:32, and “Now & Future Tools” with a duration of 3:11.
- the “Now & Future Tools” media is selected, as shown in FIG. 18B using a grey background for the “Now & Future Tools” media list item and the bold font for the “Now & Future Tools” media list entry.
- selected media and controls 1866 operate on the “Now & Future Tools” media.
- Selected media and controls 1866 include a “Play” button to play the selected media immediately, a “Delete” button to delete the selected media from the media list, a “Move” button to enable movement of the selected media within the media list, and a “New . . . ” button to permit addition of new media item(s) to the media list.
- Other dialogs and controls are possible as well.
- Usage information 1868 shows the total duration of the three media files is 9 minutes, 46 seconds (9:46) and the current usage, or amount of time mobile platform 1852 has been activated, is 3 seconds (0:03).
- the media shown in media list dialog 1864 were selected, in part, to play less in total less than the predicted session time span.
- the total duration of the three media files is 14 seconds less than the predicted duration of 10 minutes.
- Play list button 1870 can be used to play the media items in the order shown in media list dialog 1864 .
- Close button 1872 can be used to close user interface 1854 .
- media files played using the jukebox application can be played in the background, to permit the user of mobile platform 1852 to interact with other applications during the usage session.
- the jukebox application can attempt to become a foreground application to permit the user to view the video or audio/video file.
- Volume settings for mobile platforms are often changed according to predictable patterns. For example, before attending meetings, a mobile platform user can use a mute setting to turn off a telephone ringer. At that time, the user may set the mobile platform to a vibrate mode, where a user of the mobile platform is alerted to an incoming telephone call by use of a haptic or vibration system rather than the telephone ringer.
- the user may “un-mute” the mobile platform so to turn on the telephone ringer.
- the user may also set the vibrate mode to off, so to turn off the haptic or vibration system, and thus rely on the telephone ringer to alert the user of an incoming telephone call.
- the user may perform similar actions at other places than meetings where silence is often expected, such as at movie theaters, libraries, schools, lecture halls, live performances, and similar situations.
- the machine-learning service can be trained to learn patterns in changing volume settings for a mobile platform. Once trained, the machine-learning service can generate predictions for the volume settings. The predictions can be used to have the mobile platform directly or indirectly change the volume settings. For example, the machine-learning service can provide the predicted settings to a dialer application or other application that changes the settings to the predicted setting values. As another example, the machine-learning service can generate a prompt with the predicted setting values, perhaps with a reminder to the user to change the volume settings.
- the mobile-learning service can save the user the effort and time involved in repeatedly setting and resetting volume settings. Additionally, if the mobile platform is configured to prompt the user to change volume settings when predicted, the user can be reminded to change volume settings before attending an event where silence is often expected, and thus avoid the embarrassment of having the mobile platform sound off during the event.
- FIG. 19 is a flow chart of a method 1900 , in accordance with an example embodiment.
- part or all of method 1900 can be executed using one or more mobile platforms; e.g., mobile platform 200 , 502 , 602 , 802 , 1202 , 1502 , 1852 , 2002 and/or one or more computing devices; e.g., computing device 2200 .
- feature-related data can be received at a machine-learning service executing on a mobile platform.
- the feature-related data can include volume-related data and platform-related data.
- the volume-related data can include data about one or more volume-related settings for the mobile platform.
- the platform-related data can be received from the mobile platform.
- the volume-related data and the platform-related data can differ.
- the one or more volume-related settings can include a mute setting, a vibration setting, and a setting for ringer volume.
- the feature-related data can include data related to a location of the mobile platform, a current time, a call-termination time, a calling party, and a calendar event.
- the platform-related data can include data about devices proximate to the mobile platform.
- the machine-learning service can determine whether the machine-learning service is trained to perform machine-learning operations related to predicting a change in the one or more volume-related settings for the mobile platform.
- the machine-learning service can: receive a request for predicting the change in the one or more volume-related settings, determine the predicted change in the one or more volume-related settings by performing a machine-learning operation on the feature-related data, and send the predicted change in the one or more volume-related settings.
- the machine-learning service can determine the predicted change in the one or more volume-related settings by performing a machine-learning operation on the feature-related data without receiving the request for the predicted outcome of the search for establishing the electronic communication; i.e., the machine-learning service can generate the predicted outcome without being prompted by a request.
- method 1900 can further include determining one or more semantic labels for the platform-related data.
- determining one or more semantic labels for the platform-related data can include determining the one or more semantic labels for the current time, where the semantic labels include an at-home-time label, a work-time label, a sleep-time label, a traveling-time label, and a meal-time label.
- determining one or more semantic labels for the platform-related data includes determining the one or more semantic labels for the current location, where the semantic labels include a home-location label, a work-location label, a sleep-location label, a traveling-location label, and a meal-location label.
- method 1900 can further include: determining whether the predicted change in the one or more volume-related settings can include a prediction that at least one of the one or more volume-related settings change. In response to determining that the predicted change in the one or more volume-related settings includes the prediction that the at least one of the one or more volume-related settings changes, a prompt to change the at least one of the one or more volume-related settings can be generated.
- method 1900 can further include: determining whether the predicted change in the one or more volume-related settings can include a prediction that at least one of the one or more volume-related settings change. In response to determining that the predicted change in the one or more volume-related settings includes the prediction that the at least one of the one or more volume-related settings changes, the at least one of the one or more volume-related settings can be changed.
- the volume-related settings can be changed by the mobile platform without additional input, perhaps by a dialer application executing on the mobile platform that receives the at least one of the prediction that the one or more volume-related settings from the machine-learning service and sets the volume-related settings according to the prediction.
- FIG. 20A shows an example user interface 2010 used in scenario 2000 , in accordance with an example embodiment.
- User interface 2010 executing on mobile platform 2002 , includes an interface for dialer application 2020 configured to generate and display volume dialog 2022 .
- Volume dialog 2022 includes manual volume settings 2024 , smart volume setting 2026 , an OK button to save current settings before exiting volume dialog 2022 , and a Cancel button to exit volume dialog 2022 without saving current settings.
- Manual volume settings 2024 includes a slider bar that enables a user to set a microphone output setting between a minimum setting of 0, which effectively mutes a ringer that outputs the ringtone, and a maximum setting of 100, which provides the maximum output volume for the ringtone.
- manual ringtone volume setting is set at 51.
- Manual volume settings 2024 also includes check boxes for a mute setting and a vibrate setting.
- the mute setting when active or checked, inhibits ringer output; a.k.a. mutes the ringer and stops ringtone output.
- the vibrate setting when active or checked, allows a haptic output for the ringer; a.k.a. the mobile platform can vibrate or produce other haptic output when the mobile platform is in a ringing state.
- both the mute and vibrate settings are not active or checked.
- scenario 2000 when manual volume settings are allowed, the mute setting is not active and so the slider bar sets the volume for the ringer. Also, no haptic output is generated when the mobile platform is in the ringing state.
- smart volume setting 2026 can learn and predict ringer volume, mute, and vibrate settings.
- dialer application 2020 can use manual volume settings 2024 to determine output volume for a ringer of mobile platform 2020 and whether or not haptic output will be provided when mobile platform 2002 is in the ringing state.
- dialer application 2020 can use a machine-learning service to perform machine-learning operation(s) to predict volume settings, such as ringtone volume, mute, and vibrate settings. Then, upon receiving predicted volume setting value(s), dialer application 2020 can use the predicted settings values according to predicted usage setting 2028 .
- smart volume setting 2026 is set to enabled and predicted usage setting 2028 is set to apply.
- predicted volume setting value(s) such as ringtone volume, mute, and/or vibrate setting values
- dialer application 2020 are used to set the ringtone volume, mute, and vibrate settings. That is, when predicted usage setting 2028 is set to apply, mobile platform 2002 and/or dialer application 2020 , can set the ringtone volume, mute, and/or vibrate settings using the predicted volume setting values(s) without additional input, such as user input.
- mobile platform 2002 and/or dialer application 2020 can generate a dialog or other prompt to indicate to a user that the ringtone volume, mute, and/or vibrate settings have been changed to the predicted volume setting values(s).
- predicted usage setting 2028 When predicted usage setting 2028 is set to prompt, predicted volume setting value(s) received by dialer application 2020 are displayed, audibly output, and/or otherwise prompted to a user of mobile platform 2002 .
- the user Upon being displayed, the user can use volume dialog 2022 to set the ringtone volume, mute, and vibrate settings as desired, perhaps using the prompted predicted volume setting value(s).
- FIG. 20B shows example communications for the scenario 2000 , in accordance with an example embodiment.
- scenario 2000 continues with dialer application 2020 calling the getService( ) function of learning model 2030 via communication 2052 .
- learning model 2030 provides a session key S11 via communication 2054 to dialer application 2020 .
- Session key S11 is included for subsequent communications between dialer application 2020 and learning model 2030 to permit addressing the subsequent communications to the correct learning model, e.g., learning model 2030 and correct application, e.g., dialer application 2020 for a learning session keyed by S11.
- Dialer application 2020 can instruct learning model 2030 , via communication 2056 , to set up a prediction interface.
- communication 2058 includes a PredictionInterface( ) call with four parameters: session key S11, and three parameters for requested output predictions: ring tone volume settings, mute settings, and vibration settings, as shown in FIG. 20B , using the respective predefined RING_VOL, MUTE_SET, and VIB_SET values.
- learning model 2030 sends communication 2058 to built-in features 2040 with a ReqBI( ) function call requesting built-in features.
- learning model 2030 uses the ReqBI( ) call to request, using input-source descriptor “bid”, current location values, clock times, calendar-related events, and numbers of local (or proximate) devices, by passing the respective predefined LOC, CLOCK_TIME, CAL_EVENTS, and NUM_LOC_DEVS values to built-in features 2040 .
- Built-in features 2040 can include at least the functionality of built-in features 410 discussed above.
- built-in features 2040 returns a built-in session key BI11 via communication 2060 .
- Built-in session key BI11 is included with subsequent communications between learning model 2030 and built-in features 2040 to permit addressing the subsequent built-in related communications to the correct learning model.
- FIG. 20B continues with dialer application 2020 sending communication 2062 to learning model 2030 with a Push( ) function call to inform learning model 2030 that a volume settings have been set with a ring tone volume setting of 0, a mute setting of YES or active, and a vibration setting of YES.
- built-in features 2040 sends communication 2064 , with a Push( ) function call to inform learning model 2030 that mobile platform 2002 is at location “Home” at a clock time of 0808 (8:08 AM), no calendar events are active, and that one device is nearby (proximate).
- FIG. 20B shows a number of communications 2066 - 2085 for five subsequent volume setting changes made using dialer application 2020 and built-in feature values provided to learning model 2030 from built-in features with the values listed in Table 5 below.
- Scenario 2000 continues with learning model 2030 sending communication 2086 to dialer application 2020 with a Notify( ) function call.
- learning model 2030 is trained after receiving the data from communications 2082 and 2084 .
- FIG. 20B shows that learning model 2030 indicates it is trained by sending communication 2086 to dialer application 2020 with a Notify( ) function call including two parameters: a session key S11, and a predefined value of LM_TRAINED to indicate that learning model 2030 is now trained.
- FIG. 20B shows scenario 2000 continuing with dialer application 2020 sending communication 2088 a to learning model 2030 .
- Communication 2088 a includes the Pull( ) function call to request a prediction of ringer volume, mute, and vibration settings from learning model 2030 using respective pre-defined values of RING_VOL, MUTE_SET, and VIB_SET as parameters in the Pull( ) function call.
- Communication 2088 b includes feature data from built-in functions 2040 with a location of Work, a time of 2:02 PM (1402), a calendar event of NULL or no calendar events active, and a number of local devices of 7.
- Learning model 2030 can apply machine-learning operations to predict the requested volume settings.
- FIG. 20B shows that the prediction of communication 2088 c includes a ringtone volume setting of 20, a mute setting of NO or not active, and vibration setting of YES.
- Communication 2090 a includes the Pull( ) function call to request a prediction of ringer volume, mute, and vibration settings from learning model 2030 using respective pre-defined values of RING_VOL, MUTE_SET, and VIB_SET as parameters the Pull( ) function call.
- Communication 2090 b includes feature data from built-in functions 2040 with a location of Work, a time of 2:28 PM (1428), a calendar event of “Meet230PM”, and a number of local devices of 5.
- Learning model 2030 can apply machine-learning operations to predict the requested volume settings.
- FIG. 20B shows that the prediction of communication 2090 c includes a ringtone volume setting of 0, a mute setting of NO, and vibration setting of YES.
- FIG. 20B shows that built-in features 2040 then sends communication 2092 a to learning model 2030 .
- Communication 2092 a includes a Push( ) function call to inform learning model 2030 that mobile platform 2002 is at “LoudSpot” at 11:58 PM (2358) without any calendar events scheduled and 17 local devices detected.
- learning model 2030 can apply machine-learning operations to predict volume settings based on the information provided via communication 2092 a .
- Scenario 2000 continues with learning model 2030 sending a Notify( ) function call in communication 2092 b to dialer application 2020 , predicting a ringtone volume setting of 85, a mute setting of NO, and vibration setting of YES.
- mobile platform 2002 and/or dialer application 2020 can prompt a user of mobile platform 2002 of recently-predicted volume settings and/or apply the recently-predicted volume settings, perhaps based on settings of smart volume setting 2026 and prediction usage setting 2028 .
- built-in features 2040 sends communication 2094 a to learning model 2030 , where communication 2094 a includes a Push( ) function call to inform learning model 2030 that mobile platform 2002 is at “Work” at 12:57 PM (1257), with a calendar event of “Meeting1PM”, and no other local mobile platforms present.
- learning model 2030 can apply machine-learning operations to predict volume settings based on the information provided via communication 2094 a .
- Scenario 2000 continues with learning model 2030 sending a Notify( ) function call in communication 2094 b to dialer application 2020 , predicting a ringtone volume setting of 0, a mute setting of YES, and vibration setting of YES.
- mobile platform 2002 and/or dialer application 2020 can prompt a user of mobile platform 2002 of recently-predicted volume settings and/or apply the predicted volume settings, perhaps based on settings of smart volume setting 2026 and prediction usage setting 2028 .
- Scenario 2000 concludes with dialer application 2020 call Save( ) function via communication 2096 to request that learning model 2030 save a context, and perhaps other data, of a learning session thread in a VolSetModel variable.
- learning model 2030 can provide a response to communication 2096 , such as a return value to the Save( ) function and/or a communication providing a status of the Save( ) function; e.g., SaveOK or SaveFail.
- FIG. 21 shows server devices 2108 , 2110 configured to communicate, via network 2106 , with programmable devices 2104 a , 2104 b , and 2104 c .
- Network 2106 may correspond to a LAN, a wide area network (WAN), a corporate intranet, the public Internet, or any other type of network configured to provide a communications path between networked computing devices.
- the network 2106 may also correspond to a combination of one or more LANs, WANs, corporate intranets, and/or the public Internet.
- FIG. 21 only shows three programmable devices, distributed application architectures may serve tens, hundreds, or thousands of programmable devices.
- programmable devices 2104 a , 2104 b , and 2104 c may be any sort of computing device, such as an ordinary laptop computer, desktop computer, network terminal, wireless communication device (e.g., a cell phone or smart phone), and so on.
- programmable devices 2104 a , 2104 b , and 2104 c may be dedicated to the design and use of software applications.
- programmable devices 2104 a , 2104 b , and 2104 c may be general purpose computers that are configured to perform a number of tasks and need not be dedicated to software development tools.
- Server devices 2108 , 2110 can be configured to perform one or more services, as requested by programmable devices 2104 a , 2104 b , and/or 2104 c .
- server device 2108 and/or 2110 can provide content to programmable devices 2104 a - 2104 c .
- the content can include, but is not limited to, web pages, hypertext, scripts, binary data such as compiled software, images, audio, and/or video.
- the content can include compressed and/or uncompressed content.
- the content can be encrypted and/or unencrypted. Other types of content are possible as well.
- server device 2108 and/or 2110 can provide programmable devices 2104 a - 2104 c with access to software for database, search, computation, graphical, audio, video, World Wide Web/Internet utilization, and/or other functions.
- server devices Many other examples are possible as well.
- FIG. 22A is a block diagram of a computing device (e.g., system) in accordance with an example embodiment.
- computing device 2200 shown in FIG. 22A can be configured to perform one or more functions of mobile platforms 200 , 502 , 602 , 902 server devices 2108 , 2110 , network 2106 , and/or one or more of programmable devices 2104 a , 2104 b , and 2104 c .
- Computing device 2200 may include a user interface module 2201 , a network communications interface module 2202 , one or more processors 2203 , and data storage 2204 , all of which may be physically and/or communicatively linked together via a system bus, network, or other connection mechanism 2205 .
- user interface module 2201 is configured to send data to and/or receive data from external user input/output devices.
- user interface module 2201 can be configured to send and/or receive data to and/or from user input devices such as a keyboard, a keypad, a touch screen, a computer mouse, a track ball, a joystick, a camera, a voice recognition module, and/or other similar devices.
- User interface module 2201 can also be configured to provide output to user display devices, such as one or more cathode ray tubes (CRT), liquid crystal displays (LCD), light emitting diodes (LEDs), displays using digital light processing (DLP) technology, printers, light bulbs, and/or other similar devices, either now known or later developed.
- CTR cathode ray tubes
- LCD liquid crystal displays
- LEDs light emitting diodes
- DLP digital light processing
- User interface module 2201 can also be configured generate audible output(s) through device(s) such as a speaker, speaker jack, audio output port, audio output device, earphones, telephone ringers, and/or other similar devices.
- user interface module 2210 can be configured to provide haptic and/or tactile feedback using one or more vibration devices, tactile sensors, actuators including haptic actuators, tactile touchpads, piezo-haptic devices, piezo-haptic drivers, and/or other similar devices.
- Network communications interface module 2202 can include one or more wireless interfaces 2207 and/or one or more wireline interfaces 2208 that are configurable to communicate via a network, such as network 2106 shown in FIG. 21 .
- Wireless interfaces 2207 can include one or more wireless transmitters, receivers, and/or transceivers, such as a Bluetooth transceiver, a Zigbee transceiver, a Wi-Fi transceiver, a WiMAX transceiver, and/or other similar type of wireless transceiver configurable to communicate via a wireless network.
- Wireline interfaces 2208 can include one or more wireline transmitters, receivers, and/or transceivers, such as an Ethernet transceiver, a Universal Serial Bus (USB) transceiver, or similar transceiver configurable to communicate via a twisted pair wire, a coaxial cable, a fiber-optic link, or a similar physical connection to a wireline network.
- wireline transmitters such as an Ethernet transceiver, a Universal Serial Bus (USB) transceiver, or similar transceiver configurable to communicate via a twisted pair wire, a coaxial cable, a fiber-optic link, or a similar physical connection to a wireline network.
- USB Universal Serial Bus
- network communications interface module 2202 can be configured to provide reliable, secured, and/or authenticated communications. For each communication described herein, information for ensuring reliable communications (i.e., guaranteed message delivery) can be provided, perhaps as part of a message header and/or footer (e.g., packet/message sequencing information, encapsulation header(s) and/or footer(s), size/time information, and transmission verification information such as CRC and/or parity check values). Communications can be made secure (e.g., be encoded or encrypted) and/or decrypted/decoded using one or more cryptographic protocols and/or algorithms, such as, but not limited to, DES, AES, RSA, Diffie-Hellman, and/or DSA. Other cryptographic protocols and/or algorithms can be used as well or in addition to those listed herein to secure (and then decrypt/decode) communications.
- cryptographic protocols and/or algorithms can be used as well or in addition to those listed herein to secure (and then decrypt/decode) communications.
- Processors 2203 can include one or more general purpose processors and/or one or more special purpose processors (e.g., digital signal processors, application specific integrated circuits, etc.). Processors 2203 can be configured to execute computer-readable program instructions 2206 that are contained in the data storage 2204 and/or other instructions as described herein.
- processors 2203 can include one or more general purpose processors and/or one or more special purpose processors (e.g., digital signal processors, application specific integrated circuits, etc.).
- Processors 2203 can be configured to execute computer-readable program instructions 2206 that are contained in the data storage 2204 and/or other instructions as described herein.
- Data storage 2204 can include one or more computer-readable storage media that can be read and/or accessed by at least one of processors 2203 .
- the one or more computer-readable storage media can include volatile and/or non-volatile storage components, such as optical, magnetic, organic or other memory or disc storage, which can be integrated in whole or in part with at least one of processors 2203 .
- data storage 2204 can be implemented using a single physical device (e.g., one optical, magnetic, organic or other memory or disc storage unit), while in other embodiments, data storage 2204 can be implemented using two or more physical devices.
- Data storage 2204 can include computer-readable program instructions 2206 and perhaps additional data. In some embodiments, data storage 2204 can additionally include storage required to perform at least part of the herein-described methods and techniques and/or at least part of the functionality of the herein-described devices and networks.
- FIG. 22B depicts a network 2106 of computing clusters 2209 a , 2209 b , 2209 c arranged as a cloud-based server system in accordance with an example embodiment.
- Server devices 2108 and/or 2110 of FIG. 21 can be cloud-based devices that store program logic and/or data of cloud-based applications and/or services.
- server devices 2108 and/or 2110 can be a single computing device residing in a single computing center.
- server device 2108 and/or 2110 can include multiple computing devices in a single computing center, or even multiple computing devices located in multiple computing centers located in diverse geographic locations.
- FIG. 21 depicts each of server devices 2108 and 2110 residing in different physical locations.
- data and services at server devices 2108 and/or 2110 can be encoded as computer readable information stored in non-transitory, tangible computer readable media (or computer readable storage media) and accessible by programmable devices 2104 a , 2104 b , and 2104 c , and/or other computing devices.
- data at server device 2108 and/or 2110 can be stored on a single disk drive or other tangible storage media, or can be implemented on multiple disk drives or other tangible storage media located at one or more diverse geographic locations.
- FIG. 22B depicts a cloud-based server system in accordance with an example embodiment.
- the functions of server device 2108 and/or 2110 can be distributed among three computing clusters 2209 a , 2209 b , and 2209 c .
- Computing cluster 2209 a can include one or more computing devices 2200 a , cluster storage arrays 2210 a , and cluster routers 2211 a connected by a local cluster network 2212 a .
- computing cluster 2209 b can include one or more computing devices 2200 b , cluster storage arrays 2210 b , and cluster routers 2211 b connected by a local cluster network 2212 b .
- computing cluster 2209 c can include one or more computing devices 2200 c , cluster storage arrays 2210 c , and cluster routers 2211 c connected by a local cluster network 2212 c.
- each of the computing clusters 2209 a , 2209 b , and 2209 c can have an equal number of computing devices, an equal number of cluster storage arrays, and an equal number of cluster routers. In other embodiments, however, each computing cluster can have different numbers of computing devices, different numbers of cluster storage arrays, and different numbers of cluster routers. The number of computing devices, cluster storage arrays, and cluster routers in each computing cluster can depend on the computing task or tasks assigned to each computing cluster.
- computing devices 2200 a can be configured to perform various computing tasks of server device 2108 .
- the various functionalities of server device 2108 can be distributed among one or more of computing devices 2200 a , 2200 b , and 2200 c .
- Computing devices 2200 b and 2200 c in computing clusters 2209 b and 2209 c can be configured similarly to computing devices 2200 a in computing cluster 2209 a .
- computing devices 2200 a , 2200 b , and 2200 c can be configured to perform different functions.
- computing tasks and stored data associated with server devices 2108 and/or 2110 can be distributed across computing devices 2200 a , 2200 b , and 2200 c based at least in part on the processing requirements of server devices 2108 and/or 2110 , the processing capabilities of computing devices 2200 a , 2200 b , and 2200 c , the latency of the network links between the computing devices in each computing cluster and between the computing clusters themselves, and/or other factors that can contribute to the cost, speed, fault-tolerance, resiliency, efficiency, and/or other design goals of the overall system architecture.
- the cluster storage arrays 2210 a , 2210 b , and 2210 c of the computing clusters 2209 a , 2209 b , and 2209 c can be data storage arrays that include disk array controllers configured to manage read and write access to groups of hard disk drives.
- the disk array controllers alone or in conjunction with their respective computing devices, can also be configured to manage backup or redundant copies of the data stored in the cluster storage arrays to protect against disk drive or other cluster storage array failures and/or network failures that prevent one or more computing devices from accessing one or more cluster storage arrays.
- cluster storage arrays 2210 a , 2210 b , and 2210 c can be distributed across cluster storage arrays 2210 a , 2210 b , and 2210 c .
- some cluster storage arrays can be configured to store the data of server device 2108
- other cluster storage arrays can store data of server device 2110 .
- some cluster storage arrays can be configured to store backup versions of data stored in other cluster storage arrays.
- the cluster routers 2211 a , 2211 b , and 2211 c in computing clusters 2209 a , 2209 b , and 2209 c can include networking equipment configured to provide internal and external communications for the computing clusters.
- the cluster routers 2211 a in computing cluster 2209 a can include one or more internet switching and routing devices configured to provide (i) local area network communications between the computing devices 2200 a and the cluster storage arrays 2201 a via the local cluster network 2212 a , and (ii) wide area network communications between the computing cluster 2209 a and the computing clusters 2209 b and 2209 c via the wide area network connection 2213 to network 2106 .
- Cluster routers 2211 b and 2211 c can include network equipment similar to the cluster routers 2211 a , and cluster routers 2211 b and 2211 c can perform similar networking functions for computing clusters 2209 b and 2209 b that cluster routers 2211 a perform for computing cluster 2209 a.
- the configuration of the cluster routers 2211 a , 2211 b , and 2211 c can be based at least in part on the data communication requirements of the computing devices and cluster storage arrays, the data communications capabilities of the network equipment in the cluster routers 2211 a , 2211 b , and 2211 c , the latency and throughput of local networks 2212 a , 2212 b , 2212 c , the latency, throughput, and cost of wide area network links 2213 a , 2213 b , and 2213 c , and/or other factors that can contribute to the cost, speed, fault-tolerance, resiliency, efficiency and/or other design goals of the moderation system architecture.
- each block and/or communication may represent a processing of information and/or a transmission of information in accordance with example embodiments.
- Alternative embodiments are included within the scope of these example embodiments.
- functions described as blocks, transmissions, communications, requests, responses, and/or messages may be executed out of order from that shown or discussed, including substantially concurrent or in reverse order, depending on the functionality involved.
- more or fewer blocks and/or functions may be used with any of the ladder diagrams, scenarios, and flow charts discussed herein, and these ladder diagrams, scenarios, and flow charts may be combined with one another, in part or in whole.
- a block that represents a processing of information may correspond to circuitry that can be configured to perform the specific logical functions of a herein-described method or technique.
- a block that represents a processing of information may correspond to a module, a segment, or a portion of program code (including related data).
- the program code may include one or more instructions executable by a processor for implementing specific logical functions or actions in the method or technique.
- the program code and/or related data may be stored on any type of computer readable medium such as a storage device including a disk or hard drive or other storage medium.
- the computer readable medium may also include non-transitory computer readable media such as computer-readable media that stores data for short periods of time like register memory, processor cache, and random access memory (RAM).
- the computer readable media may also include non-transitory computer readable media that stores program code and/or data for longer periods of time, such as secondary or persistent long term storage, like read only memory (ROM), optical or magnetic disks, compact-disc read only memory (CD-ROM), for example.
- the computer readable media may also be any other volatile or non-volatile storage systems.
- a computer readable medium may be considered a computer readable storage medium, for example, or a tangible storage device.
- a block that represents one or more information transmissions may correspond to information transmissions between software and/or hardware modules in the same physical device. However, other information transmissions may be between software modules and/or hardware modules in different physical devices.
Abstract
Disclosed are apparatus and methods for providing machine-learning services. A context-identification system executing on a mobile platform can receive data comprising context-related data associated with the mobile platform and application-related data received from the mobile platform. The context-identification system can identify a context using the context-related data associated with the mobile platform and/or the application-related data received from the mobile platform. Based on at least one context identified, context-identification system can predict a communicative action associated with the mobile platform by performing a machine-learning operation on the received data. An instruction can be received to execute the communicative action associated with the mobile platform.
Description
- This application claims priority to U.S. Patent App. No. 61/663,438, entitled “Method to Predict a Communicative Action that is Most Likely to be Executed Given a Context”, filed Jun. 22, 2012, the contents of which are fully incorporated by reference herein for all purposes.
- Unless otherwise indicated herein, the materials described in this section are not prior art to the claims in this application and are not admitted to be prior art by inclusion in this section.
- Mobile devices are ubiquitous in modern communication networks. Many of these mobile devices are “smart phones”, capable of running one or more applications while acting as a communication device. The applications and/or the smart phone itself can have a number of settings subject to user control, such as volume settings, network addresses/names, contact data, and calendar information.
- A smart phone user can change some or all of these settings based on their context, such as location and activity. For example, the user can turn down a ringing volume and/or mute a ringer, prior to watching a movie at a movie theater. After the movie theater completes, the user can the turn up the ringing volume and/or un-mute the ringer.
- In one aspect, a method is provided. A machine-learning service executing on a mobile platform receives data related to a plurality of features. The machine-learning service determines at least one feature in the plurality of features based on the received data. The machine-learning service generates an output by performing a machine-learning operation on the at least one feature of the plurality of features. The machine-learning operation is selected from among: an operation of ranking the at least one feature, an operation of classifying the at least one feature, an operation of predicting the at least one feature, and an operation of clustering the at least one feature. The machine-learning service sends the output.
- In another aspect, a method is provided. A machine-learning service executing on a mobile platform receives feature-related data related to a plurality of features. The feature-related data includes data related to a first plurality of features received from an application executing on the mobile platform and data related to a second plurality of features received from the mobile platform. The first plurality of features and the second plurality of features differ. The machine-learning service determines at least one feature among the first plurality of features and the second plurality of feature based on the feature-related data. The machine-learning service generates an output by performing a machine-learning operation on the at least one feature. The machine-learning service sends the output to the application.
- In even another aspect, an article of manufacture is provided. The article of manufacture includes a non-transitory computer-readable storage medium having instructions stored thereon that, when executed on by a processor, cause the processor to perform functions. The functions include: (a) receiving data related to a plurality of features, (b) determining at least one feature in the plurality of features based on the received data, (c) generating an output by performing a machine-learning operation on the at least one feature of the plurality of features, where the machine-learning operation is selected from among: an operation of ranking the at least one feature, an operation of classifying the at least one feature, an operation of predicting the at least one feature, and an operation of clustering the at least one feature, and (d) sending the output.
- In yet another aspect, a mobile platform is provided. The mobile platform includes a processor and a non-transitory computer-readable storage medium, configured to store instructions that, when executed by the processor, cause the mobile platform to perform functions. The functions include: (a) receiving data related to a plurality of features, (b) determining at least one feature in the plurality of features based on the received data, (c) generating an output by performing a machine-learning operation on the at least one feature of the plurality of features, where the machine-learning operation is selected from among: an operation of ranking the at least one feature, an operation of classifying the at least one feature, an operation of predicting the at least one feature, and an operation of clustering the at least one feature, and (d) sending the output.
- In a second aspect, a method is provided. A context-identification system executing on a mobile platform receives data comprising: context-related data associated with the mobile platform and application-related data received from the mobile platform. The context-identification system identifies at least one context using the context-related data associated with the mobile platform and/or the application-related data received from the mobile platform. Based on at least one context identified, the context-identification system predicts a communicative action associated with the mobile platform by performing a machine-learning operation on the received data. An instruction is received to execute the communicative action associated with the mobile platform.
- In another aspect, an article of manufacture is provided. The article of manufacture includes a non-transitory computer-readable storage medium having instructions stored thereon that, when executed on by a processor, cause the processor to perform functions. The functions include: (a) receiving data at a context-identification system executing on a mobile platform, where the received data includes context-related data associated with the mobile platform and application-related data received from the mobile platform, (b) identifying a context using the context-related data associated with the mobile platform and/or the application-related data received from the mobile platform, (c) based on at least one context identified, predicting a communicative action associated with the mobile platform by performing a machine-learning operation on the data received; and (d) receiving an instruction to execute the communicative action associated with the mobile platform.
- In yet another aspect, a mobile platform is provided. The mobile platform includes a processor and a non-transitory computer-readable storage medium, configured to store instructions that, when executed by the processor, cause the mobile platform to perform functions. (a) receiving data at a context-identification system executing on a mobile platform, where the received data includes context-related data associated with the mobile platform and application-related data received from the mobile platform, (b) identifying a context using the context-related data associated with the mobile platform and/or the application-related data received from the mobile platform, (c) based on at least one context identified, predicting a communicative action associated with the mobile platform by performing a machine-learning operation on the data received; and (d) receiving an instruction to execute the communicative action associated with the mobile platform.
- In a third aspect, a method is provided. A machine-learning service executing on a mobile platform receives feature-related data. The feature-related data includes image-related data related to one or more images received from an application executing on the mobile platform and platform-related data received from the mobile platform. The image-related data and the platform-related data differ. The machine-learning service generates a title related to the one or more images by performing a machine-learning operation on the feature-related data. The machine-learning service sends the title related to the one or more images to the application.
- In another aspect, an article of manufacture is provided. The article of manufacture includes a non-transitory computer-readable storage medium having instructions stored thereon that, when executed on by a processor, cause the processor to perform functions. The functions include: (i) receiving feature-related data that includes image-related data related to one or more images and platform-related data, where the image-related data and the platform-related data differ, (ii) generating a title related to the one or more images by performing a machine-learning operation on the feature-related data, and (iii) sending the title related to the one or more images.
- In yet another aspect, a mobile platform is provided. The mobile platform includes a processor and a non-transitory computer-readable storage medium, configured to store instructions that, when executed by the processor, cause the mobile platform to perform functions. The functions include: (i) receiving feature-related data that includes image-related data related to one or more images and platform-related data, where the image-related data and the platform-related data differ, (ii) generating a title related to the one or more images by performing a machine-learning operation on the feature-related data, and (iii) sending the title related to the one or more images.
- In a fourth aspect, a method is provided. A machine-learning service executing on a mobile platform receives feature-related data. The feature-related data includes communications-related data related to one or more searches for establishing electronic communications received from an application executing on the mobile platform and platform-related data received from the mobile platform. The communications-related data and the platform-related data differ. The machine-learning service determines whether the machine-learning service is trained to perform machine-learning operations related to predicting outcomes of searches for establishing electronic communications. In response to determining that the machine-learning service is trained: (a) the machine-learning service receives a request for a predicted outcome of a search for establishing an electronic communication, (b) the machine-learning service generates the predicted outcome by performing a machine-learning operation on the feature-related data, and (c) the predicted outcome is sent to the application.
- In another aspect, an article of manufacture is provided. The article of manufacture includes a non-transitory computer-readable storage medium having instructions stored thereon that, when executed on by a processor, cause the processor to perform functions. The functions include: (a) receiving feature-related data at a machine-learning service, where the feature-related data includes communications-related data related to one or more searches for establishing electronic communications and platform-related data received from the mobile platform, and where the communications-related data and the platform-related data differ, (b) determining whether the machine-learning service is trained to perform machine-learning operations related to predicting outcomes of searches for establishing electronic communications, (c) in response to determining that the machine-learning service is trained: (i) receiving, at the machine-learning service, a request for a predicted outcome of a search for establishing an electronic communication, (ii) generating the predicted outcome by the machine-learning service performing a machine-learning operation on the feature-related data, and (iii) sending the predicted outcome.
- In yet another aspect, a mobile platform is provided. The mobile platform includes a processor and a non-transitory computer-readable storage medium, configured to store instructions that, when executed by the processor, cause the mobile platform to perform functions. The functions include: (a) receiving feature-related data at a machine-learning service, where the feature-related data includes communications-related data related to one or more searches for establishing electronic communications and platform-related data, and where the communications-related data and the platform-related data differ, (b) determining whether or not outcomes of searches for establishing electronic communications can be predicted, (c) in response to determining that the outcomes of searches for establishing electronic communications can be predicted: (i) receiving a request for a predicted outcome of a search for establishing an electronic communication, (ii) generating the predicted outcome by performing a machine-learning operation on the feature-related data, and (iii) sending the predicted outcome.
- In a fifth aspect, a method is provided. A machine-learning service executing on a mobile platform receives feature-related data. The feature-related data includes usage-related data about one or more time spans that the mobile platform is activated and platform-related data received from the mobile platform. The usage-related data and the platform-related data differ. The machine-learning service determines whether the machine-learning service is trained to perform machine-learning operations related to predicting a time span that the mobile platform will be activated. In response to determining that the machine-learning service is trained: (a) the machine-learning service receives a request for a predicted time span that the mobile platform will be activated, (b) the machine-learning service determines the predicted time span by performing a machine-learning operation on the feature-related data, and (c) the predicted time span is sent to the application.
- In another aspect, an article of manufacture is provided. The article of manufacture includes a non-transitory computer-readable storage medium having instructions stored thereon that, when executed on by a processor, cause the processor to perform functions. The functions include: (a) receiving feature-related data at a machine-learning service, where the feature-related data includes usage-related data about one or more time spans and platform-related data received from the mobile platform, and where the usage-related data and the platform-related data differ, (b) determining whether the machine-learning service is trained to perform machine-learning operations related to predicting a time span, (c) in response to determining that the machine-learning service is trained: (i) receiving, at the machine-learning service, a request for a predicted time span, (ii) determining the predicted time span by the machine-learning service performing a machine-learning operation on the feature-related data, and (iii) sending the predicted time span.
- In quite another aspect, a mobile platform is provided. The mobile platform includes a processor and a non-transitory computer-readable storage medium, configured to store instructions that, when executed by the processor, cause the mobile platform to perform functions. The functions include: (a) receiving feature-related data at a machine-learning service, where the feature-related data includes usage-related data about one or more time spans that the mobile platform is activated and platform-related data, and where the usage-related data and the platform-related data differ, (b) determining whether the machine-learning service is trained to perform machine-learning operations related to predicting a time span that the mobile platform will be activated, (c) in response to determining that the machine-learning service is trained: (i) receiving a request for a predicted time span that the mobile platform will be activated at the machine-learning service, (ii) determining the predicted time span by the machine-learning service performing a machine-learning operation on the feature-related data, and (iii) sending the predicted time span.
- In a sixth aspect, a method is provided. A machine-learning service executing on a mobile platform receives feature-related data. The feature-related data includes volume-related data about one or more volume-related settings for the mobile platform and platform-related data received from the mobile platform. The volume-related data and the platform-related data differ. The machine-learning service determines whether the machine-learning service is trained to perform machine-learning operations related to predicting a change in the one or more volume-related settings for the mobile platform. In response to determining that the machine-learning service is trained: (a) the machine-learning service receives a request for predicting the change in the one or more volume-related settings, (b) the machine-learning service determining the predicted change in the one or more volume-related settings by performing a machine-learning operation on the feature-related data, and (c) sending the predicted change in the one or more volume-related settings.
- In another aspect, an article of manufacture is provided. The article of manufacture includes a non-transitory computer-readable storage medium having instructions stored thereon that, when executed on by a processor, cause the processor to perform functions. The functions include: (a) receiving feature-related data at a machine-learning service executing on the processor, where the feature-related data includes volume-related data about one or more volume-related settings and platform-related data, and where the usage-related data and the platform-related data differ, (b) determining whether the machine-learning service is trained to perform machine-learning operations related to predicting a change in the one or more volume-related settings, (c) in response to determining that the machine-learning service is trained: (i) receiving, at the machine-learning service, a request for predicting the change in the one or more volume-related settings, (ii) determining the predicted change in the one or more volume-related settings by the machine-learning service performing a machine-learning operation on the feature-related data, and (iii) sending the predicted change in the one or more volume-related settings.
- In yet another aspect, a mobile platform is provided. The mobile platform includes a processor and a non-transitory computer-readable storage medium, configured to store instructions that, when executed by the processor, cause the mobile platform to perform functions. The functions include: (a) receiving feature-related data at a machine-learning service executing on the processor, where the feature-related data includes volume-related data about one or more volume-related settings and platform-related data received from the mobile platform, and where the usage-related data and the platform-related data differ, (b) determining whether the machine-learning service is trained to perform machine-learning operations related to predicting a change in the one or more volume-related settings for the mobile platform, (c) in response to determining that the machine-learning service is trained: (i) receiving, at the machine-learning service, a request for predicting the change in the one or more volume-related settings, (ii) determining the predicted change in the one or more volume-related settings by the machine-learning service performing a machine-learning operation on the feature-related data, and (iii) sending the predicted change in the one or more volume-related settings.
- In the figures:
-
FIG. 1A is a flow chart of a method, in accordance with an example embodiment. -
FIG. 1B is a flow chart of another method, in accordance with an example embodiment. -
FIG. 2 is a block diagram of a mobile platform, in accordance with an example embodiment. -
FIG. 3 is a block diagram of a machine learning and adaptation service, in accordance with an example embodiment. -
FIG. 4 depicts programming interfaces for the machine learning and adaptation service, in accordance with an example embodiment. -
FIG. 5A shows an example user interface used in a first scenario, in accordance with an example embodiment. -
FIG. 5B shows example communications used in the first scenario, in accordance with an example embodiment. -
FIG. 6A shows an example user interface used in a second scenario, in accordance with an example embodiment. -
FIG. 6B shows example communications for the second scenario, in accordance with an example embodiment. -
FIG. 7 is a flow chart of a method, in accordance with an example embodiment. -
FIG. 8A shows an example user interface used in a third scenario, in accordance with an example embodiment. -
FIG. 8B shows an example data record, in accordance with an example embodiment. -
FIG. 9A shows an example context identification system, in accordance with an example embodiment. -
FIG. 9B shows an example collective context identification system, in accordance with an example embodiment. -
FIG. 9C shows an example context identification system adaptable to feedback, in accordance with an example embodiment. -
FIG. 10 is a flow chart of a method, in accordance with an example embodiment. -
FIG. 11A shows an example image identification system, in accordance with an example embodiment. -
FIG. 11B shows an example collective image identification system, in accordance with an example embodiment. -
FIG. 11C shows an example image identification system adaptable to feedback, in accordance with an example embodiment. -
FIG. 12A shows an example user interface used in a fourth scenario, in accordance with an example embodiment. -
FIG. 12B shows example communications for the fourth scenario, in accordance with an example embodiment. -
FIG. 13 is a flow chart of a method, in accordance with an example embodiment. -
FIG. 14 shows a scenario for establishing wireless communications with a number of access points, in accordance with an example embodiment. -
FIG. 15A shows an example user interface used in a fifth scenario, in accordance with an example embodiment. -
FIG. 15B shows example communications for the fifth scenario, in accordance with an example embodiment. -
FIG. 16 is a flow chart of a method, in accordance with an example embodiment. -
FIG. 17A shows a usage log of activations of a mobile platform, in accordance with an example embodiment. -
FIG. 17B shows a usage log of activations of a mobile platform at a particular location, in accordance with an example embodiment. -
FIG. 17C is a graph of a linear regression model based on the data in the usage log shown inFIG. 17B , in accordance with an example embodiment. -
FIG. 18A is a classification diagram based on the data in the log shown inFIG. 17A , in accordance with an example embodiment. -
FIG. 18B is an example user interface for a jukebox application used in a sixth scenario, in accordance with an example embodiment. -
FIG. 19 is a flow chart of a method, in accordance with an example embodiment. -
FIG. 20A shows an example user interface used in a seventh scenario, in accordance with an example embodiment. -
FIG. 20B shows example communications for the seventh scenario, in accordance with an example embodiment. -
FIG. 21 depicts a distributed computing architecture, in accordance with an example embodiment. -
FIG. 22A is a block diagram of a computing device, in accordance with an example embodiment. -
FIG. 22B depicts a cloud-based server system, in accordance with an example embodiment. - Overview
- Most mobile platforms, such as mobile phones, do not adapt to the person(s) using the platform; rather, the person(s) adapt to use the mobile platform. Typically, customization of the mobile platform and the applications utilizing the mobile platform is often limited based on implementation decisions.
- A “machine-learning service” or machine-learning and adaptation service can support automatic adaptation of preferences of person(s) using the mobile platform. The machine-learning service is software running on the mobile platform that provides the necessary functionality for software applications to learn from interactions of person(s) using the mobile platform.
- The machine-learning service can communicate with software applications via an Application Program Interface (API). The API provides access to several commonly-used machine adaptation techniques. For example, the API can provide access to interfaces for ranking, clustering, classifying, and prediction techniques. Also, a software application can provide one or more inputs to the machine-learning service. For example, a software application controlling a volume setting of a speaker can provide volume setting values as an input to the machine-learning service.
- The API can also utilize data that the software application does not and perhaps cannot access. For example, a volume-setting application may not have access to location data for the mobile platform. However, the volume-setting application could request the machine-learning service to predict the volume setting of the speaker based on location. The machine-learning service need only provide the predictions, not the locations, to the volume-setting application. As such, the machine-learning service can encapsulate the use of sensitive data.
- The machine-learning service can include a data aggregation and representation engine (DARE) that constantly receives and stores input data, perhaps from multiple sources. The stored input data can be aggregated to discover features within the data; such as location labels based on actions and times; e.g. Home, Work, School, etc.
- The machine adaptation techniques used by the machine-learning service can be implemented to work best within the processing, memory, and other resource constraints of a mobile platform. For example, the machine adaptation techniques can use incremental learning algorithms that require limited or no historical information for training, and thus may reduce the total amount of memory needed by the machine-learning-service.
- In some embodiments, the machine-learning service can utilize network support functionality to access non-private and/or anonymized data aggregated across multiple mobile platforms. The aggregated data can be stored in one or more servers, or other devices other than the mobile platform, and downloaded as needed. For example, aggregated data can be used to train and/or set initial values for the machine adaptation techniques used by the machine-learning service.
- Example uses of the machine-learning service can include:
- Predicting a duration of a mobile session before the mobile session starts, based on location, time, calendar entries, prior behavior, etc.
- Predicting a phone number to be dialed at the onset of utilizing a phone dialing application, based on location, time, calendar entries, prior behavior, etc.
- Predicting speaker and/or mute settings for the mobile platform, based on location, time, calendar entries, prior behavior, etc.
- Classifying locations based on their ability to use Wi-Fi and/or other communication networks.
- Generating example photo names and photo album names for a camera application utilizing the mobile platform.
- Many other examples are possible as well.
- By providing application access to a number of machine adaptation techniques designed to operate on and learn about user behavior of a mobile platform, the machine-learning service can make mobile platforms easier to use, more efficient from a user's point of view, and save users time and effort in utilizing the variety of applications available on the mobile platform.
- Example Operations
- Turning to the figures,
FIG. 1A is a flow chart ofmethod 100, in accordance with an example embodiment. In some embodiments, part or all ofmethod 100 can be executed using one or more mobile platforms; e.g.,mobile platform computing device 2200. -
Method 100 begins atblock 110, where a machine-learning service executing on a mobile platform can receive data related to a plurality of features. In some embodiments, the received data can include data aggregated from a plurality of mobile platforms. - At
block 120, the machine-learning service can determine at least one feature in the plurality of features based on the received data. - At
block 130, the machine-learning service can generate an output by the performing a machine-learning operation on the at least one feature of the plurality of features. The machine-learning operation can be selected from among: an operation of ranking the at least one feature, an operation of classifying the at least one feature, an operation of predicting the at least one feature, and an operation of clustering the at least one feature. - In some embodiments, generating the output by the machine-learning service can include selecting a machine-learning algorithm to perform the machine-learning operation. In particular embodiments, selecting the machine-learning algorithm to perform the machine-learning operation can include the machine-learning service selecting the machine-learning algorithm.
- In some of the particular embodiments, selecting the machine-learning algorithm to perform the machine-learning operation can include receiving a selection related to the machine-learning algorithm from an application executing on the mobile platform, where the application differs from the machine-learning service. In specific of the particular embodiments, receiving a selection related to the machine-learning algorithm from the application can include receiving a selection related to the machine-learning algorithm from the application via an Application Programming Interface (API) of the machine-learning service.
- At
block 140, the machine-learning service can send the output. In some embodiments, the received data can include data related to a plurality of features related to communication signals and locations, where the mobile platform is located at a first location, and where the output includes an indication of whether to search for the communications signal at the first location. - In other embodiments, the received data can include data related to a plurality of features related to durations of communication sessions and the output can include a prediction of a duration of a new communication session. In still other embodiments, the received data can include data related to features of telephone calls originated by the mobile platform and the output can include a called party of a telephone call to be originated by the mobile platform. In even other embodiments, the received data can include data related to volume and mute settings of the mobile platform and the output can include a prediction of a volume setting and/or a mute setting of the mobile platform. In yet other embodiments, the received data can include data related to one or more images, and the output can include a name related to at least one image of the one or more images.
- In some embodiments,
method 100 can further include: (i) storing the at least one feature and the output for use by the machine-learning service, (ii) receiving second data related to a second plurality of features at the machine-learning service, (iii) determining at least a second feature in the second plurality of features based on the second data, (iv) generating a second output by the machine-learning service performing the machine-learning operation on the at least the second feature and the stored at least one feature and output, and (v) sending the second output from the machine-learning service. -
FIG. 1B is a flow chart ofmethod 150, in accordance with an example embodiment. In some embodiments, part or all ofmethod 150 can be executed using one or more mobile platforms; e.g.,mobile platform computing device 2200. -
Method 150 begins atblock 160, where a machine-learning service executing on a mobile platform can receive feature-related data. The feature-related data can include data related to a first plurality of features received from an application executing on the mobile platform and data related to a second plurality of features received from the mobile platform. The first plurality of features and the second plurality of features can differ. - In some embodiments, the first plurality of features can include a feature selected from among a time, a location, a duration, a signal strength, a power level, a party of a telephone call, an e-mail address, a contact, and a calendar entry. In other embodiments, the second plurality of features can include a feature selected from among an image-related feature and a speech-related feature.
- At
block 170, the machine-learning service can determine at least one feature from among the first plurality of features and the second plurality of features based on the feature-related data. - At
block 180, the machine-learning service can generate an output by performing a machine-learning operation on the at least one feature of the plurality of features. - At
block 190, the machine-learning service can send the output to the application. - Example Machine Learning and Adaptation Service
-
FIG. 2 is a block diagram ofmobile platform 200, in accordance with an example embodiment.Mobile platform 200 includesmobile platform system 210, machine learning andadaptation service 220, system application(s) 230, and user application(s) 240. -
FIG. 2 shows thatmobile platform system 210 includesmobile platform hardware 212 andoperating system software 214.Mobile platform hardware 212 can include hardware of a computing device, such ascomputing device 2200 described below.Operating system software 214 can include, but is not limited to software configured to utilizemobile platform hardware 212, provide user services such as user interfaces for the mobile platform, allocate resources, and schedule processes and/or threads. An example ofoperating system software 214 is the Android operating system developed by Google Inc. of Mountain View, Calif. Additional hardware and/or software can be used bymobile platform system 210 as well. - Application processes, such as system application 230 and user application 240, can execute using
mobile platform system 210. System application 230 can be an application provided by a provider of part or all ofmobile platform system 210. User application 240 can be an application provided by an entity other than a provider of part or all ofmobile platform system 210 - Machine learning and
adaptation service 220 can be configured to provide the functionality of a machine-learning service.FIG. 2 shows that machine learning andadaptation service 220 can communicate with bothmobile platform system 210 and applications 230, 240 via feature-related data (FRD) and machine-learning operation output (MLOO). - Feature-related data can be “built-in” or provided by
mobile platform system 210; e.g., as feature-relateddata 222. Built-in feature related data can include, but are not limited to, times, dates, locations, settings for mobile platform hardware 212 (ringing volume, mute settings, ring tones, brightness levels, power/battery data, etc.), calling data (calling party, called party, dialed digits, calling state, etc.), power/battery data, communication network data (addresses, networks, login/password information, signal strengths, etc.), current application(s) being executed, and user preference information. - Machine learning and
adaptation service 220 can determine one or more features from feature-relateddata 222. For example, suppose that feature-relateddata 222 includes the text of a Short Message Service (SMS) message stating “I'm outta here.” Machine learning andadaptation service 220 can generate features represented as feature vectors. An example feature vector for SMS messages can be a <word, frequency> pair. Using the example SMS message text, a set of corresponding example feature vectors can be {<I'm, 1>, <outta, 1>, <here, 1>}. As another example, suppose text of an e-mail reads “Scott—Either we go now, or we go after John's talk”, then a set of corresponding example feature vectors can be {<Scott-, 1>,<Either, 1>, <we, 2>, <go, 2>, <now, 1>, <or, 1>, <after, 1>, <John's, 1>, <talk, 1>}. - As another example, suppose a list of suggested labels W1, W2, W3 . . . Wn were provided to a user with a request to “click on” the one best representing the user's current location. The user clicked on label W4 to select W4 as the closest representation. Then, perhaps as part of a training exercise, the list of suggested labels can be provided as features to machine learning and
adaptation service 220. An example feature can be a <higher ranked feature, lower ranked feature> feature vector, and as applied to the list of suggested labels, the following feature vectors can be determined <W4, W1>, <W4, W2>, <W4, W3>, <W4, W5> . . . <W4, Wn>. Many other examples of features, feature vectors, and feature-related data are possible as well. - In some embodiments, feature-related data can include commands to machine learning and
adaptation service 220, such as a command to “train” or learn about input data and perform one or more machine learning operations on the learned input data. For example, machine learning andadaptation service 220 can receive a command to cluster a series of locations, which can be provided as latitude/longitude pairs, perhaps using cluster labels such as “Work”, “Home”, “School”, and “Gym”. Then, once trained, machine learning andadaptation service 220 can output one or more machine-learning operation outputs 224. Continuing the location example mentioned above, given an input latitude/longitude pair as part of feature-relateddata 222, the corresponding machine-learningoperation output 224 can be a cluster label. - Machine learning and
adaptation service 220 can also train with and utilize feature-related data provided by system application 230 as feature-relateddata 232 and/or feature-related data provided by user application 240 as feature-relateddata 242. Then, upon receiving input data, machine learning andadaptation service 220 can provide machine-learning operation output(s) to system application 230 as machine-learningoperation output 234 and/or provide machine-learning operation output(s) to user application 240 as machine-learningoperation output 244. That is, machine learning andadaptation service 220 need not have detailed information about feature-relateddata adaptation service 220 can utilize incremental learning techniques to generate a model of feature-relateddata data - Additionally, machine learning and
adaptation service 220 need not have detailed information about applications 230, 240 a priori. Rather, applications 230, 240 can use interface(s) to machine learning andadaptation service 220 to permit use of machine learning andadaptation service 220 as a toolkit of machine-learning techniques. -
FIG. 3 is a block diagram of machine learning andadaptation service 220, in accordance with an example embodiment. Machine learning andadaptation service 220 includes machine learning andadaptation service API 310, machine learning and adaptation engine (MLAE) 312, data aggregation and representation engine (DARE) 314,service manager 316, and machine learning and adaptation service (MLAS)network support 320. - Machine learning and
adaptation service API 310 provides interfaces to access a number of machine learning and adaptation techniques and to exchange data with machine learning andadaptation service 220. In some embodiments, models for machine learning and adaptation can be saved and loaded via machine learning andadaptation service API 310. - Machine learning and
adaptation engine 312 performs the machine learning and adaptation techniques of machine learning andadaptation service 220. In some embodiments, machine learning andadaptation engine 312 can learn data from one or more sources, and then classify, cluster, rank, and/or predict data from given input data. - Classifying data involves putting data with a number N of possible values into one of C1 pre-determined categories, where C1 is finite. For an example with C1=2, a mobile platform can be classified, for each value T of a set of times, into one of two categories: either “powered up” or “not powered up”. As another example with C1=3, a location specified as a latitude/longitude pair (or via another technique) can be classified into one of three categories: “having a strong accessible Wi-Fi signal”, “having an adequate accessible Wi-Fi signal”, or “having little or no accessible Wi-Fi signal.” Many other examples are possible as well.
- Classification can be performed using one or more statistical classification techniques, such as, but not limited to, linear classifiers, support vector machines, quadratic classifiers, kernel estimation, decision trees, neural networks, Bayesian techniques and/or networks, hidden Markov models, binary classifiers, and/or multi-class classifiers.
- Clustering data involves putting data with a number N of possible values into one of C2 clusters, where C2 is finite, and where the clusters are not necessarily pre-determined. Generally, each data item in a given cluster is more similar to each other than to data item(s) in other cluster(s). For example, a mobile platform can track its location throughout the day to find clusters of locations where the mobile platform can be commonly found, such as work location(s), home location(s), shopping location(s), entertainment location(s), and other location(s). Location clusters can vary from person to person; an attorney who works at a law office and then frequents a restaurant may consider the law office as a “work location” and the restaurant as an “entertainment location”, but chef may consider the law office as an “other location” and the restaurant as a “work location.”
- Clustering can be performed using one or more clustering algorithms, such as, but not limited to, connectivity-based clustering, hierarchical clustering, centroid-based clustering, distribution-based clustering, density-based clustering, and partitioning algorithms.
- Ranking a set of data items of size S involves applying a ranking function RF to the S data items and returning the highest (or lowest) N ranked data items, where 0<N≦S. For example, suppose a co-ed volleyball team had the following statistics for points scored:
-
TABLE 1 Name Points Scored Pat 10 Chris A. 9 Kelly 8 Taylor 7 Alex 6 Drew 5 M. C. 4 Jaden 3 Chris B. 2 Lucky 1 - Let the ranking function RF rank the N=10 team players listed in Table 1 from 1 to N based on points scored. For this example, RF(“Pat”)=1, RF(“Chris A”)=2, and RF(“Lucky”)=10.
- Another ranking example is ranking documents based on a query of one or more keywords. The ranking of a document D out of a total of N documents can express the relative relevance of document D to the query. In some embodiments, rankings can have two or more partitions or levels. For example, given a query with keywords K1, K2, . . . Kn, documents can first be ranked as relevant or irrelevant based on keywords K1-Kn. Then, a second ranking function can rank the subset of relevant documents, perhaps in more detail.
- Ranking can be performed using one or more ranking algorithms, such as, but not limited to, instance ranking algorithms, label ranking algorithms, subset ranking algorithms, rank aggregation algorithms, bipartite/k-partite ranking algorithms, and learning-to-rank algorithms.
- Predicting data can involve determining a predicted value given a previous pattern of values. For example, given that the previous N=20 values were: 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 1, and 2, the predicted value may be 1. Prediction can be performed using one or more prediction algorithms, such as, but not limited to, minimax, decision tree algorithms, classification tree algorithms, linear regression, polynomial regression, statistical regression, curve fitting, interpolation, and maximum likelihood estimation algorithms.
- Data aggregation and representation engine 314 aggregates, or combines, data from applications and the mobile platform system and stores the aggregated data in a model accessible to machine learning and
adaptation engine 312. In some scenarios, aggregation of data enables discovery of new inferences based on the combined data that were difficult to observe in the raw data. For example, suppose a mobile platform is located in the labeled locations listed in Table 2 below: -
TABLE 2 % of Time Location at Location Work 42 Home 38 Other 12 Store 13 Store 21 Bart 1 Jill 1 Mom/ Dad 1 Maureen 1 - Assuming a large amount of the “Other” location time is time spent driving, these data indicate that the mobile platform is at work, home, or traveling. That is, the mobile platform spends 80% of the time at work and home and more than 90% of the time at work, home, or (likely) in a vehicle. Based on this information, the mobile platform likely has easy access to commercial electrical power at least 80% of the time, and so battery life may not be as important to this mobile platform, as compared to, say, being able to recharge a battery of the mobile phone from commercial electrical power and while driving.
-
Service manager 316 can manage learning session(s), or instances of one or more machine learning and adaptation models, for the application(s) utilizing machine learning andadaptation service 220.Service manager 316 can also coordinate permissions for access and/or display of data. In some embodiments,service manager 316 can prioritize learning sessions and/or resources used by machine learning andadaptation service 220 - Machine learning and adaptation
service network support 320 can provide “back end” support to access non-private and/or anonymized data that has been authorized to be gathered from a plurality of mobile platforms. In some scenarios, the non-private and/or anonymized data can be resident onmobile platform 200, while in other scenarios, part or all of the non-private and/or anonymized data can reside on other devices, such as servers and/or other mobile platforms. In the other scenarios,mobile platform 200 may temporarily or permanently store the non-private and/or anonymized data, perhaps depending on the access to the non-private and/or anonymized data provided tomobile platform 200 and/or machine learning andadaptation service 220. - As the non-private and/or anonymized data is administered by machine learning and adaptation
service network support 320, the non-private and/or anonymized data can be used by machine learning andadaptation service 220, but not shared or “hidden” from applications. In some embodiments not shown inFIG. 3 , a built-in manager can enable access to built-in features for machine learning andadaptation service 220 while hiding some or all built-in features from applications. -
FIG. 4 depictsprogramming interfaces 400 related to machine learning andadaptation service 200, in accordance with an example embodiment.Applications service manager 316 to manage learning sessions. Each learning session is shown inFIG. 4 as a learning session thread. For example, learningsession threads applications FIG. 4 , an application can conduct zero learning sessions or conduct multiple learning sessions simultaneously. -
FIG. 4 shows thatservice manager 316 includes built-infeatures 410, learning application programming interface (API) 412, andsession information 420. In embodiments not shown inFIG. 4 , built-infeatures 410, learningAPI 412, and/orsession information 420 can be components of other portions of machine learning andadaptation service 220. In still other embodiments not shown inFIG. 4 , built-infeatures 410, learningAPI 412, and/orsession information 420 can be separate components of machine learning andadaptation service 220. - As
FIG. 4 shows just below learning sessions 432 a-43 c, built-infeatures 410 can provide various types of data toservice manager 316 and, more generally, to machine learning andadaptation service 220.FIG. 4 shows examples of data provided by built-infeatures 410, such as location, location labels, application statistics, people-related features, calling information, battery/power data, network information, and historical/usage information. Locations can be specified in terms of latitude/longitude pairs, latitude/longitude/altitude triples, street addresses, intersections, and/or using other location-specifying data. Location labels can include names such as the “Locations” shown in Table 2 above; e.g., work, home, stores, etc. - Application statistics can include a time T an application App was last executed, a location loc an application was last executed, a probability P(App|T) that the application App is executing at a given time, a probability P(App|loc) that App is executing at a given location, and a probability P(App|T & loc) that App is executing at a given location at a given time. The above-mentioned application statistics can be generated by statistical software operating on data regarding application execution times and locations. Other statistics can be determined as application statistics as well or instead.
- Information about people can be determined from contact information, calendar information, social networking accounts and pages, and from other sources. As part of built-in
features 410, information such as, but not limited to a person's name, address, phone number(s), e-mail address(es), social networking account information can be provided to machine learning andadaptation service 220. - Calling information can include per-call information such as calling party, dialed digits/called party information, call duration, call start time, and call end time. In some cases, calling information can also or instead include contact information, such as a name and/or “Caller-ID” data associated with a called party, and/or historical calling information for a number of previous calls NPC, where NPC>0.
- In embodiments not shown in
FIG. 4 , other types of communication information can be provided as part of built-infeatures 410. Examples of other types of communication information include, but are not limited to, e-mail information, SMS information, blog information, and social networking messaging information. - Battery/power information can include, but is not limited to, information about batteries and/or power for the mobile platform, such as battery levels, battery status (e.g., operating, disconnected, charging), commercial power status (e.g., connected, charging, disconnected), time(s) of connection(s) to and/or disconnection(s) from commercial power. Network information can include, but is not limited to, information about telecommunications networks, local area networks, other networks, network addresses, connection time(s), disconnection time(s), and network status (e.g., connecting, connected, acquiring address(es), communicating data, idle, disconnecting, disconnected).
- Historical/usage information can include, but is not limited to, information about previous usage of machine learning and
adaptation service 220. Example information includes a number of features “pushed” or provided to machine learning andadaptation service 220, a number of items “pulled” or requested from machine learning andadaptation service 220, a number of applications currently using machine learning andadaptation service 220, a number of applications that have used machine learning andadaptation service 220 in a unit period of time (e.g., a number of application using machine learning andadaptation service 220 per hour or per day), and maximum and/or minimum numbers of applications currently using machine learning andadaptation service 220. -
Learning API 412 can include at least the functions shown inFIG. 4 : getService( ), RankerInterface( ), PredictionInterface( ), ClassificationInterface( ), and ClusteringInterface( ). In some embodiments, more or fewer functions can be part of learningAPI 412. An application can call the getService( ) function to initialize and start a learning session; e.g., initialize and create a learning session thread. The getService( ) function can return a learning session key that can specify a specific learning session. -
FIG. 4 shows learning API 412 with four example learning interfaces: RankerInterface( ), PredictionInterface( ), ClassificationInterface( ), and ClusteringInterface( ). In some embodiments, more or fewer learning interfaces can be part of learningAPI 412. - An application can call the RankerInterface( ) function to initialize and generate a ranking model of machine learning and
adaptation service 220. The ranking model can be used to order or rank a set of data items of size S by applying a ranking function RF to the S data items and returning the highest (or lowest) N ranked data items, such as discussed above in the context ofFIG. 3 . An application can call the PredictionInterface( ) function to initialize and generate a prediction model of machine learning andadaptation service 220. The prediction model can be used to determine a predicted value given a previous pattern of values, such as discussed above in the context ofFIG. 3 . - An application can call the ClassificationInterface( ) function to initialize and generate a classification model of machine learning and
adaptation service 220. The classification model can be used to put data with a number N of possible values into one of C1 pre-determined categories, where C1 is finite, such as discussed above in the context ofFIG. 3 . An application can call the ClusteringInterface( ) function to initialize and generate a clustering model of machine learning andadaptation service 220. The clustering model can be used to put data with a number N of possible values into one of C2 clusters, where C2 is finite, and where the clusters are not necessarily pre-determined, such as discussed above in the context ofFIG. 3 . - In some embodiments, an application can use only one learning interface per learning session, while in other embodiments, the application can use multiple learning interfaces per learning session.
- Each of the learning interfaces shown in learning
API 412 can include at least the functions ofexample learning interface 440.FIG. 4 shows learning interface 440 with five functions: Push( ), Pull( ), Notify( ), Save( ), and Load( ). In some embodiments, more or fewer functions can be part of learninginterface 440. - The Push( ) function can enable an application to provide data to a learning session. The Pull( ) function can enable an application to request and receive information, such as inference(s) and/or prediction(s), from a learning session.
- The Notify( ) function can enable an application to receive information, such as inference(s) and/or prediction(s), from a learning session without requesting the information. For example, suppose a learning session is informed about an event of interest to the application and consequently uses its learning model to generate a prediction for use by the application. In this example, the application has not requested the prediction, i.e., the application did not call the Pull( ) function. Therefore, the learning session can provide the prediction to the application using the Notify( ) function.
- The Save( ) function can save learning-model data needed to start/restart a learning model, and provide a copy of the learning-model data to the application. The Load( ) function can take a copy of learning-model data from the application and to start/restart a learning model with the learning-model data.
-
Session information 420 can include information about each learning session conducted by machine learning andadaptation service 220.Example session information 420 can include, but is not limited to, a learning session key for the learning session, information about the application such as an application name, application addressing information, type(s) of learning interface(s) used in the learning session (e.g., RankerInterface( ), PredictionInterface( )), usage information for the learning interface(s), and perhaps data stored by the learning model for the learning session, such as a learning model cache. - As mentioned above,
service manager 316 can manage learning session(s) for the application(s) utilizing machine learning andadaptation service 220. Specifically, as shown inFIG. 4 , service manager can enable communications between applications 402 a-402 c, data aggregation andrepresentation engines service manager 316 can ensure that information from a given application is directed to the corresponding learning session thread, and not routed to another learning session thread, and vice versa. One technique to ensure that information from a given application is directed to the corresponding learning session thread is to verify that a learning session key is common to both the given application and the corresponding learning session thread. In some embodiments,session information 420 can include a mapping between session keys, applications, and learning session threads. - Each of data aggregation and
representation engines FIG. 3 . - Each of learning
session threads FIG. 4 shows that learningsession threads feature buffer 434 a. 434 b, 434 c, andcontext adaptation engines FIG. 3 and this figure,FIG. 4 . Each offeature buffers adaptation engines - Each of
contexts adaptation engines interface 440, and the respective context can be loaded using the Load( ) function of learninginterface 440. - Example Machine Learning and Adaptation Service Usage Scenarios
-
FIG. 5A showsexample user interface 510 used inexample scenario 500, in accordance with an example embodiment.Scenario 500 includesuser interface 510 executing onmobile platform 502. As shown inFIG. 5A ,user interface 510 includes an interface fordialer application 520 that includesmicrophone dialog 522.Microphone dialog 522 includes manual microphone setting 524, smart microphone setting 526, an OK button to save current settings before exitingdialog 522, and a Cancel button to exitdialog 522 without saving current settings. - Manual microphone setting 524 includes a slider bar that enables a user to set a microphone output setting between a minimum setting of 0, which effectively mutes the microphone, and a maximum setting of 100, which provides the maximum output volume (a.k.a. maximum gain) for a given input.
FIG. 5A shows manual microphone setting set to 53, or approximately halfway between the minimum and maximum settings. - As shown in
FIG. 5A , smart microphone setting 526 can learn and set microphone volume based on called party and can be set to either enabled or disabled. When smart microphone setting 526 is disabled,dialer application 520 can instruct a microphone application (not shown inFIG. 5A ) to use manual microphone setting 524 to determine output volume for a microphone ofmobile platform 502. When smart microphone setting 526 is enabled,dialer application 520 can use a learning service of machine learning andadaptation service 220 to perform a machine-learning operation to provide setting values for the microphone setting. Then, upon receiving a setting value,dialer application 520 can provide the setting value to the microphone application, which can then determine output volume for the microphone ofmobile platform 502 using the setting value. Inscenario 500, and as shown inFIG. 5A , smart microphone setting 526 is set to enabled. -
FIG. 5B shows example communications forscenario 500, in accordance with an example embodiment. As part of the initial communications, learningmodel 530 is partially trained based on data stored in a “DialerModel” variable and then loaded intolearning model 530 via a Load( ) function call. InFIG. 5B , communications betweendialer application 520 andlearning model 530 of machine learning andadaptation service 220 indicate thatdialer application 520 had access to calling party information. As there are no apparent privacy concerns with microphone settings, microphone settings can be requested bydialer application 520 and provided by learningmodel 530. Just before concluding, learning model data is stored back in the DialerModel variable to preserve information garnered duringscenario 500 for later use and loading intolearning model 530. - In
FIG. 5B ,scenario 500 continues withdialer application 520 calling the getService( ) function of learningmodel 530 viacommunication 552. In response, learningmodel 530 provides a session key S1 viacommunication 554 todialer application 520. Session key S1 is included for subsequent communications betweendialer application 520 andlearning model 530 to permit addressing the subsequent communications to the correct learning model, e.g., learningmodel 530 and correct application, e.g.,dialer application 520 for a learning session keyed by S1. -
Scenario 500 continues withdialer application 520 calling the Load( ) function via communication 556 to provide stored learning model data, e.g., a context, stored in a DialerModel variable passed as a parameter of the Load( ) function to learningmodel 530. Upon receipt of communication 556, learningmodel 530 can load a context and perhaps other data stored in the DialerModel variable into a learning session thread that can be utilized by learningmodel 530. In some scenarios not shown inFIG. 5B , learningmodel 530 can provide a response to communication 556, such as a return value to the Load( ) function and/or a communication providing a status of the Load( ) function; e.g., LoadOK or LoadFail. -
Dialer application 520 can instructlearning model 530, via communication 558, to set up a prediction interface. As shown inFIG. 5B , communication 558 includes a PredictionInterface( ) call with three parameters: session key S1, an input-source reference of CalledParty, and a requested output prediction of microphone volumes, as shown inFIG. 5B , using the predefined MIC_VOL value. - In response, learning
model 530 sends communication 560 to built-infeatures 540 to request a built-in feature (ReqBI) of microphone values, as shown inFIG. 5B by passing the predefined MIC_VOL value to built-infeatures 540. Built-infeatures 540 can include at least the functionality of built-infeatures 410 discussed above. In response, built-infeatures 540 returns a built-in session key BI1 via communication 562. Built-in session key BI1 is included with subsequent communications betweenlearning model 530 and built-infeatures 540 to permit addressing the subsequent built-in related communications to the correct learning model. -
FIG. 5B continues withdialer application 520 sending communication 564 to learningmodel 530 with a Push( ) function to informlearning model 530 that a call is being made to a called party “Grandma.” Additionally, built-infeatures 540 sends communication 566, with a Push( ) function to informlearning model 530 that the microphone volume is being set to a setting of 80. -
FIG. 5B shows a number of communications 568-586 for five subsequent calls made with the calling parties indicated bydialer application 520 and microphone volume indicated by built-infeatures 540 to learningmodel 530 with the values listed in Table 3 below. -
TABLE 3 Communications Called Party Microphone Volume 568 and 570 Boss 40 572 and 574 Spouse 40 576 and 578 Boss 40 580 and 582 Grandma 82 584 and 5 86 Grandpa 82 -
Scenario 500 continues withdialer application 520 sending communications 588 and 590 a to learningmodel 530. Communication 588 uses the Push( ) function to informlearning model 530 that a call is being made to a called party “Boss.” Communication 590 a requests, via the Pull( ) function, a predicted value for the MIC_VOL (microphone volume) setting. - In response to communication 590 a,
learning model 530 predicts a microphone volume setting of 37 based on data (a) stored in the DialerModel variable and loaded as a consequence of communication 556 and (b) learned from communications 564-586, particularly communications 568, 570, 576, and 578 which relate to calls to a called party of “Boss.” Subsequently, learningmodel 530 can send a Pull Response (PullResp) as communication 590 b todialer application 520 providing the predicted microphone volume setting of 37. In some embodiments, the Pull Response can be a return value of the Pull( ) function of communication 590 a. In response to communication 590 b,dialer application 520 instructsmicrophone application 550 to set the microphone volume to 37 via a SetMicVol( ) function communicated incommunication 590 c. - In some scenarios, learning
model 530 can communicate to an application, such asdialer application 520, once the learning model has been trained and is ready to perform machine-learning operations, such as predicting microphone volume levels. In other scenarios, learningmodel 530 is not trained sufficiently to provide a prediction in response to communication 590 a. In these scenarios, learningmodel 530 can inform the application that the learning model is insufficiently trained to perform machine-learning operations, such as predicting microphone volume settings. In still other scenarios,microphone application 550 can provide a response, such as a function return value or communication (e.g., “SetMicVolOK” or “SetMicVolFail”) to the SetMicVol( ) command. -
FIG. 5B showsscenario 500 continuing bydialer application 520 sending communications 592 and 594 a to learningmodel 530. Communication 592 informs learningmodel 530 that a call is being made to a called party “Grandpa” and communication 594 a requests a predicted value for the microphone volume setting. In response, learningmodel 530 predicts a microphone volume setting of 81 based on data stored in the DialerModel variable and learned from communications 564-586, particularly communications 584 and 586 which relate to a call to a called party of “Grandpa.” Subsequently, learningmodel 530 can send a Pull Response as communication 594 b to provide the predicted microphone volume setting of 81 todialer application 520. In response to communication 594 b,dialer application 520 can instruct microphone application viacommunication 594 c to set the microphone volume to 81. -
Scenario 500 concludes withdialer application 520 call Save( ) function viacommunication 596 to request that learningmodel 530 save a context, and perhaps other data, of a learning session thread in the DialerModel variable. In some scenarios not shown inFIG. 5B , learningmodel 530 can provide a response tocommunication 596, such as a return value to the Save( ) function and/or a communication providing a status of the Save( ) function; e.g., SaveOK or SaveFail. -
FIG. 6A showsexample user interface 610 used inscenario 600, in accordance with an example embodiment.Scenario 600 includesuser interface 610 executing onmobile platform 602. As shown inFIG. 6A ,user interface 610 includesmicrophone dialog 622 as an interface formicrophone application 620.Microphone dialog 622 includes manual microphone setting 624, smart microphone setting 626, an OK button to save current settings before exitingdialog 622, and a Cancel button to exitdialog 622 without saving current settings. - Manual microphone setting 624 includes a slider bar that enables a user to set a microphone output setting to between a
minimum setting 0, which effectively mutes the microphone, and a maximum setting of 100, which provides the maximum output volume (a.k.a. maximum gain) for a given input.FIG. 6A shows manual microphone setting 624 set to 48, or approximately halfway between the minimum and maximum settings. - As shown in
FIG. 6A , smart microphone setting 626 can be set to either enabled or disabled and used to learn and set microphone volume based on called party. When smart microphone setting 626 is disabled,microphone application 620 can use manual microphone setting 624 to determine an output volume for a microphone ofmobile platform 602. When smart microphone setting 626 is enabled,microphone application 620 can use a learning service of machine learning andadaptation service 220 to perform a machine-learning operation to provide setting values. Upon receiving a setting value,microphone application 620 can determine output volume for the microphone ofmobile platform 602. Inscenario 600, and as shown inFIG. 6A , smart microphone setting 626 is set to enabled. -
FIG. 6B shows example communications forscenario 600, in accordance with an example embodiment.Scenario 600 has similar communications to those ofscenario 500 shown inFIG. 5B , but there are two substantive differences. Inscenario 500,dialer application 520 has access to calling party information. In contrast,microphone application 620 does not have access to calling party information duringscenario 600; thus, calling party information is hidden frommicrophone application 620 by learningmodel 630. The second substantive difference is that inscenario 500, learningmodel 530 was trained in part using data stored in a DialerModel variable. Inscenario 600, no stored model data is loaded intolearning model 630. - In
FIG. 6B , communications inscenario 600 begin withmicrophone application 620 calling the getService( ) function of learningmodel 630 viacommunication 652. In response, learningmodel 630 provides a session key S2 viacommunication 654 tomicrophone application 620. Session key S2 is included for subsequent communications betweenmicrophone application 620 andlearning model 630 to permit addressing the subsequent communications to the correct learning model, e.g., learningmodel 630, and the correct application, e.g.,microphone application 620, for a learning session keyed by S2. -
Microphone application 620 can instructlearning model 630, viacommunication 656, to set up a prediction interface. As shown inFIG. 6B ,communication 658 includes a PredictionInterface( ) call with three parameters: session key 51, an input-source reference MicVol, and a requested output prediction of microphone volumes, as shown inFIG. 6B , using the predefined MIC_VOL value. The requested output prediction of microphone volumes is based on called parties, as indicated by the fourth parameter CALLED_PARTY to the PredictionInterface( ) function. As with the MIC_VOL value, the value of CALLED_PARTY can be predefined. In other scenarios not shown in the Figures, the output prediction can be based on multiple values as requested via the PredictionInterface( ) function. - In response, learning
model 630 sendscommunication 658 to built-in features (BIFs) 640 to request built-in feature (ReqBI) of called party values, as shown inFIG. 6B by passing the predefined CALLED_PARTY value to built-infeatures 640. Built-infeatures 640 can include at least the functionality of built-infeatures 410 discussed above. In response, built-infeatures 640 returns a built-in session key BI2 to learningmodel 630 viacommunication 660. Built-in session key BI2 is included with subsequent communications betweenlearning model 630 and built-infeatures 640 to permit addressing the subsequent built-in-feature related communications to the correct learning model. -
FIG. 6B continues withmicrophone application 620 sending communication 662 to learningmodel 630, via the Push( ) function, to indicate that the microphone volume is being set to a setting of 80. Additionally, built-infeatures 640 can send communication 664, via the Push( ) function, to learningmodel 630 to indicate that a call is being made to a called party “Grandma.” -
FIG. 6B shows a number of communications 666-678 for five subsequent calls made with the calling parties indicated by built-infeatures 640 and microphone volume indicated bymicrophone application 620 to learningmodel 630 with the values listed in Table 4 below. -
TABLE 4 Communications Called Party Microphone Volume 666 and 668 Boss 40 666 and 670 Spouse 40 666 and 672 Boss 40 674 and 676 Grandma 82 674 and 678 Grandpa 82 -
Scenario 600 continues with learningmodel 630 receiving communication 680 from built-infeatures 640. Communication 680 informs learningmodel 630 that a call is being made to a called party “Boss.” - In response to communication 680, learning
model 630 predicts a microphone volume setting of 40 based on data learned from communications 662-678, particularly communications 666, 668, and 672 which relate to calls to a called party of “Boss.” Subsequently, learningmodel 630 can sendcommunication 682 including the Notify( ) function to indicate the predicted microphone volume setting of 40 tomicrophone application 620. In response tocommunication 682,microphone application 620 can perform function 684 to set the microphone volume to 40. -
Scenario 600 continues with learningmodel 630 receiving communication 686 from built-infeatures 640, which informs learningmodel 630 that a call is being made to a called party “Grandpa.” In response to communication 686, learningmodel 630 predicts a microphone volume setting of 82 based on data learned from communications 662-678, particularly communications 674 and 678 which relate to a call to a called party of “Grandpa.” Subsequently, learningmodel 630 can sendcommunication 688 including the Notify( ) function to indicate the predicted microphone volume setting of 82 tomicrophone application 620. In response tocommunication 688,microphone application 620 can perform function 690 to set the microphone volume to 40. - Example Communication Action Prediction Service
-
FIG. 7 is a flow chart ofmethod 700, in accordance with an example embodiment. In some embodiments, part or all ofmethod 700 can be executed using one or more mobile platforms; e.g.,mobile platform computing device 2200. -
Method 700 begins atblock 710, where a context-identification system executing on a mobile platform can receive data. The received data can include context-related data associated with the mobile platform and application-related data received from the mobile platform. In some instances, the context-related data and the application-related data can differ. - In some embodiments, context-related data can be received from a network. Further, in some instances, the network can include a server, a cloud-based server, and/or a database with context-related data stored dynamically, among other possibilities to provide context-related data. Further, in some instances a mobile platform can connect to such a network to receive context-related data continuously and/or periodically, possibly by executing a network connection application. Yet further, in some instances, context-related data can be received by a network with multiple programmable devices sitting on different nodes of the network, possibly as described with respect to
FIG. 21 . In such instances, context-related data can be received through the network from multiple other programmable devices communicating with the network. - In addition, in some embodiments, the context-related data can be received and/or generated by a context-identification system (CIS) executing on an exemplary mobile platform. In particular embodiments, the CIS can be configured to extract context-related data from information streaming to the mobile platform, possibly from a network as described above. In practice, the CIS may be initiated by a context-related application executed on the mobile platform. An exemplary context-identification system is further described with respect to
FIG. 9A . - In some embodiments, a context-related application may be executed to cause the CIS to stream context-related data associated with mobile platform and identify a context. For example, the mobile platform may be turned off before a flight to Chicago and then turned back on after the flight arriving in Chicago. After turning on the mobile platform, the context-related application may cause the CIS to receive and/or generate context-related data. Further certain context signals associated with the context-related data may be extracted such as: a) the location of the mobile platform (i.e. Chicago O'Hare International Airport), b) the time zone based on the location of the mobile platform (i.e., Central Time zone), and/or c) the weather of the area near the mobile platform (e.g., rainy at 58° F.), among other possibilities. In some instances, such context signals may be used to determine that the user has just arrived at Chicago O'Hare International airport via an airplane flight.
- Yet further, in some embodiments, context-related data can be directly received and/or generated by sensors associated with the mobile platform and/or components provided inside the mobile platform. In some instances, sensors can be external to the mobile platform and capable of receiving and/or generating context-related data. By way of example and without limitation, sensors could be any one or more of a motion detector (e.g., a gyroscope, an accelerometer, and/or a shock sensor), a camera, a proximity sensor, an impact sensor, a contact sensor (e.g., capacitive sensing device), a location determination device (e.g., a GPS device), a magnetometer, and/or an orientation sensor (e.g., a theodolite), among other possibilities.
- In some embodiments, application-related data can include data related to an exemplary mobile platform. For example, application-related data can include data associated with operating the mobile platform. In some instances, application-related data can include a “dialing indication” for opening a phone dialing application and/or a “received call indication” indicating a new call has been received at the mobile platform. Further, a user can enter one or more digits of a phone number to be dialed using the phone dialing application (e.g., entering the digit “5” for the number “555-555-5555”). In some instances, received call indications can include digits of a phone number from another phone calling the mobile platform and/or a phone number from another phone that has called the mobile platform in the past. In addition, an indication can include a digit of the mobile platform's phone number. Further, in some instances, a user can vocalize a command while using voice recognition to dial the phone number (e.g., verbally reciting “call my wife” in the vicinity of the mobile platform).
- In some embodiments, application-related data can include a “messaging indication” for opening a messaging application, a short message service (SMS) text messaging application for example. Further, a user may enter one or more text characters of a message to be sent using the messaging application (e.g., entering “h” or “hey” for “hey, you want to play golf today?”). The messaging application may receive a messaging indication for an incoming text message with incoming text (e.g., “Golf sounds great—which course?”). Yet further, a user can vocalize a command and/or part of a command while using voice recognition to send a message using the phone messaging application (e.g., verbally reciting part of “hey, you want to play golf today?”) In some instances, a “contact indication” for opening a contact may be provided through a user interface of the mobile platform. In other instances, an “application indication” for executing an application on the mobile platform may be provided through the user interface of the mobile platform.
- Further, in some embodiments application-related data may include how an application is executed, opened, and/or interacted with. For example, in some instances, an application may be opened quickly, in parallel with other applications (e.g., a phone dialing application), and in some instances, an application may be opened sequentially after performing other operations (e.g., opening other applications) with the mobile platform. Yet further, in some instances, an application may be executed remotely from a mobile platform, perhaps using a laptop and/or a tablet computer. Other examples are possible as well.
- At
block 720, the context-identification system may identify at least one context using the context-related data associated with the mobile platform and/or the application-related data received from the mobile platform. - Generally, identifying a context may be identified based on context-related data. Further, in some embodiments, context-related data may include one or more “context signals.” Accordingly, an exemplary mobile platform may be configured to identify a context by determining various context signals and/or acquiring context signals from other sources, such as the external sensors and/or networks, perhaps as mentioned above.
- A context signal may be any signal that provides a measurement of or otherwise provides information pertaining to the state or the environment associated with a certain subject (e.g., with a certain person, device, event, etc.). Many types of information from many different sources may be used to establish context signals and/or provide information from which context signals may be determined. In some instances a context-identification system (CIS) may be used to receive and/or generate one or more context signals.
- By way of example and without limitation, context signals may include: (a) the current time, (b) the current date, (c) the current day of the week, (d) the current month, (e) the current season, (f) a time of a future event or future user-context, (g) a date of a future event or future user-based context, (h) a day of the week of a future event or future context, (i) a month of a future event or future user-context, (j) a season of a future event or future user-based context, (k) a time of a past event or past user-based context, (l) a date of a past event or past user-based context, (m) a day of the week of a past event or past user-based context, (n) a month of a past event or past user-based context, (o) a season of a past event or past user-based context, ambient temperature near the user (or near a monitoring device associated with a user), (p) a current, future, and/or past weather forecast at or near a current location, possibly based on the location of the mobile platform, (q) a current, future, and/or past weather forecast at or near a location of a planned event in which a user and/or a user's friends plan to participate, (r) a current, future, and/or past weather forecast at or near a location of a previous event in which a user and/or a user's friends participated, (s) information on user's calendar, such as information regarding events or statuses of a user or a user's friends, (t) information accessible via a user's social networking account, such as information relating a user's status, statuses of a user's friends in a social network group, a user's relationship with the user's friends, and/or communications between the user and the user's friends, (u) noise level or any recognizable sounds detected by the mobile platform and/or a monitoring device (v) items that are currently detected by the mobile platform and/or a monitoring device, (w) items that have been detected in the past by the monitoring device, (x) items that other devices associated with a monitoring device (e.g., a “trusted” monitoring device) are currently monitoring or have monitored in the past, (y) information derived from cross-referencing any two or more of: information on a user's calendar, information available via a user's social networking account, and/or other context signals or sources of context information, (z) health statistics or characterizations of a user's current health (e.g., whether a user has a fever or whether a user just woke up from being asleep), and (aa) a user's recent context as determined from sensors on or near the user and/or other sources of context information (e.g., whether the user is walking, running, and/or jogging, among other possibilities), (bb) a current location of the user and/or the mobile platform, (cc) a past location of the user and/or the mobile platform, and (dd) a future location of the user and/or the mobile platform. Those skilled in the art will understand that the above list of possible context signals and/or sources of context information is not intended to be limiting, and that other context signals and/or sources of context information are possible in addition, or in the alternative, to those listed above.
- Some context signals may take the form of discrete measurements. For example, a temperature measurement or a current GPS location may be received as a context signal. On the other hand, context signals may also be received over time, or may even be a continuous signal stream. For instance, an exemplary mobile platform may use the current volume of a continuous audio feed from an ambient microphone as one context signal, and the volume of a continuous audio feed from a directional microphone as another context signal.
- In some embodiments, a context may be identified by extracting and/or interpreting context signals from other types of data (e.g., weather forecast data, satellite data, GPS coordinates, etc.). Further, in some embodiments, a machine-learning operation may cluster the one or more context signals by grouping one or more context signals. In addition, a machine-learning operation may categorize or classify the context by assigning a label to one or more context-signals, possibly after grouping them. For example, one or more context signals may take the form of data indicative of the environment or state information. As such, context signals may be categorized or classified as “at home,” “at work,” “outside,” “in a car,” “outdoors,” “indoors,” “inside,” “outside,” “free,” and/or “in a meeting,” among other possibilities.
- Furthermore, a context may be a qualitative or quantitative indication that is based on one or more context signals. In addition, a context may be identified by extracting specific context signals from an aggregation of context signals. For example, context signals may indicate a change in time to 6:30 AM on a weekday (possibly setting off an alarm set by a mobile platform). The user may be located at their home and such information may be used to categorize or classify contexts such that the user went from “sleeping” to “awake.” In some instances, a context may simply be reflected in a context-related database as “getting ready for work.”
- In some embodiments, a context may be identified by a mobile platform. For example, a mobile platform may determine, based on GPS location signals, that it has changed its location from one city to another different city. As such, the mobile platform may determine the context to be changing locations from “in Los Angeles” to “in Chicago.” Many other examples are also possible. In yet another example, a context may include data indicating changes to the environment or state information such as moving from “home” to “at work,” from “outside” to “in a car,” from “outdoors” to “indoors,” from “inside” to “outside,” and/or from “free” to “in a meeting,” among other possibilities. Further, in some instances, a context may indicate “going to work,” “getting in the car,” “going inside,” “going outside,” and/or “going to a meeting,” among other possibilities.
- Further, in addition to and/or alternatively, the context-identification system may identify a context based on application-related data. For example, a user operating an exemplary mobile platform may provide an indication of a current context. In particular, after receiving a text message via the mobile platform's text messaging application, the user may respond in a text message reciting, “Can't talk, in a meeting.” In a further example, the user may update their status on a social networking site by stating, “I am so excited about this all-day meeting.” In addition, the user may have accepted a meeting invite to the all-day meeting set in the user's calendar such that the context-identification can identify the context of “attending a meeting.”
- Yet further, a context may be identified by a user indication. For example, a dialing indication may be generated by opening up a phone dialing application on the mobile platform. In some instances, a dialing indication can be received by the phone dialing application, e.g., a dialing indication for an incoming phone call. In such instances, the dialing indication may be used to identify a context for “calling co-worker.” Further, entering the first digit of the co-worker's phone number may further provide an indication for the “calling co-worker” context.
- Yet additionally, a messaging indication may be generated by opening up the messaging application on the mobile platform. In some instances, a messaging indication can be received by the messaging application, e.g., a messaging indication for an incoming text message. In such instances, the messaging indication may be used to identify a context for “messaging co-worker.” Further, selecting and/or opening up the messaging co-worker's contact may further provide an indication for such contexts directed to communicating with the messaging co-worker. Other examples are possible as well.
- At
block 730, based on at least one context identified, the context-identification system can predict at least one communicative action associated with the mobile platform by performing a machine-learning operation on the received data. - In some embodiments, the user may manually provide the context by entering the text “going to office” in a context field of the mobile platform's user interface. In addition, as noted, a context may be classified by assigning a label to one or more context-signals. In some embodiments, a machine-learning operation may be performed to assign such labels. For example, the mobile platform may determine the context from “subway” to “office” when the user leaves the subway station and arrives to their office.
- Further, the context identified may provide an indication of a communicative action. By way of example and without limitation, a “communicative action” may include any of the following: dialing a phone number using the mobile platform, ending a current call using the mobile platform, sending a message using the mobile platform, opening a contact on the mobile platform, executing an application on the mobile platform, updating a status on social networking site, and/or sending communication via email, among other possibilities.
- In some embodiments, identifying a context may help predict at least one communicative action. For instance, an exemplary mobile platform may store context signals associated with “leaving for work,” “driving to work,” “driving home from work,” and “arriving home.” Further, the mobile platform may track previously-called phone numbers associated with such contexts, possibly storing the previously-called phone numbers with their respective contexts in a context-related database. For example, if the mobile platform identifies the “leaving for work” context, the mobile platform may display (e.g., on its user interface) a phone-number for the “leaving for work” context taken from a list of all previously-stored phone numbers called associated with the “leaving for work” context. Other possibilities may also exist.
- In some embodiments, there may be more than one phone number associated with a given context identified. In such instances, a machine-learning operation may be performed to rank the phone numbers associated with the given context. For example, the mobile platform may have stored three phone numbers associated with the “driving home from work” context: 1) a son's phone number, 2) a daughter's phone number, and 3) a wife's phone number. In some instances, the mobile platform may be able to determine its velocity, possibly consistent with the movement of the user's car. For example, the mobile platform may be able to detect if it is moving toward the son's location, the daughter's location, and/or the wife's location.
- Based on the location and/or velocity of the mobile platform, the mobile platform may change the rankings of the three phone numbers stored. In this example, the mobile platform's location and velocity may indicate the mobile platform is traveling toward the son's location. Given the mobile platform's current direction toward the son's location and possibly as the mobile phone's location gets closer to the son's location, the mobile platform may suggest the user call the son, perhaps by displaying the son's phone number as a “Suggested Call” and/or “Suggested Number” on the phone dialing application's user interface.
- Further, identifying changes in context may allow a mobile platform to intelligently predict communicative actions (e.g., a phone number to dial). In some instances, the mobile platform may determine that a current context matches a context that corresponds to a previously-dialed phone number. In such instances, the mobile platform may suggest the user to call the previously-dialed phone number. This aspect is discussed in greater detail below with reference to
FIG. 8B . Other examples are possible as well. - At
block 740, an instruction can be received to execute at least one communicative action associated with the mobile platform. - In some embodiments, the instruction may be provided from a user of the mobile platform. For example, the mobile platform may provide a prompt on its user interface with a suggested number to call. In some instances, the suggested number to call may be provided based on a prediction using the machine-learning operation. Further, the user may use an instruction input on the mobile platform's user interface to initiate the call to the suggested number.
- Further, in another example, the mobile platform may identify the “leaving work and going home” context as the user is driving their car from the workplace after a day at work. In such instances, the mobile platform may prompt the user with a computerized voice communication stating, “Would you like to text your wife that you are ‘leaving work and going home’?” Utilizing voice recognition capabilities, the user may respond, “Yes,” and a text message may be sent to the user's wife indicating that the user is “leaving work and going home.” As such, the user is able to send text messages without making physical contact with the phone and/or making visual contact with the phone, possibly to refrain from distracting the user from driving.
- In some embodiments, the instruction may be provided from the context-identification system (CIS). For example, the CIS may identify a context associated with “arriving to the office” and provide an instruction to dial into a conference call as done regularly upon identifying the “arriving to the office” context. In some instances, previously-stored data may indicate a pre-determined instruction to dial into the conference call upon identifying the “arriving to the office” context. In other scenarios, pre-determined instructions can enable the CIS to automatically originate other phone calls, generate and send text messages, and/or other communicative actions, Many other examples are possible as well.
-
FIG. 8A shows anexample user interface 804 used inscenario 800, in accordance with an example embodiment.Scenario 800 includesuser interface 804 executing onmobile platform 802.User interface 804 includes an interface forcontext identification 806. Further,user interface 804 includes an interface forsuggested contact 808 to be viewed, possibly based oncontext identification 806. In addition,user interface 804 includes an interface for suggestedphone number 810 to be dialed, also possibly based oncontext identification 806. Further,user interface 804 includesinstruction input 812. - Further,
FIG. 8A showscontext identification 806, perhaps based on receiving context related-data. For example,mobile platform 802 may receive multiple context signals such as the date, time, temperature, and/or weather, as shown by context-relateddata 814. -
Context identification 806 may illustrate the identified context, “Mar. 12, 2012 8:05 AM Chicago O'Hare Airport Arrival,” by accessing a pre-stored entry in the user's calendar indicating when the user is scheduled to land in Chicago. Yet, in some instances,mobile platform 802 may detect a recent operation for turning on the power ofmobile platform 802 and use a GPS locator to detect the location ofmobile platform 802. In such instances,mobile platform 802 may make a prediction that the user has arrived at Chicago O'Hare Airport, perhaps after a flight. In other instances,mobile platform 802 may access the user's itinerary stored inmobile platform 802's memory to identify that the user has arrived at Chicago O'Hare Airport after a flight. Other possibilities may exist. -
FIG. 8A showscontext identification 806 with a “Change” button to change and/or correct the context identified, a “Save” button to save the context identified, and a “Dismiss” button to dismiss the context identified without saving the context identified. In some instances, the “Change” button may allow the user to correct the context identified to provide further context-related data, possibly to predict communicative actions. In some instances, the “Save” button may be used for identifying contexts in the future, and perhaps storing the context signals associated with the context identified. - In addition,
FIG. 8A shows suggestedcontact 808 as “Limo Driver.” In such instances, the user may press the “Limo Driver” button to see further information regarding the Limo Driver and possibly view other options for text messaging the Limo Driver. In some embodiments, multiple suggestions can be provided, and the user can press the circle enclosing a downward-facing triangle within suggestedcontact 808 to view additional suggestions. For example, pressing the circle may provide a list of other contacts, possibly including local taxi services, rental car services, airport bus services, and/or other contacts that the user may want view and/or call based on the context identified. Other possibilities may also exist. -
FIG. 8A shows suggestedphone number 810 as “555-555-5555” as the phone number to call the Limo Driver. In some examples, the user may provide an instruction throughinstruction input 812.Instruction input 812 includes a Wait button to possibly delay the phone call to the Limo Driver such that the call may be made later upon a timed prompt (not shown inFIG. 8A ) for suggestedphone number 810.Instruction input 812 also provides a Dial button to call the Limo Driver immediately and a Cancel button to exituser interface 804 without calling the Limo Driver. -
FIG. 8B shows an example airportarrival data record 820 based on received context-relateddata 822. It should be understood that airportarrival data record 820 may correspond toscenario 800 inFIG. 8A . Airportarrival data record 820 may be stored inmobile platform 802, perhaps inmobile platform 802's memory. Further, airportarrival data record 820 may be stored on a server, a network, cloud-based system, and/or other server system configured to store data records with context-related data, among other possibilities. In some embodiments, data records with context-related data may be updated by multiple mobile platforms and/or computing devices, possibly if authorized by a user ofmobile platform 802. - In some embodiments, airport
arrival data record 820 may include context-relateddata 822 corresponding to the date, time, temperature, and/or weather, as shown by context-relateddata 814 inFIG. 8A . Further, inFIG. 8A , consider the scenario such that the “save” button is pressed incontext identification 806. Referring back toFIG. 8B , context-relateddata 822 may be updated to include additional data corresponding to a recent operation for turning on the power ofmobile platform 802 and the GPS location data ofmobile platform 802 indicative of the arrival at Chicago O'Hare Airport after a flight. - As part of airport
arrival data record 820,FIG. 8B further illustrateslimo driver data 824 with its correspondingphone number data 826,contact data 828, and context-relateddata 830. In addition, user's son data 832 may include its correspondingphone number data 834,contact data 836, and context-relateddata 838. Further, user's daughter data 840 may include its correspondingphone number data 842,contact data 844, and context-relateddata 846. Yet further, user's wife data 848 may include its correspondingphone number data 850,contact data 852, and context-relateddata 854. - In some embodiments, context-related
data arrival data record 820 to predict a communicative action by a user ofmobile platform 802. As such, context-related data may includemobile platform 802's call history (e.g., contacts called most frequently), information corresponding to the user's relationships with other contacts (e.g., people that the user has spoken to the most over a given period in time), and/or browsing history of an internet browser application indicative of the user's interests. Other possibilities may also exist. - In
FIG. 8B ,phone number data phone number data phone number data 826 may correspond to the limo driver's phone number, 555-555-5555, shown as suggestedphone number 810 inFIG. 8A . Further,contact data - Further, in
FIG. 8B , context-relateddata mobile platform 802 and/or when each person has calledmobile platform 802. For example, context-relateddata 830 may correspond to context signals indicating an operation for turning on the power ofmobile platform 802 and the GPS location data ofmobile platform 802 indicative of an arrival at Chicago O'Hare Airport after a flight. Context-relateddata data mobile platform 802 indicative of an after-flight arrival. For example, it may be possible that the son, the daughter, and the wife were previously called just prior to the user's departure from Chicago O'Hare Airport. - A current context identified with
mobile platform 802 may closely match context-relateddata 822. Further, context-relateddata 822 may match most closely with context-relateddata 830 as compared to context-relateddata limo driver data 824 may be ranked higher than user's son data 832, user's daughter data 840, and user's wife data 848, indicating that the user is more likely to call the Limo Driver. Thus, a prediction can be made that the user will want to contact the Limo Driver usingphone number data 826. - In some embodiments,
contact data contact data 828 may indicate that the limo service does not open until 9:00 AM on Mar. 12, 2012. Thus, although context-relateddata 822 may match most closely with context-relateddata 830,limo driver data 824 may fall in the rankings since the limo driver is probably not available. In such instances, user's son data 832 may be next in line to prompt the user with a suggested phone number corresponding tophone number data 834. - In some embodiments, application-related data corresponding to the user's indications on the mobile platform may adjust the weighting factor. For example, as noted in
FIG. 8A ,instruction input 812 provides a Cancel button to not call the Limo Driver at all. Provided that the user presses the Cancel button, a weighting factor may be applied to lowerlimo driver data 824 in the rankings, perhaps eliminatinglimo driver data 824 from the rankings and/or eliminatinglimo driver data 824 from airportarrival data record 820. Other examples are possible as well. -
FIG. 9A shows example context identification system (CIS) 900, in accordance with an example embodiment.CIS 900 receives context signals 910. In some instances, context signals 910 can be stored as a data string in a mobile platform, a context-related database, a cloud, and/or a network, among other possibilities. - Context identification 912 of CIS 900 can receive context signals 910, such as, but not limited to (a) a current time, (b) a current date, (c) a current day of the week, (d) a current month, (e) a current season, (f) a time of a future event or future context, (g) a date of a future event or future context, (h) a day of the week of a future event or future context, (i) a month of a future event or future user-context, (j) a season of a future event or future context, (k) a time of a past event or past context, (l) a date of a past event or past context, (m) a day of the week of a past event or past context, (n) a month of a past event or past context, (o) a season of a past event or past context, (p) ambient temperature, (q) a current, future, or past weather forecast at a current location, (r) a current, future, or past weather forecast at a location of a planned event, (s) a current, future, or past weather forecast at or near a location of a previous event, (t) information on a calendar associated with a user-profile, (u) information accessible via a user's social networking account, (v) noise level or any recognizable sounds detected by a device, (w) devices that are currently available to communicate with the mobile platform, (x) devices in proximity to the mobile platform, (y) devices that are available to receive instruction from the mobile platform, (z) information derived from cross-referencing at least one of: information on the user's calendar, information sent to the user, and/or information available via the user's social networking account, (aa) health statistics or characterizations of the user's current health, (bb) a user's recent context as determined from sensors on or near the user and/or other sources of context information associated with the mobile platform, (cc) a current location of the mobile platform, (dd) a past location of the mobile platform, and (ee) a future location of the mobile platform.
- For some or all of the context signals received,
context identification 912 can copy and/or extract data from context signals 910 to identify one or more contexts. The context identified can be output fromcontext identification 912 as a vector. -
Classifier function 914 ofCIS 900 can receive the context vector as input, classify contexts into a CC context classification, CC>0, and output classification indications. For an example with CC=3, a context may be classified as “work,” “home,” and “travel” based on an activity, a state of the user, an arrival, a departure, context signal(s), etc. Many other classifications are possible as well. - A classification indication can include the classification of a context, possibly using a predetermined classification scheme. For example, suppose
classifier function 914 determines a recurring context indicative of the user going on their lunch break at noon. In this example,classifier function 914 can output a classification indication assigning a label “going out to lunch.” Many other possible classification indications are possible as well. -
Recognition function 916 can receive the classification indications and recognize certain functions. For example, upon receiving the classification indication of the mobile platform “moving towards the pizza parlor,”recognition function 916 can recognize previous phone calls made in the context of “moving towards the pizza parlor.” For example, in some prior instances, perhaps the user made a call to the pizza parlor to make reservations. Then,recognition function 916 can recognize these previous phone calls and generate a predicted communication (e.g., phone call, e-mail or text message) to the pizza parlor. -
Recognition function 916 can be performed using software and data solely resident and executing on a mobile platform, while in other embodiments,recognition function 916 can be performed using software and data both resident and executing on the mobile platform and on other computing devices, such as one or more servers configured to recognize contexts. - Application-related
data 918 can include data generated by and/or related to an application executed on the mobile platform. Examples of application-related data 818 can include, but are not limited to interacting with applications, initiating programs on the mobile platform, and/or executing communicative actions such as dialing phone numbers, among other possibilities. Additional examples of application-related data are discussed above in the context ofFIG. 7 , among other figures as well. Further, application-relateddata 918 can differ from context-related data, context vector(s) extracted bycontext identification 912, classification indications fromclassifier function 914, and recognized contexts fromrecognition function 916. - In some instances, user model 920 can take recognized contexts from
recognition function 916 and application-relateddata 918 to predict and/or suggest communicative actions, such as possible phone numbers to dial. For example, suppose that the mobile platform recognizes a context such that the user is on their lunch break and is at their office continuing to work. Further, application-related data may indicate that the user is initiating a phone dialing application and/or a restaurant browsing application. The mobile phone may predict that the user may want to order food and provide a phone number to a local restaurant, possibly a previously-called restaurant for making such a food order. -
FIG. 9B shows an example collective context identification system (CCIS) 902, in accordance with an example embodiment.CCIS 902 can be configured to receive context-related data and application-related data from a number N of mobile platforms. In some embodiments, each of the N mobile platforms can be configured to determine whether or not to provide some or all context-related data and/or application-related data toCCIS 902. In other embodiments, some or all of the context-related data and/or application-related data usedCCIS 902 can be anonymously used. - As shown in
FIG. 9B , context-related data, including context signals 922 a, 922 b, 922 c and recognizedcontexts data 918 a-918 c can be provided tocollective learner 928.Collective model 930 can take recognized contexts provided bycollective learner 928 to suggest communicative actions to execute, such as possible phone numbers to dial and/or addresses for text messages and/or emails to be sent. -
FIG. 9C shows an example feedback-adaptable context identification system (FACIS) 904, in accordance with an example embodiment.FACIS 904 takes application-relateddata 918 and context-relateddata 922 as inputs, and provides the inputs to both user model 920 andcollective model 930, which respectively generate user-specific communication addresses 940 and general communication addresses 942. For example, user-specific communication addresses 940 may include phone numbers, e-mail addresses, Instant Messaging (IM) addresses, IP addresses, Uniform Resource Locators (URL)s, and/or other communication addresses to restaurants previously contacted by a user whereas general communication addresses 942 may include phone numbers, e-mail addresses, Instant Messaging (IM) addresses, IP addresses, Uniform Resource Locators (URL)s, and/or other communication addresses to common restaurants, and perhaps other establishments, in proximity to the mobile platform's current location. User-specific communication addresses 940 and general communication addresses 942 can be provided to communication address suggestion user interface (UI) 944 for presentation and possible selection as a communication address for contacting one or more entities. - For example, a user using communication
address suggestion UI 944 can view one or more suggested user-specific communication addresses 940, and perhaps provide additional feedback, such as dismissing the suggestion (i.e. refusing to use the suggested communication address). Information about the suggested communication address and any additional feedback can be respectively provided asfeedback collective model 930. Based on this feedback, user model 920 andcollective model 930 can adapt to the feedback such as by increasing probabilities that the next communication address suggested by the other model will be accepted by the user. Continuing this example, user model 920 can examinefeedback 946, facilitatecollective model 930's prediction for another communication address to be used, and increase the probability that the next communication address provided to communication address suggestion communication address UI) will be accepted going forward. Additionally or instead,collective model 930 can examinefeedback 948, facilitate user model 920's prediction for another communication address to be used, and increase the probability that the next communication address provided to communication addresssuggestion user UI 944 will be accepted going forward. - Example Photo Name/Photo Album Suggestion Service
- Mobile platforms, such as mobile phones, can include one or more cameras to capture images. Frequently, the images are named, perhaps with a brief description of the image. For example, an image named “
Sunset 18 Mar. 2012” can be a picture of a sunset captured on Mar. 18, 2012. Labeling a large number of pictures can take a great deal of time in reviewing the image, determining a title for the image, and then naming the image with the determined title. - An image identification system (IIS), based on the learning models described above, can suggest image and album titles based on image content and other data. The other data can include social signals such as social-networking messages, postings, comments, tags, e-mail and other messages, other persons' social signals, and social status indicators, calendar data, locations, and times. The IIS can learn a collective model that represents the collective data from at least one person and their images. The collective model can be learned, stored, and executed on the mobile platform alone or, in some embodiments, on the mobile platform and one or more
other computing devices 2200, such as servers. - The IIS can also be configured with image feature recognition and detection capabilities, such as facial recognition modules, object recognition modules, landmark recognition models, and modules to recognize features in images such as lines, edges, patches of color, shapes, object areas/volumes, and other features. The IIS can detect features in one image, a sequence of images, and/or a video file or stream for a sequence of one or more images.
- The IIS and/or collective model can include or otherwise be associated with a personalized model, perhaps using data stored solely on the mobile platform. The personalized model can be equipped to adapt to feedback from a user of the mobile platform. For example, for a given image IMG1, the personalized model can suggest three titles: T1, T2, and T3. Then, if the user decides to entitle IMG1 with title T2, the personalized model can increase the probability that title T2 and/or titles similar to title T2 are suggested in the future. In response to selection of title T2 in this example, the personalized model can decrease the probabilities that unselected titles T1 and T3 and/or titles similar to title T2 are suggested in the future
-
FIG. 10 is a flow chart ofmethod 1000, in accordance with an example embodiment. In some embodiments, part or all ofmethod 1000 can be executed using one or more mobile platforms; e.g.,mobile platform computing device 2200. -
Method 1000 begins atblock 1010, where a machine-learning service executing on a mobile platform can receive feature-related data. The feature-related data can include image-related data related to one or more images received from an application executing on the mobile platform and platform-related data received from the mobile platform. The image-related data and the platform-related data can differ. - In some embodiments, the image-related data can be generated by an image-identification system (IIS) executing on the mobile platform. In particular embodiments, the IIS can be configured to extract image-related features from the one or more images. In more particular embodiments, the IIS can be configured to classify at least one image-related feature of the image-related features. In even more particular embodiments, the IIS can be configured to recognize the at least one classified image-related feature and/or object(s) and/or scene(s) that include the at least one classified image-related feature.
- In some of the even more particular embodiments, the at least one classified image-related feature can include a feature related to a face of a person, and the IIS can be configured to recognize the face of the person. In other of the even more particular embodiments, the at least one classified image-related feature can include a feature related to a landmark, and the IIS can be configured to recognize the landmark. In still other of the even more particular embodiments, the at least one classified image-related feature can include a feature related to an object, and the IIS can be configured to recognize the object.
- At
block 1020, the machine-learning service can generate a title related to the one or more images. The title can be generated by the machine-learning service performing a machine-learning operation on the feature-related data. - In some embodiments, the title related to the one or more images can include at least one datum from the image-related data and at least one datum from the platform-related data. In particular embodiments, the at least one datum from the image-related data can include at least one datum selected from among a name of a person, a name of a landmark, and/or a name of an object and the at least one datum from the platform-related data includes at least one datum selected from among a date, a time, a calendar entry, a social-networking message, and a location.
- At
block 1030, the machine-learning service can send the title related to the one or more images from the machine-learning service to the application. -
FIG. 11A shows example image identification system (IIS) 1100, in accordance with an example embodiment.IIS 1100 receives aninput image 1110.Image 1110 can be an electronically-storable image perhaps stored in a well-known image format, e.g., JPG, MPEG, GIF, BMP, etc.Feature extraction function 1112 ofIIS 1100 can detect features fromimage 1110, such as, but not limited to, lines/edges, corners, points of interest, contours, patches of color, regions of interest. For some or all of the detected features,feature extraction function 1112 can copy and/or extract a “local image patch” fromimage 1110 that includes the detected feature. The detected feature(s) and/or corresponding local image patch(es) can be output fromfeature extraction function 1112 as a feature vector. -
Classifier function 1114 ofIIS 1100 can receive the feature vector as input, classify features into one of IC image classifications, IC>0, and output classification indications. For an example with IC=3, the features can be classified as a face, a landmark, or an object. Many other classifications are possible as well. - A classification indication can include the classification of a recognized feature, and an indication of a range of pixels that include the classified feature. For example, suppose
classifier function 1114 determines an image of a face is depicted by a rectangle of pixels whose upper-left-hand corner has pixel coordinates (10, 10), and whose lower-right-hand corner has pixel coordinates (90, 100). In this example,classifier function 1114 can output a classification indication of <face, (10, 10), (90, 100)>, to indicate an image of a face can be located within the rectangle having pixel coordinates from (10, 10) to (90, 100). As another example, a classification indication of <landmark, (1,100), (400, 400)> can indicate an image of a landmark can be found within the rectangle having pixel coordinates from (1, 100) to (400, 400). Many other possible classification indications are possible as well. -
Recognition function 1116 can receive the classification indications and recognize the feature in the classification indication. For example, upon receiving the classification indication <face, (10, 10), (90, 100)>,recognition function 1116 can attempt to recognize the face in the image.Recognition function 1116 can be performed using software and data solely resident and executing on a mobile platform, while in other embodiments,recognition function 1116 can be performed using software and data both resident and executing on the mobile platform and on other computing devices, such as one or more servers configured to recognize image features. - Platform-related
data 1118 can include data generated by and/or related to a mobile platform. Examples of platform-relateddata 1118 include, but are not limited to times, dates, locations, calendar entries, social-networking messages and other messages. Additional examples of platform-related data are discussed above as built-in feature-related data in the context of at leastFIG. 2 . Other examples are possible as well. Feature-relateddata 1118 can differ from image-related data that includesimage 1110, feature vector(s) extracted byfeature extraction function 1112, classification indications fromclassifier function 1114, and recognized features fromrecognition function 1116. - User model 1120 can take recognized features from
recognition function 1116 and platform-relateddata 1118 to generate titles for images and/or collections of images (a.k.a. albums). For example, suppose an image I1 had three recognized features: recognized faces of Alice and Bob and a recognized object of a bottle, and that the platform-related data indicated that I1 was taken at 9 AM on Mar. 18, 2012, at Alice and Bob's home. Platform-relateddata 1118, authorized by Bob to examine his calendar entries, can include a calendar entry of “St. Pats Party 2012” that was held starting at 7 PM on Mar. 17, 2012. Based on this information, user model 1120 can generate example titles for image I1, such as: - “Alice_and_Bob—18Mar2012”
- “Alice_and_Bob_home—18Mar2012”
- “Alice_and_Bob_home_with_bottle—18Mar2012”
- “Alice_and_Bob_home_with_bottle_after_St Pats Party 2012”.
- Similarly, if images I2, I3, and I4 are taken in succession, all of which include images of Alice, Bob, and the bottle, the above-generated example titles can be used for a photo album that includes images I1, I2, I3, and I4. Further, the above-generated example titles could be used for a video clip of Bob, Alice, and the bottle taken at 9 AM on Mar. 18, 2012 that includes images I1, I2, I3, and I4. Images I1-I4 can reside on a mobile platform; e.g., Alice's mobile platform, and/or on a server configured to store data, such as images I1-I4.
-
FIG. 11B shows an example collective image identification system (CIIS) 1102, in accordance with an example embodiment.CIIS 1102 can be configured to receive image-related data and platform-related data from a number N of mobile platforms. In some embodiments, each of the N mobile platforms can be configured to determine whether or not to provide some or all image-related data and/or platform-related data toCIIS 1102. In other embodiments, some or all of the image-related data and/or platform-related data usedCIIS 1102 can be anonymized. - As shown in
FIG. 11B , image-related data, includingimages features data 1118 a-1118 c can be provided tocollective learner 1128.Collective model 1130 can take recognized features provided bycollective learner 1128 to generate titles for images and/or collections of images (a.k.a. albums). -
FIG. 11C shows an example feedback-adaptable image identification system (FAIIS) 1104, in accordance with an example embodiment.FAIIS 1104 takes platform-relateddata 1118 and image-related data 1122, 1124 as inputs, provides the inputs to both user model 1120 andcollective model 1130, which respectively generate user-specific titles 1140 andgeneral titles 1142. Image-related data 1122, 1124 can include images 1122 a-c and/or recognized features 1124 a-c. User-specific titles 1140 andgeneral titles 1142 can be provided to image/album title suggestion user interface (UI) 1144 for presentation and possible selection as an image, video, or album title. - For example, a user using image/album
title suggestion UI 1144 can select one of user-specific titles 1140, and perhaps provide additional feedback, such as a rating of the title. Information about the selected title and any additional feedback can be respectively provided asfeedback collective model 1130. Based on this feedback, user model 1120 andcollective model 1130 can adapt to the feedback such as by increasing probabilities of titles generated by the other model. Continuing this example, user model 1120 can examinefeedback 1146, determine thatcollective model 1130 generated the selected title, and increase the probability that titles similar to the selected title are provided to image/albumtitle suggestion UI 1144 going forward. Additionally or instead,collective model 1130 can examinefeedback 1148, determine that user model 1120 generated the selected title, and increase the probability that titles similar to the selected title are provided to image/albumtitle suggestion UI 1144 going forward. -
FIG. 12A shows an example user interface 1210 used inscenario 1200, in accordance with an example embodiment.Scenario 1200 includes user interface 1210 executing onmobile platform 1202. User interface 1210 includes an interface forcamera application 1220 and photo album/title application 1222. Inscenario 1200,FIG. 12A shows photo album/title application 1222 operating in the foreground and utilizing suggested album/title dialog 1222 a. Suggested album/title dialog 1222 a includesalbum suggestion 1226 andtitle suggestion 1228. -
FIG. 12A shows thatalbum suggestion 1226 includes an album name suggestion of “Chicago_May—2012”, a Suggest button to request another album name suggestion, a Save button to create an album named as suggested, e.g., Chicago_May—2012, an Edit button to modify the album name, and a Cancel button to exit the photo album/title application 1222 without creating an album title.FIG. 12A also showstitle suggestion 1228 with a title suggestion of “AnnJohn_ChicagoTheater—23May 2012.jpg” for a particular image, a Suggest button to request another title suggestion, a Save button to save an image named as suggested, e.g., AnnJohn_ChicagoTheater—23May 2012.jpg, an Edit button to modify the album name, and a Cancel button to exit the photo album/title application 1222 without using the suggested title. - In some embodiments, use of the Edit button to change
album suggestion 1228 and/ortitle suggestion 1228 can generatefeedback 1146 to user model 1120 and/orfeedback 1148 tocollective model 1130.Feedback Feedback collective model 1130 in response to use of the Save button to save an editedalbum suggestion 1226 and/ortitle suggestion 1228. - In
scenario 1200,image 1224 has recently been captured.Image 1224 is an image of two people, Ann and John, pictured beneath the marquee of the landmark Chicago Theater.Image 1224 has been provided to photo album/title application 1222, which in turn used an image identification system, such asIIS 800,CIIS 802, orFAIIS 804, to generate the suggested image title and image album names discussed above and shown inFIG. 12A . - In some embodiments not explicitly shown in the Figures, photo album/
title application 1222 can operate in the background if so authorized; e.g., photo album/title application 1222 operates without displaying suggested album/title dialog 1222 a. Then, when an image is captured, such asimage 1224, album/title application 1222 can provide the captured image to photo album/title application 1222 and generate suggested image title and image album names, perhaps after enough images have been captured and/or named to trainlearning model 1230. Once the suggested image title and image album names are generated, album/title application 1222 can generate a notification, such as a pop-up dialog, that presents the suggested image title and/or image album names. - Timing and/or display of the notifications can depend on user response. For example, album/
title application 1222 can use threshold times to determine “relatively quick” and “relatively slow” response. For example, if a user responds to a notification within a relatively-quick threshold period of time; e.g., within five seconds of notification display, album/title application 1222 can continue to generate notifications as soon as suggested image title and/or image album names are available. As another example, if a user does not respond to a notification within a relatively-slow threshold period of time; e.g., within sixty seconds of notification display, album/title application 1222 can either generate notifications less frequently and/or stop generating notifications in favor of waiting for album/title application 1222 to be returned to the foreground, and then provide suggested image title and/or image album names via suggested album/title dialog 1222 a. Other values for threshold periods of time are possible. -
FIG. 12B shows communications for theexample scenario 1200, in accordance with an example embodiment. Communications shown inFIG. 12B involve photo album/title application 1222 requesting ranked lists of photo titles and photo album names from learningmodel 1230.Learning model 1230 gets image-related data fromIIS 1250 and platform-related data from built-in features (BIFs) 1240. Built-infeatures 1240 can include at least the functionality of built-infeatures 410 discussed above. When photo album/title application 1222 provides an image tolearning model 1230,learning model 1230 generates a list of a photo album names and photo titles. - Photo album/
title application 1222 initiates communications inscenario 1200 by calling the getService( ) function for learningmodel 1230 viacommunication 1252. In response,learning model 1230 provides a session key S3 viacommunication 1254 to photo album/title application 1222. Session key S3 is included for subsequent communications between photo album/title application 1222 andlearning model 1230 to permit addressing the subsequent communications to the correct learning model, e.g.,learning model 1230, and to the correct application, e.g., photo album/title application 1222, for a learning session keyed by S3. - Photo album/
title application 1222 can instructlearning model 1230, viacommunication 1256, to set up a ranking interface. As shown inFIG. 12B ,communication 1256 includes a RankingInterface( ) call with three parameters: session key S3 and requests for ranked lists of photo titles and album names, as shown inFIG. 12B , using the predefined PHOTO_TITLE and PHOTO_ALBUM values. - In response,
learning model 1230 sendscommunication 1258 toIIS 1250 to request identification of image features, such as faces, objects, and landmarks. As shown inFIG. 12B ,learning model 1230 requests identifications using the RankingImageProc( ) function with three parameters: requests to identify faces, objects, and landmarks using the respective pre-defined values of FACES, OBJS, and LANDMARKS. In response,IIS 850 sends an image-identification session key of IP1 viacommunication 1260. -
Scenario 1200 continues withlearning model 1230 sendingcommunication 1262 to built-infeatures 1250 to request retrieval one or more images stored on a mobile platform, as shown inFIG. 12B by passing the predefined STORED_IMAGES parameter in a ReqBI( ) function call to built-infeatures 1240. In response, built-infeatures 1240 returns a built-in session key BI3 viacommunication 1264 to learningmodel 1230. If authorized to provide stored images by the user/owner of the mobile platform, built-in features 1244 can provide an image stored on the mobile platform tolearning model 1230 via communication 1266. Communication 1266 includes a Push( ) function call with three parameters: built-in session key BI3, identification of a stored image using the predefined value STORED_IMAGES, and a reference I1 to the stored image. - Upon receiving the stored image,
learning model 1230 provides the stored image toIIS 1250 via communication 1268 to determine image-related features related to the stored image. Communication 1268 includes a Push( ) function call with three parameters: image-identification session key IP1, identification of a stored image using the predefined value STORED_IMAGES, and a reference I1 to the stored image. -
Scenario 1200 continues withIIS 1250 determining image-related data for the image referenced by I1 and returning the image-related data tolearning model 1230 via communication 1270. Communication 1270 includes a Push( ) function call with five parameters: image-identification session key IP1, a reference I1 to the stored image, image-related data for faces in F1, image-related data for objects in O1, and image-related data for landmarks in L1. -
Scenario 1200 continues with built-infeatures 1240 continuing to provide images stored on the mobile platform tolearning model 1230, which in turn provides each stored image toIIS 1250 to generate image-related data for faces, objects, and landmarks for each stored image. -
Learning model 1230 then requests platform-related data from built-infeatures 1240 viacommunication 1272.Communication 1272 includes a ReqBI( ) function call with five parameters: built-in session key BI3, a predefined value of SOCIAL_MEDIA requesting all authorized social networking messages stored on the mobile platform, a predefined value of NAMES requesting all authorized contact names stored on the mobile platform, a predefined value of EVENTS requesting all authorized calendar entries and/or events stored on the mobile platform, and IMAGE_COMMENTS requesting all authorized comments on the STORED_IMAGES previously requested. In other scenarios, more or less information can be requested by learningmodel 1230 as platform-related data. - As shown in
FIG. 12B , in response tocommunication 1272, built-infeatures 1240 deliver a name referred to as Name1 via communication 1274 to learningmodel 1230. Built-infeatures 1240 can use additional Push( ) function calls to provide additional requested and authorized feature-related data tolearning model 1230. -
Scenario 1230 continues with photo album/title application 1222 providing an image Image1 to learning model via communication 1276. Photo album/title application 1222 then requests a list of three suggested photo album names related to Image1 from learningmodel 1230 via communication 1278. - In response to communication 1278,
learning model 1230 generates a list of three suggested photo album names related to Image1 and provides the requested list as the “Album[3]” parameter of the Pull response shown in communication 1280. -
FIG. 12B shows that photo album/title application 1222 then requests a list of five suggested photo titles related to Image1 from learningmodel 1230 viacommunication 1282. In response tocommunication 1282,learning model 1230 generates a list of five suggested photo titles related to Image1 and provides the requested list as the “Title[5]” parameter of the Pull response shown as part of communication 1284. In the present example,scenario 1200 then concludes. - Some mobile platforms can search to connect with available communication networks. In Wi-Fi systems, the mobile platform can turn on a transmitter to send one or more probe request messages requesting connection information, and/or turn on a receiver to listen for a beacon signal. If a network is available, the mobile platform either receives a probe response signal to a sent probe message, or receives a listened-for beacon signal. The mobile platform can establish a Wi-Fi connection with the network after performing authenticated and associated procedures. If no network is available, the mobile station does not receive a signal; e.g., either a probe request signal or beacon signal, and so can determine that no network is available.
- Thus, to search for a network, the mobile platform has to turn on a radio receiver to listen for signals and, if sending probe messages, perhaps also turn on a radio transmitter to send probe messages. In some embodiments, mobile platforms can continue searching until an available network is found and a Wi-Fi connection is established. This searching procedure can take a relatively large amount of power, thereby reducing power available for other mobile platform tasks, such as voice and/or data communication.
- To save power, the mobile platform can train and use a machine-learning service to predict whether or not a search to connect with available communication networks will succeed. The machine-learning service can be trained using a number of features, including, but not limited to: mobile platform location, mobile platform velocity, time, signal characteristics such as signal strength, device usage, time since a last search for wireless communication service was performed, and outcome of the last search for wireless communication service. By predicting search outcomes rather than performing actual searches, the mobile platform can save power and reduce the amount of signaling used in attempting to connect with communication networks.
- As part of training the machine-learning service, the mobile platform can carry out a number of searches to connect with available communication networks. At least while training, the mobile platform can store the above-mentioned features, along with an outcome of each search to connect, e.g., success or failure. The outcome of each search can be treated as a fact or “ground truth.” In one embodiment, the machine-learning service can include a binary classifier that predicts a likelihood of a finding an available communication network, a.k.a. predicts a result of a search to connect with available communication networks.
- In another embodiment, the machine-learning service can include a tri-nary classifier that predicts a likelihood of a finding an available communication network or indicates that the service cannot predict the outcome of a search. For example, if the machine-learning service were trained to look for communications networks in north-eastern Illinois only and the mobile platform was moved to central California, the machine-learning service can output that it cannot predict the outcome of a search to connect with available communication networks, as the current set of features, especially location, do not match any features used to train the machine-learning service.
- In still another embodiment, the machine-learning service can get features and communication search outcomes from other mobile platforms, either via direct communication with other mobile platforms or via communication with one or more intermediate computing devices, such as servers storing feature and outcome data. In the above example of a mobile platform going from north-eastern Illinois to central California, the mobile platform can download feature and outcome data from mobile platforms in central California, and use the California-based data to predict search outcomes. In still other embodiments, the machine-learning service can learn about and generate predictions regarding connections with communication networks along with or instead of Wi-Fi networks, such as, but not limited to, Wi-Max, GSM, CDMA, TDMA, 3G, 4G, Bluetooth, and Zigbee networks.
- Turning to the figures,
FIG. 13 is a flow chart of amethod 1300, in accordance with an example embodiment. In some embodiments, part or all ofmethod 1300 can be executed using one or more mobile platforms; e.g.,mobile platform computing device 2200. -
Method 1300 begins atblock 1310, where a machine-learning service executing on a mobile platform can receive feature-related data. The feature-related data can include communications-related data related to one or more searches for establishing electronic communications received from an application executing on the mobile platform and platform-related data received from the mobile platform. The communications-related data and the platform-related data can differ. - At
block 1320, the machine-learning service can determine whether or not the machine-learning service is trained to perform machine-learning operations related to predicting outcomes of searches for establishing electronic communications. - In some embodiments, the searches for establishing electronic communications can include a search to establish a wireless local-area network connection. In particular embodiments, the search to establish the wireless local-area network connection can include a search to establish the wireless local-area network connection based on an Institute of Electrical and Electronics Engineers (IEEE) 802.11 standard.
- In other embodiments, in response to determining that the machine-learning service is not trained: additional feature-related data can be received at the machine-learning service, where the additional feature-related data can include additional communications-related data related to one or more searches for establishing electronic communications received from the application and additional platform-related data from the mobile platform, and where the communications-related data, additional communications-related data, platform-related data, and additional platform-related data all differ. After receiving the additional feature-related data, determining whether the machine-learning service is now trained to perform machine-learning operations related to predicting outcomes of searches for establishing electronic communications.
- At
block 1330, in response to determining that the machine-learning service is trained: (i) the machine-learning service can receive a request for a predicted outcome of a search for establishing an electronic communication, (ii) the machine-learning service can generate the predicted outcome by performing a machine-learning operation on the feature-related data, and (iii) the predicted outcome can be sent to the application, perhaps by the machine-learning service. - In particular embodiments, the machine-learning service can generate the predicted outcome by performing a machine-learning operation on the feature-related data without receiving the request for the predicted outcome of the search; i.e., the machine-learning service can generate the predicted outcome of the search without being prompted by the request.
- In some embodiments,
method 1300 can further include: in response to determining that the machine-learning service is trained, a communication indicating that the machine-learning service is trained can be sent to the application, perhaps by the machine-learning service. - In other embodiments,
method 1300 can further include: in response to the predicted outcome indicating a search for establishing electronic communications would be successful, a wireless interface of the mobile platform can be activated. In still other embodiments, in response to the predicted outcome indicating a search for establishing electronic communications would not be successful, deferring activation of a radio of the mobile platform. In particular of the still other embodiments, after receiving the predicted outcome indicating the search for establishing electronic communications would be not successful, the application can start a periodic request for the predicted outcome of the search for establishing the electronic communication. - In more particular of the still other embodiments, the application starting the periodic request for the predicted outcome of the search for establishing an electronic communication can include: sending the request for the predicted outcome of the search for establishing the electronic communication to the machine-learning service, receiving the predicted outcome from the machine-learning service, in response to the predicted outcome indicating a search for establishing electronic communications would not be successful, waiting at least a pre-determined amount of time, and in response to the predicted outcome indicating a search for establishing electronic communications would be successful: stopping the periodic request for the predicted outcome and attempting establishment of the electronic communications.
- In even other embodiments,
method 1300 can further include: in response to the predicted outcome indicating failure to predict a search for establishing electronic communications: conducting the search to establish electronic communications, determining an outcome of the search, and sending an indication of the outcome of the search to the machine-learning service. -
FIG. 14 shows ascenario 1400 for establishing wireless communications with a number of access points, in accordance with an example embodiment. Duringscenario 1400, a mobile platform travels to a number of locations and attempts to establish wireless electronic communications at each of the locations. Some of the locations are served by one or more wireless networks, and so some of the attempts to establish wireless electronic communications are successful while other attempts are unsuccessful. Records of the attempts to establish wireless electronic communications scenario duringscenario 1400 are stored inlog 1460, which can be stored on the mobile platform. - The top portion of
FIG. 14 depicts a twelve-block map. The four north/south streets shown inFIG. 14 are numbered from 1st St., on the left or west side, 2nd St., just to the east of 1st St., 3rd St. to east of 2nd St. and 4th St., which is the right-most or east-most north/south street. The three east/west avenues shown inFIG. 14 , going from north to south are: Apple Ave, Berry Ave., and Cherry Ave. - Several of the blocks shown in
FIG. 14 have wireless communications transmitters, specifically shown astransmitters Transmitter 1420 generates network “4thApple” with a range shown usingcircle 1440. Similarly,transmitters circles - During
scenario 1400, the mobile platform travels from location A, shown inFIG. 14 as an octagon surrounding the letter “A” that is just north of Apple Ave near 2nd St, through locations B, C, D, E, F, and G. At each of locations A, B, C, D, E, F, and G, the mobile platform attempts to connect wirelessly with a wireless network, such as wireless network conducted using an IEEE 802.11 standard (a.k.a. a Wi-Fi network) or perhaps using another communications protocol. After each attempt to connect wirelessly, the mobile platform stores data related to the attempt in a log, shown inFIG. 14 aslog 1460. - As shown at the bottom of
FIG. 14 , log 1460 stores the following information: -
- a location (Loc) of the mobile platform,
- a velocity of the mobile platform,
- a time that the mobile platform attempted to connect with a wireless network,
- a signal strength (SigStr) of a wireless network detected at the time that the mobile platform attempted to connect. In the example log shown in
FIG. 14 , signal strengths are rated on a 0 (no signal) to 100 (maximum strength) scale. Other techniques for rating signal strengths are possible as well. - a wireless network name (Ntwk), if detected during the attempt to connect or 0, if no network detected,
- a time (Time(S)) since the last “search” or attempt to connect, and
- an outcome (Out(S)) of the last attempt to connect.
- In other embodiments, more or fewer data items can be collected and stored in
log 1460. - In
scenario 1400, log 1460 shows that the mobile platform was at location A at time 17:00 on Tuesday and attempted to connect to a wireless network. The outcome of the attempt was unsuccessful. At 17:00 on Tuesday, the mobile platform had been used for two minutes, and it had been five minutes since the last attempt to connect to a wireless network. - As another example, log 1460 shows that the mobile platform was at location B (just south of the
intersection 2nd St. and Apple Ave.) at time 17:10 on Tuesday and attempted to connect to a wireless network. The outcome of the attempt was successful. In particular, the mobile platform connected to the ApBer network with a signal strength of 40 out of 100. At 17:10 on Tuesday, the mobile platform had been used for twelve minutes, and it had been ten minutes since the last attempt to connect to a wireless network. - Information for attempts made by the mobile platform at locations C, D, E, F, and G is also shown in
FIG. 14 . At locations D (the intersection of 2nd St. and Cherry Ave.), E (just northwest of the intersection of 3rd St. and Cherry Ave.), and G (on the north side of Apple Ave. just east of 2nd St.), the mobile platform was able to connect to respective networks C2, Zone3, and ApBer with respective signal strengths of 25, 80, and 12. However, at locations C (midway between 1st and 2nd Streets and midway between Berry and Cherry Ayes.) and F (on the northeastern corner of 3rd St. and Berry Ave.), the mobile platform was unable to connect—at location C, the mobile platform was able to detect the C2 network, but was unable to connect, perhaps due to the low signal strength of 3 out of 100. At location F, no network was detected. -
FIG. 15A shows example user interface 1510 used inscenario 1500, in accordance with an example embodiment.Scenario 1500 includes user interface 1510 executing onmobile platform 1502. As shown inFIG. 15A , user interface 1510 includes Wi-Fi application 1520 with Wi-Fi Predictor Setting 1522.FIG. 15A shows that Wi-Fi Predictor Setting 1522 is set to active and that a Wi-Fi predictor is “Trained to Predict Wi-Fi Connections.” The Wi-Fi predictor has a Wi-Fi Predictor Dialog 1524 that includes threepredictions dialog 1522, and an close button to closedialog 1524. - Each of
predictions FIG. 15A indicates a prediction for connecting to a wireless network. For examples,prediction 1526 a indicates thatmobile platform 1502 is likely to connect to the “ApBer” Network,prediction 1526 b indicates that the Wi-Fi Predictor indicates thatmobile platform 1502 is unlikely to connect to the “C2”Network, andprediction 1526 c indicates thatmobile platform 1502 is unlikely to connect to the “FanSarBooks” Network, - Each of
predictions FIG. 15A additionally enables a user of Wi-Fi Predictor Dialog 1524 to attempt to connect with the network mentioned in the prediction. For example, by clicking on the check-mark box underneath the word “Connect?” on the right side ofprediction 1526 a,mobile platform 1502 can attempt to connect to the “ApBer” network. -
FIG. 15B shows example communications forscenario 1500, in accordance with an example embodiment.Scenario 1500 is related toScenario 1400, asscenario 1500 starts with the same attempts to communicate wirelessly as shown inlog 1460. Duringscenario 1500, the logged data is provided as training data to aclassifier learning model 1530. After training with the logged data,learning model 1530 is then requested to predict communications at three additional locations H, I, and J, which are shown in the map depicted inFIG. 14 , using the location's letter surrounded by an octagon. Location H is just southwest of location A, location I is near the northernmost point of 4th St. depicted inFIG. 14 , and location J is just south of location B. - Wi-
Fi application 1520 requests service from learningmodel 1530 using the getService( ) function incommunication 1552. In response,learning model 1530 sends a learning model session key of “S10” to Wi-Fi application 1520 using communication 1554. Session key S10 is included for subsequent communications between Wi-Fi application 1520 andlearning model 1530 to permit addressing the subsequent communications to the correct learning model, e.g.,learning model 1530 and correct application, e.g., Wi-Fi application 1520 for a learning session keyed by S10. - Wi-
Fi application 1520 can instructlearning model 1530 viacommunication 1520 to set up a classification interface. As shown inFIG. 15B ,communication 1556 includes a ClassificationInterface( ) call with five parameters: session key S10, an input-source reference of Conns, and a requested outputs of Wi-Fi signal strengths, Wi-Fi Network names, and Wi-Fi search outcomes, as shown inFIG. 15B , using the predefined WIFI_SIG_STR, WIFI_NTWK, and WIFI_OUTCOME values. - In embodiments not shown in
FIG. 15B , an application can request a similar prediction for a non-Wi-Fi network, such as, but not limited to, a Wi-Max, GSM, CDMA, TDMA, 3G, 4G, Bluetooth, or Zigbee network. - In response,
learning model 1530 then requests platform-related data from built-infeatures 1540 via communication 1578. Communication 1578 includes a ReqBI( ) function call with five parameters: a predefined value of LOC requesting a current location of the mobile platform, a predefined value of VEL requesting a current velocity of the mobile platform, a predefined value of CLOCK_TIME requesting a current clock time, a predefined value of USAGE requesting a current usage value, such as uptime, of the mobile platform a, and a predefined value of LAST_WIFI_SEARCH_TIME requesting a time of a last search for a Wi-Fi network. In other scenarios, more or less information can be requested by learningmodel 930 as platform-related data. In response, built infeatures 1540 returns a built-in session key BI10 viacommunication 1560. -
Scenario 1500 continues with the mobile platform moving to location A and searching for Wi-Fi service. As shown inFIG. 14 , at location A, the mobile platform is unable to connect to a Wi-Fi network.FIG. 15B shows Wi-Fi Application 1520 providing information about searching for Wi-Fi networks at location A via communication 1562, which includes a Push( ) function call with five parameters. The five parameters are a session key S10, an input-source reference of Conns, a Wi-Fi signal strength value of 0, a Wi-Fi Network name of 0 to indicate no network was found, and a Wi-Fi search outcome of NO to indicate the search was unsuccessful. - In response to the Push( ) function call of communication 1562,
learning model 1530 can send communication 1564 with a Push( ) function to built-infeatures 1540 to request platform-related data. The Push( ) function sent via communication 1564 has six parameters: a built-in session key of BI10, a location parameter L1, a velocity parameter V1, a clock-time parameter CT1, a usage parameter U1, and a last Wi-Fi search time parameter LWST1. In response, built-infeatures 1540 can send a Push( ) function via communication 1566 with values for the parameters requested using communication 1564. As shown inFIG. 15B , communication 1566 includes a Push( ) function with a built-in session key of BI10, a location value of “A” indicating location A as shown inFIG. 14 , a velocity value of 2 MPH, a clock-time value of 1700, a usage value of 2 minutes, and a last Wi-Fi search time value of 5 minutes. - In response to communication 1566,
learning model 1530 and/or built-infeatures 1540 can log part or all of the information provided by Wi-Fi Application 1520 and/or built-infeatures 1540, such as shown inlog 1460 ofFIG. 14 . -
Scenario 1500 continues with the mobile platform moving to location B and searching for Wi-Fi service. As shown inFIG. 14 , at location B, the mobile platform is able to connect to the “ApBer” network.FIG. 15B shows Wi-Fi Application 1520 providing information about searching for Wi-Fi networks at location B via communication 1568, which includes a Push( ) function call with five parameters. The five parameters are a session key S10, an input-source reference of Conns, a Wi-Fi signal strength value of 40, a Wi-Fi Network name of ApBer to indicate that the ApBer network was found, and a Wi-Fi search outcome of YES to indicate the search was successful. - In response to the Push( ) function call of communication 1568,
learning model 1530 can send communication 1570 with a Push( ) function to built-infeatures 1540 to request platform-related data. The Push( ) function sent via communication 1570 has six parameters: a built-in session key of BI10, a location parameter L1, a velocity parameter V1, a clock-time parameter CT1, a usage parameter U1, and a last Wi-Fi search time parameter LWST1. In response, built-infeatures 1540 can send a Push( ) function via communication 1572 with values for the parameters requested using communication 1570. As shown inFIG. 15B , communication 1572 include a Push( ) function with a built-in session key of BI10, a location value of “B” indicating location B as shown inFIG. 14 , a velocity value of 3 MPH, a clock-time value of 1710, a usage value of 12 minutes, and a last Wi-Fi search time value of 10 minutes. - In response to communication 1572,
learning model 1530 and/or built-infeatures 1540 can log part or all of the information provided by Wi-Fi Application 1520 and/or built-infeatures 1540, such as shown inlog 1460 ofFIG. 14 . -
Scenario 1500 continues with the mobile platform traveling to locations C, D, E, and F as shown inFIG. 14 . At each location, the mobile platform searches to find one or more Wi-Fi networks. The resulting communication search data and platform-related data are shown inlog 1460 ofFIG. 14 , but are not shown inFIG. 15B to save space. -
Scenario 1500 then continues with the mobile platform traveling to location G and searching for Wi-Fi service. As shown inFIG. 14 , at location G, the mobile platform is able to connect to the “ApBer” network.FIG. 15B shows Wi-Fi Application 1520 providing information about searching for Wi-Fi networks at location G via communication 1574, which includes a Push( ) function call with five parameters. The five parameters are a session key S10, an input-source reference of Conns, a Wi-Fi signal strength value of 12, a Wi-Fi Network name of ApBer to indicate that the ApBer network was found, and a Wi-Fi search outcome of YES to indicate the search was successful. - In response to the Push( ) function call of communication 1574,
learning model 1530 can send communication 1576 with a Push( ) function to built-infeatures 1540 to request platform-related data. The Push( ) function sent via communication 1576 has six parameters: a built-in session key of BI10, a location parameter L1, a velocity parameter V1, a clock-time parameter CT1, a usage parameter U1, and a last Wi-Fi search time parameter LWST1. In response, built-infeatures 1540 can send a Push( ) function via communication 1578 with values for the parameters requested using communication 1576. As shown inFIG. 15B , communication 1578 include a Push( ) function with a built-in session key of BI10, a location value of “G” indicating location G as shown inFIG. 14 , a velocity value of 2 MPH, a clock-time value of 2002, a usage value of 122 minutes, and a last Wi-Fi search time value of 17 minutes. - In response to communication 1576,
learning model 1530 and/or built-infeatures 1540 can log part or all of the information provided by Wi-Fi Application 1520 and/or built-infeatures 1540, such as shown inlog 1460 ofFIG. 14 . - In
scenario 1500,learning model 1530 is trained after receiving the data from communications 1576 and 1578.FIG. 15B shows thatlearning model 1530 indicates it is trained by sendingcommunication 1580 to Wi-Fi Application 1520 with a Notify( ) function call including two parameters: a session key S10, and a predefined value of LM_TRAINED to indicate thatlearning model 1530 is now trained. -
Scenario 1500 continues with the mobile platform traveling to location H and requesting a prediction of a Wi-Fi search outcome. To request the prediction of the Wi-Fi search outcome, Wi-Fi Application 1520 can sendcommunication 1582 with a Pull( ) function call. As shown inFIG. 15B , the Pull( ) function call has two parameters: a session key S10, and a pre-defined value WIFI_SEARCH, indicating that Wi-Fi Application 1520 is requesting a prediction of an outcome of search for Wi-Fi networks from learningmodel 1530. - In some embodiments not shown in
FIG. 15B , the Pull( ) function call as sent incommunication 1582 can have more than two parameters. For example, a third parameter specifying a network name, e.g., Zone3, can be interpreted by learningmodel 1530 as a request for a prediction for a search of wireless communications with the specified network. As another example, a third parameter specifying a location, e.g., location D, can be interpreted by learningmodel 1530 as a request for a prediction for a search of wireless communications at the specified location. Combining these examples, parameters specifying both a network name and a location can be interpreted by learningmodel 1530 as a request for a prediction for a search of wireless communications at the specified location for the specified network. Many other possible requests for prediction of searches for wireless communication are possible as well. - In response to
communication 1582,learning model 1530 can send communication 1584 a with a Push( ) function to built-infeatures 1540 to request platform-related data. The Push( ) function sent via communication 1584 a has six parameters: a built-in session key of BI10, a location parameter L1, a velocity parameter V1, a clock-time parameter CT1, a usage parameter U1, and a last Wi-Fi search time parameter LWST1. In response, built-infeatures 1540 can send a Push( ) function via communication 1584 b with values for the parameters requested using communication 1584 a. As shown inFIG. 15B , communication 1584 b includes a Push( ) function with a built-in session key of BI10, a location value of “H” indicating location H as shown inFIG. 14 , a velocity value of 0 MPH, a clock-time value of 2005, a usage value of 125 minutes, and a last Wi-Fi search time value of 3 minutes. -
Learning model 1530 can then classify the data received from built-infeatures 1540 into one of two possible predicted outcomes: a prediction that a search for a Wi-Fi network will be successful, and a prediction that a search for a Wi-Fi network will be unsuccessful. In some embodiments, a third possible outcome that the learning model is unable to predict the outcome, perhaps because the learning model is not trained to generate a prediction given the current data, is possible as well. In still other embodiments, other predictions are possible as well. - In
scenario 1500,learning model 1530 can predict that a search for a Wi-Fi connection at location H is unlikely to succeed, perhaps based on recent data from nearby-locationA. Learning model 1530 can sendcommunication 1586 a to inform Wi-Fi Application 1520 of the prediction.FIG. 15B shows thatcommunication 1586 a includes a PullResp( ) function call with five parameters: a session key S10, a input-source reference Conns, a predicted Wi-Fi signal strength of 0, a predicted Wi-Fi network of none indicated using the predefined value of NO_NTWK, and a predicted Wi-Fi search outcome of no network found indicated using the predefined value of PRED_NO. - In response to the prediction of a search for a Wi-Fi connection, Wi-
Fi Application 1520 can determine that the wireless interface (WI) activation to search for and/or connect to a Wi-Fi network can be deferred, as shown in box 1586 b ofFIG. 15B , as learningmodel 1530 has predicted the search and/or attempt to connect would be unsuccessful. Wi-Fi Application 1520 can save power for the mobile platform by deferring wireless interface activation and making fewer unsuccessful searches for Wi-Fi connections. - In some embodiments, in response to the unsuccessful prediction, Wi-
Fi Application 1520 can make one or more additional requests for predictions of searches for Wi-Fi connections. For example, Wi-Fi Application 1520 can periodically request predictions of searches for Wi-Fi connections, such as requesting one prediction every N seconds, where N>0. In other embodiments, Wi-Fi Application 1520 can request another prediction of a search for Wi-Fi connections when the mobile platform has moved more than a threshold amount from a location of an unsuccessful search. For example, if the threshold amount is 100 yards, then Wi-Fi Application 1520 can request another predicted search when mobile platform moves more than 100 yards from location H. Other responses to unsuccessful predictions are possible as well. -
Scenario 1500 continues with the mobile platform traveling to location I and requesting a prediction of a Wi-Fi search outcome. To request the prediction of the Wi-Fi search outcome, Wi-Fi Application 1520 can send communication 1588 with a Pull( ) function call. As shown inFIG. 15B , the Pull( ) function call has two parameters: a session key S10, and a pre-defined value WIFI_SEARCH, indicating that Wi-Fi Application 1520 is requesting a prediction of an outcome of search for Wi-Fi networks from learningmodel 1530. - In response to communication 1588,
learning model 1530 can send communication 1590 a with a Push( ) function to built-infeatures 1540 to request platform-related data. The Push( ) function sent via communication 1590 a has six parameters: a built-in session key of BI10, a location parameter L1, a velocity parameter V1, a clock-time parameter CT1, a usage parameter U1, and a last Wi-Fi search time parameter LWST1. In response, built-infeatures 1540 can send a Push( ) function via communication 1590 b with values for the parameters requested using communication 1590 a. As shown inFIG. 15B , communication 1590 b includes a Push( ) function with a built-in session key of BI10, a location value of “I” indicating location I as shown inFIG. 14 , a velocity value of 3 MPH, a clock-time value of 2015, a usage value of 135 minutes, and a last Wi-Fi search time value of 13 minutes. - In
scenario 1500,learning model 1530 can determine thatlearning model 1530 is unable to predict an outcome of a search for a Wi-Fi connection at location I. For example,learning model 1530 can determine that training data does not apply to locations, velocities, clock times, usage values, and/or last search times as provided in communication 1590 b, and so determine thatlearning model 1530 is unable to predict an outcome of a search for a Wi-Fi connection at location I moving at 3 MPH at 20:15 with 135 minutes of usage and with 13 minutes since a last Wi-Fi search for a Wi-Fi connection. -
Learning model 1530 can send communication 1592 to inform Wi-Fi Application 1520 of the inability of learningmodel 1530 to make a prediction.FIG. 15B shows that communication 1592 includes a PullResp( ) function call with five parameters: a session key S10, a input-source reference Conns, a predicted Wi-Fi signal strength of 0, a predicted Wi-Fi network of none indicated using the predefined value of NO_NTWK, and a predicted Wi-Fi search outcome of an unknown outcome as indicated using the predefined value of PRED_UNK. Upon observing the predefined value of PRED_UNK, Wi-Fi Application 1520 can then determine thatlearning model 1530 to unable make a prediction. - In response to the unknown prediction of a search for a Wi-Fi connection, Wi-
Fi Application 1520 can determine that activating the wireless interface to search for and/or connect to a Wi-Fi network can be deferred, as learningmodel 1530 has not predicted the search and/or attempt to connect would be successful. - In some embodiments not shown in
FIG. 15B , in response to the unknown prediction, Wi-Fi Application 1520 can activate the wireless interface, search for Wi-Fi connections, and provide search result data tolearning model 1530, such as performed via communications 1562, 1568, and 1574. Other responses to prediction unavailability are possible as well. -
Scenario 1500 continues with the mobile platform traveling to location J and requesting a prediction of a Wi-Fi search outcome. To request the prediction of the Wi-Fi search outcome, Wi-Fi Application 1520 can send communication 1594 with a Pull( ) function call. As shown inFIG. 15B , the Pull( ) function call has two parameters: a session key S10, and a pre-defined value WIFI_SEARCH, indicating that Wi-Fi Application 1520 is requesting a prediction of an outcome of search for Wi-Fi networks from learningmodel 1530. - In response to communication 1594,
learning model 1530 can send communication 1596 a with a Push( ) function to built-infeatures 1540 to request platform-related data. The Push( ) function sent via communication 1596 a has six parameters: a built-in session key of BI10, a location parameter L1, a velocity parameter V1, a clock-time parameter CT1, a usage parameter U1, and a last Wi-Fi search time parameter LWST1. In response, built-infeatures 1540 can send a Push( ) function via communication 1596 b with values for the parameters requested using communication 1596 a. As shown inFIG. 15B , communication 1596 b includes a Push( ) function with a built-in session key of BI10, a location value of “J” indicating location J as shown inFIG. 14 , a velocity value of 18 MPH, a clock-time value of 2022, a usage value of 142 minutes, and a last Wi-Fi search time value of 20 minutes. - In
scenario 1500,learning model 1530 can predict that a search for a Wi-Fi connection at location J is likely to succeed, perhaps based on recent data from nearby-locationB. Learning model 1530 can send communication 1598 a to inform Wi-Fi Application 1520 of the prediction.FIG. 15B shows that communication 1598 a includes a PullResp( ) function call with five parameters: a session key S10, a input-source reference Conns, a predicted Wi-Fi signal strength of 36, a predicted Wi-Fi network name of “ApBer”, and a predicted Wi-Fi search outcome a network found indicated using the predefined value of PRED_YES. - In response to the prediction of a successful search for a Wi-Fi connection, Wi-
Fi Application 1520 ofmobile platform 1502 can activate the wireless interface, as shown inbox 1598 ofFIG. 15B . Once the wireless interface is activated,mobile platform 1502 can search for and/or connect to a Wi-Fi network, as learningmodel 1530 has predicted the search and/or attempt to connect would be successful. Other responses to successful predictions are possible as well. - A “usage session” for a mobile platform is the span of time between when the mobile platform is awakened or perhaps powered up and when the mobile platform is put to sleep or perhaps powered down. The time span or duration of the usage session is the amount of time between the awakening of the mobile platform and putting the mobile platform to sleep. For example, if a mobile platform is powered up at 8:00 AM and powered down at 8:05 AM, the time span of the usage session starting at 8:00 AM is five minutes.
- The machine-learning service can predict a time span of a usage session either numerically or via classification. An example numerical prediction would be that a usage session will be six minutes long. An example classification of the duration of the usage session can be: short (e.g., less than five minutes), medium (e.g., between five and ten minutes), or long (greater than five minutes). In an example utilizing these classifications, the machine-learning service can predict a usage session will be either short, medium, or long.
- A software application can use a predicted session duration or time span to guide the application's behavior. For example, suppose a user powers up a mobile platform and then starts a jukebox application to play random audio and/or video files. The jukebox application can send a request to the machine-learning service to predict a time span for the current usage session. In this example, the machine-learning service predicts a “medium” (5-10 minute) time span for the usage session. Based on the predicted medium time span, the jukebox application can initially select only audio, video, and/or audio-video files that cumulatively take ten minutes or less to play based on the predicted medium time span.
- Further, suppose the jukebox application first plays an audio, video, and/or audio-video file that takes three minutes. Then, in selecting a second audio, video, and/or audio-video file, the jukebox application can select files that take no more than seven minutes long, based on a ten-minute long maximum duration of the usage session, with three minutes already consumed by playing the first file. Many other examples of application requests and using predicted session durations and/or time spans are possible as well.
- Turning to the figures,
FIG. 16 is a flow chart of amethod 1600, in accordance with an example embodiment. In some embodiments, part or all ofmethod 1600 can be executed using one or more mobile platforms; e.g.,mobile platform computing device 2200. - At
block 1610, feature-related data can be received at a machine-learning service executing on a mobile platform. The feature-related data can include usage-related data and platform-related data. The usage-related data can include data about one or more time spans that the mobile platform is activated. The platform-related data can be received from the mobile platform. The usage-related data and the platform-related data can differ. - At
block 1620, the machine-learning service can determine whether the machine-learning service is trained to perform machine-learning operations related to predicting a time span that the mobile platform will be activated. - In some embodiments, the machine-learning operations can include a classification operation.
- In other embodiments, the machine-learning operations can include a regression operation. In particular embodiments, the regression operation can include a linear regression operation. In other particular embodiments, the predicted time span can include a predicted amount for the predicted time span.
- At
block 1630, in response to determining that the machine-learning service is trained, the machine-learning service can: receive a request for a predicted time span that the mobile platform will be activated, determine the predicted time span by performing a machine-learning operation on the feature-related data, and send the predicted time span. - In particular embodiments, the machine-learning service can determine the predicted time span by performing a machine-learning operation on the feature-related data without receiving the request for the predicted time span; i.e., the machine-learning service can generate the predicted time span without being prompted by the request.
- In some embodiments, the predicted time span can include a predicted classification of the time span. In particular embodiments, the predicted classification of the time span can be selected from among a short time span, a medium time span, and a long time span. In other embodiments,
method 1600 can include selecting one or more media files based on the predicted time span and presenting the selected one or more media files using the mobile platform, such as discussed above in the context of the example jukebox application. -
FIG. 17A shows usage log 1700 for activations of a mobile platform, in accordance with an example embodiment. Usage log 1700 shows, for each activation of the mobile platform, a time span (TS), a starting time shown using military-time notation, a location number, a location label (Loc Label), and a time-span label (TS Label). - Semantic labels, such as the location label and time-span label, can be part of usage log 1700. For example, the first row of usage log 1700 corresponds to an eight-minute long usage session starting at 8:08 AM (0808) at
location 1, also known as “Home”, and the time span of “medium” duration. For the entries in usage log 1700, a usage session with a time span of zero to five minutes is labeled “short”, a usage session with a time span of five to ten minutes is labeled “medium”, and a usage session with a 10+ minute time span is labeled “long.” In some embodiments, more, fewer, and/or other semantic labels for time spans can be used. In other embodiments, other ranges of times can be used for short, medium, and/or long time spans. - Additionally, locations can have semantically labels. As shown in
FIG. 17A , seven separate locations are listed, both numerically using the numbers one through seven, and with semantic labels.FIG. 17A showslocation 1 with a semantic location label of “Home”,location 2 with a semantic location label of “Work”,location 3 with a semantic location label of “Random1”,location 4 with a semantic location label of “Groceries”,location 5 with a semantic location label of “Lunch Rest.” abbreviating Lunch Restaurant,location 6 with a semantic location label of “Movie” for a movie theater, andlocation 7 with a semantic location label of “Random2.” -
FIG. 17B shows a usage log 1710 of activations of a mobile platform at a particular location, in accordance with an example embodiment. In particular, usage log 1710 shows activations of the mobile platform logged in usage log 1700 at location four, which has a semantic location label of “Groceries” as shown inFIG. 17A . The time-span data in usage log 1710 ranges from a minimum time-span of 15 minutes in the last entry of usage log 1710 to a maximum time-span of 22 minutes in the second entry of usage log 1710. - Usage log 1710 also includes a “Start” column with starting times for usage sessions shown in military time notation and a “Mins” column with starting times for usage sessions shown in terms of the number of minutes since midnight. For example, the first row of usage log 1710 in the “Start” column is for a usage session of 17 minutes duration that started at 17:59, where the time 17:59=17*60+59=1,079 minutes after midnight.
-
FIG. 17C showsgraph 1720 of a linear regression model based on the data in usage log 1710, in accordance with an example embodiment. To provide a numerical prediction for a time span, a learning model can use regression techniques, such as, but not limited to linear regression, multiple regression, multivariate linear regression, and non-linear regression techniques. - The linear regression model shown in
FIG. 17C is based on the “TS” and “Mins” column data. Mins column data were used instead of “Start” column data, as the Mins column data covers a single range of integers from 0 (corresponding to 0000 military time of 0000) to 1439 (corresponding to 2359 military time), while the set of military time values involve 24 separate ranges of integers (0000-0059, 0100-0159, . . . 2300-2359). - Specifically, the linear regression model shown is based on the equation y=mx+b, where m is approximately −0.04 and b is approximately 61. While Mins column data were used to determine the linear regression model, the x or “Start” axis of
graph 1720 is labeled using military time labels for convenience. For example, the x axis label “0600” shown inFIG. 17C represents the time 6:00 AM. - The linear regression model can predict numerical values for time spans. For example, suppose the machine-learning model was requested to predict a time span of a usage session for the mobile platform at location “Groceries” at 6 PM or 1800 military time. The machine-learning service can use the linear regression model shown in
FIG. 17C to predict a time span of 18.85 minutes for the usage session starting at 1800. -
FIG. 18A shows classification diagram 1800, which based on the data in usage log 1700 shown inFIG. 17A , in accordance with an example embodiment. Classification diagram 1810 depicts a result of a possible analysis of the data in usage log 1700 that can be used by the machine-learning service to predict time spans of usage sessions. - Classification diagram 1800 begins at
node 1810, where a location is determined.FIG. 18A shows locations using the some of the semantic labels shown inFIG. 17A , such as Home used innode 1820, Work used innode 1822, Groceries used innode 1824, and Lunch Rest. used innode 1826.Node 1828 uses a label of “Single-Session Locs.” to group all of the locations involved in one usage session as recorded in usage log 1700.Node 1828 include data generated for locations with semantic labels “Movie”, “Random”, and “Random2” in usage log 1700. In other embodiments, numeric locations can replace the use of semantic location labels in classification diagram 1800. - In the example shown in
FIG. 18A , a short time span can range from zero to five minutes, a medium time span can range from six to ten minutes, and a long time span ranges longer than eleven minutes. Specifically,node 1820 shows an analysis of usage-session time spans from usage log 1700 at location Home, indicating that at location Home, six of sixteen of total sessions or 37.5% had a short time span, seven of sixteen total sessions or 43.8% had a medium time span, and three of sixteen total sessions or 18.8% had a long time span. In some embodiments, more, fewer, and/or other semantic labels for time spans can be used. In other embodiments, other ranges of times can be used for short, medium, and/or long time spans. -
Node 1822 shows an analysis of usage-session time spans at location Work, indicating that at location Work, ten of twelve total sessions or 83.3% had a short time span, two of twelve total sessions or 16.7% had a medium time span, and zero sessions had a long time span. - Only one semantic time label is shown as being utilized for each of
nodes node 1824, representing location Groceries, all four usage sessions had long time spans.Node 1826, regarding location Lunch Rest., indicates that both usage sessions at that location had short time spans.Node 1828, reflecting data from the Single-Session Locations, indicates that all three usage sessions at those locations had short time spans. - At the Home and Work locations, multiple possible classifications of time spans are possible. In that case, the data for those locations can be additionally analyzed. For example,
node 1830, shown connected tonode 1820 in classification diagram 1800, indicates a result of further analysis of usage sessions for location “Home” based on both usage-session time spans and time of day. Innode 1830, times of day are classified as either before work (BW), during work (DW), or after work (AW). For example, the BW times of day can range from midnight to 8:59 AM, the DW times of day can range from 9:00 AM to 5:00 PM, and the AW times of day can range from 5:01 PM to 11:59 PM. In some embodiments, more, fewer, and/or other semantic labels for times of day can be used. In other embodiments, other ranges of times can be used for the BW, DW, and AW time spans. In still other embodiments, times of day can be used without classifications. -
Node 1830 ofFIG. 18A shows that: (a) three of seven usage sessions before work at location Home have short time spans and four usage sessions at location Home have medium time spans, (b) no usage sessions were made at location Home during work, and that (c) equal numbers of short, medium, and long duration usage sessions were made at location Home after work. Using the information shown innode 1830, if a prediction is requested while the mobile platform is at location Home, one prediction is that the call is slightly more likely to be of medium duration than short duration before work, and that all durations are equally likely either during work or after work. As there are data for after-work calls, additional analysis may be performed to better refine the after work call estimate that “all durations are equally likely”; e.g., use additional time span and/or time-of-day classifications for after-work calls. - For
node 1832, times of day are classified as either “AM” for morning work hours; e.g., between 9:01 AM and 11:59 AM, “Meal” or during a meal, a.k.a. lunch or dinner; e.g., between 12:00 PM and 12:59 PM, and “PM” for post-meal afternoon work hours; e.g., between 1:00 PM and 4:59 PM. In some embodiments, more, fewer, and/or other semantic labels for times of day can be used. In other embodiments, other ranges of times can be used for the AM, Meal, and PM times of day. In still other embodiments, times of day can be used without classifications.Node 1832 ofFIG. 18A shows that: (a) all AM-time usage sessions have short time spans, (b) all Meal-time usage sessions have short time spans, and (c) it is somewhat more likely that short time-span usage sessions will be made during the PM times of day than medium time-span usage sessions. - Classifications and classification diagram 1800 can be used to generate numerically-valued predictions. For example, using the data in
node 1820, if a usage session is begun at location Home, 37.5% of all usage sessions are short, 43.8% of all usage sessions are medium, and 18.8% of all usage session are long. To generate a numerical prediction, one set of assumptions that can be used is that the mid-point value of a given range is chosen to represent usage-sessions in the range, and a long time-span usage session is arbitrarily chosen to have a 20 minute long time span. - Under these assumptions and for the data in
node 1820, a predicted time span PT for a usage session at location Home can be determined as: -
- Other techniques for generating numerically-valued predictions from classifications and classification diagrams are possible as well.
-
FIG. 18B shows example user interface 1854 forjukebox application 1860 used inscenario 1850, in accordance with an example embodiment. Inscenario 1850, user interface 1854 shows feature-relateddata display 1856 andjukebox application display 1860. Feature-related data display shows a current time and date of “12:23 Sat.”, a current location with a semantic label of “Home”, and current weather conditions of “58° F.” and “Cloudy.” - A jukebox application can be configured to request the machine-learning service to predict a time span of a usage session and then select media, including but not limited to audio, video, and audio/video files, to play during the predicted duration. In
scenario 1850, the jukebox application generatesjukebox application display 1860, which includesprediction dialog 1862,media list dialog 1864, play list button 1870, andclose button 1872. -
Prediction dialog 1862 shows the usage session time span predicted by the machine-learning service of 10 minutes, perhaps based on the feature-related data shown in feature-relateddisplay 1856, such as the displayed location, date, and time.Prediction dialog 1862 includes two buttons: a “Change” button to permit the user to change the usage session time span from the predicted value and a “Dismiss” button to closeprediction dialog 1862. - As shown in
FIG. 18B ,media list dialog 1864 displays a media list selected by the jukebox application. The media list includes three media files to be played: “Two-Minute News” with a duration of 2:03, a “Favorite Song 1” with a duration of 4:32, and “Now & Future Tools” with a duration of 3:11. - In
scenario 1850 and as shown inFIG. 18B , the “Now & Future Tools” media is selected, as shown inFIG. 18B using a grey background for the “Now & Future Tools” media list item and the bold font for the “Now & Future Tools” media list entry. As such, selected media and controls 1866 operate on the “Now & Future Tools” media. Selected media and controls 1866 include a “Play” button to play the selected media immediately, a “Delete” button to delete the selected media from the media list, a “Move” button to enable movement of the selected media within the media list, and a “New . . . ” button to permit addition of new media item(s) to the media list. Other dialogs and controls are possible as well. -
Usage information 1868 shows the total duration of the three media files is 9 minutes, 46 seconds (9:46) and the current usage, or amount of timemobile platform 1852 has been activated, is 3 seconds (0:03). Inscenario 1850, the media shown inmedia list dialog 1864 were selected, in part, to play less in total less than the predicted session time span. Inscenario 1850 and as shown inFIG. 18B , the total duration of the three media files is 14 seconds less than the predicted duration of 10 minutes. - Play list button 1870 can be used to play the media items in the order shown in
media list dialog 1864.Close button 1872 can be used to close user interface 1854. In some embodiments, media files played using the jukebox application can be played in the background, to permit the user ofmobile platform 1852 to interact with other applications during the usage session. In particular embodiments, when the jukebox application is in the background and is about to play a video or audio/video file, the jukebox application can attempt to become a foreground application to permit the user to view the video or audio/video file. - Volume settings for mobile platforms, such as ring tone volume, mute, and vibrate settings, are often changed according to predictable patterns. For example, before attending meetings, a mobile platform user can use a mute setting to turn off a telephone ringer. At that time, the user may set the mobile platform to a vibrate mode, where a user of the mobile platform is alerted to an incoming telephone call by use of a haptic or vibration system rather than the telephone ringer.
- Once the meeting is complete, the user may “un-mute” the mobile platform so to turn on the telephone ringer. At that time, the user may also set the vibrate mode to off, so to turn off the haptic or vibration system, and thus rely on the telephone ringer to alert the user of an incoming telephone call. The user may perform similar actions at other places than meetings where silence is often expected, such as at movie theaters, libraries, schools, lecture halls, live performances, and similar situations.
- The machine-learning service can be trained to learn patterns in changing volume settings for a mobile platform. Once trained, the machine-learning service can generate predictions for the volume settings. The predictions can be used to have the mobile platform directly or indirectly change the volume settings. For example, the machine-learning service can provide the predicted settings to a dialer application or other application that changes the settings to the predicted setting values. As another example, the machine-learning service can generate a prompt with the predicted setting values, perhaps with a reminder to the user to change the volume settings.
- By predicting volume settings for mobile platforms, the mobile-learning service can save the user the effort and time involved in repeatedly setting and resetting volume settings. Additionally, if the mobile platform is configured to prompt the user to change volume settings when predicted, the user can be reminded to change volume settings before attending an event where silence is often expected, and thus avoid the embarrassment of having the mobile platform sound off during the event.
- Turning to the figures,
FIG. 19 is a flow chart of amethod 1900, in accordance with an example embodiment. In some embodiments, part or all ofmethod 1900 can be executed using one or more mobile platforms; e.g.,mobile platform computing device 2200. - At
block 1910, feature-related data can be received at a machine-learning service executing on a mobile platform. The feature-related data can include volume-related data and platform-related data. The volume-related data can include data about one or more volume-related settings for the mobile platform. The platform-related data can be received from the mobile platform. The volume-related data and the platform-related data can differ. - In some embodiments, the one or more volume-related settings can include a mute setting, a vibration setting, and a setting for ringer volume. In other embodiments, the feature-related data can include data related to a location of the mobile platform, a current time, a call-termination time, a calling party, and a calendar event. In still other embodiments, the platform-related data can include data about devices proximate to the mobile platform.
- At
block 1920, the machine-learning service can determine whether the machine-learning service is trained to perform machine-learning operations related to predicting a change in the one or more volume-related settings for the mobile platform. - At
block 1930, in response to determining that the machine-learning service is trained, the machine-learning service can: receive a request for predicting the change in the one or more volume-related settings, determine the predicted change in the one or more volume-related settings by performing a machine-learning operation on the feature-related data, and send the predicted change in the one or more volume-related settings. - In particular embodiments, the machine-learning service can determine the predicted change in the one or more volume-related settings by performing a machine-learning operation on the feature-related data without receiving the request for the predicted outcome of the search for establishing the electronic communication; i.e., the machine-learning service can generate the predicted outcome without being prompted by a request.
- In some embodiments,
method 1900 can further include determining one or more semantic labels for the platform-related data. In particular of the some embodiments, determining one or more semantic labels for the platform-related data can include determining the one or more semantic labels for the current time, where the semantic labels include an at-home-time label, a work-time label, a sleep-time label, a traveling-time label, and a meal-time label. - In other particular of the some embodiments, determining one or more semantic labels for the platform-related data includes determining the one or more semantic labels for the current location, where the semantic labels include a home-location label, a work-location label, a sleep-location label, a traveling-location label, and a meal-location label.
- In still other embodiments,
method 1900 can further include: determining whether the predicted change in the one or more volume-related settings can include a prediction that at least one of the one or more volume-related settings change. In response to determining that the predicted change in the one or more volume-related settings includes the prediction that the at least one of the one or more volume-related settings changes, a prompt to change the at least one of the one or more volume-related settings can be generated. - In even other embodiments,
method 1900 can further include: determining whether the predicted change in the one or more volume-related settings can include a prediction that at least one of the one or more volume-related settings change. In response to determining that the predicted change in the one or more volume-related settings includes the prediction that the at least one of the one or more volume-related settings changes, the at least one of the one or more volume-related settings can be changed. The volume-related settings can be changed by the mobile platform without additional input, perhaps by a dialer application executing on the mobile platform that receives the at least one of the prediction that the one or more volume-related settings from the machine-learning service and sets the volume-related settings according to the prediction. -
FIG. 20A shows anexample user interface 2010 used inscenario 2000, in accordance with an example embodiment.User interface 2010, executing onmobile platform 2002, includes an interface fordialer application 2020 configured to generate anddisplay volume dialog 2022.Volume dialog 2022 includesmanual volume settings 2024,smart volume setting 2026, an OK button to save current settings before exitingvolume dialog 2022, and a Cancel button to exitvolume dialog 2022 without saving current settings. -
Manual volume settings 2024 includes a slider bar that enables a user to set a microphone output setting between a minimum setting of 0, which effectively mutes a ringer that outputs the ringtone, and a maximum setting of 100, which provides the maximum output volume for the ringtone. Inscenario 2000 as shown inFIG. 20A , manual ringtone volume setting is set at 51. -
Manual volume settings 2024 also includes check boxes for a mute setting and a vibrate setting. The mute setting, when active or checked, inhibits ringer output; a.k.a. mutes the ringer and stops ringtone output. The vibrate setting, when active or checked, allows a haptic output for the ringer; a.k.a. the mobile platform can vibrate or produce other haptic output when the mobile platform is in a ringing state. As shown inFIG. 20A , both the mute and vibrate settings are not active or checked. Inscenario 2000, when manual volume settings are allowed, the mute setting is not active and so the slider bar sets the volume for the ringer. Also, no haptic output is generated when the mobile platform is in the ringing state. - As shown in
FIG. 20A , smart volume setting 2026 can learn and predict ringer volume, mute, and vibrate settings. When smart volume setting 2026 is disabled,dialer application 2020 can usemanual volume settings 2024 to determine output volume for a ringer ofmobile platform 2020 and whether or not haptic output will be provided whenmobile platform 2002 is in the ringing state. - When smart volume setting 2026 is enabled,
dialer application 2020 can use a machine-learning service to perform machine-learning operation(s) to predict volume settings, such as ringtone volume, mute, and vibrate settings. Then, upon receiving predicted volume setting value(s),dialer application 2020 can use the predicted settings values according to predictedusage setting 2028. - In
scenario 2000, and as shown inFIG. 20A , smart volume setting 2026 is set to enabled and predictedusage setting 2028 is set to apply. When predictedusage setting 2028 is set to apply, predicted volume setting value(s), such as ringtone volume, mute, and/or vibrate setting values, received bydialer application 2020 are used to set the ringtone volume, mute, and vibrate settings. That is, when predictedusage setting 2028 is set to apply,mobile platform 2002 and/ordialer application 2020, can set the ringtone volume, mute, and/or vibrate settings using the predicted volume setting values(s) without additional input, such as user input. In some embodiments, when predictedusage setting 2028 is set to apply,mobile platform 2002 and/ordialer application 2020 can generate a dialog or other prompt to indicate to a user that the ringtone volume, mute, and/or vibrate settings have been changed to the predicted volume setting values(s). - When predicted
usage setting 2028 is set to prompt, predicted volume setting value(s) received bydialer application 2020 are displayed, audibly output, and/or otherwise prompted to a user ofmobile platform 2002. Upon being displayed, the user can usevolume dialog 2022 to set the ringtone volume, mute, and vibrate settings as desired, perhaps using the prompted predicted volume setting value(s). -
FIG. 20B shows example communications for thescenario 2000, in accordance with an example embodiment. InFIG. 20B ,scenario 2000 continues withdialer application 2020 calling the getService( ) function oflearning model 2030 viacommunication 2052. In response,learning model 2030 provides a session key S11 viacommunication 2054 todialer application 2020. Session key S11 is included for subsequent communications betweendialer application 2020 andlearning model 2030 to permit addressing the subsequent communications to the correct learning model, e.g.,learning model 2030 and correct application, e.g.,dialer application 2020 for a learning session keyed by S11. -
Dialer application 2020 can instructlearning model 2030, viacommunication 2056, to set up a prediction interface. As shown inFIG. 20B , communication 2058 includes a PredictionInterface( ) call with four parameters: session key S11, and three parameters for requested output predictions: ring tone volume settings, mute settings, and vibration settings, as shown inFIG. 20B , using the respective predefined RING_VOL, MUTE_SET, and VIB_SET values. - In response,
learning model 2030 sends communication 2058 to built-infeatures 2040 with a ReqBI( ) function call requesting built-in features. As shown inFIG. 20B ,learning model 2030 uses the ReqBI( ) call to request, using input-source descriptor “bid”, current location values, clock times, calendar-related events, and numbers of local (or proximate) devices, by passing the respective predefined LOC, CLOCK_TIME, CAL_EVENTS, and NUM_LOC_DEVS values to built-infeatures 2040. Built-infeatures 2040 can include at least the functionality of built-infeatures 410 discussed above. In response, built-infeatures 2040 returns a built-in session key BI11 via communication 2060. Built-in session key BI11 is included with subsequent communications betweenlearning model 2030 and built-infeatures 2040 to permit addressing the subsequent built-in related communications to the correct learning model. -
FIG. 20B continues withdialer application 2020 sending communication 2062 to learningmodel 2030 with a Push( ) function call to informlearning model 2030 that a volume settings have been set with a ring tone volume setting of 0, a mute setting of YES or active, and a vibration setting of YES. Additionally, built-infeatures 2040 sends communication 2064, with a Push( ) function call to informlearning model 2030 thatmobile platform 2002 is at location “Home” at a clock time of 0808 (8:08 AM), no calendar events are active, and that one device is nearby (proximate). -
FIG. 20B shows a number of communications 2066-2085 for five subsequent volume setting changes made usingdialer application 2020 and built-in feature values provided tolearning model 2030 from built-in features with the values listed in Table 5 below. -
TABLE 5 Cal Local Communications Volume Mute Vibration Location Time Events Devices 2066 and 2068 85 NO YES LoudSpot 00:07 NULL 15 2070 and 2072 0 YES YES Home 04:08 NULL 0 2074 and 2076 40 NO YES Home 13:22 NULL 0 2078 and 2080 20 NO YES Work 09:22 NULL 20 2082 and 2084 0 NO YES Work 09:57 Meeting 11 10 AM -
Scenario 2000 continues withlearning model 2030 sending communication 2086 todialer application 2020 with a Notify( ) function call. Inscenario 2000,learning model 2030 is trained after receiving the data from communications 2082 and 2084.FIG. 20B shows thatlearning model 2030 indicates it is trained by sending communication 2086 todialer application 2020 with a Notify( ) function call including two parameters: a session key S11, and a predefined value of LM_TRAINED to indicate thatlearning model 2030 is now trained. -
FIG. 20B showsscenario 2000 continuing withdialer application 2020 sending communication 2088 a tolearning model 2030. Communication 2088 a includes the Pull( ) function call to request a prediction of ringer volume, mute, and vibration settings from learningmodel 2030 using respective pre-defined values of RING_VOL, MUTE_SET, and VIB_SET as parameters in the Pull( ) function call. Communication 2088 b includes feature data from built-infunctions 2040 with a location of Work, a time of 2:02 PM (1402), a calendar event of NULL or no calendar events active, and a number of local devices of 7.Learning model 2030 can apply machine-learning operations to predict the requested volume settings.Scenario 2000 continues withlearning model 2030 sending the prediction todialer application 2020 using PullResp( ) in communication 2088 c.FIG. 20B shows that the prediction of communication 2088 c includes a ringtone volume setting of 20, a mute setting of NO or not active, and vibration setting of YES. -
Scenario 2000 continues withdialer application 2020 sending communication 2090 a tolearning model 2030. Communication 2090 a includes the Pull( ) function call to request a prediction of ringer volume, mute, and vibration settings from learningmodel 2030 using respective pre-defined values of RING_VOL, MUTE_SET, and VIB_SET as parameters the Pull( ) function call. Communication 2090 b includes feature data from built-infunctions 2040 with a location of Work, a time of 2:28 PM (1428), a calendar event of “Meet230PM”, and a number of local devices of 5.Learning model 2030 can apply machine-learning operations to predict the requested volume settings.Scenario 2000 continues withlearning model 2030 sending the prediction todialer application 2020 using PullResp( ) in communication 2090 c.FIG. 20B shows that the prediction of communication 2090 c includes a ringtone volume setting of 0, a mute setting of NO, and vibration setting of YES. -
FIG. 20B shows that built-infeatures 2040 then sends communication 2092 a tolearning model 2030. Communication 2092 a includes a Push( ) function call to informlearning model 2030 thatmobile platform 2002 is at “LoudSpot” at 11:58 PM (2358) without any calendar events scheduled and 17 local devices detected. In response to communication 2092 a,learning model 2030 can apply machine-learning operations to predict volume settings based on the information provided via communication 2092 a.Scenario 2000 continues withlearning model 2030 sending a Notify( ) function call in communication 2092 b todialer application 2020, predicting a ringtone volume setting of 85, a mute setting of NO, and vibration setting of YES. Upon reception of communication 2092 b,mobile platform 2002 and/ordialer application 2020 can prompt a user ofmobile platform 2002 of recently-predicted volume settings and/or apply the recently-predicted volume settings, perhaps based on settings ofsmart volume setting 2026 andprediction usage setting 2028. - In
scenario 2000, built-infeatures 2040 sends communication 2094 a tolearning model 2030, where communication 2094 a includes a Push( ) function call to informlearning model 2030 thatmobile platform 2002 is at “Work” at 12:57 PM (1257), with a calendar event of “Meeting1PM”, and no other local mobile platforms present. In response to communication 2094 a,learning model 2030 can apply machine-learning operations to predict volume settings based on the information provided via communication 2094 a.Scenario 2000 continues withlearning model 2030 sending a Notify( ) function call in communication 2094 b todialer application 2020, predicting a ringtone volume setting of 0, a mute setting of YES, and vibration setting of YES. Upon reception of communication 2094 b,mobile platform 2002 and/ordialer application 2020 can prompt a user ofmobile platform 2002 of recently-predicted volume settings and/or apply the predicted volume settings, perhaps based on settings ofsmart volume setting 2026 andprediction usage setting 2028. -
Scenario 2000 concludes withdialer application 2020 call Save( ) function via communication 2096 to request thatlearning model 2030 save a context, and perhaps other data, of a learning session thread in a VolSetModel variable. In some scenarios not shown inFIG. 20B ,learning model 2030 can provide a response to communication 2096, such as a return value to the Save( ) function and/or a communication providing a status of the Save( ) function; e.g., SaveOK or SaveFail. - Example Data Network
-
FIG. 21 showsserver devices 2108, 2110 configured to communicate, vianetwork 2106, withprogrammable devices 2104 a, 2104 b, and 2104 c.Network 2106 may correspond to a LAN, a wide area network (WAN), a corporate intranet, the public Internet, or any other type of network configured to provide a communications path between networked computing devices. Thenetwork 2106 may also correspond to a combination of one or more LANs, WANs, corporate intranets, and/or the public Internet. - Although
FIG. 21 only shows three programmable devices, distributed application architectures may serve tens, hundreds, or thousands of programmable devices. Moreover,programmable devices 2104 a, 2104 b, and 2104 c (or any additional programmable devices) may be any sort of computing device, such as an ordinary laptop computer, desktop computer, network terminal, wireless communication device (e.g., a cell phone or smart phone), and so on. In some embodiments,programmable devices 2104 a, 2104 b, and 2104 c may be dedicated to the design and use of software applications. In other embodiments,programmable devices 2104 a, 2104 b, and 2104 c may be general purpose computers that are configured to perform a number of tasks and need not be dedicated to software development tools. -
Server devices 2108, 2110 can be configured to perform one or more services, as requested by programmable devices 2104 a, 2104 b, and/or 2104 c. For example,server device 2108 and/or 2110 can provide content toprogrammable devices 2104 a-2104 c. The content can include, but is not limited to, web pages, hypertext, scripts, binary data such as compiled software, images, audio, and/or video. The content can include compressed and/or uncompressed content. The content can be encrypted and/or unencrypted. Other types of content are possible as well. - As another example,
server device 2108 and/or 2110 can provideprogrammable devices 2104 a-2104 c with access to software for database, search, computation, graphical, audio, video, World Wide Web/Internet utilization, and/or other functions. Many other examples of server devices are possible as well. - Computing Device Architecture
-
FIG. 22A is a block diagram of a computing device (e.g., system) in accordance with an example embodiment. In particular,computing device 2200 shown inFIG. 22A can be configured to perform one or more functions ofmobile platforms server devices 2108, 2110,network 2106, and/or one or more ofprogrammable devices 2104 a, 2104 b, and 2104 c.Computing device 2200 may include auser interface module 2201, a networkcommunications interface module 2202, one ormore processors 2203, anddata storage 2204, all of which may be physically and/or communicatively linked together via a system bus, network, orother connection mechanism 2205. - Generally,
user interface module 2201 is configured to send data to and/or receive data from external user input/output devices. For example,user interface module 2201 can be configured to send and/or receive data to and/or from user input devices such as a keyboard, a keypad, a touch screen, a computer mouse, a track ball, a joystick, a camera, a voice recognition module, and/or other similar devices.User interface module 2201 can also be configured to provide output to user display devices, such as one or more cathode ray tubes (CRT), liquid crystal displays (LCD), light emitting diodes (LEDs), displays using digital light processing (DLP) technology, printers, light bulbs, and/or other similar devices, either now known or later developed.User interface module 2201 can also be configured generate audible output(s) through device(s) such as a speaker, speaker jack, audio output port, audio output device, earphones, telephone ringers, and/or other similar devices. In some embodiments, user interface module 2210 can be configured to provide haptic and/or tactile feedback using one or more vibration devices, tactile sensors, actuators including haptic actuators, tactile touchpads, piezo-haptic devices, piezo-haptic drivers, and/or other similar devices. - Network
communications interface module 2202 can include one ormore wireless interfaces 2207 and/or one ormore wireline interfaces 2208 that are configurable to communicate via a network, such asnetwork 2106 shown inFIG. 21 .Wireless interfaces 2207 can include one or more wireless transmitters, receivers, and/or transceivers, such as a Bluetooth transceiver, a Zigbee transceiver, a Wi-Fi transceiver, a WiMAX transceiver, and/or other similar type of wireless transceiver configurable to communicate via a wireless network.Wireline interfaces 2208 can include one or more wireline transmitters, receivers, and/or transceivers, such as an Ethernet transceiver, a Universal Serial Bus (USB) transceiver, or similar transceiver configurable to communicate via a twisted pair wire, a coaxial cable, a fiber-optic link, or a similar physical connection to a wireline network. - In some embodiments, network
communications interface module 2202 can be configured to provide reliable, secured, and/or authenticated communications. For each communication described herein, information for ensuring reliable communications (i.e., guaranteed message delivery) can be provided, perhaps as part of a message header and/or footer (e.g., packet/message sequencing information, encapsulation header(s) and/or footer(s), size/time information, and transmission verification information such as CRC and/or parity check values). Communications can be made secure (e.g., be encoded or encrypted) and/or decrypted/decoded using one or more cryptographic protocols and/or algorithms, such as, but not limited to, DES, AES, RSA, Diffie-Hellman, and/or DSA. Other cryptographic protocols and/or algorithms can be used as well or in addition to those listed herein to secure (and then decrypt/decode) communications. -
Processors 2203 can include one or more general purpose processors and/or one or more special purpose processors (e.g., digital signal processors, application specific integrated circuits, etc.).Processors 2203 can be configured to execute computer-readable program instructions 2206 that are contained in thedata storage 2204 and/or other instructions as described herein. -
Data storage 2204 can include one or more computer-readable storage media that can be read and/or accessed by at least one ofprocessors 2203. The one or more computer-readable storage media can include volatile and/or non-volatile storage components, such as optical, magnetic, organic or other memory or disc storage, which can be integrated in whole or in part with at least one ofprocessors 2203. In some embodiments,data storage 2204 can be implemented using a single physical device (e.g., one optical, magnetic, organic or other memory or disc storage unit), while in other embodiments,data storage 2204 can be implemented using two or more physical devices. -
Data storage 2204 can include computer-readable program instructions 2206 and perhaps additional data. In some embodiments,data storage 2204 can additionally include storage required to perform at least part of the herein-described methods and techniques and/or at least part of the functionality of the herein-described devices and networks. - Cloud-Based Servers
-
FIG. 22B depicts anetwork 2106 ofcomputing clusters Server devices 2108 and/or 2110 ofFIG. 21 can be cloud-based devices that store program logic and/or data of cloud-based applications and/or services. In some embodiments,server devices 2108 and/or 2110 can be a single computing device residing in a single computing center. In other embodiments,server device 2108 and/or 2110 can include multiple computing devices in a single computing center, or even multiple computing devices located in multiple computing centers located in diverse geographic locations. For example,FIG. 21 depicts each ofserver devices 2108 and 2110 residing in different physical locations. - In some embodiments, data and services at
server devices 2108 and/or 2110 can be encoded as computer readable information stored in non-transitory, tangible computer readable media (or computer readable storage media) and accessible byprogrammable devices 2104 a, 2104 b, and 2104 c, and/or other computing devices. In some embodiments, data atserver device 2108 and/or 2110 can be stored on a single disk drive or other tangible storage media, or can be implemented on multiple disk drives or other tangible storage media located at one or more diverse geographic locations. -
FIG. 22B depicts a cloud-based server system in accordance with an example embodiment. InFIG. 22B , the functions ofserver device 2108 and/or 2110 can be distributed among threecomputing clusters Computing cluster 2209 a can include one ormore computing devices 2200 a, cluster storage arrays 2210 a, andcluster routers 2211 a connected by alocal cluster network 2212 a. Similarly,computing cluster 2209 b can include one ormore computing devices 2200 b, cluster storage arrays 2210 b, and cluster routers 2211 b connected by alocal cluster network 2212 b. Likewise,computing cluster 2209 c can include one ormore computing devices 2200 c,cluster storage arrays 2210 c, andcluster routers 2211 c connected by alocal cluster network 2212 c. - In some embodiments, each of the
computing clusters - In
computing cluster 2209 a, for example,computing devices 2200 a can be configured to perform various computing tasks ofserver device 2108. In one embodiment, the various functionalities ofserver device 2108 can be distributed among one or more ofcomputing devices Computing devices computing clusters computing devices 2200 a incomputing cluster 2209 a. On the other hand, in some embodiments,computing devices - In some embodiments, computing tasks and stored data associated with
server devices 2108 and/or 2110 can be distributed acrosscomputing devices server devices 2108 and/or 2110, the processing capabilities ofcomputing devices - The
cluster storage arrays 2210 a, 2210 b, and 2210 c of thecomputing clusters - Similar to the manner in which the functions of
server devices 2108 and/or 2110 can be distributed acrosscomputing devices computing clusters cluster storage arrays 2210 a, 2210 b, and 2210 c. For example, some cluster storage arrays can be configured to store the data ofserver device 2108, while other cluster storage arrays can store data of server device 2110. Additionally, some cluster storage arrays can be configured to store backup versions of data stored in other cluster storage arrays. - The
cluster routers computing clusters cluster routers 2211 a incomputing cluster 2209 a can include one or more internet switching and routing devices configured to provide (i) local area network communications between thecomputing devices 2200 a and the cluster storage arrays 2201 a via thelocal cluster network 2212 a, and (ii) wide area network communications between thecomputing cluster 2209 a and thecomputing clusters network 2106.Cluster routers 2211 b and 2211 c can include network equipment similar to thecluster routers 2211 a, andcluster routers 2211 b and 2211 c can perform similar networking functions forcomputing clusters cluster routers 2211 a perform forcomputing cluster 2209 a. - In some embodiments, the configuration of the
cluster routers cluster routers local networks area network links - The above detailed description describes various features and functions of the disclosed systems, devices, and methods with reference to the accompanying figures. In the figures, similar symbols typically identify similar components, unless context dictates otherwise. The illustrative embodiments described in the detailed description, figures, and claims are not meant to be limiting. Other embodiments can be utilized, and other changes can be made, without departing from the spirit or scope of the subject matter presented herein. It will be readily understood that the aspects of the present disclosure, as generally described herein, and illustrated in the figures, can be arranged, substituted, combined, separated, and designed in a wide variety of different configurations, all of which are explicitly contemplated herein.
- With respect to any or all of the ladder diagrams, scenarios, and flow charts in the figures and as discussed herein, each block and/or communication may represent a processing of information and/or a transmission of information in accordance with example embodiments. Alternative embodiments are included within the scope of these example embodiments. In these alternative embodiments, for example, functions described as blocks, transmissions, communications, requests, responses, and/or messages may be executed out of order from that shown or discussed, including substantially concurrent or in reverse order, depending on the functionality involved. Further, more or fewer blocks and/or functions may be used with any of the ladder diagrams, scenarios, and flow charts discussed herein, and these ladder diagrams, scenarios, and flow charts may be combined with one another, in part or in whole.
- A block that represents a processing of information may correspond to circuitry that can be configured to perform the specific logical functions of a herein-described method or technique. Alternatively or additionally, a block that represents a processing of information may correspond to a module, a segment, or a portion of program code (including related data). The program code may include one or more instructions executable by a processor for implementing specific logical functions or actions in the method or technique. The program code and/or related data may be stored on any type of computer readable medium such as a storage device including a disk or hard drive or other storage medium.
- The computer readable medium may also include non-transitory computer readable media such as computer-readable media that stores data for short periods of time like register memory, processor cache, and random access memory (RAM). The computer readable media may also include non-transitory computer readable media that stores program code and/or data for longer periods of time, such as secondary or persistent long term storage, like read only memory (ROM), optical or magnetic disks, compact-disc read only memory (CD-ROM), for example. The computer readable media may also be any other volatile or non-volatile storage systems. A computer readable medium may be considered a computer readable storage medium, for example, or a tangible storage device.
- Moreover, a block that represents one or more information transmissions may correspond to information transmissions between software and/or hardware modules in the same physical device. However, other information transmissions may be between software modules and/or hardware modules in different physical devices.
- While various aspects and embodiments have been disclosed herein, other aspects and embodiments will be apparent to those skilled in the art. The various aspects and embodiments disclosed herein are for purposes of illustration and are not intended to be limiting, with the true scope and spirit being indicated by the following claims.
Claims (20)
1. A method, comprising:
receiving data at a context-identification system executing on a mobile platform, wherein the received data comprises: context-related data associated with the mobile platform and application-related data received from the mobile platform;
identifying, by the context identification system, at least one context using the context-related data associated with the mobile platform and/or the application-related data received from the mobile platform, wherein the at least one context is indicative of a change in location from a first location to a second location;
based on the at least one context indicative of the change in location from the first location to the second location, predicting at least one communicative action associated with the mobile platform, wherein the at least one communicative action comprises a communicative action involving a phone number;
performing a machine-learning operation on the received data, wherein performing the machine-learning operation on the received data comprises ranking the phone number amongst other phone numbers associated with the mobile platform, and wherein the ranking is based at least on the change in location from the first location to the second location; and
receiving an instruction to execute the at least one communicative action associated with the mobile platform.
2. The method of claim 1 , wherein the communicative action involving the phone number comprises at least one of: dialing the phone number using the mobile platform, sending a message to the phone number using the mobile platform, opening a contact associated with the phone number on the mobile platform, and executing an application associated with the phone number on the mobile platform.
3. The method of claim 2 , wherein the application-related data received from the mobile platform includes one or more of: a dialing indication, a received call indication, a missed call indication, one or more digits of the phone number, a messaging indication, text of a message, a contact indication of a contact, data related to the contact, and an application indication of an application to be executed on the mobile platform.
4. The method of claim 2 , wherein the communicative action involving the phone number includes an instruction to execute dialing the phone number using a phone dialing application, and wherein the instruction to execute dialing the phone number is based on the application-related data received from the mobile platform.
5. The method of claim 1 , wherein the context-related data associated with the mobile platform corresponds to one or more context signals comprising one or more of the following: (a) a current time, (b) a current date, (c) a current day of the week, (d) a current month, (e) a current season, (f) a time of a future event or future context, (g) a date of a future event or future context, (h) a day of the week of a future event or future context, (i) a month of a future event or future user-context, (j) a season of a future event or future context, (k) a time of a past event or past context, (l) a date of a past event or past context, (m) a day of the week of a past event or past context, (n) a month of a past event or past context, (o) a season of a past event or past context, (p) ambient temperature, (q) a current, future, or past weather forecast at a current location, (r) a current, future, or past weather forecast at a location of a planned event, (s) a current, future, or past weather forecast at or near a location of a previous event, (t) information on a calendar associated with a user-profile, (u) information accessible via a user's social networking account, (v) noise level or any recognizable sounds detected by a device, (w) devices that are currently available to communicate with the mobile platform, (x) devices in proximity to the mobile platform, (y) devices that are available to receive instruction from the mobile platform, (z) information derived from cross-referencing at least one of: information on the user's calendar, information sent to the user, and/or information available via the user's social networking account, (aa) health statistics or characterizations of the user's current health, (bb) a user's recent context as determined from sensors on or near the user and/or other sources of context information associated with the mobile platform, (cc) a current location of the mobile platform, (dd) a past location of the mobile platform, and (ee) a future location of the mobile platform.
6. The method of claim 5 , wherein performing the machine-learning operation associated with the at least one feature comprises at least one of the following:
clustering the one or more context signals, classifying the one or more context signals, and predicting a future context signal based on the one or more context signals.
7. The method of claim 5 , wherein the mobile platform is configured to communicate with one or more other devices to obtain the one or more context signals.
8. An article of manufacture including a non-transitory computer-readable storage medium having instructions stored thereon that, when executed by a processor, cause the processor to perform functions comprising:
receiving data at a context-identification system executing on a mobile platform, wherein the received data comprises: context-related data associated with the mobile platform and application-related data received from the mobile platform;
identifying, by the context-identification system, at least one context using the context-related data associated with the mobile platform and/or the application-related data received from the mobile platform, wherein the at least one context is indicative of a change in location from a first location to a second location;
based on the at least one context indicative of the change in location from the first location to the second location, predicting at least one communicative action associated with the mobile platform, wherein the at least one communicative action comprises a communicative action involving a phone number;
performing a machine-learning operation on the received data, wherein performing the machine-learning operation on the received data comprises ranking the phone number amongst other phone numbers associated with the mobile platform, and wherein the ranking is based at least on the change in location from the first location to the second location; and
receiving an instruction to execute the at least one communicative action associated with the mobile platform.
9. The article of manufacture of claim 8 , wherein the communicative action involving the phone number comprises at least one of: dialing the phone number using the mobile platform, sending a message to the phone number using the mobile platform, opening a contact associated with the phone number on the mobile platform, and executing an application associated with the phone number on the mobile platform.
10. The article of manufacture of claim 9 , wherein the application-related data received from the mobile platform includes one or more of: a dialing indication, a received call indication, a missed call indication, one or more digits of the phone number, a messaging indication, text of a message, a contact indication of a contact, data related to the contact, and an application indication of an application to be executed on the mobile platform.
11. The article of manufacture of claim 9 , wherein the communicative action involving the phone number includes an instruction to execute dialing the phone number using a phone dialing application, and wherein the instruction to execute dialing the phone number is based on the application-related data received from the mobile platform.
12. The article of manufacture of claim 8 , wherein the context-related data associated with the mobile platform corresponds to one or more context signals comprising one or more of the following: (a) a current time, (b) a current date, (c) a current day of the week, (d) a current month, (e) a current season, (f) a time of a future event or future context, (g) a date of a future event or future context, (h) a day of the week of a future event or future context, (i) a month of a future event or future user-context, (j) a season of a future event or future context, (k) a time of a past event or past context, (l) a date of a past event or past context, (m) a day of the week of a past event or past context, (n) a month of a past event or past context, (o) a season of a past event or past context, (p) ambient temperature, (q) a current, future, or past weather forecast at a current location, (r) a current, future, or past weather forecast at a location of a planned event, (s) a current, future, or past weather forecast at or near a location of a previous event, (t) information on a calendar associated with a user-profile, (u) information accessible via a user's social networking account, (v) noise level or any recognizable sounds detected by a device, (w) devices that are currently available to communicate with the mobile platform, (x) devices in proximity to the mobile platform, (y) devices that are available to receive instruction from the mobile platform, (z) information derived from cross-referencing at least one of: information on the user's calendar, information sent to the user, and/or information available via the user's social networking account, (aa) health statistics or characterizations of the user's current health, (bb) a user's recent context as determined from sensors on or near the user and/or other sources of context information associated with the mobile platform, (cc) a current location of the mobile platform, (dd) a past location of the mobile platform, and (ee) a future location of the mobile platform.
13. The article of manufacture of claim 12 , wherein performing the machine-learning operation associated with the at least one feature comprises at least one of the following: clustering the one or more context signals, classifying the one or more context signals, and predicting a future context signal based on the one or more context signals.
14. The article of manufacture of claim 12 , wherein the mobile platform is configured to communicate with one or more other devices to obtain the one or more context signals.
15. A mobile platform, comprising:
a processor; and
a non-transitory computer-readable storage medium, configured to store instructions that, when executed by the processor, cause the mobile platform to perform functions comprising:
receiving data at a context-identification system executing on a mobile platform, wherein the received data comprises: context-related data associated with the mobile platform and application-related data received from the mobile platform;
identifying, by the context-identification system, at least one context using the context-related data associated with the mobile platform and/or the application-related data received from the mobile platform, wherein the at least one context is indicative of a change in location from a first location to a second location;
based on the at least one context indicative of the change in location from the first location to the second location, predicting at least one communicative action associated with the mobile platform, wherein the at least one communicative action comprises a communicative action involving a phone number;
performing a machine-learning operation on the received data, wherein performing the machine-learning operation on the received data comprises ranking the phone number amongst other phone numbers associated with the mobile platform, and wherein the ranking is based at least on the change in location from the first location to the second location; and
receiving an instruction to execute the at least one communicative action associated with the mobile platform.
16. The mobile platform of claim 15 , wherein the communicative action involving the phone number comprises at least one of: dialing the phone number using the mobile platform, sending a message to the phone number using the mobile platform, opening a contact associated with the phone number on the mobile platform, and executing an application associated with the phone number on the mobile platform.
17. The mobile platform of claim 16 , wherein the application-related data received from the mobile platform includes one or more of: a dialing indication, a received call indication, a missed call indication, one or more digits of the phone number, a messaging indication, text of a message, a contact indication of a contact, data related to the contact, and an application indication of an application to be executed on the mobile platform.
18. The mobile platform of claim 16 , wherein the communicative action involving the phone number includes an instruction to execute dialing the phone number using a phone dialing application, and wherein the instruction to execute dialing the phone number is based on the application-related data received from the mobile platform.
19. The mobile platform of claim 15 , wherein the context-related data associated with the mobile platform corresponds to one or more context signals comprising one or more of the following: (a) a current time, (b) a current date, (c) a current day of the week, (d) a current month, (e) a current season, (f) a time of a future event or future context, (g) a date of a future event or future context, (h) a day of the week of a future event or future context, (i) a month of a future event or future user-context, (j) a season of a future event or future context, (k) a time of a past event or past context, (l) a date of a past event or past context, (m) a day of the week of a past event or past context, (n) a month of a past event or past context, (o) a season of a past event or past context, (p) ambient temperature, (q) a current, future, or past weather forecast at a current location, (r) a current, future, or past weather forecast at a location of a planned event, (s) a current, future, or past weather forecast at or near a location of a previous event, (t) information on a calendar associated with a user-profile, (u) information accessible via a user's social networking account, (v) noise level or any recognizable sounds detected by a device, (w) devices that are currently available to communicate with the mobile platform, (x) devices in proximity to the mobile platform, (y) devices that are available to receive instruction from the mobile platform, (z) information derived from cross-referencing at least one of: information on the user's calendar, information sent to the user, and/or information available via the user's social networking account, (aa) health statistics or characterizations of the user's current health, (bb) a user's recent context as determined from sensors on or near the user and/or other sources of context information associated with the mobile platform, (cc) a current location of the mobile platform, (dd) a past location of the mobile platform, and (ee) a future location of the mobile platform.
20. The mobile platform of claim 19 , wherein performing the machine-learning operation associated with the at least one feature comprises at least one of the following: clustering the one or more context signals, classifying the one or more context signals, and predicting a future context signal based on the one or more context signals.
Priority Applications (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/568,957 US20130346347A1 (en) | 2012-06-22 | 2012-08-07 | Method to Predict a Communicative Action that is Most Likely to be Executed Given a Context |
PCT/US2013/046857 WO2013192433A1 (en) | 2012-06-22 | 2013-06-20 | Method to predict a communicative action that is most likely to be executed given a context |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201261663438P | 2012-06-22 | 2012-06-22 | |
US13/568,957 US20130346347A1 (en) | 2012-06-22 | 2012-08-07 | Method to Predict a Communicative Action that is Most Likely to be Executed Given a Context |
Publications (1)
Publication Number | Publication Date |
---|---|
US20130346347A1 true US20130346347A1 (en) | 2013-12-26 |
Family
ID=48747760
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/568,957 Abandoned US20130346347A1 (en) | 2012-06-22 | 2012-08-07 | Method to Predict a Communicative Action that is Most Likely to be Executed Given a Context |
Country Status (2)
Country | Link |
---|---|
US (1) | US20130346347A1 (en) |
WO (1) | WO2013192433A1 (en) |
Cited By (208)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20120062766A1 (en) * | 2010-09-15 | 2012-03-15 | Samsung Electronics Co., Ltd. | Apparatus and method for managing image data |
US20140095606A1 (en) * | 2012-10-01 | 2014-04-03 | Jonathan Arie Matus | Mobile Device-Related Measures of Affinity |
US20140274022A1 (en) * | 2013-03-15 | 2014-09-18 | Factual, Inc. | Apparatus, systems, and methods for analyzing movements of target entities |
US20150017965A1 (en) * | 2013-07-10 | 2015-01-15 | Lg Electronics Inc. | Mobile terminal and controlling method thereof |
US20150040071A1 (en) * | 2013-07-30 | 2015-02-05 | International Business Machines Corporation | Displaying schedule items on a device |
US20150106737A1 (en) * | 2013-10-14 | 2015-04-16 | Yahoo! Inc. | Systems and methods for providing context-based user interface |
US9032321B1 (en) * | 2014-06-16 | 2015-05-12 | Google Inc. | Context-based presentation of a user interface |
US20150302013A1 (en) * | 2014-04-21 | 2015-10-22 | Samsung Electronics Co., Ltd. | Semantic labeling apparatus and method thereof |
US9213941B2 (en) * | 2014-04-22 | 2015-12-15 | Google Inc. | Automatic actions based on contextual replies |
US20150379423A1 (en) * | 2014-06-30 | 2015-12-31 | Amazon Technologies, Inc. | Feature processing recipes for machine learning |
US20160034154A1 (en) * | 2014-07-31 | 2016-02-04 | Microsoft Technology Licensing, Llc | Application Launcher Sizing |
US20160096270A1 (en) * | 2014-10-02 | 2016-04-07 | Brain Corporation | Feature detection apparatus and methods for training of robotic navigation |
US20160150096A1 (en) * | 2014-05-19 | 2016-05-26 | Mediatek Inc. | Method and mobile communication device for changing setting of mobile communication device |
US9463571B2 (en) | 2013-11-01 | 2016-10-11 | Brian Corporation | Apparatus and methods for online training of robots |
US20160299787A1 (en) * | 2015-04-08 | 2016-10-13 | Fujitsu Limited | System, method and managing device |
US20160350136A1 (en) * | 2015-05-27 | 2016-12-01 | Google Inc. | Assist layer with automated extraction |
US20170031575A1 (en) * | 2015-07-28 | 2017-02-02 | Microsoft Technology Licensing, Llc | Tailored computing experience based on contextual signals |
US9566710B2 (en) | 2011-06-02 | 2017-02-14 | Brain Corporation | Apparatus and methods for operating robotic devices using selective state space training |
US9579789B2 (en) | 2013-09-27 | 2017-02-28 | Brain Corporation | Apparatus and methods for training of robotic control arbitration |
US9672474B2 (en) | 2014-06-30 | 2017-06-06 | Amazon Technologies, Inc. | Concurrent binning of machine learning data |
US9704205B2 (en) | 2014-02-28 | 2017-07-11 | Christine E. Akutagawa | Device for implementing body fluid analysis and social networking event planning |
US9717387B1 (en) | 2015-02-26 | 2017-08-01 | Brain Corporation | Apparatus and methods for programming and training of robotic household appliances |
US20170228107A1 (en) * | 2016-02-05 | 2017-08-10 | Airwatch Llc | Generating predictive action buttons within a graphical user interface |
US9764468B2 (en) | 2013-03-15 | 2017-09-19 | Brain Corporation | Adaptive predictor apparatus and methods |
US20170277810A1 (en) * | 2016-03-28 | 2017-09-28 | Microsoft Technology Licensing, Llc | People Relevance Platform |
US9792546B2 (en) | 2013-06-14 | 2017-10-17 | Brain Corporation | Hierarchical robotic controller apparatus and methods |
US9789605B2 (en) | 2014-02-03 | 2017-10-17 | Brain Corporation | Apparatus and methods for control of robot actions based on corrective user inputs |
US9821457B1 (en) | 2013-05-31 | 2017-11-21 | Brain Corporation | Adaptive robotic interface apparatus and methods |
WO2017213681A1 (en) * | 2016-06-09 | 2017-12-14 | Apple Inc. | Intelligent automated assistant in a home environment |
US9844873B2 (en) | 2013-11-01 | 2017-12-19 | Brain Corporation | Apparatus and methods for haptic training of robots |
US9865248B2 (en) | 2008-04-05 | 2018-01-09 | Apple Inc. | Intelligent text-to-speech conversion |
US9950426B2 (en) | 2013-06-14 | 2018-04-24 | Brain Corporation | Predictive robotic controller apparatus and methods |
US9966060B2 (en) | 2013-06-07 | 2018-05-08 | Apple Inc. | System and method for user-specified pronunciation of words for speech synthesis and recognition |
US9986419B2 (en) | 2014-09-30 | 2018-05-29 | Apple Inc. | Social reminders |
US9987752B2 (en) | 2016-06-10 | 2018-06-05 | Brain Corporation | Systems and methods for automatic detection of spills |
US10001780B2 (en) | 2016-11-02 | 2018-06-19 | Brain Corporation | Systems and methods for dynamic route planning in autonomous navigation |
US20180191647A1 (en) * | 2016-12-30 | 2018-07-05 | Getgo, Inc. | Real-time communications system with intelligent presence indication |
US10016896B2 (en) | 2016-06-30 | 2018-07-10 | Brain Corporation | Systems and methods for robotic behavior around moving bodies |
US10043516B2 (en) | 2016-09-23 | 2018-08-07 | Apple Inc. | Intelligent automated assistant |
US10049675B2 (en) | 2010-02-25 | 2018-08-14 | Apple Inc. | User profiling for voice input processing |
US10079014B2 (en) | 2012-06-08 | 2018-09-18 | Apple Inc. | Name recognition system |
US10083690B2 (en) | 2014-05-30 | 2018-09-25 | Apple Inc. | Better resolution when referencing to concepts |
US10102480B2 (en) | 2014-06-30 | 2018-10-16 | Amazon Technologies, Inc. | Machine learning service |
US10108612B2 (en) | 2008-07-31 | 2018-10-23 | Apple Inc. | Mobile device having human language translation capability with positional feedback |
US10120896B2 (en) * | 2014-02-18 | 2018-11-06 | International Business Machines Corporation | Synchronizing data-sets |
US10169715B2 (en) | 2014-06-30 | 2019-01-01 | Amazon Technologies, Inc. | Feature processing tradeoff management |
US10241514B2 (en) | 2016-05-11 | 2019-03-26 | Brain Corporation | Systems and methods for initializing a robot to autonomously travel a trained route |
US10257275B1 (en) | 2015-10-26 | 2019-04-09 | Amazon Technologies, Inc. | Tuning software execution environments using Bayesian models |
US10274325B2 (en) | 2016-11-01 | 2019-04-30 | Brain Corporation | Systems and methods for robotic mapping |
US10282849B2 (en) | 2016-06-17 | 2019-05-07 | Brain Corporation | Systems and methods for predictive/reconstructive visual object tracker |
US10293485B2 (en) | 2017-03-30 | 2019-05-21 | Brain Corporation | Systems and methods for robotic path planning |
US10303715B2 (en) | 2017-05-16 | 2019-05-28 | Apple Inc. | Intelligent automated assistant for media exploration |
US10311871B2 (en) | 2015-03-08 | 2019-06-04 | Apple Inc. | Competing devices responding to voice triggers |
US10311144B2 (en) | 2017-05-16 | 2019-06-04 | Apple Inc. | Emoji word sense disambiguation |
US10318882B2 (en) | 2014-09-11 | 2019-06-11 | Amazon Technologies, Inc. | Optimized training of linear machine learning models |
US20190188552A1 (en) * | 2017-12-15 | 2019-06-20 | International Business Machines Corporation | Communication model for cognitive systems |
US10332518B2 (en) | 2017-05-09 | 2019-06-25 | Apple Inc. | User interface for correcting recognition errors |
WO2019125082A1 (en) | 2017-12-22 | 2019-06-27 | Samsung Electronics Co., Ltd. | Device and method for recommending contact information |
US10339465B2 (en) | 2014-06-30 | 2019-07-02 | Amazon Technologies, Inc. | Optimized decision tree based models |
KR20190076870A (en) * | 2017-12-22 | 2019-07-02 | 삼성전자주식회사 | Device and method for recommeding contact information |
US10345986B1 (en) * | 2016-05-17 | 2019-07-09 | Google Llc | Information cycling in graphical notifications |
US10356243B2 (en) | 2015-06-05 | 2019-07-16 | Apple Inc. | Virtual assistant aided communication with 3rd party service in a communication session |
US10354652B2 (en) | 2015-12-02 | 2019-07-16 | Apple Inc. | Applying neural network language models to weighted finite state transducers for automatic speech recognition |
US20190222655A1 (en) * | 2018-01-15 | 2019-07-18 | Korea Advanced Institute Of Science And Technology | Spatio-cohesive service discovery and dynamic service handover for distributed iot environments |
US10377040B2 (en) | 2017-02-02 | 2019-08-13 | Brain Corporation | Systems and methods for assisting a robotic apparatus |
US10381016B2 (en) | 2008-01-03 | 2019-08-13 | Apple Inc. | Methods and apparatus for altering audio output signals |
US10395654B2 (en) | 2017-05-11 | 2019-08-27 | Apple Inc. | Text normalization based on a data-driven learning network |
US10403283B1 (en) | 2018-06-01 | 2019-09-03 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US10403278B2 (en) | 2017-05-16 | 2019-09-03 | Apple Inc. | Methods and systems for phonetic matching in digital assistant services |
US10410637B2 (en) | 2017-05-12 | 2019-09-10 | Apple Inc. | User-specific acoustic models |
US10417344B2 (en) | 2014-05-30 | 2019-09-17 | Apple Inc. | Exemplar-based natural language processing |
US10417266B2 (en) | 2017-05-09 | 2019-09-17 | Apple Inc. | Context-aware ranking of intelligent response suggestions |
US10417405B2 (en) | 2011-03-21 | 2019-09-17 | Apple Inc. | Device access using voice authentication |
CN110300052A (en) * | 2019-06-26 | 2019-10-01 | 努比亚技术有限公司 | A kind of instant messaging state identification method, equipment and computer readable storage medium |
US10431204B2 (en) | 2014-09-11 | 2019-10-01 | Apple Inc. | Method and apparatus for discovering trending terms in speech requests |
US10438595B2 (en) | 2014-09-30 | 2019-10-08 | Apple Inc. | Speaker identification and unsupervised speaker adaptation techniques |
US10445429B2 (en) | 2017-09-21 | 2019-10-15 | Apple Inc. | Natural language understanding using vocabularies with compressed serialized tries |
US10452992B2 (en) | 2014-06-30 | 2019-10-22 | Amazon Technologies, Inc. | Interactive interfaces for machine learning model evaluations |
US10453443B2 (en) | 2014-09-30 | 2019-10-22 | Apple Inc. | Providing an indication of the suitability of speech recognition |
US10474753B2 (en) | 2016-09-07 | 2019-11-12 | Apple Inc. | Language identification using recurrent neural networks |
US10482874B2 (en) | 2017-05-15 | 2019-11-19 | Apple Inc. | Hierarchical belief states for digital assistants |
US10489032B1 (en) | 2015-07-29 | 2019-11-26 | Google Llc | Rich structured data interchange for copy-paste operations |
US10496705B1 (en) | 2018-06-03 | 2019-12-03 | Apple Inc. | Accelerated task performance |
US10497365B2 (en) | 2014-05-30 | 2019-12-03 | Apple Inc. | Multi-command single utterance input method |
US10521070B2 (en) | 2015-10-23 | 2019-12-31 | Oath Inc. | Method to automatically update a homescreen |
US10529332B2 (en) | 2015-03-08 | 2020-01-07 | Apple Inc. | Virtual assistant activation |
US10540606B2 (en) | 2014-06-30 | 2020-01-21 | Amazon Technologies, Inc. | Consistent filtering of machine learning data |
US10567477B2 (en) | 2015-03-08 | 2020-02-18 | Apple Inc. | Virtual assistant continuity |
US10580409B2 (en) | 2016-06-11 | 2020-03-03 | Apple Inc. | Application integration with a digital assistant |
US10592604B2 (en) | 2018-03-12 | 2020-03-17 | Apple Inc. | Inverse text normalization for automatic speech recognition |
US10593346B2 (en) | 2016-12-22 | 2020-03-17 | Apple Inc. | Rank-reduced token representation for automatic speech recognition |
US10636424B2 (en) | 2017-11-30 | 2020-04-28 | Apple Inc. | Multi-turn canned dialog |
US10643611B2 (en) | 2008-10-02 | 2020-05-05 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US10657328B2 (en) | 2017-06-02 | 2020-05-19 | Apple Inc. | Multi-task recurrent neural network architecture for efficient morphology handling in neural language modeling |
US10657961B2 (en) | 2013-06-08 | 2020-05-19 | Apple Inc. | Interpreting and acting upon commands that involve sharing information with remote devices |
US10659399B2 (en) | 2017-12-22 | 2020-05-19 | Google Llc | Message analysis using a machine learning model |
US10684703B2 (en) | 2018-06-01 | 2020-06-16 | Apple Inc. | Attention aware virtual assistant dismissal |
US10699717B2 (en) | 2014-05-30 | 2020-06-30 | Apple Inc. | Intelligent assistant for home automation |
US10714117B2 (en) | 2013-02-07 | 2020-07-14 | Apple Inc. | Voice trigger for a digital assistant |
US10723018B2 (en) | 2016-11-28 | 2020-07-28 | Brain Corporation | Systems and methods for remote operating and/or monitoring of a robot |
US10726832B2 (en) | 2017-05-11 | 2020-07-28 | Apple Inc. | Maintaining privacy of personal information |
US10733982B2 (en) | 2018-01-08 | 2020-08-04 | Apple Inc. | Multi-directional dialog |
US10733375B2 (en) | 2018-01-31 | 2020-08-04 | Apple Inc. | Knowledge-based framework for improving natural language understanding |
US10733993B2 (en) | 2016-06-10 | 2020-08-04 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US10741185B2 (en) | 2010-01-18 | 2020-08-11 | Apple Inc. | Intelligent automated assistant |
US10748546B2 (en) | 2017-05-16 | 2020-08-18 | Apple Inc. | Digital assistant services based on device capabilities |
US10755051B2 (en) | 2017-09-29 | 2020-08-25 | Apple Inc. | Rule-based natural language processing |
US10755703B2 (en) | 2017-05-11 | 2020-08-25 | Apple Inc. | Offline personal assistant |
US10769385B2 (en) | 2013-06-09 | 2020-09-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
US10776243B1 (en) | 2019-03-19 | 2020-09-15 | Bank Of America Corporation | Prediction tool |
US10791176B2 (en) | 2017-05-12 | 2020-09-29 | Apple Inc. | Synchronization and task delegation of a digital assistant |
US10788800B2 (en) | 2015-06-05 | 2020-09-29 | Apple Inc. | Data-driven context determination |
US10789959B2 (en) | 2018-03-02 | 2020-09-29 | Apple Inc. | Training speaker recognition models for digital assistants |
US10789945B2 (en) | 2017-05-12 | 2020-09-29 | Apple Inc. | Low-latency intelligent automated assistant |
US10810274B2 (en) | 2017-05-15 | 2020-10-20 | Apple Inc. | Optimizing dialogue policy decisions for digital assistants using implicit feedback |
US10818288B2 (en) | 2018-03-26 | 2020-10-27 | Apple Inc. | Natural assistant interaction |
US10831766B2 (en) | 2015-12-21 | 2020-11-10 | Oath Inc. | Decentralized cards platform for showing contextual cards in a stream |
US10839159B2 (en) | 2018-09-28 | 2020-11-17 | Apple Inc. | Named entity normalization in a spoken dialog system |
US10845949B2 (en) | 2015-09-28 | 2020-11-24 | Oath Inc. | Continuity of experience card for index |
US10846618B2 (en) | 2016-09-23 | 2020-11-24 | Google Llc | Smart replies using an on-device model |
US10852730B2 (en) | 2017-02-08 | 2020-12-01 | Brain Corporation | Systems and methods for robotic mobile platforms |
US10892996B2 (en) | 2018-06-01 | 2021-01-12 | Apple Inc. | Variable latency device coordination |
US10904611B2 (en) | 2014-06-30 | 2021-01-26 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US10909331B2 (en) | 2018-03-30 | 2021-02-02 | Apple Inc. | Implicit identification of translation payload with neural machine translation |
US10928918B2 (en) | 2018-05-07 | 2021-02-23 | Apple Inc. | Raise to speak |
US10942702B2 (en) | 2016-06-11 | 2021-03-09 | Apple Inc. | Intelligent device arbitration and control |
US10942703B2 (en) | 2015-12-23 | 2021-03-09 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10956666B2 (en) | 2015-11-09 | 2021-03-23 | Apple Inc. | Unconventional virtual assistant interactions |
US10963810B2 (en) | 2014-06-30 | 2021-03-30 | Amazon Technologies, Inc. | Efficient duplicate detection for machine learning data sets |
US10969954B2 (en) * | 2017-03-03 | 2021-04-06 | Samsung Electronics Co., Ltd. | Electronic device for processing user input and method for processing user input |
US10984780B2 (en) | 2018-05-21 | 2021-04-20 | Apple Inc. | Global semantic word embeddings using bi-directional recurrent neural networks |
US10999390B2 (en) * | 2019-08-15 | 2021-05-04 | Bank Of America Corporation | Method and system for mobile data communication |
US11010561B2 (en) | 2018-09-27 | 2021-05-18 | Apple Inc. | Sentiment prediction from textual data |
US11010127B2 (en) | 2015-06-29 | 2021-05-18 | Apple Inc. | Virtual assistant for media playback |
US20210157308A1 (en) * | 2017-05-15 | 2021-05-27 | Doosan Heavy Industries & Construction Co., Ltd. | Fault signal recovery system and method |
US11023513B2 (en) | 2007-12-20 | 2021-06-01 | Apple Inc. | Method and apparatus for searching using an active ontology |
US11025565B2 (en) | 2015-06-07 | 2021-06-01 | Apple Inc. | Personalized prediction of responses for instant messaging |
US11030708B2 (en) | 2014-02-28 | 2021-06-08 | Christine E. Akutagawa | Method of and device for implementing contagious illness analysis and tracking |
US11048473B2 (en) | 2013-06-09 | 2021-06-29 | Apple Inc. | Device, method, and graphical user interface for enabling conversation persistence across two or more instances of a digital assistant |
US11070949B2 (en) | 2015-05-27 | 2021-07-20 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on an electronic device with a touch-sensitive display |
US11069336B2 (en) | 2012-03-02 | 2021-07-20 | Apple Inc. | Systems and methods for name pronunciation |
US11069347B2 (en) | 2016-06-08 | 2021-07-20 | Apple Inc. | Intelligent automated assistant for media exploration |
US11080012B2 (en) | 2009-06-05 | 2021-08-03 | Apple Inc. | Interface for a virtual digital assistant |
US20210256222A1 (en) * | 2018-09-06 | 2021-08-19 | Pricewaterhousecoopers Llp | Systems and methods for automatic resolutions of wireless signals |
US11100420B2 (en) | 2014-06-30 | 2021-08-24 | Amazon Technologies, Inc. | Input processing for machine learning |
US11120372B2 (en) | 2011-06-03 | 2021-09-14 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US11126400B2 (en) | 2015-09-08 | 2021-09-21 | Apple Inc. | Zero latency digital assistant |
US11127397B2 (en) | 2015-05-27 | 2021-09-21 | Apple Inc. | Device voice control |
US11132349B2 (en) | 2017-10-05 | 2021-09-28 | Adobe Inc. | Update basis for updating digital content in a digital medium environment |
US11133008B2 (en) | 2014-05-30 | 2021-09-28 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US11140099B2 (en) | 2019-05-21 | 2021-10-05 | Apple Inc. | Providing message response suggestions |
CN113491097A (en) * | 2019-04-26 | 2021-10-08 | 深圳市欢太科技有限公司 | Voice broadcast control method and device, storage medium and electronic equipment |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11170166B2 (en) | 2018-09-28 | 2021-11-09 | Apple Inc. | Neural typographical error modeling via generative adversarial networks |
US11182691B1 (en) | 2014-08-14 | 2021-11-23 | Amazon Technologies, Inc. | Category-based sampling of machine learning data |
US11204787B2 (en) | 2017-01-09 | 2021-12-21 | Apple Inc. | Application integration with a digital assistant |
US11217251B2 (en) | 2019-05-06 | 2022-01-04 | Apple Inc. | Spoken notifications |
US11227589B2 (en) | 2016-06-06 | 2022-01-18 | Apple Inc. | Intelligent list reading |
US11231904B2 (en) | 2015-03-06 | 2022-01-25 | Apple Inc. | Reducing response latency of intelligent automated assistants |
US11237797B2 (en) | 2019-05-31 | 2022-02-01 | Apple Inc. | User activity shortcut suggestions |
US11243747B2 (en) * | 2017-10-16 | 2022-02-08 | Adobe Inc. | Application digital content control using an embedded machine learning module |
US11258847B1 (en) * | 2020-11-02 | 2022-02-22 | Servicenow, Inc. | Assignments of incoming requests to servers in computing clusters and other environments |
US20220060847A1 (en) * | 2018-08-17 | 2022-02-24 | xAd, Inc. | Systems and Methods for Pacing Information Delivery to Mobile Devices |
US11269678B2 (en) | 2012-05-15 | 2022-03-08 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US11281993B2 (en) | 2016-12-05 | 2022-03-22 | Apple Inc. | Model and ensemble compression for metric learning |
US11289073B2 (en) | 2019-05-31 | 2022-03-29 | Apple Inc. | Device text to speech |
US11288299B2 (en) | 2018-04-24 | 2022-03-29 | International Business Machines Corporation | Enhanced action fulfillment using classification valency |
US11301477B2 (en) | 2017-05-12 | 2022-04-12 | Apple Inc. | Feedback analysis of a digital assistant |
US11307752B2 (en) | 2019-05-06 | 2022-04-19 | Apple Inc. | User configurable task triggers |
US11314370B2 (en) | 2013-12-06 | 2022-04-26 | Apple Inc. | Method for extracting salient dialog usage from live data |
US11348573B2 (en) | 2019-03-18 | 2022-05-31 | Apple Inc. | Multimodality in digital assistant systems |
US11350171B2 (en) * | 2019-05-31 | 2022-05-31 | Samsung Electronics Co., Ltd. | Electronic apparatus and controlling method thereof |
US11350253B2 (en) | 2011-06-03 | 2022-05-31 | Apple Inc. | Active transport based notifications |
US11360641B2 (en) | 2019-06-01 | 2022-06-14 | Apple Inc. | Increasing the relevance of new available information |
US11388291B2 (en) | 2013-03-14 | 2022-07-12 | Apple Inc. | System and method for processing voicemail |
US11386266B2 (en) | 2018-06-01 | 2022-07-12 | Apple Inc. | Text correction |
US11423908B2 (en) | 2019-05-06 | 2022-08-23 | Apple Inc. | Interpreting spoken requests |
US11423886B2 (en) | 2010-01-18 | 2022-08-23 | Apple Inc. | Task flow identification based on user intent |
US11462215B2 (en) | 2018-09-28 | 2022-10-04 | Apple Inc. | Multi-modal inputs for voice commands |
US11467802B2 (en) | 2017-05-11 | 2022-10-11 | Apple Inc. | Maintaining privacy of personal information |
US11468282B2 (en) | 2015-05-15 | 2022-10-11 | Apple Inc. | Virtual assistant in a communication session |
US11475884B2 (en) | 2019-05-06 | 2022-10-18 | Apple Inc. | Reducing digital assistant latency when a language is incorrectly determined |
US11475898B2 (en) | 2018-10-26 | 2022-10-18 | Apple Inc. | Low-latency multi-speaker speech recognition |
US11488406B2 (en) | 2019-09-25 | 2022-11-01 | Apple Inc. | Text detection using global geometry estimators |
US11495218B2 (en) | 2018-06-01 | 2022-11-08 | Apple Inc. | Virtual assistant operation in multi-device environments |
US11496600B2 (en) | 2019-05-31 | 2022-11-08 | Apple Inc. | Remote execution of machine-learned models |
US11500672B2 (en) | 2015-09-08 | 2022-11-15 | Apple Inc. | Distributed personal assistant |
US11501101B1 (en) * | 2019-12-16 | 2022-11-15 | NTT DATA Services, LLC | Systems and methods for securing machine learning models |
WO2022256547A1 (en) * | 2021-06-03 | 2022-12-08 | Mocha Holdings Llc | Apparatus and method for suggesting user-relevant digital content using edge computing |
US11526368B2 (en) | 2015-11-06 | 2022-12-13 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US11532306B2 (en) | 2017-05-16 | 2022-12-20 | Apple Inc. | Detecting a trigger of a digital assistant |
US11544743B2 (en) | 2017-10-16 | 2023-01-03 | Adobe Inc. | Digital content control based on shared machine learning properties |
US11551257B2 (en) | 2017-10-12 | 2023-01-10 | Adobe Inc. | Digital media environment for analysis of audience segments in a digital marketing campaign |
US11556354B2 (en) * | 2019-02-01 | 2023-01-17 | Samsung Electronics Co., Ltd. | Context-based user-personalized configuration method and device |
US11638059B2 (en) | 2019-01-04 | 2023-04-25 | Apple Inc. | Content playback on multiple devices |
US11657813B2 (en) | 2019-05-31 | 2023-05-23 | Apple Inc. | Voice identification in digital assistant systems |
US11671920B2 (en) | 2007-04-03 | 2023-06-06 | Apple Inc. | Method and system for operating a multifunction portable electronic device using voice-activation |
US11696060B2 (en) | 2020-07-21 | 2023-07-04 | Apple Inc. | User identification using headphones |
US11755276B2 (en) | 2020-05-12 | 2023-09-12 | Apple Inc. | Reducing description length based on confidence |
US11765209B2 (en) | 2020-05-11 | 2023-09-19 | Apple Inc. | Digital assistant hardware abstraction |
US11790914B2 (en) | 2019-06-01 | 2023-10-17 | Apple Inc. | Methods and user interfaces for voice-based control of electronic devices |
US11798547B2 (en) | 2013-03-15 | 2023-10-24 | Apple Inc. | Voice activated device for use with a voice-based digital assistant |
US11809483B2 (en) | 2015-09-08 | 2023-11-07 | Apple Inc. | Intelligent automated assistant for media search and playback |
US11829239B2 (en) | 2021-11-17 | 2023-11-28 | Adobe Inc. | Managing machine learning model reconstruction |
US11838734B2 (en) | 2020-07-20 | 2023-12-05 | Apple Inc. | Multi-device audio adjustment coordination |
US11853536B2 (en) | 2015-09-08 | 2023-12-26 | Apple Inc. | Intelligent automated assistant in a media environment |
US11914848B2 (en) | 2020-05-11 | 2024-02-27 | Apple Inc. | Providing relevant data items based on context |
US11928604B2 (en) | 2005-09-08 | 2024-03-12 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
Families Citing this family (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP4122159A4 (en) * | 2020-03-17 | 2023-05-10 | Telefonaktiebolaget LM Ericsson (publ) | Methods and systems for charging using management information |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20080176585A1 (en) * | 2007-01-19 | 2008-07-24 | Eldering Charles A | Method of Displaying Contact Information |
US20120059780A1 (en) * | 2009-05-22 | 2012-03-08 | Teknologian Tutkimuskeskus Vtt | Context recognition in mobile devices |
Family Cites Families (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7725419B2 (en) * | 2003-09-05 | 2010-05-25 | Samsung Electronics Co., Ltd | Proactive user interface including emotional agent |
US20050064916A1 (en) * | 2003-09-24 | 2005-03-24 | Interdigital Technology Corporation | User cognitive electronic device |
KR100612859B1 (en) * | 2004-08-26 | 2006-08-14 | 삼성전자주식회사 | System and method for managing conversational user interface with usage pattern |
US7194257B2 (en) * | 2004-12-27 | 2007-03-20 | Sony Ericsson Mobile Communications Ab | Automatic dialing for a wireless communications device |
US7675414B2 (en) * | 2006-08-10 | 2010-03-09 | Qualcomm Incorporated | Methods and apparatus for an environmental and behavioral adaptive wireless communication device |
US8423508B2 (en) * | 2009-12-04 | 2013-04-16 | Qualcomm Incorporated | Apparatus and method of creating and utilizing a context |
US20120135751A1 (en) * | 2010-11-30 | 2012-05-31 | Google Inc. | Use of location tagging in data communications |
-
2012
- 2012-08-07 US US13/568,957 patent/US20130346347A1/en not_active Abandoned
-
2013
- 2013-06-20 WO PCT/US2013/046857 patent/WO2013192433A1/en active Application Filing
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20080176585A1 (en) * | 2007-01-19 | 2008-07-24 | Eldering Charles A | Method of Displaying Contact Information |
US20120059780A1 (en) * | 2009-05-22 | 2012-03-08 | Teknologian Tutkimuskeskus Vtt | Context recognition in mobile devices |
Cited By (341)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11928604B2 (en) | 2005-09-08 | 2024-03-12 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
US11979836B2 (en) | 2007-04-03 | 2024-05-07 | Apple Inc. | Method and system for operating a multi-function portable electronic device using voice-activation |
US11671920B2 (en) | 2007-04-03 | 2023-06-06 | Apple Inc. | Method and system for operating a multifunction portable electronic device using voice-activation |
US11023513B2 (en) | 2007-12-20 | 2021-06-01 | Apple Inc. | Method and apparatus for searching using an active ontology |
US10381016B2 (en) | 2008-01-03 | 2019-08-13 | Apple Inc. | Methods and apparatus for altering audio output signals |
US9865248B2 (en) | 2008-04-05 | 2018-01-09 | Apple Inc. | Intelligent text-to-speech conversion |
US10108612B2 (en) | 2008-07-31 | 2018-10-23 | Apple Inc. | Mobile device having human language translation capability with positional feedback |
US11900936B2 (en) | 2008-10-02 | 2024-02-13 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US10643611B2 (en) | 2008-10-02 | 2020-05-05 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US11348582B2 (en) | 2008-10-02 | 2022-05-31 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
US11080012B2 (en) | 2009-06-05 | 2021-08-03 | Apple Inc. | Interface for a virtual digital assistant |
US11423886B2 (en) | 2010-01-18 | 2022-08-23 | Apple Inc. | Task flow identification based on user intent |
US10741185B2 (en) | 2010-01-18 | 2020-08-11 | Apple Inc. | Intelligent automated assistant |
US10692504B2 (en) | 2010-02-25 | 2020-06-23 | Apple Inc. | User profiling for voice input processing |
US10049675B2 (en) | 2010-02-25 | 2018-08-14 | Apple Inc. | User profiling for voice input processing |
US20120062766A1 (en) * | 2010-09-15 | 2012-03-15 | Samsung Electronics Co., Ltd. | Apparatus and method for managing image data |
US10417405B2 (en) | 2011-03-21 | 2019-09-17 | Apple Inc. | Device access using voice authentication |
US9566710B2 (en) | 2011-06-02 | 2017-02-14 | Brain Corporation | Apparatus and methods for operating robotic devices using selective state space training |
US11350253B2 (en) | 2011-06-03 | 2022-05-31 | Apple Inc. | Active transport based notifications |
US11120372B2 (en) | 2011-06-03 | 2021-09-14 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US11069336B2 (en) | 2012-03-02 | 2021-07-20 | Apple Inc. | Systems and methods for name pronunciation |
US11321116B2 (en) | 2012-05-15 | 2022-05-03 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US11269678B2 (en) | 2012-05-15 | 2022-03-08 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US10079014B2 (en) | 2012-06-08 | 2018-09-18 | Apple Inc. | Name recognition system |
US10257309B2 (en) * | 2012-10-01 | 2019-04-09 | Facebook, Inc. | Mobile device-related measures of affinity |
US20170091645A1 (en) * | 2012-10-01 | 2017-03-30 | Facebook, Inc. | Mobile device-related measures of affinity |
US9654591B2 (en) * | 2012-10-01 | 2017-05-16 | Facebook, Inc. | Mobile device-related measures of affinity |
US20140095606A1 (en) * | 2012-10-01 | 2014-04-03 | Jonathan Arie Matus | Mobile Device-Related Measures of Affinity |
US10714117B2 (en) | 2013-02-07 | 2020-07-14 | Apple Inc. | Voice trigger for a digital assistant |
US11862186B2 (en) | 2013-02-07 | 2024-01-02 | Apple Inc. | Voice trigger for a digital assistant |
US10978090B2 (en) | 2013-02-07 | 2021-04-13 | Apple Inc. | Voice trigger for a digital assistant |
US11636869B2 (en) | 2013-02-07 | 2023-04-25 | Apple Inc. | Voice trigger for a digital assistant |
US11557310B2 (en) | 2013-02-07 | 2023-01-17 | Apple Inc. | Voice trigger for a digital assistant |
US11388291B2 (en) | 2013-03-14 | 2022-07-12 | Apple Inc. | System and method for processing voicemail |
US11461289B2 (en) | 2013-03-15 | 2022-10-04 | Foursquare Labs, Inc. | Apparatus, systems, and methods for providing location information |
US9977792B2 (en) | 2013-03-15 | 2018-05-22 | Factual Inc. | Apparatus, systems, and methods for analyzing movements of target entities |
US9764468B2 (en) | 2013-03-15 | 2017-09-19 | Brain Corporation | Adaptive predictor apparatus and methods |
US9753965B2 (en) | 2013-03-15 | 2017-09-05 | Factual Inc. | Apparatus, systems, and methods for providing location information |
US20140274022A1 (en) * | 2013-03-15 | 2014-09-18 | Factual, Inc. | Apparatus, systems, and methods for analyzing movements of target entities |
US11468019B2 (en) | 2013-03-15 | 2022-10-11 | Foursquare Labs, Inc. | Apparatus, systems, and methods for analyzing characteristics of entities of interest |
US10579600B2 (en) | 2013-03-15 | 2020-03-03 | Factual Inc. | Apparatus, systems, and methods for analyzing movements of target entities |
US10155310B2 (en) | 2013-03-15 | 2018-12-18 | Brain Corporation | Adaptive predictor apparatus and methods |
US10255301B2 (en) | 2013-03-15 | 2019-04-09 | Factual Inc. | Apparatus, systems, and methods for analyzing movements of target entities |
US9594791B2 (en) * | 2013-03-15 | 2017-03-14 | Factual Inc. | Apparatus, systems, and methods for analyzing movements of target entities |
US10891269B2 (en) | 2013-03-15 | 2021-01-12 | Factual, Inc. | Apparatus, systems, and methods for batch and realtime data processing |
US11762818B2 (en) | 2013-03-15 | 2023-09-19 | Foursquare Labs, Inc. | Apparatus, systems, and methods for analyzing movements of target entities |
US10268708B2 (en) | 2013-03-15 | 2019-04-23 | Factual Inc. | System and method for providing sub-polygon based location service |
US10459896B2 (en) | 2013-03-15 | 2019-10-29 | Factual Inc. | Apparatus, systems, and methods for providing location information |
US10866937B2 (en) | 2013-03-15 | 2020-12-15 | Factual Inc. | Apparatus, systems, and methods for analyzing movements of target entities |
US11798547B2 (en) | 2013-03-15 | 2023-10-24 | Apple Inc. | Voice activated device for use with a voice-based digital assistant |
US10331631B2 (en) | 2013-03-15 | 2019-06-25 | Factual Inc. | Apparatus, systems, and methods for analyzing characteristics of entities of interest |
US10013446B2 (en) | 2013-03-15 | 2018-07-03 | Factual Inc. | Apparatus, systems, and methods for providing location information |
US10831725B2 (en) | 2013-03-15 | 2020-11-10 | Factual, Inc. | Apparatus, systems, and methods for grouping data records |
US10817484B2 (en) | 2013-03-15 | 2020-10-27 | Factual Inc. | Apparatus, systems, and methods for providing location information |
US10817482B2 (en) | 2013-03-15 | 2020-10-27 | Factual Inc. | Apparatus, systems, and methods for crowdsourcing domain specific intelligence |
US9821457B1 (en) | 2013-05-31 | 2017-11-21 | Brain Corporation | Adaptive robotic interface apparatus and methods |
US9966060B2 (en) | 2013-06-07 | 2018-05-08 | Apple Inc. | System and method for user-specified pronunciation of words for speech synthesis and recognition |
US10657961B2 (en) | 2013-06-08 | 2020-05-19 | Apple Inc. | Interpreting and acting upon commands that involve sharing information with remote devices |
US11048473B2 (en) | 2013-06-09 | 2021-06-29 | Apple Inc. | Device, method, and graphical user interface for enabling conversation persistence across two or more instances of a digital assistant |
US10769385B2 (en) | 2013-06-09 | 2020-09-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
US11727219B2 (en) | 2013-06-09 | 2023-08-15 | Apple Inc. | System and method for inferring user intent from speech inputs |
US9950426B2 (en) | 2013-06-14 | 2018-04-24 | Brain Corporation | Predictive robotic controller apparatus and methods |
US9792546B2 (en) | 2013-06-14 | 2017-10-17 | Brain Corporation | Hierarchical robotic controller apparatus and methods |
US20150017965A1 (en) * | 2013-07-10 | 2015-01-15 | Lg Electronics Inc. | Mobile terminal and controlling method thereof |
US9549374B2 (en) * | 2013-07-10 | 2017-01-17 | Lg Electronics Inc. | Mobile terminal operating based on use pattern and controlling method thereof |
US20150040071A1 (en) * | 2013-07-30 | 2015-02-05 | International Business Machines Corporation | Displaying schedule items on a device |
US9579789B2 (en) | 2013-09-27 | 2017-02-28 | Brain Corporation | Apparatus and methods for training of robotic control arbitration |
US20150106737A1 (en) * | 2013-10-14 | 2015-04-16 | Yahoo! Inc. | Systems and methods for providing context-based user interface |
US10834546B2 (en) * | 2013-10-14 | 2020-11-10 | Oath Inc. | Systems and methods for providing context-based user interface |
US9844873B2 (en) | 2013-11-01 | 2017-12-19 | Brain Corporation | Apparatus and methods for haptic training of robots |
US9463571B2 (en) | 2013-11-01 | 2016-10-11 | Brian Corporation | Apparatus and methods for online training of robots |
US11314370B2 (en) | 2013-12-06 | 2022-04-26 | Apple Inc. | Method for extracting salient dialog usage from live data |
US10322507B2 (en) | 2014-02-03 | 2019-06-18 | Brain Corporation | Apparatus and methods for control of robot actions based on corrective user inputs |
US9789605B2 (en) | 2014-02-03 | 2017-10-17 | Brain Corporation | Apparatus and methods for control of robot actions based on corrective user inputs |
US11010373B2 (en) | 2014-02-18 | 2021-05-18 | International Business Machines Corporation | Synchronizing data-sets |
US10120896B2 (en) * | 2014-02-18 | 2018-11-06 | International Business Machines Corporation | Synchronizing data-sets |
US11397997B2 (en) | 2014-02-28 | 2022-07-26 | Christine E. Akutagawa | Device for implementing body fluid analysis and social networking event planning |
US11030708B2 (en) | 2014-02-28 | 2021-06-08 | Christine E. Akutagawa | Method of and device for implementing contagious illness analysis and tracking |
US9704205B2 (en) | 2014-02-28 | 2017-07-11 | Christine E. Akutagawa | Device for implementing body fluid analysis and social networking event planning |
US10984486B2 (en) | 2014-02-28 | 2021-04-20 | Christine E. Akutagawa | Device for implementing body fluid analysis and social networking event planning |
CN105023020A (en) * | 2014-04-21 | 2015-11-04 | 三星电子株式会社 | Semantic labeling apparatus and method thereof |
US10762119B2 (en) * | 2014-04-21 | 2020-09-01 | Samsung Electronics Co., Ltd. | Semantic labeling apparatus and method thereof |
KR102216049B1 (en) * | 2014-04-21 | 2021-02-15 | 삼성전자주식회사 | System and method for semantic labeling |
US20150302013A1 (en) * | 2014-04-21 | 2015-10-22 | Samsung Electronics Co., Ltd. | Semantic labeling apparatus and method thereof |
KR20150121596A (en) * | 2014-04-21 | 2015-10-29 | 삼성전자주식회사 | System and method for semantic labeling |
US10552747B2 (en) | 2014-04-22 | 2020-02-04 | Google Llc | Automatic actions based on contextual replies |
US11669752B2 (en) | 2014-04-22 | 2023-06-06 | Google Llc | Automatic actions based on contextual replies |
US9213941B2 (en) * | 2014-04-22 | 2015-12-15 | Google Inc. | Automatic actions based on contextual replies |
US20160150096A1 (en) * | 2014-05-19 | 2016-05-26 | Mediatek Inc. | Method and mobile communication device for changing setting of mobile communication device |
US11133008B2 (en) | 2014-05-30 | 2021-09-28 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US10657966B2 (en) | 2014-05-30 | 2020-05-19 | Apple Inc. | Better resolution when referencing to concepts |
US10699717B2 (en) | 2014-05-30 | 2020-06-30 | Apple Inc. | Intelligent assistant for home automation |
US10714095B2 (en) | 2014-05-30 | 2020-07-14 | Apple Inc. | Intelligent assistant for home automation |
US10083690B2 (en) | 2014-05-30 | 2018-09-25 | Apple Inc. | Better resolution when referencing to concepts |
US10878809B2 (en) | 2014-05-30 | 2020-12-29 | Apple Inc. | Multi-command single utterance input method |
US11257504B2 (en) | 2014-05-30 | 2022-02-22 | Apple Inc. | Intelligent assistant for home automation |
US11670289B2 (en) | 2014-05-30 | 2023-06-06 | Apple Inc. | Multi-command single utterance input method |
US10497365B2 (en) | 2014-05-30 | 2019-12-03 | Apple Inc. | Multi-command single utterance input method |
US10417344B2 (en) | 2014-05-30 | 2019-09-17 | Apple Inc. | Exemplar-based natural language processing |
US11699448B2 (en) | 2014-05-30 | 2023-07-11 | Apple Inc. | Intelligent assistant for home automation |
US11810562B2 (en) | 2014-05-30 | 2023-11-07 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US9032321B1 (en) * | 2014-06-16 | 2015-05-12 | Google Inc. | Context-based presentation of a user interface |
US10339465B2 (en) | 2014-06-30 | 2019-07-02 | Amazon Technologies, Inc. | Optimized decision tree based models |
US9886670B2 (en) * | 2014-06-30 | 2018-02-06 | Amazon Technologies, Inc. | Feature processing recipes for machine learning |
US11544623B2 (en) | 2014-06-30 | 2023-01-03 | Amazon Technologies, Inc. | Consistent filtering of machine learning data |
US10452992B2 (en) | 2014-06-30 | 2019-10-22 | Amazon Technologies, Inc. | Interactive interfaces for machine learning model evaluations |
US11379755B2 (en) | 2014-06-30 | 2022-07-05 | Amazon Technologies, Inc. | Feature processing tradeoff management |
US11100420B2 (en) | 2014-06-30 | 2021-08-24 | Amazon Technologies, Inc. | Input processing for machine learning |
US9672474B2 (en) | 2014-06-30 | 2017-06-06 | Amazon Technologies, Inc. | Concurrent binning of machine learning data |
US11516537B2 (en) | 2014-06-30 | 2022-11-29 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US11838579B2 (en) | 2014-06-30 | 2023-12-05 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US11386351B2 (en) | 2014-06-30 | 2022-07-12 | Amazon Technologies, Inc. | Machine learning service |
US10169715B2 (en) | 2014-06-30 | 2019-01-01 | Amazon Technologies, Inc. | Feature processing tradeoff management |
US10963810B2 (en) | 2014-06-30 | 2021-03-30 | Amazon Technologies, Inc. | Efficient duplicate detection for machine learning data sets |
US10904611B2 (en) | 2014-06-30 | 2021-01-26 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US20150379423A1 (en) * | 2014-06-30 | 2015-12-31 | Amazon Technologies, Inc. | Feature processing recipes for machine learning |
US10540606B2 (en) | 2014-06-30 | 2020-01-21 | Amazon Technologies, Inc. | Consistent filtering of machine learning data |
US10102480B2 (en) | 2014-06-30 | 2018-10-16 | Amazon Technologies, Inc. | Machine learning service |
US20160034154A1 (en) * | 2014-07-31 | 2016-02-04 | Microsoft Technology Licensing, Llc | Application Launcher Sizing |
US9916059B2 (en) * | 2014-07-31 | 2018-03-13 | Microsoft Technology Licensing, Llc | Application launcher sizing |
CN106537337A (en) * | 2014-07-31 | 2017-03-22 | 微软技术许可有限责任公司 | Application launcher sizing |
US11182691B1 (en) | 2014-08-14 | 2021-11-23 | Amazon Technologies, Inc. | Category-based sampling of machine learning data |
US10431204B2 (en) | 2014-09-11 | 2019-10-01 | Apple Inc. | Method and apparatus for discovering trending terms in speech requests |
US10318882B2 (en) | 2014-09-11 | 2019-06-11 | Amazon Technologies, Inc. | Optimized training of linear machine learning models |
US10390213B2 (en) | 2014-09-30 | 2019-08-20 | Apple Inc. | Social reminders |
US10438595B2 (en) | 2014-09-30 | 2019-10-08 | Apple Inc. | Speaker identification and unsupervised speaker adaptation techniques |
US9986419B2 (en) | 2014-09-30 | 2018-05-29 | Apple Inc. | Social reminders |
US10453443B2 (en) | 2014-09-30 | 2019-10-22 | Apple Inc. | Providing an indication of the suitability of speech recognition |
US9902062B2 (en) | 2014-10-02 | 2018-02-27 | Brain Corporation | Apparatus and methods for training path navigation by robots |
US10131052B1 (en) | 2014-10-02 | 2018-11-20 | Brain Corporation | Persistent predictor apparatus and methods for task switching |
US9630318B2 (en) * | 2014-10-02 | 2017-04-25 | Brain Corporation | Feature detection apparatus and methods for training of robotic navigation |
US9687984B2 (en) | 2014-10-02 | 2017-06-27 | Brain Corporation | Apparatus and methods for training of robots |
US10105841B1 (en) | 2014-10-02 | 2018-10-23 | Brain Corporation | Apparatus and methods for programming and training of robotic devices |
US9604359B1 (en) | 2014-10-02 | 2017-03-28 | Brain Corporation | Apparatus and methods for training path navigation by robots |
US20160096270A1 (en) * | 2014-10-02 | 2016-04-07 | Brain Corporation | Feature detection apparatus and methods for training of robotic navigation |
US10376117B2 (en) | 2015-02-26 | 2019-08-13 | Brain Corporation | Apparatus and methods for programming and training of robotic household appliances |
US9717387B1 (en) | 2015-02-26 | 2017-08-01 | Brain Corporation | Apparatus and methods for programming and training of robotic household appliances |
US11231904B2 (en) | 2015-03-06 | 2022-01-25 | Apple Inc. | Reducing response latency of intelligent automated assistants |
US10529332B2 (en) | 2015-03-08 | 2020-01-07 | Apple Inc. | Virtual assistant activation |
US10311871B2 (en) | 2015-03-08 | 2019-06-04 | Apple Inc. | Competing devices responding to voice triggers |
US11842734B2 (en) | 2015-03-08 | 2023-12-12 | Apple Inc. | Virtual assistant activation |
US10567477B2 (en) | 2015-03-08 | 2020-02-18 | Apple Inc. | Virtual assistant continuity |
US10930282B2 (en) | 2015-03-08 | 2021-02-23 | Apple Inc. | Competing devices responding to voice triggers |
US11087759B2 (en) | 2015-03-08 | 2021-08-10 | Apple Inc. | Virtual assistant activation |
US20160299787A1 (en) * | 2015-04-08 | 2016-10-13 | Fujitsu Limited | System, method and managing device |
US11468282B2 (en) | 2015-05-15 | 2022-10-11 | Apple Inc. | Virtual assistant in a communication session |
US11127397B2 (en) | 2015-05-27 | 2021-09-21 | Apple Inc. | Device voice control |
US20160350136A1 (en) * | 2015-05-27 | 2016-12-01 | Google Inc. | Assist layer with automated extraction |
US11070949B2 (en) | 2015-05-27 | 2021-07-20 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on an electronic device with a touch-sensitive display |
US10356243B2 (en) | 2015-06-05 | 2019-07-16 | Apple Inc. | Virtual assistant aided communication with 3rd party service in a communication session |
US11934160B2 (en) | 2015-06-05 | 2024-03-19 | Apple Inc. | Context driven routine prediction assistance |
US10681212B2 (en) | 2015-06-05 | 2020-06-09 | Apple Inc. | Virtual assistant aided communication with 3rd party service in a communication session |
US10788800B2 (en) | 2015-06-05 | 2020-09-29 | Apple Inc. | Data-driven context determination |
US11025565B2 (en) | 2015-06-07 | 2021-06-01 | Apple Inc. | Personalized prediction of responses for instant messaging |
US11010127B2 (en) | 2015-06-29 | 2021-05-18 | Apple Inc. | Virtual assistant for media playback |
US11947873B2 (en) | 2015-06-29 | 2024-04-02 | Apple Inc. | Virtual assistant for media playback |
US20170031575A1 (en) * | 2015-07-28 | 2017-02-02 | Microsoft Technology Licensing, Llc | Tailored computing experience based on contextual signals |
US10489032B1 (en) | 2015-07-29 | 2019-11-26 | Google Llc | Rich structured data interchange for copy-paste operations |
US11500672B2 (en) | 2015-09-08 | 2022-11-15 | Apple Inc. | Distributed personal assistant |
US11126400B2 (en) | 2015-09-08 | 2021-09-21 | Apple Inc. | Zero latency digital assistant |
US11550542B2 (en) | 2015-09-08 | 2023-01-10 | Apple Inc. | Zero latency digital assistant |
US11853536B2 (en) | 2015-09-08 | 2023-12-26 | Apple Inc. | Intelligent automated assistant in a media environment |
US11954405B2 (en) | 2015-09-08 | 2024-04-09 | Apple Inc. | Zero latency digital assistant |
US11809483B2 (en) | 2015-09-08 | 2023-11-07 | Apple Inc. | Intelligent automated assistant for media search and playback |
US10845949B2 (en) | 2015-09-28 | 2020-11-24 | Oath Inc. | Continuity of experience card for index |
US10521070B2 (en) | 2015-10-23 | 2019-12-31 | Oath Inc. | Method to automatically update a homescreen |
US10257275B1 (en) | 2015-10-26 | 2019-04-09 | Amazon Technologies, Inc. | Tuning software execution environments using Bayesian models |
US11809886B2 (en) | 2015-11-06 | 2023-11-07 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US11526368B2 (en) | 2015-11-06 | 2022-12-13 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US10956666B2 (en) | 2015-11-09 | 2021-03-23 | Apple Inc. | Unconventional virtual assistant interactions |
US11886805B2 (en) | 2015-11-09 | 2024-01-30 | Apple Inc. | Unconventional virtual assistant interactions |
US10354652B2 (en) | 2015-12-02 | 2019-07-16 | Apple Inc. | Applying neural network language models to weighted finite state transducers for automatic speech recognition |
US10831766B2 (en) | 2015-12-21 | 2020-11-10 | Oath Inc. | Decentralized cards platform for showing contextual cards in a stream |
US11853647B2 (en) | 2015-12-23 | 2023-12-26 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10942703B2 (en) | 2015-12-23 | 2021-03-09 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10901573B2 (en) * | 2016-02-05 | 2021-01-26 | Airwatch Llc | Generating predictive action buttons within a graphical user interface |
US20170228107A1 (en) * | 2016-02-05 | 2017-08-10 | Airwatch Llc | Generating predictive action buttons within a graphical user interface |
US10909181B2 (en) * | 2016-03-28 | 2021-02-02 | Microsoft Technology Licensing, Llc | People relevance platform |
US20170277810A1 (en) * | 2016-03-28 | 2017-09-28 | Microsoft Technology Licensing, Llc | People Relevance Platform |
US10241514B2 (en) | 2016-05-11 | 2019-03-26 | Brain Corporation | Systems and methods for initializing a robot to autonomously travel a trained route |
US10345986B1 (en) * | 2016-05-17 | 2019-07-09 | Google Llc | Information cycling in graphical notifications |
US11227589B2 (en) | 2016-06-06 | 2022-01-18 | Apple Inc. | Intelligent list reading |
US11069347B2 (en) | 2016-06-08 | 2021-07-20 | Apple Inc. | Intelligent automated assistant for media exploration |
WO2017213681A1 (en) * | 2016-06-09 | 2017-12-14 | Apple Inc. | Intelligent automated assistant in a home environment |
US10354011B2 (en) | 2016-06-09 | 2019-07-16 | Apple Inc. | Intelligent automated assistant in a home environment |
CN108885608A (en) * | 2016-06-09 | 2018-11-23 | 苹果公司 | Intelligent automation assistant in home environment |
US10733993B2 (en) | 2016-06-10 | 2020-08-04 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US11657820B2 (en) | 2016-06-10 | 2023-05-23 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US9987752B2 (en) | 2016-06-10 | 2018-06-05 | Brain Corporation | Systems and methods for automatic detection of spills |
US11037565B2 (en) | 2016-06-10 | 2021-06-15 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US10580409B2 (en) | 2016-06-11 | 2020-03-03 | Apple Inc. | Application integration with a digital assistant |
US11809783B2 (en) | 2016-06-11 | 2023-11-07 | Apple Inc. | Intelligent device arbitration and control |
US11749275B2 (en) | 2016-06-11 | 2023-09-05 | Apple Inc. | Application integration with a digital assistant |
US10942702B2 (en) | 2016-06-11 | 2021-03-09 | Apple Inc. | Intelligent device arbitration and control |
US11152002B2 (en) | 2016-06-11 | 2021-10-19 | Apple Inc. | Application integration with a digital assistant |
US10282849B2 (en) | 2016-06-17 | 2019-05-07 | Brain Corporation | Systems and methods for predictive/reconstructive visual object tracker |
US10016896B2 (en) | 2016-06-30 | 2018-07-10 | Brain Corporation | Systems and methods for robotic behavior around moving bodies |
US10474753B2 (en) | 2016-09-07 | 2019-11-12 | Apple Inc. | Language identification using recurrent neural networks |
US10553215B2 (en) | 2016-09-23 | 2020-02-04 | Apple Inc. | Intelligent automated assistant |
US10846618B2 (en) | 2016-09-23 | 2020-11-24 | Google Llc | Smart replies using an on-device model |
US10043516B2 (en) | 2016-09-23 | 2018-08-07 | Apple Inc. | Intelligent automated assistant |
US10274325B2 (en) | 2016-11-01 | 2019-04-30 | Brain Corporation | Systems and methods for robotic mapping |
US10001780B2 (en) | 2016-11-02 | 2018-06-19 | Brain Corporation | Systems and methods for dynamic route planning in autonomous navigation |
US10723018B2 (en) | 2016-11-28 | 2020-07-28 | Brain Corporation | Systems and methods for remote operating and/or monitoring of a robot |
US11281993B2 (en) | 2016-12-05 | 2022-03-22 | Apple Inc. | Model and ensemble compression for metric learning |
US10593346B2 (en) | 2016-12-22 | 2020-03-17 | Apple Inc. | Rank-reduced token representation for automatic speech recognition |
US10616153B2 (en) * | 2016-12-30 | 2020-04-07 | Logmein, Inc. | Real-time communications system with intelligent presence indication |
US20180191647A1 (en) * | 2016-12-30 | 2018-07-05 | Getgo, Inc. | Real-time communications system with intelligent presence indication |
US11656884B2 (en) | 2017-01-09 | 2023-05-23 | Apple Inc. | Application integration with a digital assistant |
US11204787B2 (en) | 2017-01-09 | 2021-12-21 | Apple Inc. | Application integration with a digital assistant |
US10377040B2 (en) | 2017-02-02 | 2019-08-13 | Brain Corporation | Systems and methods for assisting a robotic apparatus |
US10852730B2 (en) | 2017-02-08 | 2020-12-01 | Brain Corporation | Systems and methods for robotic mobile platforms |
US10969954B2 (en) * | 2017-03-03 | 2021-04-06 | Samsung Electronics Co., Ltd. | Electronic device for processing user input and method for processing user input |
US10293485B2 (en) | 2017-03-30 | 2019-05-21 | Brain Corporation | Systems and methods for robotic path planning |
US10332518B2 (en) | 2017-05-09 | 2019-06-25 | Apple Inc. | User interface for correcting recognition errors |
US10741181B2 (en) | 2017-05-09 | 2020-08-11 | Apple Inc. | User interface for correcting recognition errors |
US10417266B2 (en) | 2017-05-09 | 2019-09-17 | Apple Inc. | Context-aware ranking of intelligent response suggestions |
US10395654B2 (en) | 2017-05-11 | 2019-08-27 | Apple Inc. | Text normalization based on a data-driven learning network |
US10755703B2 (en) | 2017-05-11 | 2020-08-25 | Apple Inc. | Offline personal assistant |
US11467802B2 (en) | 2017-05-11 | 2022-10-11 | Apple Inc. | Maintaining privacy of personal information |
US10726832B2 (en) | 2017-05-11 | 2020-07-28 | Apple Inc. | Maintaining privacy of personal information |
US10847142B2 (en) | 2017-05-11 | 2020-11-24 | Apple Inc. | Maintaining privacy of personal information |
US11599331B2 (en) | 2017-05-11 | 2023-03-07 | Apple Inc. | Maintaining privacy of personal information |
US11405466B2 (en) | 2017-05-12 | 2022-08-02 | Apple Inc. | Synchronization and task delegation of a digital assistant |
US11837237B2 (en) | 2017-05-12 | 2023-12-05 | Apple Inc. | User-specific acoustic models |
US10791176B2 (en) | 2017-05-12 | 2020-09-29 | Apple Inc. | Synchronization and task delegation of a digital assistant |
US11301477B2 (en) | 2017-05-12 | 2022-04-12 | Apple Inc. | Feedback analysis of a digital assistant |
US11580990B2 (en) | 2017-05-12 | 2023-02-14 | Apple Inc. | User-specific acoustic models |
US10410637B2 (en) | 2017-05-12 | 2019-09-10 | Apple Inc. | User-specific acoustic models |
US11380310B2 (en) | 2017-05-12 | 2022-07-05 | Apple Inc. | Low-latency intelligent automated assistant |
US11538469B2 (en) | 2017-05-12 | 2022-12-27 | Apple Inc. | Low-latency intelligent automated assistant |
US11862151B2 (en) | 2017-05-12 | 2024-01-02 | Apple Inc. | Low-latency intelligent automated assistant |
US10789945B2 (en) | 2017-05-12 | 2020-09-29 | Apple Inc. | Low-latency intelligent automated assistant |
US10482874B2 (en) | 2017-05-15 | 2019-11-19 | Apple Inc. | Hierarchical belief states for digital assistants |
US10810274B2 (en) | 2017-05-15 | 2020-10-20 | Apple Inc. | Optimizing dialogue policy decisions for digital assistants using implicit feedback |
US20210157308A1 (en) * | 2017-05-15 | 2021-05-27 | Doosan Heavy Industries & Construction Co., Ltd. | Fault signal recovery system and method |
US11709486B2 (en) * | 2017-05-15 | 2023-07-25 | Doosan Enerbility Co., Ltd. | Fault signal recovery system and method |
US11675829B2 (en) | 2017-05-16 | 2023-06-13 | Apple Inc. | Intelligent automated assistant for media exploration |
US10303715B2 (en) | 2017-05-16 | 2019-05-28 | Apple Inc. | Intelligent automated assistant for media exploration |
US10909171B2 (en) | 2017-05-16 | 2021-02-02 | Apple Inc. | Intelligent automated assistant for media exploration |
US11532306B2 (en) | 2017-05-16 | 2022-12-20 | Apple Inc. | Detecting a trigger of a digital assistant |
US10311144B2 (en) | 2017-05-16 | 2019-06-04 | Apple Inc. | Emoji word sense disambiguation |
US10403278B2 (en) | 2017-05-16 | 2019-09-03 | Apple Inc. | Methods and systems for phonetic matching in digital assistant services |
US11217255B2 (en) | 2017-05-16 | 2022-01-04 | Apple Inc. | Far-field extension for digital assistant services |
US10748546B2 (en) | 2017-05-16 | 2020-08-18 | Apple Inc. | Digital assistant services based on device capabilities |
US10657328B2 (en) | 2017-06-02 | 2020-05-19 | Apple Inc. | Multi-task recurrent neural network architecture for efficient morphology handling in neural language modeling |
US10445429B2 (en) | 2017-09-21 | 2019-10-15 | Apple Inc. | Natural language understanding using vocabularies with compressed serialized tries |
US10755051B2 (en) | 2017-09-29 | 2020-08-25 | Apple Inc. | Rule-based natural language processing |
US11132349B2 (en) | 2017-10-05 | 2021-09-28 | Adobe Inc. | Update basis for updating digital content in a digital medium environment |
US11551257B2 (en) | 2017-10-12 | 2023-01-10 | Adobe Inc. | Digital media environment for analysis of audience segments in a digital marketing campaign |
US11544743B2 (en) | 2017-10-16 | 2023-01-03 | Adobe Inc. | Digital content control based on shared machine learning properties |
US11243747B2 (en) * | 2017-10-16 | 2022-02-08 | Adobe Inc. | Application digital content control using an embedded machine learning module |
US11853723B2 (en) | 2017-10-16 | 2023-12-26 | Adobe Inc. | Application digital content control using an embedded machine learning module |
US10636424B2 (en) | 2017-11-30 | 2020-04-28 | Apple Inc. | Multi-turn canned dialog |
US11836592B2 (en) * | 2017-12-15 | 2023-12-05 | International Business Machines Corporation | Communication model for cognitive systems |
US20190188552A1 (en) * | 2017-12-15 | 2019-06-20 | International Business Machines Corporation | Communication model for cognitive systems |
KR102628042B1 (en) * | 2017-12-22 | 2024-01-23 | 삼성전자주식회사 | Device and method for recommeding contact information |
CN111512617A (en) * | 2017-12-22 | 2020-08-07 | 三星电子株式会社 | Device and method for recommending contact information |
US10659399B2 (en) | 2017-12-22 | 2020-05-19 | Google Llc | Message analysis using a machine learning model |
WO2019125082A1 (en) | 2017-12-22 | 2019-06-27 | Samsung Electronics Co., Ltd. | Device and method for recommending contact information |
KR20190076870A (en) * | 2017-12-22 | 2019-07-02 | 삼성전자주식회사 | Device and method for recommeding contact information |
US11521111B2 (en) | 2017-12-22 | 2022-12-06 | Samsung Electronics Co., Ltd. | Device and method for recommending contact information |
US10733982B2 (en) | 2018-01-08 | 2020-08-04 | Apple Inc. | Multi-directional dialog |
US20190222655A1 (en) * | 2018-01-15 | 2019-07-18 | Korea Advanced Institute Of Science And Technology | Spatio-cohesive service discovery and dynamic service handover for distributed iot environments |
US11647090B2 (en) * | 2018-01-15 | 2023-05-09 | Korea Advanced Institute Of Science And Technology | Spatio-cohesive service discovery and dynamic service handover for distributed IoT environments |
US10733375B2 (en) | 2018-01-31 | 2020-08-04 | Apple Inc. | Knowledge-based framework for improving natural language understanding |
US10789959B2 (en) | 2018-03-02 | 2020-09-29 | Apple Inc. | Training speaker recognition models for digital assistants |
US10592604B2 (en) | 2018-03-12 | 2020-03-17 | Apple Inc. | Inverse text normalization for automatic speech recognition |
US11710482B2 (en) | 2018-03-26 | 2023-07-25 | Apple Inc. | Natural assistant interaction |
US10818288B2 (en) | 2018-03-26 | 2020-10-27 | Apple Inc. | Natural assistant interaction |
US10909331B2 (en) | 2018-03-30 | 2021-02-02 | Apple Inc. | Implicit identification of translation payload with neural machine translation |
US11288299B2 (en) | 2018-04-24 | 2022-03-29 | International Business Machines Corporation | Enhanced action fulfillment using classification valency |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11169616B2 (en) | 2018-05-07 | 2021-11-09 | Apple Inc. | Raise to speak |
US11487364B2 (en) | 2018-05-07 | 2022-11-01 | Apple Inc. | Raise to speak |
US10928918B2 (en) | 2018-05-07 | 2021-02-23 | Apple Inc. | Raise to speak |
US11854539B2 (en) | 2018-05-07 | 2023-12-26 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11900923B2 (en) | 2018-05-07 | 2024-02-13 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US11907436B2 (en) | 2018-05-07 | 2024-02-20 | Apple Inc. | Raise to speak |
US10984780B2 (en) | 2018-05-21 | 2021-04-20 | Apple Inc. | Global semantic word embeddings using bi-directional recurrent neural networks |
US10892996B2 (en) | 2018-06-01 | 2021-01-12 | Apple Inc. | Variable latency device coordination |
US10984798B2 (en) | 2018-06-01 | 2021-04-20 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US11431642B2 (en) | 2018-06-01 | 2022-08-30 | Apple Inc. | Variable latency device coordination |
US10684703B2 (en) | 2018-06-01 | 2020-06-16 | Apple Inc. | Attention aware virtual assistant dismissal |
US11630525B2 (en) | 2018-06-01 | 2023-04-18 | Apple Inc. | Attention aware virtual assistant dismissal |
US10720160B2 (en) | 2018-06-01 | 2020-07-21 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US11386266B2 (en) | 2018-06-01 | 2022-07-12 | Apple Inc. | Text correction |
US11009970B2 (en) | 2018-06-01 | 2021-05-18 | Apple Inc. | Attention aware virtual assistant dismissal |
US11495218B2 (en) | 2018-06-01 | 2022-11-08 | Apple Inc. | Virtual assistant operation in multi-device environments |
US10403283B1 (en) | 2018-06-01 | 2019-09-03 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US11360577B2 (en) | 2018-06-01 | 2022-06-14 | Apple Inc. | Attention aware virtual assistant dismissal |
US10944859B2 (en) | 2018-06-03 | 2021-03-09 | Apple Inc. | Accelerated task performance |
US10504518B1 (en) | 2018-06-03 | 2019-12-10 | Apple Inc. | Accelerated task performance |
US10496705B1 (en) | 2018-06-03 | 2019-12-03 | Apple Inc. | Accelerated task performance |
US20220060847A1 (en) * | 2018-08-17 | 2022-02-24 | xAd, Inc. | Systems and Methods for Pacing Information Delivery to Mobile Devices |
US11743679B2 (en) * | 2018-08-17 | 2023-08-29 | xAd, Inc. | Systems and methods for pacing information delivery to mobile devices |
US20210256222A1 (en) * | 2018-09-06 | 2021-08-19 | Pricewaterhousecoopers Llp | Systems and methods for automatic resolutions of wireless signals |
US11010561B2 (en) | 2018-09-27 | 2021-05-18 | Apple Inc. | Sentiment prediction from textual data |
US11462215B2 (en) | 2018-09-28 | 2022-10-04 | Apple Inc. | Multi-modal inputs for voice commands |
US11893992B2 (en) | 2018-09-28 | 2024-02-06 | Apple Inc. | Multi-modal inputs for voice commands |
US10839159B2 (en) | 2018-09-28 | 2020-11-17 | Apple Inc. | Named entity normalization in a spoken dialog system |
US11170166B2 (en) | 2018-09-28 | 2021-11-09 | Apple Inc. | Neural typographical error modeling via generative adversarial networks |
US11475898B2 (en) | 2018-10-26 | 2022-10-18 | Apple Inc. | Low-latency multi-speaker speech recognition |
US11638059B2 (en) | 2019-01-04 | 2023-04-25 | Apple Inc. | Content playback on multiple devices |
US11556354B2 (en) * | 2019-02-01 | 2023-01-17 | Samsung Electronics Co., Ltd. | Context-based user-personalized configuration method and device |
US11348573B2 (en) | 2019-03-18 | 2022-05-31 | Apple Inc. | Multimodality in digital assistant systems |
US11783815B2 (en) | 2019-03-18 | 2023-10-10 | Apple Inc. | Multimodality in digital assistant systems |
US11144426B2 (en) | 2019-03-19 | 2021-10-12 | Bank Of America Corporation | Prediction tool |
US10776243B1 (en) | 2019-03-19 | 2020-09-15 | Bank Of America Corporation | Prediction tool |
CN113491097A (en) * | 2019-04-26 | 2021-10-08 | 深圳市欢太科技有限公司 | Voice broadcast control method and device, storage medium and electronic equipment |
EP3962048A4 (en) * | 2019-04-26 | 2022-04-27 | Shenzhen Heytap Technology Corp., Ltd. | Voice broadcasting control method and apparatus, storage medium, and electronic device |
US11675491B2 (en) | 2019-05-06 | 2023-06-13 | Apple Inc. | User configurable task triggers |
US11217251B2 (en) | 2019-05-06 | 2022-01-04 | Apple Inc. | Spoken notifications |
US11423908B2 (en) | 2019-05-06 | 2022-08-23 | Apple Inc. | Interpreting spoken requests |
US11475884B2 (en) | 2019-05-06 | 2022-10-18 | Apple Inc. | Reducing digital assistant latency when a language is incorrectly determined |
US11705130B2 (en) | 2019-05-06 | 2023-07-18 | Apple Inc. | Spoken notifications |
US11307752B2 (en) | 2019-05-06 | 2022-04-19 | Apple Inc. | User configurable task triggers |
US11888791B2 (en) | 2019-05-21 | 2024-01-30 | Apple Inc. | Providing message response suggestions |
US11140099B2 (en) | 2019-05-21 | 2021-10-05 | Apple Inc. | Providing message response suggestions |
US11360739B2 (en) | 2019-05-31 | 2022-06-14 | Apple Inc. | User activity shortcut suggestions |
US11350171B2 (en) * | 2019-05-31 | 2022-05-31 | Samsung Electronics Co., Ltd. | Electronic apparatus and controlling method thereof |
US11496600B2 (en) | 2019-05-31 | 2022-11-08 | Apple Inc. | Remote execution of machine-learned models |
US11237797B2 (en) | 2019-05-31 | 2022-02-01 | Apple Inc. | User activity shortcut suggestions |
US11657813B2 (en) | 2019-05-31 | 2023-05-23 | Apple Inc. | Voice identification in digital assistant systems |
US11289073B2 (en) | 2019-05-31 | 2022-03-29 | Apple Inc. | Device text to speech |
US11790914B2 (en) | 2019-06-01 | 2023-10-17 | Apple Inc. | Methods and user interfaces for voice-based control of electronic devices |
US11360641B2 (en) | 2019-06-01 | 2022-06-14 | Apple Inc. | Increasing the relevance of new available information |
CN110300052A (en) * | 2019-06-26 | 2019-10-01 | 努比亚技术有限公司 | A kind of instant messaging state identification method, equipment and computer readable storage medium |
US10999390B2 (en) * | 2019-08-15 | 2021-05-04 | Bank Of America Corporation | Method and system for mobile data communication |
US11488406B2 (en) | 2019-09-25 | 2022-11-01 | Apple Inc. | Text detection using global geometry estimators |
US11501101B1 (en) * | 2019-12-16 | 2022-11-15 | NTT DATA Services, LLC | Systems and methods for securing machine learning models |
US11924254B2 (en) | 2020-05-11 | 2024-03-05 | Apple Inc. | Digital assistant hardware abstraction |
US11914848B2 (en) | 2020-05-11 | 2024-02-27 | Apple Inc. | Providing relevant data items based on context |
US11765209B2 (en) | 2020-05-11 | 2023-09-19 | Apple Inc. | Digital assistant hardware abstraction |
US11755276B2 (en) | 2020-05-12 | 2023-09-12 | Apple Inc. | Reducing description length based on confidence |
US11838734B2 (en) | 2020-07-20 | 2023-12-05 | Apple Inc. | Multi-device audio adjustment coordination |
US11696060B2 (en) | 2020-07-21 | 2023-07-04 | Apple Inc. | User identification using headphones |
US11750962B2 (en) | 2020-07-21 | 2023-09-05 | Apple Inc. | User identification using headphones |
US11258847B1 (en) * | 2020-11-02 | 2022-02-22 | Servicenow, Inc. | Assignments of incoming requests to servers in computing clusters and other environments |
WO2022256547A1 (en) * | 2021-06-03 | 2022-12-08 | Mocha Holdings Llc | Apparatus and method for suggesting user-relevant digital content using edge computing |
US11829239B2 (en) | 2021-11-17 | 2023-11-28 | Adobe Inc. | Managing machine learning model reconstruction |
Also Published As
Publication number | Publication date |
---|---|
WO2013192433A1 (en) | 2013-12-27 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8886576B1 (en) | Automatic label suggestions for albums based on machine learning | |
US8429103B1 (en) | Native machine learning service for user adaptation on a mobile platform | |
US8510238B1 (en) | Method to predict session duration on mobile devices using native machine learning | |
US20130346347A1 (en) | Method to Predict a Communicative Action that is Most Likely to be Executed Given a Context | |
US11128979B2 (en) | Inferring user availability for a communication | |
US10871872B2 (en) | Intelligent productivity monitoring with a digital assistant | |
US20190057298A1 (en) | Mapping actions and objects to tasks | |
JP6419993B2 (en) | System and method for proactively identifying and surfaced relevant content on a touch sensitive device | |
US20190205839A1 (en) | Enhanced computer experience from personal activity pattern | |
US10446009B2 (en) | Contextual notification engine | |
US10524092B2 (en) | Task automation using location-awareness of multiple devices | |
JP6791569B2 (en) | User profile generation method and terminal | |
WO2018031378A1 (en) | Email personalization | |
CN105025173A (en) | Self-aware profile switching on a mobile computing device | |
KR101817661B1 (en) | Contextualizing seonsor, service and device data with mobile devices | |
US20200382568A1 (en) | Sharing routine for suggesting applications to share content from host application | |
KR20140113436A (en) | Computing system with relationship model mechanism and method of operation therof | |
WO2023125692A1 (en) | Service recommendation method and related apparatus | |
US20190090197A1 (en) | Saving battery life with inferred location | |
KR102185369B1 (en) | System and mehtod for generating information for conversation with user | |
US20220293096A1 (en) | User-oriented actions based on audio conversation | |
WO2020106499A1 (en) | Saving battery life using an inferred location |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:PATTERSON, ANNA;ARADHYE, HRISHIKESH;HUA, WEI;AND OTHERS;REEL/FRAME:028742/0904Effective date: 20120730 |
|
STCB | Information on status: application discontinuation |
Free format text: ABANDONED -- FAILURE TO RESPOND TO AN OFFICE ACTION |