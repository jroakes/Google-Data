EP2761584B1 - Label positioning technique to reduce crawling during zoom activities - Google Patents
Label positioning technique to reduce crawling during zoom activities Download PDFInfo
- Publication number
- EP2761584B1 EP2761584B1 EP12837289.3A EP12837289A EP2761584B1 EP 2761584 B1 EP2761584 B1 EP 2761584B1 EP 12837289 A EP12837289 A EP 12837289A EP 2761584 B1 EP2761584 B1 EP 2761584B1
- Authority
- EP
- European Patent Office
- Prior art keywords
- label
- segment
- resultant
- labels
- computer
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000000034 method Methods 0.000 title claims description 73
- 230000009193 crawling Effects 0.000 title description 7
- 230000000694 effects Effects 0.000 title description 7
- 238000009877 rendering Methods 0.000 claims description 37
- 238000002372 labelling Methods 0.000 claims description 25
- 238000012545 processing Methods 0.000 claims description 16
- 230000008569 process Effects 0.000 description 28
- 230000015654 memory Effects 0.000 description 17
- 238000004891 communication Methods 0.000 description 16
- 239000012634 fragment Substances 0.000 description 8
- 238000010586 diagram Methods 0.000 description 7
- 238000004422 calculation algorithm Methods 0.000 description 6
- 238000003384 imaging method Methods 0.000 description 6
- 238000013459 approach Methods 0.000 description 5
- 230000033001 locomotion Effects 0.000 description 4
- 238000012163 sequencing technique Methods 0.000 description 4
- 230000008859 change Effects 0.000 description 3
- 239000003086 colorant Substances 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 238000013507 mapping Methods 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 238000007792 addition Methods 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 230000007423 decrease Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000011218 segmentation Effects 0.000 description 1
- 230000008054 signal transmission Effects 0.000 description 1
- 239000000126 substance Substances 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
- XLYOFNOQVPJJNP-UHFFFAOYSA-N water Substances O XLYOFNOQVPJJNP-UHFFFAOYSA-N 0.000 description 1
- 238000004804 winding Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01C—MEASURING DISTANCES, LEVELS OR BEARINGS; SURVEYING; NAVIGATION; GYROSCOPIC INSTRUMENTS; PHOTOGRAMMETRY OR VIDEOGRAMMETRY
- G01C21/00—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00
- G01C21/26—Navigation; Navigational instruments not provided for in groups G01C1/00 - G01C19/00 specially adapted for navigation in a road network
- G01C21/34—Route searching; Route guidance
- G01C21/36—Input/output arrangements for on-board computers
- G01C21/3667—Display of a road map
- G01C21/3673—Labelling using text of road map data items, e.g. road names, POI names
Definitions
- the present disclosure relates to image rendering systems, such as electronic map display systems, and more specifically to an image rendering engine that renders a label of a feature of an electronic map in a manner that reduces a crawling motion of the labels while changing zooming levels of the map.
- Digital maps are found in and may be displayed by a wide variety of devices, including mobile phones, car navigation systems, hand-held GPS units, computers, and many websites. Although digital maps are easy to view and to use from an end-user's perspective, creating a digital map is a difficult task and can be a time-consuming process.
- every digital map begins with storing, in a map database, a set of raw data corresponding to millions of streets and intersections and other features to be displayed as part of a map.
- the raw map data that is stored in the map database and that is used to generate digital map images is derived from a variety of sources, with each source typically providing different amounts and types of information. This map data must therefore be compiled and stored in the map database before being accessed by map display or map rendering applications and hardware.
- map images there are, of course, different manners of digitally rendering map images (referred to as digital map images) based on map data stored in a map database.
- One method of rendering a map image is to store map images within the map database as sets of raster or pixilated images made up of numerous pixel data points, with each pixel data point including properties defining how a particular pixel in an image is to be displayed on an electronic display device. While this type of map data is relatively easy to create and store, the map rendering technique using this data typically requires a large amount of storage space for comprehensive digital map images, and it is difficult to manipulate the digital map images as displayed on a display device in very many useful manners.
- vector image data is typically used in high-resolution and fast-moving imaging systems, such as those associated with gaming systems, and in particular three-dimensional gaming systems.
- vector image data includes data that defines specific image objects or elements (also referred to as primitives) to be displayed as part of an image via an image display device.
- image elements or primitives may be, for example, individual roads, text labels, areas, text boxes, buildings, points of interest markers, terrain features, bike paths, map or street labels, etc.
- Each image element may generally be drawn as a set of one or more triangles (of different sizes, shapes, colors, fill patterns, etc.), with each triangle including three vertices interconnected by lines.
- the image database may store a set of vertex data points that may be used to generate one or more of the triangles.
- each vertex data point includes data pertaining to a two-dimensional or a three-dimensional position of the vertex (in an X, Y or an X, Y, Z coordinate system, for example) and various vertex attributes defining properties of the vertex, such as color properties, fill properties, line width properties for lines emanating from the vertex, etc.
- an image shader is a set of software instructions used primarily to calculate rendering effects on graphics hardware with a high degree of flexibility.
- Image shaders are well known in the art and various types of image shaders are available in various application programming interfaces (APIs) provided by, for example, OpenGL and Direct3D, to define special shading functions.
- APIs application programming interfaces
- image shaders are simple programs in a high level programming language that describe or determine the traits of either a vertex or a pixel.
- Vertex shaders for example, define the traits (e.g., position, texture coordinates, colors, etc.) of a vertex, while pixel or fragment shaders define the traits (color, z-depth and alpha value) of a pixel.
- a vertex shader is called for each vertex in an image element or primitive so that, for each vertex input into the vertex shader, the vertex shader produces one (updated) vertex output.
- Each vertex output by the vertex shader is then rendered as a series of pixels onto a block of memory that will eventually be sent to a display screen.
- Vertex shaders are run once for each vertex given to the graphics processor.
- the purpose of a vertex shader is to transform a position of a vertex in a virtual space to the two-dimensional coordinate at which it appears on the display screen (as well as a depth value for the z-buffer of the graphics processor).
- Vertex shaders can manipulate properties such as position, color, and texture coordinates by setting vertex attributes of the vertices, but cannot create new vertices.
- the output of the vertex shader is provided to the next stage in the processing pipeline, which is either a geometry shader if present or the rasterizer.
- Geometry shaders can add and remove vertices from a mesh of vertices and can be used to generate image geometry procedurally or to add volumetric detail to existing images that would be too costly to process on a central processing unit (CPU). If geometry shaders are being used, the output is then sent to the rasterizer.
- Pixel shaders which are also known as fragment shaders, calculate the color and light properties of individual pixels in an image. The input to this stage comes from the rasterizer, and the fragment shaders operate to fill in the pixel values of the polygons being sent through the graphics pipeline.
- Fragment shaders are typically used for scene lighting and related effects such as color toning. There is not a one-to-one relationship between calls to the fragment shader and pixels on the screen as fragment shaders are often called many times per pixel because they are called for every image element or object that is in the corresponding space, even if that image object is occluded.
- vector graphics can be particularly advantageous in a mobile map system in which image data is sent from a centralized map database via a communications network (such as the Internet, a wireless network, etc.) to one or more mobile or remote devices for display.
- a communications network such as the Internet, a wireless network, etc.
- vector data once sent to the receiving device, may be more easily scaled and manipulated (e.g., rotated, etc.) than pixilated raster image data.
- the processing of vector data is typically much more time consuming and processor intensive on the image rendering system that receives the data.
- vector image data that provides a higher level of detail or information to be displayed in a map leads to a higher amount of vector data or vertices that need to be sent to the map rendering system from the map database that stores this information, which can result in higher bandwidth requirements or downloading time in some cases.
- text or other legend symbols such as those used to label streets, roads and other features of a map
- text or other legend symbols are generally placed on or proximately near to a map object for which the label identifies or provides information.
- labels may be redrawn. The redrawing of labels during zooming may result in a visually distracting crawling motion or effect where the labels are perceived to move along a dimension of its associated map feature.
- EP 1 840 515 A1 proposes identifying, for a map object such as a road, the center point of the longest portion in the view and rendering a label, such as the name of the road, relative to the center point.
- US 2008/306684 A1 suggests superimposing, onto a map image, road names that do not overlap with the place names and facility names already in the map image (using normal character size), and then superimposing road names at a smaller scale that do not overlap with the place names, facility names, and road names (using smaller character size).
- US 6 565 610 B1 describes a technique for placing text, such as names of geographic feature, on a map image. The technique includes identifying candidate locations for text placement and evaluating these candidate locations using weights that reflect the rank of the feature and a size of a text box that can overlap text boxes for other candidate locations.
- a computer-implemented method for rendering a map on a display device determines a road feature to display in a viewing window and an ordered set of labels to be displayed on the road feature and labels the road feature as defined in claim 1.
- a system according to the invention is defined in claim 13.
- a graphics or image rendering system such as a map image rendering system, receives image data from an image database in the form of vector data that defines various linear features of the map, such as roads, boundaries, etc., in addition to text strings or symbols to be displayed with the features to provide, for example, labels for the features.
- a label positioning technique generally divides the displayed linear feature into a number of segments and then labels each segment in a sequence based on an ordered set of labels and a position of each segment.
- the label rendering technique positions the text string and/or symbol labels such that the labels are less likely to move along a dimension of the linear feature (e.g., a road feature) when a map zoom level is changed.
- the label movement during zooming is referred to as "crawling" and can be a distraction for map viewers.
- the image rendering system also provides a more consistent labeling process that enhances visual properties within a map image.
- a map-related imaging system 10 includes a map database 12 stored in a server 14 or in multiple server located at, for example, a central site or at various spaced apart sites, and also includes multiple map client devices 16, 18, 20, and 22, each of which stores and implements a map rendering device or a map rendering engine.
- the map client devices 16-22 may be connected to the server 14 via any hardwired or wireless communication network 25, including for example a hardwired or wireless LAN, MAN or WAN, the Internet, or any combination thereof.
- the map client devices 16-22 may be, for example, mobile phone devices (18), computers such a laptop, desktop or other types of computers (16, 20) or components of other imaging systems such as components of automobile navigation systems (22), etc.
- the client devices 16-22 may be communicatively connected to the server 14 via any suitable communication system, such as any publically available or privately owned communication network, including those that use hardwired based communication structure, such as telephone and cable hardware, and/or wireless communication structure, such as wireless communication networks, including for example, wireless LANs and WANs, satellite and cellular phone communication systems, etc.
- any suitable communication system such as any publically available or privately owned communication network, including those that use hardwired based communication structure, such as telephone and cable hardware, and/or wireless communication structure, such as wireless communication networks, including for example, wireless LANs and WANs, satellite and cellular phone communication systems, etc.
- the map database 12 may store any desired types or kinds of map data including raster image map data and vector image map data.
- the image rendering systems described herein are highly suited for use with vector image data which defines or includes a series of vertices or vertex data points for each of numerous sets of image objects, elements or primitives within an image to be displayed.
- the image rendering systems may primarily generate images based on vector image data, in some embodiments, a combination of vector image map data and raster image map data may be utilized. For example, for some zoom levels or when a slow client machine is performing image generation, raster backgrounds without any text (e.g., water, parks, roads, etc.) may be rendered with text generated from vector data.
- each of the image objects defined by the vector data will have a plurality of vertices associated therewith and these vertices will be used to display a map related image object to a user via one or more of the client devices 16-22.
- each of the client devices 16-22 includes an image rendering engine having one or more processors 30, one or more memories 32, a display device 34, and in many cases a rasterizer or graphics card 36 which are generally programmed and interconnected in known manners to implement or to render graphics (images) on the associated display device 34.
- the display device 34 for any particular client device 16-22 may be any type of electronic display device such as a liquid crystal display (LCD), a light emitting diode (LED) display, a plasma display, a cathode ray tube (CRT) display, or any other type of known or suitable electronic display.
- the map-related imaging system 10 of Fig. 1 operates such that a user, at one of the client devices 16-22, opens or executes a map application (not shown in Fig. 1 ) that operates to communicate with and to obtain map information or map related data from the map database 12 via the server 14, and that then displays or renders a map image based on the received map data.
- the map application may allow the user to view different geographical portions of the map data stored in the map database 12, to zoom in or zoom out on a particular geographical location, to rotate, spin or change the two-dimensional or three-dimensional viewing angle of the map being displayed, etc.
- each of the client devices 16-22 downloads map data in the form of vector data from the map database 12 and processes that vector data using one or more image shaders to render an image on the associated display device 34.
- the image rendering system 40 of Fig. 2 includes two processors 30a and 30b, two memories 32a and 32b, a user interface 34 and a rasterizer 36.
- the processor 30b, the memory 32b and the rasterizer 36 are disposed on a separate graphics card (denoted below the horizontal line), although this need not be the case in all embodiments.
- a single processor may be used instead.
- the image rendering system 40 includes a network interface 42, a communications and storage routine 43 and one or more map applications 48 having map display logic therein stored on the memory 32a, which may be executed on the processor 30a.
- one or more image shaders in the form of, for example, vertex shaders 44 and fragment shaders 46 are stored on the memory 32b and are executed on the processor 30b.
- the memories 32a and 32b may include either or both volatile and non-volatile memory and the routines and shaders are executed on the processors 30a and 30b to provide the functionality described below.
- the network interface 42 includes any well known software and/or hardware components that operate to communicate with, for example, the server 14 of Fig.
- the image rendering device 40 also includes a data memory 49, which may be a buffer or volatile memory for example, that stores vector data received from the map database 12, the vector data including any number of vertex data points and one or more lookup tables as will be described in more detail.
- a data memory 49 which may be a buffer or volatile memory for example, that stores vector data received from the map database 12, the vector data including any number of vertex data points and one or more lookup tables as will be described in more detail.
- the map logic of the map application 48 executes on the processor 30 to determine the particular image data needed for display to a user via the display device 34 using, for example, user input, GPS signals, prestored logic or programming, etc.
- the display or map logic of the application 48 interacts with the map database 12, using the communications routine 43, by communicating with the server 14 through the network interface 42 to obtain map data, preferably in the form of vector data or compressed vector data from the map database 12. This vector data is returned via the network interface 42 and may be decompressed and stored in the data memory 49 by the routine 43.
- the data downloaded from the map database 12 may be a compact, structured, or otherwise optimized version of the ultimate vector data to be used, and the map application 48 may operate to transform the downloaded vector data into specific vertex data points using the processor 30a.
- the image data sent from the server 14 includes vector data generally defining data for each of a set of vertices associated with a number of different image elements or image objects to be displayed on the screen 34 and possibly one or more lookup tables. If desired, the lookup tables may be sent in, or may be decoded to be in, or may be generated by the map application 48 to be in the form of vector texture maps which are known types of data files typically defining a particular texture or color field (pixel values) to be displayed as part of an image created using vector graphics.
- the vector data for each image element or image object may include multiple vertices associated with one or more triangles making up the particular element or object of an image.
- Each such triangle includes three vertices (defined by vertex data points) and each vertex data point has vertex data associated therewith.
- each vertex data point includes vertex location data defining a two-dimensional or a three-dimensional position or location of the vertex in a reference or virtual space, as well as an attribute reference.
- Each vertex data point may additionally include other information, such as an object type identifier that identifies the type of image object with which the vertex data point is associated.
- the attribute reference referred to herein as a style reference or as a feature reference, references or points to a location or a set of locations in one or more of the lookup tables downloaded and stored in the data memory 43.
- Existing map rendering systems may receive map data from a database in the form of vector data or image data and render a map surface including a set of map features on a display based on the vector data or image data.
- the map rendering systems may label one or more of the map features with text or image labels (e.g., icons) that identify the map features.
- text or image labels e.g., icons
- the map features may be re-labeled.
- Existing systems may re-label linear map features (such as roads, boundaries, etc.) in a manner that produces a crawling effect of the labels, where the labels appear to move along a dimension of the linear map feature. This can be highly distracting to map viewers.
- the labeling technique described herein may be used to reduce the crawling effect of labels during zoom level changes.
- the labeling technique for rendering a label along a map object involves iteratively bisecting a road section using labels of an ordered set of labels.
- a label may be placed in a sequence on the resultant segments of the bisections based and an ordered set of labels and a bisection order.
- the bisection labeling process may be repeated each time a zoom level is changed.
- a label set includes a primary label or icon I, and auxiliary labels Ta and Tb.
- the label set may be ordered so that the primary label I is first in the order.
- I is first
- Ta is second
- Tb is third.
- Ta has a width of 30 pixels wide
- Tb has a width of 70 pixels
- I has a width of 20 pixels.
- the labels could be any other size or dimension and labels within the set of labels may include more or less than three labels.
- Fig. 3A illustrates a straight road feature 400 that may be displayed within a viewing window (not shown).
- a viewing window may be defined by a zoom level, a viewing window position, and a viewing window size for rendering the map with the road feature 400 on the display device.
- a zoom level may correspond with a magnification level.
- Fig. 3A illustrates that the road feature 400 has a first endpoint 401 and a second endpoint 402 and a length of 200 pixels on the display screen. It should be noted that the road feature 400 may be completely shown in a single viewing window or road feature 400 may represent only a section of a larger feature that is not displayed or cannot be displayed on the viewing window. A check is made to determine whether the road feature 400 is long enough to accommodate or fit a corresponding label.
- a midpoint 403 of the road feature then is determined by, for example, dividing the length of the road feature in half. Accordingly, in Fig. 3A , a midpoint of the road feature 400 is calculated to be at 100 pixels from either endpoint 401, 402 along the center line. A length of 10 pixels (half the length of the label I) on each side of the center point (totaling a width of 20 pixels) then is reserved for the primary label I. A pair of resultant road segments 404 and 406 are produced from this bisection with resultant road segment 404 being located at 0-90 pixels and resultant road segment 406 being located at 110-200 pixels.
- the resultant road segments 404, 406 generally are produced by bisecting the road feature 400 to produce two equal length resultant segments and subtracting a portion of the length of the label from the resultant road segments. In this particular case, equal portions of the length of the label are subtracted from each of the resultant road segments so that the resultant road segments 404, 406 remain equal in length. This is the case when a center of the label is also centered at the midpoint 403 of the road feature (or road segment) being bisected.
- road segment 404, 406 that result from the bisection then are_labeled at their centers using subsequent labels in the sequence of the ordered set of labels and so on. Generally, each subsequent road segment that is processed may be labeled with the next label in the ordered set. In particular, at this stage of the bisection process, a selection is made to start with either road segment 404 or road segment 406. Because of the orientation of the road segments within the viewing window, road segment 404 is referred to herein as a left branch segment (at range 0-90 pixels) that is proximate the first endpoint 401 of road feature 400 and road segment 406 is referred to herein as a right branch segment (at range 110-200 pixels) that is proximate second endpoint 402.
- left branch segment at range 0-90 pixels
- road segment 406 is referred to herein as a right branch segment (at range 110-200 pixels) that is proximate second endpoint 402.
- a default process may select the left branch segment.
- the left branch segment 404 is labeled with the subsequent label of the set Ta and right branch segment 406 is labeled with the subsequent label Tb.
- the end of the ordered set of labels is reached (e.g., selected or placed) the order starts again from the beginning of the ordered set of labels.
- label Tb has been selected for the right branch segment 406, a subsequent label in the ordered set of labels starts over with the label I.
- Fig. 3B illustrates a process of labeling the left branch segment 404 with label Ta.
- the center of the segment 0-90 in this example is determined to be at pixel 45.
- the label Ta is 30 pixels wide and the left branch segment 404 is 90 pixels wide, and thus, the label Ta fits.
- a length of a range 30-60 pixels then is reserved for the label Ta.
- a segment may be referred to as a parent segment when it is bisected to produce a pair of resultant or child segments.
- An order of labeling a pair of resultant segments may be based on the parent segment of the pair, wherein a proximity of the parent segment to an endpoint of the road feature 400 may determine which of the pair of resultant child segments may be labeled first with the next label of the ordered set.
- a parent segment that is proximate a first endpoint determines that a corresponding resultant segment that is also proximate the first endpoint will be labeled next and vice versa.
- the left segment 408 is labeled first with the label Tb which is the next label in the ordered set of labels after the parent label Ta of the parent segment 404.
- the right segment 410 is labeled with a label subsequent Tb, which is I (since the ordered set repeats when the end of the set is reached).
- Fig. 3C illustrates an approach to labeling the segment 408.
- a center or midpoint of segment 0-30 is at pixel 15.
- the label Tb is 70 pixels wide while the segment 408 is only 30 pixels wide. Because the label Tb does not fit, the label Tb is not placed and the processing of segment 408 ends.
- Fig. 3D illustrates an approach to labeling the segment 410 (at range 60-90) with the next label in the ordered set of labels, Icon I (as the set of labels repeat).
- the center of segment 410 (at range 60-90) is determined to be at pixel 75.
- the Icon I is 20 pixels wide and the segment is 30, and thus, the label I fits and is placed along the segment at the center point at pixel 75.
- Pixels 65-85 are reserved for Icon I (which is 20 pixels wide).
- the right segment 414 (at range 85-90) is 5 pixels wide for a 30 pixel wide label, so the label Ta will not fit and processing of this branch ends.
- the left segment 412 (at range 60-65) is 5 pixels wide for a 70 pixel wide label, so the label Tb will not fit and processing of this branch ends. Because there are no more segments to process from the left branch segment (at range 0-90) of the road feature 400, the right branch segment 406 (at range 110-200) of the road feature 400 is processed with the label Tb, as illustrated in Fig. 3E . A center of the segment 110-200 is determined to be at pixel 155.
- the label Tb is 70 pixels wide and the segment 406 is 90 pixels wide, and thus, the label Tb fits. A length of 70 pixels at range 120-190 pixels are reserved for Tb. Two resultant road segments, 416 and 418 having ranges 110-120 and 190-200, respectively, remain.
- the right road segment child 416 (at range 190-200) of the remaining road segments are labeled first so that the resultant road segment (at range 190-200) 418 will be labeled with the subsequent label, I, and the resultant road segment (at range 110-120) 416 will be labeled with the label after, Ta.
- the road segment 418 (at range 190-200) is 10 pixels wide for a 20 pixel wide label, so processing for this road segment ends.
- the resultant road segment 416 (at range 110-120) has a width of 10 pixels for a 30 pixel wide label, so processing of this segment ends. Because there are no remaining road segments for processing, Figure 3E illustrates the labeling results with the label Ta segment 30-60, the label I at segment 65-85, the label I at segment 90-110, and the label Tb at segment 120-190.
- Figure 3F illustrates the same process, technique, or algorithm of Figs. 3A-3E applied to a road feature that is 100 pixels long and Figure 3G illustrates the same process, technique, or algorithm of Figs. 3A-3E applied to a road feature that is 400 pixels long.
- the 100 pixel case of Fig. 3F only two labels fit: the label Ta for segment 5-35 and the label I for segment 40-60.
- the placement of the labels Ta and I are in the same relative place: the label I in the center, the label Ta left of center.
- the same labels of Fig. 3E are placed but with more labels between them.
- a possible advantage of this labeling technique is that the Ta, I, and Tb labels are placed in roughly the same relative locations on the road or at least may visually appear so.
- the process of labeling may be performed each time a zoom level is changed to reduce the crawling motion or effect that may happen to a greater extent otherwise.
- FIG. 4A illustrates a diagram for determining the sequencing of the labels during the labeling technique described above.
- the road section that is displayed on a viewing window of a map application may be labeled with a first primary label.
- the first label for the first main segment is I.
- the process or technique may select a left branch 504 (e.g., proximate a first endpoint) or a right branch (e.g., proximate a second endpoint) 506, where the branches correspond to road segments and/or bisections.
- a left branch 504 e.g., proximate a first endpoint
- a right branch e.g., proximate a second endpoint
- FIG. 4A used a left branch approach and this left approach is illustrated in FIG. 4A .
- the numbers 1 and 2 at each branch pair level of FIG. 4A indicate the order of labeling for that branch.
- each of a pair of resultant road segments that is produced by bisecting the road feature or a road segment of the road feature is either proximate the first or the second endpoint of the road feature.
- left branches may correspond with proximity to the first endpoint while right branches may correspond with proximity to the second endpoint.
- the road segment corresponding with branch 504 may be proximate the first endpoint (e.g., a left endpoint) with respect to the road segment corresponding with branch 506, while the road segment corresponding with branch 506 may be proximate the second endpoint (e.g., a right endpoint) with respect to road segment corresponding with branch 504.
- An order of labeling each of the resultant pair of road segments is based, in part, on a proximity to the first or second endpoint (or left or right endpoint) of the parent road segment that produced the resultant pair of child road segments.
- the parent segment of a pair of resultant child road segments when the parent segment of a pair of resultant child road segments is proximate a first endpoint (left endpoint), then the resultant child road segment that is proximate the first endpoint (left endpoint) will be labeled first. Accordingly, when the parent segment of a pair of resultant child segments is proximate a second endpoint (right endpoint), then the resultant child segment that is proximate the second endpoint (right endpoint) will be labeled first. Thus, for example, at a bisection 508 that corresponds with a left parent branch or left parent road segment, the left child branch segment 510 is labeled first and the right child branch segment 512 is labeled second.
- An order of labeling each of the resultant pair of road segments is also based on the label of the parent road segment that produced the pair of child road segments.
- the first child road segment of the pair of child road segments to be labeled may be labeled with the next label of the ordered set of labels after the parent label.
- the second child road segment of the pair of child road segments may be labeled with the next label of the ordered set of labels after the first child road segment.
- the road segment corresponding with the left branch 510 may be labeled with the Ta, which is the subsequent label of the ordered set following the parent label I
- the road segment corresponding with right branch 512 may be labeled with a label subsequent the left branch road segment, which is Tb.
- Fig. 4B illustrates a similar sequencing diagram as Fig. 4A except that during the first bisection 520, a right branch 524 is selected first for labeling followed by a left branch 522.
- a right branch 524 is selected first for labeling followed by a left branch 522.
- the same rules apply as for sequencing of Fig. 4B .
- the order in which the a pair of resultant segments is labeled is based on the parent segment that produced the pair, based on the above described rules.
- Fig. 5 illustrates a process flow diagram or flow chart of a method, routine, or process 300 that may be used to render labels on a road feature of a map.
- the routine 300 may be used in a map rendering engine, such as that of Fig. 2 , to render a label on or about the map feature in a manner that reduces crawl, creep, or drift.
- the routine or process 300 may be implemented as one or more software routines or applications, including routines that are part of the map application 48. These routines may be implemented or written in any desired language, such as Javascript, C code, etc. and may be executed on the processor 30. Additionally, these routines may use the rasterizer 36 to render an image on the display device 34.
- a block 302 may determine a linear map feature to be displayed on a viewing window.
- the linear map feature may be a road, street, boundary, river segment, etc.
- the viewing window may be defined by a particular zoom level, wherein the zoom level corresponds to a magnification level of a map portion being displayed.
- zoom level corresponds to a magnification level of a map portion being displayed.
- map area decreases.
- changing zoom level may correspond to redrawing a map area with greater or lesser map data or detail, where the map data used in a prior zoom level is different than that for the redrawn level.
- a zoom level change where additional data e.g., vector data
- additional data e.g., vector data
- zoom level change where additional data (e.g., vector data) is used to redraw a map
- only a scale of the map area is changed (e.g., same level of detail but greater magnification).
- the existing labels may be removed and the labeling technique described herein repeated.
- the entirety of a linear map feature may not always be displayed entirely within a viewing window. Instead, only a portion less than the entire feature may be shown. In this case, the viewable portion of the linear feature may be limited by a size and a position of the viewing window and the zoom level. In some computer mapping applications, the position of the viewing window, the size of the viewing window, and/or the zoom level may be changed to reveal some or all of the road feature, depending on the total length or size of street. In one embodiment, a larger portion of a road feature may be processed using the described labeling technique even though the entire road feature is not displayed in a current viewing window. In this case, labels on portions of the road that are not visible are not displayed until or unless the viewing window is changed to show those portions. While the process of Fig. 5 may be applied to any linear map feature, the blocks of Fig. 5 may be discussed with respect to a road feature.
- a block 304 may determine an ordered set of labels for labeling the road feature.
- the set of labels may be text describing the road feature.
- the set of labels may include images representing, for example, icons.
- the set of labels may be generally ordered by a priority of each label.
- the first label in the sequence may be the highest priority label, with subsequent labels having less priority.
- a first label may be a primary label that may be most frequently applied or placed on a road. Ancillary second, third, etc. labels may have less placement frequency.
- the ordered set of labels may be determined by a mapping application or retrieved from a table.
- a road segment of the road feature may be selected for labeling.
- an entire displayed road section may be selected in block 310.
- a block 314 determines or selects a next label of the ordered set of labels.
- the selected label may represent a first, primary label.
- a length of the road segment along its center line may be determined along with a length of the selected label.
- a block 318 may determine whether the length of the road segment is long enough to accommodate the length of the label. If, at block 318, the segment is not long enough for the determined label, the segment may be discarded or marked as being processed at block 320. The process may then continue back at block 310 for another unprocessed road segment. The block 310 may search for road segments that are not marked processed. If the length of the road segment at block 318 is long enough to accommodate the length of the label, block 322 may place the selected label at the center of the road segment.
- a block 322 may determine where to place the selected label by dividing the length of the road segment in half (e.g., bisecting the road) and placing the selected label at the bisection or midpoint.
- a block 324 may determine a pair of resultant segments produced by bisecting the determined road segment. The block 324 may reserve a length of space about the midpoint of the road segment for the label so that the resultant segments do not include the length of the label. This may be performed by subtracting half the length of the label from each of the two segments produced from a bisection of the determined road segment. The resultant segments may represent additional unprocessed segments.
- the blocks 304 and/or 310 may include determining an order of the label processing or segmentation such that the labels are placed on the road segments according to the order illustrated in Figs. 4A or 4B .
- the process of Fig. 3 continues from the block 324 to the block 310 where a next unprocessed road segment may be determined for processing.
- the process of Fig. 3 may be repeated for every remaining unprocessed segment until a stop condition is reached.
- a stop condition may be reached when no remaining segments can fit a corresponding label from the ordered label set.
- Another stop condition may be reached when the number of bisections executed is above a threshold number of bisections.
- This threshold number may be a fixed number or may be adjusted based on a total length of a road feature that is displayable within a viewing window.
- stop conditions may be based on characteristics of the label set.
- the label set may not be a fixed set, where certain labels may have a fixed number of uses.
- the icon label may only be used a single time before the icon label is removed from the label set.
- a stop condition may be encountered when there are no remaining labels in the set.
- Fig. 6A illustrates a curved road 600 within a viewing window 602.
- the length of the road 600 may be calculated or determined at block 316 based on a flattened projection on a centerline 604.
- the road is flattened along a horizontal center line 604.
- points along the road 600 may be represented as a distance on the straight line 604, thereby eliminating a vertical length component of the road.
- Flattening the road may be advantageous in situations when the road includes a plurality of turns and windings that may tend to visually distort or compress road labels if the actual length of the road (e.g., the length along a road curve) is used for length determinations.
- Fig. 6B illustrates a curved road 610 within a viewing window 612 that is projected along a vertical centerline 614. A choice of vertical or horizontal lines for flattening may be based on the shape and orientation of the road within the viewing window.
- labeling encoding techniques described herein may be altered or varied in any number of manners to provide an image rendering system, such as a map rendering system, with the ability to render individual text characters within a text string along a curved, e.g., multi-segmented line.
- an image rendering system such as a map rendering system
- plural instances may implement components, operations, or structures described as a single instance. Although individual operations of one or more methods are illustrated and described as separate operations, one or more of the individual operations may be performed concurrently, and nothing requires that the operations be performed in the order illustrated. Structures and functionality presented as separate components in example configurations may be implemented as a combined structure or component. Similarly, structures and functionality presented as a single component may be implemented as separate components.
- the network 25 may include but is not limited to any combination of a LAN, a MAN, a WAN, a mobile, a wired or wireless network, a private network, or a virtual private network.
- client devices are illustrated in Fig. 1 to simplify and clarify the description, it is understood that any number of client computers or display devices are supported and can be in communication with the server 14.
- Modules may constitute either software modules (e.g., code embodied on a machine-readable medium or in a transmission signal) or hardware modules.
- a hardware module is tangible unit capable of performing certain operations and may be configured or arranged in a certain manner.
- one or more computer systems e.g., a standalone, client or server computer system
- one or more hardware modules of a computer system e.g., a processor or a group of processors
- software e.g., an application or application portion
- a hardware module may be implemented mechanically or electronically.
- a hardware module may comprise dedicated circuitry or logic that is permanently configured (e.g., as a special-purpose processor, such as a field programmable gate array (FPGA) or an application-specific integrated circuit (ASIC)) to perform certain operations.
- a hardware module may also comprise programmable logic or circuitry (e.g., as encompassed within a general-purpose processor or other programmable processor) that is temporarily configured by software to perform certain operations. It will be appreciated that the decision to implement a hardware module mechanically, in dedicated and permanently configured circuitry, or in temporarily configured circuitry (e.g., configured by software) may be driven by cost and time considerations.
- the term hardware should be understood to encompass a tangible entity, be that an entity that is physically constructed, permanently configured (e.g., hardwired), or temporarily configured (e.g., programmed) to operate in a certain manner or to perform certain operations described herein.
- hardware modules are temporarily configured (e.g., programmed)
- each of the hardware modules need not be configured or instantiated at any one instance in time.
- the hardware modules comprise a general-purpose processor configured using software
- the general-purpose processor may be configured as respective different hardware modules at different times.
- Software may accordingly configure a processor, for example, to constitute a particular hardware module at one instance of time and to constitute a different hardware module at a different instance of time.
- Hardware and software modules can provide information to, and receive information from, other hardware and/or software modules. Accordingly, the described hardware modules may be regarded as being communicatively coupled. Where multiple of such hardware or software modules exist contemporaneously, communications may be achieved through signal transmission (e.g., over appropriate circuits and buses) that connect the hardware or software modules. In embodiments in which multiple hardware modules or software are configured or instantiated at different times, communications between such hardware or software modules may be achieved, for example, through the storage and retrieval of information in memory structures to which the multiple hardware or software modules have access. For example, one hardware or software module may perform an operation and store the output of that operation in a memory device to which it is communicatively coupled. A further hardware or software module may then, at a later time, access the memory device to retrieve and process the stored output. Hardware and software modules may also initiate communications with input or output devices, and can operate on a resource (e.g., a collection of information).
- a resource e.g., a collection of information
- processors may be temporarily configured (e.g., by software) or permanently configured to perform the relevant operations. Whether temporarily or permanently configured, such processors may constitute processor-implemented modules that operate to perform one or more operations or functions.
- the modules referred to herein may, in some example embodiments, comprise processor-implemented modules.
- the methods or routines described herein may be at least partially processor-implemented. For example, at least some of the operations of a method may be performed by one or processors or processor-implemented hardware modules. The performance of certain of the operations may be distributed among the one or more processors, not only residing within a single machine, but deployed across a number of machines. In some example embodiments, the processor or processors may be located in a single location (e.g., within a home environment, an office environment or as a server farm), while in other embodiments the processors may be distributed across a number of locations.
- the one or more processors may also operate to support performance of the relevant operations in a "cloud computing" environment or as a “software as a service” (SaaS). For example, at least some of the operations may be performed by a group of computers (as examples of machines including processors), these operations being accessible via a network (e.g., the Internet) and via one or more appropriate interfaces (e.g., application program interfaces (APIs).)
- a network e.g., the Internet
- APIs application program interfaces
- the performance of certain of the operations may be distributed among the one or more processors, not only residing within a single machine, but deployed across a number of machines.
- the one or more processors or processor-implemented modules may be located in a single geographic location (e.g., within a home environment, an office environment, or a server farm). In other example embodiments, the one or more processors or processor-implemented modules may be distributed across a number of geographic locations.
- any reference to "one embodiment” or “an embodiment” means that a particular element, feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment.
- the appearances of the phrase “in one embodiment” in various places in the specification are not necessarily all referring to the same embodiment.
- Coupled and “connected” along with their derivatives.
- some embodiments may be described using the term “coupled” to indicate that two or more elements are in direct physical or electrical contact.
- the term “coupled,” however, may also mean that two or more elements are not in direct contact with each other, but yet still cooperate or interact with each other.
- the embodiments are not limited in this context.
- the terms “comprises,” “comprising,” “includes,” “including,” “has,” “having” or any other variation thereof, are intended to cover a non-exclusive inclusion.
- a process, method, article, or apparatus that comprises a list of elements is not necessarily limited to only those elements but may include other elements not expressly listed or inherent to such process, method, article, or apparatus.
- “or” refers to an inclusive or and not to an exclusive or. For example, a condition A or B is satisfied by any one of the following: A is true (or present) and B is false (or not present), A is false (or not present) and B is true (or present), and both A and B are true (or present).
Description
- The present disclosure relates to image rendering systems, such as electronic map display systems, and more specifically to an image rendering engine that renders a label of a feature of an electronic map in a manner that reduces a crawling motion of the labels while changing zooming levels of the map.
- Digital maps are found in and may be displayed by a wide variety of devices, including mobile phones, car navigation systems, hand-held GPS units, computers, and many websites. Although digital maps are easy to view and to use from an end-user's perspective, creating a digital map is a difficult task and can be a time-consuming process. In particular, every digital map begins with storing, in a map database, a set of raw data corresponding to millions of streets and intersections and other features to be displayed as part of a map. The raw map data that is stored in the map database and that is used to generate digital map images is derived from a variety of sources, with each source typically providing different amounts and types of information. This map data must therefore be compiled and stored in the map database before being accessed by map display or map rendering applications and hardware.
- There are, of course, different manners of digitally rendering map images (referred to as digital map images) based on map data stored in a map database. One method of rendering a map image is to store map images within the map database as sets of raster or pixilated images made up of numerous pixel data points, with each pixel data point including properties defining how a particular pixel in an image is to be displayed on an electronic display device. While this type of map data is relatively easy to create and store, the map rendering technique using this data typically requires a large amount of storage space for comprehensive digital map images, and it is difficult to manipulate the digital map images as displayed on a display device in very many useful manners.
- Another, more flexible methodology of rendering images uses what is traditionally called vector image data. Vector image data is typically used in high-resolution and fast-moving imaging systems, such as those associated with gaming systems, and in particular three-dimensional gaming systems. Generally speaking, vector image data (or vector data) includes data that defines specific image objects or elements (also referred to as primitives) to be displayed as part of an image via an image display device. In the context of a map image, such image elements or primitives may be, for example, individual roads, text labels, areas, text boxes, buildings, points of interest markers, terrain features, bike paths, map or street labels, etc. Each image element may generally be drawn as a set of one or more triangles (of different sizes, shapes, colors, fill patterns, etc.), with each triangle including three vertices interconnected by lines. For any particular image element, the image database may store a set of vertex data points that may be used to generate one or more of the triangles. Generally speaking, each vertex data point includes data pertaining to a two-dimensional or a three-dimensional position of the vertex (in an X, Y or an X, Y, Z coordinate system, for example) and various vertex attributes defining properties of the vertex, such as color properties, fill properties, line width properties for lines emanating from the vertex, etc.
- During the image rendering process, the vertices defined for various image elements of an image to be rendered are provided to and are processed in one or more image shaders which operate in conjunction with a graphics processing unit (GPU), such as a graphics card or a rasterizer, to produce a two-dimensional image on a display screen. Generally speaking, an image shader is a set of software instructions used primarily to calculate rendering effects on graphics hardware with a high degree of flexibility. Image shaders are well known in the art and various types of image shaders are available in various application programming interfaces (APIs) provided by, for example, OpenGL and Direct3D, to define special shading functions. Basically, image shaders are simple programs in a high level programming language that describe or determine the traits of either a vertex or a pixel. Vertex shaders, for example, define the traits (e.g., position, texture coordinates, colors, etc.) of a vertex, while pixel or fragment shaders define the traits (color, z-depth and alpha value) of a pixel. A vertex shader is called for each vertex in an image element or primitive so that, for each vertex input into the vertex shader, the vertex shader produces one (updated) vertex output. Each vertex output by the vertex shader is then rendered as a series of pixels onto a block of memory that will eventually be sent to a display screen.
- As a more particular example of image shader technology, Direct3D and OpenGL graphic libraries use three basic types of shaders including vertex shaders, geometry shaders, and pixel or fragment shaders. Vertex shaders are run once for each vertex given to the graphics processor. As noted above, the purpose of a vertex shader is to transform a position of a vertex in a virtual space to the two-dimensional coordinate at which it appears on the display screen (as well as a depth value for the z-buffer of the graphics processor). Vertex shaders can manipulate properties such as position, color, and texture coordinates by setting vertex attributes of the vertices, but cannot create new vertices. The output of the vertex shader is provided to the next stage in the processing pipeline, which is either a geometry shader if present or the rasterizer. Geometry shaders can add and remove vertices from a mesh of vertices and can be used to generate image geometry procedurally or to add volumetric detail to existing images that would be too costly to process on a central processing unit (CPU). If geometry shaders are being used, the output is then sent to the rasterizer. Pixel shaders, which are also known as fragment shaders, calculate the color and light properties of individual pixels in an image. The input to this stage comes from the rasterizer, and the fragment shaders operate to fill in the pixel values of the polygons being sent through the graphics pipeline. Fragment shaders are typically used for scene lighting and related effects such as color toning. There is not a one-to-one relationship between calls to the fragment shader and pixels on the screen as fragment shaders are often called many times per pixel because they are called for every image element or object that is in the corresponding space, even if that image object is occluded.
- The use of vector graphics can be particularly advantageous in a mobile map system in which image data is sent from a centralized map database via a communications network (such as the Internet, a wireless network, etc.) to one or more mobile or remote devices for display. In particular, vector data, once sent to the receiving device, may be more easily scaled and manipulated (e.g., rotated, etc.) than pixilated raster image data. However, the processing of vector data is typically much more time consuming and processor intensive on the image rendering system that receives the data. Moreover, using vector image data that provides a higher level of detail or information to be displayed in a map leads to a higher amount of vector data or vertices that need to be sent to the map rendering system from the map database that stores this information, which can result in higher bandwidth requirements or downloading time in some cases.
- Moreover, in the case of both rasterized map images and vector data generated images, text or other legend symbols, such as those used to label streets, roads and other features of a map, are generally placed on or proximately near to a map object for which the label identifies or provides information. Generally, each time a zoom level of a computerized map is changed or adjusted, labels may be redrawn. The redrawing of labels during zooming may result in a visually distracting crawling motion or effect where the labels are perceived to move along a dimension of its associated map feature.
-
EP 1 840 515 A1 proposes identifying, for a map object such as a road, the center point of the longest portion in the view and rendering a label, such as the name of the road, relative to the center point.US 2008/306684 A1 suggests superimposing, onto a map image, road names that do not overlap with the place names and facility names already in the map image (using normal character size), and then superimposing road names at a smaller scale that do not overlap with the place names, facility names, and road names (using smaller character size).US 6 565 610 B1 describes a technique for placing text, such as names of geographic feature, on a map image. The technique includes identifying candidate locations for text placement and evaluating these candidate locations using weights that reflect the rank of the feature and a size of a text box that can overlap text boxes for other candidate locations. - A computer-implemented method for rendering a map on a display device determines a road feature to display in a viewing window and an ordered set of labels to be displayed on the road feature and labels the road feature as defined in claim 1.
- A system according to the invention is defined in
claim 13. -
-
Fig. 1 is a high level block diagram of a map imaging system that implements communications between a map database stored in a server and one or more map image rendering devices. -
Fig. 2 is a high level block diagram of an image rendering engine used to render map images using map vector data. -
Figs. 3A-3G illustrate a technique for placing labels on a road feature according to an embodiment. -
Figs. 4A-4B illustrate a label sequencing approach. -
Fig. 5 illustrates a process diagram of the labeling technique. -
Fig. 6A and6B illustrate determining a length of a road section by projection on a center line. - A graphics or image rendering system, such as a map image rendering system, receives image data from an image database in the form of vector data that defines various linear features of the map, such as roads, boundaries, etc., in addition to text strings or symbols to be displayed with the features to provide, for example, labels for the features. A label positioning technique generally divides the displayed linear feature into a number of segments and then labels each segment in a sequence based on an ordered set of labels and a position of each segment.
- The label rendering technique positions the text string and/or symbol labels such that the labels are less likely to move along a dimension of the linear feature (e.g., a road feature) when a map zoom level is changed. Generally, the label movement during zooming is referred to as "crawling" and can be a distraction for map viewers. The image rendering system also provides a more consistent labeling process that enhances visual properties within a map image.
- Referring now to
Fig. 1 , a map-relatedimaging system 10 includes amap database 12 stored in aserver 14 or in multiple server located at, for example, a central site or at various spaced apart sites, and also includes multiplemap client devices server 14 via any hardwired orwireless communication network 25, including for example a hardwired or wireless LAN, MAN or WAN, the Internet, or any combination thereof. The map client devices 16-22 may be, for example, mobile phone devices (18), computers such a laptop, desktop or other types of computers (16, 20) or components of other imaging systems such as components of automobile navigation systems (22), etc. Moreover, the client devices 16-22 may be communicatively connected to theserver 14 via any suitable communication system, such as any publically available or privately owned communication network, including those that use hardwired based communication structure, such as telephone and cable hardware, and/or wireless communication structure, such as wireless communication networks, including for example, wireless LANs and WANs, satellite and cellular phone communication systems, etc. - The
map database 12 may store any desired types or kinds of map data including raster image map data and vector image map data. The image rendering systems described herein are highly suited for use with vector image data which defines or includes a series of vertices or vertex data points for each of numerous sets of image objects, elements or primitives within an image to be displayed. However, while the image rendering systems may primarily generate images based on vector image data, in some embodiments, a combination of vector image map data and raster image map data may be utilized. For example, for some zoom levels or when a slow client machine is performing image generation, raster backgrounds without any text (e.g., water, parks, roads, etc.) may be rendered with text generated from vector data. Generally speaking, each of the image objects defined by the vector data will have a plurality of vertices associated therewith and these vertices will be used to display a map related image object to a user via one or more of the client devices 16-22. - As will also be understood, each of the client devices 16-22 includes an image rendering engine having one or
more processors 30, one ormore memories 32, adisplay device 34, and in many cases a rasterizer orgraphics card 36 which are generally programmed and interconnected in known manners to implement or to render graphics (images) on the associateddisplay device 34. Thedisplay device 34 for any particular client device 16-22 may be any type of electronic display device such as a liquid crystal display (LCD), a light emitting diode (LED) display, a plasma display, a cathode ray tube (CRT) display, or any other type of known or suitable electronic display. - Generally, speaking, the map-related
imaging system 10 ofFig. 1 operates such that a user, at one of the client devices 16-22, opens or executes a map application (not shown inFig. 1 ) that operates to communicate with and to obtain map information or map related data from themap database 12 via theserver 14, and that then displays or renders a map image based on the received map data. The map application may allow the user to view different geographical portions of the map data stored in themap database 12, to zoom in or zoom out on a particular geographical location, to rotate, spin or change the two-dimensional or three-dimensional viewing angle of the map being displayed, etc. More particularly, when rendering a map image on a display device or adisplay screen 34 using the system described below, each of the client devices 16-22 downloads map data in the form of vector data from themap database 12 and processes that vector data using one or more image shaders to render an image on the associateddisplay device 34. - Referring now to
Fig. 2 , an image generation orimaging rendering device 40 associated with or implemented by one of the client devices 16-22 is illustrated in more detail. Theimage rendering system 40 ofFig. 2 includes twoprocessors memories user interface 34 and arasterizer 36. In this case, theprocessor 30b, thememory 32b and therasterizer 36 are disposed on a separate graphics card (denoted below the horizontal line), although this need not be the case in all embodiments. For example, in other embodiments, a single processor may be used instead. In addition, theimage rendering system 40 includes anetwork interface 42, a communications andstorage routine 43 and one ormore map applications 48 having map display logic therein stored on thememory 32a, which may be executed on theprocessor 30a. Likewise one or more image shaders in the form of, for example, vertex shaders 44 and fragment shaders 46 are stored on thememory 32b and are executed on theprocessor 30b. Thememories processors network interface 42 includes any well known software and/or hardware components that operate to communicate with, for example, theserver 14 ofFig. 1 via a hardwired or wireless communications network to obtain image data in the form of vector data for use in creating an image display on the user interface ordisplay device 34. Theimage rendering device 40 also includes adata memory 49, which may be a buffer or volatile memory for example, that stores vector data received from themap database 12, the vector data including any number of vertex data points and one or more lookup tables as will be described in more detail. - During operation, the map logic of the
map application 48 executes on theprocessor 30 to determine the particular image data needed for display to a user via thedisplay device 34 using, for example, user input, GPS signals, prestored logic or programming, etc. The display or map logic of theapplication 48 interacts with themap database 12, using the communications routine 43, by communicating with theserver 14 through thenetwork interface 42 to obtain map data, preferably in the form of vector data or compressed vector data from themap database 12. This vector data is returned via thenetwork interface 42 and may be decompressed and stored in thedata memory 49 by the routine 43. In particular, the data downloaded from themap database 12 may be a compact, structured, or otherwise optimized version of the ultimate vector data to be used, and themap application 48 may operate to transform the downloaded vector data into specific vertex data points using theprocessor 30a. In one embodiment, the image data sent from theserver 14 includes vector data generally defining data for each of a set of vertices associated with a number of different image elements or image objects to be displayed on thescreen 34 and possibly one or more lookup tables. If desired, the lookup tables may be sent in, or may be decoded to be in, or may be generated by themap application 48 to be in the form of vector texture maps which are known types of data files typically defining a particular texture or color field (pixel values) to be displayed as part of an image created using vector graphics. More particularly, the vector data for each image element or image object may include multiple vertices associated with one or more triangles making up the particular element or object of an image. Each such triangle includes three vertices (defined by vertex data points) and each vertex data point has vertex data associated therewith. In one embodiment, each vertex data point includes vertex location data defining a two-dimensional or a three-dimensional position or location of the vertex in a reference or virtual space, as well as an attribute reference. Each vertex data point may additionally include other information, such as an object type identifier that identifies the type of image object with which the vertex data point is associated. The attribute reference, referred to herein as a style reference or as a feature reference, references or points to a location or a set of locations in one or more of the lookup tables downloaded and stored in thedata memory 43. - Existing map rendering systems may receive map data from a database in the form of vector data or image data and render a map surface including a set of map features on a display based on the vector data or image data. The map rendering systems may label one or more of the map features with text or image labels (e.g., icons) that identify the map features. When a zoom level of a displayed map is changed, the map features may be re-labeled. Existing systems may re-label linear map features (such as roads, boundaries, etc.) in a manner that produces a crawling effect of the labels, where the labels appear to move along a dimension of the linear map feature. This can be highly distracting to map viewers. The labeling technique described herein may be used to reduce the crawling effect of labels during zoom level changes. Generally, the labeling technique for rendering a label along a map object involves iteratively bisecting a road section using labels of an ordered set of labels. A label may be placed in a sequence on the resultant segments of the bisections based and an ordered set of labels and a bisection order. The bisection labeling process may be repeated each time a zoom level is changed.
-
Figs. 3A-3G illustrate a labeling technique for placing labels on a road feature according to an embodiment the example ofFigs. 3A-3G , a label set includes a primary label or icon I, and auxiliary labels Ta and Tb. The label set may be ordered so that the primary label I is first in the order. In this example of an ordered set of labels, I is first, Ta is second, and Tb is third. Further, in this example, Ta has a width of 30 pixels wide, Tb has a width of 70 pixels, and I has a width of 20 pixels. However, the labels could be any other size or dimension and labels within the set of labels may include more or less than three labels. -
Fig. 3A illustrates astraight road feature 400 that may be displayed within a viewing window (not shown). Generally, a viewing window may be defined by a zoom level, a viewing window position, and a viewing window size for rendering the map with theroad feature 400 on the display device. A zoom level may correspond with a magnification level.Fig. 3A illustrates that theroad feature 400 has afirst endpoint 401 and asecond endpoint 402 and a length of 200 pixels on the display screen. It should be noted that theroad feature 400 may be completely shown in a single viewing window orroad feature 400 may represent only a section of a larger feature that is not displayed or cannot be displayed on the viewing window. A check is made to determine whether theroad feature 400 is long enough to accommodate or fit a corresponding label. In this case, 200 pixels is enough to accommodate the first label, I, which is 20 pixels wide. Amidpoint 403 of the road feature then is determined by, for example, dividing the length of the road feature in half. Accordingly, inFig. 3A , a midpoint of theroad feature 400 is calculated to be at 100 pixels from eitherendpoint resultant road segments resultant road segment 404 being located at 0-90 pixels andresultant road segment 406 being located at 110-200 pixels. Theresultant road segments road feature 400 to produce two equal length resultant segments and subtracting a portion of the length of the label from the resultant road segments. In this particular case, equal portions of the length of the label are subtracted from each of the resultant road segments so that theresultant road segments midpoint 403 of the road feature (or road segment) being bisected. - The two
road segments road segment 404 orroad segment 406. Because of the orientation of the road segments within the viewing window,road segment 404 is referred to herein as a left branch segment (at range 0-90 pixels) that is proximate thefirst endpoint 401 ofroad feature 400 androad segment 406 is referred to herein as a right branch segment (at range 110-200 pixels) that is proximatesecond endpoint 402. A default process may select the left branch segment. In this case, theleft branch segment 404 is labeled with the subsequent label of the set Ta andright branch segment 406 is labeled with the subsequent label Tb. Generally, when the end of the ordered set of labels is reached (e.g., selected or placed) the order starts again from the beginning of the ordered set of labels. Thus, because label Tb has been selected for theright branch segment 406, a subsequent label in the ordered set of labels starts over with the label I. -
Fig. 3B illustrates a process of labeling theleft branch segment 404 with label Ta. The center of the segment 0-90 in this example is determined to be atpixel 45. The label Ta is 30 pixels wide and theleft branch segment 404 is 90 pixels wide, and thus, the label Ta fits. A length of a range 30-60 pixels then is reserved for the label Ta. After reserving the space for the label Ta, twosegments road feature 400 may determine which of the pair of resultant child segments may be labeled first with the next label of the ordered set. In one embodiment, a parent segment that is proximate a first endpoint determines that a corresponding resultant segment that is also proximate the first endpoint will be labeled next and vice versa. In the example ofFig. 3B , theleft segment 408 is labeled first with the label Tb which is the next label in the ordered set of labels after the parent label Ta of theparent segment 404. Theright segment 410 is labeled with a label subsequent Tb, which is I (since the ordered set repeats when the end of the set is reached). -
Fig. 3C illustrates an approach to labeling thesegment 408. A center or midpoint of segment 0-30 is atpixel 15. The label Tb is 70 pixels wide while thesegment 408 is only 30 pixels wide. Because the label Tb does not fit, the label Tb is not placed and the processing ofsegment 408 ends.Fig. 3D illustrates an approach to labeling the segment 410 (at range 60-90) with the next label in the ordered set of labels, Icon I (as the set of labels repeat). The center of segment 410 (at range 60-90) is determined to be atpixel 75. The Icon I is 20 pixels wide and the segment is 30, and thus, the label I fits and is placed along the segment at the center point atpixel 75. Pixels 65-85 are reserved for Icon I (which is 20 pixels wide). Twosegments resultant segments parent segment 410 that is a right segment (being proximate to endpoint 200), the right child segment 412 (at range 85-90) is processed first with the next label of the ordered set of labels, Ta, and the left segment 412 (at range 60-65) is labeled with a subsequent label of the ordered set of labels, Tb. The right segment 414 (at range 85-90) is 5 pixels wide for a 30 pixel wide label, so the label Ta will not fit and processing of this branch ends. The left segment 412 (at range 60-65) is 5 pixels wide for a 70 pixel wide label, so the label Tb will not fit and processing of this branch ends. Because there are no more segments to process from the left branch segment (at range 0-90) of theroad feature 400, the right branch segment 406 (at range 110-200) of theroad feature 400 is processed with the label Tb, as illustrated inFig. 3E . A center of the segment 110-200 is determined to be atpixel 155. The label Tb is 70 pixels wide and thesegment 406 is 90 pixels wide, and thus, the label Tb fits. A length of 70 pixels at range 120-190 pixels are reserved for Tb. Two resultant road segments, 416 and 418 having ranges 110-120 and 190-200, respectively, remain. Because theresultant road segments road segment parent 406, the right road segment child 416 (at range 190-200) of the remaining road segments are labeled first so that the resultant road segment (at range 190-200) 418 will be labeled with the subsequent label, I, and the resultant road segment (at range 110-120) 416 will be labeled with the label after, Ta. The road segment 418 (at range 190-200) is 10 pixels wide for a 20 pixel wide label, so processing for this road segment ends. The resultant road segment 416 (at range 110-120) has a width of 10 pixels for a 30 pixel wide label, so processing of this segment ends. Because there are no remaining road segments for processing,Figure 3E illustrates the labeling results with the label Ta segment 30-60, the label I at segment 65-85, the label I at segment 90-110, and the label Tb at segment 120-190. -
Figure 3F illustrates the same process, technique, or algorithm ofFigs. 3A-3E applied to a road feature that is 100 pixels long andFigure 3G illustrates the same process, technique, or algorithm ofFigs. 3A-3E applied to a road feature that is 400 pixels long. In the 100 pixel case ofFig. 3F , only two labels fit: the label Ta for segment 5-35 and the label I for segment 40-60. The placement of the labels Ta and I are in the same relative place: the label I in the center, the label Ta left of center. There is no space for the label Tb or the second Icon label I for the 100 pixel road feature. In the 400 pixel case ofFig. 3G , the same labels ofFig. 3E are placed but with more labels between them. A possible advantage of this labeling technique is that the Ta, I, and Tb labels are placed in roughly the same relative locations on the road or at least may visually appear so. The process of labeling may be performed each time a zoom level is changed to reduce the crawling motion or effect that may happen to a greater extent otherwise. -
FIG. 4A illustrates a diagram for determining the sequencing of the labels during the labeling technique described above. The road section that is displayed on a viewing window of a map application may be labeled with a first primary label. Using the same ordered set of labels as forFIGS. 3A-3G (i.e., the ordered set of labels I, Ta, and Tb), the first label for the first main segment is I. At a first division orbisection 502, the process or technique may select a left branch 504 (e.g., proximate a first endpoint) or a right branch (e.g., proximate a second endpoint) 506, where the branches correspond to road segments and/or bisections. The example ofFIGS. 3A-3G used a left branch approach and this left approach is illustrated inFIG. 4A . Thenumbers 1 and 2 at each branch pair level ofFIG. 4A indicate the order of labeling for that branch. Generally, each of a pair of resultant road segments that is produced
by bisecting the road feature or a road segment of the road feature is either proximate the first or the second endpoint of the road feature. In the diagram ofFig. 4A , left branches may correspond with proximity to the first endpoint while right branches may correspond with proximity to the second endpoint. For example, atbisection 502, the road segment corresponding withbranch 504 may be proximate the first endpoint (e.g., a left endpoint) with respect to the road segment corresponding withbranch 506, while the road segment corresponding withbranch 506 may be proximate the second endpoint (e.g., a right endpoint) with respect to road segment corresponding withbranch 504. An order of labeling each of the resultant pair of road segments is based, in part, on a proximity to the first or second endpoint (or left or right endpoint) of the parent road segment that produced the resultant pair of child road segments. In an embodiment, when the parent segment of a pair of resultant child road segments is proximate a first endpoint (left endpoint), then the resultant child road segment that is proximate the first endpoint (left endpoint) will be labeled first. Accordingly, when the parent segment of a pair of resultant child segments is proximate a second endpoint (right endpoint), then the resultant child segment that is proximate the second endpoint (right endpoint) will be labeled first. Thus, for example, at abisection 508 that corresponds with a left parent branch or left parent road segment, the leftchild branch segment 510 is labeled first and the rightchild branch segment 512 is labeled second. - An order of labeling each of the resultant pair of road segments is also based on the label of the parent road segment that produced the pair of child road segments. The first child road segment of the pair of child road segments to be labeled may be labeled with the next label of the ordered set of labels after the parent label. The second child road segment of the pair of child road segments may be labeled with the next label of the ordered set of labels after the first child road segment. For example, the road segment corresponding with the
left branch 510 may be labeled with the Ta, which is the subsequent label of the ordered set following the parent label I, while the road segment corresponding withright branch 512 may be labeled with a label subsequent the left branch road segment, which is Tb. -
Fig. 4B illustrates a similar sequencing diagram asFig. 4A except that during thefirst bisection 520, aright branch 524 is selected first for labeling followed by aleft branch 522. Generally, the same rules apply as for sequencing ofFig. 4B . In other words, the order in which the a pair of resultant segments is labeled is based on the parent segment that produced the pair, based on the above described rules. -
Fig. 5 illustrates a process flow diagram or flow chart of a method, routine, orprocess 300 that may be used to render labels on a road feature of a map. As an example only, the routine 300 may be used in a map rendering engine, such as that ofFig. 2 , to render a label on or about the map feature in a manner that reduces crawl, creep, or drift. Generally, speaking, the routine orprocess 300 may be implemented as one or more software routines or applications, including routines that are part of themap application 48. These routines may be implemented or written in any desired language, such as Javascript, C code, etc. and may be executed on theprocessor 30. Additionally, these routines may use therasterizer 36 to render an image on thedisplay device 34. - A
block 302 may determine a linear map feature to be displayed on a viewing window. The linear map feature may be a road, street, boundary, river segment, etc. The viewing window may be defined by a particular zoom level, wherein the zoom level corresponds to a magnification level of a map portion being displayed. Generally, for a constant viewing window size, as zoom level increases, map area decreases. It should be noted that in some embodiments, changing zoom level may correspond to redrawing a map area with greater or lesser map data or detail, where the map data used in a prior zoom level is different than that for the redrawn level. A zoom level change where additional data (e.g., vector data) is used to redraw a map may be different from a zoom level change where only a scale of the map area is changed (e.g., same level of detail but greater magnification). It should be noted that in some embodiments, each time a zoom level is changed and/or a viewing window position is changed, the existing labels may be removed and the labeling technique described herein repeated. - The entirety of a linear map feature may not always be displayed entirely within a viewing window. Instead, only a portion less than the entire feature may be shown. In this case, the viewable portion of the linear feature may be limited by a size and a position of the viewing window and the zoom level. In some computer mapping applications, the position of the viewing window, the size of the viewing window, and/or the zoom level may be changed to reveal some or all of the road feature, depending on the total length or size of street. In one embodiment, a larger portion of a road feature may be processed using the described labeling technique even though the entire road feature is not displayed in a current viewing window. In this case, labels on portions of the road that are not visible are not displayed until or unless the viewing window is changed to show those portions. While the process of
Fig. 5 may be applied to any linear map feature, the blocks ofFig. 5 may be discussed with respect to a road feature. - A
block 304 may determine an ordered set of labels for labeling the road feature. The set of labels may be text describing the road feature. Alternatively, the set of labels may include images representing, for example, icons. The set of labels may be generally ordered by a priority of each label. In one embodiment, the first label in the sequence may be the highest priority label, with subsequent labels having less priority. In one embodiment, a first label may be a primary label that may be most frequently applied or placed on a road. Ancillary second, third, etc. labels may have less placement frequency. The ordered set of labels may be determined by a mapping application or retrieved from a table. - At
block 310, a road segment of the road feature may be selected for labeling. In a first iteration of this process, an entire displayed road section may be selected inblock 310. Ablock 314 determines or selects a next label of the ordered set of labels. In a first iteration of the method ofFig. 3 , the selected label may represent a first, primary label. - At
block 316, a length of the road segment along its center line may be determined along with a length of the selected label. Ablock 318 may determine whether the length of the road segment is long enough to accommodate the length of the label. If, atblock 318, the segment is not long enough for the determined label, the segment may be discarded or marked as being processed atblock 320. The process may then continue back atblock 310 for another unprocessed road segment. Theblock 310 may search for road segments that are not marked processed. If the length of the road segment atblock 318 is long enough to accommodate the length of the label, block 322 may place the selected label at the center of the road segment. Ablock 322 may determine where to place the selected label by dividing the length of the road segment in half (e.g., bisecting the road) and placing the selected label at the bisection or midpoint. Ablock 324 may determine a pair of resultant segments produced by bisecting the determined road segment. Theblock 324 may reserve a length of space about the midpoint of the road segment for the label so that the resultant segments do not include the length of the label. This may be performed by subtracting half the length of the label from each of the two segments produced from a bisection of the determined road segment. The resultant segments may represent additional unprocessed segments. - The
blocks 304 and/or 310 may include determining an order of the label processing or segmentation such that the labels are placed on the road segments according to the order illustrated inFigs. 4A or4B . The process ofFig. 3 continues from theblock 324 to theblock 310 where a next unprocessed road segment may be determined for processing. The process ofFig. 3 may be repeated for every remaining unprocessed segment until a stop condition is reached. In one embodiment, a stop condition may be reached when no remaining segments can fit a corresponding label from the ordered label set. When this labeling process is performed, the overall placement of the labels may be visually perceived to be similar at two or more different zoom levels. Another stop condition may be reached when the number of bisections executed is above a threshold number of bisections. This threshold number may be a fixed number or may be adjusted based on a total length of a road feature that is displayable within a viewing window. In some embodiments, stop conditions may be based on characteristics of the label set. For example, the label set may not be a fixed set, where certain labels may have a fixed number of uses. In the currently described label set, for example, the icon label may only be used a single time before the icon label is removed from the label set. In this embodiment, a stop condition may be encountered when there are no remaining labels in the set. -
Fig. 6A illustrates acurved road 600 within aviewing window 602. In one embodiment, the length of theroad 600 may be calculated or determined atblock 316 based on a flattened projection on acenterline 604. In this case, the road is flattened along ahorizontal center line 604. Generally, points along theroad 600 may be represented as a distance on thestraight line 604, thereby eliminating a vertical length component of the road. Flattening the road may be advantageous in situations when the road includes a plurality of turns and windings that may tend to visually distort or compress road labels if the actual length of the road (e.g., the length along a road curve) is used for length determinations.Fig. 6B illustrates acurved road 610 within aviewing window 612 that is projected along avertical centerline 614. A choice of vertical or horizontal lines for flattening may be based on the shape and orientation of the road within the viewing window. - Of course, the labeling encoding techniques described herein may be altered or varied in any number of manners to provide an image rendering system, such as a map rendering system, with the ability to render individual text characters within a text string along a curved, e.g., multi-segmented line. Throughout this specification, plural instances may implement components, operations, or structures described as a single instance. Although individual operations of one or more methods are illustrated and described as separate operations, one or more of the individual operations may be performed concurrently, and nothing requires that the operations be performed in the order illustrated. Structures and functionality presented as separate components in example configurations may be implemented as a combined structure or component. Similarly, structures and functionality presented as a single component may be implemented as separate components. These and other variations, modifications, additions, and improvements fall within the scope of the subject matter herein.
- For example, the
network 25 may include but is not limited to any combination of a LAN, a MAN, a WAN, a mobile, a wired or wireless network, a private network, or a virtual private network. Moreover, while only four client devices are illustrated inFig. 1 to simplify and clarify the description, it is understood that any number of client computers or display devices are supported and can be in communication with theserver 14. - Additionally, certain embodiments are described herein as including logic or a number of components, modules, or mechanisms. Modules may constitute either software modules (e.g., code embodied on a machine-readable medium or in a transmission signal) or hardware modules. A hardware module is tangible unit capable of performing certain operations and may be configured or arranged in a certain manner. In example embodiments, one or more computer systems (e.g., a standalone, client or server computer system) or one or more hardware modules of a computer system (e.g., a processor or a group of processors) may be configured by software (e.g., an application or application portion) as a hardware module that operates to perform certain operations as described herein.
- In various embodiments, a hardware module may be implemented mechanically or electronically. For example, a hardware module may comprise dedicated circuitry or logic that is permanently configured (e.g., as a special-purpose processor, such as a field programmable gate array (FPGA) or an application-specific integrated circuit (ASIC)) to perform certain operations. A hardware module may also comprise programmable logic or circuitry (e.g., as encompassed within a general-purpose processor or other programmable processor) that is temporarily configured by software to perform certain operations. It will be appreciated that the decision to implement a hardware module mechanically, in dedicated and permanently configured circuitry, or in temporarily configured circuitry (e.g., configured by software) may be driven by cost and time considerations.
- Accordingly, the term hardware should be understood to encompass a tangible entity, be that an entity that is physically constructed, permanently configured (e.g., hardwired), or temporarily configured (e.g., programmed) to operate in a certain manner or to perform certain operations described herein. Considering embodiments in which hardware modules are temporarily configured (e.g., programmed), each of the hardware modules need not be configured or instantiated at any one instance in time. For example, where the hardware modules comprise a general-purpose processor configured using software, the general-purpose processor may be configured as respective different hardware modules at different times. Software may accordingly configure a processor, for example, to constitute a particular hardware module at one instance of time and to constitute a different hardware module at a different instance of time.
- Hardware and software modules can provide information to, and receive information from, other hardware and/or software modules. Accordingly, the described hardware modules may be regarded as being communicatively coupled. Where multiple of such hardware or software modules exist contemporaneously, communications may be achieved through signal transmission (e.g., over appropriate circuits and buses) that connect the hardware or software modules. In embodiments in which multiple hardware modules or software are configured or instantiated at different times, communications between such hardware or software modules may be achieved, for example, through the storage and retrieval of information in memory structures to which the multiple hardware or software modules have access. For example, one hardware or software module may perform an operation and store the output of that operation in a memory device to which it is communicatively coupled. A further hardware or software module may then, at a later time, access the memory device to retrieve and process the stored output. Hardware and software modules may also initiate communications with input or output devices, and can operate on a resource (e.g., a collection of information).
- The various operations of example methods described herein may be performed, at least partially, by one or more processors that are temporarily configured (e.g., by software) or permanently configured to perform the relevant operations. Whether temporarily or permanently configured, such processors may constitute processor-implemented modules that operate to perform one or more operations or functions. The modules referred to herein may, in some example embodiments, comprise processor-implemented modules.
- Similarly, the methods or routines described herein may be at least partially processor-implemented. For example, at least some of the operations of a method may be performed by one or processors or processor-implemented hardware modules. The performance of certain of the operations may be distributed among the one or more processors, not only residing within a single machine, but deployed across a number of machines. In some example embodiments, the processor or processors may be located in a single location (e.g., within a home environment, an office environment or as a server farm), while in other embodiments the processors may be distributed across a number of locations.
- The one or more processors may also operate to support performance of the relevant operations in a "cloud computing" environment or as a "software as a service" (SaaS). For example, at least some of the operations may be performed by a group of computers (as examples of machines including processors), these operations being accessible via a network (e.g., the Internet) and via one or more appropriate interfaces (e.g., application program interfaces (APIs).)
- The performance of certain of the operations may be distributed among the one or more processors, not only residing within a single machine, but deployed across a number of machines. In some example embodiments, the one or more processors or processor-implemented modules may be located in a single geographic location (e.g., within a home environment, an office environment, or a server farm). In other example embodiments, the one or more processors or processor-implemented modules may be distributed across a number of geographic locations.
- Some portions of this specification are presented in terms of algorithms or symbolic representations of operations on data stored as bits or binary digital signals within a machine memory (e.g., a computer memory). These algorithms or symbolic representations are examples of techniques used by those of ordinary skill in the data processing arts to convey the substance of their work to others skilled in the art. As used herein, an "algorithm" or a "routine" is a self-consistent sequence of operations or similar processing leading to a desired result. In this context, algorithms, routines and operations involve physical manipulation of physical quantities. Typically, but not necessarily, such quantities may take the form of electrical, magnetic, or optical signals capable of being stored, accessed, transferred, combined, compared, or otherwise manipulated by a machine. It is convenient at times, principally for reasons of common usage, to refer to such signals using words such as "data," "content," "bits," "values," "elements," "symbols," "characters," "terms," "numbers," "numerals," or the like. These words, however, are merely convenient labels and are to be associated with appropriate physical quantities.
- Unless specifically stated otherwise, discussions herein using words such as "processing," "computing," "calculating," "determining," "presenting," "displaying," or the like may refer to actions or processes of a machine (e.g., a computer) that manipulates or transforms data represented as physical (e.g., electronic, magnetic, or optical) quantities within one or more memories (e.g., volatile memory, non-volatile memory, or a combination thereof), registers, or other machine components that receive, store, transmit, or display information.
- As used herein any reference to "one embodiment" or "an embodiment" means that a particular element, feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment. The appearances of the phrase "in one embodiment" in various places in the specification are not necessarily all referring to the same embodiment.
- Some embodiments may be described using the expression "coupled" and "connected" along with their derivatives. For example, some embodiments may be described using the term "coupled" to indicate that two or more elements are in direct physical or electrical contact. The term "coupled," however, may also mean that two or more elements are not in direct contact with each other, but yet still cooperate or interact with each other. The embodiments are not limited in this context.
- As used herein, the terms "comprises," "comprising," "includes," "including," "has," "having" or any other variation thereof, are intended to cover a non-exclusive inclusion. For example, a process, method, article, or apparatus that comprises a list of elements is not necessarily limited to only those elements but may include other elements not expressly listed or inherent to such process, method, article, or apparatus. Further, unless expressly stated to the contrary, "or" refers to an inclusive or and not to an exclusive or. For example, a condition A or B is satisfied by any one of the following: A is true (or present) and B is false (or not present), A is false (or not present) and B is true (or present), and both A and B are true (or present).
Claims (13)
- A computer-implemented method for rendering a map on a display device (42) comprising:determining, using a computer device (16), a road feature (400) to be displayed on a viewing window , said viewing window capable of being defined by a zoom level at a given time, the zoom level capable of being changed upon selection; the road feature having a first (401) and a second endpoint (402);and at a first zoom level:determining, using the computer device, an ordered set of labels ({I, Ta, Tb}) to be displayed with respect to the road feature;labeling, using the computer device, the road feature by:1) determining a segment (400, 404, 406) of the road feature to apply a label;2) determining a length of the segment;3) placing a next label from the ordered set of labels at the centre of the determined segment if a length of the next label is less than the length of the determined segment, including displaying the next label lengthwise along the determined segment, or ending the processing of the segment and marking the segment as processed if the length of the next label is not less than the length of the determined segment;4) if the label is placed in the segment in step 3, then determining a pair of resultant segments ((404, 406), 408,410)) produced by placing the label on the determined segment on which the label is placed, with a first one (404, 408) of the pair of resultant segments being on one side of the label along the determined segment and a second one (406, 410) of the pair of resultant segments being on the opposite side of the label with respect to the one side of the label along the determined segment, the determined segment defining a parent segment; andwherein , if the parent segment is proximate to one of the endpoints of the road feature (401,402), then repeating steps 2, 3 and 4 until a stop condition is reached, first for one of the pair of resultant segments that is proximate to same end point of the road feature as the parent segment, and then for the other one of the pair of resultant segments that is proximate to the other end point of the road feature; otherwise the method further comprising repeating steps 2, 3 and 4 until a stop condition is reached, first for one of the pair of resultant segments selected by the computing device, and then for the other one of the pair of resultant segments.
- The computer- implemented method of claim 1, further including determining, using the computer device, that a resultant segment of the pair of resultant segments that is proximate to the first endpoint of the road feature is to be labeled first with the next label of the ordered set of labels when the parent segment is proximate the first endpoint while the other resultant segment of the pair of resultant segments that is distal to the first endpoint is to be labeled with a label subsequent the next label of the ordered list.
- The computer-implemented method of claim 1, further including determining, using the computer device, that a resultant segment of the pair of resultant segments that is proximate to the second endpoint of the road feature is to be labeled first with the next label of the ordered set of labels when the parent segment is proximate the second endpoint while the other resultant segment of the pair that is distal to the second endpoint is to be labeled with a label subsequent the next label of the ordered list..
- The computer- implemented method of claim 1, wherein determining a segment of the road feature to apply a label includes determining whether the segment is marked as processed
- The computer- implemented method of claim 1, further including adjusting, using the computer device, the zoom level from the first zoom level to a second different zoom level and repeating steps 1, 2, 3 and 4 for the road feature at the second zoom level.
- The computer- implemented method of claim 1, wherein placing the next label from the ordered set of labels includes repeating the order of the labels after a last label of the ordered set has been selected as the next label.
- The computer- implemented method of claim 1, wherein a stop condition is reached when no resultant segment exists whose size is greater than or equal to that of a corresponding sequential label.
- The computer- implemented method of claim 1, wherein the ordered set of labels are ordered by a priority of each label.
- The computer-implemented method of claim 1, further including determining, using the computer device, a length of the road feature, being a curved road feature, by calculating the length of the road feature along a center line of the road feature.
- The computer- implemented method of claim 9, wherein the center line is one of a horizontal line or a vertical line of the viewing window.
- The computer- implemented method of claim 1, wherein placing the next label from the ordered set of labels at the center of the determined segment includes aligning a center of the next label with the center of the determined segment.
- The computer- implemented method of claim 11, wherein determining the pair of resultant segments produced by placing the label on the determined segment includes bisecting the determined segment and subtracting half a length of the placed label from each of the segments resulting from the bisection to produce the pair of resultant segments.
- A system comprising:one or more processors; anda computer-readable storage device storing instructions that, when executed by the one or more processors, cause the one or more processors to perform the method of any of the preceding claims.
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/244,742 US8237745B1 (en) | 2011-09-26 | 2011-09-26 | Label positioning technique to reduce crawling during zoom activities |
PCT/US2012/056199 WO2013048843A1 (en) | 2011-09-26 | 2012-09-20 | Label positioning technique to reduce crawling during zoom activities |
Publications (3)
Publication Number | Publication Date |
---|---|
EP2761584A1 EP2761584A1 (en) | 2014-08-06 |
EP2761584A4 EP2761584A4 (en) | 2015-06-03 |
EP2761584B1 true EP2761584B1 (en) | 2019-01-23 |
Family
ID=46583247
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP12837289.3A Active EP2761584B1 (en) | 2011-09-26 | 2012-09-20 | Label positioning technique to reduce crawling during zoom activities |
Country Status (4)
Country | Link |
---|---|
US (1) | US8237745B1 (en) |
EP (1) | EP2761584B1 (en) |
DE (1) | DE202012013450U1 (en) |
WO (1) | WO2013048843A1 (en) |
Families Citing this family (16)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP5126272B2 (en) * | 2010-03-31 | 2013-01-23 | 株式会社デンソー | Navigation system |
US9223769B2 (en) | 2011-09-21 | 2015-12-29 | Roman Tsibulevskiy | Data processing systems, devices, and methods for content analysis |
US9886794B2 (en) | 2012-06-05 | 2018-02-06 | Apple Inc. | Problem reporting in maps |
US9230556B2 (en) | 2012-06-05 | 2016-01-05 | Apple Inc. | Voice instructions during navigation |
US9482296B2 (en) * | 2012-06-05 | 2016-11-01 | Apple Inc. | Rendering road signs during navigation |
US9418672B2 (en) | 2012-06-05 | 2016-08-16 | Apple Inc. | Navigation application with adaptive instruction text |
US9052197B2 (en) | 2012-06-05 | 2015-06-09 | Apple Inc. | Providing navigation instructions while device is in locked mode |
US10176633B2 (en) | 2012-06-05 | 2019-01-08 | Apple Inc. | Integrated mapping and navigation application |
US9997069B2 (en) | 2012-06-05 | 2018-06-12 | Apple Inc. | Context-aware voice guidance |
US20130321400A1 (en) | 2012-06-05 | 2013-12-05 | Apple Inc. | 3D Map Views for 3D Maps |
US9448754B2 (en) * | 2013-05-15 | 2016-09-20 | Google Inc. | Resolving label collisions on a digital map |
US9530239B2 (en) | 2013-11-14 | 2016-12-27 | Microsoft Technology Licensing, Llc | Maintaining 3D labels as stable objects in 3D world |
US9483496B1 (en) * | 2013-12-20 | 2016-11-01 | Amazon Technologies, Inc. | Label placement for line features |
US9928572B1 (en) | 2013-12-20 | 2018-03-27 | Amazon Technologies, Inc. | Label orientation |
US10482635B2 (en) * | 2014-09-29 | 2019-11-19 | Adp, Llc | Chart labeling system |
CN111179346B (en) * | 2019-12-26 | 2023-06-06 | 广东星舆科技有限公司 | Feature extraction method and device of label image, positioning method and positioning equipment |
Family Cites Families (22)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JPH04288584A (en) | 1991-03-18 | 1992-10-13 | Pioneer Electron Corp | Map display device |
US5793310A (en) | 1994-02-04 | 1998-08-11 | Nissan Motor Co., Ltd. | Portable or vehicular navigating apparatus and method capable of displaying bird's eye view |
US5684940A (en) | 1995-03-13 | 1997-11-04 | Rutgers, The States University Of New Jersey | Computer-implemented method and apparatus for automatically labeling area regions of maps using two-step label placing procedure and for curved labeling of point features |
JP3358770B2 (en) | 1995-07-10 | 2002-12-24 | 科学技術振興事業団 | Light control substance and light control method |
KR100194762B1 (en) | 1996-10-05 | 1999-06-15 | 이계철 | Placement of Names for Point Objects Using Planar Writing |
US6565610B1 (en) * | 1999-02-11 | 2003-05-20 | Navigation Technologies Corporation | Method and system for text placement when forming maps |
WO2001071484A1 (en) | 2000-03-17 | 2001-09-27 | Vicinity Corp. | System and method for non-uniform scaled mapping |
US6405129B1 (en) * | 2000-11-29 | 2002-06-11 | Alpine Electronics, Inc. | Method of displaying POI icons for navigation apparatus |
GB0208835D0 (en) | 2002-04-18 | 2002-05-29 | Koninkl Philips Electronics Nv | A method of labelling an image on a display |
US7076505B2 (en) * | 2002-07-11 | 2006-07-11 | Metrobot Llc | Method, apparatus, and computer program product for providing a graphical user interface with a linear map component |
US7425968B2 (en) | 2003-06-16 | 2008-09-16 | Gelber Theodore J | System and method for labeling maps |
US7324896B1 (en) * | 2003-08-04 | 2008-01-29 | Aol Llc | Using a corridor search to identify locations of interest along a travel route |
US7414637B2 (en) | 2004-09-10 | 2008-08-19 | Telmap Ltd. | Placement of map labels |
ATE447160T1 (en) | 2006-03-31 | 2009-11-15 | Research In Motion Ltd | METHOD AND DEVICE FOR DYNAMIC LABELING OF MAP OBJECTS IN VISUALLY DISPLAYED CARDS OF MOBILE COMMUNICATION DEVICES |
US20080074423A1 (en) * | 2006-09-25 | 2008-03-27 | Raytheon Company | Method and System for Displaying Graphical Objects on a Digital Map |
US7692655B2 (en) | 2007-02-16 | 2010-04-06 | Mitac International Corporation | Apparatus and method of generating curved baseline for map labeling |
JP4850133B2 (en) * | 2007-06-06 | 2012-01-11 | アルパイン株式会社 | Map display device and map display method |
US8463424B2 (en) | 2007-11-07 | 2013-06-11 | Research In Motion Limited | System and method for displaying address information on a map |
TW200936985A (en) | 2008-02-21 | 2009-09-01 | Mitac Int Corp | Navigation device having region label and method thereof |
US20090244095A1 (en) | 2008-04-01 | 2009-10-01 | Research In Motion Limited | Run-time label cache for efficient map labeling |
CA2732394C (en) | 2008-10-20 | 2014-05-13 | Research In Motion Limited | Method and system for rendering of labels |
US8515666B2 (en) | 2009-02-26 | 2013-08-20 | Research In Motion Limited | Method for displaying map labels for geographical features having alternate names |
-
2011
- 2011-09-26 US US13/244,742 patent/US8237745B1/en active Active
-
2012
- 2012-09-20 EP EP12837289.3A patent/EP2761584B1/en active Active
- 2012-09-20 DE DE202012013450.7U patent/DE202012013450U1/en not_active Expired - Lifetime
- 2012-09-20 WO PCT/US2012/056199 patent/WO2013048843A1/en active Application Filing
Non-Patent Citations (1)
Title |
---|
None * |
Also Published As
Publication number | Publication date |
---|---|
US8237745B1 (en) | 2012-08-07 |
WO2013048843A1 (en) | 2013-04-04 |
EP2761584A4 (en) | 2015-06-03 |
DE202012013450U1 (en) | 2017-01-23 |
EP2761584A1 (en) | 2014-08-06 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
EP2761584B1 (en) | Label positioning technique to reduce crawling during zoom activities | |
US8803901B1 (en) | Map rendering using interpolation of style parameters across zoom levels | |
US8917276B2 (en) | Rendering a text image following a line | |
EP2727006B1 (en) | Rendering a text image following a line | |
US8243102B1 (en) | Derivative-based selection of zones for banded map display | |
US9384596B2 (en) | Visualization of obscured objects in 3D space | |
US9811879B2 (en) | Keeping map labels consistent across multiple zoom levels | |
US8730258B1 (en) | Anti-aliasing of straight lines within a map image | |
US9093006B2 (en) | Image shader using style attribute references | |
US9721363B2 (en) | Encoding polygon data for fast retrieval and rendering | |
US10319062B2 (en) | Rendering map data using descriptions of raster differences | |
US8760451B2 (en) | Rendering a text image using texture map character center encoding with character reference encoding | |
US20150178977A1 (en) | Rendering Vector Maps in a Geographic Information System | |
US9495767B2 (en) | Indexed uniform styles for stroke rendering | |
EP2766876B1 (en) | Use of banding to optimize map rendering in a three-dimensional tilt view | |
US20150130845A1 (en) | Out-of-viewpoint indicators for relevant map features | |
US9092907B2 (en) | Image shader using two-tiered lookup table for implementing style attribute references | |
KR20200070320A (en) | Dynamic re-styling of digital maps | |
US8976188B1 (en) | Optimized data communication system and method for an image rendering system | |
US20130002679A1 (en) | Rendering a text image using texture map character center encoding |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
17P | Request for examination filed |
Effective date: 20140423 |
|
AK | Designated contracting states |
Kind code of ref document: A1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
DAX | Request for extension of the european patent (deleted) | ||
RA4 | Supplementary search report drawn up and despatched (corrected) |
Effective date: 20150504 |
|
RIC1 | Information provided on ipc code assigned before grant |
Ipc: G06T 1/00 20060101ALI20150424BHEPIpc: G06F 17/00 20060101ALI20150424BHEPIpc: G01C 21/36 20060101AFI20150424BHEP |
|
17Q | First examination report despatched |
Effective date: 20160224 |
|
RAP1 | Party data changed (applicant data changed or rights of an application transferred) |
Owner name: GOOGLE LLC |
|
GRAP | Despatch of communication of intention to grant a patent |
Free format text: ORIGINAL CODE: EPIDOSNIGR1 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: GRANT OF PATENT IS INTENDED |
|
INTG | Intention to grant announced |
Effective date: 20180301 |
|
GRAS | Grant fee paid |
Free format text: ORIGINAL CODE: EPIDOSNIGR3 |
|
GRAJ | Information related to disapproval of communication of intention to grant by the applicant or resumption of examination proceedings by the epo deleted |
Free format text: ORIGINAL CODE: EPIDOSDIGR1 |
|
GRAL | Information related to payment of fee for publishing/printing deleted |
Free format text: ORIGINAL CODE: EPIDOSDIGR3 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
GRAP | Despatch of communication of intention to grant a patent |
Free format text: ORIGINAL CODE: EPIDOSNIGR1 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: GRANT OF PATENT IS INTENDED |
|
INTC | Intention to grant announced (deleted) | ||
INTG | Intention to grant announced |
Effective date: 20180716 |
|
GRAA | (expected) grant |
Free format text: ORIGINAL CODE: 0009210 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE PATENT HAS BEEN GRANTED |
|
AK | Designated contracting states |
Kind code of ref document: B1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
REG | Reference to a national code |
Ref country code: GBRef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: EP |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: REFRef document number: 1091800Country of ref document: ATKind code of ref document: TEffective date: 20190215 |
|
REG | Reference to a national code |
Ref country code: IERef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R096Ref document number: 602012056320Country of ref document: DE |
|
REG | Reference to a national code |
Ref country code: NLRef legal event code: MPEffective date: 20190123 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: NLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190123 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ESFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190123Ref country code: LTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190123Ref country code: SEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190123Ref country code: PLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190123Ref country code: FIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190123Ref country code: NOFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190423Ref country code: PTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190523 |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: MK05Ref document number: 1091800Country of ref document: ATKind code of ref document: TEffective date: 20190123 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: RSFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190123Ref country code: HRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190123Ref country code: GRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190424Ref country code: BGFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190423Ref country code: LVFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190123Ref country code: ISFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190523 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R097Ref document number: 602012056320Country of ref document: DE |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: EEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190123Ref country code: SKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190123Ref country code: CZFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190123Ref country code: ROFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190123Ref country code: ALFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190123Ref country code: DKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190123Ref country code: ITFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190123 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SMFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190123 |
|
PLBE | No opposition filed within time limit |
Free format text: ORIGINAL CODE: 0009261 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: NO OPPOSITION FILED WITHIN TIME LIMIT |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ATFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190123 |
|
26N | No opposition filed |
Effective date: 20191024 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190123 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: TRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190123 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MCFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190123 |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: PL |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LIFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20190930Ref country code: CHFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20190930Ref country code: IEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20190920Ref country code: LUFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20190920 |
|
REG | Reference to a national code |
Ref country code: BERef legal event code: MMEffective date: 20190930 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: BEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20190930 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: CYFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190123 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190123Ref country code: HUFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMIT; INVALID AB INITIOEffective date: 20120920 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20190123 |
|
P01 | Opt-out of the competence of the unified patent court (upc) registered |
Effective date: 20230510 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: GBPayment date: 20230927Year of fee payment: 12 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: FRPayment date: 20230925Year of fee payment: 12Ref country code: DEPayment date: 20230927Year of fee payment: 12 |