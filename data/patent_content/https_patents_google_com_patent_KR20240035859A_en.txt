KR20240035859A - Perform segmentation inference operations on machine learning models - Google Patents
Perform segmentation inference operations on machine learning models Download PDFInfo
- Publication number
- KR20240035859A KR20240035859A KR1020247005576A KR20247005576A KR20240035859A KR 20240035859 A KR20240035859 A KR 20240035859A KR 1020247005576 A KR1020247005576 A KR 1020247005576A KR 20247005576 A KR20247005576 A KR 20247005576A KR 20240035859 A KR20240035859 A KR 20240035859A
- Authority
- KR
- South Korea
- Prior art keywords
- machine learning
- sub
- model
- inference
- period
- Prior art date
Links
- 238000010801 machine learning Methods 0.000 title claims abstract description 300
- 230000011218 segmentation Effects 0.000 title description 10
- 238000012545 processing Methods 0.000 claims abstract description 167
- 238000000034 method Methods 0.000 claims abstract description 141
- 230000004044 response Effects 0.000 claims abstract description 20
- 230000008569 process Effects 0.000 claims description 93
- 238000013528 artificial neural network Methods 0.000 claims description 74
- 230000015654 memory Effects 0.000 claims description 52
- 238000001514 detection method Methods 0.000 claims description 17
- 230000026676 system process Effects 0.000 claims description 6
- 238000000638 solvent extraction Methods 0.000 claims description 4
- 230000004931 aggregating effect Effects 0.000 claims description 3
- 238000004590 computer program Methods 0.000 abstract description 15
- 230000009471 action Effects 0.000 description 12
- 238000004891 communication Methods 0.000 description 12
- 238000012549 training Methods 0.000 description 8
- 238000005192 partition Methods 0.000 description 7
- 238000004364 calculation method Methods 0.000 description 6
- 230000006870 function Effects 0.000 description 5
- 230000003993 interaction Effects 0.000 description 5
- 230000005540 biological transmission Effects 0.000 description 4
- 239000003795 chemical substances by application Substances 0.000 description 4
- 239000012634 fragment Substances 0.000 description 4
- 230000036541 health Effects 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 4
- 238000007726 management method Methods 0.000 description 3
- 238000003058 natural language processing Methods 0.000 description 3
- 229920002803 thermoplastic polyurethane Polymers 0.000 description 3
- 238000004422 calculation algorithm Methods 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 239000011159 matrix material Substances 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 238000012546 transfer Methods 0.000 description 2
- 241000269627 Amphiuma means Species 0.000 description 1
- 208000003028 Stuttering Diseases 0.000 description 1
- 239000008186 active pharmaceutical agent Substances 0.000 description 1
- 230000002411 adverse Effects 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 238000013473 artificial intelligence Methods 0.000 description 1
- 238000006243 chemical reaction Methods 0.000 description 1
- 238000013527 convolutional neural network Methods 0.000 description 1
- 238000013500 data storage Methods 0.000 description 1
- 238000003745 diagnosis Methods 0.000 description 1
- 238000003709 image segmentation Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000003062 neural network model Methods 0.000 description 1
- 230000002265 prevention Effects 0.000 description 1
- 230000000644 propagated effect Effects 0.000 description 1
- 230000000306 recurrent effect Effects 0.000 description 1
- 230000035945 sensitivity Effects 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 238000012163 sequencing technique Methods 0.000 description 1
- 238000004088 simulation Methods 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 230000002123 temporal effect Effects 0.000 description 1
- 238000013518 transcription Methods 0.000 description 1
- 230000035897 transcription Effects 0.000 description 1
- 239000013598 vector Substances 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
- 230000001755 vocal effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/06—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons
- G06N3/063—Physical realisation, i.e. hardware implementation of neural networks, neurons or parts of neurons using electronic means
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/46—Multiprogramming arrangements
- G06F9/48—Program initiating; Program switching, e.g. by interrupt
- G06F9/4806—Task transfer initiation or dispatching
- G06F9/4843—Task transfer initiation or dispatching by program, e.g. task dispatcher, supervisor, operating system
- G06F9/4881—Scheduling strategies for dispatcher, e.g. round robin, multi-level priority queues
- G06F9/4887—Scheduling strategies for dispatcher, e.g. round robin, multi-level priority queues involving deadlines, e.g. rate based, periodic
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/04—Inference or reasoning models
Abstract
본 문서에는 기계 학습 모델의 추론 연산을 수행하기 위해 컴퓨터 저장 매체에 인코딩된 컴퓨터 프로그램을 포함한 방법, 시스템 및 디바이스를 설명한다. 일 양태에서, 방법은 추론 연산을 포함하는 제1 기계 학습 모델을 나타내는 데이터를 수신하는 단계를 포함한다. 추론 연산을 수행하기 위해 시스템에 대한 추정 기간이 획득된다. 반복 시간 윈도우가 발생할 때마다 우선순위 기계 학습 모델의 우선순위 추론 연산을 수행하기 위해 예약된 우선순위 기간이 획득된다. 우선순위 기간을 예약한 후 남은 반복 시간 윈도우가 발생할 때마다 잔여 기간이 결정된다. 추정 기간이 잔여 기간보다 크다는 결정이 내려진다. 이에 응답하여, 제1 기계 학습 모델은 하위-모델 그룹으로 분할된다. 하드웨어 처리 유닛(들)은 잔여 기간 동안 하위-모델의 추론 연산을 수행한다.This document describes methods, systems, and devices that include a computer program encoded in a computer storage medium to perform inference operations of a machine learning model. In one aspect, the method includes receiving data representing a first machine learning model that includes an inference operation. An estimation period for the system is obtained to perform the inference operation. Each time an iteration time window occurs, a reserved priority period is obtained to perform the priority inference operation of the priority machine learning model. After reserving a priority period, the remaining period is determined whenever a remaining recurring time window occurs. A decision is made that the estimated period is greater than the remaining period. In response, the first machine learning model is split into sub-model groups. The hardware processing unit(s) perform inference operations of the sub-model during the remaining period.
Description
본 명세서는 데이터 처리, 기계 학습, 기계 학습 모델의 분할된 추론 연산을 수행하는 것에 관한 것이다.This specification relates to data processing, machine learning, and performing segmented inference operations on machine learning models.
기계 학습 모델은 패턴을 학습하고 데이터 세트, 이벤트 및 시스템에 대한 예측을 수행하기 위해 경험(예를 들어, 이력 데이터)에 대해 트레이닝된 모델이다. 신경망은 수신된 입력에 대한 출력을 예측하기 위해 하나 이상의 비선형 단위 계층을 사용하는 기계 학습 모델이다. 일부 신경망에는 출력 계층 외에 하나 이상의 은닉 계층이 포함되어 있다. 각 은닉 계층의 출력은 네트워크에 있는 다음 계층, 즉 다음 은닉 계층 또는 출력 계층의 입력으로 사용된다. 네트워크의 각 계층은 개별 네트워크 파라미터 세트의 현재 값에 따라 수신 입력으로부터 출력을 생성한다.Machine learning models are models trained on experience (e.g., historical data) to learn patterns and make predictions about data sets, events, and systems. A neural network is a machine learning model that uses one or more layers of non-linear units to predict the output given the input received. Some neural networks include one or more hidden layers in addition to the output layer. The output of each hidden layer is used as input to the next layer in the network, that is, the next hidden layer or output layer. Each layer of the network generates an output from the received input according to the current values of the individual network parameter sets.
일반적으로, 더 깊은 계층과 더 큰 계층 크기를 가진 신경망은 일반적으로 트레이닝된 후(예를 들어, 이미지 검출 또는 자연어 처리 관련 태스크에 적용되는 경우) 더 얕고 작은 신경망보다 성능이 뛰어나다. 더 크고 더 깊은 신경망은 본질적으로 더 많은 수의 파라미터를 가지며 일부는 거대 신경망으로 분류될 수 있다. 거대 신경망은 많은 네트워크 파라미터(예를 들어, 100만 개, 1천만 개, 5억 개 또는 20억 개 이상의 파라미터)를 가진 신경망이다.In general, neural networks with deeper layers and larger layer sizes typically outperform shallower, smaller neural networks once trained (for example, when applied to tasks related to image detection or natural language processing). Larger and deeper neural networks inherently have a larger number of parameters and some can be classified as giant neural networks. A large neural network is a neural network with many network parameters (e.g., more than 1 million, 10 million, 500 million, or 2 billion parameters).
신경망의 네트워크 파라미터는 신경망에 의해 수행되는 연산(oerations)에 영향을 주고 트레이닝의 일부로서 조정되는 값이다. 예를 들어, 네트워크 파라미터에는 가중치 행렬 값이 포함될 수 있으며 일부 경우에는 신경망의 네트워크 계층의 바이어스 벡터도 포함될 수 있다.Network parameters of a neural network are values that affect the operations performed by the neural network and are adjusted as part of training. For example, network parameters may include weight matrix values and, in some cases, bias vectors of the network layers of a neural network.
신경망의 하이퍼파라미터는 트레이닝 프로세스에 의해 수정되지 않는 값이다. 하이퍼파라미터에는 트레이닝 프로세스에 의해 네트워크 파라미터의 값이 업데이트되는 방식에 영향을 미치는 값(예를 들어, 역전파 중에 계산된 기울기가 네트워크 파라미터 값을 업데이트하는 데 사용되는 방법을 정의하는 학습률 또는 기타 업데이트 규칙), 목적 함수값(예를 들어, 엔트로피 비용, 목적 함수의 다양한 항에 할당된 가중치 등)이 포함될 수 있다.Hyperparameters of a neural network are values that are not modified by the training process. Hyperparameters include values that affect how the values of the network parameters are updated by the training process (for example, a learning rate or other update rule that defines how the gradients computed during backpropagation are used to update the network parameter values). ), objective function values (e.g., entropy cost, weights assigned to various terms of the objective function, etc.) may be included.
양태에 따르면, 호스트 및 다수의 기계 학습 모델의 추론 연산을 수행하도록 구성된 하나 이상의 하드웨어 처리 유닛을 포함하는 시스템에 의해 수행되는 방법이 제공된다. 이 방법은 호스트에서, 제1 추론 출력을 생성하기 위해 입력을 처리하기 위한 추론 연산을 포함하는 제1 기계 학습 모델을 나타내는 데이터를 수신하는 단계와; 시스템이 입력을 처리하여 제1 추론 출력을 생성하기 위한 제1 기계 학습 모델의 추론 연산을 수행하는 제1 추정 기간을 획득하는 단계와; 하나 이상의 하드웨어 처리 유닛이 다수의 기계 학습 모델의 추론 연산 중 적어도 일부를 수행하는 반복 시간 윈도우의 각각의 발생 동안 우선순위 기계 학습 모델의 우선순위 추론 연산을 수행하기 위해 예약된 우선순위 기간을 식별하는 단계와; 우선순위 추론 연산을 수행하기 위해 우선순위 기간을 예약한 후 남은 반복 시간 윈도우의 각 발생에 대한 제1 잔여 기간을 결정하는 단계와; 제1 추정 기간이 제1 잔여 기간보다 큰지 여부를 판단하는 단계와; 제1 추정 기간이 제1 잔여 기간보다 크다는 결정에 응답하여, 제1 기계 학습 모델을 제1 잔여 기간 이하인 개별(respective) 추정 기간을 갖는 제1 하위-모델 그룹으로 분할하는 단계 - 제1 하위-모델 그룹의 각 하위-모델은 제1 기계 학습 모델의 추론 연산의 개별 부분을 포함함 - 와; 그리고 하나 이상의 하드웨어 처리 유닛에 의해, 반복 시간 윈도우가 발생한 제1 잔여 기간 동안 제1 하위-모델 그룹의 하위-모델에 대한 추론 연산을 수행하는 단계를 포함한다.According to aspects, a method is provided that is performed by a system including a host and one or more hardware processing units configured to perform inference operations of a plurality of machine learning models. The method includes receiving, at a host, data representing a first machine learning model comprising inference operations to process input to produce a first inference output; obtaining a first inference period over which the system processes the input and performs an inference operation of a first machine learning model to produce a first inference output; Identifying a priority period reserved for performing priority inference operations of a priority machine learning model during each occurrence of an iterative time window in which one or more hardware processing units perform at least some of the inference operations of the plurality of machine learning models. Steps and; determining a first remaining period for each occurrence of the repetition time window remaining after reserving a priority period for performing a priority inference operation; determining whether the first estimate period is greater than the first remaining period; In response to determining that the first estimation period is greater than the first residual period, splitting the first machine learning model into a first group of sub-models having respective (respective) estimation periods that are less than or equal to the first residual period - a first sub- Each sub-model of the model group includes a separate portion of the inference operation of the first machine learning model - and; and performing, by the one or more hardware processing units, an inference operation on the sub-models of the first sub-model group during the first remaining period during which the repetition time window occurs.
본 명세서에 설명된 주제는 다음 장점들 중 하나 이상을 실현하기 위해 특정 실시예에서 구현될 수 있다. 본 명세서에 설명된 기술은 데이터 통신의 지터를 줄일 수 있다. 명세서 전체에서 "지터"라는 용어는 네트워크 연결을 통해 데이터 패킷을 전송하는 동안의 시간 지연을 광범위하게 나타낼 수 있다. 시간 지연은 고르지 않을 수 있는데, 예를 들어 첫 번째 패킷은 30ms 지연 내에 디바이스나 호스트에 도착할 수 있고 두 번째 패킷은 40ms 지연 내에 도착할 수 있다. 데이터 통신의 지터는 다양한 크기의 데이터 패킷을 전송함으로써 발생할 수 있다. 지터는 시스템이 주기적으로 수신한 입력 데이터를 시스템이 처리할 때 여러 계산 간의 대기 시간으로 인해 발생할 수도 있다.The subject matter described herein may be implemented in certain embodiments to realize one or more of the following advantages. The techniques described herein can reduce jitter in data communications. Throughout the specification, the term "jitter" can broadly refer to the time delay during the transmission of data packets over a network connection. The time delay can be uneven, for example, the first packet may arrive at the device or host with a 30ms delay, and the second packet may arrive with a 40ms delay. Jitter in data communications can occur by transmitting data packets of various sizes. Jitter can also occur due to latency between different calculations as the system processes input data it receives periodically.
보다 구체적으로, 설명된 기술을 수행하는 시스템은 다수의 기계 학습 모델의 우선 순위 레벨을 결정하고 우선 순위 레벨에 따라 다수의 기계 학습 모델의 순위를 지정할 수 있다. 예를 들어 우선 순위가 높은 기계 학습 모델은 에지 디바이스의 카메라 애플리케이션에 대한 얼굴 감지와 같은 태스크에 해당할 수 있다. 시스템은 높은 우선순위 레벨로 기계 학습 모델에 대한 추론 요청의 우선순위를 지정하고, 반복(recurring) 시간 윈도우가 발생할 때마다 이러한 우선순위가 지정된 기계 학습 모델에 대해 상기 수신된 입력의 각 프레임을 처리하도록 보장할 수 있다. 반복 시간 창에는 시스템(예를 들어, 회로 또는 다중 하드웨어 처리 유닛)이 각 주기 내에서 연산들을 수행하는 기간이 포함된다. 이를 고려하면, 시스템은 이러한 우선 순위가 지정된 기계 학습 모델들에 대한 추론 출력을 적시에 생성하여 데이터 통신의 지터를 줄일 수 있다.More specifically, a system performing the described techniques can determine priority levels of multiple machine learning models and rank the multiple machine learning models according to the priority levels. For example, high-priority machine learning models may correspond to tasks such as face detection for camera applications on edge devices. The system prioritizes inference requests to machine learning models at a high priority level and processes each frame of the received input for these prioritized machine learning models whenever a recurring time window occurs. It can be guaranteed that this will be done. The repetition time window includes the period during which the system (e.g., a circuit or multiple hardware processing units) performs operations within each cycle. Taking this into account, the system can generate inference output for these prioritized machine learning models in a timely manner, thereby reducing jitter in data communication.
또한, 본 명세서에 설명된 기술은 하나 이상의 기계 학습 모델의 추론 연산들을 수행하는 효율성을 향상시킬 수 있다. 설명된 기술을 수행하는 시스템은 우선순위가 높은 태스크를 위해 각 주기의 시간을 예약함으로써 각 주기 내에서 우선순위가 높은 태스크와 관련된 추론 연산들을 수행하는 것을 보장할 수 있다. 시스템은 우선순위가 높은 태스크를 위해 상기 예약된 시간을 감산함으로써 각 반복 시간 윈도우에 대해 각각의 잔여(남은) 기간을 결정할 수 있다. 시스템은 잔여 기간 전체를 차지하거나 잔여 기간을 초과하는 것으로 결정될 수 있는 우선순위가 낮은 모델을 다수의 하위-모델로 분할하고, 해당 하위-모델들을 각각의 하드웨어 처리 유닛 그룹에 배포하거나 여러 주기에 걸쳐 처리되도록 배포하거나, 둘 모두를 수행할 수 있다.Additionally, the techniques described herein can improve the efficiency of performing inference operations of one or more machine learning models. A system performing the described technique can ensure that inference operations related to the high-priority task are performed within each cycle by reserving time in each cycle for the high-priority task. The system can determine the respective remaining (remaining) duration for each repeat time window by subtracting the reserved time for higher priority tasks. The system splits low-priority models that can be determined to occupy or exceed the remaining period into multiple sub-models and distributes those sub-models to respective groups of hardware processing units or over multiple cycles. You can either distribute it for processing, or both.
특히, 시스템은 기계 학습 모델에 의해 지정된 추론 연산들을 낮은 우선순위 레벨로 수행하기 위한 추정 기간(duration)(예를 들어, 추정 기간(time period))을 획득할 수 있고, 각 주기에 대해, 다수의 하위-모델 각각이 주기에서 반복 시간 윈도우의 잔여 기간보다 작거나 같은 개별 추정 기간을 갖도록 기계 학습 모델을 다수의 하위-모델로 분할할지 여부를 결정할 수 있다. 시스템은 다수의 하위-모델을 하나 이상의 시간 윈도우로 배열 및 배포하고 이들을 개별 하드웨어 처리 유닛을 사용하여 처리할 수 있다. 각 반복 시간 윈도우의 잔여 기간은 하위-모델의 추론 연산을 수행하는 데 실질적으로 활용되므로 각 반복 시간 윈도우에 대한 유휴 시간을 줄이고 계산 효율성을 향상시킬 수 있다.In particular, the system may obtain an estimated duration (e.g., a time period) for performing inference operations specified by the machine learning model at a low priority level, and for each period, a number of You can decide whether to split the machine learning model into multiple sub-models such that each of the sub-models has a separate estimation period that is less than or equal to the remaining period of the repetition time window in the cycle. The system can arrange and distribute multiple sub-models into one or more time windows and process them using separate hardware processing units. The remaining period of each iteration time window is substantially utilized to perform the inference operation of the sub-model, thereby reducing idle time for each iteration time window and improving computational efficiency.
명세서 전체에서 "추론 연산"이라는 용어는 입력을 처리하도록 트레이닝된 파라미터를 사용하여 해당 기계 학습 모델에 지정된 동작을 광범위하게 나타낼 수 있다. 추론 연산에는 선형, 비선형 작업 또는 둘 모두가 포함될 수 있다. 트레이닝된 신경망인 기계 학습 모델의 경우, 추론 연산에는 특정 입력을 처리하기 위해 트레이닝된 신경망의 각 네트워크 계층의 각 노드에 지정된 노드 동작이 포함될 수 있다.Throughout the specification, the term "inferential operation" can broadly refer to an operation specified in a given machine learning model using parameters trained to process inputs. Inference operations can include linear, non-linear operations, or both. For machine learning models that are trained neural networks, inference operations may involve node actions assigned to each node in each network layer of the trained neural network to process a specific input.
또한, 본 명세서에 설명된 기술은 최적화된 서비스 품질(QoS)을 제공하고 사용자 경험을 향상시킬 수 있다. 전술한 바와 같이, 이 기술을 수행하는 시스템은 각 기계 학습 모델의 우선 순위 레벨을 결정하고 우선 순위가 지정된 기계 학습 모델의 추론 연산이 먼저 또는 각 반복 시간 윈도우 내에 수행되도록 보장할 수 있다. 시스템은 또한 우선 순위 레벨이 낮은 대규모 기계 학습 모델을 다수의 하위-모델로 분할하고, 반복 시간 윈도우가 여러 번 발생(예를 들어, 주기)하는 동안 그 다수의 하위-모델의 추론 연산을 수행할 수 있다. 시스템의 런타임 제어기는 우선 순위가 높은 기계 학습 모델과 우선 순위가 낮은 다양한 기계 학습 모델의 다수의 하위-모델을 반복 시간 윈도우의 다양한 주기에 배포하는 방법을 결정할 수 있다. 런타임 제어기는 또한 반복 시간 윈도우의 하나 이상의 주기에 분산된 하위-모델을 통해 다중-패스 추론을 관리하도록 구성된다. 따라서, 데이터 트래픽이 최적화되고 입력 또는 중간 출력을 기다리는 유휴 시간이 줄어들며 다수의 기계 학습 모델의 추론 연산을 수행하는 데 필요한 전체 계산 시간이 단축된다. 따라서 시스템에 의해 지원되는 하나 이상의 애플리케이션을 사용할 때 시스템의 수신자(예를 들어, 사람 또는 디바이스)는 기존 기술을 사용하는 다른 시스템에 비해 시간 지연이 적고 더 빠르게 출력을 얻을 수 있다.Additionally, the techniques described herein can provide optimized quality of service (QoS) and improve user experience. As described above, a system performing this technique can determine the priority level of each machine learning model and ensure that inference operations for the prioritized machine learning model are performed first or within each iteration time window. The system can also split a large machine learning model with a low priority level into multiple sub-models and perform inference operations on those multiple sub-models over multiple occurrences (e.g., cycles) of iteration time windows. You can. The system's runtime controller may determine how to distribute multiple sub-models of a high-priority machine learning model and various low-priority machine learning models at various periods of the iteration time window. The runtime controller is also configured to manage multi-pass inference through sub-models distributed over one or more periods of the iteration time window. As a result, data traffic is optimized, idle time waiting for input or intermediate output is reduced, and the overall computation time required to perform inference operations of multiple machine learning models is reduced. Therefore, when using one or more applications supported by the system, recipients of the system (e.g., people or devices) can obtain output faster and with less time delay compared to other systems using existing technologies.
게다가, 설명된 기술을 수행하는 시스템은 다양한 유형의 입력과 서로 다른 속도로 수신되는 다양한 입력 시퀀스에 강건하다. 시스템은 입력 데이터의 입력 속도(예를 들어, 초당 입력 프레임 수)에 기초하여 각 사이클에 대한 반복 시간 윈도우의 크기를 결정할 수 있다. 시스템은 또한 수신된 입력 프레임을 처리하기 위해 하나 이상의 우선순위 기계 학습 모델의 추론 연산들을 수행하기 위해 우선순위 기간을 할당한 후 각 시간 윈도우에 대한 잔여 기간을 결정할 수 있다.Moreover, a system performing the described techniques is robust to different types of inputs and different sequences of inputs received at different rates. The system may determine the size of the repetition time window for each cycle based on the input rate of the input data (e.g., input frames per second). The system may also determine a remaining period for each time window after assigning a priority period to perform inference operations of one or more priority machine learning models to process the received input frame.
본 명세서의 주제에 대한 하나 이상의 실시예의 세부사항은 첨부 도면 및 아래 설명에서 설명된다. 주제의 다른 특징, 측면 및 이점은 설명, 도면 및 청구범위로부터 명백해질 것이다.The details of one or more embodiments of the subject matter herein are set forth in the accompanying drawings and the description below. Other features, aspects and advantages of the subject matter will become apparent from the description, drawings and claims.
도 1은 기계 학습 모델의 추론 연산을 수행하기 위한 예시적인 추론 시스템을 도시한다.
도 2는 다양한 시나리오에서 서로 다른 시간 내에 다양한 기계 학습 모델의 추론 연산을 수행하기 위한 예시적인 프로세스를 도시한다.
도 3a는 기계 학습 모델로부터 분할된 다수의 하위-모델의 추론 연산을 수행하기 위한 예시적인 프로세스를 도시한다.
도 3b는 기계 학습 모델로부터 분할된 다수의 하위-모델을 통해 다중-패스 추론을 수행하기 위한 예시적인 프로세스를 도시한다.
도 4는 기계 학습 모델로부터 다수의 하위-모델을 생성하기 위한 예시적인 프로세스를 도시한다.
도 5는 다수의 하위-모델로 분할될 기계 학습 모델을 결정하기 위한 예시적인 프로세스를 도시한다.1 illustrates an example inference system for performing inference operations of a machine learning model.
2 illustrates an example process for performing inference operations of various machine learning models in different scenarios and in different times.
3A shows an example process for performing inference operations on multiple sub-models partitioned from a machine learning model.
3B shows an example process for performing multi-pass inference through multiple sub-models partitioned from a machine learning model.
4 shows an example process for generating multiple sub-models from a machine learning model.
Figure 5 shows an example process for determining a machine learning model to be split into multiple sub-models.
더 나은 성능을 위해, 기계 학습 모델은 더 정교한 구조로 더 큰 크기를 갖는 경향이 있는데, 예를 들어 신경망은 더 깊은 계층와 더 큰 계층 크기를 가질 수 있으며, 특히 객체 감지/인식 또는 자연어 처리와 같은 이미지 처리 작업(task)에 사용되는 신경망의 경우 더욱 그렇다. 더 큰 신경망과 같은 더 큰 기계 학습 모델은 여러 분야에서 놀라운 품질 개선을 가져왔지만, 기계 학습 모델을 확장하면 특정 빈도로 수신된 입력 시퀀스를 처리하기 위한 시간 윈도우(time window) 제한, 기계 학습 모델의 추론 연산을 트레이닝, 저장 및 수행하기 위한 메모리 제한, 호스트와 가속기 간에 데이터 및 명령을 전송하기 위한 메모리 대역폭 제한 등과 같은 실질적인 문제가 발생할 수 있다.For better performance, machine learning models tend to have larger sizes with more sophisticated structures, for example neural networks can have deeper layers and larger layer sizes, especially for things like object detection/recognition or natural language processing. This is especially true for neural networks used in image processing tasks. Larger machine learning models, such as larger neural networks, have brought about remarkable quality improvements in many areas, but scaling machine learning models limits the time window for processing input sequences received at a certain frequency, Practical issues can arise, such as memory limitations for training, storing, and performing inference operations, and memory bandwidth limitations for transferring data and instructions between the host and accelerator.
예를 들어, 신경망의 추론 연산을 트레이닝하거나 수행하기 위한 병목 현상은 각 개별 컴퓨팅 디바이스, 즉 중앙 처리 장치("CPU"), 그래픽 처리 장치("GPU"), 텐서 처리 장치("TPU")에 대한 메모리 제한일 수 있다. 다른 예로서, 병목 현상은 컴퓨팅 디바이스 간의 제한된 통신 대역폭일 수 있다. 예를 들어 GPU 또는 TPU 간의 데이터 전송 속도(rate)는 각 개별 컴퓨팅 디바이스의 컴퓨팅 속도에 비해 충분하지 않을 수 있다. 따라서, 디바이스 간 데이터 전송을 위한 대기 시간은 각 컴퓨팅 디바이스의 런타임(run time, 실행 시간)과 비슷하거나 훨씬 더 길어서 트레이닝 성능이 저하될 수 있다. 또 다른 예로, 병목 현상은 컴퓨팅 디바이스의 버블 오버헤드일 수 있다. 버블 오버헤드는 시퀀스 내 동작의 제2 부분이 할당된 후속 컴퓨팅 디바이스가 시퀀스 내 동작의 제1 부분이 할당된 이전 컴퓨팅 디바이스의 출력을 기다리는 데 소비하는 시간을 지칭한다. 즉, 후속 컴퓨팅 디바이스가 동작의 제2 부분을 수행하기 위한 입력은 해당 동작의 제1 부분을 수행하는 이전 컴퓨팅 디바이스의 출력이다. 이를 고려하면, 후속 컴퓨팅 디바이스는 유휴 상태를 유지하고 이전 컴퓨팅 디바이스가 필요한 계산을 완료할 때까지 기다려야 한다. 따라서, 버블 오버헤드 시간이 상당한 경우, 특히 한 시간 단계에서 작동하는 디바이스가 하나만 있는 경우 각 컴퓨팅 디바이스의 사용량은 시간 단계에서 낮을 수 있다.For example, the bottleneck for training or performing inference operations in a neural network lies within each individual computing device: the central processing unit (“CPU”), graphics processing unit (“GPU”), and tensor processing unit (“TPU”). It may be a memory limitation. As another example, the bottleneck may be limited communication bandwidth between computing devices. For example, the data transfer rate between GPUs or TPUs may not be sufficient compared to the computing speed of each individual computing device. Therefore, the waiting time for data transfer between devices is similar to or much longer than the run time of each computing device, which may degrade training performance. As another example, the bottleneck may be the bubble overhead of the computing device. Bubble overhead refers to the time a subsequent computing device to which a second part of an operation in a sequence is assigned waits for the output of a previous computing device to which a first part of an operation in a sequence is assigned. That is, the input for a subsequent computing device to perform the second part of the operation is the output of the previous computing device performing the first part of the operation. Given this, subsequent computing devices must remain idle and wait for the previous computing device to complete the required computations. Therefore, if the bubble overhead time is significant, the usage of each computing device may be low in a time step, especially if there is only one device operating in a time step.
시간 제한을 다시 참조하면, 입력 스트림(예를 들어, 특정 시간 간격 또는 빈도로 수신된 다수의 입력 프레임)을 처리하기 위해 다수의 기계 학습 모델에 지정된 추론 연산들을 수행하는 병목 현상은 다수의 기계 학습 모델을 통해 적시에, 예를 들어 바람직하게는 입력 프레임을 수신한 후 후속 입력 프레임을 수신하기 전에 각 입력 프레임을 처리하는 것이다. 일부 경우에는 특히 입력을 처리하는 데 사용되는 모델이 많고 시간 윈도우가 짧은 경우(예를 들어, 밀리초) 단일 시간 내에서 모든 기계 학습 모델의 모든 추론 연산을 수행하는 것이 어렵거나 심지어 불가능할 수도 있다. 예를 들어, 하나 이상의 기계 학습 모델은 클 수 있으며 이러한 모델을 모두 수행하기 위한 추정 기간은 프로세스에 할당된 반복(되는) 시간 윈도우를 초과할 수 있다. 일부 구현에서, 시스템은 각 입력 프레임(예를 들어 초당 프레임(FPS))을 수신하는 속도 또는 빈도에 기초하여 반복 시간 윈도우의 크기를 결정할 수 있다. 반복 시간 윈도우의 각 발생은 기계 학습 모델(들)을 사용하여 프레임(또는 입력의 다른 개별 인스턴스)이 처리되는 처리 주기(cycle)로 간주될 수 있다. 예를 들어, 각 반복 시간 윈도우는 초당 20개의 처리 주기가 있고 초당 20개의 입력 프레임이 처리되도록 50ms일 수 있다. 물론, 반복 시간 윈도우의 다른 크기(예를 들어, 30ms, 100ms, 1초 등)도 가능하다. 단순화를 위해, 다음 명세서에서 "반복 시간 윈도우"라는 용어는 "시간 윈도우"라고도 지칭된다.Referring back to time constraints, the bottleneck of performing inference operations specified on multiple machine learning models to process an input stream (e.g., a large number of input frames received at a certain time interval or frequency) is the bottleneck of multiple machine learning models. The model processes each input frame in a timely manner, preferably after receiving the input frame and before receiving subsequent input frames. In some cases, it may be difficult or even impossible to perform all inference operations for all machine learning models within a single time frame, especially if there are many models and the time window is short (e.g., milliseconds) used to process the input. For example, one or more machine learning models may be large and the estimated time period to run all of these models may exceed the iteration time window assigned to the process. In some implementations, the system may determine the size of the repetition time window based on the rate or frequency at which it receives each input frame (e.g., frames per second (FPS)). Each occurrence of the repetition time window can be considered a processing cycle in which a frame (or other individual instance of input) is processed using the machine learning model(s). For example, each iteration time window could be 50ms, so that there are 20 processing cycles per second and 20 input frames per second are processed. Of course, other sizes of the repetition time window are possible (e.g. 30 ms, 100 ms, 1 second, etc.). For simplicity, the term “repetition time window” is also referred to as “time window” in the following specification.
시간 윈도우 제한이 해결되지 않았다고 가정한다. 이 경우, 다수의 입력 프레임을 처리하기 위한 추론 연산들을 수행하는 시스템에는 데이터 전송 중 지터, 하드웨어 가속기의 상당한 유휴 시간, 입력 수신 속도보다 추론 출력 생성 속도가 느리고 입력 데이터의 다양한 유형 및 스트림에 대한 견고성 부족과 같은 문제가 있을 수 있다. 이러한 문제는 계산 효율성을 저하시키고 서비스 품질과 사용자 경험을 만족스럽지 못하게 만든다. 또한, 더 크고 우선순위가 낮은 기계 학습 모델은 우선 순위가 더 높은 다른 기계 학습 작업이 더 높은 우선 순위의 기계 학습 모델을 사용하여 수행되지 않도록 시간 윈도우를 지배할 수 있으며, 이로 인해 중요한 연산들이 적시에 수행되지 않아 전체 성능이 저하되고 오류가 발생할 수 있다. 예를 들어, 특정 기계 학습 모델을 사용하여 각 처리 주기 내에서 입력 프레임을 처리하는 것은 성능/오류 방지에 매우 중요할 수 있다. 더 크고 우선 순위가 낮은 기계 학습 모델을 완료하는 데 다수의 처리 주기가 걸리는 경우, 특정 기계 학습 모델은 각 처리 주기 동안 각 입력을 처리하는 데 사용되지 않아 성능이 저하되거나 오류가 발생할 수 있다.Assume that the time window limitation has not been resolved. In this case, systems that perform inference operations to process multiple input frames have the potential to suffer from jitter during data transmission, significant idle time in hardware accelerators, slower generation of inference output than reception of input, and robustness to different types and streams of input data. There may be problems such as shortages. These problems reduce computational efficiency and make service quality and user experience unsatisfactory. Additionally, larger, lower-priority machine learning models can dominate time windows such that other, higher-priority machine learning tasks are not performed using the higher-priority machine learning models, resulting in critical computations being performed in a timely manner. If it is not performed, overall performance may deteriorate and errors may occur. For example, processing input frames within each processing cycle using a specific machine learning model may be critical to performance/error prevention. If a larger, lower-priority machine learning model takes multiple processing cycles to complete, certain machine learning models may not be used to process each input during each processing cycle, resulting in poor performance or errors.
대규모 기계 학습 모델을 상이한 부분들로 분할하고 상이한 부분들을 여러 프로세서에 배포하는 것과 관련된 일부 기술은 메모리 제한, 대역폭 제한 또는 둘 모두로 인해 발생하는 문제를 해결하는 것을 목표로 한다. 이러한 기술은 파이프라인 방법을 추가로 적용하여 프로세서의 버블 시간을 줄일 수 있다. 그러나, 이러한 기술은 시간 제약(예를 들어, 시간 윈도우 제약의 크기) 하에서 입력 스트림의 각 프레임을 처리할 때 나타나는 문제를 해결하지 않는다.Some techniques, which involve splitting large machine learning models into different parts and distributing the different parts across multiple processors, aim to solve problems caused by memory limitations, bandwidth limitations, or both. These techniques can further apply pipeline methods to reduce the processor's bubble time. However, these techniques do not solve the problems that arise when processing each frame of the input stream under time constraints (e.g., size of time window constraint).
본 명세서에 설명된 기술은 위에서 언급한 문제를 해결하는 것을 목표로 한다. 특히, 추론 입력 프레임을 처리하기 위한 시간 윈도우 제약이 주어지면, 본 명세서에 설명된 기술은 다수의 기계 학습 모델의 우선 순위 레벨을 결정하고 그 우선 순위 레벨에 따라 입력 프레임을 처리하기 위한 기계 학습 모델의 추론 연산들을 수행할 수 있다. 다른 예에서, 각각의 반복 시간 윈도우에 대한 입력을 처리하는 데 사용되는 하나 이상의 식별된 우선 순위가 높은 기계 학습 모델이 있을 수 있다. 본 문서에 설명된 기술은 하나 이상의 우선순위 기계 학습 모델이 각 시간 윈도우 동안 처리되도록 보장하는 방식으로 추론 연산들을 수행할 수 있다. 더욱이, 설명된 기술은 각 하위-모델의 연산들을 수행하기 위한 추정 기간이 반복 시간 윈도우의 잔여 기간을 충족하도록 낮은 우선순위(예를 들어, 모델이 각 시간 윈도우마다 사용될 필요가 없음을 나타내는 우선순위)를 갖는 기계 학습 모델(예를 들어, 대규모 기계 학습 모델)을 다수의 하위-모델로 분할할 수 있다. 시스템은 다양한 처리 유닛(예를 들어, 병렬) 및/또는 여러 시간 윈도우에 걸쳐 다수의 기계 학습 모델 및 하위-모델에 지정된 추론 연산 수행을 배열하고 예약하도록 구성된 런타임 제어기를 더 포함할 수 있다.The technology described herein aims to solve the problems mentioned above. In particular, given time window constraints for processing inference input frames, the techniques described herein determine priority levels of multiple machine learning models and model machine learning models for processing input frames according to those priority levels. of inference operations can be performed. In another example, there may be one or more identified high-priority machine learning models that are used to process the input for each iteration time window. The techniques described herein can perform inference operations in a manner that ensures that one or more priority machine learning models are processed during each time window. Moreover, the described technique allows the estimation period for performing the operations of each sub-model to satisfy the remaining period of the repetition time window (e.g., a low priority indicating that the model does not need to be used for each time window). A machine learning model (e.g., a large-scale machine learning model) with ) can be split into multiple sub-models. The system may further include a runtime controller configured to arrange and schedule performance of specified inference operations on multiple machine learning models and sub-models across various processing units (e.g., parallel) and/or multiple time windows.
도 1은 기계 학습 모델의 추론 연산들을 수행하기 위한 예시적인 추론 시스템(100)을 도시한다. 추론 시스템(100)은 아래에 설명된 시스템, 구성요소 및 기술을 구현할 수 있는 하나 이상의 위치에 있는 하나 이상의 컴퓨터에 구현되는 시스템의 예이다. 추론 시스템(100)의 구성요소 중 일부는 하나 이상의 컴퓨터에서 실행되도록 구성된 컴퓨터 프로그램으로 구현될 수 있다.1 illustrates an example inference system 100 for performing inference operations of a machine learning model. Inference system 100 is an example of a system implemented on one or more computers at one or more locations capable of implementing the systems, components and techniques described below. Some of the components of reasoning system 100 may be implemented as computer programs configured to run on one or more computers.
예시적인 추론 시스템(100)은 다양한 유형의 컴퓨팅 디바이스에서 구현될 수 있다. 예를 들어, 추론 시스템(100)은 모바일 디바이스(예를 들어, 스마트폰 또는 태블릿 컴퓨터), 비디오 스트리밍 디바이스, 게임 콘솔, 또는 인공 지능 어시스턴트(예를 들어, 스마트 스피커)와 같은 클라이언트 디바이스의 일부일 수 있다. 일부 구현에서, 추론 시스템(100)은 카메라를 포함하는 클라이언트 디바이스에 구현되고 추론 시스템(100)은 기계 학습 모델을 사용하여 카메라에 의해 캡처된 이미지를 처리하도록 구성된다. 이러한 예에서, 추론 시스템(100)은 또한 사운드(예를 들어, 음성), 비디오 또는 텍스트 입력과 같은 클라이언트 디바이스의 다른 유형의 입력을 처리하도록 구성될 수 있다.Example inference system 100 may be implemented in various types of computing devices. For example, inference system 100 may be part of a client device, such as a mobile device (e.g., a smartphone or tablet computer), a video streaming device, a gaming console, or an artificial intelligence assistant (e.g., a smart speaker). there is. In some implementations, inference system 100 is implemented in a client device that includes a camera and inference system 100 is configured to process images captured by the camera using machine learning models. In these examples, inference system 100 may also be configured to process other types of input from the client device, such as sound (e.g., speech), video, or text input.
추론 시스템(100)은 호스트(102) 및 다수의 처리 유닛(110)을 포함할 수 있다. 본 명세서 전반에 걸쳐 "호스트"라는 용어는 네트워크에서 호스트와 결합된 사용자 또는 기타 디바이스에게 정보 자원, 서비스 또는 애플리케이션 중 적어도 하나를 제공하도록 구성된 컴퓨터 또는 서버를 광범위하게 나타낼 수 있다. 명세서 전반에 걸쳐 "처리 유닛"이라는 용어는 특정 동작을 수행하는 데 적합한 하드웨어 구성 요소를 광범위하게 나타낼 수 있으며, 예를 들어 처리 유닛은 하드웨어 기계 학습 가속기 또는 다른 유형의 프로세서, 컴퓨팅 타일 또는 코어를 포함할 수 있다.Inference system 100 may include a host 102 and a number of processing units 110. Throughout this specification, the term “host” can broadly refer to a computer or server configured to provide at least one of an information resource, service, or application to a user or other device associated with the host in a network. Throughout the specification, the term "processing unit" can refer broadly to any hardware component suitable to perform a particular operation, for example, a processing unit may include a hardware machine learning accelerator or other type of processor, compute tile, or core. can do.
호스트(102)는 다수의 하드웨어 처리 유닛(110)과 통신 가능하게, 즉 유선 또는 무선 통신으로 연결된다. 호스트(102) 및 다중 처리 유닛(110)은 하나 이상의 물리적 위치에 위치할 수 있다. 일부 구현에서, 호스트(102) 및 다중 처리 유닛(110)은 회로 상에 통합되거나 단일 프로세서로 패키징될 수 있다. 예를 들어, 단일 집적 회로는 각각의 처리 유닛(110) 및 선택적으로 호스트(102)를 포함할 수 있다. 다른 예에서, 처리 유닛(110)은 다수의 집적 회로에 걸쳐 있을 수 있다.The host 102 is communicatively connected to a plurality of hardware processing units 110, that is, by wired or wireless communication. Host 102 and multiple processing unit 110 may be located in one or more physical locations. In some implementations, host 102 and multiple processing unit 110 may be integrated on a circuit or packaged into a single processor. For example, a single integrated circuit may include each processing unit 110 and optionally a host 102. In another example, processing unit 110 may span multiple integrated circuits.
추론 시스템(100)은 호스트(102)에서, 다중 기계 학습 모델(들)(135)을 나타내는 데이터 및 입력 데이터(137a)를 수신할 수 있다. 입력 데이터(137a)는 다중 기계 학습 모델(들)(135)에 의해 처리될 입력 데이터의 다중 개별 단위(예를 들어, 프레임)를 포함할 수 있다. 입력의 개별 단위는 다양한 형태일 수 있지만, 간결함과 후속 설명의 용이성을 위해 입력은 프레임으로 지칭된다. 추론 시스템(100)은 호스트(102)로부터 수신된 입력 데이터(137b)의 각 프레임을 처리하기 위한 추론 연산들을 수행하기 위해 다중 기계 학습 모델(들)(135)을 다중 처리 디바이스(110) 중 하나 이상에 컴파일하고 배포할 수 있다. 입력 데이터(137b)는 입력 데이터(137a)에 대응한다. 즉, 호스트(102)는 입력 데이터의 각 프레임을 처리 유닛(110)에 제공할 수 있다. 추론 시스템(100)은 기계 학습 모델(들)(135)을 통해 입력 데이터(137a)를 처리한 후 추론 출력(167a)을 생성 및 출력할 수 있다. 추론 출력(167a)은 입력 데이터의 각 프레임에 대한 하나 이상의 추론, 예를 들어 입력 데이터의 프레임에 기초한 각 기계 학습 모델에 의한 개별 추론 출력을 포함할 수 있다.Inference system 100 may receive, from host 102, data representing multiple machine learning model(s) 135 and input data 137a. Input data 137a may include multiple individual units (e.g., frames) of input data to be processed by multiple machine learning model(s) 135. Individual units of input can take many forms, but for brevity and ease of subsequent description, inputs are referred to as frames. The inference system 100 uses multiple machine learning model(s) 135 on one of the multiprocessing devices 110 to perform inference operations for processing each frame of input data 137b received from the host 102. You can compile and distribute above. Input data 137b corresponds to input data 137a. That is, host 102 may provide each frame of input data to processing unit 110. The inference system 100 may process the input data 137a through the machine learning model(s) 135 and then generate and output the inference output 167a. Inference output 167a may include one or more inferences for each frame of input data, for example, individual inference outputs by each machine learning model based on the frame of input data.
일부 경우, 기계 학습 모델은 다중 프레임에 기초하여 추론을 출력하도록 구성될 수 있다. 이러한 경우, 추론 출력(167a)에는 다중 프레임에 기초하려 생성된 추론이 포함될 수 있다. In some cases, a machine learning model may be configured to output inferences based on multiple frames. In this case, the inference output 167a may include inferences generated based on multiple frames.
호스트(102)는 다중 기계 학습 모델(들)(135)로부터 하나 이상의 기계 학습 모델을 비-우선순위 기계 학습 모델로서 선택하도록 구성된 선택 엔진(140)을 포함할 수 있다. 선택 엔진(140)은 선택된 기계 학습 모델(들)(145)을 호스트(102)의 성능 추정 엔진(150)에 제공할 수 있다. 일부 구현에서, 선택 엔진(140)은 모든 기계 학습 모델(들)(135)에 대해, 호스트(102)에서 수신된 입력 데이터(137a)의 프레임을 처리하기 위한 반복 시간 윈도우를 추정하도록 구성될 수 있다. 또한, 선택 엔진(140)은 시간 윈도우 내에서 입력 데이터의 프레임을 처리하기 위해 기계 학습 모델 각각에 대한 우선순위 레벨을 결정할 수도 있다. 예를 들어, 선택 엔진(140)은 각 기계 학습 모델에 대한 우선순위 레벨에 기초하여 하나 이상의 선택된 기계 학습 모델(들)(145)을 결정할 수 있다.Host 102 may include a selection engine 140 configured to select one or more machine learning models from multiple machine learning model(s) 135 as a non-priority machine learning model. The selection engine 140 may provide the selected machine learning model(s) 145 to the performance estimation engine 150 of the host 102. In some implementations, selection engine 140 may be configured to estimate, for all machine learning model(s) 135, a repetition time window for processing a frame of input data 137a received from host 102. there is. Additionally, selection engine 140 may determine a priority level for each machine learning model to process frames of input data within a time window. For example, selection engine 140 may determine one or more selected machine learning model(s) 145 based on a priority level for each machine learning model.
선택된 기계 학습 모델(들)(145)은 때때로 본 명세서에서 비-우선순위 기계 학습 모델로 지칭되며, 각각은 우선순위 기계 학습 모델보다 낮은 개별 우선순위 레벨을 갖는다. 우선순위 기계 학습 모델은 적어도 임계 우선순위 레벨을 갖는 기계 학습 모델, 가장 높은 우선순위 레벨을 갖는 지정된 수의 기계 학습 모델, 및/또는 입력 데이터(137b)의 각 프레임을 처리하는 데 사용되는 기계 학습 모델일 수 있다. 일부 구현에서는, 각 프레임, 예를 들어 반복 시간 윈도우의 각 발생에 대해 실행되는 단일 지정된 우선순위 기계 학습 모델이 있을 수 있는 반면, 다른 구현에서는 다수의 우선순위 기계 학습 모델이 있을 수 있다.The selected machine learning model(s) 145 are sometimes referred to herein as non-priority machine learning models, and each has an individual priority level that is lower than the priority machine learning model. A priority machine learning model may be a machine learning model with at least a threshold priority level, a specified number of machine learning models with the highest priority level, and/or a machine learning model used to process each frame of input data 137b. It could be a model. In some implementations, there may be a single specified priority machine learning model that runs for each frame, e.g., each occurrence of a repetition time window, while in other implementations there may be multiple priority machine learning models.
성능 추정 엔진(150)은 선택된 기계 학습 모델(들)(145) 각각의 추론 연산을 수행하기 위한 추정 기간을 결정하도록 구성된다. 성능 추정 엔진(150)은 또한 각각의 기계 학습 모델(145)에 대해, 추정 기간이 기준을 만족하는지 여부, 예를 들어 추정 기간이 반복 시간 윈도우의 잔여 기간보다 작거나 같은지 여부를 결정하도록 구성된다. 호스트(102)는 시간 윈도우내에서 하나 이상의 우선순위 기계 학습 모델의 추론 연산을 수행하기 위한 추정 기간에 기초하여 특정 반복 시간 윈도우에 대한 잔여 기간을 결정할 수 있다. 잔여 기간, 반복 시간 윈도우, 추정 기간은 아래에 자세히 설명되어 있다.The performance estimation engine 150 is configured to determine an estimation period for performing the inference operation for each of the selected machine learning model(s) 145. Performance estimation engine 150 is also configured to determine, for each machine learning model 145, whether the estimation period satisfies a criterion, e.g., whether the estimation period is less than or equal to the remaining period of the repetition time window. . Host 102 may determine a remaining period for a particular repetition time window based on an estimated period for performing inference operations of one or more priority machine learning models within the time window. The remaining period, repetition time window, and estimation period are described in detail below.
성능 추정 엔진(150)은 기계 학습 모델의 추론 수행을 위한 추정 기간이 입력 프레임을 처리하는 시간 윈도우의 잔여 기간을 초과하는지 여부를 결정할 수 있다. 추정 기간이 잔여 기간보다 크다고 결정하는 것에 응답하여, 분할 엔진(155)은 기계 학습 모델을 다수의 하위-모델로 분할(partition)하거나 분할(segment)할 수 있다. 각각의 하위-모델은 기계 학습 모델의 추론 연산들 중 적어도 비-중첩 부분을 포함한다. 분할 엔진(155)은 입력 데이터(137b)의 각 프레임이 처리되는 반복 시간 윈도우에 기초하여 기계 학습 모델을 분할하는 방법을 더 결정할 수 있다. 기계 학습 모델 분할의 세부 사항은 도 2와 관련하여 설명된다.The performance estimation engine 150 may determine whether the estimation period for performing inference of the machine learning model exceeds the remaining period of the time window for processing the input frame. In response to determining that the estimation period is greater than the remaining period, segmentation engine 155 may partition or segment the machine learning model into multiple sub-models. Each sub-model includes at least a non-overlapping portion of the inference operations of the machine learning model. Segmentation engine 155 may further determine how to segment the machine learning model based on the repetition time window over which each frame of input data 137b is processed. Details of machine learning model segmentation are explained with respect to Figure 2.
일부 구현에서, 호스트(102)의 컴파일러(180)는 성능 추정 엔진(150) 및 분할 엔진(155) 모두를 포함할 수 있다. 일부 구현에서, 성능 추정 엔진(150) 및/또는 분할 엔진(155)은 컴파일러(180)와 별개이다. 컴파일러(180)는 분할 엔진(155)에 의해 분할된 다수의 하위-모델과 분할되지 않은 다른 기계 학습 모델을 컴파일하도록 구성된다.In some implementations, compiler 180 of host 102 may include both a performance estimation engine 150 and a segmentation engine 155. In some implementations, performance estimation engine 150 and/or segmentation engine 155 are separate from compiler 180. Compiler 180 is configured to compile multiple sub-models partitioned by partition engine 155 and other unpartitioned machine learning models.
호스트(102)는 다수의 하드웨어 처리 유닛(110)의 개별 호스트 인터페이스(130)에 데이터 및 명령(125)을 전송할 수 있다. 각각의 처리 유닛(110)은 호스트 인터페이스(130)를 포함할 수 있다. 데이터 및 명령(125)은 입력 데이터(137b)의 각 프레임, 컴파일된 하위-모델(160) 및 기타 컴파일된 기계 학습 모델을 나타내는 데이터, 다양한 컴파일된 모델/하위-모델을 다양한 처리 유닛(110)에 할당하고 배포하는 데이터, 및 할당된 처리 유닛(110)에서 상기 배포된 모델들의 추론 연산을 수행하는 데이터 정렬 및 스케줄링을 포함한다. 예를 들어, 호스트(102)는 컴파일된 하위-모델(160) 및 하나 이상의 하드웨어 처리 유닛(110)으로 분할되지 않은 다른 기계 학습 모델을 배포할 수 있다.Host 102 may transmit data and commands 125 to individual host interfaces 130 of multiple hardware processing units 110 . Each processing unit 110 may include a host interface 130. Data and instructions 125 may process each frame of input data 137b, data representing compiled sub-models 160 and other compiled machine learning models, and various compiled models/sub-models to various processing units 110. It includes data allocation and distribution, and data sorting and scheduling to perform inference operations on the distributed models in the allocated processing unit 110. For example, host 102 may distribute compiled sub-models 160 and other machine learning models that are not partitioned into one or more hardware processing units 110.
호스트 인터페이스(130)는 다수의 처리 유닛(110)과 호스트(102) 사이의 통신을 조정하고 관리하는 데 사용된다. 일반적으로, 호스트 인터페이스(130)는 적절한 조합으로 소프트웨어 및/또는 하드웨어로 인코딩되고 호스트(102) 및 다른 구성요소와 통신하도록 동작 가능한 로직을 포함한다. 보다 구체적으로, 호스트 인터페이스(130)는 네트워크(120) 및/또는 인터페이스의 하드웨어가 처리 유닛(110) 내부 및 외부에서 물리적 신호를 전달하도록 동작 가능하도록 통신과 연관된 하나 이상의 통신 프로토콜을 지원하는 소프트웨어를 포함할 수 있다. 또한, 인터페이스(130)는 하드웨어 처리 유닛(110)이 호스트(102) 및/또는 네트워크(120)와 통신하여 다양한 동작(예를 들어, 본 명세서에 설명된 추론 연산)을 수행하도록 허용할 수 있다.Host interface 130 is used to coordinate and manage communications between multiple processing units 110 and host 102. Typically, host interface 130 includes logic encoded in software and/or hardware in any suitable combination and operable to communicate with host 102 and other components. More specifically, host interface 130 may include software that supports one or more communication protocols associated with the network 120 and/or hardware of the interface to enable communication to communicate physical signals within and outside of processing unit 110. It can be included. Additionally, interface 130 may allow hardware processing unit 110 to communicate with host 102 and/or network 120 to perform various operations (e.g., inference operations described herein). .
각 하드웨어 처리 유닛(110)은 입력 데이터(137b)의 각 프레임을 처리하기 위한 할당된 하위-모델 또는 모델의 추론 연산을 포함하는 기계 학습 계산을 수행하고, 모델(들)/하위-모델을 사용하여 입력 데이터(137b)의 프레임을 처리한 후 출력 데이터(167b)를 생성하도록 구성된다. 하드웨어 처리 유닛(110)은 출력 데이터(167)를 호스트(102)에 제공할 수 있고, 호스트(102)는 수신된 출력 데이터(167)를 스트리밍 방식 또는 시퀀스로 추론 출력(167a)으로서 출력할 수 있다. 일부 구현에서, 호스트(102)는 입력 데이터(137a)의 하나 이상의 프레임에 대한 출력 데이터(167b)를 집계하고 입력 데이터(137a)의 다중 프레임에 대한 추론 출력(167a)을 생성할 수 있다.Each hardware processing unit 110 performs machine learning calculations, including inferential operations of an assigned sub-model or model, to process each frame of input data 137b, using the model(s)/sub-models. It is configured to process the frame of the input data 137b and then generate the output data 167b. Hardware processing unit 110 may provide output data 167 to host 102, and host 102 may output received output data 167 in a streaming manner or in sequence as inference output 167a. there is. In some implementations, host 102 may aggregate output data 167b for one or more frames of input data 137a and generate inference output 167a for multiple frames of input data 137a.
일부 구현에서, 추론 시스템(100)(또는 다른 디바이스)을 포함하는 컴퓨팅 디바이스는 추론 시스템(100)에 요청을 보내는 애플리케이션 또는 애플리케이션 프로그래밍 인터페이스(API)를 포함할 수 있다. 예를 들어, 여기에는 추론 시스템(100)이 하나 이상의 대응하는 기계 학습 모델을 사용하여 입력 데이터(137a)를 처리하고 그 처리에 기초하여 하나 이상의 기계 학습 출력, 예를 들어 추론 출력(167)을 제공하도록 각각 요청하는 다수의 애플리케이션이 있을 수 있다.In some implementations, a computing device that includes inference system 100 (or other device) may include an application or application programming interface (API) that sends requests to inference system 100. For example, this may include inference system 100 processing input data 137a using one or more corresponding machine learning models and producing one or more machine learning outputs, e.g., inference output 167, based on the processing. There may be multiple applications each requesting to be provided.
특정 예에서, 카메라를 포함하는 클라이언트 디바이스는 카메라에 의해 캡처된 이미지를 처리하기 위한 하나 이상의 개별 기계 학습 모델을 각각 갖는 다수의 애플리케이션 또는 API를 포함할 수 있다. 각 애플리케이션 또는 API는 카메라에 의해 캡처된 각 프레임(예를 들어, 정적 이미지)에 대한 추론 출력을 요청할 수 있다. 다른 예에서, 추론 시스템(100)은 예를 들어 출력을 요청할 필요 없이 각 애플리케이션 또는 API에 추론 출력(167)을 제공하도록 구성될 수 있다.In certain examples, a client device that includes a camera may include multiple applications or APIs, each having one or more separate machine learning models for processing images captured by the camera. Each application or API may request inference output for each frame (e.g., static image) captured by the camera. In another example, inference system 100 may be configured to provide inference output 167 to each application or API, for example, without having to request the output.
호스트(102)는 런타임 동안 비-분할된 모델의 정규 추론 및 분할된 모델의 다중-패스 추론을 관리하도록 구성된 런타임 제어기(175)를 포함할 수 있다. 다중-패스 추론을 관리하기 위해, 런타임 제어기(175)는 호스트(102) 또는 다중 하드웨어 처리 유닛(110) 상의 서로 다른 메모리(예를 들어, 메모리(106 또는 170))로부터 중간 입력 및 출력을 저장하고 페치하는 것을 관리할 수 있다. 런타임 제어기(175)는 입력 데이터(137a)(예를 들어, 입력 데이터(137b)의 프레임), 입력 데이터(137a)에 대한 개별 반복 시간 윈도우, 분할된 하위-모델, 또는 하드웨어 처리 유닛(110)의 계산 용량 중 적어도 하나에 기초하여 다중 추론 계산을 스케줄링할 수 있다.Host 102 may include a runtime controller 175 configured to manage regular inference of non-partitioned models and multi-pass inference of partitioned models during runtime. To manage multi-pass inference, runtime controller 175 stores intermediate inputs and outputs from different memories (e.g., memory 106 or 170) on host 102 or multiple hardware processing units 110. and manage fetching. Runtime controller 175 may control input data 137a (e.g., a frame of input data 137b), a separate repetition time window for input data 137a, a partitioned sub-model, or hardware processing unit 110. Multiple inference calculations may be scheduled based on at least one of the calculation capacities of .
또한, 호스트(102)는 하나 이상의 중앙 처리 장치(CPU)(104)를 포함할 수 있다. CPU(104)는 특정 제어 또는 물류 동작을 수행하기 위해 호스트에 처리를 제공할 수 있다. 일부 구현에서, CPU(104)는 추론 동안 일부 프로세스를 실행할 수 있다. 일반적으로, CPU(104)는 명령을 실행하고 데이터를 조작하여 호스트(102)의 동작을 수행한다. 각 CPU(104)는 단일 코어 또는 다중 코어를 가질 수 있으며, 각 코어는 호스트(102)에 사용 가능하고 개별 처리 스레드를 실행한다. 또한, 본 명세서에 설명된 연산들을 실행하는 데 사용되는 CPU(104)의 수, 유형 및 특정 CPU(104)는 호스트(102)와 관련된 요청, 상호 작용 및 동작의 수에 기초하여 동적으로 결정될 수 있다.Host 102 may also include one or more central processing units (CPUs) 104. CPU 104 may provide processing to the host to perform specific control or logistics operations. In some implementations, CPU 104 may execute some processes during inference. Generally, the CPU 104 executes instructions and manipulates data to perform the operations of the host 102. Each CPU 104 may have a single core or multiple cores, with each core available to the host 102 and executing a separate processing thread. Additionally, the number, type, and specific CPUs 104 of CPUs 104 used to execute the operations described herein may be dynamically determined based on the number of requests, interactions, and operations associated with the host 102. there is.
게다가, 호스트(102)는 메모리(106)를 포함할 수 있다. 호스트(102)의 메모리(106)는 단일 메모리 또는 다중 메모리를 나타낼 수 있다. 메모리(106)는 이에 한정되지 않지만 자기 매체, 광학 매체, RAM, ROM, 이동식 매체, 또는 기타 적합한 로컬 또는 원격 메모리 구성 요소를 비롯하여 임의의 메모리 또는 데이터베이스 모듈을 포함할 수 있으며 휘발성 또는 비휘발성 메모리의 형태를 취할 수 있다. 메모리(106)는 실행 그래프, 기계 학습 모델, 관리 설정, 캐시, 애플리케이션, 백업 데이터, 및 임의의 파라미터, 변수, 알고리즘, 명령, 규칙, 제약 조건 또는 이에 대한 참조를 포함하여 호스트(102)와 연관된 임의의 다른 적절한 정보를 포함하는 다양한 객체 또는 데이터를 저장할 수 있다. 호스트(102) 내에 도시되어 있지만, 메모리(106) 또는 특별히 도시된 구성 요소 중 일부 또는 전부를 포함하는 그의 임의의 부분은 일부 경우에는 클라우드 애플리케이션이나 저장소로서, 또는 호스트(102) 자체가 클라우드 기반 시스템인 경우 별도의 클라우드 애플리케이션이나 저장소를 포함하여 일부 경우에 호스트(102)로부터 원격에 위치할 수 있다. 일부 예에서, 메모리(106)에 저장된 데이터는 예를 들어 네트워크(120)를 통해 액세스 가능하고 하드웨어 처리 유닛(110)의 특정 애플리케이션 또는 기능에 의해 획득될 수 있다.Additionally, host 102 may include memory 106 . Memory 106 of host 102 may represent a single memory or multiple memories. Memory 106 may include any memory or database module, including but not limited to magnetic media, optical media, RAM, ROM, removable media, or other suitable local or remote memory components, and may include volatile or non-volatile memory. It can take any form. Memory 106 is associated with host 102, including execution graphs, machine learning models, management settings, caches, applications, backup data, and any parameters, variables, algorithms, instructions, rules, constraints, or references thereto. A variety of objects or data may be stored, including any other suitable information. Although shown within host 102, memory 106 or any portion thereof, including some or all of the components specifically shown, may in some cases be used as cloud applications or storage, or as host 102 itself as a cloud-based system. In some cases, it may be located remotely from host 102, including in a separate cloud application or storage. In some examples, data stored in memory 106 may be accessible by, for example, network 120 and obtained by a particular application or function of hardware processing unit 110.
각각의 처리 디바이스(110)는 다른 디바이스와 독립적으로 동작을 수행하는 하드웨어 리소스를 포함할 수 있다. 예를 들어, 각 처리 유닛(110)에는 하나 이상의 프로세서, 컴퓨팅 타일, 코어 등이 포함될 수 있다. 처리 유닛(110)은 GPU 및 CPU뿐만 아니라 신경망 트레이닝에 사용되는 특정 연산(예를 들어, 행렬 곱셈)을 효율적으로 수행하기 위한 특수 하드웨어 리소스를 포함할 수 있다. 특수 하드웨어 리소스의 예로는 텐서 처리 장치("TPU"), 필드 프로그래밍 가능 게이트 어레이("FGPA") 및 주문용 집적 회로("ASIC")가 있다.Each processing device 110 may include hardware resources that perform operations independently of other devices. For example, each processing unit 110 may include one or more processors, computing tiles, cores, etc. Processing unit 110 may include GPUs and CPUs, as well as specialized hardware resources to efficiently perform specific operations (e.g., matrix multiplication) used in neural network training. Examples of specialized hardware resources include tensor processing units (“TPUs”), field programmable gate arrays (“FGPAs”), and custom integrated circuits (“ASICs”).
각각의 하드웨어 처리 유닛(110)은 이종일 수 있으며, 예를 들어 디바이스마다 다른 다양한 유형의 다수의 처리 유닛을 가질 수 있다. 대안적으로, 각각의 하드웨어 처리 유닛(110)은 동일한 수 및 유형의 처리 유닛을 포함할 수 있다. 또한, 하드웨어 처리 유닛(110)은 각각의 계산 능력을 가질 수 있다. 즉, 각 하드웨어 처리 유닛은 서로 다른 양의 메모리(170), 처리 속도 또는 기타 구조적 특성을 가질 수 있다. 따라서, 일부 하드웨어 처리 유닛은 다른 하드웨어 처리 유닛이 수행할 수 없는 동작을 수행할 수 있다. 예를 들어, 일부 동작에는 특정 하드웨어 처리 유닛에만 있는 특정 양의 메모리가 필요할 수 있으며, 일부 처리 유닛은 추론 연산과 같은 특정 유형의 동작만 수행하도록 구성될 수 있다.Each hardware processing unit 110 may be heterogeneous, for example, may have multiple processing units of various types that differ from device to device. Alternatively, each hardware processing unit 110 may include the same number and type of processing units. Additionally, the hardware processing units 110 may have respective computational capabilities. That is, each hardware processing unit may have a different amount of memory 170, processing speed, or other structural characteristics. Accordingly, some hardware processing units can perform operations that other hardware processing units cannot perform. For example, some operations may require a certain amount of memory found only in certain hardware processing units, and some processing units may be configured to perform only certain types of operations, such as inferential operations.
더욱이, 각각의 하드웨어 처리 유닛(110)은 호스트(102)의 메모리(106)에 액세스할 수 있고 메모리 유닛(170)을 가질 수 있다. 메모리 유닛(170) 각각은 이에 한정되지 않지만 자기 매체, 광학 매체, RAM, ROM, 이동식 매체 또는 기타 적합한 로컬 또는 원격 메모리 구성 요소를 포함하여 임의의 메모리 또는 데이터베이스 모듈을 포함할 수 있으며 휘발성 또는 비휘발성 메모리의 형태를 취할 수 있다. 메모리 유닛(170)은 다양한 객체 또는 데이터, 관리 설정, 캐시, 애플리케이션, 백업 데이터, 동적 정보를 저장하는 저장소, 그리고 추론을 위한 임의의 파라미터, 변수, 알고리즘, 명령, 규칙, 제약 조건 또는 참조를 포함하여 하드웨어 처리 유닛(110)과 연관된 임의의 다른 적절한 정보를 저장할 수 있다. 메모리 유닛(170)은 하드웨어 처리 유닛의 각 타일에 의해 또는 다수의 하드웨어 처리 유닛에 걸쳐 액세스 가능한 공유 메모리를 포함할 수 있다. 공유 메모리는 하드웨어 처리 유닛(110)의 다중 타일 각각에 의해 사용되는 공유 주소 공간을 포함할 수 있다.Moreover, each hardware processing unit 110 may have access to memory 106 of host 102 and may have a memory unit 170 . Each of memory units 170 may include any memory or database module, including but not limited to magnetic media, optical media, RAM, ROM, removable media, or other suitable local or remote memory components, and may be volatile or non-volatile. It can take the form of memory. Memory unit 170 contains storage for storing various objects or data, management settings, caches, applications, backup data, dynamic information, and arbitrary parameters, variables, algorithms, instructions, rules, constraints, or references for inference. may store any other suitable information associated with hardware processing unit 110. Memory unit 170 may include shared memory accessible by each tile of a hardware processing unit or across multiple hardware processing units. Shared memory may include a shared address space used by each of multiple tiles of hardware processing unit 110.
도 2는 다양한 시나리오의 다양한 시간 윈도우 내에서 다수의 기계 학습 모델의 추론 연산을 수행하기 위한 예시적인 프로세스(200)를 도시한다. 편의상, 프로세스(200)는 하나 이상의 위치에 위치하는 하나 이상의 컴퓨터 시스템에 의해 수행되는 것으로 설명된다. 예를 들어, 적절하게 프로그래밍된 추론 시스템, 예를 들어 도 1의 시스템(100)은 프로세스(200)를 수행할 수 있다.2 illustrates an example process 200 for performing inference operations of multiple machine learning models within various time windows in various scenarios. For convenience, process 200 is described as being performed by one or more computer systems located at one or more locations. For example, a suitably programmed reasoning system, such as system 100 of FIG. 1, can perform process 200.
도 1의 추론 시스템(100)은 전술한 바와 같이, 시스템에 대한 시간 윈도우를 결정할 수 있다. 보다 구체적으로, 시간 윈도우는 시스템(100)이 각각의 계산 주기(cycle) 내에서 추론 연산들을 수행하는 기간을 포함할 수 있다. 시간 윈도우는 입력 데이터의 각 프레임, 예를 들어 초당 입력 프레임을 수신하는 속도(rate)에 기초하여 시스템(100)에 의해 결정될 수 있다. 예를 들어, 시간 윈도우는 1ms, 10ms, 20ms, 50ms, 100ms의 기간 또는 다른 적절한 기간을 포함할 수 있다. 위에서 설명된 바와 같이, 시간 윈도우는 반복 시간 윈도우일 수 있다. 예를 들어, 다른 계산 주기에 대응하는 또 다른 시간 윈도우는 이전 계산 주기에 대응하는 이전 시간 윈도우 종료 직후에 있을 수 있다.Inference system 100 of FIG. 1 may determine a time window for the system, as described above. More specifically, the time window may include a period of time during which system 100 performs inference operations within each calculation cycle. The time window may be determined by system 100 based on the rate of receiving each frame of input data, for example, input frames per second. For example, the time window may include a period of 1 ms, 10 ms, 20 ms, 50 ms, 100 ms, or other suitable period. As described above, the time window may be a repeating time window. For example, another time window corresponding to a different computation cycle may immediately follow the end of a previous time window corresponding to a previous computation cycle.
도 2에 도시된 바와 같이, 시간축(210)을 따라 다수의 시간 윈도우(205a, 205b, 205c)가 있다. 205a-c의 각 시간 윈도우는 시스템이 입력 데이터의 프레임을 처리하기 위한 추론 연산을 수행하는 기간을 포함할 수 있다. 서로 다른 시간 윈도우는 서로 다른 길이의 기간을 가질 수 있다. 대안적으로, 하나 이상의 서로 다른 시간 윈도우는 동일한 길이의 기간을 공유할 수 있다.As shown in Figure 2, there are multiple time windows 205a, 205b, and 205c along the time axis 210. Each time window in 205a-c may include a period during which the system performs an inference operation to process a frame of input data. Different time windows may have different length periods. Alternatively, one or more different time windows may share a period of equal length.
시스템(100)은 각각 특정 추론 연산을 수행하기 위해 지정된 다중 기계 학습 모델을 포함할 수 있다. 예를 들어, 시스템(100)이 하나 이상의 객체를 포함하는 장면의 이미지 또는 비디오(이미지의 시간 시퀀스)를 촬영하도록 구성된 카메라 시스템에 의해 사용되는 경우이다. 시스템(100)은 다양한 작업(task)을 위한 다중 기계 학습 모델을 포함할 수 있고 그 작업을 완료하기 위해 특정 빈도(예를 들어, 50밀리초당 하나의 이미지)로 수신된 이미지의 각 프레임을 처리하기 위해 기계 학습 모델 각각의 추론 연산을 수행할 수 있다. 예를 들어, 작업에는 이미지 프레임의 초점을 자동으로 결정하고, 이미지 프레임 안의 객체를 감지하고, 이미지 프레임에서 캡처된 사람 얼굴을 감지 및 인식하고, 이미지 프레임의 깊이 이미지를 결정하는 것이 포함될 수 있다.System 100 may include multiple machine learning models, each designated to perform a specific inference operation. For example, when system 100 is used by a camera system configured to capture images or video (a temporal sequence of images) of a scene containing one or more objects. System 100 may include multiple machine learning models for various tasks and processes each frame of received images at a certain frequency (e.g., one image per 50 milliseconds) to complete the task. To do this, inference operations can be performed for each machine learning model. For example, tasks may include automatically determining the focus of an image frame, detecting objects within an image frame, detecting and recognizing human faces captured in an image frame, and determining the depth image of an image frame.
바람직하게는, 시스템(100)은 단일 시간 윈도우 내에서 또는 다른 입력 프레임을 수신하기 전에 입력 프레임을 처리하기 위한 모든 기계 학습 모델의 추론 연산을 수행할 수 있다. 그러나, 시스템(100)이 다수의 작업을 수행하는 경우에는 단일 시간 윈도우보다 더 많은 시간이 필요할 것으로 예상된다. 시스템(100)은 각 입력 프레임이 적시에 처리되도록 추론 연산을 수행하는 순서를 배열하고 스케줄링하도록 구성된다.Advantageously, system 100 can perform all of the machine learning model's inference operations to process an input frame within a single time window or before receiving another input frame. However, if system 100 performs multiple tasks, it is expected to require more time than a single time window. System 100 is configured to sequence and schedule the order in which inference operations are performed such that each input frame is processed in a timely manner.
이러한 배열을 하기 위해, 시스템(100)은 기계 학습 모델 각각에 대한 우선순위 레벨을 결정할 수 있다. 일반적으로, 시스템(100)은 기계 학습 모델의 출력이 다른 기계 학습 모델의 입력으로 제공되는지 여부(즉, 출력 종속성), 기계 학습 모델의 모델 크기, 기계 학습 모델의 추론 작업을 수행하는 데 필요한 대략적인 시간 및 계산 리소스, 및 대기 시간에 대한 민감도 수준과 같은 다양한 양태에 기초하여 우선순위의 레벨을 결정할 수 있다.To make this arrangement, system 100 can determine a priority level for each machine learning model. In general, system 100 determines whether the output of a machine learning model is provided as input to another machine learning model (i.e., output dependency), the model size of the machine learning model, and the approximate amount needed to perform the inference task of the machine learning model. The level of priority can be determined based on various aspects, such as the level of sensitivity to latency, and time and computational resources.
좀 더 구체적인 예로서, 기계 학습 모델은 하나 이상의 다른 기계 학습 모델이 기계 학습 모델의 출력을 사용하여 이미지를 처리하는 경우 시스템(100)에 의해 높은 우선순위 레벨을 갖도록 결정될 수 있다. 다른 예로서, 데이터 대기 시간(latency)에 민감한 기계 학습 모델(예를 들어, 카메라 뷰파인더용 모델)은 시스템(100)에 의해 높은 우선순위 레벨을 갖도록 결정될 수 있다. 또 다른 예로서, 이미지 프레임의 깊이 이미지를 예측하도록 구성된 기계 학습 모델은 깊이 이미지를 생성하기 위한 모델이 크고 더 긴 처리 시간이 필요하며 다른 기계 학습 모델이 예측된 출력을 사용하지 않는 경우 낮은 레벨의 우선순위를 가질 수 있다. .As a more specific example, a machine learning model may be determined by system 100 to have a high priority level if one or more other machine learning models are using the output of the machine learning model to process the image. As another example, a machine learning model that is sensitive to data latency (e.g., a model for a camera viewfinder) may be determined by the system 100 to have a high priority level. As another example, a machine learning model configured to predict the depth image of an image frame may require a low-level model to generate the depth image if the model is large, requires longer processing time, and no other machine learning model uses the predicted output. You can have priorities. .
또 다른 예로, 시스템(100)은 기계 학습 모델을 우선순위 기계 학습 모델로서 선택하거나 기계 학습 모델에 의해 수행되는 작업(들)에 기초하여 기계 학습 모델에 대한 우선 순위 레벨을 결정할 수 있다. 텍스트 또는 음성을 처리하는 맥락의 특정 예에서, 시스템은 텍스트와 음성 간의 변환 작업(예를 들어, 텍스트-음성 변환(TTS) 모델에 지정된 동작과 같이 텍스트를 음성 오디오로 변환)과 관련된 기계 학습 모델은 높은 우선순위 레벨을 갖는다고 결정할 수 있다. 시스템(100)은 우선순위 기계 학습 모델인 TTS 모델에 기초하여 TTS 모델에 지정된 연산들의 우선순위를 지정할 수 있다. 이러한 방식으로, 시스템(100)은 다른 모델의 동작을 수행할 때 TTS 모델로부터의 출력(예를 들어, 오디오 프레임)을 기다리는 "유휴 시간"이 없음(또는 최소)을 보장할 수 있다. 이러한 유휴 시간을 방지함으로써 시스템(100)은 청중에게 음성을 생성하기 위해 텍스트 입력의 각 프레임을 처리할 때 오디오의 "끊김"(즉, 오디오 버퍼 언더런(under-run))을 방지한다.As another example, system 100 may select a machine learning model as a priority machine learning model or determine a priority level for a machine learning model based on the task(s) performed by the machine learning model. In certain instances in the context of text or speech processing, the system may use a machine learning model associated with a text-to-speech conversion task (e.g., converting text to spoken audio, such as an operation specified in a text-to-speech (TTS) model). can be determined to have a high priority level. System 100 may prioritize operations specified in the TTS model based on the TTS model, which is a priority machine learning model. In this way, system 100 can ensure that there is no (or minimal) “idle time” waiting for output (e.g., audio frames) from a TTS model when performing the operations of other models. By preventing this idle time, system 100 prevents "stuttering" of the audio (i.e., audio buffer under-run) as it processes each frame of text input to generate speech to the audience.
기계 학습 모델의 우선순위 레벨을 결정한 후, 시스템(100)은 규모는 크지만 낮은 우선순위 레벨(예를 들어, 하나 이상의 다른 기계 학습 모델의 우선순위 레벨보다 낮은 우선순위 레벨)을 갖는 하나의 기계 학습 모델을 선택할 수 있다. 예를 들어, 도 2에 도시된 기계 학습 모델(245a 및 245b) 또는 도 1의 선택된 모델(들)(145)은 낮은 우선순위 기계 학습 모델(들)로 간주될 수 있다.After determining the priority level of the machine learning model, system 100 configures one machine with a larger but lower priority level (e.g., a lower priority level than the priority level of one or more other machine learning models). You can select a learning model. For example, machine learning models 245a and 245b shown in Figure 2 or selected model(s) 145 in Figure 1 may be considered low priority machine learning model(s).
시스템(100)은 입력을 처리하기 위한 기계 학습 모델의 추론 연산을 수행하기 위한 추정 기간을 결정할 수 있다. 예를 들어, 시스템(100)은 선택된 기계 학습 모델의 추론 연산을 낮은 우선순위 레벨로 수행하기 위한 추정 기간을 결정할 수 있다. 도 2에 도시된 바와 같이, 시스템(100)은 입력 프레임을 처리하기 위해 기계 학습 모델(245a)의 추론 연산을 수행하기 위한 추정 기간(230a)과, 또 다른 입력 프레임을 처리하기 위해 기계 학습 모델(245b)의 추론 연산을 수행하기 위한 또 다른 추정 기간(230b)을 결정할 수 있다.System 100 may determine an estimation period for performing an inference operation of a machine learning model for processing the input. For example, system 100 may determine an estimate period for performing inference operations of a selected machine learning model at a low priority level. As shown in Figure 2, system 100 includes an estimation period 230a for performing inference operations on a machine learning model 245a to process an input frame, and an estimation period 230a for processing another input frame. Another estimation period 230b for performing the inference operation 245b may be determined.
일반적으로, 시스템(100)은 우선순위 기계 학습 모델(예를 들어, 높은 우선순위 레벨을 갖는 것으로 분류된 기계 학습 모델)의 추론 연산을 수행하기 위해 각 시간 윈도우의 일부를 예약할 수 있다. 우선순위 기계 학습 모델을 위해 예약된 각 시간 윈도우의 일부는 위에서 설명한 바와 같이 각 시간 윈도우에 대한 "우선순위 기간"이라고도 한다. 도 2에 도시된 바와 같이, 우선순위 기간(215a, 215b 및 215c)은 우선순위 기계 학습 모델(들)을 위해 예약된 각 반복 시간 윈도우(205a, 205b 및 205c)의 일부를 나타낼 수 있다. 한편, 시스템(100)은 우선순위 기간에 기초하여 각 시간 윈도우에 대한 잔여 기간을 결정할 수 있다. 예를 들어, 도 2에 도시된 바와 같이, 시스템(100)은 각각의 반복 시간 윈도우(205a, 205b, 205c)에 대해 각각 220a, 220b, 220c의 잔여 기간을 결정할 수 있다.In general, system 100 may reserve a portion of each time window to perform inference operations on priority machine learning models (e.g., machine learning models classified as having a high priority level). The portion of each time window reserved for the priority machine learning model is also referred to as the "priority period" for each time window, as described above. As shown in Figure 2, priority periods 215a, 215b, and 215c may represent portions of each repetition time window 205a, 205b, and 205c reserved for the priority machine learning model(s). Meanwhile, system 100 may determine the remaining period for each time window based on the priority period. For example, as shown in Figure 2, system 100 can determine the remaining periods of 220a, 220b, and 220c for each repetition time window 205a, 205b, and 205c, respectively.
그런 다음, 시스템(100)은 비-우선순위 기계학습 모델(예를 들어, 낮은 우선순위 레벨을 갖는 것으로 분류된 기계학습 모델)의 추론 연산을 수행하기 위한 추정 기간과 그 시간 윈도우에 대한 잔여 기간을 비교함으로써, 추정 기간이 잔여 기간보다 작거나 같은지(이하인지) 여부를 결정할 수 있다.System 100 then provides an estimate period for performing inference operations of a non-priority machine learning model (e.g., a machine learning model classified as having a low priority level) and a remaining period for that time window. By comparing , we can determine whether the estimated period is less than or equal to (less than) the remaining period.
비-우선순위 기계학습 모델의 추론 연산을 수행하기 위한 추정 기간이 잔여 기간보다 작거나 같다고 결정하는 것에 응답하여, 시스템(100)은 모델을 다수의 하위-모델로 분할하지 않고 비-우선순위 기계 학습 모델의 연산들을 수행하도록 예약할 수 있다. 예를 들어, 도 2의 시나리오 A에 도시된 바와 같이, 시스템(100)은 추정 기간(230a)이 잔여 기간(220a)보다 작다고 결정한다. 이에 응답하여, 도 1의 분할 엔진(155)과 동일한 분할 엔진(255)은 기계 학습 모델(245a)을 분할하지 않는다. 대신에, 시스템(100)은 기계 학습 모델(예를 들어, 컴파일된 모델(270a))을 직접 컴파일하고 반복 시간 윈도우(205a) 내의 우선순위 기간(215a) 이후에 상기 컴파일된 모델(270a)의 추론 연산들을 수행할 수 있다. 이 예에서는 각각의 우선순위 기간(215a - 215c)이 각각의 반복 시간 윈도우(205a-205c)의 시작 부분에 발생하는 것으로 도시되어 있지만, 우선순위 기간(205a-215c)은 끝 부분에 위치할 수 있거나 다른 구현에서는 시작과 끝 사이 어딘가에 위치할 수 있다.In response to determining that the estimate period for performing the inference operation of the non-priority machine learning model is less than or equal to the remaining period, system 100 may The operations of the learning model can be scheduled to be performed. For example, as shown in Scenario A of FIG. 2, system 100 determines that estimated period 230a is less than remaining period 220a. In response, segmentation engine 255, which is identical to segmentation engine 155 in Figure 1, does not segment machine learning model 245a. Instead, system 100 directly compiles a machine learning model (e.g., compiled model 270a) and replaces the compiled model 270a after a priority period 215a within an iteration time window 205a. Inferential operations can be performed. In this example, each priority period 215a - 215c is shown as occurring at the beginning of each repetition time window 205a - 205c, but priority periods 205a - 215c may be located at the end. or, in other implementations, it may be located somewhere between the start and the end.
일부 구현에서, 시스템은 추정 기간이 잔여 기간보다 작거나 같은 것으로 결정되는 경우에도 비-우선순위 기계 학습 모델을 분할하기로 결정할 수 있다.In some implementations, the system may decide to split a non-priority machine learning model even if the estimation period is determined to be less than or equal to the remaining period.
대안적으로, 비-우선순위 기계학습 모델의 추론 연산을 수행하기 위한 추정 기간이 전여 기간보다 크다고 결정하는 것에 응답하여, 시스템(100)은 기계 학습 모델을 다수의 하위-모델로 분할하고 시퀀스에 따라 그 다수의 하위-모델의 부분 추론 연산을 수행하도록 예약할 수 있다. 시퀀스는 원래 기계 학습 모델과 기계 학습 모델이 분할되는 방식에 기초하여 결정된다. 기계 학습 모델을 분할하여 다수의 하위-모델을 생성하는 세부 사항은 도 3a와 관련하여 설명된다. 예를 들어, 도 2의 시나리오 B에 도시된 바와 같이, 시스템(100)은 추정 기간(230b)이 잔여 기간(220b)보다 크다고 결정한다. 이에 응답하여, 도 1의 분할 엔진(155)과 동일하거나 유사할 수 있는 분할 엔진(255)은 기계 학습 모델(345b)을 4개의 하위-모델로 분할한다. 시스템(100)은 4개의 하위-모델을 추가로 컴파일하고 처리 유닛(110)에 4개의 컴파일된 하위-모델(280a, 280b, 280c, 280d)을 배포하는데, 예를 들어 그 각각은 해당 처리 유닛(110)에 할당되거나 모두 동일한 처리 유닛(110)에 할당된다.Alternatively, in response to determining that the estimation period for performing the inference operation of the non-priority machine learning model is greater than the preceding period, system 100 may partition the machine learning model into multiple sub-models and sequence them. Accordingly, partial inference operations of the multiple sub-models can be scheduled to be performed. The sequence is determined based on the original machine learning model and how the machine learning model is split. Details of splitting a machine learning model to generate multiple sub-models are described with respect to Figure 3A. For example, as shown in Scenario B of FIG. 2, system 100 determines that estimated period 230b is greater than remaining period 220b. In response, segmentation engine 255, which may be the same or similar to segmentation engine 155 of FIG. 1, divides machine learning model 345b into four sub-models. System 100 further compiles four sub-models and distributes the four compiled sub-models 280a, 280b, 280c, and 280d to processing units 110, each of which, for example, has a corresponding processing unit. 110 or are all assigned to the same processing unit 110.
바람직하게는, 컴파일된 4개의 하위-모델의 추론 연산을 수행하는 데 필요한 총 시간은 추정 기간(230b)과 실질적으로 동일해야 한다. 그러나, 데이터 저장, 전송 및 가속기 간 대기 시간에 소요되는 시간으로 인해 4개의 하위-모델에 필요한 총 시간은 추정 기간(230b)보다 클 수 있다. 따라서 시스템은 우선순위 기간(215a) 이후 반복 시간 윈도우(205b)에서 하위-모델(280a, 280b, 280c)의 추론 연산을 수행하도록 배열하고 스케줄링할 수 있고, 우선순위 기간(215c) 이후 반복 시간 윈도우(205c)에서 하위-모델(280d)의 추론 연산을 수행하도록 스케줄링한다.Preferably, the total time required to perform the inference operations of the four compiled sub-models should be substantially equal to the inference period 230b. However, due to the time spent on data storage, transmission, and latency between accelerators, the total time required for the four sub-models may be greater than the estimated period 230b. Accordingly, the system can arrange and schedule to perform the inference operations of sub-models 280a, 280b, 280c in a repetition time window 205b after priority period 215a, and in a repetition time window after priority period 215c. At 205c, the inference operation of the sub-model 280d is scheduled to be performed.
시스템(100)이 다수의 시간 윈도우에 걸쳐 다수의 하위-모델의 추론 연산을 수행하도록 스케줄링하는 상황에서, 시스템(100), 특히 런타임 제어기(175)는 하드웨어 처리 유닛(110)의 개별 메모리 유닛(170)으로부터 중간 출력 및 입력을 저장하고 페치하도록 구성된다.In situations where system 100 schedules to perform inference operations of multiple sub-models over multiple time windows, system 100, particularly runtime controller 175, may control individual memory units of hardware processing unit 110 ( It is configured to store and fetch intermediate outputs and inputs from 170).
설명의 편의를 위해 도 2에 도시된 바와 같이 기계 학습 모델(245b)로부터 분할된 하위-모델(280a-d)은 4개뿐이지만, 시스템(100)은 기계 학습 모델을 서로 다른 수의 하위-모델, 예를 들어 2, 5, 10 및 20개의 하위-모델로 분할할 수 있음을 이해해야 한다. 4개의 하위-모델(280a-d)은 설명의 편의를 위해 도 2에 도시된 것처럼 2개의 시간 윈도우에 걸쳐 스케줄링되어 있지만, 시스템(100)은 일반적으로 예를 들어 3개, 5개, 10개의 시간 윈도우에 걸쳐 다수의 하위-모델의 추론 연산 수행을 배열하고 스케줄링할 수 있다. 다른 적절한 수량의 하위-모델 및 시간 위도우도 사용될 수 있다.For convenience of explanation, as shown in FIG. 2, there are only four sub-models 280a-d split from the machine learning model 245b, but the system 100 divides the machine learning model into different numbers of sub-models. It should be understood that the model can be split into, for example, 2, 5, 10 and 20 sub-models. The four sub-models 280a-d are scheduled over two time windows as shown in FIG. 2 for ease of explanation, but system 100 typically operates over two time windows, for example three, five, or ten. The performance of inference operations of multiple sub-models can be arranged and scheduled over time windows. Other suitable quantities of sub-models and time windows may also be used.
일부 구현에서, 시스템(100)은 제1 비-우선순위 기계 학습 모델을 분석한 후 다른 기계 학습 모델에 대한 추가 잔여 기간을 결정할 수 있다. 예를 들어, 시스템(100)은 입력을 처리하여 제2 추론 출력을 생성하기 위한 제2 기계 학습 모델의 추론 연산을 수행하기 위해 제2 추정 기간(미도시)을 획득할 수 있다. 시스템(100)은 또한 (i) 우선순위 기계 학습 모델(들)의 우선순위 추론 연산을 수행하기 위한 우선순위 기간 및 (ii) 제1 기계 학습 모델(예를 들어, 시스템(100)에 의해 분할되거나, 스케줄되거나 둘 모두인 기계 학습 모델(245a 및 245b))에서 하위-모델의 추론 연산을 수행하기 위한 적어도 각각의 추정 기간을 예약한 후 하나 이상의 비-우선순위 기계 학습 모델에 대한 제2 잔여 기간(미도시)을 결정할 수 있다. 시스템은 위에서 설명한 유사한 단계를 따라 제2 비-우선순위 기계 학습 모델을 하위-모델 그룹으로 분할할지 여부를 결정할 수 있고, 현재 반복 시간 윈도우의 제2 잔여 기간 내 또는 다수의 반복 시간 윈도우에 걸쳐 하위-모델의 추론 작업 수행을 스케줄링할 수 있다.In some implementations, system 100 may determine additional remaining periods for other machine learning models after analyzing the first non-priority machine learning model. For example, system 100 may process input to obtain a second inference period (not shown) to perform an inference operation of a second machine learning model to generate a second inference output. System 100 may also provide (i) a priority period for performing a priority inference operation of the priority machine learning model(s) and (ii) a partitioning by system 100 of the first machine learning model (e.g., a second residual for one or more non-priority machine learning models after reserving at least each estimation period for performing inference operations of sub-models in the machine learning models 245a and 245b that are scheduled, scheduled, or both. A period (not shown) can be determined. The system may follow similar steps described above to determine whether to split the second non-priority machine learning model into groups of sub-models, either within a second remaining period of the current iteration time window or across multiple iteration time windows. -You can schedule the performance of the model’s inference work.
도 3a는 기계 학습 모델로부터 분할된 다수의 하위-모델의 추론 연산을 수행하기 위한 예시적인 프로세스(300)를 도시한다. 편의상, 프로세스(300)는 하나 이상의 위치에 위치하는 하나 이상의 컴퓨터 시스템에 의해 수행되는 것으로 설명된다. 예를 들어, 적절하게 프로그래밍된 추론 시스템, 예를 들어 도 1의 시스템(100)은 프로세스(300)를 수행할 수 있다.FIG. 3A shows an example process 300 for performing inference operations on multiple sub-models partitioned from a machine learning model. For convenience, process 300 is described as being performed by one or more computer systems located at one or more locations. For example, a suitably programmed reasoning system, such as system 100 of FIG. 1, can perform process 300.
도 3a에 도시된 바와 같이, 배치된 하위-모델(325)은 도 1의 컴파일된 하위-모델(160) 또는 도 2의 컴파일된 하위-모델(280a-d)과 동일할 수 있다. 배치된 하위-모델(325)은 선택된 기계 학습 모델(예를 들어, 낮은 우선순위 레벨을 갖는 대규모 기계 학습 모델)로부터 도 1의 분할 엔진(155)과 동등한 부할 엔진에 의해 파티셔닝된다. 시스템(100)은 입력 데이터(343)를 처리하고 출력 데이터(347)를 생성하기 위한 추론 연산을 수행하기 위해 하나 이상의 하드웨어 처리 유닛, 예를 들어 기계 학습 가속기에 컴파일된 하위-모델들을 배치할 수 있다.As shown in FIG. 3A, the deployed sub-model 325 may be the same as compiled sub-model 160 of FIG. 1 or compiled sub-models 280a-d of FIG. 2. The deployed sub-models 325 are partitioned from the selected machine learning model (e.g., a large machine learning model with a low priority level) by a load engine equivalent to the partitioning engine 155 in FIG. 1 . System 100 may deploy the compiled sub-models to one or more hardware processing units, such as a machine learning accelerator, to process input data 343 and perform inference operations to generate output data 347. there is.
일부 구현에서, 기계 학습 모델에 신경망이 포함될 수 있다. 각 신경망에는 시퀀스로 배열된 닷의 네트워크 계층이 포함될 수 있으며, 각 네트워크 계층에는 특정 트레이닝 샘플에 대해 트레이닝된 다중 파라미터가 포함될 수 있다. 시스템은 입력에 대한 출력을 생성하는 시퀀스에 따라 각 네트워크 계층에 지정된 추론 연산을 수행할 수 있다. 신경망의 다양한 유형과 동작은 아래에서 더 자세히 설명된다.In some implementations, machine learning models may include neural networks. Each neural network may contain network layers of dots arranged in a sequence, and each network layer may contain multiple parameters trained on specific training samples. The system can perform inference operations specified in each network layer according to the sequence that produces the output for the input. The various types and operations of neural networks are explained in more detail below.
우선순위가 낮은 대규모 기계 학습 모델이 다수의 네트워크 계층을 포함하는 신경망인 경우, 시스템(100)(또는 도 1의 성능 추정 엔진(150))은 신경망의 각 네트워크 계층의 추론 연산을 수행하기 위한 개별 추정 기간을 획득할 수 있다. 시스템(100)은 각 계층에 대한 추정 기간에 기초하여 신경망의 하위-모델(또는 하위-네트워크)을 형성하기 위해 다수의 네트워크 계층으로부터 하나 이상의 네트워크 계층을 배열하고 그룹화할 수 있다. 시스템(100)은 하위-모델에 대한 추정 지속기간이 되도록 그 하위-모델에 그룹화된 모든 계층에 대한 추정 기간을 합산할 수 있다. 일부 구현에서, 시스템(100)은 신경망을 개별 추정 기간을 각각 갖는 다수의 하위-모델로 분할할 수 있다. 대안적으로, 시스템(100)은 신경망을 실질적으로 동일한 추정 기간을 갖는 다수의 하위-모델로 분할할 수 있다.If the low-priority large-scale machine learning model is a neural network that includes multiple network layers, system 100 (or performance estimation engine 150 in Figure 1) provides individual An estimated period can be obtained. System 100 may arrange and group one or more network layers from multiple network layers to form a sub-model (or sub-network) of the neural network based on the estimation period for each layer. System 100 may sum the estimate duration for all layers grouped in a sub-model to become the estimate duration for that sub-model. In some implementations, system 100 may partition the neural network into multiple sub-models, each with a separate estimation period. Alternatively, system 100 may partition the neural network into multiple sub-models with substantially the same estimation period.
신경망의 각 계층에 대한 개별 지속 시간(duration)을 추정하기 위해, 시스템(100)은 예를 들어 분석 모델을 적용하여 계층의 다중 노드 동작의 각 동작에 대한 개별 지속 시간을 결정하고, 개별 지속 시간을 집계하여 각각의 계층 지속 시간을 추정할 수 있다. 다른 예로서, 시스템(100)은 계층 지속 시간을 추정하기 위해 대규모 시뮬레이션 기반 데이터베이스로부터의 데이터베이스를 포함할 수 있다. 다른 예로서, 시스템(100)은 하나 이상의 기계 학습 모델을 적용하여 전체 신경망 모델까지 각 계층에 대한 데이터 대기 시간을 예측하고 그 예측된 데이터 대기 시간에 기초하여 개별 계층 지속 시간을 추정할 수 있다.To estimate the individual duration for each layer of the neural network, system 100 may, for example, apply an analytical model to determine the individual duration for each action of the multiple node actions in the layer, and determine the individual duration By aggregating, the duration of each layer can be estimated. As another example, system 100 may include a database from a large simulation-based database to estimate layer duration. As another example, system 100 may apply one or more machine learning models to predict data latency for each layer up to an overall neural network model and estimate individual layer durations based on the predicted data latency.
일반적으로, 시스템(100)은 시스템이 도 2에 설명된 바와 같이 시간 윈도우의 잔여 기간 내에서 또는 다수의 시간 윈도우에 걸쳐 하위-모델의 추론 연산을 수행할 수 있도록 신경망을 각각의 네트워크 계층 수를 갖는 다수의 하위-모델로 분할할 수 있다.Generally, system 100 configures the neural network to a number of layers in each network such that the system can perform sub-model inference operations within the remainder of a time window or across multiple time windows, as illustrated in FIG. 2. It can be divided into multiple sub-models.
신경망을 다수의 하위-모델로 분할하기 위해, 시스템(100)은 원본 신경망에 지정된 시퀀스에 따라 마지막 하위-모델을 제외한 각 하위-모델에 대해, 하위-모델의 마지막 계층을 하위-모델에 대한 출력 계층으로 결정할 수 있다. 하위-모델의 출력 계층의 출력은 신경망의 중간 출력이며, 이는 시퀀스에 따라 하위-모델을 뒤따르는 하위-모델의 중간 입력 역할을 할 수 있다.To split a neural network into multiple sub-models, system 100 outputs the last layer of the sub-model for each sub-model except the last sub-model according to the sequence specified in the original neural network. It can be decided by hierarchy. The output of the output layer of the sub-model is the intermediate output of the neural network, which can serve as the intermediate input of the sub-model that follows the sub-model according to the sequence.
우사하게, 시스템(100)은 원본 신경망에 지정된 시퀀스에 따라 제1 하위-모델을 제외한 각 하위-모델에 대해, 하위-모델의 제1 계층을 하위-모델의 입력 계층 또는 필(fill) 계층으로 결정할 수 있다. 하위-모델의 필 계층은 시퀀스에 따라 하위-모델보다 앞선 하위-모델의 출력 계층로부터의 중간 출력을 중간 입력으로서 수신할 수 있다.Similarly, system 100 converts, for each sub-model except the first sub-model, the first layer of the sub-model into the input layer or fill layer of the sub-model according to the sequence specified in the original neural network. You can decide. The fill layer of the sub-model may receive intermediate output from the output layer of the sub-model preceding the sub-model according to the sequence as intermediate input.
일 예로서 도 3a를 참조하면, 시스템(100)은 기계 학습 모델(예를 들어 신경망)을 다수의 하위-모델(예를 들어 302, 304, 306 및 308)로 분할하고 컴파일할 수 있다. 시스템(100)은 하위-모델(304)의 제1 계층을 이전 하위-모델(302)로부터 중간 출력 데이터(312)를 수신하도록 구성된 필 계층(334)으로 결정할 수 있다. 유사하게, 시스템(100)은 하위-모델(306)의 제1 계층을 이전 하위-모델(304)로부터 중간 출력 데이터(314)를 수신하도록 구성된 필 계층(336)으로 결정할 수 있고, 하위-모델(308)의 제1 계층을 이전 하위 모델(306)로부터 중간 출력 데이터(316)를 수신하도록 구성된 필 계층(338)으로 결정할 수 있다. Referring to FIG. 3A as an example, system 100 may partition and compile a machine learning model (e.g., neural network) into multiple sub-models (e.g., 302, 304, 306, and 308). System 100 may determine the first layer of sub-model 304 to be a fill layer 334 configured to receive intermediate output data 312 from a previous sub-model 302 . Similarly, system 100 may determine the first layer of sub-model 306 to be a fill layer 336 configured to receive intermediate output data 314 from previous sub-model 304, and The first layer of 308 may be determined to be a fill layer 338 configured to receive intermediate output data 316 from the previous submodel 306 .
더욱이, 시스템(100)은 하위-모델(302)의 마지막 계층을 중간 출력 데이터(312)를 생성하도록 구성된 출력 계층으로 결정할 수 있다. 시스템(100)은 신경망에 지정된 시퀀스에 따라 하위-모델(302)에 후속하는 하위-모델(304)의 필 계층(334)에 대한 중간 입력으로서 중간 출력 데이터(312)를 제공할 수 있다. 유사하게, 시스템(100)은 하위-모델(304)의 마지막 계층을 중간 출력 데이터(314)를 생성하도록 구성된 출력 계층으로 결정할 수 있고, 하위-모델(306)의 마지막 계층을 중간 출력 데이터(316)를 생성하도록 구성된 출력 계층으로 결정할 수 있다. 시스템(100)은 중간 출력 데이터(314)를 하위-모델(306)의 필 계층(336)에 대한 중간 입력으로서 제공하고, 중간 출력 데이터(316)를 하위-모델(308)의 필 계층(338)에 대한 중간 입력으로서 제공할 수 있다.Moreover, system 100 may determine the last layer of sub-model 302 to be an output layer configured to generate intermediate output data 312. System 100 may provide intermediate output data 312 as intermediate input to a fill layer 334 of sub-model 304 that follows sub-model 302 according to a specified sequence in the neural network. Similarly, system 100 may determine the last layer of sub-model 304 to be an output layer configured to generate intermediate output data 314 and the last layer of sub-model 306 to be the intermediate output data 316. ) can be determined by the output layer configured to generate. System 100 provides intermediate output data 314 as intermediate input to fill layer 336 of sub-model 306, and provides intermediate output data 316 to fill layer 338 of sub-model 308. ) can be provided as an intermediate input to
런타임 동안, 도 3a에 도시된 바와 같이, 시스템(100)이 입력 데이터의 프레임을 수신하고 각각의 수신된 입력 프레임을 처리할 때, 도 1의 런타임 제어기(175)와 동일한 런타임 제어기(345)는 중간 출력 데이터(312, 314, 316)의 데이터 흐름을 관리할 수 있다. 보다 구체적으로, 런타임 제어기(345)는 중간 출력 데이터를 후속 하위-모델에 제공할 것인지, 아니면 중간 출력 데이터를 메모리 유닛에 저장할지 여부를 결정할 수 있다. 예를 들어, 시스템(100)이 제1 시간 윈도우의 잔여 기간 동안 하위-모델(302, 304, 306)의 추론 연산을 수행하고, 제2 시간 윈도우의 잔여 기간 동안 하위-모델(308)의 추론 연산을 수행하도록 스케줄링한다고 가정한다. 따라서, 제1 시간 윈도우의 잔여 기간 동안, 런타임 제어기(345)는 중간 출력 데이터(312)를 결정하여 하위-모델(304)의 필 계층(334)에 직접 제공하고, 하위-모델(306)의 필 계층(336)에 직접 중간 출력 데이터(314)를 제공한다. 그러나 런타임 제어기(345)는 먼저 중간 출력 데이터(316)를 메모리(320)에 저장할 수 있다. 제2 시간 윈도우의 잔여 기간이 시작되거나 시작되기 전, 런타임 제어기(345)는 메모리(320)로부터 중간 출력 데이터(316)를 페치 또는 프리페치하고 이를 제2 시간 윈도우의 잔여 기간 내에 추론 연산을 수행하기 위해 필 계층(338)에 제공할 수 있다. 일반적으로, 메모리(320)는 배치된 하위-모델(325)의 추론 연산을 수행하도록 할당된 하드웨어 처리 유닛에 액세스할 수 있는 임의의 적합한 메모리를 포함할 수 있다. 예를 들어, 메모리(320)는 도 1의 하드웨어 처리 유닛(110)의 다중 메모리 유닛(170)일 수 있다. 다른 예로서, 메모리(320)는 하드웨어 처리 유닛(110)에 대해 액세스 가능한 도 1의 호스트(102)의 메모리(106)일 수 있다.During runtime, as shown in Figure 3A, when system 100 receives frames of input data and processes each received input frame, runtime controller 345, which is identical to runtime controller 175 in Figure 1, The data flow of intermediate output data 312, 314, and 316 can be managed. More specifically, runtime controller 345 may determine whether to provide intermediate output data to a subsequent sub-model or store intermediate output data in a memory unit. For example, system 100 performs inference operations of sub-models 302, 304, and 306 during the remainder of a first time window, and inference operations of sub-model 308 during the remainder of a second time window. Assume that you are scheduling an operation to be performed. Accordingly, during the remainder of the first time window, the runtime controller 345 determines intermediate output data 312 and provides it directly to the fill layer 334 of the sub-model 304 and the intermediate output data 312 of the sub-model 306. Provides intermediate output data 314 directly to the fill layer 336. However, runtime controller 345 may first store intermediate output data 316 in memory 320. At or before the remaining period of the second time window begins, runtime controller 345 fetches or prefetches intermediate output data 316 from memory 320 and performs a speculative operation on it within the remaining period of the second time window. To do this, it can be provided to the fill layer 338. In general, memory 320 may include any suitable memory accessible to hardware processing units assigned to perform inferential operations of the deployed sub-model 325. For example, memory 320 may be multiple memory units 170 of hardware processing unit 110 of FIG. 1 . As another example, memory 320 may be memory 106 of host 102 of FIG. 1 that is accessible to hardware processing unit 110.
또 다른 예로서 도 2와 관련하여, 런타임 제어기(345)는 시간 윈도우(205b)에서 하위-모델(280c)로부터의 중간 출력을 저장하기로 결정할 수 있고, 시간 윈도우(205c)의 잔여 기간(220c) 동안 중간 출력을 페치하여 하위-모델(280d)에 대한 입력으로 제공한다.As another example and with reference to FIG. 2 , runtime controller 345 may decide to store intermediate outputs from sub-model 280c in time window 205b and the remaining period 220c of time window 205c. ), the intermediate output is fetched and provided as input to the sub-model 280d.
도 3b는 기계 학습 모델로부터 분할된 다수의 하위-모델을 통해 다중-패스 추론을 수행하기 위한 예시적인 프로세스(350)를 도시한다. 편의상, 프로세스(350)는 하나 이상의 위치에 위치하는 하나 이상의 컴퓨터 시스템에 의해 수행되는 것으로 설명된다. 예를 들어, 적절하게 프로그래밍된 추론 시스템, 예를 들어 도 1의 시스템(100)은 프로세스(350)를 수행할 수 있다.FIG. 3B shows an
시스템은 도 2에 설명된 바와 같이 입력 데이터의 다중 프레임 스트림 또는 시간 간격으로 각각 입력 데이터의 다중 프레임을 수신하도록 구성된다. 예를 들어, 입력 데이터는 다중 프레임의 이미지를 갖는 비디오 스트림이거나 특정 시간 간격으로 카메라 시스템에 의해 촬영된 다중 프레임의 이미지 세트일 수 있다. 시스템(100)은 다수의 기계 학습 모델 또는 분할된 하위-모델을 다수의 하드웨어 처리 유닛(110)에 할당할 수 있으며, 모델의 다중-패스 추론 연산을 수행하여 시간 윈도우 내에서 하나 이상의 입력 프레임을 처리한다.The system is configured to receive multiple frames of input data, each at a time interval or multiple frame streams of input data, as illustrated in FIG. 2 . For example, the input data may be a video stream with multiple frames of images or a set of multiple frames of images taken by a camera system at specific time intervals. System 100 may assign multiple machine learning models or partitioned sub-models to multiple hardware processing units 110 and perform multi-pass inference operations on the models to select one or more input frames within a time window. Process it.
보다 구체적으로, 다중-패스 추론 연산을 수행하기 위해, 시스템(100)은 입력 프레임을 처리하기 위해 우선순위 기계 학습 모델과 비-우선순위 기계 학습 모델을 전환함으로써 추론 연산을 수행할 수 있다. 예를 들어, 도 2와 관련하여, 특정 빈도로 순서에 따라 수신된 제1 입력 프레임에 대해, 시스템(100)은 반복 시간 윈도우(205b)의 우선순위 기간(215b) 내에 제1 입력 프레임을 처리하기 위한 하나 이상의 우선순위 기계 학습 모델의 추론 연산을 수행할 수 있다. 그런 다음 시스템은 잔여 기간(220b)에서 제1 입력 프레임을 처리하기 위해 비-우선순위 기계 학습 모델(245b)로부터 분할된 하위-모델(280a-c)의 추론 연산을 수행할 수 있다.More specifically, to perform a multi-pass inference operation, system 100 may perform the inference operation by switching between a prioritized machine learning model and a non-priority machine learning model to process the input frame. For example, with reference to Figure 2, for a first input frame received in sequence at a certain frequency, system 100 processes the first input frame within priority period 215b of repetition time window 205b. Inference operations of one or more priority machine learning models may be performed to: The system may then perform inference operations on the partitioned sub-models 280a-c from the non-priority machine learning model 245b to process the first input frame in the remaining period 220b.
시스템(100)이 시간 윈도우(205c)의 시작 부분에서 제2 입력 프레임을 수신한다고 가정하면, 시스템(100)은 시간 윈도우(205c)의 우선순위 기간(215c) 내에 제2 입력 프레임을 처리하기 위한 하나 이상의 우선순위 기계 학습 모델의 추론 연산을 수행할 수 있다. 그 후, 시스템(100)은 잔여 기간(220c)에서 제1 입력 프레임을 처리하기 위해 비-우선순위 기계 학습 모델(245b)로부터 하위-모델(280d)의 추론 연산 수행을 재개할 수 있다. 그런 다음 시스템(100)은 하나 이상의 시간 윈도우에 걸쳐 제2 입력 프레임에 대한 하위-모델(280a-d)의 추론 연산을 수행하기 시작할 수 있다.Assuming that system 100 receives a second input frame at the beginning of time window 205c, system 100 is configured to process the second input frame within priority period 215c of time window 205c. May perform inference operations on one or more priority machine learning models. System 100 may then resume performing inference operations of sub-model 280d from non-priority machine learning model 245b to process the first input frame in remaining period 220c. System 100 may then begin performing inference operations of sub-models 280a-d on the second input frame over one or more time windows.
도 3b를 다시 참조하면, 런타임 제어기(175)는 시스템(100)이 우선순위 기계 학습 모델 또는 작업을 위해 입력 데이터의 새로운 프레임을 처리해야 하기 때문에 입력 프레임에 대한 하위-모델로부터 생성된 중간 출력을 저장할 시기와, 동일하거나 하나 이상의 다른 시간 윈도우에서 입력 프레임 처리를 재개할 시기를 결정할 수 있다.Referring back to FIG. 3B, runtime controller 175 outputs intermediate outputs generated from sub-models for input frames as system 100 must process new frames of input data for priority machine learning models or tasks. You can decide when to store and when to resume processing input frames in the same or one or more different time windows.
예를 들어, 도 3b에 도시된 바와 같이, 시스템(100)은 단일 시간 윈도우(375)에서 입력 데이터(365)의 3개 프레임을 수신할 수 있다. 도 3에 도시된 예는 설명의 편의를 위한 것임을 유의한다. 그러나, 시스템(100)은 또한 다른 시간 윈도우에 대한 다른 입력 프레임을 수신할 수 있으며, 이는 런타임 제어기(175)에 의해 수행되는 방법을 변경하지 않는다.For example, as shown in FIG. 3B, system 100 may receive three frames of input data 365 in a single time window 375. Note that the example shown in FIG. 3 is for convenience of explanation. However, system 100 may also receive other input frames for other time windows, without changing the method performed by runtime controller 175.
시간 윈도우(375)의 잔여 기간 동안, 시스템(100)은 제1 입력 데이터(360)(예를 들어, 스트리밍 데이터의 제1 프레임)를 수신하고, 제1 입력 데이터(360)를 처리하기 위한 하위-모델(302)의 추론 연산을 수행함으로써 중간 출력(370a)을 생성할 수 있다.During the remainder of time window 375, system 100 receives first input data 360 (e.g., a first frame of streaming data) and processes first input data 360. -An
다음으로, 시스템(100)은 제2 입력 데이터(363)를 수신하고, 제2 입력 데이터(363)를 처리하기 위한 하위-모델(302)의 추론 연산을 수행함으로써 중간 출력(373a)을 생성할 수 있다. 한편, 시스템(100)은 중간 출력(370a)을 하위-모델(304)에 제공하고, 제1 입력 데이터(360)를 처리하기 위한 하위-모델(304)의 추론 연산을 수행함으로써 중간 출력(370b)을 생성할 수 있다.Next, system 100 may receive second input data 363 and generate
그런 다음, 시스템(100)은 제3 입력 데이터(365)를 수신하고, 제3 입력 데이터(365)를 처리하기 위한 하위-모델(302)의 추론 연산을 수행함으로써 중간 출력(375a)을 생성할 수 있다. 한편, 시스템(100)은 중간 출력(373a)을 하위-모델(304)에 제공하고, 중간 출력(373b)을 하위-모델(306)에 제공할 수 있다. 시스템은 제2 입력 데이터(363)를 처리하기 위한 하위-모델(304)의 추론 연산을 수행함으로써 중간 출력(373b)을 생성할 수 있고, 제1 입력 데이터(360)를 처리하기 위한 하위 모델(306)의 추론 연산을 수행함으로써 중간 출력(370c)(또는 하위-모델(306)이 시퀀스에 따른 기계 학습 모델의 마지막 하위-모델인 경우 추론 출력(370c))을 생성한다.System 100 may then receive third input data 365 and generate intermediate output 375a by performing inferential operations on sub-model 302 to process third input data 365. You can. Meanwhile, system 100 may provide
런타임 제어기(175)는 위에 설명된 바와 같은 여러 이유로 인해 시간 윈도우(375) 이후의 하나 이상의 시간 윈도우에서 위에 언급된 계산 중 하나가 수행되어야 하는지 여부를 결정할 수 있다. 이에 응답하여, 런타임 제어기(175)는 대응하는 입력 프레임에 대한 하나 이상의 중간 출력을 메모리 유닛에 저장하고, 저장된 중간 출력을 페치하고, 일단 시스템(100)이 대응하는 입력 프레임의 처리를 재개하면 이를 대응하는 하위-모델에 제공할 수 있다.Runtime controller 175 may determine whether one of the above-mentioned calculations should be performed in one or more time windows after time window 375 for various reasons as described above. In response, runtime controller 175 stores one or more intermediate outputs for the corresponding input frame in a memory unit and fetches the stored intermediate outputs once system 100 resumes processing of the corresponding input frame. Can be provided to the corresponding sub-model.
예를 들어, 시스템은 시간 윈도우(375)에서 중간 출력(373b) 처리를 일시 중지할 수 있다. 런타임 제어기(175)는 하위-모델(306)에 할당된 하드웨어 처리 유닛에 액세스 가능한 메모리 유닛에 중간 출력(373b)을 저장할 수 있다. 런타임 제어기(175)는 하위-모델(306)의 추론 연산을 수행하기 위한 중간 출력(373b)을 페치하여 제1 입력 데이터(360)에 대한 추론 출력(370c)을 생성할 수 있다. 대안적으로, 런타임 제어기(175)는 중간 출력(373b)이 저장되는 메모리 주소를 저장할 수 있다. 하위-모델(306)에 할당된 하드웨어 처리 유닛은 하나 이상의 데이터 버스를 사용하여 메모리 주소로부터 저장된 출력(373b)을 페치하도록 지시받을 수 있다.For example, the system may pause processing
도 3b에는 도시되지 않았지만, 위에서 설명한 다중-패스 추론 방법은 계산 요구 사항 또는 제한 사항에 따라 우선순위 기간 내에 우선순위 기계 학습 모델의 추론 연산을 수행하기 위해 확장될 수 있음을 이해해야 한다.Although not shown in Figure 3B, it should be understood that the multi-pass inference method described above can be extended to perform inference operations of a priority machine learning model within a priority period depending on computational requirements or constraints.
도 4는 기계 학습 모델로부터 다수의 하위-모델을 생성하기 위한 예시적인 프로세스(400)를 도시한다. 편의상, 프로세스(400)는 하나 이상의 위치에 위치한 하나 이상의 컴퓨터 시스템에 의해 수행되는 것으로 설명된다. 예를 들어, 적절하게 프로그래밍된 추론 시스템, 예를 들어 도 1의 시스템(100)은 프로세스(400)를 수행할 수 있다.Figure 4 shows an example process 400 for generating multiple sub-models from a machine learning model. For convenience, process 400 is described as being performed by one or more computer systems located at one or more locations. For example, a suitably programmed reasoning system, such as system 100 of FIG. 1, can perform process 400.
시스템에는 호스트와 다수의 기계 학습 모델의 추론 연산을 수행하도록 구성된 하나 이상의 하드웨어 처리 유닛이 포함될 수 있다. 시스템은 특정 속도(예를 들어, 초당 프레임 수)로 다중 프레임의 입력 데이터를 순서대로 수신하도록 구성된다.The system may include a host and one or more hardware processing units configured to perform inference operations of multiple machine learning models. The system is configured to receive multiple frames of input data in sequence at a certain rate (e.g., frames per second).
시스템은 기계 학습 모델을 나타내는 데이터를 수신할 수 있다(402). 보다 구체적으로, 시스템은 호스트에서 제1 기계 학습 모델을 나타내는 데이터를 수신할 수 있다. 제1 기계 학습 모델은 입력을 처리하여 제1 추론 출력을 생성하는 추론 연산을 포함할 수 있다. 일부 구현에서, 기계 학습 모델은 계층 파라미터를 갖는 다중 네트워크 계층을 갖는 신경망을 포함할 수 있다.The system may receive data representing a machine learning model (402). More specifically, the system may receive data representing a first machine learning model from a host. The first machine learning model may include an inference operation that processes input to produce a first inference output. In some implementations, a machine learning model may include a neural network with multiple network layers with layer parameters.
시스템은 입력을 처리하여 제1 추론 출력을 생성하기 위한 제1 기계 학습 모델의 추론 연산을 수행하기 위한 추정 기간을 획득할 수 있다(404). 시스템은 시스템에 수신되고 저장된 다수의 기계 학습 모델 중 각각의 기계 학습 모델의 추론 연산을 수행하기 위한 개별 기간을 추가로 추정할 수 있다.The system may process the input to obtain an estimation period for performing an inference operation of a first machine learning model to generate a first inference output (404). The system may further estimate an individual period for performing an inference operation for each machine learning model among the plurality of machine learning models received and stored in the system.
시스템은 우선순위 기계 학습 모델의 우선순위 추론 연산을 수행하기 위해 예약된 우선순위 기간을 식별할 수 있다(406). 시스템은 반복 시간 윈도우의 각 발생에 대해 개별 우선순위 기간을 결정할 수 있다. 하나 이상의 하드웨어 처리 유닛은 반복 윈도우의 각 발생 내에서 다수의 기계 학습 모델의 추론 연산의 적어도 일부를 수행할 수 있다.The system may identify reserved priority periods for performing priority inference operations of the priority machine learning model (406). The system can determine a separate priority period for each occurrence of the repeating time window. One or more hardware processing units may perform at least a portion of the inference operations of the number of machine learning models within each occurrence of the iteration window.
시스템은 우선순위 추론 연산을 수행하기 위한 우선순위 기간을 예약한 후 남은 반복 시간 윈도우가 발생할 때마다 잔여 기간을 결정할 수 있다(408). 각각의 반복 시간 윈도우는 개별 잔여 기간을 포함하는 개별 기간을 포함할 수 있다. 각각의 잔여 기간은 시스템이 하나 이상의 비-우선순위 기계 학습 모델의 추론 연산을 수행하는 데 사용할 수 있는 시간 윈도우의 적어도 일부를 포함할 수 있다.The system may reserve a priority period for performing a priority inference operation and then determine the remaining period whenever a remaining repetition time window occurs (408). Each repeating time window may include a separate period, including a separate residual period. Each remaining period may include at least a portion of a time window that the system can use to perform inference operations of one or more non-priority machine learning models.
시스템은 추정 기간이 잔여 기간보다 큰지 여부를 결정할 수 있다(410).The system may determine whether the estimated period is greater than the remaining period (410).
추정 기간이 잔여 기간보다 크다느 결정에 응답하여, 시스템은 제1 기계 학습 모델을 하위-모델 그룹으로 분할할 수 있다(412). 하위-모델 그룹의 각 하위-모델은 제1 기계 학습 모델에 표현된 추론 연산의 개별 부분을 포함할 수 있다. 시스템(100)은 하나 이상의 반복 시간 윈도우에 걸쳐 잔여 기간 내에서 하위-모델 그룹 중 하나 이상에 대해 추론 연산을 수행할 수 있다.In response to determining that the estimation period is greater than the remaining period, the system may partition the first machine learning model into sub-model groups (412). Each sub-model of the sub-model group may include a separate portion of the inference operation expressed in the first machine learning model. System 100 may perform inference operations on one or more of the sub-model groups within a remaining period over one or more iteration time windows.
시스템은 시스템 내의 하나 이상의 처리 디바이스에 의해, 반복 시간 윈도우(414)가 발생하는 잔여 기간 동안 제1 하위-모델 그룹 내 하위-모델의 추론 연산을 수행할 수 있다.The system may perform, by one or more processing devices within the system, inference operations of sub-models within the first sub-model group for the remainder of the repetition time window 414 .
일부 구현에서, 시스템은 하위-모델 그룹 각각을 하나 이상의 하드웨어 처리 유닛의 각각의 하드웨어 처리 유닛에 할당하기 위한 명령을 호스트에서 생성할 수 있다. 시스템은 제1 추론 출력을 생성하기 위해 대응하는 하드웨어 처리 유닛에 각각 할당된 하위-모델 그룹의 추론 연산의 개별 부분을 수행하도록 하나 이상의 하드웨어 처리 유닛을 스케줄링할 수 있다.In some implementations, the system may generate instructions in the host to assign each sub-model group to a respective hardware processing unit of one or more hardware processing units. The system may schedule one or more hardware processing units to perform individual portions of the inference operations of the sub-model group each assigned to a corresponding hardware processing unit to generate the first inference output.
일부 구현에서, 시스템은 시간 윈도우의 잔여 기간에 다중 하위-모델 중 제1 하위-모델과 관련된 추론 연산을 수행하고, 다른 시간 윈도우의 다른 잔여 기간에 다중 하위-모델 중 제2 하위-모델과 관련된 추론 연산을 수행하도록 스케줄링할 수 있다. 제1 및 제2 하위-모델은 기계 학습 모델로부터 분할된 하위-모델의 시퀀스에 따라 정렬된다. 제2 하위-모델은 시퀀스에 따라 제1 하위-모델에 후속한다. 각각의 하위-모델은 기계 학습 모델에 지정된 추론 연산의 개별 부분을 포함할 수 있다.In some implementations, the system performs an inference operation related to a first sub-model of the multiple sub-models in a remaining period of a time window and a second sub-model of the multiple sub-models in a different remaining period of a different time window. Inference operations can be scheduled to be performed. The first and second sub-models are ordered according to the sequence of sub-models split from the machine learning model. The second sub-model follows the first sub-model according to the sequence. Each sub-model may contain a separate portion of the inference operation specified in the machine learning model.
시스템은 제1 기계 학습 모델에 배치된 하위 모델 그룹의 할당 및 시퀀스에 따라, 할당된 하드웨어 처리 유닛 상의 입력을 처리하기 위한 제1 하위-모델 그룹의 개별 추론 연산을 수행할 수 있다.The system may, according to the assignment and sequencing of the sub-model groups placed in the first machine learning model, perform individual inference operations on the first sub-model group for processing input on the assigned hardware processing unit.
일부 구현에서, 시스템은 입력 데이터 프레임의 순서나 시퀀스 또는 스트림으로 수신된 다수의 입력 프레임을 비롯하여 입력 데이터를 처리하도록 구성된다. 시스템은 특정 빈도로 순서에 따라 수신된 각 입력 프레임을 처리할 수 있다. 이를 고려하면 시간 윈도우는 각 입력 프레임을 수신하는 속도 또는 빈도에 기초하여 자동으로 결정될 수 있다.In some implementations, the system is configured to process input data, including a number of input frames received as a sequence or stream of input data frames. The system can process each input frame received in order at a certain frequency. Taking this into account, the time window can be automatically determined based on the rate or frequency of receiving each input frame.
또한 시스템에는 컴파일러가 포함될 수 있다. 컴파일러는 호스트에서 다수의 하위-모델을 컴파일하고 컴파일된 하위-모델 각각을 그 컴파일된 하위-모델에 할당된 하드웨어 처리 유닛에 배포하도록 구성된다.The system may also include a compiler. The compiler is configured to compile a plurality of sub-models in the host and distribute each compiled sub-model to a hardware processing unit assigned to the compiled sub-model.
또한, 시스템에 의해 다수의 하위-모델로 분할되도록 선택 및 결정된 비-우선순위 기계 학습 모델에는 신경망이 포함될 수 있다. 신경망은 신경망에 따라 시퀀스로 배열된 다수의 네트워크 계층을 포함할 수 있다. 시스템은 시스템이 입력 프레임을 처리하기 위해 네트워크 계층에 지정된 개별 계층 동작을 수행하는 데 필요한 네트워크 계층의 각 계층에 대한 각각의 추정된 계층 기간(duration)을 결정할 수 있다. 시스템은 모든 네트워크 계층에 대한 각각의 추정된 계층 기간을 집계하여 시스템이 신경망에 지정된 모든 추론 연산을 수행하는 데 필요한 추정 기간을 생성할 수 있다.Additionally, a neural network may be included in the non-priority machine learning model selected and determined to be split into multiple sub-models by the system. A neural network may include multiple network layers arranged in a sequence according to the neural network. The system may determine each estimated layer duration for each layer of the network layer required for the system to perform individual layer operations specified in the network layer to process the input frame. The system can aggregate each estimated layer period for all network layers to generate the estimate period needed for the system to perform all inference operations specified in the neural network.
시스템은 신경망을 다수의 하위-모델로 분할할 수 있으며, 다수의 하위-모델 각각은 시퀀스에 따라 배열된 각각의 개수의 네트워크 계층을 포함하므로 추론 연산을 수행하기 위한 각각의 추정 기간을 포함한다.The system may divide the neural network into multiple sub-models, each of which includes a respective number of network layers arranged according to a sequence and thus a respective estimation period for performing the inference operation.
시스템은 시퀀스에 따라 제1 하위-모델을 제외한 각 하위-모델에 대해 각각의 필 계층을 결정할 수 있다. 각각의 필 계층은 이전 하위-모델에서 생성된 중간 출력이 필 계층을 통한 입력으로 하위-모델에 제공되도록 관련 하위-모델에 대한 입력 계층으로 구성된다. 각 필 계층은 해당 하위-모델에 포함된 각 네트워크 계층 중 제1 계층이다.The system may determine each fill layer for each sub-model except the first sub-model according to the sequence. Each fill layer consists of an input layer for the relevant sub-model such that intermediate outputs generated from the previous sub-model are provided to the sub-model as input through the fill layer. Each fill layer is the first layer among each network layer included in the corresponding sub-model.
시스템은 하나 이상의 기계 학습 모델의 다중-패스 추론 연산을 수행하도록 구성된다. 전술한 바와 같이, 입력 데이터는 입력 프레임 시퀀스를 포함할 수 있다. 입력 시퀀스는 특정 빈도로 순서에 따라 호스트에서 수신된 제1 입력 및 제2 입력을 포함할 수 있다.The system is configured to perform multi-pass inference operations of one or more machine learning models. As described above, input data may include a sequence of input frames. The input sequence may include first input and second input received from the host in an order at a certain frequency.
입력 프레임을 처리하기 위한 추론 출력을 생성하기 위해, 시스템은 제1 입력을 처리하기 위한 분할된 비-우선순위 모델의 제1 하위-모델과 관련된 추론 연산을 수행하여 제1 중간 출력을 생성할 수 있다. 시스템은 제2 하위-모델의 필 계층을 통해, 기계 학습 모델에 지정된 시퀀스에 따라 제1 하위-모델에 후속하는 제2 하위-모델에 제1 중간 입력으로서 제1 중간 출력을 제공할 수 있다.To generate an inference output for processing an input frame, the system may perform an inference operation associated with a first sub-model of the partitioned non-priority model for processing the first input to generate a first intermediate output. there is. The system may provide the first intermediate output as a first intermediate input to a second sub-model that follows the first sub-model according to a sequence specified in the machine learning model, through the fill layer of the second sub-model.
그런 다음 시스템은 제2 입력을 처리하기 위한 제1 하위-모델과 관련된 추론 연산을 수행하여 제2 중간 출력을 생성할 수 있다. 동시에, 시스템은 제1 중간 입력을 처리하기 위한 제2 하위-모델과 관련된 추론 연산을 수행할 수 있다.The system may then perform inference operations associated with the first sub-model to process the second input and produce a second intermediate output. At the same time, the system may perform inference operations associated with the second sub-model for processing the first intermediate input.
시스템은 런타임 제어기를 더 포함할 수 있다. 런타임 제어기는 시스템이 다중-패스 추론 연산을 수행할 때 데이터 흐름을 제어하도록 구성될 수 있다. 보다 구체적으로, 런타임 제어기는 하나 이상의 시간 윈도우내에서 다수의 하위-모델의 추론 연산 수행을 스케줄링할 수 있다. 런타임 제어기는 입력 프레임을 처리하기 위한 다수의 하위-모델의 하나의 하위-모델에 의해 생성된 중간 출력을 시스템의 메모리 유닛에 저장할 수 있다. 런타임 제어기는 시스템의 메모리 유닛으로부터 시퀀스에 따라 하위-모델을 뒤따르는 다른 하위-모델에 대한 중간 입력으로서 중간 출력을 추가로 검색할 수 있다. 시스템의 메모리 유닛은 하위-모델에 할당된 하드웨어 처리 유닛에 액세스할 수 있다.The system may further include a runtime controller. The runtime controller may be configured to control data flow when the system performs multi-pass inference operations. More specifically, the runtime controller may schedule performance of inference operations of multiple sub-models within one or more time windows. The runtime controller may store intermediate outputs generated by one sub-model of multiple sub-models for processing the input frame in a memory unit of the system. The runtime controller may further retrieve intermediate outputs from memory units of the system as intermediate inputs to other sub-models that follow the sub-model in sequence. The memory units of the system may access hardware processing units assigned to the sub-model.
도 5는 다수의 하위-모델로 분할될 기계 학습 모델을 결정하기 위한 예시적인 프로세스(500)를 도시한다. 편의상, 프로세스(500)는 하나 이상의 위치에 위치한 하나 이상의 컴퓨터 시스템에 의해 수행되는 것으로 설명된다. 예를 들어, 적절하게 프로그래밍된 추론 시스템, 예를 들어 도 1의 시스템(100)은 프로세스(500)를 수행할 수 있다.Figure 5 shows an example process 500 for determining a machine learning model to be split into multiple sub-models. For convenience, process 500 is described as being performed by one or more computer systems located at one or more locations. For example, a suitably programmed reasoning system, such as system 100 of FIG. 1, can perform process 500.
비-우선순위 기계 학습 모델로 분류된 기계 학습 모델을 선택하기 위해, 시스템은 다수의 기계 학습 모델을 나타내는 데이터를 수신할 수 있다(502). 다수의 기계 학습 모델 각각은 개별 작업을 구현하도록 구성되며 개별 입력을 처리하기 위해 시스템에 의해 수행되는 개별 추론 연산을 포함한다. 작업에는 예를 들어 카메라 시스템에 의해 캡처된 이미지의 경우 배경 검출, 초점 검출, 객체 검출 또는 얼굴 인식 중 적어도 하나가 포함될 수 있다. 배경 검출 작업은 캡처된 이미지에 대한 깊이 이미지를 생성하는 것을 더 포함할 수 있다.To select a machine learning model classified as a non-priority machine learning model, the system may receive data representing a number of machine learning models (502). Each of a number of machine learning models is configured to implement a separate task and includes individual inference operations performed by the system to process individual inputs. The task may include at least one of background detection, focus detection, object detection or face recognition, for example in the case of images captured by a camera system. The background detection task may further include generating a depth image for the captured image.
시스템은 개별 작업의 특성에 기초하여 다수의 기계 학습 모델 각각에 대한 개별 우선순위 레벨을 측정할 수 있다(504). 개별 작업의 특징은 작업을 수행하기 위한 기계 학습 모델의 크기, 작업에 대한 기계 학습 모델의 출력이 다른 모델에서 입력으로 사용되는지 여부 또는 작업에 대한 기계 학습 모델이 대기 시간에 민감한지 여부가 포함될 수 있다.The system may measure individual priority levels for each of the multiple machine learning models based on the characteristics of the individual tasks (504). Characteristics of an individual task may include the size of the machine learning model to perform the task, whether the output of the machine learning model for the task is used as input by another model, or whether the machine learning model for the task is latency sensitive. there is.
시스템은 개별 우선순위 레벨에 기초하여 다수의 기계 학습 모델로부터 하나의 기계 학습 모델을 비-우선순위 기계 학습 모델로서 선택할 수 있다(506). 예를 들어, 시스템은 우선순위가 낮은 기계학습 모델을 상기 선택된 비-우선순위 기계학습 모델로서 선택할 수 있다.The system may select one machine learning model as a non-priority machine learning model from multiple machine learning models based on individual priority levels (506). For example, the system may select a low priority machine learning model as the selected non-priority machine learning model.
본 명세서에 설명된 주제와 액션 및 동작의 구현은 디지털 전자 회로, 유형으로 구현된 컴퓨터 소프트웨어 또는 펌웨어, 본 명세서에 개시된 구조 및 그의 구조적 등가물을 포함하는 컴퓨터 하드웨어, 또는 이들 중 하나 이상의 조합으로 구현될 수 있다. 본 명세서에 설명된 주제의 구현은 데이터 처리 유닛에 의한 실행 또는 데이터 처리 유닛의 동작을 제어하기 위해 컴퓨터 프로그램 매체에 인코딩된 하나 이상의 컴퓨터 프로그램, 예를 들어 컴퓨터 프로그램 명령의 하나 이상의 모듈로 구현될 수 있다. 매체는 유형의 비-일시적 컴퓨터 저장 매체일 수 있다. 대안적으로 또는 추가적으로, 매체는 인공적으로 생성된 전파 신호, 예를 들어 데이터 처리 유닛에 의한 실행을 위해 적절한 수신기 장치로 전송하기 위해 정보를 인코딩하기 위해 생성된 기계 생성 전기, 광학 또는 전자기 신호일 수 있다. 컴퓨터 저장 매체는 기계 판독 가능 저장 디바이스, 기계 판독 가능 저장 기판, 랜덤 또는 직렬 액세스 메모리 디바이스, 또는 이들 중 하나 이상의 조합이거나 그 일부일 수 있다. 컴퓨터 저장 매체는 전파된 신호가 아니다.Implementations of the subject matter and actions and operations described herein may be implemented in digital electronic circuitry, tangible computer software or firmware, computer hardware including the structures disclosed herein and structural equivalents thereof, or a combination of one or more of these. You can. Implementations of the subject matter described herein may be implemented as one or more computer programs, e.g., one or more modules of computer program instructions, encoded on a computer program medium for execution by or controlling the operation of a data processing unit. there is. The medium may be any tangible, non-transitory computer storage medium. Alternatively or additionally, the medium may be an artificially generated radio signal, e.g., a machine-generated electrical, optical or electromagnetic signal generated to encode information for transmission to a suitable receiver device for execution by a data processing unit. . A computer storage medium may be or be part of a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of these. Computer storage media are not propagated signals.
"신경망"이라는 용어는 모든 종류의 작업을 수행하도록 구성된 모든 종류의 신경망을 포함한다.The term "neural network" includes all types of neural networks configured to perform all kinds of tasks.
일부 경우, 신경망은 이미지 처리 작업을 수행하도록, 즉, 입력 이미지를 수신하고 입력 이미지를 처리하여 입력 이미지에 대한 네트워크 출력을 생성하도록 구성된다. 예를 들어, 작업은 이미지 분류일 수 있고 주어진 이미지에 대해 신경망에 의해 생성된 출력은 객체 카테고리 세트 각각에 대한 스코어일 수 있으며, 각 스코어는 이미지가 카테고리에 속하는 객체의 이미지를 포함할 추정 가능성을 나타낸다. 다른 예로, 작업은 이미지 임베딩 생성일 수 있으며 신경망에 의해 생성된 출력은 입력 이미지의 수치 임베딩일 수 있다. 또 다른 예로서, 작업은 객체 검출일 수 있으며 신경망에 의해 생성된 출력은 특정 유형의 객체가 묘사된 입력 이미지의 위치를 식별할 수 있다. 또 다른 예로서, 작업은 이미지 분할일 수 있으며 신경망에 의해 생성된 출력은 입력 이미지의 각 픽셀을 카테고리 세트의 카테고리에 할당할 수 있다.In some cases, a neural network is configured to perform image processing tasks, that is, to receive an input image and process the input image to produce a network output for the input image. For example, the task may be image classification and the output produced by a neural network for a given image may be a score for each of a set of object categories, with each score representing an estimated likelihood that the image contains an image of an object belonging to that category. indicates. As another example, the task may be generating image embeddings and the output produced by the neural network may be a numerical embedding of the input image. As another example, the task may be object detection and the output produced by the neural network may identify locations in the input image where certain types of objects are depicted. As another example, the task may be image segmentation and the output produced by the neural network may assign each pixel of the input image to a category in a category set.
또 다른 예로, 신경망에 대한 입력이 인터넷 리소스(예를 들어, 웹 페이지), 문서 또는 문서의 일부이거나 인터넷 리소스, 문서 또는 문서의 일부에서 추출된 기능인 경우, 작업은 리소스나 문서를 분류하는 것일 수 있다. 즉, 주어진 인터넷 리소스, 문서 또는 문서의 일부에 대해 신경망에 의해 생성된 출력은 각 토픽 세트에 대한 스코어일 수 있으며, 각 스코어는 인터넷 리소스, 문서 또는 문서 일부가 해당 토픽에 관한 추정 가능성을 나타낸다.As another example, if the input to the neural network is an Internet resource (e.g., a web page), document, or part of a document, or features extracted from an Internet resource, document, or part of a document, the task may be to classify the resource or document. there is. That is, for a given Internet resource, document, or part of a document, the output generated by the neural network may be a score for each set of topics, with each score representing the estimated likelihood that the Internet resource, document, or part of the document is about that topic.
또 다른 예로, 신경망에 대한 입력이 특정 광고에 대한 노출 컨텍스트의 특징인 경우, 신경망에 의해 생성된 출력은 특정 광고가 클릭될 추정 가능성을 나타내는 스코어일 수 있다.As another example, if the input to the neural network is a feature of the exposure context for a particular advertisement, the output produced by the neural network may be a score representing the estimated likelihood that the particular advertisement will be clicked.
또 다른 예로서, 신경망에 대한 입력이 사용자를 위한 개인화된 추천의 특징(예를 들어, 추천에 대한 컨텍스트를 특징짓는 특징, 예를 들어 사용자가 취한 이전 액션을 특징짓는 특징)인 경우, 신경망에 의해 생성된 출력은 컨텐츠 항목 세트 각각에 대한 스코어일 수 있으며, 각 스코어는 사용자가 컨텐츠 항목 추천에 호의적으로 반응할 추정 가능성을 나타낸다.As another example, if the input to the neural network is features of a personalized recommendation for the user (e.g., features characterizing the context for the recommendation, e.g., features characterizing previous actions taken by the user), then the neural network The output generated by may be a score for each set of content items, with each score representing an estimated likelihood that the user will respond favorably to the content item recommendation.
또 다른 예로, 신경망에 대한 입력이 한 언어로 된 일련의 텍스트인 경우, 신경망에 의해 생성된 출력은 다른 언어로 된 텍스트 조각 세트 각각에 대한 스코어일 수 있으며, 각 스코어는 다른 언어로 된 텍스트 조각이 입력 텍스트를 다른 언어로 적절하게 번역할 추정 가능성을 나타낸다.As another example, if the input to a neural network is a set of text in one language, the output produced by the neural network may be a score for each of a set of text fragments in a different language, where each score is a set of text fragments in a different language. Indicates the estimated likelihood of appropriately translating this input text into another language.
또 다른 예로, 작업은 오디오 처리 작업일 수 있다. 예를 들어, 신경망에 대한 입력이 음성 발언을 나타내는 시퀀스인 경우, 신경망에 의해 생성된 출력은 텍스트 조각 세트 각각에 대한 스코어일 수 있으며, 각 스코어는 텍스트 조각이 발언에 대한 올바른 전사일 추정 가능성을 나타낸다. 또 다른 예로, 작업은 신경망에 대한 입력이 음성 발언을 나타내는 시퀀스인 경우, 신경망에 의해 생성된 출력이 발언에서 특정 단어나 문구("핫워드")가 말해졌는지 여부를 나타낼 수 있는 키워드 찾기 작업일 수 있다. 또 다른 예로, 신경망에 대한 입력이 음성 발언을 나타내는 시퀀스인 경우, 신경망에 의해 생성된 출력은 그 발언이 발화된 자연어를 식별할 수 있다.As another example, the task may be an audio processing task. For example, if the input to a neural network is a sequence representing a spoken utterance, the output produced by the neural network might be a score for each of a set of text fragments, with each score representing an estimated probability that the text fragment is a correct transcription of the utterance. indicates. As another example, the task might be to find keywords where the input to a neural network is a sequence representing a spoken utterance, and the output produced by the neural network can indicate whether a particular word or phrase (a "hotword") was said in the utterance. You can. As another example, if the input to a neural network is a sequence representing a vocal utterance, the output produced by the neural network may identify the natural language in which the utterance was spoken.
또 다른 예로, 작업은 일부 자연어로 된 텍스트 시퀀스에 대해 작동하는 자연어 처리 또는 이해 작업, 예를 들어 수반(entailment) 작업, 의역 작업, 텍스트 유사성 작업, 감정 작업, 문장 완성 작업, 문법 작업 등일 수 있다.As another example, the task may be a natural language processing or understanding task that operates on text sequences in some natural language, such as an entailment task, a paraphrase task, a text similarity task, a sentiment task, a sentence completion task, a grammar task, etc. .
또 다른 예로, 작업은 텍스트 음성 변환 작업일 수 있으며, 여기서 입력은 자연어로 된 텍스트이거나 자연어로 된 텍스트의 특징이고, 네트워크 출력은 자연어로 발화하는 텍스트의 오디오를 정의하는 스펙트로그램 또는 기타 데이터이다.As another example, the task may be a text-to-speech task, where the input is text in a natural language or a feature of the text in a natural language, and the network output is a spectrogram or other data that defines the audio of the text spoken in the natural language.
또 다른 예로, 작업은 건강 예측 작업일 수 있는데, 여기서 입력은 환자에 대한 전자 건강 기록 데이터이고 출력은 환자의 미래 건강과 관련된 예측, 예를 들어, 환자에게 처방되어야 하는 예측 치료법, 건강에 해로운 사건이 환자에게 발생할 가능성 또는 환자에 대한 예측 진단일 수 있다.As another example, the task may be a health prediction task, where the input is electronic health record data for a patient and the output is a prediction related to the patient's future health, e.g., predicting which treatment should be prescribed for the patient, adverse health events, etc. It may be a possible occurrence for this patient or a predictive diagnosis for the patient.
또 다른 예로, 작업은 에이전트 제어 작업일 수 있으며, 여기서 입력은 환경 상태를 특징짓는 관찰이고 출력은 관찰에 응답하여 에이전트가 수행할 액션을 정의한다. 에이전트는 예를 들어 실제 로봇이나 시뮬레이션된 로봇, 산업 시설의 제어 시스템 또는 다양한 종류의 에이전트를 제어하는 제어 시스템일 수 있다.As another example, the task may be an agent-controlled task, where the input is an observation characterizing the state of the environment and the output defines an action to be performed by the agent in response to the observation. An agent may be, for example, a real or simulated robot, a control system in an industrial facility, or a control system that controls various types of agents.
신경망은 특정 작업에 대한 출력을 생성하기 위해 트레이닝생 파라미터에 따라 네트워크 입력을 처리하도록 구성된 파라미터 세트("네트워크 파라미터")를 가질 수 있다. 신경망은 신경망이 특정 작업에 필요한 유형의 네트워크 입력을 수신하고 특정 작업에 필요한 형태의 네트워크 출력을 생성할 수 있도록 하는 적절한 아키텍처를 가질 수 있다. 신경망의 예로는 완전-연결 신경망, 컨볼루션 신경망, 순환 신경망, 어텐션 기반 신경망(예를 들어, 트랜스포머) 등이 포함될 수 있다.A neural network may have a set of parameters (“network parameters”) configured to process network input according to trainee parameters to produce output for a particular task. A neural network may have an appropriate architecture that allows the neural network to receive network input of the type required for a specific task and produce network output of the type required for the specific task. Examples of neural networks may include fully-connected neural networks, convolutional neural networks, recurrent neural networks, attention-based neural networks (eg, transformers), etc.
"데이터 처리 디바이스"라는 용어는 예를 들어 프로그래밍 가능한 프로세서, 컴퓨터, 또는 다중 프로세서 또는 컴퓨터를 포함하여 데이터를 처리하기 위한 모든 종류의 장치, 디바이스 및 기계를 포함한다. 데이터 처리 유닛은 특수 목적 논리 회로, 예를 들어 FPGA(필드 프로그래밍 가능 게이트 어레이), ASIC(주문형 집적 회로) 또는 GPU(그래픽 처리 장치)를 포함할 수 있다. 장치는 또한 하드웨어에 추가하여 컴퓨터 프로그램에 대한 실행 환경을 생성하는 코드, 예를 들어 프로세서 펌웨어, 프로토콜 스택, 데이터베이스 관리 시스템, 운영 체제 또는 이들 중 하나 이상의 조합을 구성하는 코드를 포함할 수 있다.The term “data processing device” includes all types of devices, devices and machines for processing data, including, for example, programmable processors, computers, or multiple processors or computers. The data processing unit may include special purpose logic circuitry, such as a field programmable gate array (FPGA), an application specific integrated circuit (ASIC), or a graphics processing unit (GPU). A device may also include, in addition to hardware, code that creates an execution environment for a computer program, such as code comprising processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of these.
프로그램, 소프트웨어, 소프트웨어 애플리케이션, 앱, 모듈, 소프트웨어 모듈, 엔진, 스크립트 또는 코드라고도 지칭되거나 설명될 수 있는 컴퓨터 프로그램은 컴파일된 언어나 해석된 언어, 선언적 언어 또는 절차적 언어를 포함한 모든 형태의 프로그래밍 언어로 작성될 수 있으며, 독립 실행형 프로그램 또는 모듈, 구성 요소, 엔진, 서브루틴, 또는 하나 이상의 위치에서 데이터 통신 네트워크에 의해 상호 연결된 하나 이상의 컴퓨터를 포함할 수 있는 컴퓨팅 환경에서 실행하기에 적합한 기타 장치를 포함하여 모든 형태로 배포될 수 있다.A computer program, which may also be referred to or described as a program, software, software application, app, module, software module, engine, script, or code, is any form of programming language, including a compiled language, an interpreted language, a declarative language, or a procedural language. may be written as a stand-alone program or module, component, engine, subroutine, or other device suitable for execution in a computing environment that may include one or more computers interconnected by a data communications network at one or more locations It can be distributed in any form, including .
컴퓨터 프로그램은 파일 시스템의 파일에 해당할 수 있지만 반드시 그럴 필요는 없다. 컴퓨터 프로그램은 다른 프로그램이나 데이터를 담고 있는 파일의 일부(예를 들어, 마크업 언어 문서에 저장된 하나 이상의 스크립트), 해당 프로그램 전용 단일 파일, 또는 다수의 조정된 파일(예를 들어, 하나 이상의 모듈, 하위 프로그램 또는 코드 일부를 저장하는 파일)에 저장될 수 있다.Computer programs can, but do not have to, correspond to files in a file system. A computer program can be a portion of a file containing other programs or data (e.g., one or more scripts stored in a markup language document), a single file dedicated to that program, or a number of coordinated files (e.g., one or more modules, A file that stores a subprogram or part of code).
본 명세서에 설명된 프로세스 및 논리 흐름은 입력 데이터에 대해 작동하고 출력을 생성함으로써 동작들을 수행하는 하나 이상의 컴퓨터 프로그램을 실행하는 하나 이상의 컴퓨터에 의해 수행될 수 있다. 프로세스 및 논리 흐름은 FPGA, ASIC 또는 GPU와 같은 특수 목적 논리 회로에 의해 수행되거나 특수 목적 논리 회로와 하나 이상의 프로그래밍된 컴퓨터의 조합에 의해 수행될 수도 있다.The processes and logic flows described herein may be performed by one or more computers executing one or more computer programs that perform operations by operating on input data and producing output. Processes and logic flows may be performed by special-purpose logic circuits, such as FPGAs, ASICs, or GPUs, or by a combination of special-purpose logic circuits and one or more programmed computers.
컴퓨터 프로그램 실행에 적합한 컴퓨터는 일반 또는 특수 목적의 마이크로프로세서 또는 둘 다를 기반으로 하거나 다른 종류의 중앙 처리 장치를 기반으로 할 수 있다. 일반적으로, 중앙 처리 장치는 판독 전용 메모리나 랜덤 액세스 메모리 또는 둘 다로부터 명령과 데이터를 수신한다. 컴퓨터의 필수 요소는 명령을 실행하는 중앙 처리 장치와 명령 및 데이터를 저장하는 하나 이상의 메모리 디바이스이다. 중앙 처리 장치와 메모리는 특수 목적 논리 회로에 의해 보완되거나 통합될 수 있다.A computer suitable for running computer programs may be based on a general or special purpose microprocessor, or both, or on another type of central processing unit. Typically, the central processing unit receives instructions and data from read-only memory, random access memory, or both. The essential elements of a computer are a central processing unit that executes instructions and one or more memory devices that store instructions and data. The central processing unit and memory can be supplemented or integrated by special-purpose logic circuits.
일반적으로, 컴퓨터는 또한 하나 이상의 대용량 저장 디바이스로부터 데이터를 수신하거나 데이터를 전송하도록 포함하거나 작동 가능하게 연결된다. 대용량 저장 디바이스는 예를 들어 자기, 광자기, 광 디스크 또는 솔리드 스테이트 드라이브일 수 있다. 그러나 컴퓨터에 그러한 디바이스가 있을 필요는 없다. 더욱이, 컴퓨터는 다른 디바이스, 예를 들어 휴대폰, PDA(Personal Digital Assistant), 모바일 오디오 또는 비디오 플레이어, 게임 콘솔, GPS(Global Positioning System) 수신기 또는 휴대용 저장 디바이스(예를 들어, USB(범용 직렬 버스) 플래시 드라이브)에 내장될 수 있다.Typically, a computer also includes or is operably connected to receive data from or transmit data to one or more mass storage devices. Mass storage devices may be, for example, magnetic, magneto-optical, optical disks, or solid state drives. However, your computer does not need to have such a device. Moreover, the computer may be connected to other devices, such as a cell phone, personal digital assistant (PDA), mobile audio or video player, game console, Global Positioning System (GPS) receiver, or portable storage device (e.g., Universal Serial Bus (USB)). can be embedded in a flash drive).
사용자와의 상호 작용을 제공하기 위해, 본 명세서에 설명된 주제의 구현은 사용자에게 정보를 디스플레이하기 위한 LCD(액정 디스플레이) 모니터 및 사용자가 컴퓨터에 입력을 제공할 수 있는 입력 디바이스(예를 들어, 키보드) 및 포인팅 디바이스(예를 들어, 마우스, 트랙볼 또는 터치패드)를 갖춘 컴퓨터에서 구현되거나 이와 통신하도록 구성될 수 있다. 사용자와의 상호작용을 제공하기 위해 다른 종류의 디바이스도 사용될 수 있는데, 예를 들어, 사용자에게 제공되는 피드백은 시각적 피드백, 청각 피드백 또는 촉각 피드백과 같은 임의의 형태의 감각 피드백일 수 있고, 사용자로부터의 입력은 음향, 음성 또는 촉각 입력을 포함한 모든 형태로 수신될 수 있다. 또한, 컴퓨터는 사용자가 사용하는 디바이스와 문서를 주고받는 방식으로써 예를 들어 웹 브라우저에서 수신된 요청에 대한 응답으로 사용자 디바이스의 웹 브라우저에 웹 페이지를 보내거나 스마트폰이나 전자 태블릿과 같은 사용자 디바이스에서 실행되는 앱과 상호 작용함으로써 사용자와 상호 작용할 수 있다. 또한, 컴퓨터는 문자 메시지 또는 다른 형태의 메시지를 개인 디바이스(예를 들어, 메시징 애플리케이션을 실행하는 스마트폰)에 보내고 그 대가로 사용자로부터 응답 메시지를 수신함으로써 사용자와 상호 작용할 수 있다.To provide interaction with a user, implementations of the subject matter described herein may include a liquid crystal display (LCD) monitor for displaying information to a user and an input device through which a user may provide input to the computer (e.g., The computer may be implemented on or configured to communicate with a computer equipped with a keyboard) and a pointing device (e.g., a mouse, trackball, or touchpad). Other types of devices may also be used to provide interaction with the user, for example, the feedback provided to the user may be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback, and may also be used to provide interaction with the user. The input may be received in any form, including acoustic, voice, or tactile input. In addition, the computer is a method of exchanging documents with the device used by the user, for example, by sending a web page to the web browser of the user device in response to a request received from the web browser, or by sending a web page to the web browser of the user device such as a smartphone or electronic tablet. You can interact with the user by interacting with the running app. Additionally, the computer may interact with the user by sending text messages or other forms of messages to a personal device (e.g., a smartphone running a messaging application) and receiving response messages from the user in return.
본 명세서에서는 시스템, 장치 및 컴퓨터 프로그램 구성 요소와 관련하여 "~로 구성된"이라는 용어를 사용한다. 하나 이상의 컴퓨터로 구성된 시스템이 특정 동작이나 액션을 수행하도록 구성된다는 것은 동작시 시스템이 그 동작이나 액션을 수행하게 하는 소프트웨어, 펌웨어, 하드웨어 또는 이들의 조합이 시스템에 설치되어 있음을 의미한다. 하나 이상의 컴퓨터 프로그램이 특정 동작 또는 액션을 수행하도록 구성된다는 것은 하나 이상의 프로그램이 데이터 처리 유닛에 의해 실행될 때 해당 유닛이 동작 또는 액션을 수행하게 하는 명령을 포함한다는 것을 의미한다. 특수 목적 논리 회로가 특정 동작이나 액션을 수행하도록 구성된다는 것은 해당 회로에 동작이나 액션을 수행하는 전자 논리가 있음을 의미한다.This specification uses the term “consisting of” in reference to system, device and computer program components. When a system consisting of one or more computers is configured to perform a specific operation or action, it means that software, firmware, hardware, or a combination thereof that allows the system to perform the operation or action when operating is installed in the system. That one or more computer programs are configured to perform a particular operation or action means that the one or more programs, when executed by a data processing unit, include instructions that cause the unit to perform the operation or action. When a special-purpose logic circuit is configured to perform a specific operation or action, it means that the circuit has electronic logic that performs the operation or action.
본 명세서에 설명된 주제의 구현은 백엔드 구성요소(예를 들어, 데이터 서버)를 포함하거나, 미들웨어 구성요소(예를 들어, 애플리케이션 서버)를 포함하거나, 프런트엔드 구성요소(예를 들어, 그래픽 기능을 갖춘 클라이언트 컴퓨터)를 포함하는 컴퓨팅 시스템에서 구현될 수 있다. 사용자가 본 사양에 설명된 주제의 구현과 상호 작용할 수 있는 사용자 인터페이스, 웹 브라우저 또는 앱, 또는 이러한 백엔드, 미들웨어 또는 프런트엔드 구성 요소 중 하나 이상의 조합을 의미한다. 시스템의 구성 요소는 통신 네트워크와 같은 디지털 데이터 통신의 모든 형태나 매체를 통해 상호 연결될 수 있다. 통신 네트워크의 예로는 LAN(Local Area Network) 및 WAN(Wide Area Network)(예를 들어, 인터넷)이 포함된다.Implementations of the subject matter described herein may include a back-end component (e.g., a data server), a middleware component (e.g., an application server), or a front-end component (e.g., a graphics function). It can be implemented on a computing system that includes a client computer equipped with a. means a user interface, web browser, or app, or a combination of one or more of these backend, middleware, or frontend components through which a user can interact with an implementation of the subject matter described in this Specification. The components of a system may be interconnected through any form or medium of digital data communication, such as a telecommunications network. Examples of communications networks include local area networks (LANs) and wide area networks (WANs) (e.g., the Internet).
컴퓨팅 시스템에는 클라이언트와 서버가 포함될 수 있다. 클라이언트와 서버는 일반적으로 서로 멀리 떨어져 있으며 일반적으로 통신 네트워크를 통해 상호 작용한다. 클라이언트와 서버의 관계는 각 컴퓨터에서 실행되고 서로 클라이언트-서버 관계를 갖는 컴퓨터 프로그램으로 인해 발생한다. 일부 구현에서, 서버는 예를 들어 클라이언트 역할을 하는 디바이스와 상호작용하는 사용자에게 데이터를 디스플레이고 사용자 입력을 수신할 목적으로 데이터(예를 들어, HTML 페이지)를 사용자 디바이스에 전송한다. 사용자 디바이스에서 생성된 데이터(예를 들어, 사용자 상호 작용의 결과)는 디바이스로부터 서버에 수신될 수 있다.A computing system may include clients and servers. Clients and servers are usually remote from each other and typically interact through a communications network. The relationship between client and server arises due to computer programs running on each computer and having a client-server relationship with each other. In some implementations, a server transmits data (e.g., an HTML page) to a user device, for example, for the purpose of displaying data and receiving user input to a user interacting with the device acting as a client. Data generated on a user device (eg, results of user interactions) may be received from the device to a server.
위에서 설명한 실시예 외에도 다음과 같은 실시예도 혁신적이다.In addition to the embodiments described above, the following embodiments are also innovative.
실시예 1: 복수의 기계 학습 모델의 추론 연산을 수행하도록 구성된 하나 이상의 하드웨어 처리 유닛 및 호스트를 포함하는 시스템에 의해 수행되는 방법으로서, 이 방법은 호스트에서, 제1 기계 학습 모델을 나타내는 데이터를 수신하는 단계 - 제1 기계 학습 모델은 제1 추론 출력을 생성하기 위해 입력을 처리하기 위한 추론 연산을 포함함 - 와; 시스템이 입력을 처리하여 제1 추론 출력을 생성하기 위한 제1 기계 학습 모델의 추론 연산을 수행하는 제1 추정 기간을 획득하는 단계와; 하나 이상의 하드웨어 처리 유닛이 복수의 기계 학습 모델의 추론 연산 중 적어도 일부를 수행하는 반복 시간 윈도우의 각각의 발생 동안 우선순위 기계 학습 모델의 우선순위 추론 연산을 수행하기 위해 예약된 우선순위 기간을 식별하는 단계와; 우선순위 추론 연산을 수행하기 위해 우선순위 기간을 예약한 후 남은 반복 시간 윈도우의 각 발생에 대한 제1 잔여 기간을 결정하는 단계와; 제1 추정 기간이 제1 잔여 기간보다 큰지 여부를 판단하는 단계와; 제1 추정 기간이 제1 잔여 기간보다 크다는 결정에 응답하여, 제1 기계 학습 모델을 제1 잔여 기간 이하인 개별 추정 기간을 갖는 제1 하위-모델 그룹으로 분할하는 단계 - 제1 하위-모델 그룹의 각 하위-모델은 제1 기계 학습 모델의 추론 연산의 개별 부분을 포함함 - 와; 그리고 하나 이상의 하드웨어 처리 유닛에 의해, 반복 시간 윈도우가 발생한 제1 잔여 기간 동안 제1 하위-모델 그룹의 하위-모델에 대한 추론 연산을 수행하는 단계를 포함한다.Embodiment 1: A method performed by a system including a host and one or more hardware processing units configured to perform inference operations of a plurality of machine learning models, the method comprising: receiving, from the host, data representing a first machine learning model; wherein the first machine learning model includes an inference operation to process the input to produce a first inference output; obtaining a first inference period over which the system processes the input and performs an inference operation of a first machine learning model to produce a first inference output; Identifying a priority period reserved for performing priority inference operations of a priority machine learning model during each occurrence of a repetition time window in which one or more hardware processing units perform at least a portion of the inference operations of the plurality of machine learning models. Steps and; determining a first remaining period for each occurrence of the repetition time window remaining after reserving a priority period for performing a priority inference operation; determining whether the first estimate period is greater than the first remaining period; In response to determining that the first estimation period is greater than the first residual period, splitting the first machine learning model into a first sub-model group having individual estimation periods that are less than or equal to the first residual period, of the first sub-model group. Each sub-model includes a separate portion of the inference operations of the first machine learning model - and; and performing, by the one or more hardware processing units, an inference operation on the sub-models of the first sub-model group during the first remaining period during which the repetition time window occurs.
실시예 2: 실시예 1에 있어서, 제1 추론 출력을 생성하는 단계는 호스트에서 하나 이상의 하드웨어 처리 유닛의 개별 하드웨어 처리 유닛에 제1 하위-모델 그룹 각각을 할당하기 위한 명령을 생성하는 단계와, 그리고 제1 기계 학습 모델에 배치된 제1 하위-모델 그룹의 명령 및 시퀀스에 따라, 할당된 하드웨어 처리 유닛에 대한 입력을 처리하기 위해 제1 하위-모델 그룹의 개별 추론 연산을 수행하는 단계를 더 포함한다.Embodiment 2: The method of Embodiment 1, wherein generating the first inference output comprises generating instructions for assigning each of the first sub-model group to an individual hardware processing unit of one or more hardware processing units in the host; and performing individual inference operations of the first sub-model group to process input to the assigned hardware processing unit according to the instructions and sequences of the first sub-model group disposed in the first machine learning model. Includes.
실시예 3: 실시예 2에 있어서, 제1 하위-모델 그룹의 개별 추론 연산을 수행하는 단계는 호스트에 의해, 제1 추론 출력을 생성하기 위해 대응하는 하드웨어 처리 유닛에 각각 할당된 제1 하위-모델 그룹의 개별 추론 연산을 수행하도록 하나 이상의 하드웨어 처리 유닛을 스케줄링하는 단계를 더 포함한다.Embodiment 3: The method of Embodiment 2, wherein performing individual inference operations of the first sub-model group includes first sub-models each assigned by the host to a corresponding hardware processing unit to generate the first inference output. It further includes scheduling one or more hardware processing units to perform individual inference operations of the model group.
실시예 4: 실시예 1에 있어서, 호스트의 컴파일러에 의해, 제1 하위-모델 그룹을 컴파일하고 컴파일된 하위-모델 각각을 하나 이상의 하드웨어 처리 유닛에 배포하는 단계를 더 포함한다.Embodiment 4: The method of Embodiment 1, further comprising compiling, by a compiler of the host, the first group of sub-models and distributing each of the compiled sub-models to one or more hardware processing units.
실시예 5: 실시예 1 내지 4 중 하나에 있어서, 제1 기계 학습 모델을 나타내는 데이터를 수신하는 단계는 복수의 기계 학습 모델을 나타내는 데이터를 수신하는 단계 - 복수의 기계 학습 모델 각각은 개별 태스크를 구현하도록 구성되며 입력을 처리하기 위해 시스템에 의해 수행될 개별 추론 연산을 포함함 - 와; 개별 태스크의 특성에 기초하여 복수의 기계 학습 모델 각각에 대한 개별 우선순위 레벨을 측정하는 단계와; 그리고 제1 기계학습 모델로서, 개별 우선순위 레벨에 기초하여 복수의 기계학습 모델로부터 하나의 기계학습 모델을 선택하는 단계를 더 포함한다.Embodiment 5: The method of any of Embodiments 1-4, wherein receiving data representing a first machine learning model comprises receiving data representing a plurality of machine learning models, each of the plurality of machine learning models performing a separate task. Configured to implement and include individual inference operations to be performed by the system to process the input - and; measuring individual priority levels for each of a plurality of machine learning models based on characteristics of individual tasks; And, as a first machine learning model, it further includes selecting one machine learning model from the plurality of machine learning models based on individual priority levels.
실시예 6: 실시예 1 내지 5 중 하나에 있어서, 제2 기계 학습 모델을 나타내는 데이터를 수신하는 단계와; 시스템이 제2 추론 출력을 생성하기 위해 입력을 처리하기 위한 제2 기계 학습 모델의 추론 연산을 수행하기 위한 제2 추정 기간을 획득하는 단계와; (i) 우선순위 추론 연산을 수행하기 위한 우선순위 기간 및 (ii) 제1 기계 학습 모델에서 하위-모델의 추론 연산을 수행하기 위한 적어도 개별 추정 기간을 예약한 후 남은 반복 시간 윈도우의 각 발생에 대한 제2 잔여 기간을 결정하는 단계와; 제2 추정 기간이 제2 잔여 기간보다 큰지 여부를 결정하는 단계와; 제2 추정 기간이 제2 잔여 기간보다 크다는 결정에 응답하여, 제2 기계 학습 모델을 제2 잔여 기간 이하인 개별 추정 기간을 갖는 제2 하위-모델 그룹으로 분할하는 단계 - 제2 하위-모델 그룹의 각 하위-모델은 제2 기계 학습 모델의 추론 연산의 개별 부분을 포함함 - 와; 그리고 하나 이상의 처리 유닛에 의해, 반복 시간 윈도우가 발생한 제2 잔여 기간 동안 제2 하위-모델 그룹의 하위-모델에 대한 추론 연산을 수행하는 단계를 더 포함한다.Example 6: The method of any of Examples 1-5, comprising: receiving data representative of a second machine learning model; obtaining a second inference period for the system to perform an inference operation of a second machine learning model to process the input to generate a second inference output; At each occurrence of the remaining iteration time window after reserving (i) a priority period for performing priority inference operations and (ii) at least a separate inference period for performing inference operations of sub-models in the first machine learning model. determining a second remaining period for; determining whether the second estimation period is greater than the second remaining period; In response to determining that the second estimation period is greater than the second residual period, splitting the second machine learning model into a second sub-model group having individual estimation periods that are less than or equal to the second residual period, of the second sub-model group. Each sub-model includes a separate portion of the inference operations of the second machine learning model - and; and performing, by the one or more processing units, an inference operation on the sub-models of the second sub-model group during the second remaining period in which the repetition time window occurs.
실시예 7: 실시예 5 또는 6에 있어서, 입력은 센서에 의해 캡처된 복수의 이미지 프레임의 이미지 프레임을 포함하고; 반복 시간 윈도우의 각각의 발생은 복수의 이미지 프레임의 이미지 프레임에 대응하고; 개별 태스크는 배경 검출, 초점 검출, 객체 검출, 사람 얼굴 인식 중 적어도 하나를 포함하고; 그리고 개별 태스크의 특성은 적어도 시스템의 하나 이상의 처리 유닛에 의해 개별 태스크를 수행하기 위한 개별 태스크 및 개별 추정 기간의 종속성을 포함한다.Example 7: The method of example 5 or 6, wherein the input comprises an image frame of a plurality of image frames captured by a sensor; Each occurrence of the repeating time window corresponds to an image frame of the plurality of image frames; The individual tasks include at least one of background detection, focus detection, object detection, and human face recognition; And the characteristics of the individual tasks include dependencies of the individual tasks and individual estimation periods for performing the individual tasks by at least one or more processing units of the system.
실시예 8: 실시예 1 내지 7 중 하나에 있어서, 시스템은 입력 시퀀스를 처리하기 위한 하나 이상의 기계 학습 모델의 추론 연산을 수행하도록 구성되고, 상기 입력 시퀀스 각각은 특정 빈도로 순서에 따라 호스트에서 수신되고, 반복 시간 윈도우의 기간은 특정 빈도에 기초하여 결정된다.Embodiment 8: The method of any of Embodiments 1-7, wherein the system is configured to perform inference operations of one or more machine learning models to process input sequences, each of the input sequences being received from the host in an order at a certain frequency. and the duration of the repetition time window is determined based on a specific frequency.
실시예 9: 실시예 1 내지 7 중 하나에 있어서, 제1 기계 학습 모델은 시퀀스로 배열된 다수의 네트워크 계층을 포함하는 신경망을 포함하고, 제1 추정 기간을 획득하는 단계는 네트워크 계층의 각 계층에 대해, 시스템이 입력을 처리하기 위해 해당 계층에 지정된 개별 계층 동작을 수행하기 위한 각각의 추정된 계층 기간을 결정하는 단계와; 그리고 제1 추정 기간을 획득하기 위해 모든 네트워크 계층에 대해 각각의 추정된 계층 기간을 집계하는 단계를 포함한다.Embodiment 9: The method of any of Embodiments 1 to 7, wherein the first machine learning model includes a neural network including a plurality of network layers arranged in a sequence, and the step of obtaining the first estimation period comprises: each layer of the network layer For , determining each estimated layer duration for the system to perform the individual layer operation assigned to that layer to process the input; and aggregating each estimated layer period for all network layers to obtain a first estimate period.
실시예 10: 실시예 2 또는 3에 있어서, 복수의 반복 시간 윈도우 중 제1 반복 시간 윈도우의 제1 잔여 기간의 시퀀스에 따라 제1 하위-모델 그룹의 제1 하위-모델과 관련된 추론 연산을 수행하는 단계와, 그리고 복수의 반복 시간 윈도우 중 제2 반복 시간 윈도우의 제1 잔여 기간의 시퀀스에 따라 제1 하위-모델 그룹의 제2 하위-모델과 관련된 추론 연산을 수행하는 단계를 더 포함한다.Embodiment 10: The method of Embodiment 2 or 3, wherein performing an inference operation related to the first sub-model of the first sub-model group according to the sequence of the first remaining period of the first repetition time window of the plurality of repetition time windows and performing an inference operation related to the second sub-model of the first sub-model group according to the sequence of the first remaining period of the second repetition time window of the plurality of repetition time windows.
실시예 11: 실시예 1 내지 7 및 9 중 하나에 있어서, 신경망을 포함하는 제1 기계 학습 모델을 분할하는 단계는 시퀀스에 따라 배열된 개별 수의 네트워크 계층을 각각 포함하는 제1 하위-모델 그룹으로 신경망을 분할하는 단계와; 그리고 하위-모델 이전의 다른 하위-모델로부터 생성된 중간 출력이 개별 필(fill) 계층에 의한 입력으로서 하위-모델에 제공되도록 제1 하위-모델을 제외한 제1 하위-모델 그룹의 각 하위-모델에 대해 개별 필 계층을 결정하는 단계를 더 포함하고, 개별 필 계층 각각은 제1 하위-모델 그룹의 대응하는 하위-모델에 포함된 네트워크 계층의 제1 계층이다.Embodiment 11: The method of any of Embodiments 1 to 7 and 9, wherein partitioning the first machine learning model comprising a neural network comprises: a first group of sub-models each comprising a respective number of network layers arranged according to a sequence; dividing the neural network into; and each sub-model of the first sub-model group, excluding the first sub-model, such that intermediate outputs generated from other sub-models before the sub-model are provided to the sub-model as input by the respective fill layer. and determining individual fill layers for , wherein each individual fill layer is a first layer of a network layer included in a corresponding sub-model of the first sub-model group.
실시예 12: 실시예 1 내지 11 중 하나에 있어서, 입력은 순서에 따라 호스트에서 수신된 제1 입력 및 제2 입력을 포함하는 입력 시퀀스를 포함하고, 상기 추론 출력을 생성하는 단계는 제1 중간 출력을 생성하기 위해 제1 입력을 처리하는 시퀀스에 따라 제1 하위-모델과 관련된 추론 연산을 수행하는 단계와; 제2 하위-모델에 대한 필 계층을 통해, 시퀀스에 따라 제1 하위-모델에 후속하는 제2 하위-모델에 제1 중간 출력을 제1 중간 입력으로서 제공하는 단계와; 그리고 제2 중간 출력을 생성하기 위해 제2 입력을 처리하는 시퀀스에 따라 제1 하위-모델과 관련된 추론 연산을 수행하는 한편, 제1 중간 입력을 처리하기 위한 제2 하위-모델과 관련된 추론 연산을 수행하는 단계를 더 포함한다.Embodiment 12: The method of any of Embodiments 1-11, wherein the input comprises an input sequence comprising a first input and a second input received from a host in order, and generating the inference output comprises a first intermediate performing an inference operation associated with the first sub-model according to a sequence of processing the first input to produce an output; providing the first intermediate output as a first intermediate input to a second sub-model that follows the first sub-model in sequence, through a fill layer for the second sub-model; and perform inference operations associated with the first sub-model according to a sequence for processing the second input to produce a second intermediate output, while performing inference operations associated with the second sub-model for processing the first intermediate input. It further includes steps to be performed.
실시예 13: 실시예 2, 3 및 10 중 하나에 있어서, 제1 추론 출력을 생성하는 단계는 입력을 처리하기 위해 제1 하위-모델 그룹의 하위-모델에 의해 생성된 중간 출력을 시스템의 메모리 유닛에 저장하는 단계와; 그리고 시스템의 메모리 유닛으로부터, 중간 출력을 시퀀스에 따라 하위-모델을 계승하는 다른 하위-모델에 대한 중간 입력으로서 검색하는 단계를 더 포함한다.Embodiment 13: The method of any of Embodiments 2, 3, and 10, wherein generating the first inference output comprises storing intermediate outputs generated by sub-models of the first sub-model group into a memory of the system to process the input. storing in the unit; and retrieving, from the memory unit of the system, the intermediate output as an intermediate input to another sub-model succeeding the sub-model according to the sequence.
실시예 14: 하나 이상의 컴퓨터와, 하나 이상의 컴퓨터에 의해 실행될 때 하나 이상의 컴퓨터로 하여금 실시예 1 내지 13 중 어느 하나의 방법을 수행하게 하는 명령들을 저장한 하나 이상의 저장 디바이스를 포함하는 시스템이다.Embodiment 14: A system including one or more computers and one or more storage devices storing instructions that, when executed by the one or more computers, cause the one or more computers to perform the method of any one of embodiments 1 to 13.
실시예 15: 컴퓨터 프로그램으로 인코딩된 컴퓨터 저장 매체로서, 상기 프로그램은 데이터 처리 유닛에 의해 실행될 때, 데이터 처리 유닛으로 하여금 실시예 1 내지 13 중 어느 하나의 방법을 수행하게 하도록 동작 가능한 명령들을 포함한다.Embodiment 15: A computer storage medium encoded with a computer program, wherein the program, when executed by a data processing unit, includes instructions operable to cause the data processing unit to perform the method of any of embodiments 1 to 13. .
본 명세서에는 많은 구체적인 구현 세부 정보가 포함되어 있지만, 이는 청구되거나 청구될 수 있는 범위에 대한 제한으로 해석되어서는 안 되며, 오히려 특정 발명의 특정 구현에 특정할 수 있는 특징에 대한 설명으로 해석되어야 한다. 별도의 구현과 관련하여 본 사양에 설명된 특정 기능은 단일 구현에서 조합하여 구현될 수도 있다. 반대로, 단일 구현의 맥락에서 설명된 다양한 기능은 여러 구현에서 개별적으로 또는 임의의 적절한 하위 조합으로 구현될 수도 있다. 더욱이, 위에서는 특정 조합으로 기능이 작동한다고 설명할 수 있고 처음에는 그렇게 주장하기도 했습니다. 청구된 조합의 하나 이상의 특징은 경우에 따라 조합에서 삭제될 수 있으며, 청구범위는 하위 조합 또는 하위 조합의 변형에 관한 것일 수 있다.Although this specification contains numerous specific implementation details, this should not be construed as a limitation on the scope of what is or may be claimed, but rather as a description of features that may be specific to particular implementations of particular inventions. Certain functions described in this specification in relation to separate implementations may also be implemented in combination in a single implementation. Conversely, various functionality described in the context of a single implementation may also be implemented in multiple implementations individually or in any suitable subcombination. Moreover, the above could explain, and initially claimed, that the features work in certain combinations. One or more features of a claimed combination may be deleted from the combination, as the case may be, and the claims may relate to sub-combinations or variations of sub-combinations.
유사하게, 동작들은 특정 순서로 도면에 묘사되고 청구범위에 기재되어 있지만, 이는 원하는 결과를 달성하기 위해 이러한 동작들이 표시된 특정 순서 또는 순차적 순서로 수행되거나 모든 설명된 동작들이 수행되어야 한다고 요구하는 것으로 이해되어서는 안 된다. 특정 상황에서는 멀티태스킹과 병렬 처리가 유리할 수 있다. 더욱이, 위에서 설명한 구현에서 다양한 시스템 모듈 및 구성요소의 분리는 모든 구현에서 이러한 분리를 요구하는 것으로 이해되어서는 안 되며, 기술된 프로그램 구성요소 및 시스템은 일반적으로 단일 소프트웨어 제품에 함께 통합되거나 여러 소프트웨어 제품에 패키지될 수 있다는 점을 이해해야 한다.Similarly, although acts are depicted in the drawings and recited in the claims in a particular order, this is to be understood as requiring that these acts be performed in the particular order shown or sequential order or that all described acts be performed to achieve the desired result. It shouldn't be. In certain situations, multitasking and parallel processing can be advantageous. Moreover, the separation of various system modules and components in the implementations described above should not be understood as requiring such separation in all implementations, and the program components and systems described are typically integrated together in a single software product or in multiple software products. You must understand that it can be packaged in .
주제의 특정 구현이 설명되었다. 다른 구현은 다음 청구범위의 범위 내에 있다. 예를 들어, 청구범위에 인용된 동작들은 다른 순서로 수행될 수 있으며 여전히 원하는 결과를 얻을 수 있다. 일 예로서, 첨부된 도면에 묘사된 프로세스는 바람직한 결과를 달성하기 위해 표시된 특정 순서 또는 순차적 순서를 반드시 필요로 하는 것은 아니다. 일부 경우에는 멀티태스킹과 병렬 처리가 유리할 수 있다.Specific implementations of the subject matter have been described. Other implementations are within the scope of the following claims. For example, the operations recited in the claims can be performed in a different order and still obtain the desired result. By way of example, the process depicted in the accompanying drawings does not necessarily require the specific order or sequential order shown to achieve the desired results. In some cases, multitasking and parallel processing can be advantageous.
Claims (20)
호스트에서, 제1 기계 학습 모델을 나타내는 데이터를 수신하는 단계 - 제1 기계 학습 모델은 제1 추론 출력을 생성하기 위해 입력을 처리하기 위한 추론 연산을 포함함 - 와;
시스템이 입력을 처리하여 제1 추론 출력을 생성하기 위한 제1 기계 학습 모델의 추론 연산을 수행하는 제1 추정 기간을 획득하는 단계와;
하나 이상의 하드웨어 처리 유닛이 복수의 기계 학습 모델의 추론 연산 중 적어도 일부를 수행하는 반복 시간 윈도우의 각각의 발생 동안 우선순위 기계 학습 모델의 우선순위 추론 연산을 수행하기 위해 예약된 우선순위 기간을 식별하는 단계와;
우선순위 추론 연산을 수행하기 위해 우선순위 기간을 예약한 후 남은 반복 시간 윈도우의 각 발생에 대한 제1 잔여 기간을 결정하는 단계와;
제1 추정 기간이 제1 잔여 기간보다 큰지 여부를 판단하는 단계와;
제1 추정 기간이 제1 잔여 기간보다 크다는 결정에 응답하여, 제1 기계 학습 모델을 제1 잔여 기간 이하인 개별 추정 기간을 갖는 제1 하위-모델 그룹으로 분할하는 단계 - 제1 하위-모델 그룹의 각 하위-모델은 제1 기계 학습 모델의 추론 연산의 개별 부분을 포함함 - 와; 그리고
하나 이상의 하드웨어 처리 유닛에 의해, 반복 시간 윈도우가 발생한 제1 잔여 기간 동안 제1 하위-모델 그룹의 하위-모델에 대한 추론 연산을 수행하는 단계를 포함하는 것을 특징으로 하는 시스템에 의한 수행 방법.A method performed by a system including a host and one or more hardware processing units configured to perform inferential operations of a plurality of machine learning models, the method comprising:
At a host, receiving data representing a first machine learning model, the first machine learning model comprising an inference operation to process the input to produce a first inference output;
obtaining a first inference period over which the system processes the input and performs an inference operation of a first machine learning model to produce a first inference output;
Identifying a priority period reserved for performing priority inference operations of a priority machine learning model during each occurrence of a repetition time window in which one or more hardware processing units perform at least a portion of the inference operations of the plurality of machine learning models. Steps and;
determining a first remaining period for each occurrence of the repetition time window remaining after reserving a priority period for performing a priority inference operation;
determining whether the first estimate period is greater than the first remaining period;
In response to determining that the first estimation period is greater than the first residual period, splitting the first machine learning model into a first sub-model group having individual estimation periods that are less than or equal to the first residual period, of the first sub-model group. Each sub-model includes a separate portion of the inference operations of the first machine learning model - and; and
and performing, by one or more hardware processing units, an inference operation on a sub-model of a first sub-model group during a first remaining period during which a repetition time window has occurred.
상기 제1 추론 출력을 생성하는 단계는,
호스트에서 하나 이상의 하드웨어 처리 유닛의 개별 하드웨어 처리 유닛에 제1 하위-모델 그룹 각각을 할당하기 위한 명령을 생성하는 단계와, 그리고
제1 기계 학습 모델에 배치된 제1 하위-모델 그룹의 명령 및 시퀀스에 따라, 할당된 하드웨어 처리 유닛에 대한 입력을 처리하기 위해 제1 하위-모델 그룹의 개별 추론 연산을 수행하는 단계를 더 포함하는 것을 특징으로 하는 시스템에 의한 수행 방법.According to paragraph 1,
The step of generating the first inference output is,
generating instructions for assigning each of the first sub-model group to an individual hardware processing unit of the one or more hardware processing units in the host; and
According to the instructions and sequences of the first group of sub-models disposed in the first machine learning model, performing individual inference operations of the first group of sub-models to process the input to the assigned hardware processing unit. A method of performance by a system characterized in that:
상기 제1 하위-모델 그룹의 개별 추론 연산을 수행하는 단계는,
호스트에 의해, 제1 추론 출력을 생성하기 위해 대응하는 하드웨어 처리 유닛에 각각 할당된 제1 하위-모델 그룹의 개별 추론 연산을 수행하도록 하나 이상의 하드웨어 처리 유닛을 스케줄링하는 단계를 더 포함하는 것을 특징으로 하는 시스템에 의한 수행 방법.According to paragraph 2,
The step of performing an individual inference operation of the first sub-model group is:
Scheduling, by the host, one or more hardware processing units to perform individual inference operations of the first sub-model group each assigned to the corresponding hardware processing unit to generate the first inference output. A method of performance by a system that does.
호스트의 컴파일러에 의해, 제1 하위-모델 그룹을 컴파일하고 컴파일된 하위-모델 각각을 하나 이상의 하드웨어 처리 유닛에 배포하는 단계를 더 포함하는 것을 특징으로 하는 시스템에 의한 수행 방법.According to paragraph 1,
Compiling, by a compiler of the host, the first group of sub-models and distributing each of the compiled sub-models to one or more hardware processing units.
상기 제1 기계 학습 모델을 나타내는 데이터를 수신하는 단계는,
복수의 기계 학습 모델을 나타내는 데이터를 수신하는 단계 - 복수의 기계 학습 모델 각각은 개별 태스크를 구현하도록 구성되며 입력을 처리하기 위해 시스템에 의해 수행될 개별 추론 연산을 포함함 - 와;
개별 태스크의 특성에 기초하여 복수의 기계 학습 모델 각각에 대한 개별 우선순위 레벨을 측정하는 단계와; 그리고
제1 기계학습 모델로서, 개별 우선순위 레벨에 기초하여 복수의 기계학습 모델로부터 하나의 기계학습 모델을 선택하는 단계를 더 포함하는 것을 특징으로 하는 시스템에 의한 수행 방법.According to paragraph 1,
Receiving data representing the first machine learning model includes:
Receiving data representing a plurality of machine learning models, each of the plurality of machine learning models configured to implement a separate task and including a separate inference operation to be performed by the system to process the input;
measuring individual priority levels for each of a plurality of machine learning models based on characteristics of individual tasks; and
A method of performing by a system, further comprising selecting, as a first machine learning model, one machine learning model from a plurality of machine learning models based on an individual priority level.
제2 기계 학습 모델을 나타내는 데이터를 수신하는 단계와;
시스템이 제2 추론 출력을 생성하기 위해 입력을 처리하기 위한 제2 기계 학습 모델의 추론 연산을 수행하기 위한 제2 추정 기간을 획득하는 단계와;
(i) 우선순위 추론 연산을 수행하기 위한 우선순위 기간 및 (ii) 제1 기계 학습 모델에서 하위-모델의 추론 연산을 수행하기 위한 적어도 개별 추정 기간을 예약한 후 남은 반복 시간 윈도우의 각 발생에 대한 제2 잔여 기간을 결정하는 단계와;
제2 추정 기간이 제2 잔여 기간보다 큰지 여부를 결정하는 단계와;
제2 추정 기간이 제2 잔여 기간보다 크다는 결정에 응답하여, 제2 기계 학습 모델을 제2 잔여 기간 이하인 개별 추정 기간을 갖는 제2 하위-모델 그룹으로 분할하는 단계 - 제2 하위-모델 그룹의 각 하위-모델은 제2 기계 학습 모델의 추론 연산의 개별 부분을 포함함 - 와; 그리고
하나 이상의 처리 유닛에 의해, 반복 시간 윈도우가 발생한 제2 잔여 기간 동안 제2 하위-모델 그룹의 하위-모델에 대한 추론 연산을 수행하는 단계를 더 포함하는 것을 특징으로 하는 시스템에 의한 수행 방법.According to paragraph 1,
receiving data representing a second machine learning model;
obtaining a second inference period for the system to perform an inference operation of a second machine learning model to process the input to generate a second inference output;
At each occurrence of the remaining iteration time window after reserving (i) a priority period for performing priority inference operations and (ii) at least a separate inference period for performing inference operations of sub-models in the first machine learning model. determining a second remaining period for;
determining whether the second estimate period is greater than the second remaining period;
In response to determining that the second estimation period is greater than the second residual period, splitting the second machine learning model into a second sub-model group having individual estimation periods that are less than or equal to the second residual period, of the second sub-model group. Each sub-model contains a separate portion of the inference operations of the second machine learning model - and; and
and performing, by the one or more processing units, an inference operation on a sub-model of the second sub-model group during a second remaining period in which the repetition time window occurs.
상기 입력은 센서에 의해 캡처된 복수의 이미지 프레임의 이미지 프레임을 포함하고;
상기 반복 시간 윈도우의 각각의 발생은 복수의 이미지 프레임의 이미지 프레임에 대응하고;
상기 개별 태스크는 배경 검출, 초점 검출, 객체 검출, 사람 얼굴 인식 중 적어도 하나를 포함하고; 그리고
상기 개별 태스크의 특성은 적어도 시스템의 하나 이상의 처리 유닛에 의해 개별 태스크를 수행하기 위한 개별 태스크 및 개별 추정 기간의 종속성을 포함하는 것을 특징으로 하는 시스템에 의한 수행 방법.According to clause 5,
the input includes an image frame of a plurality of image frames captured by a sensor;
Each occurrence of the repeating time window corresponds to an image frame of a plurality of image frames;
The individual tasks include at least one of background detection, focus detection, object detection, and human face recognition; and
The method of claim 1 , wherein the characteristics of the individual tasks include dependencies of the individual tasks and individual estimation periods for performing the individual tasks by at least one processing unit of the system.
상기 시스템은 입력 시퀀스를 처리하기 위한 하나 이상의 기계 학습 모델의 추론 연산을 수행하도록 구성되고, 상기 입력 시퀀스 각각은 특정 빈도로 순서에 따라 호스트에서 수신되고, 그리고 상기 반복 시간 윈도우의 기간은 특정 빈도에 기초하여 결정되는 것을 특징으로 하는 시스템에 의한 수행 방법.According to paragraph 1,
The system is configured to perform an inference operation of one or more machine learning models to process input sequences, each of the input sequences being received from a host in order at a certain frequency, and the duration of the repetition time window being at a certain frequency. A method of performance by a system, characterized in that it is determined based on the method.
상기 제1 기계 학습 모델은 시퀀스로 배열된 다수의 네트워크 계층을 포함하는 신경망을 포함하고,
상기 제1 추정 기간을 획득하는 단계는,
네트워크 계층의 각 계층에 대해, 시스템이 입력을 처리하기 위해 해당 계층에 지정된 개별 계층 동작을 수행하기 위한 각각의 추정된 계층 기간을 결정하는 단계와; 그리고
제1 추정 기간을 획득하기 위해 모든 네트워크 계층에 대해 각각의 추정된 계층 기간을 집계하는 단계를 포함하는 것을 특징으로 하는 시스템에 의한 수행 방법.According to paragraph 1,
The first machine learning model includes a neural network including a plurality of network layers arranged in a sequence,
The step of obtaining the first estimation period is,
For each layer of the network layer, determining each estimated layer period for the system to perform the individual layer operation assigned to that layer to process the input; and
Aggregating each estimated layer period for all network layers to obtain a first estimate period.
복수의 반복 시간 윈도우 중 제1 반복 시간 윈도우의 제1 잔여 기간의 시퀀스에 따라 제1 하위-모델 그룹의 제1 하위-모델과 관련된 추론 연산을 수행하는 단계와, 그리고
복수의 반복 시간 윈도우 중 제2 반복 시간 윈도우의 제1 잔여 기간의 시퀀스에 따라 제1 하위-모델 그룹의 제2 하위-모델과 관련된 추론 연산을 수행하는 단계를 더 포함하는 것을 특징으로 하는 시스템에 의한 수행 방법.According to paragraph 2,
performing an inference operation related to a first sub-model of the first sub-model group according to a sequence of a first residual period of a first repetition time window of the plurality of repetition time windows, and
The system further comprising performing an inference operation related to a second sub-model of the first sub-model group according to the sequence of the first remaining period of the second repetition time window of the plurality of repetition time windows. How to do it by.
상기 신경망을 포함하는 제1 기계 학습 모델을 분할하는 단계는,
시퀀스에 따라 배열된 개별 수의 네트워크 계층을 각각 포함하는 제1 하위-모델 그룹으로 신경망을 분할하는 단계와; 그리고
하위-모델 이전의 다른 하위-모델로부터 생성된 중간 출력이 개별 필(fill) 계층에 의한 입력으로서 하위-모델에 제공되도록 제1 하위-모델을 제외한 제1 하위-모델 그룹의 각 하위-모델에 대해 개별 필 계층을 결정하는 단계를 더 포함하고,
상기 개별 필 계층 각각은 제1 하위-모델 그룹의 대응하는 하위-모델에 포함된 네트워크 계층의 제1 계층인 것을 특징으로 하는 시스템에 의한 수행 방법.According to clause 9,
The step of dividing the first machine learning model including the neural network includes:
partitioning the neural network into first sub-model groups each comprising a distinct number of network layers arranged according to a sequence; and
To each sub-model of the first sub-model group, except the first sub-model, such that intermediate outputs generated from other sub-models before the sub-model are provided to the sub-model as input by the respective fill layer. Further comprising the step of determining an individual fill layer for,
Wherein each of the individual fill layers is a first layer of a network layer included in a corresponding sub-model of the first sub-model group.
상기 입력은 순서에 따라 호스트에서 수신된 제1 입력 및 제2 입력을 포함하는 입력 시퀀스를 포함하고,
상기 추론 출력을 생성하는 단계는,
제1 중간 출력을 생성하기 위해 제1 입력을 처리하는 시퀀스에 따라 제1 하위-모델과 관련된 추론 연산을 수행하는 단계와;
제2 하위-모델에 대한 필 계층을 통해, 시퀀스에 따라 제1 하위-모델에 후속하는 제2 하위-모델에 제1 중간 출력을 제1 중간 입력으로서 제공하는 단계와; 그리고
제2 중간 출력을 생성하기 위해 제2 입력을 처리하는 시퀀스에 따라 제1 하위-모델과 관련된 추론 연산을 수행하는 한편, 제1 중간 입력을 처리하기 위한 제2 하위-모델과 관련된 추론 연산을 수행하는 단계를 더 포함하는 것을 특징으로 하는 시스템에 의한 수행 방법.According to paragraph 1,
the input comprises an input sequence comprising a first input and a second input received from the host in order;
The step of generating the inference output is,
performing an inference operation associated with the first sub-model according to a sequence of processing the first input to produce a first intermediate output;
providing the first intermediate output as a first intermediate input to a second sub-model that follows the first sub-model in sequence, through a fill layer for the second sub-model; and
Performing inference operations associated with a first sub-model according to a sequence for processing a second input to produce a second intermediate output, while performing inference operations associated with a second sub-model for processing the first intermediate input. A method of performing a system, characterized in that it further comprises the step of:
상기 제1 추론 출력을 생성하는 단계는,
입력을 처리하기 위해 제1 하위-모델 그룹의 하위-모델에 의해 생성된 중간 출력을 시스템의 메모리 유닛에 저장하는 단계와; 그리고
시스템의 메모리 유닛으로부터, 중간 출력을 시퀀스에 따라 하위-모델을 계승하는 다른 하위-모델에 대한 중간 입력으로서 검색하는 단계를 더 포함하는 것을 특징으로 하는 시스템에 의한 수행 방법.According to paragraph 2,
The step of generating the first inference output is,
storing intermediate outputs generated by sub-models of the first sub-model group in a memory unit of the system for processing the input; and
A method of performing by a system, characterized in that it further comprises the step of retrieving, from a memory unit of the system, an intermediate output as an intermediate input to another sub-model that inherits the sub-model according to the sequence.
호스트에서, 제1 기계 학습 모델을 나타내는 데이터를 수신하는 동작 - 제1 기계 학습 모델은 제1 추론 출력을 생성하기 위해 입력을 처리하기 위한 추론 연산을 포함함 - 과;
시스템이 입력을 처리하여 제1 추론 출력을 생성하기 위한 제1 기계 학습 모델의 추론 연산을 수행하는 제1 추정 기간을 획득하는 동작과;
하나 이상의 하드웨어 처리 유닛이 복수의 기계 학습 모델의 추론 연산 중 적어도 일부를 수행하는 반복 시간 윈도우의 각각의 발생 동안 우선순위 기계 학습 모델의 우선순위 추론 연산을 수행하기 위해 예약된 우선순위 기간을 식별하는 동작과;
우선순위 추론 연산을 수행하기 위해 우선순위 기간을 예약한 후 남은 반복 시간 윈도우의 각 발생에 대한 제1 잔여 기간을 결정하는 동작과;
제1 추정 기간이 제1 잔여 기간보다 큰지 여부를 판단하는 동작과;
제1 추정 기간이 제1 잔여 기간보다 크다는 결정에 응답하여, 제1 기계 학습 모델을 제1 잔여 기간 이하인 각각의 추정 기간을 갖는 제1 하위-모델 그룹으로 분할하는 동작 - 제1 하위-모델 그룹의 각 하위-모델은 제1 기계 학습 모델의 추론 연산의 개별 부분을 포함함 - 과; 그리고
하나 이상의 하드웨어 처리 유닛에 의해, 반복 시간 윈도우가 발생한 제1 잔여 기간 동안 제1 하위-모델 그룹의 하위-모델에 대한 추론 연산을 수행하는 동작을 포함하는 것을 특징으로 하는 시스템.A system comprising one or more computers and one or more storage devices storing instructions that, when executed by the one or more computers, cause the one or more computers to perform respective operations, the system comprising a host and an inference operation of a plurality of machine learning models. Further comprising one or more hardware processing units configured to perform the operations:
At the host, receiving data representing a first machine learning model, the first machine learning model comprising an inference operation to process the input to produce a first inference output;
Obtaining a first inference period over which the system processes the input and performs an inference operation of a first machine learning model to produce a first inference output;
Identifying a priority period reserved for performing priority inference operations of a priority machine learning model during each occurrence of a repetition time window in which one or more hardware processing units perform at least a portion of the inference operations of the plurality of machine learning models. motion and;
determining a first remaining period for each occurrence of the repetition time window remaining after reserving a priority period for performing a priority inference operation;
determining whether the first estimation period is greater than the first remaining period;
In response to determining that the first estimation period is greater than the first residual period, splitting the first machine learning model into first sub-model groups having respective estimation periods less than or equal to the first residual period - a first sub-model group. Each sub-model of includes a separate portion of the inference operation of the first machine learning model - and; and
and performing, by one or more hardware processing units, an inference operation on sub-models of the first sub-model group during a first remaining period over which a repetition time window has occurred.
상기 제1 기계 학습 모델을 나타내는 데이터를 수신하는 동작은,
복수의 기계 학습 모델을 나타내는 데이터를 수신하는 동작 - 복수의 기계 학습 모델 각각은 개별 태스크를 구현하도록 구성되고 입력을 처리하기 위해 시스템에 의해 수행될 개별 추론 연산을 포함함 - 과;
개별 태스크의 특성에 기초하여 복수의 기계 학습 모델 각각에 대한 개별 우선순위 레벨을 측정하는 동작과; 그리고
제1 기계학습 모델로서, 개별 우선순위 레벨에 기초하여 복수의 기계학습 모델의 하나의 기계학습 모델을 선택하는 동작을 더 포함하는 것을 특징으로 하는 시스템.According to clause 14,
The operation of receiving data representing the first machine learning model includes:
Receiving data representing a plurality of machine learning models, each of the plurality of machine learning models configured to implement a separate task and including a separate inference operation to be performed by the system to process the input;
Measuring individual priority levels for each of a plurality of machine learning models based on characteristics of individual tasks; and
As a first machine learning model, the system further comprises selecting one machine learning model from the plurality of machine learning models based on an individual priority level.
제2 기계 학습 모델을 나타내는 데이터를 수신하는 동작과;
시스템이 제2 추론 출력을 생성하기 위해 입력을 처리하기 위한 제2 기계 학습 모델의 추론 연산을 수행하기 위한 제2 추정 기간을 획득하는 동작과;
(i) 우선순위 추론 연산을 수행하기 위한 우선순위 기간 및 (ii) 제1 기계 학습 모델에서 하위-모델의 추론 연산을 수행하기 위한 적어도 개별 추정 기간을 예약한 후 남은 반복 시간 윈도우의 각 발생에 대한 제2 잔여 기간을 결정하는 동작과;
제2 추정 기간이 제2 잔여 기간보다 큰지 여부를 결정하는 동작과;
제2 추정 기간이 제2 잔여 기간보다 크다는 결정에 응답하여, 제2 기계 학습 모델을 제2 잔여 기간 이하인 개별 추정 기간을 갖는 제2 하위-모델 그룹으로 분할하는 동작 - 제2 하위-모델 그룹의 각 하위-모델은 제2 기계 학습 모델의 추론 연산의 개별 부분을 포함함 - 과; 그리고
하나 이상의 처리 유닛에 의해, 반복 시간 윈도우가 발생한 제2 잔여 기간 동안 제2 하위-모델 그룹의 하위-모델에 대한 추론 연산을 수행하는 동작을 더 포함하는 것을 특징으로 하는 시스템.According to clause 14,
Receiving data representing a second machine learning model;
obtaining a second inference period for the system to perform an inference operation of a second machine learning model for processing the input to generate a second inference output;
At each occurrence of the remaining iteration time window after reserving (i) a priority period for performing priority inference operations and (ii) at least a separate inference period for performing inference operations of sub-models in the first machine learning model. determining a second remaining period for;
determining whether the second estimation period is greater than the second remaining period;
In response to determining that the second estimation period is greater than the second residual period, splitting the second machine learning model into second sub-model groups having individual estimation periods that are less than or equal to the second residual period - the second sub-model group Each sub-model contains a separate portion of the inference operations of the second machine learning model - and; and
The system further comprising performing, by the one or more processing units, an inference operation on a sub-model of the second sub-model group during a second remaining period in which the repetition time window occurs.
상기 입력은 센서에 의해 캡처된 복수의 이미지 프레임의 이미지 프레임을 포함하고;
상기 반복 시간 윈도우의 각각의 발생은 복수의 이미지 프레임의 이미지 프레임에 대응하고;
상기 개별 태스크는 배경 검출, 초점 검출, 객체 검출, 사람 얼굴 인식 중 적어도 하나를 포함하고; 그리고
상기 개별 태스크의 특성은 적어도 시스템의 하나 이상의 처리 유닛에 의해 개별 태스크를 수행하기 위한 개별 태스크 및 개별 추정 기간의 종속성을 포함하는 것을 특징으로 하는 시스템.According to clause 15,
the input includes an image frame of a plurality of image frames captured by a sensor;
Each occurrence of the repeating time window corresponds to an image frame of a plurality of image frames;
The individual tasks include at least one of background detection, focus detection, object detection, and human face recognition; and
The system of claim 1 , wherein the characteristics of the individual tasks include at least a dependency of the individual task and the respective estimation period for performing the individual task by one or more processing units of the system.
호스트에서, 제1 기계 학습 모델을 나타내는 데이터를 수신하는 동작 - 제1 기계 학습 모델은 제1 추론 출력을 생성하기 위해 입력을 처리하기 위한 추론 연산을 포함함 - 과;
시스템이 입력을 처리하여 제1 추론 출력을 생성하기 위한 제1 기계 학습 모델의 추론 연산을 수행하는 제1 추정 기간을 획득하는 동작과;
하나 이상의 하드웨어 처리 유닛이 복수의 기계 학습 모델의 추론 연산 중 적어도 일부를 수행하는 반복 시간 윈도우의 각각의 발생 동안 우선순위 기계 학습 모델의 우선순위 추론 연산을 수행하기 위해 예약된 우선순위 기간을 식별하는 동작과;
우선순위 추론 연산을 수행하기 위해 우선순위 기간을 예약한 후 남은 반복 시간 윈도우의 각 발생에 대한 제1 잔여 기간을 결정하는 동작과;
제1 추정 기간이 제1 잔여 기간보다 큰지 여부를 판단하는 동작과;
제1 추정 기간이 제1 잔여 기간보다 크다는 결정에 응답하여, 제1 기계 학습 모델을 제1 잔여 기간 이하인 각각의 추정 기간을 갖는 제1 하위-모델 그룹으로 분할하는 동작 - 제1 하위-모델 그룹의 각 하위-모델은 제1 기계 학습 모델의 추론 연산의 개별 부분을 포함함 - 과; 그리고
하나 이상의 하드웨어 처리 유닛에 의해, 반복 시간 윈도우가 발생한 제1 잔여 기간 동안 제1 하위-모델 그룹의 하위-모델에 대한 추론 연산을 수행하는 동작을 포함하는 것을 특징으로 하는 하나 이상의 컴퓨터 판독 가능 저장 매체.One storing instructions that, when executed by one or more computers, cause the one or more computers to perform individual operations performed by a system that includes a host and one or more hardware processing units configured to perform inferential operations of a plurality of machine learning models. The computer-readable storage medium above, wherein the individual operations include:
At the host, receiving data representing a first machine learning model, the first machine learning model comprising an inference operation to process the input to produce a first inference output;
Obtaining a first inference period over which the system processes the input and performs an inference operation of a first machine learning model to produce a first inference output;
Identifying a priority period reserved for performing priority inference operations of a priority machine learning model during each occurrence of a repetition time window in which one or more hardware processing units perform at least a portion of the inference operations of the plurality of machine learning models. motion and;
determining a first remaining period for each occurrence of the repetition time window remaining after reserving a priority period for performing a priority inference operation;
determining whether the first estimation period is greater than the first remaining period;
In response to determining that the first estimation period is greater than the first residual period, splitting the first machine learning model into first sub-model groups having respective estimation periods less than or equal to the first residual period - a first sub-model group. Each sub-model of includes a separate portion of the inference operation of the first machine learning model - and; and
At least one computer-readable storage medium comprising performing, by one or more hardware processing units, an inference operation on a sub-model of the first sub-model group during a first remaining period during which a repetition time window has occurred. .
상기 제1 기계 학습 모델을 나타내는 데이터를 수신하는 동작은,
복수의 기계 학습 모델을 나타내는 데이터를 수신하는 동작 - 복수의 기계 학습 모델 각각은 개별 태스크를 구현하도록 구성되고 입력을 처리하기 위해 시스템에 의해 수행될 개별 추론 연산을 포함함 - 과;
개별 태스크의 특성에 기초하여 복수의 기계 학습 모델 각각에 대한 개별 우선순위 레벨을 측정하는 동작과; 그리고
제1 기계학습 모델로서, 개별 우선순위 레벨에 기초하여 복수의 기계학습 모델의 하나의 기계학습 모델을 선택하는 동작을 더 포함하는 것을 특징으로 하는 하나 이상의 컴퓨터 판독 가능 저장 매체.According to clause 18,
The operation of receiving data representing the first machine learning model includes:
Receiving data representing a plurality of machine learning models, each of the plurality of machine learning models configured to implement a separate task and including a separate inference operation to be performed by the system to process the input;
Measuring individual priority levels for each of a plurality of machine learning models based on characteristics of individual tasks; and
As a first machine learning model, one or more computer-readable storage media further comprising selecting one machine learning model from the plurality of machine learning models based on an individual priority level.
제2 기계 학습 모델을 나타내는 데이터를 수신하는 동작과;
시스템이 제2 추론 출력을 생성하기 위해 입력을 처리하기 위한 제2 기계 학습 모델의 추론 연산을 수행하기 위한 제2 추정 기간을 획득하는 동작과;
(i) 우선순위 추론 연산을 수행하기 위한 우선순위 기간 및 (ii) 제1 기계 학습 모델에서 하위-모델의 추론 연산을 수행하기 위한 적어도 개별 추정 기간을 예약한 후 남은 반복 시간 윈도우의 각 발생에 대한 제2 잔여 기간을 결정하는 동작과;
제2 추정 기간이 제2 잔여 기간보다 큰지 여부를 결정하는 동작과;
제2 추정 기간이 제2 잔여 기간보다 크다는 결정에 응답하여, 제2 기계 학습 모델을 제2 잔여 기간 이하인 개별 추정 기간을 갖는 제2 하위-모델 그룹으로 분할하는 동작 - 제2 하위-모델 그룹의 각 하위-모델은 제2 기계 학습 모델의 추론 연산의 개별 부분을 포함함 - 과; 그리고
하나 이상의 처리 유닛에 의해, 반복 시간 윈도우가 발생한 제2 잔여 기간 동안 제2 하위-모델 그룹의 하위-모델에 대한 추론 연산을 수행하는 동작을 더 포함하는 것을 특징으로 하는 하나 이상의 컴퓨터 판독 가능 저장 매체.According to clause 18,
Receiving data representing a second machine learning model;
obtaining a second inference period for the system to perform an inference operation of a second machine learning model for processing the input to generate a second inference output;
At each occurrence of the remaining iteration time window after reserving (i) a priority period for performing priority inference operations and (ii) at least a separate inference period for performing inference operations of sub-models in the first machine learning model. determining a second remaining period for;
determining whether the second estimation period is greater than the second remaining period;
In response to determining that the second estimation period is greater than the second residual period, splitting the second machine learning model into second sub-model groups having individual estimation periods that are less than or equal to the second residual period - the second sub-model group Each sub-model contains a separate portion of the inference operations of the second machine learning model - and; and
At least one computer-readable storage medium further comprising performing, by the one or more processing units, an inference operation on a sub-model of the second sub-model group during a second remaining period in which the repetition time window occurs. .
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2021/050975 WO2023043459A1 (en) | 2021-09-17 | 2021-09-17 | Performing segmented inference operations of a machine learning model |
Publications (1)
Publication Number | Publication Date |
---|---|
KR20240035859A true KR20240035859A (en) | 2024-03-18 |
Family
ID=80050679
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
KR1020247005576A KR20240035859A (en) | 2021-09-17 | 2021-09-17 | Perform segmentation inference operations on machine learning models |
Country Status (4)
Country | Link |
---|---|
KR (1) | KR20240035859A (en) |
CN (1) | CN117882087A (en) |
TW (1) | TWI833260B (en) |
WO (1) | WO2023043459A1 (en) |
Family Cites Families (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN109102066A (en) * | 2018-07-12 | 2018-12-28 | 山东师范大学 | Online time series data prediction technique, system and storage medium based on fuzzy reasoning |
US11379713B2 (en) * | 2018-12-08 | 2022-07-05 | Apical Limited | Neural network processing |
US11763946B2 (en) * | 2020-02-27 | 2023-09-19 | Optum, Inc. | Graph-based predictive inference |
-
2021
- 2021-09-17 KR KR1020247005576A patent/KR20240035859A/en unknown
- 2021-09-17 CN CN202180101776.0A patent/CN117882087A/en active Pending
- 2021-09-17 WO PCT/US2021/050975 patent/WO2023043459A1/en active Application Filing
-
2022
- 2022-06-24 TW TW111123649A patent/TWI833260B/en active
Also Published As
Publication number | Publication date |
---|---|
TW202314601A (en) | 2023-04-01 |
TWI833260B (en) | 2024-02-21 |
CN117882087A (en) | 2024-04-12 |
WO2023043459A1 (en) | 2023-03-23 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20210295161A1 (en) | Training neural networks represented as computational graphs | |
JP6790286B2 (en) | Device placement optimization using reinforcement learning | |
KR102191444B1 (en) | Training of neural networks using prioritized experiential memory | |
CN108292241B (en) | Processing a computation graph | |
US11861474B2 (en) | Dynamic placement of computation sub-graphs | |
JP6539236B2 (en) | System and method for use in effective neural network deployment | |
WO2017176333A1 (en) | Batching inputs to a machine learning model | |
WO2018211139A1 (en) | Training action selection neural networks using a differentiable credit function | |
CN110476173B (en) | Hierarchical device placement with reinforcement learning | |
US11545157B2 (en) | Speaker diartzation using an end-to-end model | |
US20230049747A1 (en) | Training machine learning models using teacher annealing | |
JP2019517075A (en) | Categorizing Example Inputs Using Comparison Sets | |
JP2018537788A (en) | Extension of neural network using external memory | |
CN113906501A (en) | Subtask(s) to undertake a predicted action in response to a separate user interaction with an automated assistant prior to undertaking the predicted action | |
US10754712B2 (en) | Techniques for automatically allocating tasks to application programming interfaces | |
EP3360082A1 (en) | Neural programming | |
WO2019084558A1 (en) | Selecting answer spans from electronic documents using machine learning | |
KR20190045038A (en) | Method and apparatus for speech recognition | |
CN111133458B (en) | Enhanced neural network | |
CN113348472A (en) | Convolutional neural network with soft kernel selection | |
CN111460832B (en) | Method, device, system, equipment and computer storage medium for object coding | |
US20210027064A1 (en) | Parallel video processing neural networks | |
KR20240035859A (en) | Perform segmentation inference operations on machine learning models | |
Li et al. | Rt-lm: Uncertainty-aware resource management for real-time inference of language models | |
GB2607255A (en) | Dynamic subsystem operational sequencing to concurrently control and distribute supervised learning processor training and provide predictive responses |