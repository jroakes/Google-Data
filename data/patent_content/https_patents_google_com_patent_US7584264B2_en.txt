US7584264B2 - Data storage and retrieval systems and related methods of storing and retrieving data - Google Patents
Data storage and retrieval systems and related methods of storing and retrieving data Download PDFInfo
- Publication number
- US7584264B2 US7584264B2 US10/643,489 US64348903A US7584264B2 US 7584264 B2 US7584264 B2 US 7584264B2 US 64348903 A US64348903 A US 64348903A US 7584264 B2 US7584264 B2 US 7584264B2
- Authority
- US
- United States
- Prior art keywords
- data
- data storage
- servers
- storage units
- server
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L67/00—Network arrangements or protocols for supporting network services or applications
- H04L67/01—Protocols
- H04L67/10—Protocols in which an application is distributed across nodes in the network
- H04L67/1097—Protocols in which an application is distributed across nodes in the network for distributed storage of data in networks, e.g. transport arrangements for network file system [NFS], storage area networks [SAN] or network attached storage [NAS]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/10—File systems; File servers
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L51/00—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail
- H04L51/21—Monitoring or handling of messages
- H04L51/222—Monitoring or handling of messages using geographical location information, e.g. messages transmitted or received in proximity of a certain spot or area
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L61/00—Network arrangements, protocols or services for addressing or naming
- H04L61/45—Network directories; Name-to-address mapping
- H04L61/4505—Network directories; Name-to-address mapping using standardised directories; using standardised directory access protocols
- H04L61/4511—Network directories; Name-to-address mapping using standardised directories; using standardised directory access protocols using domain name system [DNS]
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L61/00—Network arrangements, protocols or services for addressing or naming
- H04L61/45—Network directories; Name-to-address mapping
- H04L61/4555—Directories for electronic mail or instant messaging
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L9/00—Cryptographic mechanisms or cryptographic arrangements for secret or secure communications; Network security protocols
- H04L9/40—Network security protocols
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L51/00—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L69/00—Network arrangements, protocols or services independent of the application payload and not provided for in the other groups of this subclass
- H04L69/30—Definitions, standards or architectural aspects of layered protocol stacks
- H04L69/32—Architecture of open systems interconnection [OSI] 7-layer type protocol stacks, e.g. the interfaces between the data link level and the physical level
- H04L69/322—Intralayer communication protocols among peer entities or protocol data unit [PDU] definitions
- H04L69/329—Intralayer communication protocols among peer entities or protocol data unit [PDU] definitions in the application layer [OSI layer 7]
Definitions
- Disclosed embodiments herein relate generally to data storage and retrieval, and more particularly to data storage and retrieval systems and related methods of storing and retrieving data employing redundant data storage over TCP/IP connection across a computer network.
- a system includes a data processing server configured to receive incoming data and to transmit the data for storage.
- the system also includes a plurality of data storage servers each coupled to one or more data storage units and configured to receive transmitted data for writing to the one or more data storage units, and configured to read data from the one or more data storage units.
- the systems in this embodiment includes a data retrieval server coupled to one or more of the plurality of data storage servers and configured to retrieve data read by the one or more data storage servers from the one or more data storage units.
- the system still further includes a plurality of process modules each associated with one of the plurality of data storage servers, where at least two of the process modules configured to write a portion of the data to corresponding data storage units.
- each of the at least two process modules are further configured to transmit an acknowledgment associated with each of the corresponding at least two data storage units upon the writing of the portion of data in the corresponding at least two data storage units.
- a data storage and retrieval system comprise a data processing server configured to receive incoming data and to transmit the data for storage, and a plurality of data storage servers each coupled to one or more of a plurality of data storage units and configured to receive a portion of the data for writing to at least two of the plurality of data storage units.
- the system also includes storage server records comprising configuration information corresponding to connection path and availability of each of the plurality of data storage servers.
- the system includes a domain name system server coupled to the data processing server and configured to store the storage server records and to supply the storage server records to the data processing server for use in identifying at least two of the plurality of data storage servers having an available connection.
- the data processing server is further configured to establish connections with the at least two data storage servers based on the identification of the at least two data storage servers using the storage server records.
- the present disclosure discloses a data storage and retrieval system, and related method, comprising a data processing server configured to receive incoming data and to transmit the data for storage.
- the system comprises a plurality of data storage servers each coupled to one or more of a plurality of data storage units and configured to receive a portion of the data for writing to at least two of the data storage units.
- the system further includes a data retrieval server coupled to one or more of the plurality of data storage servers and configured to retrieve the data portion read by the one or more data storage servers and written to the at least two data storage units from one or more of the at least two data storage units.
- the system still further includes data storage information keys corresponding to each of the data storage units and comprising offset information corresponding to the location of the data portion in the at least two data storage units.
- the system includes a key manager associated with the data retrieval server and configured to store the data storage information keys therein.
- FIG. 1 illustrates one embodiment of a data storage and retrieval system in accordance with the principles disclosed herein;
- FIG. 2 illustrates a more detailed view of a data storage and retrieval system disclosed herein;
- FIG. 3 illustrates another embodiment of a data storage and retrieval system in accordance with the principles disclosed herein;
- FIG. 4 illustrates a more detailed view of several components of a data storage and retrieval system as disclosed herein;
- FIG. 5 illustrates a flow diagram of a process for writing data to storage drives in a data storage and retrieval system in accordance with the principles disclosed herein;
- FIG. 6 illustrates a block diagram of one embodiment of a data storage information key
- FIG. 7 illustrates yet another embodiment of a data storage and retrieval system constructed according to the principles disclosed herein.
- FIG. 8 illustrates a bar graph illustrating a comparison between a conventional data storage system and a data storage and retrieval system constructed employing the present principles.
- the system 100 provides redundant data storage (and data retrieval) and includes a plurality of data processing servers 102 a, 102 , 102 n, and a plurality of data retrieval servers 104 a, 104 b, 104 n.
- the data processing servers 102 a, 102 , 102 n may be electronic mail data processing servers (sometimes called “MX servers”), however, they may also be any other type of data processing server configured to receive and transmit data via a computer network.
- MX servers electronic mail data processing servers
- the data retrieval servers 104 a, 104 b, 104 n may be electronic mail retrieving servers (sometimes called “WB servers”), which are configured to directly receive transmitting data or retrieve data from another location.
- servers 104 a, 104 b, 104 n may be used to receive or retrieve electronic message data via a computer network, as discussed in greater detail below.
- Also included in the system 100 is a plurality of data storage servers. Specifically, four data storage servers 106 , 108 , 110 , 112 are shown. In addition, first and second data storage servers 106 , 108 may be viewed as a first group of data storage servers, while third and fourth data storage servers 110 , 112 may be viewed as a second group. Each of the data storage servers 106 , 108 , 110 , 112 includes one or more data storage units 106 a, 108 a, 110 a, 112 a respectively coupled thereto. In an advantageous embodiment, the data storage units 106 a, 108 a, 110 a, 112 a are hard disk drives (HDDs) typically used for the storage of data.
- HDDs hard disk drives
- data storage units 106 a, 108 a, 110 a, 112 a are illustrated externally coupled to the data storage servers 106 , 108 , 110 , 112 , in other embodiments the data storage units 106 a, 108 a, 110 a, 112 a may be located inside the servers 106 , 108 , 110 , 112 .
- any type of HDDs or other form of data storage unit, as well as any number of HDDs may be employed in the system 100 , as each application requires.
- FIG. 1 also generally illustrates the interconnections provided by the data storage and retrieval system 100 . More specifically, when the system 100 is employed to store data chunks (i.e., portions of the incoming data), the data processing servers 102 a, 102 b , 102 n may each be connected to pairs of the data storage servers 106 , 108 , 110 , 112 . Moreover, the connections to such pairs may be randomly selected, depending on the application of the system 100 . In the illustrated embodiment, the pairs of data storage servers 106 , 108 , 110 , 112 are comprised of the first and second groups mentioned above.
- each data processing server 102 a, 102 b, 102 n may be connected to more than two data storage servers, if further data storage redundancy is desired. Exemplary manners of making such connections are discussed in greater detail below.
- the data processing servers 102 a, 102 b, 102 n are coupled to a key manager 114 for storing information therein, such as information received from the data storage servers 106 , 108 , 110 , 112 , which is also further discussed below.
- FIG. 1 Further connections shown in FIG. 1 are the data retrieval servers 104 a, 104 b, 104 n coupled to each of the data storage servers 106 , 108 , 110 , 112 .
- the data retrieval servers 104 a, 104 b, 104 n need only retrieve data from one of the data storage servers 106 , 108 , 110 , 112 at a time.
- the data retrieval servers 104 a, 104 b, 104 n may simply connect with another of the servers 106 , 108 , 110 , 112 to find and retrieve the desired data.
- the data retrieval servers 104 a, 104 b, 104 n are also connected to the key manager 114 , and such connection may be employed to retrieve information stored in the key manager 114 , such as data storage information pertaining to the location of the stored data among the data storage servers 106 , 108 , 110 , 112 , as discussed in greater detail below.
- FIG. 2 illustrated is a more detailed view of a data storage and retrieval system 200 . While FIG. 1 provided a broad view of potential connections between some of the components in a system constructed according to the principles disclosed herein, FIG. 2 provides a specific embodiment of such a data storage and retrieval system 200 . As such, FIG. 2 illustrates some components similar to those illustrated in FIG. 1 , which are like-numbered accordingly.
- the system 200 in FIG. 2 includes a plurality of data storage servers 106 , 108 , 110 , each shown coupled to respective data storage units 106 a, 108 a, 110 a.
- the data storage units 106 a, 108 a, 110 a may be located inside the data storage servers 106 , 108 , 110 , rather than being external.
- a data processing server 102 as well as the key manager 114 connected to both the data processing server 102 and the data retrieval server 104 .
- FIG. 2 also includes a sending server 202 connected to a computer network 204 .
- the sending server 202 is an electronic mail server configured to transmit electronic messages from user terminals (not illustrated) across the computer network 204 .
- the computer network 204 is a packet network, such as the Internet, but other types of computer networks, for example an Ethernet network, may also be employed.
- DNS domain name system
- the computer network 204 includes the Internet, DNS 206 may be employed using conventional techniques. More specifically, when a user sends an electronic message via the sending server 202 , the user inputs a desired destination address using the typical format, for instance, “username@postini.com”.
- the DNS server 206 converts the standard e-mail format to a numeric Internet protocol (IP) address, which is commonly expressed in four numbers separated by periods (e.g., 123.45.67.89).
- IP Internet protocol
- the DNS 206 matches the e-mail format to the exact IP address of the desired destination based on the domain portion of the e-mail address (i.e., the text immediately to the right of “@”). More specifically, available hosts/servers are first identified, then the host names are converted to the proper IP addresses, all using appropriate records associated with the DNS server 206 . Once the DNS 206 has so matched the IP address, the electronic message, or other data, may then be routed to the specific destination server, which then may transmit the message to the receiving user terminal (not illustrated).
- IP Internet protocol
- the intended destination server is the data retrieval server 104 .
- the data processing server 102 may be configured to receive the electronic message before it reaches the retrieval server 104 .
- the data processing server 102 may be configured to intercept such incoming data in an effort to filter the data before it arrives at the retrieval server 104 .
- the data processing server 102 may include e-mail spam filtering capabilities employed to lessen the burden of receiving unwanted e-mails at the retrieval server 104 .
- a specific process module associated with the data processing server 102 processes and transmits the data for storage, in accordance to the principles disclosed herein. Specifically, the process module queries an associated server 208 for available data storage servers 106 ready to receive and store the incoming data.
- the associated server 208 may be another DNS server 208 .
- DNS server 208 may then include storage server records 210 , which include configuration information corresponding to the layout and connections of all the data storage servers 106 , 108 , 110 .
- connections between the data processing server 102 and the data storage servers 106 , 108 , 110 may be TCP/IP connections, where each of the data storage servers 106 , 108 , 110 may be identified using an IP address format.
- the configuration information would include TCP/IP connection information, and thus IP addresses, associated with each of the data storage servers 106 , 108 , 110 within the storage server records 210 .
- the storage server records 210 may be updated with configuration information corresponding to data storage servers added to the plurality of data storage servers 106 , 108 , 110 as the system 200 grows, as well as in response to the removal of data storage servers from the plurality presently in the system 200 .
- the system 200 lends itself to the easy addition or removal of data storage servers by simply updating the configuration information in the storage server records 210 within DNS server 208 .
- DNS server 208 will thus be capable of constantly determining which data storage servers are present in the system 200 , and provide that information to the data processing server 102 .
- DNS server 208 is queried by the data processing server 102 to determine the IP addresses of available data storage servers 106 , 108 , 110 for transmission of the data to those available servers. More specifically, in embodiments where TCP/IP protocols are used in the system 200 , the data processing server 102 queries DNS server 208 for the IP addresses of at least two of the plurality of data storage servers 106 , 108 , 110 . At least two such servers are sought so that a redundancy in data storage in the system 200 is provided. As such, if the data stored through one data storage server is lost or corrupted, the second data storage server may be employed to recover the data. Other embodiments may employ more than two of the data storage servers 106 , 108 , 110 , depending on the level of data storage redundancy desired in the system 200 .
- the data processing server 102 establishes TCP/IP connections with the selected servers (in the illustrated embodiment, servers 106 and 108 are selected) using the information obtained from the storage server records 210 . If a connection with either server 106 , 108 cannot be established, or an established connection times-out (for example, because a storage server, such as server 108 , is nonfunctional or its associated or internal storage unit is full), the data processing server 102 can simply query DNS server 208 for another group of available data storage servers from the plurality. If the connection is established, the data processing server 102 transmits the data to both of the data storage servers 106 , 108 to be written and stored in data storage units 106 a, 108 a respectively associated with the servers 106 , 108 .
- the data is compressed in memory and, along with a descriptive header, is put into consecutive memory blocks for direct writing to the data storage units 106 a, 108 a (e.g., HDDs).
- the process module running in the associated data storage severs causes the block to be written to each of the data storage units 106 a, 108 a.
- the current transaction is held in memory until the writer thread is free.
- further incoming data chunks to be stored are concatenated with the first one already held in memory in the same data block until the current write is complete.
- the data block comprises a predetermined 64 kb
- four data chunks of 16 kb each may be concatenated in the data block and held in memory for a single continuous write.
- the spooled transactions are then written to the appropriate data storage unit 106 a, 108 a in one consecutive block for all four data chunks, which is much more efficient than executing, for example, four separate fife writes for the four chunks of data.
- the size of the concatenated data can be adjusted to maximize the efficiency of writing to the data storage units 106 a, 108 a.
- the write:read ratio involves the number of writes a data storage system may make to store data to a data storage unit, versus the number of reads of stored data the same system must make to retrieve or delete the data.
- conventional systems typically write each chunk of data to a storage unit as an individual file, whereas the present system 200 may be configured to write one single data “bucket”, rather than millions of individual files, e.g., one file on the HDD for each chunk of data written.
- the bucket itself may be divided into separate files, not based on each data chunk stored but rather for data block sizing to assist in concatenating the data chunks for writing efficiency.
- Another advantage of concatenating the data before writing it to a storage unit includes the ability to seek out large groups of separate data chunks based on storage date, rather than having to seek out each chunk as a separate file. This is especially useful in systems having the capability of mass deleting stored data based on an expiration date, such as the system 200 of FIG. 2 .
- data chunks stored on a particular date are stored in a single data bucket based on that date, rather than a separate file for each chunk.
- Such buckets are not limited to a particular data storage server; rather, they extend across all the active data storage servers and are confined to a specific physical location.
- each bucket in each data storage server which may represent millions of data chunks stored on a single date, may be deleted as a whole, on each data storage server or across all data storage servers in the system 200 , just as a single file would in a conventional system, rather than having to locate and delete each data chunk as a separate file.
- this approach greatly decreases the time necessary to complete such a task due to decreases in HDD rotational delay and seek times allowed by the sequential data storage.
- each data bucket will reside in a specific directory on a particular data storage unit 106 a, 108 a.
- a configuration (“config”) file on each data storage unit 106 a, 108 a contains configuration information about the specific bucket, while all other files stored on the data storage units 106 a, 108 a typically constitute stored incoming data.
- config configuration
- Those who are skilled in the pertinent field of art will understand how to create such configuration files, as well as how to organize and write the data within those files.
- a data storage and retrieval system employing the principles disclosed herein is not necessarily limited to any particular configuration or file format.
- the process modules associated with the data storage servers 106 , 108 create and transmit an acknowledgement ACK( 1 ), ACK( 2 ) of the successful data storage from each of the selected servers 106 , 108 .
- the acknowledgements ACK( 1 ), ACK( 2 ) are transmitted back to the data processing server 102 , which cause the data processing server 102 to stop trying to find available data storage servers in which to safely store the data.
- the data processing server 102 may be configured to abort the writing of the data in those two servers 106 , 108 , and find a new group of data storage servers through which to transmit the data for storage.
- the data processing server 102 stop trying to find a group of data storage servers for storing the data.
- the process modules associated therewith may also create and transmit data storage information keys KEY( 1 ), KEY( 2 ) from each of the data storage servers 106 , 108 back to the data processing server 102 .
- the keys include storage information pertaining to the data, such as the location of the data in the data storage units 106 a, 108 a, when the data was stored, and the size of the data. Further detail regarding keys KEY( 1 ), KEY( 2 ) are discussed below with reference to FIG. 6 .
- the data processing server 102 After receiving the keys KEY( 1 ), KEY( 2 ), the data processing server 102 stores the keys KEY( 1 ), KEY( 2 ) in the key manager 114 , creating a master index in the key manager 114 of all the different incoming data stored in the data storage units 106 a, 108 a, 110 a associated with the plurality of data storage servers 106 , 108 , 110 in the system 200 .
- the data retrieval server 104 is used to retrieve the data from the data storage unit(s) 106 a, 108 a holding the data. To do so, the data retrieval server 104 queries the key manager 114 to find the appropriate key(s) corresponding to the stored data.
- the first key KEY( 1 ) may be obtained from the key manager 114 , and used to determine the location of the data in data storage unit 108 a, which is associated with data storage server 108 . Using the information in the key KEY( 1 ), the data may be easily located and retrieved from the data storage unit 108 a. If, however, the data is not found in data storage unit 108 a, or has somehow been corrupted, the data retrieval server 104 may again query the key manager 114 , this time for the second key KEY( 2 ). The second key KEY( 2 ) would then lead the data retrieval server 104 to the data stored in the data storage unit 106 a associated with data storage server 106 . The data may then be retrieved from that location and viewed by the user.
- FIG. 3 illustrated is another embodiment of a data storage and retrieval system 300 in accordance with the principles disclosed herein.
- the system 300 of FIG. 3 illustrates the arrangement of data storage servers 106 and 108 in a master/slave connection. More specifically, the system 300 includes data storage server 106 in the master position, and data storage server 108 in the slave position. With this arrangement, the storing of data to either of the data storage units 106 a, 108 a associated with the data storage servers 106 , 108 passes through data storage server 106 , since it is the master server.
- the master/slave connection is accomplished by connecting only the master unit, data storage server 106 in this embodiment, to the data processing server 102 .
- the slave unit, data storage server 108 in this embodiment is connected directly to the master unit rather than to the data processing server 102 .
- the redundant storage of data to the data storage units of a plurality of data storage servers can be handled in a less random manner. More specifically, where other embodiments of a data storage and retrieval system according to the principles disclosed herein store data by the random selection of two or more data storage servers, the master/slave system 300 of FIG. 3 stores data through the selection of only the master data storage server 106 . Redundant storage of the data via one (or more) slave data storage server 108 is predetermined through the connection to the master data storage server 106 in the master/slave relationship.
- a DNS server (not illustrated) maybe queried by the data processing server 102 to determine the IP addresses of available master data storage servers.
- the connection between the data processing server 102 and the selected master data storage server 106 may be a TCP/IP connection.
- the DNS server may again be queried for another available master data storage server.
- the data processing server 102 transmits the data to the master data storage server 106 to be written and stored in the data storage unit 106 a associated therewith.
- the data will also be written to the data storage unit 108 a associated with the slave data storage server 108 for redundancy.
- the data is also transferred (e.g., mirrored) to the slave data storage server 108 .
- the data is stored/written in the data storage unit 108 a associated with the slave data storage server 108 .
- the data may be written immediately to the data storage unit 108 a, or it may be queued in sequential blocks, in the manner described above.
- the process module associated with the slave data storage server 108 transmits an acknowledgement ACK( 1 ) to the master data storage server 106 , rather than directly to the data processing server 102 as was done in the embodiments described above.
- the process module also transmits a key KEY( 1 ) corresponding to the stored data back to the master data storage server 106 , rather than to the data processing server 102 .
- the writing of the data to the data storage unit 106 a associated with the master data storage server 106 also typically occurs in the manner described in the embodiments discussed above. As such, once the data is successfully written to the data storage unit 106 a, the process module associated with the master data storage server 106 also generates its own acknowledgement ACK( 2 ) and key KEY( 2 ) corresponding to the stored data. In addition, in embodiments having the master/slave relationship, the process module in the master data storage server 106 is configured to receive the acknowledgement ACK( 1 ) and key KEY( 1 ) from the slave data storage server 108 , communicating to the master data storage server 106 that the data write to the slave data storage server 108 was successful.
- the process module of the master data storage server 106 may then generate its acknowledgement ACK( 2 ) and the key KEY( 2 ) based on the successful writing in both the master and slave units 106 a, 108 a.
- the master server may then transmit the single acknowledgement ACK( 2 ) and key KEY( 2 ), rather than two separate acknowledgements and two separate keys, to the data processing server 102 , for use as described above.
- the use of only one acknowledgement and key simplifies the operation of the system 300 , as well as reduces the storage space required to store keys in the key manager 114 .
- the data storage to the master/slave pair of servers is abandoned as unsuccessful, and the data processing server 102 establishes a new connection with a new pair of master/slave data storage servers.
- the data processing server 102 After receiving the key KEY( 2 ), the data processing server 102 stores the key KEY( 2 ) in the key manager 114 , as before.
- the data retrieval server 104 may then be used to retrieve the data from one or both of the data storage units 106 a, 108 a by querying the key manager 114 to find the appropriate key corresponding to the stored data.
- the data stored in the data storage units 106 a, 108 a is correlated for keeping track of the stored data. Such storage correlation may be accomplished by storing the data in identical data buckets within the data storage units 106 a, 108 a.
- the data is redundantly stored in the same order and in the same file/location on both data storage units 106 a, 108 a.
- data may more easily be located in either data storage unit 106 a, 108 a without having to scan the entire drive(s) of the units 106 a, 108 a looking for the specific data sought to be retrieved.
- the data retrieval server 104 may simply retrieve the data from the data storage unit 108 a associated with the slave data storage server 108 using the same key KEY( 2 ).
- the process module associated with the master data storage server 106 may be configured to cause the slave data storage server 108 to store the mirrored data, rather than having the data processing server 102 directly execute the writing of the data to both data storage units 106 a, 108 a.
- FIG. 4 illustrated is a more detailed view of several components of a data storage and retrieval system 400 as disclosed herein.
- the system of FIG. 4 still includes the data processing server 102 , data retrieval server 104 and data storage server 106 described in the various embodiments discussed above.
- FIG. 4 illustrates an embodiment of a fast-filter daemon (FFD) 402 associated with the data processing server 102 .
- the FFD 402 may be any of a number of filtering processes configured to filter incoming data for distribution to a plurality of destinations.
- the FFD 402 is provided and configured to filter incoming e-mail messages.
- the FFD 402 uses a connection manager to establish a connection to receive incoming mail, filter applications to filter the incoming messages, and a delivery manager for directing the message to the appropriate location.
- a process module of the type discussed above is also associated with the delivery manager to facilitate the transmission of filtered data to the appropriate servers.
- FIG. 4 illustrates both the data processing server 102 and data storage server 106 having the process module 404 , the two process modules 404 are not necessarily identical. Rather, the process module 404 is shown in both components to illustrate the fact that the process module 404 is a collection of code or other type of computer instructions that is executed in several components simultaneously for different purposes. In addition, in other embodiments, the process module 404 may be located in a central location and configured to operate with remote modules associated with the data processing server 102 and data storage server 106 .
- RAM random access memory
- the system 400 may include any number of RAM modules, located in any or all of the components, as each application requires.
- the tasks that the process module 404 may typically be configured to perform are, for example, sequentially spooling incoming data in the RAM module 406 until the current data block is filled and therefore ready to be written.
- the process module 404 causes the data to be written to one of a plurality of available data storage units 106 a, 106 b associated with the data storage server 106 .
- the system 400 also illustrates a computer network 408 , which may be a packet-based network such as the Internet.
- the data processing server 102 , data retrieval server 104 , and data storage server 106 are coupled to the computer network 408 .
- the interconnection of these devices via the computer network 208 is made using TCP/IP protocol connections, as illustrated, but the system 400 is not so limited.
- TCP/IP connections across a computer network 408 the system 400 enjoys substantial utility in that the various components comprising the system 400 may be distant from one another, using the computer network 408 as a corridor for the transmission of data.
- the system 400 functions by first receiving incoming data, for example, in the form of e-mail messages and message data, through the data processing server 102 .
- the incoming data may be sent from a user via a sending server (not illustrated).
- a sending server may be coupled to the computer network 408 as well, for transmission of the data to the system 400 .
- the FFD 402 filters the data according to predetermined parameters. By filtering the data, some data is determined to not be diverted (quarantined or re-routed), and is transmitted directly to a receiving server 410 , and then on to a receiving terminal 412 where the data may be read.
- FFD 402 determines whether spam e-mails.
- data storage server e.g., data storage server 106
- the FFD 402 may be configured to filter incoming data in any of the manners set forth in either of the above-identified patent applications, or using any other appropriate technique.
- the process module 404 When the data is ready for storage, the process module 404 then attends to the storage of the data in the data storage units 106 a, 106 b associated with the data storage server 106 in any of the various manners described above.
- An acknowledgement ACK( 1 ) of the successful data storage, as well as a data storage information key KEY( 1 ) may then be generated and transmitted back to the data processing server 102 by the process module 404 .
- the user at the receiving terminal 412 who may be the originally intended recipient of the data, determines that he would like to view the stored e-mail message or message data, the user may prompt the data retrieval server 104 to retrieve the data via the appropriate data storage server 106 .
- data retrieval may also occur across the computer network 408 , but the system 400 is not so limited.
- the data retrieval server 104 may be proximate to the data storage server 106 , while the retrieved data is transmitted from the retrieval server 104 to the receiving server 410 via the computer network 408 .
- commands and other actions are used by the process modules in order to effect the actions described in this application.
- the data processing servers 102 may establish TCP/IP connections to the data storage servers via a TCP/IP connection by initiating a CONNECTION command.
- An available data storage server 106 would than accept the connection by a unique response acknowledgement. This may typically be accomplished by using the system's operating system library call “CONNECT”.
- exemplary commands that may be sent from the data processing server 102 include a NEW BUCKET command for establishing new data buckets on the data storage unit(s) associated with a data storage server 106 in order to store the incoming data, and a PUT command for writing data into specified data buckets.
- the data storage server 106 might respond with different acknowledgements to inform the data processing servers of whether the data bucket was created, already existed, or if for some other reason the data bucket could not be created.
- This NEW BUCKET command may include fields for specifying the maximum file size to which files in the bucket may grow and for specifying the amount of time to keep a file open for the writing of data.
- the PUT command would be used to store or write data chunks in a specified data bucket, and it could include fields for establishing the boundaries of the metadata and data to follow.
- the data storage server 106 responds to the command with an acknowledgement that the data has been effectively received and written into the data storage unit(s). As described above (and with reference to FIG. 6 ), a key is sent back with or as a part of the data storage server's 106 acknowledgement of the successful write, and the key may be later used to retrieve specific data from a particular data bucket using a GET command, as described herein.
- a GET command may be used to retrieve the key identifying the location of the data and to find the bucket holding the data. If the key matches the data storage unit associated with the current data storage server 106 , then the data storage server 106 may respond with the characteristics of the particular group of data sought by the data retrieval server 104 , followed by the data sought. If the particular data sought by the GET command is not available through the data storage server 106 designated, it would provide an indication of that in its response message.
- Other commands can be used to retrieve particular metadata describing certain data, for updating or other processing, or even certain data by itself. Still other commands may be sent by the data processing server 102 or retrieval server 104 in order to delete certain data buckets, or to close the connection between a data processing server 102 or retrieval server 104 and a particular data storage server.
- an SEL (select) command may be used to search for stored data, which is specified in a key. Specifically, such a command can return the appropriate key associated with the stored data, and then the metadata and data found at the location set forth in the key. Moreover, in some embodiments, the SEL command may be used to iterate over a bucket, selecting all matching data chunks. In such an embodiment, the key of the last selected record is needed to select the next record, thus a special key value of “start” will cause the SEL command to begin searching at the start of the bucket. Still further, any retrieval of data is more efficient with the present system since the sequential storage of data chunks mentioned above, also allows the sequential reading of the data, thus reducing rotational delay and seek times for storage units.
- FIG. 5 illustrated is a flow diagram 500 of a process for writing data to storage drives in a data storage and retrieval system constructed in accordance with the principles disclosed herein. More specifically, the process is employed by a process module(s) associated with data storage servers to facilitate the storage of incoming data for later retrieval, modification or deletion.
- the illustrated embodiment of the process includes the storing of data that is too large to fit in a single block, such as the typical 64 kb data block mentioned above, into consecutive data blocks. The process begins at a start block 502 .
- the process module first determines whether the incoming data that is to be stored fits into the currently available data block. For example, if the current data block is a 64 kb block, it is determined whether the data, along with the header and any other information necessary to be stored with the data, is less than the size of the available data block. If it is determined that the incoming data does fit in the data block, the process moves to block 506 where the data is moved into the available data block, which will eventually be stored on a data storage unit, such as hard disk drive. Once the data is in the current data block, the process moves to block 508 where the writer thread associated with the data storage unit is notified that the data block is ready for writing. A write request thread is then created for the data at block 507 .
- the process moves to block 510 where the beginning portion of the incoming data is stored in the current data block. The process then moves to block 512 where the remainder of the incoming data is moved to the next available data block. This portion of the process continues until all of the incoming data to be stored is held in multiple data blocks. A write request thread is then created for the multiple data clocks at block 507 . Then, the process moves to block 508 where a writer thread associated with the data storage unit is notified that the plurality of data blocks is ready for writing.
- write request threads typically a connection from each of a plurality of data processing servers, for storing data chunks, will create a separate corresponding write request thread.
- only a limited number of writer threads are available in each data storage server.
- the write request threads are spooled pending the completion of a write by one of the writer threads. Whether spooled during such high traffic situations or not, one of the writer threads is notified of a waiting write request thread at block 508 .
- the process moves to block 514 where it is determined whether the data is held in a single data block or in multiple data blocks, as created in blocks 510 and 512 of the process. If it is determined that the data is held in a single data block, the process moves to block 516 where the writer thread proceeds to write the data block to the data storage unit. However, if it is determined that the data is held in multiple data blocks, the process moves to block 518 where the process module synchronizes all of the multiple data blocks for the incoming data. Once synchronized, the process moves to block 520 where the multiple data blocks are synchronously written to the data storage unit.
- the process moves to block 522 where the write request thread, as well as any waiting write request threads, are notified that the data block(s) has been written, and the writer thread is ready for the next block or set of blocks.
- the process module returns a data storage information key to the data processing server that transmitted the data for storage.
- the key includes file number information for the file holding the data and offset information for determining the location of the stored data in the data storage unit.
- the process module running in the data storage server may be configured to create storage server information, as well as data bucket information, in the key. The different types of information contained in the key are discussed in detail with reference to FIG. 6 below.
- an acknowledgement may be sent by the process module, as described above, acknowledging the successful storage of the data. The process then ends at block 526 .
- a read request for the stored data chunks follows a similar, yet less complicated pattern.
- a read request thread is created with a connection from a data retrieval server to the data storage server holding the data. The read thread then simply use the information in the returned key to locate the desired data and read, modify, or delete it, depending on the application.
- the process illustrated in the flow diagram of FIG. 5 illustrates the breaking up of an incoming data chunk into multiple data blocks, and then synchronizing the multiple data blocks by sequentially arranging them, such steps may also be employed for synchronizing separate data chunks into sequential data blocks.
- the concatenated data blocks may then be quickly written in a single writing step by the writer thread, rather than stopping and writing each chunk of incoming data to the appropriate data storage unit.
- the key 600 includes storage server information 602 .
- the storage server information 602 will comprise an identification of the data storage server in which the particular data chunk is stored.
- the storage server information 602 is created by the a process module running within the data storage server holding the data, but other components may also be configured to generate the storage server information 602 in the key 600 .
- the storage server information 602 is generated by the data processing server, since the data processing server has identified in which data storage server the data is to be stored.
- the key 600 also includes bucket information 604 .
- the bucket information 604 comprises any type of information used to identify a group of data.
- the bucket information 604 may includes client numbers to help organize data buckets for each client.
- the bucket information 604 comprises expiration date information related to the date the portion of incoming data stored by the data storage and retrieval system will be deleted.
- the deletion information 604 may be included in the key 600 so that all data corresponding to that expiration date (e.g., 14 days from the storage date) may be deleted in one step by employing all keys with the desired expiration date to locate all the stored data corresponding to that date information.
- only one key corresponding to the desired deletion date may be used to find all the data stored on a particular date and thus ready for deletion.
- a system may then delete all data chunks within any number of data storage servers corresponding to the date contained in the deletion information 604 .
- the key 600 of FIG. 6 also includes file numbers 606 .
- the file number 606 simply identify particular files within each bucket that holds the stored data.
- offset information 608 is also illustrated in the key 600 of FIG. 6 .
- the offset information 608 comprises location information for the data stored in a data storage unit. More specifically, the offset information 608 may include information to point the data storage server to the header of the stored data in order to locate the data in the data storage units.
- offset information 608 includes the file name, which may then point to the location of the beginning of the specific data string with respect to a known location on the disk on which the data is written (i.e., the “offset”).
- the process module in a data storage server may be prompted by a data retrieval server to access the offset information 608 in the key 600 in order to locate and retrieve the specified data.
- the data processing server may be configured to generate the storage server information 602 , as well as the bucket information 604 , since the data processing server is the component making the connection with the selected data storage server.
- the data storage server associated with the data storage unit holding the data generates the file number 606 where the data is stored, and the offset information 608 .
- the list of various types of information illustrated in FIG. 6 is not exhaustive.
- FIG. 7 illustrated is yet another embodiment of a data storage and retrieval system 700 constructed according to the principles disclosed herein.
- the system 700 includes a computer network 702 , which may be a packet network such as the Internet, or may even be an internal Ethernet-based local area network (LAN). Accordingly, the system 700 is not limited for use with any particular type of computer network 702 .
- a computer network 702 may be a packet network such as the Internet, or may even be an internal Ethernet-based local area network (LAN). Accordingly, the system 700 is not limited for use with any particular type of computer network 702 .
- LAN local area network
- the system 700 of FIG. 7 also includes a sending user terminal 704 for use by a user sending data (e.g., an e-mail message) to a receiving user.
- the sending user accesses the sending terminal 704 (which may simply be a home computer) to send the data, which is sent via the computer network 702 to the receiving user accessing a receiving terminal 708 .
- the transmitted message data is not filtered, it arrives at the receiving terminal 708 via a data retrieval server 710 , which is also a data retrieval server for the intended receiver of the data.
- a DNS server 712 is coupled to the computer network 702 for translating the address information entered by the sending user to the actual IP address of the intended receiver.
- the filtering daemon in the data processing server 714 may do the filtering. Once the data is filtered, it is then stored for a predetermined period of time, in case it needs to be accessed.
- the data processing server 714 employs a second DNS server 716 to obtain the addresses of groups of available data storage servers 718 a, 718 b, 720 a, 720 b, In the illustrated embodiment, two groups are determined to be available: a first group of data storage servers 718 a, 718 b, and a second group of data storage servers 720 a, 720 b.
- the first group of data storage servers 718 a, 718 b is coupled to the computer network 702 in a parallel configuration. As a result, both of the data storage servers 718 a, 718 b are accessed directly by the data processing server 714 to store the data. Conversely, the second group of data storage servers 720 a, 720 b are coupled to the computer network 702 in a master/slave configuration.
- the data processing server 714 accesses only the master data storage servers 720 a to store the data, and the master process module associated with the master server 720 a handles the redundant storage of the data in the slave server 720 b, Accordingly, either group of data storage servers 718 a, 718 b, 720 a, 720 b may be employed for redundant storage of the data, in accordance with the various exemplary embodiments and principles disclosed herein.
- An advantage of the data storage and retrieval system 700 in FIG. 7 is the transmission of data for storage in the data storage servers 718 a, 718 b, 720 a, 720 b across the computer network 702 , rather than only across a local connection between the data processing server 714 and the data storage servers 718 a, 718 b, 720 a, 720 b, As a result, the system 700 illustrates that any or all of the data storage servers 718 a, 718 b, 720 a, 720 b used to store the data may be geographically located a great distance away from the data processing server 714 , if desired.
- the specific group sends the appropriate acknowledgements and keys back to the data processing server 714 across the computer network 702 .
- the keys may be stored in a key manager 722 , such as an Oracle® database, for use at a later time to retrieve or mass delete the stored data.
- the user may simply access his terminal 708 .
- the receiving terminal 708 which may simply be the user's home computer, then communicates with the appropriate data storage server 718 a , 718 b , 720 a , 720 b via the receiving server 724 and across the computer network 702 to retrieve the data.
- the key(s) are accessed from the key manager 722 corresponding to the specific data sought to be retrieved from the data storage units holding the data, via the appropriate one of the data storage servers 718 a , 718 b , 720 a , 720 b . More specifically, the key allows the data to be located in its specific location on a data storage unit associated with the appropriate data storage server 718 a , 718 b , 720 a , 720 b . Once the key is used to locate the data, the data is retrieved across the computer network 702 , and presented for viewing by the user at the receiving terminal 708 .
- another key corresponding to the redundant location of the data may be retrieved from the key manager 722 and used to retrieve the data from its second storage location.
- FIG. 8 illustrated is a bar graph 800 illustrating an exemplary comparison between a conventional data storage system and a data storage and retrieval system employing the present principles.
- the graph 800 illustrates the advantages with respect to the speed of writing message data for storage, as compared to conventional data storage systems.
- the displayed results may not be achieved in comparison with every type of conventional system, the graph illustrates actual results achieved by the inventors.
- the conventional system compared was a typical file storage system, such as a large EMC Symmetrix RAID storage system employing a Veritas® file system.
- the message writing capabilities of a single data storage server in a data storage and retrieval system constructed according to the principles disclosed herein far exceed the capabilities of the conventional system, especially as the write:read ratio of data increases.
- Such capabilities are partially achieved in this example by spooling the data into data blocks of predetermined size, as described above, before writing the data blocks to the data storage server in a single consecutive write.
- the writer thread in each data storage server may store the data in a single write of concatenated data streams, rather than having to create and write a new file for each stream of data to be stored, when the system experiences periods of high traffic.
- a TCP/IP connection with the data storage servers also allows quick access to the servers for fast data storage, as well as a quick and flexible configuration for expansion or reduction in the number of data storage servers employed in the system.
- multiple data storage servers, as well as the associated process modules employing the present principles may be added to the system, further increasing the speed capabilities of the system, by simply updating a DNS server, or similar component, with the TCP/IP information of the added server(s). The benefits of expanding the system as such are even more evident when the system is configured such that each data storage server performs nearly simultaneous writes with its companion data storage servers in high traffic situations.
- redundant data storage may also be achieved without excessively taxing the entire system by simply adding more data storage servers in a mirroring configuration, or connecting data storage servers in master/slave relationships.
- the speed of the system may be maintained by selecting random pairs of available data storage servers for either writing the data immediately, or spooling the data in consecutive blocks for an efficient single write when the corresponding writer thread is free.
- the system simply finds a new pair to store the data, rather than waiting for the single writer thread in a conventional database system to finish writing each chunk of data as a separate file.
- a connection with a data storage server cannot be made or maintained, the system simply finds a new pair to store the data, rather than waiting for the connection to complete, or a reconnection to occur.
Abstract
Description
Claims (10)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US10/643,489 US7584264B2 (en) | 2003-08-19 | 2003-08-19 | Data storage and retrieval systems and related methods of storing and retrieving data |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US10/643,489 US7584264B2 (en) | 2003-08-19 | 2003-08-19 | Data storage and retrieval systems and related methods of storing and retrieving data |
Publications (2)
Publication Number | Publication Date |
---|---|
US20050044170A1 US20050044170A1 (en) | 2005-02-24 |
US7584264B2 true US7584264B2 (en) | 2009-09-01 |
Family
ID=34193891
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US10/643,489 Active 2026-08-16 US7584264B2 (en) | 2003-08-19 | 2003-08-19 | Data storage and retrieval systems and related methods of storing and retrieving data |
Country Status (1)
Country | Link |
---|---|
US (1) | US7584264B2 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10311105B2 (en) | 2010-12-28 | 2019-06-04 | Microsoft Technology Licensing, Llc | Filtering queried data on data stores |
Families Citing this family (25)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20050086477A1 (en) * | 2003-10-16 | 2005-04-21 | Taiwan Semiconductor Manufacturing Co. | Integrate PGP and Lotus Notes to encrypt / decrypt email |
US7953814B1 (en) * | 2005-02-28 | 2011-05-31 | Mcafee, Inc. | Stopping and remediating outbound messaging abuse |
US7680890B1 (en) | 2004-06-22 | 2010-03-16 | Wei Lin | Fuzzy logic voting method and system for classifying e-mail using inputs from multiple spam classifiers |
US8484295B2 (en) | 2004-12-21 | 2013-07-09 | Mcafee, Inc. | Subscriber reputation filtering method for analyzing subscriber activity and detecting account misuse |
US8738708B2 (en) | 2004-12-21 | 2014-05-27 | Mcafee, Inc. | Bounce management in a trusted communication network |
US9015472B1 (en) | 2005-03-10 | 2015-04-21 | Mcafee, Inc. | Marking electronic messages to indicate human origination |
US9160755B2 (en) | 2004-12-21 | 2015-10-13 | Mcafee, Inc. | Trusted communication network |
KR20070117585A (en) * | 2005-02-24 | 2007-12-12 | 엘지전자 주식회사 | Method of configuring network profile of network system |
US7899680B2 (en) * | 2005-03-04 | 2011-03-01 | Netapp, Inc. | Storage of administrative data on a remote management device |
US8291063B2 (en) * | 2005-03-04 | 2012-10-16 | Netapp, Inc. | Method and apparatus for communicating between an agent and a remote management module in a processing system |
US7805629B2 (en) * | 2005-03-04 | 2010-09-28 | Netapp, Inc. | Protecting data transactions on an integrated circuit bus |
US8090810B1 (en) * | 2005-03-04 | 2012-01-03 | Netapp, Inc. | Configuring a remote management module in a processing system |
US7634760B1 (en) | 2005-05-23 | 2009-12-15 | Netapp, Inc. | System and method for remote execution of a debugging utility using a remote management module |
US8600948B2 (en) * | 2005-09-15 | 2013-12-03 | Emc Corporation | Avoiding duplicative storage of managed content |
US20070061359A1 (en) * | 2005-09-15 | 2007-03-15 | Emc Corporation | Organizing managed content for efficient storage and management |
JP2007316952A (en) * | 2006-05-25 | 2007-12-06 | Canon Inc | Information processor and data management method for the processor |
US7783882B2 (en) * | 2006-09-07 | 2010-08-24 | International Business Machines Corporation | Recovering remnant encrypted data on a removable storage media |
US7877603B2 (en) * | 2006-09-07 | 2011-01-25 | International Business Machines Corporation | Configuring a storage drive to communicate with encryption and key managers |
JP2008242772A (en) * | 2007-03-27 | 2008-10-09 | Toshihiro Obara | Information transmission system |
US10354229B2 (en) | 2008-08-04 | 2019-07-16 | Mcafee, Llc | Method and system for centralized contact management |
US20120066322A1 (en) * | 2010-09-15 | 2012-03-15 | Veragen, Inc. | Computer-network-based system and methodology for enhancing personal-information storage and transfer control |
US8843446B2 (en) | 2011-07-04 | 2014-09-23 | Zerto Ltd. | Methods and apparatus for time-based dynamically adjusted journaling |
US20150169609A1 (en) * | 2013-12-06 | 2015-06-18 | Zaius, Inc. | System and method for load balancing in a data storage system |
US9613127B1 (en) * | 2014-06-30 | 2017-04-04 | Quantcast Corporation | Automated load-balancing of partitions in arbitrarily imbalanced distributed mapreduce computations |
US9674046B2 (en) * | 2014-10-21 | 2017-06-06 | At&T Intellectual Property I, L.P. | Automatic detection and prevention of network overload conditions using SDN |
Citations (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5416915A (en) | 1992-12-11 | 1995-05-16 | International Business Machines Corporation | Method and system for minimizing seek affinity and enhancing write sensitivity in a DASD array |
US5557770A (en) | 1993-03-24 | 1996-09-17 | International Business Machines Corporation | Disk storage apparatus and method for converting random writes to sequential writes while retaining physical clustering on disk |
US5742668A (en) | 1994-09-19 | 1998-04-21 | Bell Communications Research, Inc. | Electronic massaging network |
US5799307A (en) | 1995-10-06 | 1998-08-25 | Callware Technologies, Inc. | Rapid storage and recall of computer storable messages by utilizing the file structure of a computer's native operating system for message database organization |
US5964886A (en) | 1998-05-12 | 1999-10-12 | Sun Microsystems, Inc. | Highly available cluster virtual disk system |
US6021408A (en) | 1996-09-12 | 2000-02-01 | Veritas Software Corp. | Methods for operating a log device |
US6167402A (en) | 1998-04-27 | 2000-12-26 | Sun Microsystems, Inc. | High performance message store |
US6230190B1 (en) * | 1998-10-09 | 2001-05-08 | Openwave Systems Inc. | Shared-everything file storage for clustered system |
US6345368B1 (en) * | 1997-03-31 | 2002-02-05 | Lsi Logic Corporation | Fault-tolerant access to storage arrays using active and quiescent storage controllers |
US6353834B1 (en) | 1996-11-14 | 2002-03-05 | Mitsubishi Electric Research Laboratories, Inc. | Log based data architecture for a transactional message queuing system |
US20030212870A1 (en) * | 2002-05-08 | 2003-11-13 | Nowakowski Steven Edmund | Method and apparatus for mirroring data stored in a mass storage system |
US7149769B2 (en) * | 2002-03-26 | 2006-12-12 | Hewlett-Packard Development Company, L.P. | System and method for multi-destination merge in a storage area network |
US7197571B2 (en) * | 2001-12-29 | 2007-03-27 | International Business Machines Corporation | System and method for improving backup performance of media and dynamic ready to transfer control mechanism |
-
2003
- 2003-08-19 US US10/643,489 patent/US7584264B2/en active Active
Patent Citations (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5416915A (en) | 1992-12-11 | 1995-05-16 | International Business Machines Corporation | Method and system for minimizing seek affinity and enhancing write sensitivity in a DASD array |
US5557770A (en) | 1993-03-24 | 1996-09-17 | International Business Machines Corporation | Disk storage apparatus and method for converting random writes to sequential writes while retaining physical clustering on disk |
US5742668A (en) | 1994-09-19 | 1998-04-21 | Bell Communications Research, Inc. | Electronic massaging network |
US5799307A (en) | 1995-10-06 | 1998-08-25 | Callware Technologies, Inc. | Rapid storage and recall of computer storable messages by utilizing the file structure of a computer's native operating system for message database organization |
US6021408A (en) | 1996-09-12 | 2000-02-01 | Veritas Software Corp. | Methods for operating a log device |
US6353834B1 (en) | 1996-11-14 | 2002-03-05 | Mitsubishi Electric Research Laboratories, Inc. | Log based data architecture for a transactional message queuing system |
US6345368B1 (en) * | 1997-03-31 | 2002-02-05 | Lsi Logic Corporation | Fault-tolerant access to storage arrays using active and quiescent storage controllers |
US6167402A (en) | 1998-04-27 | 2000-12-26 | Sun Microsystems, Inc. | High performance message store |
US5964886A (en) | 1998-05-12 | 1999-10-12 | Sun Microsystems, Inc. | Highly available cluster virtual disk system |
US6230190B1 (en) * | 1998-10-09 | 2001-05-08 | Openwave Systems Inc. | Shared-everything file storage for clustered system |
US7197571B2 (en) * | 2001-12-29 | 2007-03-27 | International Business Machines Corporation | System and method for improving backup performance of media and dynamic ready to transfer control mechanism |
US7149769B2 (en) * | 2002-03-26 | 2006-12-12 | Hewlett-Packard Development Company, L.P. | System and method for multi-destination merge in a storage area network |
US20030212870A1 (en) * | 2002-05-08 | 2003-11-13 | Nowakowski Steven Edmund | Method and apparatus for mirroring data stored in a mass storage system |
Non-Patent Citations (2)
Title |
---|
EMC News Release, "EMC Launches Centera, Ushers in New Era of Content-Addresses Storage," New York, Apr. 29, 2002. www.emc.com/news/press-releases/view.jsp?id=1254. |
Rosenblum et al., The Design and Implementation of a Log-Structured File System, ACM Transactions on Computer Systems, Feb. 1992, pp. 26-52, vol. 10 No. 1. |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10311105B2 (en) | 2010-12-28 | 2019-06-04 | Microsoft Technology Licensing, Llc | Filtering queried data on data stores |
Also Published As
Publication number | Publication date |
---|---|
US20050044170A1 (en) | 2005-02-24 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US7584264B2 (en) | Data storage and retrieval systems and related methods of storing and retrieving data | |
US6922761B2 (en) | Method and system for migrating data | |
US10708353B2 (en) | Method and system for displaying similar email messages based on message contents | |
US7805416B1 (en) | File system query and method of use | |
US10769177B1 (en) | Virtual file structure for data storage system | |
US9952918B2 (en) | Two level addressing in storage clusters | |
US7636767B2 (en) | Method and apparatus for reducing network traffic over low bandwidth links | |
EP1049989B1 (en) | Access to content addressable data over a network | |
US7720889B1 (en) | System and method for nearly in-band search indexing | |
US7904466B1 (en) | Presenting differences in a file system | |
US8135763B1 (en) | Apparatus and method for maintaining a file system index | |
US20020122543A1 (en) | System and method of indexing unique electronic mail messages and uses for the same | |
US20030140112A1 (en) | Electronic messaging system method and apparatus | |
EP3580647A1 (en) | System for storing data in tape volume containers | |
US20220035786A1 (en) | Distributed database management system with dynamically split b-tree indexes | |
US7080102B2 (en) | Method and system for migrating data while maintaining hard links | |
US6952699B2 (en) | Method and system for migrating data while maintaining access to data with use of the same pathname | |
JP3626632B2 (en) | Database access method, method and storage medium | |
AU2002240342A1 (en) | System and method of indexing unique electronic mail messages and uses for the same |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: POSTINI, INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:COX, FREDRIC L.;CARROLL, DORION A.;LUND, PETER KEVIN;AND OTHERS;REEL/FRAME:014477/0817Effective date: 20030819 |
|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:POSTINI, INC.;REEL/FRAME:022620/0513Effective date: 20090325Owner name: GOOGLE INC.,CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:POSTINI, INC.;REEL/FRAME:022620/0513Effective date: 20090325 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
FPAY | Fee payment |
Year of fee payment: 4 |
|
FPAY | Fee payment |
Year of fee payment: 8 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044101/0610Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 12TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1553); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 12 |