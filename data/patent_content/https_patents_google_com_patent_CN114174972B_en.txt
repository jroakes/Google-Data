CN114174972B - Automated assistant controlled compressed spoken utterances for complex application GUIs - Google Patents
Automated assistant controlled compressed spoken utterances for complex application GUIs Download PDFInfo
- Publication number
- CN114174972B CN114174972B CN201980098212.9A CN201980098212A CN114174972B CN 114174972 B CN114174972 B CN 114174972B CN 201980098212 A CN201980098212 A CN 201980098212A CN 114174972 B CN114174972 B CN 114174972B
- Authority
- CN
- China
- Prior art keywords
- gui
- application
- spoken utterance
- natural language
- user
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000000034 method Methods 0.000 claims abstract description 93
- 230000009471 action Effects 0.000 claims description 60
- 230000000875 corresponding effect Effects 0.000 claims description 45
- 230000003993 interaction Effects 0.000 claims description 12
- 238000004590 computer program Methods 0.000 claims 1
- 230000008569 process Effects 0.000 abstract description 14
- 230000006835 compression Effects 0.000 abstract description 4
- 238000007906 compression Methods 0.000 abstract description 4
- 230000004044 response Effects 0.000 description 36
- 230000008859 change Effects 0.000 description 11
- 230000000153 supplemental effect Effects 0.000 description 10
- 230000001276 controlling effect Effects 0.000 description 7
- 238000010801 machine learning Methods 0.000 description 6
- 239000000463 material Substances 0.000 description 5
- 230000004048 modification Effects 0.000 description 5
- 238000012986 modification Methods 0.000 description 5
- 230000003068 static effect Effects 0.000 description 5
- 239000003795 chemical substances by application Substances 0.000 description 4
- 239000003086 colorant Substances 0.000 description 4
- 230000006870 function Effects 0.000 description 4
- 230000015654 memory Effects 0.000 description 4
- 230000000007 visual effect Effects 0.000 description 4
- 230000000694 effects Effects 0.000 description 3
- 230000002093 peripheral effect Effects 0.000 description 3
- 238000004891 communication Methods 0.000 description 2
- 238000010586 diagram Methods 0.000 description 2
- 230000002452 interceptive effect Effects 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 230000007704 transition Effects 0.000 description 2
- 230000008901 benefit Effects 0.000 description 1
- 238000004040 coloring Methods 0.000 description 1
- 230000003111 delayed effect Effects 0.000 description 1
- 125000001475 halogen functional group Chemical group 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000003062 neural network model Methods 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 230000011273 social behavior Effects 0.000 description 1
- 238000013179 statistical model Methods 0.000 description 1
- 239000002699 waste material Substances 0.000 description 1
Abstract
Implementations set forth herein relate to an automated assistant that can control Graphical User Interface (GUI) elements via voice input using natural language understanding of GUI content to address ambiguity and allow compression of GUI voice input requests. When a user is accessing an application that presents various GUI elements at a display interface, the automated assistant may operate to process actionable data corresponding to the GUI elements. The operational data may be processed to determine a correspondence between a GUI voice input request to the automated assistant and at least one of the GUI elements presented at the display interface. Upon determining that a particular spoken utterance from a user corresponds to a plurality of GUI elements, an unclear indication may be presented at a display interface in order to encourage the user to provide a more specific spoken utterance.
Description
Background
Humans may engage in human-machine conversations using an interactive software application referred to herein as an "automated assistant" (also referred to as a "digital agent," "chat robot," "interactive personal assistant," "intelligent personal assistant," "conversational agent," etc.). For example, humans (which may be referred to as "users" when they interact with an automated assistant) may provide commands and/or requests using spoken natural language input (i.e., utterances) and/or by providing text (e.g., typed) natural language input, which in some cases may be converted to text and then processed.
When a user interacts with an automated assistant and an application (e.g., a web browser) simultaneously, input to the application may cause an interruption to an action performed by the automated assistant, and vice versa. This may, for example, result in the need for the computing device on which the automated assistant and application are being implemented to restart from scratch certain actions that have been partially performed prior to the interrupt. This may impose an additional burden on the computing device. For example, the device may be required to detect and process repeated spoken utterances related to actions after the interruption, and may be required to process such repeated utterances and/or other repeated inputs to the device (e.g., screen inputs) in order to complete the actions after the interruption.
Disclosure of Invention
Implementations set forth herein relate to an automated assistant that can mediate interactions between a user and GUI elements of an application, and also allow the user to provide compressed voice input to control complex application GUIs. When such a complex application GUI is not controllable via compressed speech input, the user may need to perform various different touch gestures in order to control the various GUI elements presented at the application GUI. However, when such a complex application GUI is controllable via compressed speech input, as facilitated by the implementations discussed herein, interactions between a user and the application GUI may be made more efficient for the computing device on which the application is implemented. Such efficiency may result from reduced power consumption and processing bandwidth availability during the interaction.
In order to allow a user to provide compressed spoken utterances for controlling a complex application GUI, content description data for static and/or dynamic GUI elements may be provided to applications corresponding to the complex application GUI. The content description data may be stored at least partially in association with each GUI of the static and/or dynamic GUI elements to guide the automated assistant in responding to a spoken utterance intended to control a particular GUI element. The automated assistant can compare the natural language content of the spoken utterance with the content description data to identify one or more GUI elements that the user may be intending to control via the spoken utterance. When the automated assistant identifies a particular GUI element that the user is intending to control, the automated assistant may use the natural language content of the spoken utterance and/or data corresponding to the particular GUI element to identify an action to be initialized in response to the spoken utterance from the user.
For example, when a GUI element (e.g., a calendar for creating an event) is configured to correlate user input with a value (e.g., 1 month) selectable from a plurality of values (e.g., 1-12 months), the automated assistant can compare the natural language content of the spoken utterance with the plurality of values and/or any other information associated with the GUI element. When the automated assistant determines that the natural language content of the spoken utterance identifies a particular value of a plurality of values associated with the GUI element, the automated assistant may then identify one or more actions that are capable of controlling the GUI element to reflect the particular value identified by the user. For example, when the user has provided a spoken utterance such as "Assistant, SET THE DATE to 'July' (Assistant, set date to" 7 months ")", the automated Assistant may determine that the GUI element executing the application has corresponding content description data identifying "July (7 months)". In response, the action "setDate ()" can be identified by the automated assistant and initialized via the executing application. The action "setDate ('July')" may be initialized to cause the executing application to (i) present the GUI element in a manner reflecting the selection of "July" as the selected month, and (ii) modify the draft calendar event to be set for the month of 7 months.
In some implementations, in response to the user providing the spoken utterance, the automated assistant can cause a display panel of the computing device to present a graphical indicator indicating a selection of a particular GUI element. For example, in response to receiving the spoken utterance "Assistant," SET THE DATE to 'July' "and identifying a particular GUI element, the automated Assistant can cause the display panel to present a graphical indicator at and/or near the particular GUI element. The graphical indicator may be, but is not limited to, "halo", a pattern, a shape, and/or other coloring of a portion of the application GUI at or near a particular GUI element.
In some implementations, the automated assistant can enter the GUI control mode in response to the automated assistant determining that the user has provided a spoken utterance to facilitate control of the application GUI. When operating in the GUI control mode, the automated Assistant may respond to a spoken utterance from the user without the spoken utterance having to include a invocation phrase such as "Assistant". Alternatively or additionally, when operating in the GUI control mode, the automated assistant may respond to a compressed spoken utterance that does not explicitly identify an action of the automated assistant. Rather, in some implementations, the automated assistant can be responsive to any spoken utterance that has an association with one or more GUI elements presented at the application GUI when the user provided the particular spoken utterance.
As an example of a GUI control mode, after the automated Assistant receives the spoken utterance "Assistant, SET THE DATE to ' July '", the automated Assistant may receive a subsequent spoken utterance, such as "Also, the 15th at 8O'clock PM (again, 8 pm)". After the user has caused the automated assistant to select month of July for the event, the user may provide a subsequent spoken utterance to facilitate selecting a time for the event. In response to receiving the initial spoken utterance, "Assistant, SET THE DATE to 'July'", the automated Assistant can transition to operate in the GUI control mode, thereby making subsequent interactions between the user and the automated Assistant more efficient. Subsequent spoken utterances from the user may be processed according to the GUI control mode to facilitate determining whether the subsequent spoken utterances are directed to the application GUI selection-although the user did not explicitly invoke the automated assistant in the spoken utterances. Further, in the event of a pre-permission from the user, the subsequent spoken utterance may be processed to determine a correlation between natural language content of the subsequent spoken utterance and content description data associated with the application GUI element. For example, the subsequent spoken utterance "Also, the 15th at 8O'clock PM" may be determined to be related to a GUI element that identifies a calendar day and another GUI element that identifies hours and minutes of an event that the user is setting.
When one or more items in the natural language content correspond to unique GUI elements, the automated assistant may responsively initialize specific actions for affecting each unique GUI element. However, when one or more items in the natural language content correspond to multiple GUI elements, the automated assistant can cause a graphical indicator to be presented at the application GUI to encourage the user to resolve the ambiguity of his spoken utterance. The graphical indicator may inform the user that their spoken utterance has been interpreted as corresponding to a plurality of GUI elements, and also inform the user that no action has been taken in response to the spoken utterance. After confirming or viewing the graphical indicator, the user may provide a different spoken utterance that causes the automated assistant to notice the particular GUI element that the user originally intended to modify.
As an example, the application GUI may include a "start time" of an event and an "end time" of an event, and each time may be selectable by a user. However, each Time may have a corresponding "Date" field and "Time" field, and thus, in response to the user providing a subsequent spoken utterance, the automated assistant may identify both the "Date" field and the "Time" field of the "start Time" and the "end Time" of the event. To identify the particular field that the user is intending to affect a subsequent spoken utterance, the automated assistant may cause a graphical indicator to be presented at or near each field that has been identified based on the subsequent spoken utterance. In some implementations, the user may resolve the ambiguity by providing one or more other spoken utterances that more specifically identify the field the user is intending to affect. For example, when the start Time "field and the end Time" field are graphically indicated Fu Gaoliang, the user may provide another compressed spoken utterance, such as "start", in order to select the start Time "field without having to provide the invocation phrase (e.g.," Assistant … ") again.
Additionally or alternatively, when the start Time "field and the end Time" field are placed adjacent to each other, the user may provide another compressed spoken utterance that indicates a portion of the application GUI where the desired field is located. As an example, the user may provide another spoken utterance, such as "left" or "up", in order to indicate the position of the intended field to be affected, at least with respect to the fields that are not intended to be affected. In response to receiving another spoken utterance indicating a relative position of an intended field, the automated assistant may identify the intended field and then perform a selection of a value provided at the intended field. For example, the automated assistant may select a start Time "field as the field to be modified to indicate the selection of" 8 o' clock PM (8 PM) ".
Additionally or alternatively, because the second spoken utterance (e.g., "Also, the 15th at 8 O'clock PM") includes multiple choices (e.g., 15th and 8 pm), the automated assistant can affect multiple GUI elements based on the direction-related spoken utterance (e.g., "left" or "up"). For example, in response to receiving a spoken utterance relating to a direction, the automated assistant may modify a start Time "field to identify" 8 pm "and a start Time" date "field to identify" 15 "based at least on the start Time field being" left "or" up "of the end Time field. In this way, the user need not provide a subsequent spoken utterance after the direction-related spoken utterance, but rather can recall and act upon a previously provided compressed spoken utterance depending on the automated assistant.
The above description is provided as an overview of some implementations of the present disclosure. Further descriptions of these and other implementations are described in more detail below.
Other implementations may include a non-transitory computer-readable storage medium storing instructions executable by one or more processors (e.g., a Central Processing Unit (CPU), a Graphics Processing Unit (GPU), and/or a Tensor Processing Unit (TPU)) to perform a method such as one or more of the methods described above and/or elsewhere herein. Other implementations may also include a system of one or more computers including one or more processors operable to execute stored instructions to perform methods such as one or more of the methods described above and/or elsewhere herein.
It should be understood that all combinations of the foregoing concepts and additional concepts described in more detail herein are considered a part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are considered part of the subject matter disclosed herein.
Drawings
Fig. 1A, 1B, 1C, and 1D illustrate views of a user controlling one or more applications using compressed spoken utterances related to content description data.
Fig. 2 illustrates a system for providing an automated assistant that can control Graphical User Interface (GUI) elements via voice input using content description data and/or Natural Language Understanding (NLU) of the GUI content in order to resolve ambiguities and/or allow compression of GUI voice input requests.
Fig. 3 illustrates a method for allowing a user to provide compressed spoken utterances in order to control an application graphical user interface that may otherwise be controllable via one or more your only inputs.
FIG. 4 is a block diagram of an example computer system.
Detailed Description
Fig. 1A, 1B, 1C, and 1D illustrate views 100, 140, 150, and 160, respectively, of a user 102 using compressed spoken utterances related to content description data 124 to control one or more applications. In particular, the user 102 can provide the spoken utterance 118 to initialize an automated assistant to facilitate control of one or more application GUI elements presented at the display device 138 of the computing device 104. For example, the user 102 may access the thermostat application 110 and the alarm system application 108 via the computing device 104. The computing device 104 may provide access to an automated assistant 130, which automated assistant 130 may control the thermostat application 110 and the alert system application 108 according to spoken utterances from the user 102.
To determine whether the user 102 is providing a spoken utterance to facilitate control of one or more applications 128, the automation assistant 130 and/or the content correlation engine 126 may compare the content description data 124 to natural language content of the received spoken utterance. The content description data 124 may include information characterizing one or more attributes of one or more GUI elements that may be presented by the application 128. The GUI elements may include one or more of the following: an image, an input field, a menu, a submenu, a link, a button, a check box, a switch, a calendar, an index (e.g., page index), a slider, a carousel, a notification (e.g., pop-up message), a progress indicator, a menu element, a submenu element, a plurality of menu elements, and/or any other GUI element that may be provided at a GUI of a computing device. When presenting the particular GUI element of the application 128 on the display device 138, the information provided with the content description data 124 may be omitted from the display device 138 or presented on the display device 138. For example, the content description data 124 corresponding to the element set 112 may include items such as "On", "Off", and "Auto". However, these items may or may not be presented at the display device 138 with the element set 112. Alternatively or additionally, another set of elements 114 may include a current temperature reading (e.g., 65 degrees) of the user's home and a level controllable scroll bar indicating the temperature setting of the thermostat application 110. Content description data 124 corresponding to the other element sets 114 may include items such as "degrees," "Fahrenheit," "Celsius," and/or any other items that may be associated with adjustment of temperature.
To enable control of the thermostat application 110 and/or the alert system application 108 via compression of the spoken utterance, correlations between the content of the spoken utterance 118 can be compared to the content description data 124. For example, the user 102 may provide a natural language content spoken utterance 118 that may include, for example, "Assistant,72degrees (Assistant, 72 degrees)", and after the user 102 pauses for a few seconds, the user 102 may provide, for example, "1". 6.1.8.0.stay.a.c.; 6..1..8. 0.Stay @ is performed. In response to receiving the initial spoken utterance, the automated assistant 130 may determine that the user 102 intends to control one or more applications of the computing device 104, as indicated by state 116. Based on this determination, the automated assistant 130 may transition to operate in the GUI control mode. When operating in the GUI control mode, the user 102 may omit call items and/or phrases, such as "Assistant," from subsequent spoken utterances while controlling one or more GUI elements via the automated Assistant 130.
In some implementations, the user 102 can compare the natural language content of the spoken utterance 118 with the content description data 124 in order to identify a particular GUI element that the user 102 intends to control. The initial spoken utterance may include the terms "72" and "degrees (degrees)", which may correspond to the term "degrees" provided by the content description data 124 in association with the other set of elements 114. An initial spoken utterance may be provided without explicit recognition of the action to be performed. For example, the items "modification," "set," and/or "change" may be omitted from the initial spoken utterance, while still allowing the user 102 to control the automated assistant 130 to effect modification of the settings of the application 128. When the automated assistant 130 receives the initial spoken utterance "… 72: 72degrees (… degrees)", the automated assistant 130 can identify another set of elements 114 based on a correlation between the item "degrees" of the initial spoken utterance and the item "degrees" provided in the content description data 124 in association with the other set of elements 114.
In some implementations, the correlation between the natural language content of the spoken utterance and the GUI element can be determined based on one or more processes performed under the direction of the computing device 104. For example, in some implementations, a correlation between the natural language content of the spoken utterance and the GUI element may be identified when the natural language content of the spoken utterance shares one or more items, phonemes, words, and/or phrases with one or more items, phonemes, words, and/or phrases associated with the GUI element and/or previously provided for controlling one or more inputs of the GUI element and/or similar GUI element. As an example, one or more inputs may have been previously provided by one or more other users to control and/or initialize actions associated with GUI elements of an application. In some implementations, data characterizing one or more inputs, such as, but not limited to, search strings and/or other natural language inputs, may be used to train one or more machine learning models. The resulting trained machine learning model may thereafter be used to determine appropriate actions and/or GUI elements to control in response to spoken utterances from the user. In some implementations, one or more trained machine learning models can be trained based on interactions between one or more users and website domains and/or interactions between one or more users and a set of website domains.
Additionally or alternatively, one or more trained machine learning models can be trained using website data that indicates the location of particular GUI elements within a web page. For example, computer code for one or more websites may be arranged in a hierarchy (e.g., document object model) that may indicate the relative positions of GUI elements with respect to other GUI elements and/or features of a web page. A machine learning model trained from such information may be used to identify particular GUI elements that a user may be intending to control via spoken utterances provided to an automated assistant. For example, metadata and/or other structured marking data stored in association with a particular GUI element may be used to identify another GUI element that is also associated with similar metadata and/or structured marking data.
In some implementations, the automated assistant 130 and/or the content correlation engine 126 can identify parameters and/or slot values corresponding to the natural language content of the initial spoken utterance 118. For example, the automated assistant 130 may determine that the other element set 114 has a slot value of "65" that allows the user to modify via GUI input and another slot value of "65" that does not allow the user to modify via GUI input. Thus, based on permissions associated with other element sets 114, the automation assistant 130 can identify actions to initialize to facilitate modifying the slot value "65" to "72. In some implementations, based on the identified action (e.g., application_Int (modification_set_temp [ set, current, FAHRENHEIT ])), the automation assistant 130 can generate assistant data 120 that characterizes Application input data to be provided by the automation assistant 130 to the thermostat Application 110. In some implementations, the application input data may characterize one or more gestures (e.g., swipe gestures) that would enable the requested modification of the set temperature from 65 degrees to 72 degrees if the user 102 provided the one or more gestures to the display device 138. For example, the automation assistant 130 may generate application Input data, such as "Gesture _input (touch_display (start (17, 28), end (24, 28)))", which may be provided to the thermostat application 110 from the automation assistant 130, and/or to the operating system of the computing device 104 from the automation assistant 130, to the thermostat application 110. If the automated assistant 130 and/or the content correlation engine 126 does not identify parameters and/or slot values corresponding to the natural language content of the initial spoken utterance 118, the input data mentioned above is not generated. The automated assistant may, for example, avoid generating input data.
When the automated assistant 130 operates in the GUI control mode, the user 102 can provide a subsequent spoken utterance 118 to facilitate control of another application, such as an alert system application. In particular, the user 102 may provide a numeric code to be entered via a numeric keypad GUI element at the alarm system application 108 (e.g., "1..6..1..8..0 …") and mode values (e.g., "stay"), in order to modify another GUI element that allows switching between modes, as depicted in fig. 1A. In response, and as depicted in view 140 of fig. 1B, state 132 of computing device 104 may indicate that the alert system has been set to enter "status" as per the user. Unlocking of the alert system and changing of the mode may be performed in response to not explicitly identifying the automated assistant 130 and/or the subsequent spoken utterance 118 that includes the invocation phrase.
In some implementations, the content description data 124 can be generated by the automated assistant 130 using an image recognition process for one or more screenshots. The screen shots may be captured via the automation assistant 130 and/or another application, and may include GUI elements of the thermostat application 110 and other GUI elements of the alarm system application 108. In some implementations, the data generated from the image recognition process can be used in conjunction with content description data 124 provided by a third party entity to identify correspondence between the content of the spoken utterance being provided by the user 102 and one or more GUI elements that the user intends to control.
In some implementations, the content description data 124 may be an Application Programming Interface (API) that provides the user with the ability to provide spoken utterances to the automated assistant in order to initialize application actions that may otherwise be initialized via one or more gestures to an application GUI provided by the application. In other words, the API may "point" from an action initiated via a particular GUI element to one or more phonemes, items, and/or phrases that may be included in the spoken utterance to the automated assistant to cause the action to be initiated by the application.
In some implementations, the content description data 124 may indicate the status of each GUI element of each application 128, and thus may identify GUI elements that are in foreground, background, minimized, active, inactive, and/or in any other operational state. In this way, when the user 102 realizes that a particular application with a particular GUI element is active but not in the foreground, the user 102 may still control the particular GUI element via the compressed spoken utterance. For example, the thermostat application 110 may overlay the numeric keypad GUI elements of the alarm system application 108, but when the content description data 124 indicates that the numeric keypad GUI elements are active but in the background, the user 102 may still control the numeric keypad GUI elements via the subsequent spoken utterance 118.
In some implementations, the automated assistant 130 can cause one or more graphical indicators to be presented at or near each GUI element that the user 102 has identified. The graphical indicator may inform the user 102 that the automated assistant 130 has been identified as GUI elements of the subject matter of one or more spoken utterances from the user 102. For example, based on comparing the content of the spoken utterance 118 with the content description data 124, the automated assistant 130 can cause a first graphical indicator 144 (e.g., highlighting and/or other graphical patterns surrounding a horizontal scroll bar) and a second graphical indicator 142 (e.g., highlighting and/or other graphical patterns surrounding a numeric keypad GUI element) to be presented at the display device 138. These graphical indicators may provide a visual indication that the automated assistant 130 has identified via spoken utterances by the user 102 as targeted GUI elements.
Fig. 1C illustrates a view 150 of the automated assistant 130 identifying a plurality of different GUI elements in response to a compressed spoken utterance 158 from the user 102. In particular, when the user 102 provides the compressed spoken utterance 158"off," the automated assistant 130 can operate in a GUI control mode. In response to receiving the spoken utterance 158, the automated assistant 130 and/or the content correlation engine 126 may determine that the spoken utterance 158 corresponds to a plurality of different entries in the content description data 124. For example, the automated assistant 130 can determine that the term "Off" corresponds to an entry for the "heat" switch of the GUI element collection 112, the "fan" switch of the GUI element 112, and the "Off" switch provided at the alert system application 108.
In some implementations, the automation assistant 130 can identify entries in the content description data 124 that correspond to GUI elements presented in the foreground of the display device 138 and other GUI elements not presented in the foreground of the display device 138. Additionally or alternatively, the automated assistant 130 can determine that at least some GUI elements correspond to the spoken utterance 158, although the natural language content presented at or near the GUI elements does not include the same content from the spoken utterance 158. As an example, the automation assistant 130 and/or the content correlation engine 126 may identify the "heat" switch and the "fan" switch as having an entry in the content description data 124 that includes the term "off. Further, the automated assistant 130 may determine that one or more executing applications are presenting the item "off" at or near a particular user-controllable GUI element. For example, the automated assistant 130 may determine that the alert system application 108 is presenting the item "off" in association with a user-controllable status switch (e.g., a switch having a position such as "stand", "off" and "away"). Based on this determination, the automated assistant 130 can cause the display device 138 to present a first graphical indicator 152 at or near the "heat" switch, a second graphical indicator 154 at or near the "fan" switch, and a third graphical indicator 156 at or near the user-controllable status switch.
As provided in view 160 of fig. 1D, the user 102 may provide a subsequent spoken utterance 162 to assist the automated assistant 130 in further identifying particular GUI elements that the user intends to control. In particular, in response to the user 102 providing the subsequent spoken utterance 162 while the graphical indicator is presented at the display device 138, the automated assistant 130 can compare the content of the subsequent spoken utterance 162 with the content of the entry in the content description data 124 that corresponds to the graphical indicator. When the automated assistant 130 determines that the content of the spoken utterance 162 corresponds to an entry in the content description data 124 that has been associated with a graphical indicator, the automated assistant 130 can identify one or more actions for fulfilling the spoken utterance 158 from the user 102 and subsequent spoken utterances 162. In particular, the automated assistant 130 can identify an action to change the state 164 of the "heat" switch to an "off" position based on the content of both the spoken utterance 158 and the subsequent spoken utterance 162. In this way, the user 102 does not have to repeatedly invoke a phrase to keep the automated assistant 130 active, which may otherwise result in delayed execution of the requested action and waste of certain computing resources. Further, the user 102 may provide more compressed spoken utterances for interaction with the graphical user interface.
In some implementations, the attributes of each graphical indicator may be stored as auxiliary data 120 and may indicate the location of each graphical indicator relative to each other. In this way, the user 102 may provide a simpler spoken utterance to identify a particular GUI element that the user 102 is intending to control. For example, to identify the "heat" switch in fig. 1C, the user 102 may have provided a subsequent spoken utterance, such as "left", to identify a controllable GUI element having a graphical indicator in the leftmost portion of the display device 138.
Fig. 2 illustrates a system 200 for providing an automated assistant 204, the automated assistant 204 can control Graphical User Interface (GUI) elements via voice input by using Natural Language Understanding (NLU) of GUI content to resolve ambiguities and/or allow compression of GUI voice input requests. The automated assistant 204 may operate as part of an assistant application provided at one or more computing devices, such as the computing device 202 and/or a server device. The user may interact with the automated assistant 204 via an assistant interface 220, and the assistant interface 220 may be a microphone, a camera, a touch screen display, a user interface, and/or any other device capable of providing an interface between the user and an application. For example, a user may initialize the automated assistant 204 by providing language input, text input, and/or graphical input to the assistant interface 220 to cause the automated assistant 204 to perform functions (e.g., provide data, control peripherals, access agents, generate inputs and/or outputs, etc.). The computing device 202 may include a display device, which may be a display panel that includes a touch interface for receiving touch input and/or gestures for allowing a user to control the application 234 of the computing device 202 via the touch interface. In some implementations, the computing device 202 may lack a display device, providing audible user interface output, rather than graphical user interface output. In addition, the computing device 202 may provide a user interface, such as a microphone, for receiving spoken natural language input from a user. In some implementations, the computing device 202 may include a touch interface and may not have a camera and/or microphone, but may optionally include one or more other sensors.
The computing device 202 and/or other third party client devices may communicate with the server device over a network, such as the internet. In addition, computing device 202 and any other computing devices may communicate with each other over a Local Area Network (LAN), such as a Wi-Fi network. Computing device 202 may offload computing tasks to a server device in order to save computing resources at computing device 202. For example, the server device may host the automated assistant 204, and/or the computing device 202 may transmit input received at the one or more assistant interfaces 220 to the server device. However, in some implementations, the automated assistant 204 may be hosted at the computing device 202 and various processes that may be associated with automated assistant operations may be performed at the computing device 202.
In various implementations, all or less than all aspects of the automated assistant 204 may be implemented on the computing device 202. In some of these implementations, aspects of the automated assistant 204 are implemented via the computing device 202, and may interface with a server device, which may implement other aspects of the automated assistant 204. The server device may optionally serve multiple users and their associated assistant applications via multiple threads. In implementations in which all or less than all aspects of the automated assistant 204 are implemented via the computing device 202, the automated assistant 204 may be an application separate from (e.g., installed on top of) the operating system of the computing device 202—or alternatively may be implemented directly by (e.g., as an application considered to be integral with) the operating system of the computing device 202.
In some implementations, the automation assistant 204 can include an input processing engine 206, and the input processing engine 206 can employ a plurality of different modules to process input and/or output of the computing device 202 and/or server device. For example, input processing engine 206 may include a speech processing engine 208, which speech processing engine 208 may process audio data received at auxiliary interface 220 to identify text embodied in the audio data. The audio data may be transmitted, for example, from the computing device 202 to a server device in order to conserve computing resources at the computing device 202. Additionally or alternatively, the audio data may be processed exclusively at the computing device 202.
The process for converting audio data to text may include a speech recognition algorithm that may employ a neural network and/or statistical models for identifying groups of audio data corresponding to words or phrases. Text converted from the audio data may be parsed by the data parsing engine 210 and made available to the automated assistant 204 as text data that may be used to generate and/or identify command phrases, intents, actions, slot values, and/or any other content specified by the user. In some implementations, the output data provided by the data parsing engine 210 may be provided to the parameter engine 212 to determine whether the user provides input corresponding to a particular intent, action, and/or routine that can be performed by the automation assistant 204 and/or an application or agent that can be accessed via the automation assistant 204. For example, the assistant data 238 can be stored at the server device and/or the computing device 202 and can include data defining one or more actions that can be performed by the automated assistant 204 and parameters necessary to perform the actions. The parameter engine 212 may generate one or more parameters for intent, action, and/or slot values and provide the one or more parameters to the output generation engine 214. The output generation engine 214 may use one or more parameters to communicate with the assistant interface 220 for providing output to a user and/or with one or more applications 234 for providing output to one or more applications 234.
In some implementations, the automated assistant 204 may be an application that may be installed on top of the operating system of the computing device 202 and/or may itself form part (or whole) of the operating system of the computing device 202. Automated assistant applications include on-device speech recognition, on-device natural language understanding, and on-device fulfillment and/or have access thereto. For example, on-device speech recognition may be performed using an on-device speech recognition module that processes audio data (detected by a microphone) using an end-to-end speech recognition machine learning model stored locally at computing device 202. On-device speech recognition generates recognized text for spoken utterances (if any) present in the audio data. Further, on-device Natural Language Understanding (NLU) may be performed using, for example, an on-device NLU module that processes recognition text generated using on-device speech recognition, the generated NLU data, and optional context data. The NLU data may include an intent corresponding to the spoken utterance and optionally parameters (e.g., slot values) for the intent.
On-device fulfillment may be performed using an on-device fulfillment module that utilizes NLU data (from an on-device NLU) and optionally other local data to determine actions to be taken to parse the intent of the spoken utterance (and optionally parameters for the intent). This may include determining local and/or remote responses (e.g., answers) to the spoken utterance, interactions with locally installed applications performed based on the spoken utterance, commands transmitted to internet of things (IoT) devices based on the spoken utterance (either directly or via a corresponding remote system), and/or other resolution actions performed based on the spoken utterance. The on-device fulfillment may then initiate local and/or remote performance/execution of the determined action to address the spoken utterance.
In various implementations, at least remote speech processing, remote NLU, and/or remote fulfillment may be selectively utilized. For example, the recognized text may be at least optionally transmitted to a remote automation assistant component for remote NLU and/or remote fulfillment. For example, the recognized text may optionally be transmitted for remote performance in parallel with on-device performance, or in response to an on-device NLU and/or on-device performance failure. However, on-device speech processing, on-device NLU, on-device fulfillment, and/or on-device execution may be prioritized at least because of the reduced latency they provide in resolving the spoken utterance (because a client-server round trip is not required to resolve the spoken utterance). Furthermore, the on-device functions may be the only functions available without or with limited network connectivity.
In some implementations, the computing device 202 may include one or more applications 234, which one or more applications 234 may be provided by a third party entity that is different from the entity providing the computing device 202 and/or the automated assistant 204. The automation assistant 204 and/or the computing device 202 may access the application data 230 to determine one or more actions that can be performed by the one or more applications 234, as well as a state of each of the one or more applications 234. Further, the application data 230 and/or any other data (e.g., the device data 232) may be accessed by the automation assistant 204 to generate context data that may characterize the context in which the particular application 234 is executing at the computing device 202 and/or the context in which the particular user is accessing the computing device 202.
When one or more applications 234 are executing at the computing device 202, the device data 232 may characterize the current operating state of each application 234 executing at the computing device 202. Further, the application data 230 may characterize one or more features of the executing application 234, such as content of one or more graphical user interfaces presented under the direction of the one or more applications 234. Alternatively or additionally, the application data 230 may characterize an action pattern that may be updated by the respective application and/or the automation assistant 204 based on the current operating state of the respective application. Alternatively or additionally, one or more action patterns for one or more applications 234 may remain static, but may be accessed by the automated assistant 204 to determine appropriate actions to initialize via the automated assistant 204.
In some implementations, the computing device 202 can store content description data 236, which content description data 236 can characterize various GUI elements corresponding to one or more different applications 234. For example, when a particular application 234 is installed at the computing device 202 and/or otherwise accessed by the computing device 202, the computing device 202 may download content description data 236 for the particular application 234. The automation assistant 204 can access the content description data 236 to identify one or more GUI elements that a particular user is intending to control via a spoken utterance. Each portion of the content description data 236 corresponding to a particular GUI element of the application 234 may be static or dynamic. For example, when the content description data 236 is dynamic, the content description data 236 may be updated according to inputs to the computing device 202 and/or outputs from the computing device 202. For example, when a user provides a spoken utterance for further modification of a particular GUI element, and the automation assistant 204 causes a change to the particular graphical user interface element, content description data corresponding to the particular GUI element may be modified by the corresponding application 234, the computing device 202, and/or the automation assistant 204 to reflect the change to the particular GUI element.
The computing device 202 and/or the automated assistant 204 may also include a content correlation engine 260. The content correlation engine 216 may process the input natural language content from the user to determine whether the natural language content corresponds to one or more GUI elements. When one or more items of natural language content are provided in a particular entry in the content description data 236, a correlation between the natural language content of the spoken utterance and the particular entry in the content description data 236 can be identified. Alternatively or additionally, when one or more items of natural language content are determined to be synonymous with one or more items in a particular item of content description data 236, a correlation between the natural language content of the outlet header word and the particular item in content description data 236 may be identified. Alternatively or additionally, when one or more items of natural language content characterize a particular feature of a GUI element that an entry of the content description data 236 also characterizes, a correlation between the natural language content of the outlet header utterance and the particular entry in the content description data 236 may be identified.
For example, when the application GUI of a particular application 234 is presenting a dial GUI element that simulates volume dialing of stereo, the content description data 236 corresponding to the dial GUI element may include one or more items such as "turn", "clockwise", and/or "counter-clockwise". Thus, when the user provides a spoken utterance, such as "turn up" while simultaneously presenting an application GUI at the display panel of the computing device 202, the automated assistant 204 can determine that a correlation exists between the item "turn" of content of the spoken utterance and the item "turn" of content description data 236. In some implementations, based on determining the correlation, the automated assistant 204 can identify a current value corresponding to the GUI element and cause the value to be modified based on the spoken utterance provided by the user. For example, when a dial GUI element is presented in an intermediate position to indicate a 50% value, the automated assistant 204 may increment the value based on the user providing the custom utterance "turn up". The initial value of 50% may be identified by the content description data 236, the application data 230, and/or other data characterizing the value at the graphical user interface of the computing device 202.
For example, in some implementations, the automation assistant 204 can employ the supplemental content engine 218 to generate additional content description data based on features of the computing device 202 and/or information provided during interactions between the user and the automation assistant 204. The supplemental content engine 218 may process image data corresponding to a screenshot of a display interface of the computing device 202 to generate additional content description data. When the display interface is to present content from a plurality of different applications 234, the automation assistant 204 can use the content description data 236 for each of the applications 234 to correlate each portion of the presented content with each respective application 234. This may allow the automated assistant to receive spoken utterances directed to a plurality of different applications that are simultaneously presented and/or otherwise represented at the display interface of the computing device 202.
In some implementations, to initialize one or more actions to be performed by one or more applications 234, computing device 202 and/or automation assistant 204 can include response data engine 222. The response data engine 222 may use the content description data 236 to generate response data that may be used to initialize one or more actions to be performed by the one or more applications 234. For example, in response to receiving a spoken utterance, the automation assistant 204 can identify application-specific GUI elements that the user is intending to modify via the spoken utterance. In some implementations, while one or more user-controllable GUI elements are presented at a display interface in communication with the computing device 202, the GUI control mode engine 224 can cause the automation assistant 204 to operate in a GUI control mode in response to receiving the spoken utterance. The automation assistant 204 may use the content correlation engine 216 to identify content description data 236 corresponding to a particular application GUI element. A portion of the content description data 236 associated with the content of the spoken utterance may be accessed by the response data engine 222 to generate input data operable by a particular application corresponding to the application GUI element by the response data engine 222. In the event that the automation assistant 204 does not identify the content description data 236 corresponding to a particular GUI element, the input data may not be generated.
As an example, the response data engine 222 may generate gesture data that characterizes one or more gestures that may implement one or more changes intended to be set forth by the user via the spoken utterance. For example, when the application GUI element is a rotatable dial icon, the response data engine 222 may generate data characterizing a rotation gesture, similar to a user "turning" the rotatable dial icon at a touch display interface of the computing device 202 using one or more limbs. Alternatively, when the application GUI element is a scroll bar and the relevant portion of the content description data 236 indicates that the scroll bar is at the lowest position on the display interface, the response data engine 222 may generate data characterizing a gesture to move the scroll bar from the lowest position to a position identified by the user via the spoken utterance. For example, when the user provides the spoken utterance "up," the automated assistant may cause the response data engine 222 to generate a default value that corresponds to a default distance from the current location of the scrollbar to a particular location above the current location.
In some implementations, the supplemental content engine 218 can generate additional content description data based on one or more spoken utterances from the user to provide further context for subsequent spoken utterances provided by the user to further manipulate the application GUI. As an example, a user may interact with an application 234 corresponding to an application 234 controlling a thermostat of the user's home. The application GUI may include a text field for entering the temperature at which the thermostat will also be set. Initially, the user can provide a spoken utterance such as "Assistant, set the thermostat to 68degrees (Assistant, set thermostat to 68 degrees)" when presenting the application GUI and text fields at the display interface. In response, the content correlation engine 216 can identify correlations between the content of the spoken utterance and entries in the content description data 236. For example, the entries in the content description data 236 may include natural language content such as "temperature" and "degrees" thereby providing a correlation between the spoken utterance and the entries in the content description data 236. Based on the correlation, the automation assistant 204 may cause the application 234 to change the setting of the thermostat from the current location to a location corresponding to 68 ℃.
In addition, the supplemental content engine 218 may generate supplemental content description data that characterizes the current location of the temperature of the thermostat as a value 68. If the user wants to change the thermostat, the user can provide a subsequent spoken utterance, such as "change 68 to 72". Because the subsequent spoken utterance does not have a direct correlation with the content description data 236, or at least the natural language content "temperature" and "degrees," the automated assistant 204 can rely on the supplemental content description data in order to identify the particular application GUI element that the user is intending to control. For example, because the supplemental content description data includes a value of "68," the automated assistant 204 and/or the content correlation engine 216 can determine that the subsequent spoken utterance and the supplemental content description data have a direct correlation.
As a result, the response of the automation assistant 204 and/or the data engine 222 may generate action data for initializing actions to change the setting of the value in the text field of the temperature of the thermostat from 68 ℃ to 72 ℃. For example, the reaction data may characterize one or more gestures for selecting a text field, deleting the current value 68, and entering a new value 72. Based on the change, the supplemental content description data may be modified to indicate that a value 72 is being presented within the temperature text field of the application GUI. In this way, while the original content description data 236 may not indicate a current value, if the user again wants to change the value in the temperature field using another spoken utterance, the automated assistant 204 will be able to identify the play-specific GUI element that the user is intending to control.
Fig. 3 illustrates a method 300 for allowing a user to provide compressed spoken utterances in order to control an application graphical user interface that may otherwise be controllable via one or more user inputs. The user may control the application GUI using a spoken utterance, which may not necessarily include one or more words that invoke a phrase and/or sequence identification automation assistant. The method 300 may be performed by one or more computing devices, applications, and/or any other apparatus or module capable of providing access to an automated assistant. The method 300 may include an operation 302 of determining whether a user provided a spoken utterance to an interface of a computing device. The computing device may include one or more interfaces, such as a touch display panel, one or more speakers, and/or one or more microphones. The spoken utterance may be provided by a user when the application GUI is presented at a touch display panel of the computing device. The application GUI is to be presented by an application controllable via one or more input gestures at a touch display panel and/or any other interface. For example, the application may be a device control panel application for controlling various devices connected through a local area network accessible at the user's home. The application GUI may include a variety of different GUI elements, such as buttons, drop down menus, text entry fields, and/or any other type of GUI element. In addition, the device control panel application may control various devices, such as an air conditioner, a stove, a Wi-Fi router, various lights, a security system, a computer, and/or any other appliance or device that may be controllable via the application.
The method 300 may also include an operation 304 of determining whether the user provided a recall phrase in the spoken utterance. The invocation phrase may be one or more words that are intended by the user to identify the automated assistant to facilitate initializing interactions between the user and the automated assistant. For example, the user-provided spoken utterance may be "Assistant,68degrees (Assistant, 68 degrees)", where "Assistant" is a call phrase. In response to receiving the spoken utterance, the automated Assistant may determine that the spoken utterance includes a invoke phrase of "Assistant". Thus, in this case, the method 300 may proceed from operation 304 to operation 308. However, when the invocation phrase is not included in the spoken utterance, the message may proceed from operation 304 to operation 306.
Operation 306 may include determining whether the automated assistant is operating according to a GUI control mode. When operating in the GUI control mode, the automated assistant may be responsive to a spoken utterance, which may not necessarily include a recall phrase and/or one or more words identifying the automated assistant. As an example, when operating in the GUI control mode, the automated assistant may respond to a spoken utterance, such as "off," in order to toggle GUI elements corresponding to the on-off switch of a particular device. However, when the automated assistant is not operating in the GUI control mode and the user provides a spoken utterance that does not include a recall phrase, the method 300 may proceed from operation 306 and then return to operation 302 of detecting whether the user has provided a subsequent spoken utterance.
Operation 308 may comprise identifying content description data corresponding to the currently presented application GUI. In some implementations, the content description data may be stored in association with the application and at a location accessible to the automated assistant. For example, the automated assistant may be provided prior to allowing access to content description data characterizing various GUI elements presented by one or more different applications at a touch display panel of the computing device. In this way, in response to receiving the spoken utterance, the automated assistant may make a determination as to whether the user intends to control one or more GUI elements presented at the touch display panel. In some implementations, the content description data may be provided by a third party entity that provides the application, but that is different from the entity that provides the automation assistant and/or the operating system on which the application is executing. The content description data may include static data and/or dynamic data depending on the particular GUI element that a portion of the content description data is characterizing. In some implementations, the application and/or the automated assistant can generate supplemental content description data that can be dynamically updated according to whether the user is interacting with the application GUI and/or one or more GUI elements. In this way, if the user identifies content that was not originally identified by the content description data but is associated with the application GUI, the automated assistant may respond to the spoken utterance identifying such content.
From operation 308, the method 300 may proceed to operation 310, which operation 310 may include determining whether the content of the spoken utterance corresponds to a plurality of GUI elements. When the natural language content of the spoken utterance includes content associated with a plurality of GUI elements, the automated assistant may determine that the spoken utterance corresponds to the plurality of GUI elements. For example, the user may provide a spoken utterance such as "turn up", which may correspond to a plurality of GUI elements, such as a dial for adjusting the temperature of the thermostat and another dial for adjusting the volume of music projected by the stereo system. The content description data characterizing the dial and the other dial may include an entry having the word "up" such that the automated assistant identifies a plurality of GUI elements corresponding to the spoken utterance "turn up". However, when there is only a single dial presented at the application GUI, the automated assistant may determine that the spoken utterance corresponds to only a particular GUI element.
When the content of the spoken utterance corresponds to a plurality of GUI elements, the method 300 may proceed from operation 310 to optional operation 312, which optional operation 312 may include causing a graphical indicator to be presented at or near each of the plurality of GUI elements. As an example, when the first GUI element and the second GUI element presented at the application GUI are determined to correspond to the spoken utterance, the automated assistant may cause one or more colors to be presented at or near the first GUI element and the second GUI element. One or more colors may inform the user that multiple GUI elements are identified in response to a recent spoken utterance from the user. The user may then decide whether to further specify a particular GUI element that the user intends to control.
From operation 312, the method 300 may optionally proceed to optional operation 314, which optional operation 314 may include generating additional content description data associated with the plurality of GUI elements. In some implementations, the additional content description data may characterize one or more graphical indicators presented via operation 312. For example, a plurality of different colors, values, text, and/or other indicators may be presented at or near each of a plurality of GUI elements identified as corresponding to a spoken utterance. The additional content description data may thus characterize each of a number of different colors, values, text, and/or other indicators. In this way, when a user provides a subsequent spoken utterance to further specify a particular GUI element that they aim to control, the content of the subsequent spoken utterance can be compared to the original content description data as well as the additional content description data.
For example, the method 300 may proceed from operation 314 to operation 302 in order to detect a subsequent spoken utterance from the user. The user may provide a subsequent spoken utterance, such as "red", to identify a color that has been used to highlight at least one of the plurality of GUI elements. When the user provides a subsequent spoken utterance, the automated assistant may operate according to the GUI control mode, allowing the user to provide the subsequent spoken utterance without invoking a phrase. Thus, in response to the subsequent spoken utterance, the automated assistant can compare the content of the subsequent spoken utterance with (i) the content description data and (ii) the additional content description data in order to identify the particular GUI element that the user is referencing. When the automated assistant identifies a particular GUI element that is highlighted in "red", the automated assistant may recall (prior to providing a subsequent spoken utterance) the initial spoken utterance provided by the user and initiate performance of an action corresponding to the particular GUI element that is highlighted in red and facilitate population of the initial spoken utterance provided by the user.
When the content of the spoken utterance does not correspond to a plurality of GUI elements at operation 310, the method 300 may proceed from operation 310 to operation 316. The operation 416 may include determining whether the content of the spoken utterance corresponds to a single GUI element. When it is determined that the content of the spoken utterance does not correspond to a single GUI element, the method 300 may return to operation 302 in order to detect any subsequent spoken utterances from one or more users. However, when it is determined that the content of the spoken utterance corresponds to a single GUI element, the method 300 may proceed from operation 316 to operation 318.
Operation 318 may include generating element data consistent with the spoken utterance and associated with a single GUI element. As an example, when the user has provided the spoken utterance "Assistant,68degrees," the automated Assistant may generate element data including the value 68 and/or element identifiers that identify individual GUI elements. In this way, when a single GUI element is a text field, the automated assistant, just like a user, can cause the value "68" to be entered into the text field. Alternatively or additionally, the automated assistant can generate gesture data that characterizes a particular gesture that will modify the single GUI element (if the user performed the particular gesture) and cause the single GUI element to reflect the content of the spoken utterance. For example, when a single GUI element corresponds to a scrollbar, the automated assistant may generate data characterizing a gesture that will effect a change in the position of the scrollbar from a current position to another position that characterizes a 68degree value of the scrollbar.
In some implementations, the automated assistant can initiate actions based on gesture data to "trick" the application thinks that the user actually provided a gesture that affects the scroll bar. For example, the automated assistant may communicate with the operating system of the computing device to indicate to the operating system that the user is intending to manipulate a single GUI element. The information provided by the automated assistant to the operating system may be based on gesture data generated by the automated assistant. Thus, in some implementations, the operating system may communicate with the application on behalf of the automated assistant to control the application as if the user provided a gesture characterized by the element data.
Fig. 4 is a block diagram of an example computer system 410. The computer system 410 typically includes at least one processor 414 that communicates with a number of peripheral devices via a bus subsystem 412. These peripheral devices may include a storage subsystem 424 that includes, for example, memory 425 and file storage subsystem 426, user interface output device 420, user interface input device 422, and network interface subsystem 416. Input and output devices allow a user to interact with computer system 410. Network interface subsystem 416 provides an interface to external networks and couples to corresponding interface devices in other computer systems.
The user interface input device 422 may include a keyboard; pointing devices such as a mouse, trackball, touch pad, or tablet; a scanner; a touch screen incorporated into the display; an audio input device such as a speech recognition system; a microphone; and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices, as well as ways of inputting information into the computer system 410 or a communication network.
The user interface output device 420 may include a display subsystem, a printer, a facsimile machine, or a non-visual display such as an audio output device. The display subsystem may include a Cathode Ray Tube (CRT), a flat panel device such as a Liquid Crystal Display (LCD), a projection device, or other mechanism for creating visual images. The display subsystem may also provide for non-visual display, such as via an audio output device. In general, use of the term "output device" is intended to include all possible types of devices, as well as ways of outputting information from computer system 410 to a user or another machine or computer system.
Storage subsystem 424 stores programming and data constructs that provide the functionality of some or all of the modules described herein. For example, the storage subsystem 424 may include logic for performing selected aspects of the method 300 and/or implementing one or more of the system 200, the computing device 104, the automated assistant 130, and/or any other application, device, apparatus, and/or module discussed herein.
These software modules are typically executed by processor 414 alone or in combination with other processors. Memory 425 used in storage subsystem 424 may include a number of memories, including a main Random Access Memory (RAM) 430 for storing instructions and data during program execution and a Read Only Memory (ROM) 432 for storing fixed instructions. File storage subsystem 426 may provide persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive, and associated removable media, CD-ROM drive, optical disk drive, or removable media cartridge. Modules implementing the functionality of certain embodiments may be stored by file storage subsystem 426 in storage subsystem 524, or in other machines accessible to processor 514.
Bus subsystem 412 provides a mechanism for allowing the various components and subsystems of computer system 410 to communicate with each other as intended. Although bus subsystem 412 is shown schematically as a single bus, alternative embodiments of the bus subsystem may use multiple buses.
Computer system 410 can be of various types including a workstation, a server, a computing cluster, a blade server, a server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computer system 410 depicted in FIG. 4 is intended only as a specific example for purposes of illustrating some embodiments. Many other configurations of computer system 410 are possible with more or fewer components than the computer system depicted in FIG. 4.
Where the system described herein collects personal information about a user (or what is often referred to herein as a "participant") or may use personal information, the user may be provided with an opportunity to control whether programs or features collect user information (e.g., information about the user's social network, social behavior or activity, profession, user's preferences, or the user's current geographic location), or whether and/or how to receive content from a content server that may be more relevant to the user. In addition, certain data may be processed in one or more ways prior to storage or use such that personally identifiable information is removed. For example, the identity of the user may be treated such that no personally identifiable information can be determined for the user, or the geographic location of the user (such as a city, zip code, or state level) for which geographic location information is obtained may be summarized such that a particular geographic location of the user cannot be determined. Thus, the user may control how information about the user and/or information that has been used is collected.
Although several embodiments have been described and illustrated herein, various other means and/or structures for performing a function and/or obtaining results and/or one or more of the advantages described herein may be utilized and each of these variations and/or modifications are considered to be within the scope of the embodiments described herein. More generally, all parameters, dimensions, materials, and configurations described herein are meant to be exemplary and the actual parameters, dimensions, materials, and/or configurations will depend upon the specific application or applications for which the teachings are used. Those skilled in the art will recognize, or be able to ascertain using no more than routine experimentation, many equivalents to the specific embodiments described herein. It is, therefore, to be understood that the foregoing embodiments are presented by way of example only and that, within the scope of the appended claims and equivalents thereto, the embodiments may be practiced otherwise than as specifically described and claimed. Embodiments of the present disclosure relate to each individual feature, system, article, material, kit, and/or method described herein. Furthermore, if such features, systems, articles, materials, kits, and/or methods are not mutually inconsistent, any combination of two or more such features, systems, articles, materials, kits, and/or methods is included within the scope of the present disclosure.
In some implementations, a method implemented by one or more processors is set forth as including operations such as determining that a user has provided a spoken utterance to a computing device, wherein the spoken utterance is provided while an application Graphical User Interface (GUI) is presented at a display panel of the computing device, and wherein the computing device provides access to an automated assistant controllable via spoken natural language input. The method may further include the operation of identifying content description data provided by an application that is presenting the application GUI at the display panel based on receiving the spoken utterance, wherein the content description data is omitted from the application GUI but characterizes one or more GUI elements that: presented at the application GUI and capable of being interacted with by a user via one or more input gestures to provide input to the application. The method may also include an operation of determining, based on the content description data and the spoken utterance, whether natural language content of the spoken utterance corresponds to at least one of the one or more GUI elements characterized by the content description data. The method may further include, when the natural language content corresponds to a particular GUI element of the at least one GUI element: input data is generated that is consistent with the natural language content of the spoken utterance based on the natural language content corresponding to the particular GUI element.
In some implementations, the method may further include the operations of: when the natural language content corresponds to a particular GUI element of the at least one GUI element, causing the input data to be presented at or near the particular GUI element of the application GUI. In some implementations, the method may further include the operations of: when the natural language content corresponds to a particular GUI element of the at least one GUI element, the input data is caused to be transferred to an application or a separate computing device different from the computing device. In some implementations, the method may further include the operations of: when the natural language content corresponds to a plurality of different GUI elements of the at least one GUI element, causing a display panel of the computing device to present a graphical indicator at or near each of the plurality of different GUI elements.
In some implementations, the method may further include the operations of: when the natural language content corresponds to a plurality of different GUI elements of the at least one GUI element: the method includes determining that a user has provided a subsequent spoken utterance including natural language content, the natural language content specifying an intended GUI element for control from a plurality of different GUI elements, and causing an application to initiate another action corresponding to the intended GUI element based on determining that the user has provided the subsequent spoken utterance. In some implementations, the application is provided by a third party entity that is different from the entity that provided the automated assistant. In some implementations, the natural language content of the spoken utterance does not identify one or more words of the automated assistant. In some implementations, a method may include: when the natural language content does not correspond to a particular GUI element of the at least one GUI element: suppressing generation of input data.
In some implementations, the method may further include the operations of: when the natural language content corresponds to a particular GUI element of the at least one GUI element: the method includes identifying, by the automated assistant, an application action corresponding to a particular GUI element, and causing the application action to be initialized by the application, wherein generating the input data is further performed in accordance with the application action. In some implementations, the application action includes causing a menu having a plurality of menu elements to be presented at the application GUI, and the element data characterizes each of the plurality of menu elements. In some implementations, the method may further include the operations of: when the natural language content corresponds to a particular GUI element of the at least one GUI element: after causing the application action to be initialized and the menu to be presented at the application GUI, it is determined that the user has provided a subsequent spoken utterance, the subsequent spoken utterance: the method includes explicitly identifying a particular menu element of the plurality of menu elements and not identifying one or more words of the automated assistant, and causing a corresponding action to be initiated by the application based on the identified particular menu element based on determining that the user has provided a subsequent spoken utterance.
In other implementations, a method implemented by one or more processors is set forth that includes operations such as determining, by an automated assistant accessible via a computing device, that a user has provided a spoken utterance to the automated assistant, wherein the spoken utterance is provided while an application Graphical User Interface (GUI) is presented at a display panel of the computing device, and wherein the spoken utterance includes natural language content that identifies the automated assistant. In some implementations, the method can further include the operation of identifying content description data provided by an application that is presenting the application GUI at the display panel based on receiving the spoken utterance, wherein the content description data is omitted from the application GUI but characterizes one or more GUI elements that: presented at the application GUI and capable of being interacted with by a user via one or more input gestures to provide input to the application. In some implementations, the method can further include an operation of determining, based on the content description data and the spoken utterance, whether natural language content of the spoken utterance corresponds to at least one of the one or more GUI elements characterized by the content description data. In some implementations, the method may further include the operations of: when the natural language content corresponds to a particular GUI element of the at least one GUI element, causing the automated assistant to operate according to a GUI control mode in which a subsequent spoken utterance provided by the user can omit identifying one or more words of the automated assistant but still cause the automated assistant to initiate execution of an application action, and generating input data consistent with the natural language content of the spoken utterance based on the natural language content corresponding to the particular GUI element. In some implementations, the method may further include: when the natural language content does not correspond to a particular GUI element of the at least one GUI element, the automated assistant is not caused to operate according to the GUI control mode and generation of the input data is suppressed.
In some implementations, the content description data includes other natural language content, wherein determining whether the natural language content of the spoken utterance corresponds to at least one GUI element includes: at least a portion of the natural language content of the spoken utterance is determined to be included in other natural language content of the content description data omitted from the application GUI. In some implementations, when the natural language content corresponds to a particular GUI element of the at least one GUI element and the automated assistant operates according to the GUI control mode: the method includes determining that an additional spoken utterance has been provided as input from a computing device of a user, wherein the additional spoken utterance does not identify one or more words of an automated assistant, and generating additional input data consistent with additional natural language content of the spoken utterance based on the additional natural language content of the additional spoken utterance.
In some implementations, the additional natural language content of the additional spoken utterance identifies a position of another GUI element relative to the particular GUI element. In some implementations, the additional natural language content of the additional spoken utterance characterizes at least one of: an input gesture of the one or more input gestures and another GUI element.
In still other implementations, a method implemented by one or more processors is set forth that includes operations such as determining, by an automated assistant accessible via a computing device, that a user has provided a spoken utterance to the automated assistant, wherein the spoken utterance is provided while an application graphical user interface GUI of an application is presented at a display panel of the computing device, and wherein the spoken utterance includes natural language content that identifies the automated assistant. In some implementations, the method can further include determining, based on the spoken utterance, whether natural language content of the spoken utterance corresponds to an operation of a plurality of different GUI elements presented with the application GUI, wherein a total number of user-controllable GUI elements presented with the application GUI is greater than a total number of the plurality of different GUI elements for which the natural language content is determined to correspond. In some implementations, the method can further include an operation of causing a plurality of different graphical indicators to be presented at or near the plurality of different GUI elements based on determining that the natural language content corresponds to the plurality of different GUI elements, wherein each graphical indicator of the plurality of different graphical indicators is presented as corresponding to a GUI element of the plurality of different GUI elements. In some implementations, the method can further include an operation of determining, by the automated assistant, that the user has provided a subsequent spoken utterance to the automated assistant, wherein the subsequent spoken utterance is provided concurrently with presenting the plurality of different graphical indicators at or near the plurality of different GUI elements. In some implementations, the method can further include an operation of determining that the user has identified a particular GUI element associated with the corresponding graphical indicator based on other natural language content of the subsequent spoken utterance. In some implementations, the method can further include the operation of generating input data based on the natural language content and other natural language content to facilitate initializing one or more actions consistent with the natural language content and other natural language content.
In some implementations, the input data is associated with gesture input that, when executed by the user, causes the application to initialize an action, and the method further includes: input data is provided to the application, wherein the input data, when received by the application, causes the application to initiate an action. In some implementations, other natural language content of the subsequent spoken utterance does not identify one or more words of the automated assistant. In some implementations, other natural language content of other spoken utterances identifies the location of a particular GUI element within the application GUI. In some implementations, other natural language content identifications of additional spoken utterances are omitted from specific GUI elements within the application GUI but stored as information of content description data in association with the specific GUI elements.
Claims (25)
1. A method implemented by one or more processors, the method comprising:
Determining that the user has provided a spoken utterance to a computing device,
Wherein the spoken utterance is provided while an application graphical user interface, GUI, is presented at a display panel of the computing device, and
Wherein the computing device provides access to an automated assistant controllable via spoken natural language input;
Based on receiving the spoken utterance, identifying content description data provided by an application that is presenting the application GUI at the display panel,
Wherein the content description data is omitted from the application GUI but characterizes one or more GUI elements that: presented at the application GUI and capable of interaction by the user via one or more input gestures to provide input to the application;
Determining, based on the content description data and the spoken utterance, whether natural language content of the spoken utterance corresponds to at least one of the one or more GUI elements characterized by the content description data; and
When the natural language content corresponds to a particular GUI element of the at least one GUI element:
input data consistent with the natural language content of the spoken utterance is generated based on the natural language content corresponding to the particular GUI element.
2. The method of claim 1, further comprising:
when the natural language content corresponds to the particular GUI element of the at least one GUI element:
Causing the input data to be presented at or near the particular GUI element of the application GUI.
3. The method of claim 1, further comprising:
when the natural language content corresponds to the particular GUI element of the at least one GUI element:
causing the input data to be transferred to the application or to a separate computing device different from the computing device.
4. The method of claim 1, further comprising:
when the natural language content corresponds to a plurality of different GUI elements of the at least one GUI element:
Causing the display panel of the computing device to present a graphical indicator at or near each of the plurality of different GUI elements.
5. The method of claim 4, further comprising:
when the natural language content corresponds to a plurality of different GUI elements of the at least one GUI element:
Determining that the user has provided a subsequent spoken utterance including natural language content specifying an intended GUI element for control from the plurality of different GUI elements, and
Based on determining that the user has provided the subsequent spoken utterance, causing the application to initialize another action corresponding to the intended GUI element.
6. The method of claim 1, wherein the application is provided by a third party entity different from the entity providing the automated assistant.
7. The method of claim 1, wherein the natural language content of the spoken utterance does not identify one or more words of the automated assistant.
8. The method of claim 1, further comprising:
when the natural language content corresponds to the particular GUI element of the at least one GUI element:
identifying, by the automated assistant, an application action corresponding to the particular GUI element, an
The application action is initiated by the application, wherein generating the input data is further performed in accordance with the application action.
9. The method according to claim 8, wherein the method comprises,
Wherein the application action includes causing a menu having a plurality of menu elements to be presented at the application GUI, and
Wherein the element data characterizes each menu element of the plurality of menu elements.
10. The method of claim 9, further comprising:
when the natural language content corresponds to the particular GUI element of the at least one GUI element:
After causing the application action to be initialized and the menu to be presented at the application GUI, determining that the user has provided a subsequent spoken utterance, the subsequent spoken utterance: specifically identifying a particular menu element of the plurality of menu elements and not identifying one or more words of the automated assistant, and
Based on determining that the user has provided the subsequent spoken utterance, the application is caused to initialize a corresponding action based on the identified particular menu element.
11. The method of any of claims 1-10, further comprising:
when the natural language content does not correspond to a particular GUI element of the at least one GUI element:
Suppressing generation of the input data.
12. A method implemented by one or more processors, the method comprising:
Determining by an automated assistant accessible via a computing device that a user has provided a spoken utterance to the automated assistant,
Wherein the spoken utterance is provided while an application graphical user interface, GUI, is presented at a display panel of the computing device, and
Wherein the spoken utterance includes natural language content that identifies the automated assistant;
Based on receiving the spoken utterance, identifying content description data provided by an application that is presenting the application GUI at the display panel,
Wherein the content description data is omitted from the application GUI but characterizes one or more GUI elements that: presented at the application GUI and capable of interaction by the user via one or more input gestures to provide input to the application;
Determining, based on the content description data and the spoken utterance, whether natural language content of the spoken utterance corresponds to at least one of the one or more GUI elements characterized by the content description data; and
When the natural language content corresponds to a particular GUI element of the at least one GUI element:
Causing the automated assistant to operate according to a GUI control mode in which subsequent spoken utterances provided by the user can omit one or more words identifying the automated assistant but still enable the automated assistant to initiate execution of application actions, and
Input data consistent with the natural language content of the spoken utterance is generated based on the natural language content corresponding to the particular GUI element.
13. The method of claim 12, wherein the content description data includes other natural language content, wherein determining whether the natural language content of the spoken utterance corresponds to the at least one GUI element comprises:
at least a portion of the natural language content of the spoken utterance is determined to be included in the other natural language content of the content description data omitted from the application GUI.
14. The method of claim 12, further comprising:
When the natural language content corresponds to a particular GUI element of the at least one GUI element and the automated assistant operates according to the GUI control mode:
determining that an additional spoken utterance has been provided as input from the user to the computing device, wherein the additional spoken utterance does not identify one or more words of the automated assistant, and
Additional input data is generated that is consistent with the additional natural language content of the spoken utterance based on the additional natural language content of the additional spoken utterance.
15. The method of claim 14, wherein the additional natural language content of the additional spoken utterance identifies a position of another GUI element relative to the particular GUI element.
16. The method of claim 14, wherein the additional natural language content of the additional spoken utterance characterizes at least one of: an input gesture of the one or more input gestures and another GUI element.
17. The method of any of claims 12 to 16, further comprising:
when the natural language content does not correspond to a particular GUI element of the at least one GUI element:
causing the automated assistant to not operate according to the GUI control mode, and
Suppressing generation of the input data.
18. A method implemented by one or more processors, the method comprising:
Determining by an automated assistant accessible via a computing device that a user has provided a spoken utterance to the automated assistant,
Wherein the spoken utterance is provided while an application graphical user interface, GUI, of an application is presented at a display panel of the computing device, and
Wherein the spoken utterance includes natural language content that identifies the automated assistant;
determining whether natural language content of the spoken utterance corresponds to a plurality of different GUI elements presented with the application GUI based on the spoken utterance,
Wherein the total number of user controllable GUI elements presented with the application GUI is greater than the total number of the plurality of different GUI elements for which the natural language content is determined to correspond;
Based on determining that the natural language content corresponds to a plurality of different GUI elements, causing a plurality of different graphical indicators to be presented at or near the plurality of different GUI elements,
Wherein each graphical indicator of the plurality of different graphical indicators is presented as corresponding to a GUI element of the plurality of different GUI elements;
Determining by the automated assistant that the user has provided a subsequent spoken utterance to the automated assistant,
Wherein the subsequent spoken utterance is provided concurrently with presentation of the plurality of different graphical indicators at or near the plurality of different GUI elements;
determining that the user has identified a particular GUI element associated with a corresponding graphical indicator based on other natural language content of the subsequent spoken utterance; and
Input data is generated based on the natural language content and the other natural language content to facilitate initializing one or more actions consistent with the natural language content and the other natural language content.
19. The method of claim 18, wherein the input data is associated with a gesture input that, when executed by the user, causes the application to initiate an action, and the method further comprises:
the input data is provided to the application, wherein the input data, when received by the application, causes the application to initiate the action.
20. The method of claim 18, wherein the other natural language content of the subsequent spoken utterance does not identify one or more words of the automated assistant.
21. The method of any of claims 18-20, wherein the other natural language content of the other spoken utterance identifies a location of the particular GUI element within the application GUI.
22. A method according to any one of claims 18 to 20, wherein the other natural language content identification of additional spoken utterances is omitted from but stored in association with the particular GUI element within the application GUI as information of content description data.
23. A computer program product comprising instructions which, when executed by one or more processors, cause the one or more processors to perform the method of any of claims 1 to 22.
24. A computer-readable storage medium comprising instructions that, when executed by one or more processors, cause the one or more processors to perform the method of any of claims 1-22.
25. A system comprising one or more processors to perform the method of any of claims 1-22.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201962876323P | 2019-07-19 | 2019-07-19 | |
US62/876,323 | 2019-07-19 | ||
PCT/US2019/046145 WO2021015801A1 (en) | 2019-07-19 | 2019-08-12 | Condensed spoken utterances for automated assistant control of an intricate application gui |
Publications (2)
Publication Number | Publication Date |
---|---|
CN114174972A CN114174972A (en) | 2022-03-11 |
CN114174972B true CN114174972B (en) | 2024-05-17 |
Family
ID=
Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2016111881A1 (en) * | 2015-01-09 | 2016-07-14 | Microsoft Technology Licensing, Llc | Headless task completion within digital personal assistants |
WO2016151396A1 (en) * | 2015-03-20 | 2016-09-29 | The Eye Tribe | Method for refining control by combining eye tracking and voice recognition |
CN107615378A (en) * | 2015-05-27 | 2018-01-19 | 苹果公司 | Equipment Voice command |
Patent Citations (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2016111881A1 (en) * | 2015-01-09 | 2016-07-14 | Microsoft Technology Licensing, Llc | Headless task completion within digital personal assistants |
WO2016151396A1 (en) * | 2015-03-20 | 2016-09-29 | The Eye Tribe | Method for refining control by combining eye tracking and voice recognition |
CN107615378A (en) * | 2015-05-27 | 2018-01-19 | 苹果公司 | Equipment Voice command |
Non-Patent Citations (1)
Title |
---|
基于实例推理的人机对话系统的设计与实现;姚琳;梁春霞;张德干;;计算机应用;20070310(第03期);765-768 * |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US20230297214A1 (en) | Providing composite graphical assistant interfaces for controlling various connected devices | |
US20230012852A1 (en) | Condensed spoken utterances for automated assistant control of an intricate application gui | |
KR20200117070A (en) | Initializing a conversation with an automated agent via selectable graphical element | |
US11960837B2 (en) | Fulfillment of actionable requests ahead of a user selecting a particular autocomplete suggestion for completing a current user input | |
CN111052079B (en) | Systems/methods and apparatus for providing multi-function links for interacting with assistant agents | |
US20220276722A1 (en) | Expanding physical motion gesture lexicon for an automated assistant | |
CN113544770A (en) | Initializing non-assistant background actions by an automated assistant when accessing non-assistant applications | |
CN116830075A (en) | Passive disambiguation of assistant commands | |
KR20220127319A (en) | Determining whether and/or when to provide notifications based on application content to mitigate computationally wasted application execution behavior | |
KR20200124298A (en) | Mitigate client device latency when rendering remotely generated automated assistant content | |
KR20230047188A (en) | Undo application action(s) via user interaction(s) with automated assistant | |
US20230385022A1 (en) | Automated assistant performance of a non-assistant application operation(s) in response to a user input that can be limited to a parameter(s) | |
US20230237312A1 (en) | Reinforcement learning techniques for selecting a software policy network and autonomously controlling a corresponding software client based on selected policy network | |
CN114174972B (en) | Automated assistant controlled compressed spoken utterances for complex application GUIs | |
JP2024505792A (en) | Automated assistant for implementing or controlling search filter parameters in separate applications | |
US20240038246A1 (en) | Non-wake word invocation of an automated assistant from certain utterances related to display content | |
CN116802597A (en) | Selectively rendering keyboard interfaces in response to assistant calls in some cases |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant |