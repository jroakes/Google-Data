JP2024062993A - Attenuation of automatic speech recognition processing results - Google Patents
Attenuation of automatic speech recognition processing results Download PDFInfo
- Publication number
- JP2024062993A JP2024062993A JP2024015483A JP2024015483A JP2024062993A JP 2024062993 A JP2024062993 A JP 2024062993A JP 2024015483 A JP2024015483 A JP 2024015483A JP 2024015483 A JP2024015483 A JP 2024015483A JP 2024062993 A JP2024062993 A JP 2024062993A
- Authority
- JP
- Japan
- Prior art keywords
- asr
- processing
- level
- audio stream
- microphone
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000012545 processing Methods 0.000 title claims abstract description 330
- 230000002238 attenuated effect Effects 0.000 claims abstract description 62
- 238000000034 method Methods 0.000 claims abstract description 58
- 230000003993 interaction Effects 0.000 claims abstract description 54
- 230000004044 response Effects 0.000 claims abstract description 44
- 230000015654 memory Effects 0.000 claims description 40
- 230000006870 function Effects 0.000 claims description 25
- 230000008569 process Effects 0.000 claims description 19
- 230000000694 effects Effects 0.000 claims description 12
- 230000009471 action Effects 0.000 claims description 11
- 238000004891 communication Methods 0.000 claims description 11
- 238000013139 quantization Methods 0.000 claims description 7
- 239000003638 chemical reducing agent Substances 0.000 description 60
- 230000008859 change Effects 0.000 description 12
- 230000001755 vocal effect Effects 0.000 description 12
- 238000004590 computer program Methods 0.000 description 8
- 238000010586 diagram Methods 0.000 description 8
- 238000013518 transcription Methods 0.000 description 8
- 230000035897 transcription Effects 0.000 description 8
- 238000013528 artificial neural network Methods 0.000 description 6
- 238000013459 approach Methods 0.000 description 5
- 230000009194 climbing Effects 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 4
- 230000007704 transition Effects 0.000 description 4
- 230000006399 behavior Effects 0.000 description 3
- 230000007423 decrease Effects 0.000 description 3
- 230000002093 peripheral effect Effects 0.000 description 3
- 230000000306 recurrent effect Effects 0.000 description 3
- 235000000832 Ayote Nutrition 0.000 description 2
- 241000219122 Cucurbita Species 0.000 description 2
- 235000009854 Cucurbita moschata Nutrition 0.000 description 2
- 235000009804 Cucurbita pepo subsp pepo Nutrition 0.000 description 2
- 230000002457 bidirectional effect Effects 0.000 description 2
- 230000003247 decreasing effect Effects 0.000 description 2
- 230000000977 initiatory effect Effects 0.000 description 2
- 230000001537 neural effect Effects 0.000 description 2
- 238000013138 pruning Methods 0.000 description 2
- 235000015136 pumpkin Nutrition 0.000 description 2
- 230000004913 activation Effects 0.000 description 1
- 230000003542 behavioural effect Effects 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 238000005266 casting Methods 0.000 description 1
- 238000013016 damping Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 235000019800 disodium phosphate Nutrition 0.000 description 1
- 230000006266 hibernation Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
- 230000004043 responsiveness Effects 0.000 description 1
- 239000011435 rock Substances 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 230000007958 sleep Effects 0.000 description 1
- 230000005236 sound signal Effects 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000001052 transient effect Effects 0.000 description 1
- 210000003462 vein Anatomy 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
- 239000002699 waste material Substances 0.000 description 1
Abstract
【課題】減衰されたレベルで自動音声認識処理する方法及びシステムを提供する。
【解決手段】音声対応デバイスにおける方法３００は、スピーチによりデバイスとの起こり得るインタラクションを示すマイクロフォントリガイベントのインジケーションを受信し、マイクロフォントリガイベントのインジケーションの受信に応答して、オーディオストリームをキャプチャするため又は継続時間ウィンドウの間、マイクロフォンが開いたままであるようにし、音声認識システムに、マイクロフォンによってキャプチャされたオーディオストリームを提供する。方法はまた、継続時間の間、継続時間ウィンドウ関数に基づいてレベルを減衰させ音声認識処理し、オーディオストリームに対して減衰したレベルで音声認識処理するように音声認識システムに指示する。
【選択図】図３
A method and system for automatic speech recognition processing at an attenuated level is provided.
A method 300 in a voice-enabled device receives an indication of a microphone trigger event indicating a possible interaction with the device by speech, and in response to receiving the indication of the microphone trigger event, causes a microphone to remain open for a duration or duration window to capture an audio stream, and provides the audio stream captured by the microphone to a speech recognition system. The method also attenuates and speech recognizes the audio stream at a level based on a duration window function for the duration, and instructs the speech recognition system to perform speech recognition processing at the attenuated level on the audio stream.
[Selected figure] Figure 3
Description
本開示は、自動音声認識結果を減衰させることに関する。 This disclosure relates to attenuating automatic speech recognition results.
ユーザは、デジタルアシスタントインターフェースを通じて、スマートフォン、スマートウォッチ、およびスマートスピーカなどの音声対応デバイスと頻繁にインタラクションする。これらのデジタルアシスタントインターフェースは、すべて自然な会話によるインタラクションを通じて、ユーザがタスクを完了し、それらのユーザが持っている質問に対する答えを取得することを可能にする。 Users frequently interact with voice-enabled devices such as smartphones, smartwatches, and smart speakers through digital assistant interfaces. These digital assistant interfaces enable users to complete tasks and get answers to questions they have, all through natural conversational interaction.
理想的には、デジタルアシスタントインターフェースと会話するとき、ユーザは、デジタルアシスタントインターフェースを実行するそのユーザの音声対応デバイスに向けられた口頭の要求によって、あたかもユーザが別の人と話しているかのようにコミュニケーションすることができるべきである。デジタルアシスタントインターフェースは、アクションが実行され得るように口頭の要求を処理し、認識するために、これらの口頭の要求を自動音声認識器に提供する。しかし、実際には、スマートフォンまたはスマートウォッチなどのリソースに制約のある音声対応デバイス上で音声認識を継続的に実行することは法外なコストがかかるので、デバイスが常にこれらの口頭の要求に応答することは難しい。 Ideally, when conversing with a digital assistant interface, a user should be able to communicate as if the user were speaking to another person, through verbal requests directed to the user's voice-enabled device running the digital assistant interface. The digital assistant interface provides these verbal requests to an automatic speech recognizer to process and recognize the verbal requests so that actions can be taken. In practice, however, it is cost-prohibitive to continuously perform speech recognition on a resource-constrained voice-enabled device such as a smartphone or smartwatch, making it difficult for the device to always respond to these verbal requests.
本開示の一態様は、自動音声認識処理を減衰させる方法を提供する。方法は、音声対応デバイスのデータ処理ハードウェアにおいて、スピーチによる音声対応デバイスとの起こり得るユーザインタラクションを示すマイクロフォントリガイベントのインジケーションを受信するステップであって、音声対応デバイスがマイクロフォンを有し、マイクロフォンは、開いているときに自動音声認識(ASR)システムによる認識のためにスピーチをキャプチャするように構成される、ステップを含む。マイクロフォントリガイベントのインジケーションの受信に応答して、方法は、データ処理ハードウェアによって、音声対応デバイスの環境内でオーディオストリームをキャプチャするために、開くか、または開いたマイクロフォン継続時間ウィンドウ(open microphone duration window)の間、開いたままであるようにマイクロフォンに指示するステップと、データ処理ハードウェアによって、開いたマイクロフォンによりキャプチャされたオーディオストリームを、オーディオストリームに対してASR処理を実行するためにASRシステムに提供するステップとをさらに含む。ASRシステムが、開いたマイクロフォンによってキャプチャされたオーディオストリームに対してASR処理を実行している間に、方法は、データ処理ハードウェアによって、ASRシステムがオーディオストリームに対して実行するASR処理のレベルを、開いたマイクロフォン継続時間ウィンドウの関数に基づいて減衰させるステップと、データ処理ハードウェアによって、開いたマイクロフォンによってキャプチャされたオーディオストリームに対してASR処理の減衰されたレベルを使用するようにASRシステムに指示するステップとをさらに含む。 One aspect of the present disclosure provides a method for damping automatic speech recognition processing. The method includes receiving, at data processing hardware of a voice-enabled device, an indication of a microphone trigger event indicating a possible user interaction with the voice-enabled device by speech, where the voice-enabled device has a microphone, the microphone configured to capture speech for recognition by an automatic speech recognition (ASR) system when open. In response to receiving the indication of the microphone trigger event, the method further includes instructing, by the data processing hardware, the microphone to open or remain open for an open microphone duration window to capture an audio stream within an environment of the voice-enabled device, and providing, by the data processing hardware, the audio stream captured by the open microphone to the ASR system for performing ASR processing on the audio stream. While the ASR system is performing ASR processing on the audio stream captured by the open microphone, the method further includes attenuating, by the data processing hardware, a level of ASR processing that the ASR system performs on the audio stream based on a function of the open microphone duration window, and instructing, by the data processing hardware, the ASR system to use the attenuated level of ASR processing on the audio stream captured by the open microphone.
一部の例においては、ASRシステムが、開いたマイクロフォンによってキャプチャされたオーディオストリームに対してASR処理を実行している間に、方法は、データ処理ハードウェアによって、開いたマイクロフォンによりキャプチャされたオーディオストリーム内で音声活動が検出されるかどうかを判定するステップも含む。これらの例において、ASRシステムがオーディオストリームに対して実行するASR処理のレベルを減衰させるステップは、オーディオストリーム内で何らかの音声活動が検出されるかどうかの判定にさらに基づく。一部の実装において、方法は、データ処理ハードウェアによって、マイクロフォントリガイベントのインジケーションが受信されるときに、現在のコンテキスト(context)を取得するステップをさらに含む。これらの実装において、ASR処理の減衰されたレベルを使用するようにASRシステムに指示するステップは、現在のコンテキストに基づいて音声認識結果にバイアスをかけるようにASRシステムに指示することを含む。一部の構成においては、オーディオストリームに対してASR処理の減衰されたレベルを使用するようにASRシステムに指示した後、方法は、データ処理ハードウェアにおいて、ASRシステムによって出力された音声クエリに関する音声認識結果の信頼度が信頼度の閾値を満たすことができないというインジケーションを受信するステップと、データ処理ハードウェアによって、ASRシステムに、ASR処理のレベルを減衰されたレベルから上げ、ASR処理の上げられたレベルを使用して音声クエリを再処理するように指示するステップとをさらに含む。一部の例においては、ASRシステムが、開いたマイクロフォンによってキャプチャされたオーディオストリームに対してASR処理を実行している間に、方法は、データ処理ハードウェアによって、開いたマイクロフォンの継続時間の関数に基づいてASRがオーディオストリームに対して実行するASR処理の減衰されたレベルがゼロに等しいときを判定するステップと、ASR処理の減衰されたレベルがゼロに等しいときに、データ処理ハードウェアによって、マイクロフォンに閉じるよう指示するステップとをさらに含む。任意で、方法は、データ処理ハードウェアによって、音声対応デバイスのグラフィカルユーザインターフェースにおいて、オーディオストリームに対してASRシステムによって実行されたASR処理の減衰されたレベルを示すグラフィカルなインジケータを表示するステップも含んでよい。 In some examples, while the ASR system is performing ASR processing on the audio stream captured by the open microphone, the method also includes determining, by the data processing hardware, whether voice activity is detected in the audio stream captured by the open microphone. In these examples, attenuating a level of ASR processing that the ASR system performs on the audio stream is further based on determining whether any voice activity is detected in the audio stream. In some implementations, the method further includes obtaining, by the data processing hardware, a current context when an indication of a microphone trigger event is received. In these implementations, instructing the ASR system to use the attenuated level of ASR processing includes instructing the ASR system to bias the speech recognition results based on the current context. In some configurations, after instructing the ASR system to use the attenuated level of ASR processing on the audio stream, the method further includes receiving, in the data processing hardware, an indication that the confidence of the speech recognition result for the speech query output by the ASR system fails to meet a confidence threshold, and instructing, by the data processing hardware, the ASR system to increase the level of ASR processing from the attenuated level and reprocess the speech query using the increased level of ASR processing. In some examples, while the ASR system is performing ASR processing on the audio stream captured by the open microphone, the method further includes determining, by the data processing hardware, when the attenuated level of ASR processing that the ASR performs on the audio stream equals zero based on a function of the duration of the open microphone, and instructing, by the data processing hardware, the microphone to close when the attenuated level of ASR processing equals zero. Optionally, the method may also include displaying, by the data processing hardware, a graphical indicator in a graphical user interface of the speech-enabled device indicating the attenuated level of ASR processing performed by the ASR system on the audio stream.
本開示の別の態様は、自動音声認識処理を減衰させるためのシステムを提供する。システムは、データ処理ハードウェアと、データ処理ハードウェアと通信するメモリハードウェアとを含む。メモリハードウェアは、データ処理ハードウェア上で実行されるときにデータ処理ハードウェアに動作を実行させる命令を記憶する。動作は、音声対応デバイスにおいて、スピーチによる音声対応デバイスとの起こり得るユーザインタラクションを示すマイクロフォントリガイベントのインジケーションを受信する動作であって、音声対応デバイスがマイクロフォンを有し、マイクロフォンは、開いているときに自動音声認識(ASR)システムによる認識のためにスピーチをキャプチャするように構成される、動作を含む。マイクロフォントリガイベントのインジケーションの受信に応答して、動作は、音声対応デバイスの環境内でオーディオストリームをキャプチャするために、開くか、または開いたマイクロフォン継続時間ウィンドウの間、開いたままであるようにマイクロフォンに指示する動作と、開いたマイクロフォンによりキャプチャされたオーディオストリームを、オーディオストリームに対してASR処理を実行するためにASRシステムに提供する動作とをさらに含む。ASRシステムが、開いたマイクロフォンによってキャプチャされたオーディオストリームに対してASR処理を実行している間に、動作は、ASRシステムがオーディオストリームに対して実行するASR処理のレベルを、開いたマイクロフォン継続時間ウィンドウの関数に基づいて減衰させる動作と、開いたマイクロフォンによってキャプチャされたオーディオストリームに対してASR処理の減衰されたレベルを使用するようにASRシステムに指示する動作とをさらに含む。 Another aspect of the present disclosure provides a system for attenuating automatic speech recognition processing. The system includes data processing hardware and memory hardware in communication with the data processing hardware. The memory hardware stores instructions that, when executed on the data processing hardware, cause the data processing hardware to perform operations. The operations include receiving, at a voice-enabled device, an indication of a microphone trigger event indicative of a possible user interaction with the voice-enabled device by speech, where the voice-enabled device has a microphone, the microphone configured to capture speech for recognition by an automatic speech recognition (ASR) system when open. In response to receiving the indication of the microphone trigger event, the operations further include instructing the microphone to open or remain open for an open microphone duration window to capture an audio stream within an environment of the voice-enabled device, and providing the audio stream captured by the open microphone to the ASR system to perform ASR processing on the audio stream. While the ASR system is performing ASR processing on the audio stream captured by the open microphone, the operations further include attenuating a level of ASR processing that the ASR system performs on the audio stream based on a function of the open microphone duration window, and instructing the ASR system to use the attenuated level of ASR processing on the audio stream captured by the open microphone.
この態様は、以下の任意の特徴のうちの1つまたは複数を含む場合がある。一部の例においては、ASRシステムが、開いたマイクロフォンによってキャプチャされたオーディオストリームに対してASR処理を実行している間に、動作は、開いたマイクロフォンによりキャプチャされたオーディオストリーム内で音声活動が検出されるかどうかを判定する動作も含む。これらの例において、ASRシステムがオーディオストリームに対して実行するASR処理のレベルを減衰させる動作は、オーディオストリーム内で何らかの音声活動レベルが検出されるかどうかの判定にさらに基づく。一部の実装において、動作は、マイクロフォントリガイベントのインジケーションが受信されるときに、現在のコンテキストを取得する動作をさらに含む。これらの実装において、ASR処理の減衰されたレベルを使用するようにASRシステムに指示する動作は、現在のコンテキストに基づいて音声認識結果にバイアスをかけるようにASRシステムに指示することを含む。一部の構成においては、オーディオストリームに対してASR処理の減衰されたレベルを使用するようにASRシステムに指示した後、動作は、ASRシステムによって出力された音声クエリに関する音声認識結果の信頼度が信頼度の閾値を満たすことができないというインジケーションを受信する動作と、ASRシステムに、ASR処理のレベルを減衰されたレベルから上げ、ASR処理の上げられたレベルを使用して音声クエリを再処理するように指示する動作とをさらに含む。一部の実装においては、ASRシステムが、開いたマイクロフォンによってキャプチャされたオーディオストリームに対してASR処理を実行している間に、動作は、開いたマイクロフォンの継続時間の関数に基づいてASRがオーディオストリームに対して実行するASR処理の減衰されたレベルがゼロに等しいときを判定する動作と、ASR処理の減衰されたレベルがゼロに等しいときに、マイクロフォンに閉じるよう指示する動作とをさらに含む。任意で、動作は、音声対応デバイスのグラフィカルユーザインターフェースにおいて、オーディオストリームに対してASRシステムによって実行されたASR処理の減衰されたレベルを示すグラフィカルなインジケータを表示する動作も含んでよい。 This aspect may include one or more of any of the following features. In some examples, while the ASR system is performing ASR processing on the audio stream captured by the open microphone, the operations also include determining whether voice activity is detected in the audio stream captured by the open microphone. In these examples, the operations of attenuating a level of ASR processing that the ASR system performs on the audio stream are further based on determining whether any voice activity level is detected in the audio stream. In some implementations, the operations further include obtaining a current context when an indication of a microphone trigger event is received. In these implementations, the operations of instructing the ASR system to use the attenuated level of ASR processing include instructing the ASR system to bias the speech recognition results based on the current context. In some configurations, after instructing the ASR system to use the attenuated level of ASR processing on the audio stream, the operations further include receiving an indication that a confidence of the speech recognition result for the speech query output by the ASR system fails to meet a confidence threshold, and instructing the ASR system to increase a level of ASR processing from the attenuated level and reprocess the speech query using the increased level of ASR processing. In some implementations, while the ASR system is performing ASR processing on the audio stream captured by the open microphone, the operations further include determining when an attenuated level of ASR processing that the ASR performs on the audio stream is equal to zero based on a function of the duration of the open microphone, and instructing the microphone to close when the attenuated level of ASR processing is equal to zero. Optionally, the operations may also include displaying a graphical indicator in a graphical user interface of the speech-enabled device indicating the attenuated level of ASR processing performed by the ASR system on the audio stream.
システムまたは方法の実装は、以下の任意の特徴のうちの1つまたは複数を含む場合がある。一部の実装において、ASRシステムは、最初に、開いたマイクロフォン継続時間ウィンドウの開始時に、オーディオストリームに対してASR処理を実行するために第1の処理レベルを使用し、第1の処理レベルは、ASRシステムの最大処理能力に関連付けられる。これらの実装において、ASRシステムがオーディオストリームに対して実行するASR処理のレベルを、開いたマイクロフォン継続時間ウィンドウの関数に基づいて減衰させることは、開いたマイクロフォン継続時間ウィンドウを開始してから第1の時間間隔が経過したかどうかを判定することと、第1の時間間隔が経過したとき、ASRシステムがオーディオストリームに対して実行するASR処理のレベルを、ASR処理のレベルを第1の処理レベルから第2の処理レベルに下げることによって減衰させることであって、第2の処理レベルが、第1の処理レベルよりも低い、減衰させることとを含む。一部の例において、ASR処理の減衰されたレベルを使用するようにASRシステムに指示することは、音声対応デバイスと通信するリモートサーバ上でのASR処理の実行から、音声対応デバイスのデータ処理ハードウェア上でのASR処理の実行に切り替えるようにASRシステムに指示することを含む。一部の構成において、ASR処理の減衰されたレベルを使用するようにASRシステムに指示することは、オーディオストリームに対してASR処理を実行するために第1のASRモデルの使用から第2のASRモデルの使用に切り替えるようにASRシステムに指示することであって、第2のASRモデルが、第1のASRモデルよりも少ないパラメータを含む、指示することを含む。ASR処理の減衰されたレベルを使用するようにASRシステムに指示することは、オーディオストリームに対して実行されるASR処理ステップの数を減らすようにASRシステムに指示することを含んでよい。ASR処理の減衰されたレベルを使用するようにASRシステムに指示することは、ASRシステムの復号探索空間を小さくするためにビームサーチパラメータを調整するようにASRシステムに指示することを含んでもよい。追加的にまたは代替的に、ASR処理の減衰されたレベルを使用するようにASRシステムに指示することは、ASRシステムの1つまたは複数のパラメータに対して量子化および/またはスパース化を実行するようにASRシステムに指示することを含んでよい。一部の構成において、ASR処理の減衰されたレベルを使用するようにASRシステムに指示することは、オーディオストリームに対してASR処理を実行するためのシステムオンチップベースの(SOCベースの)処理から、オーディオストリームに対してASR処理を実行するためのデジタル信号プロセッサベースの(DSPベースの)処理に切り替えるようにASRシステムに指示することを含んでよい。ASRシステムが、開いたマイクロフォンによってキャプチャされたオーディオストリームに対してASR処理の減衰されたレベルを使用している間に、ASRシステムは、ユーザによって話されたクエリに対応するオーディオデータの音声認識結果を生成し、クエリによって指定されたアクションを実行するために、音声認識結果をアプリケーションに提供するように構成される。 Implementations of the system or method may include one or more of any of the following features. In some implementations, the ASR system initially uses a first processing level to perform ASR processing on the audio stream at the beginning of the open microphone duration window, the first processing level being associated with a maximum processing capacity of the ASR system. In these implementations, attenuating a level of ASR processing that the ASR system performs on the audio stream based on a function of the open microphone duration window includes determining whether a first time interval has elapsed since starting the open microphone duration window, and attenuating a level of ASR processing that the ASR system performs on the audio stream when the first time interval has elapsed by reducing the level of ASR processing from the first processing level to a second processing level, the second processing level being lower than the first processing level. In some examples, instructing the ASR system to use the attenuated level of ASR processing includes instructing the ASR system to switch from performing ASR processing on a remote server in communication with the voice-enabled device to performing ASR processing on data processing hardware of the voice-enabled device. In some configurations, instructing the ASR system to use an attenuated level of ASR processing includes instructing the ASR system to switch from using a first ASR model to using a second ASR model to perform ASR processing on the audio stream, the second ASR model including fewer parameters than the first ASR model. Instructing the ASR system to use an attenuated level of ASR processing may include instructing the ASR system to reduce a number of ASR processing steps performed on the audio stream. Instructing the ASR system to use an attenuated level of ASR processing may include instructing the ASR system to adjust beam search parameters to reduce a decoding search space of the ASR system. Additionally or alternatively, instructing the ASR system to use an attenuated level of ASR processing may include instructing the ASR system to perform quantization and/or sparsification on one or more parameters of the ASR system. In some configurations, instructing the ASR system to use the attenuated level of ASR processing may include instructing the ASR system to switch from system-on-chip based (SOC-based) processing for performing ASR processing on the audio stream to digital signal processor based (DSP-based) processing for performing ASR processing on the audio stream. While the ASR system is using the attenuated level of ASR processing on the audio stream captured by the open microphone, the ASR system is configured to generate speech recognition results for the audio data corresponding to a query spoken by a user and provide the speech recognition results to an application to perform an action specified by the query.
本開示の1つまたは複数の実装の詳細が、添付の図面および以下の説明に記載されている。その他の態様、特徴、および利点は、説明および図面から、ならびに請求項から明らかになるであろう。 Details of one or more implementations of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will become apparent from the description and drawings, and from the claims.
様々な図面における同様の参照符号は、同様の要素を示す。 Like reference numbers in the various drawings indicate like elements.
理想的には、デジタルアシスタントインターフェースと会話するとき、ユーザは、デジタルアシスタントインターフェースを実行するそのユーザの音声対応デバイスに向けられた口頭の要求によって、あたかもユーザが別の人と話しているかのようにコミュニケーションすることができるべきである。デジタルアシスタントインターフェースは、アクションが実行され得るように口頭の要求を処理し、認識するために、これらの口頭の要求を自動音声認識器に提供する。しかし、実際には、スマートフォンまたはスマートウォッチなどのリソースに制約のある音声対応デバイス上で音声認識を継続的に実行することは法外なコストがかかるので、デバイスが常にこれらの口頭の要求に応答することは難しい。 Ideally, when conversing with a digital assistant interface, a user should be able to communicate as if the user were speaking to another person, through verbal requests directed to the user's voice-enabled device running the digital assistant interface. The digital assistant interface provides these verbal requests to an automatic speech recognizer to process and recognize the verbal requests so that actions can be taken. In practice, however, it is cost-prohibitive to continuously perform speech recognition on a resource-constrained voice-enabled device such as a smartphone or smartwatch, making it difficult for the device to always respond to these verbal requests.
デジタルアシスタントインターフェースとの会話は、通常、ユーザが決まったフレーズ(たとえば、ホットワード/キーワード/ウェイクワード)を言うか、または何らかの予め定義されたジェスチャ(たとえば、音声対応デバイスを持ち上げるかもしくは強く握る)を使用するなど、ユーザによる便利なインタラクションによって開始される。しかし、会話が始まると、相次ぐ口頭の要求/クエリのたびに同じ決まったフレーズを言うかまたは同じ予め定義されたジェスチャを使用することをユーザに要求することは、ユーザにとって面倒で不便になり得る。この要件を緩和するために、音声対応デバイスのマイクロフォンは、マイクロフォンがユーザによって話されたすぐ後の追加クエリをはるかに自然な方法でキャプチャすることを可能にするために、インタラクションの直後のいくらかの予め定義された量の時間、開いたままにされ得る。しかし、マイクロフォンがインタラクションの直後にどのくらい開いたままにされるべきかに関してはいくつかのトレードオフがある。たとえば、マイクロフォンをあまりにも長く開けっ放しにすることは、音声認識を実行するための電力を不必要に消費すること、および音声対応デバイスの環境内の意図的でないスピーチをキャプチャする可能性を高めることにつながり得る。一方、マイクロフォンをあまりにも早く閉じることは、ユーザが決まったフレーズ、ジェスチャ、または不便な場合があるその他の手段によって会話を再び始めることを要求されるので、悪いユーザエクスペリエンスを生じる。 A conversation with a digital assistant interface is typically initiated by a convenient interaction by the user, such as the user saying a fixed phrase (e.g., a hot word/keyword/wake word) or using some predefined gesture (e.g., lifting or squeezing the voice-enabled device). However, once a conversation has begun, requiring the user to say the same fixed phrase or use the same predefined gesture for each successive verbal request/query can become tedious and inconvenient for the user. To alleviate this requirement, the microphone of the voice-enabled device can be left open for some predefined amount of time immediately after the interaction to allow the microphone to capture additional queries spoken by the user in a much more natural way. However, there are some trade-offs regarding how long the microphone should be left open immediately after the interaction. For example, leaving the microphone open too long can lead to unnecessarily consuming power to perform voice recognition and increasing the likelihood of capturing unintentional speech in the voice-enabled device's environment. On the other hand, closing the microphone too quickly creates a poor user experience, as the user is required to restart the conversation with a set phrase, gesture, or other means that may be inconvenient.
本明細書の実装は、音声対応デバイスのマイクロフォンを開き、音声認識器の応答性と処理能力との両方を徐々に減衰させることによって、イベントに応答して音声認識を実行するために音声認識器を開始することを対象とする。より詳細には、本明細書の実装は、音声対応デバイスとの、最初のクエリの後の追加クエリなどのユーザインタラクションの確率に基づいて音声認識処理のレベルを減衰させることを含む。ある任意の時点でマイクロフォンを閉じるか、音声認識器によるさらなる処理を防止するかという二者択一の判断を行うのとは対照的に、音声認識器による処理を経時的に減衰させることは、音声対応デバイスに向けられた追加スピーチをキャプチャするためにマイクロフォンをより長く開いたままにしておくことによってユーザエクスペリエンスを向上させ、かつ、やがて起こるユーザインタラクションの信頼のレベルに応じて音声認識器が異なる電力モードで実行されることを可能にすることによって電力消費の節約を向上させる。 Implementations herein are directed to opening a microphone of a voice-enabled device and initiating a speech recognizer to perform speech recognition in response to an event by gradually attenuating both the responsiveness and processing capabilities of the speech recognizer. More specifically, implementations herein include attenuating the level of speech recognition processing based on the probability of a user interaction, such as an additional query after an initial query, with the voice-enabled device. As opposed to making an all-or-nothing decision to close the microphone at any given time or to prevent further processing by the speech recognizer, attenuating processing by the speech recognizer over time improves the user experience by keeping the microphone open longer to capture additional speech directed to the voice-enabled device, and improves power consumption conservation by allowing the speech recognizer to run in different power modes depending on the level of confidence of the upcoming user interaction.
図1Aおよび図1Bを参照すると、一部の実装において、システム100は、音声対応デバイス110(デバイス110またはユーザデバイス110とも呼ばれる)とインタラクションするためにユーザインタラクション12を提供するユーザ10を含む。ここで、ユーザインタラクション12とは、デバイス110からの応答を請うか、またはクエリによって指定されたタスクをデバイス110に実行させるためのクエリまたはコマンドに対応する口頭の発話12、12Uである。この意味で、ユーザ10は、コンピューティング活動を実行するかまたは質問に対する答えを見つけるために、音声対応デバイス110と会話のインタラクションをしてよい。 1A and 1B, in some implementations, the system 100 includes a user 10 providing a user interaction 12 to interact with a voice-enabled device 110 (also referred to as a device 110 or a user device 110). Here, the user interaction 12 is a verbal utterance 12, 12U corresponding to a query or command to solicit a response from the device 110 or to cause the device 110 to perform a task specified by the query. In this sense, the user 10 may have a conversational interaction with the voice-enabled device 110 to perform a computing activity or find an answer to a question.
デバイス110は、スピーチ環境内の1人または複数のユーザ10からのスピーチなどのユーザインタラクション12をキャプチャするように構成される。ユーザ10によって話された発話12Uは、デバイス110によってキャプチャされてよく、デバイス110上で実行されるデジタルアシスタントインターフェース120が動作/タスクを実行するためのクエリまたはコマンドに対応する場合がある。デバイス110は、ユーザ10に関連し、オーディオ信号を受信することができる任意のコンピューティングデバイスに対応する場合がある。ユーザデバイス110のいくつかの例は、モバイルデバイス(たとえば、モバイル電話、タブレット、ラップトップ、電子書籍リーダなど)、コンピュータ、ウェアラブルデバイス(たとえば、スマートウォッチ)、音楽プレーヤー、キャスティングデバイス(casting device)、スマート家電(たとえば、スマートテレビ)およびモノのインターネット(IoT)デバイス、リモコン、スマートスピーカなどを含むがこれらに限定されない。デバイス110は、データ処理ハードウェア112と、データ処理ハードウェア112と通信し、データ処理ハードウェア112によって実行されるときにデータ処理ハードウェア112に音声処理に関連する1つまたは複数の動作を実行させる命令を記憶するメモリハードウェア114とを含む。 The device 110 is configured to capture user interactions 12, such as speech, from one or more users 10 in a speech environment. The utterances 12U spoken by the users 10 may be captured by the device 110 and may correspond to queries or commands for a digital assistant interface 120 executing on the device 110 to perform an action/task. The device 110 may correspond to any computing device associated with the user 10 and capable of receiving an audio signal. Some examples of user devices 110 include, but are not limited to, mobile devices (e.g., mobile phones, tablets, laptops, e-book readers, etc.), computers, wearable devices (e.g., smart watches), music players, casting devices, smart appliances (e.g., smart TVs) and Internet of Things (IoT) devices, remote controls, smart speakers, etc. The device 110 includes data processing hardware 112 and memory hardware 114 that communicates with the data processing hardware 112 and stores instructions that, when executed by the data processing hardware 112, cause the data processing hardware 112 to perform one or more actions related to speech processing.
デバイス110は、スピーチ環境内のオーディオデータをキャプチャし、電気信号に変換するためのオーディオキャプチャデバイス(たとえば、1つまたは複数のマイクロフォンのアレイ)116を有するオーディオサブシステムをさらに含む。デバイス110は、示された例において、オーディオキャプチャデバイス116(概してマイクロフォン116とも呼ばれる)を実装するが、オーディオキャプチャデバイス116は、デバイス110上に物理的に存在せず、オーディオサブシステム(たとえば、デバイス110の周辺機器)と通信する場合がある。たとえば、デバイス110は、車両全体に配置されたマイクロフォンのアレイを利用する車両インフォテインメントシステムに対応する場合がある。 The device 110 further includes an audio subsystem having an audio capture device 116 (e.g., an array of one or more microphones) for capturing audio data in the speech environment and converting it into an electrical signal. Although the device 110 implements the audio capture device 116 (also generally referred to as a microphone 116) in the illustrated example, the audio capture device 116 may not be physically present on the device 110 and may communicate with the audio subsystem (e.g., peripherals of the device 110). For example, the device 110 may correspond to a vehicle infotainment system that utilizes an array of microphones located throughout the vehicle.
音声対応インターフェース(たとえば、デジタルアシスタントインターフェース)120は、デバイス110によってキャプチャされた口頭の発話12Uで伝えられた問い合わせまたはコマンドをさばく場合がある。音声対応インターフェース120(インターフェース120またはアシスタントインターフェース120とも呼ばれる)は、概して、発話12Uに対応するオーディオデータ124を受信し、オーディオデータ124に対する音声処理または発話12Uに由来するその他の活動を調整して応答122を生成することを容易にする。インターフェース120は、デバイス110のデータ処理ハードウェア112上で実行されてよい。インターフェース120は、発話12Uを含むオーディオデータ124を音声処理に関連する様々なシステムに流してよい。たとえば、図1は、インターフェース120が音声認識システム150と通信することを示す。ここで、インターフェース120は、発話12Uに対応するオーディオデータ124を受信し、音声認識システム150にオーディオデータ124を提供する。一部の構成において、インターフェース120は、デバイス110のマイクロフォン116と音声認識システム150との間の開いた通信チャネルとして働く。言い換えると、マイクロフォン116は、オーディオストリーム16内の発話12Uをキャプチャし、インターフェース120は、オーディオストリーム16から変換された発話12Uに対応するオーディオデータ124を処理のために音声認識システム150に伝達する。より詳細には、音声認識システム150は、オーディオデータ124を処理して発話12Uの文字起こし(transcription)152を生成し、文字起こし152に対して意味解釈を実行して、実行する適切なアクションを特定してよい。デバイス110においてユーザ10とインタラクションするために使用されるインターフェース120は、インターフェース120の機能を実行するように構成された任意の種類のプログラムまたはアプリケーションであってよい。たとえば、インターフェース120は、デバイス110上でホストされる、またはデバイス110と通信するその他のプログラムとインターフェースをとるアプリケーションプログラミングインターフェース(API)である。 The voice-enabled interface (e.g., digital assistant interface) 120 may field queries or commands conveyed in verbal utterances 12U captured by the device 110. The voice-enabled interface 120 (also referred to as the interface 120 or assistant interface 120) generally receives audio data 124 corresponding to the utterances 12U and facilitates coordinating voice processing on the audio data 124 or other activities derived from the utterances 12U to generate responses 122. The interface 120 may execute on the data processing hardware 112 of the device 110. The interface 120 may stream the audio data 124 including the utterances 12U to various systems related to voice processing. For example, FIG. 1 shows that the interface 120 communicates with a voice recognition system 150. Here, the interface 120 receives the audio data 124 corresponding to the utterances 12U and provides the audio data 124 to the voice recognition system 150. In some configurations, the interface 120 serves as an open communication channel between the microphone 116 of the device 110 and the speech recognition system 150. In other words, the microphone 116 captures the speech 12U in the audio stream 16, and the interface 120 conveys audio data 124 corresponding to the speech 12U converted from the audio stream 16 to the speech recognition system 150 for processing. More specifically, the speech recognition system 150 may process the audio data 124 to generate a transcription 152 of the speech 12U and perform semantic interpretation on the transcription 152 to identify appropriate actions to perform. The interface 120 used to interact with the user 10 at the device 110 may be any type of program or application configured to perform the functions of the interface 120. For example, the interface 120 may be an application programming interface (API) that interfaces with other programs hosted on or in communication with the device 110.
特に図1Aの例を参照すると、ユーザ10による第1の発話12U、12Uaは、「Hey computer, who is the president of France?(ねえ、コンピュータ。フランスの大統領は誰)」と述べる。ここで、第1の発話12Uは、オーディオデータ124内で検出されるときに、マイクロフォン116を開き、クエリ「who is the president of France?(フランスの大統領は誰)」に対応する続いてキャプチャされるオーディオデータを、処理のために音声認識システム150に中継するようにインターフェース120をトリガするホットワード14を含む。すなわち、デバイス110は、スリープまたはハイバネーション状態であり、オーディオストリーム16内のホットワード14の存在を検出するためにホットワード検出器を実行してよい。呼び出しフレーズとして働いて、ホットワード14は、ホットワード検出器によって検出されるとき、ウェイクアップし、ホットワード14および/またはホットワード14に続く1つもしくは複数の語に対する音声認識を開始するようにデバイス110をトリガする。ホットワード検出器は、音声認識または意味解析を実行することなく、ホットワードを示す音響的特徴を検出するように構成されたニューラルネットワークベースのモデルであってよい。 1A, a first utterance 12U, 12Ua by a user 10 states, "Hey computer, who is the president of France?" Here, the first utterance 12U includes a hot word 14 that, when detected in the audio data 124, triggers the interface 120 to open the microphone 116 and relay subsequently captured audio data corresponding to the query "who is the president of France?" to the speech recognition system 150 for processing. That is, the device 110 may be in a sleep or hibernation state and execute a hot word detector to detect the presence of the hot word 14 in the audio stream 16. Acting as an invocation phrase, the hot word 14, when detected by the hot word detector, triggers the device 110 to wake up and initiate speech recognition for the hot word 14 and/or one or more words following the hot word 14. The hot word detector may be a neural network-based model configured to detect acoustic features indicative of hot words without performing speech recognition or semantic analysis.
ホットワード検出器によるオーディオストリーム16内のホットワード14の検出に応答して、インターフェース120は、この発話12Uaに対応するオーディオデータ124を音声認識システム150に中継し、音声認識システム150は、オーディオデータ124に対して音声認識を実行して、発話12Uaの音声認識結果(たとえば、文字起こし)152を生成する。音声認識システム150および/またはインターフェース120は、音声認識結果152に対して意味解釈を実行して、発話12Uaがフランスの大統領のアイデンティティ(identity)に関する検索クエリに対応すると判定する。ここで、インターフェース120は、「Who is the president of France?(フランスの大統領は誰)」というクエリに関して「Emmanuel Macron(エマニュエル・マクロン)」という検索結果162を検索し、返す検索エンジン160に文字起こし152を送ってよい。インターフェース120は、検索エンジン160から「Emmanuel Macron(エマニュエル・マクロン)」というこの検索結果162を受信し、さらに、第1の発話12Uaのクエリに対する応答122として、「Emmanuel Macron(エマニュエル・マクロン)」をユーザ10に伝達する。一部の例において、応答122は、デバイス110から可聴出力される合成音声を含む。 In response to detection of the hot word 14 in the audio stream 16 by the hot word detector, the interface 120 relays the audio data 124 corresponding to the utterance 12Ua to the speech recognition system 150, which performs speech recognition on the audio data 124 to generate a speech recognition result (e.g., a transcription) 152 of the utterance 12Ua. The speech recognition system 150 and/or the interface 120 perform semantic interpretation on the speech recognition result 152 to determine that the utterance 12Ua corresponds to a search query regarding the identity of the president of France. The interface 120 may then send the transcription 152 to a search engine 160, which searches and returns search results 162 of "Emmanuel Macron" for the query "Who is the president of France?" The interface 120 receives this search result 162 of "Emmanuel Macron" from the search engine 160 and further communicates "Emmanuel Macron" to the user 10 as a response 122 to the query of the first utterance 12Ua. In some examples, the response 122 includes a synthesized voice that is audibly output from the device 110.
アシスタントインターフェース120の機能を実行するために、インターフェース120は、デバイス110の1つまたは複数の周辺機器(たとえば、オーディオサブシステムの1つまたは複数の構成要素)を制御するように構成されてよい。一部の例において、インターフェース120は、いつマイクロフォン116が開いている、つまり、何らかの音声処理の目的のためにオーディオデータ124をアクティブに受信しているか、または閉じている、つまり、オーディオデータ124を受信していないか、もしくは音声処理の目的のために制限された量のオーディオデータ124を受信しているかを指令するためにマイクロフォン116を制御する。ここで、マイクロフォン116が「開いている」かまたは「閉じている」かは、インターフェース120が、マイクロフォン116において受信されたオーディオストリーム16に含まれる任意の発話12Uの音声認識を可能にするために音声認識システム150との通信の開いたチャネルを有するか、または音声認識システム150がオーディオストリーム16に対して音声認識を実行することを不可能にするために音声認識システム150との通信の閉じたチャネルを有するように、インターフェース120がマイクロフォン116において受信されたオーディオデータ124を音声認識システム150に伝達するかどうかを指す場合がある。一部の実装において、インターフェース120は、インターフェース120がインタラクション12および/またはトリガ14をともなうインタラクション12を受信するまたは受信したかどうかに基づいて、そのようなチャネルが開いているのかまたは閉じているのかを指定または指示する。たとえば、インターフェース120がトリガ14をともなうインタラクション12(たとえば、ホットワードをともなう発話12U)を受信するとき、インターフェース120は、マイクロフォン116に開くように指示し、マイクロフォン116によってキャプチャされたオーディオから変換されたオーディオデータ124を音声認識システム150に中継する。発話が完了した後、インターフェース120は、音声認識システム150が追加のオーディオデータ124を処理することを防止するためにマイクロフォン116に閉じるように指示してよい。 To perform the functions of the assistant interface 120, the interface 120 may be configured to control one or more peripherals of the device 110 (e.g., one or more components of an audio subsystem). In some examples, the interface 120 controls the microphone 116 to command when the microphone 116 is open, i.e., actively receiving audio data 124 for some voice processing purposes, or closed, i.e., not receiving audio data 124 or receiving a limited amount of audio data 124 for voice processing purposes. Here, whether the microphone 116 is "open" or "closed" may refer to whether the interface 120 communicates audio data 124 received at the microphone 116 to the voice recognition system 150 such that the interface 120 has an open channel of communication with the voice recognition system 150 to enable voice recognition of any utterance 12U contained in the audio stream 16 received at the microphone 116, or has a closed channel of communication with the voice recognition system 150 to prevent the voice recognition system 150 from performing voice recognition on the audio stream 16. In some implementations, the interface 120 designates or indicates whether such a channel is open or closed based on whether the interface 120 receives or has received an interaction 12 and/or an interaction 12 with a trigger 14. For example, when the interface 120 receives an interaction 12 with a trigger 14 (e.g., an utterance 12U with a hotword), the interface 120 instructs the microphone 116 to open and relays audio data 124 converted from the audio captured by the microphone 116 to the voice recognition system 150. After the utterance is completed, the interface 120 may instruct the microphone 116 to close to prevent the voice recognition system 150 from processing additional audio data 124.
一部の実装において、デバイス110は、ネットワーク130を介してリモートシステム140と通信する。リモートシステム140は、リモートデータ処理ハードウェア144(たとえば、リモートサーバもしくはCPU)および/またはリモートメモリハードウェア146(たとえば、リモートデータベースもしくはその他のストレージハードウェア)などのリモートリソース142を含んでよい。デバイス110は、リモートリソース142を利用して、音声処理に関連する様々な機能を実行する場合がある。たとえば、検索エンジン160が、リモートシステム140に存在する場合があり、および/または音声認識システム150の機能の一部が、リモートシステム140に存在する場合がある。一例において、音声認識システム150は、オンデバイス自動音声認識(ASR)を実行するためにデバイス110上に存在する場合がある。別の例において、音声認識システム150は、サーバサイドASRを提供するためにリモートシステム上に存在する。さらに別の例において、音声認識システム150の機能は、デバイス110とサーバ140とに分けられる。たとえば、図1Aは、音声認識システム150および検索エンジン160がデバイス110上にまたはサーバサイドに(すなわち、リモートシステム140に)存在する場合があることを示すために、これらの構成要素を点線のボックスで示す。 In some implementations, the device 110 communicates with the remote system 140 over the network 130. The remote system 140 may include remote resources 142, such as remote data processing hardware 144 (e.g., a remote server or CPU) and/or remote memory hardware 146 (e.g., a remote database or other storage hardware). The device 110 may utilize the remote resources 142 to perform various functions related to speech processing. For example, a search engine 160 may reside on the remote system 140, and/or some of the functionality of the speech recognition system 150 may reside on the remote system 140. In one example, the speech recognition system 150 may reside on the device 110 to perform on-device automatic speech recognition (ASR). In another example, the speech recognition system 150 resides on the remote system to provide server-side ASR. In yet another example, the functionality of the speech recognition system 150 is split between the device 110 and the server 140. For example, FIG. 1A illustrates the speech recognition system 150 and the search engine 160 with dotted boxes to indicate that these components may reside on the device 110 or on the server side (i.e., on the remote system 140).
一部の構成においては、異なる種類の音声認識モデルが、モデルに応じて異なる場所(たとえば、オンデバイスまたはリモート)に存在する。同様に、エンドツーエンドまたはストリーミングベースの音声認識モデルは、その空間効率の良いサイズのおかげでデバイス110上に存在する場合があり、一方、複数のモデル(たとえば、音響モデル(AM)、発音モデル(PM)、および言語モデル(LM))から構築されるより大きくより従来の音声認識モデルは、オンデバイスではなくリモートシステム140に存在するサーバベースのモデルである。言い換えると、音声認識の所望のレベル、および/または音声認識を実行する所望の速度に応じて、音声認識は、オンデバイス(すなわち、ユーザサイド)またはリモート(すなわち、サーバサイド)に存在する場合がある。 In some configurations, different types of speech recognition models reside in different locations (e.g., on-device or remote) depending on the model. Similarly, end-to-end or streaming-based speech recognition models may reside on the device 110 due to their space-efficient size, while larger, more traditional speech recognition models built from multiple models (e.g., acoustic models (AM), pronunciation models (PM), and language models (LM)) are server-based models that reside on the remote system 140 rather than on-device. In other words, depending on the desired level of speech recognition and/or the desired speed at which to perform the speech recognition, the speech recognition may reside on-device (i.e., user-side) or remote (i.e., server-side).
ユーザ10がインターフェース120と会話式に関わり合うとき、ユーザ10がインターフェース120からの何らかのフィードバック(たとえば、応答122)を望む各インタラクション12に関して、ユーザ10が同じ呼び出しフレーズ(たとえば、ホットワード)14を言うことを繰り返すことはかなり不便になり得る。言い換えると、ユーザ10による相次ぐクエリのたびに同じホットワードを言うことをユーザに要求することは、非常に面倒で不便になり得る。さらに残念なことに、デバイス110が、マイクロフォン116において受信されたすべてのオーディオデータ124に対して音声認識を継続的に実行していることは、コンピューティングリソースの無駄でもあり、計算コストが高くなり得る。 When a user 10 conversationally engages with the interface 120, it can become quite inconvenient for the user 10 to repeatedly say the same invocation phrase (e.g., hot word) 14 for each interaction 12 for which the user 10 desires some feedback (e.g., response 122) from the interface 120. In other words, requiring the user 10 to say the same hot word for each successive query by the user 10 can become very tedious and inconvenient. Even more unfortunately, the device 110 continually performing speech recognition on all audio data 124 received at the microphone 116 can also be a waste of computing resources and computationally expensive.
ユーザ10がインターフェース120とアクティブに会話(またはインタラクション12のセッション)をしている間にユーザ10が新しいクエリ14を伝達することを望むたびにホットワード14を言うことを繰り返すようにユーザ10に要求する不便さに対処するために、インターフェース120は、ホットワード14を言うようにユーザ10に要求することなく、インターフェース120が前のクエリに応答した後にユーザ10が追加クエリを与えることを可能にしてよい。すなわち、クエリに対する応答122は、スピーチによる音声対応デバイス110との起こり得るユーザインタラクション12を示すマイクロフォントリガイベント202(図1B)のインジケーションとして働き、それによって、デバイス110に、オーディオストリーム16をキャプチャし、オーディオストリーム16に対して音声認識処理を実行するために音声認識システム150にキャプチャされた音声ストリーム16を提供するためにマイクロフォン116に開くようにまたは開いたままであるように指示させてよい。ここで、マイクロフォン116は、クエリを受け入れるために開くようにマイクロフォン116をトリガするためにホットワード14を最初にもう一度言うようにユーザ10に要求することなくユーザ10からのクエリを受け入れるために開いていてよい。 To address the inconvenience of requiring the user 10 to repeatedly say the hotword 14 each time the user 10 wishes to communicate a new query 14 while the user 10 is actively engaged in a conversation (or interaction 12 session) with the interface 120, the interface 120 may allow the user 10 to give an additional query after the interface 120 has responded to a previous query without requiring the user 10 to say the hotword 14. That is, the response 122 to the query may serve as an indication of a microphone trigger event 202 (FIG. 1B) indicating a possible user interaction 12 with the voice-enabled device 110 by speech, thereby causing the device 110 to instruct the microphone 116 to open or remain open to capture the audio stream 16 and provide the captured audio stream 16 to the speech recognition system 150 for performing speech recognition processing on the audio stream 16. Here, the microphone 116 may be open to accept queries from the user 10 without requiring the user 10 to first re-say the hotword 14 to trigger the microphone 116 to open to accept queries.
マイクロフォントリガイベント202(トリガイベント202とも呼ばれる)は、概して、ユーザ10がスピーチによってデバイス110とインタラクションすることができる可能性を示し、したがって、音声処理のためにすべてのスピーチをキャプチャするためにマイクロフォン116のアクティブ化を必要とするイベントの発生を指す。ここで、トリガイベント202が、起こり得るユーザインタラクション12を示すので、トリガイベント202は、ジェスチャから、認識されたユーザの特性(たとえば、行動のパターン)、デバイス110が潜在的なインタラクション12として見分けてよいユーザ10による任意のアクションにまで及ぶ場合がある。たとえば、ユーザ10は、キッチンに入るときに天気とユーザの日程表に何かイベントがあるかどうかとをデバイス110に問い合わせるという平日のルーチンを有する可能性がある。行動のこのパターンに起因して、デバイス110は、特定の時間の頃にユーザ10がキッチンに入ることを認識し(たとえば、キッチンの入口の方向での動きを聞き)、朝にユーザ10がキッチンに入るというアクションを、起こり得るユーザインタラクション12のトリガイベント202として扱ってよい。ジェスチャの場合、トリガイベント202は、ユーザ10がデバイス110を持ち上げる、デバイス110を強く握る、デバイス110のボタンを押す、デバイス110の画面をタップする、予め定義された方法で手を動かすというインタラクション12、またはユーザ10がアシスタントインターフェース120との会話に従事する意図がある場合があることを示すための任意のその他の種類の予めプログラミングされたジェスチャであってよい。トリガイベント202の別の例は、デバイス110(たとえば、インターフェース120)が、ユーザ10からのクエリに対する応答122を伝達するときである。言い換えると、インターフェース120が応答122をユーザ10に中継するとき、応答122は、デバイス110の代わりにユーザ10に対するコミュニケーションインタラクションとして働き、つまり、ユーザ10によって話される追加問い合わせが、応答122を受信した後に続いて発生する可能性が高いか、または単にユーザ10がデバイス110と現在会話しているために発生する場合がある。この可能性に基づいて、応答122は、デバイス110が応答122を出力した後、ホットワード14を追加クエリの前に置くことをユーザ10に要求することなく、ユーザ10によって話された追加クエリをキャプチャし、その音声処理を可能にするために、マイクロフォン116が開くかまたは開いたままであるかのどちらかであるようなトリガイベント202と見なされてよい。 A microphone trigger event 202 (also referred to as a trigger event 202) generally refers to the occurrence of an event that indicates the possibility that the user 10 may interact with the device 110 by speech and thus requires activation of the microphone 116 to capture all speech for voice processing. Here, as the trigger event 202 indicates a possible user interaction 12, the trigger event 202 may range from a gesture to a recognized user characteristic (e.g., a pattern of behavior) to any action by the user 10 that the device 110 may discern as a potential interaction 12. For example, the user 10 may have a weekday routine of querying the device 110 for the weather and whether there are any events on the user's calendar when entering the kitchen. Due to this pattern of behavior, the device 110 may recognize that the user 10 enters the kitchen around a certain time (e.g., by listening for movement in the direction of the kitchen entrance) and may treat the action of the user 10 entering the kitchen in the morning as a trigger event 202 for a possible user interaction 12. In the case of gestures, the trigger event 202 may be an interaction 12 in which the user 10 lifts the device 110, squeezes the device 110, presses a button on the device 110, taps the screen of the device 110, moves the hand in a predefined manner, or any other type of pre-programmed gesture to indicate that the user 10 may intend to engage in a conversation with the assistant interface 120. Another example of the trigger event 202 is when the device 110 (e.g., the interface 120) conveys a response 122 to a query from the user 10. In other words, when the interface 120 relays the response 122 to the user 10, the response 122 acts as a communication interaction for the user 10 on behalf of the device 110, i.e., additional queries spoken by the user 10 are likely to occur subsequently after receiving the response 122, or may occur simply because the user 10 is currently having a conversation with the device 110. Based on this possibility, the response 122 may be considered a trigger event 202 such that after the device 110 outputs the response 122, the microphone 116 either opens or remains open to capture and enable audio processing of any additional queries spoken by the user 10 without requiring the user 10 to precede the additional queries with the hotword 14.
マイクロフォントリガイベント202を実施し、処理リソースを損なうことなくデバイス110とユーザ10との間の会話を提供するために、デバイス110は、ユーザ10との会話全体を通じて音声認識も維持しながら、ユーザ10がアシスタントインターフェース120に話しかけていることを認識するインタラクション分析器200(分析器200とも呼ばれる)を配備する。言い換えると、分析器200は、インタラクション12の端点を決定し(endpoint)、音声認識を非アクティブ化するために、ユーザ10とインターフェース120との間の会話がいつ始まるかと、いつ会話が終了したと考えることが最善であるかとを特定することによって、端点決定機能(endpointing functionality)を提供してよい。さらに、端点を決定することに加えて、分析器200は、ユーザ10による1つまたは複数のインタラクション12の性質に応じて音声認識の処理レベル222(図2Aおよび図2B)を下げること(すなわち、音声認識システム150を使用すること)によって、音声認識のプロセスを修正してもよい。特に、分析器200は、マイクロフォン116によってキャプチャされたオーディオストリーム16に対して音声認識の減衰されたレベル222を使用するように音声認識システム150に指示することができてよい。たとえば、デバイス110が、マイクロフォントリガイベント202のインジケーションの受信に応答して、オーディオストリーム16をキャプチャし、音声認識システム150に提供するために、開くように、または開いたマイクロフォン継続時間ウィンドウの間、開いたままであるようにマイクロフォン116に指示するとき、分析器200は、音声認識システム150がオーディオストリーム16に対して実行する音声処理のレベル222を、開いたマイクロフォン継続時間ウィンドウ212の関数として減衰させてよい。ここで、分析器200は、音声認識および/またはスピーチに関連する処理を制御するためにインターフェース120と連携して動作する。 To implement the microphone trigger event 202 and provide a conversation between the device 110 and the user 10 without compromising processing resources, the device 110 deploys an interaction analyzer 200 (also referred to as analyzer 200) that recognizes that the user 10 is speaking to the assistant interface 120 while also maintaining voice recognition throughout the conversation with the user 10. In other words, the analyzer 200 may provide endpointing functionality by identifying when the conversation between the user 10 and the interface 120 begins and when it is best to consider the conversation ended in order to determine the endpoints of the interaction 12 and deactivate voice recognition. Furthermore, in addition to determining the endpoints, the analyzer 200 may modify the process of voice recognition by lowering the processing level 222 (FIGS. 2A and 2B) of voice recognition (i.e., using the voice recognition system 150) depending on the nature of one or more interactions 12 by the user 10. In particular, the analyzer 200 may instruct the speech recognition system 150 to use an attenuated level 222 of speech recognition for the audio stream 16 captured by the microphone 116. For example, when the device 110 instructs the microphone 116 to open or remain open for an open microphone duration window to capture and provide the audio stream 16 to the speech recognition system 150 in response to receiving an indication of the microphone trigger event 202, the analyzer 200 may attenuate the level 222 of speech processing that the speech recognition system 150 performs on the audio stream 16 as a function of the open microphone duration window 212. Here, the analyzer 200 works in conjunction with the interface 120 to control speech recognition and/or speech-related processing.
図1Bを参照すると、デバイス110(たとえば、インターフェース120)がトリガイベント202のインジケーションを受信するとき、インターフェース120は、マイクロフォン116に開くようにまたは開いたままであるように指示し、マイクロフォン116によってキャプチャされたオーディオストリーム16に関連するオーディオデータ124を処理のために音声認識システム150に伝達する。ここで、インターフェース120がマイクロフォン116に開くように指示するとき、分析器200は、マイクロフォン116が、トリガイベント202による開始後、何らかの開いたマイクロフォン継続時間ウィンドウ212の間、開いたままであり続けることを指定するよう構成されてよい。たとえば、開いたマイクロフォン継続時間ウィンドウ212は、インターフェース120がマイクロフォン116によってキャプチャされたオーディオデータ124のオーディオストリーム16を音声認識システム150に伝達する期間を指定する。ここで、マイクロフォン継続時間ウィンドウ212とは、トリガイベント202の受信時に開始し、設定された継続時間後に終わる(たとえば、マイクロフォンを閉じるイベントを経る)設定された継続時間を指す。一部の例において、分析器200は、インターフェース120がマイクロフォン継続時間ウィンドウ212の間に別のトリガイベント202(たとえば、後続のトリガイベント202)を受信するとき、マイクロフォン継続時間ウィンドウ212を延長するか、またはマイクロフォン継続時間ウィンドウ212をリフレッシュする(すなわち、新しくする)ようにインターフェース120に指示してよい。例として、図1Aおよび図1Bは、ユーザ10が、「Hey computer, who is the president of France?(ねえ、コンピュータ。フランスの大統領は誰)」と述べる第1の発話12Uaと、フランスの大統領はエマニュエル・マクロンであるという応答122に対する追加質問として、「how old is he?(彼は何歳)」という第2の発話12U、12Ubとを生成することを示す。特に図1Bを参照すると、「hey computer(ねえ、コンピュータ)」というホットワード14は、発話の続きの部分「who is the president of France?(フランスの大統領は誰)」の音声認識を開始する決まり文句に対応する。インターフェース120が「Emmanuel Macron(エマニュエル・マクロン)」であると応答するとき、分析器200は、応答122を、第1のマイクロフォン継続時間ウィンドウ212、212aを開始させるマイクロフォントリガイベント202として確立する。ここで、ウィンドウ212は、開始点214および終了点216によって定義される継続時間を有してよく、終了点216の時点で、マイクロフォン継続時間ウィンドウ212は満了し、インターフェース120および/または分析器200はマイクロフォンを閉じるイベントを実行する。さらに、この例において、ユーザ10は、第1のマイクロフォン継続時間ウィンドウ212aの当初指定された終了点216a、216の前に、「how old is he?(彼は何歳)」という追加質問をする。第2の発話12Ubの追加質問(すなわち、別のユーザインタラクション12)が原因で、インターフェース120は、エマニュエル・マクロンの年齢が「47」であると述べる第2の応答122、122bを生成する。「Emmanuel Macron(エマニュエル・マクロン)」という第1の応答122、122aと同様に、第2の応答122、122bは、たとえ第1のマイクロフォン継続時間ウィンドウ212aが満了した可能性があるかまたは満了しなかった可能性があるとしても(たとえば、図1Bは、第1のウィンドウ212aが第2の応答122bの前に満了したことを示す)、第2のマイクロフォン継続時間ウィンドウ212bとして新しいマイクロフォン継続時間ウィンドウ212、212bを開始する第2のトリガイベント202、202bである。マイクロフォン継続時間ウィンドウ212がまだ終了していない間にインターフェース120が新しいマイクロフォン継続時間ウィンドウ212を開始するとき、これは、終了していないウィンドウ212を延長するか、または終了していないウィンドウ212が開いたままにすることと見なされてよい。一部の実装において、継続時間ウィンドウ212がまだ終了していないとき、そのウィンドウ212の間のトリガイベント202は、継続時間ウィンドウ212を完全に新しく(すなわち、継続時間ウィンドウをあらためて開始する)のではなく、終了していない継続時間ウィンドウ212をある指定された量の時間だけ延長してよい。たとえば、継続時間ウィンドウ212が10秒であり、継続時間ウィンドウ212が現在終了していない場合、この終了していない継続時間ウィンドウ212の間のトリガイベント202は、終了していないウィンドウ212を、新たな追加の丸々10秒ではなく、追加の5秒延長する。図1Bにおいて、第1のウィンドウ212aは、第2のトリガイベント202bが第2の開始点214、214bでまったく新しい継続時間ウィンドウ212、212bを開始するように、第2の応答122bの前に終了し、この継続時間ウィンドウ212、212bは、(たとえば、1つまたは複数の追加のトリガイベント202が発生しない限り)第2の終了点216、216bで終了する。 With reference to FIG. 1B, when the device 110 (e.g., the interface 120) receives an indication of a trigger event 202, the interface 120 instructs the microphone 116 to open or remain open and transmits audio data 124 associated with the audio stream 16 captured by the microphone 116 to the speech recognition system 150 for processing. Here, when the interface 120 instructs the microphone 116 to open, the analyzer 200 may be configured to specify that the microphone 116 remains open for some open microphone duration window 212 after initiation by the trigger event 202. For example, the open microphone duration window 212 specifies the period during which the interface 120 transmits the audio stream 16 of the audio data 124 captured by the microphone 116 to the speech recognition system 150. Here, the microphone duration window 212 refers to a set duration that begins upon receipt of the trigger event 202 and ends after the set duration (e.g., via a microphone closing event). In some examples, the analyzer 200 may instruct the interface 120 to extend the microphone duration window 212 or refresh (i.e., refresh) the microphone duration window 212 when the interface 120 receives another trigger event 202 (e.g., a subsequent trigger event 202) during the microphone duration window 212. By way of example, FIGS. 1A and 1B show a user 10 generating a first utterance 12Ua stating "Hey computer, who is the president of France?" and a second utterance 12U, 12Ub of "how old is he?" as a follow-up question to a response 122 that the president of France is Emmanuel Macron. With particular reference to FIG. 1B, the hot word 14 "hey computer" corresponds to a phrase that initiates speech recognition of the subsequent portion of the utterance "who is the president of France?". When the interface 120 responds that it is "Emmanuel Macron", the analyzer 200 establishes the response 122 as a microphone trigger event 202 that initiates a first microphone duration window 212, 212a, where the window 212 may have a duration defined by a start point 214 and an end point 216, at which point the microphone duration window 212 expires and the interface 120 and/or the analyzer 200 executes a microphone closing event. Furthermore, in this example, the user 10 asks an additional question "how old is he?" before the originally specified end point 216a, 216 of the first microphone duration window 212a. The additional question of the second utterance 12Ub (i.e., another user interaction 12) causes the interface 120 to generate a second response 122, 122b stating that Emmanuel Macron's age is "47". Similar to the first response 122, 122a of "Emmanuel Macron," the second response 122, 122b is a second triggering event 202, 202b that starts a new microphone duration window 212, 212b as the second microphone duration window 212b, even though the first microphone duration window 212a may or may not have expired (e.g., FIG. 1B shows that the first window 212a expired before the second response 122b). When the interface 120 starts a new microphone duration window 212 while the microphone duration window 212 has not yet expired, this may be considered to extend the unexpired window 212 or to keep the unexpired window 212 open. In some implementations, when a duration window 212 has not yet ended, a trigger event 202 during that window 212 may extend the unended duration window 212 by some specified amount of time, rather than starting the duration window 212 entirely anew (i.e., starting the duration window anew). For example, if the duration window 212 is 10 seconds and the duration window 212 is not currently ending, a trigger event 202 during the unended duration window 212 will extend the unended window 212 an additional 5 seconds, rather than a new additional full 10 seconds. In FIG. 1B, the first window 212a ends before the second response 122b, such that the second trigger event 202b starts an entirely new duration window 212, 212b at a second starting point 214, 214b, which ends at a second ending point 216, 216b (unless, for example, one or more additional trigger events 202 occur).
引き続き図1Bを参照すると、分析器200は、ウィンドウ212のある部分の間に減衰状態204をさらに生成するように構成される。たとえば、減衰状態204に遷移することによって、音声認識の処理レベル222は、ウィンドウ212の間に変更される(たとえば、ある量だけ下げられる)場合がある。言い換えると、オーディオデータ124に対して音声認識システム150によって実行される音声処理のレベル222は、ウィンドウ212の関数に基づいてよい。音声処理の(たとえば、音声認識に関する)レベル222の基礎をウィンドウ212に置くことによって、オーディオデータ124のオーディオストリーム16に対して実行される音声処理の量は、時間の関数となる。つまり、時間が経過し、インターフェース120とユーザ10との間の会話がまだ続いている可能性がより低そうである(たとえば、オーディオデータ124がユーザのスピーチを含まない、またはスピーチがデバイス110に向けられていない)とき、分析器200は、この低下する可能性を認識し、それに応じて音声認識を引き下げる。たとえば、図1Bは、第1のマイクロフォン継続時間ウィンドウ212aの最後の半分の間、音声処理が、音声処理のレベル222が引き下げられる減衰状態204に遷移することを示す。この手法は、分析器200および/またはインターフェース120が、デバイス110において音声処理を実行するために使用されているコンピューティングリソースの量を削減することを可能にする場合がある。同様に、図1Bのこの例においては、分析器200が音声認識を減衰状態204に遷移させたとき、第1のウィンドウ212aの最後の3分の1の間に、たとえユーザ10が「how old is he?(彼は何歳)」という追加質問を生成するとしても、分析器200は、音声認識が第1のウィンドウ212において既に減衰状態204にあり、ユーザ10が第2のインタラクション12Ubを実行する可能性が既にいくらか低かったので、第2のウィンドウ212bの減衰状態204がより迅速に発生すべきであると決定し、したがって、分析器200は、減衰状態204が第2の継続時間ウィンドウ212bにおいてより長い期間、たとえば、第2の継続時間ウィンドウ212bの約90%の間発生すべきであると決定する。分析器200を利用することによって、デバイス110および/またはインターフェース120は、ホットワード14なしで音声認識を実行する能力を維持することと、ユーザ10がアシスタントインターフェース120とさらにインタラクションしようと意図する可能性が低いときに、音声認識をアクティブに実行することを避けようとすることとの間のバランスを取るように努める。 Continuing to refer to FIG. 1B, the analyzer 200 is further configured to generate an attenuated state 204 during a portion of the window 212. For example, by transitioning to the attenuated state 204, the processing level 222 of the speech recognition may be altered (e.g., lowered by a certain amount) during the window 212. In other words, the level 222 of speech processing performed by the speech recognition system 150 on the audio data 124 may be based on a function of the window 212. By basing the level 222 of speech processing (e.g., with respect to speech recognition) on the window 212, the amount of speech processing performed on the audio stream 16 of the audio data 124 is a function of time. That is, as time passes and it becomes less likely that a conversation between the interface 120 and the user 10 is still ongoing (e.g., the audio data 124 does not include the user's speech or speech is not directed to the device 110), the analyzer 200 recognizes this possibility of lowering and lowers the speech recognition accordingly. 1B shows that during the last half of the first microphone duration window 212a, the audio processing transitions to an attenuated state 204 where the level of audio processing 222 is reduced. This approach may allow the analyzer 200 and/or interface 120 to reduce the amount of computing resources being used to perform audio processing at the device 110. Similarly, in this example of FIG. 1B, when the analyzer 200 transitions the speech recognition to the attenuated state 204, even if the user 10 generates an additional question, "how old is he?", during the last third of the first window 212a, the analyzer 200 determines that the attenuated state 204 in the second window 212b should occur more quickly because the speech recognition was already in the attenuated state 204 in the first window 212 and the likelihood of the user 10 performing the second interaction 12Ub was already somewhat low, and therefore the analyzer 200 determines that the attenuated state 204 should occur for a longer period in the second duration window 212b, for example, for approximately 90% of the second duration window 212b. By utilizing the analyzer 200, the device 110 and/or interface 120 strives to strike a balance between maintaining the ability to perform speech recognition without the hotwords 14 and trying to avoid actively performing speech recognition when the user 10 is unlikely to intend to further interact with the assistant interface 120.
図2Aおよび図2Bを参照すると、分析器200は、概して、ウィンドウ生成器210およびリデューサ(reducer)220を含む。ウィンドウ生成器210(生成器210とも呼ばれる)は、オーディオストリーム16をキャプチャし、処理のために音声認識システム150に提供するためにマイクロフォン116が開いている継続時間を決める開いたマイクロフォン継続時間ウィンドウ212を生成するように構成される。各マイクロフォン継続時間ウィンドウ212は、マイクロフォン116において受信されたオーディオストリーム16が処理のために音声認識システム150に送信され始めるマイクロフォン継続時間ウィンドウ212の始めの時間を指定する開始点214と、開いたマイクロフォン継続時間ウィンドウ212が終了し、その後オーディオストリーム16が処理のために音声認識システム150にもはや伝達されなくなる時間を指定する終了点216とを含む。ウィンドウ生成器210は、最初に、インターフェース120がユーザ10からトリガイベント202を受信したと判定されるときに開いたマイクロフォン継続時間ウィンドウ212を生成する。 2A and 2B, the analyzer 200 generally includes a window generator 210 and a reducer 220. The window generator 210 (also referred to as generator 210) is configured to generate an open microphone duration window 212 that determines the duration during which the microphone 116 is open to capture and provide the audio stream 16 to the speech recognition system 150 for processing. Each microphone duration window 212 includes a start point 214 that specifies the time at which the microphone duration window 212 begins when the audio stream 16 received at the microphone 116 begins to be transmitted to the speech recognition system 150 for processing, and an end point 216 that specifies the time at which the open microphone duration window 212 ends after which the audio stream 16 is no longer transmitted to the speech recognition system 150 for processing. The window generator 210 initially generates the open microphone duration window 212 when it is determined that the interface 120 has received a trigger event 202 from the user 10.
一部の実装において、ウィンドウ生成器210は、分析器200の構成に基づいて、またはマイクロフォン116において発生する条件に基づいてインテリジェントに、異なるサイズのウィンドウ212(すなわち、異なる長さの時間を有するウィンドウ212)を生成するように構成される。たとえば、分析器200の管理者は、ウィンドウ生成器210によって生成されるウィンドウ212のデフォルトの継続時間を設定する(たとえば、各ウィンドウ212は10秒の長さである)。対照的に、生成器210は、ユーザ10による行動のパターン、またはユーザ10による発話12Uの態様/特徴を認識し、これらの認識された特性に一致または対応するサイズを有するウィンドウ212をインテリジェントに生成してよい。たとえば、特定のユーザ10が、インターフェース120との会話を開始するためにトリガ14を生成するたびに、複数の質問に従事するのが普通である可能性がある。ここで、デバイス110に関連する音声処理システムは、ユーザ10のアイデンティティを特定してよく、このアイデンティティに基づいて、生成器210は、インタラクションセッション(すなわち、インターフェース120との会話)中のそのユーザ10のインタラクションの頻度に対応するまたは応じるサイズを有するウィンドウ212を生成する。たとえば、生成器210は、5秒の継続時間を有するデフォルトのウィンドウ212を生成する代わりに、ホットワード14を送ったユーザ10がインターフェース120とのより高い頻度のインタラクション12を生成する傾向があるので、10秒の継続時間を有するウィンドウ212を生成する。一方、ホットワード14を送ったユーザ10が、インターフェース120と対話するときにはいつも単一のクエリのみを尋ねる傾向がある場合、生成器210は、ユーザ10についてのこの行動情報を受信し、デフォルトのウィンドウ212を5秒から3秒のカスタムウィンドウ212に短縮してよい。 In some implementations, the window generator 210 is configured to generate windows 212 of different sizes (i.e., windows 212 having different lengths of time) based on the configuration of the analyzer 200 or intelligently based on conditions occurring at the microphone 116. For example, an administrator of the analyzer 200 sets a default duration for the windows 212 generated by the window generator 210 (e.g., each window 212 is 10 seconds long). In contrast, the generator 210 may recognize patterns of behavior by the user 10 or aspects/characteristics of the utterances 12U by the user 10 and intelligently generate windows 212 having sizes that match or correspond to these recognized characteristics. For example, it may be common for a particular user 10 to engage in multiple questions each time they generate a trigger 14 to initiate a conversation with the interface 120. Here, a voice processing system associated with the device 110 may determine the identity of the user 10, and based on this identity, the generator 210 may generate a window 212 having a size that corresponds or responds to the frequency of interactions of that user 10 during an interaction session (i.e., a conversation with the interface 120). For example, instead of generating a default window 212 having a duration of 5 seconds, the generator 210 may generate a window 212 having a duration of 10 seconds, since the user 10 who sent the hotword 14 tends to generate a higher frequency of interactions 12 with the interface 120. On the other hand, if the user 10 who sent the hotword 14 tends to only ask a single query whenever interacting with the interface 120, the generator 210 may receive this behavioral information about the user 10 and shorten the default window 212 from 5 seconds to a custom window 212 of 3 seconds.
生成器210は、発話12Uまたは発話12Uに対する応答122の態様または特徴に基づいて、カスタムの開いたマイクロフォン継続時間ウィンドウ212を生成してもよい。基本的な例として、ユーザ10は、「hey computer, play the new Smashing Pumpkins album, Cyr.(ねえ、コンピュータ。スマッシング・パンプキンズの新しいアルバム『Cyr』を再生して)」という発話12Uを述べる。しかし、インターフェース120がこのコマンドに応答しようとするとき、インターフェース120は、スマッシング・パンプキンズの新しいアルバムCyrがその月のもっと後にリリースされる予定であると判定し、アルバムが現在入手可能でないことを示す応答122を提供する。応答122は、さらに、ユーザ10が他の何かを聴きたいかどうかを尋ねてよい。ここでは、インターフェース120による応答122に続いて、生成部210は、インターフェース120および/またはデバイス110が、要求されたアルバムが利用可能でないという応答122をインターフェース120が生成するとき、ユーザ10による追加インタラクション12がある可能性が高いと判定するという事実に基づいて、より大きなウィンドウ212(すなわち、より長い継続時間にわたり持続するウィンドウ212)を生成する場合があり、または最初に生成されたウィンドウ212を自動的に延長する場合がある。言い換えると、インターフェース120および/またはデバイス110は、ユーザ10によるコマンドの結果が原因で、ユーザ10が応答122に続いて別の音楽の要求を送る可能性が高いことをインテリジェントに認識する。 The generator 210 may generate a custom open microphone duration window 212 based on aspects or characteristics of the utterance 12U or the response 122 to the utterance 12U. As a basic example, the user 10 utters the utterance 12U, "hey computer, play the new Smashing Pumpkins album, Cyr." However, when the interface 120 attempts to respond to this command, the interface 120 determines that the new Smashing Pumpkins album, Cyr, is scheduled to be released later in the month, and provides a response 122 indicating that the album is not currently available. The response 122 may further ask if the user 10 would like to listen to something else. Here, following a response 122 by the interface 120, the generator 210 may generate a larger window 212 (i.e., a window 212 that lasts for a longer duration) or may automatically extend the initially generated window 212 based on the fact that the interface 120 and/or the device 110 determine that there is a high probability of additional interaction 12 by the user 10 when the interface 120 generates a response 122 that the requested album is not available. In other words, the interface 120 and/or the device 110 intelligently recognize that due to the outcome of the command by the user 10, the user 10 is likely to send another music request following the response 122.
図1Bが示すように、生成器210は、トリガイベント202が最初に受信されたとき(たとえば、インターフェース120が応答122を伝達するとき)にウィンドウ212を生成するだけでなく、開いたウィンドウ212の間にトリガイベント202がインターフェース120によって受信されるときに、新しいウィンドウ212を生成するか、または現在開いているウィンドウ212を延長してよい。開いたウィンドウ212の間にトリガイベント202が受信されるときに新しいウィンドウ212を生成すること、または開いたウィンドウ212を延長することによって、生成器210は、ユーザ10とインターフェース120との間の会話が続いているとき、ウィンドウ212を開いたままにし続けるように機能する。 1B illustrates, the generator 210 may not only generate the window 212 when the trigger event 202 is first received (e.g., when the interface 120 communicates the response 122), but may also generate a new window 212 or extend a currently open window 212 when a trigger event 202 is received by the interface 120 during an open window 212. By generating a new window 212 or extending an open window 212 when a trigger event 202 is received during an open window 212, the generator 210 functions to keep the window 212 open as the conversation between the user 10 and the interface 120 continues.
図2Bなどの一部の例においては、生成器210が継続中の会話においてウィンドウ212を延長するかまたは新しいウィンドウ212を生成するとき、生成器210は、ウィンドウ212の継続時間を割り引くように構成される。たとえば、生成器210は、ユーザ10によるそれぞれの後続のインタラクション12の後に、より短い継続時間の後続のウィンドウ212を生成するように構成される。この手法は、最初のインタラクション12の後、2番目のまたは追加のインタラクション12が発生する第1の確率を有するが、その後、さらなるインタラクション12が発生する確率が減少するという事実を考慮してよい。たとえば、第1の発話12Uaにおける最初の問い合わせの後、第2の発話12Ubにおける追加問い合わせは、ユーザ10とインターフェース120との間のインタラクションセッションの間に、約20%の確率で発生する可能性があるが、第2の発話12Ubにおける追加問い合わせの後の第3の発話12Uは、約5%の確率でのみ発生する。このパターンに基づいて、生成器210は、別のトリガイベント202(すなわち、起こり得るインタラクション12)が発生するこの確率の関数として、ウィンドウ212のサイズまたは開いたウィンドウ212が延長される長さを減らす場合がある。たとえば、図2Bは、3つのトリガイベント202、202a～cを示し、各トリガイベント202は、生成器210が第1のトリガイベント202aのための第1のウィンドウ212aを生成し、続いて、第1のトリガイベント202aの後に発生する第2のトリガイベント202bのための第2のウィンドウ212bを生成し、それから続いて、第2のトリガイベント202bの後に発生する第3のトリガイベント202cのための第3のウィンドウ212cを生成するように、インターフェース120からの後続の応答122(たとえば、3つの応答122a～c)に対応する。この例において、生成器210は、第3のウィンドウ212cが第2のウィンドウ212bよりも短い継続時間を有し、第2のウィンドウ212bが第1のウィンドウ212aよりも短い継続時間を有するように、生成されるウィンドウ212を短縮する。追加的または代替的に、生成器210は、オーディオストリーム16のオーディオデータ124を分析して、オーディオデータ124内の音声活動レベルが、トリガイベント202が発生する可能性が高いという何らかのインジケーションを提供するかどうかを判定してよい。この情報により、生成器210は、現在の開いたウィンドウ212のサイズを修正するか、または任意のその後生成/延長されるウィンドウ212のサイズを修正してよい。 In some examples, such as FIG. 2B, when the generator 210 extends the window 212 or generates a new window 212 in an ongoing conversation, the generator 210 is configured to discount the duration of the window 212. For example, the generator 210 is configured to generate a subsequent window 212 of shorter duration after each subsequent interaction 12 by the user 10. This approach may take into account the fact that after a first interaction 12, there is a first probability that a second or additional interaction 12 will occur, but thereafter the probability of further interactions 12 occurring decreases. For example, after a first inquiry in a first utterance 12Ua, an additional inquiry in a second utterance 12Ub may occur with a probability of about 20% during an interaction session between the user 10 and the interface 120, but a third utterance 12U after the additional inquiry in the second utterance 12Ub may occur with a probability of only about 5%. Based on this pattern, the generator 210 may reduce the size of the window 212 or the length that the open window 212 is extended to as a function of this probability that another trigger event 202 (i.e., a possible interaction 12) will occur. For example, FIG. 2B shows three trigger events 202, 202a-c, each of which corresponds to a subsequent response 122 (e.g., three responses 122a-c) from the interface 120 such that the generator 210 generates a first window 212a for the first trigger event 202a, followed by a second window 212b for a second trigger event 202b that occurs after the first trigger event 202a, followed by a third window 212c for a third trigger event 202c that occurs after the second trigger event 202b. In this example, the generator 210 shortens the generated windows 212 such that the third window 212c has a shorter duration than the second window 212b, which has a shorter duration than the first window 212a. Additionally or alternatively, the generator 210 may analyze the audio data 124 of the audio stream 16 to determine whether the voice activity level in the audio data 124 provides any indication that the trigger event 202 is likely to occur. With this information, the generator 210 may modify the size of the currently open window 212 or modify the size of any subsequently generated/extended windows 212.
リデューサ220は、オーディオデータ124のオーディオストリーム16に対して実行されるべき音声認識の処理レベル222を指定するように構成される。処理レベル222は、音声認識システム150のための音声認識を実行する音声認識モデルの種類、音声認識が行われる場所、音声認識を実行するために使用される音声認識パラメータ、音声モデルが最大能力で動作するように指定されているかまたは何らかのより低い程度の能力で動作するように指定されているかなどを含むがこれらに限定されないいくつかの変数に基づく場合がある。処理レベル222は、概して、専用の、または任意の所与の時点で音声認識などの音声処理によって消費されているコンピューティングリソース(たとえば、データ処理ハードウェアおよびメモリハードウェアなどのローカルリソース、またはリモートリソース)の量を指す場合がある。これは、たとえば、第1の状態において音声処理専用のコンピューティングリソースまたはコンピューティングパワーの量が第2の状態よりも少ないとき、第1の状態における第1の処理レベル222が第2の状態における第2の処理レベル222よりも低いことを意味する。 The reducer 220 is configured to specify a processing level 222 of speech recognition to be performed on the audio stream 16 of the audio data 124. The processing level 222 may be based on several variables, including, but not limited to, the type of speech recognition model performing the speech recognition for the speech recognition system 150, where the speech recognition is performed, the speech recognition parameters used to perform the speech recognition, whether the speech model is specified to operate at full capacity or at some lower degree of capacity, etc. The processing level 222 may generally refer to the amount of computing resources (e.g., local resources such as data processing hardware and memory hardware, or remote resources) that are dedicated or consumed by speech processing such as speech recognition at any given time. This means, for example, that the first processing level 222 in the first state is lower than the second processing level 222 in the second state when the amount of computing resources or computing power dedicated to speech processing in the first state is less than the second state.
一部の例においては、(たとえば、最大処理能力と比較したとき)下げられた処理レベル222で音声認識が行われるとき、下げられた処理レベル222は、スピーチを認識するための第1のパスとして機能するが、その後、第1のパスよりも高い処理レベル222を有する第2のパスをもたらす場合がある。たとえば、音声認識システム150が下げられた処理レベル222で動作しているとき、音声認識システム150は、オーディオデータ124がインターフェース120に命令または問い合わせするための発話12Uを含むという低い信頼度の音声認識結果(たとえば、低い信頼度の仮説(hypothesis))を特定する。この低い信頼度の音声認識結果が原因で、リデューサ220は、この低い信頼度の音声認識結果が、オーディオデータ124がインターフェース120に命令または問い合わせするための発話12Uを含むというより高い信頼度の音声認識結果(たとえば、高い信頼度の仮説)に実際に対応するかどうかがより高い処理レベル222で判定されてよいように、処理レベル222を上げる場合がある。一部の例において、低い信頼度の音声認識結果は、音声認識中に信頼度の閾値を満たすことができない音声認識結果である。言い換えると、リデューサ220は、上述の変数に基づくだけでなく、特定の処理レベル222で音声認識中に得られた結果にも基づいて、処理レベル222を変更する場合がある。 In some examples, when speech recognition is performed at a reduced processing level 222 (e.g., when compared to a maximum processing capacity), the reduced processing level 222 may function as a first pass to recognize speech, but then result in a second pass having a processing level 222 higher than the first pass. For example, when the speech recognition system 150 is operating at the reduced processing level 222, the speech recognition system 150 identifies a low-confidence speech recognition result (e.g., a low-confidence hypothesis) that the audio data 124 includes an utterance 12U for commanding or querying the interface 120. This low-confidence speech recognition result may cause the reducer 220 to increase the processing level 222 so that it may be determined at the higher processing level 222 whether this low-confidence speech recognition result actually corresponds to a higher-confidence speech recognition result (e.g., a high-confidence hypothesis) that the audio data 124 includes an utterance 12U for commanding or querying the interface 120. In some examples, a low-confidence speech recognition result is a speech recognition result that fails to meet a confidence threshold during speech recognition. In other words, the reducer 220 may change the processing level 222 based not only on the variables mentioned above, but also on the results obtained during speech recognition at a particular processing level 222.
一部の実装において、リデューサ220は、ウィンドウ212の関数として処理レベル222を指定する。つまり、リデューサ220は、開いたマイクロフォン継続時間ウィンドウ212が存在するとき、音声認識の処理レベル222を下げることができる。たとえば、(たとえば、図2Aに示されたように)ウィンドウ212が10秒の継続時間に対応するとき、ウィンドウ212の最初の5秒の間、リデューサ220は、音声認識のために最大パワー/処理(たとえば、第1の処理レベル222、222a)で動作するように音声認識システム150に指示する。最大処理でこの5秒が発生した後、音声処理は、減衰状態204に移行し、処理レベル222が最大処理よりも低いある程度まで下げられる。例として、ウィンドウ212の継続時間の5秒から7秒まで、リデューサ220は、音声認識システム150の最大処理の50%に対応する第2の処理レベル222、222bで働くように音声認識システム150に指示する。それから、継続時間の7秒目の後、ウィンドウ212の継続時間の終わりまで、リデューサ220は、音声認識システム150の最大処理の25%に対応する第3の処理レベル222、222cで働くように音声認識システム150に指示する。したがって、リデューサ220は、開いたマイクロフォン継続時間ウィンドウ212の間、音声認識システム150の処理レベル222を制御する。 In some implementations, the reducer 220 specifies the processing level 222 as a function of the window 212. That is, the reducer 220 can lower the processing level 222 of the speech recognition when an open microphone duration window 212 exists. For example, when the window 212 corresponds to a duration of 10 seconds (e.g., as shown in FIG. 2A), during the first 5 seconds of the window 212, the reducer 220 instructs the speech recognition system 150 to operate at maximum power/processing (e.g., a first processing level 222, 222a) for speech recognition. After these 5 seconds at maximum processing occur, the speech processing transitions to the attenuated state 204, and the processing level 222 is lowered to a degree that is lower than maximum processing. As an example, from 5 seconds to 7 seconds of the duration of the window 212, the reducer 220 instructs the speech recognition system 150 to work at a second processing level 222, 222b, which corresponds to 50% of the maximum processing of the speech recognition system 150. Then, after the seventh second of the duration, until the end of the duration of the window 212, the reducer 220 instructs the speech recognition system 150 to operate at a third processing level 222, 222c, which corresponds to 25% of the maximum processing of the speech recognition system 150. Thus, the reducer 220 controls the processing level 222 of the speech recognition system 150 during the open microphone duration window 212.
一部の実装において、生成器210およびリデューサ220は、ウィンドウ212の継続時間が減衰状態204に依存するように連携して働く。生成器210は、指定された時間に終了点216を生成せず、代わりに、リデューサ220が処理レベル222をマイクロフォン116を閉じるのと同じ効果を有するレベルまで下げることを可能にしてよい。たとえば、音声認識システム150の最大処理の25%に対応する第3の処理レベル222cの後、処理のさらに25%の削減は、処理レベル222を音声認識システム150の最大処理の0%(すなわち、処理なしまたは「閉じている」)まで下げるので、リデューサ220は、マイクロフォン116を閉じる。この特定の例は、処理レベル222を離散的に徐々に下げる段階的手法であるが、閉じたマイクロフォン116をもたらすためのその他の種類の減衰も可能である。たとえば、処理レベル222は、開いたマイクロフォン継続時間ウィンドウ212の間のどの時点でも直線的に減衰する場合がある。マイクロフォン116が閉じられるまでリデューサ220が処理レベル222を下げることを可能にすることによって、この技術は、(たとえば、インタラクション12が発生しておらず、マイクロフォン116が開いているときに)音声処理の連続的な減衰を進める場合がある。 In some implementations, the generator 210 and the reducer 220 work together such that the duration of the window 212 depends on the attenuation state 204. The generator 210 may not generate the end point 216 at a specified time, but instead allow the reducer 220 to reduce the processing level 222 to a level that has the same effect as closing the microphone 116. For example, after the third processing level 222c, which corresponds to 25% of the maximum processing of the speech recognition system 150, a further 25% reduction in processing would reduce the processing level 222 to 0% of the maximum processing of the speech recognition system 150 (i.e., no processing or "closed"), so that the reducer 220 closes the microphone 116. This particular example is a stepwise approach that discretely and gradually reduces the processing level 222, although other types of attenuation to result in a closed microphone 116 are possible. For example, the processing level 222 may be linearly attenuated at any point during the open microphone duration window 212. By allowing the reducer 220 to lower the processing level 222 until the microphone 116 is closed, this technique may facilitate a continuous attenuation of audio processing (e.g., when no interaction 12 is occurring and the microphone 116 is open).
図2Bなどの一部の構成において、ウィンドウ212内の処理レベル222は、インターフェース120がユーザ10による最後のトリガイベント202を受信した時間に基づく。これらの構成において、リデューサ220は、インターフェース120が最後のトリガイベント202を受信したときから現在の時間までの時間期間224が時間の閾値226を満たすかどうかを判定してよい。時間期間224が時間の閾値226を満たす(すなわち、インタラクション12が閾値の時間の間発生しなかった)とき、リデューサ220は、現在の時間に処理レベル222を生成してよい。たとえば、インターフェース120がユーザ10による発話12Uに対して応答122を送信した場合、ウィンドウ212は、インターフェース120が応答122を伝達したときに始まる。時間の閾値226が5秒に設定されているとき、リデューサ220は、インターフェース120が応答122を伝達したときから5秒が経過したかどうかを判定する。リデューサ220は、5秒が経過したと判定するとき、たとえば、リデューサ220が5秒が経過したと判定した現在の時間またはその後の何らかの特定の時間に開始する音声認識の何らかの処理レベル222を指定してよい。 In some configurations, such as FIG. 2B, the processing level 222 within the window 212 is based on the time when the interface 120 received the last trigger event 202 by the user 10. In these configurations, the reducer 220 may determine whether the time period 224 from when the interface 120 received the last trigger event 202 to the current time meets the time threshold 226. When the time period 224 meets the time threshold 226 (i.e., no interaction 12 occurred for the threshold time), the reducer 220 may generate the processing level 222 at the current time. For example, if the interface 120 sends a response 122 to an utterance 12U by the user 10, the window 212 begins when the interface 120 communicates the response 122. When the time threshold 226 is set to 5 seconds, the reducer 220 determines whether 5 seconds have passed since the interface 120 communicated the response 122. When the reducer 220 determines that five seconds have elapsed, the reducer 220 may, for example, specify some processing level 222 of speech recognition to begin at the current time that the reducer 220 determined that five seconds have elapsed or at some particular time thereafter.
一部の構成において、処理レベル222は、音声認識システム150の1つまたは複数のパラメータを調整することによって変わる。音声認識システム150における音声認識の処理レベル222を変更する1つの手法において、音声認識は、それが生じる場所に応じて変更される。音声認識の場所は、サーバサイド(つまり、リモート)での発生からオンデバイス(つまり、ローカル)での発生に変わる場合がある。言い換えると、第1の処理レベル222aは、サーバベースの音声認識モデルを使用したリモート音声認識に対応し、一方、第2の処理レベル222bは、オンデバイスで発生するローカル音声認識に対応する。音声認識システム150が「オンデバイス」でホストされるとき、デバイス110は、オーディオデータ124を受信し、そのプロセッサ(たとえば、データ処理ハードウェア112およびメモリハードウェア114)を使用して音声認識システム150の機能を実行する。サーバベースのモデルは(たとえば、帯域幅および送信のオーバーヘッドなどのその他のコストとともに)より多くの数のリモート処理リソースを利用する場合があるので、サーバベースのモデルを使用する音声認識は、オンデバイス音声認識モデルよりも高い処理レベル222を有すると考えられる場合がある。処理の量がより多いので、サーバベースのモデルは、オンデバイスのモデルよりもサイズが潜在的に大きく、および/またはオンデバイスのモデルよりも大きな探索グラフ(search graph)を使用して復号を実行する場合がある。たとえば、サーバベースの音声認識モデルは、専用の音声認識の目的のために特に訓練された複数のより大きなモデル(たとえば、音響モデル(AM)、発音モデル(PM)、および言語モデル(LM))を利用する場合があり、一方、オンデバイスのモデルは、デバイス110の有限の処理リソースで効果的に、空間効率良く動作するために、これらの異なるモデルをより小さなパッケージに統合しなければならないことが多い。したがって、リデューサ220が処理レベル222を減衰状態204まで下げるとき、リデューサ220は、サーバベースのモデルを使用してリモートで発生する音声認識を、オンデバイスのモデルを使用してローカルで行われるように変更する場合がある。 In some configurations, the processing level 222 is changed by adjusting one or more parameters of the speech recognition system 150. In one approach to changing the processing level 222 of the speech recognition in the speech recognition system 150, the speech recognition is changed according to where it occurs. The location of the speech recognition may change from occurring on the server side (i.e., remote) to occurring on the device (i.e., local). In other words, the first processing level 222a corresponds to remote speech recognition using a server-based speech recognition model, while the second processing level 222b corresponds to local speech recognition occurring on the device. When the speech recognition system 150 is hosted "on device", the device 110 receives the audio data 124 and uses its processor (e.g., data processing hardware 112 and memory hardware 114) to perform the functions of the speech recognition system 150. Because the server-based model may utilize a greater number of remote processing resources (e.g., along with other costs such as bandwidth and transmission overhead), speech recognition using a server-based model may be considered to have a higher processing level 222 than an on-device speech recognition model. Due to the larger amount of processing, the server-based model may potentially be larger in size than the on-device model and/or may use a larger search graph to perform the decoding than the on-device model. For example, the server-based speech recognition model may utilize multiple larger models (e.g., Acoustic Model (AM), Pronunciation Model (PM), and Language Model (LM)) trained specifically for dedicated speech recognition purposes, whereas the on-device model must often consolidate these different models into a smaller package to operate effectively and space-efficiently with the finite processing resources of the device 110. Thus, when the reducer 220 reduces the processing level 222 to the attenuation state 204, the reducer 220 may modify the speech recognition that occurs remotely using the server-based model to occur locally using the on-device model.
状況によっては、口頭の発話12Uなどのインタラクション12を生成しているユーザ110の近くに2つ以上のデバイス110が存在する場合がある。複数のデバイス110がユーザ10の近傍にあるとき、各デバイス110が、音声認識の何らかの態様を実行することができる場合がある。同じ口頭の発話12Uに対して音声認識を実行する複数のデバイス110は重複する場合があるので、リデューサ220は、別のデバイス110が音声認識を処理しているかまたは処理するように構成されていることを知って、特定のデバイス110のマイクロフォン116を閉じることによってそのデバイス110における処理レベル222を下げる場合がある。例として、ユーザ10がモバイルデバイスおよびスマートウォッチを持っているとき、両方のデバイス110が音声認識を実行することができる場合がある。ここで、リデューサ220は、モバイルデバイスが実行する必要がある場合がある幅広いその他のコンピューティングタスクのためにモバイルデバイスの処理リソースを節約するために、モバイルデバイスの音声認識のためのマイクロフォン116を閉じてよい。たとえば、ユーザ10は、自分のスマートウォッチのバッテリよりも自分のモバイルデバイスのバッテリを節約することを望む場合がある。一部の例においては、複数のデバイスが存在するとき、リデューサ220は、どのデバイス110が開いたままのマイクロフォン116を有するのに最適であるかと、どのデバイスがそれらのマイクロフォン116を閉じさせるのに最適であるかとを特定するために、各デバイス110の特性(たとえば、現在のプロセッサの消耗、現在の電池の持ちなど)を決定しようとしてよい。 In some situations, there may be two or more devices 110 in the vicinity of a user 110 generating an interaction 12, such as an oral utterance 12U. When multiple devices 110 are in the vicinity of the user 10, each device 110 may be capable of performing some aspect of speech recognition. Since multiple devices 110 performing speech recognition for the same oral utterance 12U may overlap, the reducer 220 may reduce the processing level 222 in a particular device 110 by closing the microphone 116 of that device, knowing that another device 110 is processing or configured to process speech recognition. As an example, when a user 10 has a mobile device and a smartwatch, both devices 110 may be capable of performing speech recognition. Here, the reducer 220 may close the microphone 116 for speech recognition on the mobile device to conserve the processing resources of the mobile device for a wide range of other computing tasks that the mobile device may need to perform. For example, the user 10 may want to conserve the battery of his mobile device over the battery of his smartwatch. In some examples, when multiple devices are present, the reducer 220 may attempt to determine the characteristics of each device 110 (e.g., current processor drain, current battery life, etc.) to identify which devices 110 are best suited to have their microphones 116 left open and which devices are best suited to have their microphones 116 closed.
リモート音声認識システム150とオンデバイス音声認識システム150との間に処理レベルの違いがあることに加えて、オンデバイス音声認識モデルまたはサーバサイド音声認識モデルの異なるバージョンが存在する場合がある。異なるバージョンがある状態では、リデューサ220は、音声認識に使用されているモデルまたはモデルのバージョンを変更することによって処理レベル222を変更する場合がある。大まかに言って、モデルは、処理レベル222が高い大バージョン、処理レベル222が中程度の中バージョン、および処理レベル222が低い小バージョンを有する場合がある。この意味で、リデューサ220が処理レベル222を下げたい場合、リデューサ220は、モデルの第1のバージョンで実行されている音声認識を、モデルの第1のバージョンよりも低い処理レベル222を有するモデルの第2のバージョンに移行させてよい。オンデバイスのモデルかまたはサーバサイドのモデルかのどちらかのバージョン間で変更することに加えて、リデューサ220は、サーバサイドのモデルのあるバージョンからオンデバイスのモデルの特定のバージョンに変更する場合もある。モデルおよびこれらのモデルのバージョンを持つことによって、リデューサ220は、開いたマイクロフォンウィンドウ212の間に音声認識処理レベル222を減衰させるために、より多くの数の処理レベル222を自由に使うことができる。 In addition to differences in processing levels between the remote speech recognition system 150 and the on-device speech recognition system 150, there may be different versions of the on-device or server-side speech recognition model. With different versions, the reducer 220 may change the processing level 222 by changing the model or the version of the model that is being used for speech recognition. Broadly speaking, a model may have a large version with a high processing level 222, a medium version with a medium processing level 222, and a small version with a low processing level 222. In this sense, if the reducer 220 wants to lower the processing level 222, the reducer 220 may transition the speech recognition that is being performed on a first version of the model to a second version of the model that has a lower processing level 222 than the first version of the model. In addition to changing between versions of either the on-device or server-side model, the reducer 220 may also change from a version of the server-side model to a specific version of the on-device model. By having models and versions of these models, the reducer 220 has a greater number of processing levels 222 at its disposal to attenuate the speech recognition processing levels 222 during an open microphone window 212.
一部の実装において、オンデバイス音声認識モデルのバージョンは、リデューサ220が異なる処理レベル222のためにそのようなバージョンを指定してよいように、異なる処理の需要を有する。オンデバイス音声認識モデルのいくつかの例は、リカレントニューラルネットワークトランスデューサ(RNN-T)モデル、リッスン・アテンド・スペル(LAS: listen-attend-spell)モデル、ニューラルトランスデューサ(neural transducer)モデル、モノトニックアラインメント(monotonic alignment)モデル、リカレントニューラルアラインメント(RNA: recurrent neural alignment)モデルなどのシーケンスツーシーケンス(sequence-to-sequence)モデルを含む。RNN-TモデルとLASモデルとを組み合わせる2パスモデルなどの、これらのモデルのハイブリッドであるオンデバイスのモデルも存在する可能性がある。オンデバイスモデルのこれらの異なるバージョンがある状態では、リデューサ220は、異なる処理レベル222を生成するために、これらのバージョンの各々の処理要件をランク付けまたは特定する場合がある。たとえば、2パスモデルは、RNN-Tネットワークの第1のパスと、それに続くLASネットワークの第2のパスとを含む。この2パスモデルは複数のネットワークを含むので、リデューサ220は、2パスモデルを、音声認識のための比較的高い処理レベル222を有する大規模なオンデバイスのモデルとして指定する場合がある。音声認識の処理レベル222を2パスモデルの処理レベル222から下げるために、リデューサ220は、2パスモデルからLASモデルに変更してよい。ここで、LASモデルは、文字起こし152を形成する文字列を生成するために、その復号プロセス中にアテンションを実行するアテンションベースのモデルである。概して、アテンションベースの手法は、所与のスピーチ入力に関する特定の特徴に傾注するためにより計算負荷が高い傾向がある。比較のため、RNN-Tモデルは、アテンションメカニズムを採用せず、また、大規模なデコーダグラフ(decoder graph)の代わりに単一のニューラルネットワークによってそのビームサーチを実行し、したがって、RNN-Tモデルは、LASモデルよりもコンパクトで、計算負荷が低い場合がある。これらの理由から、リデューサ220は、処理レベル222を下げるために、音声認識のための2パスモデルと、LASモデルと、RNN-Tモデルとの間で変更をしてよい。すなわち、第1のパスとしてRNN-Tネットワークを採用し、第2のパスとしてのLASネットワークによって第1のパスを再採点することによって、2パスモデルは、LASモデルまたはRNN-Tモデルのうちのどちらか単独よりも高い処理レベル222を有し、一方、アテンションベースのモデルとしてのLASモデルは、RNN-Tモデルよりも高い処理レベル222を有する。オンデバイス音声認識モデルの異なるバージョンに関する音声認識の処理要件を特定することによって、リデューサ220は、オンデバイスのモデルの異なるバージョンのいずれかの間を切り替えることによって、音声処理を減衰させる(または処理を高める)ことができる。さらに、リデューサ220が、オンデバイスのモデルの処理レベルの選択肢とサーバサイドのモデルの処理レベルの選択肢とを組み合わせるとき、リデューサ220による音声処理の減衰は、多数の潜在的な処理レベルの段階を有する。 In some implementations, versions of on-device speech recognition models have different processing demands, such that the reducer 220 may designate such versions for different processing levels 222. Some examples of on-device speech recognition models include sequence-to-sequence models, such as recurrent neural network transducer (RNN-T) models, listen-attend-spell (LAS) models, neural transducer models, monotonic alignment models, and recurrent neural alignment (RNA) models. There may also be on-device models that are hybrids of these models, such as a two-pass model that combines an RNN-T model and a LAS model. With these different versions of on-device models, the reducer 220 may rank or identify the processing requirements of each of these versions to generate different processing levels 222. For example, a two-pass model includes a first pass of an RNN-T network followed by a second pass of a LAS network. Since the two-pass model includes multiple networks, the reducer 220 may designate the two-pass model as a large-scale on-device model with a relatively high processing level 222 for speech recognition. To lower the processing level 222 for speech recognition from that of the two-pass model, the reducer 220 may change from the two-pass model to the LAS model, where the LAS model is an attention-based model that performs attention during its decoding process to generate the strings that form the transcription 152. In general, attention-based approaches tend to be more computationally intensive to focus on specific features for a given speech input. For comparison, the RNN-T model does not employ an attention mechanism and also performs its beam search by a single neural network instead of a large decoder graph, and thus the RNN-T model may be more compact and less computationally intensive than the LAS model. For these reasons, the reducer 220 may change between the two-pass model for speech recognition, the LAS model, and the RNN-T model to lower the processing level 222. That is, by employing an RNN-T network as the first pass and rescoring the first pass with an LAS network as the second pass, the two-pass model has a higher processing level 222 than either the LAS model or the RNN-T model alone, while the LAS model as an attention-based model has a higher processing level 222 than the RNN-T model. By identifying the processing requirements of speech recognition for different versions of the on-device speech recognition model, the reducer 220 can attenuate (or increase) the speech processing by switching between any of the different versions of the on-device model. Furthermore, when the reducer 220 combines the processing level options of the on-device model and the processing level options of the server-side model, the attenuation of the speech processing by the reducer 220 has multiple potential processing level stages.
処理レベルの段階の潜在的な数をさらに拡大するために、リデューサ220は、所与のモデルの音声処理ステップまたは音声処理パラメータを修正して、その特定のモデルの処理レベルを変更するように構成されてよい。たとえば、特定のモデルは、ニューラルネットワーク(たとえば、長期短期記憶(LSTM: long-short-term memory)の層を持つリカレントニューラルネットワーク)の1つまたは複数の層を含む。一部の例において、出力層は、その出力を生成するために、過去の状態(逆方向)および将来の状態(順方向)からの情報を受け取る場合がある。層は、逆方向および順方向の状態を受け取るとき、双方向であると考えられる。一部の構成において、リデューサ220は、音声認識モデルを、双方向の(すなわち、順方向および逆方向の)動作から、単に単方向の(たとえば、順方向の)動作に変更するために、モデルの処理ステップを修正するように構成される。追加的または代替的に、リデューサ220は、特定のモデルの処理レベル222を変更するために、モデルが音声認識を実行するために使用するニューラルネットワークの層の数を減らす場合がある。 To further expand the potential number of stages of processing levels, the reducer 220 may be configured to modify the speech processing steps or parameters of a given model to change the processing level of that particular model. For example, a particular model may include one or more layers of a neural network (e.g., a recurrent neural network with a layer of long-short-term memory (LSTM)). In some examples, an output layer may receive information from past states (backward) and future states (forward) to generate its output. A layer is considered to be bidirectional when it receives backward and forward states. In some configurations, the reducer 220 is configured to modify the processing steps of the model to change the speech recognition model from bidirectional (i.e., forward and backward) operation to merely unidirectional (e.g., forward) operation. Additionally or alternatively, the reducer 220 may reduce the number of layers of a neural network that the model uses to perform speech recognition in order to change the processing level 222 of a particular model.
リデューサ220は、モデルのビームサーチパラメータ(またはその他の枝刈り/探索モードパラメータ)を変更することによって、音声認識モデルの処理レベル222を下げてもよい。概して、ビームサーチは、音声認識結果を生成するために、最良の潜在的な解(たとえば、仮説または候補)のうちのいくつを評価するかを指定するビームサイズまたはビーム幅パラメータを含む。したがって、ビームサーチプロセスは、音声認識結果を形成するために評価される解の数を減らすために、潜在的な解の一種の枝刈りを実行する。すなわち、ビームサーチプロセスは、限られた数のアクティブなビームを使用して、音声認識結果(たとえば、発話12Uの文字起こし152)を生成するために、発話12Uにおいて話された単語の最も可能性の高いシーケンスを探索することによって、含まれる計算を制限することができる。ここで、リデューサ220は、ビームサイズを調整して、評価する最良の潜在的な解の数を減らしてよく、これは、ひいては、ビームサーチプロセスに含まれる計算量を削減する。たとえば、リデューサ220は、音声認識モデルに5つの最良の候補ではなく2つの最良の候補を評価させるために、ビームサイズを5からビームサイズ2に変更する。 The reducer 220 may reduce the processing level 222 of the speech recognition model by modifying the model's beam search parameters (or other pruning/search mode parameters). Generally, the beam search includes a beam size or beam width parameter that specifies how many of the best potential solutions (e.g., hypotheses or candidates) are evaluated to generate the speech recognition result. Thus, the beam search process performs a kind of pruning of the potential solutions to reduce the number of solutions evaluated to form the speech recognition result. That is, the beam search process may limit the computation involved by using a limited number of active beams to search the most likely sequences of words spoken in the utterance 12U to generate the speech recognition result (e.g., the transcription 152 of the utterance 12U). Here, the reducer 220 may adjust the beam size to reduce the number of best potential solutions to evaluate, which in turn reduces the amount of computation involved in the beam search process. For example, the reducer 220 changes the beam size from 5 to beam size 2 to have the speech recognition model evaluate the two best candidates instead of the five best candidates.
一部の例において、リデューサ220は、モデルのより低い処理レベル222を生成するために、音声認識モデルの1つまたは複数のパラメータに対して量子化またはスパース化を実行する。音声認識結果を生成するとき、音声認識モデルは、通常、多数の重みを生成する。たとえば、音声認識モデルは、音声認識結果(たとえば、文字起こし152)を出力するために、異なるスピーチパラメータおよび/またはスピーチに関連する特徴を重み付けする。この多数の重みが原因で、リデューサ220は、量子化を実行することによって、これらの重みの値を離散化する場合がある。たとえば、量子化プロセスは、浮動小数点の重みを固定小数点整数として表される重みに変換する。この量子化プロセスは、いくらかの情報または品質を喪失するが、これらの量子化されたパラメータを処理するリソースがより少ないメモリを使用することを可能にし、特定のハードウェア上でより効率的な演算(たとえば、乗算)が実行されることを可能にする場合がある。 In some examples, the reducer 220 performs quantization or sparsification on one or more parameters of the speech recognition model to generate a lower processing level 222 of the model. When generating a speech recognition result, the speech recognition model typically generates a large number of weights. For example, the speech recognition model weights different speech parameters and/or speech-related features to output a speech recognition result (e.g., transcription 152). Due to this large number of weights, the reducer 220 may discretize the values of these weights by performing quantization. For example, the quantization process converts floating-point weights to weights represented as fixed-point integers. This quantization process loses some information or quality, but may allow resources to process these quantized parameters to use less memory and allow more efficient operations (e.g., multiplication) to be performed on a particular hardware.
同様の点で、スパース化も、モデルを実行するための処理の量を減らすことを目標とする。ここで、スパース化は、より関連性の高い特徴に焦点を合わせるために、音声認識モデルにおいて冗長なパラメータまたは特徴を削除するプロセスを指す。たとえば、音声認識結果を決定する間に、音声モデルは、たとえすべてのスピーチに関連する特徴(たとえば、文字、記号、または単語)が特定のスピーチ入力に関連しているとは限らないとしても、すべてのスピーチに関連する特徴に関する確率を決定する場合がある。スパース化を使用することにより、モデルは、すべてのスピーチに関連する特徴の代わりに、入力に関連しているスピーチに関連する特徴に関する確率を決定し、入力に関連していないスピーチに関連する特徴をスパース化プロセスが無視することを可能にすることによって、より少ない計算リソースを費やす場合がある。 In a similar vein, sparsification also aims to reduce the amount of processing required to run a model. Here, sparsification refers to the process of removing redundant parameters or features in a speech recognition model in order to focus on more relevant features. For example, while determining a speech recognition result, a speech model may determine probabilities for all speech-related features (e.g., letters, symbols, or words) even though not all speech-related features are associated with a particular speech input. By using sparsification, the model may expend fewer computational resources by determining probabilities for speech-related features that are associated with the input instead of all speech-related features, allowing the sparsification process to ignore speech-related features that are not associated with the input.
任意で、リデューサ220は、開いたマイクロフォン継続時間ウィンドウ212を元々生成したまたはもたらしたインタラクション12(たとえば、口頭の発話12U)のコンテキストを決定することによって、音声認識のより低い処理レベル222を生成する場合がある。リデューサ220がコンテキストを特定すると、リデューサ220は、コンテキストに基づいて音声認識結果にバイアスをかけることによって音声認識システム150の処理レベル222を下げるために、コンテキストを使用してよい。一部の実装において、リデューサ220は、音声認識システム150をコンテキストに関連する語彙に制限することによって、コンテキストに基づいて音声認識結果にバイアスをかける。例として、ユーザ10は、インターフェース120に「how do you tie a prusik hitch?(プルージック結びはどうやって結ぶの)」と尋ねてよい。この質問から、リデューサ220は、プルージック結びが主に登山またはロッククライミングで使用されていると判定する。言い換えると、リデューサ220は、インタラクション12のコンテキストが登山であることを特定する。この例においては、リデューサ220が音声認識の処理レベル222を下げ始めるとき、リデューサ220は、音声認識出力を登山に関する語彙に制限する。したがって、その後、ユーザ10がアパラチア山脈についての追加質問をする場合、音声認識システム150は、用語「Application(アプリケーション)」および「Appalachian(アパラチア山脈の)」を含む考えられ得る音声認識結果を生成する場合があるが、「Appalachian(アパラチア山脈の)」は「Mountaineering(登山)」のコンテキストに関連しているので、リデューサ220は、確実に音声認識システム150が用語「Appalachian(アパラチア山脈の)」の方にバイアスをかけられるようにする。たとえば、リデューサ220は、特定されたコンテキスト(たとえば、登山)に関連する潜在的な結果の確率スコアを上げるように音声認識システム150に指示する。言い換えると、音声認識システム150は、登山に関連する語彙を含む潜在的な結果の確率スコアを上げる。 Optionally, the reducer 220 may generate a lower processing level 222 of the speech recognition by determining the context of the interaction 12 (e.g., the verbal utterance 12U) that originally generated or resulted in the open microphone duration window 212. Once the reducer 220 identifies the context, the reducer 220 may use the context to lower the processing level 222 of the speech recognition system 150 by biasing the speech recognition results based on the context. In some implementations, the reducer 220 biases the speech recognition results based on the context by restricting the speech recognition system 150 to vocabulary relevant to the context. As an example, the user 10 may ask the interface 120, "how do you tie a prusik hitch?" From this question, the reducer 220 determines that the prusik hitch is primarily used in mountaineering or rock climbing. In other words, the reducer 220 identifies that the context of the interaction 12 is mountaineering. In this example, when reducer 220 begins to reduce speech recognition processing level 222, reducer 220 limits speech recognition output to mountain climbing vocabulary. Thus, if user 10 subsequently asks a follow-up question about the Appalachian Mountains, speech recognition system 150 may generate possible speech recognition results that include the terms "Application" and "Appalachian," but reducer 220 ensures that speech recognition system 150 is biased toward the term "Appalachian" because "Appalachian" is associated with the "Mountaineering" context. For example, reducer 220 instructs speech recognition system 150 to increase the probability score of potential results that are associated with the identified context (e.g., mountain climbing). In other words, speech recognition system 150 increases the probability score of potential results that include mountain climbing related vocabulary.
音声認識システム150がデバイス110上で動作しているとき、音声認識システム150は、概して、音声認識を実行するためにシステムオンチップベース(SOCベース)のプロセッサを使用する場合がある。システムオンチップ(SOC)プロセッサは、汎用プロセッサ、信号プロセッサ、および追加的な周辺機器を指す。リデューサ220は、SOCベースの処理からデジタル信号プロセッサ(DSP)に変更するように音声認識システム150に指示することによって、音声認識がSOCベースの処理を使用するときに、下げられた処理レベル222を生成してよい。ここで、DSPはSOCベースの処理よりも少ない電力およびメモリを消費する傾向があるので、この変更は、より低い処理レベル222をもたらす。 When the speech recognition system 150 is running on the device 110, the speech recognition system 150 may generally use a system-on-chip (SOC-based) processor to perform speech recognition. A system-on-chip (SOC) processor refers to a general-purpose processor, a signal processor, and additional peripherals. The reducer 220 may generate a reduced processing level 222 when the speech recognition uses SOC-based processing by instructing the speech recognition system 150 to change from SOC-based processing to a digital signal processor (DSP). Here, this change results in a lower processing level 222 because DSPs tend to consume less power and memory than SOC-based processing.
リデューサ220が音声認識の処理レベル222を減衰させるとき、減衰が起こっているというある程度のインジケーションをユーザ10に提供することが有利である場合がある。このインジケーションを提供するために、デバイス110に関連するグラフィカルユーザインターフェース(GUI)は、音声認識システム150の現在の処理レベル222を示すためのグラフィカルなインジケータを含んでよい。一部の例において、グラフィカルなインジケータは、処理レベル222の減衰に比例して次第に低くなるように構成された輝度レベルを有する。たとえば、デバイス110の画面は、マイクロフォン116が開いている(すなわち、インタラクション12を聞いている)ことを示す赤いマイクロフォンのドットを示すGUIを含み、このマイクロフォンのドットは、リデューサ220が音声認識の処理レベル222を減衰させるにつれて徐々にフェードアウトする。ここで、マイクロフォン116が閉じるとき、赤いマイクロフォンのドットは消える。追加的または代替的に、マイクロフォン116が開いていることおよび/または音声認識の処理レベル222の減衰の程度を示すインジケータは、デバイス110上のライト(たとえば、発光ダイオード(LED))のようなハードウェアインジケータである場合がある。たとえば、LEDは、マイクロフォン116が閉じられるまで、処理レベル222が減少するにつれてオフに向かって次第に消えていくか、または消えるまで減少する速度で(たとえば、次第に遅く)点滅する。 When the reducer 220 attenuates the speech recognition processing level 222, it may be advantageous to provide the user 10 with some indication that attenuation is occurring. To provide this indication, a graphical user interface (GUI) associated with the device 110 may include a graphical indicator to indicate the current processing level 222 of the speech recognition system 150. In some examples, the graphical indicator has a brightness level configured to become gradually lower in proportion to the attenuation of the processing level 222. For example, the screen of the device 110 may include a GUI showing a red microphone dot indicating that the microphone 116 is open (i.e., listening to the interaction 12), which gradually fades out as the reducer 220 attenuates the speech recognition processing level 222. Here, when the microphone 116 is closed, the red microphone dot disappears. Additionally or alternatively, the indicator indicating that the microphone 116 is open and/or the degree of attenuation of the speech recognition processing level 222 may be a hardware indicator such as a light (e.g., a light emitting diode (LED)) on the device 110. For example, the LED may fade toward off as the processing level 222 decreases, or flash at a decreasing rate (e.g., slower and slower) until the microphone 116 is closed, at which point the LED may fade toward off as the processing level 222 decreases, or flash at a decreasing rate (e.g., slower and slower) until it is off.
図3は、音声処理を減衰させる方法300の動作の例示的な配列の流れ図である。動作302において、方法300は、音声対応デバイス110において、スピーチによる音声対応デバイス110との起こり得るユーザインタラクションを示すマイクロフォントリガイベント202のインジケーションを受信し、音声対応デバイス110はマイクロフォン116を有し、マイクロフォン116は、開いているときに自動音声認識(ASR)システム150による認識のためにスピーチをキャプチャするように構成される。動作304は、マイクロフォントリガイベント202のインジケーションの受信に応答して発生する2つの下位動作304、304a～bを含む。動作304aにおいて、方法300は、音声対応デバイス110の環境内でオーディオストリーム16をキャプチャするために、開くか、または開いたマイクロフォン継続時間ウィンドウ212の間、開いたままであるようにマイクロフォン116に指示する。動作304bにおいて、方法300は、開いたマイクロフォン116によってキャプチャされたオーディオストリーム16を、オーディオストリーム16に対してASR処理を実行するためにASRシステム150に提供する。動作306は、ASRシステム150が開いたマイクロフォン116によってキャプチャされたオーディオストリーム16に対してASR処理を実行している間に発生する2つの下位動作306、306a～bを含む。動作306において、方法300は、ASRシステム150がオーディオストリーム16に対して実行するASR処理のレベル222を、開いたマイクロフォン継続時間ウィンドウ212の関数に基づいて減衰させる。動作306bにおいて、方法300は、開いたマイクロフォン116によってキャプチャされたオーディオストリーム16に対してASR処理の減衰されたレベル204、222を使用するようにASRシステム150に指示する。 3 is a flow diagram of an example sequence of operations of a method 300 for attenuating audio processing. At operation 302, the method 300 receives an indication of a microphone trigger event 202 at a voice-enabled device 110 indicating a possible user interaction with the voice-enabled device 110 by speech, the voice-enabled device 110 having a microphone 116 configured to capture speech for recognition by an automatic speech recognition (ASR) system 150 when open. Operation 304 includes two sub-operations 304, 304a-b that occur in response to receiving the indication of the microphone trigger event 202. At operation 304a, the method 300 instructs the microphone 116 to open or remain open for an open microphone duration window 212 to capture an audio stream 16 within an environment of the voice-enabled device 110. In operation 304b, the method 300 provides the audio stream 16 captured by the open microphone 116 to the ASR system 150 for performing ASR processing on the audio stream 16. Operation 306 includes two sub-operations 306, 306a-b that occur while the ASR system 150 is performing ASR processing on the audio stream 16 captured by the open microphone 116. In operation 306, the method 300 attenuates the level 222 of ASR processing that the ASR system 150 performs on the audio stream 16 based on a function of the open microphone duration window 212. In operation 306b, the method 300 instructs the ASR system 150 to use the attenuated level 204, 222 of ASR processing on the audio stream 16 captured by the open microphone 116.
図4は、本明細書において説明されたシステム(たとえば、デバイス110、インターフェース120、リモートシステム140、音声認識システム150、検索エンジン160、および/または分析器200)ならびに方法(たとえば、方法300)を実装するために使用されてよい例示的なコンピューティングデバイス400の概略図である。コンピューティングデバイス400は、ラップトップ、デスクトップ、ワークステーション、携帯情報端末、サーバ、ブレードサーバ、メインフレーム、およびその他の適切なコンピュータなどの様々な形態のデジタルコンピュータを表すように意図される。本明細書に示される構成要素、それらの構成要素の接続および関係、ならびにそれらの構成要素の機能は、単に例示的であるように意図されており、本明細書において説明および/または特許請求される本発明の実装を限定するように意図されていない。 FIG. 4 is a schematic diagram of an exemplary computing device 400 that may be used to implement the systems (e.g., device 110, interface 120, remote system 140, speech recognition system 150, search engine 160, and/or analyzer 200) and methods (e.g., method 300) described herein. Computing device 400 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other suitable computers. The components shown herein, the connections and relationships of those components, and the functions of those components are intended to be merely exemplary and are not intended to limit the implementation of the invention described and/or claimed herein.
コンピューティングデバイス400は、プロセッサ410、メモリ420、ストレージデバイス430、メモリ420および高速拡張ポート450に接続する高速インターフェース/コントローラ440、ならびに低速バス470およびストレージデバイス430に接続する低速インターフェース/コントローラ460を含む。構成要素410、420、430、440、450、および460の各々は、様々なバスを使用して相互接続されており、共通のマザーボードに搭載されるか、または適宜その他の方法で搭載される場合がある。プロセッサ410は、メモリ420内またはストレージデバイス430上に記憶された命令を含む、コンピューティングデバイス400内で実行するための命令を処理して、高速インターフェース440に結合されたディスプレイ480などの外部入力/出力デバイス上のグラフィカルユーザインターフェース(GUI)のためのグラフィカルな情報を表示することができる。その他の実装においては、複数のプロセッサおよび/または複数のバスが、複数のメモリおよび複数の種類のメモリと一緒に適宜使用される場合がある。また、複数のコンピューティングデバイス400が、各デバイスが必要な動作の一部を提供するようにして(たとえば、サーババンク、一群のブレードサーバ、またはマルチプロセッサシステムとして)接続される場合がある。 The computing device 400 includes a processor 410, a memory 420, a storage device 430, a high-speed interface/controller 440 that connects to the memory 420 and a high-speed expansion port 450, and a low-speed interface/controller 460 that connects to a low-speed bus 470 and the storage device 430. Each of the components 410, 420, 430, 440, 450, and 460 are interconnected using various buses and may be mounted on a common motherboard or otherwise mounted as appropriate. The processor 410 can process instructions for execution within the computing device 400, including instructions stored in the memory 420 or on the storage device 430, to display graphical information for a graphical user interface (GUI) on an external input/output device, such as a display 480 coupled to the high-speed interface 440. In other implementations, multiple processors and/or multiple buses may be used along with multiple memories and multiple types of memories as appropriate. Additionally, multiple computing devices 400 may be connected together (e.g., as a server bank, a collection of blade servers, or a multiprocessor system) with each device providing a portion of the required operations.
メモリ420は、コンピューティングデバイス400内で情報を非一時的に記憶する。メモリ420は、コンピュータ可読媒体、揮発性メモリユニット、または不揮発性メモリユニットであってよい。非一時的メモリ420は、コンピューティングデバイス400による使用のために、プログラム(たとえば、命令のシーケンス)またはデータ(たとえば、プログラムの状態情報)を一時的または永続的に記憶するために使用される物理的デバイスであってよい。不揮発性メモリの例は、フラッシュメモリおよび読み出し専用メモリ(ROM)/プログラマブル読み出し専用メモリ(PROM)/消去可能プログラマブル読み出し専用メモリ(EPROM)/電子的消去可能プログラマブル読み出し専用メモリ(EEPROM)(たとえば、典型的にはブートプログラムなどのファームウェアのために使用される)を含むがこれらに限定されない。揮発性メモリの例は、ランダムアクセスメモリ(RAM)、ダイナミックランダムアクセスメモリ(DRAM)、スタティックランダムアクセスメモリ(SRAM)、相変化メモリ(PCM)、およびディスクまたはテープを含むがこれらに限定されない。 The memory 420 stores information non-temporarily within the computing device 400. The memory 420 may be a computer-readable medium, a volatile memory unit, or a non-volatile memory unit. The non-transient memory 420 may be a physical device used to temporarily or permanently store programs (e.g., sequences of instructions) or data (e.g., program state information) for use by the computing device 400. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electronically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware such as boot programs). Examples of volatile memory include, but are not limited to, random access memory (RAM), dynamic random access memory (DRAM), static random access memory (SRAM), phase change memory (PCM), and disk or tape.
ストレージデバイス430は、コンピューティングデバイス400に大容量ストレージを提供することができる。一部の実装において、ストレージデバイス430は、コンピュータ可読媒体である。様々な異なる実装において、ストレージデバイス430は、フロッピーディスクデバイス、ハードディスクデバイス、光ディスクデバイス、またはテープデバイス、フラッシュメモリもしくはその他の同様のソリッドステートメモリデバイス、またはストレージエリアネットワークもしくはその他の構成内のデバイスを含むデバイスのアレイであってよい。追加的な実装においては、コンピュータプログラム製品が、情報担体内に有形で具現化される。コンピュータプログラム製品は、実行されるときに上述の方法などの1つまたは複数の方法を実行する命令を含む。情報担体は、メモリ420、ストレージデバイス430、またはプロセッサ410上のメモリなどのコンピュータ可読媒体または機械可読媒体である。 The storage device 430 can provide mass storage for the computing device 400. In some implementations, the storage device 430 is a computer-readable medium. In various different implementations, the storage device 430 can be a floppy disk device, a hard disk device, an optical disk device, or an array of devices including a tape device, a flash memory or other similar solid-state memory device, or a device in a storage area network or other configuration. In additional implementations, a computer program product is tangibly embodied in an information carrier. The computer program product includes instructions that, when executed, perform one or more methods, such as the methods described above. The information carrier is a computer-readable or machine-readable medium, such as the memory 420, the storage device 430, or a memory on the processor 410.
高速コントローラ440は、コンピューティングデバイス400に関する帯域幅を大量に消費する動作を管理し、一方、低速コントローラ460は、帯域幅をそれほど消費しない動作を管理する。役割のそのような割り当ては、例示的であるに過ぎない。一部の実装において、高速コントローラ440は、メモリ420に、(たとえば、グラフィックスプロセッサまたはアクセラレータを通じて)ディスプレイ480に、および様々な拡張カード(図示せず)を受け入れてよい高速拡張ポート450に結合される。一部の実装において、低速コントローラ460は、ストレージデバイス430および低速拡張ポート490に結合される。様々な通信ポート(たとえば、USB、Bluetooth、イーサネット、ワイヤレスイーサネット)を含んでよい低速拡張ポート490は、キーボード、ポインティングデバイス、スキャナなどの1つもしくは複数の入力/出力デバイスに結合される場合があり、またはたとえばネットワークアダプタを介してスイッチもしくはルータなどのネットワークデバイスに結合される場合がある。 The high-speed controller 440 manages bandwidth-intensive operations for the computing device 400, while the low-speed controller 460 manages less bandwidth-intensive operations. Such assignment of roles is merely exemplary. In some implementations, the high-speed controller 440 is coupled to the memory 420, to the display 480 (e.g., through a graphics processor or accelerator), and to a high-speed expansion port 450 that may accept various expansion cards (not shown). In some implementations, the low-speed controller 460 is coupled to the storage device 430 and to a low-speed expansion port 490. The low-speed expansion port 490, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet), may be coupled to one or more input/output devices, such as a keyboard, pointing device, scanner, or may be coupled to a network device, such as a switch or router, for example, via a network adapter.
コンピューティングデバイス400は、図に示されるように、いくつかの異なる形態で実装されてよい。たとえば、コンピューティングデバイス400は、標準的なサーバ400aとして、もしくはそのようなサーバ400aのグループ内で複数回、ラップトップコンピュータ400bとして、またはラックサーバシステム400cの一部として実装されてよい。 The computing device 400 may be implemented in a number of different forms, as shown in the figure. For example, the computing device 400 may be implemented as a standard server 400a, or multiple times within a group of such servers 400a, as a laptop computer 400b, or as part of a rack server system 400c.
本明細書に記載のシステムおよび技術の様々な実装は、デジタル電子および/もしくは光回路、集積回路、特別に設計されたASIC(特定用途向け集積回路)、コンピュータハードウェア、ファームウェア、ソフトウェア、ならびに/またはこれらの組合せで実現され得る。これらの様々な実装は、ストレージシステム、少なくとも1つの入力デバイス、および少なくとも1つの出力デバイスからデータおよび命令を受信し、それらにデータおよび命令を送信するために結合された、専用または汎用であってよい少なくとも1つのプログラミング可能なプロセッサを含むプログラミング可能なシステム上の、実行可能および/または解釈可能な1つまたは複数のコンピュータプログラムへの実装を含み得る。 Various implementations of the systems and techniques described herein may be realized in digital electronic and/or optical circuitry, integrated circuits, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations may include implementation in one or more computer programs executable and/or interpretable on a programmable system including at least one programmable processor, which may be special purpose or general purpose, coupled to receive data and instructions from and transmit data and instructions to a storage system, at least one input device, and at least one output device.
これらのコンピュータプログラム(プログラム、ソフトウェア、ソフトウェアアプリケーション、またはコードとしても知られる)は、プログラミング可能なプロセッサ用の機械命令を含み、高レベル手続き型プログラミング言語および/もしくはオブジェクト指向プログラミング言語、ならびに/またはアセンブリ/機械言語で実装され得る。本明細書において使用されるとき、用語「機械可読媒体」および「コンピュータ可読媒体」は、機械命令を機械可読信号として受け取る機械可読媒体を含む、プログラミング可能なプロセッサに機械命令および/またはデータを提供するために使用される任意のコンピュータプログラム製品、非一時的コンピュータ可読媒体、装置、および/またはデバイス(たとえば、磁気ディスク、光ディスク、メモリ、プログラマブルロジックデバイス(PLD))を指す。用語「機械可読信号」は、プログラミング可能なプロセッサに機械命令および/またはデータを提供するために使用される任意の信号を指す。 These computer programs (also known as programs, software, software applications, or code) contain machine instructions for a programmable processor and may be implemented in high-level procedural and/or object-oriented programming languages, and/or assembly/machine languages. As used herein, the terms "machine-readable medium" and "computer-readable medium" refer to any computer program product, non-transitory computer-readable medium, apparatus, and/or device (e.g., magnetic disk, optical disk, memory, programmable logic device (PLD)) used to provide machine instructions and/or data to a programmable processor, including machine-readable media that receive machine instructions as machine-readable signals. The term "machine-readable signal" refers to any signal used to provide machine instructions and/or data to a programmable processor.
本明細書に記載のプロセスおよび論理フローは、入力データに基づいて動作し、出力を生成することによって機能を実行するために1つまたは複数のコンピュータプログラムをデータ処理ハードウェアとも呼ばれる1つまたは複数のプログラミング可能なプロセッサが実行することによって実行され得る。また、プロセスおよび論理フローは、専用の論理回路、たとえば、FPGA(フィールドプログラマブルゲートアレイ)またはASIC(特定用途向け集積回路)によって実行され得る。コンピュータプログラムの実行に好適なプロセッサは、例として、汎用マイクロプロセッサと専用マイクロプロセッサとの両方、および任意の種類のデジタルコンピュータの任意の1つまたは複数のプロセッサを含む。概して、プロセッサは、読み出し専用メモリ、またはランダムアクセスメモリ、またはこれらの両方から命令およびデータを受け取る。コンピュータの必須の要素は、命令を実行するためのプロセッサ、ならびに命令およびデータを記憶するための1つまたは複数のメモリデバイスである。また、概して、コンピュータは、データを記憶するための1つもしくは複数の大容量ストレージデバイス、たとえば、磁気ディスク、光磁気ディスク、もしくは光ディスクを含むか、またはそれらの大容量ストレージデバイスからデータを受信するか、もしくはそれらの大容量ストレージデバイスにデータを転送するか、もしくはその両方を行うために動作可能なように結合される。しかし、コンピュータは、そのようなデバイスを有していなくてもよい。コンピュータプログラム命令およびデータを記憶するのに好適なコンピュータ可読媒体は、例として、半導体メモリデバイス、たとえば、EPROM、EEPROM、およびフラッシュメモリデバイス、磁気ディスク、たとえば、内蔵ハードディスクまたはリムーバブルディスク、光磁気ディスク、ならびにCD ROMディスクおよびDVD-ROMディスクを含む、すべての形態の不揮発性メモリ、媒体、およびメモリデバイスを含む。プロセッサおよびメモリは、専用の論理回路によって補完され得るか、または専用の論理回路に組み込まれ得る。 The processes and logic flows described herein may be executed by one or more programmable processors, also referred to as data processing hardware, executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows may also be executed by dedicated logic circuitry, for example, an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). Processors suitable for executing computer programs include, by way of example, both general-purpose and dedicated microprocessors, and any one or more processors of any type of digital computer. Generally, a processor receives instructions and data from a read-only memory, or a random access memory, or both. The essential elements of a computer are a processor for executing instructions, and one or more memory devices for storing instructions and data. Generally, a computer also includes one or more mass storage devices, for example, magnetic disks, magneto-optical disks, or optical disks, for storing data, or is operatively coupled to receive data from or transfer data to the mass storage devices, or both. However, a computer may not have such devices. Suitable computer-readable media for storing computer program instructions and data include all forms of non-volatile memory, media, and memory devices, including, by way of example, semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices, magnetic disks, e.g., internal hard disks or removable disks, magneto-optical disks, and CD ROM and DVD-ROM disks. The processor and memory may be supplemented by, or incorporated in, dedicated logic circuitry.
ユーザとのインタラクションを提供するために、本開示の1つまたは複数の態様は、ユーザに対して情報を表示するためのディスプレイデバイス、たとえば、CRT(ブラウン管)、LCD(液晶ディスプレイ)モニタ、またはタッチスクリーンと、任意で、ユーザがコンピュータに入力を与えることができるキーボードおよびポインティングデバイス、たとえば、マウスまたはトラックボールとを有するコンピュータ上に実装され得る。その他の種類のデバイスが、ユーザとのインタラクションを提供するためにやはり使用されることが可能であり、たとえば、ユーザに提供されるフィードバックは、任意の形態の感覚フィードバック、たとえば、視覚フィードバック、聴覚フィードバック、または触覚フィードバックであることが可能であり、ユーザからの入力は、音響、スピーチ、または触覚による入力を含む任意の形態で受け取られることが可能である。加えて、コンピュータは、ユーザによって使用されるデバイスに文書を送信し、そのデバイスから文書を受信することによって、たとえば、ウェブブラウザから受信された要求に応答してユーザのクライアントデバイスのウェブブラウザにウェブページを送信することによってユーザとインタラクションすることができる。 To provide interaction with a user, one or more aspects of the present disclosure may be implemented on a computer having a display device, e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor, or touch screen, for displaying information to the user, and optionally a keyboard and pointing device, e.g., a mouse or trackball, by which the user can provide input to the computer. Other types of devices may also be used to provide interaction with the user, e.g., feedback provided to the user may be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback, and input from the user may be received in any form, including acoustic, speech, or tactile input. In addition, the computer may interact with the user by sending documents to and receiving documents from a device used by the user, e.g., by sending a web page to a web browser on the user's client device in response to a request received from the web browser.
いくつかの実装が、説明された。しかしながら、本開示の精神および範囲を逸脱することなく様々な修正がなされてよいことは、理解されるであろう。したがって、その他の実装は、添付の請求項の範囲内にある。 Several implementations have been described. However, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly, other implementations are within the scope of the following claims.
10 ユーザ
12 ユーザインタラクション
12U 口頭の発話
12Ua 第1の発話
12Ub 第2の発話
14 ホットワード、トリガ
16 オーディオストリーム
100 システム
110 音声対応デバイス、デバイス、ユーザデバイス
112 データ処理ハードウェア
114 メモリハードウェア
116 オーディオキャプチャデバイス、マイクロフォン
120 デジタルアシスタントインターフェース、音声対応インターフェース、インターフェース、アシスタントインターフェース
122、122a～c 応答
122a 第1の応答
122b 第2の応答
124 オーディオデータ
130 ネットワーク
140 リモートシステム
142 リモートリソース
144 リモートデータ処理ハードウェア
146 リモートメモリハードウェア
150 音声認識システム
152 文字起こし、音声認識結果
160 検索エンジン
162 検索結果
200 インタラクション分析器、分析器
202 マイクロフォントリガイベント、トリガイベント
202a 第1のトリガイベント
202b 第2のトリガイベント
202c 第3のトリガイベント
204 減衰状態
210 ウィンドウ生成器、生成器
212 開いたマイクロフォン継続時間ウィンドウ
212a 第1のマイクロフォン継続時間ウィンドウ、第1のウィンドウ
212b 第2のマイクロフォン継続時間ウィンドウ、新しいマイクロフォン継続時間ウィンドウ、第2のウィンドウ
212c 第3のウィンドウ
214 開始点
214b 第2の開始点
216 終了点
216b 第2の終了点
220 リデューサ
222 処理レベル
222a 第1の処理レベル
222b 第2の処理レベル
222c 第3の処理レベル
224 時間期間
226 時間の閾値
300 方法
400 コンピューティングデバイス
400a 標準的なサーバ
400b ラップトップコンピュータ
400c ラックサーバシステム
410 プロセッサ
420 メモリ
430 ストレージデバイス
440 高速インターフェース/コントローラ
450 高速拡張ポート
460 低速インターフェース/コントローラ
470 低速バス
480 ディスプレイ
490 低速拡張ポート
10 users
12 User Interaction
12U Oral Speech
12Ua First utterance
12Ub Second utterance
14 Hotwords, Triggers
16 Audio Streams
100 Systems
110 Voice-enabled devices, devices, user devices
112 Data Processing Hardware
114 Memory Hardware
116 Audio capture device, microphone
120 Digital assistant interface, voice-enabled interface, interface, assistant interface
122, 122a-c Responses
122a First Response
122b Second Response
124 Audio Data
130 Network
140 Remote Systems
142 Remote Resources
144 Remote Data Processing Hardware
146 Remote Memory Hardware
150 Voice Recognition System
152 Transcription and speech recognition results
160 Search Engines
162 Results
200 Interaction Analyzer, Analyzer
202 Microphone trigger event, trigger event
202a First trigger event
202b Second trigger event
202c Third trigger event
204 Attenuation state
210 Window Generator, Generator
212 Open Microphone Duration Windows
212a first microphone duration window, first window
212b second microphone duration window, new microphone duration window, second window
212c Third Window
214 Starting Point
214b Second starting point
216 End Point
216b Second End Point
220 Reducer
222 Processing Level
222a First Processing Level
222b Second Processing Level
222c Third Processing Level
224 hour period
226 Hour Threshold
300 Ways
400 computing devices
400a Standard Server
400b Laptop Computer
400c Rack Server System
410 Processor
420 Memory
430 Storage Devices
440 High Speed Interface/Controller
450 High Speed Expansion Port
460 Low Speed Interface/Controller
470 Slow Bus
480 Display
490 Low Speed Expansion Port
Claims (28)
前記マイクロフォントリガイベント(202)の前記インジケーションの受信に応答して(122)、
前記データ処理ハードウェア(112)によって、前記音声対応デバイス(110)の環境内でオーディオストリーム(16)をキャプチャするために、開くか、または開いたマイクロフォン継続時間ウィンドウ(212)の間、開いたままであるように前記マイクロフォン(116)に指示するステップと、
前記データ処理ハードウェア(112)によって、前記開いたマイクロフォン(116)によりキャプチャされた前記オーディオストリーム(16)を、前記オーディオストリーム(16)に対してASR処理を実行するために前記ASRシステム(150)に提供するステップと、
前記ASRシステム(150)が前記開いたマイクロフォン(116)によってキャプチャされた前記オーディオストリーム(16)に対して前記ASR処理を実行している間に、
前記データ処理ハードウェア(112)によって、前記ASRシステム(150)が前記オーディオストリーム(16)に対して実行する前記ASR処理のレベルを、前記開いたマイクロフォン継続時間ウィンドウ(212)の関数に基づいて減衰させるステップと、
前記データ処理ハードウェア(112)によって、前記開いたマイクロフォン(116)によってキャプチャされた前記オーディオストリーム(16)に対して前記ASR処理の前記減衰されたレベル(204、222)を使用するように前記ASRシステム(150)に指示するステップと
を含む方法(300)。 receiving, at data processing hardware (112) of a voice-enabled device (110), an indication of a microphone trigger event (202) indicative of a possible user interaction (12) with the voice-enabled device (110) by speech, the voice-enabled device (110) having a microphone (116), the microphone (116) configured, when open, to capture speech for recognition by an automatic speech recognition (ASR) system (150);
In response to receiving (122) the indication of the microphone trigger event (202),
instructing, by the data processing hardware (112), the microphone (116) to open or remain open for an open microphone duration window (212) to capture an audio stream (16) within the environment of the voice-enabled device (110);
providing, by the data processing hardware (112), the audio stream (16) captured by the open microphone (116) to the ASR system (150) for performing ASR processing on the audio stream (16);
While the ASR system (150) is performing the ASR processing on the audio stream (16) captured by the open microphone (116),
attenuating, by the data processing hardware (112), a level of the ASR processing performed by the ASR system (150) on the audio stream (16) based on a function of the open microphone duration window (212);
and instructing, by the data processing hardware (112), the ASR system (150) to use the attenuated level (204, 222) of ASR processing on the audio stream (16) captured by the open microphone (116).
前記データ処理ハードウェア(112)によって、前記開いたマイクロフォン(116)によりキャプチャされた前記オーディオストリーム(16)内で音声活動が検出されるかどうかを判定するステップをさらに含み、
前記ASRシステム(150)が前記オーディオストリーム(16)に対して実行する前記ASR処理の前記レベルを減衰させるステップが、前記オーディオストリーム(16)内で何らかの音声活動が検出されるかどうかの前記判定にさらに基づく、請求項1に記載の方法(300)。 While the ASR system (150) is performing the ASR processing on the audio stream (16) captured by the open microphone (116),
determining, by the data processing hardware (112), whether voice activity is detected within the audio stream (16) captured by the open microphone (116);
2. The method of claim 1, wherein attenuating the level of the ASR processing performed by the ASR system on the audio stream is further based on the determination of whether any voice activity is detected within the audio stream.
前記ASRシステム(150)が前記オーディオストリーム(16)に対して実行する前記ASR処理の前記レベルを、前記開いたマイクロフォン継続時間ウィンドウ(212)の前記関数に基づいて減衰させるステップが、
前記開いたマイクロフォン継続時間ウィンドウ(212)を開始してから第1の時間間隔が経過したかどうかを判定することと、
前記第1の時間間隔が経過したとき、前記ASRシステム(150)が前記オーディオストリーム(16)に対して実行する前記ASR処理の前記レベルを、前記ASR処理の前記レベルを前記第1の処理レベル(222)から第2の処理レベル(222)に下げることによって減衰させることであって、前記第2の処理レベル(222)が、前記第1の処理レベル(222)よりも低い、減衰させることとを含む、請求項1または2に記載の方法(300)。 the ASR system (150) initially uses a first processing level (222) to perform the ASR processing on the audio stream (16) at the beginning of the open microphone duration window (212), the first processing level (222) being associated with a maximum processing capability of the ASR system (150);
attenuating the level of the ASR processing that the ASR system (150) performs on the audio stream (16) based on the function of the open microphone duration window (212),
determining whether a first time interval has elapsed since starting the open microphone duration window (212);
3. The method of claim 1, further comprising attenuating, when the first time interval has elapsed, the level of the ASR processing that the ASR system performs on the audio stream by reducing the level of the ASR processing from the first processing level to a second processing level, the second processing level being lower than the first processing level.
前記ASR処理の前記減衰されたレベル(204、222)を使用するように前記ASRシステム(150)に指示するステップが、前記現在のコンテキストに基づいて音声認識結果(152)にバイアスをかけるように前記ASRシステム(150)に指示することを含む、請求項1から8のいずれか一項に記載の方法(300)。 obtaining, by the data processing hardware (112), a current context when the indication of the microphone trigger event (202) is received;
9. The method (300) of claim 1, wherein the step of instructing the ASR system (150) to use the attenuated level of ASR processing (204, 222) comprises instructing the ASR system (150) to bias speech recognition results (152) based on the current context.
前記ユーザによって話されたクエリに対応するオーディオデータ(124)の音声認識結果(152)を生成し、
前記クエリによって指定されたアクションを実行するために、前記音声認識結果(152)をアプリケーションに提供するように構成される、請求項1から10のいずれか一項に記載の方法(300)。 While the ASR system (150) is using the attenuated level (204, 222) of the ASR processing on the audio stream (16) captured by the open microphone (116), the ASR system (150)
generating speech recognition results (152) for the audio data (124) corresponding to a query spoken by the user;
The method (300) of any one of claims 1 to 10, configured to provide the speech recognition results (152) to an application for performing an action specified by the query.
前記データ処理ハードウェア(112)において、前記ASRシステム(150)によって出力された音声クエリに関する音声認識結果(152)の信頼度が信頼度の閾値を満たすことができないというインジケーションを受信するステップと、
前記データ処理ハードウェア(112)によって、前記ASRシステム(150)に、
前記ASR処理の前記レベルを前記減衰されたレベル(204、222)から上げ、
前記ASR処理の前記上げられたレベルを使用して前記音声クエリを再処理するように指示するステップとをさらに含む、請求項1から11のいずれか一項に記載の方法(300)。 after instructing the ASR system (150) to use the attenuated level (204, 222) of ASR processing on the audio stream (16);
receiving, at the data processing hardware (112), an indication that a confidence level of a speech recognition result (152) for a spoken query output by the ASR system (150) fails to meet a confidence threshold;
The data processing hardware (112) causes the ASR system (150) to
increasing the level of the ASR processing from the attenuated level (204, 222);
and instructing the spoken query to be reprocessed using the increased level of ASR processing.
前記データ処理ハードウェア(112)によって、前記開いたマイクロフォン(116)の継続時間の前記関数に基づいて前記ASRが前記オーディオストリーム(16)に対して実行する前記ASR処理の前記減衰されたレベル(204、222)がゼロに等しいときを判定するステップと、
前記ASR処理の前記減衰されたレベル(204、222)がゼロに等しいときに、前記データ処理ハードウェア(112)によって、前記マイクロフォン(116)に閉じるよう指示するステップとをさらに含む、請求項1から12のいずれか一項に記載の方法(300)。 While the ASR system (150) is performing the ASR processing on the audio stream (16) captured by the open microphone (116),
determining, by the data processing hardware (112), when the attenuated level (204, 222) of the ASR processing that the ASR performs on the audio stream (16) based on the function of the duration of the open microphone (116) is equal to zero;
and instructing, by the data processing hardware (112), the microphone (116) to close when the attenuated level of the ASR processing (204, 222) is equal to zero.
前記データ処理ハードウェア(112)と通信するメモリハードウェア(114)であって、前記データ処理ハードウェア(112)上で実行されるときに、前記データ処理ハードウェア(112)に、
音声対応デバイス(110)において、スピーチによる前記音声対応デバイス(110)との起こり得るユーザインタラクション(12)を示すマイクロフォントリガイベント(202)のインジケーションを受信する動作であって、前記音声対応デバイス(110)がマイクロフォン(116)を有し、前記マイクロフォン(116)は、開いているときに自動音声認識(ASR)システム(150)による認識のためにスピーチをキャプチャするように構成される、動作と、
前記マイクロフォントリガイベント(202)の前記インジケーションの受信に応答して(122)、
前記音声対応デバイス(110)の環境内でオーディオストリーム(16)をキャプチャするために、開くか、または開いたマイクロフォン継続時間ウィンドウ(212)の間、開いたままであるように前記マイクロフォン(116)に指示する動作と
前記開いたマイクロフォン(116)によりキャプチャされた前記オーディオストリーム(16)を、前記オーディオストリーム(16)に対してASR処理を実行するために前記ASRシステム(150)に提供する動作と
前記ASRシステム(150)が、前記開いたマイクロフォン(116)によってキャプチャされた前記オーディオストリーム(16)に対して前記ASR処理を実行している間に、
前記ASRシステム(150)が前記オーディオストリーム(16)に対して実行する前記ASR処理のレベルを、前記開いたマイクロフォン継続時間ウィンドウ(212)の関数に基づいて減衰させる動作と、
前記開いたマイクロフォン(116)によってキャプチャされた前記オーディオストリーム(16)に対して前記ASR処理の前記減衰されたレベル(204、222)を使用するように前記ASRシステム(150)に指示する動作と
を含む動作を実行させる命令を記憶するメモリハードウェア(114)と
を含むシステム(100)。 Data processing hardware (112);
memory hardware (114) in communication with the data processing hardware (112), the memory hardware (114) being configured to, when executed on the data processing hardware (112), cause the data processing hardware (112) to:
An operation of receiving, at a voice-enabled device (110), an indication of a microphone trigger event (202) indicative of a possible user interaction (12) with the voice-enabled device (110) by speech, the voice-enabled device (110) having a microphone (116), the microphone (116) configured, when open, to capture speech for recognition by an automatic speech recognition (ASR) system (150);
In response to receiving (122) the indication of the microphone trigger event (202),
instructing a microphone (116) to open or remain open for an open microphone duration window (212) to capture an audio stream (16) within the environment of the voice-enabled device (110); providing the audio stream (16) captured by the open microphone (116) to the ASR system (150) for performing ASR processing on the audio stream (16); and while the ASR system (150) is performing the ASR processing on the audio stream (16) captured by the open microphone (116),
attenuating a level of the ASR processing that the ASR system (150) performs on the audio stream (16) based on a function of the open microphone duration window (212);
and instructing the ASR system (150) to use the attenuated level of ASR processing (204, 222) on the audio stream (16) captured by the open microphone (116).
前記開いたマイクロフォン(116)によりキャプチャされた前記オーディオストリーム(16)内で音声活動が検出されるかどうかを判定する動作をさらに含み、
前記ASRシステム(150)が前記オーディオストリーム(16)に対して実行する前記ASR処理の前記レベルを減衰させる動作が、前記オーディオストリーム(16)内で何らかの音声活動が検出されるかどうかの前記判定にさらに基づく、請求項15に記載のシステム(100)。 The operation includes, while the ASR system (150) is performing the ASR processing on the audio stream (16) captured by the open microphone (116),
further comprising the act of determining whether voice activity is detected within the audio stream (16) captured by the open microphone (116);
16. The system (100) of claim 15, wherein the action of attenuating the level of the ASR processing performed by the ASR system (150) on the audio stream (16) is further based on the determination of whether any voice activity is detected within the audio stream (16).
前記ASRシステム(150)が前記オーディオストリーム(16)に対して実行する前記ASR処理の前記レベルを、前記開いたマイクロフォン継続時間ウィンドウ(212)の前記関数に基づいて減衰させる動作が、
前記開いたマイクロフォン継続時間ウィンドウ(212)を開始してから第1の時間間隔が経過したかどうかを判定することと、
前記第1の時間間隔が経過したとき、前記ASRシステム(150)が前記オーディオストリーム(16)に対して実行する前記ASR処理の前記レベルを、前記ASR処理の前記レベルを前記第1の処理レベル(222)から第2の処理レベル(222)に下げることによって減衰させることであって、前記第2の処理レベル(222)が、前記第1の処理レベル(222)よりも低い、減衰させることとを含む、請求項15または16に記載のシステム(100)。 the ASR system (150) initially uses a first processing level (222) to perform the ASR processing on the audio stream (16) at the beginning of the open microphone duration window (212), the first processing level (222) being associated with a maximum processing capability of the ASR system (150);
attenuating the level of the ASR processing that the ASR system (150) performs on the audio stream (16) based on the function of the open microphone duration window (212),
determining whether a first time interval has elapsed since starting the open microphone duration window (212);
17. The system (100) of claim 15 or 16, further comprising attenuating, when the first time interval has elapsed, the level of the ASR processing performed by the ASR system (150) on the audio stream (16) by lowering the level of the ASR processing from the first processing level (222) to a second processing level (222), the second processing level (222) being lower than the first processing level (222).
前記マイクロフォントリガイベント(202)の前記インジケーションが受信されるときに、現在のコンテキストを取得する動作をさらに含み、
前記ASR処理の前記減衰されたレベル(204、222)を使用するように前記ASRシステム(150)に指示する動作が、前記現在のコンテキストに基づいて音声認識結果(152)にバイアスをかけるように前記ASRシステム(150)に指示することを含む、請求項15から22のいずれか一項に記載のシステム(100)。 The operation,
When the indication of the microphone trigger event (202) is received, the method further includes an operation of obtaining a current context;
23. The system (100) of claim 15, wherein the act of instructing the ASR system (150) to use the attenuated level of ASR processing (204, 222) comprises instructing the ASR system (150) to bias speech recognition results (152) based on the current context.
前記ユーザによって話されたクエリに対応するオーディオデータ(124)の音声認識結果(152)を生成し、
前記クエリによって指定されたアクションを実行するために、前記音声認識結果(152)をアプリケーションに提供するように構成される、請求項15から24のいずれか一項に記載のシステム(100)。 While the ASR system (150) is using the attenuated level (204, 222) of the ASR processing on the audio stream (16) captured by the open microphone (116), the ASR system (150)
generating speech recognition results (152) for the audio data (124) corresponding to a query spoken by the user;
25. The system (100) of any one of claims 15 to 24, configured to provide the speech recognition results (152) to an application for performing an action specified by the query.
前記ASRシステム(150)によって出力された音声クエリに関する音声認識結果(152)の信頼度が信頼度の閾値を満たすことができないというインジケーションを受信する動作と、
前記ASRシステム(150)に、
前記ASR処理の前記レベルを前記減衰されたレベル(204、222)から上げ、
前記ASR処理の前記上げられたレベルを使用して前記音声クエリを再処理するように指示する動作と
をさらに含む、請求項15から25のいずれか一項に記載のシステム(100)。 After the operation instructs the ASR system (150) to use the attenuated level (204, 222) of ASR processing on the audio stream (16),
receiving an indication that a confidence level of a speech recognition result (152) for a speech query output by the ASR system (150) fails to meet a confidence level threshold;
The ASR system (150),
increasing the level of the ASR processing from the attenuated level (204, 222);
and instructing the spoken query to be reprocessed using the increased level of ASR processing.
前記開いたマイクロフォン(116)の継続時間の前記関数に基づいて前記ASRが前記オーディオストリーム(16)に対して実行する前記ASR処理の前記減衰されたレベル(204、222)がゼロに等しいときを判定する動作と、
前記ASR処理の前記減衰されたレベル(204、222)がゼロに等しいときに、前記マイクロフォン(116)に閉じるよう指示する動作とをさらに含む、請求項15から26のいずれか一項に記載のシステム(100)。 The operation includes, while the ASR system (150) is performing the ASR processing on the audio stream (16) captured by the open microphone (116),
determining when the attenuated level (204, 222) of the ASR processing that the ASR performs on the audio stream (16) based on the function of the duration of the open microphone (116) is equal to zero;
and instructing the microphone to close when the attenuated level of the ASR processing is equal to zero.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/111,467 | 2020-12-03 |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2023534026A Division JP7436757B2 (en) | 2020-12-03 | 2021-11-16 | Attenuation of automatic speech recognition processing results |
Publications (1)
Publication Number | Publication Date |
---|---|
JP2024062993A true JP2024062993A (en) | 2024-05-10 |
Family
ID=
Similar Documents
Publication | Publication Date | Title |
---|---|---|
TWI683306B (en) | Control method of multi voice assistant | |
US20240096320A1 (en) | Decaying Automated Speech Recognition Processing Results | |
US11132509B1 (en) | Utilization of natural language understanding (NLU) models | |
WO2020228270A1 (en) | Speech processing method and device, computer device and storage medium | |
US11676625B2 (en) | Unified endpointer using multitask and multidomain learning | |
US11763819B1 (en) | Audio encryption | |
US11823664B2 (en) | Correcting speech misrecognition of spoken utterances | |
US11705117B2 (en) | Adaptive batching to reduce recognition latency | |
US20230298575A1 (en) | Freeze Words | |
KR20230020523A (en) | Automatic hotword threshold tuning | |
CN114708856A (en) | Voice processing method and related equipment thereof | |
JPWO2020003851A1 (en) | Audio processing device, audio processing method and recording medium | |
US20230223014A1 (en) | Adapting Automated Speech Recognition Parameters Based on Hotword Properties | |
JP2024062993A (en) | Attenuation of automatic speech recognition processing results | |
TW201937480A (en) | Adaptive waiting time system for voice input system and method thereof | |
KR20220010034A (en) | Enter voice-controlled content into a graphical user interface | |
US20230186909A1 (en) | Selecting between multiple automated assistants based on invocation properties | |
US20240013782A1 (en) | History-Based ASR Mistake Corrections | |
US20210350802A1 (en) | Method and system for performing speech recognition in an electronic device | |
WO2023113877A1 (en) | Selecting between multiple automated assistants based on invocation properties | |
CN114572781A (en) | Elevator control method, device, electronic equipment and readable storage medium | |
KR20240036701A (en) | Preservation of engagement state based on context signals |