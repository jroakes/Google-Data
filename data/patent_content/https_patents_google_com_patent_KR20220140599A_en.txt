KR20220140599A - Synthetic speech audio data generated on behalf of a human participant in a conversation - Google Patents
Synthetic speech audio data generated on behalf of a human participant in a conversation Download PDFInfo
- Publication number
- KR20220140599A KR20220140599A KR1020227031429A KR20227031429A KR20220140599A KR 20220140599 A KR20220140599 A KR 20220140599A KR 1020227031429 A KR1020227031429 A KR 1020227031429A KR 20227031429 A KR20227031429 A KR 20227031429A KR 20220140599 A KR20220140599 A KR 20220140599A
- Authority
- KR
- South Korea
- Prior art keywords
- client device
- user
- additional
- given user
- speech
- Prior art date
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
- G10L13/04—Details of speech synthesis systems, e.g. synthesiser structure or memory management
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
- G10L13/033—Voice editing, e.g. manipulating the voice of the synthesiser
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/08—Text analysis or generation of parameters for speech synthesis out of text, e.g. grapheme to phoneme translation, prosody generation or stress or intonation determination
- G10L13/10—Prosody rules derived from text; Stress or intonation
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L17/00—Speaker identification or verification
- G10L17/02—Preprocessing operations, e.g. segment selection; Pattern representation or modelling, e.g. based on linear discriminant analysis [LDA] or principal components; Feature selection or extraction
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/78—Detection of presence or absence of voice signals
- G10L25/87—Detection of discrete points within a voice signal
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
Abstract
대화에서 주어진 사용자를 대신하여 합성 스피치 오디오 데이터를 생성한다. 합성 스피치 오디오 데이터는 텍스트 세그먼트(들)를 통합하는 합성 스피치를 포함한다. 텍스트 세그먼트(들)는 스피치 인식 모델을 사용하여 주어진 사용자의 음성 입력을 프로세싱한 결과인 인식된 텍스트를 포함할 수 있고 및/또는 상기 텍스트 세그먼트(들)를 전달하는 렌더링된 제안의 선택을 포함할 수 있다. 일부 구현예는 텍스트 세그먼트의 스피치 합성에 사용하기 위한 하나 이상의 운율 프로퍼티들을 동적으로 결정하고, 하나 이상의 결정된 운율 프로퍼티들을 사용하여 합성 스피치를 생성한다. 운율 프로퍼티들은 스피치 합성에 사용된 텍스트 세그먼트(들), 추가 참가자(들)의 최근 음성 입력에 대응하는 텍스트 세그먼트(들), 대화에서 주어진 사용자와 추가 참가자(들) 간의 관계(들)의 속성(들) 및/또는 대화에 대한 현재 위치의 피처(들)에 기초하여 결정될 수 있다.Generate synthetic speech audio data on behalf of a given user in a conversation. Synthetic speech audio data includes synthesized speech incorporating text segment(s). The text segment(s) may contain recognized text that is the result of processing a given user's speech input using a speech recognition model and/or may contain a selection of rendered suggestions to convey the text segment(s). can Some implementations dynamically determine one or more prosody properties for use in speech synthesis of a text segment, and use the one or more determined prosody properties to generate synthesized speech. Prosody properties include: the text segment(s) used in speech synthesis, the text segment(s) corresponding to the recent voice input of the additional participant(s), and the attribute(s) of the relationship(s) between a given user and the additional participant(s) in the conversation ( ) and/or the feature(s) of the current location for the conversation.
Description
음성 기반 사용자 인터페이스들은 컴퓨터 및 기타 전자 디바이스의 제어에 점점 더 많이 사용되고 있다. 많은 음성 기반 사용자 인터페이스들은 음성 입력에 대해 스피치 인식(예: 스피치-텍스트 모델 사용)을 수행하여, 대응하는 텍스트를 생성하고, 음성 입력의 의미를 결정하기 위해 텍스트의 시맨틱 분석을 수행하고, 결정된 의미에 기초한 하나 이상의 액션에 착수한다. 착수된 액션(들)은 스마트 디바이스(들) 제어, 클라이언트 디바이스 제어 및/또는 자동 응답 결정 및/또는 제공을 포함할 수 있다. 일부 상황에서, 액션(들)은 자동화된 응답을 전달하고 및/또는 스마트 디바이스(들) 및/또는 클라이언트 디바이스가 제어되었음을 나타내는 합성 스피치를 생성하는 것을 포함한다.Voice-based user interfaces are increasingly used to control computers and other electronic devices. Many speech-based user interfaces perform speech recognition (e.g., using a speech-to-text model) on speech input to generate corresponding text, perform semantic analysis of the text to determine the meaning of the speech input, and determine the meaning. Initiate one or more actions based on The action(s) undertaken may include smart device(s) control, client device control and/or automatic response determination and/or provision. In some situations, the action(s) includes delivering an automated response and/or generating synthetic speech indicating that the smart device(s) and/or the client device has been controlled.
본 명세서에 개시된 구현예는 주어진 사용자 및 추가 참가자(들)(즉, 다른 사람 참가자(들))를 포함하는 대화와 같은 대화에서 주어진 사용자(즉, 사람 참가자)를 대신하여 합성 스피치 오디오 데이터를 생성하는 것에 관한 것이다. 예를 들어, 대화는 주어진 사용자와 주어진 사용자와 함께 환경에 있는 추가 참가자(들) 사이에 있을 수 있다. 또한, 합성 스피치 오디오 데이터는 환경에서 클라이언트 디바이스(들)의 하드웨어 스피커(들)를 통해 청각적으로 렌더링될 수 있으며, 이에 의해 추가 참가자(들)가 청각적으로 인지할 수 있다. 합성 스피치 오디오 데이터는 텍스트 세그먼트(들)를 통합하는 합성 스피치를 포함하며, 텍스트 세그먼트(들)는 주어진 사용자의 사용자 인터페이스 입력(들)에 기초하여 결정된다. Implementations disclosed herein generate synthetic speech audio data on behalf of a given user (ie, a human participant) in a conversation, such as a conversation involving a given user and additional participant(s) (ie, other human participant(s)). it's about doing For example, the conversation may be between a given user and additional participant(s) in the environment with the given user. Additionally, the synthesized speech audio data may be rendered audibly through the hardware speaker(s) of the client device(s) in the environment, thereby aurally perceptible by the additional participant(s). The synthesized speech audio data includes synthesized speech incorporating text segment(s), the text segment(s) being determined based on a given user's user interface input(s).
일부 구현예에서, 텍스트 세그먼트(들)가 결정되는 사용자 인터페이스 입력(들)은 주어진 사용자 음성 입력(또는 주어진 사용자의 음성 입력)을 포함할 수 있고, 텍스트 세그먼트(들)는 스피치 인식 모델을 사용하여 음성 입력을 프로세싱한 결과로 인식되는 것에 기초하여 결정될 수 있다. 이러한 구현예 중 일부에서, 스피치 인식 모델을 사용하여 음성 입력을 프로세싱하여, 음성 입력을 정확하게 반영하는 (주어진 사용자) 인식된 텍스트를 생성할 수 있다. 스피치 인식 모델을 사용하여 인식할 수 있지만, 음성 입력의 전부 또는 일부는 음성 입력 자체의 특정 특성(들)(예: 주어진 사용자의 영구적 또는 일시적 언어 장애) 및/또는 불리한 환경 조건(예: 과도한 배경 소음, 음성 입력과 겹치는 다른 음성 입력(들) 등)으로 인해 추가 사람 참가자(들) 중 한 명 이상에 의해 확인될 수 없을 수 있다. 반면에, 텍스트 세그먼트(들)를 포함하는 합성 스피치는 정확하게 인식된 텍스트 세그먼트(들)에 기초하여 생성될 수 있으며, 음성 입력의 확인을 방해하는 음성 입력의 특정 특성(들) 및/또는 불리한 환경 조건(들)을 포함하지 않을 것이다. 따라서, 렌더링될 때, 합성 스피치는 추가 참가자(들)에 의해 확인될 수 있다. 이러한 방식 및 다른 방식으로, 주어진 사용자와 추가 참가자(들) 간의 효율적인 대화를 촉진하기 위해 스피치 인식 및 스피치 합성이 조합되어 사용될 수 있다. 이것은 언어 장애가 있는 사용자에게 특히 유용할 수 있다. In some implementations, the user interface input(s) from which the text segment(s) are determined may include a given user speech input (or a given user's speech input), and the text segment(s) may be configured using a speech recognition model. It may be determined based on recognition as a result of processing the voice input. In some of these implementations, speech recognition models may be used to process speech input to generate (given user) recognized text that accurately reflects the speech input. Although it can be recognized using speech recognition models, all or part of the speech input may be derived from certain characteristic(s) of the speech input itself (e.g., permanent or temporary language impairment for a given user) and/or adverse environmental conditions (e.g., excessive background may not be ascertainable by one or more of the additional human participant(s) due to noise, other voice input(s) overlapping the voice input, etc.). On the other hand, synthesized speech comprising text segment(s) may be generated based on the correctly recognized text segment(s), certain characteristic(s) of the spoken input and/or adverse circumstances that would prevent confirmation of the spoken input. condition(s) shall not be included. Thus, when rendered, the synthesized speech can be confirmed by additional participant(s). In this and other ways, speech recognition and speech synthesis may be used in combination to facilitate an efficient conversation between a given user and additional participant(s). This can be particularly useful for users with language impairments.
사용자 인터페이스 입력(들)이 음성 입력을 포함하는 일부 구현예에서, 주어진 사용자는 언어 장애(들)를 갖고 스피치 인식 모델은 선택적으로 그러한 언어 장애(들)를 가진 사용자의 스피치를 인식하도록 트레이닝된 맞춤형 스피치 인식 모델일 수 있다. 예를 들어, 맞춤형 스피치 인식 모델은 장애 음성 입력(예를 들어, Mel-Frequency Cepstral Coefficient(들) 또는 대응하는 오디오 데이터의 다른 피처(들))을 나타내는 대응하는 트레이닝 인스턴스 입력 및 장애 음성 입력에 대응하는 실제 텍스트 세그먼트(들)를 나타내는 트레이닝 인스턴스 출력을 각각 포함하는 트레이닝 인스턴스에 기초하여 트레이닝될 수 있다. 예를 들어, 장애 음성 입력은 구음 장애(들), 유창성 장애(들) 및/또는 음성 장애(들)의 결과로 손상될 수 있다. 이러한 방식 및 기타 방식으로 스피치 인식 모델을 사용하여 장애 음성 입력에 대해 인식된 텍스트 세그먼트를 생성할 수 있다. 또한, 인식된 텍스트 세그먼트(들)를 통합하는 합성 스피치를 포함하는 합성 스피치 오디오 데이터가 생성될 수 있고, 합성 스피치 오디오 데이터는 추가 참가자(들)가 청각적으로 인지할 수 있도록 청각적으로 렌더링될 수 있다. 따라서, 본 명세서에 개시된 구현예 및 기타 구현예는 언어 장애가 있는 사용자에 대한 대화를 용이하게 할 수 있다. In some implementations where the user interface input(s) comprises spoken input, a given user has a language impairment(s) and the speech recognition model is optionally tailored to be trained to recognize the speech of a user with such language impairment(s). It may be a speech recognition model. For example, a custom speech recognition model may correspond to a impaired speech input and a corresponding training instance input representing the impaired speech input (eg, Mel-Frequency Cepstral Coefficient(s) or other feature(s) of the corresponding audio data) and the impaired speech input. may be trained based on the training instance, each comprising a training instance output representing the actual text segment(s) of For example, impaired speech input may be impaired as a result of dysarthria(s), fluency(s), and/or voice impairment(s). In this and other ways, speech recognition models can be used to generate recognized text segments for impaired speech input. In addition, synthetic speech audio data can be generated comprising synthetic speech incorporating the recognized text segment(s), the synthetic speech audio data being rendered audibly to be aurally perceptible by the additional participant(s). can Accordingly, implementations and other implementations disclosed herein may facilitate conversations for users with language impairments.
일부 추가 또는 대안 구현예에서, 텍스트 세그먼트(들)의 기반이 되는 사용자 인터페이스 입력(들)은 텍스트 세그먼트(들)를 전달하는 렌더링된 제안의 선택(예를 들어, 단일 "탭" 또는 다른 단일 선택)을 포함할 수 있다. 예를 들어, 텍스트 세그먼트가 추가 참가자의 최근 음성 입력에 대한 후보 응답이라는 결정에 기초하여 텍스트 세그먼트를 전달하는 제안이 주어진 사용자의 클라이언트 디바이스에서 렌더링될 수 있다. 예를 들어, 최근 음성 입력에 대해 스피치 인식을 수행하여 대응하는 텍스트 세그먼트를 생성하고, 텍스트 세그먼트를 생성된 대응하는 텍스트 세그먼트에 대한 후보 응답으로 결정할 수 있다. 텍스트 세그먼트가 후보 응답이라는 결정은 주어진 사용자와 추가 참가자 간의 관계 속성들 중 적어도 하나에 기초할 수 있다. 일부 예에서, 이것은 추가 참가자 텍스트 세그먼트(추가 참가자의 음성 입력으로부터 인식된 텍스트 세그먼트)에 기초하여 초기 후보 응답(또는 복수의 가능한 후보 응답들)의 수퍼세트를 생성하고, 초기 후보 응답의 수퍼세트로부터 관계의 속성들에 기초하여 텍스트 세그먼트를 후보 응답으로서 선택하는 것을 포함할 수 있다. 제안은 클라이언트 디바이스에서 그래픽 엘리먼트로서, 선택적으로 추가 참가자의 최근 음성 입력에 대한 후보 응답으로 결정된 대응하는 대체 텍스트 세그먼트를 각각 전달하는 다른 제안(들)과 함께(즉, 수퍼세트의 다른 후보 응답과 함께) 렌더링될 수 있다. 이러한 방식 및 기타 방식으로, 주어진 사용자는 텍스트 세그먼트(들)로 향하는 효율적인 사용자 인터페이스 입력(들)(예를 들어, 단일 선택)을 제공하여, 대응하는 합성 스피치가 추가 참가자에게 청각적으로 렌더링되게 할 수 있다. 이것은 주어진 사용자의 음성 입력의 스피치 인식 프로세싱을 우회할 수 있게 하여, 주어진 사용자의 청각적 합성 응답을 보다 빠르게 제공할 수 있게 하여, 대화의 전체 지속시간 및 클라이언트 디바이스 리소스가 대화를 용이하게 하는데 활용되는 전체 지속시간을 단축할 수 있다. 따라서 언어 장애가 있는 사용자를 위한 대화는 계산 및/또는 네트워크 리소스의 상당한 증가 없이 개선될 수 있다. In some additional or alternative implementations, the user interface input(s) upon which the text segment(s) are based is a selection of rendered suggestions that convey the text segment(s) (eg, a single “tap” or other single selection). ) may be included. For example, an offer to deliver the text segment may be rendered at the given user's client device based on a determination that the text segment is a candidate response to the additional participant's recent voice input. For example, speech recognition may be performed on a recent voice input to generate a corresponding text segment, and the text segment may be determined as a candidate response to the generated corresponding text segment. The determination that the text segment is a candidate response may be based on at least one of relationship attributes between the given user and the additional participant. In some examples, this generates a superset of an initial candidate response (or a plurality of possible candidate responses) based on the additional participant text segment (a text segment recognized from the additional participant's speech input), and from the superset of the initial candidate response. may include selecting a text segment as a candidate response based on attributes of the relationship. The proposal is a graphical element at the client device, optionally along with other proposal(s) each conveying a corresponding alternative text segment determined as a candidate response to the additional participant's recent speech input (ie, along with a superset of other candidate responses). ) can be rendered. In this and other manners, a given user may provide efficient user interface input(s) (eg, a single selection) directed to the text segment(s) so that the corresponding synthesized speech is audibly rendered to additional participants. can This allows bypassing speech recognition processing of a given user's speech input, thereby providing a given user's auditory synthesized response faster, allowing the overall duration of the conversation and client device resources to be utilized to facilitate the conversation. The overall duration can be shortened. Thus, conversations for users with language impairments can be improved without a significant increase in computational and/or network resources.
다양한 구현예에서, 추가 참가자(들)의 최근 음성 입력에 대응하는 생성된 텍스트 세그먼트(들)에 기초하여 결정되는 후보 응답은 음성 입력이 주어진 사용자가 아닌 추가 참가자에 의해 제공되었음을 식별하는 것에 응답하여 주어진 사용자의 클라이언트 디바이스에서 렌더링된 제안으로 결정 및/또는 전달(또는 제공)될 수 있다. 달리 말하면, 후보 응답을 결정하는 것 및/또는 후보 응답을 렌더링하는 것은 음성 입력을 검출하고 음성 입력이 주어진 사용자에 추가된 추가 참가자(및 선택적으로 추가 참가자가 클라이언트 디바이스에 알려진 인식된 사용자)로부터의 것이라고 식별하는 것에 응답하여 발생할 수 있다. 상기 구현예 중 일부 버전에서, 화자 식별은 주어진 사용자의 음성 입력과 주어진 사용자와의 대화에 참여하는 추가 참가자(들)의 음성 입력을 구별하기 위해 수행되거나 활용될 수 있다(선택적으로 추가 참가자들 사이에서 구별하기 위해). 예를 들어, 검출된 음성 입력은 대응하는 화자 임베딩(또는 음성 입력 임베딩, 즉 음성 입력의 임베딩)을 생성하기 위해 화자 식별 모델을 사용하여 프로세싱될 수 있다. 해당 화자 임베딩은 주어진 사용자에 대한 미리 저장된(화자) 임베딩 및/또는 추가 사용자(들)에 대한 미리 저장된(화자) 임베딩(들)과 비교되어, 음성 입력이 주어진 사용자 또는 추가 사용자에 의해 제공되었는지 여부를 결정할 수 있다. 일부 추가 또는 대안 버전에서, 얼굴 인식 기법이 선택적으로 입 움직임 검출 기법과 함께 활용되어, 주어진 사용자의 음성 입력과 주어진 사용자와의 대화에 참여하는 추가 참가자(들)의 음성 입력을 구별할 수 있다. In various implementations, a candidate response determined based on the generated text segment(s) corresponding to the recent voice input of the additional participant(s) is in response to identifying that the voice input was provided by the additional participant other than the given user. It may be determined and/or communicated (or presented) as a rendered offer at a given user's client device. In other words, determining the candidate response and/or rendering the candidate response may include detecting the voice input and receiving the voice input from an additional participant added to the given user (and optionally a recognized user for which the additional participant is known to the client device). may occur in response to identifying that In some versions of the above implementations, speaker identification may be performed or utilized (optionally between additional participants) to differentiate between the voice input of a given user and the voice input of additional participant(s) participating in a conversation with the given user. to distinguish from). For example, the detected speech input may be processed using the speaker identification model to generate a corresponding speaker embedding (or speech input embedding, ie, an embedding of the speech input). That speaker embedding is compared to a pre-stored (speaker) embedding for a given user and/or a pre-stored (speaker) embedding(s) for an additional user(s) to determine whether the voice input was provided by the given user or additional users. can be decided In some additional or alternative versions, facial recognition techniques may optionally be utilized in conjunction with mouth movement detection techniques to differentiate between the voice input of a given user and the voice input of additional participant(s) participating in a conversation with the given user.
주어진 사용자의 것으로 결정되는 경우, 대응하는 인식된 텍스트는 대응하는 합성 오디오 데이터를 생성 및 렌더링하는데 사용될 수 있다. 추가 참가자의 것으로 결정되는 경우, 대응하는 인식된 텍스트는 대응하는 후보 응답(들)을 결정하고 후보 응답(들)을 전달하는 대응하는 제안(들)을 렌더링하는데 사용될 수 있다. 선택적으로, 대응하는 후보 응답(들)은 또한 추가 참가자를 포함하는 과거 대화에서 주어진 사용자를 대신하여 합성 스피치를 생성하는데 사용되는 텍스트 세그먼트(들)에 기초하여 렌더링을 위해 결정 및/또는 선택될 수 있다. 예를 들어, 주어진 사용자를 대신하여, 추가 참가자가 참여한 대화 동안 합성 스피치에 이전에 통합된 텍스트 세그먼트와 일치하는지(부드럽게 또는 정확하게) 결정하는 것에 기초하여 (예를 들어, 대응하는 제안에서) 후보 응답이 렌더링하기 위해 선택될 수 있다. 선택적으로, 주어진 사용자가 언어 장애가 있는 사용자이고 맞춤형 스피치 인식 모델이 주어진 사용자로부터의 음성 입력을 프로세싱하는데 사용되는 경우, 추가 참가자(들)로부터의 최근 음성 입력의 스피치 인식은 장애 스피치의 스피치 인식을 위해 맞춤화되지 않은 추가 스피치 인식 모델을 사용하여 수행될 수 있다. If determined to belong to a given user, the corresponding recognized text may be used to generate and render the corresponding synthetic audio data. If determined to belong to the additional participant, the corresponding recognized text may be used to determine the corresponding candidate response(s) and render the corresponding proposal(s) conveying the candidate response(s). Optionally, corresponding candidate response(s) may also be determined and/or selected for rendering based on text segment(s) used to generate synthetic speech on behalf of a given user in past conversations involving additional participants. have. Candidate responses (e.g., in a corresponding proposal) based on determining (smoothly or accurately) whether, for example, on behalf of a given user, matches (smoothly or accurately) a text segment previously incorporated into synthesized speech during a conversation in which the additional participant participated. This can be selected for rendering. Optionally, if the given user is a user with a speech impairment and a custom speech recognition model is used to process speech input from the given user, speech recognition of recent speech input from additional participant(s) may be used for speech recognition of speech with disabilities. This can be done using an additional non-customized speech recognition model.
본 명세서에 개시된 일부 구현예는 주어진 사용자에 대한 텍스트 세그먼트의 스피치 합성에 사용하기 위한 하나 이상의 운율 프로퍼티들을 동적으로 결정하고, 하나 이상의 결정된 운율 프로퍼티들을 사용하여 합성 스피치를 생성한다. 운율 프로퍼티들은 예를 들어 억양, 톤, 강세, 리듬, 템포 및 일시 중지와 같은 언어적 기능을 포함하여 음절 및 더 큰 스피치 단위의 하나 이상의 프로퍼티들을 포함할 수 있다. 텍스트 세그먼트에 대한 합성 스피치의 하나 이상의 운율 프로퍼티들은 조합하여 예를 들어 감정 상태; 형식(예: 진술, 질문 또는 명령); 반어; 풍자; 및/또는 강조를 반영할 수 있다. 따라서 정보는 음성 합성을 사용하는 접근법에서는 정보가 부족할 수 있는 일반적 스피치의 운율 프로퍼티들로 전달될 수 있고; 합성 스피치의 운율 프로퍼티들의 부족은 언어 장애가 있는 사용자에 대해 좌절 및/또는 고립으로 이어질 수 있다. 하나의 비제한적 예로서, 운율 프로퍼티들의 제1 세트를 사용하면 톤이 더 형식적이고 템포가 느린 합성 스피치가 생성될 수 있는 반면, 운율 프로퍼티들의 제2 세트를 사용하면 톤이 덜 형식적이고 템포가 더 빠른 합성 스피치가 생성될 수 있다. 따라서, 스피치 합성에 사용되는 운율 프로퍼티들은 합성 스피치 자체의 다양한 구성에 영향을 미치지만, 운율 프로퍼티들을 조정하는 것은 단독으로 합성 스피치의 기본 "음성"을 변경하지 않는다. 달리 말하면, 유명 배우 1의 음성을 모방한 합성 스피치의 운율 프로퍼티들을 조정하는 것은(예: 스피치 합성 모델에 유명 배우 1에 대한 음성 임베딩을 적용하거나 유명 배우 1에 대해서만 스피치 합성 모델 트레이닝)은 여전히 유명 배우 1처럼 들리는 합성 스피치를 생성하지만 조정된 운율 프로퍼티들만 포함한다. Some implementations disclosed herein dynamically determine one or more prosody properties for use in speech synthesis of a text segment for a given user, and use the one or more determined prosody properties to generate synthesized speech. Prosody properties may include one or more properties in units of syllables and larger speech units, including, for example, linguistic functions such as intonation, tone, stress, rhythm, tempo and pause. The one or more prosody properties of the synthesized speech for the text segment may be combined to, for example, an emotional state; form (eg, statement, question, or command); irony; satire; and/or emphasizing. Thus information can be conveyed in prosody properties of general speech, which may lack information in approaches using speech synthesis; The lack of prosody properties of synthesized speech can lead to frustration and/or isolation for users with language disabilities. As one non-limiting example, using a first set of prosody properties may produce synthesized speech that is more formal in tone and slower in tempo, while using a second set of prosody properties is less formal in tone and more tempo. Fast synthetic speech can be produced. Thus, while the prosody properties used in speech synthesis affect the various configurations of the synthesized speech itself, adjusting the prosody properties alone does not change the basic "voice" of the synthesized speech. In other words, adjusting the prosody properties of a synthetic speech that mimics the voice of famous actor 1 (e.g., applying voice embeddings for famous actor 1 to a speech synthesis model or training a speech synthesis model for famous actor 1 only) is still popular. Creates a synthetic speech that sounds like actor 1, but only contains adjusted prosody properties.
결정된 운율 프로퍼티들을 가진 합성 스피치를 생성하기 위해 다양한 기법이 사용될 수 있다. 일 예시로서, 초기에 생성된 합성 스피치 오디오 데이터의 후처리는 하나 이상의 운율 프로퍼티들을 통합하기 위해 수행될 수 있다. 예를 들어, 초기에 생성된 오디오 데이터의 주파수, 지속시간, 강도 및/또는 스펙트럼 특성은 특정 운율 프로퍼티들을 달성하기 위해 특정 방식으로 조정되거나 특정 대체 운율 프로퍼티들을 달성하기 위해 대체 방식으로 조정될 수 있다. 다른 예로서, 운율 프로퍼티들의 표시는 표시된 운율 프로퍼티들을 통합하는 초기 합성 스피치 오디오 데이터를 생성하기 위해 스피치 합성 모델을 사용하여 프로세싱될 수 있다. 다시 말해, 스피치 합성 모델은 텍스트 세그먼트의 음소(들) 및 선택적으로 음성 임베딩과 함께 운율 프로퍼티들의 표시를 프로세싱하고, 프로세싱되는 표시된 운율 프로퍼티들에 의존하는 합성 스피치 오디오 데이터를 생성하도록 트레이닝될 수 있다. 따라서, 음성 합성이 개선될 수 있으며, 이는 언어 장애가 있는 사용자에게 특히 유용할 수 있다. Various techniques may be used to generate synthetic speech with determined prosody properties. As an example, post-processing of the initially generated synthetic speech audio data may be performed to incorporate one or more prosody properties. For example, the frequency, duration, intensity, and/or spectral characteristics of the initially generated audio data may be adjusted in a particular manner to achieve particular prosody properties or may be adjusted in an alternative manner to achieve particular alternative prosody properties. As another example, the indication of prosody properties can be processed using the speech synthesis model to generate initial synthesized speech audio data incorporating the indicated prosody properties. In other words, the speech synthesis model may be trained to process the representation of prosody properties along with the phoneme(s) of the text segment and optionally the speech embedding, and generate synthetic speech audio data dependent on the displayed prosody properties being processed. Thus, speech synthesis may be improved, which may be particularly useful for users with language impairments.
하나 이상의 운율 프로퍼티들이 동적으로 결정되는 구현예에서, 어떤 운율 프로퍼티들이 대화에서 텍스트 세그먼트에 사용되는지는 예를 들어 텍스트 세그먼트(들) 자체 및/또는 추가 참가자(들)의 최근 음성 입력에 대응하여 생성된 텍스트 세그먼트(들), 대화에서 주어진 사용자와 추가 참가자(들) 간의 관계(들)의 속성(들) 및/또는 분류(들) 또는 대화의 현재 위치의 기타 피처(들)에 기초할 수 있다. In embodiments in which one or more prosody properties are determined dynamically, which prosody properties are used for a text segment in a conversation is generated, for example, in response to recent voice input of the text segment(s) itself and/or additional participant(s). may be based on the given text segment(s), the attribute(s) and/or classification(s) of the relationship(s) between a given user and additional participant(s) in the conversation, or other feature(s) of the current location of the conversation. .
일 예시로서, 주어진 사용자와 한 명의 추가 참가자 간의 대화를 가정한다. 추가 참가자는 화자 식별(예: 화자를 식별하기 위해 추가 참가자의 음성 입력 프로세싱에 기초함) 및/또는 얼굴 식별(예: 프로세싱 이미지(들) 및/또는 추가 참가자의 기타 비전 데이터에 기초함)과 같은 하나 이상의 기법들을 사용하여 식별될 수 있다. 예를 들어, 추가 참가자는 음성 입력에 대해 생성된 화자 임베딩이 임의의 저장된 화자 임베딩에 대해 충분히 일치하는데 실패함(예를 들어, 거리 임계값에 도달하거나 만족하지 못함)을 나타내는 화자 식별 및/또는 캡처된 이미지(들)에 대한 생성된 시각 임베딩이 임의의 저장된 시각 임베딩과 충분히 일치하지 않음을 표시하는 얼굴 식별에 기초하여 "알 수 없음" 사용자로 식별될 수 있다. 추가 참가자를 알 수 없는 경우, 주어진 사용자와 추가 참가자 간의 관계에 "알 수 없음" 속성이 운율 프로퍼티들의 제1 세트(예: "알 수 없음" 속성을 갖는 관계에 명시적으로 매핑된 것)를 선택하는데 사용될 수 있다. 예를 들어, 운율 프로퍼티들의 제1 세트는 톤이 형식적이고 상대적으로 느린 템포를 갖는 합성 스피치를 생성할 수 있다.As an example, assume a conversation between a given user and one additional participant. The additional participant may include speaker identification (eg, based on processing of the additional participant's speech input to identify the speaker) and/or face identification (eg, based on processing image(s) and/or other vision data of the additional participant); The same may be identified using one or more techniques. For example, the additional participant may identify a speaker and/or indicate that speaker embeddings generated for speech input fail to sufficiently match any stored speaker embeddings (e.g., do not reach or satisfy a distance threshold) and/or An “unknown” user may be identified based on face identification indicating that the generated visual embeddings for the captured image(s) do not sufficiently match any stored visual embeddings. If the additional participant is unknown, add a first set of prosody properties to the relationship between the given user and the additional participant in which the "unknown" attribute (e.g., explicitly mapped to a relationship with the "unknown" attribute) can be used to select For example, the first set of prosody properties may produce synthetic speech in which the tone is formal and has a relatively slow tempo.
또한, 예를 들어, 추가 참가자는 음성 입력에 대해 생성된 화자 임베딩이 추가 참가자에 대한 저장된 화자 임베딩(예를 들어, 추가 사용자로부터의 명시적 허가로 클라이언트 디바이스(들)에 로컬적으로 저장됨)과 충분히 일치한다고 표시하는 화자 식별에 기초하여 특정한 알려진 사용자로서 식별될 수 있다. 다른 속성(들)은 예를 들어, 주어진 사용자와 추가 참가자 간의 인터렉션(예: 대화 인터렉션)의 양, 주어진 사용자와 추가 참가자 간의 인터렉션 빈도, 및/또는 주어진 사용자와 추가 참가자 간의 관계의 시맨틱 표시(들)(예를 들어, 상사, 친구, 친한 친구, 동료, 가족 구성원 및/또는 기타 시맨틱 표시(들)과 같은 시맨틱 표시(들))를 포함할 수 있다. 이러한 관계에 기초하여 결정된 운율 프로퍼티들을 사용하면 언어 장애가 있는 사용자에게 정상적인 언어로 가능한 감정과 뉘앙스의 범위를 전달할 수 있는 능력과 관계의 친밀도에 따라 운율 프로퍼티들을 조정할 수 있는 능력을 제공할 수 있어서, 통신 품질이 개선될 수 있다.Also, for example, an additional participant may be aware that speaker embeddings generated for voice input are stored speaker embeddings for the additional participant (eg, stored locally on the client device(s) with explicit permission from the additional user). may be identified as a particular known user based on the speaker identification indicating a sufficient match with The other attribute(s) may be, for example, the amount of interactions (eg, conversational interactions) between the given user and the additional participant, the frequency of interactions between the given user and the additional participant, and/or the semantic indication(s) of the relationship between the given user and the additional participant. ) (eg, semantic indication(s) such as boss, friend, close friend, colleague, family member, and/or other semantic indication(s)). Using prosody properties determined on the basis of such a relationship can provide a user with a language impairment the ability to convey a range of possible emotions and nuances in normal language and the ability to adjust prosody properties according to the intimacy of the relationship. Quality can be improved.
속성(들)의 다른 그룹화는 하나 이상의 운율 프로퍼티들의 다른 그룹화에 매핑될 수 있다. 예를 들어, 상사 속성은 운율 프로퍼티들의 제2 세트에 매핑되어 톤이 보다 형식적이고 풍자를 피하는 합성 스피치를 생성할 수 있다. 또한, 예를 들어, 친한 친구 속성 및/또는 빈번한 인터렉션 속성은 운율 프로퍼티들의 제3 세트에 매핑되어 톤이 일상적이고 비교적 빠른 템포를 갖는 합성 스피치를 생성할 수 있다. 보다 빠른 템포는 합성 스피치를 더 빨리 렌더링할 수 있게 하고, 대화를 더 빨리 마무리할 수 있다. 보다 빠른 템포는 잠재적으로 합성 스피치를 이해하기 어렵게 만들 수 있지만 친한 친구 및/또는 빈번한 인터렉션 속성(들)을 결정하는데 응답하여 보다 빠른 템포를 활용하면 친한 친구 및/또는 빈번한 인터렉션 속성(들)이 주어진 사용자의 합성 스피치의 친밀도를 표시한다는 관점에서 이 위험을 완화할 수 있다. 이러한 방식 및 기타 방식으로, 합성 스피치의 더 빠른 렌더링(및 연관된 계산 리소스의 더 짧은 사용 지속시간)으로 이어지는 운율 프로퍼티(들)은 선택적으로 활용될 수 있으며, 합성 스피치이 이해되지 않으면 렌더링을 반복해야 할 필요성의 위험을 표시하는 관계 속성(들)로 그러한 효율성들을 밸런싱한다. Different groupings of attribute(s) may be mapped to different groupings of one or more prosody properties. For example, a similarity attribute may be mapped to a second set of prosody properties to produce synthetic speech that is more formal in tone and avoids satire. Also, for example, the close friend attribute and/or the frequent interaction attribute may be mapped to a third set of prosody properties to produce synthetic speech in which the tone is normal and has a relatively fast tempo. A faster tempo allows synthetic speech to be rendered faster, and dialogue can be finished faster. A faster tempo can potentially make synthesized speech difficult to understand, but utilizing a faster tempo in response to determining the close friend and/or frequent interaction attribute(s) is given by the close friend and/or frequent interaction attribute(s). This risk can be mitigated in terms of indicating the familiarity of the user's synthesized speech. In this and other ways, prosody property(s) that lead to faster rendering of the synthesized speech (and shorter duration of use of the associated computational resources) can optionally be exploited, and rendering must be repeated if the synthesized speech is not understood. Balance those efficiencies with the relationship attribute(s) that indicate the risk of necessity.
더 일반적으로, 관계(들)의 속성(들), 대화를 위한 현재 위치의 피처(들), 합성될 텍스트 세그먼트(및/또는 최근 텍스트 세그먼트) 및/또는 다른 팩터(들)에 따라 운율 프로퍼티들을 동적으로 결정하는 것은 팩터(들)의 관점에서 결정되는 결정된 운율 프로퍼티들을 통합하는 합성 스피치를 생성할 수 있게 한다. 그 결과, 합성 스피치가 렌더링될 때, 팩터(들)의 관점에서 특별히 조정되기 때문에 대화의 추가 참가자(들)가 더 쉽게 이해할 수 있다. 이것은 추가 참가자(들)에 의한 합성 스피치의 이해 부족의 발생을 완화할 수 있으며, 이는 합성 스피치(또는 텍스트 세그먼트의 재구성을 위한 대체 합성 스피치)가 다시 렌더링되는 결과를 초래할 수 있다. 따라서, 합성 스피치 또는 대체 합성 스피치를 다시 렌더링할 때 리소스(들)의 활용이 완화될 수 있다. 또한, 합성 스피치의 추가 렌더링 발생을 완화함으로써 대화의 전체 지속시간을 단축할 수 있으며, 이에 따라 더 긴 대화를 용이하게 하는데 활용될 다양한 계산 리소스를 절약할 수 있다. More generally, set prosody properties according to the attribute(s) of the relationship(s), the feature(s) of the current location for the conversation, the text segment to be synthesized (and/or the most recent text segment) and/or other factor(s). Determining dynamically makes it possible to generate synthetic speech incorporating the determined prosody properties determined in terms of factor(s). As a result, when the synthesized speech is rendered, it is more easily understood by the additional participant(s) of the conversation because it is specially tuned in terms of the factor(s). This may mitigate the occurrence of a lack of understanding of the synthetic speech by the additional participant(s), which may result in the synthetic speech (or alternative synthetic speech for reconstruction of a text segment) being re-rendered. Accordingly, utilization of the resource(s) may be relaxed when re-rendering the synthesized speech or alternative synthesized speech. In addition, the overall duration of the dialogue can be shortened by alleviating the occurrence of additional rendering of the synthesized speech, thereby saving various computational resources that will be utilized to facilitate longer dialogues.
위에서 언급한 바와 같이, 운율 프로퍼티들은 추가로 및/또는 대안적으로 대화에 대한 현재 위치의 분류(들) 또는 다른 피처(들)에 기초하여 동적으로 결정될 수 있다. 예를 들어, 운율 프로퍼티들의 제1 세트는 집 분류(즉, 주어진 사용자에 대한 집 분류 - 주어진 사용자의 집)를 갖는 위치에서 결정 및 활용될 수 있고, 운율 프로퍼티들의 제2 세트는 직장 분류가 있는 위치(즉, 주어진 사용자에 대한 직장 분류 - 주어진 사용자의 직장 장소)를 갖는 결정 및 활용될 수 있고 운율 프로퍼티들의 제3 세트는 커피숍 분류를 갖는 위치(모든 사용자에게 공통됨)에서 결정 및 활용될 수 있고, 그리고 운율 프로퍼티들의 제4 세트는 바 분류를 갖는 위치(모든 사용자에게 공통됨)에서 결정 및 활용될 수 있다. 이러한 위치 기반 접근 방식은 언어 장애로 인한 의사 소통 문제를 줄이거나 완화함으로써 일상 생활에서 언어 장애가 있는 사용자의 참여를 향상시킬 수 있다.As noted above, prosody properties may additionally and/or alternatively be determined dynamically based on classification(s) or other feature(s) of the current location for the conversation. For example, a first set of prosody properties may be determined and utilized at a location having a house classification (ie, a house class for a given user - a given user's house), and a second set of prosody properties may be determined and utilized at a location having a work class. A third set of prosody properties can be determined and utilized with a location (i.e., a work classification for a given user - a given user's work place) and a third set of prosody properties to be determined and utilized at a location with a coffee shop classification (common to all users). and a fourth set of prosody properties may be determined and utilized at a location with a bar classification (common to all users). This location-based approach could improve the participation of speech-impaired users in their daily lives by reducing or alleviating the communication problems caused by speech impairments.
하나 이상의 운율 프로퍼티들을 결정하기 위해 이용되는 기법에 관계없이, 일부 구현예에서, 음성 입력을 수신함에 응답하여, 운율 프로퍼티들의 결정된 세트가 결정될 수 있고, 결정된 운율 프로퍼티들로 합성되고 수신된 음성 입력에 포함된 텍스트 세그먼트(들)를 포함하는 합성 스피치를 포함하는 합성 스피치 오디오 데이터가 자동으로 합성될 수 있고, 그리고 합성 스피치가 주어진 사용자로부터 음성 입력을 수신함에 응답하여 클라이언트 디바이스에서 청각적으로 렌더링될 수 있다. 일부 다른 구현예에서, 음성 입력을 수신함에 응답하여, 운율 프로퍼티들의 결정된 세트가 결정될 수 있고, 결정된 운율 프로퍼티들로 합성되고 수신된 음성 입력에 포함된 텍스트 세그먼트(들)를 포함하는 합성 스피치를 포함하는 합성 스피치 오디오 데이터가 자동으로 생성될 수 있지만, 타임아웃 기간이 종료될 때까지(예를 들어, 3초, 5초 등) 합성 스피치의 청각적 렌더링이 보류될 수 있다. 이 타임아웃 기간 동안, 주어진 사용자는 운율 프로퍼티들의 결정된 세트를 수동으로 조정할 수 있다. 또 다른 구현예에서, 음성 입력을 수신함에 응답하여, 운율 프로퍼티들의 결정된 세트가 결정될 수 있고, 결정된 운율 프로퍼티들로 합성되고 수신된 음성 입력에 포함된 텍스트 세그먼트(들)를 포함하는 합성 스피치를 포함하는 합성 스피치 오디오 데이터가 자동으로 생성될 수 있지만, 주어진 사용자로부터의 추가 사용자 인터페이스 입력이 클라이언트 디바이스에서 검출될 때까지 합성 스피치의 청각적 렌더링이 보류될 수 있고, 합성 스피치는 운율 프로퍼티들의 결정된 세트로 청각적으로 렌더링되어야 함을 확인한다. 이것은 주어진 사용자가 결정된 운율 프로퍼티들을 수동으로 조정할 수 있게 할 뿐만 아니라 주어진 사용자는 합성 스피치의 렌더링 이전에 합성 스피치에 포함된 결정된 텍스트 세그먼트(들)의 전사를 수정할 수도 있다. Regardless of the technique used to determine the one or more prosody properties, in some implementations, in response to receiving the voice input, a determined set of prosody properties can be determined, synthesized from the determined prosody properties, and added to the received voice input. Synthetic speech audio data comprising synthesized speech comprising the included text segment(s) may be automatically synthesized, and the synthesized speech may be rendered audibly at the client device in response to receiving voice input from a given user. have. In some other implementations, in response to receiving the voice input, a determined set of prosody properties may be determined, comprising synthesized speech synthesized with the determined prosody properties and comprising text segment(s) included in the received voice input. synthesized speech audio data may be automatically generated, but auditory rendering of the synthesized speech may be withheld until the timeout period expires (eg, 3 seconds, 5 seconds, etc.). During this timeout period, a given user may manually adjust the determined set of prosody properties. In yet another implementation, in response to receiving the spoken input, a determined set of prosody properties may be determined, comprising synthesized speech synthesized with the determined prosody properties and comprising text segment(s) included in the received voice input. synthesized speech audio data may be automatically generated, but auditory rendering of the synthesized speech may be withheld until additional user interface input from a given user is detected at the client device, wherein the synthesized speech is converted to a determined set of prosody properties. Make sure it must be rendered audibly. This not only allows a given user to manually adjust the determined prosody properties, but also allows the given user to modify the transcription of the determined text segment(s) included in the synthesized speech prior to rendering of the synthesized speech.
더욱이, 다양한 구현예에서, 추가 사용자 인터페이스 입력(들)은 주어진 사용자와의 대화에 참여하는 추가 참가자(들)의 추가 사용자 음성 입력을 포함할 수 있다(즉, 추가 사용자 인터페이스 입력은 대화에서 추가 참가자(들)로부터의 음성 입력을 포함할 수 있음). 또한, 대응하는 텍스트 세그먼트(들)는 스피치 인식 모델을 사용하여 추가 사용자 음성 입력을 프로세싱한 결과인 인식된 텍스트에 기초하여 결정될 수 있다. 상기 구현예 중 일부에서, 추가 사용자 음성 입력은 스피치 인식 모델을 사용하여 대응하는 텍스트 세그먼트(들)로 프로세싱될 수 있고, 대응하는 텍스트 세그먼트(들)는 주어진 사용자의 클라이언트 디바이스에서 사용자 인터페이스 상의 전사로서 시각적으로 렌더링될 수 있다. 더욱이, 상기 구현예 중 일부 추가 버전에서, 추가 사용자 음성 입력을 말한 추가 참가자(들) 중 주어진 하나를 식별하는 그래픽 엘리먼트는 대응하는 세그먼트(들)의 전사와 함께 시각적으로 렌더링될 수 있다. 그래픽 엘리먼트는 선택가능할 수 있고, 그래픽 엘리먼트에 대한 주어진 사용자로부터 사용자 인터페이스 입력(들)을 수신하면, 클라이언트 디바이스는 운율 프로퍼티 사용자 인터페이스가 클라이언트 디바이스에서 시각적으로 렌더링되게 할 수 있다. 선택 가능한 그래픽 엘리먼트는 언어 장애가 있는 사용자에게 유용할 수 있으며, 이는 이러한 엘리먼트를 선택하면 전사를 선택하기 위해 추가 음성 입력을 오해할 위험이 완화되기 때문이다.Moreover, in various implementations, the additional user interface input(s) may include additional user voice input of the additional participant(s) participating in the conversation with the given user (ie, the additional user interface input is the additional participant(s) in the conversation). may include voice input from (s)). Further, the corresponding text segment(s) may be determined based on recognized text that is the result of processing additional user speech input using a speech recognition model. In some of the above implementations, additional user voice input may be processed into corresponding text segment(s) using a speech recognition model, wherein the corresponding text segment(s) are transcribed on a user interface at a given user's client device. It can be rendered visually. Moreover, in some further versions of the above implementations, a graphical element identifying a given one of the additional participant(s) speaking the additional user voice input may be rendered visually along with the transcription of the corresponding segment(s). The graphical element may be selectable, and upon receiving user interface input(s) from a given user for the graphical element, the client device may cause the prosody property user interface to be visually rendered on the client device. Selectable graphic elements can be useful for users with language impairments, as selecting these elements mitigates the risk of misinterpreting additional voice input to select transcription.
다양한 구현예에서, 운율 프로퍼티 사용자 인터페이스는 클라이언트 디바이스의 주어진 사용자가 그래픽 엘리먼트에 의해 식별된 추가 참가자(들) 중 주어진 한 명에 대한 하나 이상의 결정된 운율 프로퍼티들을 수정할 수 있게 한다. 상기 구현예 중 일부에서, 클라이언트 디바이스의 주어진 사용자는 하나 이상의 결정된 운율 프로퍼티들에 "글로벌" 수정을 수행하기 위해 운율 프로퍼티 사용자 인터페이스와 인터렉션할 수 있다. 예를 들어, 운율 프로퍼티 사용자 인터페이스는 하나 이상의 결정된 운율 프로퍼티들로 합성되는 "형식적" 또는 "일상적" 합성 스피치가 어떻게 될 것인지를 나타내는 스케일을 포함할 수 있으며, 주어진 사용자는 스케일 상의 표시자와 인터렉션하여, 하나 이상의 결정된 운율 프로퍼티들을 수정하고, 보다 형식적인 합성 스피치 또는 보다 일상적인 합성 스피치를 반영한다. 상기 구현예 중 일부에서, 클라이언트 디바이스의 주어진 사용자는 결정된 운율 프로퍼티들 중 하나 이상에 "개별" 수정을 하기 위해 운율 프로퍼티 사용자 인터페이스와 인터렉션할 수 있다. 예를 들어, 운율 프로퍼티 사용자 인터페이스는 하나 이상의 결정된 운율 프로퍼티들의 목록 및 하나 이상의 결정된 운율 프로퍼티들 각각에 대한 대응 필드를 포함할 수 있고, 사용자는 대응 필드와 인터렉션하여, 결정된 운율 프로퍼티들 중 주어진 하나의 프로퍼티를 수정한다(예: 대응하는 필드인 드롭다운 상자에서 "보통" 또는 "느린" 템포를 선택, 대응하는 필드에서 50% 또는 30% 템포 지정, 결정된 운율 프로퍼티들 중 주어진 하나의 프로퍼티 및/또는 운율 프로퍼티들 중 주어진 하나의 프로퍼티를 수정하기 위한 다른 필드를 활성화 또는 비활성화). 따라서, 추가 참가자(들) 중 주어진 한 명에 대한 하나 이상의 결정된 운율 프로퍼티들은 또한 운율 프로퍼티 사용자 인터페이스와의 인터렉션에 기초하여 클라이언트 디바이스의 주어진 사용자에 의해 수동으로 조정될 수 있다.In various implementations, the prosody properties user interface enables a given user of the client device to modify one or more determined prosody properties for a given one of the additional participant(s) identified by the graphical element. In some of the above implementations, a given user of the client device may interact with a prosody properties user interface to perform a “global” modification to one or more determined prosody properties. For example, the prosody properties user interface may include a scale indicating what "formal" or "routine" synthetic speech will be synthesized with one or more determined prosody properties, a given user interacting with an indicator on the scale to , modify one or more determined prosody properties, and reflect the more formal synthesized speech or the more ordinary synthesized speech. In some of the above implementations, a given user of the client device may interact with a prosody property user interface to make "individual" modifications to one or more of the determined prosody properties. For example, the prosody properties user interface may include a list of one or more determined prosody properties and a corresponding field for each of the one or more determined prosody properties, wherein the user interacts with the corresponding field to select a given one of the determined prosody properties. Modify a property (e.g. select a "normal" or "slow" tempo from the drop-down box in the corresponding field, specify a 50% or 30% tempo in the corresponding field, a property given one of the determined prosody properties and/or Enables or disables another field for modifying a given one of the prosody properties). Accordingly, one or more determined prosody properties for a given one of the additional participant(s) may also be manually adjusted by a given user of the client device based on interaction with the prosody properties user interface.
상기 설명은 본 명세서에 개시된 단지 일부 구현예의 개요로서 제공된다. 이러한 구현예 및 기타 구현예에 대한 추가 설명은 본 명세서에서 더 자세히 설명된다.The above description is provided as an overview of only some implementations disclosed herein. Additional descriptions of these and other embodiments are set forth in greater detail herein.
본 명세서에 개시된 기법은 클라이언트 디바이스에서 로컬로, 하나 이상의 네트워크를 통해 클라이언트 디바이스에 연결된 서버(들)에 의해 원격으로 및/또는 둘 모두를 통해 구현될 수 있음을 이해해야 한다. It should be understood that the techniques disclosed herein may be implemented locally at a client device, remotely by server(s) coupled to the client device via one or more networks, and/or via both.
다양한 구현예는 프로세서에 의해 실행가능한 명령어들을 저장하는 비일시적 컴퓨터 판독가능 저장 매체를 포함할 수 있으며, 상기 명령어들은 본 명세서에 기술된 하나 이상의 방법들과 같은 방법을 수행한다. 또 다른 다양한 구현예는 메모리 및 본 명세서에 기술된 방법들 중 하나 이상과 같은 방법을 수행하기 위한 상기 메모리에 저장된 명령어들을 실행하기 위해 동작가능한 하나 이상의 하드웨어 프로세서들을 포함하는 시스템을 포함할 수 있다.Various implementations may include a non-transitory computer-readable storage medium storing instructions executable by a processor, the instructions performing a method such as one or more methods described herein. Still other various implementations may include a system comprising a memory and one or more hardware processors operable to execute instructions stored in the memory for performing a method, such as one or more of the methods described herein.
도 1은 본 개시의 다양한 양태를 시연하고 본 명세서에 개시된 구현예가 구현될 수 있는 예시적 환경의 블록도를 도시한다.
도 2a 및 도 2b는 다양한 구현예에 따라 하나 이상의 운율 프로퍼티들로 합성되는 합성 스피치를 생성하는 예시적 방법을 도시하는 흐름도를 도시한다.
도 3은 다양한 구현예에 따라 본 개시의 다양한 양태를 보여주는 예시적 환경에서의 시나리오를 도시한다.
도 4a, 4b, 4c, 4d, 4e 및 4f는 다양한 구현예에 따라 주어진 사용자에 대한 스피치 임베딩을 설정하고 하나 이상의 운율 프로퍼티들로 합성되는 합성 스피치를 생성하는데 활용되는 사용자 인터페이스들의 다양한 비제한적인 예들을 도시한다.
도 5는 다양한 구현예에 따른 컴퓨팅 디바이스의 예시적 아키텍처를 도시한다.1 illustrates a block diagram of an exemplary environment in which implementations disclosed herein may be implemented and demonstrating various aspects of the present disclosure;
2A and 2B show flow diagrams illustrating an example method of generating synthesized speech that is synthesized with one or more prosody properties in accordance with various implementations.
3 depicts a scenario in an example environment illustrating various aspects of the present disclosure in accordance with various implementations.
4A, 4B, 4C, 4D, 4E, and 4F are various non-limiting examples of user interfaces utilized to establish a speech embedding for a given user and generate synthetic speech synthesized with one or more prosody properties in accordance with various implementations; show them
5 depicts an example architecture of a computing device in accordance with various implementations.
도 1은 본 개시의 다양한 양태를 시연하는 예시적 환경의 블록도를 도시한다. 클라이언트 디바이스(110)가 도 1에 도시되며, 다양한 구현예에서, 사용자 인터페이스 입력 엔진(111), 식별 엔진(112), 스피치 인식 엔진(120A1), 스피치 합성 엔진(130A1) 및 렌더링 엔진(113)을 포함한다. 1 depicts a block diagram of an example environment demonstrating various aspects of the present disclosure. A
사용자 인터페이스 입력 엔진(111)은 클라이언트 디바이스(110)에서 사용자 인터페이스 입력을 검출할 수 있다. 클라이언트 디바이스(110)에서 검출된 사용자 인터페이스 입력은 클라이언트 디바이스(110)의 하나 이상의 마이크로폰을 통해 검출된 음성 입력 및/또는 클라이언트 디바이스(110)의 사용자 인터페이스를 통해 검출된 터치 입력을 포함할 수 있다. 본 명세서에 설명된 바와 같이, 클라이언트 디바이스(110)는 검출된 사용자 인터페이스 입력을 프로세싱하여 대화에서 전달될 검출된 사용자 인터페이스 입력에 대응하는 텍스트 세그먼트를 결정할 수 있다. 예를 들어, 사용자 인터페이스 입력 엔진(111)은 클라이언트 디바이스(110)의 주어진 사용자의 음성 입력 및/또는 클라이언트 디바이스(110)의 주어진 사용자와 대화에 참여하는 추가 참가자(들)의 음성 입력을 검출할 수 있다(즉, 추가 참가자 음성 입력을 검출할 수 있음). 클라이언트 디바이스(110)는 하나 이상의 스피치 인식 모델(들)(120A)을 사용하여, 주어진 사용자 및/또는 추가 참가자(들)의 음성 입력에 대응하는 텍스트 세그먼트를 결정할 수 있다(즉, 추가 참가자 텍스트 세그먼트를 결정할 수 있음). 다른 예로서, 사용자 인터페이스 입력 엔진(111)은 합성 스피치에 통합될(예를 들어, 도 1의 스피치 합성 모델(들)(130A)을 사용하여 생성됨) 후보 텍스트 세그먼트(예를 들어, 도 1의 자동 제안 엔진(150)을 사용하여 결정됨)의 선택을 검출할 수 있다. The user
일부 구현예에서, 식별 엔진(112)은 음성 입력을 캡처하는 오디오 데이터에 기초하여 음성 입력을 말한 사용자를 식별할 수 있다(예를 들어, 클라이언트 디바이스(110)의 주어진 사용자 또는 주어진 사용자와의 대화에 참여하는 추가 참가자(들)). 식별 엔진(112)은 화자 식별 모델을 사용하여 출력을 생성하기 위해 사용자 인터페이스 입력 엔진(111)에 의해 검출된 음성 입력을 캡처하는 오디오 데이터를 프로세싱할 수 있다. 또한, 식별 엔진(112)은 생성된 출력에 기초하여 화자 임베딩을 생성할 수 있다(예를 들어, 출력은 화자 임베딩일 수 있음). 생성된 화자 임베딩은 저차원 화자 임베딩 공간, 보다 구체적으로 화자 임베딩 공간의 특정 부분에서의 (오디오 데이터에 대한) 저차원 표현일 수 있다. 화자 임베딩 공간의 특정 부분은 화자 임베딩 공간의 특정 부분에 매핑된 생성된 화자 임베딩과 연관된 사용자의 식별자와 연관될 수 있다. 또한, 식별 엔진(112)은 생성된 화자 임베딩을 화자 임베딩(들) 데이터베이스(112A)에서 클라이언트 디바이스(110)에 로컬적으로 저장된 복수의 화자 임베딩들과 비교하여 화자 임베딩 공간에 일치하는 화자 임베딩을 식별할 수 있다. 예를 들어, 생성된 화자 임베딩과 복수의 화자 임베딩들 중 주어진 하나의 임베딩 간의 거리 메트릭이 화자 임베딩 공간에서 임계값(예: 일치하는 화자 임베딩)을 충족하는 경우, 일치하는 화자 임베딩과 연관된 식별자와 연관된 사용자는 음성 발화를 말한 사용자로 식별될 수 있다. 예를 들어, 추가 참가자 "Bob"에 대한 화자 임베딩은 Bob이 이전에 화자 임베딩 공간의 제1 부분과 연관되어 있음에 기초하여 "Bob"의 식별자와 연관된 화자 임베딩 공간의 제1 부분에 매핑될 수 있고(예를 들어, 클라이언트 디바이스(110)에서 검출된 사용자 인터페이스 입력에 기초하여), 추가 참가자 "Jane"에 대한 화자 임베딩은 Jane이 이전에 화자 임베딩 공간의 제2 부분과 연관되어 있음에 기초하여 "Jane"의 식별자와 연관된 화자 임베딩 공간의 제2 부분에 매핑될 수 있다(예를 들어, 클라이언트 디바이스(110)에서 검출된 사용자 인터페이스 입력에 기초하여). 따라서, 후속적으로 생성된 화자 임베딩은 대화에서 사용자(들)를 식별하기 위해 화자 임베딩 공간의 이러한 부분 또는 다른 부분에 매핑될 수 있다.In some implementations, the
사용자를 위한 화자 임베딩은 사용자로부터의 음성 입력을 캡처하는 오디오 데이터의 하나 이상의 인스턴스의 프로세싱에 기초하여 생성된 화자 임베딩(들)에 기초하여 생성될 수 있다. 예를 들어, 화자 임베딩은 사용자로부터 대응하는 음성 입력을 캡처하는 오디오 데이터의 대응하는 인스턴스의 프로세싱에 기초하여 각각 생성되는 복수의 상이한 임베딩들의 평균(또는 다른 조합)에 기초할 수 있다. 일단 설정되면, 사용자의 화자 임베딩은 화자 임베딩(들) 데이터베이스(112A)에 저장될 수 있다. 상기 구현예 중 일부 버전에서, 클라이언트 디바이스(110)의 주어진 사용자는 클라이언트 디바이스(110)의 주어진 사용자에 대한 화자 임베딩을 설정하기 위해 클라이언트 디바이스에서 적어도 부분적으로 실행되는 자동화된 어시스턴트와 인터렉션할 수 있고(예를 들어, 도 4a와 관련하여 설명됨), 주어진 사용자에 대해 설정된 화자 임베딩은 화자(들) 임베딩(들) 데이터베이스(112A)에 주어진 사용자와 연관되어 저장될 수 있다. 클라이언트 디바이스(110)의 주어진 사용자에 대해 설정된 화자 임베딩은 클라이언트 디바이스(110)의 주어진 사용자의 스피치를 나타내는 합성 스피치 오디오 데이터를 생성할 때 스피치 합성 엔진(들)(130A1 및/또는 130A2)에 의해 사용될 수 있다. 상기 구현예 중 일부 버전에서, 음성 입력이 클라이언트 디바이스(110)의 주어진 사용자로부터 발생하지 않는다고 결정하면(예를 들어, 화자 임베딩(들) 데이터베이스(112A)에 로컬적으로 저장된 임의의 화자 임베딩과 일치하지 않는 음성 입력에 대해 생성된 화자 임베딩에 기초하여), 클라이언트 디바이스(110)는 클라이언트 디바이스(110) 상에 로컬적으로 생성된 화자 임베딩을 저장하기 위해 추가 참가자로부터 승인을 요청하는 프롬프트를 생성할 수 있다(예를 들어, 도 4d와 관련하여 설명됨). 또한, 생성된 화자 임베딩은 생성된 화자 임베딩을 저장하기 위해 추가 참가자로부터 승인을 수신하는 것에 응답하여 화자 임베딩(들) 데이터베이스(112A)에 추가 참가자와 연관되어 저장될 수 있다. 그렇지 않으면, 생성된 화자 임베딩은 클라이언트 디바이스(110)에 의해 폐기될 수 있다. 화자 임베딩(들) 데이터베이스(112A)에 클라이언트 디바이스(110) 상에 로컬적으로 화자 임베딩을 저장함으로써, 화자 임베딩 공간은 추가 참가자(들)로 채워질 수 있어서, 클라이언트 디바이스(110)는 추가 참가자(들)가 추가 참가자(들)의 대응하는 화자 임베딩(들)을 사용하여 클라이언트 디바이스(110)의 주어진 사용자와 미래의 대화에 참여하는 경우 추가 참가자(들)를 식별할 수 있도록 한다. The speaker embeddings for the user may be generated based on the speaker embedding(s) generated based on processing of one or more instances of audio data that captures speech input from the user. For example, a speaker embedding may be based on an average (or other combination) of a plurality of different embeddings, each generated based on processing of a corresponding instance of audio data that captures a corresponding speech input from a user. Once established, the user's speaker embeddings may be stored in the speaker embedding(s)
일부 추가적인 및/또는 대안적인 구현예에서, 식별 엔진(112)은 클라이언트 디바이스(110)의 환경에 있는 다수의 사람들(예를 들어, 클라이언트 디바이스(110)의 주어진 사용자 및/또는 주어진 사용자와의 대화에 참여하는 추가 참가자(들))을 포함하는 비전 데이터(예를 들어, 클라이언트 디바이스(110)의 하나 이상의 비전 센서에 의해 캡처된 이미지(들))에 기초하여 대화에 참여한 사용자(들)를 식별할 수 있다. 클라이언트 디바이스의 환경 또는 클라이언트 디바이스의 주변은 임의의 공간 또는 지리적 영역(실내 또는 실외)을 포함할 수 있다; 다른 참가자는 클라이언트 디바이스가 들을 수 있도록 및/또는 클라이언트 디바이스로부터의 가청 출력을 검출하기에 충분히 클라이언트 디바이스에 가까이 있는 경우 환경에 있는 것으로 간주될 수 있다. 상기 구현예 중 일부 버전에서, 식별 엔진(112)은 비전 데이터로부터 클라이언트 디바이스(110)의 주어진 사용자 및/또는 대화의 적어도 하나의 추가 참가자를 자동으로 식별하기 위해 하나 이상의 얼굴 검출 기법을 활용할 수 있다. 상기 구현예 중 일부 다른 버전에서, 사용자 인터페이스 입력 엔진(111)은 대화에 적어도 한 명의 추가 참가자를 포함하는 캡처된 이미지(및 이와 관련된 비전 데이터)의 영역을 지정하는 클라이언트 디바이스(110)의 주어진 사용자로부터의 입력(예를 들어, 터치 또는 음성)을 검출할 수 있다. 식별 엔진(112)은 시각적 식별 모델을 사용하여 비전 데이터를 프로세싱하여 출력을 생성할 수 있다. 또한, 식별 엔진(112)은 생성된 출력에 기초하여 비주얼 임베딩을 생성할 수 있다(예를 들어, 출력은 비주얼 임베딩일 수 있음). 생성된 비주얼 임베딩은 저차원 비주얼 임베딩 공간, 보다 구체적으로 화자 임베딩 공간의 특정 부분에서의 (비전 데이터에 대한) 저차원 표현일 수 있다. 임베딩 공간의 특정 부분은 화자 임베딩 공간의 특정 부분에 매핑된 생성된 화자 임베딩과 연관된 사용자의 식별자와 연관될 수 있다. 또한, 식별 엔진(112)은 생성된 비주얼 임베딩을 비주얼 임베딩(들) 데이터베이스(112B)에서 클라이언트 디바이스(110)에 로컬적으로 저장된 복수의 비주얼 임베딩들과 비교하여 비주얼 임베딩 공간에 일치하는 비주얼 임베딩을 식별할 수 있다. 예를 들어, 생성된 비주얼 임베딩과 복수의 비주얼 임베딩들 중 주어진 하나의 임베딩 간의 거리 메트릭이 비주얼 임베딩 공간에서 임계값(예: 일치하는 비주얼 임베딩)을 충족하는 경우, 일치하는 비주얼 임베딩과 연관된 식별자와 연관된 사용자(클라이언트 디바이스(110)의 주어진 사용자 및/또는 대화의 적어도 하나의 추가 참가자)는 대화의 참가자로서 식별될 수 있다. 예를 들어, 추가 참가자 "Bob"에 대한 비주얼 임베딩은 Bob이 이전에 비주얼 임베딩 공간의 제1 부분과 연관되어 있음에 기초하여 "Bob"의 식별자와 연관된 비주얼 임베딩 공간의 제1 부분에 매핑될 수 있고(예를 들어, 클라이언트 디바이스(110)에서 검출된 사용자 인터페이스 입력에 기초하여), 추가 참가자 "Jane"에 대한 비주얼 임베딩은 Jane이 이전에 비주얼 임베딩 공간의 제2 부분과 연관되어 있음에 기초하여 "Jane"의 식별자와 연관된 비주얼 임베딩 공간의 제2 부분에 매핑될 수 있다(예를 들어, 클라이언트 디바이스(110)에서 검출된 사용자 인터페이스 입력에 기초하여). 따라서, 후속적으로 생성된 비주얼 임베딩은 대화에서 사용자(들)를 식별하기 위해 비주얼 임베딩 공간의 이러한 부분 또는 다른 부분에 매핑될 수 있다.In some additional and/or alternative implementations, the
사용자를 위한 비주얼 임베딩은 클라이언트 디바이스(110)에 의해 캡처된 하나 이상의 이미지들의 프로세싱에 기초하여 생성된 비주얼 임베딩(들)에 기초하여 생성될 수 있다. 예를 들어, 비주얼 임베딩은 적어도 사용자(예를 들어, 클라이언트 디바이스(110)의 주어진 사용자 및/또는 대화의 적어도 하나의 추가 참가자)를 캡처하는 대응하는 비전 데이터의 프로세싱에 기초하여 각각 생성되는 복수의 상이한 비주얼 임베딩들의 평균(또는 다른 조합)에 기초할 수 있다. 일단 설정되면, 사용자의 비주얼 임베딩은 비주얼 임베딩(들) 데이터베이스(112B)에 저장될 수 있다. 일부 구현예에서, 클라이언트 디바이스(110)의 주어진 사용자는 클라이언트 디바이스(110)의 주어진 사용자에 대한 비주얼 임베딩을 설정하기 위해 클라이언트 디바이스에서 적어도 부분적으로 실행되는 자동화된 어시스턴트와 인터렉션할 수 있고(예를 들어, 도 4a와 관련하여 설명됨), 주어진 사용자에 대해 설정된 비주얼 임베딩은 비주얼 임베딩(들) 데이터베이스(112B)에 주어진 사용자와 연관되어 저장될 수 있다. 일부 구현예에서, 클라이언트 디바이스(110)의 환경에서 클라이언트 디바이스(110)의 주어진 사용자와의 대화에 알 수 없는 사용자가 참여하고 있다고 결정하면(예를 들어, 음성 입력에 대한 생성된 화자 임베딩이 화자 임베딩(들) 데이터베이스(112A)에 로컬적으로 저장된 임의의 화자 임베딩과 일치하는 않는 것에 기초 및/또는 생성된 비주얼 임베딩이 비주얼 임베딩(들) 데이터베이스(112B)에 로컬적으로 저장된 임의의 비주얼 임베딩과 일치하지 않는 것에 기초함), 클라이언트 디바이스(110)는 클라이언트 디바이스(110)에 로컬적으로 생성된 비주얼 임베딩을 저장하기 위해 알 수 없는 비주얼 임베딩과 연관된 추가 참가자로부터 승인을 요청하는 프롬프트를 생성할 수 있다(도 4d와 관련하여 설명됨). 또한, 생성된 비주얼 임베딩은 생성된 비주얼 임베딩을 저장하기 위해 추가 참가자로부터 승인을 수신하는 것에 응답하여 비주얼 임베딩(들) 데이터베이스(112B)에 추가 참가자와 연관되어 저장될 수 있다. 그렇지 않으면, 생성된 비주얼 임베딩은 클라이언트 디바이스(110)에 의해 폐기될 수 있다. 비주얼 임베딩(들) 데이터베이스(112B)에 클라이언트 디바이스(110) 상에 로컬적으로 비주얼 임베딩을 저장함으로써, 클라이언트 디바이스(110)는 추가 참가자(들)가 추가 참가자(들)의 대응하는 비주얼 임베딩(들)을 사용하여 클라이언트 디바이스(110)의 주어진 사용자와 미래의 대화에 참여하는 경우 추가 참가자(들)를 식별할 수 있다.The visual embedding for the user may be generated based on the visual embedding(s) generated based on processing of one or more images captured by the
일부 구현예에서, 사용자(예를 들어, 클라이언트 디바이스(110)의 주어진 사용자 및/또는 주어진 사용자와 대화에 참여하는 추가 참가자)의 화자 임베딩은 화자 임베딩 공간 및 비주얼 임베딩 공간에 걸쳐 사용자의 대응하는 비주얼 임베딩에 매핑될 수 있다. 따라서, 사용자의 화자 임베딩이 식별되면, 사용자를 포함하는 비전 데이터가 클라이언트 디바이스(110)에 의해 캡처되지 않더라도 사용자의 대응하는 비주얼 임베딩도 식별될 수 있다. 또한, 사용자의 비주얼 임베딩이 식별되면, 사용자의 음성 입력을 포함하는 오디오 데이터가 클라이언트 디바이스(110)에 의해 검출되지 않더라도 사용자의 대응하는 화자 임베딩도 식별될 수 있다. 예를 들어, 추가 참가자 "Bob"에 대한 화자 임베딩은 Bob이 이전에 화자 임베딩 공간의 제1 부분과 연관되어 있음에 기초하여 "Bob"의 식별자와 연관된 화자 임베딩 공간의 제1 부분에 매핑될 수 있고(예를 들어, 클라이언트 디바이스(110)에서 검출된 사용자 인터페이스 입력에 기초하여), 예를 들어, "Bob"에 대한 비주얼 임베딩은 Bob이 이전에 화자 임베딩 공간의 제1 부분과 연관되어 있음에 기초하여 "Bob"의 식별자와 연관된 화자 임베딩 공간의 제1 부분에 매핑될 수 있다(예를 들어, 클라이언트 디바이스(110)에서 검출된 사용자 인터페이스 입력에 기초하여). 화자 임베딩 공간의 제1 부분은 둘 모두 식별자 Bob과 연관되는 것에 기초하여 비주얼 임베딩 공간의 제1 부분에 매핑될 수 있다. 사용자의 화자 임베딩을 사용자의 대응하는 비주얼 임베딩에 매핑함으로써, 사용자의 화자 임베딩 및 사용자의 비주얼 임베딩 둘 모두가 식별 엔진(112)에 의해 식별되는 경우 식별될 수 있다.In some implementations, speaker embeddings of a user (eg, a given user of the
더욱이, 일부 구현예에서, 식별 엔진(112)은 오디오 데이터에 캡처된 음성 입력을 말한 사람을 식별하기 위해 적어도 하나의 사용자(예를 들어, 클라이언트 디바이스(110)의 주어진 사용자 및/또는 주어진 사용자와 대화에 참여하는 추가 참가자)를 캡처하는 비전 데이터와 함께 음성 입력을 캡처하는 오디오 데이터를 활용할 수 있다. 클라이언트 디바이스(110)는 사용자의 음성 입력을 캡처하는 오디오 데이터를 프로세싱하는 동시에 적어도 하나의 사용자를 캡처하는 비전 데이터를 또한 프로세싱할 수 있고, 오디오 데이터 및 비전 데이터의 프로세싱에 기초하여 적어도 하나의 사용자가 오디오 데이터를 말했다고 결정할 수 있다. 예를 들어, 클라이언트 디바이스(110)의 주어진 사용자가 3명의 추가 참가자와 대화에 참여하고 있고, 클라이언트 디바이스(110)가 음성 입력을 포함하는 오디오 데이터를 캡처하는 경우, 클라이언트 디바이스(110)는 또한 주어진 사용자 및 3명의 추가 참가자를 포함하는 비전 데이터를 캡처할 수 있다. 또한, 식별 엔진(112)은 3명의 추가 참가자 중 주어진 한 사람의 입 움직임을 식별하고, 오디오 데이터가 캡처되었던 시간과 중첩하는 입 움직임의 인스턴스에 기초하여, 3명의 추가 참가자 중 주어진 한 사람으로부터의 음성 입력을 포함한다고 결정할 수 있다. 이러한 구현예는 클라이언트 디바이스(110)의 주어진 사용자가 복수의 추가 참가자들과 대화에 참여할 때 유리할 수 있다. Moreover, in some implementations, the
일부 추가 및/또는 대안적 구현예에서, 식별 엔진(112)은 대화에 참여하는 사용자(들)의 신체 자세 및/또는 머리 자세를 결정하기 위해 대화에 참여하는 사용자(들)(예를 들어, 클라이언트 디바이스(110)의 주어진 사용자 및/또는 주어진 대화에 참여하는 추가 참가자)를 캡처하는 비전 데이터를 프로세싱할 수 있다. 또한, 식별 엔진(112)은 사용자(들)의 신체 자세 및/또는 머리 자세에 기초하여, 사용자(들) 중 클라이언트 디바이스(110)에서 검출된 음성 입력 및/또는 음성 입력의 사용자(들)의 의도된 타겟(들)을 제공한 사용자를 결정할 수 있다. 이러한 방식으로, 식별 엔진(112)은 하나 이상의 사용자(들)로부터의 음성 입력이 클라이언트 디바이스(110)에서 검출되지 않더라도 대화에 참여하는 사용자(들)를 식별할 수 있다. 예를 들어, 클라이언트 디바이스(110)의 주어진 사용자가 제1 추가 참가자 " Bob" 및 제2 추가 참가자 "Jane"과 대화에 참여하고 있다고 가정한다. 또한, 식별 엔진(112)은 클라이언트 디바이스에 의해 캡처된 비전 데이터를 프로세싱하고, 비전 데이터에 기초하여, Bob도 Jane도 클라이언트 디바이스(110)의 주어진 사용자 또는 클라이언트 디바이스(110) 자체에 향하지 않더라도, Bob의 신체 자세 및/또는 머리 자세가 클라이언트 디바이스(110)에 의해 검출된 음성 입력이 Bob으로부터 유래한다는 것을 결정하고, 음성 입력이 Jane을 향한다(예를 들어, Bob은 Jane을 향하고 Jane은 Bob을 향하고 있음)고 표시한다고 결정한다.In some additional and/or alternative implementations, the
상기 구현예 중 일부 버전에서, 비전 데이터에서 캡처된 추가 참가자(들)의 신체 자세 및/또는 머리 자세가 클라이언트 디바이스(110)의 주어진 사용자가 클라이언트 디바이스(110)에서 검출된 음성 입력의 의도된 타겟임을 표시한다고 결정하는 것에 기초하여 클라이언트 디바이스(110)는 식별된 참가자(들) 각각에 대응하는 클라이언트 디바이스(110) 상의 하나 이상의 그래픽 엘리먼트들을 렌더링할 수 있다. 예를 들어, 클라이언트 디바이스(110)가 Bob으로부터의 음성 입력이 Jane에게만 향하는 것이라고 결정하면(예를 들어, Bob의 신체 자세 및/또는 머리 자세에 의해 표시된 바와 같이), 클라이언트 디바이스(110)의 주어진 사용자는 Bob으로부터의 음성 입력의 의도된 타겟이 아닐 수 있고 Bob 및/또는 Jane에 대응하는 그래픽 엘리먼트는 클라이언트 디바이스(110)에서 렌더링되지 않을 수 있으며, 이에 의해 클라이언트 디바이스(110)의 주어진 사용자가 Bob과 Jane 간의 대화의 참가자가 아님을 표시한다. 대조적으로, 클라이언트 디바이스(110)가 Bob으로부터의 음성 입력이 Jane과 클라이언트 디바이스(110)의 주어진 사용자 모두에게 향한다고 결정하면(예를 들어, Bob의 신체 자세 및/또는 머리 자세에 의해 표시되는 바와 같이), 클라이언트 디바이스(110)의 주어진 사용자는 Bob으로부터의 음성 입력의 의도된 타겟일 수 있고, Bob 및/또는 Jane에 대응하는 그래픽 엘리먼트는 클라이언트 디바이스(110)에서 렌더링될 수 있으며, 이에 의해 클라이언트 디바이스(110)의 주어진 사용자가 Bob과 Jane 간의 대화의 참가자임을 표시할 수 있다. 상기 구현예 중 일부 버전에서, 클라이언트 디바이스(110)는 클라이언트 디바이스(110)의 주어진 사용자가 클라이언트 디바이스(110)에서 검출된 음성 입력의 의도된 타겟이라는 결정에 응답하여, 클라이언트 디바이스(110) 상에서 제안(들)을 렌더링할 수 있다(예를 들어, 자동 제안 엔진(150)을 사용하여). 예를 들어, 클라이언트 디바이스(110)가 Bob으로부터의 음성 입력이 Jane에게만 향하는 것이라고 결정하면(예를 들어, Bob의 신체 자세 및/또는 머리 자세에 의해 표시된 바와 같이), 클라이언트 디바이스(110)의 주어진 사용자는 Bob으로부터의 음성 입력의 의도된 타겟이 아닐 수 있고 제안(들)은 클라이언트 디바이스(110)에서 렌더링되지 않을 수 있다. 대조적으로, 클라이언트 디바이스(110)가 Bob으로부터의 음성 입력이 Jane과 클라이언트 디바이스(110)의 주어진 사용자 모두에게 향한다고 결정하면(예를 들어, Bob의 신체 자세 및/또는 머리 자세에 의해 표시되는 바와 같이), 클라이언트 디바이스(110)의 주어진 사용자는 Bob으로부터의 음성 입력의 의도된 타겟일 수 있고, Bob으로부터의 음성 입력에 응답하는 제안(들)이 클라이언트 디바이스(110)에서 렌더링될 수 있다. 더욱이, 이러한 구현예 중 일부 추가 버전에서, 제안(들)에 포함된 콘텐츠는 대화에서 식별된 추가 참가자(들)에 따라 달라질 수 있다. In some versions of the above implementations, the body posture and/or head posture of the additional participant(s) captured in the vision data is the intended target of the voice input detected at the
다양한 구현예에서, 스피치 어시스턴트 시스템(160)은 스피치 인식 엔진(120A2), 스피치 합성 엔진(130A2), 운율 프로퍼티 엔진(140) 및 자동 제안 엔진(150)을 포함할 수 있다. 예를 들어, 다양한 구현예에서, 스피치 어시스턴트 시스템(160)은 클라이언트 디바이스(110) 상에서 로컬적으로 구현될 수 있다. 추가적인 및/또는 대안적인 구현예에서, 스피치 어시스턴트 시스템(160)은 서버(들)에 의해 구현될 수 있고, 네트워크(들)(190)(예를 들어, Wi-Fi, 블루투스 , 근거리 통신, 근거리 통신망(들), 광역 통신망(들) 및/또는 기타 네트워크)를 통해 클라이언트 디바이스(110)에 데이터를 송수신할 수 있다. 데이터는 예를 들어 음성 입력을 캡처하는 오디오 데이터, 대화에서 식별된 사용자(들)의 표시, 클라이언트 디바이스(110)의 센서(들)에 의해 생성된 센서 데이터(예를 들어, 비전 센서(들)에 의해 생성된 비전 데이터, GPS 센서에 의해 생성된 위치 데이터, 가속도계에 의해 생성된 가속도계 데이터, 자이로스코프에 의해 생성된 자이로스코프 데이터 및/또는 기타 센서 데이터), 운율 프로퍼티들, 합성 스피치 오디오 데이터, 클라이언트 디바이스(110)의 주어진 사용자의 화자 임베딩, 클라이언트 디바이스(110)에서 검출된 사용자 인터페이스 입력에 대응하는 텍스트 세그먼트(들), 하나 이상의 제안들에 포함된 후보 텍스트 세그먼트(들), 클라이언트 디바이스(110)에 의해 검출된 대화에 대한 시맨틱 데이터, 기계 학습 모델(들)을 통해 생성된 예측 출력 및/또는 본 명세서에 기술된 기타 데이터를 포함할 수 있다. 비록 도 1의 스피치 어시스턴트 시스템(160)은 서버(들)에 의해 구현되는 것으로 본 명세서에 설명되지만, 스피치 어시스턴트 시스템(160)의 다양한 엔진(들) 및/또는 스피치 어시스턴트 시스템(160)에 의해 수행되는 동작들은 클라이언트 디바이스(110)에 위치되고 및/또는 클라이언트 디바이스(110)에 의해 수행될 수 있다. In various implementations,
일부 구현예에서, 스피치 인식 엔진(들)(120A1)은 예측된 출력(예를 들어, 인식된 텍스트)을 생성하기 위해 사용자 인터페이스 입력 엔진(111)에 의해 검출된 음성 입력을 캡처하는 오디오 데이터를 프로세싱할 수 있다. 일부 추가적 및/또는 대안적 구현예에서, 클라이언트 디바이스(110)는 하나 이상의 네트워크(190)(예를 들어, Wi-Fi, 블루투스, 근거리 통신, 근거리 네트워크(들), 광역 네트워크(들) 및/또는 기타 네트워크)를 통해, 사용자 인터페이스 입력 엔진(111)에 의해 검출된 음성 입력을 캡처하는 오디오 데이터를 스피치 어시스턴트 시스템(160)에 전송할 수 있다. 스피치 인식 엔진(들)(120A1 및/또는 120A2)은 스피치 인식 모델(들)(120A)을 사용하여 음성 입력을 캡처하는 오디오 데이터를 프로세싱하여 인식된 텍스트를 생성할 수 있다. 또한, 스피치 인식 엔진(들)(120A1 및/또는 120A2)은 프로세싱된 출력에 기초하여, 대화에서 전달될 음성 입력에 대응하는 텍스트 세그먼트(들)를 생성할 수 있다. 일부 구현예에서, 스피치 인식 모델(들)(120A)은 음성 입력을 말한 사용자에 관계없이 음성 입력을 캡처하는 오디오 데이터를 프로세싱하도록 트레이닝된 단일 스피치 인식 모델을 포함한다. 일부 다른 구현예에서, 스피치 인식 모델(들)(120A)은 서로 다른 방식으로 각각 트레이닝되는 다수의 스피치 인식 모델들을 포함한다. 예를 들어, 스피치 인식 모델(들)(120A)은 언어 장애가 있는 사용자가 말한 음성 입력을 캡처하는 오디오 데이터를 프로세싱하도록 트레이닝된 제1 스피치 인식 모델 및 언어 장애가 없는 사용자가 말한 음성 입력을 캡처하는 다른 모든 오디오 데이터를 프로세싱하도록 트레이닝된 제2 스피치 인식 모델을 포함할 수 있다. 상기 구현예 중 일부 버전에서, 음성 입력을 캡처하는 오디오 데이터는 제1 인식된 텍스트 및 제2 인식된 텍스트를 생성하기 위해 제1 스피치 인식 모델 및 제2 스피치 인식 모델 둘 모두에 걸쳐 입력으로서 적용될 수 있다. 스피치 인식 엔진(들)(120A1 및/또는 120A2)은 제1 인식된 텍스트 및 제2 인식된 텍스트와 연관된 신뢰도 메트릭에 기초하여 음성 입력에 대응하는 것으로서 제1 인식된 텍스트 또는 제2 인식된 텍스트 중 하나를 선택할 수 있다. 상기 구현예 중 일부 다른 버전에서, 음성 입력을 캡처하는 오디오 데이터는 음성 입력과 연관된 식별된 사용자의 표시를 포함할 수 있으며, 식별된 사용자가 언어 장애가 있는 사용자인 경우 음성 입력과 연관된 오디오 데이터는 제1 스피치 인식 모델에 입력으로 적용되고, 언어 장애가 없는 사용자의 음성 입력을 캡처하는 모든 오디오 데이터는 제2 스피치 인식 모델에 입력으로 적용될 수 있다. In some implementations, speech recognition engine(s) 120A1 receives audio data that captures speech input detected by user
일부 구현예에서, 스피치 인식 모델(들)(120A)은 종단간 스피치 인식 모델(들)이며, 따라서 스피치 인식 엔진(들)(120A1 및/또는 120A2)은 모델을 사용하여 직접 음성 입력에 대응하는 텍스트 세그먼트(들)를 생성할 수 있다. 예를 들어, 스피치 인식 모델(들)(120A)은 문자별 기반(또는 다른 토큰별 기반)으로 텍스트 세그먼트(들)를 생성하는데 사용되는 종단간 모델(들)일 수 있다. 문자 단위 기반으로 텍스트 세그먼트(들)를 생성하는데 사용되는 이러한 종단간 모델(들)의 한 비제한적인 예는 순환 신경망 트랜스듀서(RNN-T) 모델이다. RNN-T 모델은 주의 메커니즘을 사용하지 않는 시퀀스 대 시퀀스 모델의 한 형태이다. 예측된 출력을 생성하기 위해 일반적으로 전체 입력 시퀀스(예: 오디오 데이터 파형, MFCC(멜 주파수 Cepstral 계수) 또는 기타 표현)를 프로세싱해야 하는 대부분의 시퀀스 대 시퀀스 모델과 달리, RNN-T 모델은 연속적으로 입력 샘플들을 프로세싱하고 출력 기호들을 스트리밍하는데 사용된다(예: 알파벳 문자). 또한, 예를 들어, 스피치 인식 모델(들)이 종단간 스피치 인식 모델(들)이 아닌 경우, 스피치 인식 엔진(들)(120A1 및/또는 120A2)은 대신 예측된 음소(들)(및 /또는 기타 표현)을 생성할 수 있다. 예를 들어, 이러한 모델로 예측된 음소(들)(및/또는 다른 표현들)는 예측된 음소(들)에 부합하는 텍스트 세그먼트(들)를 결정하기 위해 스피치 인식 엔진(들)(120A1 및/또는 120A2)에 의해 활용된다. 그렇게 함으로써, 스피치 인식 엔진(들)(120A1 및/또는 120A2)은 디코딩 그래프, 어휘 및/또는 다른 리소스(들)를 선택적으로 사용할 수 있다. In some implementations, the speech recognition model(s) 120A is an end-to-end speech recognition model(s), so that the speech recognition engine(s) 120A1 and/or 120A2 uses the model to respond to direct speech input. You can create text segment(s). For example, speech recognition model(s) 120A may be an end-to-end model(s) used to generate text segment(s) on a per-character (or other per-token basis) basis. One non-limiting example of such an end-to-end model(s) used to generate text segment(s) on a character-by-character basis is the Recurrent Neural Network Transducer (RNN-T) model. The RNN-T model is a form of sequence-to-sequence model that does not use an attention mechanism. Unlike most sequence-to-sequence models, which typically require processing the entire input sequence (e.g., an audio data waveform, Mel Frequency Cepstral Coefficient (MFCC), or other representation) to produce a predicted output, the RNN-T model is Used to process input samples and stream output symbols (eg alphabetic characters). Also, for example, if the speech recognition model(s) is not an end-to-end speech recognition model(s), the speech recognition engine(s) 120A1 and/or 120A2 instead of the predicted phoneme(s) (and/or other expressions) can be created. For example, the phoneme(s) (and/or other representations) predicted with this model can be used with speech recognition engine(s) 120A1 and/or to determine text segment(s) that match the predicted phoneme(s). or 120A2). In doing so, the speech recognition engine(s) 120A1 and/or 120A2 may optionally use the decoding graph, vocabulary, and/or other resource(s).
일부 구현예에서, 클라이언트 디바이스(110)의 주어진 사용자로부터의 음성 입력에 대응하는 텍스트 세그먼트(들)는 (예를 들어, 클라이언트 디바이스(110)의 주어진 사용자에 대한 화자 임베딩에 추가하여) 클라이언트 디바이스(110)의 주어진 사용자의 스피치를 나타내는 합성 스피치 오디오 데이터를 생성할 때 스피치 합성 엔진(130A1 및/또는 130A2)에 의해 사용될 수 있다. 상기 구현예 중 일부 버전에서, 텍스트 세그먼트(들)는 렌더링 엔진(113)을 사용하여, 클라이언트 디바이스(110)의 사용자 인터페이스에 의해 시각적으로 렌더링될 수 있다. 일부 구현예에서, 클라이언트 디바이스(110)의 주어진 사용자와의 대화에 참여하는 추가 참가자(들)로부터의 음성 입력에 대응하는 텍스트 세그먼트(들)는 또한 렌더링 엔진(113)을 사용하여 클라이언트 디바이스(110)의 사용자 인터페이스에 의해 시각적으로 렌더링될 수 있다. 이러한 방식으로, 클라이언트 디바이스(110)의 주어진 사용자 및 대화의 추가 참가자(들)와의 대화의 전사가 클라이언트 디바이스(110)에서 시각적으로 렌더링될 수 있다.In some implementations, text segment(s) corresponding to voice input from a given user of
운율 프로퍼티 엔진(140)은 예를 들어 관계 엔진(141), 환경 엔진(142), 및 시맨틱 엔진(143)을 포함할 수 있다. 본 명세서에 설명된 바와 같이, 운율 프로퍼티 엔진(140)은 클라이언트 디바이스(110)의 주어진 사용자에 대한 스피치를 합성하는데 사용될 하나 이상의 운율 프로퍼티들을 결정할 수 있다. 본 명세서에서 사용된 바와 같이, "운율 프로퍼티들"은 예를 들어 억양, 톤, 강세, 리듬, 템포, 피치 및 일시 중지와 같은 언어적 기능을 포함하여 음절 및 더 큰 스피치 단위의 하나 이상의 프로퍼티들을 포함할 수 있다. 또한, 텍스트 세그먼트에 대한 합성 스피치를 생성하는데 운율 프로퍼티들을 사용하면 조합하여 예를 들어 감정 상태; 형식(예: 진술, 질문 또는 명령); 반어; 풍자; 및/또는 강조를 반영할 수 있다. 즉, 운율 프로퍼티들은 주어진 사용자의 개별 음성 특성과 무관하고 합성 스피치의 기준 "음성"을 변경하거나 및 또는 합성 스피치에 포함된 텍스트 세그먼트의 의미를 변경하기 위해 대화 중에 동적으로 조정될 수 있는 스피치의 피처들이다. The
본 명세서에 기술된 바와 같이, 운율 프로퍼티 엔진(140)은 클라이언트 디바이스(110)의 주어진 사용자와 주어진 사용자와의 대화에 참여하는 추가 참가자(들) 간의 관계 엔진(131)에 의해 결정된 관계의 속성(들); 환경 엔진(142)에 의해 결정된, 주어진 사용자와 대화의 추가 참가자(들) 간의 대화가 발생하는 위치의 분류(들); 및/또는 시맨틱 엔진(143)에 의해 결정된, 주어진 사용자와 대화의 추가 참가자(들) 간의 대화의 시맨틱(들)에 기초하여 운율 프로퍼티들을 결정할 수 있다. 일부 구현예에서, 운율 프로퍼티 엔진(140)은 클라이언트 디바이스(110)의 주어진 사용자로부터 음성 입력을 수신하는 것에 응답하여, 운율 프로퍼티들을 자동으로 결정하고, 결정된 운율 프로퍼티들로 합성되는 합성 스피치를 포함하고 주어진 사용자로부터 수신된 음성 입력에 포함된 텍스트 세그먼트(들)를 포함하는 합성 스피치 오디오 데이터를 자동으로 생성하고, 주어진 사용자로부터 음성 입력을 수신하는 것에 응답하여 합성 스피치를 청각적으로 렌더링할 수 있다. 일부 다른 구현예에서, 운율 프로퍼티 엔진(140)은 클라이언트 디바이스(110)의 주어진 사용자로부터 음성 입력을 수신하는 것에 응답하여, 운율 프로퍼티들을 자동으로 결정하고, 결정된 운율 프로퍼티들로 합성되는 합성 스피치를 포함하고 주어진 사용자로부터 수신된 음성 입력에 포함된 텍스트 세그먼트(들)를 포함하는 합성 스피치 오디오 데이터를 자동으로 생성하지만, 타임아웃 기간(예: 3초, 5초 등)이 종료할 때까지 합성 스피치의 청각적 렌더링을 보류할 수 있다. 이 타임아웃 기간 동안, 주어진 사용자는 결정된 운율 프로퍼티들을 수동으로 조정할 수 있다(예를 들어, 도 4b 내지 도 4d와 관련하여 설명된 바와 같이). 또 다른 구현예에서, 운율 프로퍼티 엔진(140)은 클라이언트 디바이스(110)의 주어진 사용자로부터 음성 입력을 수신하는 것에 응답하여, 운율 프로퍼티들을 자동으로 결정하고, 결정된 운율 프로퍼티들로 합성되는 합성 스피치를 포함하고 주어진 사용자로부터 수신된 음성 입력에 포함된 텍스트 세그먼트(들)를 포함하는 합성 스피치 오디오 데이터를 자동으로 생성하지만, 합성 스피치가 청각적으로 렌더링되어야 한다고 확인하는 주어진 사용자로부터의 추가 사용자 인터페이스 입력이 사용자 인터페이스 입력 엔진(111)에 의해 검출될 때까지 합성 스피치의 청각적 렌더링을 억제할 수 있다. 이것은 주어진 사용자가 결정된 운율 프로퍼티들을 수동으로 조정하는 것을 허용할 뿐만 아니라(예를 들어, 도 4b 내지 도 4d과 관련하여 설명된 바와 같이) 주어진 사용자는 결정된 텍스트 세그먼트(들)의 전사를 수정할 수 있게 한다(예를 들어, 도 4f와 관련하여 설명된 바와 같이). As described herein, the
운율 프로퍼티들은 클라이언트 디바이스(110)의 주어진 사용자와 상기 주어진 사용자와 대화에 참여하는 추가 참가자(들) 간의 관계의 속성(들)에 기초하여 동적으로 결정될 수 있다. 일부 구현예에서, 관계 엔진(141)은 추가 참가자(들)가 알 수 없는 사용자(들) 또는 알려진 사용자(들)인지 여부에 기초하여(예를 들어, 식별 엔진(112)에 의해 결정됨) 클라이언트 디바이스(110)의 주어진 사용자와 상기 주어진 사용자와 대화에 참여하는 추가 참가자(들) 간의 관계(들)의 속성(들)을 결정할 수 있다. 상기 구현예 중 일부 버전에서, 추가 참가자(들) 중 주어진 사람이 알 수 없는 사용자인 경우(예를 들어, 클라이언트 디바이스(110)에 로컬적으로 저장되는 화자 임베딩 및 비주얼 임베딩이 없다는 것에 기초하여 결정됨), 관계 엔진(141)은 알 수 없는 사용자에게 "알 수 없음" 속성을 할당한다. 또한, 운율 프로퍼티 데이터베이스(140A)에 저장된 기본 운율 프로퍼티들의 세트는 알려지지 않은(unknow) 속성에 매핑될 수 있다. 예를 들어, 알려지지 않은 속성은 톤이 형식적이고 상대적으로 느린 템포 및/또는 리듬을 갖는 합성 스피치를 생성하는 기본 운율 프로퍼티들의 세트와 연관될 수 있다. 합성 스피치는 알 수 없는 사용자가 주어진 사용자의 합성 음성에 익숙하지 않을 수 있기 때문에 형식적인 톤과 비교적 느린 템포 및/또는 리듬을 포함할 수 있다. 상기 구현예 중 일부 추가 버전에서, 추가 참가자(들) 중 주어진 한 명이 알 수 없는 사용자인 경우, 클라이언트 디바이스(110)에 로컬적으로 알 수 없는 사용자의 화자 임베딩 및/또는 비주얼 임베딩을 저장하기 위한 승인을 요청하는 프롬프트가 클라이언트 디바이스(110)에서 렌더링될 수 있어서, 알 수 없는 사용자가 알려진 사용자가 될 수 있도록 하고, 클라이언트 디바이스(110)의 주어진 사용자와의 향후 대화에서 인식될 수 있도록 한다.Prosody properties may be dynamically determined based on attribute(s) of a relationship between a given user of the
상기 구현예 중 일부 버전에서, 추가 참가자(들) 중 주어진 한 사람이 특정한 알려진 사용자(예를 들어, 클라이언트 디바이스(110)에 로컬적으로 저장된 화자 임베딩 및/또는 비주얼 임베딩에 기초하여 식별됨)인 경우, 관계 엔진(141)은 클라이언트 디바이스의 주어진 사용자와 특정한 알려진 사용자 간의 관계의 속성(들)을 결정할 수 있다. 속성(들)은 예를 들어, 주어진 사용자와 특정한 알려진 사용자 간의 인터렉션(예: 대화 인터렉션, 전화 통화, 텍스트 메시지 인터렉션, SMS 메시지 인터렉션, 이메일 인터렉션 및/또는 기타 인터렉션)의 양, 주어진 사용자와 특정한 알려진 사용자 간의 인터렉션 빈도 및/또는 주어진 사용자와 특정한 알려진 사용자 간의 관계의 시맨틱 식별자들 또는 표시(들)(예를 들어, 상사, 친구, 친한 친구, 동료, 가족 구성원 및/또는 기타 시맨틱 표시(들)과 같은 시맨틱 표시(들))를 포함할 수 있다. 상기 구현예 중 일부 추가 버전에서, 특정한 알려진 사용자와 연관된 속성(들)은 관계 속성(들) 데이터베이스(141A)에 저장될 수 있다. 일부 예에서, 그래픽 표시 또는 엘리먼트는 클라이언트 디바이스 상에 디스플레이되거나 렌더링될 수 있고, 그래픽 표시 또는 엘리먼트는 추가 사용자 또는 참가자의 시맨틱 식별자 또는 표시를 포함한다. In some versions of the above implementations, a given one of the additional participant(s) is a particular known user (eg, identified based on speaker embeddings and/or visual embeddings stored locally on the client device 110 ). If so, the relationship engine 141 may determine the attribute(s) of a relationship between a given user of the client device and a particular known user. The attribute(s) can be, for example, the amount of interactions between a given user and a particular known user (eg, conversational interactions, phone calls, text message interactions, SMS message interactions, email interactions and/or other interactions), the amount of interactions between a given user and a particular known user. frequency of interactions between users and/or semantic identifiers or indication(s) of a relationship between a given user and a particular known user (e.g., boss, friend, close friend, co-worker, family member, and/or other semantic indication(s); same semantic indication(s)). In some further versions of the above implementations, the attribute(s) associated with a particular known user may be stored in the relational attribute(s)
상기 구현예 중 일부 추가 버전에서, 운율 프로퍼티 데이터베이스(140A)에 저장된 운율 프로퍼티들의 상이한 그룹화는 상이한 속성(들)에 매핑될 수 있다. 예를 들어, 상사 속성은 운율 프로퍼티들의 제1 세트에 매핑되어 톤이 보다 형식적이고 풍자를 피하는 합성 스피치를 생성할 수 있다. 또한, 예를 들어, 친한 친구 속성 및/또는 빈번한 인터렉션 속성은 운율 프로퍼티들의 제2 세트에 매핑되어 톤이 일상적이고 비교적 빠른 템포를 갖는 합성 스피치를 생성할 수 있다. 보다 빠른 템포는 합성 스피치를 클라이언트 디바이스(110)에서 더 빨리 렌더링할 수 있게 하고, 대화를 더 빨리 마무리할 수 있다. 보다 빠른 템포는 잠재적으로 합성 스피치를 이해하기 어렵게 만들 수 있지만 친한 친구 및/또는 빈번한 인터렉션 속성(들)을 결정하는데 응답하여 보다 빠른 템포를 활용하면 친한 친구 및/또는 빈번한 인터렉션 속성(들)이 합성 음성의 친밀도를 표시한다는 관점에서 이 위험을 완화할 수 있다. 이러한 방식 및 기타 방식으로, 합성 스피치의 더 빠른 렌더링(및 연관된 계산 리소스의 더 짧은 사용 지속시간)으로 이어지는 운율 프로퍼티들은 선택적으로 활용될 수 있으며, 합성 스피치이 이해되지 않으면 렌더링을 반복해야 할 필요성의 위험을 표시하는 관계 속성(들)로 그러한 효율성들을 밸런싱한다. 더욱이, 관계 속성(들)은 클라이언트 디바이스(110)의 주어진 사용자와 특정 알려진 사용자가 시간이 지남에 따라 인터렉션함에 따라 업데이트될 수 있다.In some further versions of the above implementations, different groupings of prosody properties stored in
일부 구현예에서, 대화가 다수의 추가 참가자들을 포함하는 경우, 운율 프로퍼티 엔진(140)은 클라이언트 디바이스(110)의 주어진 사용자에 대한 합성 스피치를 생성하는데 사용될 결정된 운율 프로퍼티들로서 대화의 추가 참가자들 각각에 대한 운율 프로퍼티들의 평균을 활용할 수 있다. 예를 들어, 클라이언트 디바이스(110)의 주어진 사용자가 주어진 사용자의 상사(예를 들어, 톤이 더 형식적이고 상기 표시된 바와 같이 풍자를 피하는 합성된 스피치를 생성할 수 있는 운율 프로퍼티들의 제1 세트와 연관됨) 및 친한 친구(예: 톤이 일상적이고 비교적 빠른 템포를 갖는 합성된 스피치를 생성할 수 있는 운율 프로퍼티의 제2 세트와 연관됨)와 대화에 참여하는 경우, 운율 프로퍼티들의 결정된 세트는 일부 풍자를 포함하고 주어진 사용자가 상사 또는 개인적으로 친한 친구와 단순한 대화를 가졌을 때 보다 낮은 스피치를 포함시키기 위해 중간 템포를 포함하는 합성 스피치를 생성할 수 있다. 일부 다른 구현예에서, 대화가 다수의 추가 참가자들을 포함하는 경우, 운율 프로퍼티 엔진(140)은 추가 참가자들 각각에 대해 결정된 운율 프로퍼티들 중에서 보다 형식적인 스피치에 대응하는 상기 운율 프로퍼티들을 클라이언트 디바이스(110)의 주어진 사용자에 대한 합성 스피치를 생성하는데 사용될 결정된 운율 프로퍼티로서 활용할 수 있다. 이러한 방식으로, 주어진 사용자와 추가 참가자 간의 관계가 주어진 사용자와 추가 참가자 간의 추가 관계보다 더 형식적이라는 결정에 응답하여, 하나 이상의 운율 프로퍼티들은 주어진 사용자와 다른 추가 참가자 간의 추가 관계의 하나 이상의 추가 속성을 사용하는 대신(또는 대신), 주어진 사용자와 추가 참가자 간의 관계의 속성들에 기초할 수 있다. 예를 들어, 클라이언트 디바이스(110)의 주어진 사용자가 주어진 사용자의 상사(예를 들어, 톤이 더 형식적이고 상기 표시된 바와 같이 풍자를 피하는 합성된 스피치를 생성할 수 있는 운율 프로퍼티들의 제1 세트와 연관됨) 및 친한 친구(예: 톤이 일상적이고 비교적 빠른 템포를 갖는 합성된 스피치를 생성할 수 있는 운율 프로퍼티의 제2 세트와 연관됨)와 대화에 참여하는 경우, 운율 프로퍼티들의 결정된 세트는 대화가 보다 형식적인 스피치와 연관되므로 운율 프로퍼티들의 제1 세트일 수 있다. In some implementations, when the dialog includes multiple additional participants, the
운율 프로퍼티들은 클라이언트 디바이스(110)의 주어진 사용자와 상기 주어진 사용자와 대화에 참여하는 추가 참가자(들) 사이에서 발생하는 대화 위치의 분류(들) 및/또는 다른 피처들에 기초하여 추가적으로 및/또는 대안적으로 동적으로 결정될 수 있다. 일부 구현예에서, 환경 엔진(142)은 클라이언트 디바이스(110)에 의해 생성되고 네트워크(들)(190)를 통해 스피치 어시스턴트 시스템(160)으로 전송된 센서 데이터(예를 들어, 클라이언트 디바이스(110)의 GPS 센서에 의해 생성된 위치 데이터)에 기초하여 대화의 위치를 결정할 수 있다. 위치는 알려진 위치(예를 들어, 주어진 사용자의 집, 주어진 사용자의 직장)와 비교될 수 있고 및/또는 다른 위치를 식별하기 위해 지도와 비교될 수 있다. 상기 구현예 중 일부 버전에서, 운율 프로퍼티 데이터베이스(140A)에 저장된 운율 프로퍼티들의 상이한 그룹화는 대화 위치의 상이한 분류(들)에 매핑될 수 있다. 예를 들어, 운율 프로퍼티들의 제1 세트는 집 분류(즉, 주어진 사용자에 대한 집 분류 - 주어진 사용자의 집)를 갖는 위치에서 결정 및 활용될 수 있고, 운율 프로퍼티들의 제2 세트는 직장 분류가 있는 위치(즉, 주어진 사용자에 대한 직장 분류 - 주어진 사용자의 직장 장소)를 갖는 결정 및 활용될 수 있고 운율 프로퍼티들의 제3 세트는 커피숍 분류를 갖는 위치(모든 사용자에게 공통됨)에서 결정 및 활용될 수 있고, 운율 프로퍼티들의 제4 세트는 바 분류를 갖는 위치(모든 사용자에게 공통됨)에서 결정 및 활용될 수 있고, 그리고 운율 프로퍼티들의 제5 세트는 음식점 분류를 갖는 위치(모든 사용자에게 공통됨)에서 결정 및 활용될 수 있다. Prosody properties may additionally and/or alternatively be based on classification(s) and/or other features of a conversation location occurring between a given user of the
운율 프로퍼티들은 클라이언트 디바이스(110)의 주어진 사용자와 상기 주어진 사용자와 대화에 참여하는 추가 참가자(들) 간의 대화의 시맨틱(들)에 기초하여 추가적으로 및/또는 대안적으로 동적으로 결정될 수 있다. 일부 구현예에서, 시맨틱 엔진(143)은 대화의 추가 참가자(들)로부터의 이전 음성 입력에 대응하는 이전 음성 입력의 피처 및/또는 이전 텍스트 세그먼트(들)의 피처를 프로세싱할 수 있다. 또한, 시맨틱 엔진(143)은 또한 대응하는 응답 음성 입력의 피처 및/또는 주어진 사용자로부터의 응답 음성 입력에 대응하는 응답 텍스트 세그먼트(들)의 피처를 프로세싱할 수 있으며, 여기서 주어진 사용자로부터의 응답 음성 입력은 추가 참가자(들)의 이전 음성 입력에 대한 응답이다. 이전 텍스트 세그먼트(들) 및/또는 텍스트 세그먼트(들)의 프로세싱에 기초하여, 시맨틱 엔진(143)은 대화의 컨텍스트에서 시맨틱 피처를 반영하는 운율 프로퍼티들의 세트를 결정할 수 있다. 예를 들어, 클라이언트 디바이스(110)의 주어진 사용자가 해임되고, 진술에서 추가 참가자로부터 "Why did you say she stole your money?"라는 질문에 응답하고, 주어진 사용자는 "I never said she stole my money"라고 응답한다고 가정한다. 이 경우, 운율 프로퍼티들의 제1 세트가 "stole"이라는 단어에 강세를 포함하는 합성 스피치를 생성하는 경우, 합성 스피치는 돈이 도난당했을 수도 있고 도난당하지 않았을 수도 있음을 나타낼 수 있지만, 음성 입력을 제공한 주어진 사용자가 이전에 돈이 실제로 도난되었는지 여부를 표시한 적이 없다. 대조적으로, 운율 프로퍼티의 제2 세트가 "never"라는 단어에 강세를 포함하는 합성 스피치를 생성하는 경우, 합성 스피치는 돈이 실제로 도난당하지 않았음을 나타낼 수 있으며, 음성 입력을 제공한 주어진 사용자는 돈에 대한 이전 진술은 부적절하게 해석되지 않았음을 보장한다. 또한, 운율 프로퍼티의 제3 세트가 "I NEVER SAID SHE STOLE MY MONEY"라는 전체 문구에 대해 거칠거나 단단한 톤과 빠른 리듬을 포함하는 합성 스피치를 생성하는 경우, 합성 스피치는 주어진 사용자가 질문에 화가 났고 및/또는 주어진 사용자가 이전에 자신이 돈을 훔쳤다고 표시한 적이 없다는 강한 확신을 가지고 있다고 표시할 수 있다. 그러나 합성 스피치이 운율 프로퍼티들에 기초한 조정 없이 주어진 사용자에 대한 기준 "음성"으로 단순히 합성된다면, 합성된 "I never said she stole my money"라는 문구의 실제 의미를 잃을 수 있다. Prosody properties may additionally and/or alternatively be dynamically determined based on semantic(s) of a conversation between a given user of the
따라서, 운율 프로퍼티 엔진(140)은 클라이언트 디바이스(110)의 주어진 사용자에 대한 합성 스피치를 생성하는데 사용하기 위해 결정된 운율 프로퍼티들의 세트를 결정할 때 관계 엔진(141)에 의해 결정된 운율 프로퍼티들의 제1 세트, 환경 엔진(142)에 의해 결정된 운율 프로퍼티들의 제2 세트 및/또는 시맨틱 엔진(143)에 의해 결정된 운율 프로퍼티들의 제3 세트를 고려할 수 있다. 예를 들어, 주어진 사용자가 주어진 사용자의 집에서 친한 친구와 대화에 참여하고 있다면, 운율 프로퍼티 엔진(140)은 주어진 사용자의 집은 주어진 사용자가 자유롭게 말할 수 있는 사적인 장소이므로 대화가 빠른 템포와 일상적인 톤을 포함할 수 있도록 운율 프로퍼티들의 제1 세트 및 운율 프로퍼티들의 제2 세트에 기초하여, 운율 프로퍼티들의 제4 세트를 결정할 수 있다. 그러나, 주어진 사용자와 친한 친구가 열띤 토론에 참여하는 경우, 운율 프로퍼티 엔진(140)은 특정 단어 또는 문구에 더 많은 감정 및/또는 강조 또는 강세를 포함하기 위해 운율 프로퍼티들의 제3 세트 및 운율 프로퍼티들의 제4 세트에 기초하여, 운율 프로퍼티들의 제5 세트를 결정할 수 있다. 다른 예로서, 주어진 사용자가 바에서 친한 친구와 대화에 참여하는 경우, 운율 프로퍼티 엔진(140)은 운율 프로퍼티들의 제1 세트 및 운율 프로퍼티들의 제2 세트에 기초하여 일상적인 톤을 유지하지만 이전 예에 표시된 바와 같이 주어진 사용자의 집에서 사용되는 빠른 템포와 비교하여 바 환경에서 더 큰 주변 소음을 고려하기 위해 느린 템포를 포함하는 운율 프로퍼티들의 제4 세트를 운율 프로퍼티들의 결정된 세트로 결정할 수 있다. 또 다른 예로서, 주어진 사용자가 커피숍에서 친한 친구와 대화에 참여하는 경우, 운율 프로퍼티 엔진(140)은 운율 프로퍼티들의 제1 세트 및 운율 프로퍼티들의 제2 세트에 기초하여, 빠른 템포를 포함하지만 보다 캐주얼한 바에서와 비교하여 커피숍 환경에서 더 약한 주변 소음을 고려하기 위해 보다 형식적인 톤을 포함하는 운율 프로퍼티들의 제4 세트를 운율 프로퍼티들의 결정된 세트로 결정할 수 있다. 따라서, 운율 프로퍼티 엔진(140)은 관계 속성(들), 대화 위치의 분류(들), 대화의 시맨틱 및/또는 기타 팩터(들)에 기초하여 클라이언트 디바이스(110)의 주어진 사용자에 대한 합성 스피치를 생성하는데 사용할 운율 프로퍼티들의 세트를 결정할 수 있다.Accordingly, the
본 명세서에 기술된 바와 같이, 합성 스피치 오디오 스피치가 생성될 수 있고, 클라이언트 디바이스(110)의 주어진 사용자의 화자 임베딩을 사용하여 합성되고, 결정된 운율 프로퍼티들로 합성되고, 클라이언트 디바이스(110)에서 검출된 사용자 인터페이스에 기초하여 결정된 텍스트 세그먼트(들)를 포함하는 합성 스피치를 포함할 수 있다. 합성 스피치가 클라이언트 디바이스(110)의 주어진 사용자의 화자 임베딩을 사용하여 합성되는 경우, 합성 스피치는 클라이언트 디바이스(110)의 주어진 사용자에 대한 합성 스피치의 기준 "음성"을 나타낼 수 있다. 그러나, 합성 스피치도 결정된 운율 프로퍼티들로 합성되는 경우, 클라이언트 디바이스(110)의 주어진 사용자에 대한 합성 스피치의 기준 음성이 조정될 수 있다. 위에서 언급한 바와 같이, 결정된 운율 프로퍼티들은 주어진 사용자에 대한 합성 스피치의 기준 음성을 변경하고 및/또는 합성 스피치에 포함된 텍스트 세그먼트의 의미를 변경하는데 활용될 수 있다. 따라서, 결정된 운율 프로퍼티들은 제1 남성 합성 음성 대 제2 남성 합성 음성, 남성 합성 음성 대 여성 합성 음성 또는 제1 언어의 합성 음성 대 제2 언어 합성 음성을 생성하는데 활용되지 않는다. 오히려, 본 명세서에 기술된 합성 스피치는 합성 스피치의 기준 음성 및/또는 합성 스피치의 의미를 변경하기 위해 결정된 운율 프로퍼티들을 활용할 수 있다. 예를 들어, 동일한 기준 음성은 합성 스피치를 생성하기 위해 동일한 화자 임베딩을 사용하는 기능일 수 있으며, 운율 프로퍼티들은 합성 스피치의 기준 음성 및/또는 합성 스피치의 의미를 변경할 수 있다. 이러한 예들 중 일부에서, 스피치 합성 모델은 합성 스피치를 생성하기 위해 상이한 화자 임베딩을 활용할 수 있고, 운율 프로퍼티들은 합성 스피치의 기준 음성 및/또는 스피치 합성 모델 각각에 대한 합성 스피치의 의미를 변경할 수 있다. 따라서, 생성된 합성 스피치은 결정된 운율 프로퍼티들에 기초하여 클라이언트 디바이스의 주어진 사용자로부터 동일한 음성 입력을 수신하는 것에 응답하여 상이할 수 있다.As described herein, synthesized speech audio speech can be generated, synthesized using a speaker embedding of a given user of the
일부 구현예에서, 스피치 어시스턴트 시스템(160)은 클라이언트 디바이스(110)의 주어진 사용자에 대한 음성을 합성하는데 사용될 결정된 운율 프로퍼티들을 네트워크(들)(190)를 통해 클라이언트 디바이스(110)에 전송할 수 있고, 스피치 합성 엔진(130A1)은 스피치 합성 모델(들)(130A)을 사용하여 합성 스피치 오디오 데이터를 생성할 수 있다. 일부 다른 구현예에서, 스피치 어시스턴트 시스템(160)은 클라이언트 디바이스(110)로부터 네트워크(들)(190)를 통해 클라이언트 디바이스(110)의 주어진 사용자와 연관된 화자 임베딩 및 클라이언트 디바이스(110)의 주어진 사용자의 음성 입력에 대응하는 텍스트 세그먼트(들)를 수신할 수 있다. 스피치 합성 엔진(들)(130A1 및/또는 130A2)은 스피치 합성 모델(들)(130A)을 사용하여 클라이언트 디바이스(110)의 주어진 사용자와 연관된 화자 임베딩을 사용하여 결정된 운율 프로퍼티들로 합성되고 클라이언트 디바이스(110)의 주어진 사용자의 음성 입력에 대응하는 텍스트 세그먼트(들)를 포함하는 합성 스피치 오디오 데이터를 생성할 수 있다. 예를 들어, 스피치 합성 엔진(들)(130A1 및/또는 130A2)은 클라이언트 디바이스(110)의 주어진 사용자의 음성 입력에 대응하는 텍스트 세그먼트(들)에 대응하여 결정된 음소들의 시퀀스를 결정할 수 있고, 합성 스피치 오디오 데이터를 생성하기 위해 스피치 합성 모델(들)(130A)을 사용하여 음소들의 시퀀스를 프로세싱할 수 있다. 합성 스피치 오디오 데이터는 예를 들어 오디오 파형의 형태일 수 있다. 클라이언트 디바이스(110)의 주어진 사용자의 음성 입력에 대응하는 텍스트 세그먼트(들)에 대응하는 음소들의 시퀀스를 결정할 때, 스피치 합성 엔진(들)(130A1 및/또는 130A2)은 클라이언트 디바이스(110)에 로컬적으로 저장된 토큰-음소 매핑에 액세스할 수 있다. In some implementations,
일부 구현예에서, 스피치 합성 엔진(들)(130A1 및/또는 130A2)은 스피치 합성 모델(들)(130A)을 사용하여, 클라이언트 디바이스(110)의 주어진 사용자와 연관된 화자 임베딩, 결정된 운율 프로퍼티들 및 클라이언트 디바이스(110)의 주어진 사용자의 음성 입력에 대응하는 텍스트 세그먼트(들)를 스피치 합성 모델(들)(130A)에 걸친 입력으로서 적용함으로써 합성 스피치 오디오 데이터를 생성할 수 있다. 따라서, 합성 스피치 오디오 데이터에 포함된 합성 스피치는 클라이언트 디바이스(110)의 주어진 사용자의 음성 입력의 콘텐츠를 나타낼 뿐만 아니라(예를 들어, 텍스트 세그먼트(들)을 통해), 합성 스피치는 또한 음성 입력이 수신되는 다양한 시나리오에 대한 다양한 운율 프로퍼티들을 나타낸다(예를 들어, 도 3 및 4a-4f와 관련하여 설명된 바와 같음). 일부 다른 구현예에서, 스피치 합성 엔진(들)(130A1 및/또는 130A2)은 스피치 합성 모델(들)(130A)을 사용하여, 클라이언트 디바이스(110)의 주어진 사용자와 연관된 화자 임베딩 및 클라이언트 디바이스(110)의 주어진 사용자의 음성 입력에 대응하는 텍스트 세그먼트(들)를 스피치 합성 모델(들)(130A)에 걸친 입력으로서 적용함으로써 합성 스피치 오디오 데이터를 생성할 수 있다. 상기 구현예 중 일부 버전에서, 스피치 합성 엔진(들)(130A1 및/또는 130A2)은 다양한 오디오 프로세싱 기법을 사용하여 합성 스피치 오디오 데이터가 생성된 후 결정된 운율 프로퍼티들을 통합하기 위해 합성 스피치 오디오 데이터를 추가로 프로세싱(예를 들어, 후처리)할 수 있다. 따라서, 합성 스피치 오디오 데이터에 포함된 합성 스피치는 클라이언트 디바이스(110)의 주어진 사용자의 음성 입력의 기준 합성 스피치 콘텐츠를 나타내며(예를 들어, 텍스트 세그먼트(들)을 통해), 추가 프로세싱은 합성 스피치를 조정하여 음성 입력이 수신되는 다양한 시나리오에 대한 다양한 운율 프로퍼티들을 나타낸다(예를 들어, 도 3 및 4a-4f와 관련하여 설명된 바와 같음). 또 다른 구현예에서, 스피치 합성 엔진(들)(130A1 및/또는 130A2)은 스피치 합성 모델(들)(130A)을 사용하여, 클라이언트 디바이스(110)의 주어진 사용자의 음성 입력에 대응하는 텍스트 세그먼트(들)를 스피치 합성 모델(들)(130A)에 걸친 입력으로서 적용함으로써 합성 스피치 오디오 데이터를 생성할 수 있다. 상기 구현예 중 일부 버전에서, 스피치 합성 모델(들)은 합성 스피치를 생성하기 위해 스피치 합성 모델(들)(130A)과 연관된 화자 임베딩을 활용한다. 상기 구현예 중 일부 버전에서, 운율 프로퍼티들은 또한 상기 기술된 바와 같이, 텍스트 세그먼트(들)와 함께 스피치 합성 모델(들)(130A)에 걸쳐 입력으로서 적용될 수 있거나 또는 상기 기술된 바와 같이 운율 프로퍼티들은 후-처리를 사용하여 합성 스피치에 통합될 수 있다.In some implementations, speech synthesis engine(s) 130A1 and/or 130A2 may use speech synthesis model(s) 130A to determine speaker embeddings associated with a given user of
일부 구현예에서, 클라이언트 디바이스(110)는 렌더링 엔진(113)을 사용하여 그리고 클라이언트 디바이스 및/또는 추가 클라이언트 디바이스(들)의 스피커(들)에 의해 합성 스피치 오디오 데이터에 포함된, 클라이언트 디바이스(110)의 주어진 사용자로부터의 음성 입력을 나타내는 합성 스피치를 청각적으로 렌더링할 수 있다. 합성 스피치는 결정된 운율 프로퍼티들로 합성되고, 클라이언트 디바이스(110)의 주어진 사용자의 음성 입력에 대응하는 텍스트 세그먼트(들)를 포함하고, 대화에서 정보를 전달하거나 제공하기 위해 주어진 사용자의 실제 스피치에 대한 프록시로서 사용될 수 있다. 일부 다른 구현예에서, 스피치 어시스턴트 엔진(160)은 네트워크(들)(190)를 통해 클라이언트 디바이스(110)에 합성 스피치 오디오 데이터를 전송할 수 있다. 상기 구현예 중 일부 버전에서, 합성 스피치 오디오 데이터에 포함된 합성 스피치는 클라이언트 디바이스(110)에서 수신되는 합성 스피치 오디오 데이터에 응답하여 렌더링 엔진(113)을 사용하여 클라이언트 디바이스 및/또는 추가 클라이언트 디바이스(들)의 스피커(들)에 의해 청각적으로 렌더링될 수 있다. 본 명세서에 기술된 기법은 합성 스피치가 주어진 사용자의 실제 스피치에 대한 프록시로 사용될 수 있고, 클라이언트 디바이스(110)의 주어진 사용자의 음성 입력이 수신되는 다양한 상이한 시나리오들에 동적으로 적응될 수 있으므로 클라이언트 디바이스(110)의 주어진 사용자가 언어 장애가 있는 경우 유리할 수 있다(예를 들어, 도 3 및 4a-4f와 관련하여 설명된 바와 같이). 특히, 일부 구현예에서, 사용자 인터페이스 입력 엔진(111)에 의해 검출된 음성 입력은 제1 언어이고, 사용자 인터페이스 입력 엔진(111)에 의해 검출된 음성 입력에 대응하는 생성된 합성 스피치도 제1 언어이다. In some implementations, the
자동 제안 엔진(150)은 대화에서 전달될 후보 텍스트 세그먼트(들)를 결정할 수 있고, 클라이언트 디바이스(110)의 주어진 사용자에 대한 합성 스피치에 포함하기 위한 제안(들)으로서 후보 텍스트 세그먼트(들)를 제시할 수 있다. 일부 구현예에서, 후보 텍스트 세그먼트(들)는 네트워크(190)를 통해 클라이언트 디바이스(110)에 전송될 수 있다. 또한, 후보 텍스트 세그먼트(들)는 클라이언트 디바이스(110)의 주어진 사용자에게 제시하기 위한 제안(들)으로서 렌더링 엔진(113)을 사용하여 클라이언트 디바이스(110)의 사용자 인터페이스에 의해 시각적으로 렌더링된다. 상기 구현예 중 일부 버전에서, 제안(들)은 사용자 인터페이스 입력이 상기 제안을 지시하는 경우(예를 들어, 사용자 인터페이스 입력 엔진(111)에 의해 결정된 바와 같이), 선택된 제안이 클라이언트 디바이스(110)에 의해 청각적으로 렌더링되는 합성 스피치에 통합될 수 있도록 선택가능할 수 있다. 이러한 방식으로, 클라이언트 디바이스(110)의 주어진 사용자는 제안(들) 중 주어진 하나의 선택을 선택할 수 있고, 선택된 제안(들)의 후보 텍스트 세그먼트(들)을 포함하는 클라이언트 디바이스(110)의 주어진 사용자에 대한 합성 스피치가 생성될 수 있다. 특히, 클라이언트 디바이스(110)의 주어진 사용자의 스피치를 나타내고 선택된 후보 텍스트 세그먼트(들)를 포함하는 합성 스피치는 주어진 사용자가 선택된 후보 텍스트 세그먼트(들)에 대응하는 어떠한 스피치 입력도 제공하지 않은 경우라도 생성될 수 있다. 따라서, 클라이언트 디바이스(110)의 주어진 사용자에 대한 합성 스피치는 여전히 주어진 사용자의 스피치를 나타낼 수 있는데, 이는 합성 스피치가 클라이언트 디바이스(110)의 주어진 사용자의 화자 임베딩을 사용하여 합성되기 때문이다(예를 들어, 주어진 사용자의 실제 스피치 또는 주어진 사용자의 실제 스피치와 일치하지 않지만 사용자의 피드백에 기초하여 선택된 것).The auto-suggestion engine 150 may determine candidate text segment(s) to be conveyed in the conversation, and may select the candidate text segment(s) as suggestion(s) for inclusion in synthesized speech for a given user of the
자동 제안 엔진(150)은 클라이언트 디바이스(110)의 주어진 사용자와 상기 주어진 사용자와의 대화에 참여하는 추가 참가자(들) 간의 대화의 컨텍스트에 기초하여 후보 텍스트 세그먼트(들)를 생성할 수 있다. 일부 구현예에서, 자동 제안 엔진(150)은 클라이언트 디바이스(110)에서 임의의 음성 입력을 검출하기 전에 클라이언트 디바이스(110)의 주어진 사용자에 의해 대화를 개시하는 컨텍스트에서 콘텐츠를 포함하는 후보 텍스트 세그먼트(들)를 생성할 수 있다. 상기 구현예 중 일부 버전에서, 자동 제안 엔진(150)은 하나 이상의 소프트웨어 애플리케이션이 시작되는 것에 응답하여 대화를 개시하기 위한 콘텐츠를 포함하는 후보 텍스트 세그먼트(들)를 생성할 수 있다. 예를 들어, 클라이언트 디바이스(110)의 주어진 사용자가 자동화된 어시스턴트 애플리케이션을 시작하고 클라이언트 디바이스가 사용자에게 속하지 않은 음성 입력을 검출하지 못한 경우, 대화(예: “Hello, how are you doing?” “Hi, how has your day been?”, “What’s up?” 등)를 개시하기 위한 후보 텍스트 세그먼트(들)이 클라이언트 디바이스(110)의 사용자 인터페이스에 의해 주어진 사용자에게 제시하기 위해 시각적으로 렌더링될 수 있다. 또한, 후보 텍스트 세그먼트(들)를 포함하는 콘텐츠는 추가 참가자(들)가 알려진 사용자인 경우 대화에서 추가 참가자(들)를 식별할 수 있다(예:“Hi, Jim”, “What’s up, Jim?” 등). 상기 구현예 중 일부 다른 버전에서, 자동 제안 엔진(150)은 음성 입력을 제공하기 위한 사용자 인터페이스 입력이 클라이언트 디바이스(110)에서 검출되지만 사용자 인터페이스 입력을 수신하는 임계 시간(예를 들어, 3초, 4초, 5초 등) 내에 클라이언트 디바이스(110)에서 음성 입력이 수신되지 않았다고 결정함에 응답하여 대화를 개시하기 위한 콘텐츠를 포함하는 후보 텍스트 세그먼트(들)를 생성할 수 있다. 예를 들어, 클라이언트 디바이스(110)의 주어진 사용자가 그래픽 엘리먼트에 대한 사용자 인터페이스 입력을 지시하는 경우, 선택시 주어진 사용자가 음성을 제공할 수 있게 하지만 그래픽 엘리먼트를 선택한 후 4초 이내에 음성 입력이 수신되지 않은 경우, 대화를 개시하기 위한 텍스트 세그먼트(들)가 클라이언트 디바이스(110)의 사용자 인터페이스에 의해 주어진 사용자에게 제시하기 위해 시각적으로 렌더링될 수 있다. The auto-suggestion engine 150 may generate the candidate text segment(s) based on the context of a conversation between a given user of the
일부 구현예에서, 자동 제안 엔진(150)은 클라이언트 디바이스(110)의 주어진 사용자와의 대화에 참여한 추가 참가자(들)의 음성 입력에 응답하는 컨텍스트의 콘텐츠를 포함하는 후보 텍스트 세그먼트(들)(예를 들어, 추가 참가자(들)의 음성 입력에 대응하는 텍스트 세그먼트(들)에 응답하는 후보 텍스트 세그먼트(들))을 생성할 수 있다. 상기 구현예 중 일부 버전에서, 자동 제안 엔진(150)은 예측 모델(들)을 사용하여 추가 참가자(들)의 음성 입력에 대응하는 텍스트 세그먼트(들)의 피처들을 예측 모델(들) 전반에 걸쳐 입력으로서 적용하는 것에 기초하여 후보 텍스트 세그먼트(들)를 생성할 수 있다. 예측 모델(들)은 예를 들어 LSTM 신경 네트워크 모델 및/또는 텍스트 세그먼트(들)의 큰 코퍼스 및 텍스트 세그먼트(들)에 대한 응답인 복수의 사용자들로부터의 대응하는 텍스트 세그먼트(들)에 기초하여 트레이닝된 다른 기계 학습 모델을 포함할 수 있다. 텍스트 세그먼트(들)의 피처들은 텍스트 세그먼트(들)로부터의 토큰(예를 들어, 텍스트 토큰(들) 및/또는 그래픽 토큰(들))을 포함할 수 있고, 자동 제안 엔진(150)은 후보 텍스트 세그먼트(들)를 나타내는 출력을 생성하기 위해 상기 토큰을 하나 이상의 예측 모델들에 걸쳐 입력으로서 적용한다. 텍스트 세그먼트(들)로부터 적용된 토큰은 모든 토큰들, 또는 토큰들의 하나 이상의 서브세트(예: 텍스트 세그먼트(들)에 포함된 첫 번째 문장 및/또는 마지막 문장, 정지 단어가 생략된 텍스트 세그먼트(들) 전부)를 포함할 수 있다. 그런 다음 자동 제안 엔진(150)은 출력을 사용하여 후보 텍스트 세그먼트(들)를 결정한다. 예를 들어, 자동 제안 엔진(150)은 단어에 대한 확률 분포를 각각 나타내는 하나 이상의 출력을 모델에 대해 생성하기 위해 토큰 기반으로 순차적인 토큰에 대해 예측 모델(들)에 걸쳐 토큰을 입력으로서 적용할 수 있다. In some implementations, the auto-suggestion engine 150 provides candidate text segment(s) (e.g., candidate text segment(s) containing content in context responsive to voice input of additional participant(s) participating in a conversation with a given user of the client device 110 ). For example, candidate text segment(s) may be generated that respond to text segment(s) corresponding to the voice input of the additional participant(s). In some versions of the above implementations, the auto-suggestion engine 150 uses the predictive model(s) to map features of the text segment(s) corresponding to the speech input of the additional participant(s) throughout the predictive model(s). Can generate candidate text segment(s) based on application as input. The predictive model(s) may be based on, for example, an LSTM neural network model and/or a large corpus of text segment(s) and corresponding text segment(s) from a plurality of users in response to the text segment(s). It can include other trained machine learning models. The features of the text segment(s) may include tokens (eg, text token(s) and/or graphic token(s)) from the text segment(s), and the auto-suggestion engine 150 provides the candidate text Apply the token as input across one or more predictive models to produce an output representing the segment(s). A token applied from a text segment(s) may be all tokens, or one or more subsets of tokens (e.g., the first and/or last sentence included in the text segment(s), text segment(s) with stop words omitted) all) may be included. The auto-suggestion engine 150 then uses the output to determine candidate text segment(s). For example, the auto-suggestion engine 150 may apply the token as input across the predictive model(s) to sequential tokens on a token-by-token basis to produce to the model one or more outputs, each representing a probability distribution for a word. can
자동 제안 엔진(150)은 예측 모델(들)의 출력(들)을 활용하여 후보 텍스트 세그먼트(들)의 세트를 결정하고, 후보 텍스트 세그먼트(들)에 점수를 할당(또는 달리 순위 지정)한다. 일부 구현예에서, 후보 텍스트 세그먼트(들)는 공통 응답의 선별된 목록과 같은 후보 텍스트 세그먼트(들)의 화이트리스트로부터 식별된다. 일부 구현예에서, 후보 텍스트 세그먼트(들) 중 하나 이상은 추가로 또는 대안적으로 토큰 방식(즉, 공통 응답의 화이트리스트로부터 식별되지 않음)에 의해 무제한 토큰으로 생성될 수 있다. 활용된 기법(들)에 관계없이, 자동 제안 엔진(150)은 다수의 후보 텍스트 세그먼트(들)(예를 들어, 3, 5, 10 또는 그 이상)를 식별할 수 있고, 선택적으로 예를 들어, 예측 모델(들)에 대해 생성된 출력에 기초하여 후보 텍스트 세그먼트(들)에 할당된 점수에 기초하여 식별된 후보 텍스트 세그먼트(들)에 순위를 매길 수 있다. 후보 텍스트 세그먼트(들)의 순위는 후보 텍스트 세그먼트(들)에 대한 점수, 후보 텍스트 세그먼트(들)의 순서(예를 들어, 제1 후보 텍스트 세그먼트가 가장 높은 순위인 목록, 제2 후보 텍스트 세그먼트가 다음으로 가장 높은 순위 등)로 전달될 수 있다. The auto-suggestion engine 150 utilizes the output(s) of the predictive model(s) to determine a set of candidate text segment(s) and assigns (or otherwise ranks) a score to the candidate text segment(s). In some implementations, the candidate text segment(s) are identified from a whitelist of candidate text segment(s), such as a curated list of common responses. In some implementations, one or more of the candidate text segment(s) may additionally or alternatively be tokenized (ie, not identified from a whitelist of common responses) as unlimited tokens. Regardless of the technique(s) utilized, the auto-suggestion engine 150 may identify a number of candidate text segment(s) (eg, 3, 5, 10 or more), and optionally, for example, , rank the identified candidate text segment(s) based on the score assigned to the candidate text segment(s) based on the output generated for the predictive model(s). The ranking of the candidate text segment(s) is based on the score for the candidate text segment(s), the order of the candidate text segment(s) (e.g., a list in which the first candidate text segment is ranked highest, the second candidate text segment is next highest rank, etc.).
후보 텍스트 세그먼트(들)는 클라이언트 디바이스(110)의 주어진 사용자에게 주어진 사용자에 대한 합성 스피치에 통합될 제안(들)으로서 제시하기 위해 시각적으로 렌더링될 수 있다. 일부 구현예에서, 자동 제안 엔진(150)은 최대 X개의 후보 텍스트 세그먼트(들)를 제안(들)(여기서 X는 1보다 크거나 같은 양의 정수임)으로 식별하고 및/또는 임계값을 사용하여 예측 모델(들)을 통해 생성된 출력에 대해 최소한 임계값 정도의 일치도를 갖는 제안(들)으로서 후보 텍스트 세그먼트(들)만 식별한다. 상기 구현예 중 일부 버전에서, 제안(들) 중 하나 이상은 (예를 들어, 도 4b, 4c 및 4f와 관련하여 본 명세서에서 더 상세하게 설명된 바와 같이) 다른 제안(들) 중 하나 이상보다 시각적으로 더 두드러지게 렌더링될 수 있다. 상기 구현예 중 일부 버전에서, 클라이언트 디바이스(110)에서 시각적으로 렌더링되는 제안(들)은 의미적으로 다양한 후보 텍스트 세그먼트(들)만을 포함할 수 있다. 자동 제안 엔진(150)은 인코더 모델을 통해 복수의 후보 텍스트 세그먼트(들) 중 하나에 각각 기초하는 복수의 후보 텍스트 세그먼트 임베딩을 생성하는 것에 기초하여 후보 텍스트 세그먼트(들)이 의미적으로 다양하다고 결정할 수 있다. 생성된 후보 텍스트 세그먼트 임베딩은 후보 텍스트 세그먼트(들)를 저차원 후보 텍스트 세그먼트 임베딩 공간에 매핑하는 저차원 표현일 수 있다. 후보 텍스트 세그먼트(들) 중 주어진 하나에 대한 임베딩은 제안(들) 중 하나로 이미 선택된 후보 텍스트 세그먼트(들)의 임베딩(들)과 비교될 수 있고, 후보 텍스트 세그먼트(들) 중 주어진 하나는 비교 결과 차이 메트릭(들)이 충족되었음을 나타내는 경우에만 제안으로 식별될 수 있다. 예를 들어, 차이 메트릭(들)은 주어진 후보 텍스트 세그먼트와 이미 선택된 후보 텍스트 세그먼트 간의 시맨틱 차이의 충분한 정도를 나타내는 임계값의 만족일 수 있다. 의미적으로 다양한 후보 텍스트 세그먼트(들)를 제안(들)으로 결정하고 제공하는 것은 후보 텍스트 세그먼트(들) 중 하나가 사용자의 의도된 응답의 본질을 전달하기에 충분할 가능성을 증가시킬 수 있다. 그 결과, 사용자가 합성 스피치에 포함시키기 위해 후보 텍스트 세그먼트(들) 중 하나를 선택할 가능성이 증가할 수 있다. 또한, 제안(들) 중 하나를 선택하면 사용자가 반드시 입력해야 하는 사용자 입력의 수를 줄일 수 있으며, 이는 다양한 클라이언트 디바이스 계산 리소스의 사용을 줄이고 및/또는 손재주가 낮은 사용자(또는 그렇지 않으면 클라이언트 디바이스(110)에 대한 사용자 입력을 만드는 데 어려움이 있는 사람)에게 특히 유익하다.The candidate text segment(s) may be rendered visually for presentation to a given user of the
더욱이, 일부 구현예에서, 후보 텍스트 세그먼트(들)는 클라이언트 디바이스의 주어진 사용자와 대화의 추가 참가자(들) 간의 대화 환경 및/또는 클라이언트 디바이스의 주어진 사용자와 대화의 추가 참가자(들) 간의 관계에 기초하여 결정될 수 있다. 예를 들어, 대화가 발생하고 주어진 사용자의 집이 주어진 사용자와 주어진 사용자의 딸 사이의 대화인 경우, 결정된 후보 텍스트 세그먼트(들)는 더 유쾌하고 가벼운 텍스트 세그먼트(들)를 포함할 수 있다. 다른 예로서, 대화가 사용자의 집에서 발생하고 주어진 사용자와 주어진 사용자의 친구 사이의 대화인 경우, 결정된 후보 텍스트 세그먼트(들)는 더 많은 속어 및 풍자 텍스트 세그먼트(들)를 포함할 수 있다. 대조적으로, 대화가 커피숍에서 발생하고 주어진 사용자와 주어진 사용자의 친구 사이의 대화인 경우, 결정된 후보 텍스트 세그먼트(들)는 텍스트 세그먼트(들)에 더 적은 속어를 포함할 수 있지만 여전히 풍자적인 텍스트 세그먼트(들)를 포함할 수 있다. 일부 구현예에서, 텍스트 세그먼트가 유쾌한지, 냉소적인지 또는 다른 유형(들)을 갖는지를 결정하는 것은 텍스트 세그먼트에 대한 임베딩에 기초할 수 있다. 예를 들어, 텍스트 세그먼트에 대한 임베딩은 이전 단락에서 설명된 바와 같이 결정할 수 있다. 더욱이, 그 임베딩, 또는 그 임베딩이 포함된 임베딩 공간의 영역은 하나 이상의 유형(예를 들어, 냉소적)으로 라벨링될 수 있으며, 이에 의해 임베딩(및 따라서 텍스트 세그먼트)이 유형(들)을 갖는다는 것을 나타낸다. 따라서, 풍자적 텍스트 세그먼트는 텍스트 세그먼트에 대한 임베딩이 풍자 라벨에 매핑된다는 결정에 기초하여, 복수의 후보 응답으로부터 선택될 수 있다.Moreover, in some implementations, the candidate text segment(s) are based on a conversation environment between the given user of the client device and the additional participant(s) of the conversation and/or the relationship between the given user of the client device and the additional participant(s) of the conversation. can be determined by For example, if the conversation takes place and the given user's home is a conversation between the given user and the given user's daughter, the determined candidate text segment(s) may include the more pleasant and light text segment(s). As another example, if the conversation occurs at the user's home and is a conversation between the given user and a friend of the given user, the determined candidate text segment(s) may include more slang and satire text segment(s). In contrast, if the conversation occurs in a coffee shop and is a conversation between a given user and a given user's friend, the determined candidate text segment(s) may contain less slang in the text segment(s) but still satirical text segments may include (s). In some implementations, determining whether a text segment is jolly, cynical, or of other type(s) may be based on embeddings for the text segment. For example, embeddings for text segments may be determined as described in the previous paragraph. Moreover, the embedding, or the region of the embedding space in which the embedding is contained, may be labeled with one or more types (e.g., cynical), thereby indicating that the embedding (and thus the text segment) has type(s). indicates. Accordingly, a satirical text segment may be selected from a plurality of candidate responses based on a determination that an embedding for the text segment maps to a satirical label.
후보 텍스트 세그먼트(들)가 결정됨에 따라, 후보 텍스트 세그먼트(들)는 이전에 결정된 후보 텍스트 세그먼트(들)에 빠르고 효율적으로 액세스하기 위해 후보 텍스트 세그먼트(들) 데이터베이스(150A)에 저장될 수 있다. 또한, 후보 텍스트 세그먼트(들) 데이터베이스(150A)에 저장된 후보 텍스트 세그먼트(들)는 특정 후보 텍스트 세그먼트(들)의 유형 및/또는 사용 빈도에 의해 인덱싱될 수 있다. 후보 텍스트 세그먼트(들)의 유형은 예를 들어, 대화를 개시하는 후보 텍스트 세그먼트(들), 웃음을 포함하는 후보 텍스트 세그먼트(들), 집 환경과 연관된 후보 텍스트 세그먼트(들), 직장 환경과 연관된 후보 텍스트 세그먼트(들), 공공 환경과 연관된 후보 텍스트 세그먼트(들), 음식점 서비스와 연관된 후보 텍스트 세그먼트(들)(예: “Water, please”, “I’ll have the cheeseburger” 등), 친구와 연관된 후보 텍스트 세그먼트(들), 가족과 연관된 후보 텍스트 세그먼트(들) 및/또는 후보 텍스트 세그먼트(들)을 인덱싱하기 위한 기타 유형들을 포함할 수 있다. 스피치 어시스턴트 시스템(160)을 사용함으로써, 자동 제안 엔진(150)은 클라이언트 디바이스(110)의 주어진 사용자에 의해 일반적으로 사용되는 일반적인 용어(들), 문구(들), 응답(들), 속어 및/또는 기타 스피치를 학습할 수 있고, 후보 텍스트 세그먼트(들) 데이터베이스(150A) 또는 그 인덱스는 이러한 텍스트 세그먼트(들)를 포함하도록 업데이트될 수 있다. 이러한 방식으로, 스피치 어시스턴트 시스템(160)은 클라이언트 디바이스의 사용자에게 더 유용한 제안(들)을 제공할 수 있고, 결과적으로 음성 사용자 입력은 클라이언트 디바이스(110)에서 감소될 수 있고, 음성 사용자 입력을 프로세싱하기 위한 대응하는 계산 리소스(들) (및/또는 그 전송을 위한 네트워크 리소스)가 감소될 수 있다.As the candidate text segment(s) are determined, the candidate text segment(s) may be stored in the candidate text segment(s)
도 2a 및 도 2b는 다양한 구현예에 따라 하나 이상의 운율 프로퍼티들로 합성되는 합성 스피치를 생성하는 예시적 방법(200A 및 200B)을 도시하는 흐름도를 도시한다. 편의상, 방법(200A 및 200B)의 동작은 동작들을 수행하는 시스템을 참조하여 기술된다. 방법(200A 및 200B)의 이 시스템은 컴퓨팅 디바이스(들)(예를 들어, 도 1의 클라이언트 디바이스(110), 도 3 및 4a 내지 4d의 클라이언트 디바이스(410), 도 5의 클라이언트 디바이스(510), 하나 이상의 서버 및/또는 기타 컴퓨팅 디바이스)의 하나 이상의 프로세서 및/또는 다른 컴포넌트(들)을 포함한다. 추가로, 방법(200A 및 200B)의 동작들이 특정한 순서로 도시되었지만, 이는 그러한 제한을 의미하지 않는다. 하나 이상의 동작은 재순서화, 생략 또는 추가될 수 있다.2A and 2B show flow diagrams illustrating
방법(200A)의 동작이 방법(200B)의 동작 이전에 발생하는 것으로 여기에서 설명되지만, 이는 단지 설명의 편의를 위한 것이며 제한하려는 것이 아니다. 방법(200A)의 동작은 방법(200B)의 동작 이전에 수행될 수 있고, 방법(200B)의 동작은 방법(200A)의 동작보다 먼저 수행될 수 있거나, 방법(200A 및 200B)의 동작은 병렬로 수행될 수 있다는 점에 유의해야 한다. 다양한 구현예에서, 방법(200A 및 200B)의 동작 순서는 주어진 사용자와 대화의 하나 이상의 추가 참가자 간의 대화의 흐름에 기초할 수 있다. 하나의 비제한적인 예로서, 주어진 사용자가 대화를 개시하면, 방법(200A)의 동작은 방법(200B)의 동작 이전에 수행되어 하나 이상의 운율 프로퍼티들로 합성된 합성 스피치를 생성함으로써 주어진 사용자에 대한 스피치 지원을 제공할 수 있다. 다른 비제한적인 예로서, 주어진 사용자가 대화에서 하나 이상의 추가 참가자에게 응답하고 있다면, 방법(200B)의 동작은 방법(200A)의 동작 이전에 수행되어 하나 이상의 운율 프로퍼티들로 합성된 합성 스피치를 생성함으로써 주어진 사용자에 대한 스피치 지원을 제공할 수 있다. 그러나, 사용자 간의 대화의 동적 특성이 주어지면 방법(200A 및 200B)의 동작은 주어진 사용자와 하나 이상의 추가 참가자가 대화에 참여함에 따라 병렬로 수행될 수도 있다. Although the operation of
초기 두 도면 도 2a 및 도 2b로 돌아가면, 도 2a의 블록 252A에서, 시스템은 주어진 사용자의 클라이언트 디바이스에서 사용자 인터페이스 입력을 모니터링한다. 주어진 사용자의 클라이언트 디바이스에서의 사용자 인터페이스 입력은 주어진 사용자의 음성 입력 및/또는 텍스트 세그먼트를 전달하는 그래픽 엘리먼트의 선택을 포함할 수 있다. 사용자 인터페이스 입력이 주어진 사용자의 음성 입력인 구현예에서, 음성 입력은 클라이언트 디바이스의 하나 이상의 마이크로폰들을 통해 검출될 수 있고, 음성 입력에 포함된 특정한 핫워드 또는 문구를 검출함에 응답하여(예를 들어, "Ok, Assistant", "Assistant" 및/또는 기타 특정한 핫워드들 또는 문구들을 검출함에 응답하여), 하나 이상의 핫워드-프리 기법을 사용하여 임의의 특정한 핫워드들 또는 문구들 없이 음성 입력을 검출함에 응답하여(예를 들어 주어진 사용자의 입 또는 입술 움직임(들), 클라이언트 디바이스를 향하는 주어진 사용자의 시선(들), 클라이언트 디바이스를 향하는 주어진 사용자의 제스처(들) 및/또는 기타 핫워드-프리 기법들을 검출함에 응답하여), 또는 음성 입력을 제공하기 위한 표시를 수신함에 응답하여(예를 들어, 클라이언트 디바이스의 하드웨어 또는 소프트웨어 버튼의 선택, 클라이언트 디바이스의 스퀴즈 및/또는 음성 입력을 제공하기 위한 기타 표시) 검출될 수 있다. 또한, 음성 입력은 하나 이상의 스피치 인식 모델(예를 들어, 도 1의 스피치 인식 모델(들)(120A))을 사용하여 프로세싱될 수 있다. 사용자 인터페이스 입력이 텍스트 세그먼트를 전달하는 그래픽 엘리먼트의 선택인 구현예에서, 그래픽 엘리먼트는 클라이언트 디바이스의 사용자 인터페이스에 디스플레이되는 복수의 그래픽 엘리먼트들 중 하나일 수 있고, 텍스트 세그먼트는 대화의 하나 이상의 추가 참가자와 대화를 개시하거나 대화의 추가 참가자 중 하나 이상의 추가 사용자 음성 입력에 응답하는 텍스트 세그먼트일 수 있다. Initial Two Figures Turning to FIGS. 2A and 2B , at
만약, 도 2a의 블록(252A)의 반복에서, 시스템이 주어진 사용자의 클라이언트 디바이스에서 사용자 인터페이스 입력이 검출되지 않는 것으로 결정하면, 시스템은 도 2b의 블록(252B)로 진행할 수 있다. 도 2a의 블록(252A) 및 도 2b의 블록(252b)는 병렬로 수행될 수 있다. 도 2b의 블록(252B)에서, 시스템은 클라이언트 디바이스에서 하나 이상의 다른 사용자들(예를 들어, 대화의 추가 참가자(들))의 음성 활동을 모니터링한다. 특히, 음성 활동은 클라이언트 디바이스의 하나 이상의 마이크로폰들을 통해 주어진 사용자의 클라이언트 디바이스 환경에서 검출될 수 있지만, 클라이언트 디바이스의 주어진 사용자로부터의 것은 아니다. 화자 식별에 기초하여 주어진 사용자로부터의 것이 아닌 음성 활동이 있다고 결정하는 것은 상기 클라이언트 디바이스에서, 상기 추가 사용자 음성 입력 프로세싱에 기초하여 음성 입력 임베딩을 생성하는 단계; 및 상기 음성 입력 임베딩이 상기 주어진 사용자에 대해 미리 저장된 임베딩과 일치하지 않는지 결정하는 단계를 포함할 수 있다. 오히려, 음성 활동은 주어진 사용자의 클라이언트 디바이스의 환경에 있는 추가 사용자(들)로부터 발생한다. 일부 구현예에서, 음성 활동은 클라이언트 디바이스의 주어진 사용자에 대한 추가적인 사용자 음성 입력을 포함하고, 음성 활동은 하나 이상의 스피치 인식 모델(예를 들어, 도 1의 스피치 인식 모델(들)(120A))을 사용하여 프로세싱될 수 있다.If, at iteration of
만약, 도 2b의 블록(252B)의 반복에서, 시스템이 하나 이상의 다른 사용자들의 사용자 음성 활동이 주어진 사용자의 클라이언트 디바이스에서 검출된다고 결정하면, 시스템은 도 2b의 블록(254BA)로 진행할 수 있다. 그러나, 도 2b의 블록(252B)의 반복에서, 시스템이 하나 이상의 다른 사용자들의 사용자 음성 활동이 주어진 사용자의 클라이언트 디바이스에서 검출되지 않는다고 결정하면, 시스템은 도 2a의 블록(252A)으로 돌아갈 수 있다. s이러한 방식으로, 시스템은 적어도 선택적으로(예를 들어, 사용자 입력에 기초하여 활성화될 때) 도 2a의 블록(252A)에서 클라이언트 디바이스에서 주어진 사용자의 사용자 인터페이스 입력 및/또는 도 2b의 블록(252B)에서 주어진 사용자의 클라이언트 디바이스의 환경에서 다른 사용자들의 음성 활동을 지속적으로 모니터링할 수 있다. 도 2a의 블록(252A)에서 주어진 사용자의 사용자 인터페이스 입력 및 도 2b의 블록(252B)에서 다른 사용자들의 음성 활동을 지속적으로 모니터링함으로써, 본 명세서에 기술된 기법은 주어진 사용자와 하나 이상의 다른 사용자 간의 대화에서 동적으로 적응될 수 있는 주어진 사용자에 대한 스피치 지원을 제공한다. If, in repetition of
이제 도 2a로 돌아가면, 블록(252A)의 반복에서, 시스템이 사용자 인터페이스 입력이 주어진 사용자의 클라이언트 디바이스에서 검출되었다고 결정하면, 시스템은 블록(254A)으로 진행할 수 있다. 블록(254A)에서, 시스템은 주어진 사용자의 클라이언트 디바이스에서 사용자 인터페이스 입력에 기초하여 대화에서 전달하기 위한 텍스트 세그먼트를 결정한다. 일부 구현예에서, 블록(254A)은 하나 이상의 서브-블록들을 포함할 수 있다. 사용자 인터페이스 입력이 주어진 사용자의 음성 입력을 포함하는 경우, 서브블록(254A1)에서, 시스템은 스피치 인식 모델(예를 들어, 도 1의 스피치 인식 모델(들)(120A))을 사용하여 주어진 사용자의 음성 입력을 프로세싱하는 것에 기초하여 텍스트 세그먼트를 결정할 수 있다. 서브블록(254A1)을 포함하는 구현예에서, 주어진 사용자의 클라이언트 디바이스는 하나 이상의 마이크로폰을 사용하여 주어진 사용자의 음성 입력을 검출할 수 있다. 사용자 인터페이스 입력이 그래픽 엘리먼트의 선택을 포함하는 경우, 서브블록(254A2)에서, 시스템은 텍스트 세그먼트를 전달하는 그래픽 엘리먼트의 선택에 기초하여 텍스트 세그먼트를 결정할 수 있다. 서브블록(254A2)을 포함하는 구현예에서, 그래픽 엘리먼트는 대화를 개시하라는 표시를 제공하는 주어진 사용자에 응답하여 및/또는 대화에서 하나 이상의 다른 사용자들의 음성 활동을 검출하는 것에 응답하여 렌더링될 수 있다. 또한, 그래픽 엘리먼트는 복수의 그래픽 엘리먼트들 중 하나일 수 있으며, 그래픽 엘리먼트들 각각은 주어진 사용자를 대신하여 대화를 개시하고 및/또는 대화의 하나 이상의 다른 사용자들의 음성 활동에 응답하는 후보 텍스트 세그먼트의 서로 다른 제안들을 포함할 수 있다. 후보 텍스트 세그먼트의 제안 및 그것의 선택은 본 명세서에서 보다 상세하게 설명된다(예를 들어, 도 1 및 4b, 4c 및 4f와 관련하여). 일부 구현예에서 제안들은 대화에서 주어진 사용자와 하나 이상의 다른 사용자들 간의 관계에 따라 달라질 수 있다. Turning now to FIG. 2A , if, at a repetition of
블록(256A)에서, 시스템은 대화에서 하나 이상의 추가 참가자들을 식별한다. 대화의 하나 이상의 추가 참가자들은 음성 활동이 주어진 사용자의 클라이언트 디바이스의 환경에서 검출된 다른 사용자들 중 하나 이상을 포함할 수 있다. 특히, 대화의 하나 이상의 추가 참가자들은 클라이언트 디바이스에서 사용자 인터페이스 입력에 대한 모니터링 및/또는 클라이언트 디바이스의 환경에서 다른 사용자들 중 하나 이상의 음성 활동에 대한 모니터링과 함께 위에서 설명된 바와 같이 대화의 이전 차례에 기초하여 이미 식별될 수 있다. 일부 구현예에서, 블록(256A)은 하나 이상의 서브-블록들을 포함할 수 있다. 시스템이 다른 사용자들 중 한 명 이상의 음성 활동을 검출한 경우, 서브블록(256A1)에서, 시스템은 주어진 사용자의 클라이언트 디바이스의 하나 이상의 마이크로폰들을 통해 검출된 추가 사용자 음성 입력에 화자 식별을 수행함으로써 상기 다름 사용자들 중 한 명 이상을 추가 참가자(들)로서 식별한다. 서브블록(256A1)을 포함하는 구현예에서, 추가 사용자 음성 입력은 하나 이상의 스피치 인식 모델(예를 들어, 도 1의 스피치 인식 모델(들)(120A))을 사용하여 프로세싱되어 추가 사용자 음성 입력을 제공한 추가 참가자(들) 중 대응하는 사람에 대한 화자 임베딩(또는 음성 입력 임베딩)을 생성한다. 본 명세서에 기술된 바와 같이(예를 들어, 도 1 및 2B와 관련하여)에 도시된 바와 같이, 화자 임베딩은 클라이언트 디바이스에 로컬적으로 저장된 복수의 (사전 저장된) 화자 임베딩들과 비교되어 추가 참가자(들) 중 대응하는 사람에 대한 일치하는 화자 임베딩이 클라이언트 디바이스에 로컬적으로 저장되어 있는지 여부를 식별할 수 있다. 다시 말해서, 음성 입력 임베딩은 추가 사용자 음성 입력의 프로세싱에 기초하여 생성될 수 있고, 음성 입력 임베딩이 추가 사용자 또는 참가자에 대해 미리 저장된 임베딩과 일치하는지 여부가 결정될 수 있고; 따라서 추가 참가자가 식별될 수 있다. 선택적으로 추가 사용자에 대한 사전 저장된 임베딩은 추가 사용자에 의한 승인에 응답하여 클라이언트 디바이스에 로컬적으로 미리 저장된다. 일부 예에서, 추가 사용자의 시맨틱 식별자가 저장되거나 사전 저장된 임베딩과 연관될 수 있다.At block 256A, the system identifies one or more additional participants in the conversation. One or more additional participants of the conversation may include one or more of other users whose voice activity has been detected in the environment of the given user's client device. In particular, the one or more additional participants of the conversation based on a previous turn of the conversation as described above with monitoring for user interface input at the client device and/or monitoring for voice activity of one or more of the other users in the environment of the client device. can already be identified. In some implementations, block 256A may include one or more sub-blocks. If the system has detected voice activity of one or more of the other users, then, at subblock 256A1, the system performs the other by performing speaker identification on additional user voice input detected via one or more microphones of the given user's client device. Identify one or more of the users as additional participant(s). In implementations including subblock 256A1 , the additional user voice input is processed using one or more speech recognition models (eg, speech recognition model(s) 120A of FIG. 1 ) to produce additional user voice input. Create speaker embeddings (or speech input embeddings) for corresponding ones of the additional participant(s) you provide. As described herein (eg, with respect to FIGS. 1 and 2B ), the speaker embedding is compared to a plurality of (pre-stored) speaker embeddings stored locally on the client device for additional participant identify whether the matching speaker embeddings for the corresponding one(s) are stored locally on the client device. In other words, the voice input embedding may be generated based on processing of the additional user's voice input, and it may be determined whether the voice input embedding matches a pre-stored embedding for the additional user or participant; Thus, additional participants may be identified. Optionally, the pre-stored embeddings for the additional user are pre-stored locally on the client device in response to approval by the additional user. In some examples, an additional user's semantic identifier may be stored or associated with a pre-stored embedding.
서브블록(256A2)에서, 시스템은 추가 참가자(들)를 포함하여 다수의 사람들을 캡처하는 이미지를 프로세싱함으로써 추가 참가자 중 하나 이상을 추가로 또는 대안적으로 식별할 수 있다. 예를 들어, 시스템은 한 명 이상의 다른 사용자들의 음성 활동이 서브 블록(256A2)에서 검출되지 않으면 서브 블록(256A2)을 수행할 수 있다. 이미지는 주어진 사용자의 클라이언트 디바이스의 하나 이상의 비전 센서들(예를 들어, 카메라(들))에 의해 캡처될 수 있다. 카메라는 조정 가능한 뷰포트를 포함할 수 있고, 이미지(또는 이미지들)는 조정 가능한 뷰포트의 조정 후에 캡처될 수 있다. 조정은 클라이언트 디바이스에서 사용자 인터페이스 입력에 대한 응답일 수 있다; 예를 들어, 사용자는 뷰포트의 각도를 조정하거나 한 명 이상의 참가자를 확대할 수 있다. 서브블록(256A1)을 포함하는 구현예에서, 이미지는 주어진 클라이언트 디바이스의 사용자 인터페이스에서 렌더링될 수 있고, 주어진 사용자는 추가 참가자들 중 대응하는 사람을 포함하는 이미지의 영역을 지정할 수 있고(예를 들어, 얼굴 선택, 얼굴 주위에 경계 상자를 그리는 것 및/또는 이미지의 영역을 지정하기 위한 다른 기법), 이미지의 영역은 하나 이상의 이미지 프로세싱 기법을 사용하여 프로세싱되어 추가 사용자들 중 대응하는 사람의 비주얼 임베딩을 생성할 수 있고; 따라서 사용자는 프로세싱으로부터 이미지 내의 다른 참가자 또는 사람을 제외하도록 지정할 수 있으며, 이는 그렇지 않으면 불필요하게 소비될 계산 리소스를 절약할 수 있다. 본 명세서에 기술된 바와 같이(예를 들어, 도 1 및 2B와 관련하여)에 도시된 바와 같이, 비주얼 임베딩은 클라이언트 디바이스에 로컬적으로 저장된 복수의 비주얼 임베딩들과 비교되어 추가 참가자(들) 중 대응하는 사람에 대한 일치하는 비주얼 임베딩이 클라이언트 디바이스에 로컬적으로 저장되어 있는지 여부를 식별할 수 있다. 일부 추가적 및/또는 대안적 구현예에서, 시스템은 또한 비전 데이터에서 캡처된 사용자(들)의 신체 자세 및/또는 머리 자세를 결정하기 위해 비전 데이터 프로세싱에 기초하여 대화에서 추가 참가자(들)을 식별할 수 있다. 예를 들어, 도 1과 관련하여 설명됨). 이러한 방식으로, 시스템은 대응하는 화자 임베딩(들) 및/또는 비주얼 임베딩(들)에 기초하여 추가 참가자(들) 각각을 식별할 수 있다. At subblock 256A2 , the system may further or alternatively identify one or more of the additional participants by processing an image that captures multiple people including the additional participant(s). For example, the system may perform sub-block 256A2 if no voice activity of one or more other users is detected in sub-block 256A2. The image may be captured by one or more vision sensors (eg, camera(s)) of a given user's client device. The camera may include an adjustable viewport, and an image (or images) may be captured after adjustment of the adjustable viewport. The adjustment may be in response to a user interface input at the client device; For example, the user can adjust the angle of the viewport or zoom in on one or more participants. In implementations including sub-block 256A1 , the image may be rendered in a user interface of a given client device, and the given user may specify an area of the image containing a corresponding one of the additional participants (eg, , face selection, drawing a bounding box around the face, and/or other techniques for specifying regions of the image), regions of the image may be processed using one or more image processing techniques to embed visual embeddings of corresponding ones of the additional users. can create; Thus, the user may specify to exclude other participants or persons in the image from processing, which may save computational resources that would otherwise be consumed unnecessarily. As described herein (eg, with respect to FIGS. 1 and 2B ), the visual embedding is compared to a plurality of visual embeddings stored locally on the client device to select among the additional participant(s). It may identify whether a matching visual embedding for the corresponding person is stored locally on the client device. In some additional and/or alternative implementations, the system further identifies additional participant(s) in the conversation based on the vision data processing to determine a body posture and/or head posture of the user(s) captured in the vision data. can do. For example, as described with respect to FIG. 1 ). In this manner, the system may identify each of the additional participant(s) based on the corresponding speaker embedding(s) and/or visual embedding(s).
블록(258A)에서, 시스템은 하나 이상의 운율 프로퍼티들을 결정한다. 본 명세서에 기술된 바와 같이(예를 들어, 도 3 및 도 4a 내지 도 4f), 블록(258A)에서 결정된 운율 프로퍼티들 중 하나 이상은 블록(256A)에서 식별된 대화의 추가 참가자들 중 하나 이상에 기초하여 변할 수 있다. 일부 구현예에서, 블록(258A)은 하나 이상의 서브-블록들을 포함할 수 있다. 서브블록(258A1)에서, 시스템은 주어진 사용자와 대화의 추가 참가자(들) 간의 관계의 하나 이상의 속성들에 기초하여 하나 이상의 운율 프로퍼티들을 결정할 수 있다. 서브블록(258A2)에서, 시스템은 주어진 사용자가 추가 참가자(들)과 대화에 참여하는 클라이언트 디바이스의 위치의 분류(들)(예를 들어, 집 환경 , 작업 환경, 공공 환경 등)에 기초하여 하나 이상의 운율 프로퍼티들을 결정할 수 있다. 하나 이상의 운율 프로퍼티들은 서브-블록(258A1), 서브-블록(258A2)에서의 결정 및/또는 다른 결정에 기초할 수 있다는 점에 유의해야 한다. 운율 프로퍼티들의 예 및 운율 프로퍼티들이 이러한 결정에 기초하여 어떻게 변하는지는 본 명세서에서 보다 상세하게 설명된다(예를 들어, 도 1, 3 및 4a-4f와 관련하여). At block 258A, the system determines one or more prosody properties. As described herein (eg, FIGS. 3 and 4A-4F ), one or more of the prosody properties determined at block 258A may include one or more of the additional participants of the conversation identified at block 256A. can be changed based on In some implementations, block 258A may include one or more sub-blocks. At subblock 258A1 , the system may determine one or more prosody properties based on one or more attributes of the relationship between the given user and the additional participant(s) of the conversation. At subblock 258A2, the system selects one based on the classification(s) (eg, home environment, work environment, public environment, etc.) of the location of the client device in which the given user participates in the conversation with the additional participant(s). The above prosody properties may be determined. It should be noted that the one or more prosody properties may be based on a determination in sub-block 258A1 , sub-block 258A2 , and/or other determination. Examples of prosody properties and how prosody properties change based on this determination are described in greater detail herein (eg, with respect to FIGS. 1 , 3 and 4A-4F ).
블록(260A)에서, 시스템은 블록(254A)에서 결정된 텍스트 세그먼트를 통합하고 블록(258A)에서 결정된 하나 이상의 운율 프로퍼티들로 합성되는 합성 스피치를 포함하는 합성 스피치 오디오 데이터를 생성한다. 일부 구현예에서, 합성 스피치 오디오 데이터를 생성하는 것은 주어진 사용자의 화자 임베딩, 텍스트 세그먼트(또는 그것의 음소들) 및 하나 이상의 운율 프로퍼티들을 스피치 합성 모델(예를 들어, 도 1의 스피치 합성 모델(들)(130A))에 걸쳐 입력으로서 적용하는 합성 스피치를 합성하는 것을 포함한다. 일부 다른 구현예에서, 합성 스피치 오디오 데이터를 생성하는 것은 주어진 사용자의 화자 임베딩 및 텍스트 세그먼트(또는 그것의 음소들)를 스피치 합성 모델(예를 들어, 도 1의 스피치 합성 모델(들)(130A))에 걸쳐 입력으로서 적용하는 합성 스피치를 합성하는 것을 포함한다. 상기 구현예 중 일부 추가 버전에서, 합성 스피치가 하나 이상의 운율 프로퍼티들로 합성되도록 합성 스피치 오디오 데이터가 생성된 후에 합성 스피치 오디오 데이터를 수정하기 위해 합성 스피치 오디오 데이터의 후처리에서 하나 이상의 운율 프로퍼티들이 활용될 수 있다. 임의의 구현예에서, 결과로 생성된 합성 스피치 오디오 데이터 및 합성 스피치는 주어진 사용자에 대해 특정적이고 고유하며, 블록(258A)에서 결정된 운율 프로퍼티들에 기초하여 변할 수 있는 합성 스피치를 나타낸다. 다양한 구현예에서, 블록(258)은 생략될 수 있고, 블록(260)은 운율 프로퍼티들을 가질 합성 스피치를 생성하는 것을 포함할 수 있지만, 이러한 운율 프로퍼티들은 블록(258)에 기초하여 결정된 것이 아닐 것이다(예를 들어, 그것들은 고정될 수 있음). At
블록(262A)에서, 시스템은 합성 스피치가 클라이언트 디바이스 및/또는 추가 클라이언트 디바이스의 하나 이상의 스피커들을 통해 렌더링되게 한다. 클라이언트 디바이스 및/또는 추가 클라이언트 디바이스의 스피커들 중 하나 이상을 통해 합성 스피치를 렌더링함으로써, 합성 스피치는 대화의 추가 참가자(들)가 청각적으로 인지할 수 있다. 사용자 인터페이스 입력이 주어진 사용자의 음성 입력인 구현예에서, 합성 스피치는 주어진 사용자가 음성 입력을 제공한 후에 렌더링될 수 있다. 사용자 인터페이스 입력이 주어진 사용자의 음성 입력인 일부 다른 구현예에서, 주어진 사용자가 음성 입력을 제공하는 동안 합성 스피치가 렌더링될 수 있다. 또한, 일부 구현예에서, 합성 스피치의 전사는 전사되고, 클라이언트 디바이스의 사용자 인터페이스 상에 디스플레이될 수 있다. 상기 구현예 중 일부 버전에서, 합성 스피치의 전사는 선택 가능하고, 선택될 때 시스템은 합성 스피치가 클라이언트 디바이스 및/또는 추가 클라이언트 디바이스의 하나 이상의 스피커들을 통해 다시 렌더링되게 하여, 주어진 사용자가 추가 음성 입력을 제공하지 않고도 합성 스피치를 반복할 수 있도록 한다. At
블록(264A)에서, 상기 기술된 도 2b의 블록(252B)과 유사하게, 시스템은 클라이언트 디바이스에서 하나 이상의 다른 사용자의 음성 활동(즉, 주어진 사용자의 음성 이외의 음성에 대해)을 모니터링한다. 블록(264A)의 반복에서, 시스템이 클라이언트 디바이스에서 다른 사용자(들)의 임의의 음성 활동(예를 들어, 적어도 대화의 추가 참가자(들) 포함)을 검출하지 않으면, 시스템은 블록(252A)으로 돌아가서, 주어진 사용자의 클라이언트 디바이스에서의 추가적인 사용자 인터페이스 입력 및/또는 위에서 설명된 방식으로 클라이언트 디바이스에서의 하나 이상의 다른 사용자의 음성 활동을 지속적으로 모니터링한다. 블록(264A)의 반복에서, 시스템이 클라이언트 디바이스에서 다른 사용자(들)의 음성 활동을 검출하면, 시스템은 블록(252B)를 우회하고 블록(254B)로 진행할 수 있다. At
이제 도 2b로 돌아가면, 블록(254B)에서, 시스템은 대화의 추가 참가자(들)를 식별한다. 시스템은 도 2a의 블록(256A)와 관련하여 설명된 것과 동일한 방식으로 대화의 추가 참가자(들)를 식별할 수 있다. 특히, 도 2a의 블록(256A)과 관련하여 설명된 바와 같이, 시스템은 대화에서 추가 참가자(들)의 스피치 임베딩 및/또는 비주얼 임베딩을 생성할 수 있다. 일부 구현예에서(예를 들어, 점선으로 표시된 바와 같이), 도 2a의 블록(264A) 또는 도 2b의 블록(252B)에서 검출된 음성 활동이 주변 음성활동인 경우와 같이(예를 들어, 음성 활동이 클라이언트 디바이스에서 검출되었지만 주어진 사용자를 향하거나 주어진 사용자 및/또는 대화의 임의의 다른 추가 참가자(들)로부터의 것이 아님) 다른 사용자(들)이 대화의 추가 참가자(들)로서 식별되지 않는 경우, 시스템은 도 2a의 블록(254A) 또는 도 2b의 블록(252B)으로 돌아가 상기 설명된 방식으로 주어진 사용자의 클라이언트 디바이스에서 추가 사용자 인터페이스 입력 및/또는 클라이언트 디바이스에서 하나 이상의 다른 사용자들의 음성 활동을 계속 모니터링한다. 이러한 방식으로, 시스템은 검출된 음성 활동의 오탐지를 식별할 수 있지만 주어진 사용자 또는 사용자와 추가 참가자(들) 간의 대화의 일부에 대한 것이 아닐 수 있다. Turning now to FIG. 2B , at
블록(256B)에서, 시스템은 다른 사용자(들)의 음성 활동을 전사하고, 그 전사를 음성 활동의 그래픽 표시로서 클라이언트 디바이스의 사용자 인터페이스에 디스플레이하며, 이 그래픽 표시는 다른 사용자의 시맨틱 표시 또는 식별자를 포함할 수 있다. 일부 구현예에서, 시스템은 다른 사용자(들)가 대화의 추가 참가자(들)인 경우 및/또는 다른 사용자(들)가 알려진 사용자(들)인 경우에만 다른 사용자(들)의 음성 활동을 전사한다(예를 들어, 도 2b의 블록(258B)와 관련하여 본 명세서에 설명된 바와 같이). 상기 구현예 중 일부 버전에서, 주어진 사용자의 클라이언트 디바이스는 클라이언트 디바이스에서 음성 활동이 검출되었다는 표시를 제공하고, 음성 활동이 클라이언트 디바이스에서 검출되었다는 표시에 대한 사용자 입력을 수신하는 것에 응답하여 전사가 제시된다(예를 들어, 도 4e 및 4f에 대해 설명된 바와 같이). 다른 구현예에서, 시스템은 다른 사용자(들)가 알려진 사용자(들)가 아닌 경우라도 다른 사용자(들)의 음성 활동을 전사한다. At
블록(258B)에서, 시스템은 대화의 추가 참가자(들)가 알려진 사용자(들)인지 여부를 결정한다. 본 명세서에 설명된 바와 같이(예를 들어, 도 2a의 블록(256A) 및 도 2b의 블록(254B)과 관련하여), 시스템은 주어진 클라이언트 디바이스에서 검출된 음성 활동에 기초하여 추가 참가자(들) 중 대응하는 사람에 대한 화자 임베딩을 생성할 수 있고, 시스템은 클라이언트 디바이스에서 캡처한 이미지에서 추가 사용자 중 대응하는 사람의 비주얼 임베딩을 생성할 수 있다. 일부 구현예에서, 시스템은 생성된 화자 임베딩을 주어진 사용자의 클라이언트 디바이스에 로컬적으로 저장된 복수의 화자 임베딩들(예를 들어, 도 1의 화자 임베딩 데이터베이스(112A))과 비교함으로써 대화의 추가 참가자(들)가 알려진 사용자(들)인지 결정할 수 있다. 시스템은 생성된 화자 임베딩이 임베딩 공간에서 복수의 화자 임베딩들 중 주어진 하나에 대한 임계 거리 내에 있는 경우, 생성된 화자 임베딩이 복수의 화자 임베딩들 중 주어진 하나와 일치한다고 결정할 수 있다. 생성된 화자 임베딩이 복수의 화자 임베딩들 중 주어진 하나와 일치한다고 결정하는 것에 기초하여, 시스템은 복수의 화자 임베딩들 중 주어진 하나와 연관된 대화의 대응하는 추가 참가자로부터 발생된 음성 활동을 결정할 수 있다. 일부 추가적 및/또는 대안적 구현예에서, 시스템은 생성된 비주얼 임베딩을 주어진 사용자의 클라이언트 디바이스에 로컬적으로 저장된 복수의 비주얼 임베딩들과 비교함으로써 대화의 추가 참가자(들)가 알려진 사용자(들)인지 결정할 수 있다. 시스템은 생성된 비주얼 임베딩 또는 그 피처들이 복수의 비주얼 임베딩들 중 주어진 하나와 일치하는 경우(예를 들어, 하나 이상의 이미지 프로세싱 기법을 사용하여 결정된 바와 같이), 생성된 비주얼 임베딩이 복수의 비주얼 임베딩들 중 주어진 하나와 일치한다고 결정할 수 있다. 생성된 비주얼 임베딩이 복수의 비주얼 임베딩들 중 주어진 하나와 일치한다고 결정하는 것에 기초하여, 시스템은 복수의 비주얼 임베딩들 중 주어진 하나와 연관된 대화의 대응하는 추가 참가자로부터 발생된 음성 활동을 결정할 수 있다. At block 258B, the system determines whether the additional participant(s) of the conversation are known user(s). As described herein (eg, with respect to block 256A of FIG. 2A and block 254B of FIG. 2B ), the system may select additional participant(s) based on the detected voice activity at the given client device. may generate speaker embeddings for corresponding ones of the additional users, and the system may generate visual embeddings of corresponding ones of the additional users from images captured by the client device. In some implementations, the system compares the generated speaker embedding to a plurality of speaker embeddings stored locally on a given user's client device (eg,
블록(258B)의 반복에서 시스템이 추가 참가자(들) 중 주어진 한 사람이 알려진 사용자가 아니라고 결정하면 시스템은 블록(260B)으로 진행할 수 있다. 블록(260B)에서, 시스템은 생성된 음성 임베딩 및/또는 생성된 비주얼 임베딩을 저장하기 위한 승인을 위해 추가 참가자(들) 중 주어진 사람에게 프롬프트를 표시한다. 일부 구현예에서, 프롬프트는 주어진 사용자의 클라이언트 디바이스에서 시각적으로 및/또는 청각적으로 렌더링될 수 있고, 추가 참가자(들) 중 주어진 한 사람은 클라이언트 디바이스에서 로컬적으로 스피치 임베딩 또는 비주얼 임베딩의 저장을 승인하기 위해 음성 또는 터치 입력을 제공할 수 있다. 다른 구현예에서, 프롬프트는 추가 참가자(들) 중 주어진 한 사람의 추가 클라이언트 디바이스(들)에서 렌더링될 수 있다. 상기 구현예 중 일부 버전에서, 프롬프트는 주어진 사용자의 클라이언트 디바이스로부터의 프롬프트를 팝업 알림, 텍스트 메시지, SMS 메시지 및/또는 기타 통신 채널로서 하나 이상의 네트워크들(예: 근거리 네트워크, 광역 네트워크, 블루투스, 근거리 통신 및/또는 기타 네트워크)을 통해 추가 참가자(들) 중 주어진 한 사람의 추가 클라이언트 디바이스(들)에 전송하는 것에 응답하여 추가 클라이언트 디바이스(들)에서 렌더링될 수 있다. If at a repetition of block 258B the system determines that a given one of the additional participant(s) is not a known user, the system may proceed to block 260B. At
블록(262B)에서, 시스템은 블록(260B)에서 렌더링된 프롬프트에 응답하여 추가 참가자(들)가 승인을 제공했는지 여부를 결정한다. 블록(262B)의 반복에서, 시스템이 추가 참가자(들)가 승인을 제공했다고 결정하면 시스템은 블록(264B)으로 진행할 수 있다. 블록(264B)에서, 시스템은 생성된 스피치 임베딩 및/또는 생성된 비주얼 임베딩을 추가 참가자(들)와 관련하여 주어진 사용자의 클라이언트 디바이스에 로컬적으로 저장할 수 있다. 추가 참가자(들)의 스피치 임베딩 및/또는 비주얼 임베딩은 주어진 사용자와의 향후 대화에서 추가 참가자(들)를 인식하는데 활용될 수 있다. 블록(262B)의 반복에서, 시스템이 추가 참가자(들)가 승인을 제공하지 않았다고 결정하면 시스템은 블록(266B)으로 진행할 수 있다. 블록(266B)에서, 시스템은 생성된 스피치 임베딩 및/또는 생성된 비주얼 임베딩을 폐기할 수 있다. 블록(264B 및 266B) 모두로부터, 시스템은 블록(268B)으로 진행할 수 있다. 또한, 블록(258B)의 반복에서, 시스템이 일치하는 스피치 임베딩 및/또는 일치하는 비주얼 임베딩을 식별하는 것에 기초하여 추가 참가자(들)가 알려진 사용자(들)라고 결정하면, 시스템은 또한 블록(268B)으로 진행한다. At
블록(268B)에서, 시스템은 주어진 사용자의 클라이언트 디바이스에서 추가 사용자 인터페이스 입력에 기초하여 대화에서 전달 또는 제공하기 위한 텍스트 세그먼트를 결정한다. 시스템은 도 2a의 블록(254A)와 관련하여 설명된 것과 동일한 방식으로 대화에서 전달하기 위한 텍스트 세그먼트를 결정할 수 있다. 특히, 블록(268B)의 대화에서 전달하기 위한 텍스트 세그먼트는 도 2a의 블록(264A) 또는 도 2b의 블록(252B)에서 검출된 추가 참가자(들)의 음성 활동에 대한 응답이며, 특히, 음성 활동에 포함된 추가 음성 사용자 입력에 응답 대한 응답이다. 블록(270B)에서, 시스템은 하나 이상의 운율 프로퍼티들을 결정한다. 시스템은 도 2a의 블록(258A)과 관련하여 설명된 것과 동일한 방식으로 하나 이상의 운율 프로퍼티들을 결정할 수 있다. 블록(260B)에서, 시스템은 텍스트 세그먼트를 통합하고 운율 프로퍼티들로 합성되는 합성 스피치를 포함하는 합성 스피치 오디오 데이터를 생성한다. 시스템은 도 2a의 블록(260A)과 관련하여 설명된 것과 동일한 방식으로 합성 스피치를 포함하는 합성 스피치 오디오 데이터를 생성할 수 있다.. 도 2의 블록(274B)에서, 시스템은 합성 스피치가 클라이언트 디바이스 및/또는 추가 클라이언트 디바이스의 하나 이상의 스피커들을 통해 렌더링되게 한다. 시스템은 도 2a의 블록(262A)과 연관하여 설명된 것과 동일한 방식으로 합성 스피치가 렌더링되게 할 수 있다. At
블록(276B)에서, 위에서 설명된 도 2a의 블록(252A)와 유사하게, 시스템은 주어진 사용자의 클라이언트 디바이스에서 주어진 사용자의 추가 사용자 인터페이스 입력을 모니터링한다. 블록(264B)의 반복에서, 시스템이 주어진 사용자의 클라이언트 디바이스에서 주어진 사용자의 임의의 추가 사용자 인터페이스 입력을 검출하지 않으면, 시스템은 블록(252B)으로 돌아가서, 주어진 사용자의 클라이언트 디바이스에서의 추가적인 사용자 인터페이스 입력 및/또는 위에서 설명된 방식으로 클라이언트 디바이스에서의 하나 이상의 다른 사용자의 음성 활동을 지속적으로 모니터링한다. 블록(276B)의 반복에서, 시스템이 주어진 사용자의 클라이언트 디바이스에서 주어진 사용자의 추가 사용자 인터페이스 입력을 검출하면, 시스템은 블록(252A)을 우회하고 블록(254A)으로 진행할 수 있다. At
따라서, 방법(200A 및 200B)은 주어진 사용자가 하나 이상의 추가 참가자들과의 대화에서 스피치 지원을 동적으로 사용할 수 있게 한다. 또한, 방법(200A 및 200B)의 스피치 지원을 사용하여 생성된 합성 스피치는 주어진 사용자와 대화의 추가 참가자(들) 간의 관계 및/또는 주어진 사용자의 클라이언트 디바이스의 위치의 분류(들)에 기초하여 하나 이상의 운율 프로퍼티들을 결정함으로써 주어진 사용자를 위해 대화의 추가 참가자(들)에 대해 맞춰질 수 있다. 따라서, 합성 스피치는 주어진 사용자에 대해 특정적이고 고유할 수 있으며, 대화의 다른 추가 참가자(들) 및/또는 대화의 다른 환경에 맞게 조정될 수 있다. Accordingly,
도 3은 다양한 구현예에 따라 본 개시의 다양한 양태를 보여주는 예시적 환경(300)에서의 시나리오를 도시한다. 환경(300)은 추가 참가자(302)와 대화에 참여하는 주어진 사용자(301)를 포함한다. 주어진 사용자(301) 및 추가 참가자(302)가 둘 모두 환경(300)에 존재하는 것으로 도시되어 있지만, 이는 단지 설명을 용이하게 하기 위한 것이며 제한하려는 것이 아님을 이해해야 한다. 예를 들어, 주어진 사용자(301)는 도 3의 환경에 존재할 수 있고, 대화의 추가 참가자(302)는 도 3의 환경(300)과 위치적으로 구별되는 원격 환경에 위치할 수 있다. 이 예에서, 주어진 사용자(301)는 전화 통화, VoIP 통화, 화상 채팅 및/또는 오디오 데이터의 전송을 포함하는 다른 형태의 원격 통신을 통해 추가 참가자(302)와의 대화에 참여할 수 있다. 특히, 추가 참가자(302)가 도 3의 환경(300)에서 주어진 사용자(301)와 위치적으로 구별되는 환경에 위치하는 구현예에서. 스피치 어시스턴트 시스템(160)은 주어진 사용자(301)와 대화의 추가 참가자(302) 간의 대화를 여전히 용이하게 할 수 있다. 3 depicts a scenario in an
본 명세서 더 자세히 설명된 바와 같이(예를 들어, 도 4a 내지 도 4f와 관련하여), 주어진 사용자(301)는 컴퓨팅 디바이스(310A) 및/또는 주어진 사용자(301)의 클라이언트 디바이스(410)를 활용하여 스피치 어시스턴트 시스템(160)과 인터렉션할 수 있다. 일부 구현예에서, 스피치 어시스턴트 시스템(160)은 주어진 사용자(301)와 대화의 추가 참가자(302) 간의 대화를 용이하게 하기 위해 컴퓨팅 디바이스(310A) 및/또는 주어진 사용자(301)의 클라이언트 디바이스(410)에서 로컬적으로 실행될 수 있다. 다른 구현예에서, 스피치 어시스턴트 시스템(160)은 하나 이상의 서버에서 원격으로 실행될 수 있고, 데이터는 컴퓨팅 디바이스(310A) 및/또는 주어진 사용자(301)의 클라이언트 디바이스(410)에 의해 네트워크(들)(예를 들어, 도 1의 네트워크(들)(190))를 통해 하나 이상의 서버에 전송될 수 있고 및/또는 컴퓨팅 디바이스(310A) 및/또는 주어진 사용자(301)의 클라이언트 디바이스(410)에서 네트워크(들)을 통해 하나 이상의 서버로부터 수신될 수 있다. 상기 구현예 중 일부 버전에서, 네트워크(들)를 통해 전송 및/또는 수신되는 데이터는 예를 들어 합성 스피치에 포함될 텍스트 세그먼트, 주어진 사용자(301)에 대한 화자 임베딩, 합성 스피치에 대한 하나 이상의 운율 프로퍼티들, 합성 스피치 오디오 데이터 및/또는 기타 데이터를 포함할 수 있다. As described in greater detail herein (eg, with respect to FIGS. 4A-4F ), a given
스피치 어시스턴트 시스템(160)은 주어진 사용자(301)의 스피치를 나타내는 합성 스피치를 포함하는 합성 스피치 오디오 데이터를 생성할 수 있고, 합성 스피치는 음파(376) 및/또는 음파(476)에 표시된 바와 같이 주어진 사용자(301)의 클라이언트 디바이스(410)에 의해 청각적으로 및/또는 컴퓨팅 디바이스(310A)의 사용자 인터페이스(380A) 및/또는 주어진 사용자(301)의 클라이언트 디바이스(410)의 사용자 인터페이스(480)에 시각적으로 렌더링될 수 있다. 합성 스피치는 컴퓨팅 디바이스(310A)에서 검출된 및/또는 주어진 사용자(301)의 클라이언트 디바이스(410)에서 검출된 사용자 인터페이스 입력에 기초하여 결정된 텍스트 세그먼트를 포함할 수 있다. 또한, 합성 스피치는 컴퓨팅 디바이스(310A) 및/또는 주어진 사용자(301)의 클라이언트 디바이스(410)에 로컬적으로 저장된 주어진 사용자(301)의 화자 임베딩을 사용하여 생성될 수 있고, 스피치 어시스턴트 시스템(160)에 의해 결정된 하나 이상의 운율 프로퍼티들로 합성될 수 있다. 본 명세서에서 더 자세히 설명된 바와 같이(예를 들어, 도 4a-4f와 관련하여)에서, 하나 이상의 운율 프로퍼티들은 환경(300)의 위치의 분류, 주어진 사용자(301)와 대화의 추가 참가자(302) 간의 관계의 적어도 하나의 속성 및/또는 주어진 사용자(301)와 대화의 추가 참가자(302) 간의 대화의 시맨틱에 기초하여 결정될 수 있다. 따라서, 합성 스피치는 주어진 사용자(301)의 화자 임베딩에 기초하여 주어진 사용자(301)에 고유하고, 하나 이상의 운율 프로퍼티들은 환경(300)의 위치의 분류, 주어진 사용자(301)와 대화의 추가 참가자(302) 간의 관계의 적어도 하나의 속성 및/또는 주어진 사용자(301)와 대화의 추가 참가자(302) 간의 대화의 시맨틱에 기초하여 변할 수 있다. 본 명세서에 기술된 기법은 주어진 사용자(301)가 언어 장애가 있는 사용자인 경우 유리할 수 있다.
도 4a 내지 4를 참조하면, 다양한 구현예에 따라 주어진 사용자에 대한 스피치 임베딩을 설정하고 하나 이상의 운율 프로퍼티들로 합성되는 합성 스피치를 생성하는데 활용되는 사용자 인터페이스들의 다양한 비제한적인 예들을 도시한다. 도 3의 주어진 사용자(301)의 클라이언트 디바이스(410)가 도시되고 사용자 인터페이스(480)를 포함한다. 비록 4a 내지 도 4f의 기법은 도 3의 주어진 사용자(301)의 클라이언트 디바이스(410)에 의해 구현되는 것으로 도시되어 있지만, 이것은 단지 설명의 편의를 위한 것이며 제한하려는 것이 아님을 이해해야 한다. 예를 들어, 도 4a 내지 도 4f의 기법은 추가적으로 및/또는 대안적으로 하나 이상의 다른 디바이스들(예를 들어, 도 3의 컴퓨팅 디바이스(310A), 도 5의 컴퓨팅 디바이스(510), 다른 사용자의 컴퓨팅 디바이스 및/또는 다른 컴퓨팅 디바이스)에 의해 구현될 수 있다. 클라이언트 디바이스(410)의 그래픽 사용자 인터페이스(480)는 클라이언트 디바이스(410)가 하나 이상의 액션들을 수행하게 하기 위해 주어진 사용자(301)와 인터렉션할 수 있는 다양한 시스템 인터페이스 엘리먼트들(491, 492, 493)(예: 하드웨어 및/또는 소프트웨어 인터페이스 엘리먼트)을 포함한다. 또한, 클라이언트 디바이스(410)의 사용자 인터페이스(480)는 터치 입력에 의해(예를 들어, 사용자 입력을 사용자 인터페이스(480) 또는 그 일부를 향하게 함으로써) 및/또는 음성 입력에 의해(예를 들어, 마이크로폰 인터페이스 엘리먼트(494)를 선택함으로써 또는 클라이언트 디바이스(410)에서 마이크로폰 인터페이스 엘리먼트(494)를 선택할 필요없이 단지 말함으로써(즉, 클라이언트 디바이스(410)에서 적어도 부분적으로 실행하는 자동화된 어시스턴트가 하나 이상의 용어 또는 문구, 제스처(들), 시선(들), 입 움직임(들), 입술 움직임(들) 및/또는 음성 입력을 활성화하기 위한 다른 조건들을 모니터링할 수 있음)) 주어진 사용자(301)가 사용자 인터페이스(480)에서 렌더링된 콘텐츠와 인터렉션하게 한다. 4A-4 illustrate various non-limiting examples of user interfaces utilized to establish a speech embedding for a given user and generate synthetic speech synthesized with one or more prosody properties, in accordance with various implementations. A
일부 구현예에서, 클라이언트 디바이스(410)의 사용자 인터페이스(480)는 대화의 참가자들 각각을 식별하는 그래픽 엘리먼트들을 포함할 수 있다. 대화의 참가자는 본 명세서에 기술된 임의의 방식(예를 들어, 화자 임베딩(들), 비주얼 임베딩(들), 신체 자세 및/또는 머리 자세 및/또는 기타 방식)과 같은 다양한 방식으로 식별될 수 있다. 도 4a 내지 도 4f에 걸쳐 도시된 바와 같이, 그래픽 엘리먼트(462A)는 자동화된 어시스턴트에 대응하고, 그래픽 엘리먼트(462T)는 Tim에 대응하고, 그래픽 엘리먼트(462R)는 Randy에 대응하고, 그래픽 엘리먼트(462J)는 Jim에 대응하고, 그래픽 엘리먼트(462U)는 알 수 없는 사용자에 대응하고, 그래픽 엘리먼트(462S)는 Stan에 대응한다. 상기 구현예 중 일부 버전에서, 대화의 참가자를 식별하는 이러한 그래픽 엘리먼트는 자동화된 어시스턴트로부터의 대응하는 프롬프트(예를 들어, 도 4a에 도시된 바와 같이) 및/또는 클라이언트 디바이스(410)의 주어진 사용자(301)로부터의 사용자 인터페이스 입력의 전사 및 대화의 추가 참가자의 추가 사용자 음성 입력의 전사(예를 들어, 도 4b 내지 도 4f에 도시된 바와 같이)와 함께 시각적으로 렌더링될 수 있다. 상기 구현예의 일부 추가적 및/또는 대안적 버전에서, 대화의 참가자를 식별하는 이러한 그래픽 엘리먼트는 클라이언트 디바이스(410)의 사용자 인터페이스(480)의 상단 부분에서 시각적으로 렌더링될 수 있다. 이러한 그래픽 엘리먼트가 클라이언트 디바이스(410)의 사용자 인터페이스(480)의 상단 부분에서 시각적으로 렌더링되는 것으로 도시되어 있지만, 제한하려는 것이 아니며, 이러한 그래픽 엘리먼트가 사용자 인터페이스(480)의 측면 부분 또는 사용자 인터페이스(480)의 하단 부분에 렌더링될 수 있다는 점에 유의해야 한다. 더욱이, 그러한 추가적 및/또는 대안적 구현예의 일부 버전에서, 그래픽 엘리먼트(462)는 또한 대화의 참가자를 식별하는 이러한 그래픽 엘리먼트와 함께 사용자 인터페이스(480)의 상단 부분에서 렌더링될 수 있다. 그래픽 엘리먼트(462)는 선택될 때 클라이언트 디바이스(410)의 주어진 사용자(301)가 대화에 추가 참가자를 추가하는 것을 가능하게 할 수 있다. 예를 들어, 그래픽 엘리먼트(462)가 선택되면, 주어진 사용자(301)의 연락처 목록이 사용자 인터페이스(480)에서 렌더링될 수 있고, 연락처 중 하나 이상이 대화에 추가될 수 있고, 알려진 사용자들의 목록이 사용자 인터페이스(480) 상에 렌더링될 수 있고, 알려진 사용자 중 하나 이상이 대화에 추가될 수 있고 및/또는 대화에 추가 참가자를 수동으로 추가하기 위한 다른 기법이 사용될 수 있다. In some implementations, the
먼저 도 4a를 참조하면, 클라이언트 디바이스(410)의 주어진 사용자(301)에 대한 화자 임베딩이 설정될 수 있다. 일부 구현예에서, 주어진 사용자(301)에 대한 화자 임베딩은 (예를 들어, 도 1의 식별 엔진(112)과 관련하여) 본 명세서에 기술된 바와 같이 생성될 수 있고, 주어진 사용자(301)의 스피치를 나타내는 합성 스피치를 생성하는데 활용될 수 있다. 상기 구현예 중 일부 버전에서, 클라이언트 디바이스(410) 상에서 적어도 부분적으로 실행되는 자동화된 어시스턴트는 복수의 프롬프트들이 클라이언트 디바이스(410) 및/또는 추가 컴퓨팅 디바이스(예를 들어, 도 3의 컴퓨팅 디바이스(310A))의 스피커(들)을 통해 청각적으로 및/또는 클라이언트 디바이스(410)의 사용자 인터페이스(480)를 통해 시각적으로 렌더링되게 할 수 있다. 복수의 프롬프트들은 주어진 사용자(301)와 자동화된 어시스턴트 간의 대화의 일부일 수 있고, 프롬프트들 각각은 클라이언트 디바이스(410)의 주어진 사용자(301)로부터 음성 입력을 요청할 수 있다. 또한, 각각의 프롬프트들에 응답하는 주어진 사용자(301)로부터의 음성 입력은 클라이언트 디바이스(410)의 하나 이상의 마이크로폰을 통해 검출될 수 있다. 일부 구현예에서, 주어진 사용자(301)는 사용자 인터페이스 입력을 그래픽 엘리먼트(499)로 향하게 함으로써 프롬프트들 각각을 통해 순환할 수 있으며, 이는 선택될 때 자동화된 어시스턴트가 후속 프롬프트를 렌더링하게 한다. 일부 구현예에서, 자동화된 어시스턴트는 클라이언트 디바이스(410)의 주어진 사용자(301)에 대한 화자 임베딩이 설정될 때까지 프롬프트들이 클라이언트 디바이스(410)에서 계속 렌더링되게 할 수 있다. 일부 다른 구현예에서, 자동화된 어시스턴트는 프롬프트들의 임계값 수가 렌더링될 때까지 클라이언트 디바이스(410)에서 프롬프트들이 계속 렌더링되게 할 수 있다. Referring first to FIG. 4A , speaker embedding for a given
예를 들어, 자동화된 어시스턴트는 "Could you introduce yourself?"라는 프롬프트(452A1)가 클라이언트 디바이스(410)에서 렌더링되게 할 수 있고, 클라이언트 디바이스는 프롬프트(452A1)의 렌더링에 응답하여 "Hello, I’m Tim, nice to meet you"의 주어진 사용자(301)로부터의 음성 입력(454A1)을 검출할 수 있다. 단순함을 위해, 도 4a-4f에 걸쳐서 클라이언트 디바이스(410)의 주어진 사용자(301)는 종종 "Tim"으로 지칭된다. 또한, 자동화된 어시스턴트는 "Nice to meet you as well. Where are you from?"라는 다른 프롬프트(452A2)가 클라이언트 디바이스(410)에서 렌더링되게 한다. 클라이언트 디바이스(410)가 Tim으로부터 추가 음성 입력을 검출함에 따라, "I was born in Denver, CO, but I moved to Louisville, KY when I was ..."의 추가 음성 입력의 전사(470)가 클라이언트 디바이스(410)에 의해 디스플레이될 수 있다. 특히, 추가 음성 입력의 전사(470)는 불완전하고(예를 들어, 타원(472) 및 커서(474)로 표시된 바와 같이), Tim은 추가 음성 입력이 완료되었다는 임의의 표시 전에(예: 음성 입력의 완료를 검출하기 위한 엔드-포인팅 및/또는 기타 기법) 텍스트 세그먼트 사용자 인터페이스(481)를 향해 터치 입력 및/또는 음성 입력을 제공함으로써 전사(470)의 임의의 부분을 편집할 수 있다. 자동화된 어시스턴트는 Tim에 대한 화자 임베딩이 설정될 때까지 클라이언트 디바이스(410)에서 추가 프롬프트가 렌더링되게 할 수 있다. 예를 들어, 자동화된 어시스턴트는 클라이언트 디바이스(410)에 의해 농담이 렌더링되게 할 수 있고, 농담에 응답하는 Tim의 웃음은 Tim의 스피치 임베딩에 통합될 수 있다. For example, the automated assistant may cause a prompt 452A1 of “Could you introduce yourself?” rendered at the
더욱이, 일부 구현예에서, 자동화된 어시스턴트는 클라이언트 디바이스(410)의 사용자 인터페이스(480) 상의 후보 화자 임베딩들에 대응하는 복수의 표현들을 렌더링할 수 있고, 주어진 사용자는 후보 화자 임베딩들 각각과 연관된 합성 스피치를 들을 수 있고, 클라이언트 디바이스(410)는 후보 화자 임베딩들 중 주어진 하나의 선택을 수신할 수 있다. 선택된 후보 화자 임베딩은 클라이언트 디바이스(110)의 주어진 사용자와 연관되어 저장될 수 있고, 클라이언트 디바이스의 주어진 사용자의 스피치를 나타내는 합성 스피치를 생성하는데 사용될 수 있다. 예를 들어, 클라이언트 디바이스(410)가 깊은 남성 음성과 연관된 제1 표현, 남부 억양을 갖는 남성 음성과 연관된 제2 표현 및 중서부 억양을 갖는 남성 음성과 연관된 제3 표현을 렌더링한다고 가정한다. 또한 Tim이 각 표현과 연관된 합성 스피치를 듣고 제3 표현을 선택한다고 가정한다. 그 다음, 제3 표현과 연관된 화자 임베딩은 Tim과 연관되어 저장될 수 있고, Tim의 스피치를 나타내는 합성 스피치를 생성하는데 사용될 수 있다. 따라서, 이러한 구현예에서 Tim은 사용자 인터페이스 입력(들)을 통해 원하는 음성이 Tim의 실제 음성과 일치하지 않는 경우, Tim 대신 스피치 합성을 위해 원하는 음성을 지정할 수 있다. 일부 추가적 및/또는 대안적 구현예에서, 자동화된 어시스턴트는 프롬프트에 응답하여 클라이언트 디바이스(410)의 사용자 인터페이스(480)에 기초하여 클라이언트 디바이스(410)의 주어진 사용자에 대한 화자 임베딩을 결정할 수 있다. 자동화된 어시스턴트는 클라이언트 디바이스(410)의 사용자 인터페이스(480)를 통해, 결정된 화자 임베딩의 확인을 요청하는 추가 프롬프트를 렌더링할 수 있고, 결정된 화자 임베딩은 클라이언트 디바이스(410)의 주어진 사용자와 연관되어 저장될 수 있고, 클라이언트 디바이스(410)의 주어진 사용자의 스피치를 나타내는 합성 스피치를 생성하는데 사용될 수 있다. 예를 들어, Tim이 현재 중서부(예를 들어, Louisville, KY)에 살고 있음을 나타내는 전사(470)에 기초하여, 자동화된 어시스턴트는 중서부 억양과 연관된 화자 임베딩이 Tim에게 할당되어야 한다고 결정할 수 있고, Tim에게 중서부 억양과 연관된 화자 임베딩을 확인하도록 프롬프트할 수 있다. Tim에 대한 확인 수신에 대한 응답으로, Tim과 연관하여 화자 임베딩을 저장할 수 있다. 더욱이, 화자 임베딩이 저장되기 전에, 클라이언트 디바이스(410)의 주어진 사용자는 화자 임베딩의 다양한 수정을 생성하기 위해 임베딩 공간에서 화자를 이리저리 이동시킴으로써 화자 임베딩을 편집할 수 있다. Tim의 화자 임베딩을 설정함으로써, 클라이언트 디바이스(410)는 Tim의 화자 임베딩에 기초하여 Tim의 스피치를 나타내는 합성 스피치를 생성함으로써 Tim의 음성을 모델링할 수 있다. Moreover, in some implementations, the automated assistant may render a plurality of representations corresponding to candidate speaker embeddings on the
본 명세서에 기술된 바와 같이, 합성 스피치는 하나 이상의 운율 프로퍼티들로 합성될 수 있다. 예를 들어, 그리고 도 4b와 관련하여, 도 3의 추가 참가자(302)는 Randy이고 Randy는 Tim의 상사이고(예를 들어, 도 1의 관계 엔진(141)을 사용하여 및/또는 도 1의 관계 속성(들) 데이터베이스에 저장된 속성(들)에 기초하여 결정됨), 환경(300)은 직장 환경으로 분류된다(예를 들어, Tim이 그의 직장과 연관된 위치에 있음을 나타내는 클라이언트 디바이스(410)의 GPS 센서 데이터와 같은 센서 데이터에 기초하여 환경 엔진(142)을 사용하여 결정됨)고 가정한다. 또한 클라이언트 디바이스(410)가 클라이언트 디바이스(410)에서 검출된 추가 사용자 음성 입력에 대응하는 "How was the meeting today?"의 텍스트 세그먼트(452B1)를 결정하고, 사용자 인터페이스(480)를 통해 텍스트 세그먼트(452B1)를 렌더링한다고 가정한다. 일부 구현예에서, 클라이언트 디바이스(410)는 추가 사용자 음성 입력에 기초하여 화자 임베딩을 생성하고 생성된 화자 임베딩을 클라이언트 디바이스(410)에 로컬적으로 저장된 복수의 화자 임베딩들(예를 들어, 도 1의 스피치 임베딩(들) 데이터베이스(112A))과 비교함으로써 Randy와 연관된 일치하는 화자 임베딩을 식별함으로써, Randy로부터 유래된 텍스트 세그먼트(452B1)에 대응하는 추가 사용자 음성 입력을 결정할 수 있다. 일부 추가적 및/또는 대안적 구현예에서, 클라이언트 디바이스는 클라이언트 디바이스(410)의 하나 이상의 비전 센서를 사용하여 Randy를 포함하고, 그래픽 엘리먼트(495)에 대한 사용자 인터페이스 입력에 응답하여 캡처되는 이미지를 캡처함으로써 Randy가 대화의 추가 참가자임을 결정할 수 있다. 클라이언트 디바이스(410)는 이미지를 프로세싱하여 Randy와 연관된 비주얼 임베딩을 생성하고, 생성된 비주얼 임베딩을 클라이언트 디바이스(410)에 로컬적으로 저장된 복수의 비주얼 임베딩들(예를 들어, 도 1의 비주얼 임베딩(들) 데이터베이스)과 비교함으로써 Randy와 연관된 일치하는 비주얼 임베딩을 식별할 수 있다. As described herein, synthesized speech may be synthesized with one or more prosody properties. For example, and with reference to FIG. 4B , the
또한, 클라이언트 디바이스(410)는 Tim으로부터 "It went very well, we are back on track to finish the project by the 17th"의 사용자 인터페이스 입력을 검출할 수 있다. 사용자 인터페이스 입력이 Tim으로부터의 음성 입력이라고 가정하면, 클라이언트 디바이스(410)는 하나 이상의 스피치 인식 모델(들)(예를 들어, 도 1의 스피치 인식 모델(들)(120A))을 사용하여 사용자 인터페이스 입력을 프로세싱하여, Tim의 사용자 인터페이스 입력에 대응하는 "It went very well, we are back on track to finish the project by the 17th"의 텍스트 세그먼트(454B1A)를 결정한다. 더욱이, 클라이언트 디바이스(410)는 추가 참가자가 Randy(예를 들어, Tim의 상사)이고, Tim과 Randy 간의 대화 환경이 직장 환경으로 분류되고, 대화의 시맨틱(예: 팀의 상사가 업무 회의에 대해 묻고 있음)에 기초하여 운율 프로퍼티들의 제1 세트(454B1B)를 결정할 수 있다. 따라서, 클라이언트 디바이스(410)는 (예를 들어, 음파들(476)에 의해 표시된 바와 같이 그리고 클라이언트 디바이스(410) 및/또는 추가 컴퓨팅 디바이스(예를 들어, 도 3의 컴퓨팅 디바이스(310A))의 스피커(들)를 사용하여) 클라이언트 디바이스(410)에서 Tim으로부터 사용자 인터페이스 입력을 수신하는 것에 응답하여 텍스트 세그먼트(454B1A)를 포함하고 운율 프로퍼티의 제1 세트(454B1B)로 합성되는 합성 스피치를 생성하고 청각적으로 렌더링할 수 있다. 또한, 텍스트 세그먼트(454B1A)에 대한 사용자 인터페이스 입력은 Tim이 임의의 추가 음성 입력을 제공할 필요 없이 합성 스피치가 다시 청각적으로 렌더링되게 할 수 있다.Also, the
대조적으로, 그리고 도 4와 관련하여, 도 3의 추가 참가자(302)는 Jim이고 Jim은 Tim의 친구이고(예를 들어, 도 1의 관계 엔진(141)을 사용하여 및/또는 도 1의 관계 속성(들) 데이터베이스에 저장된 속성(들)에 기초하여 결정됨), 환경(300)은 집 환경으로 분류된다(예를 들어, Tim이 그의 집과 연관된 위치에 있음을 나타내는 클라이언트 디바이스(410)의 GPS 센서 데이터와 같은 센서 데이터에 기초하여 환경 엔진(142)을 사용하여 결정됨)고 가정한다. 또한 클라이언트 디바이스(410)가 클라이언트 디바이스(410)에서 검출된 추가 사용자 음성 입력에 대응하는 "How was the meeting today?"의 텍스트 세그먼트(452C1)를 결정하고, 사용자 인터페이스(480)를 통해 텍스트 세그먼트(452C1)를 렌더링한다고 가정한다. 일부 구현예에서, 클라이언트 디바이스(410)는 추가 사용자 음성 입력에 기초하여 화자 임베딩을 생성하고 생성된 화자 임베딩을 클라이언트 디바이스(410)에 로컬적으로 저장된 복수의 화자 임베딩들(예를 들어, 도 1의 스피치 임베딩(들) 데이터베이스(112A))과 비교함으로써 Jim과 연관된 일치하는 화자 임베딩을 식별함으로써, Jim으로부터 유래된 텍스트 세그먼트(452C1)에 대응하는 추가 사용자 음성 입력을 결정할 수 있다. 일부 추가적 및/또는 대안적 구현예에서, 클라이언트 디바이스는 클라이언트 디바이스(410)의 하나 이상의 비전 센서를 사용하여 Jim을 포함하고, 그래픽 엘리먼트(495)에 대한 사용자 인터페이스 입력에 응답하여 캡처되는 이미지를 캡처함으로써 Jim이 대화의 추가 참가자임을 결정할 수 있다. 클라이언트 디바이스(410)는 이미지를 프로세싱하여 Jim과 연관된 비주얼 임베딩을 생성하고, 생성된 비주얼 임베딩을 클라이언트 디바이스(410)에 로컬적으로 저장된 복수의 비주얼 임베딩들(예를 들어, 도 1의 비주얼 임베딩(들) 데이터베이스)과 비교함으로써 Jim과 연관된 일치하는 비주얼 임베딩을 식별할 수 있다. In contrast, and with respect to FIG. 4 , the
또한, 클라이언트 디바이스(410)는 Tim으로부터 "It went very well, we are back on track to finish the project by the 17th"의 사용자 인터페이스 입력을 검출할 수 있다. 사용자 인터페이스 입력이 음성 입력이라고 가정하면, 클라이언트 디바이스(410)는 하나 이상의 스피치 인식 모델(들)(예를 들어, 도 1의 스피치 인식 모델(들)(120A))을 사용하여 사용자 인터페이스 입력을 프로세싱하여, Tim의 사용자 인터페이스 입력에 대응하는 "It went very well, we are back on track to finish the project by the 17th"의 텍스트 세그먼트(454C1A)를 결정한다. 더욱이, 클라이언트 디바이스(410)는 추가 참가자가 Jim(예를 들어, Tim의 친구)이고, Tim과 Jim 간의 대화 환경이 집 환경으로 분류되고, 대화의 시맨틱(예: 팀의 친구가 업무 회의에 대해 묻고 있음)에 기초하여 운율 프로퍼티들의 제2 세트(454C1B)를 결정할 수 있다. 따라서, 클라이언트 디바이스(410)는 (예를 들어, 도 1의 스피치 합성 모델(들)(130A)을 사용하여) 도 4a에서 설정된 Tim의 스피치 임베딩에 기초하여, 텍스트 세그먼트(454C1A)를 포함하고 운율 프로퍼티의 제2 세트(454C1B)로 합성되는 합성 스피치 오디오 데이터를 생성할 수 있다. 클라이언트 디바이스(410)는 (예를 들어, 음파들(476)에 의해 표시된 바와 같이 그리고 클라이언트 디바이스(410) 및/또는 추가 컴퓨팅 디바이스(예를 들어, 도 3의 컴퓨팅 디바이스(310A))의 스피커(들)를 사용하여) 클라이언트 디바이스(410)에서 사용자 인터페이스 입력을 수신하는 것에 응답하여 합성 스피치 오디오 데이터에 포함된 합성 스피치를 청각적으로 렌더링할 수 있다. 또한, 텍스트 세그먼트(454C1A)에 대한 사용자 인터페이스 입력은 Tim이 임의의 추가 음성 입력을 제공할 필요 없이 합성 스피치가 다시 청각적으로 렌더링되게 할 수 있다.Also, the
특히, Randy의 텍스트 세그먼트(452B1)에 대응하는 추가 사용자 음성 입력 및 Jim의 텍스트 세그먼트(452C1)에 대응하는 추가 사용자 음성 입력 모두 "How was the meeting today?"를 묻고 있고, Tim의 사용자 인터페이스 입력에 기초한 텍스트 세그먼트(452B1 및 454C1A)는 동일하다. 그러나, 도 4b 및 도 4c에서 합성된 스피치는 운율 속성들의 상이한 세트들(예를 들어, 도 4b의 운율 속성들의 제1 세트 및 도 4b의 운율 속성들의 제2 세트)을 사용하여 생성된다. 운율 프로퍼티들의 이러한 차이는 상이한 관계(예: 상사 대 친구), 상이한 환경(예: 직장 환경 대 집 환경) 및/또는 대화의 시맨틱(예: 상사가 질문 대 친구가 질문을 하는 경우)에 기초할 수 있고, 이러한 차이는 운율 프로퍼티들의 제1 세트(454B1B)에 있는 적어도 하나의 운율 프로퍼티는 운율 프로퍼티들의 제2 세트(454C1B)와 다르다는 것을 나타낸다. 예를 들어, 도 4b의 운율 프로퍼티들의 제1 세트는 Tim이 그의 상사 Randy와 말할 때 더 전문적인 스피치를 반영하기 위해, 단조로운 톤, 약간의 감정, 일관된 리듬 및/또는 다른 운율 프로퍼티들을 포함할 수 있다(예를 들어, 마침표로 끝나는 텍스트 세그먼트(454B1B)에 의해 표시됨). 대조적으로, 도 4c의 운율 프로퍼티들의 제2 세트는 Tim이 그의 친구 Jim과 말할 때 더 캐주얼한 스피치를 반영하기 위해 흥분된 톤, 더 많은 감정, 다양한 리듬 및/또는 다른 운율 프로퍼티들을 포함할 수 있다(예를 들어, 느낌표로 끝나는 텍스트 세그먼트(454C1B)로 표시됨). In particular, both the additional user voice input corresponding to Randy's text segment 452B1 and the additional user voice input corresponding to Jim's text segment 452C1 are asking "How was the meeting today?" The underlying text segments 452B1 and 454C1A are identical. However, the synthesized speech in FIGS. 4B and 4C is generated using different sets of prosody attributes (eg, the first set of prosody attributes of FIG. 4B and the second set of prosody attributes of FIG. 4B ). These differences in prosody properties may be based on different relationships (e.g. boss vs. friend), different circumstances (e.g. work environment vs. home environment), and/or the semantics of the conversation (e.g. if boss asks a question versus a friend asks a question). , and this difference indicates that at least one prosody property in the first set of prosody properties 454B1B is different from the second set of prosody properties 454C1B. For example, the first set of prosody properties in FIG. 4B may include a monotonous tone, some emotion, a coherent rhythm, and/or other prosody properties to reflect a more professional speech when Tim speaks with his boss Randy. There is (eg, indicated by period-ending text segment 454B1B). In contrast, the second set of prosody properties of FIG. 4C may include an excited tone, more emotion, various rhythms, and/or other prosody properties to reflect a more casual speech when Tim speaks with his friend Jim. For example, a text segment ending with an exclamation point (denoted by 454C1B).
일부 구현예에서, 운율 프로퍼티 사용자 인터페이스(482)는 선택적으로 추가 사용자 음성 입력이 클라이언트 디바이스(410)에서 검출될 때 클라이언트 디바이스(410)의 사용자 인터페이스(480)를 통해 시각적으로 렌더링될 수 있다(예를 들어, 도 4b-4d에 점선으로 도시된 바와 같이). 다른 구현예에서, 운율 프로퍼티 사용자 인터페이스(482)는 도 4b 및 4c에서 각각 그래픽 엘리먼트(462R 및 462J)에 대한 사용자 인터페이스 입력(예를 들어, 터치 입력 또는 음성 입력)을 검출하는 것에 응답하여 클라이언트 디바이스(410)의 사용자 인터페이스(480)를 통해 시각적으로 렌더링될 수 있다. 운율 프로퍼티 사용자 인터페이스(482)는 예를 들어 클라이언트 디바이스(410)에 의해 생성된 임의의 합성 스피치가 추가 참가자에 대해 얼마나 "비형식적", "중립적" 또는 "형식적"인지를 나타내는 표시자(444)를 갖는 스케일(442)을 포함할 수 있다. 예를 들어, 도 4b의 표시자(444)는 Randy에게 향하는 클라이언트 디바이스(410)에 의해 생성된 임의의 합성 스피치가 더 형식적일 것임을 나타낸다. 대조적으로, 도 4c의 표시자(444)는 Jim에게 향하는 클라이언트 디바이스(410)에 의해 생성된 임의의 합성 스피치가 더 비형식적일 것임을 나타낸다. 상기 구현예 중 일부 버전에서, 표시자(444)는 표시자(444)에 대한 사용자 인터페이스 입력에 응답하여 스케일(442)을 따라 슬라이딩 가능할 수 있다. 표시자(444)가 스케일(442)을 따라 조정될 때, 운율 프로퍼티들의 세트 중 적어도 하나의 운율 프로퍼티는 상기 조정을 반영하도록 조정된다. 예를 들어, 표시자(444)가 더 비형식적인 스피치를 반영하도록 스케일(442)에서 조정되면, Randy에 대한 클라이언트 디바이스(410)에 의해 후속적으로 생성된 임의의 합성 스피치는 (단조로운 톤과 비교하여) 더 흥분된 톤을 포함하지만, Jim에 대한 클라이언트 디바이스(410)에 의해 생성된 합성 스피치만큼 많은 감정은 여전히 포함하지 않는다. 따라서, 표시자(444)는 대화의 주어진 추가 참가자에 대한 운율 프로퍼티들의 세트 중 적어도 운율 프로퍼티들을 수정하도록 스케일(442)에서 조정될 수 있다. 비록 도 4b 내지 도 4d는 클라이언트 디바이스(410)에 의해 생성된 임의의 합성 스피치가 얼마나 "비형식적", "중립적" 또는 "형식적"인지를 나타내는 표시자(444) 및 스케일(442)를 갖는 것으로 도시되어 있지만, 이는 단지 설명의 편의를 위한 것이며, 제한하려는 의도가 아님을 이해해야 한다. 예를 들어, 스케일(442) 및 표시자(444)는 합성 스피치가 얼마나 "전문적" 또는 "일상적"인지, 합성 스피치가 얼마나 "우호적" 또는 "적대적"인지 등을 나타낼 수 있다. In some implementations, prosody
더욱이, 상기 구현예 중 일부 버전에서, 운율 프로퍼티 사용자 인터페이스(482)는 선택될 때 복수의 운율 프로퍼티들 운율 프로퍼티 사용자 인터페이스(482) 상에 렌더링되게 하는 "More ..." 선택가능한 엘리먼트(446)를 포함할 수 있다. 운율 프로퍼티들은 대응하는 스케일 및 표시자(예: 스케일(442) 및 표시자(444)와 같이), 각 운율 프로퍼티들에 대한 값을 입력하기 위한 필드, 특정 운율 프로퍼티들을 활성화 또는 비활성화하기 위한 필드 및/또는 운율 프로퍼티들이 개별적으로 조정되게 하기 위한 기타 기법과 함께 렌더링될 수 있다. 예를 들어, "More..." 선택가능한 엘리먼트(446)가 선택되면, "억양", "톤", "강세", "리듬", "템포", "피치", "일시 정지" 각각에 대한 표시 및/또는 기타 운율 프로퍼티들 및 대응하는 스케일 및 표시자는 운율 프로퍼티 사용자 인터페이스(482)를 통해 클라이언트 디바이스(410)에 시각적으로 렌더링될 수 있다. 다른 예로서, "More ..." 선택가능한 엘리먼트(446)가 선택되면, "억양", "톤", "강세", "리듬", "템포", "피치", "일시 정지" 각각에 대한 표시 및/또는 기타 운율 프로퍼티들 및 대응하는 운율 프로퍼티가 활성화 또는 비활성화되게 하는 운율 프로퍼티들 각각에 대한 대응하는 필드는 운율 프로퍼티 사용자 인터페이스(482)를 통해 클라이언트 디바이스(410)에서 시각적으로 렌더링될 수 있다. Moreover, in some versions of the above implementations, the prosody
구체적으로 도 4b를 참조하면, 클라이언트 디바이스가 Randy로부터의 추가 사용자 음성 입력에 대응하는 "Good work, thanks for covering for me while I was sick"의 텍스트 세그먼트(452B2)를 결정하고, 사용자 인터페이스(480)를 통해 텍스트 세그먼트(452B2)를 렌더링한다고 가정한다. 본 명세서에 설명된 바와 같이, 일부 구현예에서, 클라이언트 디바이스(410)에서 검출된 추가 참가자(들)의 음성 입력에 응답하는 제안(들)은 클라이언트 디바이스(410)의 주어진 사용자가 음성 입력의 의도된 타겟이라고 결정함에 응답하여서만 렌더링될 수 있다. 일부 구현예에서, 클라이언트 디바이스(410)는 Randy로부터의 다른 추가 사용자 음성 입력에 응답하는 대응하는 후보 텍스트 세그먼트를 각각 포함하는 하나 이상의 제안들(456B1-456B4)을 생성할 수 있다. 하나 이상의 제안들(456B1-456B4)은 텍스트 세그먼트 사용자 인터페이스(481)를 통해 클라이언트 디바이스(410)에서 시각적으로 렌더링될 수 있다. 클라이언트 디바이스(410)는 (예를 들어, 도 1의 자동 제안 엔진(150)과 관련하여) 본 명세서에서 더 상세히 설명되는 자동 제안 엔진을 사용하여 하나 이상의 제안들을 생성할 수 있다. 상기 구현예 중 일부 버전에서, 하나 이상의 제안들(456B1-456B4)은 다른 제안들(456B1-456B4)보다 시각적으로 더 두드러지게 렌더링될 수 있다. 제안들(456B1-456B4) 중 주어진 하나는 제안들(456B1-456B4)의 순위에 기초하여 더 두드러지게 렌더링될 수 있으며, 순위는 자동 제안 엔진(예: 도 1의 자동 제안 엔진(150))에 의해 결정될 수 있다. 예를 들어, 도 4b의 제안들(456B1-456B4)은 버블로 도시되고, 제1 제안(456B1)은 가장 큰 버블로 도시되고, 제2 제안(456B2)은 두 번째로 큰 버블로 도시되는 식이다. 제안들(456B1-456B4) 중 하나에 대한 클라이언트 디바이스(410)에서의 추가 사용자 인터페이스 입력을 수신하면, 클라이언트 디바이스(410)는 선택된 제안의 후보 텍스트 세그먼트를 포함하고, 운율 프로퍼티들의 세트로 합성되는 도 4a에서 설정된 Tim의 음성 임베딩을 사용하여 합성 스피치를 생성할 수 있다. 특히, 다른 추가 사용자 음성 입력에 응답하여 합성 스피치를 생성하는데 사용되는 운율 프로퍼티들의 세트는 운율 프로퍼티들의 제1 세트(454B1B)와 다를 수 있다. 예를 들어, 운율 프로퍼티들의 세트는 운율 프로퍼티들의 제1 세트(454B1B)보다 더 많은 감정과 강조를 포함하여 질병에서 회복하는 Randy에 대한 Tim의 동정을 반영할 수 있다. 따라서 운율 프로퍼티들은 추가 참가자(예: Randy)와 환경(예: 작업 환경)이 동일하게 유지되더라도 주어진 대화 내에서 달라질 수 있다. 제안들(456B1-456B4) 중 하나의 선택에 추가로 또는 그 대신에 추가 음성 입력 및 제안(456B1-456B4)은 설명의 편의를 위해 제공되며 제한하려는 의도가 아님을 유의해야 한다.Referring specifically to FIG. 4B , the client device determines a text segment 452B2 of “Good work, thanks for covering for me while I was sick” corresponding to an additional user voice input from Randy, and a
구체적으로 도 4c를 참조하면, 클라이언트 디바이스(410)가 Jim으로부터의 추가 사용자 음성 입력에 대응하는 "Great to hear! We should celebrate when the project is done"의 텍스트 세그먼트(452C2)를 결정하고, 사용자 인터페이스(480)를 통해 텍스트 세그먼트(452C2)를 렌더링한다고 가정한다. 본 명세서에 설명된 바와 같이, 일부 구현예에서, 클라이언트 디바이스(410)에서 검출된 추가 참가자(들)의 음성 입력에 응답하는 제안(들)은 클라이언트 디바이스(410)의 주어진 사용자가 음성 입력의 의도된 타겟이라고 결정함에 응답하여서만 렌더링될 수 있다. 일부 구현예에서, 클라이언트 디바이스(410)는 Jim으로부터의 다른 추가 사용자 음성 입력에 응답하는 대응하는 후보 텍스트 세그먼트를 각각 포함하는 하나 이상의 제안들(456C1-456C4)을 생성할 수 있다. 하나 이상의 제안들(456C1-456C4)은 텍스트 세그먼트 사용자 인터페이스(481)를 통해 클라이언트 디바이스(410)에서 시각적으로 렌더링될 수 있다. 클라이언트 디바이스(410)는 (예를 들어, 도 1의 자동 제안 엔진(150)과 관련하여) 본 명세서에서 더 상세히 설명되는 자동 제안 엔진을 사용하여 하나 이상의 제안들을 생성할 수 있다. 상기 구현예 중 일부 버전에서, 하나 이상의 제안들(456C1-456C4)은 다른 제안들(456C1-456C4)보다 시각적으로 더 두드러지게 렌더링될 수 있다. 제안들(456C1-456C4) 중 주어진 하나는 제안들(456C1-456C4)의 순위에 기초하여 더 두드러지게 렌더링될 수 있으며, 순위는 자동 제안 엔진(예: 도 1의 자동 제안 엔진(150))에 의해 결정될 수 있다. 예를 들어, 도 4c의 제안들(456C1-456C4)은 버블로 도시되고, 제1 제안(456C1)은 가장 큰 버블로 도시되고, 제2 제안(456C2)은 두 번째로 큰 버블로 도시되는 식이다. 제안들(456C1-456C4) 중 하나에 대한 클라이언트 디바이스(410)에서의 추가 사용자 인터페이스 입력을 수신하면, 클라이언트 디바이스(410)는 선택된 제안의 후보 텍스트 세그먼트를 포함하고, 운율 프로퍼티들의 세트로 합성되는 도 4a에서 설정된 Tim의 음성 임베딩을 사용하여 합성 스피치를 생성할 수 있다. 특히, 다른 추가 사용자 음성 입력에 응답하여 합성 스피치를 생성하는데 사용되는 운율 프로퍼티들의 세트는 운율 프로퍼티들의 제2 세트(454C1B)와 다를 수 있다. 예를 들어, 운율 프로퍼티들의 세트는 프로젝트를 끝내고자 하는 Tim의 흥분과 친구 Jim과 시간을 보내고자 하는 욕구를 반영하기 위해 운율 프로퍼티들의 제2 세트(454C1B)보다 더 큰 강조와 더 큰 소리를 포함할 수 있다. 따라서 운율 프로퍼티들은 추가 참가자(예: Jim)와 환경(예: 집 환경)이 동일하게 유지되더라도 주어진 대화 내에서 달라질 수 있다. 제안들(456C1-456C4) 중 하나의 선택에 추가로 또는 그 대신에 추가 음성 입력 및 제안(456C1-456C4)은 설명의 편의를 위해 제공되며 제한하려는 의도가 아님을 유의해야 한다.Referring specifically to FIG. 4C , the
이제 도 4d로 돌아가면, 클라이언트 디바이스(410)에서 알 수 없는 사용자의 추가 사용자 음성 입력이 검출되고, 알 수 없는 사용자에 대한 화자 임베딩 및/또는 비주얼 임베딩을 저장하기 위한 승인에 위해 알 수 없는 사용자에게 프롬프트된다. 알 수 없는 사용자의 화자 임베딩 및/또는 비주얼 임베딩을 저장함으로써, 알 수 없는 사용자는 알려진 사용자가 되며, 향후 화자 임베딩과 일치하는 음성 활동이 클라이언트 디바이스(410)에서 검출된 경우 및/또는 비주얼 임베딩과 일치하는 비주얼 임베딩이 클라이언트 디바이스(410)에서 이미지로 캡처된 경우 인식된다. 일부 구현예에서, 자동화된 어시스턴트는 클라이언트 디바이스(410)에 대응하는 음성 임베딩 및/또는 비주얼 임베딩을 저장하기 위한 승인을 위해 알 수 없는 사용자에게 프롬프트할 수 있다(예를 들어, 도 2b와 관련하여 설명된 바와 같이). 일부 추가적 및/또는 대안적 구현예에서, 자동화된 어시스턴트는 추가 클라이언트 디바이스에 대응하는 음성 임베딩 및/또는 비주얼 임베딩을 저장하기 위한 승인을 위해 알 수 없는 사용자에게 프롬프트할 수 있다. 상기 구현예 중 일부 버전에서, 프롬프트는 네트워크(들)(예를 들어, 도 1의 네트워크(들)(190))를 통해 클라이언트 디바이스(410)로부터 추가 클라이언트 디바이스에 전송될 수 있고, (예를 들어, 도 3의 추가 참가자(302)의 추가 클라이언트 디바이스(310B)의 사용자 인터페이스(380B)를 통해) 추가 클라이언트 디바이스의 사용자 인터페이스에 렌더링된다. 클라이언트 디바이스(410)는 프롬프트에 응답하여, 화자 임베딩 및/또는 비주얼 임베딩이 클라이언트 디바이스(410)에 로컬적으로 저장될 수 있는지 여부를 나타내는 알 수 없는 사용자로부터 입력(예를 들어, 터치 입력 또는 음성 입력)을 수신할 수 있다. 클라이언트 디바이스가 화자 임베딩 및/또는 비주얼 임베딩이 저장될 수 있음을 나타내는 입력을 수신하면, 알 수 없는 사용자는 알려진 사용자가 되고, 화자 임베딩 및/또는 비주얼 임베딩은 알려진 사용자와 연관하여 클라이언트 디바이스(410)에 로컬적으로 저장될 수 있다. 클라이언트 디바이스가 화자 임베딩 및/또는 비주얼 임베딩을 저장할 수 없음을 나타내는 입력을 수신하면, 알 수 없는 사용자는 알 수 없는 사용자로 남고, 화자 임베딩 및/또는 비주얼 임베딩은 폐기될 수 있다. Returning now to FIG. 4D , additional user voice input of an unknown user is detected at the
예를 들면, 도 5d에 도시된 바와 같이, 클라이언트 디바이스(410)가 클라이언트 디바이스(410)에서 검출된 Tim의 친구 Jim으로부터의 추가 사용자 음성 입력에 대응하는 "Hey Tim, I want you to meet my friend Stan"의 텍스트 세그먼트(452D1)를 결정하고, 사용자 인터페이스(480)를 통해 텍스트 세그먼트(452D1)를 렌더링한다고 가정한다. 클라이언트 디바이스(410)는 Jim으로부터의 추가 사용자 음성 입력에 기초하여 화자 임베딩을 생성할 수 있고, 본 명세서에 기술된 바와 같이(예를 들어, 도 2b, 4b 및 4c와 관련하여) 클라이언트 디바이스(410)에 로컬적으로 저장된 일치하는 화자 임베딩을 식별하는 것에 기초하여 Jim으로부터 발생한 텍스트 세그먼트(452D1)에 대응하는 추가 사용자 음성 입력을 결정할 수 있다. 또한 클라이언트 디바이스(410)가 클라이언트 디바이스(410)에서 검출된 Tim의 친구 Stan으로부터의 추가 사용자 음성 입력에 대응하는 "Nice to meet you, Tim"의 텍스트 세그먼트(454D1)를 결정하고, 사용자 인터페이스(480)를 통해 텍스트 세그먼트(454D1)를 렌더링한다고 가정한다. 클라이언트 디바이스(410)는 Stan으로부터의 추가 사용자 음성 입력에 기초하여 화자 임베딩을 생성할 수 있고, 본 명세서에 기술된 바와 같이(예를 들어, 도 2b, 4b 및 4c와 관련하여) 클라이언트 디바이스(410)에 로컬적으로 저장된 일치하는 화자 임베딩이 없다고 식별하는 것에 기초하여 알 수 없는 사용자로부터 발생한 텍스트 세그먼트(452D2)에 대응하는 추가 사용자 음성 입력을 결정할 수 있다. 비록 도 4d는 화자 임베딩을 사용하여 대화에서 추가 참가자를 식별하는 것과 관련하여 여기에서 설명되지만, 추가 참가자는 비주얼 임베딩에 기초하여 식별될 수도 있음을 이해해야 한다(예: 추가 참가자의 이미지가 본 명세서에 기술된 바와 같이 그래픽 엘리먼트(495)에 대한 사용자 인터페이스 입력에 응답하여 캡처되는 경우). 더욱이, 일부 구현예에서, 클라이언트 디바이스(410)가 알 수 없는 사용자에 대한 대응하는 화자 임베딩을 저장하기 위한 승인을 수신하는 경우, 클라이언트 디바이스(410)는 또한 알 수 없는 사용자에 대한 비주얼 임베딩이 알 수 없는 사용자를 위한 화자 임베딩에 추가하여 설정될 수 있도록 알 수 없는 사용자의 이미지를 요청하는 프롬프트를 렌더링할 수 있다. For example, as shown in FIG. 5D , the
또한, 알 수 없는 사용자로부터 유래된 텍스트 세그먼트(452D2)에 대응하는 추가 사용자 음성 입력을 결정하는 것에 기초하여, 자동화된 어시스턴트는 "Hi Stan, can I store your speaker embedding locally on this device to recognize you in the future?"의 프롬프트(456D1)를 생성할 수 있고, 클라이언트 디바이스(410) 또는 알 수 없는 사용자의 추가 클라이언트 디바이스(예를 들어, 도 3의 추가 클라이언트 디바이스(310B))에 의해 청각적으로 및/또는 시각적으로 프롬프트(456D1)를 렌더링할 수 있다. 또한 클라이언트 디바이스(410)가 클라이언트 디바이스(410)에서 검출된 Stan으로부터의 다른 추가 사용자 음성 입력에 대응하는 "Yes"의 텍스트 세그먼트(454D2)를 결정하고, 사용자 인터페이스(480)를 통해 텍스트 세그먼트(454D2)를 렌더링한다고 가정한다. 따라서, 텍스트 세그먼트(454D2)에 의해 표시된 바와 같이, Stan으로부터 승인을 수신하는 클라이언트 디바이스(410)에 기초하여, 클라이언트 디바이스(410)는 Stan과 연관하여 클라이언트 디바이스에 로컬적으로 Stan에 대한 화자 임베딩을 저장하고(예를 들어, 도 1의 화자 임베딩(들) 데이터베이스(112a), Stan은 클라이언트 디바이스(410)에 대해 알려진 사용자가 될 수 있다. 또한, Stan에 대한 화자 임베딩은 클라이언트 디바이스(410)에 의해 활용되어, 미래에 Stan으로부터 발생하는 임의의 추가 사용자 음성 입력을 식별할 수 있다. Further, based on determining additional user speech input corresponding to text segment 452D2 originating from an unknown user, the automated assistant may display "Hi Stan, can I store your speaker embedding locally on this device to recognize you in the future?" prompt 456D1, audibly and by
또한, 클라이언트 디바이스(410)는 Tim으로부터 "Very nice to meet you, Stan"의 사용자 인터페이스 입력을 검출할 수 있다. 사용자 인터페이스 입력이 음성 입력이라고 가정하면, 클라이언트 디바이스(410)는 하나 이상의 스피치 인식 모델(들)(예를 들어, 도 1의 스피치 인식 모델(들)(120A))을 사용하여 사용자 인터페이스 입력을 프로세싱하여, Tim으로부터의 음성 입력에 대응하는 "Very nice to meet you, Stan!"의 텍스트 세그먼트(458D1A)를 결정한다. 더욱이, 클라이언트 디바이스(410)는 적어도 추가 참가자들이 Jim(예를 들어, Tim의 친구) 및 Stan(예를 들어, Jim의 친구)인 것에 기초하여 운율 프로퍼티들의 제3 세트(458D1B)를 결정할 수 있다. 따라서, 클라이언트 디바이스(410)는 (예를 들어, 음파들(476)에 의해 표시된 바와 같이 그리고 클라이언트 디바이스(410) 및/또는 추가 컴퓨팅 디바이스(예를 들어, 도 3의 컴퓨팅 디바이스(310A))의 스피커(들)를 사용하여) 클라이언트 디바이스(410)에서 사용자 인터페이스 입력을 수신하는 것에 응답하여 텍스트 세그먼트(458D1A)를 포함하고 운율 프로퍼티의 제3 세트(458D1B)로 합성되는 합성 스피치를 생성하고 청각적으로 렌더링할 수 있다. 또한, 텍스트 세그먼트(458D1A)에 대한 사용자 인터페이스 입력은 Tim이 임의의 추가 음성 입력을 제공할 필요 없이 합성 스피치가 다시 청각적으로 렌더링되게 할 수 있다.Also, the
더욱이, 일부 구현예에서 대화에 다수의 추가 참가자들이 있는 경우, 클라이언트 디바이스(410)는 합성 스피치를 생성할 때 다수의 추가 참가자들 각각에 대한 운율 프로퍼티들의 상이한 세트들 중에서 운율 프로퍼티들의 보다 "형식적인" 세트를 활용할 수 있다. 예를 들어, 비록 도 4d에 도시된 대화는 Tim의 친구 Jim을 포함하지만, 운율 프로퍼티의 제3 세트(458D1B)는 Tim이 Tim과만 대화를 나누고 있는 경우 활용되는 운율 프로퍼티들의 제2 세트(454C1B)보다 "형식적"이다(예를 들어, 도 4c 및 도 4d의 비교 표시자(444)에 의해 표시된 바와 같음). 이 예에서, Stan이 Jim의 친구이지만, Tim과 Stan은 이제 막 만났을 뿐이고 클라이언트 디바이스(410)는 알 수 없는 사용자 및/또는 클라이언트 디바이스(410)에 최근에 알려진 사용자에 대해 기본 운율 프로퍼티들의 더 "형식적인" 세트(예를 들어, 운율 프로퍼티의 제3 세트(458D1B))를 활용할 수 있다. 일부 다른 구현예에서 대화에 다수의 추가 참가자들이 있는 경우, 클라이언트 디바이스(410)는 합성 스피치를 생성할 때 다수의 추가 참가자들 각각에 대한 운율 프로퍼티들의 상이한 세트들 중에서 운율 프로퍼티들의 평균 세트를 활용할 수 있다. 예를 들어, 운율 프로퍼티들의 제2 세트(454C1B)가 Jim과 연관되어 있다고 가정하고, 운율 프로퍼티들의 제1 세트(454B1B)가 알 수 없는 사용자 및/또는 최근에 클라이언트 디바이스(410)에 알려진 사용자에 대한 기본 운율 프로퍼티 세트로서 Stan과 연관되어 있다고 가정한다. 이 예에서, 도 4d의 표시자(444)는 운율 프로퍼티들의 제3 세트(458D1B)가 운율 프로퍼티의 제1 세트(454B1B)(예를 들어, 도 4b의 표시자(444)에 의해 도시됨) 및 운율 프로퍼티의 제2 세트(454C1B)(예를 들어, 도 4c의 표시자(444)에 의해 도시됨)의 평균임을 표시한다. Moreover, in some implementations where there are multiple additional participants in a conversation, the
또한, 일부 구현예에서, 대화의 주어진 추가 참가자와 연관된 운율 프로퍼티들은 시간이 지남에 따라 조정될 수 있다. 예를 들어, Tim이 직접 또는 전화 통화, 문자 메시지, SMS 메시지, 이메일 및/또는 기타 형태의 의사 소통을 통해 Stan과 추가 대화에 참여한다고 가정한다. Tim이 Stan과 이러한 추가 대화에 참여함에 따라, Stan과 연관된 운율 프로퍼티들은 시간이 지남에 따라 관계의 변화를 반영하기 위해 보다 일상적이거나 비형식적으로 조정될 수 있다. 이러한 방식으로 합성 스피치를 생성하는데 사용되는 운율 프로퍼티들의 세트는 주어진 대화의 다수의 추가 참가자 각각에 기초하여 주어진 대화 전체에서 조정될 수 있으며, 시간이 지남에 따라 관계가 어떻게 발전하는지에 따라 조정될 수도 있다. Also, in some implementations, prosody properties associated with a given additional participant of a conversation may be adjusted over time. For example, suppose Tim engages in further conversations with Stan, either directly or through phone calls, text messages, SMS messages, emails and/or other forms of communication. As Tim engages in these additional conversations with Stan, the prosody properties associated with Stan can be adjusted more routinely or informally to reflect changes in the relationship over time. The set of prosody properties used to generate synthetic speech in this way may be adjusted throughout a given conversation based on each of a number of additional participants in the given conversation, and may also be adjusted as the relationship develops over time.
이제 도 4e 및 도 4f를 참조하면, 잠금 화면이 클라이언트 디바이스(410)의 사용자 인터페이스(480)에 도시된다(예를 들어, 클라이언트 디바이스(410)의 사용자 인터페이스(480) 상의 시간 및 날짜 정보에 의해 표시됨). 일부 구현예에서, 클라이언트 디바이스(410)는 클라이언트 디바이스(410)가 잠겨 있을 때 추가 사용자 음성 입력을 여전히 검출할 수 있다. 상기 구현예 중 일부 버전에서, 추가 사용자가 Tim의 친구 Jim과 같은 알려진 사용자인 경우 추가 사용자의 표시는 도 4e의 그래픽 엘리먼트(462J)와 같이 잠금 화면 상에 렌더링될 수 있다. 더욱이, 그래픽 엘리먼트(464)는 클라이언트 디바이스(410)의 사용자 인터페이스(480) 상에서 렌더링될 수 있다. 클라이언트 디바이스(410)는, 그래픽 엘리먼트(464)가 선택된 경우, 추가 사용자 음성 입력의 전사가 클라이언트 디바이스(410)의 잠금 화면에 디스플레이되게 할 수 있거나(예를 들어, 도 4f에 도시된 바와 같이) 또는 클라이언트 디바이스(410)가 잠금 해제되고 전사가 사용자 인터페이스(480)를 통해 디스플레이되게 할 수 있다(예를 들어, 도 4a-4d에 도시된 바와 같이). Referring now to FIGS. 4E and 4F , a lock screen is shown in the
예를 들어, 클라이언트 디바이스(410)가 그래픽 엘리먼트(464)에 대한 사용자 인터페이스 입력을 검출하고, 클라이언트 디바이스(410)에서 검출된 Jim으로부터의 추가 사용자 음성 입력에 대응하는 "What did you think of the new Acme movie?"의 텍스트 세그먼트(452F1)를 결정하고, 사용자 인터페이스(480)를 통해 텍스트 세그먼트(452F1)를 렌더링한다고 가정한다. 도 4f에 도시된 바와 같이, 클라이언트 디바이스(410)는 Jim으로부터의 추가 사용자 음성 입력에 응답하는 Tim으로부터의 음성 입력에 대응하는 전사(470)를 결정하고, 텍스트 세그먼트 사용자 인터페이스(481)를 통해 전사를 렌더링한다. 특히, 추가 음성 입력의 전사(470)는 불완전하고(예를 들어, 타원(472) 및 커서(474)로 표시된 바와 같이), Tim은 추가 음성 입력이 완료되었다는 임의의 표시 전에(예: 음성 입력의 완료를 검출하기 위한 엔드-포인팅 및/또는 기타 기법) 텍스트 세그먼트 사용자 인터페이스(481)를 향해 터치 입력 및/또는 음성 입력을 제공함으로써 전사(470)의 임의의 부분을 편집할 수 있다. 또한, 후보 텍스트 세그먼트를 포함하는 제1 제안(454F1)은 전사(470)와 함께 시각적으로 렌더링될 수 있다. 클라이언트 디바이스(410)는 (예를 들어, 도 1의 자동 제안 엔진(150)과 관련하여) 본 명세서에서 더 상세히 설명되는 자동 제안 엔진을 사용하여 제1 제안(454F1)을 생성할 수 있다. 또한, 제안(454F2)은 전사(470) 및 제1 제안(454F2)과 함께 시각적으로 렌더링될 수 있으며, 선택된 경우 추가 제안이 텍스트 세그먼트 사용자 인터페이스(481)를 통해 시각적으로 렌더링되게 할 수 있다. 클라이언트 디바이스(410)에서 제1 제안(454F1)에 대한 추가 사용자 인터페이스 입력을 수신하면, 클라이언트 디바이스(410)는 후보 텍스트 세그먼트, 제1 제안(454F2)(예를 들어, "It was really good!")을 전사(470)에 포함하거나, 클라이언트 디바이스(410)는 도 4a에서 설정된 Tim의 스피치 임베딩을 사용하여 (예를 들어, 전사(470)에 포함된 텍스트 세그먼트 대신에) 후보 텍스트 세그먼트를 포함하고, 본 명세서에 기술된 운율 속성들의 세트로 합성되는, 합성 스피치를 생성할 수 있다. 클라이언트 디바이스가 잠겨 있는 동안 클라이언트 디바이스(410)가 텍스트 세그먼트(452F1) 및 전사(470)를 사용자 인터페이스(480)에 렌더링하는 구현예에서, 사용자 인터페이스 입력은 그래픽 엘리먼트(499)에 대한 것인 경우, 클라이언트 디바이스(410)를 다시 도 4e에 도시된 잠금 화면으로 되돌릴 수 있다. For example,
도 5은 본 명세서에 기술된 기법들의 하나 이상의 양태들을 수행하기 위해 선택적으로 활용될 수 있는 예시적 컴퓨팅 디바이스(510)의 블록도이다. 일부 구현예에서, 클라이언트 디바이스, 클라우드 기반 자동화된 어시스턴트 컴포넌트(들) 및/또는 다른 컴포넌트(들) 중 하나 이상은 예시적 컴퓨팅 디바이스(510) 중 하나 이상의 컴포넌트를 포함할 수 있다. 5 is a block diagram of an
컴퓨팅 디바이스(510)는 일반적으로 적어도 하나의 프로세서(514)를 포함하며, 버스 서브시스템(512)을 통해 다수의 주변 디바이스들과 통신한다. 이들 주변 디바이스들은 예를 들면, 메모리 서브시스템(525) 및 파일 저장 서브시스템(526)을 포함하는 저장 서브시스템(524), 사용자 인터페이스 출력 디바이스(520), 사용자 인터페이스 입력 디바이스(522) 및 네트워크 인터페이스 서브시스템(516)을 포함할 수 있다. 입력 및 출력 디바이스는 컴퓨팅 디바이스(510)와 사용자 인터렉션을 하게 한다. 네트워크 인터페이스 서브시스템(516)은 외부 네트워크에 대한 인터페이스를 제공하며, 다른 컴퓨팅 디바이스들의 대응하는 인터페이스 디바이스들과 연결된다.
사용자 인터페이스 입력 디바이스(522)는 키보드, 마우스, 트랙볼, 터치패드 또는 그래픽 태블릿, 스캐너, 디스플레이에 통합된 터치스크린과 같은 포인팅 디바이스, 음성 인식 시스템, 마이크로폰과 같은 오디오 입력 디바이스 및/또는 다른 유형의 입력 디바이스를 포함한다. 일반적으로, 용어 “입력 디바이스”의 사용은 정보를 컴퓨팅 디바이스(510) 또는 통신 네트워크에 입력하기 위한 모든 가능한 유형의 디바이스들과 방식들을 포함하도록 의도된다.User
사용자 인터페이스 출력 디바이스(520)는 디스플레이 서브시스템, 프린터, 팩스 기계 또는 오디오 출력 디바이스와 같은 비-시각적 디스플레이를 포함할 수 있다. 디스플레이 서브시스템은 CRT, LCD와 같은 평면 패널 디바이스, 프로젝션 디바이스 또는 시각적 이미지를 생성하기 위한 일부 기타 메커니즘을 포함할 수 있다. 또한, 디스플레이 서브시스템은 오디오 출력 디바이스와 같은 비-시각적 디스플레이를 제공할 수 있다. 일반적으로, 용어 “출력 디바이스”의 사용은 정보를 컴퓨팅 디바이스(510)로부터 사용자에게 또는 다른 기계 또는 컴퓨팅 디바이스에 정보를 출력하기 위한 모든 가능한 유형의 디바이스들과 방식들을 포함하도록 의도된다.User
저장 서브시스템(524)은 본 명세서에 기술된 일부 또는 전부의 모듈들의 기능을 제공하기 위한 프로그래밍 및 데이터 구조를 저장한다. 예를 들면, 저장 서브시스템(524)은 본 명세서에 개시된 방법의 선택된 양태들을 수행하고 뿐만 아니라 도 1에 도시된 다양한 컴포넌트들을 구현하기 위한 로직을 포함할 수 있다.
이들 소프트웨어 모듈들은 일반적으로 프로세서(514) 단독으로 또는 다른 프로세서들과의 조합에 의해 실행된다. 저장 서브시스템(524)에서 사용된 메모리(525)는 프로그램 실행 중에 명령어들 및 데이터의 저장을 위한 메인 RAM(530) 및 고정된 명령어들이 저장되는 ROM(532)을 포함하는 다수의 메모리들을 포함할 수 있다. 파일 저장 서브시스템(526)은 프로그램 및 데이터 파일에 대한 영구적 저장을 제공할 수 있고, 하드 디스크 드라이브, 연관된 이동식 매체와 함께인 플로피 디스크 드라이브, CD-ROM 드라이브, 광학 드라이브 또는 이동식 매체 카트리지들을 포함할 수 있다. 특정 구현예들의 기능을 구현하는 모듈들은 파일 저장 서브시스템(526)에 의해 저장 서브시스템(524)에 또는 프로세서(들)(514)에 의해 액세스가능한 다른 기계에 저장될 수 있다.These software modules are typically executed by the
버스 서브시스템(512)은 의도된 대로 컴퓨팅 디바이스(510)의 다양한 컴포넌트들 및 서브시스템들이 서로 통신하게 하기 위한 메커니즘을 제공한다. 버스 서브시스템(512)이 개략적으로 단일의 버스로 도시되었지만, 버스 서브시스템(512)의 대안적 구현예들은 다수의 버스들을 사용할 수 있다.
컴퓨팅 디바이스(510)는 워크스테이션, 서버, 컴퓨팅 클러스터, 블레이드 서버, 서퍼팜 또는 임의의 기타 데이터 프로세싱 시스템 또는 컴퓨팅 디바이스를 포함하는 다양한 유형들일 수 있다. 컴퓨터 및 네트워크의 끊임없이 변화하는 특성으로 인해, 도 5에 도시된 컴퓨팅 디바이스(510)는 일부 구현예를 설명하기 위한 목적의 특정 예로서만 의도된다. 컴퓨팅 디바이스(510)의 많은 다른 구성이 도 5에 도시된 컴퓨팅 디바이스보다 많거나 적은 컴포넌트를 가질 수 있다.
본 명세서에서 논의된 시스템들이 사용자들에 관한 개인 정보를 수집 또는 모니터링하거나 또는 개인 및/또는 모니터링된 정보를 사용하는 경우에 있어서, 사용자에게 프로그램 또는 구성이 사용자 정보(예를 들면, 사용자의 소셜 네트워크, 소셜 액션 또는 활동, 직업, 사용자의 선호 또는 사용자의 현재 지리적 위치)에 관한 정보를 수집할 것인지 여부를 제어할, 사용자와 더 관련된 콘텐츠 서버로부터의 콘텐츠를 수신할지 및/또는 어떻게 수신할지 제어할 기회가 제공될 수 있다. 추가로, 특정 데이터는 그것이 저장되거나 사용되기 전에 하나 이상의 다양한 방식들로 취급되어, 개인적으로 식별가능한 정보는 제거된다. 예를 들면, 사용자의 신원은 사용자에 관한 개인적으로 식별가능한 정보가 결정될 수 없도록 취급되거나 또는 사용자의 지리적 위치는 위치 정보가 획득된 곳에서 일반화되어(시, 우편번호 또는 주 수준으로), 사용자의 특정한 지리적 위치가 결정될 수 없도록 한다. 따라서, 사용자는 사용자에 관한 정보가 어떻게 수집되는지 그리고 사용되는지에 관한 제어를 가질 수 있다. In the case where the systems discussed herein collect or monitor personal information about users or use personal and/or monitored information, the program or configuration to the user informs the user that the user information (eg, the user's social network , to control whether information about social actions or activities, occupation, user preferences or current geographic location) is collected; to control whether and/or how to receive content from content servers that are more relevant to the user; Opportunities may be provided. Additionally, certain data is treated in one or more various ways before it is stored or used, so that personally identifiable information is removed. For example, the user's identity may be treated such that personally identifiable information about the user cannot be determined, or the user's geographic location may be generalized (at the city, zip code or state level) from where the location information was obtained. Ensure that no specific geographic location can be determined. Thus, the user can have control over how information about the user is collected and used.
일부 구현예에서, 방법은 하나 이상의 프로세서들에 의해 구현되고, 주어진 사용자의 클라이언트 디바이스에서 적어도 하나의 사용자 인터페이스 입력에 기초하여, 상기 주어진 사용자가 참가자인 대화에서 전달하기 위한 텍스트 세그먼트를 결정하는 단계, 상기 대화에서 추가 참가자를 식별하는 단계, 상기 추가 참가자는 상기 주어진 사용자에 추가되며, 상기 주어진 사용자와 상기 추가 참가자 간의 관계의 적어도 하나의 속성에 기초하여 하나 이상의 운율 프로퍼티들을 결정하는 단계, 및 텍스트 세그먼트를 통합하고 하나 이상의 운율 프로퍼티들로 합성되는 합성 스피치를 포함하는 합성 스피치 오디오 데이터를 생성하는 단계를 포함한다. 합성 스피치 오디오 데이터를 생성하는 단계는 주어진 사용자와 추가 참가자 간의 관계의 속성에 기초하여 운율 프로퍼티들을 결정하는 것에 응답하여 하나 이상의 운율 프로퍼티들을 가진 합성 스피치를 합성하는 단계를 포함한다. 방법은 합성 스피치가 클라이언트 디바이스 및/또는 추가 클라이언트 디바이스의 하나 이상의 스피커들을 통해 렌더링되게 하는 단계를 더 포함한다. 렌더링된 합성 스피치는 추가 참가자가 청각적으로 인지할 수 있다.In some implementations, a method is implemented by one or more processors, comprising: determining, based on at least one user interface input at a client device of a given user, a text segment to convey in a conversation in which the given user is a participant; identifying an additional participant in the conversation, the additional participant being added to the given user, determining one or more prosody properties based on at least one attribute of a relationship between the given user and the additional participant, and a text segment and generating synthesized speech audio data comprising synthesized speech synthesized with one or more prosody properties. Generating the synthesized speech audio data includes synthesizing the synthesized speech having one or more prosody properties in response to determining the prosody properties based on a property of a relationship between the given user and the additional participant. The method further includes causing the synthesized speech to be rendered through one or more speakers of the client device and/or the additional client device. The rendered synthetic speech can be perceived aurally by an additional participant.
본 명세서에서 개시된 기술의 이들 또는 다른 구현예들은 다음 구성들 중 하나 이상을 각각 선택적으로 포함할 수 있다. These or other implementations of the techniques disclosed herein may each optionally include one or more of the following features.
일부 구현예에서, 상기 적어도 하나의 사용자 인터페이스 입력은 상기 클라이언트 디바이스의 하나 이상의 마이크로폰들을 통해 검출되는 주어진 사용자의 음성 입력을 포함할 수 있고, 상기 클라이언트 디바이스에서 적어도 하나의 사용자 인터페이스 입력에 기초하여, 상기 텍스트 세그먼트를 결정하는 단계는 상기 텍스트 세그먼트를 생성하기 위해 스피치 인식 모델을 사용하여 상기 음성 입력을 프로세싱하는 단계를 포함할 수 있다. 상기 구현예 중 일부 버전에서, 상기 스피치 인식 모델은 온-디바이스(on-device) 스피치 인식 모델이고 및/또는 언어 장애가 있는 사용자들의 스피치를 인식하도록 트레이닝된다.In some implementations, the at least one user interface input may include a given user's voice input detected via one or more microphones of the client device, and based on at least one user interface input at the client device, the Determining the text segment may include processing the speech input using a speech recognition model to generate the text segment. In some versions of the above implementations, the speech recognition model is an on-device speech recognition model and/or is trained to recognize speech of users with language disabilities.
일부 구현예에서, 상기 적어도 하나의 사용자 인터페이스 입력은 텍스트 세그먼트를 전달하는 그래픽 엘리먼트의 선택을 포함할 수 있고, 상기 텍스트 세그먼트를 전달하는 그래픽 엘리먼트는 대안적인 텍스트 세그먼트를 전달하는 적어도 하나의 대안적인 선택가능한 그래픽 엘리먼트와 함께 상기 클라이언트 디바이스의 디스플레이에 디스플레이될 수 있다. 이러한 구현예의 일부 버전에서, 상기 방법은, 클라이언트 디바이스에서 사용자 인터페이스 입력 전에 상기 클라이언트 디바이스의 하나 이상의 마이크로폰들을 통해, 상기 추가 참가자의 추가 참가자 음성 입력을 검출하는 단계, 상기 추가 참가자 음성 입력의 인식인 추가 참가자 텍스트 세그먼트를 생성하기 위해 스피치 인식 모델을 사용하여 상기 추가 참가자 음성 입력을 프로세싱하는 단계, 상기 텍스트 세그먼트가 상기 추가 참가자 텍스트 세그먼트에 대한 후보 응답이라고 결정하는 단계, 및 상기 텍스트 세그먼트가 상기 추가 참가자 텍스트 세그먼트에 대한 후보 응답이라는 결정에 응답하여 상기 텍스트 세그먼트를 전달하는 상기 그래픽 엘리먼트를 디스플레이하도록 결정하는 단계를 포함한다.In some implementations, the at least one user interface input can include a selection of a graphical element carrying a text segment, wherein the graphical element carrying an alternative text segment is at least one alternative selection carrying an alternative text segment. It may be displayed on the display of the client device along with possible graphical elements. In some versions of this implementation, the method further comprises: detecting, via one or more microphones of the client device, an additional participant voice input of the additional participant prior to user interface input at the client device, recognition of the additional participant voice input processing the additional participant voice input using a speech recognition model to generate a participant text segment, determining that the text segment is a candidate response to the additional participant text segment, and wherein the text segment is the additional participant text and determining to display the graphical element conveying the text segment in response to determining that it is a candidate response for the segment.
상기 구현예 중 일부 추가 버전에서, 대화에서 추가 참가자를 식별하는 단계는 상기 추가 참가자 음성 입력을 사용하여 화자 식별을 수행하는 단계, 및 상기 화자 식별에 기초하여 상기 추가 참가자를 식별하는 단계를 더 포함한다. 상기 구현예 중 또 다른 버전에서, 화자 식별을 수행하는 단계는 상기 클라이언트 디바이스에서, 화자 식별 모델을 사용하여 상기 추가 참가자 음성 입력을 프로세싱하는 것에 기초하여 음성 입력 임베딩을 생성하는 단계, 및 상기 클라이언트 디바이스에서, 상기 음성 입력 임베딩을 추가 참가자에 대한 미리 저장된 임베딩과 비교하는 단계를 포함하며, 상기 미리 저장된 임베딩은 상기 추가 참가자에 의한 승인에 응답하여 상기 클라이언트 디바이스에 미리 로컬적으로 저장되어 있다.In some further versions of the above embodiments, identifying the additional participant in the conversation further comprises: performing speaker identification using the additional participant voice input; and identifying the additional participant based on the speaker identification. do. In yet another version of the implementation, performing speaker identification comprises generating, at the client device, a voice input embedding based on processing the additional participant voice input using a speaker identification model, and at the client device; comparing the speech input embedding to a pre-stored embedding for an additional participant, wherein the pre-stored embedding is previously stored locally on the client device in response to approval by the additional participant.
상기 구현예 중 일부 추가 버전에서, 상기 텍스트 세그먼트가 상기 추가 참가자 텍스트 세그먼트에 대한 후보 응답이라고 결정하는 단계는 상기 주어진 사용자와 상기 추가 참가자 간의 관계의 속성들 중 적어도 하나에 더 기초할 수 있다. 상기 구현예 중 또 다른 버전에서, 상기 텍스트 세그먼트가 상기 추가 참가자 텍스트 세그먼트에 대한 후보 응답이라고 결정하는 단계 상기 주어진 사용자와 상기 추가 참가자 간의 관계의 속성들 중 적어도 하나에 더 기초한다는 것은 상기 추가 참가자 텍스트 세그먼트에 기초하여 초기 후보 응답들의 수퍼세트를 생성하는 단계, 상기 수퍼세트는 상기 텍스트 세그먼트를 포함하며, 및 상기 초기 후보 응답의 슈퍼세트로부터, 상기 주어진 사용자와 상기 추가 참가자 사이의 관계의 속성들 중 적어도 하나에 기초하여 상기 후보 응답으로서 상기 텍스트 세그먼트를 선택하는 단계를 포함할 수 있다.In some further versions of the embodiments above, determining that the text segment is a candidate response to the additional participant text segment may further be based on at least one of attributes of a relationship between the given user and the additional participant. In yet another version of the embodiment, determining that the text segment is a candidate response to the additional participant text segment is further based on at least one of attributes of a relationship between the given user and the additional participant text. generating a superset of initial candidate responses based on a segment, the superset including the text segment, and from the superset of the initial candidate responses, among the attributes of the relationship between the given user and the additional participant. selecting the text segment as the candidate response based on at least one.
이러한 구현예의 일부 추가 버전에서, 상기 방법은, 상기 클라이언트 디바이스의 위치에 대한 적어도 하나의 분류를 결정하는 단계를 더 포함할 수 있다. 상기 텍스트 세그먼트가 상기 추가 참가자 텍스트 세그먼트에 대한 후보 응답이라고 결정하는 단계는 상기 위치의 적어도 하나의 분류에 더 기초할 수 있다.In some further versions of this implementation, the method may further comprise determining at least one classification for the location of the client device. Determining that the text segment is a candidate response to the additional participant text segment may further be based on the at least one classification of the location.
일부 구현예에서, 상기 방법은, 상기 대화에서 다른 추가 참가자를 식별하는 단계, 상기 다른 추가 참가자는 상기 주어진 사용자에 추가되고 상기 추가 참가자에 추가되며, 그리고 상기 하나 이상의 운율 프로퍼티들을 결정하는 단계를 포함하며, 상기 하나 이상의 운율 프로퍼티들을 결정하는 단계는: (a) 상기 주어진 사용자와 상기 추가 참가자 간의 관계의 속성, 및 (b) 상기 주어진 사용자와 상기 다른 추가 참가자 간의 추가 관계의 하나 이상의 추가 속성들에 기초한다.In some implementations, the method includes identifying another additional participant in the conversation, the additional additional participant being added to the given user and added to the additional participant, and determining the one or more prosody properties. wherein determining the one or more prosody properties comprises: (a) an attribute of the relationship between the given user and the additional participant, and (b) one or more additional attributes of the additional relationship between the given user and the other additional participant. based on
일부 구현예에서, 상기 방법은, 상기 대화에서 다른 추가 참가자를 식별하는 단계, 상기 다른 추가 참가자는 상기 주어진 사용자에 추가되고 상기 추가 참가자에 추가되며, 상기 주어진 사용자와 상기 다른 추가 참가자 간의 추가 관계의 하나 이상의 추가 속성들 대신에, 상기 주어진 사용자와 상기 추가 참가자 간의 관계의 속성에 기초하여 상기 하나 이상의 운율 프로퍼티들을 결정하는 단계를 포함하며, 상기 하나 이상의 운율 프로퍼티들을 결정하는 단계는: 상기 주어진 사용자와 상기 추가 참가자 간의 관계가 상기 주어진 사용자와 상기 추가 참가자 간의 추가 관계보다 더 형식적이라고 결정하는 것에 대한 응답이다.In some embodiments, the method further comprises: identifying another additional participant in the conversation, the other additional participant being added to the given user and added to the additional participant, wherein the further relationship between the given user and the other additional participant is determining the one or more prosody properties based on an attribute of a relationship between the given user and the additional participant, instead of one or more additional attributes, wherein determining the one or more prosody properties comprises: the given user and in response to determining that the relationship between the additional participant is more formal than the additional relationship between the given user and the additional participant.
일부 구현예에서, 대화에서 추가 참가자를 식별하는 단계는 상기 클라이언트 디바이스의 카메라에 의해 캡처된 하나 이상의 이미지들에 기초하여 상기 추가 참가자를 식별하는 단계를 더 포함할 수 있다. 상기 구현예 중 일부 버전에서, 상기 카메라는 조정 가능한 뷰포트를 포함하고, 상기 하나 이상의 이미지들은 상기 주어진 사용자의 클라이언트 디바이스에서 다른 사용자 인터페이스 입력에 응답하여 상기 조정 가능한 뷰포트의 조정 후에 캡처될 수 있다. 상기 구현예 중 일부 버전에서, 상기 클라이언트 디바이스의 카메라에 의해 캡처된 이미지에 기초하여 상기 추가 참가자를 식별하는 단계는: 상기 클라이언트 디바이스의 디스플레이 상에 상기 하나 이상의 이미지들 중 이미지를 렌더링하는 단계, 상기 이미지는 상기 추가 참가자를 포함하는 다수의 사람들을 캡처하며, 상기 추가 참가자를 캡처하고 상기 다른 다수의 사람들을 모두 제외하는 상기 이미지의 영역을 지정하는 상기 주어진 사용자의 상기 클라이언트 디바이스에서 다른 사용자 인터페이스 입력을 수신하는 단계, 및 상기 이미지 영역의 프로세싱에 기초하여 상기 추가 참가자를 식별하는 단계를 포함할 수 있다.In some implementations, identifying the additional participant in the conversation can further include identifying the additional participant based on one or more images captured by a camera of the client device. In some versions of the implementations, the camera includes an adjustable viewport, and the one or more images can be captured after adjustment of the adjustable viewport in response to another user interface input at the given user's client device. In some versions of the implementations, identifying the additional participant based on an image captured by a camera of the client device comprises: rendering an image of the one or more images on a display of the client device; the image captures a plurality of people including the additional participant, and further user interface input at the client device of the given user specifying an area of the image that captures the additional participant and excludes all of the other plurality of people. receiving, and identifying the additional participant based on processing of the image region.
일부 구현예에서, 상기 방법은 상기 클라이언트 디바이스의 위치에 대한 적어도 하나의 분류를 결정하는 단계를 더 포함할 수 있다. 상기 운율 프로퍼티들을 결정하는 단계는 상기 클라이언트 디바이스의 위치 분류에 추가로 기초할 수 있다.In some implementations, the method may further comprise determining at least one classification for the location of the client device. Determining the prosody properties may be further based on a location classification of the client device.
일부 구현예에서, 상기 하나 이상의 운율 프로퍼티들은 억양, 톤, 강세 및 리듬 중 하나 이상을 포함할 수 있다. In some embodiments, the one or more prosody properties may include one or more of intonation, tone, stress and rhythm.
일부 구현예에서, 방법은 하나 이상의 프로세서들에 의해 구현되고, 클라이언트 디바이스의 환경에서 주어진 사용자로부터의 것이 아닌 음성 활동이 있다고 결정하기 위해, 주어진 사용자의 클라이언트 디바이스의 하나 이상의 마이크로폰들을 통해 검출된 추가 사용자 음성 입력을 프로세싱하는 단계를 포함하며, 상기 추가 사용자 음성 입력은 상기 클라이언트 디바이스와 상기 주어진 사용자가 있는 환경에 있는 추가 사용자의 것이다. 방법은 또한 상기 주어진 사용자로부터의 것이 아닌 음성 활동이 있다는 결정에 응답하여, 상기 음성 활동의 그래픽 표시가 상기 클라이언트 디바이스의 디스플레이에서 렌더링되게 하는 단계, 및 상기 그래픽 표시의 선택을 수신하는 단계를 더 포함한다. 상기 방법은 상기 선택을 수신함에 응답하여: 주어진 사용자 인식 텍스트를 생성하기 위해 스피치 인식 모델을 사용하여 주어진 사용자 음성 입력을 프로세싱하는 단계, 상기 주어진 사용자 음성 입력은 상기 클라이언트 디바이스의 하나 이상의 마이크로폰들을 통해 검출되고, 상기 주어진 사용자 음성 입력은 주어진 사용자의 것이고 상기 추가 사용자 음성 입력에 후속하여 제공되며; 상기 주어진 사용자 인식 텍스트를 통합하는 합성 스피치를 포함하는 합성 스피치 오디오 데이터를 생성하는 단계, 및 합성 스피치가 클라이언트 디바이스 및/또는 추가 클라이언트 디바이스의 하나 이상의 스피커들을 통해 렌더링되게 하는 단계를 더 포함한다. 렌더링된 합성 스피치는 추가 사용자가 청각적으로 인지할 수 있다.In some implementations, the method is implemented by one or more processors, to determine that there is voice activity in the environment of the client device that is not from the given user, the additional user detected via one or more microphones of the given user's client device. processing voice input, wherein the additional user voice input is of an additional user in an environment with the client device and the given user. The method also includes, in response to determining that there is a voice activity that is not from the given user, causing a graphical representation of the voice activity to be rendered on a display of the client device, and receiving a selection of the graphical representation. do. In response to receiving the selection, the method includes: processing a given user voice input using a speech recognition model to generate a given user recognized text, the given user voice input detected via one or more microphones of the client device wherein the given user voice input is of a given user and is provided subsequent to the additional user voice input; generating synthesized speech audio data comprising synthesized speech incorporating the given user recognized text, and causing the synthesized speech to be rendered via one or more speakers of the client device and/or additional client device. The rendered synthetic speech may be perceived aurally by a further user.
본 명세서에서 개시된 기술의 이들 또는 다른 구현예들은 다음 구성들 중 하나 이상을 각각 선택적으로 포함할 수 있다. These or other implementations of the techniques disclosed herein may each optionally include one or more of the following features.
일부 구현예에서, 상기 주어진 사용자로부터의 것이 아닌 음성 활동이 있다고 결정하기 위해 상기 추가 사용자 음성 입력을 프로세싱하는 단계는: 상기 추가 사용자 음성 입력을 사용하여 화자 식별을 수행하는 단계, 및 상기 화자 식별에 기초하여 상기 주어진 사용자로부터의 것이 아닌 음성 활동이 있다고 결정하는 단계를 포함할 수 있다. 상기 구현예 중 일부 버전에서, 상기 화자 식별에 기초하여 상기 주어진 사용자로부터의 것이 아닌 음성 활동이 있다고 결정하는 단계는: 상기 클라이언트 디바이스에서, 상기 추가 사용자 음성 입력 프로세싱에 기초하여 음성 입력 임베딩을 생성하는 단계, 및 상기 음성 입력 임베딩이 상기 주어진 사용자에 대해 미리 저장된 임베딩과 일치하지 않는지 결정하는 단계를 포함할 수 있다.In some implementations, processing the additional user voice input to determine that there is voice activity that is not from the given user comprises: performing speaker identification using the additional user voice input; based on determining that there is voice activity that is not from the given user. In some versions of the implementations, determining that there is voice activity that is not from the given user based on the speaker identification comprises: generating, at the client device, a voice input embedding based on the further user voice input processing; and determining whether the speech input embedding does not match a pre-stored embedding for the given user.
상기 구현예 중 일부 버전에서, 상기 화자 식별에 기초하여 상기 주어진 사용자로부터의 것이 아닌 음성 활동이 있다고 결정하는 단계는: 상기 클라이언트 디바이스에서, 상기 추가 사용자 음성 입력 프로세싱에 기초하여 음성 입력 임베딩을 생성하는 단계, 및 상기 음성 입력 임베딩이 상기 추가 사용자에 대한 미리 저장된 임베딩과 일치하는지를 결정하는 단계를 포함하며, 상기 추가 사용자에 대한 미리 저장된 임베딩은 상기 추가 사용자에 의한 승인에 응답하여 상기 클라이언트 디바이스에 로컬적으로 미리 저장된다. 상기 구현예 중 일부 추가 버전에서, 상기 음성 활동의 그래픽 표시는 상기 추가 사용자의 시맨틱 식별자를 포함할 수 있고, 상기 시맨틱 식별자는 상기 미리 저장된 임베딩과 미리 연관된다.In some versions of the implementations, determining that there is voice activity that is not from the given user based on the speaker identification comprises: generating, at the client device, a voice input embedding based on the further user voice input processing; and determining whether the voice input embedding matches a pre-stored embedding for the additional user, wherein the pre-stored embedding for the additional user is local to the client device in response to approval by the additional user. stored in advance as In some further versions of the above embodiments, the graphical representation of the voice activity may include a semantic identifier of the additional user, the semantic identifier being pre-associated with the pre-stored embedding.
일부 구현예에서, 상기 방법은 하나 이상의 합성 스피치 프로퍼티들을 자동으로 선택하는 단계를 포함할 수 있다. 합성 스피치 오디오 데이터를 생성하는 단계는 상기 자동으로 선택된 하나 이상의 합성 스피치 프로퍼티들로 상기 합성 스피치를 합성하는 단계를 포함할 수 있다. 상기 구현예 중 일부 버전에서, 하나 이상의 합성 스피치 프로퍼티들을 자동으로 선택하는 단계는 상기 클라이언트 디바이스의 위치에 대한 적어도 하나의 분류를 결정하는 단계, 및 상기 클라이언트 디바이스의 위치의 분류에 기초하여 상기 하나 이상의 합성 스피치 프로퍼티들을 자동으로 선택하는 단계를 포함할 수 있다. 상기 구현예 중 일부 버전에서, 하나 이상의 합성 스피치 프로퍼티들을 자동으로 선택하는 단계는 상기 클라이언트 디바이스와 상기 주어진 사용자가 있는 환경에 다른 사용자가 있는지 여부를 결정하는 단계, 및 상기 주어진 사용자에 추가되고 상기 추가 사용자에 추가되는 다른 사용자가 상기 클라이언트 디바이스와 상기 주어진 사용자가 있는 환경에 있는지 여부에 기초하여 상기 하나 이상의 합성 스피치 프로퍼티들을 자동으로 선택하는 단계를 포함하는, 방법. 상기 구현예 중 일부 버전에서, 하나 이상의 합성 스피치 프로퍼티들을 자동으로 선택하는 단계는 상기 추가 사용자를 식별하는 단계, 및 상기 주어진 사용자와 상기 추가 사용자 간의 관계의 하나 이상의 속성들에 기초하여 상기 하나 이상의 합성 스피치 프로퍼티들을 자동으로 선택하는 단계를 포함할 수 있다.In some implementations, the method may include automatically selecting one or more synthetic speech properties. Generating synthesized speech audio data may include synthesizing the synthesized speech with the automatically selected one or more synthesized speech properties. In some versions of the implementations, automatically selecting one or more synthetic speech properties comprises: determining at least one classification for a location of the client device; and determining at least one classification for a location of the client device based on the classification of the location of the client device. automatically selecting synthetic speech properties. In some versions of the implementations, automatically selecting one or more synthetic speech properties comprises: determining whether there is another user in the environment in which the client device and the given user are; and adding to the given user and adding the automatically selecting the one or more synthesized speech properties based on whether another user being added to the user is in the environment with the client device and the given user. In some versions of the implementations, automatically selecting one or more synthesized speech properties comprises identifying the additional user and the one or more synthesized speech properties based on one or more attributes of a relationship between the given user and the additional user. automatically selecting speech properties.
일부 구현예에서, 하나 이상의 합성 스피치 프로퍼티들은 운율 프로퍼티들을 포함한다.In some implementations, the one or more synthetic speech properties include prosody properties.
일부 구현예에서, 방법은 하나 이상의 프로세서들에 의해 구현되고, 주어진 사용자의 클라이언트 디바이스에서, 주어진 사용자와 클라이언트 디바이스가 있는 환경에서 주어진 사용자가 추가 사용자와 대화하기를 원함을 표시하는 사용자 입력을 수신하는 단계를 포함한다. 상기 방법은 상기 사용자 입력을 수신함에 후속하여: 추가 사용자 인식된 텍스트를 생성하기 위해 스피치 인식 모델을 사용하여 추가 사용자 음성 입력을 프로세싱하는 단계, 상기 추가 사용자 음성 입력은 클라이언트 디바이스의 하나 이상의 마이크로폰들을 통해 검출되며, 상기 사용자 입력을 수신함에 기초하여 텍스트 세그먼트가 상기 추가 사용자 인식된 텍스트에 대한 후보 응답이라고 결정하는 단계, 및 상기 텍스트 세그먼트가 후보 응답이라고 결정함에 기초하여, 상기 텍스트 세그먼트를 전달하는 그래픽 엘리먼트를 디스플레이하는 단계, 및 상기 그래픽 엘리먼트 선택을 수신하는 단계를 더 포함한다. 상기 방법은 상기 선택을 수신함에 응답하여: 상기 텍스트 세그먼트를 통합하는 합성 스피치를 포함하는 합성 스피치 오디오 데이터를 생성하는 단계, 및 합성 스피치가 클라이언트 디바이스 및/또는 추가 클라이언트 디바이스의 하나 이상의 스피커들을 통해 렌더링되게 하는 단계를 더 포함한다. 렌더링된 합성 스피치는 추가 사용자가 청각적으로 인지할 수 있다.In some implementations, a method is implemented by one or more processors, comprising receiving, at a client device of a given user, user input indicating that the given user desires to interact with an additional user in an environment with the given user and the client device. includes steps. The method includes, subsequent to receiving the user input: processing additional user voice input using a speech recognition model to generate additional user recognized text, wherein the additional user voice input is via one or more microphones of a client device. detected and determining that a text segment is a candidate response for the additional user recognized text based on receiving the user input; and based on determining that the text segment is a candidate response, a graphical element conveying the text segment. and displaying the graphical element, and receiving a selection of the graphical element. The method comprises, in response to receiving the selection: generating synthesized speech audio data comprising synthesized speech incorporating the text segment, and rendering the synthesized speech via one or more speakers of a client device and/or a further client device It further includes the step of making The rendered synthetic speech may be perceived aurally by a further user.
추가로, 일부 구현예들은 하나 이상의 컴퓨팅 디바이스들의 하나 이상의 프로세서들(예를 들어, 중앙 프로세싱 유닛(들)(CPU(들)), 그래픽 프로세싱 유닛(들)(GPU(들)) 및/또는 텐서 프로세싱 유닛(들)(TPU(들)))을 포함하며, 상기 하나 이상의 프로세서들은 연관된 메모리에 저장된 명령어들을 실행하도록 동작가능하며, 상기 명령어들은 전술된 임의의 방법들을 수행하게 하도록 구성된다. 또한, 일부 구현예들은 앞서 언급된 방법들 중 임의의 것을 수행하도록 하나 이상의 프로세서들에 의해 실행가능한 컴퓨터 명령어들을 저장하는 하나 이상의 비일시적 컴퓨터 판독가능 저장 매체를 포함한다. 또한, 일부 구현예들은 앞서 언급된 방법들 중 임의의 것을 수행하도록 하나 이상의 프로세서들에 의해 실행가능한 컴퓨터 명령어들을 저장하는 하나 이상의 비일시적 컴퓨터 판독가능 저장 매체를 포함한다.Additionally, some implementations may implement one or more processors (eg, central processing unit(s) (CPU(s)), graphics processing unit(s) (GPU(s)) and/or tensor of one or more computing devices). processing unit(s) (TPU(s)), wherein the one or more processors are operable to execute instructions stored in an associated memory, the instructions being configured to perform any of the methods described above. Also, some implementations include one or more non-transitory computer-readable storage media storing computer instructions executable by one or more processors to perform any of the aforementioned methods. Also, some implementations include one or more non-transitory computer-readable storage media storing computer instructions executable by one or more processors to perform any of the aforementioned methods.
본 명세서에서 매우 상세히 기술된 상기 개념들 및 추가적 개념들의 모든 조합들은 본 명세서에 개시된 본 발명의 일부인 것으로 고려되어야 한다. 예를 들면, 본 명세서의 끝부분에 나타나는 청구된 발명의 모든 조합들은 본 명세서에 개시된 본 발명의 일부인 것으로 고려된다.All combinations of the above and additional concepts described in great detail herein are to be considered as being part of the invention disclosed herein. For example, all combinations of claimed invention appearing at the end of this specification are considered to be part of the invention disclosed herein.
Claims (27)
주어진 사용자의 클라이언트 디바이스에서 적어도 하나의 사용자 인터페이스 입력에 기초하여, 상기 주어진 사용자가 참가자인 대화에서 전달하기 위한 텍스트 세그먼트를 결정하는 단계;
상기 대화에서 추가 참가자를 식별하는 단계, 상기 추가 참가자는 상기 주어진 사용자에 추가되며;
상기 주어진 사용자와 상기 추가 참가자 간의 관계의 적어도 하나의 속성에 기초하여 하나 이상의 운율 프로퍼티들을 결정하는 단계;
상기 텍스트 세그먼트를 통합하고 상기 하나 이상의 운율 프로퍼티들로 합성된, 합성 스피치를 포함하는 합성 스피치 오디오 데이터를 생성하는 단계, 상기 합성 스피치 오디오 데이터를 생성하는 단계는 상기 주어진 사용자와 상기 추가 참가자 간의 관계의 속성에 기초하여 상기 운율 프로퍼티들을 결정하는 것에 응답하여 상기 하나 이상의 운율 프로퍼티들을 포함하는 상기 합성 스피치를 합성하는 단계를 포함하며; 및
상기 합성 스피치가 상기 클라이언트 디바이스 및/또는 추가 클라이언트 디바이스의 하나 이상의 스피커들을 통해 렌더링되게 하는 단계를 포함하며, 상기 렌더링된 합성 스피치는 상기 추가 참가자가 청각적으로 인지할 수 있는, 방법.A method performed by one or more processors, comprising:
determining, based on at least one user interface input at the given user's client device, a text segment to convey in a conversation in which the given user is a participant;
identifying an additional participant in the conversation, the additional participant being added to the given user;
determining one or more prosody properties based on at least one attribute of the relationship between the given user and the additional participant;
consolidating the text segment and generating synthesized speech audio data comprising synthesized speech synthesized with the one or more prosody properties, wherein the generating of the synthesized speech audio data is a function of the relationship between the given user and the additional participant. synthesizing the synthesized speech including the one or more prosody properties in response to determining the prosody properties based on an attribute; and
causing the synthesized speech to be rendered through one or more speakers of the client device and/or additional client device, wherein the rendered synthesized speech is aurally perceptible by the additional participant.
상기 텍스트 세그먼트를 생성하기 위해 스피치 인식 모델을 사용하여 상기 음성 입력을 프로세싱하는 단계를 포함하는, 방법.The text segment of claim 1 , wherein the at least one user interface input comprises a given user's voice input detected via one or more microphones of the client device, and based on at least one user interface input at the client device, the text segment The steps to determine are:
processing the speech input using a speech recognition model to generate the text segment.
상기 클라이언트 디바이스의 하나 이상의 마이크로폰들을 통해, 상기 추가 참가자의 추가 참가자 음성 입력을 검출하는 단계;
상기 추가 참가자 음성 입력의 인식인 추가 참가자 텍스트 세그먼트를 생성하기 위해 스피치 인식 모델을 사용하여 상기 추가 참가자 음성 입력을 프로세싱하는 단계;
상기 텍스트 세그먼트가 상기 추가 참가자 텍스트 세그먼트에 대한 후보 응답이라고 결정하는 단계; 및
상기 텍스트 세그먼트가 상기 추가 참가자 텍스트 세그먼트에 대한 후보 응답이라는 결정에 응답하여 상기 텍스트 세그먼트를 전달하는 상기 그래픽 엘리먼트를 디스플레이하도록 결정하는 단계를 포함하는, 방법.5. The method of claim 4, prior to entering the user interface at the client device:
detecting, via one or more microphones of the client device, an additional participant voice input of the additional participant;
processing the additional participant voice input using a speech recognition model to generate an additional participant text segment that is a recognition of the additional participant voice input;
determining that the text segment is a candidate response to the additional participant text segment; and
determining to display the graphical element conveying the text segment in response to determining that the text segment is a candidate response for the additional participant text segment.
상기 추가 참가자 음성 입력을 사용하여 화자 식별을 수행하는 단계; 및
상기 화자 식별에 기초하여 상기 추가 참가자를 식별하는 단계를 포함하는, 방법.6. The method of claim 5, wherein identifying the additional participant in the conversation comprises:
performing speaker identification using the additional participant voice input; and
and identifying the additional participant based on the speaker identification.
상기 클라이언트 디바이스에서, 화자 식별 모델을 사용하여 상기 추가 참가자 음성 입력을 프로세싱하는 것에 기초하여 음성 입력 임베딩을 생성하는 단계; 및
상기 클라이언트 디바이스에서, 상기 음성 입력 임베딩을 추가 참가자에 대한 미리 저장된 임베딩과 비교하는 단계를 포함하며, 상기 미리 저장된 임베딩은 상기 추가 참가자에 의한 승인에 응답하여 상기 클라이언트 디바이스에 미리 로컬적으로 저장되어 있는, 방법.7. The method of claim 6, wherein performing speaker identification comprises:
generating, at the client device, a speech input embedding based on processing the additional participant speech input using a speaker identification model; and
comparing, at the client device, the speech input embedding to a pre-stored embedding for an additional participant, wherein the pre-stored embedding is previously stored locally on the client device in response to approval by the additional participant. , Way.
상기 추가 참가자 텍스트 세그먼트에 기초하여 초기 후보 응답들의 수퍼세트를 생성하는 단계, 상기 수퍼세트는 상기 텍스트 세그먼트를 포함하며; 및
상기 초기 후보 응답의 슈퍼세트로부터, 상기 주어진 사용자와 상기 추가 참가자 사이의 관계의 속성들 중 적어도 하나에 기초하여 상기 후보 응답으로서 상기 텍스트 세그먼트를 선택하는 단계를 포함하는, 방법.The method of claim 8 , wherein determining that the text segment is a candidate response to the additional participant text segment is further based on at least one of attributes of the relationship between the given user and the additional participant:
generating a superset of initial candidate responses based on the additional participant text segment, the superset comprising the text segment; and
selecting, from the superset of initial candidate responses, the text segment as the candidate response based on at least one of attributes of a relationship between the given user and the additional participant.
상기 클라이언트 디바이스의 위치에 대한 적어도 하나의 분류를 결정하는 단계를 더 포함하며;
상기 텍스트 세그먼트가 상기 추가 참가자 텍스트 세그먼트에 대한 후보 응답이라고 결정하는 단계는 상기 위치의 적어도 하나의 분류에 더 기초하는, 방법.10. The method according to any one of claims 5 to 9,
determining at least one classification for the location of the client device;
and determining that the text segment is a candidate response to the additional participant text segment is further based on the at least one classification of the location.
상기 대화에서 다른 추가 참가자를 식별하는 단계, 상기 다른 추가 참가자는 상기 주어진 사용자에 추가되고 상기 추가 참가자에 추가되며;
상기 하나 이상의 운율 프로퍼티들을 결정하는 단계를 포함하며, 상기 하나 이상의 운율 프로퍼티들을 결정하는 단계는:
(a) 상기 주어진 사용자와 상기 추가 참가자 간의 관계의 속성, 및
(b) 상기 주어진 사용자와 상기 다른 추가 참가자 간의 추가 관계의 하나 이상의 추가 속성들에 기초하는, 방법.In any preceding claim,
identifying another additional participant in the conversation, the further additional participant being added to the given user and added to the additional participant;
determining the one or more prosody properties, wherein determining the one or more prosody properties comprises:
(a) the nature of the relationship between the given user and the additional participant, and
(b) based on one or more additional attributes of the additional relationship between the given user and the other additional participant.
상기 대화에서 다른 추가 참가자를 식별하는 단계, 상기 다른 추가 참가자는 상기 주어진 사용자에 추가되고 상기 추가 참가자에 추가되며;
상기 주어진 사용자와 상기 다른 추가 참가자 간의 추가 관계의 하나 이상의 추가 속성들 대신에, 상기 주어진 사용자와 상기 추가 참가자 간의 관계의 속성에 기초하여 상기 하나 이상의 운율 프로퍼티들을 결정하는 단계를 포함하며, 상기 하나 이상의 운율 프로퍼티들을 결정하는 단계는:
상기 주어진 사용자와 상기 추가 참가자 간의 관계가 상기 주어진 사용자와 상기 추가 참가자 간의 추가 관계보다 더 형식적이라고 결정하는 것에 대한 응답인, 방법.12. The method according to any one of claims 1 to 11,
identifying another additional participant in the conversation, the further additional participant being added to the given user and added to the additional participant;
determining the one or more prosody properties based on an attribute of the relationship between the given user and the additional participant instead of one or more additional attributes of the additional relationship between the given user and the additional participant; The steps for determining the prosody properties are:
responsive to determining that the relationship between the given user and the additional participant is more formal than the further relationship between the given user and the additional participant.
상기 클라이언트 디바이스의 카메라에 의해 캡처된 하나 이상의 이미지들에 기초하여 상기 추가 참가자를 식별하는 단계를 포함하는, 방법.5. The method of any preceding claim, wherein identifying the additional participant in the conversation comprises:
identifying the additional participant based on one or more images captured by a camera of the client device.
상기 클라이언트 디바이스의 디스플레이 상에 상기 하나 이상의 이미지들 중 이미지를 렌더링하는 단계, 상기 이미지는 상기 추가 참가자를 포함하는 다수의 사람들을 캡처하며;
상기 추가 참가자를 캡처하고 상기 다른 다수의 사람들을 모두 제외하는 상기 이미지의 영역을 지정하는 상기 주어진 사용자의 상기 클라이언트 디바이스에서 다른 사용자 인터페이스 입력을 수신하는 단계; 및
상기 이미지 영역의 프로세싱에 기초하여 상기 추가 참가자를 식별하는 단계를 포함하는, 방법.15. The method of claim 13 or 14, wherein identifying the additional participant based on an image captured by a camera of the client device comprises:
rendering an image of the one or more images on a display of the client device, the image capturing a plurality of people including the additional participant;
receiving another user interface input at the client device of the given user specifying an area of the image that captures the additional participant and excludes all of the other plurality of people; and
and identifying the additional participant based on processing of the image region.
상기 클라이언트 디바이스의 위치에 대한 적어도 하나의 분류를 결정하는 단계를 더 포함하며;
상기 운율 프로퍼티들을 결정하는 단계는 상기 클라이언트 디바이스의 위치 분류에 추가로 기초하는, 방법.In any preceding claim,
determining at least one classification for the location of the client device;
wherein determining the prosody properties is further based on a location classification of the client device.
클라이언트 디바이스의 환경에서 주어진 사용자로부터의 것이 아닌 음성 활동이 있다고 결정하기 위해, 주어진 사용자의 클라이언트 디바이스의 하나 이상의 마이크로폰들을 통해 검출된 추가 사용자 음성 입력을 프로세싱하는 단계, 상기 추가 사용자 음성 입력은 상기 클라이언트 디바이스와 상기 주어진 사용자가 있는 환경에 있는 추가 사용자의 것이며;
상기 주어진 사용자로부터의 것이 아닌 음성 활동이 있다는 결정에 응답하여, 상기 음성 활동의 그래픽 표시가 상기 클라이언트 디바이스의 디스플레이에서 렌더링되게 하는 단계;
상기 그래픽 표시의 선택을 수신하는 단계;
상기 선택을 수신함에 응답하여:
주어진 사용자 인식 텍스트를 생성하기 위해 스피치 인식 모델을 사용하여 주어진 사용자 음성 입력을 프로세싱하는 단계, 상기 주어진 사용자 음성 입력은 상기 클라이언트 디바이스의 하나 이상의 마이크로폰들을 통해 검출되고, 상기 주어진 사용자 음성 입력은 주어진 사용자의 것이고 상기 추가 사용자 음성 입력에 후속하여 제공되며;
상기 주어진 사용자 인식 텍스트를 통합하는 합성 스피치를 포함하는 합성 스피치 오디오 데이터를 생성하는 단계; 및
상기 합성 스피치가 상기 클라이언트 디바이스 및/또는 추가 클라이언트 디바이스의 하나 이상의 스피커들을 통해 렌더링되게 하는 단계를 포함하며, 상기 렌더링된 합성 스피치는 상기 추가 사용자가 청각적으로 인지할 수 있는, 방법.A method performed by one or more processors, comprising:
processing additional user voice input detected via one or more microphones of a client device of a given user to determine that there is voice activity in the environment of the client device that is not from the given user, wherein the additional user voice input is from the client device and an additional user in the environment in which the given user is located;
in response to determining that there is a voice activity that is not from the given user, causing a graphical representation of the voice activity to be rendered on a display of the client device;
receiving a selection of the graphical representation;
In response to receiving said selection:
processing a given user voice input using a speech recognition model to generate a given user recognition text, wherein the given user voice input is detected via one or more microphones of the client device, the given user voice input is the given user's and provided subsequent to the further user voice input;
generating synthesized speech audio data comprising synthesized speech incorporating the given user-recognized text; and
causing the synthesized speech to be rendered via one or more speakers of the client device and/or additional client device, wherein the rendered synthesized speech is aurally perceptible by the additional user.
상기 추가 사용자 음성 입력을 사용하여 화자 식별을 수행하는 단계; 및
상기 화자 식별에 기초하여 상기 주어진 사용자로부터의 것이 아닌 음성 활동이 있다고 결정하는 단계를 포함하는, 방법.19. The method of claim 18, wherein processing the additional user voice input to determine that there is voice activity that is not from the given user comprises:
performing speaker identification using the additional user voice input; and
determining that there is voice activity that is not from the given user based on the speaker identification.
상기 클라이언트 디바이스에서, 상기 추가 사용자 음성 입력 프로세싱에 기초하여 음성 입력 임베딩을 생성하는 단계; 및
상기 음성 입력 임베딩이 상기 주어진 사용자에 대해 미리 저장된 임베딩과 일치하지 않는지 결정하는 단계를 포함하는, 방법.20. The method of claim 19, wherein determining that there is voice activity that is not from the given user based on the speaker identification comprises:
generating, at the client device, a voice input embedding based on the further user voice input processing; and
determining whether the speech input embedding does not match a pre-stored embedding for the given user.
상기 클라이언트 디바이스에서, 상기 추가 사용자 음성 입력 프로세싱에 기초하여 음성 입력 임베딩을 생성하는 단계; 및
상기 음성 입력 임베딩이 상기 추가 사용자에 대한 미리 저장된 임베딩과 일치하는지를 결정하는 단계를 포함하며, 상기 추가 사용자에 대한 미리 저장된 임베딩은 상기 추가 사용자에 의한 승인에 응답하여 상기 클라이언트 디바이스에 로컬적으로 미리 저장되는, 방법.20. The method of claim 19, wherein determining that there is voice activity that is not from the given user based on the speaker identification comprises:
generating, at the client device, a voice input embedding based on the further user voice input processing; and
determining whether the voice input embedding matches a pre-stored embedding for the additional user, wherein the pre-stored embedding for the additional user is pre-stored locally on the client device in response to approval by the additional user. How to become.
하나 이상의 합성 스피치 프로퍼티들을 자동으로 선택하는 단계를 포함하며;
합성 스피치 오디오 데이터를 생성하는 단계는 상기 자동으로 선택된 하나 이상의 합성 스피치 프로퍼티들로 상기 합성 스피치를 합성하는 단계를 포함하는, 방법.23. The method according to any one of claims 18 to 22,
automatically selecting one or more synthetic speech properties;
wherein generating synthesized speech audio data comprises synthesizing the synthesized speech with the automatically selected one or more synthesized speech properties.
상기 클라이언트 디바이스의 위치에 대한 적어도 하나의 분류를 결정하는 단계; 및
상기 클라이언트 디바이스의 위치의 분류에 기초하여 상기 하나 이상의 합성 스피치 프로퍼티들을 자동으로 선택하는 단계를 포함하는, 방법.24. The method of claim 23, wherein automatically selecting one or more synthetic speech properties comprises:
determining at least one classification for the location of the client device; and
automatically selecting the one or more synthesized speech properties based on the classification of the location of the client device.
상기 클라이언트 디바이스와 상기 주어진 사용자가 있는 환경에 다른 사용자가 있는지 여부를 결정하는 단계; 및
상기 주어진 사용자에 추가되고 상기 추가 사용자에 추가되는 다른 사용자가 상기 클라이언트 디바이스와 상기 주어진 사용자가 있는 환경에 있는지 여부에 기초하여 상기 하나 이상의 합성 스피치 프로퍼티들을 자동으로 선택하는 단계를 포함하는, 방법.25. The method of claim 23 or 24, wherein automatically selecting one or more synthetic speech properties comprises:
determining whether there are other users in the environment with the client device and the given user; and
automatically selecting the one or more synthetic speech properties based on whether another user added to the given user and added to the additional user is in the environment in which the client device and the given user are present.
상기 추가 사용자를 식별하는 단계; 및
상기 주어진 사용자와 상기 추가 사용자 간의 관계의 하나 이상의 속성들에 기초하여 상기 하나 이상의 합성 스피치 프로퍼티들을 자동으로 선택하는 단계를 포함하는, 방법.26. The method of any one of claims 23-25, wherein automatically selecting one or more synthetic speech properties comprises:
identifying the additional user; and
automatically selecting the one or more synthetic speech properties based on one or more attributes of the relationship between the given user and the additional user.
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2020/017562 WO2021162675A1 (en) | 2020-02-10 | 2020-02-10 | Synthesized speech audio data generated on behalf of human participant in conversation |
Publications (1)
Publication Number | Publication Date |
---|---|
KR20220140599A true KR20220140599A (en) | 2022-10-18 |
Family
ID=69771211
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
KR1020227031429A KR20220140599A (en) | 2020-02-10 | 2020-02-10 | Synthetic speech audio data generated on behalf of a human participant in a conversation |
Country Status (5)
Country | Link |
---|---|
US (1) | US20230046658A1 (en) |
EP (1) | EP4091161A1 (en) |
KR (1) | KR20220140599A (en) |
CN (1) | CN115088033A (en) |
WO (1) | WO2021162675A1 (en) |
Families Citing this family (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11783812B2 (en) * | 2020-04-28 | 2023-10-10 | Bloomberg Finance L.P. | Dialogue act classification in group chats with DAG-LSTMs |
US11545133B2 (en) * | 2020-10-12 | 2023-01-03 | Google Llc | On-device personalization of speech synthesis for training of speech model(s) |
US11580955B1 (en) * | 2021-03-31 | 2023-02-14 | Amazon Technologies, Inc. | Synthetic speech processing |
US20230074406A1 (en) * | 2021-09-07 | 2023-03-09 | Google Llc | Using large language model(s) in generating automated assistant response(s |
WO2023215132A1 (en) * | 2022-05-04 | 2023-11-09 | Cerence Operating Company | Interactive modification of speaking style of synthesized speech |
Family Cites Families (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US7277855B1 (en) * | 2000-06-30 | 2007-10-02 | At&T Corp. | Personalized text-to-speech services |
US9570066B2 (en) * | 2012-07-16 | 2017-02-14 | General Motors Llc | Sender-responsive text-to-speech processing |
US10199034B2 (en) * | 2014-08-18 | 2019-02-05 | At&T Intellectual Property I, L.P. | System and method for unified normalization in text-to-speech and automatic speech recognition |
KR102341144B1 (en) * | 2015-06-01 | 2021-12-21 | 삼성전자주식회사 | Electronic device which ouputus message and method for controlling thereof |
WO2018168427A1 (en) * | 2017-03-13 | 2018-09-20 | ソニー株式会社 | Learning device, learning method, speech synthesizer, and speech synthesis method |
US10187579B1 (en) * | 2017-06-30 | 2019-01-22 | Polycom, Inc. | People detection method for auto-framing and tracking in a video conference |
US11763821B1 (en) * | 2018-06-27 | 2023-09-19 | Cerner Innovation, Inc. | Tool for assisting people with speech disorder |
WO2020170441A1 (en) * | 2019-02-22 | 2020-08-27 | ソニー株式会社 | Information processing device, information processing method, and program |
US11276410B2 (en) * | 2019-09-13 | 2022-03-15 | Microsoft Technology Licensing, Llc | Convolutional neural network with phonetic attention for speaker verification |
-
2020
- 2020-02-10 US US17/792,012 patent/US20230046658A1/en active Pending
- 2020-02-10 KR KR1020227031429A patent/KR20220140599A/en not_active Application Discontinuation
- 2020-02-10 WO PCT/US2020/017562 patent/WO2021162675A1/en unknown
- 2020-02-10 EP EP20709976.3A patent/EP4091161A1/en active Pending
- 2020-02-10 CN CN202080096237.8A patent/CN115088033A/en active Pending
Also Published As
Publication number | Publication date |
---|---|
CN115088033A (en) | 2022-09-20 |
EP4091161A1 (en) | 2022-11-23 |
US20230046658A1 (en) | 2023-02-16 |
WO2021162675A1 (en) | 2021-08-19 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
KR102627948B1 (en) | Automated assistants that accommodate multiple age groups and/or vocabulary levels | |
JP6803351B2 (en) | Managing agent assignments in man-machine dialogs | |
US20200279553A1 (en) | Linguistic style matching agent | |
KR20220140599A (en) | Synthetic speech audio data generated on behalf of a human participant in a conversation | |
US11183187B2 (en) | Dialog method, dialog system, dialog apparatus and program that gives impression that dialog system understands content of dialog | |
US11810557B2 (en) | Dynamic and/or context-specific hot words to invoke automated assistant | |
CN112262430A (en) | Automatically determining language for speech recognition of a spoken utterance received via an automated assistant interface | |
JP6719741B2 (en) | Dialogue method, dialogue device, and program | |
KR20210008521A (en) | Dynamic and/or context-specific hot words to invoke automated assistants | |
KR20210070213A (en) | Voice user interface | |
US10699706B1 (en) | Systems and methods for device communications | |
KR20240007261A (en) | Use large-scale language models to generate automated assistant response(s) | |
JP2024508033A (en) | Instant learning of text-speech during dialogue | |
US20220335953A1 (en) | Voice shortcut detection with speaker verification | |
Aneja et al. | Understanding conversational and expressive style in a multimodal embodied conversational agent | |
JP2022539674A (en) | Speaker Recognition Using Speaker-Specific Speech Models | |
JP2023548157A (en) | Other speaker audio filtering from calls and audio messages | |
JP2017208003A (en) | Dialogue method, dialogue system, dialogue device, and program | |
KR20230062612A (en) | Enabling natural conversations for automated assistants | |
JP2024510698A (en) | Contextual suppression of assistant commands | |
JPWO2017200077A1 (en) | Dialogue method, dialogue system, dialogue apparatus, and program | |
US20230053341A1 (en) | Enabling natural conversations with soft endpointing for an automated assistant | |
US11756533B2 (en) | Hot-word free pre-emption of automated assistant response presentation | |
KR20230158615A (en) | Enable natural conversations using soft endpointing for automated assistants | |
Bosdriesz | Adding Speech to Dialogues with a Council of Coaches |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
E902 | Notification of reason for refusal |