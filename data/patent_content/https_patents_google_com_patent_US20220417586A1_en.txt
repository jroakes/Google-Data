US20220417586A1 - Non-occluding video overlays - Google Patents
Non-occluding video overlays Download PDFInfo
- Publication number
- US20220417586A1 US20220417586A1 US17/776,652 US202017776652A US2022417586A1 US 20220417586 A1 US20220417586 A1 US 20220417586A1 US 202017776652 A US202017776652 A US 202017776652A US 2022417586 A1 US2022417586 A1 US 2022417586A1
- Authority
- US
- United States
- Prior art keywords
- video
- frames
- identifying
- regions
- inclusion
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000007717 exclusion Effects 0.000 claims abstract description 96
- 238000000034 method Methods 0.000 claims abstract description 34
- 230000004931 aggregating effect Effects 0.000 claims abstract description 11
- 230000015654 memory Effects 0.000 claims description 14
- 238000001514 detection method Methods 0.000 claims description 13
- 238000012015 optical character recognition Methods 0.000 claims description 12
- 238000013527 convolutional neural network Methods 0.000 claims description 10
- 238000003860 storage Methods 0.000 description 22
- 238000013459 approach Methods 0.000 description 18
- 238000010801 machine learning Methods 0.000 description 14
- 238000004590 computer program Methods 0.000 description 12
- 230000008569 process Effects 0.000 description 11
- 238000012545 processing Methods 0.000 description 11
- 230000008901 benefit Effects 0.000 description 6
- 238000004891 communication Methods 0.000 description 6
- 238000010586 diagram Methods 0.000 description 5
- 230000002776 aggregation Effects 0.000 description 4
- 238000004220 aggregation Methods 0.000 description 4
- 230000000295 complement effect Effects 0.000 description 4
- 230000033001 locomotion Effects 0.000 description 4
- 210000005010 torso Anatomy 0.000 description 4
- 241000282326 Felis catus Species 0.000 description 3
- 241001465754 Metazoa Species 0.000 description 3
- 238000009825 accumulation Methods 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 230000000644 propagated effect Effects 0.000 description 3
- 238000000926 separation method Methods 0.000 description 3
- 238000012549 training Methods 0.000 description 3
- 241000282472 Canis lupus familiaris Species 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000003491 array Methods 0.000 description 1
- 238000013528 artificial neural network Methods 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 238000009826 distribution Methods 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000007726 management method Methods 0.000 description 1
- 238000010295 mobile communication Methods 0.000 description 1
- 230000002688 persistence Effects 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
- 238000005303 weighing Methods 0.000 description 1
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/80—Generation or processing of content or additional data by content creator independently of the distribution process; Content per se
- H04N21/81—Monomedia components thereof
- H04N21/812—Monomedia components thereof involving advertisement data
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/20—Analysis of motion
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/20—Image preprocessing
- G06V10/22—Image preprocessing by selection of a specific region containing or referencing a pattern; Locating or processing of specific regions to guide the detection or recognition
- G06V10/225—Image preprocessing by selection of a specific region containing or referencing a pattern; Locating or processing of specific regions to guide the detection or recognition based on a marking or identifier characterising the area
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/82—Arrangements for image or video recognition or understanding using pattern recognition or machine learning using neural networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/41—Higher-level, semantic clustering, classification or understanding of video scenes, e.g. detection, labelling or Markovian modelling of sport events or news items
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V30/00—Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition
- G06V30/10—Character recognition
- G06V30/14—Image acquisition
- G06V30/1444—Selective acquisition, locating or processing of specific regions, e.g. highlighted text, fiducial marks or predetermined fields
- G06V30/1448—Selective acquisition, locating or processing of specific regions, e.g. highlighted text, fiducial marks or predetermined fields based on markings or identifiers characterising the document or the area
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V30/00—Character recognition; Recognising digital ink; Document-oriented image-based pattern recognition
- G06V30/10—Character recognition
- G06V30/14—Image acquisition
- G06V30/146—Aligning or centring of the image pick-up or image-field
- G06V30/147—Determination of region of interest
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V40/00—Recognition of biometric, human-related or animal-related patterns in image or video data
- G06V40/10—Human or animal bodies, e.g. vehicle occupants or pedestrians; Body parts, e.g. hands
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/02—Editing, e.g. varying the order of information signals recorded on, or reproduced from, record carriers
- G11B27/031—Electronic editing of digitised analogue information signals, e.g. audio or video signals
- G11B27/036—Insert-editing
-
- G—PHYSICS
- G11—INFORMATION STORAGE
- G11B—INFORMATION STORAGE BASED ON RELATIVE MOVEMENT BETWEEN RECORD CARRIER AND TRANSDUCER
- G11B27/00—Editing; Indexing; Addressing; Timing or synchronising; Monitoring; Measuring tape travel
- G11B27/10—Indexing; Addressing; Timing or synchronising; Measuring tape travel
- G11B27/19—Indexing; Addressing; Timing or synchronising; Measuring tape travel by using information detectable on the record carrier
- G11B27/28—Indexing; Addressing; Timing or synchronising; Measuring tape travel by using information detectable on the record carrier by using information signals recorded by the same method as the main recording
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/23—Processing of content or additional data; Elementary server operations; Server middleware
- H04N21/234—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs
- H04N21/23418—Processing of video elementary streams, e.g. splicing of video streams, manipulating MPEG-4 scene graphs involving operations for analysing video streams, e.g. detecting features or characteristics
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/20—Servers specifically adapted for the distribution of content, e.g. VOD servers; Operations thereof
- H04N21/25—Management operations performed by the server for facilitating the content distribution or administrating data related to end-users or client devices, e.g. end-user or client device authentication, learning user preferences for recommending movies
- H04N21/251—Learning process for intelligent management, e.g. learning user preferences for recommending movies
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/43—Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware
- H04N21/431—Generation of visual interfaces for content selection or interaction; Content or additional data rendering
- H04N21/4312—Generation of visual interfaces for content selection or interaction; Content or additional data rendering involving specific graphical features, e.g. screen layout, special fonts or colors, blinking icons, highlights or animations
- H04N21/4316—Generation of visual interfaces for content selection or interaction; Content or additional data rendering involving specific graphical features, e.g. screen layout, special fonts or colors, blinking icons, highlights or animations for displaying supplemental content in a region of the screen, e.g. an advertisement in a separate window
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N21/00—Selective content distribution, e.g. interactive television or video on demand [VOD]
- H04N21/40—Client devices specifically adapted for the reception of or interaction with content, e.g. set-top-box [STB]; Operations thereof
- H04N21/43—Processing of content or additional data, e.g. demultiplexing additional data from a digital video stream; Elementary client operations, e.g. monitoring of home network or synchronising decoder's clock; Client middleware
- H04N21/44—Processing of video elementary streams, e.g. splicing a video clip retrieved from local storage with an incoming video stream, rendering scenes according to MPEG-4 scene graphs
- H04N21/44008—Processing of video elementary streams, e.g. splicing a video clip retrieved from local storage with an incoming video stream, rendering scenes according to MPEG-4 scene graphs involving operations for analysing video streams, e.g. detecting features or characteristics in the video stream
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N5/00—Details of television systems
- H04N5/222—Studio circuitry; Studio devices; Studio equipment
- H04N5/262—Studio circuits, e.g. for mixing, switching-over, change of character of image, other special effects ; Cameras specially adapted for the electronic generation of special effects
- H04N5/265—Mixing
Definitions
- Videos that are streamed to a user can include additional content that is overlaid on top of the original video stream.
- the overlaid content may be provided to the user within a rectangular region that overlays and blocks a portion of the original video screen.
- the rectangular region for provision of the overlaid content is positioned at the center bottom of the video screen. If important content of the original video stream is positioned at the center bottom of the video screen, it can be blocked or obstructed by the overlaid content.
- This specification describes technologies related to overlaying content on top of a video stream, while at the same time avoiding areas of the video screen that feature useful content in the underlying video stream, e.g., areas in the original video stream that contain faces, text, or significant objects such as fast-moving objects.
- a first innovative aspect of the subject matter described in this specification can be embodied in methods that include identifying, for each video frame among a sequence of frames of a video, a corresponding exclusion zone from which to exclude overlaid content based on the detection of a specified object in a region of the video frame that is within the corresponding exclusion zone; aggregating the corresponding exclusion zones for the video frames in a specified duration or number of the sequence of frames; defining, within the specified duration or number of the sequence of frames of the video, an inclusion zone within which overlaid content is eligible for inclusion, the inclusion zone being defined as an area of the video frames in the specified duration or number that is outside of the aggregated corresponding exclusion zones; and providing overlaid content for inclusion in the inclusion zone of the specified duration or number of the sequence of frames of the video during display of the video at a client device.
- Other implementations of this aspect include corresponding apparatus, systems, and computer programs, configured to perform the aspects of the methods, encoded on computer storage devices.
- the identifying of the exclusion zones can include identifying, for each video frame in the sequence of frames, one or more regions in which text is displayed in the video, the methods further including generating one or more bounding boxes that delineate the one or more regions from other parts of the video frame.
- the identifying of the one or more regions in which text is displayed can include identifying the one or more regions with an optical character recognition system.
- the identifying of the exclusion zones can include identifying, for each video frame in the sequence of frames, one or more regions in which human features are displayed in the video, the methods further including generating one or more bounding boxes that delineate the one or more regions from other parts of the video frame.
- the identifying the one or more regions in which human features are displayed can include identifying the one or more regions with a computer vision system trained to identify human features.
- the computer vision system can be a convolutional neural network system.
- the identifying of the exclusion zones can include identifying, for each video frame in the sequence of frames, one or more regions in which significant objects are displayed in the video, wherein identifying of the regions in which significant objects are displayed is identifying with a computer vision system configured to recognize objects from a selected set of object categories not including text or human features.
- the identifying of the exclusion zones can include identifying the one or more regions in which the significant objects are displayed in the video, based on detection of objects that move more than a selected distance between consecutive frames or detection of objects that move during a specified number of sequential frames.
- the aggregating of the corresponding exclusion zones can include generating a union of bounding boxes that delineate the corresponding exclusion zones from other parts of the video.
- the defining of the inclusion zone can include identifying, within the sequence of frames of the video, a set of rectangles that do not overlap with the aggregated corresponding exclusion zones over the specified duration or number; and the providing overlaid content for inclusion in the inclusion zone can include: identifying an overlay having dimensions that fit within one or more rectangles among the set of rectangles; and providing the overlay within the one or more rectangles during the specified duration or number.
- the content of value to the user within that video screen area may not fill the entire area of the video screen.
- the valuable content e.g., faces, text, or significant objects such as fast-moving objects, may occupy only a portion of the video screen area.
- aspects of the present disclosure provide the advantage of identifying exclusion zones from which to exclude overlaid content, because overlaying content over these exclusion zones would block or obscure valuable content that is included in the underlying video stream, which would result in wasted computing resources by delivering video to users when the valuable content is not perceivable to the users.
- machine learning engines such as Bayesian classifiers, optical character recognition systems, or neural networks
- the user can receive the overlaid content without obstruction of the valuable content of the underlying video stream, such that the computing resources required to deliver the video are not wasted.
- aspects of the present disclosure provide for overlaying additional content outside of that fraction of the viewing area, leading to more efficient utilization of the screen area to deliver useful content to the viewer.
- the overlaid content includes a box or other icon that the viewer can click to remove the overlaid content, for example, if the overlaid content obstructs valuable content in the underlying video.
- a further advantage of the present disclosure is that, because the overlaid content is less likely to obstruct valuable content in the underlying video, there is less disruption of the viewing experience and a greater likelihood that the viewer will not “click away” the overlaid content that has been presented.
- FIG. 1 depicts an overview of aggregating exclusion zones and defining an inclusion zone for a video that includes a sequence of frames.
- FIG. 2 depicts an example of frame-by-frame aggregation of exclusion zones for the example of FIG. 1 .
- FIG. 3 depicts an example of a machine learning system for identifying and aggregating exclusion zones and selecting overlaid content.
- FIG. 4 depicts a flow diagram for a process that includes aggregating exclusion zones and selecting overlaid content.
- FIG. 5 is a block diagram of an example computer system.
- the present specification presents a solution to this technical problem by describing machine learning methods and systems that can identify exclusion zones that correspond to regions of the video stream that are more likely to contain valuable content; aggregating these exclusion zones over time; and then positioning the overlaid content in an inclusion zone that is outside of the aggregated exclusion zones, so that the overlaid content is less likely to obstruct valuable content in the underlying video.
- FIG. 1 depicts an illustrative example of exclusion zones, inclusion zones, and overlaid content for a video stream.
- an original, underlying video stream 100 is depicted as frames 101 , 102 , and 103 on the left side of the figure.
- Each frame can include regions that are likely to contain valuable content that should not be occluded by overlaid content features.
- the frames of the video may include one or more regions of text 111 , such as closed caption text or text that appears on features within a video, e.g., text that appears on product labels, road signs, white boards on screen within a video of a school lecture, etc. Note that features within the video are part of the video stream itself, whereas any overlaid content is separate from the video stream itself.
- a machine learning system such as an optical character recognition (OCR) system can be used to identify regions with the frames that contain text, and that identifying can include identifying bounding boxes that enclose the identified text, as shown.
- OCR optical character recognition
- the text 111 might be situated at different locations within the different frames, so overlaid content that persists for a duration of time that includes multiple frames 101 , 102 , 103 should not be positioned anywhere that the text 111 is situated across the multiple frames.
- regions 112 that include persons or human features are regions 112 that include persons or human features.
- the frames may include one or more persons, or portions thereof, such as human faces, torsos, limbs, hands, etc.
- a machine learning system such as a convolutional neural network (CNN) system can be used to identify regions within the frames that contain persons (or portions thereof, such as faces, torsos, limbs, hands, etc.), and that identifying can include identifying bounding boxes that enclose the identified human features, as shown. As illustrated in the example of FIG.
- CNN convolutional neural network
- the human features 112 might be situated at different locations within the different frames, so overlaid content that persists for a duration of time that includes multiple frames 101 , 102 , 103 should not be positioned anywhere that the human features 112 are situated across the multiple frames.
- the human features 112 may be discriminated by limiting the human features detection to larger human features, i.e. features that are in the foreground and closer to the point of view of the video, as opposed to background human features. For example, larger human faces corresponding to persons in the foreground of a video may be included in the detection scheme, while smaller human faces corresponding to persons in the background of a video, such as faces in a crowd, may be excluded from the detection scheme.
- regions 113 that contain other potential objects of interest are regions 113 that contain other potential objects of interest, such as animals, plants, street lights or other road features, bottles or other containers, furnishings, etc.
- a machine learning system such as a convolutional neural network (CNN) system can be used to identify regions within the frames that contain potential objects of interest.
- CNN convolutional neural network
- a machine learning system can be trained to classify various objects selected from a list of object categories, e.g. to identify dogs, cats, vases, flowers, or any other category of object that may be of potential interest to the viewer.
- the identifying can include identifying bounding boxes that enclose the detected objects, as shown. As illustrated in the example of FIG.
- the detected object 113 (a cat in this example) might be situated at different locations within the different frames, so overlaid content that persists for a duration of time that includes multiple frames 101 , 102 , 103 should not be positioned anywhere that the detected objects 113 are situated across the multiple frames.
- the detected objects 113 may be discriminated by limiting the object detection to objects that are in motion. Objects that are in motion are generally more likely to convey important content to the viewer and therefore potentially less suitable to be occluded by overlaid content. For example, the detected objects 113 can be limited to objects that move a certain minimum distance within a selected interval of time (or selected interval of frames), or that move during a specified number of sequential frames.
- exclusion zones from which to exclude overlaid content can be identified based on the detected features in the video that are more likely to be of interest to the viewer.
- the exclusion zones can include regions 121 in which text has been identified as appearing in frames of the video (compare with identified text 111 in original video frames 101 , 102 , 103 ); the exclusion zones can also include regions 122 in which human features have been identified as appearing in frames of the video (compare with identified human features 112 in original video frames 101 , 102 , 103 ); and the exclusion zones can also include regions 123 in which other objects of interest (such as faster-moving objects, or objects identified from a selected list of object categories) have been identified as appearing in frames of the video (compare with identified object 113 in original video frames 101 , 102 , 103 ).
- other objects of interest such as faster-moving objects, or objects identified from a selected list of object categories
- the exclusion zones can be aggregated over the selected duration of time (or selected span of frames) to define an aggregate exclusion zone.
- the aggregate exclusion zone can be the union of all exclusion zones corresponding to detected features of interest in each video frame among a sequence of frames of a video.
- the selected duration of time could be 1 second, 5 seconds, 10 seconds, 1 minute, or any other duration of time that is suitable for display of the overlaid content.
- a selected span of frames could be 24 frames, 60 frames, 240 frames, or any other span of frames that is suitable for display of the overlaid content.
- a selected duration of time can correspond to a selected span of frames, or vice versa, as determined by a frame rate of the underlying video. While the example of FIG. 1 shows aggregation over only three frames 101 , 102 , 103 , this is only for purposes of illustration and is not intended to be limiting.
- FIG. 2 presents an example of how the aggregation of exclusion zones can proceed frame-by-frame of the example of FIG. 1 .
- a minimum number of consecutive frames can be selected over which exclusion zones can be aggregated (or, in some situations, a minimum interval of time can be selected, and converted to a number of consecutive frames based upon the video frame rate).
- the minimum number of consecutive frames is three frames, corresponding to frames 101 , 102 , and 103 as shown.
- each frame 101 , 102 , and 103 in the underlying video frame may contain features of interest such as text 111 , human features 112 , or other potential objects of interest 113 .
- a machine learning system such as an optical character recognition system, a Bayesian classifier, or a convolutional neural network classifier, can be used to detect the features of interest within the frame.
- the detection of a feature within a frame can include determining a bounding box that encloses the feature.
- the machine learning system can output bounding boxes 211 enclosing detected text each frame, bounding boxes 212 enclosing human features 112 within each frame, and/or bounding boxes 213 enclosing other potential objects of interest 213 within each frame.
- the bounding boxes 212 enclosing human features 112 can be selected to enclose the entirety of any human features detected within the frame, or they can be selected to enclose only a portion of any human features detected within the frame (e.g. enclosing only faces, heads and shoulders, torsos, hands, etc.). As shown in column 220 of FIG.
- the bounding boxes 211 , 212 , 213 for the consecutive frames can correspond to exclusion zones 221 , 222 , 223 , respectively, which can be accumulated or aggregated frame-by-frame, with newly-added exclusion zones 230 being accumulated for exclusion zones of frame 102 and newly-added exclusion zones 240 being accumulated for exclusion zones of frame 103 , so that as seen in the rightmost bottom frame of FIG. 2 , the aggregation includes all bounding boxes for all features detected within all frames within the selected interval of consecutive frames.
- the aggregated exclusion zones seen at bottom right of FIG. 2 are the aggregated exclusion zones for frame 101 for the selected interval of consecutive frames (three in this instance).
- the aggregated exclusion zones would include the single-frame exclusion zones of frame 102 , frame 103 , and a fourth frame that is not shown; similarly, the aggregated exclusion zones for frame 102 would include the single-frame exclusion zones of frame 103 , a fourth frame that is not shown, and a fifth frame that is not shown; and so on.
- an inclusion zone can be defined within which overlaid content is eligible for display.
- the inclusion zone 125 in FIG. 1 corresponds to a region of the viewing area 120 in which neither text 111 , human features 112 , or other objects of interest 113 appears across the span of frames 101 , 102 , 103 .
- the inclusion zone can be defined as a set of inclusion zone rectangles whose union defines the entirety of the inclusion zone.
- the set of inclusion zone rectangles can be calculated by iterating over all of the bounding boxes (e.g. rectangles 211 , 212 , and 213 in FIG. 2 ) that have been accumulated to define the aggregate exclusion zone. For a given bounding box in the accumulated bounding boxes, select the top right corner as a starting point (x,y), and then expand up, left, and right to find the largest box that does not overlap any of the other bounding boxes (or the edge of the viewing area), and add that largest box to the list of inclusion zone rectangles.
- the inclusion zone thus defined is an inclusion zone for frame 101 because it defines areas in which overlaid content can be positioned in frame 101 and subsequent frame for the selected interval of consecutive frames (three in this instance) without occluding any detected features in any of frames 101 , 102 , 103 , i.e. within any frames within the selected interval of consecutive frames.
- An inclusion zone for frame 102 can be similarly defined but it would involve the complement of single-frame exclusion zones for frame 102 , frame 103 , and a fourth frame that is not shown; similarly, an inclusion zone for frame 103 would involve the complement of single-frame exclusion zones for frame 103 , a fourth frame that is not shown, and a fifth frame that is not shown; and so on.
- suitable overlaid content can be selected for display within the inclusion zone.
- a set of candidate overlaid content may be available, where each item in the set of candidate overlaid content has specifications that can include, for example, the width and height of each item, a minimum duration of time during which each item would be provided to the user, etc.
- One or more items from the set of candidate overlaid content may be selected to fit within the defined inclusion zone. For example, as shown in viewing area of FIG. 1 , two items of overlaid content 126 might be selected to fit within the inclusion zone 105 .
- the number of overlaid content features may be limited to one, two, or more features.
- a first overlaid content feature may be provided during a first span of time (or span of frames), and a second overlaid content feature may be provided during a second span of time (or span of frames), etc., and the first, second, etc. spans of time (or spans of frames) may be completely overlapping, partially overlapping, or non-overlapping.
- FIG. 1 depicts an example of a video stream 130 that includes both the underlying video and the overlaid content.
- the overlaid content 126 does not occlude or obstruct the features of interest that were detected in the underlying video 100 and used to define exclusion zones for the overlaid content.
- the system may operate as a video pipeline, receiving, as input, an original video on which to overlay content, and providing, as output, a video with overlaid content.
- the system 300 can include a video preprocessor unit 301 which may be used to provide for downstream uniformity of video specifications, such as frame rate (which may be adjusted by a resampler), video size/quality/resolution (which may be adjusted by a rescaler), and video format (which may be adjusted by a format converter).
- the output of the video preprocessor is a video stream 302 in a standard format for further processing by the downstream components of the system.
- the system 300 includes a text detector unit 311 that receives as input the video stream 302 and provides as output a set of regions in which text appears in the video stream 302 .
- the text detector unit can be a machine learning unit, such as an optical character recognition (OCR) module.
- OCR optical character recognition
- the OCR module need only find regions in which text appears in the video without actually recognizing the text that is present within those regions.
- the text detector unit 311 can generate (or specify) a bounding box delineating (or otherwise defining) the regions within each frame that have been determined to include text, which can be used in identifying exclusion zones for overlaid content.
- the text detector unit 311 can output the detected text bounding boxes as, for example, as an array (indexed by frame number) where each element of the array is a list of the rectangles defining text bounding boxes detected within that frame.
- the detected bounded boxes can be added to the video stream as metadata information for each frame of the video.
- the system 300 also includes a person or human features detector unit 312 that receives as input the video stream 302 and provides as output a set of regions of the video that contain persons (or portions thereof, such as faces, torsos, limbs, hands, etc.).
- the person detector unit can be a computer vision system such as a machine learning system, e.g., a Bayesian image classifier or convolutional neural network (CNN) image classifier.
- the person or human features detector unit 312 can be trained, for example, on labeled training samples that are labeled with the human features depicted by the training samples.
- the person or human features detector unit 312 can output a label identifying one or more human features that are detected in each frame of a video and/or a confidence value indicating the level of confidence that the one or more human features are located within each frame.
- the person or human features detector unit 312 can also generate a bounding box delineating an area in which the one or more human features have been detected, which can be used in identifying exclusion zones for overlaid content.
- the human features detector unit need only find regions in which human features appear in the video without actually recognizing the identities of persons that are present within those regions (e.g.
- the human features detector unit 312 can output the detected human features bounding boxes as, for example, as an array (indexed by frame number) where each element of the array is a list of the rectangles defining human features bounding boxes detected within that frame.
- the detected bounded boxes can be added to the video stream as metadata information for each frame of the video.
- the system 300 also includes an object detector unit 313 that receives as input the video stream 302 and provides as output a set of regions of the video that contain potential objects of interest.
- the potential objects of interest can be objects that are classified as belonging to an object category in a selected list of object categories (e.g. animals, plants, road or terrain features, containers, furnishings, etc.).
- the potential objects of interest can also be limited to identified objects that are in motion, e.g. objects that move a certain minimum distance within a selected interval of time (or selected interval of frames) or that move during a specified number of sequential frames in the video stream 302 .
- the object detector unit can be a computer vision system such as a machine learning system, e.g., a Bayesian image classifier or convolutional neural network image classifier.
- the object detector unit 313 can be trained, for example, on labeled training samples that are labeled with objects that are classified as belonging to an object category in a selected list of object categories.
- the object detector can be trained to recognize animals such as cats or dogs; or the object detector can be trained to recognize furnishings such as tables and chairs; or the object detector can be trained to recognize terrain or road features such as trees or road signs; or any combination of selected object categories such as these.
- the object detector unit 313 can also generate bounding boxes delineating (or otherwise specifying) areas of video frames in which the identified objects have been identified.
- the object detector unit 312 can output the detected object bounding boxes as, for example, as an array (indexed by frame number) where each element of the array is a list of the rectangles defining object bounding boxes detected within that frame.
- the detected bounded boxes can be added to the video stream as metadata information for each frame of the video.
- system 300 may comprise at least one of text detector 311 , person detector 312 or object detector 313 .
- the system 300 also includes an inclusion zone calculator unit or module 320 that receives input from one or more of the text detector unit 311 (with information about regions in which text appears in the video stream 302 ), the person detector unit 312 (with information about regions in which persons or portions thereof appear in the video stream 302 ), and the object detector unit 313 (with information about regions in which various potential objects of interest appear in the video stream 302 ).
- Each of these regions can define an exclusion zone; the inclusion zone calculator unit can aggregate those exclusion zones; and then the inclusion zone calculator can define an inclusion zone within which overlaid content is eligible for inclusion.
- the aggregated exclusion zone can be defined as the union of a list of rectangles that each include a potential interesting feature such as text, a person, or another object of interest. It may be represented as an accumulation of bounding boxes that are generated by the detector units 311 , 312 , and 313 , over a selected number of consecutive frames. First, the bounding boxes can be aggregated for each frame.
- the text detector unit 311 outputs a first array (indexed by frame number) of lists of text bounding boxes in each frame
- the human features detector 312 outputs a second array (indexed by frame number) of lists of human features bounding boxes in each frame
- the object detector unit 313 outputs a third array (indexed by frame number) of lists of bounding boxes for detected objects in each frame
- these first, second, and third arrays can be merged to define a single array (again indexed by frame number), where each element is a single list that merges all bounding boxes for all features (text, human, or other object) detected within that frame.
- the bounded boxes can be aggregated over a selected interval of consecutive frames.
- a new array (again index by frame number) might be defined, where each element is a single list that merges all bounding boxes for all features detected within frames i, i+1, i+2, . . . , i+(N ⁇ 1), where N is the number of frames in the selected interval of consecutive frames.
- the aggregated exclusion zone data can be added to the video stream as metadata information for each frame of the video.
- the inclusion zone calculated by the inclusion zone calculator unit 320 can be then be defined as the complement of the accumulation of bounding boxes that are generated by the detector units 311 , 312 , and 313 , over a selected number of consecutive frames.
- the inclusion zone can be specified, for example, as another list of rectangles, the union thereof forming the inclusion zone; or as a polygon with horizontal and vertical sides, which may be described, e.g., by a list of vertices of the polygon; or as a list of such polygons if the inclusion zone, e.g., if the inclusion zone includes disconnected areas of the viewing screen.
- the inclusion zone calculator unit 320 can store the inclusion zone information as a new array (again indexed by frame number) where each element is a list of inclusion rectangles for that frame, taking into account all of the bounding boxes accumulated over that frame and the following N ⁇ 1 consecutive frames by iterating over each accumulated bounding box and over the four corners of each bounding box, as discussed above in the context of FIG. 2 .
- these inclusion zone rectangles can be overlapping rectangles that collectively define the inclusion zone.
- this inclusion zone data can be added to the video stream as metadata information for each frame of the video.
- the system 300 also includes an overlaid content matcher unit or module 330 that receives input from the inclusion zone calculator unit or module 320 , for example, in the form of a specification for an inclusion zone.
- the overlaid content matcher unit can select suitable content for overlay on the video within the inclusion zone.
- the overlaid content matcher may have access to a catalog of candidate overlaid content, where each item in the catalog of candidate overlaid content has specifications that can include, for example, the width and height of each item, a minimum duration of time during which each item should be provided to the user, etc.
- the overlaid content matcher unit can select one or more items from the set of candidate overlaid content to fit within the inclusion zone provided by the inclusion zone calculator unit 320 .
- the overlaid content matcher can identify inclusion zone rectangles within the array that are large enough to fit the selected item; they can be ranked in order of size, and/or in order of persistence (e.g. if the same rectangle appears in multiple consecutive elements of the array, indicating that the inclusion zone is available for even more than the minimum number of consecutive frames N), and then an inclusion zone rectangle can be selected from that ranked list for inclusion of the selected overlaid content.
- the overlaid content may be scalable, e.g. over a range of possible x or y dimension or over a range of possible aspect ratios; in these approaches, an inclusion zone rectangle matching the overlaid content item may be selected to the, for example, the largest area inclusion zone rectangle that could fit the scalable overlaid content, or the inclusion zone rectangle of sufficient size that can persist for the longest duration of consecutive frames.
- the system 300 also includes an overlay unit 340 which receives as input both the underlying video stream 302 and the selected overlaid content 332 (and location(s) thereof) from the overlaid content matcher 330 .
- the overlay 340 can then provide a video stream 342 that includes both the underlying video content 302 and the selected overlaid content 332 .
- a video visualizer 350 e.g. a video player embedded within a web browser, or a video app on a mobile device displays the video stream with the overlaid content to the user.
- the overlay unit 340 may reside on the user device and/or be embedded within the video visualizer 350 ; in other words, both the underlying video stream 302 and the selected overlaid content 332 may be delivered to the user (e.g. over the internet), and they may be combined on the user device to display an overlaid video to the user.
- inclusion zones can correspond to regions of the viewing area containing features of the video that are more likely to be of interest to the viewer, such as regions contain text (e.g. regions 111 in FIG. 1 ), regions containing persons or human features (e.g. regions 112 in FIG. 1 ), and regions containing particular objects of interest (e.g. regions 113 in FIG. 1 ).
- These regions can be detected, for example, using machine learning systems such as an OCR detector for text (e.g. text detector 311 in FIG. 3 ), a computer vision system for person or human features (e.g. person detector 312 in FIG. 3 ), and a computer vision system for other objects of interest (e.g. object detector 313 in FIG. 3 ).
- machine learning systems such as an OCR detector for text (e.g. text detector 311 in FIG. 3 ), a computer vision system for person or human features (e.g. person detector 312 in FIG. 3 ), and a computer vision system for other objects of interest (e.g. object detector 313 in FIG. 3 ).
- the process also includes 420 —aggregating the corresponding exclusion zones for the video frames in a specified duration or number of the sequence of frames. For example, as shown in FIG. 1 , rectangles 121 , 122 , and 123 that are bounding boxes of potential features of interest can be aggregated over a sequence of frames to define an aggregate exclusion zone that is a union of the exclusion zones for the sequence of frames. This union of the exclusion zone rectangles can be calculated, for example, by the inclusion zone calculator unit 320 of FIG. 3 .
- the process further includes 430 —defining, within the specified duration or number of the sequence of frames of the video, an inclusion zone within which overlaid content is eligible for inclusion, the inclusion zone being defined as an area of the video frames in the specified duration or number that is outside of the aggregated corresponding exclusion zones.
- the inclusion zone 125 in FIG. 1 can be defined as a complement of the aggregated exclusion zones, and the inclusion zone may be described as a union of rectangles that collectively fill the inclusion zone.
- the inclusion zone may be calculated, for example, by the inclusion zone calculator unit 320 of FIG. 3 .
- the process further includes 440 —providing overlaid content for inclusion in the inclusion zone of the specified duration or number of the sequence of frames of the video during display of the video at a client device.
- overlaid content may be selected from a catalog of candidate overlaid content, based, e.g., on dimensions of the items in the catalog of candidate overlaid content.
- two overlaid content features 126 are selected for inclusion within the inclusion zone 125 .
- the overlaid content (and its positioning within the viewing area) may be selected by the overlaid content matcher 330 of FIG. 3 , and the overlay unit 340 can superimpose the overlaid content on top of the underlying video stream 302 to define a video stream 342 with overlaid content to be provided to the user for viewing on a client device.
- the overlaid content that is selected from the catalog of overlaid content may be chosen in the following manner.
- an inclusion zone may be defined as a union of a list of inclusion area rectangles, which are the rectangles that do not intersect any of the exclusion zones (bounding boxes for detected objects) in frames i, i+1, . . i+(N ⁇ 1), where N is a selected minimum number of consecutive frames.
- the inclusion area rectangles are selected from the list of inclusion area rectangles that could fit the candidate item. These are inclusion area rectangles that could fit the candidate item for the selected minimum number of consecutive frames N.
- the same process can be performed for frame i+1; then, by taking an intersection of the results for frame i and for frame i+1, a list of inclusion area rectangles can be obtained that could fit the candidate item for N+1 consecutive frames. Again performing an intersection with the results for frame i+2, a set of inclusion areas that could fit the candidate item for N+2 consecutive frames can be obtained.
- the process can be iterated for any selected span of frames (including the entire duration of the video) to obtain rectangles suitable for inclusion of the candidate item for frame durations N, N+1, . . . , N+(k ⁇ 1) where N+k is the longest possible duration.
- a location for the overlaid content may be selected from the list of inclusion area rectangles that can persist for the longest duration without occluding detected features, i.e. for N+k frames.
- more than one content feature may be included at the same time. For example, a first item of overlaid content may be selected, and then a second item of overlaid content may be selected by defining an additional exclusion zone that encloses the first item of overlaid content.
- the second item of overlaid content may be placed by regarding the video overlaid with the first item of overlaid content as a new underlying video suitable for overlay of additional content.
- the exclusion zone for the first item of overlaid content may be made significantly larger than the overlaid content itself, to increase the spatial separation between different items of overlaid content within the viewing area.
- the selecting of overlaid content may include selecting that allows a specified level of encroachment on an exclusion zone. For example, some area-based encroachment could be tolerated by weighing inclusion zone rectangles by the extent to which the overlaid content spatially extends outside of each inclusion zone rectangle. Alternatively or additionally, some time-based encroachment could be tolerated by ignoring ephemeral exclusion zones that only exist for a relatively short time. For example, if the exclusion zone is only defined for a single frame out of 60 frames, it could be lower weighted and therefore more likely to be occluded than an area in which the exclusion zone exists for the entire 60 frames.
- some content-based encroachment could be tolerated by ranking the relative importance of different types of exclusion zones corresponding to different types of detected features. For example, detected text features could be ranked as more important than detected non-text features, and/or detected human features could be ranked as more important than detected non-human features, and/or more rapidly moving features could be ranked as more important than more slowly moving features.
- FIG. 5 is a block diagram of an example computer system 500 that can be used to perform operations described above.
- the system 500 includes a processor 510 , a memory 520 , a storage device 530 , and an input/output device 540 .
- Each of the components 510 , 520 , 530 , and 540 can be interconnected, for example, using a system bus 550 .
- the processor 510 is capable of processing instructions for execution within the system 500 .
- the processor 510 is a single-threaded processor.
- the processor 510 is a multi-threaded processor.
- the processor 510 is capable of processing instructions stored in the memory 520 or on the storage device 530 .
- the memory 520 stores information within the system 500 .
- the memory 520 is a computer-readable medium.
- the memory 520 is a volatile memory unit.
- the memory 520 is a non-volatile memory unit.
- the storage device 530 is capable of providing mass storage for the system 500 .
- the storage device 530 is a computer-readable medium.
- the storage device 530 can include, for example, a hard disk device, an optical disk device, a storage device that is shared over a network by multiple computing devices (e.g., a cloud storage device), or some other large capacity storage device.
- the input/output device 540 provides input/output operations for the system 500 .
- the input/output device 540 can include one or more of a network interface devices, e.g., an Ethernet card, a serial communication device, e.g., and RS-232 port, and/or a wireless interface device, e.g., and 802.11 card.
- the input/output device can include driver devices configured to receive input data and send output data to external devices 460 , e.g., keyboard, printer and display devices.
- Other implementations, however, can also be used, such as mobile computing devices, mobile communication devices, set-top box television client devices, etc.
- Embodiments of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on computer storage media (or medium) for execution by, or to control the operation of, data processing apparatus.
- the computer storage media (or medium) may be transitory or non-transitory.
- the program instructions can be encoded on an artificially-generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- a computer storage medium can be, or be included in, a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them.
- a computer storage medium is not a propagated signal, a computer storage medium can be a source or destination of computer program instructions encoded in an artificially-generated propagated signal.
- the computer storage medium can also be, or be included in, one or more separate physical components or media (e.g., multiple CDs, disks, or other storage devices).
- the operations described in this specification can be implemented as operations performed by a data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
- the term “data processing apparatus” encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or multiple ones, or combinations, of the foregoing.
- the apparatus can include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- the apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them.
- the apparatus and execution environment can realize various different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures.
- a computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment.
- a computer program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code).
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output.
- the processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors.
- a processor will receive instructions and data from a read-only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing actions in accordance with instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a universal serial bus (USB) flash drive), to name just a few.
- Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network.
- Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), an inter-network (e.g., the Internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
- LAN local area network
- WAN wide area network
- inter-network e.g., the Internet
- peer-to-peer networks e.g., ad hoc peer-to-peer networks.
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- a server transmits data (e.g., an HTML page) to a client device (e.g., for purposes of displaying data to and receiving user input from a user interacting with the client device).
- client device e.g., for purposes of displaying data to and receiving user input from a user interacting with the client device.
- Data generated at the client device e.g., a result of the user interaction
Abstract
Description
- Videos that are streamed to a user can include additional content that is overlaid on top of the original video stream. The overlaid content may be provided to the user within a rectangular region that overlays and blocks a portion of the original video screen. In some approaches, the rectangular region for provision of the overlaid content is positioned at the center bottom of the video screen. If important content of the original video stream is positioned at the center bottom of the video screen, it can be blocked or obstructed by the overlaid content.
- This specification describes technologies related to overlaying content on top of a video stream, while at the same time avoiding areas of the video screen that feature useful content in the underlying video stream, e.g., areas in the original video stream that contain faces, text, or significant objects such as fast-moving objects.
- In general, a first innovative aspect of the subject matter described in this specification can be embodied in methods that include identifying, for each video frame among a sequence of frames of a video, a corresponding exclusion zone from which to exclude overlaid content based on the detection of a specified object in a region of the video frame that is within the corresponding exclusion zone; aggregating the corresponding exclusion zones for the video frames in a specified duration or number of the sequence of frames; defining, within the specified duration or number of the sequence of frames of the video, an inclusion zone within which overlaid content is eligible for inclusion, the inclusion zone being defined as an area of the video frames in the specified duration or number that is outside of the aggregated corresponding exclusion zones; and providing overlaid content for inclusion in the inclusion zone of the specified duration or number of the sequence of frames of the video during display of the video at a client device. Other implementations of this aspect include corresponding apparatus, systems, and computer programs, configured to perform the aspects of the methods, encoded on computer storage devices.
- In some aspects, the identifying of the exclusion zones can include identifying, for each video frame in the sequence of frames, one or more regions in which text is displayed in the video, the methods further including generating one or more bounding boxes that delineate the one or more regions from other parts of the video frame. The identifying of the one or more regions in which text is displayed can include identifying the one or more regions with an optical character recognition system.
- In some aspects, the identifying of the exclusion zones can include identifying, for each video frame in the sequence of frames, one or more regions in which human features are displayed in the video, the methods further including generating one or more bounding boxes that delineate the one or more regions from other parts of the video frame. The identifying the one or more regions in which human features are displayed can include identifying the one or more regions with a computer vision system trained to identify human features. The computer vision system can be a convolutional neural network system.
- In some aspects, the identifying of the exclusion zones can include identifying, for each video frame in the sequence of frames, one or more regions in which significant objects are displayed in the video, wherein identifying of the regions in which significant objects are displayed is identifying with a computer vision system configured to recognize objects from a selected set of object categories not including text or human features. The identifying of the exclusion zones can include identifying the one or more regions in which the significant objects are displayed in the video, based on detection of objects that move more than a selected distance between consecutive frames or detection of objects that move during a specified number of sequential frames.
- In some aspects, the aggregating of the corresponding exclusion zones can include generating a union of bounding boxes that delineate the corresponding exclusion zones from other parts of the video. The defining of the inclusion zone can include identifying, within the sequence of frames of the video, a set of rectangles that do not overlap with the aggregated corresponding exclusion zones over the specified duration or number; and the providing overlaid content for inclusion in the inclusion zone can include: identifying an overlay having dimensions that fit within one or more rectangles among the set of rectangles; and providing the overlay within the one or more rectangles during the specified duration or number.
- The subject matter described in this specification can be implemented in particular embodiments so as to realize one or more of the following advantages.
- While a user is viewing a video stream that fills a video screen, the content of value to the user within that video screen area may not fill the entire area of the video screen. For example, the valuable content, e.g., faces, text, or significant objects such as fast-moving objects, may occupy only a portion of the video screen area. There is an opportunity, therefore, to present additional useful content to the user in the form of overlaid content that does not obstruct the portion of the video screen area that contains the valuable underlying content. Aspects of the present disclosure provide the advantage of identifying exclusion zones from which to exclude overlaid content, because overlaying content over these exclusion zones would block or obscure valuable content that is included in the underlying video stream, which would result in wasted computing resources by delivering video to users when the valuable content is not perceivable to the users. In some situations, machine learning engines (such as Bayesian classifiers, optical character recognition systems, or neural networks) can identify interesting features within the video stream, such as faces, text, or other significant objects; exclusion zones can be identified that encompass these interesting features; and then the overlaid content can be presented outside of these exclusion zones. As a result, the user can receive the overlaid content without obstruction of the valuable content of the underlying video stream, such that the computing resources required to deliver the video are not wasted. This results in a more efficient video distribution system that prevents computing system resources (e.g., network bandwidth, memory, processor cycles, and limited client device display space) from being wasted through the delivery of videos in which the valuable content is occluded, or otherwise not perceivable by the user.
- This has the further advantage of improving the efficiency of the screen area in terms of the bandwidth of useful content delivered to the viewer. If the user is viewing a video in which, as is typical, the valuable content of the video occupies only a fraction of the viewing area, the available bandwidth to deliver useful content to the viewer is underutilized. By using machine learning systems to identify that fraction of the viewing area that contains the valuable content of the underlying video stream, aspects of the present disclosure provide for overlaying additional content outside of that fraction of the viewing area, leading to more efficient utilization of the screen area to deliver useful content to the viewer.
- In some approaches, the overlaid content includes a box or other icon that the viewer can click to remove the overlaid content, for example, if the overlaid content obstructs valuable content in the underlying video. A further advantage of the present disclosure is that, because the overlaid content is less likely to obstruct valuable content in the underlying video, there is less disruption of the viewing experience and a greater likelihood that the viewer will not “click away” the overlaid content that has been presented.
- Various features and advantages of the foregoing subject matter are described below with respect to the figures. Additional features and advantages are apparent from the subject matter described herein and the claims.
-
FIG. 1 depicts an overview of aggregating exclusion zones and defining an inclusion zone for a video that includes a sequence of frames. -
FIG. 2 depicts an example of frame-by-frame aggregation of exclusion zones for the example ofFIG. 1 . -
FIG. 3 depicts an example of a machine learning system for identifying and aggregating exclusion zones and selecting overlaid content. -
FIG. 4 depicts a flow diagram for a process that includes aggregating exclusion zones and selecting overlaid content. -
FIG. 5 is a block diagram of an example computer system. - Like reference numbers and designations in the various drawings indicate like elements.
- It is generally desirable to provide overlaid content on an underlying video stream, to provide additional content to a viewer of the video stream and to improve the quantity of content delivered within the viewing area for a given video streaming bandwidth. However, there is a technical problem of determining how to place the overlaid content so that it does not occlude valuable content in the underlying video. This is a particularly difficult problem in the context of overlaying content on video because the locations of important content in a video can change quickly over time. As such, even if a particular location within the video is a good candidate for overlay content at one point in time, that location may be a bad candidate for overlay content at a later point in time (e.g., due to movement of characters within the video).
- The present specification presents a solution to this technical problem by describing machine learning methods and systems that can identify exclusion zones that correspond to regions of the video stream that are more likely to contain valuable content; aggregating these exclusion zones over time; and then positioning the overlaid content in an inclusion zone that is outside of the aggregated exclusion zones, so that the overlaid content is less likely to obstruct valuable content in the underlying video.
-
FIG. 1 depicts an illustrative example of exclusion zones, inclusion zones, and overlaid content for a video stream. In this example, an original,underlying video stream 100 is depicted asframes text 111, such as closed caption text or text that appears on features within a video, e.g., text that appears on product labels, road signs, white boards on screen within a video of a school lecture, etc. Note that features within the video are part of the video stream itself, whereas any overlaid content is separate from the video stream itself. As further discussed below, a machine learning system such as an optical character recognition (OCR) system can be used to identify regions with the frames that contain text, and that identifying can include identifying bounding boxes that enclose the identified text, as shown. As illustrated in the example ofFIG. 1 , thetext 111 might be situated at different locations within the different frames, so overlaid content that persists for a duration of time that includesmultiple frames text 111 is situated across the multiple frames. - Other regions that should not be occluded by overlaid content features (e.g., to ensure that the important content of the underlying video are not occluded) are
regions 112 that include persons or human features. For example, the frames may include one or more persons, or portions thereof, such as human faces, torsos, limbs, hands, etc. As further discussed below, a machine learning system such as a convolutional neural network (CNN) system can be used to identify regions within the frames that contain persons (or portions thereof, such as faces, torsos, limbs, hands, etc.), and that identifying can include identifying bounding boxes that enclose the identified human features, as shown. As illustrated in the example ofFIG. 1 , thehuman features 112 might be situated at different locations within the different frames, so overlaid content that persists for a duration of time that includesmultiple frames human features 112 are situated across the multiple frames. In some approaches, thehuman features 112 may be discriminated by limiting the human features detection to larger human features, i.e. features that are in the foreground and closer to the point of view of the video, as opposed to background human features. For example, larger human faces corresponding to persons in the foreground of a video may be included in the detection scheme, while smaller human faces corresponding to persons in the background of a video, such as faces in a crowd, may be excluded from the detection scheme. - Other regions that should not be occluded by overlaid content features are
regions 113 that contain other potential objects of interest, such as animals, plants, street lights or other road features, bottles or other containers, furnishings, etc. As further discussed below, a machine learning system such as a convolutional neural network (CNN) system can be used to identify regions within the frames that contain potential objects of interest. For example, a machine learning system can be trained to classify various objects selected from a list of object categories, e.g. to identify dogs, cats, vases, flowers, or any other category of object that may be of potential interest to the viewer. The identifying can include identifying bounding boxes that enclose the detected objects, as shown. As illustrated in the example ofFIG. 1 , the detected object 113 (a cat in this example) might be situated at different locations within the different frames, so overlaid content that persists for a duration of time that includesmultiple frames objects 113 are situated across the multiple frames. - In some approaches, the detected
objects 113 may be discriminated by limiting the object detection to objects that are in motion. Objects that are in motion are generally more likely to convey important content to the viewer and therefore potentially less suitable to be occluded by overlaid content. For example, the detectedobjects 113 can be limited to objects that move a certain minimum distance within a selected interval of time (or selected interval of frames), or that move during a specified number of sequential frames. - As illustrated by the
video screen 120 inFIG. 1 , exclusion zones from which to exclude overlaid content can be identified based on the detected features in the video that are more likely to be of interest to the viewer. Thus, for example, the exclusion zones can includeregions 121 in which text has been identified as appearing in frames of the video (compare with identifiedtext 111 in original video frames 101, 102, 103); the exclusion zones can also includeregions 122 in which human features have been identified as appearing in frames of the video (compare with identifiedhuman features 112 in original video frames 101, 102, 103); and the exclusion zones can also includeregions 123 in which other objects of interest (such as faster-moving objects, or objects identified from a selected list of object categories) have been identified as appearing in frames of the video (compare with identifiedobject 113 in original video frames 101, 102, 103). - Because the overlaid content may be included for a selected duration of time (or a selected number of frames of the underlying video), the exclusion zones can be aggregated over the selected duration of time (or selected span of frames) to define an aggregate exclusion zone. For example, the aggregate exclusion zone can be the union of all exclusion zones corresponding to detected features of interest in each video frame among a sequence of frames of a video. The selected duration of time could be 1 second, 5 seconds, 10 seconds, 1 minute, or any other duration of time that is suitable for display of the overlaid content. Alternatively, a selected span of frames could be 24 frames, 60 frames, 240 frames, or any other span of frames that is suitable for display of the overlaid content. A selected duration of time can correspond to a selected span of frames, or vice versa, as determined by a frame rate of the underlying video. While the example of
FIG. 1 shows aggregation over only threeframes -
FIG. 2 presents an example of how the aggregation of exclusion zones can proceed frame-by-frame of the example ofFIG. 1 . First, a minimum number of consecutive frames can be selected over which exclusion zones can be aggregated (or, in some situations, a minimum interval of time can be selected, and converted to a number of consecutive frames based upon the video frame rate). In this example, for purposes of the illustration only, the minimum number of consecutive frames is three frames, corresponding toframes - As shown in
column 200 ofFIG. 2 , eachframe text 111,human features 112, or other potential objects ofinterest 113. For each frame, a machine learning system such as an optical character recognition system, a Bayesian classifier, or a convolutional neural network classifier, can be used to detect the features of interest within the frame. As shown incolumn 210 ofFIG. 2 , the detection of a feature within a frame can include determining a bounding box that encloses the feature. Thus, the machine learning system canoutput bounding boxes 211 enclosing detected text each frame, boundingboxes 212 enclosinghuman features 112 within each frame, and/or boundingboxes 213 enclosing other potential objects ofinterest 213 within each frame. The boundingboxes 212 enclosinghuman features 112 can be selected to enclose the entirety of any human features detected within the frame, or they can be selected to enclose only a portion of any human features detected within the frame (e.g. enclosing only faces, heads and shoulders, torsos, hands, etc.). As shown incolumn 220 ofFIG. 2 , the boundingboxes exclusion zones exclusion zones 230 being accumulated for exclusion zones offrame 102 and newly-addedexclusion zones 240 being accumulated for exclusion zones offrame 103, so that as seen in the rightmost bottom frame ofFIG. 2 , the aggregation includes all bounding boxes for all features detected within all frames within the selected interval of consecutive frames. Note that in this example, the aggregated exclusion zones seen at bottom right ofFIG. 2 are the aggregated exclusion zones forframe 101 for the selected interval of consecutive frames (three in this instance). Thus, forframe 102, for example, the aggregated exclusion zones would include the single-frame exclusion zones offrame 102,frame 103, and a fourth frame that is not shown; similarly, the aggregated exclusion zones forframe 102 would include the single-frame exclusion zones offrame 103, a fourth frame that is not shown, and a fifth frame that is not shown; and so on. - Having aggregated exclusion zones over a selected duration of time or selected span of frames, an inclusion zone can be defined within which overlaid content is eligible for display. For example, the
inclusion zone 125 inFIG. 1 corresponds to a region of theviewing area 120 in which neithertext 111,human features 112, or other objects ofinterest 113 appears across the span offrames - In some approaches, the inclusion zone can be defined as a set of inclusion zone rectangles whose union defines the entirety of the inclusion zone. The set of inclusion zone rectangles can be calculated by iterating over all of the bounding boxes (
e.g. rectangles FIG. 2 ) that have been accumulated to define the aggregate exclusion zone. For a given bounding box in the accumulated bounding boxes, select the top right corner as a starting point (x,y), and then expand up, left, and right to find the largest box that does not overlap any of the other bounding boxes (or the edge of the viewing area), and add that largest box to the list of inclusion zone rectangles. Next, select the bottom right corner as a starting point (x,y), and then expand up, down, and right to find the largest box that does not overlap any of the other bounding boxes (or the edge of the viewing area), and add that largest box to the list of inclusion zone rectangles. Next, select the top left corner as a starting point (x,y), and then expand up, left, and right to find the largest box that does not overlap any of the other bounding boxes (or the edge of the viewing area), and add that largest box to the list of inclusion zone rectangles. Next, select the bottom left corner as a starting point (x,y), and then expand down, left, and and right to find the largest box that does not overlap any of the other bounding boxes (or the edge of the viewing area), and add that largest box to the list of inclusion zone rectangles. Then repeat these steps for the next bounding box in the accumulation of bounding boxes. Note that these steps can be completed in any order. The inclusion zone thus defined is an inclusion zone forframe 101 because it defines areas in which overlaid content can be positioned inframe 101 and subsequent frame for the selected interval of consecutive frames (three in this instance) without occluding any detected features in any offrames frame 102 can be similarly defined but it would involve the complement of single-frame exclusion zones forframe 102,frame 103, and a fourth frame that is not shown; similarly, an inclusion zone forframe 103 would involve the complement of single-frame exclusion zones forframe 103, a fourth frame that is not shown, and a fifth frame that is not shown; and so on. - With the inclusion zone defined, suitable overlaid content can be selected for display within the inclusion zone. For example, a set of candidate overlaid content may be available, where each item in the set of candidate overlaid content has specifications that can include, for example, the width and height of each item, a minimum duration of time during which each item would be provided to the user, etc. One or more items from the set of candidate overlaid content may be selected to fit within the defined inclusion zone. For example, as shown in viewing area of
FIG. 1 , two items of overlaidcontent 126 might be selected to fit within the inclusion zone 105. In various approaches, the number of overlaid content features may be limited to one, two, or more features. In some approaches, a first overlaid content feature may be provided during a first span of time (or span of frames), and a second overlaid content feature may be provided during a second span of time (or span of frames), etc., and the first, second, etc. spans of time (or spans of frames) may be completely overlapping, partially overlapping, or non-overlapping. - Having selected the overlaid
content 126 to be provided to the viewer along with the underlying video stream,FIG. 1 depicts an example of avideo stream 130 that includes both the underlying video and the overlaid content. As illustrated in this example, the overlaidcontent 126 does not occlude or obstruct the features of interest that were detected in theunderlying video 100 and used to define exclusion zones for the overlaid content. - With reference now to
FIG. 3 , an illustrative example is depicted as a block diagram for a system of selecting inclusion zones and providing overlaid content on a video stream. The system may operate as a video pipeline, receiving, as input, an original video on which to overlay content, and providing, as output, a video with overlaid content. Thesystem 300 can include avideo preprocessor unit 301 which may be used to provide for downstream uniformity of video specifications, such as frame rate (which may be adjusted by a resampler), video size/quality/resolution (which may be adjusted by a rescaler), and video format (which may be adjusted by a format converter). The output of the video preprocessor is avideo stream 302 in a standard format for further processing by the downstream components of the system. - The
system 300 includes atext detector unit 311 that receives as input thevideo stream 302 and provides as output a set of regions in which text appears in thevideo stream 302. The text detector unit can be a machine learning unit, such as an optical character recognition (OCR) module. For purposes of efficiency, the OCR module need only find regions in which text appears in the video without actually recognizing the text that is present within those regions. Once the regions have been identified, thetext detector unit 311 can generate (or specify) a bounding box delineating (or otherwise defining) the regions within each frame that have been determined to include text, which can be used in identifying exclusion zones for overlaid content. Thetext detector unit 311 can output the detected text bounding boxes as, for example, as an array (indexed by frame number) where each element of the array is a list of the rectangles defining text bounding boxes detected within that frame. In some approaches, the detected bounded boxes can be added to the video stream as metadata information for each frame of the video. - The
system 300 also includes a person or humanfeatures detector unit 312 that receives as input thevideo stream 302 and provides as output a set of regions of the video that contain persons (or portions thereof, such as faces, torsos, limbs, hands, etc.). The person detector unit can be a computer vision system such as a machine learning system, e.g., a Bayesian image classifier or convolutional neural network (CNN) image classifier. The person or humanfeatures detector unit 312 can be trained, for example, on labeled training samples that are labeled with the human features depicted by the training samples. Once trained, the person or humanfeatures detector unit 312 can output a label identifying one or more human features that are detected in each frame of a video and/or a confidence value indicating the level of confidence that the one or more human features are located within each frame. The person or humanfeatures detector unit 312 can also generate a bounding box delineating an area in which the one or more human features have been detected, which can be used in identifying exclusion zones for overlaid content. For purposes of efficiency, the human features detector unit need only find regions in which human features appear in the video without actually recognizing the identities of persons that are present within those regions (e.g. recognizing the faces of specific persons that are present within those regions).The human featuresdetector unit 312 can output the detected human features bounding boxes as, for example, as an array (indexed by frame number) where each element of the array is a list of the rectangles defining human features bounding boxes detected within that frame. In some approaches, the detected bounded boxes can be added to the video stream as metadata information for each frame of the video. - The
system 300 also includes anobject detector unit 313 that receives as input thevideo stream 302 and provides as output a set of regions of the video that contain potential objects of interest. The potential objects of interest can be objects that are classified as belonging to an object category in a selected list of object categories (e.g. animals, plants, road or terrain features, containers, furnishings, etc.). The potential objects of interest can also be limited to identified objects that are in motion, e.g. objects that move a certain minimum distance within a selected interval of time (or selected interval of frames) or that move during a specified number of sequential frames in thevideo stream 302. The object detector unit can be a computer vision system such as a machine learning system, e.g., a Bayesian image classifier or convolutional neural network image classifier. Theobject detector unit 313 can be trained, for example, on labeled training samples that are labeled with objects that are classified as belonging to an object category in a selected list of object categories. For example, the object detector can be trained to recognize animals such as cats or dogs; or the object detector can be trained to recognize furnishings such as tables and chairs; or the object detector can be trained to recognize terrain or road features such as trees or road signs; or any combination of selected object categories such as these. Theobject detector unit 313 can also generate bounding boxes delineating (or otherwise specifying) areas of video frames in which the identified objects have been identified. Theobject detector unit 312 can output the detected object bounding boxes as, for example, as an array (indexed by frame number) where each element of the array is a list of the rectangles defining object bounding boxes detected within that frame. In some approaches, the detected bounded boxes can be added to the video stream as metadata information for each frame of the video. In other illustrative examples,system 300 may comprise at least one oftext detector 311,person detector 312 or objectdetector 313. - The
system 300 also includes an inclusion zone calculator unit or module 320 that receives input from one or more of the text detector unit 311 (with information about regions in which text appears in the video stream 302), the person detector unit 312 (with information about regions in which persons or portions thereof appear in the video stream 302), and the object detector unit 313 (with information about regions in which various potential objects of interest appear in the video stream 302). Each of these regions can define an exclusion zone; the inclusion zone calculator unit can aggregate those exclusion zones; and then the inclusion zone calculator can define an inclusion zone within which overlaid content is eligible for inclusion. - The aggregated exclusion zone can be defined as the union of a list of rectangles that each include a potential interesting feature such as text, a person, or another object of interest. It may be represented as an accumulation of bounding boxes that are generated by the
detector units text detector unit 311 outputs a first array (indexed by frame number) of lists of text bounding boxes in each frame, the human featuresdetector 312 outputs a second array (indexed by frame number) of lists of human features bounding boxes in each frame, and theobject detector unit 313 outputs a third array (indexed by frame number) of lists of bounding boxes for detected objects in each frame, these first, second, and third arrays can be merged to define a single array (again indexed by frame number), where each element is a single list that merges all bounding boxes for all features (text, human, or other object) detected within that frame. Next, the bounded boxes can be aggregated over a selected interval of consecutive frames. For example, a new array (again index by frame number) might be defined, where each element is a single list that merges all bounding boxes for all features detected within frames i, i+1, i+2, . . . , i+(N−1), where N is the number of frames in the selected interval of consecutive frames. In some approaches, the aggregated exclusion zone data can be added to the video stream as metadata information for each frame of the video. - The inclusion zone calculated by the inclusion zone calculator unit 320 can be then be defined as the complement of the accumulation of bounding boxes that are generated by the
detector units FIG. 2 . Note that these inclusion zone rectangles can be overlapping rectangles that collectively define the inclusion zone. In some approaches, this inclusion zone data can be added to the video stream as metadata information for each frame of the video. - The
system 300 also includes an overlaid content matcher unit ormodule 330 that receives input from the inclusion zone calculator unit or module 320, for example, in the form of a specification for an inclusion zone. The overlaid content matcher unit can select suitable content for overlay on the video within the inclusion zone. For example, the overlaid content matcher may have access to a catalog of candidate overlaid content, where each item in the catalog of candidate overlaid content has specifications that can include, for example, the width and height of each item, a minimum duration of time during which each item should be provided to the user, etc. The overlaid content matcher unit can select one or more items from the set of candidate overlaid content to fit within the inclusion zone provided by the inclusion zone calculator unit 320. For example, if the inclusion zone information is stored in an array (index by frame number), where each element in the array is a list of inclusion zone rectangles for that frame (taking into account all of the bounding boxes accumulated over that frame and the following N−1 consecutive frames), then, for each item in the catalog of candidate overlaid content, the overlaid content matcher can identify inclusion zone rectangles within the array that are large enough to fit the selected item; they can be ranked in order of size, and/or in order of persistence (e.g. if the same rectangle appears in multiple consecutive elements of the array, indicating that the inclusion zone is available for even more than the minimum number of consecutive frames N), and then an inclusion zone rectangle can be selected from that ranked list for inclusion of the selected overlaid content. In some approaches, the overlaid content may be scalable, e.g. over a range of possible x or y dimension or over a range of possible aspect ratios; in these approaches, an inclusion zone rectangle matching the overlaid content item may be selected to the, for example, the largest area inclusion zone rectangle that could fit the scalable overlaid content, or the inclusion zone rectangle of sufficient size that can persist for the longest duration of consecutive frames. - The
system 300 also includes anoverlay unit 340 which receives as input both theunderlying video stream 302 and the selected overlaid content 332 (and location(s) thereof) from the overlaidcontent matcher 330. Theoverlay 340 can then provide avideo stream 342 that includes both theunderlying video content 302 and the selected overlaidcontent 332. At the user device, a video visualizer 350 (e.g. a video player embedded within a web browser, or a video app on a mobile device) displays the video stream with the overlaid content to the user. In some approaches, theoverlay unit 340 may reside on the user device and/or be embedded within thevideo visualizer 350; in other words, both theunderlying video stream 302 and the selected overlaidcontent 332 may be delivered to the user (e.g. over the internet), and they may be combined on the user device to display an overlaid video to the user. - With reference now to
FIG. 4 , an illustrative example is depicted as a process flow diagram for a method of providing overlaid content on a video stream. The process includes 410—identifying, for each video frame among a sequence of frames of a video, a corresponding exclusion zone from which to exclude overlaid content. For example, inclusion zones can correspond to regions of the viewing area containing features of the video that are more likely to be of interest to the viewer, such as regions contain text (e.g.regions 111 inFIG. 1 ), regions containing persons or human features (e.g.regions 112 inFIG. 1 ), and regions containing particular objects of interest (e.g.regions 113 inFIG. 1 ). These regions can be detected, for example, using machine learning systems such as an OCR detector for text (e.g. text detector 311 inFIG. 3 ), a computer vision system for person or human features (e.g. person detector 312 inFIG. 3 ), and a computer vision system for other objects of interest (e.g. object detector 313 inFIG. 3 ). - The process also includes 420—aggregating the corresponding exclusion zones for the video frames in a specified duration or number of the sequence of frames. For example, as shown in
FIG. 1 ,rectangles FIG. 3 . - The process further includes 430—defining, within the specified duration or number of the sequence of frames of the video, an inclusion zone within which overlaid content is eligible for inclusion, the inclusion zone being defined as an area of the video frames in the specified duration or number that is outside of the aggregated corresponding exclusion zones. For example, the
inclusion zone 125 inFIG. 1 can be defined as a complement of the aggregated exclusion zones, and the inclusion zone may be described as a union of rectangles that collectively fill the inclusion zone. The inclusion zone may be calculated, for example, by the inclusion zone calculator unit 320 ofFIG. 3 . - The process further includes 440—providing overlaid content for inclusion in the inclusion zone of the specified duration or number of the sequence of frames of the video during display of the video at a client device. For example, overlaid content may be selected from a catalog of candidate overlaid content, based, e.g., on dimensions of the items in the catalog of candidate overlaid content. In the example of
FIG. 1 , two overlaid content features 126 are selected for inclusion within theinclusion zone 125. The overlaid content (and its positioning within the viewing area) may be selected by the overlaidcontent matcher 330 ofFIG. 3 , and theoverlay unit 340 can superimpose the overlaid content on top of theunderlying video stream 302 to define avideo stream 342 with overlaid content to be provided to the user for viewing on a client device. - In some approaches, the overlaid content that is selected from the catalog of overlaid content may be chosen in the following manner. For a frame i, an inclusion zone may be defined as a union of a list of inclusion area rectangles, which are the rectangles that do not intersect any of the exclusion zones (bounding boxes for detected objects) in frames i, i+1, . . i+(N−1), where N is a selected minimum number of consecutive frames. Then for a frame i and for a given candidate item from the catalog of overlaid content, the inclusion area rectangles are selected from the list of inclusion area rectangles that could fit the candidate item. These are inclusion area rectangles that could fit the candidate item for the selected minimum number of consecutive frames N. The same process can be performed for frame i+1; then, by taking an intersection of the results for frame i and for frame i+1, a list of inclusion area rectangles can be obtained that could fit the candidate item for N+1 consecutive frames. Again performing an intersection with the results for frame i+2, a set of inclusion areas that could fit the candidate item for N+2 consecutive frames can be obtained. The process can be iterated for any selected span of frames (including the entire duration of the video) to obtain rectangles suitable for inclusion of the candidate item for frame durations N, N+1, . . . , N+(k−1) where N+k is the longest possible duration. Thus, for example, a location for the overlaid content may be selected from the list of inclusion area rectangles that can persist for the longest duration without occluding detected features, i.e. for N+k frames.
- In some approaches, more than one content feature may be included at the same time. For example, a first item of overlaid content may be selected, and then a second item of overlaid content may be selected by defining an additional exclusion zone that encloses the first item of overlaid content. In other words, the second item of overlaid content may be placed by regarding the video overlaid with the first item of overlaid content as a new underlying video suitable for overlay of additional content. The exclusion zone for the first item of overlaid content may be made significantly larger than the overlaid content itself, to increase the spatial separation between different items of overlaid content within the viewing area.
- In some approaches, the selecting of overlaid content may include selecting that allows a specified level of encroachment on an exclusion zone. For example, some area-based encroachment could be tolerated by weighing inclusion zone rectangles by the extent to which the overlaid content spatially extends outside of each inclusion zone rectangle. Alternatively or additionally, some time-based encroachment could be tolerated by ignoring ephemeral exclusion zones that only exist for a relatively short time. For example, if the exclusion zone is only defined for a single frame out of 60 frames, it could be lower weighted and therefore more likely to be occluded than an area in which the exclusion zone exists for the entire 60 frames. Alternatively or additional, some content-based encroachment could be tolerated by ranking the relative importance of different types of exclusion zones corresponding to different types of detected features. For example, detected text features could be ranked as more important than detected non-text features, and/or detected human features could be ranked as more important than detected non-human features, and/or more rapidly moving features could be ranked as more important than more slowly moving features.
-
FIG. 5 is a block diagram of anexample computer system 500 that can be used to perform operations described above. Thesystem 500 includes aprocessor 510, amemory 520, astorage device 530, and an input/output device 540. Each of thecomponents system bus 550. Theprocessor 510 is capable of processing instructions for execution within thesystem 500. In some implementations, theprocessor 510 is a single-threaded processor. In another implementation, theprocessor 510 is a multi-threaded processor. Theprocessor 510 is capable of processing instructions stored in thememory 520 or on thestorage device 530. - The
memory 520 stores information within thesystem 500. In one implementation, thememory 520 is a computer-readable medium. In some implementations, thememory 520 is a volatile memory unit. In another implementation, thememory 520 is a non-volatile memory unit. - The
storage device 530 is capable of providing mass storage for thesystem 500. In some implementations, thestorage device 530 is a computer-readable medium. In various different implementations, thestorage device 530 can include, for example, a hard disk device, an optical disk device, a storage device that is shared over a network by multiple computing devices (e.g., a cloud storage device), or some other large capacity storage device. - The input/
output device 540 provides input/output operations for thesystem 500. In some implementations, the input/output device 540 can include one or more of a network interface devices, e.g., an Ethernet card, a serial communication device, e.g., and RS-232 port, and/or a wireless interface device, e.g., and 802.11 card. In another implementation, the input/output device can include driver devices configured to receive input data and send output data to external devices 460, e.g., keyboard, printer and display devices. Other implementations, however, can also be used, such as mobile computing devices, mobile communication devices, set-top box television client devices, etc. - Although an example processing system has been described in
FIG. 5 , implementations of the subject matter and the functional operations described in this specification can be implemented in other types of digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. - Embodiments of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on computer storage media (or medium) for execution by, or to control the operation of, data processing apparatus. The computer storage media (or medium) may be transitory or non-transitory. Alternatively, or in addition, the program instructions can be encoded on an artificially-generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus. A computer storage medium can be, or be included in, a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them. Moreover, while a computer storage medium is not a propagated signal, a computer storage medium can be a source or destination of computer program instructions encoded in an artificially-generated propagated signal. The computer storage medium can also be, or be included in, one or more separate physical components or media (e.g., multiple CDs, disks, or other storage devices).
- The operations described in this specification can be implemented as operations performed by a data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
- The term “data processing apparatus” encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or multiple ones, or combinations, of the foregoing. The apparatus can include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). The apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them. The apparatus and execution environment can realize various different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures.
- A computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. A computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for performing actions in accordance with instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a universal serial bus (USB) flash drive), to name just a few. Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's client device in response to requests received from the web browser.
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), an inter-network (e.g., the Internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
- The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, a server transmits data (e.g., an HTML page) to a client device (e.g., for purposes of displaying data to and receiving user input from a user interacting with the client device). Data generated at the client device (e.g., a result of the user interaction) can be received from the client device at the server.
- While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any inventions or of what may be claimed, but rather as descriptions of features specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
- Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
- Thus, particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. In some cases, the actions recited in the claims can be performed in a different order and still achieve desirable results. In addition, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In certain implementations, multitasking and parallel processing may be advantageous.
Claims (21)
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
PCT/US2020/044068 WO2022025883A1 (en) | 2020-07-29 | 2020-07-29 | Non-occluding video overlays |
Publications (1)
Publication Number | Publication Date |
---|---|
US20220417586A1 true US20220417586A1 (en) | 2022-12-29 |
Family
ID=72139671
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US17/776,652 Pending US20220417586A1 (en) | 2020-07-29 | 2020-07-29 | Non-occluding video overlays |
Country Status (6)
Country | Link |
---|---|
US (1) | US20220417586A1 (en) |
EP (1) | EP4042707A1 (en) |
JP (1) | JP2023511816A (en) |
KR (1) | KR20220097945A (en) |
CN (1) | CN114731461A (en) |
WO (1) | WO2022025883A1 (en) |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2023081051A1 (en) * | 2021-11-04 | 2023-05-11 | Tencent America LLC | Method and apparatus for signaling occlude-free regions in 360 video conferencing |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20140359656A1 (en) * | 2013-05-31 | 2014-12-04 | Adobe Systems Incorporated | Placing unobtrusive overlays in video content |
US20150326912A1 (en) * | 2014-05-12 | 2015-11-12 | Echostar Technologies L.L.C. | Selective placement of progress bar |
US20190253747A1 (en) * | 2016-07-22 | 2019-08-15 | Vid Scale, Inc. | Systems and methods for integrating and delivering objects of interest in video |
US20200213680A1 (en) * | 2019-03-10 | 2020-07-02 | Ben Avi Ingel | Generating videos with a character indicating a region of an image |
US10757347B1 (en) * | 2019-05-08 | 2020-08-25 | Facebook, Inc. | Modifying display of an overlay on video data based on locations of regions of interest within the video data |
Family Cites Families (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8988609B2 (en) * | 2007-03-22 | 2015-03-24 | Sony Computer Entertainment America Llc | Scheme for determining the locations and timing of advertisements and other insertions in media |
US8451380B2 (en) * | 2007-03-22 | 2013-05-28 | Sony Computer Entertainment America Llc | Scheme for determining the locations and timing of advertisements and other insertions in media |
US8817188B2 (en) * | 2007-07-24 | 2014-08-26 | Cyberlink Corp | Systems and methods for automatic adjustment of text |
CN102047654A (en) * | 2008-09-02 | 2011-05-04 | 松下电器产业株式会社 | Content display processing device and content display processing method |
JP5465620B2 (en) * | 2010-06-25 | 2014-04-09 | Ｋｄｄｉ株式会社 | Video output apparatus, program and method for determining additional information area to be superimposed on video content |
CN110620946B (en) * | 2018-06-20 | 2022-03-18 | 阿里巴巴（中国）有限公司 | Subtitle display method and device |
CN110620947A (en) * | 2018-06-20 | 2019-12-27 | 北京优酷科技有限公司 | Subtitle display area determining method and device |
CN110996020B (en) * | 2019-12-13 | 2022-07-19 | 浙江宇视科技有限公司 | OSD (on-screen display) superposition method and device and electronic equipment |
-
2020
- 2020-07-29 JP JP2022533180A patent/JP2023511816A/en active Pending
- 2020-07-29 EP EP20757722.2A patent/EP4042707A1/en active Pending
- 2020-07-29 WO PCT/US2020/044068 patent/WO2022025883A1/en unknown
- 2020-07-29 KR KR1020227018701A patent/KR20220097945A/en not_active Application Discontinuation
- 2020-07-29 US US17/776,652 patent/US20220417586A1/en active Pending
- 2020-07-29 CN CN202080083613.XA patent/CN114731461A/en active Pending
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20140359656A1 (en) * | 2013-05-31 | 2014-12-04 | Adobe Systems Incorporated | Placing unobtrusive overlays in video content |
US20150326912A1 (en) * | 2014-05-12 | 2015-11-12 | Echostar Technologies L.L.C. | Selective placement of progress bar |
US20190253747A1 (en) * | 2016-07-22 | 2019-08-15 | Vid Scale, Inc. | Systems and methods for integrating and delivering objects of interest in video |
US20200213680A1 (en) * | 2019-03-10 | 2020-07-02 | Ben Avi Ingel | Generating videos with a character indicating a region of an image |
US10757347B1 (en) * | 2019-05-08 | 2020-08-25 | Facebook, Inc. | Modifying display of an overlay on video data based on locations of regions of interest within the video data |
Cited By (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2023081051A1 (en) * | 2021-11-04 | 2023-05-11 | Tencent America LLC | Method and apparatus for signaling occlude-free regions in 360 video conferencing |
Also Published As
Publication number | Publication date |
---|---|
JP2023511816A (en) | 2023-03-23 |
CN114731461A (en) | 2022-07-08 |
KR20220097945A (en) | 2022-07-08 |
EP4042707A1 (en) | 2022-08-17 |
WO2022025883A1 (en) | 2022-02-03 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN109791600B (en) | Method for converting horizontal screen video into vertical screen mobile layout | |
CN104219559B (en) | Unobvious superposition is launched in video content | |
Schreck et al. | Visual analysis of social media data | |
KR102348016B1 (en) | Modifying advertisement sizing for presentation in a digital magazine | |
US11978238B2 (en) | Method for converting landscape video to portrait mobile layout using a selection interface | |
US8793604B2 (en) | Spatially driven content presentation in a cellular environment | |
US10366405B2 (en) | Content viewability based on user interaction in a flip-based digital magazine environment | |
CN109690471B (en) | Media rendering using orientation metadata | |
KR102626274B1 (en) | Image Replacement Restore | |
CN111177467A (en) | Object recommendation method and device, computer-readable storage medium and electronic equipment | |
CN113766330A (en) | Method and device for generating recommendation information based on video | |
US10580046B2 (en) | Programmatic generation and optimization of animation for a computerized graphical advertisement display | |
US20240007703A1 (en) | Non-occluding video overlays | |
Tang et al. | Videomoderator: A risk-aware framework for multimodal video moderation in e-commerce | |
CN114117128A (en) | Method, system and equipment for video annotation | |
US20220417586A1 (en) | Non-occluding video overlays | |
CN112101344A (en) | Video text tracking method and device | |
CN109213894A (en) | A kind of displaying, providing method, client and the server of results for video item | |
CN112738629B (en) | Video display method and device, electronic equipment and storage medium | |
US9165339B2 (en) | Blending map data with additional imagery | |
CA3000845C (en) | Dynamic generation and layout of media assets in a campaign management system |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:ERBICEANU-TENER, ELENA;GALASHOV, ALEXANDRE;CHIU, ANDY;AND OTHERS;SIGNING DATES FROM 20200723 TO 20200729;REEL/FRAME:060276/0391 |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: DOCKETED NEW CASE - READY FOR EXAMINATION |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: NON FINAL ACTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE TO NON-FINAL OFFICE ACTION ENTERED AND FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: FINAL REJECTION MAILED |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: RESPONSE AFTER FINAL ACTION FORWARDED TO EXAMINER |
|
STPP | Information on status: patent application and granting procedure in general |
Free format text: ADVISORY ACTION MAILED |