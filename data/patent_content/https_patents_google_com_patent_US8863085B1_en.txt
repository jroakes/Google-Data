US8863085B1 - Monitoring web applications - Google Patents
Monitoring web applications Download PDFInfo
- Publication number
- US8863085B1 US8863085B1 US13/362,885 US201213362885A US8863085B1 US 8863085 B1 US8863085 B1 US 8863085B1 US 201213362885 A US201213362885 A US 201213362885A US 8863085 B1 US8863085 B1 US 8863085B1
- Authority
- US
- United States
- Prior art keywords
- test
- browser
- client
- executing
- server
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Expired - Fee Related, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F11/00—Error detection; Error correction; Monitoring
- G06F11/36—Preventing errors by testing or debugging software
- G06F11/3668—Software testing
Definitions
- This disclosure generally relates to monitoring applications over a network.
- web pages include interactive features allowing users to interact and collaborate with each other.
- These web pages may include client-side code to be executed in a browser.
- client-side code may be executed in different browsers, operating systems, and computing devices (e.g., tablets, smartphones, and desktops).
- browsers exist with many different configurations, language settings, types, and versions. These differences may result in problems at the client that are not detectable by a conventional monitoring system. For example, the code may break on one system and not on other systems.
- Web applications are increasingly susceptible to outages caused by faulty code executed in a browser.
- the code may fail in ways ranging from minor malfunctions to complete outages of the web application.
- These outages are not detectable using traditional production monitoring.
- white box monitoring is a traditional approach to monitoring web applications.
- White box monitoring monitors key metrics at the server and looks at internal variables within the server to ensure that metric expectations are satisfied.
- Many outages occurring at the browser do not affect the monitored metrics. Consequently, these outages are difficult to detect by an administrator of the web application's serving system when traditional production monitoring is used. This leads to unnecessary service unavailability resulting from a long time-to-detection because administrators rely on users to manually report the outage.
- An exemplary method for monitoring a web application over a network includes executing a first test in a first browser residing on a server. Executing the first test includes sending a first command to the first browser to perform a first operation. Executing the first test may also include, executing, in the first browser, first client-side code included in a first response responsive to the first operation. Executing the first test may further include determining a status of the first test based at least on the first response and a result of the executed first client-side code.
- the exemplary method also includes executing a second test in a second browser residing on the server.
- Executing the second test includes sending a second command to the second browser to perform a second operation.
- Executing the second test may also include, executing, in the second browser, second client-side code included in a second response responsive to the second operation.
- Executing the second test may further include determining a status of the second test based at least on the second response and a result of the executed second client-side code.
- the first and second browsers are heterogeneous browsers.
- FIG. 1 shows an example communication system which relates to embodiments.
- FIG. 2 shows an example system for monitoring a web application, according to an embodiment.
- FIG. 3 shows an example system for obtaining test results from different servers, according to an embodiment.
- FIG. 4 shows an example screenshot of an operation being performed, according to an embodiment.
- FIG. 5 shows an example screenshot that lists previously executed tests, according to an embodiment.
- FIGS. 6-8 show example screenshots resulting from previously executed tests, according to an embodiment.
- FIGS. 9-11 show flowcharts of an exemplary method for monitoring a web application, according to an embodiment
- An application monitoring system may execute tests in browsers residing in heterogeneous browser environments.
- references to “one embodiment”, “an embodiment”, “an example embodiment”, etc. indicate that the embodiment described may include a particular feature, structure, or characteristic, but every embodiment may not necessarily include the particular feature, structure, or characteristic. Moreover, such phrases are not necessarily referring to the same embodiment. Further, when a particular feature, structure, or characteristic is described in connection with an embodiment, it is submitted that it is within the knowledge of one skilled in the art to affect such feature, structure, or characteristic in connection with other embodiments, whether or not explicitly described.
- FIG. 1 shows an example communication system 100 which relates to embodiments.
- System 100 includes an application monitoring system 110 that is communicatively coupled to a server 120 via a network 130 .
- system 100 is described herein with reference to one application monitoring system and one server, one of skill in the art will recognize that system 100 may include more than one application monitoring system and more than one server without departing from the spirit and scope of the present disclosure.
- system 100 is described as having two heterogeneous browsers and two heterogeneous browser environments.
- system 100 may include more than two heterogeneous browsers and more than two heterogeneous browser environments without departing from the spirit and scope of the present disclosure.
- Server 120 may include a web application that users may access over a network (e.g., network 130 ).
- a user may use a client browser to send a request to server 120 , such as a request for a web page, over network 130 .
- Server 120 may receive the request and return a response, such as content for the requested web page.
- the web page may be an HTML document that includes client-side code to be executed in the browser.
- the client-side code can be, for example, Java Script code.
- Application monitoring system 110 includes a tester 140 , environment 145 , and environment 150 .
- Application monitoring system 110 can monitor a web application by executing tests to ensure that the client-side code is compatible with and does not break the browser.
- application monitoring system 110 is a server that runs programmatically controlled browsers that execute client-side code included in a response from the server.
- the controlled browsers may exchange messages with one or more servers (e.g., server 120 ) that user-controlled browsers would exchange.
- Tester 140 controls the flow of executing tests in a browser.
- a first browser 155 resides in environment 145 and a second browser 160 resides in environment 150 .
- First browser 155 and second browser 160 are heterogeneous browsers, and environments 145 and 150 are heterogeneous browser environments.
- Tester 140 communicates and executes tests in browsers residing in environments 145 and 150 .
- tester 140 is run in a virtual machine that supports testing heterogeneous browsers residing in environments 145 and 150 .
- Tester 140 may test different browser and operating system combinations.
- application monitoring system 110 runs a first set of browsers natively on a first operating system and runs a second set of browsers in a virtual machine running a second operation system, different from the first operating system.
- the first browser runs on a first operating system
- the second browser runs on a second operating system
- the first operating system is different from the second operating system.
- Application monitoring system 110 may run tests against a production environment.
- application monitoring system 110 is a server, in a data center, that includes a set of virtual machines running interactive tests against production Web services.
- tester 140 , first browser 155 , and second browser 160 may run in a production environment.
- Running browsers in production environments is typically more complex than running browsers on end-user desktop workstations, because browsers are designed to run on end-user desktop workstations and not on servers.
- servers typically do not have attached displays and browsers typically expect to have attached displays.
- many servers run operating systems that are incompatible with the operating systems that end-users commonly run.
- Application monitoring system 110 accounts for these differences. By simulating the heterogeneity of real browsers, application monitoring system 110 has a higher likelihood of catching code breakage compared to other web application monitoring systems.
- tester 140 , first browser 155 , and second browser 160 run on a single machine (e.g., application monitoring system 110 ). In this way, network traffic and complexity may be reduced and difficulties in change management may be mitigated.
- a computer system typically has active processes and threads.
- a process is the execution of a program and may have a self-contained execution environment. Each process has one or more threads.
- a thread is code that may be executed within a process. Multiple threads may exist within the same process and share resources such as memory.
- a first process may run on tester 140
- a second process may run in environment 145 and simulate a machine
- a third process may run in environment 150 and simulate a machine.
- tester 140 may be a virtual machine that runs as a normal application inside an operating system and supports a single process.
- tester 140 is the main binary that is run to monitor a web application.
- Tester 140 may be written in any programming language.
- tester 140 may be a Python application that coordinates and controls test runs to monitor a web application.
- Network 130 can be any network or combination of networks that can carry data communication, and may be referred to herein as a computer network.
- Such network 130 can include, but is not limited to, a wired (e.g., Ethernet) or a wireless (e.g., Wi-Fi and 3G) network, a local area network, medium area network, and/or wide area network such as the Internet.
- Network 130 can support protocols and technology including, but not limited to, World Wide Web protocols and/or services.
- Intermediate web servers, gateways, or other servers may be provided between components of communication system 100 depending upon a particular application or environment.
- FIG. 2 shows an example system 200 for monitoring a web application, according to an embodiment.
- System 200 includes an application monitoring system 110 that communicates over network 130 .
- Application monitoring system 110 includes a tester 140 .
- Tester 140 includes a test executor 210 that controls the flow of executing individual tests in a controlled browser. Executing a test in a browser residing on a server may include sending a command to the browser to perform an operation, executing, in the browser, client-side code included in a response responsive to the operation, and determining a status of the test based at least on the response and a result of the executed client-side code. There may be several rounds of communication between the browser and the server.
- Test executor 210 instructs a browser provider 212 to start up or communicate with a particular browser residing in environments 145 or 150 .
- Test executor 210 can send a command to first browser 155 or second browser 160 to perform an operation using browser provider 212 .
- the command can simulate a user action or operation.
- a user may interact with a web page by performing operations such as starting a browser, pointing the browser to a Uniform Resource Locator (URL), displaying the web page located at the URL, scrolling around the web page, selecting an item on the web page, sending a keyboard command to perform actions or to fill out a form (e.g., a search box), or other user operations.
- test executor 210 can send a command to first browser 155 or second browser 160 to perform any of these operations or other operations.
- Test executor 210 may determine a status of the test (e.g., success, failure, partial failure, and service degradation). For example, test executor 210 may communicate with a browser to determine whether a test finished. Based on a result of the test, test executor 210 may determine whether the test passed or failed. Additionally, test executor 210 may gather debugging data about the test. In an example, test executor 210 communicates with the browser to obtain logs about the testing process. A log may be very detailed and show in which step the test failed. For example, a log may be a step-by-step collection of the events that occurred during the test. Tester 140 may export the logs of each test run.
- a status of the test e.g., success, failure, partial failure, and service degradation. For example, test executor 210 may communicate with a browser to determine whether a test finished. Based on a result of the test, test executor 210 may determine whether the test passed or failed. Additionally, test executor 210 may gather debugging data about the test
- Test executor 210 may be implemented in a number of ways. According to an embodiment, test executor 210 executes tests by finding the Java binary that belongs to the test and executing it. The test may be, for example, on the local disk. Test logs can be extracted and a status of the test can be determined by the test's exit code.
- test executor 210 navigates a browser to a URL of a test to be executed.
- the test file can be loaded into the browser and executed automatically. This test may run automatically when the code executing the test is, for example, JavaScript code. Once the test executes, test executor 210 may retrieve the test result in a number of ways.
- a test result is determined by inspecting the browser's Document Object Model (DOM) for a particular DOM element.
- DOM Document Object Model
- the presence of this DOM element may indicate that the test completed and the contents may indicate a status of the test.
- the log can be extracted from the browser's DOM.
- a test result is determined by instructing the testing framework to send out logs of its progress via special requests to a monitoring front end 266 .
- Monitoring front end 266 may be configured to forward these requests to a proxy 268 .
- the requests pass through monitoring front end 266 to test executor 210 .
- Test executor 210 may process the requests and determine the test outcome.
- the logs may be provided to test executor 210 without being extracted from the browser's DOM.
- Browser provider 212 starts up and communicates with browsers residing in environments 145 and 150 .
- browser provider 212 provides a browser that can execute client-side code to, for example, monitor a web application.
- Browser provider 212 may be an abstract interface in which test executor 210 can request a type of browser.
- test executor 210 may send a request to browser provider 212 to start up a particular browser running on a particular operating system. After receiving the request, browser provider 212 launches the particular browser on the particular operating system. Once the particular browser is launched, test executor 210 can execute a test in the browser by using browser provider 212 to send a command to the browser to perform an operation. After a test completes, browser provider 212 may terminate processes started and remove temporary files created as a result of the test execution.
- Tester 140 also includes configuration 214 , scheduler 216 , and result repository 218 .
- Configuration 214 manages the configured list of tests.
- configuration 214 may store a database of these tests, instructions on how to run them, and a timeline for when these tests are executed.
- configuration 214 manages the list of tests by reading a text file from local disk.
- Scheduler 216 determines when and in which order to run tests. Scheduler 216 may select the appropriate test to run and instruct test executor 210 to run that test. Scheduler 216 may use a variety of scheduling algorithms. For example, scheduler 216 may run all configured tests in a round-robin fashion. Other scheduling algorithms may also be used.
- scheduler 216 may implement a special scheduling algorithm. In an embodiment, scheduler 216 automatically re-executes tests that fail. Scheduler 216 may execute failing tests more often to gain more confidence about whether the test is failing consistently. Determining whether a test has failed may depend on factors such as how quickly scheduler 216 can detect failure (e.g., 1-10 seconds), how many times a test must have executed before it is determined to have failed (e.g., 8, 10, or 12 times), and a minimum ratio of test executions that must have failed before the test is determined to have failed.
- Scheduler 216 may enforce a policy guaranteeing that only one test executes at any given time for each process running on tester 140 .
- the scheduling of tests may be divided into a number of partitions called “shards.”
- One or more tests may be executed across a set of shards based on a function.
- scheduler 216 supports modulo-sharding. This may ensure that tests are distributed across many tasks without any interaction between the individual tasks.
- Result repository 218 stores a list of previous test attempts and their results. The list may be ordered. Result repository 218 may also store statistics on test executions and metadata associated with previous test executions. Metadata may include, for example, details of the test (e.g., name, operating system, and browser version), the test URL, a status of the test (e.g., passed or failed), a number of attempted test executions, when the test executed, timing information (e.g., a total time spent executing each test), a number of test attempts that failed, a number of test attempts that succeeded, a timestamp of the last test success, and debugging output of the test.
- details of the test e.g., name, operating system, and browser version
- the test URL e.g., a status of the test (e.g., passed or failed)
- a number of attempted test executions e.g., when the test executed
- timing information e.g., a total time spent executing each test
- Some data to be stored in result repository 218 may be kept in memory while other data to be stored in result repository 218 may be written to disk as soon as the test completes.
- the test URL, browser and operating system used, and a status of the test e.g., pass or fail
- screenshots and HTTP logs are written to disk as soon as the test completes.
- a monitoring system may monitor data in result repository 218 and generate alerts based on the data.
- tester 140 exports data in result repository 218 to, for example, an external monitoring system that keeps track of test executions on multiple servers. For example, statistics on test runs may be exported to the external monitoring system.
- the exported data may be high-level data and may contain, for example, which tests were executed, when the tests last executed, and how many times the tests were executed.
- a first counter associated with the test is incremented.
- the first counter keeps track of a number of times the test has previously succeeded.
- a second counter associated with the test is incremented.
- the second counter keeps track of a number of times the test has previously failed.
- an alert is generated. Based on the data, a monitoring system can determine whether a test is failing too often and whether to generate an alert. An alert may generate a page to, for example, a systems administrator or to a team of people responsible for monitoring the web application.
- a user may access data in result repository 218 via a user interface 220 .
- a browser may display data in result repository 218 to the user.
- a browser may display in a web page screenshot the debugging output of a test at the end of the test or the logs of the HTTP network traffic caused by the browser during test execution.
- data in result repository 218 may be used to generate historical graphs. For example, a report on the number of tests executed over a specified time period (e.g., the past couple of days or weeks) and how often tests are failing can be generated.
- Tester 140 communicates and executes tests in browsers residing on application monitoring system 110 .
- Tester 140 includes webdriver clients that communicate with heterogeneous browsers residing in heterogeneous browser environments 145 and 150 .
- Tester 140 includes a first webdriver client 230 and a second webdriver client 232 .
- Browser provider 212 communicates with first browser 155 via first webdriver client 230 and communicates with second browser 160 via second webdriver client 232 .
- Environment 145 includes a command executor 250 , jobs 256 and 258 , and first browser 155 .
- application monitoring system 110 and first browser 155 run on the same operating system.
- first browser 155 runs natively on a particular operating system.
- first browser 155 may run natively on a UNIX operating system.
- the chroot environment in UNIX simulates a virtual UNIX environment that may be configured differently from the host environment. Accordingly, heterogeneous browsers depending on different system configurations may be run on the same machine.
- the chroot environment may isolate directory structures from each other and be used to create a separate tree (e.g., a virtual tree) that only command executor 250 can access.
- Command executor 250 may be a binary that provides a remote procedure call server interface allowing tester 140 to execute commands inside environment 145 .
- a separate process may be run to control first browser 155 and execute commands.
- Job 256 enables access to environment 145 for debugging purposes via a protocol (e.g., Virtual Network Computing protocol), and job 258 provides a server interface for first browser 155 . Jobs 256 and 258 may simulate to first browser 155 that a display is present on application monitoring system 110 . As described, application monitoring system 110 may be a server running in a big data center and may not have an actual display or keyboard attached. Job 256 may display a browser window in first browser 155 , and job 258 may store the content displayed in the browser window to be used later to obtain a screenshot.
- a protocol e.g., Virtual Network Computing protocol
- First browser 155 may run in a virtual operating system. By running first browser 155 in a virtual operating system, it may be unnecessary to install first browser 155 on the production machine. This may be advantageous because an application monitoring team may not have permission and privileges to install first browser 155 on the production machine.
- first browser 155 is a particular browser and first webdriver client 230 is a driver for the browser.
- a special subclass of a webdriver client library may be used that injects a custom implementation of a specific webdriver protocol for the browser.
- Browser provider 212 may include logic to use command executor 250 to launch the browser and any necessary auxiliary processes inside environment 145 .
- application monitoring system 110 and environment 145 run on Unix.
- Application monitoring system 110 may run on an operating system that is incompatible with operating systems that end users use. It may be desirable to monitor web applications using browsers that end users use.
- Application monitoring system. 110 may leverage virtual machine technology to run a browser that is incompatible with the operating system of application monitoring system 110 .
- a process may run and simulate a machine in environment 150 .
- environment 150 is a virtual machine, and a browser runs in an operating system residing in the virtual machine.
- Environment 150 includes a command executor 260 and second browser 160 .
- Application monitoring system 110 and second browser 160 may run on different operating systems.
- tester 140 resides in a first virtual machine and command executor 260 resides in a second virtual machine, different from the first virtual machine.
- Command executor 260 starts and controls second browser 160 and allows networking protocols to be used to communicate with second browser 160 .
- test executor 210 may send a message (e.g., command) to command executor 260 to instruct it to perform a first operation.
- Test executor 210 may then send a message to command executor 260 to perform a second operation.
- command executor 260 may automate browsers and test web applications.
- Second webdriver client 232 may be a webdriver client library that communicates via a webdriver-specific HTTP protocol with command executor 260 . Second webdriver client 232 may be used to start and control browsers running in environment 150 . In an embodiment, second webdriver client 232 is the standard Python binding for the webdriver. Tester 140 may subclass some of the classes from that library to extend the functionality and expose more powerful functions. In an embodiment, browser provider 212 includes logic to start up a virtual machine and launch second browser 160 in the virtual machine.
- tester 140 runs in a virtual machine and test executor 210 executes tests that are not stored in the virtual machine.
- Application monitoring system 110 may include webdriver tests 262 .
- Webdriver tests 262 communicate with the browser through webdriver forwarder 272 .
- Webdriver tests 262 are external programs that may be written, for example, in the Java programming language.
- Webdriver tests 262 may be compiled and copied into application monitoring system 110 .
- Test executor 210 may instruct browser provider 212 to communicate with a particular browser and then run webdriver tests 262 on the browser.
- Running webdriver tests 262 may entail running code that can control the browser outside of the browser.
- Webdriver tests 262 may allow for out-of-the-box testing and result in a more powerful and realistic testing environment.
- Local test environment provider 270 and webdriver forwarder 272 provide webdriver tests 262 with the environment that is needed to run the tests.
- Local test environment provider 270 provides tests with a pointer to the webdriver interface.
- a particular browser may not provide a native interface that implements the protocol to communicate with command executor 260 .
- webdriver forwarder 272 may implement a server-side interface for command executor 260 and forward requests to the server running inside the virtual machine or to the browser driver instance, which will send the commands to the browser.
- Modern web user interfaces use a constant stream of messages between the client and server. For example, when a user navigates to a homepage, the browser sends a request for the homepage to a server. The user may perform an action on the homepage that sends another request to the server. In an example, if the homepage is a web search page, the user can submit a query on the homepage and receive a search results page from the server.
- State may be transported between the first web page (homepage) and the second web page (search results page).
- the entire second web page is not reloaded in the browser.
- Java Script code may be used to load additional pieces of information from the server.
- Java Script code may be downloaded by the homepage and may execute at the client as part of the search results page.
- Java Script dynamically changes and modifies the existing page rather than completely reloading it.
- the code breakage typically occurs later than at the initial two messages (e.g., first HTTP request and first HTTP response).
- a state of a subsequent message may depend on, for example, the first HTTP request and response.
- Application monitoring system 110 may have session-awareness between different HTTP transactions.
- a complete session representative of a user's behavior is tested.
- First browser 155 or second browser 160 may communicate with a web application and handle the session awareness and state transfers that a user's actual browser would go through.
- an executed test represents a set of sessions, and each session may be a sequence of actions that individual users take with their browsers. This may closely mirror user behavior and exercise the core functionality provided by the web application's user interface.
- An example session may include the following operations: navigating to a web search homepage, suggesting search queries based on what has already been typed into the search query box, performing a web search of the search query (e.g., by hitting a submit button), and selecting a search result.
- Another example session may include the following operations: navigating to a homepage, typing a query, and selecting one of the suggestions listed.
- Another example session may include the following operations: navigating to a web page and selecting an option on the web page.
- test executor 210 executes a test that includes at least two commands that are associated with the same session. A state of the second command may be dependent on a state of the first command.
- Test executor 210 sends a first command to, for example, first browser 155 to perform a first operation (e.g., request a web page from a server).
- First browser 155 receives a first response that includes client-side code to be executed in first browser 155 from the server in response to the first operation.
- client-side code is JavaScript code that can be included in the first response or along with the first response.
- First browser 155 executes the client-side code included in the first response.
- First browser 155 may generate additional requests to the server as a result of executing the client-side code.
- the server processes the additional requests and returns responses responsive to the requests to first browser 155 .
- the client-side code may cause search queries to be suggested to the user based on what has already been typed into the search query box. For example, if the user enters “apartm” into the search query box, search queries “apartments” and “apartment ratings” may be suggested to the user.
- Test executor 210 may determine a status of the test based at least on the first response and a result of executing the client-side code included in the first response. For example, a screenshot of the suggested search queries displayed on the web page may be taken to ensure that search queries “apartments” and “apartment ratings” were actually suggested as a result of “apartm” being entered into the search query box.
- Test executor 210 then sends a second command to first browser 155 to perform a second operation.
- the second operation may be dependent on the first operation.
- the second operation may be selecting one of the suggested search queries displayed on the web page.
- First browser 155 receives a second response that includes client-side code to be executed in first browser 155 from the server in response to the second operation.
- First browser 155 executes the client-side code included in the second response.
- Test executor 210 determines a status of the test based at least on the first and second responses and results of executing the client-side code included in the first and second responses.
- a session log may be generated that includes metadata of the test.
- the metadata may include a status of the test, a timestamp of the test, a number of attempted test runs of the test, a total time spent executing the test, a number of times the test has previously failed, or a number of times the test has previously succeeded.
- Application monitoring system 110 may also monitor production traffic.
- application monitoring system includes monitoring front end 266 and proxy 268 .
- a browser may send requests and traffic to monitoring front end 266 .
- Monitoring front end 266 may perform the following tasks. Monitoring front end 266 may send log traffic and events that occurred as a result of a request to tester 140 . Monitoring front end 266 may forward the requests to a public front end coupled to network 130 , where the requests will be processed as normal requests. Based on the requests, monitoring front end 266 may generate a log of the network traffic. A user may access the log of network traffic caused by the browser.
- Requests from a browser may be redirected to proxy 268 .
- monitoring front end 266 sends logs of proxied HTTP transactions to tester 140 .
- all HTTP requests from the browsers are redirected to an HTTP proxy.
- Proxy 268 may forward some requests (e.g., requests for external domains) to tester 140 .
- Proxy 268 may also maintain a log of the network transactions. The log is available for debugging purposes and may contain, for example, the HTTP headers included in the HTTP transactions.
- Tester 140 may rely on inherently unreliable infrastructure (e.g., operating systems, PCs, browsers). To increase reliability, tests may be replicated on different physical servers and across many data centers. In an embodiment, tester 140 has sharding support to scale across many servers. For example, several processes executing tests may run on different physical servers and across many data centers.
- inherently unreliable infrastructure e.g., operating systems, PCs, browsers.
- FIG. 3 shows an example system 300 for obtaining test results from different servers, according to an embodiment.
- System 300 includes servers 310 , 320 , and 330 .
- Server 310 includes tester 140 and result repository 218
- server 320 includes tester 340 and result repository 345
- server 330 includes tester 350 and result repository 355 .
- servers 310 , 320 , and 330 are different physical machines and may be in different data centers.
- Application monitoring system 110 may be replicated across servers 310 , 320 , and 330 .
- tester 140 is replicated across server 320 and/or server 330 .
- a test that tester 140 executes may also be executed by tester 340 . In this way, breakages occurring on individual servers may be detected. If a server breaks and all tests executing on that server fail, the same test can be executed on another server such that it can be determined whether the failure of the test is linked to a breakage on the server itself. For example, when a test fails on server 310 , the test may be automatically re-executed in server 320 . In this way, if there is a problem with server 310 , the test can be re-executed in a different server rather than paging an on-call person right away.
- each server runs different tests. In this way, test execution may be faster because tests can be spread across multiple servers compared to a single server.
- System 300 includes a logging system 365 that obtains test data from testers 140 , 340 , and 350 .
- Logging system 365 includes a cumulative result repository 370 and a user interface 380 .
- Logging system 365 stores the collection of test data in cumulative result repository 370 .
- the individual tasks running in testers 140 , 340 , and 350 show per-shard information.
- logging system 365 provides a collection of data of an entire tester 140 instance rather than only per-shard information. This may be beneficial for tester 140 instances that have many replicas.
- Logging system 365 may run on a physical server different from servers 310 , 320 , and 330 .
- a user may access the test data using user interface 380 .
- the user can access the test data from one machine rather than accessing each of the servers 310 , 320 , and 330 .
- the system administrator can, for example, use user interface 380 rather than access each of servers 310 , 320 , and 330 to determine in which server a test failed.
- FIG. 4 shows an example screenshot 400 of an operation being performed, according to an embodiment.
- test executor 210 instructs a browser to navigate to a search web page and then enter a search query into the web page.
- a first portion 410 of screenshot 400 shows what is displayed to a user at a particular moment in time.
- the web page displays suggestions that align with what has been typed into the search query input field at that particular moment in time.
- a second portion 420 displays a log of the events that occurred as a result of executing the test. For example, portion 420 shows the test passed after 5.944956 seconds elapsed, debugging data was collected, a screenshot was acquired, proxy logs were saved, and a browser log was retrieved. A user can obtain screenshot 400 via user interface 220 .
- FIG. 5 shows an example screenshot 500 that lists previously executed tests, according to an embodiment.
- Screenshot 500 is an overview page that shows a timeline of which tests have executed in the last hour. The most recently executed tests are shown on the right-hand side.
- tester 140 compartmentalizes tests and supports the notion of services. This makes it possible to run tester 140 as a shared service. As shown in FIG. 5 , tests may be run for multiple services (e.g., services 304 , 308 , 312 , and 316 ). Screenshot 500 displays the names of the executed tests (e.g., test 1 and test 2), the browser and operating system configurations (e.g., first browser-OS 1 and second browser-OS 1), and a quantity of test replicas. A box 510 indicates that a test failed and a box 520 indicates that a test passed.
- tests may be run for multiple services (e.g., services 304 , 308 , 312 , and 316 ).
- Screenshot 500 displays the names of the executed tests (e.g., test 1 and test 2), the browser and operating system configurations (e.g., first browser-OS 1 and second browser-OS 1), and a quantity of test replicas.
- a box 510 indicates that a test failed and a box
- FIGS. 6-8 show example screenshots 600 , 700 , and 800 resulting from previously executed tests, according to an embodiment. These screenshots show a log of a test that passed and may be displayed, for example, when a user selects box 520 . The screenshots can be used by the system administrator charged with monitoring the web application to debug what happened during the test execution.
- screenshot 600 shows the test executed against service 308 on Monday, May 9, 2011 at 07:34:23 AM.
- the test name is “Test 2”
- the browser and operating system configuration is “Second Browser-OS1”
- a status of the test is “PASSED.”
- a description of the test is also displayed. The description may help a user understand the purpose of the test.
- a test framework log is displayed that shows a step-by-step process of the test execution. For example, at line 610 , the system checks that when “new” is entered into the search text box, “news” is suggested in the list of suggestions to a user.
- FIGS. 7 and 8 show a log file with the headers of the HTTP messages that were part of the test.
- Screenshot 700 shows a log including HTTP request headers included in a request sent in response to executing a test.
- the browser sent a request to host www.data.com.
- Screenshot 800 shows a log including HTTP request and responses headers that were sent and received in response to executing a test.
- the host sent a response to the browser indicating that the request succeeded (HTTP/1.1 200 OK).
- FIGS. 9-11 show flowcharts of an exemplary method for monitoring a web application, according to an embodiment.
- a first test is executed in a first browser residing on server 905 .
- a second test is executed in a second browser residing on server 905 .
- test executor 210 executes a first test in a first browser residing on a server and executes a second test in a second browser residing on the server.
- FIG. 10 shows a flowchart of stage 910 .
- a first command is sent to the first browser to perform a first operation.
- first client-side code included in a first response is executed in the first browser.
- the first response is responsive to the first operation.
- a status of the first test is determined based at least on the first response and a result of the executed first client-side code.
- test executor 210 sends a first command to first browser 155 to perform a first operation, first browser 155 executes first client-side code included in a first response responsive to the first operation, and test executor 210 determines a status of the first test based at least on the first response and a result of the executed first client-side code.
- First browser 155 may automatically execute the client-side code as a result of, for example, one or more previous commands relayed by command executor 250 or one or more commands sent to first browser 155 directly via the webdriver protocol.
- FIG. 11 shows a flowchart of stage 950 .
- a second command is sent to the second browser to perform a second operation.
- second client-side code included in a second response is executed in the second browser.
- the second response is responsive to the second operation.
- a status of the second test is determined based at least on the second response and a result of the executed second client-side code.
- test executor 210 sends a second command to second browser 160 to perform a second operation
- second browser 160 executes second client-side code included in a second response responsive to the second operation
- test executor 210 determines a status of the second test based at least on the second response and a result of the executed second client-side code.
- Second browser 160 may automatically execute the client-side code as a result of, for example, one or more previous commands relayed by command executor 260 or one or more commands sent to second browser 160 directly via the webdriver protocol.
- method 900 is described with reference to systems 100 and 200 , method 900 is not meant to be limiting and may be used in other applications.
Abstract
Description
-
- A. Test Execution
- B. Example Heterogeneous Browser Environments
- 1. First Example Environment
- 2. Second Example Environment
- C. External Programs
- D. Session-Oriented Tests
- E. Production Traffic Monitoring
Claims (21)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/362,885 US8863085B1 (en) | 2012-01-31 | 2012-01-31 | Monitoring web applications |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/362,885 US8863085B1 (en) | 2012-01-31 | 2012-01-31 | Monitoring web applications |
Publications (1)
Publication Number | Publication Date |
---|---|
US8863085B1 true US8863085B1 (en) | 2014-10-14 |
Family
ID=51661293
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/362,885 Expired - Fee Related US8863085B1 (en) | 2012-01-31 | 2012-01-31 | Monitoring web applications |
Country Status (1)
Country | Link |
---|---|
US (1) | US8863085B1 (en) |
Cited By (25)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20140372984A1 (en) * | 2013-06-18 | 2014-12-18 | Disney Enterprises, Inc. | Safe low cost web services software deployments |
US20150082279A1 (en) * | 2013-09-13 | 2015-03-19 | Sap Ag | Software testing system and method |
US20150089299A1 (en) * | 2013-09-25 | 2015-03-26 | Microsoft Corporation | Online application testing across browser environments |
US9047411B1 (en) * | 2013-10-22 | 2015-06-02 | The Mathworks, Inc. | Programming environment for executing program code despite errors and for providing error indicators |
US9053235B1 (en) * | 2013-10-22 | 2015-06-09 | The Mathworks, Inc. | Program code interface for providing program code and corresponding results of evaluating the program code |
US9053228B1 (en) * | 2013-10-22 | 2015-06-09 | The Mathworks, Inc. | Determining when to evaluate program code and provide results in a live evaluation programming environment |
US9064052B1 (en) * | 2013-10-22 | 2015-06-23 | The Mathworks, Inc. | Providing intermediate results of evaluating program code that includes a compound statement |
US9075916B1 (en) * | 2013-10-22 | 2015-07-07 | The Mathworks, Inc. | Undoing/redoing program code execution |
US9122794B2 (en) | 2012-10-30 | 2015-09-01 | Oracle International Corporation | System and method for debugging domain specific languages |
US9146834B2 (en) * | 2013-08-22 | 2015-09-29 | Oracle International Corporation | Targeted cloud-based debugging |
US9146840B2 (en) * | 2012-06-15 | 2015-09-29 | Cycle Computing, Llc | Method and system for automatically detecting and resolving infrastructure faults in cloud infrastructure |
US9183113B2 (en) | 2011-09-29 | 2015-11-10 | Oracle International Corporation | Debugging analysis in running multi-user systems |
US20150370622A1 (en) * | 2014-06-24 | 2015-12-24 | International Business Machines Corporation | System verification of interactive screenshots and log files between client systems and server systems within a network computing environment |
US20160004628A1 (en) * | 2014-07-07 | 2016-01-07 | Unisys Corporation | Parallel test execution framework for multiple web browser testing |
US9262311B1 (en) * | 2013-12-03 | 2016-02-16 | Amazon Technologies, Inc. | Network page test system and methods |
US9317398B1 (en) * | 2014-06-24 | 2016-04-19 | Amazon Technologies, Inc. | Vendor and version independent browser driver |
US9336126B1 (en) | 2014-06-24 | 2016-05-10 | Amazon Technologies, Inc. | Client-side event logging for heterogeneous client environments |
US9430361B1 (en) * | 2014-06-24 | 2016-08-30 | Amazon Technologies, Inc. | Transition testing model for heterogeneous client environments |
US9645915B2 (en) | 2006-12-27 | 2017-05-09 | The Mathworks, Inc. | Continuous evaluation of program code and saving state information associated with program code |
US10097565B1 (en) | 2014-06-24 | 2018-10-09 | Amazon Technologies, Inc. | Managing browser security in a testing context |
US20190079854A1 (en) * | 2017-09-12 | 2019-03-14 | Facebook, Inc. | Systems and methods for executing tests |
US20190132640A1 (en) * | 2017-10-31 | 2019-05-02 | Bose Corporation | Automated Playback and Redistribution of Internet Streaming Content |
US10303516B1 (en) | 2018-01-03 | 2019-05-28 | Sas Institute Inc. | Allocating computing resources for providing various datasets to client devices |
US10482002B2 (en) * | 2016-08-24 | 2019-11-19 | Google Llc | Multi-layer test suite generation |
US20200151241A1 (en) * | 2018-11-14 | 2020-05-14 | International Business Machines Corporation | Webpage component replication system and method |
Citations (10)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20020097268A1 (en) * | 2001-01-22 | 2002-07-25 | Dunn Joel C. | Method, system, and program for a platform-independent, browser-based, client-side, test automation facility for verifying web site operation |
US20040205565A1 (en) * | 2001-10-23 | 2004-10-14 | Sun Microsystems, Inc. | XML based report generator |
US20050268288A1 (en) * | 2004-05-27 | 2005-12-01 | National Instruments Corporation | Graphical program analyzer with framework for adding user-defined tests |
US7698688B2 (en) * | 2008-03-28 | 2010-04-13 | International Business Machines Corporation | Method for automating an internationalization test in a multilingual web application |
US20110252404A1 (en) * | 2009-08-03 | 2011-10-13 | Knu-Industry Cooperation Foundation | Web-based software debugging apparatus and method for remote debugging |
US20110289489A1 (en) * | 2010-05-20 | 2011-11-24 | Verizon Patent And Licensing Inc. | Concurrent cross browser testing |
US8332821B2 (en) * | 2009-03-25 | 2012-12-11 | Microsoft Corporation | Using encoding to detect security bugs |
US8392890B2 (en) * | 2007-10-15 | 2013-03-05 | Software Research, Inc. | Method and system for testing websites |
US20130086554A1 (en) * | 2011-09-29 | 2013-04-04 | Sauce Labs, Inc. | Analytics Driven Development |
US8490059B2 (en) * | 2009-09-29 | 2013-07-16 | International Business Machines Corporation | Cross-browser testing of a web application |
-
2012
- 2012-01-31 US US13/362,885 patent/US8863085B1/en not_active Expired - Fee Related
Patent Citations (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20020097268A1 (en) * | 2001-01-22 | 2002-07-25 | Dunn Joel C. | Method, system, and program for a platform-independent, browser-based, client-side, test automation facility for verifying web site operation |
US20040205565A1 (en) * | 2001-10-23 | 2004-10-14 | Sun Microsystems, Inc. | XML based report generator |
US20050268288A1 (en) * | 2004-05-27 | 2005-12-01 | National Instruments Corporation | Graphical program analyzer with framework for adding user-defined tests |
US8392890B2 (en) * | 2007-10-15 | 2013-03-05 | Software Research, Inc. | Method and system for testing websites |
US7698688B2 (en) * | 2008-03-28 | 2010-04-13 | International Business Machines Corporation | Method for automating an internationalization test in a multilingual web application |
US8332821B2 (en) * | 2009-03-25 | 2012-12-11 | Microsoft Corporation | Using encoding to detect security bugs |
US20110252404A1 (en) * | 2009-08-03 | 2011-10-13 | Knu-Industry Cooperation Foundation | Web-based software debugging apparatus and method for remote debugging |
US8589881B2 (en) * | 2009-08-03 | 2013-11-19 | Knu-Industry Cooperation Foundation | Web-based software debugging apparatus and method for remote debugging |
US8490059B2 (en) * | 2009-09-29 | 2013-07-16 | International Business Machines Corporation | Cross-browser testing of a web application |
US20110289489A1 (en) * | 2010-05-20 | 2011-11-24 | Verizon Patent And Licensing Inc. | Concurrent cross browser testing |
US20130086554A1 (en) * | 2011-09-29 | 2013-04-04 | Sauce Labs, Inc. | Analytics Driven Development |
Non-Patent Citations (3)
Title |
---|
Artzi et al., A framework for automated testing of javascript web applications, May 2011, 10 pages. * |
Collins et al., Crossfire: multiprocess, cross-browser, open-web debugging protocol, Oct. 2011, 9 pages. * |
Vasar et al., Framework for monitoring and testing web application scalability on the cloud, Aug. 2012, 8 pages. * |
Cited By (38)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9645915B2 (en) | 2006-12-27 | 2017-05-09 | The Mathworks, Inc. | Continuous evaluation of program code and saving state information associated with program code |
US9183113B2 (en) | 2011-09-29 | 2015-11-10 | Oracle International Corporation | Debugging analysis in running multi-user systems |
US9514026B2 (en) | 2011-09-29 | 2016-12-06 | Oracle International Corporation | Debugging analysis in running multi-user systems |
US10025678B2 (en) | 2012-06-15 | 2018-07-17 | Microsoft Technology Licensing, Llc | Method and system for automatically detecting and resolving infrastructure faults in cloud infrastructure |
US9146840B2 (en) * | 2012-06-15 | 2015-09-29 | Cycle Computing, Llc | Method and system for automatically detecting and resolving infrastructure faults in cloud infrastructure |
US9122794B2 (en) | 2012-10-30 | 2015-09-01 | Oracle International Corporation | System and method for debugging domain specific languages |
US20140372984A1 (en) * | 2013-06-18 | 2014-12-18 | Disney Enterprises, Inc. | Safe low cost web services software deployments |
US9383986B2 (en) * | 2013-06-18 | 2016-07-05 | Disney Enterprises, Inc. | Safe low cost web services software deployments |
US9146834B2 (en) * | 2013-08-22 | 2015-09-29 | Oracle International Corporation | Targeted cloud-based debugging |
US20150082279A1 (en) * | 2013-09-13 | 2015-03-19 | Sap Ag | Software testing system and method |
US9575873B2 (en) * | 2013-09-13 | 2017-02-21 | Sap Se | Software testing system and method |
US9830254B2 (en) | 2013-09-25 | 2017-11-28 | Microsoft Technology Licensing, Llc | Online application testing across browser environments |
US20150089299A1 (en) * | 2013-09-25 | 2015-03-26 | Microsoft Corporation | Online application testing across browser environments |
US9223684B2 (en) * | 2013-09-25 | 2015-12-29 | Microsoft Technology Licensing, Llc | Online application testing across browser environments |
US9053228B1 (en) * | 2013-10-22 | 2015-06-09 | The Mathworks, Inc. | Determining when to evaluate program code and provide results in a live evaluation programming environment |
US9047411B1 (en) * | 2013-10-22 | 2015-06-02 | The Mathworks, Inc. | Programming environment for executing program code despite errors and for providing error indicators |
US9665471B1 (en) | 2013-10-22 | 2017-05-30 | The Mathworks, Inc. | Program code interface for providing program code and corresponding results of evaluating the program code |
US9053235B1 (en) * | 2013-10-22 | 2015-06-09 | The Mathworks, Inc. | Program code interface for providing program code and corresponding results of evaluating the program code |
US9075916B1 (en) * | 2013-10-22 | 2015-07-07 | The Mathworks, Inc. | Undoing/redoing program code execution |
US9582400B1 (en) | 2013-10-22 | 2017-02-28 | The Mathworks, Inc. | Determining when to evaluate program code and provide results in a live evaluation programming environment |
US9064052B1 (en) * | 2013-10-22 | 2015-06-23 | The Mathworks, Inc. | Providing intermediate results of evaluating program code that includes a compound statement |
US9547580B1 (en) | 2013-10-22 | 2017-01-17 | The Mathworks, Inc. | Providing intermediate results of evaluating program code that includes a compound statement |
US9262311B1 (en) * | 2013-12-03 | 2016-02-16 | Amazon Technologies, Inc. | Network page test system and methods |
US9336126B1 (en) | 2014-06-24 | 2016-05-10 | Amazon Technologies, Inc. | Client-side event logging for heterogeneous client environments |
US10353760B2 (en) * | 2014-06-24 | 2019-07-16 | International Business Machines Corporation | System verification of interactive screenshots and log files between client systems and server systems within a network computing environment |
US20150370622A1 (en) * | 2014-06-24 | 2015-12-24 | International Business Machines Corporation | System verification of interactive screenshots and log files between client systems and server systems within a network computing environment |
US9317398B1 (en) * | 2014-06-24 | 2016-04-19 | Amazon Technologies, Inc. | Vendor and version independent browser driver |
US9430361B1 (en) * | 2014-06-24 | 2016-08-30 | Amazon Technologies, Inc. | Transition testing model for heterogeneous client environments |
US9846636B1 (en) | 2014-06-24 | 2017-12-19 | Amazon Technologies, Inc. | Client-side event logging for heterogeneous client environments |
US20150372884A1 (en) * | 2014-06-24 | 2015-12-24 | International Business Machines Corporation | System verification of interactive screenshots and log files between client systems and server systems within a network computing environment |
US10097565B1 (en) | 2014-06-24 | 2018-10-09 | Amazon Technologies, Inc. | Managing browser security in a testing context |
US10445166B2 (en) * | 2014-06-24 | 2019-10-15 | International Business Machines Corporation | System verification of interactive screenshots and log files between client systems and server systems within a network computing environment |
US20160004628A1 (en) * | 2014-07-07 | 2016-01-07 | Unisys Corporation | Parallel test execution framework for multiple web browser testing |
US10482002B2 (en) * | 2016-08-24 | 2019-11-19 | Google Llc | Multi-layer test suite generation |
US20190079854A1 (en) * | 2017-09-12 | 2019-03-14 | Facebook, Inc. | Systems and methods for executing tests |
US20190132640A1 (en) * | 2017-10-31 | 2019-05-02 | Bose Corporation | Automated Playback and Redistribution of Internet Streaming Content |
US10303516B1 (en) | 2018-01-03 | 2019-05-28 | Sas Institute Inc. | Allocating computing resources for providing various datasets to client devices |
US20200151241A1 (en) * | 2018-11-14 | 2020-05-14 | International Business Machines Corporation | Webpage component replication system and method |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8863085B1 (en) | Monitoring web applications | |
US10423521B2 (en) | Automatically executing stateless transactions with data dependency in test cases | |
CN109495308B (en) | Automatic operation and maintenance system based on management information system | |
US20160004628A1 (en) | Parallel test execution framework for multiple web browser testing | |
US8584079B2 (en) | Quality on submit process | |
US8762929B2 (en) | System and method for exclusion of inconsistent objects from lifecycle management processes | |
US8533532B2 (en) | System identifying and inferring web session events | |
US9323647B2 (en) | Request-based activation of debugging and tracing | |
US20150331779A1 (en) | Framework to accommodate test plan changes without affecting or interrupting test execution | |
US10965766B2 (en) | Synchronized console data and user interface playback | |
US10853227B2 (en) | Systems and methods for modular test platform for applications | |
CN111026635B (en) | Software project testing system, method, device and storage medium | |
US20110022899A1 (en) | Producing or executing a script for an operation test of a terminal server | |
US20130339931A1 (en) | Application trace replay and simulation systems and methods | |
CN105745645A (en) | Determining web page processing state | |
US11706084B2 (en) | Self-monitoring | |
US20030120776A1 (en) | System controller for use in a distributed processing framework system and methods for implementing the same | |
US20190317736A1 (en) | State machine representation of a development environment deployment process | |
Kessin | Programming HTML5 applications: building powerful cross-platform environments in JavaScript | |
US9892019B2 (en) | Use case driven stepping component automation framework | |
US20200396303A1 (en) | Network latency detection | |
US20090125580A1 (en) | Displaying server errors on the client machine that caused the failed request | |
CN111245917B (en) | Katalon-based work order entry device and implementation method thereof | |
US10599750B2 (en) | Capturing an application state in a conversation | |
US20180219752A1 (en) | Graph search in structured query language style query |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:STAHLBERG, PATRICK;REEL/FRAME:027683/0115Effective date: 20120128 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044277/0001Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551)Year of fee payment: 4 |
|
FEPP | Fee payment procedure |
Free format text: MAINTENANCE FEE REMINDER MAILED (ORIGINAL EVENT CODE: REM.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
LAPS | Lapse for failure to pay maintenance fees |
Free format text: PATENT EXPIRED FOR FAILURE TO PAY MAINTENANCE FEES (ORIGINAL EVENT CODE: EXP.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
STCH | Information on status: patent discontinuation |
Free format text: PATENT EXPIRED DUE TO NONPAYMENT OF MAINTENANCE FEES UNDER 37 CFR 1.362 |
|
FP | Lapsed due to failure to pay maintenance fee |
Effective date: 20221014 |