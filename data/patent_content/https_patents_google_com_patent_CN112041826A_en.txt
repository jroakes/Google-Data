CN112041826A - Fine-grained traffic shaping offload for network interface cards - Google Patents
Fine-grained traffic shaping offload for network interface cards Download PDFInfo
- Publication number
- CN112041826A CN112041826A CN201980026359.7A CN201980026359A CN112041826A CN 112041826 A CN112041826 A CN 112041826A CN 201980026359 A CN201980026359 A CN 201980026359A CN 112041826 A CN112041826 A CN 112041826A
- Authority
- CN
- China
- Prior art keywords
- network interface
- interface card
- packet
- communication
- memory
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 238000007493 shaping process Methods 0.000 title claims abstract description 46
- 238000000034 method Methods 0.000 claims abstract description 67
- 230000005540 biological transmission Effects 0.000 claims description 174
- 238000004891 communication Methods 0.000 claims description 105
- 238000012546 transfer Methods 0.000 claims description 15
- 230000004044 response Effects 0.000 claims description 11
- 230000007246 mechanism Effects 0.000 abstract description 31
- 230000003111 delayed effect Effects 0.000 abstract description 11
- 238000004422 calculation algorithm Methods 0.000 description 25
- 230000008569 process Effects 0.000 description 14
- 238000010586 diagram Methods 0.000 description 12
- 238000012545 processing Methods 0.000 description 7
- 238000004590 computer program Methods 0.000 description 6
- 230000008713 feedback mechanism Effects 0.000 description 3
- 239000000872 buffer Substances 0.000 description 2
- 238000004364 calculation method Methods 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 238000000926 separation method Methods 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 230000001360 synchronised effect Effects 0.000 description 2
- RYGMFSIKBFXOCR-UHFFFAOYSA-N Copper Chemical compound [Cu] RYGMFSIKBFXOCR-UHFFFAOYSA-N 0.000 description 1
- 230000009286 beneficial effect Effects 0.000 description 1
- 229910052802 copper Inorganic materials 0.000 description 1
- 239000010949 copper Substances 0.000 description 1
- 230000001934 delay Effects 0.000 description 1
- 239000000284 extract Substances 0.000 description 1
- 239000004744 fabric Substances 0.000 description 1
- 239000000835 fiber Substances 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 239000002184 metal Substances 0.000 description 1
- 229910052751 metal Inorganic materials 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000007618 network scheduling algorithm Methods 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000000717 retained effect Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 230000002123 temporal effect Effects 0.000 description 1
- 239000002699 waste material Substances 0.000 description 1
Images
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L47/00—Traffic control in data switching networks
- H04L47/10—Flow control; Congestion control
- H04L47/22—Traffic shaping
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F13/00—Interconnection of, or transfer of information or other signals between, memories, input/output devices or central processing units
- G06F13/10—Program control for peripheral devices
- G06F13/12—Program control for peripheral devices using hardware independent of the central processor, e.g. channel or peripheral processor
- G06F13/122—Program control for peripheral devices using hardware independent of the central processor, e.g. channel or peripheral processor where hardware performs an I/O function other than control of data transfer
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F13/00—Interconnection of, or transfer of information or other signals between, memories, input/output devices or central processing units
- G06F13/14—Handling requests for interconnection or transfer
- G06F13/20—Handling requests for interconnection or transfer for access to input/output bus
- G06F13/28—Handling requests for interconnection or transfer for access to input/output bus using burst mode transfer, e.g. direct memory access DMA, cycle steal
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F13/00—Interconnection of, or transfer of information or other signals between, memories, input/output devices or central processing units
- G06F13/38—Information transfer, e.g. on bus
- G06F13/382—Information transfer, e.g. on bus using universal interface adapter
- G06F13/387—Information transfer, e.g. on bus using universal interface adapter for adaptation of different data processing systems to different peripheral devices, e.g. protocol converters for incompatible systems, open system
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F9/00—Arrangements for program control, e.g. control units
- G06F9/06—Arrangements for program control, e.g. control units using stored programs, i.e. using an internal store of processing equipment to receive or retain programs
- G06F9/44—Arrangements for executing specific programs
- G06F9/455—Emulation; Interpretation; Software simulation, e.g. virtualisation or emulation of application or operating system execution engines
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L47/00—Traffic control in data switching networks
- H04L47/10—Flow control; Congestion control
- H04L47/20—Traffic policing
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L47/00—Traffic control in data switching networks
- H04L47/10—Flow control; Congestion control
- H04L47/32—Flow control; Congestion control by discarding or delaying data units, e.g. packets or frames
- H04L47/323—Discarding or blocking control packets, e.g. ACK packets
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L47/00—Traffic control in data switching networks
- H04L47/50—Queue scheduling
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L47/00—Traffic control in data switching networks
- H04L47/50—Queue scheduling
- H04L47/56—Queue scheduling implementing delay-aware scheduling
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L47/00—Traffic control in data switching networks
- H04L47/50—Queue scheduling
- H04L47/56—Queue scheduling implementing delay-aware scheduling
- H04L47/568—Calendar queues or timing rings
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L49/00—Packet switching elements
- H04L49/90—Buffering arrangements
- H04L49/901—Buffering arrangements using storage descriptor, e.g. read or write pointers
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L49/00—Packet switching elements
- H04L49/90—Buffering arrangements
- H04L49/9063—Intermediate storage in different physical parts of a node or terminal
- H04L49/9068—Intermediate storage in different physical parts of a node or terminal in the network interface card
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L49/00—Packet switching elements
- H04L49/90—Buffering arrangements
- H04L49/9084—Reactions to storage capacity overflow
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L69/00—Network arrangements, protocols or services independent of the application payload and not provided for in the other groups of this subclass
- H04L69/22—Parsing or analysis of headers
Abstract
A network interface card (140) having traffic shaping capabilities and a method for network traffic shaping using the network interface card (140) are provided. The network interface card (140) and method may shape traffic originating from one or more applications (150a-c) executing on a host network device. The application (150a-c) may be implemented in a virtual machine or a containerized computing environment. The network interface card (140) and method may perform or include several traffic shaping mechanisms including, for example and without limitation: a delayed completion mechanism, a time index data structure (130), a packet builder (142), and a memory manager (147).
Description
Cross Reference to Related Applications
This application is a continuation of U.S. patent application No.16/146,373 filed on 28.9.2018, the disclosure of which is incorporated herein by reference.
Background
Traffic shaping (traffic shaping) is a technique that leverages various mechanisms to regulate network data traffic to shape, rate limit, pace, prioritize, or delay traffic flows determined to be less important or less desirable than prioritized traffic flows, or to enforce the distribution of network resources across equally prioritized packet flows. Mechanisms for shaping traffic include: a classifier for matching and moving packets between different queues based on a policy; a queue-specific shaping algorithm for delaying, dropping or marking packets; and a scheduling algorithm for reasonably prioritizing packet assignments across different queues. Traffic shaping systems employing these mechanisms are difficult to scale when considering the requirements of maintaining the desired network performance for a large number of traffic classes, or when deploying traffic shaping systems in various network host architectures.
Disclosure of Invention
At least one aspect relates to a network interface card. The network interface card includes a network interface card memory configured to store a time index data structure that stores identifiers and sets of pointers associated with respective communications to be transmitted by the network interface card. Each set of pointers corresponds to one of the identifiers stored in the time index data structure. Each pointer in each set of pointers points to a location in the network interface card memory or a location in a host memory of a host computing device to which the network interface device is coupled. The network interface card includes scheduling logic. The scheduling logic is configured to receive a notification of a new communication to be transmitted by the network interface card, the new communication originating from a source virtualized computing environment of a plurality of virtualized computing environments executing on the host computing device. The notification includes header data and a set of pointers to data to be included in the communication. The scheduling logic is configured to determine whether to drop packets or schedule transmission of new communications based on available capacity in a network interface card memory allocated to the source virtualized computing environment. The scheduling logic is configured to, in response to determining to schedule transmission of the communication, store an identifier associated with the communication in a time index data structure at a scheduled transmission time and store a set of pointers associated with the communication in a network interface card memory. The network interface card includes packet generation logic configured to generate a data packet using a set of pointers stored in a network interface card memory associated with the communication upon arrival of a scheduled transmission time for the communication for which the identifier is stored in a time-indexed data structure. The network interface card includes a transceiver configured to transmit the generated data packets. The network interface card includes transmission completion logic configured to generate, upon completion of transmission of a data packet generated for a communication, a transmission completion message to be transmitted to an application executing in a source virtualized computing environment that initiated the communication.
At least one aspect relates to a method of network traffic shaping. The method includes receiving, at a network interface card of a host computing device, a notification of a new communication to be transmitted by the network interface card, the new communication originating from a source virtualized computing environment of a plurality of virtualized computing environments executing on the host computing device. The notification includes header data and a set of pointers to data to be included in the communication. The method includes determining whether to drop packets or schedule transmission of new communications based on available capacity in a network interface card memory space allocated to the source virtualized computing environment. The method comprises the following steps: in response to determining to schedule transmission of the communication, storing an identifier associated with the communication in a time index data structure at a scheduled transmission time; and stores a set of pointers associated with the communication in a network interface card memory. The set of pointers corresponds to identifiers stored in a time index data structure, and each pointer in the set of pointers points to a location in a network interface card memory or a location in a host memory of the host computing device. The method comprises the following steps: upon arrival of a scheduled transmission time for a communication for which an identifier is stored in a time-indexed data structure, a set of pointers stored in a network interface card memory associated with the communication is used to generate a data packet. The method includes transmitting the generated data packet. The method comprises the following steps: upon completion of transmission of the data packet generated for the communication, a transmission completion message is generated to be communicated to an application executing in the source virtualized computing environment that initiated the communication.
At least one aspect relates to a non-transitory computer-readable medium having instructions stored thereon, the instructions configured to cause one or more processors of a network interface card of a host computing device to perform a method of network traffic shaping. The method comprises the following steps: a notification of a new communication to be transmitted by a network interface card is received, the new communication originating from a source virtualized computing environment of a plurality of virtualized computing environments executing on a host computing device. The notification includes header data and a set of pointers to data to be included in the communication. The method includes determining whether to drop packets or schedule transmission of new communications based on available capacity in a network interface card memory space allocated to the source virtualized computing environment. The method comprises the following steps: in response to determining to schedule transmission of the communication, an identifier associated with the communication is stored in a time index data structure at a scheduled transmission time and a set of pointers associated with the communication is stored in a network interface card memory. The set of pointers corresponds to identifiers stored in a time index data structure, and each pointer in the set of pointers points to a location in a network interface card memory or a location in a host memory of the host computing device. The method comprises the following steps: upon arrival of a scheduled transmission time for a communication for which an identifier is stored in a time-indexed data structure, a set of pointers stored in a network interface card memory associated with the communication is used to generate a data packet. The method comprises the following steps: upon completion of transmission of the data packet generated for the communication, a transmission completion message is generated to be communicated to an application executing in the source virtualized computing environment that initiated the communication.
These and other aspects and embodiments are discussed in detail below. The foregoing information and the following detailed description include illustrative examples of various aspects and embodiments, and provide an overview or framework for understanding the nature and character of the claimed aspects and embodiments. The accompanying drawings are included to provide an illustration and a further understanding of the various aspects and embodiments, and are incorporated in and constitute a part of this specification.
Drawings
The drawings are not intended to be drawn to scale. Like reference numbers and designations in the various drawings indicate like elements. For purposes of clarity, not every component may be labeled in every drawing. In the drawings:
FIG. 1 is a block diagram of a network environment having network devices, according to some embodiments;
FIG. 2A is a block diagram of an exemplary virtual machine environment;
FIG. 2B is a block diagram of an exemplary containerization environment;
FIG. 3 is a flow diagram illustrating the operation of a network interface card according to some embodiments;
4A-4C are block diagrams illustrating the operation of a network interface card according to some embodiments; and is
FIG. 5 is a block diagram of an exemplary computing system that may be employed to implement elements of the systems and methods described and illustrated herein, in accordance with an illustrative embodiment.
Detailed Description
The present disclosure relates generally to systems and methods for fine granularity traffic shaping for network interface cards. Traffic shaping is a technique that leverages various mechanisms to condition network data traffic to shape, rate limit, pace, prioritize, or delay traffic flows that are determined to be less important or less desirable than prioritized traffic flows, or to enforce the distribution of network resources across equally prioritized packet flows. Mechanisms for shaping traffic include: a classifier for matching and moving packets between different queues based on a policy; a queue-specific shaping algorithm for delaying, dropping or marking packets; and a scheduling algorithm for reasonably prioritizing packet assignments across different queues. Traffic shaping systems employing these mechanisms are difficult to scale when considering the requirements of maintaining the desired network performance for a large number of traffic classes, or when deploying traffic shaping systems in various network host architectures.
Traffic shaping systems should be designed to achieve efficient memory usage and host processor power consumption while managing higher levels of congestion control, such as that used in the Transmission Control Protocol (TCP). Traffic shaping systems may include several different mechanisms and/or techniques for efficiently and reliably controlling the flow of application instances from multiple to many different virtual machines and/or sharing bandwidth through a network interface card. Various traffic shaping mechanisms and techniques described herein include delayed completion mechanisms, time-indexed data structures, packet builders, and memory managers.
Typically, the traffic shaping mechanism has been implemented within the OS or network interface driver of the network device. Embodiments of the present disclosure may implement a delayed completion mechanism, time index data structure, packet builder, and memory manager on logic in the network interface card itself. Implementing these mechanisms in a network interface card may increase the throughput of the network interface card while reducing the computational load on the network device. In some embodiments, the traffic shaping mechanism may be implemented on logic in a microprocessor, Field Programmable Gate Array (FPGA), or Application Specific Integrated Circuit (ASIC) incorporated into the network interface card.
The delayed completion mechanism may delay, pace, or rate limit packets to avoid bursty or unnecessary transmission delays. Using delayed completion may achieve higher utilization of network resources and host processing resources. The packet delay mechanism may reduce the need for large memory buffers. For example, when packets are delayed, the feedback mechanism may apply "backpressure," that is, send feedback to the sending module (e.g., a device or software component, such as a software application) in order to cause the sending module to reduce its rate at which packets are sent. The delayed completion mechanism may prevent the application from sending additional packets for transmission until the application receives a message confirming that the previously forwarded packet was successfully transmitted. For example, the delayed completion mechanism communicates a packet transfer completion message to a sending module, such as a software application or guest operating system, which may suppress the network interface card from requesting additional packet transfers until it receives a transfer completion message indicating that the previously requested packet transfer has been completed. As described herein, in some embodiments, a network interface card processes received packets based on traffic shaping policies stored on the network interface card to determine a transmission time for each packet. For example, the rate limiting policy may include a rate pacing policy or a target rate limit. Additionally or alternatively, the rate limiting policy may include a specific policy associated with a particular packet class or a total rate for a particular packet class. Without a packet delay mechanism, the sending module may continue to generate packets that may be buffered or dropped so as to waste additional memory and host processor power to queue or regenerate packets.
The network interface card may store an identifier associated with a respective packet in the time index data structure at a location associated with a transmission time determined for the respective packet. Time lineThe reference data structure may comprise a single time-based queue, such as a timing wheel or calendar queue data structure, to receive identifiers associated with packets from multiple queues or TCP sockets. The packet identifier may be inserted and extracted based on the determined transmission time. In some implementations, the network interface card may determine that a time indexed in a single time index queue has been reached and, in response, transmit a packet associated with an identifier stored in a time index data structure at a location associated with the time of arrival. For example, the network interface card or the network interface card may determine that time t has been reached0And thus, the network interface card and/or network interface driver may cause the assignment of t0The packet associated with the identifier of the transmission time is transmitted by a network interface card of the network device. In some implementations, after the network interface card transmits the packet, the network interface card may communicate a transmission completion notification back to the application that initiated the transmitted packet. The time index data structure may be used in conjunction with a delayed completion mechanism. In some implementations, a network device is configured to receive packets from a plurality of applications at a TCP layer of the network device. The received packets originate from an application executing on a host computing device, such as on one or more virtual machines or containerized execution environments (collectively referred to herein as "virtual computing environments") hosted by the host computing device.
The packet builder may build packets for transmission based on the packet identifiers in the time-indexed data structure and the associated respective sets of pointers stored in the network interface card memory. Each identifier may be, for example, a pointer to a region in memory containing a set of pointers. Each set of pointers includes a plurality of pointers, where each pointer points to a memory location in host memory that stores a portion of the data that will make up the packet associated with the identifier. When the time index data structure reaches a time corresponding to the transmission time of the identifier in the time index data structure, the packet builder may use the identifier to retrieve a set of pointers from the network interface card memory and use the set of pointers to build a packet for transmission. In some embodiments, the set of pointers may take the form of a list of scattered sets (scatter gather). Each pointer in the set of pointers may point to an area in memory containing the appropriate field, component, or payload of the packet. The packet builder can thus build the packet header and payload based on the set of pointers. In some embodiments, the packet builder may directly access the host memory using, for example, Direct Memory Access (DMA). By maintaining only a single identifier in the time index data structure, the size of the time index data structure may be minimized. Furthermore, by storing only the set of pointers in the network interface card memory, the network interface card does not have to store the entirety of all queued packets for transmission in its own local memory; thus, the memory size of the network interface card may be minimized and the transmission between the host network device and the network card may be optimized. Further, by using the packet identifier and set of pointers for scheduling and other traffic shaping operations in the network interface card, the network interface card may receive notifications and schedule (or not schedule) packet transmissions without having to receive large amounts of packet data. Thus, the traffic burst (in bytes per second) from the application to the network interface card may be much smaller and less likely to overwhelm the available bandwidth of the interface between the host device and the network interface card. When a packet is ready to be transmitted, the network interface card need only receive a large portion of the data for the packet. Furthermore, the network interface card never needs to receive a large portion of the data of the packet that is not scheduled for transmission (e.g., the notification is discarded due to a lack of available capacity in the network interface card memory allocated to the virtual machine that initiated the notification). Thus, the host-to-network interface card's interface bandwidth is reserved for packets that are actually to be transmitted and essentially uses bandwidth at the transmission rate, which is controlled by the network interface card itself rather than by the application, virtual machine, or container that generated the packet. This may be particularly valuable when there is little coordination between the packet generation applications, virtual machines, or containers.
The network interface card may implement a memory manager that may control the amount of network interface card resources available to each virtual machine or container implemented on the network device. In some cases, each virtual machine may be executing many applications (tens or more). Other methods of traffic shaping (such as delayed completion) may control traffic originating at individual application instances or on a flow-by-flow basis. However, in a cloud computing environment, it is possible for a single virtual machine or container to simultaneously exercise many applications. For example, it is possible for a single virtual machine or container to host one hundred or more applications, all of which attempt to transmit packets simultaneously. Other traffic shaping mechanisms may employ different transmission queues for different traffic classes; however, these mechanisms also lack the ability to differentiate traffic by source. Thus, for a host device or containerized environment hosting virtual machines, it may be beneficial to have separate traffic shaping control capabilities for each virtual machine or container. The memory manager may provide separate traffic shaping control for each virtual machine or container by allocating portions of the network interface card memory available to each virtual machine or container. Thus, the memory manager may ensure that no single virtual machine or container can overload the resources of the network interface card resulting in damage to other virtual machines or containers. For example, based on a lack of available capacity in a network interface card memory allocated by a memory manager to the source virtualized computing environment, the scheduling logic of the network interface card may cause packets to be dropped. However, if there is capacity available for the source virtualized computing environment, the scheduling logic may schedule transmission of the packet.
In the above embodiments, a packet source, such as a software application running on the real OS of a host network device, on the guest OS of a virtual machine, or on an upper layer of a TCP stack in a guest OS managed by a hypervisor, does not have to be aware of the traffic shaping policy or algorithm implemented on the network interface card. Accordingly, the cost of implementing a network interface driver and guest operating system in a virtual machine environment may be reduced. Furthermore, the packet source does not have to be aware of other configuration parameters, such as packet classification rules and other rate limiting policies. Thus, traffic shaping may be performed in a more reliable manner than in methods where an application or user configures such detailed algorithms and policies.
Fig. 1 is a block diagram of an exemplary network environment 100 having a network device 110. In general, the illustrated network environment 100 includes a network 700 of interconnected network nodes 750. Network node 750 participates in network 700 as a data source, a data destination (or data sink), and/or as an intermediate node (such as a switch, router, and/or gateway) that propagates data from a source to a destination through network 700. Network 700 includes network device 110, network device 110 having links 600 to various other participating network nodes 750. Referring in more detail to FIG. 1, a network 700 is a network that facilitates interaction between participant devices. The illustrative example network 700 is the internet; however, in other embodiments, the network 700 may be another network, such as a local area network within a data center, a network fabric, or any other local or wide area network. Network 700 may be comprised of multiple connected sub-networks or an autonomous network. Network 700 may be a Local Area Network (LAN), such as a corporate intranet, a Metropolitan Area Network (MAN), a Wide Area Network (WAN), an internetwork, such as the internet, or a peer-to-peer network (e.g., an ad hoc (ad hoc) WiFi peer-to-peer network). Any type and/or form of data network and/or communication network may be used for network 700. It may be a public network, a private network, or a combination of public and private networks. In general, network 700 is used to communicate information between computing devices (e.g., network node 750), and network devices 110 of a data traffic shaping system facilitate this communication according to their configuration.
As shown in fig. 1, network device 110 is a host or server that hosts one or more applications 150 a-150 c (generally application 150) executing on a real Operating System (OS). As discussed further below, in other embodiments, network device 110 may be a server hosting a virtual machine or container that is executing application 150. The network device 110 includes a network interface driver 120, a memory 115, a network interface card 140, a real OS 220, and applications 150. Network interface card 140 includes scheduler 141, packet builder 142, transceiver 143, transfer completion logic 144, memory manager 147, and memory 146. The memory 146 may include a time-indexed data structure (such as the timing wheel 130), although the timing wheel 130 is illustrated as being external to the memory 146 for clarity. In some implementations, network device 110 has a configuration similar to that of computing system 1010 as shown in fig. 5. For example, memory 115 may have a configuration similar to that of memory 1070 as shown in fig. 5, and network interface card 140 may have a configuration similar to that of network interface card 1022 and/or network interface controller 1020 as shown in fig. 5. The computing system 1010 is described in more detail below with reference to fig. 5. In some embodiments of network device 110 illustrated in fig. 1, not all of the elements shown in computing system 1010 illustrated in fig. 5 need be present.
Referring again to fig. 1, in some embodiments, network device 110 hosts one or more applications 150 (e.g., applications 150a, 150b, and 150 c). One or more of the applications 150 a-150 c may be software applications running on the real operating system of the network device 110. As discussed further with respect to fig. 2A and 2B, in some embodiments, one or more of the software applications 150 a-150 c may be software applications running on a guest OS managed by a hypervisor in a virtual machine environment, and may also be an upper layer of a protocol stack (e.g., a TCP stack) of the guest OS of the virtual machine environment. For example, referring to fig. 2A, in fig. 2A, applications 150a through 150c may each be a software application 230 running on real OS 220, a software application 265 running on guest OS 260 of virtual machine 1 managed by hypervisor 250, or an upper layer of protocol stack 261 of guest OS 260 of virtual machine 1. The hypervisor 250 and its associated virtual machine environment are described in more detail below with reference to FIG. 2A.
Referring back to fig. 1, network device 110 includes memory 115. In some implementations, memory 115 may store computer-implementable instructions for a transport protocol module 145 (such as a TCP protocol module or a TCP layer of a network stack) to be implemented on a processor of network device 110. In some other embodiments, the memory 115 may store computer-implementable instructions for the network interface driver 120. The memory 115 may store data and/or instructions related to the operation and use of the network interface driver 120. The memory 115 may include, for example, Random Access Memory (RAM), Dynamic Random Access Memory (DRAM), Static Random Access Memory (SRAM), Synchronous Dynamic Random Access Memory (SDRAM), Ferroelectric Random Access Memory (FRAM), Read Only Memory (ROM), Programmable Read Only Memory (PROM), Erasable Programmable Read Only Memory (EPROM), Electrically Erasable Programmable Read Only Memory (EEPROM), and/or flash memory.
The functionality described above as occurring within the TCP layer of a network device may additionally or alternatively be implemented in another network protocol module within the transport layer, network layer, or combined transport/network layer of a network protocol stack. For example, the functionality may be implemented in a User Datagram Protocol (UDP) module, a Reliable Datagram Protocol (RDP) module, a Reliable User Datagram Protocol (RUDP) module, or a Datagram Congestion Control Protocol (DCCP) module. As used herein, the network layer, transport layer, or combined transport/network layer will generally be referred to as the packet layer of the network protocol stack.
The network interface driver 120 may comprise a network interface driver software module running on a real OS. A network interface driver, such as network interface driver 120, may be a set of computer-executable instructions stored in memory 115 that, when executed by a processor, help facilitate network communications. In some other embodiments, the network interface driver 120 may be implemented as logic implemented in a hardware processor or other integrated circuit, or as a combination of hardware and software logic. The network interface driver 120 may communicate directly with one of the software applications 150a through 150c (e.g., application 265 in fig. 2A) via the guest OS of the virtual machine (or, in some embodiments, by the hypervisor and guest OS) if operating in a virtual machine environment or via the container manager of the containerization environment (if operating on the real OS 220 of the network device 110). In some implementations, the network interface driver 120 is included within a first layer of a Transmission Control Protocol (TCP) stack of a real OS of the network device 110 and communicates with software modules or applications included in upper layers of the TCP stack. In one example, the network interface driver 120 is included within the transport layer of the TCP stack and communicates with a software module or application included in the application layer of the TCP stack. In another example, the network interface driver 120 is included within the link layer of the TCP stack and communicates with a TCP/IP module included in the internet/transport layer of the TCP stack. In some embodiments, the functionality is additionally or alternatively configured to receive a packet from another network or transport layer protocol module, such as a User Datagram Protocol (UDP) module, a Reliable Datagram Protocol (RDP) module, a Reliable User Datagram Protocol (RUDP) module, or a Datagram Congestion Control Protocol (DCCP) module. In some other implementations, the network interface driver 120 may be included as part of the network interface card 140.
The network interface card 140 includes a network interface card memory 146. In some other implementations, the memory 146 may store computer-implementable instructions for the network interface card 140. Memory 146 may store data and/or instructions related to the operation and use of network interface card 140. The memory 146 may include, for example, Random Access Memory (RAM), Dynamic Random Access Memory (DRAM), Static Random Access Memory (SRAM), Synchronous Dynamic Random Access Memory (SDRAM), Ferroelectric Random Access Memory (FRAM), Read Only Memory (ROM), Programmable Read Only Memory (PROM), Erasable Programmable Read Only Memory (EPROM), Electrically Erasable Programmable Read Only Memory (EEPROM), and/or flash memory. Additionally or alternatively, memory 146 may store rate limiting algorithms, rate limiting policies, or computer-implementable instructions utilized by scheduler 141 as well as other modules and logic of network interface card 140. In some implementations, memory 146 may store statistics or metrics associated with flows or classes of packets that have been transmitted by network device 110 and/or that have been scheduled for future transmission. For example, the memory 146 may store statistical information or metrics such as previous and upcoming transmission times and historical transmission rates of packets in each packet category to which rate limiting is to be applied. The statistics may also include the number of packets currently in the timing wheel 130 (discussed further below) associated with each category. In some embodiments, the memory 146 stores computer-executable instructions that, when executed by the network interface card 140, cause the network interface card 140 to perform the processing stages shown in fig. 3, described further below.
As mentioned above, the network interface card 140 includes the scheduler 141. A scheduler, such as scheduler 141, may be a set of computer-executable instructions stored, for example, in memory 146 that when executed by a processor result in the implementation of the functionality discussed below. In some other embodiments, scheduler 141 may be implemented as logic implemented in a hardware processor or other integrated circuit, or as a combination of hardware and software logic. In some embodiments, scheduler 141 is utilized to manage the sequence of packet identifiers inserted into and extracted from timing wheel data structure 130. In some embodiments, prior to scheduling or making any decisions, scheduler 141 may: a) copying at least the packet header to the network interface card memory 146; b) preparing descriptors pointing to the packet header and packet payload (which may or may not be copied to the network interface card memory 146); and c) making a scheduling decision and queuing an identifier of the descriptor in the time index data structure 130.
Additionally or alternatively, scheduler 141 may implement known, existing network scheduling algorithms that may be used for different operating system kernels. In some embodiments, scheduler 141 may implement a customized, user-defined scheduling algorithm. For example, scheduler 141 may include a rate limiting policy algorithm capable of calculating a timestamp of a received packet. In some embodiments, scheduler 141 may implement a weighted fair queuing algorithm to ensure that multiple packet flows share bandwidth in proportion to their weights in a min-max fair allocation scheme. Additionally or alternatively, scheduler 141 may consolidate timestamps such that a larger timestamp represents a smaller target transmission rate. In some embodiments, scheduler 141 may store and/or retrieve rate limiting scheduling algorithms from memory 146. Additionally or alternatively, scheduler 141 may evaluate packets received by network interface driver 120 and store packet identifiers in timing wheel data structure 130. In some embodiments, scheduler 141 may evaluate received packet data to determine a transmission timestamp associated with the received packet. Additionally or alternatively, scheduler 141 may determine an updated transmission timestamp for a received packet that already has a timestamp applied by the application, virtual machine, or container that originated the packet, and may apply the updated transmission timestamp to the packet identifier. In some embodiments, scheduler 141 may direct timing wheel data structure 130 to store packet identifiers with transmission timestamps at appropriate time slots in timing wheel data structure 130. Additionally or alternatively, scheduler 141 may direct timing wheel 130 to extract stored packet identifiers, e.g., packet identifiers including transmission time stamps, when the transmission time has been reached. The scheduler 141 is described in more detail below with reference to fig. 4A to 4C.
The network interface card 140 may also include memory allocation logic such as a memory manager 147. The memory manager 147 can enforce limits on the amount of memory space in the memory 146 available to each virtual machine or container executing on the network device 110. (the relationship between virtual machines or containers and applications implemented on network device 110 is described in further detail below with reference to FIGS. 2A and 2B). In some cases, each virtual machine or container may carry out many applications (tens or more). The memory manager 147 can assign portions of the memory 146 to each virtual machine or container. Thus, the memory manager 147 can ensure that no single virtual machine or container can overload the memory 146 resulting in damage to applications executing on other virtual machines or containers.
The network interface card 140 has a configurable amount of memory 146 that may be allocated for pointers (i.e., packet descriptors). A configurable amount of memory 146 for descriptors must be balanced against the capacity requirements for the routing tables and/or the executable code. A configurable amount of memory 146 must then be shared between all producers of packets to be transmitted by the network interface card 140, where the producers may include virtual machines, jobs on the host OS, and the like. Sharing among producers may be oversubscribed (based on the assumption that not all producers will use all of their allocations at the same time). The memory manager 147 may have several configurable limits for each producer, e.g., different limits for packets having different quality of service (QoS) flags. For example, memory 146 may store up to 10 ten thousand packets, with a capacity equivalent to 2 ten thousand packets being allocated to each of five producers. For each producer, memory manager 147 may allow the producer to fill all 2-ten-thousand packet capacities with high priority packets, but limit the upper limit of low priority packets to 1-ten-thousand per producer. In this example, it is also possible to allocate 2 ten thousand packets of capacity to each of the six producers, resulting in a capacity equivalent to 2 ten thousand packets being oversubscribed. But unless the producers of the combination each maximize their allocation using high priority packets, the producers will not maximize the total allocated memory 146.
In some embodiments, memory manager 147 may allocate memory based on one or more of a variety of other factors or calculations. For example, memory manager 147 can allocate space in memory 146 based on historical usage of memory 146 by certain virtual machines or containers. If a virtual machine or container typically uses up to, for example, 10MB of space in memory 146, memory manager 147 can allocate 10MB or 10MB plus 1MB, 2MB, 5MB, or 10MB, etc. of additional margin for that virtual machine or container. In another example, memory 147 may allocate space in memory 146 based on a prediction of how much capacity a given virtual machine or container will require. Some virtual machines or containers may use more capacity during different times of the day or week; thus, the memory manager 147 can employ a dynamic allocation scheme that changes memory allocation according to predictable changes in usage. In some embodiments, the memory allocation by the memory manager 147 may be user configurable.
In an exemplary operation, when network interface card 140 receives notification of a new communication originating from a source virtualized computing environment (e.g., a virtual machine or container), scheduler 141 may determine whether any capacity is available in memory 146 for the source based on allocations made by memory manager 147. If no capacity is available in memory 146 for the source, network interface card 140 may drop the packet. Scheduler 141 may drop packets through a variety of mechanisms. For example, scheduler 141 may simply ignore the notification; however, this mechanism is typically only used if scheduler 141 implements a data rate cap on application 150 (such as a 20Gbps cap). This mechanism will result in traffic shaping in a more arbitrary manner that may not be compatible with the fine-grained traffic shaping disclosed herein. The second mechanism for dropping packets may include: if the application 150 has no quota remaining for the packet transmission, a notification is acknowledged, the packet is dropped, and a completion message is sent back to the application 150. The completion message will be further discussed below with respect to transfer completion logic 144. A third mechanism involves slowing down the speed of the packet transmitter (i.e., application 150) rather than dropping the packet altogether. If scheduler 141 determines that there is capacity available in memory 146 for the source and that application 150 has some remaining quota for transmission, scheduler 141 may acknowledge the notification, queue the packet to timing wheel 130, transmit the packet at the scheduled time, and deliver a delayed completion message to application 150.
As mentioned above and as shown in fig. 1, network interface card 140 includes timing wheel data structure 130 (also referred to as timing wheel 130). The timing wheel data structure is a time index queue that can be implemented as a circular buffer for queuing objects at a given time in O (1) and fetching objects to be processed at a particular time in O (1). The temporal complexity of the algorithm can be estimated from the number of basic operations performed by the algorithm. This estimate may be expressed in the form of O (n). For example, if the value of the run time t (n) is bounded by a value that is independent of the input size, the algorithm may have a constant time (e.g., o (n), where n ═ 1). As described above, accessing a single element (e.g., a group identifier) in a timing wheel data structure requires a constant time (e.g., O (1)) because only one operation needs to be performed to locate the element. In some embodiments, timing wheel data structure 130 may store the packet identifier provided by scheduler 141 in a time slot associated with a timestamp specified by application 150 generating the packet or according to an updated transmission timestamp determined by scheduler 141. When the current time reaches or exceeds the timestamp, the timing wheel 130 may push the packet identifier to the packet builder 142 for packet generation. In some implementations, another component, such as the packet builder 142 or scheduler 141, periodically polls the timing wheel 130 and extracts packet identifiers whose timestamps have reached or exceeded the current time. The timing wheel data structure 130 and its operation are described in more detail below with reference to fig. 4A through 4C. In some other embodiments, instead of a timing wheel, a different time index data structure, such as a calendar queue, may be used to schedule transmission of packets.
The network interface card 140 includes a packet builder 142. The packet builder 142 includes packet generation logic that may generate data packets using a set of pointers associated with communications stored in memory 146. Each pointer may be a network interface card descriptor. The descriptor may describe the packet; for example by specifying the memory location and the length of the packet data. In some embodiments, the packet data may be in host memory 115. In some embodiments, the packet data may be in the network interface card memory 146. Packet builder 142 may build packets for transmission based on the identifiers in timing wheel 130.
An exemplary operation of the packet builder 142 is as follows. When the current time reaches or exceeds the time slot associated with the timestamp of the packet identifier, the timing wheel 130 can push the packet identifier to the packet builder 142 for packet generation and transmission. In some embodiments, packet builder 142 may be configured to query timing wheel 130 to determine whether the times indexed in timing wheel 130 have been reached, and to extract the appropriate packet identifiers from timing wheel 130 based on determining that the transmission times they have been indexed in timing wheel 130 have been reached. When the time index data structure 130 reaches a time corresponding to the transmission time of the identifier in the time index data structure, the timing wheel 130 can push the packet identifier to the packet builder 142, which packet builder 142 can use the packet identifier to generate a packet for transmission. The identifier may be, for example, a pointer to a region in memory 146 containing a set of pointers associated with the packet identifier. The set of pointers may point to memory locations on network interface card 140 or network device 110 that each store portions of data that make up the packet associated with the identifier. In some implementations, the pointer can indicate a memory address assigned by and accessible by the real OS of network device 110. In some implementations, the pointer can indicate a memory address assigned by and accessible by a virtual machine hypervisor or container manager. In some implementations, the set of pointers can take the form of a list of supersets. Each pointer in the set of pointers may point to an area in memory 115 that contains the appropriate field, component, or payload portion of the packet. Packet builder 142 may thus build a packet header and payload based on the set of pointers. In some embodiments, packet builder 142 may directly access host memory 115 using, for example, Direct Memory Access (DMA). By maintaining only a single identifier for each packet or communication in the time index data structure 130, the size of the time index data structure may be minimized.
The network interface card 140 also includes a transceiver 143. Transceiver 143 comprises hardware configured to send and receive communications to and from network node 750. In some embodiments, network interface card 140 may be capable of supporting high-speed data reception and transmission in fiber channels, for example, where the data frame rate may be close to 100 gigabits per second, as desired. In some implementations, the network interface card 140 may be configured to support lower speed communications, such as over copper (or other metal) wires, wireless channels, or other communications media.
FIG. 2A illustrates a block diagram of an exemplary server 200a implementing a virtual machine environment. In some embodiments, server 200a includes hardware 210, a real Operating System (OS)220 running on hardware 210, a hypervisor 250, and two virtual machines with guest operating systems (guest OSs) 260 and 270. Hardware 210 may include a Network Interface Card (NIC)215, among other components. Hardware 210 may have a configuration similar to that of computing system 1010 shown in fig. 5. The NIC 215 of the hardware 210 may have a configuration similar to that of the network interface card 140 as shown in fig. 1. In some embodiments, the real OS 220 has a protocol stack 225 (e.g., a TCP stack) or transport protocol module 145 as shown in fig. 1. In some implementations, the real OS 220 includes a software application running on the real OS 220. In some embodiments, guest OSs 260 and 270 include protocol stacks 261 and 271, respectively. Each of guest OSs 260 and 270 may host a variety of applications, such as software applications 265, 266, 275, and 276. The server 200a may be a file server, an application server, a web server, a proxy server, an appliance, a network appliance, a gateway server, a virtualization server, a deployment server, an SSL VPN server, or a firewall.
Referring again to fig. 2A, server 200a implements hypervisor 250, which hypervisor 250 instantiates and manages a first guest OS 260 on virtual machine 1 and a second guest OS 270 on virtual machine 2, respectively. The first guest OS 260 configured on the virtual machine 1 hosts a first software application 265 and a second software application 266. The second guest OS 260 configured on the virtual machine 2 hosts a third software application 275 and a fourth software application 276. For example, applications may include database servers, data warehouse programs, stock market trading software, online banking applications, content distribution and management systems, hosted video games, hosted desktops, email servers, travel reservation systems, customer relationship management applications, inventory control management databases, and enterprise resource management systems. In some implementations, the guest OS hosts other kinds of applications.
FIG. 2B illustrates a block diagram of an exemplary server 200B implementing a containerization environment. In some embodiments, server 200b includes hardware 210, a real Operating System (OS)220 running on hardware 210, a container manager 240, two containerization environments (e.g., container 1 and container 2) that execute applications 241 and 242, respectively. Hardware 210 may include, among other components, a Network Interface Card (NIC) 215. The hardware 210 may have a similar configuration as the computing system 1010 shown in fig. 5. The NIC 215 of the hardware 210 may have a configuration similar to that of the network interface card 140 shown in fig. 1. In some embodiments, real OS 220 has a protocol stack 225 (e.g., a TCP stack) and has software applications running on real OS 220. Each of the containers (e.g., container 1 and container 2) may host various applications, such as software applications 241 and 242. Server 200b may be a file server, an application server, a Web server, a proxy server, an appliance, a network appliance, a gateway server, a virtualization server, a deployment server, an SSL VPN server, or a firewall.
Referring again to FIG. 2B, the server 200B implements a container manager 240, which container manager 240 instantiates and manages container 1 and container 2, respectively. The container 1 hosts a software application 241. The container 2 hosts a software application 242. For example, the applications may include database servers, data warehouse programs, stock market trading software, online banking applications, content distribution and management systems, hosted video games, hosted desktops, email servers, travel reservation systems, customer relationship management applications, inventory control management databases, and enterprise resource management systems. In some embodiments, a container (e.g., container 1 or container 2) may host other kinds of applications. In some implementations, each container can host multiple applications simultaneously.
Fig. 3 is a flow diagram of network traffic shaping using an exemplary method 300 performed by a network interface card, such as network interface card 140 shown in fig. 1. Method 300 includes receiving a notification of a new communication originating from a source virtualized computing environment (stage 310). Method 300 includes determining whether to drop the packet or schedule transmission of a new communication (stage 320). Method 300 includes storing an identifier associated with the communication in a time-indexed data structure and storing a set of pointers associated with the communication in a network interface card memory (stage 330). Method 300 includes generating a data packet using a set of pointers stored in a network interface card memory (stage 340). Method 300 includes transmitting the generated packet (stage 350). Method 300 includes transmitting a transmission completion notification back to the application (stage 360).
Also in stage 330, the scheduler may store a set of pointers associated with the communication in the network interface card memory upon determining that the source has available capacity allocated to it in memory.
In some embodiments, the scheduler may process the received data packets based on a rate limiting algorithm or policy stored in memory to determine a transmission time for each packet. For example, the scheduler may process packets and apply transmission timestamps according to a rate limiting algorithm or policy associated with a particular class of packets. In some embodiments, the scheduler is configured to determine a transmission time for each packet based on a rate pacing policy or a target rate limit. For example, the scheduler may determine a transmission time for each packet based on a rate pacing policy, such as a packet class rate policy and/or an aggregate rate policy. In some embodiments, the scheduler may determine the transmission time to process multiple packet flows based on a rate walking policy, such as a weighted fair queuing policy. Additionally or alternatively, each packet may have a transmission timestamp requested by the application that generated the packet. In some implementations, the scheduler can receive a packet that includes a transmission time of a request assigned to the packet by one of the plurality of applications before being received at the TCP layer and before being processed by the scheduler. The scheduler may process the packets in substantially real time to determine an updated transmission time based on the at least one rate limiting policy being exceeded and invoking a rate limiting algorithm associated with the packets. For example, if a received packet is processed and the scheduler determines that the transmission time of the packet will exceed the rate limit for a packet class, the scheduler may update the transmission time with an adjusted transmission timestamp that enables the packet to be transmitted at a later time to avoid exceeding the rate limit defined by the rate limit policy for the particular packet class. The scheduler may be configured to find the associated rate limiting algorithm via a hash table or a mapping that identifies the rate limiting algorithm associated with the received packet.
In some implementations, the scheduler may determine that a particular transmission time associated with a packet identifier stored in the time index data structure 130 has been reached. The scheduler may query the time index data structure 130 with the current time to determine if there are any packets to be transmitted. For example, the scheduler may use the current CPU clock time (or some other reference time value, such as a regularly incremented integer value) to query the data structure. Frequent polling may provide greater consistency with packet scheduling and rate limiting strategies and reduced overhead than using a separate timer, which may cause significant CPU overhead due to interrupts. In some embodiments, the time index data structure 130 may be implemented on a dedicated CPU core. Additionally or alternatively, the time index data structure 130 may be implemented on an interrupt-based system that may perform polling of data structures at constant intervals to determine packet transmission schedules. For example, the time index data structure 130 may be periodically polled with a period equal to or a multiple of the length of time associated with each time slot. In some embodiments, polling of the timing wheel may be performed by logic distinct from the scheduler, such as a packet builder similar to packet builder 142.
In some implementations, the method 300 may prevent one of the applications from sending additional packets for transmission until the application receives a transmission completion notification. In some embodiments, traffic shaping may be achieved in part by rate limiting the forwarding of additional data packets by the application to the TCP layer until a message is received indicating that the packet transmission has been completed. For example, network interface card 140 (as shown in fig. 1 and described in more detail later in fig. 6A-6B) may generate a completion notification back to application 150 indicating that the packet has been transmitted over the network. This transmission completion notification provides a feedback mechanism to the application and limits the forwarding of additional packets by the application 150 to the TCP layer. This mechanism can be used in conjunction with existing TCP functionality, such as acting as a small queue of TCP that effectively limits the number of bytes that can be significant between the sender and receiver.
The functionality described above as occurring within the TCP layer of a network device may additionally or alternatively be implemented in another network protocol module within the transport layer, network layer, or combined transport/network layer of a network protocol stack. For example, the functionality may be implemented in a User Datagram Protocol (UDP) module, a Reliable Datagram Protocol (RDP) module, a Reliable User Datagram Protocol (RUDP) module, or a Datagram Congestion Control Protocol (DCCP) module.
Fig. 4A-4C are block diagrams representing exemplary operations for shaping network traffic using a scheduler, a time-indexed data structure, memory, a memory manager, and a packet builder as performed by a network interface card, such as network interface card 140. In summary, as shown in fig. 4A, the network interface card 140 receives notification of new communications from applications 150 (e.g., applications 150a, 150b, and 150c) via the network driver 120. A notification, such as the notification labeled REQ: a1, may include a header (or portions of a header) and a set of pointers corresponding to memory locations in host memory that store additional data for the packet header and for the packet payload. Network interface card 140 includes one or more memory devices 146 for storing instructions and data, and allocation logic such as memory manager 147 for allocating portions of memory 146 to different virtual machines, containers, or applications 150. Shown in FIGS. 4A to 4CIn the exemplary operation of (a), each application 150 is considered to be carried out in a different virtual machine or container for simplicity. The network interface card 140 includes a scheduler 141 to process packets according to a rate limiting algorithm or policy stored in memory 146. Network interface card 140 also includes a time index data structure 130, also referred to as a timing wheel 130, to store packet identifiers, such as the packet identifier labeled as ID: a1, according to their transmission times. Network interface card 140 also includes one or more packet builders 142 to generate packets based on packet identifiers and associated sets of pointers. If network interface card 140 receives notification of a new communication from an application, scheduler 141 may determine whether the memory allocation in memory 146 associated with a particular virtual machine or container has capacity to handle the set of pointers associated with the packet. If the memory allocation is sufficient to store the set of pointers, scheduler 141 may store the packet identifiers in timing wheel 130 and the associated set of pointers in memory 146. If the memory allocation lacks capacity to store the set of pointers, network interface card 140 may ignore the notification, essentially dropping the packet requested for transmission. When the transmission time for indexing the packet identifier has been reached (e.g., at time)Now it is420) The timing wheel 130 may push the packet identifier to the packet builder 142 to generate the packet. Fig. 4A to 4C illustrate these operations.
Referring to fig. 4A, the network interface card 140 receives a notification REQ of a new communication a1 from the application 150a at its scheduler 141. Notification REQ: a1 corresponds to a data packet for transmission from application 150a via network interface card 140. Notification REQ a1 may include a header or header data and a set of pointers. The set of pointers may point to a memory area in host memory that stores data for the packet header and/or payload. At network interface card 140, a set of pointers (A1 in this case) is associated with an identifier (ID: A1 in this case). Thus, the identifier is associated with: a notification of a received new communication; a packet for transmission; and a set of pointers to data that will ultimately be used by the network interface card 140 to generate packets.
The memory manager 147 has allocated space in the memory 146 for each virtual machine or container. In fig. 4A-4C, to illustrate the operation of the network interface card 140 and its components, each application 150 is considered to be carried out in a separate virtual machine or container. In fig. 4A, the memory manager 147 has allocated space in memory 146 for three sets of pointers from each of the virtual machines hosting applications 150a and 150b, respectively. In practice, the memory allocation for a virtual machine may be a single set of pointers or many sets of pointers. In some implementations, memory space may be allocated in bytes, kilobytes, or megabytes rather than by the number of sets of pointers, and the size of each set of pointers may be different such that the number of sets of pointers that fit into a particular memory allocation depends on the total size of the set of pointers.
As shown in fig. 4A, the scheduler 141 of the network interface card 140 receives the notification REQ: a1 from the application 150a, and checks the available capacity of the memory 146. In the example shown in fig. 4A, the memory 146 has available capacity allocated to the virtual machine that is executing the application 150 a. Thus, the scheduler adds the packet identifier associated with the notification to the timing wheel 130 and adds the set of pointers associated with the packet to the memory 146.
As further shown in fig. 4A, scheduler 141 processes the received notification to determine the transmission time of each packet. Scheduler 141 may determine the transmission time of each packet by identifying a rate limiting algorithm or policy associated with the packet and assigning an initial or updated transmission time to an identifier associated with the packet. Scheduler 141 may retrieve a rate limiting algorithm or policy from memory 146 to determine an initial or updated transmission time for each packet. Scheduler 141 may identify the received notification as belonging to a particular class of packets. A particular class of packet may require a particular rate limiting algorithm or policy associated with the packet class. Scheduler 141 may utilize a particular rate limiting algorithm or policy to determine the initial or updated transmission time for each packet of the class. In some embodiments, scheduler 141 may evaluate the packet transmission times requested by application 150 and determine whether the requested transmission times exceed a rate limiting algorithm or policy associated with the packet class (e.g., whether transmission at the requested time would result in a transmission rate for the packet class that is too high, taking into account the transmission history of other recently transmitted packets or packets in the class that have been scheduled for future transmission). Scheduler 141 may process the notification and determine that the transmission time requested by application 150 violates a rate limit or policy associated with the packet class. If the rate limit or policy is exceeded or otherwise violated, scheduler 141 may determine an updated transmission time that does not exceed or violate the rate limit or policy for each packet. Scheduler 141 may determine that the requested or updated transmission time is the current time and may carry out instructions to immediately forward the identifier to packet builder 142 to generate a packet corresponding to the identifier.
As shown in fig. 4A, scheduler 141 stores the identifier ID a1 associated with the packet at a location in timing wheel 130 associated with the transmission time determined for the packet. Timing wheel 130 may be a time-indexed data structure or queue that is capable of storing and extracting packet identifiers based on a determined transmission time of an associated packet. In some implementations, each time slot in the timing wheel 130 stores a single data element or event (e.g., an identifier associated with a packet). In some embodiments, each time slot may store multiple data elements or events. The timing wheel 130 may include a preconfigured number of time slots or positions, and each time slot or position may represent a particular time increment. In some embodiments, the number of time slots or positions may be dynamically adjusted based on the changing level of data traffic and congestion at the network interface card 140. Timing wheel 130 may include any number of time slots or positions, where each time slot is defined as an amount needed to adequately process traffic to be shaped. The sum of all time slots or positions in the timing wheel 130 represents the time frame or forward queuing time frame that the timing wheel 130 can support. Suitable time ranges and timing wheel granularity (e.g., number of time slots) may be configured based on the rate limiting policy to be implemented. For example, to implement a rate of 1 megabit per second (Mb), a suitable time range would be 12 milliseconds. A suitable number of time slots or positions of the timing wheel 130 may be in the range of 10 to 1,000,000 time slots or positions. A suitable time range for the timing wheel 130 may be in the range of 10 microseconds to 1 second. For example, as shown in fig. 4A, the timing wheel 130 has 10 time slots, and each time slot may represent 2 microseconds. Thus, the time range of the example timing wheel 130 shown in fig. 4A is 20 microseconds, and the granularity of the timing wheel 130 is 2 microseconds. In some embodiments, the timing wheel 130 will have a maximum time range over which no packet identifier will be scheduled. The timing wheel 130 may not need to store packet identifiers having earlier time stamps than now because packets having earlier transmission times than now should be transmitted immediately. Once a time slot in the timing wheel 130 becomes earlier than now, the elements in that time slot may be dequeued and ready for transmission.
For example, as shown in fig. 4A, assume that the scheduler 141 has processed a packet associated with packet identifier ID a 1. The set of pointers associated with identifier a1 may be retained in memory 146 and scheduler 141 stores identifier ID a1 in timing wheel 130 at a location associated with the determined transmission time for the packet associated with identifier ID a 1. A1 includes a transmission time t determined by scheduler 1410. Having packet identifier ID A1 corresponding to transmission time t0Is inserted into the timing wheel 130. The timing wheel 130 stores the packet identifier ID a1 until it is determined that the transmission time determined for the packet associated with identifier a1 has been reached. In some embodiments, time is elapsedNow it isBecomes t0The timing wheel 130 may push the packet identifier ID a1 to the packet builder. In some embodiments, scheduler 141 or packet builder 142 may query time index data structure 130 with the current time to determine if there are any packets to transmit. For example, the packet builder 142 may poll the data structure with the CPU clock time (or some other value representing the current time, such as a regularly incremented integer). The packet builder 142 may determine that the transmission time identified in packet identifier ID A1 has been reached, and therefore use the set of pointers A1 to generateA packet for transmission.
As shown in fig. 4B, the scheduler 141 processes the next notification. In this case, the scheduler receives the notification REQ: B4 from the application 150B. The scheduler checks the available capacity in the space in memory 146 allocated to the virtual machine or container that implements application 150 b. In the example shown in fig. 4A, memory 146 lacks capacity for the set of pointers associated with notification REQ: B4. Thus, scheduler 141 discards the notification without having to queue the packet identifier in timing wheel 130 or store the associated set of pointers in memory 146.
As shown in FIG. 4C, the current time has reached or exceeded t0T of the0Is the time at which packet a1 is scheduled for transmission. Thus, the timing wheel 130 pushes the packet identifier ID a1 to the packet builder 142 for packet generation and transmission. The packet builder 142 retrieves the set of pointers a1 from the memory 146 using the packet identifier ID a 1. The packet builder 142 retrieves packet header data and payload data from host memory using the set of pointers a1 to generate a packet PKT a 1. The packet builder 142 may then send the packet PKT: a1 to the transceiver for transmission.
After transmission of the packet, transmission completion logic 144 may determine whether the network interface card successfully transmitted the packet associated with the identifier stored in the time index data structure at the location associated with the time of arrival. If transmission completion logic 144 determines that the packet associated with the identifier stored in the time index data structure at the location associated with the time of arrival was successfully transmitted, transmission completion logic 144 may transmit a transmission completion notification to application 150, which application 150 has awaited receipt of the transmission completion notification from network interface card 140 before forwarding additional data packets to the network interface card. Exemplary operation of transfer completion logic 144 is described in more detail below.
In response to transmission completion logic 144 determining that the packet associated with the identifier stored at the location in the time index data structure associated with the arrival time has been successfully transmitted, transmission completion logic 144 may transmit a transmission completion notification to application 150, which application 150 has awaited receipt of the transmission completion notification from the network interface card before forwarding additional data packets to the network interface card. In some implementations, each of the applications 150 may be configured to wait to receive a transmission completion notification from the network interface card 140 before forwarding additional packets to the network interface card. In some embodiments, each of the applications 150 may be configured to wait to receive a transmission completion message from the network interface card for a particular class of packets before forwarding additional packets of the same class to the network interface card.
FIG. 5 is a block diagram illustrating a general architecture of a computer system 1000 that may be used to implement the elements of the systems and methods described and illustrated herein, in accordance with an illustrative embodiment.
In general, computing system 1010 includes at least one processor 1050 for performing actions in accordance with instructions; and one or more storage devices 1070 or 1075, the one or more storage devices 1070 or 1075 to store instructions and data. The illustrated example computing system 1010 includes one or more processors 1050, the one or more processors 1050 communicating via a bus 1015 with at least one network interface driver controller 1020 having one or more network interface cards 1022 connected to one or more network devices 1024, a memory 1070, and any other devices 1080; such as an I/O interface. The network interface card 1022 may have one or more network interface driver ports to communicate with connected devices or components. Typically, the processor 1050 will execute instructions received from memory. The illustrated processor 1050 incorporates, or is directly connected to, cache memory 1075.
In more detail, the processor 1050 may be any logic circuitry that processes instructions fetched, for example, from the memory 1070 or cache 1075. In many embodiments, the processor 1050 is a microprocessor unit or a special purpose processor. Computing device 1000 may be based on any processor or collection of processors capable of operating as described herein. Processor 1050 can be a single core or multi-core processor. The processor 1050 can be a plurality of processors. In some embodiments, the processor 1050 may be configured to run multi-threaded operations. In some implementations, the processor 1050 can host one or more virtual machines or containers and a hypervisor or container manager for managing the operation of the virtual machines or containers. In such embodiments, the methods illustrated in fig. 3 and 5 may be implemented within a virtualized or containerized environment provided on the processor 1050.
Memory 1070 may be any device suitable for storing computer-readable data. The memory 1070 may be a device with fixed storage or a device for reading removable storage media. Examples include all forms of non-volatile memory, media and memory devices, semiconductor memory devices (e.g., EPROM, EEPROM, SDRAM, and flash memory devices), magnetic disks, magneto-optical disks, and optical disks (e.g., CD ROM, DVD-ROM, and Blu-ray)Computing system 1000 may have any number of memory devices 1070. In some embodiments, memory 1070 supports virtualized or containerized memory that is accessible by a virtual machine or container execution environment provided by computing system 1010.
The cache memory 1075 is typically a form of computer memory that is placed in close proximity to the processor 1050 for fast reads. In some embodiments, the cache memory 1075 is part of the processor 1050 or on the same chip as the processor 1050. In some embodiments, there are multiple levels of cache 1075, e.g., the L2 and L3 cache levels.
The network interface driver controller 1020 manages the exchange of data via the network interface driver 1022 (also referred to as a network interface driver port). Network interface driver controller 1020 handles the physical and data link layers of the OSI model for network communications. In some embodiments, some of the tasks of the network interface driver controller are processed by the processor 1050. In some embodiments, the network interface driver controller 1020 is part of the processor 1050. In some implementations, the computing system 1010 has multiple network interface driver controllers 1020. The network interface driver port configured in network interface card 1022 is the connection point for the physical network link. In some embodiments, the network interface controller 1020 supports wireless network connections and the interface port associated with the network interface card 1022 is a wireless receiver/transmitter. In general, the computing device 1010 exchanges data with other network devices 1024 via a physical or wireless link that interfaces with a network interface driver port configured in the network interface card 1022. In some embodiments, the network interface controller 1020 implements a network protocol such as Ethernet.
Other devices 1080 may include I/O interfaces, external serial device ports, and any additional coprocessors. For example, computing system 1010 may include an interface (e.g., a Universal Serial Bus (USB) interface) for connecting input devices (e.g., a keyboard, microphone, mouse, or other pointing device), output devices (e.g., a video display, speakers, or printer), or other storage devices (e.g., a portable flash drive or external media drive). In some implementations, the computing device 1000 includes additional devices 1080 such as coprocessors, for example, mathematical coprocessors may assist the processor 1050 in high-precision or complex calculations.
Implementations of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software embodied in tangible media, firmware, or hardware, including the structures disclosed in this specification and their equivalents, or in combinations of one or more of them. Implementations of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on one or more computer storage media for execution by, or to control the operation of, data processing apparatus. The computer storage media may be or be included in a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more thereof. The computer storage media may also be or be included in one or more separate components or media (e.g., multiple CDs, disks, or other storage devices). The computer storage medium may be tangible and non-transitory.
The operations described in this specification can be implemented as operations performed by data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources. Operations may be performed within a native environment of the data processing apparatus or within one or more virtual machines or containers hosted by the data processing apparatus.
A computer program (also known as a program, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment. A computer program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers or on one or more virtual machines or containers that are located at one site or distributed across multiple sites and interconnected by a communication network. Examples of communication networks include a local area network ("LAN") and a wide area network ("WAN"), the Internet (e.g., the Internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or of what may be claimed, but rather as descriptions of features specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the implementations described above should not be understood as requiring such separation in all implementations, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
References to "or" may be construed as inclusive such that any term described using "or" may refer to any single, more than one, or all of the described terms. The labels "first", "second", "third", etc. do not necessarily indicate an order and are generally only used to distinguish between similar items or elements.
Various modifications to the embodiments described in this disclosure will be readily apparent to those skilled in the art, and the generic principles defined herein may be applied to other embodiments without departing from the spirit or scope of the disclosure. Thus, the claims are not intended to be limited to the embodiments shown herein but are to be accorded the widest scope consistent with the present disclosure, principles and novel features disclosed herein.
Claims (20)
1. A network interface card, comprising:
a network interface card memory configured to store:
a time index data structure storing identifiers associated with respective communications to be transmitted by a network interface card;
a set of pointers, wherein:
each set of pointers corresponds to one of the identifiers stored in the time index data structure; and is
Each pointer in each set of pointers points to a location in the network interface card memory or a location in a host memory of a host computing device to which the network interface device is coupled;
scheduling logic configured to:
receiving a notification of a new communication to be transmitted by the network interface card, the new communication originating from a source virtualized computing environment of a plurality of virtualized computing environments executing on the host computing device, wherein the notification includes header data and a set of pointers to data to be included in the communication;
determining whether to drop the packet or schedule transmission of the new communication based on available capacity in the network interface card memory allocated to the source virtualized computing environment; and
in response to determining to schedule transmission of the communication, storing an identifier associated with the communication in the time index data structure at a scheduled transmission time and storing the set of pointers associated with the communication in the network interface card memory;
packet generation logic configured to:
generating a data packet using the set of pointers stored in the network interface card memory associated with the communication upon arrival of a scheduled transmission time for the communication for which an identifier is stored in the time index data structure;
a transceiver configured to transmit the generated data packet; and
transmission completion logic configured to generate a transmission completion message to be transmitted to an application executing in the source virtualized computing environment that initiated the communication upon completion of transmission of a data packet generated for the communication.
2. The network interface card of claim 1, wherein the set of pointers is stored as a list of supersets.
3. The network interface card of claim 1, further comprising network interface card memory allocation logic configured to allocate portions of the network interface card memory among the plurality of virtualized computing environments.
4. The network interface card of claim 1, wherein the scheduling logic is further configured to calculate the scheduled transmission time for communication based on a traffic shaping policy stored in the network interface card memory.
5. The network interface card of claim 4, wherein the scheduling logic is configured to calculate the scheduled transmission time based on the header information associated with the communication.
6. A network interface card according to claim 4, wherein the traffic shaping policies comprise packet class rate policies and/or aggregate rate policies.
7. The network interface card of claim 1, wherein the source virtualized computing environment refrains from requesting additional packet transfers until receiving notification that a previously requested packet transfer is complete.
8. A method of network traffic shaping, comprising:
receiving, at a network interface card of a host computing device, a notification of a new communication to be transmitted by the network interface card, the new communication originating from a source virtualized computing environment of a plurality of virtualized computing environments executing on the host computing device, wherein the notification includes header data and a set of pointers to data to be included in the communication;
determining whether to drop the packet or schedule transmission of the new communication based on available capacity in a network interface card memory space allocated to the source virtualized computing environment;
in response to determining to schedule transmission of the communication:
storing an identifier associated with the communication in a time index data structure at a scheduled transmission time; and is
Storing a set of pointers associated with the communication in the network interface card memory, wherein the set of pointers corresponds to the identifiers stored in the time-indexed data structure, and each pointer in the set of pointers points to a location in the network interface card memory or a location in a host memory of the host computing device;
generating a data packet using the set of pointers stored in the network interface card memory associated with the communication upon arrival of a scheduled transmission time of the communication for which the identifier is stored in the time index data structure;
transmitting the generated data packet; and
upon completion of transmission of a data packet generated for a communication, a transmission completion message is generated to be transmitted to an application executing in the source virtualized computing environment that initiated the communication.
9. The method of claim 8, comprising storing the set of pointers as a list of supersets.
10. The method of claim 8, comprising allocating portions of the network interface card memory among the plurality of virtualized computing environments.
11. The method of claim 8, comprising calculating the scheduled transmission time for communication based on a traffic shaping policy stored in the network interface card memory.
12. The method of claim 11, comprising calculating the scheduled transmission time based on the header information associated with the communication.
13. The method of claim 11, wherein the traffic shaping policy comprises a packet class rate policy and/or an aggregate rate policy.
14. The method of claim 8, wherein the source virtualized computing environment refrains from requesting additional packet transfers until receiving notification that a previously requested packet transfer is complete.
15. A non-transitory computer-readable medium having instructions stored thereon, the non-transitory computer-readable medium configured to cause one or more processors of a network interface card of a host computing device to perform a method of network traffic shaping, the method comprising:
receiving a notification of a new communication to be transmitted by the network interface card, the new communication originating from a source virtualized computing environment of a plurality of virtualized computing environments executing on the host computing device, wherein the notification includes header data and a set of pointers to data to be included in the communication;
determining whether to drop the packet or schedule transmission of the new communication based on available capacity in a network interface card memory space allocated to the source virtualized computing environment;
in response to determining to schedule transmission of the communication:
storing an identifier associated with the communication in a time index data structure at a scheduled transmission time; and is
Storing a set of pointers associated with the communication in the network interface card memory, wherein the set of pointers corresponds to the identifiers stored in the time-indexed data structure, and each pointer in the set of pointers points to a location in the network interface card memory or a location in a host memory of the host computing device;
generating a data packet using the set of pointers stored in the network interface card memory associated with the communication upon arrival of a scheduled transmission time of the communication for which the identifier is stored in the time index data structure;
upon completion of transmission of a data packet generated for a communication, a transmission completion message is generated to be transmitted to an application executing in the source virtualized computing environment that initiated the communication.
16. The computer-readable medium of claim 15, wherein the method comprises storing the set of pointers as a list of supersets.
17. The computer-readable medium of claim 15, wherein the method comprises allocating portions of the network interface card memory among the plurality of virtualized computing environments.
18. The computer-readable medium of claim 15, wherein the method comprises calculating the scheduled transmission time for communication based on a traffic shaping policy stored in the network interface card memory.
19. The computer-readable medium of claim 18, wherein the method comprises calculating the scheduled transmission time based on the header information associated with the communication.
20. The computer-readable medium of claim 18, wherein the traffic shaping policy comprises a packet class rate policy and/or an aggregate rate policy.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US16/146,373 | 2018-09-28 | ||
US16/146,373 US11258714B1 (en) | 2018-09-28 | 2018-09-28 | Fine grain traffic shaping offload for a network interface card |
PCT/US2019/052560 WO2020068725A1 (en) | 2018-09-28 | 2019-09-24 | Fine grain traffic shaping offload for a network interface card |
Publications (2)
Publication Number | Publication Date |
---|---|
CN112041826A true CN112041826A (en) | 2020-12-04 |
CN112041826B CN112041826B (en) | 2024-03-29 |
Family
ID=68165752
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980026359.7A Active CN112041826B (en) | 2018-09-28 | 2019-09-24 | Fine-grained traffic shaping offload for network interface cards |
Country Status (6)
Country | Link |
---|---|
US (2) | US11258714B1 (en) |
EP (2) | EP3776226B1 (en) |
CN (1) | CN112041826B (en) |
DK (2) | DK4006735T3 (en) |
FI (1) | FI4006735T3 (en) |
WO (1) | WO2020068725A1 (en) |
Families Citing this family (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11909642B2 (en) * | 2020-09-03 | 2024-02-20 | Intel Corporation | Offload of acknowledgements to a network device |
US20220201066A1 (en) * | 2020-12-22 | 2022-06-23 | Microsoft Technology Licensing, Llc | Proactive placement of virtualized computing resources and tuning of network resources based on seasonal variations |
US20230102843A1 (en) * | 2021-09-27 | 2023-03-30 | Nvidia Corporation | User-configurable memory allocation |
US11533362B1 (en) | 2021-12-01 | 2022-12-20 | International Business Machines Corporation | Network interface controller aware placement of virtualized workloads |
Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6377583B1 (en) * | 1996-06-27 | 2002-04-23 | Xerox Corporation | Rate shaping in per-flow output queued routing mechanisms for unspecified bit rate service |
US6956818B1 (en) * | 2000-02-23 | 2005-10-18 | Sun Microsystems, Inc. | Method and apparatus for dynamic class-based packet scheduling |
US7292578B1 (en) * | 2001-06-19 | 2007-11-06 | Cisco Technology, Inc. | Flexible, high performance support for QoS on an arbitrary number of queues |
US9762502B1 (en) * | 2014-05-12 | 2017-09-12 | Google Inc. | Method and system for validating rate-limiter determination made by untrusted software |
CN108337186A (en) * | 2017-01-20 | 2018-07-27 | 谷歌有限责任公司 | Device and method for scalable traffic shaping |
Family Cites Families (25)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5802278A (en) | 1995-05-10 | 1998-09-01 | 3Com Corporation | Bridge/router architecture for high performance scalable networking |
US6501731B1 (en) * | 1998-06-27 | 2002-12-31 | Intel Corporation | CBR/VBR traffic scheduler |
US7106693B1 (en) * | 2000-11-02 | 2006-09-12 | Cisco Technology, Inc. | Method and apparatus for pacing the flow of information sent from a device |
US7934020B1 (en) * | 2003-09-19 | 2011-04-26 | Vmware, Inc. | Managing network data transfers in a virtual computer system |
FI20045222A0 (en) * | 2004-06-15 | 2004-06-15 | Nokia Corp | Network devices and traffic processing procedures |
GB0517304D0 (en) * | 2005-08-23 | 2005-10-05 | Netronome Systems Inc | A system and method for processing and forwarding transmitted information |
US8660137B2 (en) | 2005-09-29 | 2014-02-25 | Broadcom Israel Research, Ltd. | Method and system for quality of service and congestion management for converged network interface devices |
US7813277B2 (en) * | 2007-06-29 | 2010-10-12 | Packeteer, Inc. | Lockless bandwidth management for multiprocessor networking devices |
US7962587B2 (en) * | 2007-12-10 | 2011-06-14 | Oracle America, Inc. | Method and system for enforcing resource constraints for virtual machines across migration |
US8385202B2 (en) * | 2008-08-27 | 2013-02-26 | Cisco Technology, Inc. | Virtual switch quality of service for virtual machines |
US8667187B2 (en) * | 2008-09-15 | 2014-03-04 | Vmware, Inc. | System and method for reducing communication overhead between network interface controllers and virtual machines |
US8174984B2 (en) * | 2009-05-29 | 2012-05-08 | Oracle America, Inc. | Managing traffic on virtualized lanes between a network switch and a virtual machine |
US8499094B2 (en) * | 2010-03-04 | 2013-07-30 | Coraid, Inc. | Modification of small computer system interface commands to exchange data with a networked storage device using AT attachment over ethernet |
US9331963B2 (en) * | 2010-09-24 | 2016-05-03 | Oracle International Corporation | Wireless host I/O using virtualized I/O controllers |
US8856518B2 (en) * | 2011-09-07 | 2014-10-07 | Microsoft Corporation | Secure and efficient offloading of network policies to network interface cards |
JP5962493B2 (en) * | 2012-12-20 | 2016-08-03 | 富士通株式会社 | Program, information processing apparatus, and object transmission method |
US9544239B2 (en) * | 2013-03-14 | 2017-01-10 | Mellanox Technologies, Ltd. | Methods and systems for network congestion management |
US9019826B2 (en) * | 2013-05-07 | 2015-04-28 | Vmare, Inc. | Hierarchical allocation of network bandwidth for quality of service |
US9632901B2 (en) * | 2014-09-11 | 2017-04-25 | Mellanox Technologies, Ltd. | Page resolution status reporting |
US9774540B2 (en) * | 2014-10-29 | 2017-09-26 | Red Hat Israel, Ltd. | Packet drop based dynamic receive priority for network devices |
US10116772B2 (en) * | 2014-11-14 | 2018-10-30 | Cavium, Inc. | Network switching with co-resident data-plane and network interface controllers |
US10097478B2 (en) * | 2015-01-20 | 2018-10-09 | Microsoft Technology Licensing, Llc | Controlling fair bandwidth allocation efficiently |
US9838321B2 (en) * | 2016-03-10 | 2017-12-05 | Google Llc | Systems and method for single queue multi-stream traffic shaping with delayed completions to avoid head of line blocking |
US10687129B2 (en) * | 2016-04-25 | 2020-06-16 | Telefonaktiebolaget Lm Ericsson (Publ) | Data center network |
US20190044832A1 (en) * | 2018-03-16 | 2019-02-07 | Intel Corporation | Technologies for optimized quality of service acceleration |
-
2018
- 2018-09-28 US US16/146,373 patent/US11258714B1/en active Active
-
2019
- 2019-09-24 EP EP19783811.3A patent/EP3776226B1/en active Active
- 2019-09-24 CN CN201980026359.7A patent/CN112041826B/en active Active
- 2019-09-24 DK DK22151467.2T patent/DK4006735T3/en active
- 2019-09-24 FI FIEP22151467.2T patent/FI4006735T3/en active
- 2019-09-24 WO PCT/US2019/052560 patent/WO2020068725A1/en unknown
- 2019-09-24 DK DK19783811.3T patent/DK3776226T3/en active
- 2019-09-24 EP EP22151467.2A patent/EP4006735B1/en active Active
-
2021
- 2021-12-28 US US17/563,551 patent/US11831550B2/en active Active
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6377583B1 (en) * | 1996-06-27 | 2002-04-23 | Xerox Corporation | Rate shaping in per-flow output queued routing mechanisms for unspecified bit rate service |
US6956818B1 (en) * | 2000-02-23 | 2005-10-18 | Sun Microsystems, Inc. | Method and apparatus for dynamic class-based packet scheduling |
US7292578B1 (en) * | 2001-06-19 | 2007-11-06 | Cisco Technology, Inc. | Flexible, high performance support for QoS on an arbitrary number of queues |
US9762502B1 (en) * | 2014-05-12 | 2017-09-12 | Google Inc. | Method and system for validating rate-limiter determination made by untrusted software |
CN108337186A (en) * | 2017-01-20 | 2018-07-27 | 谷歌有限责任公司 | Device and method for scalable traffic shaping |
Also Published As
Publication number | Publication date |
---|---|
US11831550B2 (en) | 2023-11-28 |
EP3776226A1 (en) | 2021-02-17 |
EP3776226B1 (en) | 2022-02-23 |
EP4006735B1 (en) | 2023-11-01 |
WO2020068725A1 (en) | 2020-04-02 |
EP4006735A1 (en) | 2022-06-01 |
US11258714B1 (en) | 2022-02-22 |
DK4006735T3 (en) | 2024-01-29 |
DK3776226T3 (en) | 2022-05-23 |
US20220124039A1 (en) | 2022-04-21 |
FI4006735T3 (en) | 2024-01-30 |
CN112041826B (en) | 2024-03-29 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN108337186B (en) | Apparatus and method for scalable traffic shaping | |
CN112041826B (en) | Fine-grained traffic shaping offload for network interface cards | |
CN108337185B (en) | Apparatus and method for scalable traffic shaping with time-indexed data structure | |
EP2865147B1 (en) | Guarantee of predictable and quantifiable network performance | |
Son et al. | Priority-aware VM allocation and network bandwidth provisioning in software-defined networking (SDN)-enabled clouds | |
US7243351B2 (en) | System and method for task scheduling based upon the classification value and probability | |
US9479457B2 (en) | High-performance, scalable and drop-free data center switch fabric | |
US9294304B2 (en) | Host network accelerator for data center overlay network | |
US9703743B2 (en) | PCIe-based host network accelerators (HNAS) for data center overlay network | |
US8149846B2 (en) | Data processing system and method | |
US9485191B2 (en) | Flow-control within a high-performance, scalable and drop-free data center switch fabric | |
CN107181698B (en) | System and method for single queue multi-stream traffic shaping | |
Shahzad et al. | Reduce VM migration in bandwidth oversubscribed cloud data centres | |
US10097474B1 (en) | Shared rate limiting | |
Susanto et al. | Creek: Inter many-to-many coflows scheduling for datacenter networks | |
Du et al. | R-AQM: Reverse ACK active queue management in multitenant data centers | |
Munir et al. | PASE: synthesizing existing transport strategies for near-optimal data center transport | |
Sun et al. | PACCP: a price-aware congestion control protocol for datacenters | |
Meyer et al. | Low latency packet processing in software routers | |
Wu et al. | Adaptive data transmission in the cloud | |
Shen et al. | Rendering differential performance preference through intelligent network edge in cloud data centers | |
Banerjee et al. | Experience-based efficient scheduling algorithm (EXES) for serving requests in cloud using SDN controller | |
Wang et al. | An Incast-Coflow-Aware Minimum-Rate-Guaranteed Congestion Control Protocol for Datacenter Applications | |
Amaro Jr | Improving Bandwidth Allocation in Cloud Computing Environments via" Bandwidth as a Service" Partitioning Scheme | |
CN117795926A (en) | Data packet prioritization in multiplexed sessions |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
REG | Reference to a national code |
Ref country code: HKRef legal event code: DERef document number: 40038518Country of ref document: HK |
|
GR01 | Patent grant | ||
GR01 | Patent grant |