CN117355840A - Regularized word segmentation - Google Patents
Regularized word segmentation Download PDFInfo
- Publication number
- CN117355840A CN117355840A CN202280025027.9A CN202280025027A CN117355840A CN 117355840 A CN117355840 A CN 117355840A CN 202280025027 A CN202280025027 A CN 202280025027A CN 117355840 A CN117355840 A CN 117355840A
- Authority
- CN
- China
- Prior art keywords
- subword
- input word
- word
- units
- speech recognition
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000011218 segmentation Effects 0.000 title claims abstract description 52
- 238000012549 training Methods 0.000 claims abstract description 54
- 238000000034 method Methods 0.000 claims abstract description 43
- 230000015654 memory Effects 0.000 claims description 34
- 238000012545 processing Methods 0.000 claims description 27
- 238000013528 artificial neural network Methods 0.000 claims description 12
- 238000004891 communication Methods 0.000 claims description 9
- 230000000306 recurrent effect Effects 0.000 claims description 6
- 238000005070 sampling Methods 0.000 claims description 6
- 230000007246 mechanism Effects 0.000 claims description 4
- 230000007787 long-term memory Effects 0.000 claims description 3
- 230000008569 process Effects 0.000 description 11
- 238000013518 transcription Methods 0.000 description 10
- 230000035897 transcription Effects 0.000 description 10
- 238000004590 computer program Methods 0.000 description 8
- 238000010586 diagram Methods 0.000 description 7
- 239000013598 vector Substances 0.000 description 6
- 238000003058 natural language processing Methods 0.000 description 5
- 230000003287 optical effect Effects 0.000 description 4
- 230000006870 function Effects 0.000 description 3
- 230000036961 partial effect Effects 0.000 description 3
- 230000004044 response Effects 0.000 description 3
- 108091023037 Aptamer Proteins 0.000 description 2
- 230000008859 change Effects 0.000 description 2
- 230000000694 effects Effects 0.000 description 2
- 239000012634 fragment Substances 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 230000006855 networking Effects 0.000 description 2
- 230000006403 short-term memory Effects 0.000 description 2
- 230000009471 action Effects 0.000 description 1
- 230000002411 adverse Effects 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 238000003066 decision tree Methods 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 238000011161 development Methods 0.000 description 1
- 230000018109 developmental process Effects 0.000 description 1
- 238000009499 grossing Methods 0.000 description 1
- 230000010354 integration Effects 0.000 description 1
- 230000002452 interceptive effect Effects 0.000 description 1
- 230000000670 limiting effect Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000007774 longterm Effects 0.000 description 1
- 238000012423 maintenance Methods 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000003062 neural network model Methods 0.000 description 1
- 238000010606 normalization Methods 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 230000005236 sound signal Effects 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 239000013589 supplement Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/04—Segmentation; Word boundary detection
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/279—Recognition of textual entities
- G06F40/284—Lexical analysis, e.g. tokenisation or collocates
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/044—Recurrent networks, e.g. Hopfield networks
- G06N3/0442—Recurrent networks, e.g. Hopfield networks characterised by memory or gating, e.g. long short-term memory [LSTM] or gated recurrent units [GRU]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/04—Architecture, e.g. interconnection topology
- G06N3/045—Combinations of networks
- G06N3/0455—Auto-encoder networks; Encoder-decoder networks
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N3/00—Computing arrangements based on biological models
- G06N3/02—Neural networks
- G06N3/08—Learning methods
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/27—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the analysis technique
- G10L25/30—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 characterised by the analysis technique using neural networks
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/02—Feature extraction for speech recognition; Selection of recognition unit
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L15/16—Speech classification or search using artificial neural networks
Abstract
A method (600) for subword segmentation includes receiving an input word (302) to be segmented into a plurality of subword units (119). The method further includes performing a subword segmentation routine (300) to segment the input word into a plurality of subword units by accessing a training word set (350) of subword units and selecting the plurality of subword units from the input word by greedily finding a longest subword unit from the input words present in the training word set until an end of the input word is reached.
Description
Technical Field
The present disclosure relates to regularized word segmentation.
Background
Automatic Speech Recognition (ASR) systems have evolved from multiple models (e.g., acoustic, pronunciation, and language models) where each model has a dedicated purpose to an integrated model where audio waveforms (i.e., input sequences) are mapped directly to output sentences (i.e., output sequences) using a single neural network. This integration produces a sequence-to-sequence method that generates a sequence of words or graphemes when given a sequence of audio features. Using the integrated architecture, all components of the model can be co-trained as a single end-to-end (E2E) neural network. Here, the E2E model refers to a model whose architecture is entirely constructed of a neural network. The full neural network operates without external and/or manually designed components (e.g., finite state transducers, dictionary or text normalization modules). Additionally, in training E2E models, these models typically do not need to be guided from the decision tree or time aligned from a separate system.
Disclosure of Invention
One aspect of the present disclosure provides a computer-implemented method for subword segmentation. The computer-implemented method, when executed on data processing hardware, causes the data processing hardware to perform operations including receiving an input word to be divided into a plurality of subword units. The operations further include performing a subword segmentation routine to segment the input word into a plurality of subword units by: accessing a training word set of the sub word units; and selecting a plurality of sub-word units from the input word by greedily finding the longest sub-word unit present in the training vocabulary set from the input word until the end of the input word is reached.
Implementations of the disclosure may include one or more of the following optional features. In some implementations, selecting the plurality of subword units includes, for each corresponding position of a plurality of different positions of the input word: identifying all possible candidate subword units present in the training vocabulary set from the input words at the corresponding locations; and randomly sampling from all possible candidate subword units by assigning 1-p probabilities to the longest possible candidate subword units and dividing the remaining p probabilities evenly among all possible candidate subword units from the input word at the corresponding positions. The operations may further include: before performing the sub-word segmentation routine, misspellings for the input word are created by randomly deleting characters from the input word independently using pre-specified probabilities.
In some examples, the operations further comprise: prior to performing the subword segmentation routine, misspellings for the input word are created by: pre-specifying probabilities for exchanging sequences of adjacent character pairs; and for each adjacent character pair in the input word, exchanging an order of characters from the adjacent character pair in the input word based on the pre-specified probabilities. Here, the order of any given character in the input word is limited to at most one exchange.
In some implementations, the operations further comprise: receiving training examples including audio data characterizing an utterance of an input word; and processing the audio data to generate a speech recognition result for the utterance of the input word for output by the speech recognition model. Here, the speech recognition result includes a sequence of hypothesized subword units, each hypothesized subword unit being output from the speech recognition model at a corresponding output step. In these embodiments, the operations further comprise: determining a supervision penalty term based on the hypothesized sequence of subword units and the plurality of subword units selected from the input word by the subword segmentation routine; and updating parameters of the speech recognition model based on the supervised loss term. In some examples, the speech recognition model includes a recurrent neural network transducer (RNN-T) model architecture having an audio encoder, a prediction network, and a joint network. Here, the audio encoder may include one of a plurality of Long Short Term Memory (LSTM) layers, a plurality of transducer layers, or a plurality of aptamer layers. In other examples, the speech recognition model includes a listen, attention, spelling (LAS) model architecture with an audio encoder, an attention mechanism, and a decoder.
Another aspect of the present disclosure provides a system for subword segmentation that includes data processing hardware and memory hardware in communication with the data processing hardware. The memory hardware stores instructions that, when executed on the data processing hardware, cause the data processing hardware to perform operations including receiving an input word to be divided into a plurality of subword units. The operations further include executing a subword segmentation routine to segment the input word into a plurality of subword units by: accessing a training word set of the sub word units; and selecting a plurality of sub-word units from the input word by greedily finding the longest sub-word unit present in the training vocabulary from the input word until the end of the input word is reached.
Implementations of the disclosure may include one or more of the following optional features. In some implementations, selecting the plurality of subword units includes, for each corresponding position of a plurality of different positions of the input word: identifying all possible candidate subword units present in the training vocabulary set from the input words at the corresponding locations; and randomly sampling from all possible candidate subword units by assigning 1-p probabilities to the longest possible candidate subword units and dividing the remaining p probabilities evenly among all possible candidate subword units from the input word at the corresponding positions. The operations may further include: before performing the sub-word segmentation routine, misspellings for the input word are created by randomly deleting characters from the input word independently using pre-specified probabilities.
In some examples, the operations further comprise: before performing the subword segmentation routine, a misspelling of the input word is created by: pre-specifying probabilities for exchanging sequences of adjacent character pairs; and exchanging, for each adjacent character pair in the input word, an order of characters from the adjacent character pair in the input word based on the pre-specified probabilities. Here, the order of any given character in the input word is limited to at most one exchange.
In some implementations, the operations further comprise: receiving training examples including audio data characterizing an utterance of an input word; and processing the audio data to generate a speech recognition result of the utterance of the input word for output by the speech recognition model. Here, the speech recognition result includes a sequence of hypothesized subword units, each hypothesized subword unit being output from the speech recognition model at a corresponding output step. In these embodiments, the operations further comprise: determining a supervision penalty term based on the hypothesized sequence of subword units and the plurality of subword units selected from the input word by the subword segmentation routine; and updating parameters of the speech recognition model based on the supervised loss term. In some examples, the speech recognition model includes a recurrent neural network transducer (RNN-T) model architecture having an audio encoder, a prediction network, and a joint network. Here, the audio encoder may include one of a plurality of Long Short Term Memory (LSTM) layers, a plurality of transducer layers, or a plurality of aptamer layers. In other examples, the speech recognition model includes a listen, attention, spelling (LAS) model architecture with an audio encoder, an attention mechanism, and a decoder.
The details of one or more embodiments of the disclosure are set forth in the accompanying drawings and the description below. Other aspects, features, and advantages will be apparent from the description and drawings, and from the claims.
Drawings
FIG. 1 is a schematic diagram of an example system for subword segmentation for training a speech recognition model.
FIG. 2A is a schematic diagram of an example recurrent neural network transducer (RNN-T) model architecture.
FIG. 2B is a schematic diagram illustrating an example listen, note, and spell model architecture.
FIG. 3 is an example of a subword segmentation routine that segments an input word into a plurality of subword segments.
Fig. 4 is an example algorithm representing the subword segmentation routine of fig. 3.
FIG. 5 is a schematic diagram representing an example partial graph of candidate subword units sampled from all possible subword units.
FIG. 6 is a flow chart of an example operational arrangement of a computer-implemented method for segmenting subword units.
FIG. 7 is a schematic diagram of an example computing device that may be used to implement the systems and methods described herein.
Like reference symbols in the various drawings indicate like elements.
Detailed Description
Modern Automatic Speech Recognition (ASR) systems are focused not only on providing high quality (e.g., low Word Error Rate (WER)), but also on providing low latency (e.g., short latency between user speech and transcription occurrence). The development of end-to-end sequence ASR models provides a single neural network model to directly receive audio representing speech as input and output recognition text without the use of different and separately trained acoustic, language and pronunciation models. For the end-to-end sequence ASR model, word-based and grapheme-based text representations are two simple techniques to represent the output recognition text. Generally, word-based representations result in shorter lengths of text sequences than grapheme-based representations, thereby making it easier for an ASR model to learn cross-context dependencies. However, implementing an ASR model to use word-based representations requires a predefined vocabulary to identify possible words, thereby limiting the ability to handle out-of-vocabulary (OOV) terms/words that are not present in training data. Another disadvantage of word-based representations is that the ASR model cannot learn the relationships between language-dependent words, such as "catch" and "catch", "book" and "books", "hellp" and "hellpful", etc., without incorporating additional information into the generated word embeddings.
On the other hand, a grapheme-based representation includes the output of a single sequence of characters that form a word. While ASR models using grapheme-based representations are theoretically capable of learning relationships between spelled similar words and handling OOV terms/words, one major drawback is the greater memory footprint and computation time required to generate longer text sequences. In other words, generating a grapheme-based representation adds a number of decoding steps, which has the adverse effect of reducing the speed of inference. Furthermore, grapheme-based representations inhibit the ability to learn patterns from data, resulting in poor performance.
Embodiments herein relate to using an end-to-end sequence ASR model that maps input audio data (i.e., an input sequence) directly to an output sequence in the form of a subword-based text representation that includes a sequence of subword units that form one or more words. The subword-based text representation achieves a trade-off between the word-based representation and the grapheme-based representation. Embodiments herein are more particularly directed to a subword segmentation routine (also referred to as a "word tokenization routine/algorithm" or simply "word segmentation model") that improves subword-based text representations for training sequence ASR models. As will be discussed in more detail below, the perform subword segmentation routine segments the input word into a plurality of subword units by: a training word set of subword units is accessed and the subword units are sequentially selected for inclusion in the plurality of subword units by greedy finding the longest subword unit present in the training word set from the input word until the end of the input word is reached. As used herein, the term "subword unit" may be interchangeably referred to as a "word segment" and may range from a grapheme up to a whole word. Notably, the vocabulary set of subword units accessed by the subword segmentation routine need not include any OOV terms/words, and the subword segmentation routine needs to be trained to maximize only the language model likelihood across the training set of input words. The subword segmentation routine may be associated with a statistical word segment model trained using word counts obtained from text data for individually segmenting each input word into subword units. Symbols representing separate spaces may be included in the subword unit.
The input word may include one of one or more words in a true value transcription of the training utterance characterized by the training audio data. In this way, the plurality of subword units segmented from the input word and selected by the subword segmentation routine may be used as true value subword units for training the ASR model to learn to predict an output sequence of subword units from the input training audio data characterizing the training speech. The audio data characterizing the training utterance may include a non-synthesized speech representation corresponding to a human speaking the training utterance; or a synthesized speech representation converted from input text (e.g., a true value transcription) by text-to-speech.
Additional embodiments relate to applying one or more regularization techniques to the input word prior to segmentation by the subword segmentation routine. As discussed in more detail below, these regularization techniques may include altering the spelling of the input word prior to performing the subword segmentation routine, thereby providing a complex form of label smoothing to improve the performance of the ASR model.
FIG. 1 is an example of an example system 100 for a speech environment. In a speech environment, the manner in which the user 104 interacts with a computing device, such as the user device 10, may be through voice input. User device 10 (also commonly referred to as device 10) is configured to capture sound (e.g., streaming audio data) from one or more users 104 within a speech environment. Here, the streaming audio data may refer to a spoken utterance 106 issued by the user 104, the spoken utterance 106 being used as an audible query, a command to the device 10, or an audible communication captured by the device 10. The speech-enabled system of device 10 may respond to a query or command by answering the query and/or causing the command to be executed/fulfilled by one or more downstream applications.
User device 10 may correspond to any computing device associated with user 104 and capable of receiving audio data. Some examples of user devices 10 include, but are not limited to, mobile devices (e.g., mobile phones, tablet computers, notebook computers, etc.), computers, wearable devices (e.g., smart watches, smart headsets, etc.), smart appliances, internet of things (IoT) devices, vehicle entertainment systems, smart displays, smart speakers, etc. The user device 10 includes data processing hardware 12 and memory hardware 14 in communication with the data processing hardware 12 and storing instructions that, when executed by the data processing hardware 12 Shi Hangshi, cause the data processing hardware 12 to perform one or more operations. The user device 10 further comprises an audio system 16 having an audio capturing device (e.g., microphone) 16, 16a for capturing and converting spoken utterances 106 within the speech environment into electrical signals; and a voice output device (e.g., speaker) 16b,16b for conveying audible audio signals (e.g., as output audio data from device 10). Although in the illustrated example the user device 10 implements a single audio capture device 16a, the user device 10 may implement an array of audio capture devices 16a without departing from the scope of the disclosure, whereby one or more capture devices 16a in the array may not physically reside on the user device 10, but communicate with the audio system 16.
The system 100 includes an Automatic Speech Recognition (ASR) system 118 implementing an end-to-end sequence ASR model 200, the end-sequence ASR model 200 being trained to map input audio data 110 corresponding to an utterance 106 directly to a subword-based text representation 119, the subword-based text representation 119 including a sequence of subword units 119 that form words in a transcription 120 of the utterance 106. The ASR system 118 implements a subword segmentation routine 300 (also referred to as a "word tokenization routine/algorithm" or simply "word segmentation model") that improves the subword-based text representation 119 for training the sequence ASR model 200. As discussed in more detail below with reference to FIG. 3, the ASR system 118 implements a subword segmentation routine 300 to segment an input word 302 (FIG. 3) into a plurality of subword units 119 for improving the speech recognition accuracy of the ASR model 200 trained to predict output tags in the form of subword-based text representations 119.
Notably, the subword units 119 segmented by the subword segmentation routine 300 better reflect human speech with accents, varying pronunciation, and/or non-fluency. In this way, the subword segmentation routine 300 supplements the multilingual ASR model 200 that is capable of recognizing speech spoken in different languages. For example, the multilingual ASR model 200 may be trained to recognize speech from multilingual speakers (such as speakers that code-switch between different Indian languages), whereby the subword units 119 segmented by the subword segmentation routine 300 improve performance on the ASR model 200 when: when these multilingual speakers switch between languages and change the pronunciation of words and their neighboring words. Following the same concept, the subword unit 119 segmented by the subword segmentation routine 300 may enable the ASR model 200 (single-or multi-lingual) to include recognition of all types of atypical/accent speech that may be spoken by different speakers.
The ASR system 118 may reside on the user device 10 of the user 104 and/or on a remote computing device 60 (e.g., one or more remote servers of a distributed system implemented in a cloud computing environment) in communication with the user device 10 via the network 40. The user device 10 and/or the remote computing device 60 also includes an audio subsystem 108, the audio subsystem 108 being configured to receive the utterance 106 spoken by the user 104 and captured by the audio capture device 16a, and to convert the utterance 106 into a corresponding digital format associated with input acoustic frames (e.g., audio data) 110 that can be processed by the ASR system 118. In the illustrated example, the user speaks the corresponding utterance 106 and the audio subsystem 108 converts the utterance 106 into corresponding audio data (e.g., acoustic frames) 110 for input to the ASR system 118. Thereafter, the ASR model 200 receives as input the audio data 110 corresponding to the utterance 106 and generates/predicts corresponding subword units as output at each of the plurality of output steps to form a transcription 120 (e.g., recognition result/hypothesis) of the utterance 106. In the illustrated example, the ASR model 200 may perform streaming speech recognition to produce an initial speech recognition result 120, which may be rescaled to produce a final speech recognition result 120.
The user device 10 and/or the remote computing device 60 also implements a user interface generator 107, the user interface generator 107 configured to present a representation of the transcription 120 of the utterance 106 to the user 104 of the user device 10. As described in more detail below, the user interface generator 107 may stream the initial speech recognition result 120 and then display the final speech recognition result 120. In some configurations, the transcription 120 output from the ASR system 118 is processed, for example, by a Natural Language Understanding (NLU) module implemented on the user device 10 or the remote computing device 60, to perform the user command/query specified by the utterance 106. Additionally or alternatively, a text-to-speech system (not shown) (e.g., performed on any combination of the user device 10 or the remote computing device 60) may convert the transcription into synthesized speech for audible output by the user device 10 and/or another device.
In the illustrated example, the user 104 interacts with a program or application 50 (e.g., a digital assistant application 50) of the user device 10 using the ASR system 118. For example, fig. 1 depicts user 104 in communication with digital assistant application 50, and digital assistant application 50 displays digital assistant interface 18 on a screen of user device 10 to depict a conversation between user 104 and digital assistant application 50. In this example, user 104 asks digital assistant application 50: "is a concert point tonight? The "this question from the user 104 is the spoken utterance 106 captured by the audio capturing device 16a and processed by the audio system 16 of the user device 10. In this example, the audio system 16 receives the spoken utterance 106 and converts it into an acoustic frame 110 for input to the ASR system 118.
In the illustrated example of fig. 1, digital assistant application 50 may use natural language processing to respond to questions posed by user 104. Natural language processing generally refers to the process of interpreting a written language (e.g., initial speech recognition result 120a and/or final speech recognition result 120 b) and determining whether the written language suggests any actions. In this example, the digital assistant application 50 uses natural language processing to identify questions from the user 104 that relate to the user's calendar and, more particularly, to concerts on the user's calendar. By identifying these details using natural language processing, the automated assistant returns a response 19 to the user query, where the response 19 indicates "floor gate open at 6:30 pm, and concert start at 8 pm". In some configurations, natural language processing occurs on a remote server 60 in communication with the data processing hardware 12 of the user device 10.
Referring to FIG. 2A, in some implementations, the ASR model 200 includes a recurrent neural network transducer (RNN-T) model 200a architecture (or other type of frame alignment-based transducer model) that complies with delay constraints associated with interactive applications. The RNN-T model 200a provides a small computational footprint and utilizes less memory requirements than conventional ASR architectures, making the RNN-T model architecture suitable for performing speech recognition entirely on the user device 102 (e.g., without requiring communication with a remote server). The RNN-T model 200a includes an encoder network 210, a prediction network 220, and a federated network 230. The predictive network 220 and the federated network 230 may collectively provide an RNN-T decoder 231. The encoder network 210, which is substantially similar to the Acoustic Model (AM) in a conventional ASR system, may include a circular network of stacked long term memory (LSTM) layers. For example, the encoder reads a d-dimensional feature vector sequence (e.g., acoustic frame 110 (figure 1))x＝(x 1 ,x 2 ,···,x T ) Wherein, the method comprises the steps of, wherein,and a higher order feature representation is generated at each time step. This higher order feature representation is denoted +.>
Similarly, the predictive network 220 is also an LSTM network, which, like the Language Model (LM), will so far output a sequence of non-blank subword units y by the final Softmax layer 240 0 ,...,y ui-1 Processing into a representation225. Notably, the non-blank subword unit sequence (y 0 ,...,y ui-1 ) The language dependency between the non-blank symbols predicted during the previous time step so far is captured to assist the federated network 230 in predicting the probability of the next output subword unit 219 or blank symbol during the current time step.
Finally, using the RNN-T model architecture, the representations generated by the encoder and prediction networks 210, 220 are combined by the federated network 230. Then joint network predictionWhich is the distribution over the next subword unit 119. In other words, the union network 230 generates a probability distribution over the hypothesized subword units 119 at each output step (e.g., time step). Here, a "hypothesized subword unit" corresponds to a possible subword unit from the training word set 350 (fig. 3) of subword units, each of which represents a corresponding subword unit in the specified natural language. The sub-word units within the vocabulary 350 range from a single grapheme, a word segment, and an entire word. In some examples, training word set 350 includes 4,096 different subword units in the specified natural language. In these examples, the federated network 230 is trained to predict/output 4,096 Different subword units. The output distribution of the federated network 230 can include a posterior probability value for each different hypothesized subword unit. Thus, if there are 4,096 different subword units representing different graphemes, word fragments, or words, then the output y of the union network 230 i 4,096 different probability values can be included, one for each output label. The probability distribution can then be used to select candidate sub-word units (e.g., graphemes, word fragments, and/or words) and assign scores thereto during a beam search process (e.g., through Softmax layer 240) to determine the transcription 120.
The Softmax layer 240 may employ any technique to select the output subword unit with the highest probability in the distribution as the next subword unit predicted by the RNN-T model 200a at the corresponding output step. In this way, the RNN-T model 200a does not make a condition independent assumption, but rather predicts each subword unit not only on acoustic conditions but also on the sequence of subword units output so far. The RNN-T model 200a does assume that the output subword unit is independent of the future acoustic frame 110, which allows the RNN-T model to be employed in a streaming manner.
In some examples, the encoder network 210 of the RNN-T model 200 is comprised of eight 2048-dimensional LSTM layers, each layer followed by a 640-dimensional projection layer. In other implementations, the encoder network 210 includes a network of conformers or transformer layers. The predictive network 220 may have two 2,048-dimensional LSTM layers, each further followed by a 640-dimensional projection layer and a 128-element embedded layer. Finally, the federated network 230 may also have 640 hidden units. The Softmax layer 240 may consist of a unified word segment/subword unit set generated using all the unique word segment/subword units in the training data.
Referring to FIG. 2B, in other embodiments, the ASR model 200 includes an listen, attention, and spelling (LAS) model 200B architecture that provides a single neural network that includes a listener encoder module 211 that is similar to a conventional acoustic model, an attention module 221 that acts as an alignment model, and a decoder 231 that is similar to a language model in a conventional system. Specifically, the listener encoder module 211 obtains an input feature (e.g., acoustic frame 110 (FIG. 1)) x andand map them to higher level feature representations h enc . Generating the encoding feature representation h can be done for each of a plurality of input frames representing different input time steps enc Is a process of (2). These time steps are indicated using the following subscript u. Thus, for a group of frames { f 1 ,f 2 ,f 3 ,...f u A corresponding set of code outputs { h }, can be 1 ,h 2 ,h 3 ,...h u }。
The output of the listener encoder module 211 is passed to the attention module 221, which attention module 221 determines that h should be paid attention to enc Which encoder characteristics of (a) are used to predict the next output subword unit y i Similar to a Dynamic Time Warping (DTW) alignment module. In some examples, the notifier module 221 is referred to herein as a notifier neural network or a notifier 221. Note that the viewer 221 is able to generate a context output c for each of a plurality of output steps i i . For each context output vector c i The attention 221 can calculate the attention based on the encoding of one or more input steps u (e.g., the encoding of the current input step and the encoding of the previous input step). For example, the attention 221 can be able to determine the position of the speech in the set of all encoder outputs of the utterance (e.g., the entire set { h } 1 ,h 2 ,h 3 ,...h u }) generating an attention context output c) i . The attention context vector can be a vector representing a weighted summary of the current and previous encodings of the frame (e.g., portion) of the utterance being recognized.
Finally, the output of the attention 221 is passed to a decoder 231, which decoder 231 obtains the attention context (e.g., context vector or attention profile) c output by the attention 221 i And previously predicted embedded y i -1 to produce a decoder output. Given a hypothesized subword unit { y } i-1 ,...,y 0 And input x, the decoder output can be the current hypothesized subword unit y i Probability distribution P (y) i |y i-1 ,...,y 0 X). Thus, the decoder 231 generates a probability distribution over the hypothesized subword unit 119 at each output step. And (3) withAs with the RNN-T model 200a discussed above with reference to FIG. 2A, a "hypothesized subword unit" corresponds to a possible subword unit from the training word set 350 (FIG. 3) of subword units, each of which represents a corresponding subword unit in a specified natural language.
Although not shown, LAS model 200b may include a softmax layer that receives the output of decoder 231. In some implementations, the softmax layer is separate from the decoder 231 and processes the output y from the decoder 231 i And then the output of the softmax layer is used in a beam search process to select sub-word units of the words that form the result transcript 120. In some implementations, the softmax layer is integrated with the decoder 231 such that the output y of the decoder 231 i Representing the output of the softmax layer.
The decoder 231 and/or the associated softmax layer may be trained to output a set of values that indicate the likelihood of occurrence of each of the predetermined set of subword units 119. The set of values can be vectors and can indicate a probability distribution over the set of subword units. The output profile of the decoder 231 and/or softmax layer can include a posterior probability value for each different subword unit. Thus, if there are 4,096 different subword unit tags, the decoder output y i Or receive and process the output y i The output of the softmax layer of (c) can include 4,096 different probability values, one for each output label. The probability distribution can then be used to select and assign scores to candidate subword units 119 in the beam search process used to determine transcription 120.
FIG. 3 illustrates an example of a subword segmentation routine 300 (e.g., lightweight WPM) that is implemented to segment an input word 302 into a plurality of subword units 119, 119 a-b. During execution of the subword segmentation routine 300, the routine 300 accesses a training word set 350 of subword units. In some examples, training vocabulary set 350 includes 4,096 different sub-word units, including word segments as well as graphemes and whole words. The training vocabulary 350 may be stored on a data store 360 of the remote system 60. During execution of the subword segmentation routine 300, the routine 300 selects a plurality of subword units 219 from the input word 302 by greedy finding the longest subword unit present in the training word set 350 from the input word 302 until the end of the input word 302 is reached. That is, all processing performed by routine 300 is performed strictly from left to right in order to sequentially find the longest subword at each position/index present in training word set 350. For example, when the input word 302 includes the word "Interstrech", the longest subword unit selected at the first position/index will be "Inter", followed by "sp", "ee" and "ch" to the end. FIG. 4 illustrates an example algorithm 400 representing the subword segmentation routine 300.
FIG. 3 also shows the use of multiple subword units 119, 119a-n as true value subword units for training the ASR model 200. Here, training examples including audio data 110T characterizing an utterance of an input word 302 may be paired with a true value subword unit 119. The ASR model 200 processes the training audio data 110 to generate as output speech recognition results 201 of the utterances of the input words 302. Here, the speech recognition result 201 comprises a sequence of hypothesized subword units 119, wherein each hypothesized subword unit in the sequence of hypothesized subword units is output from the speech recognition model 200 at a corresponding output step. Thereafter, a penalty function 380 (e.g., cross entropy penalty or RNN-T penalty) determines a supervised penalty term 390 based on the sequence of hypothesized subword units and the plurality of subword units 119 selected from the input word by the subword segmentation routine 300. The supervised penalty term 390 is used to update parameters of the ASR model 200 during training.
In some implementations, randomness is added to the subword unit selection step (e.g., line 4 of algorithm 400 of fig. 4) by sampling the selection of the subword unit at each index from among all possible subword units, rather than just always selecting the longest subword unit at each index. That is, for each corresponding location (e.g., index) of the plurality of different locations of the input word, the routine 300 may identify from the input word 302 at the corresponding location all possible candidate subword units present in the training vocabulary. Thereafter, for the corresponding position of the input word 302, the routine 300 may then randomly sample from all possible candidate subword units by: the 1-p probability is assigned to the longest one of the possible candidate subword units and the remainder of the p probability is evenly divided among all possible candidate subword units from the input word 302 at the corresponding locations. Continuing with the example above, where the input word 302 is "Interstreech," FIG. 5 shows a partial diagram 500 of a training process for randomly sampling the selection of subword units at each location from among all possible subword units. Here, the regularization rate may be uniform and include a value of 0.1 such that the longest candidate subword unit at each location is assigned a probability of 0.9, while the remaining 0.1 is uniformly assigned among all candidate subword units (including the longest candidate). Notably, at different locations, multiple subword unit candidates sharing the same prefix may be sampled, thereby having the following two effects: the golden profile of the longest subword unit is always selected to be less sharp, so that the model is prevented from being excessively trusted; and the model becomes aware of the prefix relationships of the subword units.
Referring back to FIG. 3, in some embodiments, one or more regularization techniques 305 are applied prior to performing the subword segmentation routine 300 in order to introduce randomness into the routine 300 to further improve the gain in speech recognition performance of the ASR model 200. In some examples, regularization technique 305 includes creating a misspelling of input word 302 by randomly deleting characters from the input word independently using pre-specified probabilities. Applying the example above, the input word 302"interspeech" would become "interspeech". Notably, and referring to the partial graph 500 of FIG. 5, since any letters in the input word are potentially skipped/deleted to create a misspelling, all candidate subword units will still exist, albeit with a different probability distribution. Additionally, by deleting random characters to create a spelling error, the routine 300 may learn additional relationships between words other than the prefix structure by learning the corresponding suffix structure (and similarity of general spelling). For example, if the golden label is "ing" and the letter "i" is skipped, the model assigns probabilities to the subwords "ng" so that the relationship between "ing" and "ng" is learned. Furthermore, since multiple letters may be skipped, the golden label at any location may come from a more distant future, enabling the routine 300 to better learn the long-term dependencies in the data.
In an additional example, the regularization technique 305 applied prior to performing the subword segmentation routine (e.g., lightweight WPM) 300 includes creating spelling errors for the input word 302 by: the probabilities of exchanging the order of adjacent character pairs are pre-specified, and for each adjacent character pair in the input word, the order of characters from the adjacent character pair in the input word is exchanged based on the pre-specified probabilities. Here, the order of any given character in the input word is limited to at most one exchange. For example, if the original word is "the", and we swap t and h to "hte", no additional swap is performed for the "te" pair after the first swap. As a second example, the input word 302"center" may become "center".
Fig. 6 illustrates an example operational arrangement of a subword unit segmentation method 600 for improving speech recognition. At operation 602, the method 600 includes receiving an input word 302 to be segmented into a plurality of subword units 119. Operations 604 and 606 are performed concurrently with the execution of the subword segmentation routine 300 to segment the input word 302 into the plurality of subword units 119. At operation 604, the method 600 includes accessing a training vocabulary 350 of subword units. At operation 606, the method 600 includes selecting a plurality of subword units 119 from the input word 302 by greedily finding the longest subword unit present in the training word set 350 from among the input word 302 until the end of the input word 302 is reached.
FIG. 7 is a schematic diagram of an example computing device 700 that may be used to implement the systems and methods described in this document. Computing device 700 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. The components illustrated herein, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the invention described and/or claimed in this document.
Computing device 700 includes a processor 710, memory 720, storage device 730, high-speed interface/controller 740 coupled to memory 720 and high-speed expansion ports 750, and low-speed interface/controller 760 coupled to low-speed bus 770 and storage device 730. Each of the components 710, 720, 730, 740, 750, and 760 are interconnected using various buses, and may be mounted on a common motherboard or in other manners as appropriate. The processor (e.g., data processing hardware 710 of remote server 60) 710 is capable of processing instructions for execution within computing device 700, including instructions stored in memory 720 or on storage device 730, to display graphical information for a Graphical User Interface (GUI) on an external input/output device, such as display 780 coupled to high-speed interface 740. In other embodiments, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and memory types. In addition, multiple computing devices 700 may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multiprocessor system).
Memory (e.g., memory hardware 720 of remote server 60) 720 stores information non-transitory within computing device 700. Memory 720 may be a computer-readable medium, volatile memory unit(s), or non-volatile memory unit(s). Non-transitory memory 720 may be a physical device for temporarily or permanently storing programs (e.g., sequences of instructions) or data (e.g., program state information) for use by computing device 700. Examples of non-volatile memory include, but are not limited to, flash memory and read-only memory (ROM)/programmable read-only memory (PROM)/erasable programmable read-only memory (EPROM)/electrically erasable programmable read-only memory (EEPROM) (e.g., typically used for firmware such as a boot strap). Examples of volatile memory include, but are not limited to, random Access Memory (RAM), dynamic Random Access Memory (DRAM), static Random Access Memory (SRAM), phase Change Memory (PCM), and magnetic disk or tape.
Storage device 730 is capable of providing mass storage for computing device 700. In some implementations, the storage device 730 is a computer-readable medium. In various different implementations, storage device 730 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state storage device, or an array of devices, including devices in a storage area network or other configurations. In additional embodiments, the computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer-or machine-readable medium, such as memory 720, storage device 730, or memory on processor 710.
High speed controller 740 manages bandwidth-intensive operations for computing device 700, while low speed controller 770 manages lower bandwidth-intensive operations. This allocation of responsibilities is merely exemplary. In some implementations, high-speed controller 740 is coupled to memory 720, display 780 (e.g., via a graphics processor or accelerator), and to high-speed expansion port 750, which high-speed expansion port 750 can accept various expansion cards (not shown). In some implementations, a low-speed controller 770 is coupled to the storage device 730 and to the low-speed expansion port 790. The low-speed expansion port 790, which may include various communication ports (e.g., USB, bluetooth, ethernet, wireless ethernet), may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, for example, through a network adapter.
Computing device 700 may be implemented in a number of different forms, as shown. For example, it may be implemented as a standard server 700a or as a laptop 700b multiple times in a group of such servers 700a, or as part of a rack server system 700 c.
Various implementations of the systems and techniques described here can be realized in digital electronic and/or optical circuits, integrated circuits, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various embodiments may include embodiments in one or more computer programs executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage system, at least one input device, and at least one output device.
A software application (i.e., a software resource) may refer to computer software that causes a computing device to perform tasks. In some examples, a software application may be referred to as an "application," app, "or" program. Example applications include, but are not limited to, system diagnostic applications, system management applications, system maintenance applications, word processing applications, spreadsheet applications, messaging applications, media streaming applications, social networking applications, and gaming applications.
These computer programs (also known as programs, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms "machine-readable medium" and "computer-readable medium" refer to any computer program product, non-transitory computer-readable medium, apparatus and/or device (e.g., magnetic discs, optical disks, memory, programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term "machine-readable signal" refers to any signal used to provide machine instructions and/or data to a programmable processor.
The processes and logic flows described in this specification can be performed by one or more programmable processors (also referred to as data processing hardware) executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit). Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only memory or a random access memory or both. The essential elements of a computer are a processor for executing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include one or more mass storage devices, such as magnetic, magneto-optical disks, or optical disks, for storing data, or be operatively coupled to receive data therefrom or transfer data thereto or both. However, the computer need not have such a device. Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and storage devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, such as internal hard disks or removable disks; magneto-optical disk; and CD ROM and DVD-ROM discs. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
To provide for interaction with a user, one or more aspects of the present disclosure can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube), LCD (liquid crystal display) monitor) or a keyboard and a pointing device (e.g., a mouse or a trackball) for displaying information to the user and, optionally, by which the user can provide input to the computer. Other types of devices can also be used to provide interaction with a user; for example, feedback provided to the user may be any form of sensory feedback, such as visual feedback, auditory feedback, or tactile feedback; also, input from the user can be received in any form, including acoustic, speech, or tactile input. Further, the computer is capable of interacting with the user by sending and receiving documents to and from the device used by the user; for example, by sending a web page to a web browser on a user client device in response to a request received from the web browser.
Various embodiments have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. Accordingly, other embodiments are within the scope of the following claims.
Claims (20)
1. A computer-implemented method (600), which when executed on data processing hardware (710), causes the data processing hardware (710) to perform operations comprising:
receiving an input word (302) to be divided into a plurality of subword units (119); and
a subword segmentation routine (300) is performed to segment the input word (302) into the plurality of subword units (119) by:
accessing a training word set (350) of the subword unit (119); and
-selecting the plurality of subword units (119) from the input word (302) by greedily finding the longest subword unit present in the training word set (350) from the input word (302) until the end of the input word (302) is reached.
2. The method (600) of claim 1, wherein selecting the plurality of subword units (119) includes, for each corresponding position of a plurality of different positions of the input word (302):
identifying all possible candidate subword units (119) from the input word (302) that are present in the training word set (350) at the corresponding locations; and
-randomly sampling from all the possible candidate subword units (119) by assigning a 1-p probability to the longest one of the possible candidate subword units (119) and dividing the remaining p probabilities evenly among all the possible candidate subword units (119) from the input word (302) at the corresponding positions.
3. The method (600) of claim 1 or 2, wherein the operations further comprise, prior to the execution of the subword segmentation routine (300), creating a misspelling of the input word (302) by randomly deleting characters from the input word (302) independently using pre-specified probabilities.
4. The method (600) of any of claims 1-3, wherein the operations further comprise, prior to performing the subword segmentation routine (300), creating a misspelling of the input word (302) by:
pre-specifying probabilities for exchanging sequences of adjacent character pairs; and
for each adjacent character pair in the input word (302), exchanging an order of the characters from the adjacent character pair in the input word (302) based on the pre-specified probabilities.
5. The method (600) of claim 4, wherein the order of any given character in the input word (302) is limited to at most one exchange.
6. The method (600) of any of claims 1-5, wherein the operations further comprise:
receiving training examples including training audio data (110T) characterizing an utterance (106) of the input word (302);
Processing the training audio data (110T) to generate a speech recognition result (201) for the utterance (106) of the input word (302) for output by a speech recognition model (200), the speech recognition result (201) comprising a sequence of hypothesized sub-word units, each hypothesized sub-word unit in the sequence of hypothesized sub-word units (119) being output from the speech recognition model (200) at a corresponding output step;
determining a supervision penalty term (390) based on the sequence of hypothesized subword units and the plurality of subword units (119) selected from the input word (302) by the subword segmentation routine (300); and
parameters of the speech recognition model (200) are updated based on the supervised loss term (390).
7. The method (600) of claim 6, wherein the speech recognition model (200) comprises a recurrent neural network transducer (RNN-T) model architecture including an audio encoder (210), a predictive network (220), and a federated network (230).
8. The method (600) of claim 7, wherein the audio encoder (210) comprises a plurality of long-term memory (LSTM) layers.
9. The method (600) of claim 7, wherein the audio encoder (210) comprises a plurality of transducer layers or a conformal layer.
10. The method (600) of claim 6, wherein the speech recognition model (200) comprises an listen, attention, spelling (LAS) model architecture comprising an audio encoder (211), an attention mechanism (221), and a decoder (231).
11. A system (100) comprising:
data processing hardware (710); and
memory hardware (720), the memory hardware (720) in communication with the data processing hardware (710) and storing instructions that, when executed on the data processing hardware (710), cause the data processing hardware (710) to perform operations comprising:
receiving an input word (302) to be divided into a plurality of subword units (119); and
a subword segmentation routine (300) is performed to segment the input word (302) into the plurality of subword units (119) by:
accessing a training word set (350) of the subword unit (119); and
-selecting the plurality of subword units (119) from the input word (302) by greedily finding the longest subword unit present in the training word set (350) from the input word (302) until the end of the input word (302) is reached.
12. The system (100) of claim 11, wherein selecting the plurality of subword units (119) includes, for each corresponding position of a plurality of different positions of the input word (302):
Identifying all possible candidate subword units (119) from the input word (302) that are present in the training word set (350) at the corresponding locations; and
-randomly sampling from all the possible candidate subword units (119) by assigning a 1-p probability to the longest one of the possible candidate subword units (119) and dividing the remaining p probabilities evenly among all the possible candidate subword units (119) from the input word (302) at the corresponding positions.
13. The system (100) of claim 11 or 12, wherein the operations further comprise, prior to the execution of the subword segmentation routine (300), creating a misspelling of the input word (302) by randomly deleting characters from the input word (302) independently using pre-specified probabilities.
14. The system (100) of any of claims 11-13, wherein the operations further comprise, prior to performing the subword segmentation routine (300), creating a misspelling of the input word (302) by:
pre-specifying probabilities for exchanging sequences of adjacent character pairs; and
for each adjacent character pair in the input word (302), exchanging an order of the characters from the adjacent character pair in the input word (302) based on the pre-specified probabilities.
15. The system (100) of claim 14, wherein the order of any given character in the input word (302) is limited to at most one exchange.
16. The system (100) of any of claims 11-15, wherein the operations further comprise:
receiving training examples including training audio data (110T) characterizing an utterance (106) of the input word (302);
processing the training audio data (110T) to generate a speech recognition result (201) for the utterance (106) of the input word (302) for output by a speech recognition model (200), the speech recognition result (201) comprising a sequence of hypothesized sub-word units, each hypothesized sub-word unit in the sequence of hypothesized sub-word units (119) being output from the speech recognition model (200) at a corresponding output step;
determining a supervision penalty term (390) based on the sequence of hypothesized subword units and the plurality of subword units (119) selected from the input word (302) by the subword segmentation routine (300); and
parameters of the speech recognition model (200) are updated based on the supervised loss term (390).
17. The system (100) of claim 16, wherein the speech recognition model (200) comprises a recurrent neural network transducer (RNN-T) model architecture including an audio encoder (210), a predictive network (220), and a federated network (230).
18. The system (100) of claim 17, wherein the audio encoder (210) includes a plurality of long-term memory (LSTM) layers.
19. The system (100) of claim 17, wherein the audio encoder (210) includes a plurality of transducer layers or a conformal layer.
20. The system (100) of claim 16, wherein the speech recognition model (200) comprises an listen, attention, spelling (LAS) model architecture comprising an audio encoder (211), an attention mechanism (221), and a decoder (231).
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US202163166958P | 2021-03-26 | 2021-03-26 | |
US63/166,958 | 2021-03-26 | ||
PCT/US2022/021621 WO2022204334A1 (en) | 2021-03-26 | 2022-03-24 | Regularizing word segmentation |
Publications (1)
Publication Number | Publication Date |
---|---|
CN117355840A true CN117355840A (en) | 2024-01-05 |
Family
ID=81345955
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202280025027.9A Pending CN117355840A (en) | 2021-03-26 | 2022-03-24 | Regularized word segmentation |
Country Status (6)
Country | Link |
---|---|
US (1) | US20220310061A1 (en) |
EP (1) | EP4305544A1 (en) |
JP (1) | JP2024512607A (en) |
KR (1) | KR20230156795A (en) |
CN (1) | CN117355840A (en) |
WO (1) | WO2022204334A1 (en) |
Family Cites Families (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN111368996B (en) * | 2019-02-14 | 2024-03-12 | 谷歌有限责任公司 | Retraining projection network capable of transmitting natural language representation |
GB201916307D0 (en) * | 2019-11-08 | 2019-12-25 | Polyal Ltd | A dialogue system, a method of obtaining a response from a dialogue system, and a method of training a dialogue system |
CN114270434A (en) * | 2019-12-04 | 2022-04-01 | 谷歌有限责任公司 | Two-pass end-to-end speech recognition |
-
2022
- 2022-03-23 US US17/656,225 patent/US20220310061A1/en active Pending
- 2022-03-24 CN CN202280025027.9A patent/CN117355840A/en active Pending
- 2022-03-24 EP EP22717968.6A patent/EP4305544A1/en active Pending
- 2022-03-24 JP JP2023558846A patent/JP2024512607A/en active Pending
- 2022-03-24 WO PCT/US2022/021621 patent/WO2022204334A1/en active Application Filing
- 2022-03-24 KR KR1020237036313A patent/KR20230156795A/en unknown
Also Published As
Publication number | Publication date |
---|---|
WO2022204334A1 (en) | 2022-09-29 |
JP2024512607A (en) | 2024-03-19 |
KR20230156795A (en) | 2023-11-14 |
US20220310061A1 (en) | 2022-09-29 |
EP4305544A1 (en) | 2024-01-17 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN113692616B (en) | Phoneme-based contextualization for cross-language speech recognition in an end-to-end model | |
CN113924619A (en) | Large-scale multi-language speech recognition through streaming end-to-end model | |
US11929060B2 (en) | Consistency prediction on streaming sequence models | |
US11615779B2 (en) | Language-agnostic multilingual modeling using effective script normalization | |
JP7351018B2 (en) | Proper noun recognition in end-to-end speech recognition | |
CN116250038A (en) | Transducer of converter: unified streaming and non-streaming speech recognition model | |
CN116670757A (en) | Concatenated encoder for simplified streaming and non-streaming speech recognition | |
CN117378004A (en) | Supervised and unsupervised training with loss of alignment of sequences | |
CN117043856A (en) | End-to-end model on high-efficiency streaming non-recursive devices | |
US11823697B2 (en) | Improving speech recognition with speech synthesis-based model adapation | |
JP2024512579A (en) | Lookup table recurrent language model | |
US20220310061A1 (en) | Regularizing Word Segmentation | |
US20230298565A1 (en) | Using Non-Parallel Voice Conversion for Speech Conversion Models | |
US20240013777A1 (en) | Unsupervised Data Selection via Discrete Speech Representation for Automatic Speech Recognition | |
WO2024086265A1 (en) | Context-aware end-to-end asr fusion of context, acoustic and text representations | |
CN117378005A (en) | Multilingual re-scoring model for automatic speech recognition |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |