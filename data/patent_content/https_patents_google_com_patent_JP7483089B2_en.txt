JP7483089B2 - Personalized automatic video cropping - Google Patents
Personalized automatic video cropping Download PDFInfo
- Publication number
- JP7483089B2 JP7483089B2 JP2023060294A JP2023060294A JP7483089B2 JP 7483089 B2 JP7483089 B2 JP 7483089B2 JP 2023060294 A JP2023060294 A JP 2023060294A JP 2023060294 A JP2023060294 A JP 2023060294A JP 7483089 B2 JP7483089 B2 JP 7483089B2
- Authority
- JP
- Japan
- Prior art keywords
- crop
- score
- video
- frame
- implementations
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
- 238000000034 method Methods 0.000 claims description 82
- 238000004458 analytical method Methods 0.000 claims description 18
- 238000009432 framing Methods 0.000 claims description 17
- 230000001815 facial effect Effects 0.000 claims description 15
- 238000001514 detection method Methods 0.000 claims description 12
- 241001465754 Metazoa Species 0.000 claims description 11
- 238000012015 optical character recognition Methods 0.000 claims description 7
- 230000008859 change Effects 0.000 claims description 2
- 238000010801 machine learning Methods 0.000 description 41
- 230000015654 memory Effects 0.000 description 23
- 238000012545 processing Methods 0.000 description 22
- 238000012549 training Methods 0.000 description 14
- 238000010586 diagram Methods 0.000 description 13
- 238000004891 communication Methods 0.000 description 12
- 230000006870 function Effects 0.000 description 12
- 238000013528 artificial neural network Methods 0.000 description 8
- 230000008569 process Effects 0.000 description 8
- 230000008901 benefit Effects 0.000 description 5
- 230000003287 optical effect Effects 0.000 description 5
- 230000002123 temporal effect Effects 0.000 description 4
- 238000004590 computer program Methods 0.000 description 3
- 230000000694 effects Effects 0.000 description 3
- 239000011521 glass Substances 0.000 description 3
- 238000009499 grossing Methods 0.000 description 3
- 239000011159 matrix material Substances 0.000 description 3
- 230000006855 networking Effects 0.000 description 3
- 230000001133 acceleration Effects 0.000 description 2
- 230000009471 action Effects 0.000 description 2
- 230000004913 activation Effects 0.000 description 2
- 238000004364 calculation method Methods 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 238000007726 management method Methods 0.000 description 2
- 230000001537 neural effect Effects 0.000 description 2
- 238000005457 optimization Methods 0.000 description 2
- 239000004065 semiconductor Substances 0.000 description 2
- 241000282472 Canis lupus familiaris Species 0.000 description 1
- 241000282326 Felis catus Species 0.000 description 1
- 241000282412 Homo Species 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- 238000003491 array Methods 0.000 description 1
- 230000003190 augmentative effect Effects 0.000 description 1
- 238000013475 authorization Methods 0.000 description 1
- 230000005540 biological transmission Effects 0.000 description 1
- 239000006227 byproduct Substances 0.000 description 1
- 230000000981 bystander Effects 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 238000013527 convolutional neural network Methods 0.000 description 1
- 238000010348 incorporation Methods 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 230000007246 mechanism Effects 0.000 description 1
- 238000003062 neural network model Methods 0.000 description 1
- 230000008520 organization Effects 0.000 description 1
- 238000005192 partition Methods 0.000 description 1
- 230000004044 response Effects 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 238000012706 support-vector machine Methods 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Images
Description
関連出願
本出願は、２０１９年１２月１３日に出願された、「Personalized Automatic Video Cropping（パーソナライズされた自動ビデオクロッピング）」と題された米国仮特許出願
第６２／９４８，１７９号の利益を主張し、その全体が本明細書において参照により援用される。
RELATED APPLICATIONS This application claims the benefit of U.S. Provisional Patent Application No. 62/948,179, entitled "Personalized Automatic Video Cropping," filed December 13, 2019, the entirety of which is incorporated by reference herein.
背景
デバイスで動画（および画像）をレビューする場合、デバイスの表示アスペクト比および／または方向が、メディアのアスペクト比に一致しないことがよくある。その結果、メディアは、表示用にレターボックス化されることが多い（たとえば、側面に大きな黒枠があり、枠の間の動画サイズまたは静止画サイズが縮小されている）。場合によっては、ビューアソフトウェアアプリケーションは、レターボックス化を避けるために、オリジナルのメディアをクロップすることがある。
Background When reviewing video (and images) on a device, the display aspect ratio and/or orientation of the device often does not match the aspect ratio of the media. As a result, the media is often letterboxed for display (e.g., with large black borders on the sides and a reduced video or still size between the borders). In some cases, the viewer software application may crop the original media to avoid letterboxing.
本明細書で提供される背景の説明は、本開示の文脈を一般的に提示することを目的とする。本背景の項に記載されている範囲での本願発明者の業績、および出願時に先行技術として認められない可能性のある説明の側面は、本開示に対する先行技術として明示的にも黙示的にも認められるものでない。 The background description provided herein is intended to generally present the context of the present disclosure. The inventors' work to the extent described in this Background section, and aspects of the description that may not be admitted as prior art at the time of filing, are not admitted expressly or impliedly as prior art to the present disclosure.
概要
いくつかの実装は方法を備え得る。方法は、複数のフレームを含む入力動画を取得することと、入力動画の各フレームにおける１つ以上のクロップ候補領域について、フレームごとのクロップスコアを決定することとを備え得る。方法はさらに、訓練済み機械学習モデルを用いて、入力動画の各フレーム内の１つ以上のクロップ候補領域について、顔信号を生成することと、１つ以上のクロップ候補領域の顔信号に基づいて、各フレームごとのクロップスコアを調整することとを備え得る。いくつかの実装では、顔信号は、少なくとも１つの重要な顔がクロップ候補領域で検出されるかどうかを示してもよい。
Some implementations may include a method. The method may include obtaining an input video including a plurality of frames and determining a per-frame crop score for one or more crop candidate regions in each frame of the input video. The method may further include generating a face signal for the one or more crop candidate regions in each frame of the input video using a trained machine learning model and adjusting the per-frame crop score based on the face signals of the one or more crop candidate regions. In some implementations, the face signal may indicate whether at least one significant face is detected in the crop candidate region.
方法はさらに、動きコストと、１つ以上のクロップ候補領域について調整されたフレームごとのクロップスコアとに基づいて、入力動画についてクロップ領域位置を表す最小コスト経路を決定することと、最小コスト経路に沿って、クロップ領域位置に対応するクロップキーフレーミングを生成することとを備え得、クロップキーフレーミングは、開始フレームと、終了フレームと、クロップ領域位置とを含む。方法はさらに、入力動画の対応する入力アスペクト比または入力方向と異なる出力アスペクト比または出力方向のうちの１つ以上を有する修正済み動画を出力することを備え得、入力アスペクト比または入力方向は、入力動画の取込み中に使用されるパラメータである。 The method may further comprise determining a minimum cost path representing the crop region location for the input video based on the motion cost and the adjusted per-frame crop score for one or more crop candidate regions, and generating a crop key framing along the minimum cost path corresponding to the crop region location, the crop key framing including a start frame, an end frame, and the crop region location. The method may further comprise outputting a modified video having one or more of an output aspect ratio or output orientation different from a corresponding input aspect ratio or input orientation of the input video, the input aspect ratio or input orientation being parameters used during capture of the input video.
いくつかの実装では、各フレームごとのクロップスコアを調整することは、顔が、フレームごとのクロップスコアに対応するクロップ候補領域に存在すると判断される場合、フレームごとのクロップスコアを第１の値だけ増加させること、または、少なくとも重要な顔が、フレームごとのクロップスコアに対応するクロップ候補領域に存在すると判断される場合、フレームごとのクロップスコアを第２の値だけ増加させることのうちの１つを含み、第２の値は第１の値より大きい。 In some implementations, adjusting the crop score for each frame includes one of increasing the per-frame crop score by a first value if a face is determined to be present in a crop candidate region corresponding to the per-frame crop score, or increasing the per-frame crop score by a second value if at least a significant face is determined to be present in a crop candidate region corresponding to the per-frame crop score, the second value being greater than the first value.
方法はさらに、クロップキーフレーミングの品質スコアを決定することと、品質スコアに基づいて、入力動画の自動ビデオクロッピングを行うこととを備え得る。方法はさらに、クロップキーフレーミングの信頼度スコアを決定することと、信頼度スコアに基づいて、入力動画の自動ビデオクロッピングを行うこととを備え得る。 The method may further comprise determining a quality score for the crop key framing and performing automatic video cropping of the input video based on the quality score. The method may further comprise determining a confidence score for the crop key framing and performing automatic video cropping of the input video based on the confidence score.
いくつかの実装では、フレームごとのクロップスコアを決定することは、クロップ候補領域ごとに、美的スコア、顔分析スコア、またはアクティブスピーカプレゼンスのうちの１つ以上を決定することを含む。いくつかの実装では、クロップキーフレーミングを生成することは、２つのキーフレームの間で補間することを含む。いくつかの実装では、補間することは、ベジエスプラインを適用することを含む。 In some implementations, determining a per-frame crop score includes determining, for each crop candidate region, one or more of an aesthetic score, a facial analysis score, or an active speaker presence. In some implementations, generating the crop keyframing includes interpolating between two keyframes. In some implementations, the interpolating includes applying a Bézier spline.
いくつかの実装では、顔信号を生成することは、１つ以上のパーソナライズされたパラメータにアクセスすることを含む。いくつかの実装では、１つ以上のパーソナライズされたパラメータは、１つ以上の重要な顔についての顔識別情報を含む。いくつかの実装では、修正済み動画を出力することは、修正済み動画をディスプレイに表示することを含む。 In some implementations, generating the face signal includes accessing one or more personalized parameters. In some implementations, the one or more personalized parameters include facial identification information for one or more significant faces. In some implementations, outputting the modified video includes displaying the modified video on a display.
方法はさらに、入力動画を取得する前に、デバイスにおいて動画再生コマンドを受信することと、動画再生コマンドを受信することに応答して、デバイスについてデバイス方向と表示アスペクト比とを検出することとを備え得る。方法はさらに、デバイスについて、デバイス方向と表示アスペクト比とに基づいて、クロップ領域を決定することを備え得る。 The method may further comprise receiving a video play command at the device prior to obtaining the input video, and detecting a device orientation and a display aspect ratio for the device in response to receiving the video play command. The method may further comprise determining a crop region for the device based on the device orientation and the display aspect ratio.
いくつかの実装は、１つ以上のプロセッサによって実行されると、１つ以上のプロセッサに動作を実行させるソフトウェア命令を格納した非一時的なコンピュータ読取可能媒体を備え得る。動作は、複数のフレームを含む入力動画を取得することと、入力動画の各フレームにおける１つ以上のクロップ候補領域について、フレームごとのクロップスコアを決定することと、訓練済み機械学習モデルを用いて、入力動画の各フレーム内の１つ以上のクロップ候補領域について、顔信号を生成することとを含み得る。いくつかの実装では、顔信号は、少なくとも１つの重要な顔がクロップ候補領域で検出されるかどうかを示してもよい。動作はさらに、１つ以上のクロップ候補領域の顔信号に基づいて、各フレームごとのクロップスコアを調整することと、動きコストと、１つ以上のクロップ候補領域について調整されたフレームごとのクロップスコアとに基づいて、入力動画についてクロップ領域位置を表す最小コスト経路を決定することとを備え得る。 Some implementations may include a non-transitory computer-readable medium having stored thereon software instructions that, when executed by one or more processors, cause the one or more processors to perform operations. The operations may include obtaining an input video including a plurality of frames, determining per-frame crop scores for one or more crop candidate regions in each frame of the input video, and generating a face signal for the one or more crop candidate regions in each frame of the input video using a trained machine learning model. In some implementations, the face signal may indicate whether at least one significant face is detected in the crop candidate region. The operations may further include adjusting each per-frame crop score based on the face signals of the one or more crop candidate regions, and determining a minimum cost path representing a crop region location for the input video based on the motion cost and the adjusted per-frame crop scores for the one or more crop candidate regions.
動作はさらに、最小コスト経路に沿って、クロップ領域位置に対応するクロップキーフレーミングを生成することを含み得、クロップキーフレーミングは、開始フレームと、終了フレームと、クロップ領域位置とを含み、動作はさらに、入力動画の対応する入力アスペクト比または入力方向と異なる出力アスペクト比または出力方向のうちの１つ以上を有する修正済み動画を出力することを含み得、入力アスペクト比または入力方向は、入力動画の取込み中に使用されるパラメータである。 The operations may further include generating a crop key framing along a minimum cost path that corresponds to the crop area locations, the crop key framing including a start frame, an end frame, and a crop area location, and the operations may further include outputting a modified video having one or more of an output aspect ratio or output orientation that differs from a corresponding input aspect ratio or input orientation of the input video, the input aspect ratio or input orientation being parameters used during capture of the input video.
いくつかの実装では、各フレームごとのクロップスコアを調整することは、顔が、フレームごとのクロップスコアに対応するクロップ候補領域に存在すると判断される場合、フレームごとのクロップスコアを第１の値だけ増加させること、または、少なくとも重要な顔が、フレームごとのクロップスコアに対応するクロップ候補領域に存在すると判断される場合、フレームごとのクロップスコアを第２の値だけ増加させることのうちの１つを含み、第２の値は、第１の値より大きい。 In some implementations, adjusting the crop score for each frame includes one of increasing the per-frame crop score by a first value if a face is determined to be present in a crop candidate region corresponding to the per-frame crop score, or increasing the per-frame crop score by a second value if at least a significant face is determined to be present in a crop candidate region corresponding to the per-frame crop score, the second value being greater than the first value.
動作はさらに、クロップキーフレーミングの品質スコアを決定することと、品質スコア
に基づいて、入力動画の自動ビデオクロッピングを行うこととを含み得る。動作はさらに、クロップキーフレーミングの信頼度スコアを決定することと、信頼度スコアに基づいて、入力動画の自動ビデオクロッピングを行うこととを含み得る。
The operations may further include determining a quality score for the crop key framing and performing automatic video cropping of the input video based on the quality score. The operations may further include determining a confidence score for the crop key framing and performing automatic video cropping of the input video based on the confidence score.
いくつかの実装では、フレームごとのクロップスコアを決定することは、クロップ候補領域ごとに、美的スコア、顔分析スコア、またはアクティブスピーカプレゼンスのうちの１つ以上を決定することを含む。いくつかの実装では、クロップキーフレーミングを生成することは、２つのキーフレームの間で補間することを含む。いくつかの実装では、補間することは、ベジエスプラインを適用することを含む。いくつかの実装では、顔信号を生成することは、１つ以上のパーソナライズされたパラメータにアクセスすることを含む。 In some implementations, determining a per-frame crop score includes determining, for each crop candidate region, one or more of an aesthetic score, a facial analysis score, or an active speaker presence. In some implementations, generating a crop keyframing includes interpolating between two keyframes. In some implementations, interpolating includes applying a Bézier spline. In some implementations, generating a face signal includes accessing one or more personalized parameters.
いくつかの実装は、非一時的なコンピュータ読取可能媒体に結合された１つ以上のプロセッサを備え、非一時的なコンピュータ読取可能媒体は、１つ以上のプロセッサによって実行されると、１つ以上のプロセッサに動作を実行させるソフトウェア命令を格納する。動作は、複数のフレームを含む入力動画を取得することと、入力動画の各フレームにおける１つ以上のクロップ候補領域について、フレームごとのクロップスコアを決定することと、訓練済み機械学習モデルを用いて、入力動画の各フレーム内の１つ以上のクロップ候補領域について、顔信号を生成することとを含み得る。いくつかの実装では、顔信号は、少なくとも１つの重要な顔がクロップ候補領域で検出されるかどうかを示してもよい。動作はさらに、１つ以上のクロップ候補領域の顔信号に基づいて、各フレームごとのクロップスコアを調整することと、動きコストと、１つ以上のクロップ候補領域について調整されたフレームごとのクロップスコアとに基づいて、入力動画についてクロップ領域位置を表す最小コスト経路を決定することとを含み得る。 Some implementations include one or more processors coupled to a non-transitory computer-readable medium, the non-transitory computer-readable medium storing software instructions that, when executed by the one or more processors, cause the one or more processors to perform operations. The operations may include obtaining an input video including a plurality of frames, determining per-frame crop scores for one or more crop candidate regions in each frame of the input video, and generating a face signal for the one or more crop candidate regions in each frame of the input video using a trained machine learning model. In some implementations, the face signal may indicate whether at least one significant face is detected in the crop candidate region. The operations may further include adjusting each per-frame crop score based on the face signals of the one or more crop candidate regions, and determining a minimum cost path representing a crop region location for the input video based on the motion cost and the adjusted per-frame crop scores for the one or more crop candidate regions.
動作はさらに、最小コスト経路に沿って、クロップ領域位置に対応するクロップキーフレーミングを生成することを含み得、クロップキーフレーミングは、開始フレームと、終了フレームと、クロップ領域位置とを含み、動作はさらに、入力動画の対応する入力アスペクト比または入力方向と異なる出力アスペクト比または出力方向のうちの１つ以上を有する修正済み動画を出力することを含み得、入力アスペクト比または入力方向は、入力動画の取込み中に使用されるパラメータである。 The operations may further include generating a crop key framing along a minimum cost path that corresponds to the crop area locations, the crop key framing including a start frame, an end frame, and a crop area location, and the operations may further include outputting a modified video having one or more of an output aspect ratio or output orientation that differs from a corresponding input aspect ratio or input orientation of the input video, the input aspect ratio or input orientation being parameters used during capture of the input video.
詳細な説明
本明細書に記載されるいくつかの実装は、動画を自動的にクロップする方法、システム、およびコンピュータ読取可能媒体に関する。説明される実装は、訓練済みの機械学習モデルを使用して、パーソナライズされたパラメータで動画を自動的にクロップすることができる。モデルのための訓練データは、ユーザの許可を得てアクセスされる、ユーザについてのパーソナライズされた情報を含んでもよい。パーソナライズされた情報は、ローカルストレージ（たとえば、デバイス）に格納された顔について顔識別情報を含み得る。
DETAILED DESCRIPTION Some implementations described herein relate to methods, systems, and computer-readable media for automatically cropping videos. The described implementations can use a trained machine learning model to automatically crop videos with personalized parameters. Training data for the model may include personalized information about the user, accessed with the user's permission. The personalized information may include face identification information for faces stored in local storage (e.g., on the device).
本明細書に記載されるいくつかの実装は、パーソナライズされたビデオクロッピングを自動的に実行する方法、システム、およびコンピュータ読取可能媒体に関する。異なる動画プラットフォームおよび配信デバイスは、４：３（ランドスケープ、横長）、９：１６（ポートレート、縦長）、および１：１（スクエア）を含む異なるアスペクト比を有してもよく、ここで、最初の数字は動画の幅を指し、２番目の数字は動画の高さを指す。 Some implementations described herein relate to methods, systems, and computer-readable media that automatically perform personalized video cropping. Different video platforms and delivery devices may have different aspect ratios, including 4:3 (landscape), 9:16 (portrait), and 1:1 (square), where the first number refers to the width of the video and the second number refers to the height of the video.
記載された技術は、ユーザにとって重要な（たとえば、ユーザの画像ライブラリ内の画像／動画に基づく、ユーザにとって認識された、なじみのある、または既知の）顔などのパーソナライズされたパラメータに基づいて、ユーザを妨げることなく、動画再生時にパーソナライズされたビデオクロッピング（たとえば、ポートレートフォーマットまたはスクエアフォーマットで表示するための横方向動画の表示用のクロッピング）を自動的に実行することができる。ユーザは、ユーザによって取込まれた、または他の態様ではライブラリに追加された複数の画像および／または動画（たとえば、他のユーザによってユーザに共有された画像）を含む画像ならびに動画ライブラリ（たとえば、画像管理ソフトウェアアプリケーションによって格納および管理された画像）を有してもよい。ライブラリは、ユーザデバイス（たとえば、スマートフォン）上および／またはサーバ（たとえば、クラウドベースの画像／動画ホスティングサービス）上でローカルでもよい。たとえば、ユーザは、スマートフォン、デジタルカメラ、ウェアラブルデバイスなどの１つ以上のデバイスを使用して、さまざまな人物の画像および／または動画を取込み、そのような画像をライブラリに保存してもよい。ライブラリは、ユーザにとって既知の人物（または動物）、たとえば、家族、友人、同僚、ペットなどの画像／動画を含んでもよい。いくつかの実装では、重要な顔はまた、ユーザのライブラリにない顔を含んでもよいが、ユーザの許可を得てアクセスした、ユーザのソーシャルグラフ、ソーシャルメディアアカウント、電子メールアカウント、ユーザの電子アドレス帳といった、画像の他のソースに基づいて識別されてもよい。 The described techniques can automatically perform personalized video cropping (e.g., cropping for display of landscape video to display in portrait or square format) during video playback without disturbing the user based on personalized parameters such as faces that are important to the user (e.g., based on images/videos in the user's image library, recognized, familiar, or known to the user). A user may have an image and video library (e.g., images stored and managed by an image management software application) that includes multiple images and/or videos (e.g., images shared to the user by other users) captured by the user or otherwise added to the library. The library may be local on the user device (e.g., smartphone) and/or on a server (e.g., cloud-based image/video hosting service). For example, a user may use one or more devices, such as a smartphone, digital camera, wearable device, etc., to capture images and/or videos of various people and store such images in the library. The library may include images/videos of people (or animals) known to the user, such as family members, friends, coworkers, pets, etc. In some implementations, significant faces may also include faces that are not in the user's library, but may be identified based on other sources of images, such as the user's social graph, social media accounts, email accounts, or the user's electronic address book, accessed with the user's permission.
ユーザがライブラリまたは他の情報源のいずれにもアクセスする許可を拒否する場合、それらの情報源にはアクセスできず、重要顔判定は行われない。さらに、ユーザは、１つ以上の顔を、重要な顔として認識される、および／または含まれることから除外可能である。さらに、ユーザには、重要な顔を手動で示すオプション（たとえば、ユーザインターフェイス）を提供することができる。本明細書で使用する顔という用語は、人間の顔、および／または顔検出技術を使用して検出することができる他の任意の顔（たとえば、ペットまたは他の動物の顔）を指し得る。 If the user denies permission to access any of the library or other information sources, those sources will not be accessed and no significant face determination will be made. Additionally, the user can exclude one or more faces from being recognized and/or included as significant faces. Additionally, the user can be provided with an option (e.g., in the user interface) to manually indicate significant faces. As used herein, the term face may refer to a human face and/or any other face that can be detected using face detection technology (e.g., the face of a pet or other animal).
画像管理アプリケーションの中には、ユーザの許可を得て有効化される、画像もしくは動画中の人物および／またはペットの顔を検出する機能を含むものがある。ユーザが許可すれば、そのような画像管理アプリケーションは、ユーザのライブラリ内の画像／動画内
の顔を検出し、各顔の出現頻度を決定可能である。たとえば、配偶者、兄弟、親、親友などの人物の顔は、ユーザのライブラリ内の画像／動画において高い頻度で出現することがあるが、（たとえば、公共の場における）傍観者などの他の人物は、出現頻度が低いことがある。いくつかの実装では、高い頻度で出現する顔は、重要な顔として識別されてもよい。いくつかの実装では、たとえば、ライブラリが、ユーザが顔にタグ付けまたはラベル付けすることを可能にする場合、ユーザは、自分のライブラリで出現する顔に対して名前（または他の情報）を与えることができる。これらの実装では、ユーザが名前または他の情報を与えた顔は、重要な顔として識別されてもよい。画像ライブラリで検出された他の顔は、重要でない顔として識別されてもよい。
Some image management applications include a feature, enabled with the user's permission, to detect faces of people and/or pets in images or videos. If the user permits, such an image management application can detect faces in images/videos in the user's library and determine the frequency of occurrence of each face. For example, faces of people such as spouses, siblings, parents, best friends, etc. may appear frequently in images/videos in the user's library, while other people such as bystanders (e.g., in public places) may appear less frequently. In some implementations, faces that appear frequently may be identified as important faces. In some implementations, for example, if the library allows the user to tag or label faces, the user may give names (or other information) to faces that appear in their library. In these implementations, faces for which the user has given names or other information may be identified as important faces. Other faces detected in the image library may be identified as unimportant faces.
さまざまな実装により、ビデオクロッピングおよび重要な顔の判定は、クライアントデバイス上でローカルに実行され、ネットワーク接続を必要としない。記載される技術によって、動画が、動画のアスペクト比と異なるアスペクト比を有するデバイスで、または動画が取込まれたもしくは格納された方向には適さないデバイスの方向（たとえば、縦長と横長）で視聴されている場合に、動画再生体験の改善が可能になる。説明される技術は、動画を再生する任意のデバイス、たとえば、モバイルデバイスに実装することができる。 In various implementations, video cropping and significant face determination are performed locally on the client device and do not require a network connection. The described techniques enable an improved video playback experience when videos are viewed on devices with aspect ratios different from the aspect ratio of the video or in device orientations (e.g., portrait and landscape) that are not suitable for the orientation in which the video was captured or stored. The described techniques can be implemented on any device that plays videos, e.g., mobile devices.
いくつかの実装では、自動ビデオクロッピングは、フレームごとのクロップスコアリング、時間的コヒーレンス、および動き平滑化の３つの段階を含み得る。フレームごとのクロップスコアリングは、画像ベースであり得るが、これはノイズが多く、さまざまな異なるスコアを含み得る。いくつかの実装では、ヒューリスティックな組合わせを使用して、クロップ候補領域について単一のフレームごとのスコアを生成することができる。第２の段階は、時間的コヒーレンスを含み得、これは、空間および時間を通じて滑らかで最適な経路に適合させる動作を含み得る。時間的コヒーレンスは、シーンの動きの表現を含み得る。第３の段階は、動きの平滑化およびヒューリスティックの取込みを含み得る。この段階では、グローバルに処理できない可能性のある動画の側面について、局所的な最適化を行うことができる。特定のヒューリスティックとルールは、特定のケースに対処するために適用可能である。 In some implementations, automatic video cropping may include three stages: frame-by-frame crop scoring, temporal coherence, and motion smoothing. Frame-by-frame crop scoring may be image-based, which may be noisy and include a variety of different scores. In some implementations, a combination of heuristics may be used to generate a single frame-by-frame score for crop candidate regions. The second stage may include temporal coherence, which may include operations that fit smooth, optimal paths through space and time. Temporal coherence may include a representation of scene motion. The third stage may include motion smoothing and the incorporation of heuristics. This stage may perform local optimizations for aspects of the video that may not be globally addressable. Specific heuristics and rules may be applicable to address specific cases.
図１は、本明細書で説明するいくつかの実装で使用され得る、ネットワーク環境１００の例を示すブロック図である。いくつかの実装では、ネットワーク環境１００は、１つ以上のサーバシステム、たとえば、図１の例ではサーバシステム１０２を含む。サーバシステム１０２は、たとえば、ネットワーク１３０と通信可能である。サーバシステム１０２は、サーバデバイス１０４と、データベース１０６または他のストレージデバイスとを含み得る。いくつかの実装では、サーバデバイス１０４は、ビデオアプリケーション１５８を提供し得る。
1 is a block diagram illustrating an example of a
また、ネットワーク環境１００は、１つ以上のクライアントデバイス、たとえば、クライアントデバイス１２０，１２２，１２４および１２６を含むことができ、これらは、ネットワーク１３０を介して互いに、ならびに／またはサーバシステム１０２および／もしくは第２のサーバシステム１４０と通信可能である。ネットワーク１３０は、インターネット、ローカルエリアネットワーク（ＬＡＮ）、無線ネットワーク、スイッチまたはハブ接続などのうちの１つ以上を含む任意のタイプの通信ネットワークであり得る。
The
図示を容易にするために、図１では、サーバシステム１０２、サーバデバイス１０４、データベース１０６について１つのブロックを示し、クライアントデバイス１２０，１２２，１２４および１２６について４つのブロックを示す。サーバブロック１０２，１０４および１０６は、複数のシステム、サーバデバイス、およびネットワークデータベースを表してもよく、ブロックは、図示とは異なる構成で提供可能である。たとえば、サーバシステム１０２は、ネットワーク１３０を介して他のサーバシステムと通信可能な複数のサ
ーバシステムを表すことができる。いくつかの実装では、サーバシステム１０２は、たとえば、クラウドホスティングサーバを含み得る。いくつかの例では、データベース１０６および／または他のストレージデバイスは、サーバデバイス１０４とは別個であり、かつ、ネットワーク１３０を介してサーバデバイス１０４および他のサーバシステムと通信可能なサーバシステムブロック（複数可）において、提供することができる。
For ease of illustration, FIG. 1 shows one block for
また、任意の数のクライアントデバイスが存在してもよい。各クライアントデバイスは、通信可能な任意のタイプの電子デバイス、たとえば、デスクトップコンピュータ、ラップトップコンピュータ、ポータブルデバイスまたはモバイルデバイス、携帯電話、スマートフォン、タブレットコンピュータ、テレビ、ＴＶセットトップボックスまたは娯楽デバイス、ウェアラブルデバイス（たとえば、ディスプレイグラスもしくはゴーグル、腕時計、ヘッドセット、アームバンド、宝石など）、パーソナルデジタルアシスタント（ＰＤＡ）などであり得る。いくつかのクライアントデバイスは、データベース１０６または他のストレージと同様のローカルデータベースを有してもよい。いくつかの実装では、ネットワーク環境１００は、示された構成要素のすべてを有していなくてもよい、および／または、本明細書に記載された要素の代わりに、もしくはそれらに加えて、他のタイプの要素を含む他の要素を有してもよい。
Also, there may be any number of client devices. Each client device may be any type of electronic device capable of communication, such as a desktop computer, a laptop computer, a portable or mobile device, a mobile phone, a smartphone, a tablet computer, a television, a TV set-top box or entertainment device, a wearable device (e.g., display glasses or goggles, a watch, a headset, an armband, jewelry, etc.), a personal digital assistant (PDA), etc. Some client devices may have a local database similar to
さまざまな実装において、エンドユーザＵ１，Ｕ２，Ｕ３およびＵ４は、それぞれのクライアントデバイス１２０，１２２，１２４および１２６を使用して、サーバシステム１０２と、および／または互いに通信し得る。いくつかの例では、ユーザＵ１，Ｕ２，Ｕ３およびＵ４は、それぞれのクライアントデバイスおよび／もしくはサーバシステム１０２上で動作するアプリケーションを介して、ならびに／またはサーバシステム１０２上で実装されるネットワークサービス、たとえばソーシャルネットワークサービスまたは他のタイプのネットワークサービスを介して、互いに対話し得る。たとえば、それぞれのクライアントデバイス１２０，１２２，１２４および１２６は、１つ以上のサーバシステム、たとえばサーバシステム１０２との間でデータを通信してもよい。
In various implementations, end users U1, U2, U3, and U4 may communicate with
いくつかの実装では、サーバシステム１０２は、各クライアントデバイスがサーバシステム１０２にアップロードされた通信コンテンツまたは共有コンテンツを受信できるように、クライアントデバイスに適切なデータを提供し得る。いくつかの例では、ユーザＵ１～Ｕ４は、音声もしくはビデオ会議、音声、動画、またはテキストチャット、または他の通信モードもしくはアプリケーションを介して対話することができる。
In some implementations, the
サーバシステム１０２によって実装されるネットワークサービスは、ユーザがさまざまな通信を行い、リンクおよび関連付けを形成し、画像、テキスト、動画、オーディオ、および他のタイプのコンテンツなどの共有コンテンツをアップロードおよびポストし、ならびに／または他の機能を実行することを可能にするシステムを含み得る。たとえば、クライアントデバイスは、クライアントデバイスに送信またはストリーミングされ、かつサーバおよび／もしくはネットワークサービスを介して（もしくは異なるクライアントデバイスから直接）異なるクライアントデバイスから発信された、またはサーバシステムおよび／もしくはネットワークサービスから発信されたコンテンツポストなどの受信データを表示することが可能である。
The network services implemented by the
いくつかの実装では、クライアントデバイス１２０，１２２，１２４および／または１２６のいずれかが、１つ以上のアプリケーションを提供可能である。たとえば、図１に示すように、クライアントデバイス１２０は、自動ビデオクロッピングアプリケーション１５２を提供可能である。クライアントデバイス１２２～１２６もまた、同様のアプリケーションを提供可能である。自動ビデオクロッピングアプリケーション１５２は、クライアントデバイス１２０のハードウェアおよび／またはソフトウェアを使用して実装されても
よい。異なる実装では、自動ビデオクロッピングアプリケーション１５２は、たとえば、クライアントデバイス１２０～１２４のいずれかで実行される、スタンドアロンクライアントアプリケーションでもよい。自動ビデオクロッピングアプリケーション１５２は、動画に関するさまざまな機能、たとえば、あるアスペクト比から別のアスペクト比に変更するために動画を自動的にクロップすること等を提供してもよい。
In some implementations, any of
クライアントデバイス１２０，１２２，１２４および／または１２６上のユーザインターフェイスは、画像、動画、データ、および他のコンテンツ、ならびに通信、設定、通知、および他のデータを含むユーザコンテンツおよび他のコンテンツの表示を可能にすることができる。このようなユーザインターフェイスは、クライアントデバイス上のソフトウェア、サーバデバイス上のソフトウェア、および／またはサーバデバイス１０４上で実行されるクライアントソフトウェアとサーバソフトウェアとの組合わせを使用して表示することができる。ユーザインターフェイスは、クライアントデバイスの表示デバイス、たとえば、タッチスクリーンまたは他の表示画面、プロジェクタ等によって表示することができる。いくつかの実装では、サーバは、単に、ユーザがネットワークを介して動画をストリーミング／ダウンロードすることを可能にし、ユーザの許可を得て、ユーザによって送信された動画のアップロード／格納を可能にし得る。
User interfaces on
本明細書に記載された特徴の他の実装は、任意のタイプのシステムおよび／またはサービスを使用することができる。たとえば、ソーシャルネットワーキングサービスの代わりに、またはこれに加えて、（たとえば、インターネットに接続されている）他のネットワークサービスを使用することができる。任意のタイプの電子デバイスが、本明細書に記載される特徴を利用することができる。いくつかの実装では、コンピュータネットワークから切断された、またはコンピュータネットワークに断続的に接続された１つ以上のクライアントもしくはサーバデバイス上で、本明細書に記載された１つ以上の機能を提供することができる。 Other implementations of the features described herein may use any type of system and/or service. For example, other network services (e.g., connected to the Internet) may be used instead of or in addition to a social networking service. Any type of electronic device may utilize the features described herein. In some implementations, one or more functions described herein may be provided on one or more client or server devices that are disconnected from or intermittently connected to a computer network.
図２Ａは、いくつかの実装に係るランドスケープフォーマットの動画を示す図である。図２Ｂは、図２Ａに示された動画を、動画がピラーボックス化されている縦方向のデバイスで見たときの図である。図２Ａは、動画をランドスケープモード２０４で表示するユーザデバイス２０２を示す。図２Ｂは、デバイスがポートレートモードであり、縦方向で表示するためにピラーボックス化された（２０６）動画２０４が表示されていることを示す。図２Ｂで分かるように、縦方向で表示するために動画がピラーボックス化される場合、動画が占有するデバイスの表示画面の部分は、実質的に小さくなっている。
Figure 2A illustrates video in landscape format according to some implementations. Figure 2B illustrates the video shown in Figure 2A as it would appear on a portrait-oriented device where the video has been pillarboxed. Figure 2A shows a
図３Ａは、いくつかの実装に係る、水平動画上のクロッピング矩形を示す図である。図３Ｂは、いくつかの実装に係る、縦方向で表示されるクロップされた動画を示す図である。図３Ａは、クロップ領域３０２が破線で示されている横方向の動画の単一フレームを示す図である。クロップ領域３０２は、ｘ位置３０４に設けられている。後述するように、自動クロッピングプロセスは、動画の時間領域にわたってクロップ領域（ランドスケープからポートレートへのクロッピング）用のｘ位置を生成する。より一般的には、このプロセスによって、ソースビデオに対するクロップウィンドウ（ｘ、ｙ、幅、高さ）が生成される。
Figure 3A illustrates a cropping rectangle on a horizontal video, according to some implementations. Figure 3B illustrates cropped video displayed in portrait orientation, according to some implementations. Figure 3A illustrates a single frame of landscape video with a
図４は、いくつかの実装形式に係る、ユーザに合わせてパーソナライズされた動画を自動的にクロップする方法４００の例を示すフロー図である。いくつかの実装では、方法４００は、たとえば、図１に示すように、サーバシステム１０２上で実装することができる。いくつかの実装では、方法４００の一部または全部は、図１に示すような１つ以上のクライアントデバイス１２０，１２２，１２４もしくは１２６、１つ以上のサーバデバイス、および／またはサーバデバイス（複数可）とクライアントデバイス（複数可）との両方
で実装することができる。説明された例では、実装システムは、１つ以上のデジタルプロセッサまたは処理回路（「プロセッサ」）、および１つ以上のストレージデバイス（たとえば、データベース１０６または他のストレージ）を含む。いくつかの実装では、１つ以上のサーバおよび／またはクライアントの異なる構成要素は、方法４００の異なるブロックまたは他の部分を実行することができる。いくつかの例では、第１のデバイスは、方法４００のブロックを実行するものとして説明される。いくつかの実装は、結果またはデータを第１のデバイスに送信することができる１つ以上の他のデバイス（たとえば、他のクライアントデバイスまたはサーバデバイス）によって実行される方法４００の１つ以上のブロックを有し得る。
FIG. 4 is a flow diagram illustrating an example of a method 400 for automatically cropping a video personalized for a user according to some implementation forms. In some implementations, the method 400 can be implemented on the
いくつかの実装では、方法４００、または方法の一部は、システムによって自動的に開始され得る。いくつかの実装では、実装システムは第１のデバイスである。たとえば、方法（またはその一部）は、１つ以上の特定のイベントまたは条件、たとえば、クライアントデバイスでの動画の再生、クライアントデバイスからのアップロードのための動画の準備、および／または方法によって読み取られる設定において指定可能な１つ以上の他の条件の発生に基づいて実行することができる。 In some implementations, method 400, or portions of the method, may be initiated automatically by a system. In some implementations, the implementation system is the first device. For example, the method (or portions thereof) may be executed based on the occurrence of one or more particular events or conditions, e.g., playing a video on a client device, preparing a video for upload from a client device, and/or one or more other conditions that may be specified in settings read by the method.
方法４００は、ブロック４０２で開始可能である。ブロック４０２では、方法２００の実現においてユーザデータを使用するユーザの同意（たとえば、ユーザ許可）が得られているかどうかがチェックされる。たとえば、ユーザデータは、重要な顔および追加のユーザ基準、画像コレクション内のユーザ画像（たとえば、ユーザによって取込まれた画像、ユーザによってアップロードされた画像、または他の態様ではユーザに関連する画像）、ユーザのソーシャルネットワークおよび／またはコンタクトに関する情報、ユーザ特性（アイデンティティ、名前、年齢、性別、職業など）、ソーシャルおよびその他のタイプのアクションおよびアクティビティ、カレンダーおよび予定、ユーザによって作成または送信されたコンテンツ、評価および意見、ユーザの地理位置、過去のユーザデータなどを含み得る。本明細書に記載される方法の１つ以上のブロックは、いくつかの実装において、そのようなユーザデータを使用することができる。ブロック４０２は、自動クロッピングアプリケーションの実行についてのユーザ同意がフレームワークレベルで得られた場合にのみブロック４０４等が起動されるように、自動ビデオクロッピングフレームワークレベルの一部として実行され得る。ユーザ同意が、方法４００においてユーザデータが使用され得る関連ユーザから得られている場合、ブロック４０４において、本明細書の方法のブロックは、それらのブロックについて説明されるようなユーザデータの潜在的な使用で実装され得ると判断され、方法はブロック４０６に進む。ユーザの同意が得られていない場合、ブロック４０６において、ブロックがユーザデータを使用せずに実装されることが決定され、方法はブロック４０６に進む。いくつかの実装では、ユーザ同意が得られていない場合、方法４００の残りは実行されない、および／またはユーザデータを必要とする特定のブロックは実行されない。たとえば、ユーザが許可を与えない場合、ブロック４１２～４１４はスキップされる。また、重要な顔の認識は、ローカルに格納されたデータに基づいて行うことができ、ユーザデバイス上でローカルに実行することができる。ユーザは、特定の重要な顔を認識するもしくは認識しないように指定する、指定を削除する、または重要な顔に基づく自動クロッピングの使用をいつでも停止することができる。
Method 400 can begin at
ブロック４０８で、入力動画が取得される。たとえば、ユーザデバイス上のメモリに格納された動画がアクセスされる。動画は、複数のフレームを含み得る。入力動画は、方向（垂直／水平）およびアスペクト比、たとえば、４：３，１６：９，１８：９などを有する。たとえば、アスペクト比は、動画取込み時に、たとえば、動画を取込むデバイスのカメラパラメータに基づいて選択されてもよい。ブロック４０８の後に、ブロック４１０が続く場合がある。
At
ブロック４１０で、入力動画のフレームごとに、１つ以上のクロップ候補領域についてフレームごとのクロップスコアが決定される。クロップ候補領域は、領域にクロップされた動画が実質的に画面全体（動画がウィンドウ型ユーザインターフェイスで再生される場合はウィンドウ）を占有するように、動画が視聴されるデバイスの視聴方向に一致し、デバイスと同じアスペクト比を有してもよい。たとえば、ランドスケープ（水平寸法が垂直寸法より大きい）であり、４０００×３０００画素のアスペクト比を有する入力動画が、２０００×２０００画素の正方形のディスプレイに表示される場合、各クロップ候補領域は、３０００×３０００画素でもよい（３０００画素の寸法と一致してもよい）。選択された３０００×３０００画素のクロップ領域は、正方形のディスプレイに合わせてスケールされてもよい、たとえば、２０００×２０００画素にスケールダウンされてもよい。より高い解像度のクロップ領域の選択とそれに続くスケーリングは、元のコンテンツの大きな割合を保存することができる。または、２０００×２０００画素の表示に一致するクロップ候補領域が選択されてもよい。
At
クロップスコアは、１つ以上の個別スコアを含み得る。１つ以上のスコアが使用される場合、個別のスコアがどのように１つのスコアに結合されるかを決定するヒューリスティックが存在し得る。個別スコアは、美的スコア（たとえば、５０８から）、顔／人物分析に基づくスコア（たとえば、５０６）、および／またはアクティブスピーカ分析（たとえば、５０４）を含み得る。さらに、いくつかの実装は、オブジェクト検出、ペットもしくは動物検出、または光学文字認識（ＯＣＲ）に基づく１つ以上の追加のスコアを含んでもよい。たとえば、オブジェクト検出技術を使用して識別された顕著なオブジェクトを含むクロップ領域は、顕著なオブジェクトが検出されない、または部分的なオブジェクトのみが検出される領域よりも高いスコアが割り当てられる場合がある。たとえば、動画が自然のシーンを描いている場合、木、山、または他のオブジェクトなどの顕著なオブジェクトを有するクロップ領域は、顕著なオブジェクトを有さない、たとえば、空だけを含むクロップ領域よりも高いスコアが割り当てられてもよい。 The crop score may include one or more individual scores. When more than one score is used, there may be a heuristic that determines how the individual scores are combined into one score. The individual scores may include aesthetic scores (e.g., from 508), scores based on face/person analysis (e.g., 506), and/or active speaker analysis (e.g., 504). Additionally, some implementations may include one or more additional scores based on object detection, pet or animal detection, or optical character recognition (OCR). For example, a crop region that includes a salient object identified using object detection techniques may be assigned a higher score than a region in which no salient object is detected or in which only a partial object is detected. For example, if the video depicts a natural scene, a crop region that has salient objects such as trees, mountains, or other objects may be assigned a higher score than a crop region that does not have a salient object, e.g., includes only the sky.
別の例では、ペット（たとえば、ユーザの許可を得てアクセスされる、ユーザの個人的な画像／動画ライブラリにタグ付けされる犬、猫、もしくは他のペット動物）または他の動物を描写するクロップ領域は、ペットもしくは動物を除外する領域、またはペットもしくは動物を部分的にしか描写しない領域よりも高いスコアを割り当てられる場合がある。さらに別の例では、ＯＣＲを使用して認識されたテキストを含む領域は、より高いスコアが割り当てられる場合がある。たとえば、動画がテキストを含む看板を有する店先を含む場合、看板を含むクロップ領域は、看板を除外する、または部分的にしか描写しないクロップ領域よりも高いスコアを割り当てられてもよい。フレームごとのクロップスコアは、クロップ候補領域（たとえば、動画フレーム内の所与のｘ位置におけるクロップ矩形）についてのスコアを含み得る。ブロック４１０の後に、ブロック４１２が続く場合がある。
In another example, crop regions depicting pets (e.g., dogs, cats, or other pet animals tagged to a user's personal image/video library, accessed with the user's permission) or other animals may be assigned a higher score than regions that exclude the pet or animal or that only partially depict the pet or animal. In yet another example, regions that include text recognized using OCR may be assigned a higher score. For example, if a video includes a storefront with a sign that includes text, crop regions that include the sign may be assigned a higher score than crop regions that exclude or only partially depict the sign. The per-frame crop score may include a score for the crop candidate regions (e.g., crop rectangles at a given x-location in the video frame).
４１２で、顔信号が生成され、パーソナライズされたスコアが決定される。いくつかの実装では、顔信号は、少なくとも１つの重要な顔がクロップ候補領域で検出されるかどうかを示してもよい。いくつかの実装では、パーソナライズされたスコアは、フレーム内の顔を検出し、フレーム内の顔の少なくとも１つが重要な顔に一致するかどうかを決定する顔検出技術を使用して決定される（たとえば、ユーザのライブラリにある人の顔の以前の動画もしくは写真などのユーザが許可したデータに基づいて決定されるような、または視聴しているユーザのソーシャルグラフ接続もしくは電子メール、電話、チャット、動画通話などにおける通信履歴といった、他のユーザが許可した信号により決定されるような）、重要な顔が存在するかどうかに基づくスコアを含み得る。パーソナライズされたスコアは、１つ以上の重要な顔が、機械学習モデルによって決定されるクロッピング候補領域にある度合いを表す機械学習モデルからの信号に基づいて決定することができる。１つ以上の重要な顔がクロップ候補領域にあると判断することに加えて、パーソナライズされたス
コアモジュールは、クロップ候補領域内の１つ以上の重要な顔の位置、たとえば、顔がクロップ候補領域の中心にあるか、クロップ候補領域の端に近いか等を判断することもできる。ブロック４１２の後に、ブロック４１４が続く場合がある。
At 412, a face signal is generated and a personalized score is determined. In some implementations, the face signal may indicate whether at least one significant face is detected in the crop candidate area. In some implementations, the personalized score may include a score based on whether a significant face is present (e.g., as determined based on user-authorized data such as previous videos or photos of the person's face in the user's library, or as determined by other user-authorized signals such as the viewing user's social graph connections or communication history in email, phone, chat, video calls, etc.) using face detection technology that detects faces in the frame and determines whether at least one of the faces in the frame matches a significant face). The personalized score can be determined based on a signal from the machine learning model that represents the degree to which one or more significant faces are in the crop candidate area as determined by the machine learning model. In addition to determining that one or more significant faces are in the crop candidate area, the personalized score module can also determine the location of the one or more significant faces in the crop candidate area, e.g., whether the face is in the center of the crop candidate area, near an edge of the crop candidate area, etc.
４１４で、フレームごとのクロップスコアは、顔信号に基づいて調整される。たとえば、顔がクロップ候補領域で検出された場合、その領域のスコアは、第１の係数だけ大きくされてもよい。クロップ候補領域が重要な顔を含むことが検出された場合、その領域のスコアは、第１の係数よりも大きい第２の係数だけ大きくされてもよい。 At 414, the crop score for each frame is adjusted based on the face signal. For example, if a face is detected in a crop candidate region, the score for that region may be increased by a first factor. If the crop candidate region is detected to contain a significant face, the score for that region may be increased by a second factor that is greater than the first factor.
いくつかの実装では、クロップ領域と顔を含む境界ボックスとの交点が決定されてもよい。たとえば、顔検出技術は、境界ボックスを決定するために利用され得る。クロップスコアは、交点に基づいて調整されてもよい。たとえば、いくつかの実装では、完全な交点（顔全体がクロップ領域内に存在する）は完全なスコアブーストを受け取ってもよく、部分的な顔（顔の一部がクロップ領域から欠けている）は低いスコアブーストを受け取ってもよく、たとえば、スコアブーストは、交点の面積と顔の境界ボックスの面積との比によって重み付けされてもよい。ブロック４０８の後に、ブロック４１６が続く場合がある。
In some implementations, an intersection of the crop region with a bounding box containing the face may be determined. For example, face detection techniques may be utilized to determine the bounding box. The crop score may be adjusted based on the intersection. For example, in some implementations, a full intersection (where the entire face is within the crop region) may receive a full score boost and a partial face (where a portion of the face is missing from the crop region) may receive a lower score boost, e.g., the score boost may be weighted by the ratio of the area of the intersection to the area of the bounding box of the face.
４１６で、動きコストが決定される。いくつかの実装では、動きコストは、１つ以上の以前の時間（たとえば、動画内の以前のタイムスタンプ）における潜在的なクロップ経路と、動画に存在する動きとを考慮した、特定の時間（たとえば、動画内の特定のタイムスタンプ）におけるクロップ候補領域の選択に関連付けられたコストでもよい。いくつかの実装では、動きコストの決定は、たとえば、オプティカルフローまたは他の技術を使用して、クロップ領域のフレーム間の動きを分析することを含み得る。結果は、少数の動きクラスタ（たとえば、互いに近接する位置のグループにおけるクロップ領域動きを含むクラスタ）にクラスタ化することができる。いくつかの実装では、疎なオプティカルフローは、テクスチャのない領域でより良い性能を発揮することができる。動きは、少数のクラスタ（たとえば、互いに比較的近接する領域の周りのクラスタ）に減らすことができる。クラスタは、時間的な一貫性を提供しなくてもよい。 At 416, a motion cost is determined. In some implementations, the motion cost may be a cost associated with selecting a crop candidate region at a particular time (e.g., a particular timestamp in the video) taking into account potential crop paths at one or more previous times (e.g., previous timestamps in the video) and the motion present in the video. In some implementations, determining the motion cost may include analyzing frame-to-frame motion of the crop region, for example, using optical flow or other techniques. The results may be clustered into a small number of motion clusters (e.g., clusters that include crop region motion in groups of locations that are close to each other). In some implementations, sparse optical flow may perform better in texture-free regions. The motion may be reduced to a small number of clusters (e.g., clusters around regions that are relatively close to each other). The clusters may not provide temporal consistency.
実装例では、動きコストは、以前の時間に対するクロップ領域の動きと、最もよく一致する動きクラスタの動きとの比較に基づいて計算されてもよい。たとえば、最もよく一致する運きクラスタは、クロップ領域内の空間セントロイドと、クロップ領域の運きに最も類似する動きベクトルとを有するクラスタでもよい。動きコストは、最もよく一致するクラスタとクロップ候補領域の動きとの間の速度の絶対差の関数でもよい。コスト値は、クロップ領域を移動させるために割り当てられ、動きコストを決定するために使用され得る。たとえば、図８Ａ～８Ｃは、動画８０２内のクロップ領域８０４を示し、クロップ領域８０２は、本明細書に記載の技術に基づいて、動画の再生中に（たとえば、８０８および８１２）、異なる場所に移動する。ブロック４１６の後に、ブロック４１８が続く場合がある。
In an implementation, the motion cost may be calculated based on a comparison of the motion of the crop region relative to a previous time with the motion of the best matching motion cluster. For example, the best matching motion cluster may be the cluster with a spatial centroid within the crop region and a motion vector that is most similar to the motion of the crop region. The motion cost may be a function of the absolute difference in speed between the motion of the best matching cluster and the crop candidate region. A cost value may be assigned to move the crop region and used to determine the motion cost. For example, FIGS. 8A-8C show a
４１８で、最小コスト経路が、フレームごとのクロップスコアと、４１６において決定された動きコストとに基づいて決定される。いくつかの実装では、最小コスト経路は、クロップスコアに基づいてクロップ候補領域を取得することと、クロップ領域を移動させるためのコストを含み得る最小コスト経路探索動作を行うこととを含み得る（たとえば、クロップ領域を移動させるコストは、クロップ領域がフレームからフレームまで連続して、またはフレームと後続フレームとの間で移動されている距離に基づき得る）。最小コスト経路は、最小コストの経路を解くことによって見出される。図７は、グラフとしてプロットされた最小コスト経路および他の要因を表すグラフ７００の例を示し、ｙ軸７０２は動画内のクロップ領域のｘ位置であり、ｘ軸は時間である。いくつかの実装は、クロップ領域の経路を滑らかにし、不連続性を除去するために、入力動画内のクロップ領域位置から
外れ値が除去される外れ値除去を含み得る。これは、最小コスト経路の副産物であり得る。ブロック４１８に、ブロック４２０が続く場合がある。
At 418, a minimum cost path is determined based on the crop scores for each frame and the motion costs determined at 416. In some implementations, the minimum cost path may include obtaining crop candidate regions based on the crop scores and performing a minimum cost path search operation, which may include a cost for moving the crop region (e.g., the cost of moving the crop region may be based on the distance the crop region is moved consecutively from frame to frame or between a frame and a subsequent frame). The minimum cost path is found by solving the minimum cost path. FIG. 7 shows an example of a graph 700 representing a minimum cost path and other factors plotted as a graph, where the y-axis 702 is the x-position of the crop region in the video and the x-axis is time. Some implementations may include outlier removal, where outliers are removed from the crop region positions in the input video to smooth the crop region path and remove discontinuities. This may be a by-product of the minimum cost path. Block 418 may be followed by
４２０で、クロップキーフレーミングが生成される。クロップキーフレーミングは、開始フレームおよび終了フレームと、入力動画内のクロップ領域ｘ位置とを含み得る。たとえば、クロップ経路の生成は、３０フレーム／秒（ｆｐｓ）のフレームレートを有する動画の場合、５ｆｐｓで実行されてもよい。この例では、キーフレームは５ｆｐｓで生成され、ベジエスプラインなどの補間技術を使用して、動画の３０ｆｐｓのフルフレームレートで滑らかな補間を生成することができる。たとえば、図８Ａ～図８Ｃに示すように、各クロップキーフレーミングが異なるｘ位置のクロップ領域を含む、動画の３つのキーフレーミングセクションが存在してもよい。ブロック４２０の後に、ブロック４２２が続く場合がある。
At 420, a crop keyframing is generated. The crop keyframing may include a start frame and an end frame, and a crop area x-location within the input video. For example, the crop path generation may be performed at 5 frames per second (fps) for a video having a frame rate of 30 fps. In this example, keyframes are generated at 5 fps, and an interpolation technique such as a Bézier spline may be used to generate a smooth interpolation at the full frame rate of the video of 30 fps. For example, as shown in Figures 8A-8C, there may be three keyframing sections of the video, with each crop keyframing including a crop area at a different x-location.
４２２で、入力動画およびクロップキーフレーミングに基づいて、クロップされた動画が出力される。たとえば、クロップされた動画は、ユーザデバイスのディスプレイに表示することができる。別の例では、クロップされた動画は、動画共有サイトなどにアップロードすることができる。クロップされた動画は、入力動画のものとは異なるアスペクト比または方向を有し得る。 At 422, a cropped video is output based on the input video and the crop keyframing. For example, the cropped video may be displayed on a display of a user device. In another example, the cropped video may be uploaded to a video sharing site, etc. The cropped video may have a different aspect ratio or orientation than that of the input video.
いくつかの実装では、出力は、クロップされた動画の代わりに、またはそれに加えて、クロップキーフレームおよび経路を含んでもよい。たとえば、クロップキーフレームおよび経路は、動画メタデータなどの動画に関連して格納されてもよい。ビューアアプリケーションが動画の再生を開始すると、ビューアアプリケーションのアスペクト比または方向（動画が表示されるデバイスに基づく場合がある）に一致するクロップキーフレームおよび経路が決定され、ビューアアプリケーションに提供されてもよい。ビューアアプリケーションは、再生中に動画をクロップするためにクロップキーフレームおよび経路を利用してもよい。このような実装により、クロップキーフレームおよび経路を認識し、再生時に動画をクロップするために情報を利用できるビューアアプリケーションで動画が視聴されているときに、（ビューアアプリケーションに一致する）別のビデオアセットを生成する必要がなくなる。 In some implementations, the output may include crop keyframes and paths instead of or in addition to the cropped video. For example, the crop keyframes and paths may be stored in association with the video, such as video metadata. When a viewer application begins playing the video, crop keyframes and paths that match the viewer application's aspect ratio or orientation (which may be based on the device on which the video is displayed) may be determined and provided to the viewer application. The viewer application may utilize the crop keyframes and paths to crop the video during playback. Such an implementation avoids the need to generate a separate video asset (that matches the viewer application) when the video is being viewed with a viewer application that is aware of the crop keyframes and paths and can utilize the information to crop the video during playback.
方法４００のさまざまなブロックは、組合わされてもよい、複数のブロックに分割されてもよい、または並行して実行されてもよい。たとえば、ブロック４０６および４０８は組合わされてもよい。いくつかの実装では、ブロックは異なる順序で実行されてもよい。たとえば、ブロック４０４～４０８およびブロック４１２～４１４は、並行して実行されてもよい。 Various blocks of method 400 may be combined, divided into multiple blocks, or performed in parallel. For example, blocks 406 and 408 may be combined. In some implementations, blocks may be performed in a different order. For example, blocks 404-408 and blocks 412-414 may be performed in parallel.
方法４００、またはその一部は、追加の入力（たとえば、追加の動画）を使用して任意の回数繰り返されることがある。方法４００は、特定のユーザ許可を得て実現することができる。たとえば、自動的なパーソナライズされたビデオクロッピングを有効にするかどうかをユーザが指定することを可能にする動画再生ユーザインターフェイスが提供されてもよい。再生に際して自動的なパーソナライズされたビデオクロッピングを実行することは、（たとえば、ユーザデバイスに格納されている）パーソナライズされたパラメータを使用して顔識別を実行することを利用し得るという情報を、ユーザに提供することが可能であり、自動的なパーソナライズされたビデオクロッピングを完全に無効にするオプションが提供される。 Method 400, or portions thereof, may be repeated any number of times using additional inputs (e.g., additional videos). Method 400 may be implemented with specific user permission. For example, a video playback user interface may be provided that allows a user to specify whether to enable automatic personalized video cropping. The user may be provided with information that performing automatic personalized video cropping upon playback may utilize performing face identification using personalized parameters (e.g., stored on the user device), and an option is provided to disable automatic personalized video cropping entirely.
顔検出および重要な顔判定を含む方法４００は、特定のユーザの許可を得て、動画を再生またはアップロードしているクライアントデバイス上で完全に実行されてもよい。また、顔は人間でも、またはその他（たとえば、動物もしくはペット）でもよい。さらに、デ
バイス上で自動的なパーソナライズされたビデオクロッピングを実行する技術的利点は、記載された方法が、クライアントデバイスがアクティブなインターネット接続を有することを必要としないため、デバイスがインターネットに接続されていないときでも自動ビデオクロッピングが可能になることである。さらに、本方法はローカルに実行されるため、ネットワークリソースは消費されない。さらに、ユーザデータがサーバまたは他のサードパーティデバイスに送信されることはない。したがって、記載された技術は、ユーザデータの共有を必要としない態様で、パーソナライズされたパラメータの利点を用いて、動画が取込まれたアスペクト比または方向とは異なるアスペクト比または方向での動画再生の問題に対処することができる。
The method 400, including face detection and significant face determination, may be performed entirely on the client device playing or uploading the video, with the permission of a particular user. Also, the face may be human or other (e.g., animal or pet). Furthermore, a technical advantage of performing automatic personalized video cropping on the device is that the described method does not require the client device to have an active Internet connection, thus enabling automatic video cropping even when the device is not connected to the Internet. Furthermore, since the method is performed locally, no network resources are consumed. Furthermore, no user data is sent to a server or other third party device. Thus, the described technique can take advantage of personalized parameters to address the issue of video playback in an aspect ratio or orientation different from the aspect ratio or orientation in which the video was captured, in a manner that does not require the sharing of user data.
いくつかの実装では、動画の再生中に、デバイスの方向またはアスペクト比の変化を検出することができ（たとえば、ユーザが再生中にデバイスを９０度回転させたとき、または折り畳み式デバイスを開いてアスペクト比を倍にしたとき）、これに応じて、クロッピングを調整することができる（たとえば、所望の出力方向および／またはアスペクト比に合うようにクロップ領域を調整できる）。 In some implementations, changes in device orientation or aspect ratio can be detected during video playback (e.g., when a user rotates the device 90 degrees during playback or unfolds a foldable device doubling the aspect ratio) and cropping can be adjusted accordingly (e.g., crop regions can be adjusted to fit the desired output orientation and/or aspect ratio).
記載された技術は、有利なことに、ユーザ、たとえば、クロップされた動画を視聴しているユーザに合わせてパーソナライズされた、クロップされた動画を生成することができる。たとえば、横方向の（高さよりも大きい幅を有する）動画であって、２人の人物が動画の両側に描かれており、たとえば、第１の人物が画像の左端近くに現れ、第２の人物が画像の右端近くに描かれている動画を考える。このような動画が縦方向で視聴されている場合、たとえば、画面が高さよりも小さい幅を有するスマートフォンまたは他のデバイスにおいて、従来のパーソナライズされていないクロッピングでは、視聴しているユーザから独立して選択された、２人の人物の一方を描写するクロップ領域となる可能性がある。対照的に、本明細書に記載のパーソナライズされたクロッピングでは、（たとえば、重要な顔を有する）特定の人物を動画の焦点として自動的に選択し、その人物を描写するクロップ領域を選択することができる。たとえば、異なるビューアは、動画に描かれた人物を重要であると認める可能性があり、したがって、異なるビューアのためのクロップ領域は異なる可能性があることが理解され得る。より一般的には、動画が複数の被写体を描写する場合、本明細書に記載の技術によれば、異なるビューアのためのクロップ領域は、関心のある被写体（たとえば、重要な顔、ペットなど）がクロップされた動画に保存されるようにパーソナライズされてもよい。 The described techniques can advantageously generate cropped video that is personalized to a user, e.g., a user viewing the cropped video. For example, consider a landscape (having a width greater than the height) video in which two people are depicted on either side of the video, e.g., a first person appears near the left edge of the image and a second person is depicted near the right edge of the image. If such a video is viewed in portrait orientation, e.g., on a smartphone or other device whose screen has a width less than its height, traditional non-personalized cropping may result in a crop region depicting one of the two people selected independently of the viewing user. In contrast, personalized cropping as described herein may automatically select a particular person (e.g., having an important face) as the focus of the video and select a crop region depicting that person. For example, it may be understood that different viewers may perceive a person depicted in the video as important, and thus the crop regions for different viewers may be different. More generally, if a video depicts multiple objects, in accordance with the techniques described herein, crop regions for different viewers may be personalized such that objects of interest (e.g., important faces, pets, etc.) are preserved in the cropped video.
図５は、いくつかの実装に係る、動画を自動的にクロップするモジュール５００の例を示す図である。動画は５０２で取得される。フレームごとの初期スコアリングは、アクティブスピーカ分析モジュール５０４、人物／顔分析モジュール５０６、または美的スコアリングモジュール５０８を使用して行うことができる。パーソナライズされたスコア組合わせモジュール５１２は、パーソナライズパラメータに基づき、パーソナライズ値を含むスコアを生成する。組合わされたパーソナライズされたスコアは、動画に写っている可能性がある重要な顔のアイデンティティを含む個人的基準５１０および／または他の基準に基づくスコアと組合わされた５０４～５０８からの個別のスコアを含み得る。パーソナライズされたスコアは、画像内のユーザにとって重要な顔を認識するように訓練された機械学習モデルからの値を含むことができ、モデルは、クロップ候補領域内の重要な顔の表示を提供することができる。重要な顔の表示は、クロップ領域内で識別された各重要な顔の位置を含み得る。重要な顔は、ユーザがデバイスを使用して過去に動画または写真を撮った顔、次のうち少なくとも１つの顔が含まれ得る。少なくとも閾値回数発生する顔、ユーザのライブラリの少なくとも閾値割合で（たとえば、ライブラリ内の画像および動画の少なくとも５％で）現れる顔、少なくとも閾値頻度で（たとえば、画像／動画がライブラリ内にある年の大半について、少なくとも年に１回）現れる顔などのうちの少なくとも１つである顔。
FIG. 5 illustrates an example of a module 500 for automatically cropping video, according to some implementations. A video is acquired at 502. An initial scoring for each frame can be performed using an active speaker analysis module 504, a person/face analysis module 506, or an aesthetic scoring module 508. A personalized
パーソナライズされたスコア組合せ５１２と並行して、動作分析５１４ならびに／または動きおよび加速度コスト計算５１６を実行することができる。運きコスト計算器５１６およびパーソナライズされたスコア組合せ５１２の出力は、最小コスト経路探索５１８によって使用することができる。最小コスト経路探索５１８の出力は、局所的最適化およびヒューリスティック５２０でさらに処理され、その後、クロップキーフレーミング５２２に使用することができる。また、最小コスト経路探索５１８の出力は、品質または信頼度スコア５２４を計算するために使用可能である。
In parallel with the
品質スコアまたは信頼度スコアは、動画を自動的にクロップするか、動画をクロップしないかを決定するために使用可能である。たとえば、一部の動画は、縦長にうまくクロップすることができない。動画をうまくクロップするとができないと示す品質または信頼度基準を有することは、ビデオクロッピングを試みないとシステムに示し、代わりに動画をレターボックス付きフォーマットで表示することにフォールバックすることができる。他の例では、動画に２つ以上の重要な顔があり、その顔は、クロッピングによって１つ以上の重要な顔が動画から切り取られるように配置されてもよい。これは、自動ビデオクロッピング動作を行わない他の場合である。 The quality or confidence score can be used to determine whether to automatically crop the video or not to crop the video. For example, some videos cannot be cropped vertically well. Having a quality or confidence metric that indicates the video cannot be cropped well can indicate to the system not to attempt video cropping and instead fall back to displaying the video in a letterboxed format. In another example, there may be two or more significant faces in a video, and the faces may be positioned such that cropping would remove one or more significant faces from the video. This is another case where an automatic video cropping operation would not occur.
いくつかの実装は、クロップ領域をどこに配置するかを決定するための追加の入力信号を含み得る。追加の入力信号は、動画の顕著性（たとえば、美観だけではない）、顔の品質、関心のあるオブジェクト（たとえば、人間、動物など）、またはパーソナライズされた信号（たとえば、重要な顔）のうちの１つ以上を含み得る。いくつかの実装では、システムは、人物を追うカメラの検出、誰がカメラを見ているか、カメラでの持続時間などのうちの１つ以上を使用して、動画で誰が重要であるかをプログラム的に決定するよう試みることができる。 Some implementations may include additional input signals to determine where to place the crop regions. The additional input signals may include one or more of video saliency (e.g., not just aesthetics), facial quality, objects of interest (e.g., humans, animals, etc.), or personalized signals (e.g., important faces). In some implementations, the system may attempt to programmatically determine who is important in the video using one or more of camera detection following people, who is looking at the camera, duration on camera, etc.
いくつかの実装は、カメラ加速度を平滑化するためのルーチンを含み得る。いくつかの実装では、ベジエスプラインを使用するためのキーフレーム補間を含み得る。いくつかの実装では、システムは、カメラ速度の変化を制御可能である。 Some implementations may include routines for smoothing camera acceleration. Some implementations may include keyframe interpolation using Bézier splines. In some implementations, the system can control changes in camera velocity.
図６は、本明細書に記載される１つ以上の特徴を実装するために使用され得るデバイス６００の例を示すブロック図である。一例では、デバイス６００は、クライアントデバイス、たとえば、図１に示されるクライアントデバイスのいずれかを実装するために使用され得る。または、デバイス６００は、サーバデバイス、たとえばサーバ１０４を実装可能である。いくつかの実装では、デバイス６００は、クライアントデバイス、サーバデバイス、またはクライアントデバイスとサーバデバイスとの両方を実装するために使用され得る。デバイス６００は、上述したように、任意の適切なコンピュータシステム、サーバ、または他の電子もしくはハードウェアデバイスであり得る。 FIG. 6 is a block diagram illustrating an example of a device 600 that may be used to implement one or more features described herein. In one example, the device 600 may be used to implement a client device, such as any of the client devices shown in FIG. 1. Alternatively, the device 600 may implement a server device, such as server 104. In some implementations, the device 600 may be used to implement a client device, a server device, or both a client device and a server device. The device 600 may be any suitable computer system, server, or other electronic or hardware device, as described above.
本明細書に記載される１つ以上の方法は、任意のタイプのコンピューティングデバイス上で実行される別のプログラムの一部として、またはモバイルコンピューティングデバイス（たとえば、携帯電話、スマートフォン、タブレットコンピュータ、ウェアラブルデバイス（腕時計、アームバンド、宝石、ヘッドウェア、仮想現実ゴーグルもしくは眼鏡、拡張現実ゴーグルもしくは眼鏡、ヘッドマウントディスプレイなど）、ラップトップコンピュータなど）上で実行されるモバイルアプリケーション（「アプリ」）もしくはモバイルアプリの一部として、任意のタイプのコンピューティングデバイス上で実行可能なスタンドアロンプログラムで実行可能である。 One or more of the methods described herein may be performed in a standalone program executable on any type of computing device, as part of another program executing on any type of computing device, or as part of a mobile application ("app") or mobile app executing on a mobile computing device (e.g., a mobile phone, a smartphone, a tablet computer, a wearable device (such as a watch, an armband, jewelry, headwear, virtual reality goggles or glasses, augmented reality goggles or glasses, head mounted displays, etc.), a laptop computer, etc.).
いくつかの実装では、デバイス６００は、プロセッサ６０２、メモリ６０４、および入出力（Ｉ／Ｏ）インターフェイス６０６を含む。プロセッサ６０２は、プログラムコード
を実行し、デバイス６００の基本動作を制御する１つ以上のプロセッサおよび／または処理回路であり得る。「プロセッサ」は、データ、信号、または他の情報を処理する任意の適切なハードウェアシステム、機構またはコンポーネントを含む。プロセッサは、１つ以上のコア（たとえば、シングルコア、デュアルコア、またはマルチコア構成）を有する汎用中央処理装置（ＣＰＵ）、複数の処理ユニット（たとえば、マルチプロセッサ構成）、グラフィックス処理ユニット（ＧＰＵ）、フィールドプログラマブルゲートアレイ（ＦＰＧＡ）、特定用途向け集積回路（ＡＳＩＣ）、複合プログラマブル論理デバイス（ＣＰＬＤ）、機能を実現するための専用回路、ニューラルネットワークモデルに基づく処理を実現するための専用プロセッサ、ニューラル回路、行列計算（たとえば行列乗算）用に最適化されたプロセッサを有するシステム、または他のシステムを含み得る。いくつかの実装では、プロセッサ６０２は、ニューラルネットワーク処理を実現する１つ以上のコプロセッサを含んでもよい。いくつかの実装では、プロセッサ６０２は、確率的出力を生成するためにデータを処理するプロセッサでもよく、たとえば、プロセッサ６０２によって生成される出力は不正確でもよい、または予想出力から範囲内で正確でもよい。処理は、特定の地理的位置に限定される必要はない、または、時間的な制限を有する必要はない。たとえば、プロセッサは、「リアルタイム」、「オフライン」、「バッチモード」などでその機能を実行してもよい。処理の一部は、異なる時間に異なる場所で異なる（または同じ）処理システムによって実行されてもよい。コンピュータは、メモリと通信している任意のプロセッサでもよい。
In some implementations, the device 600 includes a
メモリ６０４は、典型的には、プロセッサ６０２によるアクセスのためにデバイス６００に設けられ、プロセッサによる実行のための命令を格納するのに適したランダムアクセスメモリ（ＲＡＭ）、読取専用メモリ（ＲＯＭ）、電気消去可能読取専用メモリ（ＥＥＰＲＯＭ）、フラッシュメモリなどの任意の好適なプロセッサ読取可能ストレージ媒体でもよく、プロセッサ６０２とは別に、および／またはそれと統合されて配置され得る。メモリ６０４は、オペレーティングシステム６０８、機械学習アプリケーション６３０、他のアプリケーション６１２、およびアプリケーションデータ６１４を含む、プロセッサ６０２によってサーバデバイス６００上で動作するソフトウェアを格納することができる。他のアプリケーション６１２は、ビデオクロッピングアプリケーション、データ表示エンジン、ウェブホスティングエンジン、画像表示エンジン、通知エンジン、ソーシャルネットワーキングエンジンなどのアプリケーションを含み得る。いくつかの実装では、機械学習アプリケーション６３０および／または他のアプリケーション６１２は、プロセッサ６０２が本明細書に記載の機能、たとえば図４および図５の方法の一部または全部を実行することを可能にする命令を含み得る。
The memory 604 is typically provided in the device 600 for access by the
他のアプリケーション６１２は、たとえば、ビデオアプリケーション、メディア表示アプリケーション、通信アプリケーション、ウェブホスティングエンジンもしくはアプリケーション、マッピングアプリケーション、メディア共有アプリケーションなどを含み得る。本明細書に開示される１つ以上の方法は、いくつかの環境およびプラットフォームで動作可能であり、たとえば、任意のタイプのコンピューティングデバイス上で実行可能なスタンドアロンコンピュータプログラムとして、モバイルコンピューティングデバイス上で実行されるモバイルアプリケーション（「アプリ」）などとして動作することが可能である。 The other applications 612 may include, for example, a video application, a media viewing application, a communication application, a web hosting engine or application, a mapping application, a media sharing application, etc. One or more methods disclosed herein may operate in a number of environments and platforms, for example, as a standalone computer program executable on any type of computing device, as a mobile application ("app") executing on a mobile computing device, etc.
さまざまな実装において、機械学習アプリケーションは、ベイズ分類器、サポートベクターマシン、ニューラルネットワーク、または他の学習技法を利用してもよい。いくつかの実装では、機械学習アプリケーション６３０は、訓練済みモデル６３４と、推論エンジン６３６と、データ６３２とを含んでもよい。いくつかの実装では、データ６３２は、訓練データ、たとえば、訓練済みモデル６３４を生成するために使用されるデータを含んでもよい。たとえば、訓練データは、ユーザがユーザデバイス上で撮った写真または動画、
ユーザデバイス上の写真または動画に描かれた人の顔識別情報など、ユーザの許可を得てアクセスされる任意のタイプのデータを含んでもよい。訓練済みモデル６３４が顔信号を生成するモデルである場合、訓練データは、写真、動画、および関連するメタデータを含んでもよい。
In various implementations, the machine learning application may utilize a Bayesian classifier, a support vector machine, a neural network, or other learning techniques. In some implementations, the machine learning application 630 may include a trained model 634, an inference engine 636, and data 632. In some implementations, the data 632 may include training data, e.g., data used to generate the trained model 634. For example, the training data may include photos or videos taken by a user on a user device,
It may include any type of data accessed with the user's permission, such as facial identification information of people depicted in photos or videos on the user device. If the trained model 634 is a model that generates a face signal, the training data may include photos, videos, and associated metadata.
訓練データは、任意のソース、たとえば、訓練用に特にマークされたデータリポジトリ、機械学習のための訓練データとして使用するための許可が与えられたデータ等から取得されてもよい。１人以上のユーザが、機械学習モデル、たとえば、訓練済みモデル６３４を訓練するために、それぞれのユーザデータの使用を許可する実装では、訓練データは、そのようなユーザデータを含んでもよい。 The training data may be obtained from any source, e.g., a data repository specifically marked for training, data for which permission has been given for use as training data for machine learning, etc. In implementations in which one or more users authorize the use of their respective user data to train a machine learning model, e.g., trained model 634, the training data may include such user data.
いくつかの実装では、訓練データは、訓練されているコンテキストにおけるユーザ入力またはアクティビティに基づかないデータ、たとえば、動画から生成されるデータなど、訓練を目的として生成された合成データを含んでもよい。いくつかの実装では、機械学習アプリケーション６３０は、データ６３２を除外する。たとえば、これらの実装では、訓練済みモデル６３４は、たとえば、異なるデバイスで生成され、機械学習アプリケーション６３０の一部として提供されてもよい。さまざまな実装では、訓練済みモデル６３４は、モデル構造または形式、および関連する重みを含むデータファイルとして提供されてもよい。推論エンジン６３６は、訓練済みモデル６３４のデータファイルを読み取り、訓練済みモデル６３４で指定されたモデル構造または形式に基づいて、ノード接続性、層、および重みを有するニューラルネットワークを実装してもよい。 In some implementations, the training data may include data that is not based on user input or activity in the context being trained, e.g., synthetic data generated for training purposes, such as data generated from a video. In some implementations, the machine learning application 630 excludes the data 632. For example, in these implementations, the trained model 634 may be generated, for example, on a different device and provided as part of the machine learning application 630. In various implementations, the trained model 634 may be provided as a data file that includes the model structure or format and associated weights. The inference engine 636 may read the trained model 634 data file and implement a neural network with node connectivity, layers, and weights based on the model structure or format specified in the trained model 634.
いくつかの実装では、訓練済みモデル６３４は、１つ以上のモデル形式または構造を含んでもよい。たとえば、モデル形式または構造は、任意のタイプのニューラルネットワーク、たとえば、線形ネットワーク、複数の層（たとえば、入力層と出力層との間の「隠れ層」、各層は線形ネットワークである）を実装する深層ニューラルネットワーク、畳み込みニューラルネットワーク（たとえば、入力データを複数の部分またはタイルに分割または区分し、１つ以上のニューラルネットワーク層を使用して各タイルを別々に処理し、各タイルの処理から結果を集約するネットワーク）、シーケンスからシーケンスへのニューラルネットワーク（たとえば、文中の単語、動画中のフレームなどの連続データを入力として取り、結果シーケンスを出力として生成するネットワーク）などが含まれ得る。モデル形式または構造は、さまざまなノード間の接続性、およびノードの層への編成を指定することができる。 In some implementations, the trained model 634 may include one or more model formats or structures. For example, the model format or structure may include any type of neural network, such as a linear network, a deep neural network implementing multiple layers (e.g., a "hidden layer" between the input layer and the output layer, where each layer is a linear network), a convolutional neural network (e.g., a network that splits or partitions the input data into multiple portions or tiles, processes each tile separately using one or more neural network layers, and aggregates the results from the processing of each tile), a sequence-to-sequence neural network (e.g., a network that takes sequential data as input, such as words in a sentence, frames in a video, etc., and produces a sequence of results as output), etc. The model format or structure may specify the connectivity between the various nodes and the organization of the nodes into layers.
たとえば、第１の層（たとえば、入力層）のノードは、入力データ６３２またはアプリケーションデータ６１４としてデータを受信してもよい。たとえば、訓練済みモデル６３４が顔信号を生成する場合、入力データは、ユーザデバイスによって取込まれた写真または動画を含んでもよい。後続の中間層は、モデル形式または構造で指定された接続性に従って、前の層のノードの出力を入力として受け取ることができる。これらの層は、隠れ層または潜在層と呼ばれることもある。 For example, nodes in a first layer (e.g., input layer) may receive data as input data 632 or application data 614. For example, if the trained model 634 generates a face signal, the input data may include photos or videos captured by a user device. Subsequent intermediate layers may receive as input the outputs of nodes in the previous layer according to the connectivity specified in the model format or structure. These layers may also be referred to as hidden or latent layers.
最終層（たとえば、出力層）は、機械学習アプリケーションの出力を生成する。たとえば、出力は、重要な顔が動画フレーム（またはフレーム）に存在するかどうかの表示でもよい。いくつかの実装では、モデル形式または構造は、各層のノードの数および／またはタイプも指定する。 The final layer (e.g., output layer) generates an output for the machine learning application. For example, the output may be an indication of whether a significant face is present in a video frame (or frames). In some implementations, the model format or structure also specifies the number and/or type of nodes in each layer.
異なる実装では、訓練済みモデル６３４は、モデル構造または形式ごとに層に配置された、複数のノードを含み得る。いくつかの実装では、ノードは、たとえば、１単位の入力を処理して１単位の出力を生成するように構成された、メモリを有さない計算ノードでもよい。ノードによって実行される計算は、たとえば、複数のノード入力の各々に重みを乗
算すること、加重和を取得すること、およびバイアスまたはインターセプト値で加重和を調整してノード出力を生成することを含んでもよい。いくつかの実装では、ノードによって実行される計算は、調整された加重和にステップ／活性化関数を適用することも含み得る。いくつかの実装では、ステップ／活性化関数は、非線形関数でもよい。さまざまな実装では、そのような計算は、行列乗算などの演算を含んでもよい。いくつかの実装では、複数のノードによる計算は、たとえば、マルチコアプロセッサの複数のプロセッサコアを用いて、ＧＰＵの個別の処理ユニットを用いて、または特殊用途のニューラル回路を用いて、並行して実行されてもよい。いくつかの実装では、ノードは、メモリを含んでもよく、たとえば、後続の入力を処理する際に１つ以上の以前の入力を格納し、使用してもよい。たとえば、メモリを有するノードは、長短記憶（long short-term memory：ＬＳＴＭ）ノードを含んでもよい。ＬＳＴＭノードは、メモリを使用して、ノードが有限状態マシン（finite state machine：ＦＳＭ）のように動作することを可能にする「状態」を維持することができる。このようなノードを有するモデルは、文または段落の単語、動画のフレーム、スピーチまたはその他の音声など、連続したデータを処理するのに有用な場合がある。
In different implementations, the trained model 634 may include multiple nodes arranged in layers per model structure or format. In some implementations, the node may be a computational node without memory, for example, configured to process a unit of input to generate a unit of output. The computation performed by the node may include, for example, multiplying each of the multiple node inputs by a weight, obtaining a weighted sum, and adjusting the weighted sum with a bias or intercept value to generate the node output. In some implementations, the computation performed by the node may also include applying a step/activation function to the adjusted weighted sum. In some implementations, the step/activation function may be a nonlinear function. In various implementations, such computations may include operations such as matrix multiplication. In some implementations, the computations by the multiple nodes may be performed in parallel, for example, using multiple processor cores of a multi-core processor, using individual processing units of a GPU, or using special-purpose neural circuitry. In some implementations, the node may include memory, for example, to store and use one or more previous inputs in processing a subsequent input. For example, a node with memory may include a long short-term memory (LSTM) node. An LSTM node can use memory to maintain a "state" that allows the node to operate like a finite state machine (FSM). Models with such nodes may be useful for processing continuous data, such as words in a sentence or paragraph, frames of a video, speech or other audio.
いくつかの実装では、訓練済みモデル６３４は、個別のノードのための重みを含んでもよい。たとえば、モデルは、モデル形式または構造によって指定されるように層に編成された複数のノードとして初期化されてもよい。初期化時に、それぞれの重みが、モデル形式に従って接続されるノードの各組、たとえば、ニューラルネットワークの連続する層のノード間の接続に適用されてもよい。たとえば、それぞれの重みは、ランダムに割り当てられてもよい、またはデフォルト値に初期化されてもよい。モデルはその後、たとえば、データ６３２を使用して、結果を生成するように訓練されてもよい。 In some implementations, the trained model 634 may include weights for individual nodes. For example, the model may be initialized as a number of nodes organized into layers as specified by the model format or structure. At initialization, a respective weight may be applied to each set of nodes that are connected according to the model format, e.g., to the connections between nodes in successive layers of a neural network. For example, the respective weights may be assigned randomly or may be initialized to default values. The model may then be trained to generate results, e.g., using the data 632.
たとえば、訓練は、教師あり学習技法を適用することを含み得る。教師あり学習では、学習データは、複数の入力（写真または動画）と、入力ごとに対応する予想出力（たとえば、１つ以上の重要な顔の存在など）とを含み得る。モデルの出力と予想出力との比較に基づいて、重みの値は、たとえば、同様の入力が提供されるとモデルが予想出力を生成する確率を増加させる態様で、自動的に調整される。 For example, training may include applying supervised learning techniques. In supervised learning, the training data may include multiple inputs (photos or videos) and corresponding expected outputs for each input (e.g., the presence of one or more significant faces). Based on a comparison of the model's outputs to the expected outputs, the values of the weights are automatically adjusted, for example, in a manner that increases the probability that the model will generate the expected output when provided with similar inputs.
いくつかの実装では、訓練は、教師なし学習技法を適用することを含んでもよい。教師なし学習では、入力データのみが提供され、モデルは、データを区別するように、たとえば、入力データを複数のグループにクラスター化するように訓練されてもよく、各グループは、何らかの態様で類似している入力データ、たとえば、写真または動画フレームに存在する類似の重要な顔を有する入力データを含む。たとえば、モデルは、重要な顔を含む動画フレームまたはクロッピング矩形を、重要でない顔を含むフレームまたは顔を含まないフレームと区別するように訓練されてもよい。 In some implementations, training may include applying unsupervised learning techniques. In unsupervised learning, only input data is provided and a model may be trained to distinguish between the data, e.g., to cluster the input data into groups, each group containing input data that is similar in some manner, e.g., input data that have similar important faces present in photos or video frames. For example, the model may be trained to distinguish video frames or cropping rectangles that contain important faces from frames that contain unimportant faces or frames that do not contain faces.
いくつかの実装では、教師なし学習は、たとえば、機械学習アプリケーション６３０によって使用され得る知識表現を生成するために使用されてもよい。たとえば、教師なし学習は、図４および図５を参照して上述したように利用されるパーソナライズされたパラメータ信号を生成するために使用されてもよい。さまざまな実装において、訓練済みモデルは、モデル構造に対応する重みのセットを含む。データ６３２が省略される実装では、機械学習アプリケーション６３０は、たとえば、機械学習アプリケーション６３０の開発者、サードパーティなどによる、事前の訓練に基づく訓練済みモデル６３４を含んでもよい。いくつかの実装では、訓練済みモデル６３４は、固定された、たとえば、重みを提供するサーバからダウンロードされた重みのセットを含んでもよい。 In some implementations, unsupervised learning may be used, for example, to generate a knowledge representation that may be used by the machine learning application 630. For example, unsupervised learning may be used to generate a personalized parameter signal that is utilized as described above with reference to FIGS. 4 and 5. In various implementations, the trained model includes a set of weights that correspond to the model structure. In implementations in which the data 632 is omitted, the machine learning application 630 may include a trained model 634 that is based on prior training, for example, by the developer of the machine learning application 630, a third party, etc. In some implementations, the trained model 634 may include a set of weights that are fixed, for example, downloaded from a server that provides the weights.
また、機械学習アプリケーション６３０は、推論エンジン６３６を含む。推論エンジン６３６は、訓練済みモデル６３４をアプリケーションデータ６１４などのデータに適用し
て、推論を提供するように構成されている。いくつかの実装では、推論エンジン６３６は、プロセッサ６０２によって実行されるソフトウェアコードを含んでもよい。いくつかの実装では、推論エンジン６３６は、プロセッサ６０２が訓練済みモデルを適用することを可能にする（たとえば、プログラマブルプロセッサのための、フィールドプログラマブルゲートアレイ（ＦＰＧＡ）のための、など）回路構成を指定してもよい。いくつかの実装では、推論エンジン６３６は、ソフトウェア命令、ハードウェア命令、または組合わせを含み得る。いくつかの実装では、推論エンジン６３６は、推論エンジン６３６を呼び出すために、たとえば、訓練済みモデル６３４をアプリケーションデータ６１４に適用して推論を生成するために、オペレーティングシステム６０８および／または他のアプリケーション６１２によって使用できるアプリケーションプログラミングインターフェイス（ＡＰＩ）を提供してもよい。たとえば、重要な顔モデルの推論は、たとえば、１つ以上の重要な顔を有する以前に取込まれた写真または動画との比較に基づく、動画フレームまたはクロッピング矩形の分類でもよい。
The machine learning application 630 also includes an inference engine 636. The inference engine 636 is configured to apply the trained model 634 to data, such as the application data 614, to provide inferences. In some implementations, the inference engine 636 may include software code executed by the
機械学習アプリケーション６３０は、いくつかの技術的利点を提供し得る。たとえば、訓練済みモデル６３４が教師なし学習に基づいて生成される場合、訓練済みモデル６３４は、入力データ、たとえば、アプリケーションデータ６１４から知識表現（たとえば、数値表現）を生成するために推論エンジン６３６によって適用可能である。たとえば、顔信号を生成するように訓練されたモデルは、データサイズ（たとえば、１ＫＢ）が入力音声記録（たとえば、１ＭＢ）よりも小さい通話の表現を生成することができる。いくつかの実装では、そのような表現は、出力（たとえば、ラベル、分類など）を生成するための処理コスト（たとえば、計算コスト、メモリ使用量など）を低減するのに役立つ場合がある。 The machine learning application 630 may provide several technical advantages. For example, if the trained model 634 is generated based on unsupervised learning, the trained model 634 can be applied by the inference engine 636 to generate a knowledge representation (e.g., a numerical representation) from input data, e.g., application data 614. For example, a model trained to generate a facial signal can generate a representation of a phone call that is smaller in data size (e.g., 1 KB) than the input voice recording (e.g., 1 MB). In some implementations, such a representation may help reduce the processing cost (e.g., computational cost, memory usage, etc.) for generating an output (e.g., a label, a classification, etc.).
いくつかの実装では、そのような表現は、推論エンジン６３６の出力から出力を生成する異なる機械学習アプリケーションへの入力として提供されてもよい。いくつかの実装では、機械学習アプリケーション６３０によって生成された知識表現は、たとえば、ネットワークを介して、さらに他の処理を行う異なるデバイスに提供されてもよい。たとえば、図４または図５を参照して説明した技術を用いて生成された顔信号は、図４または図５を参照して説明したように、パーソナライズされたパラメータを用いて自動ビデオクロッピングで使用するためにクライアントデバイスに提供することができる。このような実装では、重要な顔の写真または動画ではなく、知識表現を提供することにより、技術的な利点、たとえば、より低コストでより速いデータ伝送を可能にし得る。他の例では、重要な顔をクラスター化するように訓練されたモデルは、入力写真または動画からクラスタを生成することができる。クラスタは、元の写真または動画へのアクセスを必要としないさらに他の処理（たとえば、重要な顔が動画フレームまたはクロッピング矩形に存在するか否かの判定など）に適している場合があり、したがって、計算コストを節約することができる。 In some implementations, such representations may be provided as input to a different machine learning application that generates an output from the output of the inference engine 636. In some implementations, the knowledge representations generated by the machine learning application 630 may be provided, for example, over a network, to a different device that performs further processing. For example, face signals generated using the techniques described with reference to FIG. 4 or FIG. 5 may be provided to a client device for use in automatic video cropping with personalized parameters, as described with reference to FIG. 4 or FIG. 5. In such implementations, providing knowledge representations rather than photos or videos of important faces may enable technical advantages, for example, faster data transmission at lower cost. In other examples, a model trained to cluster important faces may generate clusters from an input photo or video. The clusters may be suitable for further processing that does not require access to the original photo or video (e.g., determining whether important faces are present in a video frame or cropping rectangle, etc.), thus saving computational costs.
いくつかの実装では、機械学習アプリケーション６３０は、オフライン方式で実装されてもよい。これらの実装では、訓練済みモデル６３４は、第１段階で生成され、機械学習アプリケーション６３０の一部として提供されてもよい。いくつかの実装では、機械学習アプリケーション６３０は、オンライン方式で実装されてもよい。たとえば、そのような実装では、機械学習アプリケーション６３０を呼び出すアプリケーション（たとえば、オペレーティングシステム６０８、他のアプリケーション６１２の１つ以上）は、機械学習アプリケーション６３０によって生成された推論を利用し、たとえば、推論をユーザに提供し、システムログ（たとえば、ユーザによって許可されている場合、推論に基づいてユーザによって行われたアクション、またはさらに他の処理についての入力として利用される場合、さらに他の処理の結果）を生成してもよい。システムログは、定期的に、たとえば、時間ごとに、月ごとに、四半期ごとなどに生成されてもよく、ユーザの許可を得て、
訓練済みモデル６３４を更新するため、たとえば、訓練済みモデル６３４のための重要な顔データを更新するために使用されてもよい。
In some implementations, the machine learning application 630 may be implemented in an offline manner. In these implementations, the trained model 634 may be generated in a first stage and provided as part of the machine learning application 630. In some implementations, the machine learning application 630 may be implemented in an online manner. For example, in such implementations, an application (e.g., the operating system 608, one or more of the other applications 612) that invokes the machine learning application 630 may utilize the inferences generated by the machine learning application 630, e.g., provide the inferences to a user, and generate a system log (e.g., actions taken by the user based on the inferences, if permitted by the user, or results of further processing, if utilized as input for further processing). The system log may be generated periodically, e.g., hourly, monthly, quarterly, etc., and, with the user's permission, may be generated.
It may be used to update the trained model 634, for example, to update important facial data for the trained model 634.
いくつかの実装では、機械学習アプリケーション６３０は、機械学習アプリケーション６３０が実行されるデバイス６００の特定の構成に適応可能な態様で実装されてもよい。たとえば、機械学習アプリケーション６３０は、利用可能な計算リソース、たとえば、プロセッサ６０２を利用する計算グラフを決定してもよい。たとえば、機械学習アプリケーション６３０は、プロセッサ６０２が特定の数（たとえば、１０００個）のＧＰＵコアを有するＧＰＵを含むと決定し、それに応じて（たとえば、１０００個の個別のプロセスまたはスレッドとして）推論エンジンを実装してもよい。
In some implementations, the machine learning application 630 may be implemented in a manner that is adaptable to the particular configuration of the device 600 on which the machine learning application 630 executes. For example, the machine learning application 630 may determine a computational graph that utilizes available computational resources, e.g., the
いくつかの実装では、機械学習アプリケーション６３０は、訓練済みモデルのアンサンブルを実装してもよい。たとえば、訓練済みモデル６３４は、各々が同じ入力データに適用可能な複数の訓練済みモデルを含んでもよい。これらの実装では、機械学習アプリケーション６３０は、たとえば、利用可能な計算リソース、事前推論成功率などに基づいて、特定の訓練済みモデルを選択してもよい。いくつかの実装では、機械学習アプリケーション６３０は、複数の訓練済みモデルが適用されるように、推論エンジン６３６を実行してもよい。これらの実装では、機械学習アプリケーション６３０は、たとえば、各訓練済みモデルの適用から個別の出力の得点を付ける投票技術を使用して、または１つ以上の特定の出力を選択することによって、個別のモデルの適用からの出力を組合わせてもよい。さらに、これらの実装において、機械学習アプリケーションは、個別の訓練済みモデルを適用するための時間閾値を適用し（たとえば、０．５ｍｓ）、時間閾値内で利用可能なこれらの個別の出力のみを利用してもよい。時間閾値内に受信されない出力は、利用されなくてもよい、たとえば、破棄されてもよい。たとえば、このようなアプローチは、たとえば、オペレーティングシステム６０８または１つ以上のアプリケーション６１２によって、たとえば、１つ以上の重要な顔が検出されたかどうかおよび他のパーソナライズされた基準に基づいて、動画を自動的にクロップする機械学習アプリケーションを呼び出す間に指定された時間制限が設けられている場合に適していることがある。 In some implementations, the machine learning application 630 may implement an ensemble of trained models. For example, the trained models 634 may include multiple trained models, each applicable to the same input data. In these implementations, the machine learning application 630 may select a particular trained model based on, for example, available computational resources, prior inference success rates, etc. In some implementations, the machine learning application 630 may execute an inference engine 636 such that multiple trained models are applied. In these implementations, the machine learning application 630 may combine outputs from the application of the individual models, for example, using a voting technique to score the individual outputs from the application of each trained model, or by selecting one or more particular outputs. Furthermore, in these implementations, the machine learning application may apply a time threshold (e.g., 0.5 ms) for applying the individual trained models and utilize only those individual outputs that are available within the time threshold. Outputs that are not received within the time threshold may not be utilized, e.g., may be discarded. For example, such an approach may be appropriate where, for example, the operating system 608 or one or more applications 612 have a specified time limit between invoking a machine learning application that automatically crops the video, for example, based on whether one or more significant faces are detected and other personalized criteria.
異なる実装において、機械学習アプリケーション６３０は、異なるタイプの出力を生成することができる。たとえば、機械学習アプリケーション６３０は、表現またはクラスタ（たとえば、入力データの数値表現）、ラベル（たとえば、画像、文書、音声記録などを含む入力データ用）などを提供することができる。いくつかの実装では、機械学習アプリケーション６３０は、起動するアプリケーション、たとえばオペレーティングシステム６０８または１つ以上のアプリケーション６１２によって指定されるフォーマットに基づいて、出力を生成してもよい。 In different implementations, the machine learning application 630 can generate different types of output. For example, the machine learning application 630 can provide representations or clusters (e.g., numerical representations of the input data), labels (e.g., for input data including images, documents, audio recordings, etc.), etc. In some implementations, the machine learning application 630 may generate output based on a format specified by an invoking application, e.g., the operating system 608 or one or more applications 612.
メモリ６０４内のソフトウェアのいずれかは、代替的に、任意の他の適切な格納場所またはコンピュータ読取可能媒体に格納することができる。さらに、メモリ６０４（および／または他の接続されたストレージデバイス（複数可））は、１つ以上のメッセージ、１つ以上の分類法、電子百科事典、辞書、シソーラス、知識ベース、メッセージデータ、文法、顔識別子（たとえば、重要な顔）、および／または本明細書に記載する機能で使用する他の命令とデータとを格納可能である。メモリ６０４および任意の他のタイプのストレージ（磁気ディスク、光ディスク、磁気テープ、または他の有形媒体）は、「ストレージ」または「ストレージデバイス」とみなすことができる。 Any of the software in memory 604 may alternatively be stored in any other suitable storage location or computer readable medium. Additionally, memory 604 (and/or other connected storage device(s)) may store one or more messages, one or more taxonomies, electronic encyclopedias, dictionaries, thesauri, knowledge bases, message data, grammars, face identifiers (e.g., important faces), and/or other instructions and data for use in the functionality described herein. Memory 604 and any other type of storage (magnetic disk, optical disk, magnetic tape, or other tangible medium) may be considered "storage" or "storage device."
入出力インターフェイス６０６は、デバイス６００を他のシステムおよびデバイスとインターフェイス接続することを可能にする機能を提供可能である。インターフェイス接続されたデバイスは、デバイス６００の一部として含まれ得る、または別個であってデバイス６００と通信し得る。たとえば、ネットワーク通信デバイス、ストレージデバイス（た
とえば、メモリおよび／またはデータベース１０６）、ならびに入出力デバイスは、入出力インターフェイス６０６を介して通信可能である。いくつかの実装では、入出力インターフェイスは、入力デバイス（キーボード、ポインティングデバイス、タッチスクリーン、マイク、カメラ、スキャナ、センサなど）および／または出力デバイス（表示デバイス、スピーカデバイス、プリンタ、モータなど）等のインターフェイスデバイスに接続可能である。入出力インターフェイス６０６は、たとえば、デバイス６００をセルラーネットワークまたは他のテレフォニーネットワークに結合するために、テレフォニーインターフェイスも含み得る。
The input/output interface 606 can provide functionality that allows the device 600 to interface with other systems and devices. The interfaced devices can be included as part of the device 600 or can be separate and communicate with the device 600. For example, network communication devices, storage devices (e.g., memory and/or database 106), and input/output devices can communicate through the input/output interface 606. In some implementations, the input/output interface can be connected to interface devices such as input devices (keyboards, pointing devices, touch screens, microphones, cameras, scanners, sensors, etc.) and/or output devices (display devices, speaker devices, printers, motors, etc.). The input/output interface 606 can also include a telephony interface, for example, to couple the device 600 to a cellular network or other telephony network.
入出力インターフェイス６０６に接続可能なインターフェイス付きデバイスのいくつかの例は、コンテンツ、たとえば、画像、動画、および／または本明細書に記載されるような出力アプリケーションのユーザインターフェイスを表示するために使用することができる１つ以上の表示デバイス６２０を含み得る。表示デバイス６２０は、ローカル接続（たとえば、ディスプレイバス）を介して、および／またはネットワーク接続を介してデバイス６００に接続することができ、任意の適切な表示デバイスであり得る。表示デバイス６２０は、ＬＣＤ、ＬＥＤ、またはプラズマディスプレイスクリーン、ＣＲＴ、テレビ、モニタ、タッチスクリーン、３Ｄディスプレイスクリーン、または他の視覚表示デバイスといった、任意の適切な表示デバイスを含み得る。たとえば、表示デバイス６２０は、モバイルデバイスに設けられたフラットディスプレイ画面、ゴーグルまたはヘッドセットデバイスに設けられた複数のディスプレイ画面、またはコンピュータデバイスのモニタ画面であり得る。
Some examples of interfaced devices connectable to the input/output interface 606 may include one or
図示を容易にするために、図６では、プロセッサ６０２、メモリ６０４、入出力インターフェイス６０６、およびソフトウェアブロック６０８，６１２，６３０の各々について１つのブロックを示している。これらのブロックは、１つ以上のプロセッサまたは処理回路、オペレーティングシステム、メモリ、入出力インターフェイス、アプリケーション、および／またはソフトウェアモジュールを表してもよい。他の実装では、デバイス６００は、示されたすべての構成要素を有していなくてもよく、および／または、本明細書に示された要素の代わりに、もしくはそれらに加えて、他のタイプの要素を含む他の要素を有してもよい。いくつかの構成要素は、本明細書のいくつかの実装で説明されるようなブロックおよび動作を実行するものとして説明されるが、環境１００、デバイス６００、類似のシステム、またはそのようなシステムに関連付けられた任意の適切な１つまたは複数のプロセッサの構成要素の任意の適切な構成要素または組合せは、説明するブロックおよび動作を実行してもよい。
For ease of illustration, FIG. 6 shows one block for each of the
本明細書に記載される方法は、コンピュータ上で実行可能なコンピュータプログラム命令またはコードによって実現することができる。たとえば、コードは、１つ以上のデジタルプロセッサ（たとえば、マイクロプロセッサまたは他の処理回路）によって実装することができ、非一時的コンピュータ読取可能媒体（たとえば、ストレージデバイス）、たとえば、半導体もしくは固体メモリ、磁気テープ、取外し可能コンピュータディスケット、ランダムアクセスメモリ（ＲＡＭ）、読取専用メモリ（ＲＯＭ）、フラッシュメモリ、硬磁気ディスク、光ディスク、固体メモリドライブなどの磁気、光学、電磁、または半導体ストレージ媒体などを含むコンピュータプログラム製品に格納可能である。プログラム命令を、たとえば、サーバ（たとえば、分散システムおよび／またはクラウドコンピューティングシステム）から配信されるサービスとしてのソフトウェア（ＳａａＳ）の形式で、電子信号に含み、電子信号として提供することも可能である。または、１つ以上の方法は、ハードウェア（論理ゲートなど）、またはハードウェアとソフトウェアとの組合わせで実装可能である。ハードウェアの例は、プログラマブルプロセッサ（たとえば、フィールドプログラマブルゲートアレイ（ＦＰＧＡ）、複合プログラマブル論理デバイス）、汎用プロセッサ、グラフィックスプロセッサ、特定用途向け集積回路（ＡＳＩＣ）等であり得
る。１つ以上の方法は、システム上で実行されるアプリケーションの一部もしくはコンポーネントとして、または他のアプリケーションおよびオペレーティングシステムと連動して実行されるアプリケーションもしくはソフトウェアとして実行可能である。
The methods described herein can be realized by computer program instructions or code executable on a computer. For example, the code can be implemented by one or more digital processors (e.g., microprocessors or other processing circuits) and can be stored in a computer program product including a non-transitory computer-readable medium (e.g., storage device), such as a magnetic, optical, electromagnetic, or semiconductor storage medium, such as a semiconductor or solid-state memory, a magnetic tape, a removable computer diskette, a random access memory (RAM), a read-only memory (ROM), a flash memory, a hard magnetic disk, an optical disk, a solid-state memory drive, etc. The program instructions can also be included in and provided as an electronic signal, for example, in the form of a software as a service (SaaS) delivered from a server (e.g., a distributed system and/or a cloud computing system). Alternatively, one or more methods can be implemented in hardware (e.g., logic gates), or a combination of hardware and software. Examples of hardware can be programmable processors (e.g., field programmable gate arrays (FPGAs), complex programmable logic devices), general-purpose processors, graphics processors, application-specific integrated circuits (ASICs), etc. One or more of the methods may be implemented as part or component of an application executing on the system, or as an application or software running in conjunction with other applications and the operating system.
本明細書ではその特定の実装に関して説明したが、これらの特定の実装は例示に過ぎず、制限的なものではない。例に示された概念は、他の例および実装に適用可能である。 Although described herein with respect to specific implementations thereof, these specific implementations are illustrative only and not limiting. The concepts illustrated in the examples are applicable to other examples and implementations.
本明細書で議論される特定の実装が、ユーザに関する個人情報（たとえば、ユーザデータ、顔認識データ、ユーザのソーシャルネットワークに関する情報、ユーザの位置およびその位置での時間、ユーザの生体情報、ユーザの活動および人口統計情報）を収集または使用し得る状況において、ユーザには、情報を収集するか否か、個人情報を格納するか否か、個人情報を使用するか否か、ならびにユーザについて情報を収集、格納および使用する態様を制御する１つ以上の機会が提供される。すなわち、本明細書で議論されるシステムおよび方法では、ユーザの個人情報の具体的な収集、格納および／または使用は、関連するユーザからそうするための明示的な認可を受けると行われる。たとえば、ユーザには、プログラムまたは機能が、その特定のユーザ、またはプログラムもしくは機能に関連する他のユーザについてのユーザ情報を収集するか否かの制御が提供される。個人情報が収集される各ユーザには、当該ユーザに関連する情報収集の制御を可能にし、情報が収集されるかどうか、および情報のどの部分が収集されるかに関して許可または承認を与えるための１つ以上のオプションが提示される。たとえば、ユーザには、通信ネットワークを介して１つ以上のそのような制御オプションを提供可能である。さらに、特定のデータは、格納または使用される前に、個人を特定可能な情報を削除できるように、１つ以上の方法で処理可能である。一例として、ユーザのアイデンティティは、個人を特定できる情報が決定されないように取り扱われてもよい。別の例として、ユーザのデバイスの地理的位置は、ユーザの特定の位置を特定できないように、より大きな地域へと一般化されてもよい。 In situations where certain implementations discussed herein may collect or use personal information about a user (e.g., user data, facial recognition data, information about the user's social network, the user's location and time at that location, the user's biometric information, the user's activity and demographic information), the user is provided with one or more opportunities to control whether information is collected, whether the personal information is stored, whether the personal information is used, and the manner in which information is collected, stored, and used about the user. That is, in the systems and methods discussed herein, the specific collection, storage, and/or use of a user's personal information occurs upon explicit authorization to do so from the associated user. For example, a user is provided with control over whether a program or function collects user information about that particular user, or about other users associated with the program or function. Each user for whom personal information is collected is presented with one or more options to enable control of the collection of information associated with that user and to give permission or approval regarding whether and what portions of the information are collected. For example, a user may be provided with one or more such control options over a communications network. Additionally, certain data may be processed in one or more ways to remove personally identifiable information before it is stored or used. As one example, a user's identity may be treated such that no personally identifiable information can be determined. As another example, the geographic location of a user's device may be generalized to a larger region such that the user's specific location cannot be determined.
なお、本開示で説明する機能ブロック、動作、特徴、方法、デバイス、およびシステムは、当業者に知られているようなシステム、デバイス、および機能ブロックの異なる組合わせに統合または分割され得る。任意の適切なプログラミング言語およびプログラミング技術を使用して、特定の実装のルーチンを実現することができる。異なるプログラミング技法、たとえば手続き的またはオブジェクト指向の技法を採用してもよい。ルーチンは、単一の処理デバイスまたは複数のプロセッサ上で実行することができる。ステップ、動作、または計算は、特定の順序で提示され得るが、順序は、異なる特定の実装で変更されてもよい。いくつかの実装では、本明細書で順次として示される複数のステップまたは動作を同時に実行してもよい。 It should be noted that the functional blocks, operations, features, methods, devices, and systems described in this disclosure may be combined or divided into different combinations of systems, devices, and functional blocks as known to those skilled in the art. Any suitable programming language and programming techniques may be used to implement the routines of a particular implementation. Different programming techniques, e.g., procedural or object-oriented techniques, may be employed. The routines may be executed on a single processing device or multiple processors. Although steps, operations, or computations may be presented in a particular order, the order may be changed in different particular implementations. In some implementations, multiple steps or operations shown as sequential in this specification may be executed simultaneously.
Claims (16)
入力動画の複数のクレームの各フレームにおける２つ以上のクロップ候補領域について、フレームごとのクロップスコアを決定することを備え、前記クロップスコアは、顔分析に基づくスコア、アクティブスピーカ分析に基づくスコア、または美的スコアのうちの１つ以上に基づき、前記クロップ候補領域は各々、前記入力動画の入力アスペクト比と異なる出力アスペクト比を有し、前記方法はさらに、
各クロップ候補領域について、当該クロップ候補領域のフレーム間の動きを分析することによって、それぞれの動きコストを決定することと、
前記それぞれの動きコストと、前記２つ以上のクロップ候補領域の前記フレームごとのクロップスコアとに基づいて、前記入力動画についてクロップ領域位置を表す最小コスト経路を決定することと、
前記最小コスト経路に沿って、前記クロップ領域位置に対応するクロップキーフレーミングを生成することとを備え、前記クロップキーフレーミングは、開始フレームと、終了フレームと、前記クロップ領域位置とを含み、前記方法はさらに、
前記クロップキーフレーミングに基づいて前記入力動画から得られる修正済み動画を出力することを備える、コンピュータにより実現される方法。 1. A computer-implemented method comprising:
determining a per-frame crop score for two or more crop candidate regions in each frame of a plurality of frames of an input video, the crop score being based on one or more of a facial analysis based score, an active speaker analysis based score, or an aesthetic score, each of the crop candidate regions having an output aspect ratio different from an input aspect ratio of the input video, the method further comprising:
determining, for each crop candidate region, a respective motion cost by analyzing frame-to-frame motion of the crop candidate region;
determining a minimum cost path representing a crop region location for the input video based on the respective motion costs and the per-frame crop scores of the two or more crop candidate regions;
generating a crop key framing along the least cost path corresponding to the crop region locations, the crop key framing including a start frame, an end frame, and the crop region locations, the method further comprising:
outputting a modified video derived from the input video based on the crop key framing.
入力動画の複数のクレームの各フレームにおける２つ以上のクロップ候補領域について、フレームごとのクロップスコアを決定することを含み、前記クロップスコアは、顔分析に基づくスコア、アクティブスピーカ分析に基づくスコア、または美的スコアのうちの１つ以上に基づき、前記クロップ候補領域は各々、前記入力動画の入力アスペクト比と異なる出力アスペクト比を有し、前記複数の動作はさらに、
前記クロップ候補領域のフレーム間の動きを分析することによって、各クロップ候補領域についてそれぞれの動きコストを決定することと、
前記それぞれの動きコストと、前記２つ以上のクロップ候補領域の前記フレームごとのクロップスコアとに基づいて、前記入力動画についてクロップ領域位置を表す最小コスト経路を決定することと、
前記最小コスト経路に沿って、前記クロップ領域位置に対応するクロップキーフレーミングを生成することとを含み、前記クロップキーフレーミングは、開始フレームと、終了フレームと、前記クロップ領域位置とを含み、前記複数の動作はさらに、
前記クロップキーフレーミングに基づいて前記入力動画から得られる修正済み動画を出力することを含む、システム。 1. A system comprising one or more processors coupled to a non-transitory computer readable medium, the non-transitory computer readable medium storing software instructions that, when executed by the one or more processors, cause the one or more processors to perform a plurality of operations, the plurality of operations including:
determining a per-frame crop score for two or more crop candidate regions in each frame of a plurality of frames of an input video, the crop scores being based on one or more of a facial analysis based score, an active speaker analysis based score, or an aesthetic score, each of the crop candidate regions having an output aspect ratio different from an input aspect ratio of the input video, and the plurality of operations further comprising:
determining a respective motion cost for each crop candidate region by analyzing frame-to-frame motion of the crop candidate regions;
determining a minimum cost path representing crop region locations for the input video based on the respective motion costs and the per-frame crop scores of the two or more crop candidate regions;
generating a crop key framing along the least cost path corresponding to the crop region location, the crop key framing including a start frame, an end frame, and the crop region location, the plurality of operations further comprising:
outputting a modified video derived from the input video based on the crop key framing.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201962948179P | 2019-12-13 | 2019-12-13 | |
US62/948,179 | 2019-12-13 | ||
JP2022519751A JP7257591B2 (en) | 2019-12-13 | 2020-12-08 | Personalized automatic video cropping |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2022519751A Division JP7257591B2 (en) | 2019-12-13 | 2020-12-08 | Personalized automatic video cropping |
Publications (3)
Publication Number | Publication Date |
---|---|
JP2023089048A JP2023089048A (en) | 2023-06-27 |
JP2023089048A5 JP2023089048A5 (en) | 2023-07-07 |
JP7483089B2 true JP7483089B2 (en) | 2024-05-14 |
Family
ID=
Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2013172446A (en) | 2012-02-23 | 2013-09-02 | Sony Corp | Information processor, terminal, imaging apparatus, information processing method, and information provision method in imaging apparatus |
JP2015170973A (en) | 2014-03-06 | 2015-09-28 | キヤノン株式会社 | Image processing apparatus and image processing method |
US20160173944A1 (en) | 2014-12-15 | 2016-06-16 | Vessel Group, Inc. | Processing techniques in audio-visual streaming systems |
JP2018081701A (en) | 2013-03-14 | 2018-05-24 | フェイスブック，インク． | Image cropping according to points of interest |
Patent Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JP2013172446A (en) | 2012-02-23 | 2013-09-02 | Sony Corp | Information processor, terminal, imaging apparatus, information processing method, and information provision method in imaging apparatus |
JP2018081701A (en) | 2013-03-14 | 2018-05-24 | フェイスブック，インク． | Image cropping according to points of interest |
JP2015170973A (en) | 2014-03-06 | 2015-09-28 | キヤノン株式会社 | Image processing apparatus and image processing method |
US20160173944A1 (en) | 2014-12-15 | 2016-06-16 | Vessel Group, Inc. | Processing techniques in audio-visual streaming systems |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11587319B2 (en) | Gating model for video analysis | |
EP3815042B1 (en) | Image display with selective depiction of motion | |
JP7257591B2 (en) | Personalized automatic video cropping | |
JP2022528294A (en) | Video background subtraction method using depth | |
CA3027414A1 (en) | Combining faces from source images with target images based on search queries | |
US11949848B2 (en) | Techniques to capture and edit dynamic depth images | |
JP7483089B2 (en) | Personalized automatic video cropping | |
JP2024514728A (en) | Selective Image Blur Using Machine Learning |