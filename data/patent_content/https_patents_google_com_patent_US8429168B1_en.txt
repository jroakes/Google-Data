US8429168B1 - Learning semantic image similarity - Google Patents
Learning semantic image similarity Download PDFInfo
- Publication number
- US8429168B1 US8429168B1 US12/638,704 US63870409A US8429168B1 US 8429168 B1 US8429168 B1 US 8429168B1 US 63870409 A US63870409 A US 63870409A US 8429168 B1 US8429168 B1 US 8429168B1
- Authority
- US
- United States
- Prior art keywords
- image
- images
- collection
- similarity
- sparse feature
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V10/00—Arrangements for image or video recognition or understanding
- G06V10/70—Arrangements for image or video recognition or understanding using pattern recognition or machine learning
- G06V10/74—Image or video pattern matching; Proximity measures in feature spaces
- G06V10/75—Organisation of the matching processes, e.g. simultaneous or sequential comparisons of image or video features; Coarse-fine approaches, e.g. multi-scale approaches; using context analysis; Selection of dictionaries
- G06V10/758—Involving statistics of pixels or of feature values, e.g. histogram matching
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/50—Information retrieval; Database structures therefor; File system structures therefor of still image data
- G06F16/58—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/583—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F18/00—Pattern recognition
- G06F18/20—Analysing
- G06F18/24—Classification techniques
- G06F18/241—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches
- G06F18/2413—Classification techniques relating to the classification model, e.g. parametric or non-parametric approaches based on distances to training or reference patterns
- G06F18/24147—Distances to closest patterns, e.g. nearest neighbour classification
Definitions
- This specification relates to identifying similar images.
- Search engines aim to identify resources (e.g., images, audio, video, web pages, text, documents) that are relevant to a user's needs and to present information about the resources in a manner that is most useful to the user.
- Search engines return a set of search results in response to a user submitted text query. For example, in response to an image search text query (i.e., a query to identify image resources), the search engine returns a set of search results identifying image resources responsive to the query (e.g., as a group of thumbnail representations of the image resources).
- users may want to enter a query that is not textual. For example, a user that has an image may wish to search for similar or related images. Additionally, a user can be interested in refining the results of a previous image search to identify images similar to an image in the presented search results.
- Some conventional techniques for learning image similarity rely on human raters that determine the relative similarity of image pairs. For example, a human rater can be presented with several object pairs and asked to select the pair that is most similar. Relative similarity can also be identified using common labels associated with images or that are provided in response to a common query.
- This specification describes technologies relating to identifying similar images.
- one aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving a collection of images and data associated with each image in the collection of images; generating a sparse feature representation for each image in the collection of images; and training an image similarity function using image triplets sampled from the collection of images and corresponding sparse feature representations.
- Other embodiments of this aspect include corresponding systems, apparatus, and computer program products.
- the collection of images is identified using collection of image search queries and a specified number of image resources identified in ranked results responsive to each image query.
- the data associated with each image in the collection of images is the corresponding image search query.
- the data associated with each image in the collection of images is a label associated with the image.
- Each image triplet identifies an image, a less relevant image, and a more relevant image.
- the similarity function is trained such that: S(p i ,p i + )>S(p i ,p i ⁇ )+c, ⁇ p i ,p i + ,p i ⁇ ⁇ P, where S(p i ,p i + ) is a similarity score for a pair of more relevant images, S(p i ,p i ⁇ ) is a similarity score for a pair of less relevant images for a collection of images P, and c is a constant.
- Training the image similarity function further comprises iteratively sampling image triplets p,p i + ,p i ⁇ , from a collection of images such that the relative similarity, r, for r(p i ,p i + )>r(p i ,p i ⁇ ) and using sparse feature representations of the sampled images to update a similarity matrix for each iteration.
- Generating a sparse feature representation for an image further includes dividing the image into blocks; generating an edge histogram and a color histogram for each block to determine local descriptors for the image; and combining the local descriptors to obtain a sparse feature representation of the image.
- one aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving an image search query; presenting image search results responsive to the image search query; receiving an input requesting images similar to an image of the image search results; identifying similar images in response to the input using a similarity matrix; and presenting one or more similar images.
- Other embodiments of this aspect include corresponding systems, apparatus, and computer program products.
- a system can learn similarity for a large collection of images efficiently and with high accuracy. Semantically similar images to an identified image can be provided to users.
- FIG. 1 shows a representation illustrating relative similarity between images.
- FIG. 2 is a flow chart of an example method for mapping similar images.
- FIG. 3 is a flow chart of an example method for extracting sparse features from an image.
- FIG. 4 is a diagram representing an example of generating an edge histogram for an image block.
- FIG. 5 is a flow chart of an example method for training an image similarity function.
- FIG. 6 is a flow chart of an example method for providing similar images.
- FIG. 7 illustrates an example system architecture
- FIG. 1 shows a representation 100 illustrating relative similarity between images.
- Relative similarity is a measure of similarity of one image as compared with another image.
- a similarity of an image relative to the other two images can be determined e.g., based on labeling, query data, or other information about the images.
- the system will give the first pair a higher similarity score.
- FIG. 1 shows an example query image 102 (e.g., an image as a query as opposed to a text query) showing a dog.
- the goal is for the system to identify the dog to be more similar to the query image 102 than to the lizard (e.g., the similarity score of the pair (dog, dog) is greater than the similarity score for (dog, lizard).
- the relevance can be used to determine the similarity between images.
- the system can use this general relevance relationship to learn a similarity scoring function that matches the degree of relevance for all triplets, (image, image + , image ⁇ ) such that S w (image, image + )>S w (image, image ⁇ )+constant, where Sw represents a similarity score and the constant (e.g., “1”) allows control over the degree of difference in similarity scores (i.e., a degree of how similar and non-similar images are required to be).
- FIG. 2 is a flow chart of an example method 200 for training an image similarity function. For convenience, the method 200 will be described with respect to a system that performs the method 200 .
- the system receives 202 a collection of images and associated data.
- the images in the collection of images can be received from one or more sources.
- the images can be images from a repository returned in response to one or more of a collection of image search text-queries.
- the associated data e.g., metadata
- the associated data for each image can also include the one or more image search queries to which the image was responsive.
- a group of responsive images are identified.
- the images can be received from an annotated corpus of images.
- the annotation can include a labeling associated with the respective images (e.g., as image metadata, image file name, text associated with the image in a particular resource, for example, in a web page).
- image metadata e.g., as image metadata, image file name, text associated with the image in a particular resource, for example, in a web page.
- one or more labels can belong to broader categories. Thus, two images can have separate labels but belong to the same category.
- an image sharing a query or a label with another image is likely to be more relevant relative to an image that does not share a label or query with the image.
- an image sharing a category with another image is likely to be more relevant relative to an image that does not share the category.
- the system extracts 204 sparse features from each image of the collection of images.
- the system extracts sparse features in order to generate a sparse feature vector representation of each image in the collection of images.
- One method for generating sparse feature vectors from images will be described below respect to FIG. 3 .
- the system trains 205 a similarity function using the images of the collection.
- the similarity function is represented in the form of a similarity matrix, and is learned from examples of image pairs. Once trained, the similarity function provides a similarity measure (e.g., a score) given any pair of two images.
- a similarity measure e.g., a score
- the system uses image triplets satisfying a relative relevance criteria along with the sparse feature vectors in order to train the similarity function.
- An iterative training process using a passive-aggressive learning technique is described in greater detail below with respect to FIG. 5 .
- the system stores 208 the similarity matrix for use in retrieving similar images to an image in the collection.
- the system can identify and provide similar images using the similarity matrix.
- FIG. 3 is a flow chart of an example method 300 for extracting sparse features from an image.
- the system receives 302 an image.
- the image can be, for example, an image from a collection of images (received, for example, as described above) to be used in building a similarity matrix for the collection.
- the system divides 304 the image into blocks.
- the blocks are rectangular or square shaped. In some alternative implementations, other shapes, both polygonal and non-polygonal are used. In some implementations, each block overlaps adjacent blocks by a specified amount along one or more axes.
- the system generates 306 an edge and color histogram for each block.
- the edge histogram for each block can be generated using uniform local binary patterns that provide texture descriptors. Local binary patterns estimate a texture histogram of a block by considering differences in intensity at circular neighborhoods centered on each pixel. In some implementations, a local binary pattern 8,2 is used. This means that a circle of radius 2 is considered centered on each block. For each circle, the intensity of the center pixel is compared to the interpolated intensities located at 8 equally-spaced locations on the circle. If the center pixel has a higher intensity value than an interpolated intensity at a location, a binary 0 is given for that location on the circle. If the center pixel has a lower intensity value than an interpolated intensity at a location, a binary 1 is given for that location on the circle. An 8-bit sequence of binary values is generated from the comparison.
- FIG. 4 is a diagram 400 representing an example of generating an edge histogram for an image block.
- a pixel grid 402 includes a center pixel 404 (having intensity value 83) and a circle 406 showing the interpolated intensities located at 8 equally-spaced location on the circle 406 (e.g., 67, 101, 185).
- a pixel grid 408 shows the circle 406 along with binary values at the locations responsive to the comparison of location intensities with the intensity of the center pixel. For example, since the center pixel's intensity value of 83 is less than the location interpolated intensity value of 101, a binary 1 is recorded for that location.
- the binary values are used to generate a sequence 410 of binary values (e.g., 11000111).
- a given block e.g., corresponding to a region of 64 ⁇ 64 pixels
- the color histogram for each block can be generated using a clustering technique, for example, k-means clustering.
- a palette of colors is selected by training a color codebook from the red, green, and blue pixels of a large training set of images using k-means.
- the color histogram for the block is obtained by mapping each block pixel to the closest color in the codebook palette.
- the system concatenates 308 the histograms for each block. Concatenating the histograms results in a single vector descriptor per block of the image, e.g., ⁇ edge histogram, color histogram>. These block vectors can be referred to as local descriptors of the image.
- the system combines the local descriptors of each block to obtain a sparse feature representation for the image.
- Each local descriptor of an image p is represented as a discrete index referred to as a visual term (“visterm”).
- the image is therefore represented by a bag-of-visterms vector (analogous to a bag-of-words used in text analysis) in which each component p i is related to the presence or absence of visterm i in p.
- the system maps the descriptors to discrete indexes according to a codebook C.
- the codebook can be learned from the local descriptors of the training images through k-means clustering.
- the codebook represents each image block by a nearest codeword. This creates a sparse code, for each image, where the sum of the sparse vector is the number of patches. The number of non-zero entries could be smaller since some vector entries have values >1. All the non-zero counts in the vector are normalized by assigning particular weights p i of each visterm. The assignment of the weight p i of visterm i in image p is defined by:
- f i is the term frequency if i in p, which refers to the number of occurrences of i in p
- d j is the inverse document frequency of i, which is defined as ⁇ log(r j ).
- r j is the fraction of training images containing at least one occurrence of visterm j.
- FIG. 5 is a flow chart of an example method 500 for training an image similarity function. For convenience, the method 500 will be described with respect to a system that performs the method 500 .
- the system identifies 502 image triplets from a collection of images.
- Each image triplet includes an image, an image of greater similarity, and an image of lesser similarity (p i , p i + , p i ⁇ ).
- the triplets are not stored in memory. Instead, information about labels or queries associated with images can be used to construct triplets during the training process. Thus, for labels or queries, lists of relevant images are maintained.
- the system uniformly samples an image p i from the collection of images P.
- the system samples an image p i + from the images sharing the same labels or query with p i .
- the system also samples an image p i ⁇ from the images that do not share a label or query with p i .
- sampling p i ⁇ directly from P, but not within the group of p i + (e.g., same label or query) results in a non-relevant image with very high probability. Sampling can be repeated until a non-relevant image is found, or stopped after a few steps without adding a significant amount of noise.
- relevance feedback information r(p i ,p j ) are provided as real numbers (e.g., not as “within a relevant set” or “not within a relevant set”)
- the system can use these number values to bias training toward those pairs that have a higher relevance feedback value. For example, if the similarity between two images is measured as the fraction of queries that both images are relevant for, a real number relevance value can be obtained.
- a human evaluation experiment can be used to compare pairs of images where the evaluators are asked to provide numerical similarity values. This can be done by considering r(p i ,p j ) as frequencies of appearance and sampling pairs according to the distribution of these frequencies.
- the system uses 504 the triplets and the corresponding sparse feature vectors to iteratively train a similarity function.
- the system trains a pairwise similarity function S.
- a pairwise relevance measure r ij r(p i ,p j ) ⁇ where p i and p j represent respective images of a pair of images.
- the pairwise relevance measure identifies how strongly p j ⁇ P is related to p i ⁇ P.
- the relevance measure encodes information that the two images of the pair belong to the same category or were responsive to a same query. However, more generally, full access to the values of r is not required.
- the system can compare some pairwise relevance scores to determiner which pair is more relevant (e.g., r(p i ,p j ) and r(p i ,p k ). Additionally, if a relevance value for a particular pair is not available, the value is zero.
- the system seeks to learn a similarity function S(p i ,p j ) that assigns higher similarity scores to pairs of more relevant images, S(p i ,p i + )>S(p i ,p i ⁇ ), ⁇ p i ,p i + ,p i ⁇ ⁇ P such that r(p i ,p i + )>r(p i ,p i ⁇ ).
- p i is used to denote both the image and its representation as a column vector p i ⁇ d .
- a parametric similarity function is used having the bilinear form: S w ( p i ,p j ) ⁇ p i T Wp j (equation 1) with W ⁇ d ⁇ d. If the images of p i are represented as sparse vectors, namely, only a number k i ⁇ d of the d entries in the vector p i are non-zeros, then the value of equation 1 an be computed efficiently with a large d. Specifically, S W can be calculated with complexity O(k i k j ) regardless of the dimensionality of d.
- a passive-aggressive online learning technique is used to process triplets of images p i ,p i + ,p i ⁇ ⁇ P from the set of images such that S(p i ,p i + )>S(p i ,p i ⁇ )+1, where the +1 provides a safety margin to prevent false similarity results.
- W is initialized to some initial value W 0 .
- the system randomly selects a triplet (p i ,p i + ,p i ⁇ ), and uses the triplet to solve the following convex problem with soft margin:
- W i arg ⁇ ⁇ min w ⁇ 1 2 ⁇ ⁇ W - W i - 1 ⁇ Fro 2 + C ⁇ ⁇ ⁇ , such that l W (p i ,p i + ,p i ⁇ ) ⁇ and ⁇ 0. ⁇ W ⁇ W i ⁇ 1 ⁇ Fro 2 is the Frobenius norm (point-wise L2 norm).
- W i is selected to optimize a trade-off between remaining close to the previous parameters W i ⁇ 1 and minimizing the loss on the current triplet l W (p i ,p i + ,p i ⁇ ).
- An aggressiveness parameter C controls the trade-off.
- ⁇ i min ⁇ ⁇ C , ⁇ l w i - 1 ⁇ ( p i , p i + , p i - ) ⁇ V i ⁇ 2 ⁇ .
- the similarity matrix W is not necessarily positive or symmetric. However, a variation of the passive-aggressive technique can be used to generate a symmetric solution.
- ⁇ ⁇ i min ⁇ ⁇ ⁇ C , l ⁇ w i - 1 ⁇ ( p i , p i + , p i - ) ⁇ V ⁇ i ⁇ 2 ⁇ . Since the matrix is symmetric, each update of W preserves its symmetry. Hence, if initialized with a symmetric W 0 , a symmetric solution W i is guaranteed at any step i.
- the system stores 506 the learned similarity matrix for the collection of images.
- the similarity matrix can be used to identify and provide similar images to an image in the collection.
- ⁇ ⁇ ⁇ i min ⁇ ⁇ ⁇ C , l w i - 1 ⁇ ( p i , p i + , p i - ) ⁇ V i ⁇ 2 ⁇
- FIG. 6 is a flow chart of an example method 600 for providing similar images. For convenience, the method 600 will be described with respect to a system that performs the method 600 .
- the system receives 602 an image.
- the image can be received based on a user selection of an image (e.g., selection of an image representation presented in a set of image search results provided in response to a text query for images or selected from an image collection).
- image search results responsive to the search query are identified.
- the system can receive image search results from a search engine.
- the search results can include references to particular image resources represented, for example, by thumbnail representations of the resources.
- image search results responsive to an image query “San Francisco” can include a thumbnail representing a number of different resources and represent various content associated with San Francisco.
- the image resources can include images of the city skyline, maps, cable cars, or various landmarks (e.g., the Golden Gate Bridge, Alcatraz, or Alamo Square).
- the image resources often include images that are not similar to one another.
- the system receives 604 a request for images similar to an identified image of the image search results.
- the request can be part of the user selection providing the image, or a separate action.
- a user can select a particular image within a search result interface and indicate a request for similar images to the selected image (e.g., using a right-click menu or interface menu).
- the user first selects a menu for identifying similar images and then identifies a particular image of the search results as a base image for finding similar images.
- the user can request similar images to an image resource of the search results representing the Golden Gate Bridge.
- the system identifies 606 similar images to the received image using a similarity matrix.
- the system identifies the selected image within the similarity matrix.
- the system then identifies images in the similarity matrix having a similarity to the selected image that is greater than a threshold similarity, where the threshold can be selected based on empirical data.
- the system presents 608 one or more similar images.
- the system can present the one or more similar images in the search results interface with respect to the base image.
- the image query can be submitted to a particular labeled repository of images where the query is matched to one or more labels and corresponding images are returned based on the matched label or labels. Similar images to a returned image can be identified using a similarity matrix as described above.
- collections of audio data can be trained in a similar manner using sampled audio triplets from a collection of audio to generate a similarity matrix for identifying similar audio.
- a collection of audio files can also include associated data, for example, labeling information or query record information for organizing the audio files in the collection.
- audio files can be sampled into triplets where one audio file is more similar to a particular audio file and one audio file is less similar to the particular audio file.
- Sparse features can also be extracted from each audio file.
- An audio file can be converted into an auditory image for a number of frames, each corresponding to specified time range of the audio data in the file.
- the auditory image can then be divided into sub-images (e.g., by box-cutting) and processed to identify a bag-of-features representation.
- a feature extractor can be applied to each sub-image to generate a local sparse code vector for that sub-image.
- the system approximates each of the vectors that represent the sub-images in the auditory image with sparse codes.
- the system can use either vector quantization or matching pursuit to approximate the vectors.
- the system collects the sparse codes from all the sub-images to make a large sparse code vector for the entire audio frame.
- each rectangle has been converted into a sparse code they are concatenated into one high-dimensional sparse vector, representing the entire auditory image.
- the system combines 306 the sparse vectors representing individual frames into a sparse vector representing the audio data of the audio file, e.g., by simply summing them up.
- Sampled audio file triplets and their corresponding sparse feature representations can be used to train a similarity function to generate a similarity matrix as described above.
- similar audio can be identified for received input audio. For example, for a given sound a user can identify sounds that are semantically similar to the given sound.
- FIG. 7 illustrates an example system architecture 700 .
- the system architecture 700 is capable of performing operations for learning image similarity and retrieval of similar images.
- the system architecture 700 includes one or more processors 702 (e.g., IBM PowerPC, Intel Pentium 4, etc.), one or more display devices 704 (e.g., CRT, LCD), graphics processing units 706 (e.g., NVIDIA GeForce, etc.), a network interface 708 (e.g., Ethernet, FireWire, USB, etc.), input devices 710 (e.g., keyboard, mouse, etc.), and one or more computer-readable mediums 712 .
- processors 702 e.g., IBM PowerPC, Intel Pentium 4, etc.
- display devices 704 e.g., CRT, LCD
- graphics processing units 706 e.g., NVIDIA GeForce, etc.
- a network interface 708 e.g., Ethernet, FireWire, USB, etc.
- input devices 710 e.g
- the term “computer-readable medium” refers to any medium that participates in providing instructions to a processor 702 for execution.
- the computer-readable medium 712 further includes an operating system 716 (e.g., Mac OS®, Windows®, Linux, etc.), a network communication module 718 , a feature extractor 720 , and a similarity engine 722 .
- the operating system 716 can be multi-user, multiprocessing, multitasking, multithreading, real-time and the like.
- the operating system 716 performs basic tasks, including but not limited to: recognizing input from input devices 710 ; sending output to display devices 704 ; keeping track of files and directories on computer-readable mediums 712 (e.g., memory or a storage device); controlling peripheral devices (e.g., disk drives, printers, etc.); and managing traffic on the one or more buses 714 .
- the network communications module 718 includes various components for establishing and maintaining network connections (e.g., software for implementing communication protocols, such as TCP/IP, HTTP, Ethernet, etc.).
- the feature extractor 720 and similarity engine 722 provide various software components for performing the various functions for extracting sparse features from images and training image similarity for a collection of images using the sparse features as described with respect to FIGS. 1-6 .
- Embodiments of the subject matter and the operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions, encoded on computer storage medium for execution by, or to control the operation of, data processing apparatus.
- the program instructions can be encoded on an artificially-generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- a computer storage medium can be, or be included in, a computer-readable storage device, a computer-readable storage substrate, a random or serial access memory array or device, or a combination of one or more of them.
- a computer storage medium is not a propagated signal, a computer storage medium can be a source or destination of computer program instructions encoded in an artificially-generated propagated signal.
- the computer storage medium can also be, or be included in, one or more separate physical components or media (e.g., multiple CDs, disks, or other storage devices).
- the operations described in this specification can be implemented as operations performed by a data processing apparatus on data stored on one or more computer-readable storage devices or received from other sources.
- the term “data processing apparatus” encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, a system on a chip, or multiple ones, or combinations, of the foregoing
- the apparatus can include special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- the apparatus can also include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, a cross-platform runtime environment, a virtual machine, or a combination of one or more of them.
- the apparatus and execution environment can realize various different computing model infrastructures, such as web services, distributed computing and grid computing infrastructures.
- a computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, declarative or procedural languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, object, or other unit suitable for use in a computing environment.
- a computer program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub-programs, or portions of code).
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform actions by operating on input data and generating output.
- the processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application-specific integrated circuit).
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read-only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing actions in accordance with instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks.
- a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device (e.g., a universal serial bus (USB) flash drive), to name just a few.
- Devices suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back-end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front-end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back-end, middleware, or front-end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network.
- Examples of communication networks include a local area network (“LAN”) and a wide area network (“WAN”), an inter-network (e.g., the Internet), and peer-to-peer networks (e.g., ad hoc peer-to-peer networks).
- LAN local area network
- WAN wide area network
- inter-network e.g., the Internet
- peer-to-peer networks e.g., ad hoc peer-to-peer networks.
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- a server transmits data (e.g., an HTML page) to a client device (e.g., for purposes of displaying data to and receiving user input from a user interacting with the client device).
- client device e.g., for purposes of displaying data to and receiving user input from a user interacting with the client device.
- Data generated at the client device e.g., a result of the user interaction
Abstract
Description
S w(p i ,p j)≡p i T Wp j (equation 1)
with Wε
l W(p i ,p i + ,p i −)=max{0,1−S W(p i ,p i +)+S W(p i ,p i −)}
such that lW(pi,pi +,pi −)≦ξ and ξ≧0. ∥W−Wi−1∥Fro 2 is the Frobenius norm (point-wise L2 norm). Thus, at each iteration i, Wi is selected to optimize a trade-off between remaining close to the previous parameters Wi−1 and minimizing the loss on the current triplet lW(pi,pi +,pi −). An aggressiveness parameter C controls the trade-off.
Since the matrix is symmetric, each update of W preserves its symmetry. Hence, if initialized with a symmetric W0, a symmetric solution Wi is guaranteed at any step i.
-
- Sample three images p,pi +,pi −, such that r(pi,pi +)>r(pi,pi −).
- Update Wi=Wi−1+τiVi
-
-
- and Vi=[pi 1(pk +−pk −), . . . , pi d(pk +−pk −)]T
-
Claims (27)
S(p i ,p i +)>S(p i ,p i −)+c, ∀p i ,p i + ,p i − εP,
S(p i ,p i +)>S(p i ,p i −)+c, ∀p i ,p i + ,p i − εP,
S(p i ,p i +)>S(p i ,p i −)+c, ∀p i + ,p i − εP,
Priority Applications (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/638,704 US8429168B1 (en) | 2009-12-15 | 2009-12-15 | Learning semantic image similarity |
US13/867,937 US8805812B1 (en) | 2009-12-15 | 2013-04-22 | Learning semantic image similarity |
US14/455,350 US9087271B2 (en) | 2009-12-15 | 2014-08-08 | Learning semantic image similarity |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/638,704 US8429168B1 (en) | 2009-12-15 | 2009-12-15 | Learning semantic image similarity |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/867,937 Continuation US8805812B1 (en) | 2009-12-15 | 2013-04-22 | Learning semantic image similarity |
Publications (1)
Publication Number | Publication Date |
---|---|
US8429168B1 true US8429168B1 (en) | 2013-04-23 |
Family
ID=48094978
Family Applications (3)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US12/638,704 Active 2030-08-20 US8429168B1 (en) | 2009-12-15 | 2009-12-15 | Learning semantic image similarity |
US13/867,937 Active US8805812B1 (en) | 2009-12-15 | 2013-04-22 | Learning semantic image similarity |
US14/455,350 Active US9087271B2 (en) | 2009-12-15 | 2014-08-08 | Learning semantic image similarity |
Family Applications After (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/867,937 Active US8805812B1 (en) | 2009-12-15 | 2013-04-22 | Learning semantic image similarity |
US14/455,350 Active US9087271B2 (en) | 2009-12-15 | 2014-08-08 | Learning semantic image similarity |
Country Status (1)
Country | Link |
---|---|
US (3) | US8429168B1 (en) |
Cited By (33)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20120014607A1 (en) * | 2010-07-15 | 2012-01-19 | Postech Academy-Industry Foundation | Method and camera for detecting a region having a specific shape |
US20120330957A1 (en) * | 2007-05-30 | 2012-12-27 | International Business Machines Corporation | Information processing method for determining weight of each feature in subjective hierarchical clustering |
US20130006991A1 (en) * | 2011-06-28 | 2013-01-03 | Toru Nagano | Information processing apparatus, method and program for determining weight of each feature in subjective hierarchical clustering |
US20130121570A1 (en) * | 2011-09-17 | 2013-05-16 | Zhe Lin | Methods and Apparatus for Visual Search |
US20130148881A1 (en) * | 2011-12-12 | 2013-06-13 | Alibaba Group Holding Limited | Image Classification |
CN103607603A (en) * | 2013-11-28 | 2014-02-26 | Tcl集团股份有限公司 | Method and device for identifying station caption |
US8805812B1 (en) | 2009-12-15 | 2014-08-12 | Google Inc. | Learning semantic image similarity |
US8831358B1 (en) * | 2011-11-21 | 2014-09-09 | Google Inc. | Evaluating image similarity |
US8874557B2 (en) | 2011-09-02 | 2014-10-28 | Adobe Systems Incorporated | Object retrieval and localization using a spatially-constrained similarity model |
US8880563B2 (en) | 2012-09-21 | 2014-11-04 | Adobe Systems Incorporated | Image search by query object segmentation |
US20150120760A1 (en) * | 2013-10-31 | 2015-04-30 | Adobe Systems Incorporated | Image tagging |
US20150134688A1 (en) * | 2013-11-12 | 2015-05-14 | Pinterest, Inc. | Image based search |
US20160125274A1 (en) * | 2014-10-31 | 2016-05-05 | Bolei Zhou | Discovering visual concepts from weakly labeled image collections |
US20160217341A1 (en) * | 2015-01-28 | 2016-07-28 | Industrial Technology Research Institute | Encoding method and encoder |
WO2016142293A1 (en) * | 2015-03-06 | 2016-09-15 | Thomson Licensing | Method and apparatus for image search using sparsifying analysis and synthesis operators |
WO2016142285A1 (en) * | 2015-03-06 | 2016-09-15 | Thomson Licensing | Method and apparatus for image search using sparsifying analysis operators |
WO2017160688A1 (en) * | 2016-03-14 | 2017-09-21 | Siemens Aktiengesellschaft | Method and system for efficiently mining dataset essentials with bootstrapping strategy in 6dof pose estimate of 3d objects |
CN108205684A (en) * | 2017-04-25 | 2018-06-26 | 北京市商汤科技开发有限公司 | Image disambiguation method, device, storage medium and electronic equipment |
US10108620B2 (en) * | 2010-04-29 | 2018-10-23 | Google Llc | Associating still images and videos |
US10269055B2 (en) | 2015-05-12 | 2019-04-23 | Pinterest, Inc. | Matching user provided representations of items with sellers of those items |
CN109726794A (en) * | 2017-10-27 | 2019-05-07 | 谷歌有限责任公司 | Image based on concern generates neural network |
CN110235083A (en) * | 2017-01-02 | 2019-09-13 | 广州异构智能科技有限公司 | The unsupervised learning of object identifying method and system |
US10679269B2 (en) | 2015-05-12 | 2020-06-09 | Pinterest, Inc. | Item selling on multiple web sites |
CN112074828A (en) * | 2019-02-01 | 2020-12-11 | 谷歌有限责任公司 | Training image embedding model and text embedding model |
US10942966B2 (en) | 2017-09-22 | 2021-03-09 | Pinterest, Inc. | Textual and image based search |
US10949708B2 (en) * | 2014-06-20 | 2021-03-16 | Google Llc | Fine-grained image similarity |
US11055343B2 (en) | 2015-10-05 | 2021-07-06 | Pinterest, Inc. | Dynamic search control invocation and visual search |
US11126653B2 (en) | 2017-09-22 | 2021-09-21 | Pinterest, Inc. | Mixed type image based search results |
US11205103B2 (en) | 2016-12-09 | 2021-12-21 | The Research Foundation for the State University | Semisupervised autoencoder for sentiment analysis |
US11609946B2 (en) | 2015-10-05 | 2023-03-21 | Pinterest, Inc. | Dynamic search input selection |
US11645733B2 (en) | 2020-06-16 | 2023-05-09 | Bank Of America Corporation | System and method for providing artificial intelligence architectures to people with disabilities |
US11704692B2 (en) | 2016-05-12 | 2023-07-18 | Pinterest, Inc. | Promoting representations of items to users on behalf of sellers of those items |
US11841735B2 (en) | 2017-09-22 | 2023-12-12 | Pinterest, Inc. | Object based image search |
Families Citing this family (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN107870923B (en) * | 2016-09-26 | 2020-05-12 | 北京眼神科技有限公司 | Image retrieval method and device |
KR20180075224A (en) * | 2016-12-26 | 2018-07-04 | 삼성전자주식회사 | Electronic device and method for providing recognition result of object |
CN108304431B (en) * | 2017-06-14 | 2021-11-09 | 腾讯科技（深圳）有限公司 | Image retrieval method and device, equipment and storage medium |
CN109145991B (en) * | 2018-08-24 | 2020-07-31 | 北京地平线机器人技术研发有限公司 | Image group generation method, image group generation device and electronic equipment |
US20230022057A1 (en) * | 2021-07-16 | 2023-01-26 | Taiwan Semiconductor Manufacturing Company, Ltd. | Method for retrieving images from database |
Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5751286A (en) * | 1992-11-09 | 1998-05-12 | International Business Machines Corporation | Image query system and method |
US20070179921A1 (en) * | 2006-01-27 | 2007-08-02 | Microsoft Corporation | Object instance recognition using feature symbol triplets |
US7773800B2 (en) * | 2001-06-06 | 2010-08-10 | Ying Liu | Attrasoft image retrieval |
US20120102066A1 (en) * | 2009-06-30 | 2012-04-26 | Nokia Corporation | Method, Devices and a Service for Searching |
Family Cites Families (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
AU2001249805A1 (en) * | 2000-04-03 | 2001-10-15 | 3-Dimensional Pharmaceuticals, Inc. | Method, system, and computer program product for representing object relationships in a multidimensional space |
US8126647B2 (en) * | 2008-04-07 | 2012-02-28 | Chevron U.S.A. Inc. | Lithofacies classification system and method |
US8429168B1 (en) | 2009-12-15 | 2013-04-23 | Google Inc. | Learning semantic image similarity |
US8831358B1 (en) * | 2011-11-21 | 2014-09-09 | Google Inc. | Evaluating image similarity |
-
2009
- 2009-12-15 US US12/638,704 patent/US8429168B1/en active Active
-
2013
- 2013-04-22 US US13/867,937 patent/US8805812B1/en active Active
-
2014
- 2014-08-08 US US14/455,350 patent/US9087271B2/en active Active
Patent Citations (5)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5751286A (en) * | 1992-11-09 | 1998-05-12 | International Business Machines Corporation | Image query system and method |
US7773800B2 (en) * | 2001-06-06 | 2010-08-10 | Ying Liu | Attrasoft image retrieval |
US20070179921A1 (en) * | 2006-01-27 | 2007-08-02 | Microsoft Corporation | Object instance recognition using feature symbol triplets |
US7412427B2 (en) * | 2006-01-27 | 2008-08-12 | Microsoft Corporation | Object instance recognition using feature symbol triplets |
US20120102066A1 (en) * | 2009-06-30 | 2012-04-26 | Nokia Corporation | Method, Devices and a Service for Searching |
Non-Patent Citations (2)
Title |
---|
Granger et al. "A Discriminative Kernal-based Model to Rank Images from Text Queries." Retrieved from internet http://uchicago.edu., 14 pages. |
Maji et al. "Classification using Intersection Kernal Support Vector Machines is Efficient." Retrieved from internet http://cs.berkeley.edu., 8 pages. |
Cited By (59)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20120330957A1 (en) * | 2007-05-30 | 2012-12-27 | International Business Machines Corporation | Information processing method for determining weight of each feature in subjective hierarchical clustering |
US8972407B2 (en) * | 2007-05-30 | 2015-03-03 | International Business Machines Corporation | Information processing method for determining weight of each feature in subjective hierarchical clustering |
US9087271B2 (en) | 2009-12-15 | 2015-07-21 | Google Inc. | Learning semantic image similarity |
US8805812B1 (en) | 2009-12-15 | 2014-08-12 | Google Inc. | Learning semantic image similarity |
US10108620B2 (en) * | 2010-04-29 | 2018-10-23 | Google Llc | Associating still images and videos |
US10394878B2 (en) * | 2010-04-29 | 2019-08-27 | Google Llc | Associating still images and videos |
US20190026268A1 (en) * | 2010-04-29 | 2019-01-24 | Google Llc | Associating still images and videos |
US10922350B2 (en) | 2010-04-29 | 2021-02-16 | Google Llc | Associating still images and videos |
US20120014607A1 (en) * | 2010-07-15 | 2012-01-19 | Postech Academy-Industry Foundation | Method and camera for detecting a region having a specific shape |
US8588530B2 (en) * | 2010-07-15 | 2013-11-19 | Samsung Techwin Co., Ltd. | Method and camera for detecting a region having a specific shape |
US8918396B2 (en) * | 2011-06-28 | 2014-12-23 | International Business Machines Corporation | Information processing apparatus, method and program for determining weight of each feature in subjective hierarchical clustering |
US20130006991A1 (en) * | 2011-06-28 | 2013-01-03 | Toru Nagano | Information processing apparatus, method and program for determining weight of each feature in subjective hierarchical clustering |
US8983940B2 (en) | 2011-09-02 | 2015-03-17 | Adobe Systems Incorporated | K-nearest neighbor re-ranking |
US8874557B2 (en) | 2011-09-02 | 2014-10-28 | Adobe Systems Incorporated | Object retrieval and localization using a spatially-constrained similarity model |
US8805116B2 (en) * | 2011-09-17 | 2014-08-12 | Adobe Systems Incorporated | Methods and apparatus for visual search |
US8781255B2 (en) | 2011-09-17 | 2014-07-15 | Adobe Systems Incorporated | Methods and apparatus for visual search |
US20130121570A1 (en) * | 2011-09-17 | 2013-05-16 | Zhe Lin | Methods and Apparatus for Visual Search |
US8831358B1 (en) * | 2011-11-21 | 2014-09-09 | Google Inc. | Evaluating image similarity |
US20150170004A1 (en) * | 2011-11-21 | 2015-06-18 | Google Inc. | Evaluating image similarity |
US9275310B2 (en) * | 2011-11-21 | 2016-03-01 | Google Inc. | Evaluating image similarity |
US20130148881A1 (en) * | 2011-12-12 | 2013-06-13 | Alibaba Group Holding Limited | Image Classification |
US9342758B2 (en) * | 2011-12-12 | 2016-05-17 | Alibaba Group Holding Limited | Image classification based on visual words |
US8880563B2 (en) | 2012-09-21 | 2014-11-04 | Adobe Systems Incorporated | Image search by query object segmentation |
US20150120760A1 (en) * | 2013-10-31 | 2015-04-30 | Adobe Systems Incorporated | Image tagging |
US9607014B2 (en) * | 2013-10-31 | 2017-03-28 | Adobe Systems Incorporated | Image tagging |
US11436272B2 (en) * | 2013-11-12 | 2022-09-06 | Pinterest, Inc. | Object based image based search |
US20170220602A1 (en) * | 2013-11-12 | 2017-08-03 | Pinterest, Inc. | Object based image based search |
US10515110B2 (en) * | 2013-11-12 | 2019-12-24 | Pinterest, Inc. | Image based search |
US20150134688A1 (en) * | 2013-11-12 | 2015-05-14 | Pinterest, Inc. | Image based search |
CN103607603A (en) * | 2013-11-28 | 2014-02-26 | Tcl集团股份有限公司 | Method and device for identifying station caption |
US10949708B2 (en) * | 2014-06-20 | 2021-03-16 | Google Llc | Fine-grained image similarity |
US20160125274A1 (en) * | 2014-10-31 | 2016-05-05 | Bolei Zhou | Discovering visual concepts from weakly labeled image collections |
US9965704B2 (en) * | 2014-10-31 | 2018-05-08 | Paypal, Inc. | Discovering visual concepts from weakly labeled image collections |
US9892338B2 (en) * | 2015-01-28 | 2018-02-13 | Industrial Technology Research Institute | Encoding method and encoder for constructing an initial color table |
US20160217341A1 (en) * | 2015-01-28 | 2016-07-28 | Industrial Technology Research Institute | Encoding method and encoder |
WO2016142293A1 (en) * | 2015-03-06 | 2016-09-15 | Thomson Licensing | Method and apparatus for image search using sparsifying analysis and synthesis operators |
WO2016142285A1 (en) * | 2015-03-06 | 2016-09-15 | Thomson Licensing | Method and apparatus for image search using sparsifying analysis operators |
US11935102B2 (en) | 2015-05-12 | 2024-03-19 | Pinterest, Inc. | Matching user provided representations of items with sellers of those items |
US11443357B2 (en) | 2015-05-12 | 2022-09-13 | Pinterest, Inc. | Matching user provided representations of items with sellers of those items |
US10679269B2 (en) | 2015-05-12 | 2020-06-09 | Pinterest, Inc. | Item selling on multiple web sites |
US10269055B2 (en) | 2015-05-12 | 2019-04-23 | Pinterest, Inc. | Matching user provided representations of items with sellers of those items |
US11609946B2 (en) | 2015-10-05 | 2023-03-21 | Pinterest, Inc. | Dynamic search input selection |
US11055343B2 (en) | 2015-10-05 | 2021-07-06 | Pinterest, Inc. | Dynamic search control invocation and visual search |
US10803619B2 (en) | 2016-03-14 | 2020-10-13 | Siemens Mobility GmbH | Method and system for efficiently mining dataset essentials with bootstrapping strategy in 6DOF pose estimate of 3D objects |
WO2017160688A1 (en) * | 2016-03-14 | 2017-09-21 | Siemens Aktiengesellschaft | Method and system for efficiently mining dataset essentials with bootstrapping strategy in 6dof pose estimate of 3d objects |
US11704692B2 (en) | 2016-05-12 | 2023-07-18 | Pinterest, Inc. | Promoting representations of items to users on behalf of sellers of those items |
US11205103B2 (en) | 2016-12-09 | 2021-12-21 | The Research Foundation for the State University | Semisupervised autoencoder for sentiment analysis |
CN110235083A (en) * | 2017-01-02 | 2019-09-13 | 广州异构智能科技有限公司 | The unsupervised learning of object identifying method and system |
US11144800B2 (en) * | 2017-04-25 | 2021-10-12 | Beijing Sensetime Technology Development Co., Ltd. | Image disambiguation method and apparatus, storage medium, and electronic device |
CN108205684B (en) * | 2017-04-25 | 2022-02-11 | 北京市商汤科技开发有限公司 | Image disambiguation method, device, storage medium and electronic equipment |
CN108205684A (en) * | 2017-04-25 | 2018-06-26 | 北京市商汤科技开发有限公司 | Image disambiguation method, device, storage medium and electronic equipment |
US10942966B2 (en) | 2017-09-22 | 2021-03-09 | Pinterest, Inc. | Textual and image based search |
US11126653B2 (en) | 2017-09-22 | 2021-09-21 | Pinterest, Inc. | Mixed type image based search results |
US11620331B2 (en) | 2017-09-22 | 2023-04-04 | Pinterest, Inc. | Textual and image based search |
US11841735B2 (en) | 2017-09-22 | 2023-12-12 | Pinterest, Inc. | Object based image search |
CN109726794B (en) * | 2017-10-27 | 2024-03-12 | 谷歌有限责任公司 | Generating a neural network based on an image of interest |
CN109726794A (en) * | 2017-10-27 | 2019-05-07 | 谷歌有限责任公司 | Image based on concern generates neural network |
CN112074828A (en) * | 2019-02-01 | 2020-12-11 | 谷歌有限责任公司 | Training image embedding model and text embedding model |
US11645733B2 (en) | 2020-06-16 | 2023-05-09 | Bank Of America Corporation | System and method for providing artificial intelligence architectures to people with disabilities |
Also Published As
Publication number | Publication date |
---|---|
US20150161485A1 (en) | 2015-06-11 |
US8805812B1 (en) | 2014-08-12 |
US9087271B2 (en) | 2015-07-21 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9087271B2 (en) | Learning semantic image similarity | |
US20240078258A1 (en) | Training Image and Text Embedding Models | |
US11295090B2 (en) | Multi-scale model for semantic matching | |
US10387430B2 (en) | Geometry-directed active question selection for question answering systems | |
US10671675B2 (en) | Systems and methods for a scalable continuous active learning approach to information classification | |
US20210097089A1 (en) | Knowledge graph building method, electronic apparatus and non-transitory computer readable storage medium | |
US8131786B1 (en) | Training scoring models optimized for highly-ranked results | |
EP2321787B1 (en) | Annotating images | |
US8374914B2 (en) | Advertising using image comparison | |
US20170140248A1 (en) | Learning image representation by distilling from multi-task networks | |
US11586927B2 (en) | Training image and text embedding models | |
US20110191336A1 (en) | Contextual image search | |
US9110923B2 (en) | Ranking over hashes | |
CN110083729B (en) | Image searching method and system | |
CN101305368A (en) | Semantic visual search engine | |
US20120102018A1 (en) | Ranking Model Adaptation for Domain-Specific Search | |
US20090254519A1 (en) | Method and system for building a support vector machine binary tree for fast object search | |
Coleman | Managing bias when library collections become data | |
CN111881900A (en) | Corpus generation, translation model training and translation method, apparatus, device and medium | |
CN116756281A (en) | Knowledge question-answering method, device, equipment and medium | |
Wang et al. | Grasp detection via visual rotation object detection and point cloud spatial feature scoring | |
CN116049434A (en) | Construction method and device of power construction safety knowledge graph and electronic equipment | |
CN112101015B (en) | Method and device for identifying multi-label object | |
US11501071B2 (en) | Word and image relationships in combined vector space | |
Li et al. | Intuitively searching for the rare colors from digital artwork collections by text description: a case demonstration of japanese ukiyo-e print retrieval |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:CHECHIK, GAL;BENGIO, SAMY;SHARMA, VARUN;SIGNING DATES FROM 20091209 TO 20091215;REEL/FRAME:023749/0602 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
CC | Certificate of correction | ||
CC | Certificate of correction | ||
FPAY | Fee payment |
Year of fee payment: 4 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044101/0299Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |