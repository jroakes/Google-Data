CN111465962A - Depth of motion for augmented reality of handheld user devices - Google Patents
Depth of motion for augmented reality of handheld user devices Download PDFInfo
- Publication number
- CN111465962A CN111465962A CN201980006353.3A CN201980006353A CN111465962A CN 111465962 A CN111465962 A CN 111465962A CN 201980006353 A CN201980006353 A CN 201980006353A CN 111465962 A CN111465962 A CN 111465962A
- Authority
- CN
- China
- Prior art keywords
- image
- depth map
- bilateral
- depth
- user device
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 230000003190 augmentative effect Effects 0.000 title claims description 10
- 230000033001 locomotion Effects 0.000 title description 40
- 230000002146 bilateral effect Effects 0.000 claims abstract description 103
- 238000000034 method Methods 0.000 claims abstract description 71
- 230000008569 process Effects 0.000 claims abstract description 25
- 238000009877 rendering Methods 0.000 claims abstract description 10
- 238000012937 correction Methods 0.000 claims description 18
- 238000012545 processing Methods 0.000 claims description 15
- 230000006870 function Effects 0.000 claims description 10
- 238000005259 measurement Methods 0.000 claims description 8
- 230000001413 cellular effect Effects 0.000 claims description 2
- 238000003860 storage Methods 0.000 description 18
- 230000008901 benefit Effects 0.000 description 15
- 238000013459 approach Methods 0.000 description 10
- 239000011159 matrix material Substances 0.000 description 10
- 239000013598 vector Substances 0.000 description 10
- 238000010586 diagram Methods 0.000 description 8
- 238000005457 optimization Methods 0.000 description 7
- 230000000694 effects Effects 0.000 description 6
- 230000002123 temporal effect Effects 0.000 description 6
- 230000003287 optical effect Effects 0.000 description 5
- 238000007796 conventional method Methods 0.000 description 4
- 230000000007 visual effect Effects 0.000 description 4
- 238000000354 decomposition reaction Methods 0.000 description 3
- 230000009977 dual effect Effects 0.000 description 3
- 230000003993 interaction Effects 0.000 description 3
- 230000009466 transformation Effects 0.000 description 3
- 230000006399 behavior Effects 0.000 description 2
- 238000003066 decision tree Methods 0.000 description 2
- 238000001914 filtration Methods 0.000 description 2
- 238000009499 grossing Methods 0.000 description 2
- 230000006872 improvement Effects 0.000 description 2
- 238000004519 manufacturing process Methods 0.000 description 2
- 238000013507 mapping Methods 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 238000011084 recovery Methods 0.000 description 2
- 230000004044 response Effects 0.000 description 2
- 230000001133 acceleration Effects 0.000 description 1
- 230000004931 aggregating effect Effects 0.000 description 1
- 238000004364 calculation method Methods 0.000 description 1
- 230000001364 causal effect Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 230000001427 coherent effect Effects 0.000 description 1
- 230000006835 compression Effects 0.000 description 1
- 238000007906 compression Methods 0.000 description 1
- 238000010276 construction Methods 0.000 description 1
- 230000008094 contradictory effect Effects 0.000 description 1
- 238000013461 design Methods 0.000 description 1
- 238000011161 development Methods 0.000 description 1
- 230000018109 developmental process Effects 0.000 description 1
- 239000006185 dispersion Substances 0.000 description 1
- 238000009826 distribution Methods 0.000 description 1
- 238000011156 evaluation Methods 0.000 description 1
- 230000001939 inductive effect Effects 0.000 description 1
- 238000010801 machine learning Methods 0.000 description 1
- 239000003550 marker Substances 0.000 description 1
- 239000000203 mixture Substances 0.000 description 1
- 229910052754 neon Inorganic materials 0.000 description 1
- GKAOGPIIYCISHV-UHFFFAOYSA-N neon atom Chemical compound [Ne] GKAOGPIIYCISHV-UHFFFAOYSA-N 0.000 description 1
- 238000010606 normalization Methods 0.000 description 1
- 238000012805 post-processing Methods 0.000 description 1
- 238000013138 pruning Methods 0.000 description 1
- 230000035807 sensation Effects 0.000 description 1
- 238000010561 standard procedure Methods 0.000 description 1
- 238000006467 substitution reaction Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/50—Depth or shape recovery
- G06T7/55—Depth or shape recovery from multiple images
- G06T7/579—Depth or shape recovery from multiple images from motion
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T19/00—Manipulating 3D models or images for computer graphics
- G06T19/006—Mixed reality
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T7/00—Image analysis
- G06T7/50—Depth or shape recovery
- G06T7/55—Depth or shape recovery from multiple images
- G06T7/571—Depth or shape recovery from multiple images from focus
Abstract
A handheld user device comprising: a monocular camera for capturing an image feed of a local scene; and a processor to select a key frame from a feed and, for a first image from the feed, perform stereo matching using the first image, the key frame, and a relative pose based on a pose associated with the first image and a pose associated with the key frame to generate a sparse disparity map representing disparity between the first image and the key frame. The processor is further to determine a dense depth map from the disparity map using a bilateral solver algorithm, and process a viewfinder image generated from the fed second image with occlusion rendering based on the depth map to incorporate one or more virtual objects into the viewfinder image to generate an AR viewfinder image. Further, the processor will provide the AR viewfinder image for display.
Description
Background
Computing-enabled cell phones ("smart phones"), tablet computers, and other handheld user devices are increasingly being used to provide Augmented Reality (AR) experiences, such devices are being used as viewfinders on the augmented world, where virtual content is presented in real-time on top of the device' S camera feed, these portable devices use some form of six degree of freedom (6DoF) tracking capability to support such AR capabilities, which uses only typical sensors found inside of these devices, such as color cameras and Inertial Measurement Units (IMUs), resulting in Visual Inertial Odometers (VIOs) and many developments of simultaneous positioning and mapping (S L AM).
To achieve a more realistic immersive sensation by more accurately handling occlusions, placement of virtual content, and virtual-real world interactions (such as collisions of virtual objects with physical objects), handheld device-based AR applications rely on dense depth maps generated in real-time at the device. However, given computational or sensor limitations, many handheld devices are often not feasible to provide real-time dense depth maps. Many currently available handheld devices lack stereo cameras for depth sensing or use time-of-flight (ToF) sensors or other dedicated depth sensors, and the cost and complexity of adding such additional sensor capabilities is generally not feasible due to cost, power, and form factor considerations. Furthermore, while dense and edge-preserving depth map generation techniques with monocular cameras have been developed, these conventional techniques typically require dedicated hardware or significant computing resources that are typically not available on many handheld devices, and thus, replacing dedicated depth sensors for real-time depth map generation on many widely available handheld devices is not satisfactorily implemented.
Drawings
The present disclosure may be understood, and its numerous features and advantages made apparent to those skilled in the art by referencing the accompanying drawings. The use of the same reference symbols in different drawings indicates similar or identical items.
Fig. 1 is a schematic diagram illustrating a process of providing an Augmented Reality (AR) experience of a handheld user device as a viewfinder into an monocular camera sensor configuration based on a real-time dense depth map generation process compatible with the AR scene, according to some embodiments.
Fig. 2 is a schematic diagram illustrating an example hardware configuration of the handheld user device of fig. 1, in accordance with some embodiments.
FIG. 3 is a flow diagram illustrating an example method for a motion depth pipeline implemented by a handheld user device, in accordance with some embodiments.
FIG. 4 is a schematic diagram illustrating a motion depth pipeline according to some embodiments.
FIG. 5 is a schematic diagram illustrating a comparison of results from a conventional bilateral solver and results from a planar bilateral solver, according to some embodiments.
Detailed Description
The present disclosure describes systems and methods for providing a motion depth pipeline for providing dense and low latency depth maps for handheld user devices at a sufficient rate even when such devices lack a stereoscopic camera configuration (i.e., have only a monocular camera) or have limited computational resources (e.g., have only a single Central Processing Unit (CPU) core available and lack the use of a Graphics Processor (GPU)). The described techniques are capable of operating on a variety of different device configurations with different camera sensors for which control of various parameters (such as exposure or focus) may not be available. These techniques can provide dense depth with relatively low latency and relatively low computational requirements, even under relatively poor 6DoF tracking conditions or in a texture-free environment.
As described in more detail below, these systems and techniques rely on a motion depth pipeline that includes some or all of the following aspects: efficient stereo matching of images based on polarity correction; an efficient key frame selection process; efficient stereo matching algorithms that take advantage of the advantages of, for example, the PatchMatch stereo algorithm and the HashMatch algorithm; a planar bilateral solver that improves upon conventional bilateral solvers for deep post-processing of point clouds resulting in higher quality by encouraging planar solutions, employing efficient initialization schemes resulting in faster convergence and lower computation, and employing methods for producing temporally stable results; and a post-rendering stage that provides a smooth low-latency experience.
Fig. 1 illustrates an example offering of an AR experience on a handheld user device 100 deployed in a real-world scene 102 using a motion depth pipeline as described herein. As shown, a handheld user device 100 (e.g., a smartphone, a tablet computer, a personal digital assistant, a portable video game device) employs a monocular camera sensor (not shown in fig. 1) to capture a camera feed 104 (note that "image" and "frame" are used interchangeably herein) that consists of a series of captured real-world images of a real-world scene 102. A software-implemented motion depth pipeline on the handheld user device 100 uses position/pose tracking data from an IMU (not shown in fig. 1) to determine the current position/pose (6DoF) of the handheld user device 100 relative to the real world scene 102, and calculates depth information 108 of the real world scene in real time in the form of a series of dense depth maps 110 from the 6DoF data and at least some of the captured real world images 106 of the color camera feed 104.
The AR software application executing on the handheld user device 100 provides one or more virtual objects 112 to be incorporated into the captured real-world image and information specifying the intended orientation and placement of the one or more virtual objects 112 relative to the coordinate system of the real-world scene or relative to the current pose/position or current viewpoint of the handheld user device 100. These virtual objects 112 may include, for example, stationary objects or moving objects, directional arrows or other navigation aids, information overlays, fantasy overlays, and the like.
The motion depth pipeline then uses the depth information represented by the current depth map 110 to render one or more virtual objects 112 into the currently captured real world image 106, including: implementing accurate occlusion of one or more virtual objects 112 by real world objects in the currently captured real world image 106, and vice versa; implementing accurate interactions between one or more virtual objects 112, and the like. The resulting AR viewfinder image 114 is then displayed on the handheld user device 100, where the AR viewfinder image 114 presents the included AR content to the real world scene 102 from the current viewpoint of the handheld user device 100. Thus, the process presents the user with an AR experience in which the display of the handheld user device 100 appears as a viewfinder in the AR scene based on the real scene 102.
Fig. 2 illustrates a hardware implementation 200 for a handheld user device 100, the handheld user device 100 configured to support a motion depth pipeline that supports an AR experience, in accordance with some embodiments. As shown, the hardware implementation 200 includes an application processor 202 (e.g., CPU), a display controller 204, a display panel 206, a system memory 208, a sensor hub 210, an Inertial Measurement Unit (IMU)212, a monocular color (e.g., red-green-blue or RGB) camera 214, one or more other input/output devices 216 (such as a keyboard, a touch screen, speakers, etc.), and a mass storage device 218. IMU212 includes one or more movement-based sensors, such as a gyroscope 220, magnetometer 222 and accelerometer 224, Global Positioning System (GPS) sensor 226, and the like.
As a general operational overview, the monocular camera 214 operates to capture one or more sequences of real world images 106 (fig. 1) for inclusion in the camera feed 104, while the various sensors of the IMU212 capture motion-related data representing gestures, positions, and movements of the handheld user device 100 for inclusion in the gesture/position sensor feed 228 the sensor hub 210 operates to format, synchronize, and otherwise process the camera feed 104 and the gesture/position feed 228, feed the feed and provide the resulting processed sensor stream for access by the processor 202 (e.g., by direct input or via temporary storage in the memory 208, mass storage 218, or other storage component), the application processor 202, sensor 210, or other component operates to periodically determine the gesture of the handheld user device 100 relative to the real world position 102 (the gesture of the handheld user device 100 relative to the real world position 102 in the DoF coordinate system 634, at least one degree of freedom coordinate system 230 in the DoF coordinate system embodiment) using any one or a combination of these techniques including S L, VIO, etc., using the sensor data from the IMU212, visual data from the monocular camera 214, and other available data, and the gesture of the handheld user device 100 relative to the current position of the real world position 102 (the DoF coordinate system in the DoF coordinate system 634, the three degree of freedom scene system 634 embodiment).
The application processor 202 executes executable instructions stored in the memory 208 or mass storage device 218 of one or more AR-enabled software applications 232 (referred to herein as the singular "AR application 232" for ease of reference) to cause the display panel 206 to appear as a viewfinder in the AR-augmented version of the real-world scene 102 in which the user is located by displaying the captured real-world image reality of the real-world scene 102 augmented by the inclusion of embedded virtual objects and other virtual content at the display panel 206. For ease of description, references to the AR application 232 performing the following specific operations are understood to mean corresponding instructions of the AR application 232 that, when executed by the application processor 202, cause the application processor 202 and/or other components of the handheld user device 100 to perform the specific operations.
As shown, AR application 232 implements at least three stages of loop or parallel execution, including a virtual object generation stage 234, a motion depth pipeline 236, and an AR image display stage 238. In the virtual object generation stage 234, the AR application 232 generates or otherwise identifies the virtual object 112 representing AR content to incorporate the currently captured image 106 of the camera feed 104. These virtual objects 112 may be determined based on, for example, a gaming application that generates virtual content based on user interaction with the gaming application, these virtual objects 112 may be determined based on a navigation program (e.g., AR graphical navigation assistance that depicts the current direction in which the user should go to reach the intended destination), graphical content that replaces or augments real world objects, and so forth. In at least one embodiment, each virtual object 112 includes a graphical representation of a virtual asset or other object (such as a two-dimensional (2D) image, a three-dimensional (3D) model, etc.) and positioning data identifying an expected pose/position of the virtual object, which may be relative to a coordinate system of the real-world scene 102, relative to a viewpoint represented by a current pose/position 230 of the handheld user device 100, etc.
The motion depth pipeline 236 operates to incorporate AR content represented by the virtual object 112 into the captured real-world image 106 using the captured real-world image 106 of the camera feed 104, the stream of current gestures/positions 230 from the gesture/position feed 228, and the virtual object 112 provided by the virtual object generation stage 234 to generate a stream of viewfinder-represented AR images 240 representing an AR version of the real-world scene 102. As described in more detail below, the motion depth pipeline 236 uses a combination of techniques to efficiently determine a current depth map using the limited computational resources of the application processor 202 and monocular camera 214, rather than relying on stereo, ToF, or other expensive, complex, or cumbersome dedicated depth sensor configurations.
The AR image display stage 238 controls, via the display controller 204 and the display panel 206, the display of a stream of AR images 240 generated by the motion depth pipeline 236 to render an AR viewfinder representation of an AR augmented version of the real world scene 102 from the viewpoint of the current pose/position 230 of the handheld user device 100. This may include, for example, applying a homography transformation or other warping process to compensate for any motion or change in pose/position since the current AR image 240 was generated, adjustment or improvement of various image parameters (including focus, contrast, brightness, etc.).
Fig. 3 shows a flow diagram 300, the flow diagram 300 showing an example overview method flow of the motion depth pipeline 236 of fig. 2, and fig. 4 provides a graphical representation 400 of the motion depth pipeline 236 relative to the process of the flow diagram 300. The various processes briefly described with reference to flowchart 300 are then described separately in more detail below.
The motion depth pipeline 236 implements two decoupled paths, a depth mesh generation path 302 and an on-demand post-rendering path 304 in the case of a hand of a user of the handheld user device 100, the depth mesh generation path 302 begins at block 306 with capturing real world camera images 106 (referred to herein as "scene images 106") via a monocular camera 214 and at block 308 tracks the current pose/position using, for example, an off-the-shelf VIO platform of ARCore available from Google LL C or using any of the various pose/position tracking techniques of the IMU and/or monocular camera the next step toward computing the depth map 110 (fig. 1) at block 310 is selecting a keyframe 402 (fig. 4) from a keyframe pool 404 (fig. 4) suitable for performing stereo matching that is composed of the most recent subset of past scene images 106 at block 312, next at block 312, determining that the relative between the keyframe pool 404 (fig. 4) and the relative pose of the keyframe 106 and correcting the scene images 408 is performed using a relative polar correction process to reduce the relative pose/position of the corresponding DoF correction in embodiments, reducing the relative polar correction of the frames 408 at block 408 and reducing the relative polar correction of the current keyframe 408 at block 408.
At block 316, a correspondence between the pair of images 406, 408 is computed using a fast Conditional Random Field (CRF) solver, and at block 318, the erroneous estimates are discarded via an efficient machine learning-based solution, resulting in the generation of one or more disparity maps that are relatively free of outliers. At block 320, a sparse depth map 410 (fig. 4) is estimated by triangulation using one or more disparity maps. At block 322, the sparse depth map 410 is then fed into a planar bilateral solver 412 (fig. 4), which planar bilateral solver 412 generates a bilateral depth grid 414 (fig. 4) (also referred to herein as a "bilateral grid 414") rather than the enhanced fast bilateral solver of the depth map.
With respect to the on-demand post-rendering path 304, in response to receiving the latest viewfinder image 415 (which represents a color latest scene image sized and otherwise formatted for display on the display panel 206), at block 324, the bilateral mesh 414 is converted to the dense, temporally and spatially consistent depth map 110 (fig. 1) on-demand by employing a slicing process 416 on the bilateral mesh 414 with the latest viewfinder image 415 (as opposed to the scene image 106 used to populate the bilateral mesh 414), which ensures that edges of the generated depth map 110 are aligned with the RGB viewfinder image 415 to be displayed on the display panel 206. In at least one embodiment, the generation of the bilateral mesh 414 of the path 302 and the slicing of the block 324 of the path 304 are decoupled and run on separate threads. Thus, the depth map 110 may be generated at a relatively higher frame rate with a relatively lower, effectively making it independent of the run-time of the CRF inference of blocks 312-320, allowing the techniques described herein to be deployed on a wide variety of handheld user devices with different levels of computing resources, without sacrificing quality or effective frame rate. At block 326, the AR application 232 utilizes this real-time depth estimation represented by the generated depth map 110 to perform occlusion rendering with one or more virtual objects 112 intended to be included in the current viewfinder image generated from the latest scene image 106 to generate a current AR viewfinder image 240, which current AR viewfinder image 240 is provided for display in the display panel 206 as AR viewfinder image 418 at block 328.
Key frame selection
As described above, the method for depth estimation in the motion depth pipeline 236 is based on stereo matching between the latest image and past keyframes. In one embodiment, the keyframe selection is based on one or more different factors, each of which contributes to the potential match quality of the candidate keyframes. For example, higher depth accuracy is obtained by raising the stereo baseline between the selected keyframe and the current position, but such frames also fall back further in time, which may lead to temporal inconsistencies. Conventional methods for motion stereo typically rely on fixed time-delayed keyframes. This approach is robust when under constant movement, such as a vehicle, but the use of a handheld device often results in a dispersed, uneven motion. Other methods rely on feature or geometric tracking rather than matching correspondences to particular frames, but these algorithms often result in sparse depths or cannot run at sufficient frame rates.
In one embodiment, the key frame selection process of the motion depth pipeline 236 includes: a soft cost function is defined to select the best keyframe of the latest target image using one or more metrics. A fixed capacity pool of potential keyframes is maintained, wherein if 6-DoF visual inertial tracking succeeds, each newly captured image is added to the pool, replacing outdated images. In one embodiment, the cost metric for selecting the best keyframe from the pool is:
-bi，j: 3D Baseline distance between two frames i and j (which should generally be larger)
-ti，j: the distance in time when frames i and j are captured (which should generally be minimized)
-ai，j: in the range [0,1 ] of image regions of frames i and j]Partial overlap in which the partial overlap is calculated based on its cone of view (which should generally be maximized)
-ei，j: measurement error of the pose tracking statistics for two frames i and j (which should be smaller in general)
Selecting a key frame K for pairing with the latest reference image r using a multidimensional cost function yields the minimum total cost from the key frame pool K:
wherein the weight ω is selected for the baselinebRelative to a nominal expected baseline for the target scene depth. Selecting candidate frames based on a known baseline is a common technique for motion stereo, but for handheld devices this metric may also be weighted for other considerations. By a scalar ωtTo the time difference tr,kWeighting is performed to select frames that are close together. Other frames are separated in time and matching is more difficult for scenes with animated or dynamic content. Usually by using ωaCost term a for region overlapr,kThe re-weighting is performed because although a successful match may occur between frames having only partial overlap, the larger overlap reduces the need for depth values to be extrapolated to other parts of the target image. If the 6-DoF motion tracking process produces frames that are measured to have poor confidence, then in one embodiment,the frame is never added to the key frame pool. However, even candidate key-frames with high confidence may have some relative error with the latest reference frame. Using omegaeTo weight the relative error cost relative to other costs. With the keyframes thus identified, stereo correction is next performed, as described above.
Stereo correction
Given two cameras and their respective poses, a base matrix can be computed that controls how pixels corresponding to the same 3D point are related. For the projection from the 3D point X and the two corresponding pixels X and X ' of the fundamental matrix F one can observe that X ' lies on the line l ' ═ Fx. The correspondence search is then constrained to a one-dimensional problem. Once the correspondences have been estimated, for example using the matching algorithm described below, the depth of any given pixel can be estimated by triangulation.
Performing this one-dimensional (1D) search along horizontal lines in a stereo corrected image is more efficient due to the linear cache prefetch behavior of modern CPUs. This image has the property that all epipolar (epipolar) lines are parallel to the horizontal axis of the image. A standard dual camera setup with a fixed baseline has the following advantages: at each frame, the captured image is already close to being stereo corrected. In most cases, due to mechanical inaccuracies, these images still need to be corrected stereoscopically in software. For these settings, planar correction is often used due to the availability of common implementations at the production level.
However, when dealing with a single camera that is free to move (as if the handheld user device 100 employed the monocular camera 214), the epipolar may be anywhere on the image plane. In particular, when the user moves forward with his camera, the epipolar is inside the image, which often results in failure of conventional techniques such as planar correction. In contrast, the polarity correction techniques described below allow the user to move freely with the handheld user device 100.
When given a pair of images and their relative pose, some conventional correction algorithms transform the images such that their epipolar lines are parallel and the corresponding epipolar lines have a phaseThe same vertical coordinate. For the computational reasons described earlier, it is generally desirable to: for point (x, y) in the left corrected image, the correspondence is located at (x ', y) in the right corrected image, where x'<x, and Tmin<x-x′<TmaxWherein, TminIs a relatively small constant, typically 0 or 1, and TmaxIs the maximum parallax such as, for example, 40. Such conventional algorithms do not provide for obtaining a corrected image with corresponding pixels located in a fixed disparity range. Instead, a method is described herein that constrains the solution to a known disparity interval that can be effectively utilized in estimating the correspondence.
Some camera configurations may result in corrected images whose modulus disparities are flipped versions of each other. These configurations break the requirement of x' < x. These cases are detected when the dot product between a ray passing through an optical center and its corresponding image center in world coordinates and the vector linking the two optical centers have opposite signs. In this case, one of the images should be flipped in general.
As mentioned earlier, efficient stereo matching assumes that all correspondences are within a predefined range of disparity values Tmin,Tmax]Therein, Tmin>0. This assumption may be violated with a negative disparity or a disparity exceeding the maximum disparity assumed by the stereo matching algorithm. Thus, in at least one embodiment, a shift is applied to bring the disparity into the valid range. First, given two frames expected to be processed by the process and a predefined range of depth values [ D ]min，Dmax]Relative transformation between them, estimation
When operating on a texture scene, well-known standard techniques (like PatchMatch stereo or HashMatch) have very similar sub-pixel accuracy around 0.2 pixels. Thus, to maximize depth accuracy, in one embodiment, the size of the corrected image is adjusted to more fully utilize [ T ] while keeping the total number of pixels unchangedmin；Tmax]。
The least accuracy in the relative transformation between the two images leads to an overall correction problem. In fact, systems like the ARCore suffer from only minor gesture inaccuracies. When the antipodal "away" image, as opposed to 0 or 1, is passed through the image at, for example, [5,10]Is provided with T in betweenminTo enable compensation for inaccuracies in some small gesture. When the epipolar(s) are located within the image, a disk of pixels (e.g., 20 pixels in radius) located around the epipolar can be preemptively invalidated. At this stage, a corrected image pair is generated regardless of the trajectory followed by the handheld user device 100. The next section describes performing stereo matching on these corrected images.
Stereo matching
The likelihood of a solution is assumed to be well captured by the conditional probability from the index family:
there is a general tendency for the following forms of factorization:
this factorization constitutes a pair of CRFs. Here, Y: is given as { y ═ y1...ynIs equal to the pixel x1...xnThe associated set of potential variables. Each yi∈ Y may take on values in L, which L is a subset of real numbers andcorresponding to the parallax. N is a radical ofiIs a group of pixels adjacent to pixel i. While NP is generally difficult to solve, this decomposition has been widely used for many computer vision tasks. Term psiuCommonly referred to as unary potentials, and in the case of depth estimation via stereo matching, the likelihood of two pixels corresponding is measured. Function psipCommonly referred to as paired potentials, and act as a regularizer that facilitates piecewise smooth solutions. The component that makes the minimization of the cost high is the evaluation psiuAnd the number of steps required for the selected optimizer to converge to a good solution. For the following, unary potentials with lower computational requirements are used, wherein truncated linear pairs of potentials serve as pairs of potentials. A cost function is optimized that uses a mix of PatchMatch and HashMatch that is particularly efficient on CPU architectures, and is described below.
The basic method of CRF inference using PatchMatch exploits the local smoothness of the mark space to propagate good marks to neighboring pixels. Different propagation/update strategies have been explored in the literature, some of which are designed for GPU architectures. The following describes a propagation strategy tailored to modern CPU instruction sets that rely on Single Instruction Multiple Data (SIMD) instructions to achieve parallelism within registers and improved throughput. Achieving high performance on such architectures (e.g., ARM NEON for mobile devices) typically requires efficient utilization of these vector registers. In particular, loading data from memory benefits from using coherent loading-i.e. for vector registers having n lanes of width b bits, SIMD Instruction Set Architectures (ISAs) typically provide instructions that load n x b consecutive bits, filling all lanes with one instruction. Corresponding is a scatter load, where each lane of the vector register is inserted into the vector register one at a time. Such a scattered loading is required when the memory to be loaded is not contiguous. This behavior presents challenges to stereo algorithms, which typically sample the dispersion shift per pixel when exploring the solution space. To complicate matters, this data parallelism is directional in nature.
For a typical image layout in memory, the vector registers of pixels represent a subset of a particular image row, which prevents a typical inference scheme (e.g., PatchMatch) from mapping well to a SIMD architecture. The horizontal propagation of information in the image prevents efficient use of the vector registers. HashMatch typically requires more iterations of information to propagate further through the image than PatchMatch, but each iteration is substantially less expensive and can be performed independently. In contrast, PatchMatch is sequential in nature. With the advantages of each approach, in one embodiment, motion depth pipeline 236 employs a hybrid variant well suited for SIMD architectures. Instead of performing multiple independent propagation traversals for each of the 8 directions, k traversals are performed in sequence, each designed to exploit the data parallelism of the underlying vector architecture. For a typical scenario, k varying in the range of 2 to 4 is sufficient. During even-numbered traversals, each pixel considers hypotheses from its three neighbors above it (i.e., the pixel at (x, y) considers hypotheses from (x-1, y-1), (x +1, y-1)) in addition to the currently stored hypotheses. During an odd number of traversals, each pixel takes into account hypotheses from the three neighbors below it, in addition to the currently stored hypotheses. The rows are processed sequentially, starting with the top of the even traversal image and the bottom of the odd traversal image. . Thus, all pixels of a given row are independent of all other pixels in the same row. As a criterion for this approach, each hypothesis is evaluated by summing the stereo matching unary cost and weighted smoothness term, the truncated linear pair potentials described above. A one-component cost estimate cannot be completely parallelized due to the different disparity values in each lane of the vector register. However, we can completely parallelize the rest of the data movement: the load of the initial disparity values, the load of the neighboring disparity values, and the smoothness cost calculation.
Unfortunately, when a scene lacks texture (e.g., white walls) or includes repeating patterns, the MAP solution for the corresponding pixel may be erroneous.
However, by decomposing the entire CRF process into small clusters that include each pixel and its immediate neighbors, one can calculate the negative log-likelihood using the following equation
This formula leads to better nullifying results than considering only a single element potential. As will be appreciated, a less likely solution to pruning removes most of the undesired pixels. Unfortunately: in the case of non-textured regions, the likelihood of the solution may be high, but not correct. Therefore, the smaller connected component is ineffective in the parallax space. The resulting depth map 110 does not have most of the unstable predictions. Depending on the computing architecture, performing this last failure step may become computationally intensive as solving the above factorized CRF equation. Thus, a single decision tree may be used to approximate the connected component invalidation step to minimize the required computational resources. The decision tree consists of split nodes and leaves. Each split node n stores a pass function parameter θnAnd scalar threshold TnAnd parameterized "weak learners". To perform an inference on the tree for pixel p, one starts at the root of the tree and evaluates:
s(p，n)＝1[f(p，θn)＞τn]
if s (p, n) evaluates to 0, then it is inferred to continue on the left child node of node n, and otherwise on the right child node. This process repeats until a leaf is reached that contains a binary probability distribution over the prediction space, which is not valid in this case.
Consistent with the convention, θ is greedily optimized by selecting f as the dot product between the values of two pixel indices located around pnAnd TnTo maximize the Information Gain (IG):
With the disparity map without outliers thus generated, the depth can be finally inferred. When using planar correction, the depth Z can simply be calculated as the disparity d, the baseline b, and the focal length f, taking into accountmotion depth pipeline 236 to handle these cases is optimal triangulation, which requires minimizing a 6 th order polynomial, which may be inefficient on mobile architectures. Thus, in one embodiment, motion depth pipeline 236 instead resorts to solving a simpler linear problem, as known in the art, that is not optimal but can be solved quickly.
Bilateral solver
The previous description provides how to obtain a depth map with relatively few false positives. However, these depth maps may be sparse (including information only in texture regions), temporally inconsistent, and may not align with the edges of the image. Thus, the following describes an effective improvement over conventional bilateral solvers to efficiently generate dense, temporally stable and edge-aligned depth maps with low latency.
The bilateral solver is defined as an optimization problem with respect to a "reference" image r (in the present case, a grayscale image from the monocular camera 214 of fig. 2), a "target" image t of noisy observations (in the present case, a noise depth map as calculated according to the above description), and a "confidence" image c (in the present case, an inverse mask of the invalid mask as described above). The solver recovers an "output" image x that is close to the target with greater confidence while being maximally smooth with respect to the edges in the reference image, by solving the following optimization problem:
the first term facilitates all pixel pairs (i, j) according to a bilateral affinity matrix
wherein, for each pixel i,
It is well known that large and dense dual random bilateral affinity matrices can be represented using a bilateral mesh with compact factorization:
wherein splat S matrix and slice STThe matrix is splattered and sliced into a bilateral grid, respectively, and B is [1,2,1 ] along three dimensions of the bilateral grid]The blur and the vectors m and n cause a normalization which leads to
x＝STy
where y comprises the value of each bilateral mesh vertex and x comprises the value of each pixel. Suppose σ＊If the parameters are not small, then y will be much smaller than x. Thus, the computationally expensive pixel space optimization problem described above can be converted into a tractable bilateral space optimization problem:
A＝λ(diag(m)-diag(n)Bdiag(n))+diag(Sc)
Where y is the solution to the problem in bilateral space, and
the sparse linear system is solved using conventional methods using pre-processed heavy sphere optimization, which yields similar results to the pre-processed conjugate gradients that are often used, while being more suitable for fast implementation.
The bilateral solver as described previously is capable of producing edge-aware smooth depth maps from noisy or incomplete inputs and can be made to produce real-time results running on the mobile CPU. However, in at least one embodiment, the motion depth pipeline 236 utilizes a modified enhanced bilateral solver, referred to herein as a "planar bilateral solver" and described below. First, the solver (or indeed any linear smoothing operator) is generalized to produce an output that is smooth in the co-planar sense, rather than in the face-parallel sense. This formula results in significantly improved output over scenes containing shortened planes (walls, floors, etc.), which are common in photographic and AR environments. Second, a relatively simple and computationally inexpensive method for inducing real-time temporal consistency in the solver and a method for "hot-starting" multiple instances of the solver that increases convergence speed in the context of real-time/video processing are employed. Third, a "late" slicing is performed with a solver, where an edge-aware depth map is generated from the latest viewfinder frame using a bilateral depth grid computed from earlier stereo inputs. The result is a low latency output that is still edge-aware.
Plane bilateral solver
The bilateral solver solves for a per-pixel label that, along with the data items, minimizes the squared distance between spatially adjacent pixels having similar color or grayscale intensity. Thus, the lowest cost output of the solver (ignoring the data items) is an image comprising a single constant value, which in a stereoscopic environment means: the solver strongly favors the production of a front-parallel output depth map. This bias is problematic because real world environments typically contain smooth or flat but not front-parallel surfaces such as floors, walls, and ceilings. To illustrate, where there is a flat but not front-side parallel surface, the output of a conventional bilateral solver may be significantly incorrect, resulting in the erroneous recovery of the shortened surface to a "billboard" -like flat surface oriented perpendicular to the camera.
In other systems, this front-parallel bias in stereo or depth recovery pipelines has been addressed using specialized optimization algorithms designed to recover depth maps that minimize second-order variations rather than first-order variations. These methods can work, but are generally too computationally expensive for real-time use and do not appear to be suitable for fast bilateral space optimization. Thus, in at least one embodiment, the motion depth pipeline 236 implements a bilateral solver process that embeds a bilateral solver in a per-pixel plane fitting algorithm such that the minimum first order variation assumption of the bilateral solver results in a lower second order variation in the resulting final output of the algorithm. Local plane fitting is well known in the art, and it is generally assumed that the spatial support for each plane fitting is very limited. By using a bilateral solver as an engine for aggregating plane fit information, the resulting plane fit is global and edge-aware. Using such a global plane fit allows the recovered depth map of the pipeline to be not confused by a shortened surface, as it is possible to interpret such a plane as simply being a tilted plane.
This method, referred to as a "planar bilateral solver" as described above, fits a plane to each pixel in the image in a moving least squares context, where the interpolator in the least squares fit for each pixel is the output of the bilateral solver. A 3D linear system at each pixel is constructed implicitly, where the left and right hand sides of each linear system use a bilateral solver to compute the "scatter" matrix used in standard plane fitting. This method does not rely on any characteristic of the bilateral solver, except that the planar bilateral solver is an example of a linear filter with non-negative filter weights everywhere, and therefore, the plane fitting process is described herein in entirely general terms.
Since the mathematics for plane fitting are tedious but well understood, for the sake of brevity, the "plane filtering" method is described primarily in algorithm 1 below using pseudo code, the planar _ filter () operator taking as input some images Z to be filtered (i.e., the sparse depth map generated in the motion depth pipeline 236), some non-negative filter operator filter () which is, in this case, a bilateral solver, and some regularization parameters ∈, the algorithm filters the outer product of (x, y, Z,1) with itself, which provides the left and right hand sides of the linear system under each pixel, which in one embodiment is represented as 6-channel image a and 3-channel image b (x and y are the coordinates of each pixel, 1 is an image of all 1,
Parameter ∈ biases the output of the planar filtering to be front-side parallel by applying Tikhonov regularization to the surface normal of the recovered surface As ∈ approaches infinity, the output of the planar _ filter (-) approaches the output of the filter (-) therefore, the standard bilateral solver can be viewed as an instance of a planar bilateral solver that has been largely regularized to produce the maximum front-side parallel output-further demonstrating the deviation of the standard bilateral solver from front-side parallel output.
Since applying a planar filter involves invoking the base filter (-) nine (3x3) times, and since invoking this filter is computationally more expensive than performing element-by-element per-pixel mathematics, applying a planar bilateral solver may be approximately nine times more computationally expensive than applying a standard bilateral solver. This can be reduced somewhat by noting that in the present case filter (1) ═ 1 and therefore no computation is required, and by computing the other 8 bilateral solver instances in parallel, which allows easy vectorization. The following paragraphs demonstrate that: in practice, the planar solver essentially benefits from a time-warm-boot initialization, giving it no more runtime than a conventional bilateral solver.
Algorithm 1: planar _ filter (Z, filter (), ∈) -input: image Z to be filtered, some linear average filter (-) and some non-negative regularization parameter ∈, output: 3-channel image fitted per pixel plane (Z)x,Zy,Zz)。
Algorithm 2: solution _ image _ ldl3(A, b) -input: 6-channel image A and 3-channel image b, where the channels in A correspond to the upper triangular part of the 3 × 3 matrix, output: 3-channel x, where for each pixel i in the input linear system, x (i) ═ A (i)/b (i), where the linear solution is performed using L D L compression.
FIG. 5 illustrates an example comparison of a planar bilateral solver employed by the motion depth pipeline 236 and a conventional planar bilateral solver. The leftmost column 502 includes two input images 504, 506 (scene image 106) captured by the monocular camera 214 in an indoor environment, where the image 504 captures primarily a portion of a wall in the indoor environment, and the image 506 captures primarily a perspective view of a hallway that includes the portion of the wall represented in the image 504. In the next column 508, original point clouds 510 and 512 are generated from the stereo matching process described above from the images 504 and 506, respectively. Note that for ease of illustration, these raw point clouds 510, 512 are visualized from a different angle than the camera's angle used to capture the images 504, 506. The column 514 includes an example output 516 of a conventional bilateral solver applied to the image 504 using the original point cloud 510 and an example output 518 of a conventional bilateral solver applied to the image 506 using the original point cloud 512. As shown in output 516, a conventional bilateral solver produces reasonable results when the surfaces happen to be frontally parallel (e.g., in the portion of the wall as shown in image 504), but shows significant artifacts when the frontally parallel bias is incorrect, as shown in output 518. The last column 520 includes an example output 522 of a planar bilateral solver as described herein applied to the image 504 using the original point cloud 510 and an example output 524 of a planar bilateral solver applied to the image 506 using the original point cloud 512. As shown by output 524 compared to output 518, the planar bilateral solver solves the front-to-parallel deviation problem and produces a significantly improved output in which a large number of shortened walls and floors are more accurately recovered.
Temporal smoothness, warm start initialization and late slicing
To produce a compelling user experience, the depth map produced by the motion depth pipeline 236 is smoothed over time. This target of temporal consistency is somewhat contradictory to the target of a responsive low-latency output that is closely aligned with the edge of the current viewfinder frame. The following describes a relatively simple and efficient method for a bilateral solver and a planar bilateral solver to produce high quality, time-consistent, real-time results.
Conventional approaches for consistency using a bilateral solver include: for example, an additional "time" dimension is added to the bilateral solver, and one instance of the solver is used to solve for the per-pixel depth markers for the entire video sequence. This approach yields high quality results, but is not feasible for real-time use cases where frames are processed as they are acquired. Thus, in at least one embodiment, a causal Infinite Impulse Response (IIR) class approach to time smoothness is employed in which a single bilateral mesh of estimated depths is tracked, and in which the bilateral mesh is repeatedly updated using the output of a single image bilateral solver instance on each incoming frame. The method can run in real-time and allows the use of "late" slices to produce low-latency edge-aware outputs, which is very useful in augmented reality applications.
As explained above, the baseline single-image bilateral solver solves linear system A by implicitly constructing and solving it in bilateral space-1b, and then, using a "slice matrix ST"generating a per-pixel marker
in a temporally consistent solution, the bilateral depth grid is tracked as
where α is a parameter that controls how much temporal smoothness is promoted, blu (-) along
The temporal smoothing method achieves another acceleration in which a "warm boot" of the bilateral solver instance for each frame is performed by initializing the bilateral grid to be solved via gradient descent using the solution of the previous frame, since adjacent frames have similar image content except for extreme motion, the method greatly improves convergence and performs fewer iterations of gradient descent in all but the first image.
According to one aspect, a method for providing an Augmented Reality (AR) experience on a handheld user device comprises: capturing an image feed of a local scene via a monocular camera of a handheld user device; and selecting a key frame from the feed. The method further comprises the following steps: for a first image from an image feed, stereo matching is performed using the first image, a key frame, and a relative pose based on a pose associated with the first image and a pose associated with the key frame to generate a disparity map representing disparity between the first image and the key frame, and then a bilateral solver algorithm is used to determine a depth map from the disparity map. The method further comprises the following steps: processing a viewfinder image generated from the fed second image with occlusion rendering based on the depth map to incorporate one or more virtual objects into the viewfinder image to generate an AR viewfinder image; and displaying the AR viewfinder image on the handheld user device.
In another aspect, the method further comprises: performing polarity correction on the keyframe and the first image, and wherein performing stereo matching comprises: stereo matching is performed using the keyframes and the polarity corrected representation of the first image. Determining the depth map may include: generating a sparse depth map from the disparity map using triangulation; applying a bilateral solver algorithm to the sparse depth map to generate a depth grid; and slice the depth grid with the second image to generate a depth map. In one aspect, the bilateral solver algorithm comprises a planar bilateral solver algorithm based on fitting each pixel plane in a sparse depth map. Further, in one aspect, selecting a key frame comprises: selecting a key frame based on a minimization of a cost function that implements at least one of: a baseline distance between two candidate frames; capturing a time difference between two candidate frames; partial overlap of image regions of two candidate frames based on a viewing cone; and measurement error of the pose tracking statistics of the two candidate frames. In some aspects, the processes of selecting keyframes, performing stereo matching, determining a depth map, and processing viewfinder images are performed in real-time by a Central Processing Unit (CPU) of the handheld user device.
According to another aspect, a computer-readable medium stores a set of executable instructions configured to manipulate a processor of a handheld user device to perform the above-described method.
According to another aspect, a handheld user device comprises: a monocular camera for capturing an image feed of a local scene; a display panel; a memory for storing a software application; and a processor coupled to the memory and the monocular camera. The processor will execute instructions of the software application to: selecting a key frame from the feed; for a first image from an image feed, performing stereo matching using the first image, a key frame, and a relative pose based on a pose associated with the first image and a pose associated with the key frame to generate a disparity map representing disparity between the first image and the key frame; determining a depth map from the disparity map using a bilateral solver algorithm; processing a viewfinder image generated from the fed second image with occlusion rendering based on the depth map to incorporate one or more virtual objects into the viewfinder image to generate an AR viewfinder image; and providing the AR viewfinder image for display in the display panel.
Further, in one aspect, the processor is to execute instructions of the software application to further: the keyframes and the first images are polarity corrected, and wherein the stereo matching uses a polarity corrected representation of the keyframes and the first images. The processor will determine the depth map by: generating a sparse depth map from the disparity map using triangulation; applying a bilateral solver algorithm to the sparse depth map to generate a depth grid; and slice the depth grid with the second image to generate a depth map. In one aspect, the bilateral solver algorithm comprises a planar bilateral solver algorithm based on fitting each pixel plane in a sparse depth map. In one aspect, the processor is to select the key frame based on a minimization of a cost function that implements at least one of: a baseline distance between two candidate frames; capturing a time difference between two candidate frames; partial overlap of image regions of two candidate frames based on a viewing cone; and measurement error of the pose tracking statistics of the two candidate frames. Further, the processor may include a Central Processing Unit (CPU), wherein the CPU is to determine a depth map and process the viewfinder image in real time. The handheld user device may be, for example, one of a computing-enabled cellular telephone, a tablet computer, and a portable gaming device.
In some embodiments, certain aspects of the techniques described above are implemented by one or more processors of a processing system executing software. The software includes one or more sets of executable instructions stored on or otherwise tangibly embodied on a non-transitory computer-readable storage medium. The software may include instructions and certain data that, when executed by one or more processors, manipulate the one or more processors to perform one or more aspects of the techniques described above. The non-transitory computer-readable storage medium may include: for example, a magnetic or optical disk storage device, a solid-state storage device (such as flash memory), cache memory, Random Access Memory (RAM), or one or more other non-volatile storage devices, to name a few. Executable instructions stored on a non-transitory computer-readable storage medium may be in source code, assembly language code, object code, or other instruction format that is interpreted by one or more processors or otherwise executable by one or more processors.
Computer-readable storage media includes any storage medium or combination of storage media that is accessible by a computer system during use to provide instructions and/or data to the computer system. Such storage media may include, but are not limited to: optical media (e.g., Compact Disk (CD), digital versatile disk ((DVD), Blu-ray disk), magnetic media (e.g., floppy disk, magnetic tape, or magnetic hard drive)), volatile memory (e.g., Random Access Memory (RAM) or cache memory), non-volatile memory (e.g., Read Only Memory (ROM) or flash memory), or microelectromechanical systems (MEMS) based storage media, computer-readable storage media may be embedded in a computing system (e.g., system RAM or ROM), fixedly attached to the computing system (e.g., magnetic hard drive), removably attachable to a computing system, e.g., an optical disk or Universal Serial Bus (USB) based flash memory, or coupled to a computer system (e.g., a Network Accessible Storage (NAS)) via a wired or wireless network.
Note that not all of the activities or elements described above in the general description are required, that a portion of a particular activity or apparatus may not be required, and that one or more other activities or included elements may be performed in addition to the activities and elements described. Further, the order in which activities are listed are not necessarily the order in which the activities are performed. Moreover, these concepts have been described with reference to specific embodiments. However, one of ordinary skill in the art appreciates that various modifications and changes can be made without departing from the scope of the present disclosure as set forth in the claims below. Accordingly, the specification and figures are to be regarded in an illustrative rather than a restrictive sense, and all such modifications are intended to be included within the scope of present disclosure.
Benefits, other advantages, and solutions to problems have been described above with regard to specific embodiments. The benefits, advantages, solutions to problems, and any feature(s) that may cause any benefit, advantage, or solution to occur or become more pronounced, however, are not to be construed as a critical, required, or essential feature or element of any or all the claims. Moreover, the particular embodiments disclosed above are illustrative only, as the disclosed subject matter may be modified and practiced in different but equivalent manners apparent to those skilled in the art having the benefit of the teachings herein. No limitations are intended to the details of construction or design herein shown, other than as described in the claims below. It is therefore evident that the particular embodiments disclosed above may be altered or modified and all such variations are considered within the scope of the disclosed subject matter. Accordingly, the protection sought herein is as set forth in the claims below.
Claims (20)
1. A method for providing an augmented reality, AR, experience on a handheld user device, the method comprising:
capturing a feed of an image of a local scene via a monocular camera of the handheld user device;
selecting a key frame from the feed;
for a first image from the feed of images, performing stereo matching using the first image, the keyframe, and a relative pose based on a pose associated with the first image and a pose associated with the keyframe to generate a sparse disparity map representing disparity between the first image and the keyframe;
determining a dense depth map from the disparity map using a bilateral solver algorithm;
processing a viewfinder image generated from the fed second image with occlusion rendering based on the depth map to incorporate one or more virtual objects into the viewfinder image to generate an AR viewfinder image; and
displaying the AR viewfinder image on the handheld user device.
2. The method of claim 1, further comprising:
performing polarity correction on the key frame and the first image; and
wherein performing stereo matching comprises: performing stereo matching using the keyframe and the polarity corrected representation of the first image.
3. The method of claim 1 or 2, wherein determining the depth map comprises:
generating a sparse depth map from the disparity map using triangulation;
applying the bilateral solver algorithm to the sparse depth map to generate a bilateral depth grid; and
slice the bilateral depth mesh with the second image to generate the depth map.
4. The method of claim 3, wherein the bilateral solver algorithm comprises a planar bilateral solver algorithm based on fitting each pixel plane in the sparse depth map.
5. The method of claim 1, wherein selecting the key frame comprises:
selecting the keyframe based on a minimization of a cost function that implements at least one of: a baseline distance between two candidate frames; said capturing a time difference between two candidate frames; partial overlap of image regions of two candidate frames based on viewing cones; and measurement error of the pose tracking statistics of the two candidate frames.
6. The method of any of claims 1 to 5, wherein the processes of selecting the keyframes, performing stereo matching, determining the depth map, and processing the viewfinder images are performed in real-time by a Central Processing Unit (CPU) of the handheld user device.
7. A handheld user equipment, comprising:
a monocular camera to capture a feed of an image of a local scene;
a display panel;
a memory for storing a software application; and
a processor coupled to the memory and the monocular camera, wherein the processor is to execute instructions of the software application to:
selecting a key frame from the feed;
for a first image from the feed of images, performing stereo matching using the first image, the keyframe, and a relative pose based on a pose associated with the first image and a pose associated with the keyframe to generate a sparse disparity map representing disparity between the first image and the keyframe;
determining a density depth map from the disparity map using a bilateral solver algorithm;
processing a viewfinder image generated from the fed second image with occlusion rendering based on the depth map to incorporate one or more virtual objects into the viewfinder image to generate an AR viewfinder image; and
providing the AR viewfinder image for display in the display panel.
8. The handheld user device of claim 7, wherein the processor is to execute instructions of the software application to further:
performing polarity correction on the key frame and the first image; and
the stereo matching uses the keyframes and a polarity corrected representation of the first image.
9. The handheld user device of claim 7 or 8, wherein the processor is to determine the depth map by:
generating a sparse depth map from the disparity map using triangulation;
applying the bilateral solver algorithm to the sparse depth map to generate a bilateral depth grid; and
slice the bilateral depth mesh with the second image to generate the depth map.
10. The handheld user device of claim 9, wherein the bilateral solver algorithm comprises a planar bilateral solver algorithm based on fitting each pixel plane in the sparse depth map.
11. The handheld user device of claim 7, wherein the processor is to select the keyframe based on a minimization of a cost function that implements at least one of: a baseline distance between two candidate frames; said capturing a time difference between two candidate frames; partial overlap of image regions of two candidate frames based on a viewing cone; and measurement error of the pose tracking statistics of the two candidate frames.
12. The handheld user device of claim 7, wherein the processor is a Central Processing Unit (CPU).
13. The handheld user device of claim 7 wherein the CPU is to determine the depth map and process the viewfinder image in real time.
14. The handheld user device of claim 7, wherein the handheld user device is one of a computing-enabled cellular telephone, a tablet computer, and a portable gaming device.
15. A computer-readable medium storing a set of executable instructions configured to manipulate a processor of a handheld user device to:
selecting a keyframe from a feed of images captured by a monocular camera;
for a first image from the feed of images, performing stereo matching using the first image, the keyframe, and a relative pose based on a pose associated with the first image and a pose associated with the keyframe to generate a sparse disparity map representing disparity between the first image and the keyframe;
determining a dense depth map from the disparity map using a bilateral solver algorithm;
processing a viewfinder image generated from the fed second image with occlusion rendering based on the depth map to incorporate one or more virtual objects into the viewfinder image to generate an AR viewfinder image; and
providing the AR viewfinder image for display on a display panel of the handheld user device.
16. The computer-readable medium of claim 15, wherein the executable instructions are configured to manipulate the processor to further:
performing polarity correction on the key frame and the first image; and
the stereo matching uses the keyframes and a polarity corrected representation of the first image.
17. The computer readable medium of claim 15 or 16, wherein the executable instructions are to manipulate the processor to determine the depth map by:
generating a sparse depth map from the disparity map using triangulation;
applying the bilateral solver algorithm to the sparse depth map to generate a bilateral depth grid; and
slice the bilateral depth mesh with the second image to generate the depth map.
18. The computer-readable medium of claim 17, wherein the bilateral solver algorithm comprises a planar bilateral solver algorithm based on fitting each pixel plane in the sparse depth map.
19. The computer-readable medium of claim 15, wherein the processor is to select the keyframe based on a minimization of a cost function that implements at least one of: a baseline distance between two candidate frames; said capturing a time difference between two candidate frames; partial overlap of image regions of two candidate frames based on viewing cones; and measurement error of the pose tracking statistics of the two candidate frames.
20. The computer-readable medium of any of claims 15 to 19, wherein the processor is a Central Processing Unit (CPU).
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201862741256P | 2018-10-04 | 2018-10-04 | |
US62/741,256 | 2018-10-04 | ||
PCT/US2019/054696 WO2020072905A1 (en) | 2018-10-04 | 2019-10-04 | Depth from motion for augmented reality for handheld user devices |
Publications (2)
Publication Number | Publication Date |
---|---|
CN111465962A true CN111465962A (en) | 2020-07-28 |
CN111465962B CN111465962B (en) | 2024-03-01 |
Family
ID=68343461
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201980006353.3A Active CN111465962B (en) | 2018-10-04 | 2019-10-04 | Depth of motion for augmented reality of handheld user device |
Country Status (4)
Country | Link |
---|---|
US (1) | US11145075B2 (en) |
EP (1) | EP3698323B1 (en) |
CN (1) | CN111465962B (en) |
WO (1) | WO2020072905A1 (en) |
Cited By (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN112422967A (en) * | 2020-09-24 | 2021-02-26 | 北京金山云网络技术有限公司 | Video encoding method and device, storage medium and electronic equipment |
CN117078975A (en) * | 2023-10-10 | 2023-11-17 | 四川易利数字城市科技有限公司 | AR space-time scene pattern matching method based on evolutionary algorithm |
Families Citing this family (18)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9766074B2 (en) | 2008-03-28 | 2017-09-19 | Regents Of The University Of Minnesota | Vision-aided inertial navigation |
US20230349693A1 (en) * | 2009-09-03 | 2023-11-02 | Electronic Scripting Products, Inc. | System and method for generating input data from pose estimates of a manipulated object by using light data and relative motion data |
US10012504B2 (en) | 2014-06-19 | 2018-07-03 | Regents Of The University Of Minnesota | Efficient vision-aided inertial navigation using a rolling-shutter camera with inaccurate timestamps |
WO2018026544A1 (en) | 2016-07-22 | 2018-02-08 | Regents Of The University Of Minnesota | Square-root multi-state constraint kalman filter for vision-aided inertial navigation system |
US11940277B2 (en) | 2018-05-29 | 2024-03-26 | Regents Of The University Of Minnesota | Vision-aided inertial navigation system for ground vehicle localization |
CN110221690B (en) * | 2019-05-13 | 2022-01-04 | Oppo广东移动通信有限公司 | Gesture interaction method and device based on AR scene, storage medium and communication terminal |
US20210248824A1 (en) * | 2020-02-10 | 2021-08-12 | B/E Aerospace, Inc. | System and Method for Locking Augmented and Mixed Reality Applications to Manufacturing Hardware |
US11810313B2 (en) | 2020-02-21 | 2023-11-07 | Google Llc | Real-time stereo matching using a hierarchical iterative refinement network |
US11688073B2 (en) | 2020-04-14 | 2023-06-27 | Samsung Electronics Co., Ltd. | Method and system for depth map reconstruction |
WO2022000266A1 (en) * | 2020-06-30 | 2022-01-06 | Guangdong Oppo Mobile Telecommunications Corp., Ltd. | Method for creating depth map for stereo moving image and electronic device |
US20220051372A1 (en) * | 2020-08-12 | 2022-02-17 | Niantic, Inc. | Feature matching using features extracted from perspective corrected image |
US11615594B2 (en) | 2021-01-21 | 2023-03-28 | Samsung Electronics Co., Ltd. | Systems and methods for reconstruction of dense depth maps |
WO2022158890A1 (en) * | 2021-01-21 | 2022-07-28 | Samsung Electronics Co., Ltd. | Systems and methods for reconstruction of dense depth maps |
US20220276696A1 (en) * | 2021-02-26 | 2022-09-01 | BigBox VR, Inc. | Asynchronous multi-engine virtual reality system with reduced vestibular-ocular conflict |
US11481871B2 (en) | 2021-03-12 | 2022-10-25 | Samsung Electronics Co., Ltd. | Image-guided depth propagation for space-warping images |
CN113392879B (en) * | 2021-05-26 | 2023-02-24 | 中铁二院工程集团有限责任公司 | Multi-view matching method for aerial images |
WO2023009113A1 (en) * | 2021-07-28 | 2023-02-02 | Innopeak Technology, Inc. | Interactive guidance for mapping and relocalization |
US20230140170A1 (en) * | 2021-10-28 | 2023-05-04 | Samsung Electronics Co., Ltd. | System and method for depth and scene reconstruction for augmented reality or extended reality devices |
Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20100194856A1 (en) * | 2007-07-26 | 2010-08-05 | Koninklijke Philips Electronics N.V. | Method and apparatus for depth-related information propagation |
US20140241612A1 (en) * | 2013-02-23 | 2014-08-28 | Microsoft Corporation | Real time stereo matching |
CN105210113A (en) * | 2013-04-30 | 2015-12-30 | 高通股份有限公司 | Monocular visual SLAM with general and panorama camera movements |
CN106559659A (en) * | 2015-09-25 | 2017-04-05 | 台达电子工业股份有限公司 | Three-dimensional image depth map generator and method |
US20170148222A1 (en) * | 2014-10-31 | 2017-05-25 | Fyusion, Inc. | Real-time mobile device capture and generation of art-styled ar/vr content |
CN107798702A (en) * | 2016-08-30 | 2018-03-13 | 成都理想境界科技有限公司 | A kind of realtime graphic stacking method and device for augmented reality |
Family Cites Families (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20120306850A1 (en) * | 2011-06-02 | 2012-12-06 | Microsoft Corporation | Distributed asynchronous localization and mapping for augmented reality |
-
2019
- 2019-10-04 US US16/767,401 patent/US11145075B2/en active Active
- 2019-10-04 CN CN201980006353.3A patent/CN111465962B/en active Active
- 2019-10-04 WO PCT/US2019/054696 patent/WO2020072905A1/en unknown
- 2019-10-04 EP EP19794308.7A patent/EP3698323B1/en active Active
Patent Citations (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20100194856A1 (en) * | 2007-07-26 | 2010-08-05 | Koninklijke Philips Electronics N.V. | Method and apparatus for depth-related information propagation |
US20140241612A1 (en) * | 2013-02-23 | 2014-08-28 | Microsoft Corporation | Real time stereo matching |
CN105210113A (en) * | 2013-04-30 | 2015-12-30 | 高通股份有限公司 | Monocular visual SLAM with general and panorama camera movements |
US20170148222A1 (en) * | 2014-10-31 | 2017-05-25 | Fyusion, Inc. | Real-time mobile device capture and generation of art-styled ar/vr content |
CN106559659A (en) * | 2015-09-25 | 2017-04-05 | 台达电子工业股份有限公司 | Three-dimensional image depth map generator and method |
CN107798702A (en) * | 2016-08-30 | 2018-03-13 | 成都理想境界科技有限公司 | A kind of realtime graphic stacking method and device for augmented reality |
Non-Patent Citations (4)
Title |
---|
OLAF K¨ AHLER ET AL.: "Very High Frame Rate Volumetric Integration of Depth Images on Mobile Devices", pages 1 - 10 * |
刘自强: "增强现实中深度一致性问题的研究", pages 138 - 521 * |
宁瑞忻等: "基于视觉的虚拟现实与增强现实融合技术", vol. 36, no. 9, pages 25 - 30 * |
李晶皎;马利;王爱侠;马帅;: "基于改进Patchmatch及切片采样粒子置信度传播的立体匹配算法", no. 05, pages 609 - 613 * |
Cited By (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN112422967A (en) * | 2020-09-24 | 2021-02-26 | 北京金山云网络技术有限公司 | Video encoding method and device, storage medium and electronic equipment |
CN112422967B (en) * | 2020-09-24 | 2024-01-19 | 北京金山云网络技术有限公司 | Video encoding method and device, storage medium and electronic equipment |
CN117078975A (en) * | 2023-10-10 | 2023-11-17 | 四川易利数字城市科技有限公司 | AR space-time scene pattern matching method based on evolutionary algorithm |
CN117078975B (en) * | 2023-10-10 | 2024-01-02 | 四川易利数字城市科技有限公司 | AR space-time scene pattern matching method based on evolutionary algorithm |
Also Published As
Publication number | Publication date |
---|---|
WO2020072905A1 (en) | 2020-04-09 |
US11145075B2 (en) | 2021-10-12 |
EP3698323A1 (en) | 2020-08-26 |
EP3698323B1 (en) | 2021-09-08 |
CN111465962B (en) | 2024-03-01 |
US20210004979A1 (en) | 2021-01-07 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN111465962B (en) | Depth of motion for augmented reality of handheld user device | |
JP7403700B2 (en) | Fully convolutional point of interest detection and description via homography fitting | |
US11632533B2 (en) | System and method for generating combined embedded multi-view interactive digital media representations | |
Luo et al. | Every pixel counts++: Joint learning of geometry and motion with 3d holistic understanding | |
Valentin et al. | Depth from motion for smartphone AR | |
JP7173772B2 (en) | Video processing method and apparatus using depth value estimation | |
AU2018292610B2 (en) | Method and system for performing simultaneous localization and mapping using convolutional image transformation | |
CN108986136B (en) | Binocular scene flow determination method and system based on semantic segmentation | |
US10368062B2 (en) | Panoramic camera systems | |
US11610331B2 (en) | Method and apparatus for generating data for estimating three-dimensional (3D) pose of object included in input image, and prediction model for estimating 3D pose of object | |
EP2992508B1 (en) | Diminished and mediated reality effects from reconstruction | |
JP7453470B2 (en) | 3D reconstruction and related interactions, measurement methods and related devices and equipment | |
CN110220493B (en) | Binocular distance measuring method and device | |
CN112561978B (en) | Training method of depth estimation network, depth estimation method of image and equipment | |
US10438405B2 (en) | Detection of planar surfaces for use in scene modeling of a captured scene | |
WO2019213392A1 (en) | System and method for generating combined embedded multi-view interactive digital media representations | |
CN111598927B (en) | Positioning reconstruction method and device | |
CN112562087A (en) | Method and apparatus for estimating pose | |
CN112085842A (en) | Depth value determination method and device, electronic equipment and storage medium | |
CN115362478A (en) | Reinforcement learning model for spatial relationships between labeled images | |
Yue et al. | High-dimensional camera shake removal with given depth map | |
JP6967150B2 (en) | Learning device, image generator, learning method, image generation method and program | |
US20230217001A1 (en) | System and method for generating combined embedded multi-view interactive digital media representations | |
US20230316574A1 (en) | Matching objects in images | |
JP2023082681A (en) | Object posture estimation device and method |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |