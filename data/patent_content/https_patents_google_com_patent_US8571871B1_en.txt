US8571871B1 - Methods and systems for adaptation of synthetic speech in an environment - Google Patents
Methods and systems for adaptation of synthetic speech in an environment Download PDFInfo
- Publication number
- US8571871B1 US8571871B1 US13/633,231 US201213633231A US8571871B1 US 8571871 B1 US8571871 B1 US 8571871B1 US 201213633231 A US201213633231 A US 201213633231A US 8571871 B1 US8571871 B1 US 8571871B1
- Authority
- US
- United States
- Prior art keywords
- speech
- environment
- parameters
- text
- speech parameters
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L13/00—Speech synthesis; Text to speech systems
- G10L13/02—Methods for producing synthetic speech; Speech synthesisers
- G10L13/033—Voice editing, e.g. manipulating the voice of the synthesiser
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L21/00—Processing of the speech or voice signal to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility
- G10L21/003—Changing voice quality, e.g. pitch or formants
Definitions
- voice interfaces are becoming more common for devices often used in “eyes-busy” and/or “hands-busy” environments, such as smart phones or devices associated with vehicles.
- devices in eyes-busy and/or hands-busy environments are asked to perform repetitive tasks, such as, but not limited to, searching the Internet, looking up addresses, and purchasing goods or services.
- An example voice interface includes a speech-to-text system (or text-to-speech (TTS) system) that converts normal language into speech (or text into speech).
- TTS text-to-speech
- Other systems are available that may render symbolic linguistic representations like phonetic transcriptions into speech to facilitate voice interfacing.
- Speech synthesis is artificial production of human speech.
- a computer system used for this purpose is called a speech synthesizer, and can be implemented in software or hardware.
- the present application discloses systems and methods for adaptation of synthetic speech in an environment.
- a method may comprise determining one or more characteristics of an environment of a device.
- the device may include a text-to-speech module.
- the method also may comprise determining, based on the one or more characteristics of the environment, one or more speech parameters that characterize a voice output of the text-to-speech module.
- the method further may comprise processing, by the text-to-speech module, a text to obtain the voice output corresponding to the text based on the one or more speech parameters to account for the one or more characteristics of the environment.
- a system may comprise a device including a text-to-speech module.
- the system also may comprise a processor coupled to the device, and the processor is configured to determine one or more characteristics of an environment of the device.
- the processor also may be configured to determine, based on the one or more characteristics of the environment, one or more speech parameters that characterize a voice output of the text-to-speech module.
- the processor further may be configured to process a text to obtain the voice output corresponding to the text based on the one or more speech parameters to account for the one or more characteristics of the environment.
- a computer readable medium having stored thereon instructions that, when executed by a computing device, cause the computing device to perform functions.
- the functions may comprise determining one or more characteristics of an environment.
- the functions also may comprise determining, based on the one or more characteristics of the environment, one or more speech parameters that characterize a voice output of a text-to-speech module coupled to the computing device.
- the functions further may comprise processing, by the text-to-speech module, a text to obtain the voice output corresponding to the text based on the one or more speech parameters to account for the one or more characteristics of the environment.
- FIG. 1A illustrates an overview of an example general unit-selection technique, in accordance with an embodiment.
- FIG. 1B illustrates an overview of an example clustering-based unit-selection technique, in accordance with an embodiment.
- FIG. 2 illustrates block diagram of an example HMM-based speech synthesis system, in accordance with an embodiment.
- FIG. 3 illustrates an overview of an example HMM-based speech synthesis technique, in accordance with an embodiment.
- FIG. 4 is a flowchart of an example method for adaptation of synthetic speech in an environment, in accordance with an embodiment.
- FIG. 5 illustrates an example environment space, in accordance with an embodiment.
- FIG. 6 illustrates an example system for generating a speech waveform, in accordance with an embodiment.
- FIG. 7 illustrates an example distributed computing architecture, in accordance with an example embodiment.
- FIG. 8A is a block diagram of an example computing device, in accordance with an example embodiment illustrates.
- FIG. 8B illustrates a cloud-based server system, in accordance with an example embodiment.
- FIG. 9 is a schematic illustrating a conceptual partial view of an example computer program product that includes a computer program for executing a computer process on a computing device, arranged according to at least some embodiments presented herein.
- FIG. 1A illustrates an overview of an example general unit-selection technique, in accordance with an embodiment.
- FIG. 1A illustrates use of a target cost, i.e., how well a candidate unit from a database matches a required unit, and a concatenation cost, which defines how well two selected units may be combined.
- the target cost between a candidate unit, u i , and a required unit, t i may be represented by the following Equation:
- j indexes over all features (phonetic and prosodic contexts may be used as features)
- C is the target cost
- w j is a weight associated with the j-th target cost.
- the concatenation cost can be defined as:
- k may include spectral and acoustic features.
- the target cost and the concatenation cost may then be optimized to find a string of units, u 1 n , from the database that minimizes an overall cost, C(t 1 n ,u 1 n ), as:
- u ⁇ 1 n arg ⁇ ⁇ min u 1 n ⁇ ⁇ C ⁇ ( t 1 n , u 1 n ) ⁇ Equation ⁇ ⁇ ( 3 )
- FIG. 1B illustrates an overview of an example clustering-based unit-selection technique, in accordance with an embodiment.
- FIG. 1B describes another technique that uses a clustering method that may allow the target cost to be pre-calculated. Units of the same type may be clustered into a decision tree that depicts questions about features available at the time of synthesis.
- the cost functions may be formed from a variety of heuristic or ad hoc quality measures based on features of an acoustic signal and given texts, for which the acoustic signal is to be synthesized.
- target cost and concatenation cost functions based on statistical models can be used.
- Weights may be determined for each feature, and a combination of trained and manually-tuned weights can be used. In examples, these techniques may depend on an acoustic distance measure that can be correlated with human perception.
- an optimal size (e.g., length of time) of units can be determined. The longer the unit, the larger the database may be to cover a given domain.
- short units short pre-recorded waveforms
- continuity can also be affected with more joining points.
- different-sized units i.e., from frame-sized, half-phones, diphones, and non-uniform units can be used.
- statistical parametric speech synthesis can be used to synthesize speech.
- Statistical parametric synthesis may be described as generating an average of sets of similarly sounding speech segments. This may contrast with the target of unit-selection synthesis, i.e., retaining natural unmodified speech units.
- Statistical parametric synthesis may include modeling spectral, prosody (rhythm, stress, and intonation of speech), and residual/excitation features.
- An example of statistical parametric synthesis is Hidden Markov Model (HMM)-based speech synthesis.
- parametric representations of speech including spectral and excitation parameters from a speech database can be extracted and then modeled using a set of generative models (e.g., HMMs).
- a maximum likelihood (ML) criterion can be used to estimate the model parameters as:
- ⁇ ⁇ arg ⁇ ⁇ max ⁇ ⁇ ⁇ p ⁇ ( O
- ⁇ is a set of model parameters
- O is a set of training data
- W is a set of word sequences corresponding to O.
- Speech parameters o can then be generated for a given word sequence to be synthesized w, from the set of estimated models ⁇ circumflex over ( ⁇ ) ⁇ , so as to maximize output probabilities as:
- a speech waveform can be constructed from the parametric representation of speech.
- FIG. 2 illustrates block diagram of an example HMM-based speech synthesis system, in accordance with an embodiment.
- the system in FIG. 2 includes a training portion and a synthesis portion.
- the training portion may be configured to perform the maximum likelihood estimation of Equation (5).
- spectrum e.g., mel-cepstral coefficients and dynamic features of the spectrum
- excitation e.g., log F0 and dynamic features of the excitation
- linguistic and prosodic contexts may be taken into account in addition to phonetic ones.
- the contexts used in an HMM-based synthesis system may include phoneme (current phoneme, preceding and succeeding two phonemes, and position of current phoneme within current syllable); syllable (number of phonemes within preceding, current, and succeeding syllables, stress and accent of preceding, current, and succeeding syllables, position of current syllable within current word and phrase, number of preceding and succeeding stressed syllables within current phrase, number of preceding and succeeding accented syllables within current phrase, number of syllables from previous stressed syllable, number of syllables to next stressed syllable, number of syllables from previous accented syllable, number of syllables to next accented syllable, and vowel identity within current syllable); word (guess at part of speech of preceding, current, and succeeding words, number of syllable
- single multi-variate Gaussian distributions can be used as stream-output distributions for the model.
- the HMM-based speech synthesis system may be configured to use multi-space probability distributions as stream output distributions.
- Each HMM may have a state-duration distribution to model temporal structure of speech.
- Choices for state-duration distributions may include Gaussian distribution and Gamma distribution. These distributions may be estimated from statistical variables obtained at a last iteration of a forward-backward algorithm, for example.
- Each of spectrum, excitation, and duration parameters may be clustered individually by phonetic decision trees because each of these parameters has respective context-dependency. As a result, the system may be configured to model the spectrum, excitation, and duration in a unified framework.
- Contexts can be generated for a corpus of input speech, and linguistic features or contexts of the HMM can be clustered, or grouped together to form the decision trees. Clustering can simplify the decision trees by finding distinctions that readily group the input speech. In some examples, a “tied” or “clustered” decision tree can be generated that does not distinguish all features that make up full contexts for all phonemes; rather, a clustered decision tree may stop when a subset of features in the contexts can be identified.
- a group of decision trees can form a “trained acoustic model” or “speaker-independent acoustic model” that uses likelihoods of training data to cluster the input speech and split the training data based on features in the contexts of the input speech.
- Each stream of information can have a separately trained decision tree in the trained acoustic model.
- the synthesis portion may be configured to perform the maximization in Equation (6).
- Speech synthesis may be considered as an inverse operation of speech recognition.
- a given word sequence may be converted to a context dependent label sequence, and then an utterance HMM may be constructed by concatenating context-dependent HMMs according to the label sequence.
- a speech parameter generation algorithm generates sequences of spectral and excitation parameters from the utterance HMM.
- a speech waveform may be synthesized from the generated spectral and excitation parameters via excitation generation and a speech synthesis filter, e.g., mel log spectrum approximation (MLSA) filter.
- a speech synthesis filter e.g., mel log spectrum approximation (MLSA) filter.
- FIG. 3 illustrates an overview of an example HMM-based speech synthesis technique, in accordance with an embodiment.
- Equation (6) can be approximated as:
- o ⁇ arg ⁇ ⁇ max o ⁇ ⁇ p ⁇ ( o
- w , ⁇ ⁇ ) ⁇ Equation ⁇ ⁇ ( 8 ) ⁇ arg ⁇ ⁇ max o ⁇ ⁇ ⁇ q ⁇ p ⁇ ( o , q
- w , ⁇ ⁇ ) ⁇ Equation ⁇ ⁇ ( 10 ) ⁇ arg ⁇ ⁇ max o ⁇ ⁇ max q ⁇ ⁇ P ⁇ ( q
- T is a state-output vector sequence to be generated
- T is the total number of frames in o.
- the state sequence ⁇ circumflex over (q) ⁇ is determined so as to maximize state-duration probability of the state sequence as:
- ô may be piece-wise stationary where a time segment corresponding to each state may adopt the mean vector of the state.
- speech parameters vary smoothly in real speech.
- the speech parameter generation algorithm may introduce relationships between static and dynamic features of speech as constraints for the maximization problem.
- o f and c t can be arranged in a matrix form as:
- the state-output vectors thus may be considered as a linear transform of the static features. Therefore, maximizing N(o; ⁇ ⁇ circumflex over (q) ⁇ , ⁇ ⁇ circumflex over (q) ⁇ ) with respect to o may be equivalent to that with respect to c:
- the ML method is used as an example illustration only. Methods other than ML can be used; for example, a recursive a-posteriori-based traversal algorithm, such as the Constrained Structural Maximum a Posteriori Linear Regression (CSMAPLR) algorithm, which uses piece-wise linear regression functions to estimate paths to leaf nodes of a decision tree, can be used. Other examples are possible as well.
- CSMAPLR Constrained Structural Maximum a Posteriori Linear Regression
- Statistical parametric synthesis can be used to account for changing voice characteristics, speaking styles, emotions, and characteristics of an environment.
- the term ‘environment’ may refer to an auditory or acoustic environment where a device resides, and may represent a combination of sounds originating from several sources, propagating, reflecting upon objects and affecting an audio capture device (e.g., a microphone) or a listener's ear.
- a speech synthesis system may be configured to mimic Lombard effect or Lombard reflex, which includes an involuntary tendency of a speaker to increase vocal effort when speaking in generally loud or altered noise to enhance intelligibility of voice of the speaker.
- the increase in vocal effort may include an increase in loudness as well as other changes in acoustic features such as pitch and rate, duration of sound syllables, spectral tilt, formant positions, etc. These adjustments or changes may result in an increase in auditory signal-to-noise ratio of words spoken by the speaker (or speech output by the speech system), and thus make the words intelligible.
- a device that includes a text-to-speech (TTS) module may be configured to determine characteristics of an environment of the device (e.g., characteristics of background sound in the environment). The device also may be configured to determine, based on the one or more characteristics of the environment, speech parameters of an HMM-based speech model that characterizes a voice output of the text-to-speech module. Further, the device may be configured to process a text to obtain the voice output corresponding to the text based on the speech parameters to account for the characteristics of the environment (e.g., mimic Lombard reflex).
- TTS text-to-speech
- FIG. 4 illustrates a flowchart of an example method 400 for adaptation of synthetic speech in an environment, in accordance with an embodiment.
- the method 400 may include one or more operations, functions, or actions as illustrated by one or more of blocks 402 - 406 . Although the blocks are illustrated in a sequential order, these blocks may in some instances be performed in parallel, and/or in a different order than those described herein. Also, the various blocks may be combined into fewer blocks, divided into additional blocks, and/or removed based upon the desired implementation
- each block may represent a module, a segment, or a portion of program code, which includes one or more instructions executable by a processor for implementing specific logical functions or steps in the process.
- the program code may be stored on any type of computer readable medium or memory, for example, such as a storage device including a disk or hard drive.
- the computer readable medium may include a non-transitory computer readable medium or memory, for example, such as computer-readable media that stores data for short periods of time like register memory, processor cache and Random Access Memory (RAM).
- the computer readable medium may also include non-transitory media or memory, such as secondary or persistent long term storage, like read only memory (ROM), optical or magnetic disks, compact-disc read only memory (CD-ROM), for example.
- the computer readable media may also be any other volatile or non-volatile storage systems.
- the computer readable medium may be considered a computer readable storage medium, a tangible storage device, or other article of manufacture, for example.
- each block in FIG. 4 may represent circuitry that is wired to perform the specific logical functions in the process.
- the method 400 includes determining one or more characteristics of an environment of a device, and the device may include a text-to-speech module.
- the device can be, for example, a mobile telephone, personal digital assistant (PDA), laptop, notebook, or netbook computer, tablet computing device, a wearable computing device, etc.
- the device may be configured to include a text-to-speech (TTS) module to convert text into speech to facilitate interaction of a user with the device, for example.
- TTS text-to-speech
- a user of a mobile phone may be driving, and the mobile phone may be configured to cause the TTS module to speak out text displayed on the mobile phone to the user in order to allow interaction with the user without the user being distracted by looking at the displayed text.
- the user may have limited sight
- the mobile phone may be configured to convert text related to functionality of various software applications of the mobile phone to voice to facilitate interaction of the user with the mobile phone.
- the TTS module may include and be configured to execute software (e.g., speech synthesis algorithm) as well as include hardware components (e.g., memory configured to store instructions, a speaker, etc.).
- the TTS module may include two portions: a front-end portion and a back-end portion.
- the front-end portion may have two tasks; first, the front end portion may be configured to convert raw text containing symbols like numbers and abbreviations into equivalent written-out words. This process may be referred to as text normalization, pre-processing, or tokenization.
- the front-end portion also may be configured to assign phonetic transcriptions to each word, and divide and mark the text into prosodic units, such as phrases, clauses, and sentences.
- the process of assigning phonetic transcriptions to words may be referred to as text-to-phoneme or grapheme-to-phoneme conversion.
- Phonetic transcriptions and prosody information together may make up a symbolic linguistic representation that is output by the front-end portion.
- the back-end portion referred to as synthesizer, may be configured to convert the symbolic linguistic representation into sound.
- this part may include computation of a target prosody (pitch contour, phoneme durations), which may then be imposed on output speech.
- the device may include a processor in communication with the device and the TTS module.
- the processor may be included in the device; however, in another example, the device may be coupled to a remote server (e.g., cloud-based server) that is in wired/wireless communication with the device and processing functions may be performed by the server.
- functionality of the TTS module may be performed in the device or remotely at a server or may be divided between both the device and a remote server.
- the device may be configured to determine characteristics of an environment of the device.
- the device may include sensors (cameras, microphones, etc.) that can receive information about the environment of the device.
- the device may be configured to determine numerical parameters, based on the information received from the sensors, to determine characteristics of the environment.
- the device may include an audio capture unit (e.g., the device may be a mobile phone including a microphone) that may be configured to capture an audio signal from the environment.
- the audio signal may be indicative of characteristics of a background sound in the environment of the device, for example.
- the processor may be configured to analyze the audio signal, and determine signal parameters to infer noise level in the environment. For instance, the processor may be configured to determine an absolute measurement of noise (e.g., in Decibels) in the environment. In another example, the processor may be configured to determine a signal-to-noise ratio (SNR) between noise in the environment and a synthesized TTS signal.
- SNR signal-to-noise ratio
- the processor may be configured to determine a type of noise in the environment (e.g., car noise, office noise, another speaker talking, singing, etc.) based on the audio signal.
- determining noise type may comprise two stages: a training stage and an estimation stage.
- a training computing device may be configured to have access to data sets corresponding to different types of noise (white noise, bubble noise, car noise, airplane noise, party nose, crowd cheers, etc.)
- the training computing device may be configured to extract a spectral envelop features (e.g., AutoRegressive Coefficients, Line-Spectrum-Pairs, Line-Spectrum-Frequencies, cepstrum coefficient, etc.) for each data set; and may be configured to train a Gaussian Mixture Model (GMM) using the features of each data set.
- GMM Gaussian Mixture Model
- the processor of the device present in a given environment may be configured to extract respective spectral envelop features from the audio signal captured from the given environment; and may be configured to utilize a maximum likelihood classifier to determine which GMM represents the respective spectral envelop features extracted from the audio signal, and thus determine the type of noise in the given environment.
- a classifier such as GMM, a support vector machine (SVM), a neural network, etc.
- GMM GMM
- SVM support vector machine
- a neural network etc.
- the method 400 includes determining, based on the one or more characteristics of the environment, one or more speech parameters that characterize a voice output of the text-to-speech module.
- the processor may be configured to determine the speech parameters for a statistical parametric model (e.g., HMM-based model) that characterizes a synthesized speech signal output of the TTS module in order to account for or adapt to the characteristics (e.g., background noise) of the environment.
- a statistical parametric model e.g., HMM-based model
- the processor using the speech parameters determined based on the characteristics of the environment, can cause speech output of the TTS module intelligible in the environment of the device.
- speech parameters for a given environment may be predetermined and stored in a memory coupled to the device.
- the processor may be configured to cause the TTS module to transform the stored speech parameters into modified speech parameters adapted to a different environment (e.g., a current environment of the device that is different from the given environment).
- the device may be configured to store or have access to a first set of speech parameters that have been determined, e.g., using an HMM-based statistical synthesis model, for a substantially background sound-free environment.
- the device may be configured to store or have access to a second set of speech parameters determined for a given environment with a predetermined background sound condition, i.e., a voice output or speech signal generated by the TTS module using the second set of speech parameters may be intelligible in the predetermined background sound condition (i.e., mimics Lombard effect in the predetermined background sound condition).
- the processor may be configured to use the first set of speech parameters and the second set of speech parameters to determine speech parameters adapted to another environmental condition.
- the processor may be configured to determine the speech parameters by extrapolating or interpolating between the first set of speech parameters and the second set of speech parameters. Interpolation (or extrapolation) may enable synthesizing speech that is intelligible in a current environment of the device using speech parameters that were determined for different environments with different characteristics.
- speech parameters can be determined for three noise levels: substantially noise-free, moderate noise, and extreme noise.
- Averaged A-weighted sound pressure levels for example, can be selected to be about 65 dB for moderate and about 72 dB for extreme noise, and average SNRs can be selected to be about ⁇ 1 dB and about ⁇ 8 dB for moderate and extreme noises, respectively. These numbers are examples for illustration only. Other examples are possible.
- Speech samples can be recorded in these three conditions and respective HMM-based speech models or speech parameters can be generated that make a respective voice output of a TTS module intelligible in the respective noise level.
- the speech parameters can be stored in a memory coupled to the processor.
- the processor may be configured to interpolate (or extrapolate) using the stored speech parameters determined for the three noise levels to determine speech parameters for a different noise level of a current environment of the device.
- a numerical parameter such as SNR
- the numerical parameter can be used to define an interpolation weight between the stored speech parameters determined for three noise levels.
- FIG. 5 illustrates an example environment space, in accordance with an embodiment.
- the example environment space may be defined by three variables: signal-to-noise ratio (SNR), noise type (e.g., car noise, song, etc.), and sound pressure level in dB. These variables are for illustration only, and other variables can be used to define an environment.
- the noise type can be qualitative or can be characterized by numerical values of parameters indicative of the noise type.
- a vector ‘z’ can be determined for a given environment, and can be used for interpolation among sets of speech parameters determined for other ‘z’ vectors representing other environments, for example.
- the processor may be configured to determine a transform to convert the first set of speech parameters to the second set of speech parameters.
- the processor also may be configured to modify, based on the characteristics of a current environment of the device, the transform; and apply the modified transform to the first set of speech parameters or the second the second set of speech parameters to obtain the speech parameters for the current environment of the device.
- the processor may be configured to determine the speech parameters in real time.
- the processor may be configured to determine time-varying characteristics of an environment in real time, and also determine time varying speech parameters that adapt to the changing characteristics of the environment in real time.
- a user may be at a party and may be using a mobile phone or a wearable computing device.
- the mobile phone or wearable computing device may include a microphone configured to continuously capture audio signals indicative of background sound that may be changing overtime (e.g., gradual increase in background noise loudness, different songs being played with different sound characteristics, etc.).
- the processor may be configured to continuously update, based on the changing characteristics of the environment, the speech parameters used by the TTS to generate the voice output at the mobile phone such that the voice output may remain intelligible despite the changing background sound.
- the device may be configured to store sets of parameters determined for different environmental conditions, and the processor may be configured to select a given set of the stored sets of speech parameters based on the characteristics of the environment.
- the processor may be configured to select a given set of the stored sets of speech parameters based on the characteristics of the environment.
- Other examples are possible.
- the method 400 includes processing, by the text-to-speech module, a text to obtain the voice output corresponding to the text based on the one or more speech parameters to account for the one or more characteristics of the environment.
- a characteristic of the environment means to adjust the output of the text-to-speech module so that attributes of the output (speech) are at desired levels, such as volume, pitch, rate and duration of syllables, and so forth.
- the TTS module may be configured to convert text into speech by preprocessing the text, assigning phonetic transcriptions to each word, dividing and marking the text into prosodic units, like phrases, clauses, and sentences; and then the TTS module may be configured to convert symbolic linguistic representation of a text into sound.
- the TTS may thus be configured to generate or synthesize a speech waveform that corresponds to the text.
- FIG. 6 illustrates an example system for generating the speech waveform, in accordance with an embodiment.
- the speech waveform can be described mathematically by a discrete-time model that represents sampled speech signals, as shown in FIG. 6 .
- the TTS module may be configured to utilize the speech parameters determined to generate a transfer function H(z) that models structure of vocal tract.
- Excitation source may be chosen by a switch which may be configured to control voiced/unvoiced characteristics of speech.
- An excitation signal can be modeled as either a quasi-periodic train of pulses for voiced speech, or a random noise sequence for unvoiced sounds. Speech parameters of the speech model may change with time to produce speech signals x(n).
- the excitation e(n) may be filtered by a slowly time-varying linear system H(z) to generate speech signals x(n).
- the processor may be configured to cause the speech waveform or voice output corresponding to the text to be played through a speaker coupled to the device, for example.
- unit-selection synthesis uses large databases of recorded speech.
- each recorded utterance is segmented into some or all of the following: individual phones, diphones, half-phones, syllables, morphemes, words, phrases, and sentences. Division into segments may be done, for example, using a modified speech recognizer set to a “forced alignment” mode with manual correction afterward, using visual representations such as the waveform and a spectrogram.
- An index of the units in the speech database can then be created based on the segmentation and acoustic parameters like fundamental frequency (pitch), duration, position in the syllable, and neighboring phones.
- a desired target utterance is created by determining a chain of candidate units from the database (unit-selection) that meets certain criteria (e.g., optimization of target cost and concatenation cost).
- the processor may be configured to synthesize (by unit-selection) a voice signal using speech waveforms pre-recorded in a given environment having predetermined characteristics such as predetermined background sound characteristics (e.g., a substantially background sound-free environment).
- the processor may be configured then to modify, using the speech parameters determined at block 404 of the method 400 , the synthesized voice signal to obtain the voice output of the text that is intelligible in a current environment of the device.
- the processor may be configured to scale, based on the speech parameters, signal parameters of the synthesized voice signal by a factor (e.g., volume ⁇ 1.2, duration ⁇ 1.3, frequency ⁇ 0.8 etc).
- the voice output may differ from the synthesized voice signal in one or more of volume, duration, pitch, and spectrum to account for the characteristics of the current environment of the device.
- the processor may be configured to utilize a Pitch Synchronous Overlap Add (PSOLA) method to generate the voice output by modifying, based on the speech parameters determined for the environment of the device, the pitch and duration of the synthesized voice signal.
- PSOLA Pitch Synchronous Overlap Add
- the processor may be configured to divide the synthesized voice signal waveform in small overlapping segments. To change the pitch of the signal, the segments may be moved further apart (to decrease the pitch) or closer together (to increase the pitch). To change the duration of the signal, the segments may then be repeated multiple times (to increase the duration) or some segments are eliminated (to decrease the duration). The segments may then be combined using the overlap add technique known in the art. PSOLA can thus be used to change the prosody of the synthesized voice signal.
- the processor may be configured to determine a transform for each state of an HMM-based speech model; the transform may include an estimation of spectral and prosodic parameters that may cause the voice output to be intelligible in the environment of the device.
- the processor may be configured to synthesize a speech signal using unit-selection (concatenative method) from a database that includes waveforms pre-recorded in a background sound-free environment. This synthesized speech signal can be referred to as a modal speech signal.
- the modal speech signal may be split into a plurality of frames, each frame with a predetermined length of time (e.g., 5 ms per frame).
- the processor may be configured to identify a corresponding HMM state, and further identify a corresponding transform for the corresponding HMM state; thus, the processor may be configured to determine a sequence of transforms, one for each speech frame.
- the processor may be configured to apply a low-pass smoothing filter to the sequence of transforms over time to avoid rapid variations that may introduce artifacts in the voice output.
- the processor may be configured to apply the transforms to spectral envelopes and prosody of the modal speech signal by means of non-stationary filtering and PSOLA to synthesize a speech signal that is intelligible in the environment of the device.
- FIG. 7 illustrates an example distributed computing architecture, in accordance with an example embodiment.
- FIG. 7 shows server devices 702 and 704 configured to communicate, via network 706 , with programmable devices 708 a , 708 b , and 708 c .
- the network 706 may correspond to a LAN, a wide area network (WAN), a corporate intranet, the public Internet, or any other type of network configured to provide a communications path between networked computing devices.
- the network 706 may also correspond to a combination of one or more LANs, WANs, corporate intranets, and/or the public Internet.
- FIG. 7 shows three programmable devices, distributed application architectures may serve tens, hundreds, or thousands of programmable devices.
- the programmable devices 708 a , 708 b , and 708 c may be any sort of computing device, such as an ordinary laptop computer, desktop computer, network terminal, wireless communication device (e.g., a tablet, a cell phone or smart phone, a wearable computing device, etc.), and so on.
- the programmable devices 708 a , 708 b , and 708 c may be dedicated to the design and use of software applications.
- the programmable devices 708 a , 708 b , and 708 c may be general purpose computers that are configured to perform a number of tasks and may not be dedicated to software development tools.
- the server devices 702 and 704 can be configured to perform one or more services, as requested by programmable devices 708 a , 708 b , and/or 708 c .
- server device 702 and/or 704 can provide content to the programmable devices 708 a - 708 c .
- the content can include, but is not limited to, web pages, hypertext, scripts, binary data such as compiled software, images, audio (e.g., synthesized text-to-speech signal), and/or video.
- the content can include compressed and/or uncompressed content.
- the content can be encrypted and/or unencrypted. Other types of content are possible as well.
- server device 702 and/or 704 can provide the programmable devices 708 a - 708 c with access to software for database, search, computation, graphical, audio (e.g. speech synthesis), video, World Wide Web/Internet utilization, and/or other functions.
- programmable devices 708 a - 708 c with access to software for database, search, computation, graphical, audio (e.g. speech synthesis), video, World Wide Web/Internet utilization, and/or other functions.
- server devices are possible as well.
- the server devices 702 and/or 704 can be cloud-based devices that store program logic and/or data of cloud-based applications and/or services.
- the server devices 702 and/or 704 can be a single computing device residing in a single computing center.
- the server device 702 and/or 704 can include multiple computing devices in a single computing center, or multiple computing devices located in multiple computing centers in diverse geographic locations.
- FIG. 7 depicts each of the server devices 702 and 704 residing in different physical locations.
- data and services at the server devices 702 and/or 704 can be encoded as computer readable information stored in non-transitory, tangible computer readable media (or computer readable storage media) and accessible by programmable devices 708 a , 708 b , and 708 c , and/or other computing devices.
- data at the server device 702 and/or 704 can be stored on a single disk drive or other tangible storage media, or can be implemented on multiple disk drives or other tangible storage media located at one or more diverse geographic locations.
- FIG. 8A is a block diagram of a computing device (e.g., system) in accordance with an example embodiment.
- computing device 800 shown in FIG. 8A can be configured to perform one or more functions of the server devices 702 , 704 , network 706 , and/or one or more of the programmable devices 708 a , 708 b , and 708 c .
- the computing device 800 may include a user interface module 802 , a network communications interface module 804 , one or more processors 806 , and data storage 808 , all of which may be linked together via a system bus, network, or other connection mechanism 810 .
- the user interface module 802 can be operable to send data to and/or receive data from external user input/output devices.
- user interface module 802 can be configured to send and/or receive data to and/or from user input devices such as a keyboard, a keypad, a touch screen, a computer mouse, a track ball, a joystick, a camera, a voice recognition/synthesis module, and/or other similar devices.
- the user interface module 802 can also be configured to provide output to user display devices, such as one or more cathode ray tubes (CRT), liquid crystal displays (LCD), light emitting diodes (LEDs), displays using digital light processing (DLP) technology, printers, light bulbs, and/or other similar devices, either now known or later developed.
- the user interface module 802 can also be configured to generate audible output(s) (e.g., synthesized speech), and may include a speaker, speaker jack, audio output port, audio output device, earphones, and/or other similar devices.
- the network communications interface module 804 can include one or more wireless interfaces 812 and/or one or more wireline interfaces 814 that are configurable to communicate via a network, such as network 706 shown in FIG. 7 .
- the wireless interfaces 812 can include one or more wireless transmitters, receivers, and/or transceivers, such as a Bluetooth transceiver, a Zigbee transceiver, a Wi-Fi transceiver, a LTE transceiver, and/or other similar type of wireless transceiver configurable to communicate via a wireless network.
- the wireline interfaces 814 can include one or more wireline transmitters, receivers, and/or transceivers, such as an Ethernet transceiver, a Universal Serial Bus (USB) transceiver, or similar transceiver configurable to communicate via a twisted pair wire, a coaxial cable, a fiber-optic link, or a similar physical connection to a wireline network.
- wireline transmitters such as an Ethernet transceiver, a Universal Serial Bus (USB) transceiver, or similar transceiver configurable to communicate via a twisted pair wire, a coaxial cable, a fiber-optic link, or a similar physical connection to a wireline network.
- USB Universal Serial Bus
- the network communications interface module 804 can be configured to provide reliable, secured, and/or authenticated communications. For each communication described herein, information for ensuring reliable communications (i.e., guaranteed message delivery) can be provided, perhaps as part of a message header and/or footer (e.g., packet/message sequencing information, encapsulation header(s) and/or footer(s), size/time information, and transmission verification information such as CRC and/or parity check values). Communications can be made secure (e.g., be encoded or encrypted) and/or decrypted/decoded using one or more cryptographic protocols and/or algorithms, such as, but not limited to, DES, AES, RSA, Diffie-Hellman, and/or DSA. Other cryptographic protocols and/or algorithms can be used as well or in addition to those listed herein to secure (and then decrypt/decode) communications.
- cryptographic protocols and/or algorithms can be used as well or in addition to those listed herein to secure (and then decrypt/decode) communications.
- the processors 806 can include one or more general purpose processors and/or one or more special purpose processors (e.g., digital signal processors, application specific integrated circuits, etc.).
- the processors 806 can be configured to execute computer-readable program instructions 815 that are contained in the data storage 808 and/or other instructions as described herein (e.g., the method 400 ).
- the data storage 808 can include one or more computer-readable storage media that can be read and/or accessed by at least one of processors 806 .
- the one or more computer-readable storage media can include volatile and/or non-volatile storage components, such as optical, magnetic, organic or other memory or disc storage, which can be integrated in whole or in part with at least one of the processors 806 .
- the data storage 808 can be implemented using a single physical device (e.g., one optical, magnetic, organic or other memory or disc storage unit), while in other examples, the data storage 808 can be implemented using two or more physical devices.
- the data storage 808 can include computer-readable program instructions 815 and perhaps additional data, such as but not limited to data used by one or more processes and/or threads of a software application.
- data storage 808 can additionally include storage required to perform at least part of the herein-described methods (e.g., the method 400 ) and techniques and/or at least part of the functionality of the herein-described devices and networks.
- FIG. 8B depicts a cloud-based server system, in accordance with an example embodiment.
- functions of the server device 702 and/or 704 can be distributed among three computing clusters 816 a , 816 b , and 816 c .
- the computing cluster 816 a can include one or more computing devices 818 a , cluster storage arrays 820 a , and cluster routers 822 a connected by a local cluster network 824 a .
- the computing cluster 816 b can include one or more computing devices 818 b , cluster storage arrays 820 b , and cluster routers 822 b connected by a local cluster network 824 b .
- computing cluster 816 c can include one or more computing devices 818 c , cluster storage arrays 820 c , and cluster routers 822 c connected by a local cluster network 824 c.
- each of the computing clusters 816 a , 816 b , and 816 c can have an equal number of computing devices, an equal number of cluster storage arrays, and an equal number of cluster routers. In other examples, however, each computing cluster can have different numbers of computing devices, different numbers of cluster storage arrays, and different numbers of cluster routers. The number of computing devices, cluster storage arrays, and cluster routers in each computing cluster can depend on the computing task or tasks assigned to each computing cluster.
- the computing devices 818 a can be configured to perform various computing tasks of the server device 702 .
- the various functionalities of the server device 702 can be distributed among one or more of computing devices 818 a , 818 b , and 818 c .
- the computing devices 818 b and 818 c in the computing clusters 816 b and 816 c can be configured similarly to the computing devices 818 a in computing cluster 816 a .
- the computing devices 818 a , 818 b , and 818 c can be configured to perform different functions.
- computing tasks and stored data associated with server devices 702 and/or 704 can be distributed across computing devices 818 a , 818 b , and 818 c based at least in part on the processing requirements of the server devices 702 and/or 704 , the processing capabilities of computing devices 818 a , 818 b , and 818 c , the latency of the network links between the computing devices in each computing cluster and between the computing clusters themselves, and/or other factors that can contribute to the cost, speed, fault-tolerance, resiliency, efficiency, and/or other design goals of the overall system architecture.
- the cluster storage arrays 820 a , 820 b , and 820 c of the computing clusters 816 a , 816 b , and 816 c can be data storage arrays that include disk array controllers configured to manage read and write access to groups of hard disk drives.
- the disk array controllers alone or in conjunction with their respective computing devices, can also be configured to manage backup or redundant copies of the data stored in the cluster storage arrays to protect against disk drive or other cluster storage array failures and/or network failures that prevent one or more computing devices from accessing one or more cluster storage arrays.
- cluster storage arrays 820 a , 820 b , and 820 c can be configured to store the data of the server device 702 , while other cluster storage arrays can store data of the server device 704 . Additionally, some cluster storage arrays can be configured to store backup versions of data stored in other cluster storage arrays.
- the cluster routers 822 a , 822 b , and 822 c in computing clusters 816 a , 816 b , and 816 c can include networking equipment configured to provide internal and external communications for the computing clusters.
- the cluster routers 822 a in computing cluster 816 a can include one or more internet switching and routing devices configured to provide (i) local area network communications between the computing devices 818 a and the cluster storage arrays 820 a via the local cluster network 824 a , and (ii) wide area network communications between the computing cluster 816 a and the computing clusters 816 b and 816 c via the wide area network connection 826 a to network 706 .
- the cluster routers 822 b and 822 c can include network equipment similar to the cluster routers 822 a , and the cluster routers 822 b and 822 c can perform similar networking functions for the computing clusters 816 b and 816 c that the cluster routers 822 a perform for the computing cluster 816 a.
- the configuration of the cluster routers 822 a , 822 b , and 822 c can be based at least in part on the data communication requirements of the computing devices and cluster storage arrays, the data communications capabilities of the network equipment in the cluster routers 822 a , 822 b , and 822 c , the latency and throughput of the local networks 824 a , 824 b , 824 c , the latency, throughput, and cost of wide area network links 826 a , 826 b , and 826 c , and/or other factors that can contribute to the cost, speed, fault-tolerance, resiliency, efficiency and/or other design goals of the moderation system architecture.
- FIG. 9 is a schematic illustrating a conceptual partial view of an example computer program product that includes a computer program for executing a computer process on a computing device, arranged according to at least some embodiments presented herein.
- the example computer program product 900 is provided using a signal bearing medium 901 .
- the signal bearing medium 901 may include one or more programming instructions 902 that, when executed by one or more processors may provide functionality or portions of the functionality described above with respect to FIGS. 1-8 .
- the signal bearing medium 901 may encompass a computer-readable medium 903 , such as, but not limited to, a hard disk drive, a Compact Disc (CD), a Digital Video Disk (DVD), a digital tape, memory, etc.
- the signal bearing medium 901 may encompass a computer recordable medium 904 , such as, but not limited to, memory, read/write (R/W) CDs, R/W DVDs, etc.
- the signal bearing medium 901 may encompass a communications medium 905 , such as, but not limited to, a digital and/or an analog communication medium (e.g., a fiber optic cable, a waveguide, a wired communications link, a wireless communication link, etc.).
- a communications medium 905 such as, but not limited to, a digital and/or an analog communication medium (e.g., a fiber optic cable, a waveguide, a wired communications link, a wireless communication link, etc.).
- the signal bearing medium 901 may be conveyed by a wireless form of the communications medium 905 (e.g., a wireless communications medium conforming to the IEEE 802.11 standard or other transmission protocol).
- the one or more programming instructions 902 may be, for example, computer executable and/or logic implemented instructions.
- a computing device such as the programmable devices 708 a - c in FIG. 7 , or the computing devices 818 a - c of FIG. 8B may be configured to provide various operations, functions, or actions in response to the programming instructions 902 conveyed to programmable devices 708 a - c or the computing devices 818 a - c by one or more of the computer readable medium 903 , the computer recordable medium 904 , and/or the communications medium 905 .
Abstract
Description
where j indexes over all features (phonetic and prosodic contexts may be used as features), C is the target cost, and wj is a weight associated with the j-th target cost. The concatenation cost can be defined as:
In examples, k may include spectral and acoustic features.
where:
where λ is a set of model parameters, O is a set of training data, and W is a set of word sequences corresponding to O. Speech parameters o can then be generated for a given word sequence to be synthesized w, from the set of estimated models {circumflex over (λ)}, so as to maximize output probabilities as:
Then, a speech waveform can be constructed from the parametric representation of speech.
b j(o t)=N(o t;μj,τj) Equation (7)
where ot is the state-output vector at frame t, bj(•), μj, and Σj correspond to the j-th state-output distribution, mean vector, and covariance matrix of the distribution. Under the HMM-based speech synthesis framework, Equation (6) can be approximated as:
where o=[o1 T, . . . , oT T]T is a state-output vector sequence to be generated, q={q1, . . . , qT} is a state sequence, μq=[μq1 T, . . . , μqT T]T is the mean vector for q, Σq=diag└Σq1, . . . , ΣqT┘ is the covariance matrix for q, and T is the total number of frames in o. The state sequence {circumflex over (q)} is determined so as to maximize state-duration probability of the state sequence as:
o t =[c t T ,Δc t T]T Equation (15)
and the dynamic feature
Δc t =c t −c t-1 Equation (16)
In this example, the relationship between of and ct can be arranged in a matrix form as:
where c=[c1 T, . . . cT T]T a static feature vector sequence and W is a matrix, which may append dynamic features to c. I and 0 correspond to the identity and zero matrices.
By equating
a set of linear equations to determines can be obtained as:
W TΣ{circumflex over (q)} −1 Wĉ=W TΣ{circumflex over (q)} −1μ{circumflex over (q)} Equation (19)
x(n)=h(n)*e(n) Equation (20)
where the symbol * stands for discrete convolution.
Claims (17)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/633,231 US8571871B1 (en) | 2012-10-02 | 2012-10-02 | Methods and systems for adaptation of synthetic speech in an environment |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/633,231 US8571871B1 (en) | 2012-10-02 | 2012-10-02 | Methods and systems for adaptation of synthetic speech in an environment |
Publications (1)
Publication Number | Publication Date |
---|---|
US8571871B1 true US8571871B1 (en) | 2013-10-29 |
Family
ID=49448701
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/633,231 Active US8571871B1 (en) | 2012-10-02 | 2012-10-02 | Methods and systems for adaptation of synthetic speech in an environment |
Country Status (1)
Country | Link |
---|---|
US (1) | US8571871B1 (en) |
Cited By (27)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20130262109A1 (en) * | 2012-03-14 | 2013-10-03 | Kabushiki Kaisha Toshiba | Text to speech method and system |
US20140207447A1 (en) * | 2013-01-24 | 2014-07-24 | Huawei Device Co., Ltd. | Voice identification method and apparatus |
US20140207460A1 (en) * | 2013-01-24 | 2014-07-24 | Huawei Device Co., Ltd. | Voice identification method and apparatus |
WO2015092943A1 (en) * | 2013-12-17 | 2015-06-25 | Sony Corporation | Electronic devices and methods for compensating for environmental noise in text-to-speech applications |
US20150348535A1 (en) * | 2014-05-28 | 2015-12-03 | Interactive Intelligence, Inc. | Method for forming the excitation signal for a glottal pulse model based parametric speech synthesis system |
US20160027430A1 (en) * | 2014-05-28 | 2016-01-28 | Interactive Intelligence Group, Inc. | Method for forming the excitation signal for a glottal pulse model based parametric speech synthesis system |
US20160140953A1 (en) * | 2014-11-17 | 2016-05-19 | Samsung Electronics Co., Ltd. | Speech synthesis apparatus and control method thereof |
US20170092258A1 (en) * | 2015-09-29 | 2017-03-30 | Yandex Europe Ag | Method and system for text-to-speech synthesis |
US9886954B1 (en) * | 2016-09-30 | 2018-02-06 | Doppler Labs, Inc. | Context aware hearing optimization engine |
US20180096677A1 (en) * | 2016-10-04 | 2018-04-05 | Nuance Communications, Inc. | Speech Synthesis |
US20180158447A1 (en) * | 2016-04-01 | 2018-06-07 | Intel Corporation | Acoustic environment understanding in machine-human speech communication |
US20180232511A1 (en) * | 2016-06-07 | 2018-08-16 | Vocalzoom Systems Ltd. | System, device, and method of voice-based user authentication utilizing a challenge |
US20180268807A1 (en) * | 2017-03-14 | 2018-09-20 | Google Llc | Speech synthesis unit selection |
US10529314B2 (en) * | 2014-09-19 | 2020-01-07 | Kabushiki Kaisha Toshiba | Speech synthesizer, and speech synthesis method and computer program product utilizing multiple-acoustic feature parameters selection |
CN111260529A (en) * | 2020-01-08 | 2020-06-09 | 上海船舶研究设计院（中国船舶工业集团公司第六0四研究院） | Ship environment data determination method and device and ship |
US10832652B2 (en) * | 2016-10-17 | 2020-11-10 | Tencent Technology (Shenzhen) Company Limited | Model generating method, and speech synthesis method and apparatus |
CN112289298A (en) * | 2020-09-30 | 2021-01-29 | 北京大米科技有限公司 | Processing method and device for synthesized voice, storage medium and electronic equipment |
CN113763924A (en) * | 2021-11-08 | 2021-12-07 | 北京优幕科技有限责任公司 | Acoustic deep learning model training method, and voice generation method and device |
US11335325B2 (en) | 2019-01-22 | 2022-05-17 | Samsung Electronics Co., Ltd. | Electronic device and controlling method of electronic device |
US11361750B2 (en) * | 2017-08-22 | 2022-06-14 | Samsung Electronics Co., Ltd. | System and electronic device for generating tts model |
US20220208174A1 (en) * | 2020-12-31 | 2022-06-30 | Spotify Ab | Text-to-speech and speech recognition for noisy environments |
US11468879B2 (en) * | 2019-04-29 | 2022-10-11 | Tencent America LLC | Duration informed attention network for text-to-speech analysis |
US11468878B2 (en) * | 2019-11-01 | 2022-10-11 | Lg Electronics Inc. | Speech synthesis in noisy environment |
US20220366890A1 (en) * | 2020-09-25 | 2022-11-17 | Deepbrain Ai Inc. | Method and apparatus for text-based speech synthesis |
US20230146178A1 (en) * | 2021-11-11 | 2023-05-11 | Kickback Space Inc. | Attention based audio adjustment in virtual environments |
US20230267925A1 (en) * | 2022-02-22 | 2023-08-24 | Samsung Electronics Co., Ltd. | Electronic device for generating personalized automatic speech recognition model and method of the same |
US11922923B2 (en) | 2016-09-18 | 2024-03-05 | Vonage Business Limited | Optimal human-machine conversations using emotion-enhanced natural speech using hierarchical neural networks and reinforcement learning |
Citations (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5742928A (en) | 1994-10-28 | 1998-04-21 | Mitsubishi Denki Kabushiki Kaisha | Apparatus and method for speech recognition in the presence of unnatural speech effects |
US5864809A (en) | 1994-10-28 | 1999-01-26 | Mitsubishi Denki Kabushiki Kaisha | Modification of sub-phoneme speech spectral models for lombard speech recognition |
US20030061049A1 (en) | 2001-08-30 | 2003-03-27 | Clarity, Llc | Synthesized speech intelligibility enhancement through environment awareness |
US20030182114A1 (en) * | 2000-05-04 | 2003-09-25 | Stephane Dupont | Robust parameters for noisy speech recognition |
US20040230420A1 (en) * | 2002-12-03 | 2004-11-18 | Shubha Kadambe | Method and apparatus for fast on-line automatic speaker/environment adaptation for speech/speaker recognition in the presence of changing environments |
US20070129022A1 (en) * | 2005-12-02 | 2007-06-07 | Boillot Marc A | Method for adjusting mobile communication activity based on voicing quality |
US20070239444A1 (en) * | 2006-03-29 | 2007-10-11 | Motorola, Inc. | Voice signal perturbation for speech recognition |
US20070253578A1 (en) * | 2006-04-19 | 2007-11-01 | Verdecanna Michael T | System and method for adjusting microphone gain based on volume setting of a mobile device |
US20080189109A1 (en) * | 2007-02-05 | 2008-08-07 | Microsoft Corporation | Segmentation posterior based boundary point determination |
US20090076819A1 (en) * | 2006-03-17 | 2009-03-19 | Johan Wouters | Text to speech synthesis |
US20090192705A1 (en) * | 2006-11-02 | 2009-07-30 | Google Inc. | Adaptive and Personalized Navigation System |
US20100057465A1 (en) * | 2008-09-03 | 2010-03-04 | David Michael Kirsch | Variable text-to-speech for automotive application |
US20130013304A1 (en) * | 2011-07-05 | 2013-01-10 | Nitish Krishna Murthy | Method and Apparatus for Environmental Noise Compensation |
-
2012
- 2012-10-02 US US13/633,231 patent/US8571871B1/en active Active
Patent Citations (13)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5742928A (en) | 1994-10-28 | 1998-04-21 | Mitsubishi Denki Kabushiki Kaisha | Apparatus and method for speech recognition in the presence of unnatural speech effects |
US5864809A (en) | 1994-10-28 | 1999-01-26 | Mitsubishi Denki Kabushiki Kaisha | Modification of sub-phoneme speech spectral models for lombard speech recognition |
US20030182114A1 (en) * | 2000-05-04 | 2003-09-25 | Stephane Dupont | Robust parameters for noisy speech recognition |
US20030061049A1 (en) | 2001-08-30 | 2003-03-27 | Clarity, Llc | Synthesized speech intelligibility enhancement through environment awareness |
US20040230420A1 (en) * | 2002-12-03 | 2004-11-18 | Shubha Kadambe | Method and apparatus for fast on-line automatic speaker/environment adaptation for speech/speaker recognition in the presence of changing environments |
US20070129022A1 (en) * | 2005-12-02 | 2007-06-07 | Boillot Marc A | Method for adjusting mobile communication activity based on voicing quality |
US20090076819A1 (en) * | 2006-03-17 | 2009-03-19 | Johan Wouters | Text to speech synthesis |
US20070239444A1 (en) * | 2006-03-29 | 2007-10-11 | Motorola, Inc. | Voice signal perturbation for speech recognition |
US20070253578A1 (en) * | 2006-04-19 | 2007-11-01 | Verdecanna Michael T | System and method for adjusting microphone gain based on volume setting of a mobile device |
US20090192705A1 (en) * | 2006-11-02 | 2009-07-30 | Google Inc. | Adaptive and Personalized Navigation System |
US20080189109A1 (en) * | 2007-02-05 | 2008-08-07 | Microsoft Corporation | Segmentation posterior based boundary point determination |
US20100057465A1 (en) * | 2008-09-03 | 2010-03-04 | David Michael Kirsch | Variable text-to-speech for automotive application |
US20130013304A1 (en) * | 2011-07-05 | 2013-01-10 | Nitish Krishna Murthy | Method and Apparatus for Environmental Noise Compensation |
Non-Patent Citations (4)
Title |
---|
Gopala Krishna Anumanchipalli, "Improving Speech Synthesis for Noisy Environments," 7th ISCA Workshop on Speech Synthesis (SSW-7) Kyoto, Japan, Sep. 22-24, 2010. |
Junichi Yamagishi, "HMM-Based Expressive Seech Synthesis-Towards TTS With Arbitrary Sspeaking Styles and Emotions," Proc. of Special Workshop in Maui (SWIM), 2004. |
Takayoshi Yoshimura, "Speaker interpolation for HMM-based speech synthesis system," Proc. of EUROSPEECH, vol. 5, pp. 2523-2526, 1997. |
Tuomo Raitio, "Analysis of HMM-Based Lombard Speech Synthesis," in Proc. Interspeech, Florence, Italy, Aug. 2011. |
Cited By (43)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9454963B2 (en) * | 2012-03-14 | 2016-09-27 | Kabushiki Kaisha Toshiba | Text to speech method and system using voice characteristic dependent weighting |
US20130262109A1 (en) * | 2012-03-14 | 2013-10-03 | Kabushiki Kaisha Toshiba | Text to speech method and system |
US20140207447A1 (en) * | 2013-01-24 | 2014-07-24 | Huawei Device Co., Ltd. | Voice identification method and apparatus |
US20140207460A1 (en) * | 2013-01-24 | 2014-07-24 | Huawei Device Co., Ltd. | Voice identification method and apparatus |
US9666186B2 (en) * | 2013-01-24 | 2017-05-30 | Huawei Device Co., Ltd. | Voice identification method and apparatus |
US9607619B2 (en) * | 2013-01-24 | 2017-03-28 | Huawei Device Co., Ltd. | Voice identification method and apparatus |
US9711135B2 (en) * | 2013-12-17 | 2017-07-18 | Sony Corporation | Electronic devices and methods for compensating for environmental noise in text-to-speech applications |
US20160275936A1 (en) * | 2013-12-17 | 2016-09-22 | Sony Corporation | Electronic devices and methods for compensating for environmental noise in text-to-speech applications |
WO2015092943A1 (en) * | 2013-12-17 | 2015-06-25 | Sony Corporation | Electronic devices and methods for compensating for environmental noise in text-to-speech applications |
US20160027430A1 (en) * | 2014-05-28 | 2016-01-28 | Interactive Intelligence Group, Inc. | Method for forming the excitation signal for a glottal pulse model based parametric speech synthesis system |
US20150348535A1 (en) * | 2014-05-28 | 2015-12-03 | Interactive Intelligence, Inc. | Method for forming the excitation signal for a glottal pulse model based parametric speech synthesis system |
US10014007B2 (en) * | 2014-05-28 | 2018-07-03 | Interactive Intelligence, Inc. | Method for forming the excitation signal for a glottal pulse model based parametric speech synthesis system |
US10621969B2 (en) * | 2014-05-28 | 2020-04-14 | Genesys Telecommunications Laboratories, Inc. | Method for forming the excitation signal for a glottal pulse model based parametric speech synthesis system |
US20190172442A1 (en) * | 2014-05-28 | 2019-06-06 | Genesys Telecommunications Laboratories, Inc. | Method for forming the excitation signal for a glottal pulse model based parametric speech synthesis system |
US10255903B2 (en) * | 2014-05-28 | 2019-04-09 | Interactive Intelligence Group, Inc. | Method for forming the excitation signal for a glottal pulse model based parametric speech synthesis system |
US10529314B2 (en) * | 2014-09-19 | 2020-01-07 | Kabushiki Kaisha Toshiba | Speech synthesizer, and speech synthesis method and computer program product utilizing multiple-acoustic feature parameters selection |
US20160140953A1 (en) * | 2014-11-17 | 2016-05-19 | Samsung Electronics Co., Ltd. | Speech synthesis apparatus and control method thereof |
US20170092258A1 (en) * | 2015-09-29 | 2017-03-30 | Yandex Europe Ag | Method and system for text-to-speech synthesis |
US9916825B2 (en) * | 2015-09-29 | 2018-03-13 | Yandex Europe Ag | Method and system for text-to-speech synthesis |
US20180158447A1 (en) * | 2016-04-01 | 2018-06-07 | Intel Corporation | Acoustic environment understanding in machine-human speech communication |
US20180232511A1 (en) * | 2016-06-07 | 2018-08-16 | Vocalzoom Systems Ltd. | System, device, and method of voice-based user authentication utilizing a challenge |
US10635800B2 (en) * | 2016-06-07 | 2020-04-28 | Vocalzoom Systems Ltd. | System, device, and method of voice-based user authentication utilizing a challenge |
US11922923B2 (en) | 2016-09-18 | 2024-03-05 | Vonage Business Limited | Optimal human-machine conversations using emotion-enhanced natural speech using hierarchical neural networks and reinforcement learning |
US11501772B2 (en) | 2016-09-30 | 2022-11-15 | Dolby Laboratories Licensing Corporation | Context aware hearing optimization engine |
US9886954B1 (en) * | 2016-09-30 | 2018-02-06 | Doppler Labs, Inc. | Context aware hearing optimization engine |
US20180096677A1 (en) * | 2016-10-04 | 2018-04-05 | Nuance Communications, Inc. | Speech Synthesis |
US11069335B2 (en) * | 2016-10-04 | 2021-07-20 | Cerence Operating Company | Speech synthesis using one or more recurrent neural networks |
US10832652B2 (en) * | 2016-10-17 | 2020-11-10 | Tencent Technology (Shenzhen) Company Limited | Model generating method, and speech synthesis method and apparatus |
US20180268807A1 (en) * | 2017-03-14 | 2018-09-20 | Google Llc | Speech synthesis unit selection |
US10923103B2 (en) * | 2017-03-14 | 2021-02-16 | Google Llc | Speech synthesis unit selection |
US11393450B2 (en) | 2017-03-14 | 2022-07-19 | Google Llc | Speech synthesis unit selection |
US11361750B2 (en) * | 2017-08-22 | 2022-06-14 | Samsung Electronics Co., Ltd. | System and electronic device for generating tts model |
US11335325B2 (en) | 2019-01-22 | 2022-05-17 | Samsung Electronics Co., Ltd. | Electronic device and controlling method of electronic device |
US11468879B2 (en) * | 2019-04-29 | 2022-10-11 | Tencent America LLC | Duration informed attention network for text-to-speech analysis |
US11468878B2 (en) * | 2019-11-01 | 2022-10-11 | Lg Electronics Inc. | Speech synthesis in noisy environment |
CN111260529A (en) * | 2020-01-08 | 2020-06-09 | 上海船舶研究设计院（中国船舶工业集团公司第六0四研究院） | Ship environment data determination method and device and ship |
CN111260529B (en) * | 2020-01-08 | 2024-03-08 | 上海船舶研究设计院（中国船舶工业集团公司第六0四研究院） | Ship environment data determining method and device and ship |
US20220366890A1 (en) * | 2020-09-25 | 2022-11-17 | Deepbrain Ai Inc. | Method and apparatus for text-based speech synthesis |
CN112289298A (en) * | 2020-09-30 | 2021-01-29 | 北京大米科技有限公司 | Processing method and device for synthesized voice, storage medium and electronic equipment |
US20220208174A1 (en) * | 2020-12-31 | 2022-06-30 | Spotify Ab | Text-to-speech and speech recognition for noisy environments |
CN113763924A (en) * | 2021-11-08 | 2021-12-07 | 北京优幕科技有限责任公司 | Acoustic deep learning model training method, and voice generation method and device |
US20230146178A1 (en) * | 2021-11-11 | 2023-05-11 | Kickback Space Inc. | Attention based audio adjustment in virtual environments |
US20230267925A1 (en) * | 2022-02-22 | 2023-08-24 | Samsung Electronics Co., Ltd. | Electronic device for generating personalized automatic speech recognition model and method of the same |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8571871B1 (en) | Methods and systems for adaptation of synthetic speech in an environment | |
CN108573693B (en) | Text-to-speech system and method, and storage medium therefor | |
US11605368B2 (en) | Speech recognition using unspoken text and speech synthesis | |
Donovan | Trainable speech synthesis | |
US20230058658A1 (en) | Text-to-speech (tts) processing | |
JP4328698B2 (en) | Fragment set creation method and apparatus | |
US20220051654A1 (en) | Two-Level Speech Prosody Transfer | |
US10692484B1 (en) | Text-to-speech (TTS) processing | |
US11763797B2 (en) | Text-to-speech (TTS) processing | |
US20100057435A1 (en) | System and method for speech-to-speech translation | |
EP3376497B1 (en) | Text-to-speech synthesis using an autoencoder | |
WO2013018294A1 (en) | Speech synthesis device and speech synthesis method | |
EP4266306A1 (en) | A speech processing system and a method of processing a speech signal | |
US20160005392A1 (en) | Devices and Methods for a Universal Vocoder Synthesizer | |
JP6631883B2 (en) | Model learning device for cross-lingual speech synthesis, model learning method for cross-lingual speech synthesis, program | |
JP6330069B2 (en) | Multi-stream spectral representation for statistical parametric speech synthesis | |
Phan et al. | A study in vietnamese statistical parametric speech synthesis based on HMM | |
Deka et al. | Development of assamese text-to-speech system using deep neural network | |
WO2010104040A1 (en) | Voice synthesis apparatus based on single-model voice recognition synthesis, voice synthesis method and voice synthesis program | |
Mullah | A comparative study of different text-to-speech synthesis techniques | |
Sharma et al. | Polyglot speech synthesis: a review | |
JP2021099454A (en) | Speech synthesis device, speech synthesis program, and speech synthesis method | |
WO2014061230A1 (en) | Prosody model learning device, prosody model learning method, voice synthesis system, and prosody model learning program | |
Phung et al. | A hybrid TTS between unit selection and HMM-based TTS under limited data conditions | |
US11335321B2 (en) | Building a text-to-speech system from a small amount of speech data |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:STUTTLE, MATTHEW NICHOLAS;AGIOMYRGIANNAKIS, IOANNIS;REEL/FRAME:029060/0487Effective date: 20121001 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
FPAY | Fee payment |
Year of fee payment: 4 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044101/0299Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |