US9262406B1 - Semantic frame identification with distributed word representations - Google Patents
Semantic frame identification with distributed word representations Download PDFInfo
- Publication number
- US9262406B1 US9262406B1 US14/271,997 US201414271997A US9262406B1 US 9262406 B1 US9262406 B1 US 9262406B1 US 201414271997 A US201414271997 A US 201414271997A US 9262406 B1 US9262406 B1 US 9262406B1
- Authority
- US
- United States
- Prior art keywords
- server
- model
- word
- learned
- text
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/02—Knowledge representation; Symbolic representation
- G06N5/027—Frames
-
- G06F17/28—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/31—Indexing; Data structures therefor; Storage structures
- G06F16/316—Indexing structures
- G06F16/322—Trees
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/31—Indexing; Data structures therefor; Storage structures
- G06F16/316—Indexing structures
- G06F16/328—Management therefor
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/95—Retrieval from the web
- G06F16/958—Organisation or management of web site content, e.g. publishing, maintaining pages or automatic linking
-
- G06F17/289—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/10—Text processing
- G06F40/12—Use of codes for handling textual entities
- G06F40/151—Transformation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/58—Use of machine translation, e.g. for multi-lingual retrieval, for server-side translation for client devices or for real-time translation
-
- G06F17/274—
-
- G06F17/2755—
-
- G06F17/2785—
-
- G06F17/2872—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/253—Grammatical analysis; Style critique
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/268—Morphological analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/55—Rule-based translation
Definitions
- the present disclosure generally relates to computational linguistics and, more particularly, to semantic frame identification with distributed word representations, also known as word embeddings.
- Frame semantics is a theory of language meaning that relates linguistic utterances to word knowledge, such as event types and their participants.
- a semantic frame refers to a collection of facts or a coherent structure of related concepts that specify features (attributes, functions, interactions, etc.) that are typically associated with the specific word.
- One example semantic frame is the situation of a commercial transfer or transaction, which can involve a seller, a buyer, goods, and other related things.
- a computer-implemented technique can include receiving, at a server having one or more processors, labeled training data including a plurality of groups of words, each group of words having a predicate word, each word having generic word embeddings.
- the technique can include extracting, at the server, the plurality of groups of words in a syntactic context of their predicate words.
- the technique can include concatenating, at the server, the generic word embeddings to create a high dimensional vector space representing features for each word.
- the technique can include obtaining, at the server, a model having a learned mapping from the high dimensional vector space to a low dimensional vector space and learned embeddings for each possible semantic frame in the low dimensional vector space.
- the technique can also include outputting, by the server, the model for storage, the model being configured to identify a specific semantic frame for an input.
- obtaining the model includes training, at the server, the model based on the learned mapping and the learned embeddings.
- the labeled training data includes (i) frames for verbs and (ii) possible semantic roles for each frame, and modifier roles in the labeled training data are shared across different frames.
- the learned mapping and the learned embeddings are determined by the server using a linear transformation algorithm.
- determining the learned mapping and the learned embeddings includes using the linear transformation algorithm with a weighted approximate-rank pairwise loss learned with a stochastic gradient.
- the features include at least one of (i) direct dependents from a dependency parse tree for a specific predicate word and (ii) dependency paths from the dependency parse tree for each word.
- the technique further includes learning, at the server, a technique for filling semantic roles for an identified semantic frame using a rule-based candidate argument extraction algorithm, wherein obtaining the model is further based on the learned technique, and wherein the model is further configured to identify specific roles for a specific semantic frame.
- the technique further includes: indexing, at the server, a plurality of web pages using the model to obtain an indexed plurality of web pages, and utilizing, by the server, the indexed plurality of web pages to provide search results in response to a search query.
- the technique further includes: receiving, at the server, speech input representing a question, converting, at the server, the speech input to a text, analyzing, at the server, the text using the model, and generating and outputting, by the server, an answer to the question based on the analyzing of the text using the model.
- the technique further includes: receiving, at the server, a text to be translated from a source language to a target language, the source language being a same language as a language associated with the model, analyzing, at the server, the text using the model, and generating and outputting, by the server, a translation of the text from the source language to the target language based on the analyzing of the text using the model.
- a server having one or more processors configured to perform operations is also presented.
- the operations can include receiving labeled training data including a plurality of groups of words, each group of words having a predicate word, each word having generic word embeddings.
- the operations can include extracting the plurality of groups of words in a syntactic context of their predicate words.
- the operations can include concatenating the generic word embeddings to create a high dimensional vector space representing features for each word.
- the operations can include obtaining a model having a learned mapping from the high dimensional vector space to a low dimensional vector space and learned embeddings for each possible semantic frame in the low dimensional vector space.
- the operations can include outputting the model for storage, the model being configured to identify a specific semantic frame for an input.
- obtaining the model includes training the model based on the learned mapping and the learned embeddings.
- the labeled training data includes (i) frames for verbs and (ii) possible semantic roles for each frame, and modifier roles in the labeled training data are shared across different frames.
- the learned mapping and the learned embeddings are determined by the server using a linear transformation algorithm.
- determining the learned mapping and the learned embeddings includes using the linear transformation algorithm with a weighted approximate-rank pairwise loss learned with a stochastic gradient.
- the features include at least one of (i) direct dependents from a dependency parse tree for a specific predicate word and (ii) dependency paths from the dependency parse tree for each word.
- the operations further include learning a technique for filling semantic roles for an identified semantic frame using a rule-based candidate argument extraction algorithm, and obtaining the model is further based on the learned technique, and wherein the model is further configured to identify specific roles for a specific semantic frame.
- the operations further include: indexing a plurality of web pages using the model to obtain an indexed plurality of web pages, and utilizing the indexed plurality of web pages to provide search results in response to a search query.
- the operations further include: receiving speech input representing a question, converting the speech input to a text, analyzing the text using the model, and generating and outputting an answer to the question based on the analyzing of the text using the model.
- the operations further include: receiving a text to be translated from a source language to a target language, the source language being a same language as a language associated with the model, analyzing the text using the model, and generating and outputting a translation of the text from the source language to the target language based on the analyzing of the text using the model.
- FIGS. 1A-1D are illustrations of example semantic frames and example semantic roles according to some implementations of the present disclosure
- FIG. 2 is a diagram of a computing network including an example server according to some implementations of the present disclosure
- FIG. 3 is a functional block diagram of the example server of FIG. 1 ;
- FIG. 4 is a flow diagram of an example technique for semantic frame identification with word embeddings according to some implementations of the present disclosure.
- Parsing refers to the determination of a parse tree (a grammatical analysis) of a specific sentence. Because the grammar for natural languages can be ambiguous, the specific sentence can have multiple possible parses. More specifically, each word may have different meanings depending on its context.
- One approach to word representation is to learn a distributed word representation (also known as a “word embedding”), which is not to be confused with a distributional word representation.
- word embeddings can be dense, low-dimensional, and real-valued representations of features for words. Because word embeddings can be dense (compact), they can be used to represent a large number of clusters in a low number of dimensions.
- ⁇ For semantic frame identification with word embeddings. These techniques can leverage automatic syntactic parses and a generic set of word embeddings. Given labeled training data annotated with frame-semantic parses, a model can be obtained that projects the set of word representations for the syntactic context around a predicate to a low dimensional representation. This low dimensional representation can represent a model used for semantic frame identification. Additionally, the model can be trained/modified and used for argument identification for an identified semantic frame, and thus the techniques can perform full frame-semantic parsing. Example uses for the model include indexing of web pages for improved web query/search performance, question analysis for improved answer performance, and text analysis for improved machine translation performance.
- FIGS. 1A-1B frame-semantic parsing can include the resolution of a predicate sense into a frame and the analysis of the frame's arguments.
- a similar, related task is semantic role labeling.
- FIGS. 1A-1B illustrate example semantic frames and example semantic roles according to a first labeled (or annotated) lexical database.
- labeled lexical database and “annotated lexical database” can refer to any suitable labeled/annotated training data.
- the first labeled lexical database is the FrameNet lexical database.
- the first labeled lexical database can include information about words and phrases (represented as lemmas conjoined with a coarse part-of-speech (POS) tag) termed as lexical units, with a set of semantic frames that they could evoke. For each frame, there can be a list of associated frame elements (roles) that can also be distinguished as core or non-core. Sentences can be annotated using this universal frame inventory.
- POS coarse part-of-speech
- FIG. 1A illustrates a sentence 100 : “John bought a car.”
- a frame 104 (Commerce-Buy) can represent a frame that can be evoked by morphological variants of an example lexical unit 108 (Buy. V, where “V” indicates the word “buy” as a verb).
- the word “bought” is one of the morphological variants of this lexical unit 108 .
- Core roles 112 and 116 (Buyer and Goods, respectively) represent example core roles for this frame 104 . Specifically, core role 112 can correspond to “John” and core role 116 can correspond to “a car.”
- FIG. 1A illustrates a sentence 100 : “John bought a car.”
- a frame 104 (Commerce-Buy) can represent a frame that can be evoked by morphological variants of an example lexical unit 108 (Buy. V, where “V” indicates the word “buy” as a verb).
- the word “bought” is one of the
- FIG. 1B illustrates a similar sentence 120 : “Mary sold a car.”
- the frame 104 can also be evoked by morphological variants of another example lexical unit 120 (Sell. V, where “V” indicates the word “sell” as a verb).
- the word “sold” is one of the morphological variants of this lexical unit 120 .
- core role 124 (Seller) can represent another example core role for this frame 104 .
- FIGS. 1C-1D illustrate example semantic frames and example semantic roles according to a second labeled (or annotated) lexical database.
- the second labeled lexical database is the PropBank lexical database.
- the second labeled lexical database can include information about verbs, in the form of sense frames, and the possible semantic roles that each frame could take.
- modifier roles can be shared across verb frames, somewhat similar to the non-core roles discussed above.
- role labels although few in number, take specific meaning for each verb frame in the second labeled lexical database.
- FIG. 1C illustrates the sentence 104 : “John bought a car.”
- a frame 150 (Buy.01) can represent a verb frame that can be evoked by morphological variants of lexical unit 108 (Buy. V). Again, the word “bought” is one of the morphological variants of this lexical unit 108 .
- Generic core roles 154 and 158 (A 0 and A 1 , respectively) represent example generic core roles for this frame 150 . Specifically, generic core role 154 can correspond to “John” and generic core role 158 can correspond to “a car.”
- FIG. 1D illustrates the sentence 120 : “Mary sold a car.”
- a frame 162 (Sell.01) can represent a verb frame that can be evoked by morphological variants of lexical unit 120 (Sell. V). Again, the word “sold” is one of the morphological variants of this lexical unit 120 .
- Generic core roles 154 and 158 (A 0 and A 1 , respectively) are shared by this frame 162 . Specifically, generic core role 154 can correspond to “Mary” and generic core role 158 can again correspond to “a car.”
- the frame-semantic parsing problem can be modeled in two stages: (1) frame identification and (2) argument identification.
- the frame identification stage can correspond to a frame disambiguation stage for a sentence
- the argument identification stage ca correspond to a stage that finds the various arguments that fulfill the identified frame's semantic roles within the sentence.
- the model can be learned/generated using a large labeled training data including a plurality of groups of words, each group of words having a predicate word. Examples of a group of words include a sentence and a phrase.
- the model can receive word embeddings as input and learn to identify semantic frames.
- a word embedding can be a distributed representation of a meaning where each word is represented as a vector in R n , where R represents the vector space and n represents its dimension.
- R represents the vector space
- n represents its dimension.
- word embeddings can be used to represent the syntactic context of a particular predicate instance as a vector.
- the predicate “runs” has two syntactic dependents: a subject and a direct object.
- the sentence also has no prepositional phrases or clausal complements.
- the syntactic context of “runs” can be represented as a vector with blocks for all the possible dependents warranted by a syntactic parser.
- the positions 0 . . . n in the vector may correspond to the subject dependents, n+1 . . . 2n may correspond to the clausal complement dependent, and so forth.
- the context can be a vector in R kn with the embedding of “He” at the subject position, the embedding of “company” in the direct object position, and zeros or null values everywhere else, where k is a number of possible syntactic context types.
- a matrix can be learned that maps this high dimensional and sparse representation into a lower dimensional space.
- this model can learn an embedding for all the possible labels, i.e., the frames in a given lexicon.
- the predicate-context can be mapped to the low dimensional space, and a nearest frame label can be chosen as the classification.
- the other words in the sentence “He runs the company” can be extracted in the syntactic context of the word “runs.”
- their word embeddings can be concatenated to create an initial vector space representation.
- This initial vector space representation may be in a high dimensional space.
- a mapping from the initial vector space representation into a low dimensional space can be learned.
- An embedding for each possible frame label in the same low dimensional space can also be learned. For example, the goal of learning may be to make sure that the correct frame label is as close as possible to the mapped context, while competing frame labels are further away.
- x represent the actual sentence with a marked predicate, along with the associated syntactic parse tree.
- An initial representation of the predicate context can be g(x).
- Initial word embeddings can be of dimension n.
- g can be a function from a parsed sentence x to R nk .
- g can select some important positions relative to the predicate, and can reserve a block in its output space for the embedding of words found at that position.
- the function g can consider clausal complements and direct objects.
- the low dimensional space that is mapped to can be R m and the learned mapping can be M:R nk ⁇ R m .
- the mapping M can be a linear transformation, and it can be learned using a linear transformation algorithm.
- the linear transformation algorithm is the Wsabie algorithm.
- the linear transformation algorithm can learn an embedding for each frame label (hereinafter y).
- y can correspond to a point in R m .
- parameters can be stored in an F ⁇ m matrix, one m-dimensional point for each frame, which can be referred to as a linear mapping Y.
- a lexical unit e.g., the lemma conjoined with a coarse POS tag
- l A lexical unit for the marked predicate
- the frames associated with l in the frame lexicon and the training corpora can be denoted as F l , which can also be referred to as a confusion set.
- the linear transformation algorithm can perform gradient-based updates on an objective that tries to minimize the distance between M(g(x)) and the embedding of a correct label Y(y), while maintaining a large distance between M(g(x)) and other possible labels Y( y ) in the confusion set F l .
- model learning can be performed using a margin ranking loss function, which is described in greater detail below. Because the linear transformation algorithm can learn a single mapping from g(x) to R m , parameters can be shared between different words and different frames.
- the sentence “He runs the company” may help the model disambiguate the sentence “He owns the company.” Moreover, because g(x) relies upon word embeddings rather than word identities, information can be shared between words. For example, the sentence “He runs the company” may help the model learn about the sentence “She runs a corporation.”
- the representation can be a block vector where each block corresponds to a syntactic position relative to the predicate, and each block's value can correspond to the embedding of the word at that position.
- the first variant can be direct dependents. More specifically, the positions of interest can be the labels of the direct dependents of the predicate, so k can be the number of labels that the dependency parser can produce. For example, if the label on an edge between “runs” and “He” is nsubj, the embedding of “He” may be put in the block corresponding to nsubj. If a particular label occurs multiple times, then the embeddings of the words below this particular label can be averaged.
- Topicalization can place discriminating information further from the predicate.
- the sentence “It was the company that he runs” is an alternative of the sentence “He runs the company.”
- the word “company” may dominate the predicate “runs.”
- predicates in embedded clauses may have a distant agent which cannot be captured using direct dependents.
- the sentence “The athlete prepared himself for three months to run the marathon” is an alternative of the sentence “The athlete ran the marathon.”
- the agent “The athlete” is not a direct dependent, but is connected via a longer dependency path.
- Dependency paths are the second variant of the context function g(x).
- the training data for a given task can be scanned for the dependency paths that connected known predicates to known semantic arguments.
- This set of dependency paths can be deemed as possible positions in the initial vector space representation.
- all dependency labels e.g., paths of length l
- the block cardinality k can be the sum of the number of scanned known dependency paths and the number of dependency labels. Given a predicate in its sentential context, only those context words that appear in positions warranted by this set can be extracted.
- the objective function can be modeled using a weighted approximate-rank pairwise loss, which may be learned with stochastic gradient descent.
- the mapping from g(x) to the low dimensional space R m can be the linear transformation, and thus the model parameters to be learned can be the matrix M ⁇ R kn ⁇ m as well as the embedding of each possible frame label, which can be represented as another matrix Y ⁇ R F ⁇ m where there are F frames in total.
- the training objective function can minimize:
- rank y (x) represents the rank of the positive frame y relative for all the negative frames:
- a stochastic gradient can be employed. For example, for speed, the computation of rank y (x) can be replaced with a sampled approximation: sample N items y until a violation is found, e.g., max(0, ⁇ +s(x, y ) ⁇ s(x,y))>0, and then approximate the rank with (F ⁇ 1)/N.
- Various values for the stochastic learning rate, the margin ⁇ , and/or dimensionality in can be selected.
- An alternative approach may learn only the matrix M, and then use a k-nearest neighbor classifier in R m .
- One advantage of learning an embedding for the frame labels is that at inference time, only the set of labels for classification can be considered, rather than all training examples. Additionally, since a frame lexicon can be used that gives all possible frames for a given predicate, only a handful of candidate labels can be considered. If more or all candidates were considered, the process may be very slow.
- an argument identification model Given the sentence x with a marked predicate, an argument identification model can assume that the predicate frame y has been disambiguated. From a frame lexicon, a set of semantic roles R y associated with y can be identified. This set can also contain a null role r ⁇ . From the sentence x, a rule-based candidate argument extraction algorithm can extract a set of spans A that may potentially server as overt arguments A y for y.
- the following log-likelihood can be optimized to train the model:
- the set of argument identification features used by the feature function h include:
- this learning mechanism uses a local log-linear model
- hard structural constraints can be applied to satisfy certain global requirements in the argument output.
- ⁇ a positive real-valued score
- the solution to the global ILP can be treated as the final output of the method. Any suitable global ILP solver may be utilized to solve the global ILP and obtain the final output.
- the server 204 can communicate with a computing device 208 via a network 212 .
- Examples of the computing device 208 include a desktop computer, a laptop computer, a tablet computer, a mobile phone, and wearable technology such as a smartwatch or eyewear incorporating a computing device.
- the network 212 can include a local area network (LAN), a wide area network (WAN), e.g., the Internet, or a combination thereof.
- a user 216 can operate the computing device 208 .
- the server 204 can implement at least a portion of the techniques of the present disclosure.
- the server 204 can generate or train a model using the semantic frame identification techniques with word embedding discussed herein.
- the server 204 can also utilize the model to perform tasks in various scenarios, at least some of which are described in greater detail below. It should be appreciated, however, that the model can be utilized for any suitable tasks relating to computational linguistics and, more particularly, to any suitable tasks involving natural language parsing.
- the server 204 can index a plurality of web pages using the model to obtain a plurality of indexed web pages. After obtaining the plurality of indexed web pages using the model, the server 204 can then utilize the plurality of indexed web pages to provide search results in response to a search query.
- the search query may be input by the user 216 to the computing device 208 , and the search results may be output to the user 216 at the computing device 208 .
- the server 204 can receive speech input representing a question and the server 204 can convert the speech input to a text.
- the speech-to-text conversion may be performed elsewhere and the text can be received by the server 204 .
- the server 204 can analyze the text using the model and generate an answer to the question based on the analyzing of the text using the model.
- the question may be input by the user 216 at the computing device, and the answer (speech and/or text) may be output to the user 216 at the computing device 208 .
- the server 204 can receive a text to be translated from a source language to a target language, the source language being a same language as a language associated with the model. The server 204 can then analyze the text using the model, and generate a translation of the text from the source language to the target language based on the analyzing of the text using the model. For example, the text may be input by the user 216 at the computing device 208 and the translated text may be output to the user 216 at the computing device 208 .
- the server 204 can include a communication device 300 , a processor 304 , and a memory 308 .
- the communication device 300 can include any suitable components (a transceiver) configured to communicate with other devices via the network 212 .
- the processor 304 can control operation of the server 204 and can implement at least a portion of the techniques of the present disclosure as discussed herein. It should be appreciated that the term “processor” as used herein can refer to both a single server and a plurality of servers operating in a parallel or distributed architecture.
- the memory 208 can be any suitable storage medium (flash, hard disk, etc.) configured to store information at the server 204 .
- the memory 308 may store the first and/or second labeled lexical databases discussed herein.
- the server 204 can receive labeled training data including a plurality of groups of words, each group of words having a predicate word, and each word having generic word embeddings.
- the labeled training data may be one of the first and second labeled lexical databases, which may be stored at the memory 208 or accessed at another device.
- the server 204 can extract the plurality of groups of words in a syntactic context of their predicate words.
- the server 204 can concatenate the generic word embeddings to create a high dimensional vector space representing features for each word.
- the server 204 can obtain a model having a learned mapping from the high dimensional vector space to a low dimensional vector space and learned embeddings for each possible semantic frame in the low dimensional vector space.
- the server 204 can output the model for storage, e.g., at the memory 308 .
- the technique 400 can then end or return to 404 for one or more additional cycles.
- Example embodiments are provided so that this disclosure will be thorough, and will fully convey the scope to those who are skilled in the art. Numerous specific details are set forth such as examples of specific components, devices, and methods, to provide a thorough understanding of embodiments of the present disclosure. It will be apparent to those skilled in the art that specific details need not be employed, that example embodiments may be embodied in many different forms and that neither should be construed to limit the scope of the disclosure. In some example embodiments, well-known procedures, well-known device structures, and well-known technologies are not described in detail.
- first, second, third, etc. may be used herein to describe various elements, components, regions, layers and/or sections, these elements, components, regions, layers and/or sections should not be limited by these terms. These terms may be only used to distinguish one element, component, region, layer or section from another region, layer or section. Terms such as “first,” “second,” and other numerical terms when used herein do not imply a sequence or order unless clearly indicated by the context. Thus, a first element, component, region, layer or section discussed below could be termed a second element, component, region, layer or section without departing from the teachings of the example embodiments.
- module may refer to, be part of, or include: an Application Specific Integrated Circuit (ASIC); an electronic circuit; a combinational logic circuit; a field programmable gate array (FPGA); a processor or a distributed network of processors (shared, dedicated, or grouped) and storage in networked clusters or datacenters that executes code or a process; other suitable components that provide the described functionality; or a combination of some or all of the above, such as in a system-on-chip.
- the term module may also include memory (shared, dedicated, or grouped) that stores code executed by the one or more processors.
- code may include software, firmware, byte-code and/or microcode, and may refer to programs, routines, functions, classes, and/or objects.
- shared means that some or all code from multiple modules may be executed using a single (shared) processor. In addition, some or all code from multiple modules may be stored by a single (shared) memory.
- group means that some or all code from a single module may be executed using a group of processors. In addition, some or all code from a single module may be stored using a group of memories.
- the techniques described herein may be implemented by one or more computer programs executed by one or more processors.
- the computer programs include processor-executable instructions that are stored on a non-transitory tangible computer readable medium.
- the computer programs may also include stored data.
- Non-limiting examples of the non-transitory tangible computer readable medium are nonvolatile memory, magnetic storage, and optical storage.
- the present disclosure also relates to an apparatus for performing the operations herein.
- This apparatus may be specially constructed for the required purposes, or it may comprise a general-purpose computer selectively activated or reconfigured by a computer program stored on a computer readable medium that can be accessed by the computer.
- a computer program may be stored in a tangible computer readable storage medium, such as, but is not limited to, any type of disk including floppy disks, optical disks, CD-ROMs, magnetic-optical disks, read-only memories (ROMs), random access memories (RAMs), EPROMs, EEPROMs, magnetic or optical cards, application specific integrated circuits (ASICs), or any type of media suitable for storing electronic instructions, and each coupled to a computer system bus.
- the computers referred to in the specification may include a single processor or may be architectures employing multiple processor designs for increased computing capability.
- the present disclosure is well suited to a wide variety of computer network systems over numerous topologies.
- the configuration and management of large networks comprise storage devices and computers that are communicatively coupled to dissimilar computers and storage devices over a network, such as the Internet.
Abstract
Description
g(x)=[0, . . . , 0, embedding of “company”].
where x and y represent training inputs and their corresponding correct frames,
and L(η) converts the rank to a weight. Choosing L(η)=C(η) for any positive constant C optimizes the mean rank, whereas a weighting such as L(η)=Σi=1 n1/i can optimize the top of the ranked list.
M={(r,a):rεR y ,aεA∪A y} (3),
can represent a set of tuples that associates each role r in Ry with a span a according to the known dependency path data. This mapping may associate spans with the null role rφ as well. The following log-likelihood can be optimized to train the model:
Above, θ represents the model parameters, a represents candidate arguments and h represents a feature function that uses a set of argument identification features.
-
- Starting word of a;
- POS of the starting word of a;
- Ending word of a;
- POS of the ending word of a;
- Head word of a (hereinafter “the head”);
- POS of the head word;
- Bag of words in a;
- Bag of POS tags in a;
- A bias feature;
- Voice of the predicate use;
- Word cluster of the head;
- Word cluster of the head conjoined with word cluster of the predicate;
- Dependency path between the head and the predicate;
- The set of dependency labels of the predicate's children;
- Dependency path conjoined with the POS tag of the head;
- Dependency path conjoined with the word cluster of the head;
- Position of a with respect to the predicate (before, after, overlap, or identical);
- Whether the subject of the predicate is missing (missingsubj);
- missingsubj, conjoined with the dependency path; and
- missingsubj, conjoined with the dependency path from the verb dominating the predicate to the head.
Claims (20)
Priority Applications (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/271,997 US9262406B1 (en) | 2014-05-07 | 2014-05-07 | Semantic frame identification with distributed word representations |
US15/008,794 US10289952B2 (en) | 2014-05-07 | 2016-01-28 | Semantic frame identification with distributed word representations |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/271,997 US9262406B1 (en) | 2014-05-07 | 2014-05-07 | Semantic frame identification with distributed word representations |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/008,794 Continuation US10289952B2 (en) | 2014-05-07 | 2016-01-28 | Semantic frame identification with distributed word representations |
Publications (1)
Publication Number | Publication Date |
---|---|
US9262406B1 true US9262406B1 (en) | 2016-02-16 |
Family
ID=55275432
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/271,997 Active 2034-08-15 US9262406B1 (en) | 2014-05-07 | 2014-05-07 | Semantic frame identification with distributed word representations |
US15/008,794 Active 2035-10-10 US10289952B2 (en) | 2014-05-07 | 2016-01-28 | Semantic frame identification with distributed word representations |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/008,794 Active 2035-10-10 US10289952B2 (en) | 2014-05-07 | 2016-01-28 | Semantic frame identification with distributed word representations |
Country Status (1)
Country | Link |
---|---|
US (2) | US9262406B1 (en) |
Cited By (21)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20160147737A1 (en) * | 2014-11-20 | 2016-05-26 | Electronics And Telecommunications Research Institute | Question answering system and method for structured knowledgebase using deep natual language question analysis |
US20160239739A1 (en) * | 2014-05-07 | 2016-08-18 | Google Inc. | Semantic frame identification with distributed word representations |
CN106021272A (en) * | 2016-04-04 | 2016-10-12 | 上海大学 | Keyword automatic extraction method based on distributed expression word vector calculation |
US20170032035A1 (en) * | 2015-07-28 | 2017-02-02 | Microsoft Technology Licensing, Llc | Representation Learning Using Multi-Task Deep Neural Networks |
US20170286403A1 (en) * | 2016-03-31 | 2017-10-05 | International Business Machines Corporation | System, method, and recording medium for natural language learning |
US9805028B1 (en) * | 2014-09-17 | 2017-10-31 | Google Inc. | Translating terms using numeric representations |
US10019437B2 (en) * | 2015-02-23 | 2018-07-10 | International Business Machines Corporation | Facilitating information extraction via semantic abstraction |
US10042846B2 (en) * | 2016-04-28 | 2018-08-07 | International Business Machines Corporation | Cross-lingual information extraction program |
CN108959246A (en) * | 2018-06-12 | 2018-12-07 | 北京慧闻科技发展有限公司 | Answer selection method, device and electronic equipment based on improved attention mechanism |
US10556348B2 (en) * | 2014-12-01 | 2020-02-11 | At&T Intellectual Property I, L.P. | System and method for semantic processing of natural language commands |
CN111382234A (en) * | 2018-12-11 | 2020-07-07 | 航天信息股份有限公司 | Reply providing method and device based on customer service |
CN111859977A (en) * | 2019-06-06 | 2020-10-30 | 北京嘀嘀无限科技发展有限公司 | Semantic analysis method and device, electronic equipment and storage medium |
CN112905186A (en) * | 2021-02-07 | 2021-06-04 | 中国科学院软件研究所 | High signal-to-noise ratio code classification method and device suitable for open-source software supply chain |
CN112989767A (en) * | 2021-04-21 | 2021-06-18 | 腾讯科技（深圳）有限公司 | Medical term labeling method, medical term mapping device and medical term mapping equipment |
US11120221B2 (en) | 2019-03-26 | 2021-09-14 | Tata Consultancy Services Limited | Method and system to resolve ambiguities in regulations |
WO2021179956A1 (en) * | 2020-03-13 | 2021-09-16 | 华为技术有限公司 | Translation method, related apparatus, device, and computer-readable storage medium |
US20220358291A1 (en) * | 2021-04-22 | 2022-11-10 | Adobe Inc. | Dependency path reasoning for measurement extraction |
US11538468B2 (en) | 2019-09-12 | 2022-12-27 | Oracle International Corporation | Using semantic frames for intent classification |
CN116681957A (en) * | 2023-08-03 | 2023-09-01 | 富璟科技（深圳）有限公司 | Image recognition method based on artificial intelligence and computer equipment |
CN116720004A (en) * | 2023-08-09 | 2023-09-08 | 腾讯科技（深圳）有限公司 | Recommendation reason generation method, device, equipment and storage medium |
CN117436459A (en) * | 2023-12-20 | 2024-01-23 | 商飞智能技术有限公司 | Verb-verb semantic relationship identification method and device |
Families Citing this family (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9659005B2 (en) * | 2014-05-16 | 2017-05-23 | Semantix Technologies Corporation | System for semantic interpretation |
US10628735B2 (en) * | 2015-06-05 | 2020-04-21 | Deepmind Technologies Limited | Reading comprehension neural networks |
CN107391575B (en) * | 2017-06-20 | 2020-08-04 | 浙江理工大学 | Implicit feature recognition method based on word vector model |
CN109344385B (en) * | 2018-01-30 | 2020-12-22 | 深圳壹账通智能科技有限公司 | Natural language processing method, device, computer equipment and storage medium |
WO2021124535A1 (en) * | 2019-12-19 | 2021-06-24 | 富士通株式会社 | Information processing program, information processing method, and information processing device |
US11797610B1 (en) * | 2020-09-15 | 2023-10-24 | Elemental Cognition Inc. | Knowledge acquisition tool |
US11409958B2 (en) * | 2020-09-25 | 2022-08-09 | International Business Machines Corporation | Polar word embedding |
Family Cites Families (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
KR20150082783A (en) * | 2014-01-08 | 2015-07-16 | 한국전자통신연구원 | Semantic Frame Operating Method Based on Text Big-data and Electronic Device supporting the same |
US9262406B1 (en) * | 2014-05-07 | 2016-02-16 | Google Inc. | Semantic frame identification with distributed word representations |
-
2014
- 2014-05-07 US US14/271,997 patent/US9262406B1/en active Active
-
2016
- 2016-01-28 US US15/008,794 patent/US10289952B2/en active Active
Non-Patent Citations (8)
Title |
---|
Baker, C. et al., "SemEval'07 Task 19: Frame Semantic Structure Extraction," Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), Prague, Jun. 2007, Association for Computational Linguistics, pp. 99-104. |
Carreras, X. et al., "Introduction to the CoNLL-2004 Shared Task: Semantic Role Labeling," 2004, 9 pages. |
Carreras, X. et al., "Introduction to the CoNLL-2005 Shared Task: Semantic Role Labeling," 2005, 13 pages. |
Das, D. et al., "Frame-Semantic Parsing," 2014 Association for Computational Linguistics, vol. 40, No. 1, pp. 9-56. |
Das, D. et al., "Probabilistic Frame-Semantic Parsing," Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, Los Angeles, California, Jun. 2010, Association for Computational Linguistics, pp. 948-956. |
Hermann, K. et al., "Semantic Frame Identification with Distributed Word Representations," Proceedings of the 52th Annual Meeting of the Association for Computational Linguistics (2014), 11 pages. |
Marquez, L. et al., "Semantic Role Labeling: An Introduction to the Special Issue," 2008 Association for Computational Linguistics, vol. 34, No. 2, pp. 145-159. |
Palmer, M. et al., "The Proposition Bank: An Annotated Corpus of Semantic Roles," 2005 Association for Computational Linguistics, vol. 31, No. 1, pp. 71-105. |
Cited By (36)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10289952B2 (en) * | 2014-05-07 | 2019-05-14 | Google Llc | Semantic frame identification with distributed word representations |
US20160239739A1 (en) * | 2014-05-07 | 2016-08-18 | Google Inc. | Semantic frame identification with distributed word representations |
US9805028B1 (en) * | 2014-09-17 | 2017-10-31 | Google Inc. | Translating terms using numeric representations |
US10503837B1 (en) | 2014-09-17 | 2019-12-10 | Google Llc | Translating terms using numeric representations |
US9633006B2 (en) * | 2014-11-20 | 2017-04-25 | Electronics And Telecommunications Research Institute | Question answering system and method for structured knowledgebase using deep natural language question analysis |
US20160147737A1 (en) * | 2014-11-20 | 2016-05-26 | Electronics And Telecommunications Research Institute | Question answering system and method for structured knowledgebase using deep natual language question analysis |
US11724403B2 (en) * | 2014-12-01 | 2023-08-15 | Hyundai Motor Company | System and method for semantic processing of natural language commands |
US20200171670A1 (en) * | 2014-12-01 | 2020-06-04 | At&T Intellectual Property I, L.P. | System and Method for Semantic Processing of Natural Language Commands |
US10556348B2 (en) * | 2014-12-01 | 2020-02-11 | At&T Intellectual Property I, L.P. | System and method for semantic processing of natural language commands |
US10019437B2 (en) * | 2015-02-23 | 2018-07-10 | International Business Machines Corporation | Facilitating information extraction via semantic abstraction |
US20170032035A1 (en) * | 2015-07-28 | 2017-02-02 | Microsoft Technology Licensing, Llc | Representation Learning Using Multi-Task Deep Neural Networks |
US10089576B2 (en) * | 2015-07-28 | 2018-10-02 | Microsoft Technology Licensing, Llc | Representation learning using multi-task deep neural networks |
US20170286403A1 (en) * | 2016-03-31 | 2017-10-05 | International Business Machines Corporation | System, method, and recording medium for natural language learning |
US10282411B2 (en) * | 2016-03-31 | 2019-05-07 | International Business Machines Corporation | System, method, and recording medium for natural language learning |
CN106021272B (en) * | 2016-04-04 | 2019-11-19 | 上海大学 | The keyword extraction method calculated based on distributed expression term vector |
CN106021272A (en) * | 2016-04-04 | 2016-10-12 | 上海大学 | Keyword automatic extraction method based on distributed expression word vector calculation |
US10042846B2 (en) * | 2016-04-28 | 2018-08-07 | International Business Machines Corporation | Cross-lingual information extraction program |
CN108959246A (en) * | 2018-06-12 | 2018-12-07 | 北京慧闻科技发展有限公司 | Answer selection method, device and electronic equipment based on improved attention mechanism |
CN108959246B (en) * | 2018-06-12 | 2022-07-12 | 北京慧闻科技(集团)有限公司 | Answer selection method and device based on improved attention mechanism and electronic equipment |
CN111382234A (en) * | 2018-12-11 | 2020-07-07 | 航天信息股份有限公司 | Reply providing method and device based on customer service |
US11120221B2 (en) | 2019-03-26 | 2021-09-14 | Tata Consultancy Services Limited | Method and system to resolve ambiguities in regulations |
CN111859977A (en) * | 2019-06-06 | 2020-10-30 | 北京嘀嘀无限科技发展有限公司 | Semantic analysis method and device, electronic equipment and storage medium |
US11538468B2 (en) | 2019-09-12 | 2022-12-27 | Oracle International Corporation | Using semantic frames for intent classification |
US20230091886A1 (en) * | 2019-09-12 | 2023-03-23 | Oracle International Corporation | Using semantic frames for intent classification |
WO2021179956A1 (en) * | 2020-03-13 | 2021-09-16 | 华为技术有限公司 | Translation method, related apparatus, device, and computer-readable storage medium |
CN112905186A (en) * | 2021-02-07 | 2021-06-04 | 中国科学院软件研究所 | High signal-to-noise ratio code classification method and device suitable for open-source software supply chain |
CN112905186B (en) * | 2021-02-07 | 2023-04-07 | 中国科学院软件研究所 | High signal-to-noise ratio code classification method and device suitable for open-source software supply chain |
CN112989767A (en) * | 2021-04-21 | 2021-06-18 | 腾讯科技（深圳）有限公司 | Medical term labeling method, medical term mapping device and medical term mapping equipment |
CN112989767B (en) * | 2021-04-21 | 2021-09-03 | 腾讯科技（深圳）有限公司 | Medical term labeling method, medical term mapping device and medical term mapping equipment |
US20220358291A1 (en) * | 2021-04-22 | 2022-11-10 | Adobe Inc. | Dependency path reasoning for measurement extraction |
US11893352B2 (en) * | 2021-04-22 | 2024-02-06 | Adobe Inc. | Dependency path reasoning for measurement extraction |
CN116681957A (en) * | 2023-08-03 | 2023-09-01 | 富璟科技（深圳）有限公司 | Image recognition method based on artificial intelligence and computer equipment |
CN116681957B (en) * | 2023-08-03 | 2023-10-17 | 富璟科技（深圳）有限公司 | Image recognition method based on artificial intelligence and computer equipment |
CN116720004A (en) * | 2023-08-09 | 2023-09-08 | 腾讯科技（深圳）有限公司 | Recommendation reason generation method, device, equipment and storage medium |
CN116720004B (en) * | 2023-08-09 | 2023-12-15 | 腾讯科技（深圳）有限公司 | Recommendation reason generation method, device, equipment and storage medium |
CN117436459A (en) * | 2023-12-20 | 2024-01-23 | 商飞智能技术有限公司 | Verb-verb semantic relationship identification method and device |
Also Published As
Publication number | Publication date |
---|---|
US20160239739A1 (en) | 2016-08-18 |
US10289952B2 (en) | 2019-05-14 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10289952B2 (en) | Semantic frame identification with distributed word representations | |
Hermann et al. | Semantic frame identification with distributed word representations | |
Jung | Semantic vector learning for natural language understanding | |
Kim et al. | Two-stage multi-intent detection for spoken language understanding | |
Shoukry et al. | Sentence-level Arabic sentiment analysis | |
Pham et al. | Learning distributed representations for multilingual text sequences | |
Alhumoud et al. | Arabic sentiment analysis using recurrent neural networks: a review | |
Zeng et al. | Domain-specific Chinese word segmentation using suffix tree and mutual information | |
Gokul et al. | Sentence similarity detection in Malayalam language using cosine similarity | |
Das et al. | Part of speech tagging in odia using support vector machine | |
Monisha et al. | Classification of bengali questions towards a factoid question answering system | |
Nazare et al. | Sentiment analysis in Twitter | |
Greiner-Petter et al. | Why machines cannot learn mathematics, yet | |
US20230229936A1 (en) | Extraction of tasks from documents using weakly supervision | |
Li et al. | Fast coupled sequence labeling on heterogeneous annotations via context-aware pruning | |
Choi et al. | How to generate data for acronym detection and expansion | |
Shams et al. | Intent Detection in Urdu Queries Using Fine-Tuned BERT Models | |
Mishra et al. | Germeval 2017: sequence based models for customer feedback analysis | |
Aghaebrahimian et al. | Ontology-aware biomedical relation extraction | |
Croce et al. | Grammatical Feature Engineering for Fine-grained IR Tasks. | |
Karisani et al. | Multi-view active learning for short text classification in user-generated data | |
Chen et al. | Learning word embeddings from intrinsic and extrinsic views | |
Bach et al. | Paraphrase identification in Vietnamese documents | |
Fernandes et al. | Using wikipedia for cross-language named entity recognition | |
Olivo et al. | CRFPOST: Part-of-Speech Tagger for Filipino Texts using Conditional Random Fields |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:DAS, DIPANJAN;GANCHEV, KUZMAN;WESTON, JASON;AND OTHERS;SIGNING DATES FROM 20140509 TO 20140512;REEL/FRAME:032879/0555 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044566/0657Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |