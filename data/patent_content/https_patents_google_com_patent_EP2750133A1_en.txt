EP2750133A1 - Noise compensation using geotagged audio signals - Google Patents
Noise compensation using geotagged audio signals Download PDFInfo
- Publication number
- EP2750133A1 EP2750133A1 EP14162078.1A EP14162078A EP2750133A1 EP 2750133 A1 EP2750133 A1 EP 2750133A1 EP 14162078 A EP14162078 A EP 14162078A EP 2750133 A1 EP2750133 A1 EP 2750133A1
- Authority
- EP
- European Patent Office
- Prior art keywords
- geographic location
- audio signals
- noise
- utterance
- mobile device
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 230000005236 sound signal Effects 0.000 title claims abstract description 281
- 230000007613 environmental effect Effects 0.000 claims abstract description 127
- 238000000034 method Methods 0.000 claims abstract description 30
- 238000004590 computer program Methods 0.000 claims abstract description 14
- 238000013518 transcription Methods 0.000 claims description 28
- 230000035897 transcription Effects 0.000 claims description 28
- 238000012549 training Methods 0.000 claims description 13
- 238000012545 processing Methods 0.000 claims description 8
- 239000000203 mixture Substances 0.000 claims description 6
- 230000002708 enhancing effect Effects 0.000 abstract description 2
- 230000008569 process Effects 0.000 description 21
- 238000004891 communication Methods 0.000 description 8
- 238000010586 diagram Methods 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 4
- 238000004422 calculation algorithm Methods 0.000 description 3
- 230000000694 effects Effects 0.000 description 3
- 238000000926 separation method Methods 0.000 description 3
- 230000005540 biological transmission Effects 0.000 description 2
- 238000010276 construction Methods 0.000 description 2
- 230000003993 interaction Effects 0.000 description 2
- 230000000644 propagated effect Effects 0.000 description 2
- 238000013515 script Methods 0.000 description 2
- 230000001629 suppression Effects 0.000 description 2
- 239000013598 vector Substances 0.000 description 2
- 230000000007 visual effect Effects 0.000 description 2
- 241000282472 Canis lupus familiaris Species 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 230000006870 function Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 238000005457 optimization Methods 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
- 239000004065 semiconductor Substances 0.000 description 1
- 230000001953 sensory effect Effects 0.000 description 1
- 239000000758 substrate Substances 0.000 description 1
- 238000012546 transfer Methods 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L21/00—Processing of the speech or voice signal to produce another audible or non-audible signal, e.g. visual or tactile, in order to modify its quality or its intelligibility
- G10L21/02—Speech enhancement, e.g. noise reduction or echo cancellation
- G10L21/0208—Noise filtering
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/20—Speech recognition techniques specially adapted for robustness in adverse environments, e.g. in noise, of stress induced speech
Definitions
- This specification relates to speech recognition.
- a “search query” includes one or more query terms that a user submits to a search engine when the user requests the search engine to execute a search query, where a "term” or a “query term” includes one or more whole or partial words, characters, or strings of characters.
- a "result” (or a "search result") of the search query includes a Uniform Resource Identifier (URI) that references a resource that the search engine determines to be responsive to the search query.
- the search result may include other things, such as a title, preview image, user rating, map or directions, description of the corresponding resource, or a snippet of text that has been automatically or manually extracted from, or otherwise associated with, the corresponding resource.
- a user may enter query terms of a search query by typing on a keyboard or, in the context of a voice query, by speaking the query terms into a microphone of a mobile device.
- the microphone of the mobile device may record ambient noises or sounds, or "environmental audio," in addition to spoken utterances of the user.
- environmental audio may include background chatter or babble of other people situated around the user, or noises generated by nature (e.g., dogs barking) or man-made objects (e.g., office, airport, or road noise, or construction activity).
- the environmental audio may partially obscure the voice of the user, making it difficult for an automated speech recognition (“ASR”) engine to accurately recognize spoken utterances.
- ASR automated speech recognition
- one innovative aspect of the subject matter described in this specification may be embodied in methods for adapting, training, selecting or otherwise generating, by an ASR engine, a noise model for a geographic area, and for applying this noise model to "geotagged" audio signals (or “samples,” or “waveforms") that are received from a mobile device that is located in or near this geographic area.
- "geotagged” audio signals refer to signals that have been associated, or "tagged,” with geographical location metadata or geospatial metadata.
- the location metadata may include navigational coordinates, such as latitude and longitude, altitude information, bearing or heading information, or a name or an address associated with the location.
- the methods include receiving geotagged audio signals that correspond to environmental audio recorded by multiple mobile devices in multiple geographic locations, storing the geotagged audio signals, and generating a noise model for a particular geographic region using a selected subset of the geotagged audio signals.
- the ASR engine may perform noise compensation on the audio signal using the noise model that is generated for the particular geographic region, and may perform speech recognition on the noise-compensated audio signal.
- the noise model for the particular geographic region may be generated before, during, or after receipt of the utterance.
- another innovative aspect of the subject matter described in this specification may be embodied in methods that include the actions of receiving geotagged audio signals that correspond to environmental audio recorded by multiple mobile devices in multiple geographic locations, receiving an audio signal that corresponds to an utterance recorded by a particular mobile device, determining a particular geographic location associated with the particular mobile device, generating a noise model for the particular geographic location using a subset of the geotagged audio signals, where noise compensation is performed on the audio signal that corresponds to the utterance using the noise model that has been generated for the particular geographic location.
- speech recognition is performed on the utterance using the noise-compensated audio signal; generating the noise model further includes generating the noise model before receiving the audio signal that corresponds to the utterance; generating the noise model further includes generating the noise model after receiving the audio signal that corresponds to the utterance; for each of the geotagged audio signals, a distance between the particular geographic location and a geographic location associated the geotagged audio signal is determined, and the geotagged audio signals that are associated with geographic locations which are within a predetermined distance of the particular geographic location, or that are associated with geographic locations which are among the N closest geographic locations to the particular geographic location, are selected as the subset of the geotagged audio signals; the geotagged audio signals that are associated with the particular geographic location are selected as the subset of the geotagged audio signals; the subset of the geotagged audio signals are selected based on the particular geographic location, and based on context data associated with the utterance; the context data includes data that references a time or a
- the ASR engine may provide for better noise suppression of the audio signal. Speech recognition accuracy may be improved. Noise models may be generated using environmental audio signals that accurately reflect the actual ambient noise in a geographic area. Speech recognition and noise model generation may be performed at the server side, instead of on the client device, to allow for better process optimization and to increase computational efficiency.
- FIG. 1 is a diagram of an example system 100 that uses geotagged environmental audio to enhance speech recognition accuracy.
- FIG. 1 also illustrates a flow of data within the system 100 during states (a) to (i), as well as a user interface 158 that is displayed on a mobile device 104 during state (i).
- the system 100 includes a server 106 and an ASR engine 108, which are in communication with mobile client communication devices, including mobile devices 102 and the mobile device 104, over one or more networks 110.
- the server 106 may be a search engine, a dictation engine, a dialogue system, or any other engine or system that uses transcribed speech.
- the networks 110 may include a wireless cellular network, a wireless local area network (WLAN) or Wi-Fi network, a Third Generation (3G) or Fourth Generation (4G) mobile telecommunications network, a private network such as an intranet, a public network such as the Internet, or any appropriate combination thereof.
- WLAN wireless local area network
- 4G Fourth Generation
- the states (a) through (i) depict a flow of data that occurs when an example process is performed by the system 100.
- the states (a) to (i) may be time-sequenced states, or they may occur in a sequence that is different than the illustrated sequence.
- the ASR engine 108 receives geotagged, environmental audio signals 130 from the mobile devices 102 and generates geo-specific noise models 112 for multiple geographic locations.
- a particular geographic location associated with the mobile device 104 (or the user of the mobile device 104) is determined.
- the ASR engine 108 transcribes the utterance using the geo-specific noise model that matches, or that is otherwise suitable for, the particular geographic location, and one or more candidate transcriptions 146 are communicated from the ASR engine 108 to the server 106.
- the server 106 is a search engine
- the server 106 executes one or more search queries using the candidate transcriptions 146, generates search results 152, and communicates the search results 152 to the mobile device 104 for display.
- the mobile devices 102 communicate geotagged audio signals 130 that include environmental audio (referred to by this specification as "environmental audio signals") to the ASR engine 108 over the networks 110.
- environmental audio may include any ambient sounds that occur (naturally or otherwise) at a particular location.
- Environmental audio typically excludes the sounds, utterances, or voice of the user of the mobile device.
- the device 102a communicates an audio signal 130a that has been tagged with metadata 132a that references "Location A,” the device 102b communicates an audio signal 130b that has been tagged with metadata 132b that references “Location B,” and the device 102c communicates an audio signal 130c that has been tagged with metadata 132c that also references "Location B.”
- the metadata 132 may be associated with the audio signals 130 by mobile devices 102, as illustrated, or the metadata may be associated with the audio signals 130 by the ASR engine 108 or by another server after inferring a location of a mobile device 102 (or of the user of the mobile device 102).
- the environmental audio signals 130 may each include a two-second (or more) snippet of relatively high quality audio, such as sixteen kilohertz lossless audio signals.
- the environmental audio signals 130 may be associated with metadata that references the geographic location of the respective mobile device 102 when the environmental audio was recorded, captured or otherwise obtained.
- the environmental audio signals 130 may be manually uploaded from the mobile devices 102 to the ASR engine 108. For instance, environmental audio signals 130 may be generated and communicated in conjunction with the generation and communication of images to a public image database or repository. Alternatively, for users who opt to participate, environmental audio signals 130 may be automatically obtained and communicated from the mobile devices 102 to the ASR engine 108 without requiring an explicit, user actuation before each environmental audio signal is communicated to the ASR engine 108.
- the metadata 132 may describe locations in any number of different formats or levels of detail or granularity.
- the metadata 132a may include a latitude and longitude associated with the then-present location of the mobile device 102a
- the metadata 132c may include an address or geographic region associated with the then-present location of the mobile device 102c.
- the metadata 132b may describe a path of the vehicle ( e.g., including a start point and an end point, and motion data).
- the metadata 132 may describe locations in terms of location type (e.g., “moving vehicle,” “on a beach,” “in a restaurant,” “in tall building,” “South Asia,” “rural area,” “someplace with construction noise,” “amusement park,” “on a boat,” “indoors,” “underground,” “on a street,” “forest”).
- location type e.g., "moving vehicle,” “on a beach,” “in a restaurant,” “in tall building,” “South Asia,” “rural area,” “someplace with construction noise,” “amusement park,” “on a boat,” “indoors,” “underground,” “on a street,” “forest”
- a single audio signal may be associated with metadata that describes one or more locations.
- the geographic location associated with the audio signal 138 may instead be described in terms of a bounded area, expressed as a set of coordinates that define the bounded arrea.
- the geographic location may be defined using a region identifier, such as a state name or identifier, city name, idiomatic name (e.g., "Central Park"), a country name, or the identifier of arbitrarily defined region (e.g., "cell/region ABC123").
- the mobile devices 102 or the ASR engine 108 may process the metadata to adjust the level of detail of the location information (e.g., to determine a state associated with a particular set of coordinates), or the location information may be discretized ( e.g., by selecting a specific point along the path, or a region associated with the path).
- the level of detail of the metadata may also be adjusted by specifying or adding location type metadata, for example by adding an "on the beach" tag to an environmental audio signal whose associated geographic coordinates are associated with a beach location, or by adding a "someplace with lots of people" tag to an environmental audio signal that includes the sounds of multiple people talking in the background.
- the ASR engine 108 receives the geotagged environmental audio signals 130 from the mobile devices 102, and stores the geotagged audio signals (or portions thereof) in the collection 114 of environmental audio signals, in the data store 111. As described below, the collection is used for training, adapting, or otherwise generating one or more geographic location-specific (or "geo-specific") noise models 112.
- the ASR engine 108 may use a voice activity detector to verify that the collection 114 of environmental audio signals only includes audio signals 130 that correspond to ambient noise, or to filter out or otherwise identify or exclude audio signals 130 (or portions of the audio signals 130) that include voices of the various users of the mobile devices 102.
- the collection 114 of the ambient audio signals stored by the ASR engine 108 may include hundreds, thousands, millions, or hundreds of millions of environmental audio signals.
- a portion or all of the geo-tagged environmental audio signal 130a may be stored in the collection 114 as the environmental audio signal 124
- a portion or all of the geo-tagged environmental audio signal 130b may be stored in the collection 114 as the environmental audio signal 126a
- a portion or all of the geotagged environmental audio signal 130c may be stored in the collection 114 as the environmental audio signal 120b.
- Storing an environmental audio signal 130 in the collection may include determining whether a user's voice is encoded in the audio signal 130, and determining to store or determining not to store the environmental audio signal 130 in the collection based on determining that the user's voice is or is not encoded in the audio signal 130, respectively.
- storing an environmental audio signal in the collection may include identifying a portion of the environmental audio signal 130 that includes the user's voice, altering the environmental audio signal 130 by removing the portion that includes the user's voice or by associating metadata which references the portion that includes the user's voice, and storing the altered environmental audio signal 130 in the collection.
- the environmental audio signals included in the collection 114 can, in some implementations, include other metadata tags, such as tags that indicate whether background voices (e.g., cafeteria chatter) are present within the environmental audio, tags that identify the date on which a particular environmental audio signal was obtained (e.g., used to determine a sample age), or tags that identify whether a particular environmental audio signal deviates in some way from other environmental audio signals of the collection that were obtained in the same or similar location.
- tags such as tags that indicate whether background voices (e.g., cafeteria chatter) are present within the environmental audio, tags that identify the date on which a particular environmental audio signal was obtained (e.g., used to determine a sample age), or tags that identify whether a particular environmental audio signal deviates in some way from other environmental audio signals of the collection that were obtained in the same or similar location.
- the collection 114 of environmental audio signals may optionally be filtered to exclude particular environmental audio signals that satisfy or that do not satisfy particular criteria, such as to exclude particular environmental audio signals that are older than a certain age, or that include background chatter that may identify an individual or otherwise be proprietary or private in nature.
- data referencing whether the environmental audio signals of the collection 114 were manually or automatically uploaded may be tagged in metadata associated with the environmental audio signals.
- some of the noise models 112 may be generated using only those environmental audio signals that were automatically uploaded, or that were manually uploaded, or different weightings may be assigned to each category of upload during the generating of the noise models.
- the environmental audio signals of the collection 114 have been described as including an explicit tag that identifies a respective geographic location, in other implementations, such as where the association between an audio signal and a geographic location may be derived, the explicit use of a tag is not required.
- a geographic location may be implicitly associated with an environmental audio signal by processing search logs (e.g., stored with the server 106) to determine geographic location information for a particular environmental audio signal.
- receipt of a geo-tagged environmental audio signals by the ASR engine 108 may include obtaining an environmental audio signal that does not expressly include a geo-tag, and deriving and associating one or more geo-tags for the environmental audio signal.
- an audio signal 138 is communicated from the mobile device 104 to the ASR engine 108 over the networks 110.
- the mobile device 102 is illustrated as being different a different device than the mobile devices 104, in other implementations the audio signal 138 is communicated from one of the mobile devices 104 that provided an geo-tagged environmental audio signal 130.
- the audio signal 138 includes an utterance 140 ("Gym New York") recorded by the mobile device 104 (e.g., when the user implicitly or explicitly initiates a voice search query).
- the audio signal 138 includes metadata 139 that references the geographic location "Location B.”
- the audio signal 138 may also include a snippet of environmental audio, such as a two second snippet of environmental audio that was recorded before or after the utterance 140 was spoken. While the utterance 140 is described an illustrated in FIG. 1 as a voice query, in other example implementations the utterance may be an voice input to dictation system or to a dialog system.
- the geographic location (“Location B") associated with the audio signal 138 may be defined using a same or different level of detail as the geographic locations associated with the environmental audio signals included in the collection 114.
- the geographic locations associated with the environmental audio signals included in the collection 114 may correspond to geographic regions, while the geographic location associated with the audio signal 138 may correspond to a particular geographic coordinate.
- the ASR engine 108 may process the geographic metadata 139 or the metadata associated with the environmental audio signals of the collection 114 to align the level of detail, so that a subset selection process can be performed.
- the metadata 139 may be associated with the audio signal 138 by the mobile device 104 (or the user of the mobile device 104) based on location information that is current when the utterance 140 is recorded, and may be communicated with the audio signal 138 from the mobile device 104 to the ASR engine 108.
- the metadata may be associated with the audio signal 138 by the ASR engine 108, based on a geographic location that the ASR engine 108 infers for the mobile device 104 (or the user of the mobile device 104).
- the ASR engine 108 may infer the geographic location using the user's calendar schedule, user preferences (e.g., as stored in a user account of the ASR engine 108 or the server 106, or as communicated from the mobile device 104), a default location, a past location (e.g., the most recent location calculated by a GPS module of the mobile device 104), information explicitly provided by the user when submitting the voice search query, from the utterances 104 themselves, triangulation (e.g., WiFi or cell tower triangulation), a GPS module in the mobile device 104, or dead reckoning.
- the metadata 139 may include accuracy information that specifies an accuracy of the geographic location determination, signifying a likelihood that the mobile device 104 was actually in the particular geographic location specified by the metadata 139 at the time when the utterance 140 was recorded.
- Metadata included with the audio signals may include a location or locale associated with the respective mobile device 102.
- the locale information may describe, among other selectable parameters, a region in which the mobile device 102 is registered, or the language or dialect of the user of the mobile device 102.
- the speech recognition module 118 may use this information to select, train, adapt, or otherwise generate noise, speech, acoustic, popularity, or other models that match the context of the mobile device 104.
- the ASR engine 108 selects a subset of the environmental audio signals in the collection 114, and uses a noise model generating module 116 to train, adapt, or otherwise generate one or more noise models 112 (e.g., Gaussian Mixture Models (GMMs)) using the subset of the environmental audio signals, for example by using the subset of the environmental audio signals as a training set for the noise model.
- the subset may include all, or fewer than all of the environmental audio signals in the collection 114.
- the noise models 112 are applied to the audio signal 138 to translate or transcribe the spoken utterance 140 into one or more textual, candidate transcriptions 146, and to generate speech recognition confidence scores to the candidate transcriptions.
- the noise models are used for noise suppression or noise compensation, to enhance the intelligibility of the spoken utterance 140 to the ASR engine 108.
- the noise model generating module 116 may generate a noise model 120b for the geographic location ("Location B") associated with the audio signal 138 using the collection 114 of audio signals, specifically the environmental audio signals 126a and 126b that were geotagged as having been recorded at or near that geographic location, or at a same or similar type of location. Since the audio signal 138 is associated with this geographic location ("Location B"), the environmental audio included in the audio signal 138 itself may be used to generate a noise model for that geographic location, in addition to or instead of the environmental audio signals 126a and 126b.
- the noise model generating module 116 may generate a noise model 120a for another geographic location ("Location A"), using the environmental audio signal 124 that was geotagged as having been recorded at or near that other geographic location, or at a same or similar type of location. If the noise model generating module 116 is configured to select environmental audio signal that were geotagged as having been recorded near the geographic location associated with the audio signal 138, and if "Location A" is near "Location B," the noise model generating module 116 may generate a noise model 120b for "Location B" also using the environmental audio signal 124.
- other context data associated with the environmental audio signals of the collection 114 may be used to select the subset of the environmental audio signals to use to generate the noise models 112, or to adjust a weight or effect that a particular audio signal is to have upon the generation .
- the ASR engine 108 may select a subset of the environmental audio signals in the collection 114 whose contextual information indicates that they are longer than or shorter than a predetermined period of time, or that they satisfy certain quality or recency criteria.
- the ASR engine 108 may select, as the subset, environmental audio signals in the collection 114 whose contextual information indicates that they were recorded using a mobile device that has a similar audio subsystem as the mobile device 104.
- context data which may be used to select the subset of the environmental audio signals from the collection 114 may include, in some examples, the time information, date information, data referencing a speed or an amount of motion measured by the particular mobile device during recording, other device sensor data, device state data (e.g., Bluetooth headset, speaker phone, or traditional input method), a user identifier if the user opts to provide one, or information identifying the type or model of mobile device.
- the context data may provide an indication of conditions surrounding the recording of the audio signal 138.
- context data supplied with the audio signal 138 by the mobile device 104 may indicate that the mobile device 104 is traveling at highway speeds along a path associated with a highway.
- the ASR 108 may infer that the audio signal 138 was recorded within a vehicle, and may select a subset of the environmental audio signals in the collection 114 that are associated with an "inside moving vehicle" location type.
- context data supplied with the audio signal 138 by the mobile device 104 may indicate that the mobile device 104 is in a rural area, and that the utterance 140 was recorded on a Sunday at 6:00 am. Based on this context data, the ASR 108 may infer that it accuracy of the speech recognition would not be improved if the subset included environmental audio signals that were recorded in urban areas during rush hour.
- the context data may be used by the noise model generating module 116 to filter the collection 114 of environmental audio signals when generating noise models 112, or by the speech recognition module 118 to select an appropriate noise model 112 for a particular utterance.
- the noise model generating module 116 may select a weighted combination of the environmental audio signals of the collection 114 based upon the proximity of the geographic locations associated with the audio signals to the geographic location associated with the audio signal 138.
- the noise model generating module 116 may also generate the noise models 112 using environmental audio included in the audio signal 138 itself, for example environmental audio recorded before or after the utterances were spoken, or during pauses between utterances.
- the noise model generating module 116 can first determine the quality of the environmental audio signals stored in the collection 114 relative to the quality of the environmental audio included in the audio signal 138, and can choose to generate a noise model using the audio signals stored in the collection 114 only, using the environmental audio included in the audio signal 138 only, or any appropriate weighted or unweighted combination thereof. For instance, the noise model generating module 116 may determine that the audio signal 138 includes an insignificant amount of environmental audio, or that high quality environmental audio is stored for that particular geographic location in the collection 114, and may choose to generate the noise model without using (or giving little weight to) the environmental audio included in the audio signal 138.
- the noise model generating module 116 selects, as the subset, the environmental audio signals from the collection 114 that are associated with the N (e.g., five, twenty, or fifty) closest geographic locations to the geographic location associated with the audio signal 138.
- N e.g., five, twenty, or fifty
- a geometric shape e.g., a circle or square
- the noise model generating module 116 may select, as the subset, audio signals from the collection 114 that are associated with geographic regions that are wholly or partially located within the defined geometric shape.
- ASR engine 108 may select environmental audio signals that are associated with a same or a similar location type, even if the physical geographic locations associated with the selected audio signals are not physically near the geographic location associated with the audio signal 138. For instance, a noise model for an audio signal that was recorded on the beach in Florida may be tagged with "on the beach” metadata, and the noise model generating module 116 may select, as the subset, environmental audio signals from the collection 114 whose associated metadata indicate that they were also recorded on beaches, despite the fact that they were recorded on beaches in Australia, Hawaii, or in Iceland.
- the noise model generating module 116 may revert to selecting the subset based on matching location types, instead of matching actual, physical geographic locations, if the geographic location associated with the audio signal 138 does not match (or does not have a high quality match) with any physical geographic location associated with an environmental audio signal of the collection 114.
- Other matching processes such as clustering algorithms, may be used to match audio signals with environmental audio signals.
- the noise model generating module 116 may generate geo-specific noise models that are targeted or specific to other criteria as well, such as geo-specific noise models that are specific to different device types or times of day.
- a targeted sub-model may be generated based upon detecting that a threshold criterion has been satisfied, such as determining that a threshold number of environmental audio signals of the collection 114 refer to the same geographic location, and share another same or similar context (e.g., time of day, day of the week, motion characteristics, device type, etc.).
- the noise models 112 may be generated before, during, or after the utterance 140 has been received. For example, multiple environmental audio signals, incoming from a same or similar location as the utterance 140, may be processed in parallel with the processing of the utterance, and may be used to generate noise models 112 in real time or near real time, to better approximate the live noise conditions surrounding the mobile device 104.
- the speech recognition module 118 of the ASR engine 108 performs noise compensation on the audio signal 138 using the geo-specific noise model 120b for the geographic location associated with the audio signal 138, to enhance the accuracy of the speech recognition, and subsequently performs the speech recognition on the noise-compensated audio signal.
- the ASR engine 108 may apply a noise model 122 that is specific to both the geographic location associated with the audio signal, and to the device type of the mobile device 104.
- the speech recognition module 118 may generate one or more candidate transcriptions 146 that match the utterance encoded in the audio signal 138, and speech recognition confidence values for the candidate transcriptions.
- one or more of the candidate transcriptions 146 generated by the speech recognition module 118 are communicated from the ASR engine 108 to the server 106.
- the candidate transcriptions may be used as candidate query terms, to execute one or more search queries.
- the ASR engine 108 may rank the candidate transcriptions 146 by their respective speech recognition confidence scores before transmitting them to the server 106.
- the ASR engine 108 may provide a voice search query capability, a dictation capability, or a dialogue system capability to the mobile device 104.
- the server 106 may execute one or more search queries using the candidate query terms, generates a file 152 that references search results 160.
- the server 106 may include a web search engine used to find references within the Internet, a phone book type search engine used to find businesses or individuals, or another specialized search engine (e.g., a search engine that provides references to entertainment listings such as restaurants and movie theater information, medical and pharmaceutical information, etc.).
- the server 106 provides the file 152 that references the search results 160 to the mobile device 104.
- the file 152 may be a markup language file, such as an extensible Markup Language (XML) or HyperText Markup Language (HTML) file.
- XML extensible Markup Language
- HTML HyperText Markup Language
- the mobile device 104 displays the search results 160 on a user interface 158.
- the user interface includes a search box 157 that displays the candidate query term with the highest speech recognition confidence score ("Gym New York”), an alternate query term suggestion region 159 that displays another of the candidate query term that may have been intended by the utterance 140 ("Jim Newark"), a search result 160a that includes a link to a resource for "New York Fitness” 160a, and a search result 160b that includes a link to a resource for "Manhattan Body Building" 160b.
- the search result 160a may further include a phone number link that, when selected, may be dialed by the mobile device 104.
- FIG. 2 is a flowchart of an example of a process 200.
- the process 200 includes receiving one or more geotagged environmental audio signals, receiving an utterance associated with a geographic location, and generating a noise model based in part upon the geographic location. Noise compensation may be performed on the audio signal, with the noise model contributing to improving an the accuracy of speech recognition.
- a geotagged audio signal corresponding to environmental audio is received (202).
- the geotagged audio signal may be recorded by a mobile device in a particular geographic location.
- the geotagged audio signal may include associated context data such as a time, date, speed, or amount of motion measured during the recording of the geotagged audio signal or a type of device which recorded the geotagged audio signal.
- the received geotagged audio signal may be processed to exclude portions of the environmental audio that include a voice of a user of the mobile device. Multiple geotagged audio signals recorded in one or more geographic locations may be received and stored.
- the utterance may include a voice search query, or may be an input to a dictation or dialog application or system.
- the utterance may include associated context data such as a time, date, speed, or amount of motion measured during the recording of the geotagged audio signal or a type of device which recorded the geotagged audio signal.
- a particular geographic location associated with the mobile device is determined (206). For example, data referencing the particular geographic location may be received from the mobile device, or a past geographic location or a default geographic location associated with the mobile device may be determined.
- a noise model is generated for the particular geographic location using a subset of geotagged audio signals (208).
- the subset of geotagged audio signals may be selected by determining, for each of the geotagged audio signals, a distance between the particular geographic location and a geographic location associated the geotagged audio signal; and selecting those geotagged audio signals which are within a predetermined distance of the particular geographic location, or that are associated with geographic locations which are among the N closest geographic locations to the particular geographic location.
- the subset of geotagged audio signals may be selected by identifying the geotagged audio signals associated with the particular geographic location, and/or by identifying the geotagged audio signals that are acoustically similar to the utterance.
- the subset of geotagged audio signals may be selected based both on the particular geographic location and on context data associated with the utterance.
- Generating the noise model may include training a GMM using the subset of geotagged audio signals as a training set.
- Some noise reduction or separation algorithms such as non-negative matrix factorization (NMF)
- NMF non-negative matrix factorization
- Other algorithms such as Alqonquin, can use either GMMs or the feature vectors themselves, with artificial variances.
- Noise compensation is performed on the audio signal that corresponds to the utterance, using the noise model that has been generated for the particular geographic location, to enhance the audio signal or otherwise take decrease the uncertainty of the utterance due to noise (210).
- Speech recognition is performed on the noise-compensated audio signal (212).
- Performing the speech recognition may include generating one or more candidate transcriptions of the utterance.
- a search query may be executed using the one or more candidate transcriptions, or one or more of the candidate transcriptions can be provided as an output of a digital dictation application.
- one or more of the candidate transcriptions may be provided as an input to a dialog system, to allow a computer system to converse with the user of the particular mobile device.
- FIG. 3 is a flowchart of an example of a process 300.
- the process 300 includes collecting geotagged audio signals and generating multiple noise models based, in part, upon particular geographic locations associated with each of the geotagged audio signals.
- One or more of these noise models may be selected when performing speech recognition upon an utterance based, in part, upon a geographic location associated with the utterance.
- a geotagged audio signal corresponding to environmental audio is received (302).
- the geotagged audio signal may be recorded by a mobile device in a particular geographic location.
- the received geotagged audio signal may be processed to exclude portions of the environmental audio that include the voice of the user of the mobile device.
- Multiple geotagged audio signals recorded in one or more geographic locations may be received and stored.
- context data associated with the geotagged audio signal is received (304).
- the geotagged audio signal may include associated context data such as a time, date, speed, or amount of motion measured during the recording of the geotagged audio signal or a type of device which recorded the geotagged audio signal.
- Each noise model may be generated for a particular geographic location or, optionally, a location type, using a subset of geotagged audio signals.
- the subset of geotagged audio signals may be selected by determining, for each of the geotagged audio signals, a distance between the particular geographic location and a geographic location associated the geotagged audio signal and selecting those geotagged audio signals which are within a predetermined distance of the particular geographic location, or that are associated with geographic locations which are among the N closest geographic locations to the particular geographic location.
- the subset of geotagged audio signals may be selected by identifying the geotagged audio signals associated with the particular geographic location.
- the subset of geotagged audio signals may be selected based both on the particular geographic location and on context data associated with the geotagged audio signals.
- Generating the noise model may include training a Gaussian Mixture Model (GMM) using the subset of geotagged audio signals.
- GMM Gaussian Mixture Model
- the utterance may include a voice search query.
- the utterance may include associated context data such as a time, date, speed, or amount of motion measured during the recording of the geotagged audio signal or a type of device which recorded the geotagged audio signal.
- a geographic location is detected (310). For example, data referencing the particular geographic location may be received from a GPS module of the mobile device.
- a noise model is selected (312).
- the noise model may be selected from among multiple noise models generated for multiple geographic locations.
- Context data may optionally contribute to selection of a particular noise model among multiple noise models for the particular geographic location.
- Speech recognition is performed on the utterance using the selected noise model (314). Performing the speech recognition may include generating one or more candidate transcriptions of the utterance. A search query may be executed using the one or more candidate transcriptions.
- FIG. 4 shows a swim lane diagram of an example of a process 400 for enhancing speech recognition accuracy using geotagged environmental audio.
- the process 400 may be implemented by a mobile device 402, an ASR engine 404, and a search engine 406.
- the mobile device 402 may provide audio signals, such as environmental audio signals or audio signals that correspond to an utterance, to the ASR engine 404. Although only one mobile device 402 is illustrated, the mobile device 402 may represent a large quantity of mobile devices 402 contributing environmental audio signals and voice queries to the process 400.
- the ASR engine 404 may generate noise models based upon the environmental audio signals, and may apply one or more noise models to an incoming voice search query when performing speech recognition.
- the ASR engine 404 may provide transcriptions of utterances within a voice search query to the search engine 406 to complete the voice search query request.
- the process 400 begins with the mobile device 402 providing 408 a geotagged audio signal to the ASR engine 404.
- the audio signal may include environmental audio along with an indication regarding the location at which the environmental audio was recorded.
- the geotagged audio signal may include context data, for example in the form of metadata.
- the ASR engine 404 may store the geotagged audio signal in an environmental audio data store.
- the mobile device 402 provides 410 an utterance to the ASR engine 404.
- the utterance may include a voice search query.
- the recording of the utterance may optionally include a sample of environmental audio, for example recorded briefly before or after the recording of the utterance.
- the mobile device 402 provides 412 a geographic location to the ASR engine 404.
- the mobile device may provide navigational coordinates detected using a GPS module, a most recent (but not necessarily concurrent with recording) GPS reading, a default location, a location derived from the utterance previously provided, or a location estimated through dead reckoning or triangulation of transmission towers.
- the mobile device 402 may optionally provide context data, such as sensor data, device model identification, or device settings, to the ASR engine 404.
- the ASR engine 404 generates 414 a noise model.
- the noise model may be generated, in part, by training a GMM.
- the noise model may be generated based upon the geographic location provided by the mobile device 402. For example, geotagged audio signals submitted from a location at or near the location of the mobile device 402 may contribute to a noise model.
- context data provided by the mobile device 402 may be used to filter geotagged audio signals to select those most appropriate to the conditions in which the utterances were recorded. For example, the geotagged audio signals near the geographic location provided by the mobile device 402 may be filtered by a day of the week or a time of day. If a sample of environmental audio was included with the utterance provided by the mobile device 402, the environmental audio sample may optionally be included in the noise model.
- the ASR engine 404 performs speech recognition 416 upon the provided utterance. Using the noise model generated by the ASR engine 404, the utterance provided by the mobile device 402 may be transcribed into one or more sets of query terms.
- the ASR engine 404 forwards 418 the generated transcription(s) to the search engine 406. If the ASR engine 404 generated more than one transcription, the transcriptions may optionally be ranked in order of confidence.
- the ASR engine 404 may optionally provide context data to the search engine 406, such as the geographic location, which the search engine 406 may use to filter or rank search results.
- the search engine 406 performs 420 a search operation using the transcription(s).
- the search engine 406 may locate one or more URIs related to the transcription term(s).
- the search engine 406 provides 422 search query results to the mobile device 402.
- the search engine 406 may forward HTML code which generates a visual listing of the URI(s) located.
- Embodiments and all of the functional operations described in this specification may be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments may be implemented as one or more computer program products, i.e., one or more modules of computer program instructions encoded on a computer readable medium for execution by, or to control the operation of, data processing apparatus.
- the computer readable medium may be a machine-readable storage device, a machine-readable storage substrate, a memory device, a composition of matter effecting a machine-readable propagated signal, or a combination of one or more of them.
- data processing apparatus encompasses all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus may include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a propagated signal is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.
- a computer program (also known as a program, software, software application, script, or code) may be written in any form of programming language, including compiled or interpreted languages, and it may be deployed in any form, including as a stand alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a computer program does not necessarily correspond to a file in a file system.
- a program may be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code).
- a computer program may be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- the processes and logic flows described in this specification may be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows may also be performed by, and apparatus may also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- FPGA field programmable gate array
- ASIC application specific integrated circuit
- processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer.
- a processor will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks.
- a computer need not have such devices.
- a computer may be embedded in another device, e.g., a tablet computer, a mobile telephone, a personal digital assistant (PDA), a mobile audio player, a Global Positioning System (GPS) receiver, to name just a few.
- Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- the processor and the memory may be supplemented by, or incorporated in, special purpose logic circuitry.
- embodiments may be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user may provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices may be used to provide for interaction with a user as well; for example, feedback provided to the user may be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input.
- Embodiments may be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user may interact with an implementation, or any combination of one or more such back end, middleware, or front end components.
- the components of the system may be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network ("LAN”) and a wide area network (“WAN”), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system may include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- HTML file In each instance where an HTML file is mentioned, other file types or formats may be substituted. For instance, an HTML file may be replaced by an XML, JSON, plain text, or other types of files. Moreover, where a table or hash table is mentioned, other data structures (such as spreadsheets, relational databases, or structured files) may be used.
Abstract
Description
- This application claims priority to
U.S. Application Serial No. 12/760,147, filed on April 14, 2010 - This specification relates to speech recognition.
- As used by this specification, a "search query" includes one or more query terms that a user submits to a search engine when the user requests the search engine to execute a search query, where a "term" or a "query term" includes one or more whole or partial words, characters, or strings of characters. Among other things, a "result" (or a "search result") of the search query includes a Uniform Resource Identifier (URI) that references a resource that the search engine determines to be responsive to the search query. The search result may include other things, such as a title, preview image, user rating, map or directions, description of the corresponding resource, or a snippet of text that has been automatically or manually extracted from, or otherwise associated with, the corresponding resource.
- Among other approaches, a user may enter query terms of a search query by typing on a keyboard or, in the context of a voice query, by speaking the query terms into a microphone of a mobile device. When submitting a voice query, the microphone of the mobile device may record ambient noises or sounds, or "environmental audio," in addition to spoken utterances of the user. For example, environmental audio may include background chatter or babble of other people situated around the user, or noises generated by nature (e.g., dogs barking) or man-made objects (e.g., office, airport, or road noise, or construction activity). The environmental audio may partially obscure the voice of the user, making it difficult for an automated speech recognition ("ASR") engine to accurately recognize spoken utterances.
- In general, one innovative aspect of the subject matter described in this specification may be embodied in methods for adapting, training, selecting or otherwise generating, by an ASR engine, a noise model for a geographic area, and for applying this noise model to "geotagged" audio signals (or "samples," or "waveforms") that are received from a mobile device that is located in or near this geographic area. As used by this specification, "geotagged" audio signals refer to signals that have been associated, or "tagged," with geographical location metadata or geospatial metadata. Among other things, the location metadata may include navigational coordinates, such as latitude and longitude, altitude information, bearing or heading information, or a name or an address associated with the location.
- In further detail, the methods include receiving geotagged audio signals that correspond to environmental audio recorded by multiple mobile devices in multiple geographic locations, storing the geotagged audio signals, and generating a noise model for a particular geographic region using a selected subset of the geotagged audio signals. Upon receiving an utterance recorded by a mobile device within or near the same particular geographic area, the ASR engine may perform noise compensation on the audio signal using the noise model that is generated for the particular geographic region, and may perform speech recognition on the noise-compensated audio signal. Notably, the noise model for the particular geographic region may be generated before, during, or after receipt of the utterance.
- In general, another innovative aspect of the subject matter described in this specification may be embodied in methods that include the actions of receiving geotagged audio signals that correspond to environmental audio recorded by multiple mobile devices in multiple geographic locations, receiving an audio signal that corresponds to an utterance recorded by a particular mobile device, determining a particular geographic location associated with the particular mobile device, generating a noise model for the particular geographic location using a subset of the geotagged audio signals, where noise compensation is performed on the audio signal that corresponds to the utterance using the noise model that has been generated for the particular geographic location.
- Other embodiments of these aspects include corresponding systems, apparatus, and computer programs, configured to perform the actions of the methods, encoded on computer storage devices.
- These and other embodiments may each optionally include one or more of the following features. In various examples, speech recognition is performed on the utterance using the noise-compensated audio signal; generating the noise model further includes generating the noise model before receiving the audio signal that corresponds to the utterance; generating the noise model further includes generating the noise model after receiving the audio signal that corresponds to the utterance; for each of the geotagged audio signals, a distance between the particular geographic location and a geographic location associated the geotagged audio signal is determined, and the geotagged audio signals that are associated with geographic locations which are within a predetermined distance of the particular geographic location, or that are associated with geographic locations which are among the N closest geographic locations to the particular geographic location, are selected as the subset of the geotagged audio signals; the geotagged audio signals that are associated with the particular geographic location are selected as the subset of the geotagged audio signals; the subset of the geotagged audio signals are selected based on the particular geographic location, and based on context data associated with the utterance; the context data includes data that references a time or a date when the utterance was recorded by the mobile device, data that references a speed or an amount of motion measured by the particular mobile device when the utterance was recorded, data that references settings of the mobile device, or data that references a type of the mobile device; the utterance represents a voice search query, or an input to a digital dictation application or a dialog system; determining the particular geographic location further includes receiving data referencing the particular geographic location from the mobile device; determining the particular geographic location further includes determining a past geographic location or a default geographic location associated with the device; generating the noise model includes training a Gaussian Mixture Model (GMM) using the subset of the geotagged audio signals as a training set; one or more candidate transcriptions of the utterance are generated, a search query is executed using the one or more candidate transcriptions; the received geotagged audio signals are processed to exclude portions of the environmental audio that include voices of users of the multiple mobile devices; the noise model generated for the particular geographic location is selected from among multiple noise models generated for the multiple geographic locations; an area surrounding the particular geographic location is defined, a plurality of noise models associated with geographic locations within the area are selected from among the multiple noise models, a weighted combination of the selected noise models is generated, where the noise compensation is performed using the weighted combination of selected noise models; generating the noise model further includes generating the noise model for the particular geographic location using the subset of the geotagged audio signals and using an environmental audio portion of the audio signal that corresponds to the utterance; and/or an area is defined surrounding the particular geographic location, and the geotagged audio signals recorded within the area are selected as the subset of the geotagged audio signals.
- Particular embodiments of the subject matter described in this specification may be implemented to realize one or more of the following advantages. The ASR engine may provide for better noise suppression of the audio signal. Speech recognition accuracy may be improved. Noise models may be generated using environmental audio signals that accurately reflect the actual ambient noise in a geographic area. Speech recognition and noise model generation may be performed at the server side, instead of on the client device, to allow for better process optimization and to increase computational efficiency.
- The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other potential features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
-
-
FIG. 1 is a diagram of an example system that uses geotagged environmental audio to enhance speech recognition accuracy. -
FIG. 2 is a flow chart of an example of a process. -
FIG. 3 is a flow chart of another example of a process. -
FIG. 4 is a swim lane diagram of an example of a process. - Like reference symbols in the various drawings indicate like elements.
-
FIG. 1 is a diagram of anexample system 100 that uses geotagged environmental audio to enhance speech recognition accuracy.FIG. 1 also illustrates a flow of data within thesystem 100 during states (a) to (i), as well as auser interface 158 that is displayed on amobile device 104 during state (i). - In more detail, the
system 100 includes aserver 106 and anASR engine 108, which are in communication with mobile client communication devices, including mobile devices 102 and themobile device 104, over one ormore networks 110. Theserver 106 may be a search engine, a dictation engine, a dialogue system, or any other engine or system that uses transcribed speech. Thenetworks 110 may include a wireless cellular network, a wireless local area network (WLAN) or Wi-Fi network, a Third Generation (3G) or Fourth Generation (4G) mobile telecommunications network, a private network such as an intranet, a public network such as the Internet, or any appropriate combination thereof. - The states (a) through (i) depict a flow of data that occurs when an example process is performed by the
system 100. The states (a) to (i) may be time-sequenced states, or they may occur in a sequence that is different than the illustrated sequence. - Briefly, according the example process illustrated in
FIG. 1 , the ASRengine 108 receives geotagged, environmental audio signals 130 from the mobile devices 102 and generates geo-specific noise models 112 for multiple geographic locations. When an audio signal 138 that corresponds to an utterance recorded by themobile device 104 is received, a particular geographic location associated with the mobile device 104 (or the user of the mobile device 104) is determined. The ASRengine 108 transcribes the utterance using the geo-specific noise model that matches, or that is otherwise suitable for, the particular geographic location, and one ormore candidate transcriptions 146 are communicated from theASR engine 108 to theserver 106. Where theserver 106 is a search engine, theserver 106 executes one or more search queries using thecandidate transcriptions 146, generatessearch results 152, and communicates thesearch results 152 to themobile device 104 for display. - In more detail, during state (a), the mobile devices 102 communicate geotagged audio signals 130 that include environmental audio (referred to by this specification as "environmental audio signals") to the
ASR engine 108 over thenetworks 110. In general, environmental audio may include any ambient sounds that occur (naturally or otherwise) at a particular location. Environmental audio typically excludes the sounds, utterances, or voice of the user of the mobile device. - The
device 102a communicates anaudio signal 130a that has been tagged withmetadata 132a that references "Location A," thedevice 102b communicates anaudio signal 130b that has been tagged withmetadata 132b that references "Location B," and thedevice 102c communicates anaudio signal 130c that has been tagged withmetadata 132c that also references "Location B." The metadata 132 may be associated with the audio signals 130 by mobile devices 102, as illustrated, or the metadata may be associated with the audio signals 130 by theASR engine 108 or by another server after inferring a location of a mobile device 102 (or of the user of the mobile device 102). - The environmental audio signals 130 may each include a two-second (or more) snippet of relatively high quality audio, such as sixteen kilohertz lossless audio signals. The environmental audio signals 130 may be associated with metadata that references the geographic location of the respective mobile device 102 when the environmental audio was recorded, captured or otherwise obtained.
- The environmental audio signals 130 may be manually uploaded from the mobile devices 102 to the ASR
engine 108. For instance, environmental audio signals 130 may be generated and communicated in conjunction with the generation and communication of images to a public image database or repository. Alternatively, for users who opt to participate, environmental audio signals 130 may be automatically obtained and communicated from the mobile devices 102 to the ASRengine 108 without requiring an explicit, user actuation before each environmental audio signal is communicated to theASR engine 108. - The metadata 132 may describe locations in any number of different formats or levels of detail or granularity. For example, the
metadata 132a may include a latitude and longitude associated with the then-present location of themobile device 102a, and themetadata 132c may include an address or geographic region associated with the then-present location of themobile device 102c. Furthermore, since themobile device 102b is illustrated as being in a moving vehicle, themetadata 132b may describe a path of the vehicle (e.g., including a start point and an end point, and motion data). Additionally, the metadata 132 may describe locations in terms of location type (e.g., "moving vehicle," "on a beach," "in a restaurant," "in tall building," "South Asia," "rural area," "someplace with construction noise," "amusement park," "on a boat," "indoors," "underground," "on a street," "forest"). A single audio signal may be associated with metadata that describes one or more locations. - The geographic location associated with the audio signal 138 may instead be described in terms of a bounded area, expressed as a set of coordinates that define the bounded arrea. Alternatively, the geographic location may be defined using a region identifier, such as a state name or identifier, city name, idiomatic name (e.g., "Central Park"), a country name, or the identifier of arbitrarily defined region (e.g., "cell/region ABC123").
- Before associating a location with the environmental audio signal, the mobile devices 102 or the
ASR engine 108 may process the metadata to adjust the level of detail of the location information (e.g., to determine a state associated with a particular set of coordinates), or the location information may be discretized (e.g., by selecting a specific point along the path, or a region associated with the path). The level of detail of the metadata may also be adjusted by specifying or adding location type metadata, for example by adding an "on the beach" tag to an environmental audio signal whose associated geographic coordinates are associated with a beach location, or by adding a "someplace with lots of people" tag to an environmental audio signal that includes the sounds of multiple people talking in the background. - During state (b), the
ASR engine 108 receives the geotagged environmental audio signals 130 from the mobile devices 102, and stores the geotagged audio signals (or portions thereof) in thecollection 114 of environmental audio signals, in thedata store 111. As described below, the collection is used for training, adapting, or otherwise generating one or more geographic location-specific (or "geo-specific")noise models 112. - Because environmental audio signals in the
collection 114 should not include users' voices, theASR engine 108 may use a voice activity detector to verify that thecollection 114 of environmental audio signals only includes audio signals 130 that correspond to ambient noise, or to filter out or otherwise identify or exclude audio signals 130 (or portions of the audio signals 130) that include voices of the various users of the mobile devices 102. - The
collection 114 of the ambient audio signals stored by theASR engine 108 may include hundreds, thousands, millions, or hundreds of millions of environmental audio signals. In the illustrated example, a portion or all of the geo-taggedenvironmental audio signal 130a may be stored in thecollection 114 as theenvironmental audio signal 124, a portion or all of the geo-taggedenvironmental audio signal 130b may be stored in thecollection 114 as theenvironmental audio signal 126a, and a portion or all of the geotaggedenvironmental audio signal 130c may be stored in thecollection 114 as theenvironmental audio signal 120b. - Storing an environmental audio signal 130 in the collection may include determining whether a user's voice is encoded in the audio signal 130, and determining to store or determining not to store the environmental audio signal 130 in the collection based on determining that the user's voice is or is not encoded in the audio signal 130, respectively. Alternatively, storing an environmental audio signal in the collection may include identifying a portion of the environmental audio signal 130 that includes the user's voice, altering the environmental audio signal 130 by removing the portion that includes the user's voice or by associating metadata which references the portion that includes the user's voice, and storing the altered environmental audio signal 130 in the collection.
- Other context data or metadata associated with the environmental audio signals 130 may be stored in the
collection 114 as well. For example, the environmental audio signals included in thecollection 114 can, in some implementations, include other metadata tags, such as tags that indicate whether background voices (e.g., cafeteria chatter) are present within the environmental audio, tags that identify the date on which a particular environmental audio signal was obtained (e.g., used to determine a sample age), or tags that identify whether a particular environmental audio signal deviates in some way from other environmental audio signals of the collection that were obtained in the same or similar location. In this manner, thecollection 114 of environmental audio signals may optionally be filtered to exclude particular environmental audio signals that satisfy or that do not satisfy particular criteria, such as to exclude particular environmental audio signals that are older than a certain age, or that include background chatter that may identify an individual or otherwise be proprietary or private in nature. - In an additional example, data referencing whether the environmental audio signals of the
collection 114 were manually or automatically uploaded may be tagged in metadata associated with the environmental audio signals. For example, some of thenoise models 112 may be generated using only those environmental audio signals that were automatically uploaded, or that were manually uploaded, or different weightings may be assigned to each category of upload during the generating of the noise models. - Although the environmental audio signals of the
collection 114 have been described as including an explicit tag that identifies a respective geographic location, in other implementations, such as where the association between an audio signal and a geographic location may be derived, the explicit use of a tag is not required. For example, a geographic location may be implicitly associated with an environmental audio signal by processing search logs (e.g., stored with the server 106) to determine geographic location information for a particular environmental audio signal. Accordingly, receipt of a geo-tagged environmental audio signals by theASR engine 108 may include obtaining an environmental audio signal that does not expressly include a geo-tag, and deriving and associating one or more geo-tags for the environmental audio signal. - During state (c), an audio signal 138 is communicated from the
mobile device 104 to theASR engine 108 over thenetworks 110. Although the mobile device 102 is illustrated as being different a different device than themobile devices 104, in other implementations the audio signal 138 is communicated from one of themobile devices 104 that provided an geo-tagged environmental audio signal 130. - The audio signal 138 includes an utterance 140 ("Gym New York") recorded by the mobile device 104 (e.g., when the user implicitly or explicitly initiates a voice search query). The audio signal 138 includes
metadata 139 that references the geographic location "Location B." In addition to including theutterance 140, the audio signal 138 may also include a snippet of environmental audio, such as a two second snippet of environmental audio that was recorded before or after theutterance 140 was spoken. While theutterance 140 is described an illustrated inFIG. 1 as a voice query, in other example implementations the utterance may be an voice input to dictation system or to a dialog system. - The geographic location ("Location B") associated with the audio signal 138 may be defined using a same or different level of detail as the geographic locations associated with the environmental audio signals included in the
collection 114. For example, the geographic locations associated with the environmental audio signals included in thecollection 114 may correspond to geographic regions, while the geographic location associated with the audio signal 138 may correspond to a particular geographic coordinate. Where the level of detail is different, theASR engine 108 may process thegeographic metadata 139 or the metadata associated with the environmental audio signals of thecollection 114 to align the level of detail, so that a subset selection process can be performed. - The
metadata 139 may be associated with the audio signal 138 by the mobile device 104 (or the user of the mobile device 104) based on location information that is current when theutterance 140 is recorded, and may be communicated with the audio signal 138 from themobile device 104 to theASR engine 108. Alternatively, the metadata may be associated with the audio signal 138 by theASR engine 108, based on a geographic location that theASR engine 108 infers for the mobile device 104 (or the user of the mobile device 104). - The
ASR engine 108 may infer the geographic location using the user's calendar schedule, user preferences (e.g., as stored in a user account of theASR engine 108 or theserver 106, or as communicated from the mobile device 104), a default location, a past location (e.g., the most recent location calculated by a GPS module of the mobile device 104), information explicitly provided by the user when submitting the voice search query, from theutterances 104 themselves, triangulation (e.g., WiFi or cell tower triangulation), a GPS module in themobile device 104, or dead reckoning. Themetadata 139 may include accuracy information that specifies an accuracy of the geographic location determination, signifying a likelihood that themobile device 104 was actually in the particular geographic location specified by themetadata 139 at the time when theutterance 140 was recorded. - Other metadata may also be included with the audio signal 138. For example, metadata included with the audio signals may include a location or locale associated with the respective mobile device 102. For example, the locale information may describe, among other selectable parameters, a region in which the mobile device 102 is registered, or the language or dialect of the user of the mobile device 102. The
speech recognition module 118 may use this information to select, train, adapt, or otherwise generate noise, speech, acoustic, popularity, or other models that match the context of themobile device 104. - In state (d), the
ASR engine 108 selects a subset of the environmental audio signals in thecollection 114, and uses a noisemodel generating module 116 to train, adapt, or otherwise generate one or more noise models 112 (e.g., Gaussian Mixture Models (GMMs)) using the subset of the environmental audio signals, for example by using the subset of the environmental audio signals as a training set for the noise model. The subset may include all, or fewer than all of the environmental audio signals in thecollection 114. - In general, the
noise models 112, along with speech models, acoustic models, popularity models, and/or other models, are applied to the audio signal 138 to translate or transcribe the spokenutterance 140 into one or more textual,candidate transcriptions 146, and to generate speech recognition confidence scores to the candidate transcriptions. The noise models, in particular, are used for noise suppression or noise compensation, to enhance the intelligibility of the spokenutterance 140 to theASR engine 108. - In more detail, the noise
model generating module 116 may generate anoise model 120b for the geographic location ("Location B") associated with the audio signal 138 using thecollection 114 of audio signals, specifically theenvironmental audio signals environmental audio signals model generating module 116 may generate anoise model 120a for another geographic location ("Location A"), using theenvironmental audio signal 124 that was geotagged as having been recorded at or near that other geographic location, or at a same or similar type of location. If the noisemodel generating module 116 is configured to select environmental audio signal that were geotagged as having been recorded near the geographic location associated with the audio signal 138, and if "Location A" is near "Location B," the noisemodel generating module 116 may generate anoise model 120b for "Location B" also using theenvironmental audio signal 124. - In addition to the geotagged location, other context data associated with the environmental audio signals of the
collection 114 may be used to select the subset of the environmental audio signals to use to generate thenoise models 112, or to adjust a weight or effect that a particular audio signal is to have upon the generation . For example, theASR engine 108 may select a subset of the environmental audio signals in thecollection 114 whose contextual information indicates that they are longer than or shorter than a predetermined period of time, or that they satisfy certain quality or recency criteria. Furthermore, theASR engine 108 may select, as the subset, environmental audio signals in thecollection 114 whose contextual information indicates that they were recorded using a mobile device that has a similar audio subsystem as themobile device 104. - Other context data which may be used to select the subset of the environmental audio signals from the
collection 114 may include, in some examples, the time information, date information, data referencing a speed or an amount of motion measured by the particular mobile device during recording, other device sensor data, device state data (e.g., Bluetooth headset, speaker phone, or traditional input method), a user identifier if the user opts to provide one, or information identifying the type or model of mobile device. The context data, for example, may provide an indication of conditions surrounding the recording of the audio signal 138. - In one example, context data supplied with the audio signal 138 by the
mobile device 104 may indicate that themobile device 104 is traveling at highway speeds along a path associated with a highway. TheASR 108 may infer that the audio signal 138 was recorded within a vehicle, and may select a subset of the environmental audio signals in thecollection 114 that are associated with an "inside moving vehicle" location type. In another example, context data supplied with the audio signal 138 by themobile device 104 may indicate that themobile device 104 is in a rural area, and that theutterance 140 was recorded on a Sunday at 6:00 am. Based on this context data, theASR 108 may infer that it accuracy of the speech recognition would not be improved if the subset included environmental audio signals that were recorded in urban areas during rush hour. Accordingly, the context data may be used by the noisemodel generating module 116 to filter thecollection 114 of environmental audio signals when generatingnoise models 112, or by thespeech recognition module 118 to select anappropriate noise model 112 for a particular utterance. - In some implementations, the noise
model generating module 116 may select a weighted combination of the environmental audio signals of thecollection 114 based upon the proximity of the geographic locations associated with the audio signals to the geographic location associated with the audio signal 138. The noisemodel generating module 116 may also generate thenoise models 112 using environmental audio included in the audio signal 138 itself, for example environmental audio recorded before or after the utterances were spoken, or during pauses between utterances. - For instance, the noise
model generating module 116 can first determine the quality of the environmental audio signals stored in thecollection 114 relative to the quality of the environmental audio included in the audio signal 138, and can choose to generate a noise model using the audio signals stored in thecollection 114 only, using the environmental audio included in the audio signal 138 only, or any appropriate weighted or unweighted combination thereof. For instance, the noisemodel generating module 116 may determine that the audio signal 138 includes an insignificant amount of environmental audio, or that high quality environmental audio is stored for that particular geographic location in thecollection 114, and may choose to generate the noise model without using (or giving little weight to) the environmental audio included in the audio signal 138. - In some implementations, the noise
model generating module 116 selects, as the subset, the environmental audio signals from thecollection 114 that are associated with the N (e.g., five, twenty, or fifty) closest geographic locations to the geographic location associated with the audio signal 138. When the geographic location associated with the audio signal 138 describes a point or a place (e.g., coordinates), a geometric shape (e.g., a circle or square) may be defined relative to that that geographic location, and the noisemodel generating module 116 may select, as the subset, audio signals from thecollection 114 that are associated with geographic regions that are wholly or partially located within the defined geometric shape. - If the geographic location associated with the audio signal 138 has been defined in terms of a location type (i.e., "on the beach," "city"), and
ASR engine 108 may select environmental audio signals that are associated with a same or a similar location type, even if the physical geographic locations associated with the selected audio signals are not physically near the geographic location associated with the audio signal 138. For instance, a noise model for an audio signal that was recorded on the beach in Florida may be tagged with "on the beach" metadata, and the noisemodel generating module 116 may select, as the subset, environmental audio signals from thecollection 114 whose associated metadata indicate that they were also recorded on beaches, despite the fact that they were recorded on beaches in Australia, Hawaii, or in Iceland. - The noise
model generating module 116 may revert to selecting the subset based on matching location types, instead of matching actual, physical geographic locations, if the geographic location associated with the audio signal 138 does not match (or does not have a high quality match) with any physical geographic location associated with an environmental audio signal of thecollection 114. Other matching processes, such as clustering algorithms, may be used to match audio signals with environmental audio signals. - In addition to generating general, geo-
specific noise models 112, the noisemodel generating module 116 may generate geo-specific noise models that are targeted or specific to other criteria as well, such as geo-specific noise models that are specific to different device types or times of day. A targeted sub-model may be generated based upon detecting that a threshold criterion has been satisfied, such as determining that a threshold number of environmental audio signals of thecollection 114 refer to the same geographic location, and share another same or similar context (e.g., time of day, day of the week, motion characteristics, device type, etc.). - The
noise models 112 may be generated before, during, or after theutterance 140 has been received. For example, multiple environmental audio signals, incoming from a same or similar location as theutterance 140, may be processed in parallel with the processing of the utterance, and may be used to generatenoise models 112 in real time or near real time, to better approximate the live noise conditions surrounding themobile device 104. - In state (e), the
speech recognition module 118 of theASR engine 108 performs noise compensation on the audio signal 138 using the geo-specific noise model 120b for the geographic location associated with the audio signal 138, to enhance the accuracy of the speech recognition, and subsequently performs the speech recognition on the noise-compensated audio signal. When the audio signal 138 includes metadata that describes a device type of themobile device 104, theASR engine 108 may apply anoise model 122 that is specific to both the geographic location associated with the audio signal, and to the device type of themobile device 104. Thespeech recognition module 118 may generate one ormore candidate transcriptions 146 that match the utterance encoded in the audio signal 138, and speech recognition confidence values for the candidate transcriptions. - During state (f), one or more of the
candidate transcriptions 146 generated by thespeech recognition module 118 are communicated from theASR engine 108 to theserver 106. Where theserver 106 is a search engine, the candidate transcriptions may be used as candidate query terms, to execute one or more search queries. TheASR engine 108 may rank thecandidate transcriptions 146 by their respective speech recognition confidence scores before transmitting them to theserver 106. By transcribing spoken utterances and providing candidate transcriptions to theserver 106, theASR engine 108 may provide a voice search query capability, a dictation capability, or a dialogue system capability to themobile device 104. - The
server 106 may execute one or more search queries using the candidate query terms, generates afile 152 that references search results 160. Theserver 106, in some examples, may include a web search engine used to find references within the Internet, a phone book type search engine used to find businesses or individuals, or another specialized search engine (e.g., a search engine that provides references to entertainment listings such as restaurants and movie theater information, medical and pharmaceutical information, etc.). - During state (h), the
server 106 provides thefile 152 that references the search results 160 to themobile device 104. Thefile 152 may be a markup language file, such as an extensible Markup Language (XML) or HyperText Markup Language (HTML) file. - During state (i), the
mobile device 104 displays the search results 160 on auser interface 158. Specifically, the user interface includes asearch box 157 that displays the candidate query term with the highest speech recognition confidence score ("Gym New York"), an alternate queryterm suggestion region 159 that displays another of the candidate query term that may have been intended by the utterance 140 ("Jim Newark"), asearch result 160a that includes a link to a resource for "New York Fitness" 160a, and a search result 160b that includes a link to a resource for "Manhattan Body Building" 160b. Thesearch result 160a may further include a phone number link that, when selected, may be dialed by themobile device 104. -
FIG. 2 is a flowchart of an example of aprocess 200. Briefly, theprocess 200 includes receiving one or more geotagged environmental audio signals, receiving an utterance associated with a geographic location, and generating a noise model based in part upon the geographic location. Noise compensation may be performed on the audio signal, with the noise model contributing to improving an the accuracy of speech recognition. - In more detail, when
process 200 begins, a geotagged audio signal corresponding to environmental audio is received (202). The geotagged audio signal may be recorded by a mobile device in a particular geographic location. The geotagged audio signal may include associated context data such as a time, date, speed, or amount of motion measured during the recording of the geotagged audio signal or a type of device which recorded the geotagged audio signal. The received geotagged audio signal may be processed to exclude portions of the environmental audio that include a voice of a user of the mobile device. Multiple geotagged audio signals recorded in one or more geographic locations may be received and stored. - An utterance recorded by a particular mobile device is received (204). The utterance may include a voice search query, or may be an input to a dictation or dialog application or system. The utterance may include associated context data such as a time, date, speed, or amount of motion measured during the recording of the geotagged audio signal or a type of device which recorded the geotagged audio signal.
- A particular geographic location associated with the mobile device is determined (206). For example, data referencing the particular geographic location may be received from the mobile device, or a past geographic location or a default geographic location associated with the mobile device may be determined.
- A noise model is generated for the particular geographic location using a subset of geotagged audio signals (208). The subset of geotagged audio signals may be selected by determining, for each of the geotagged audio signals, a distance between the particular geographic location and a geographic location associated the geotagged audio signal; and selecting those geotagged audio signals which are within a predetermined distance of the particular geographic location, or that are associated with geographic locations which are among the N closest geographic locations to the particular geographic location.
- The subset of geotagged audio signals may be selected by identifying the geotagged audio signals associated with the particular geographic location, and/or by identifying the geotagged audio signals that are acoustically similar to the utterance. The subset of geotagged audio signals may be selected based both on the particular geographic location and on context data associated with the utterance.
- Generating the noise model may include training a GMM using the subset of geotagged audio signals as a training set. Some noise reduction or separation algorithms, such as non-negative matrix factorization (NMF), can use the feature vectors themselves, not averages that are represented by the Gaussian components. Other algorithms, such as Alqonquin, can use either GMMs or the feature vectors themselves, with artificial variances.
- Noise compensation is performed on the audio signal that corresponds to the utterance, using the noise model that has been generated for the particular geographic location, to enhance the audio signal or otherwise take decrease the uncertainty of the utterance due to noise (210).
- Speech recognition is performed on the noise-compensated audio signal (212). Performing the speech recognition may include generating one or more candidate transcriptions of the utterance. A search query may be executed using the one or more candidate transcriptions, or one or more of the candidate transcriptions can be provided as an output of a digital dictation application. Alternatively, one or more of the candidate transcriptions may be provided as an input to a dialog system, to allow a computer system to converse with the user of the particular mobile device.
-
FIG. 3 is a flowchart of an example of a process 300. Briefly, the process 300 includes collecting geotagged audio signals and generating multiple noise models based, in part, upon particular geographic locations associated with each of the geotagged audio signals. One or more of these noise models may be selected when performing speech recognition upon an utterance based, in part, upon a geographic location associated with the utterance. - In more detail, when process 300 begins, a geotagged audio signal corresponding to environmental audio is received (302). The geotagged audio signal may be recorded by a mobile device in a particular geographic location. The received geotagged audio signal may be processed to exclude portions of the environmental audio that include the voice of the user of the mobile device. Multiple geotagged audio signals recorded in one or more geographic locations may be received and stored.
- Optionally, context data associated with the geotagged audio signal is received (304). The geotagged audio signal may include associated context data such as a time, date, speed, or amount of motion measured during the recording of the geotagged audio signal or a type of device which recorded the geotagged audio signal.
- One or more noise models are generated (306). Each noise model may be generated for a particular geographic location or, optionally, a location type, using a subset of geotagged audio signals. The subset of geotagged audio signals may be selected by determining, for each of the geotagged audio signals, a distance between the particular geographic location and a geographic location associated the geotagged audio signal and selecting those geotagged audio signals which are within a predetermined distance of the particular geographic location, or that are associated with geographic locations which are among the N closest geographic locations to the particular geographic location. The subset of geotagged audio signals may be selected by identifying the geotagged audio signals associated with the particular geographic location. The subset of geotagged audio signals may be selected based both on the particular geographic location and on context data associated with the geotagged audio signals. Generating the noise model may include training a Gaussian Mixture Model (GMM) using the subset of geotagged audio signals.
- An utterance recorded by a particular mobile device is received (308). The utterance may include a voice search query. The utterance may include associated context data such as a time, date, speed, or amount of motion measured during the recording of the geotagged audio signal or a type of device which recorded the geotagged audio signal.
- A geographic location is detected (310). For example, data referencing the particular geographic location may be received from a GPS module of the mobile device.
- A noise model is selected (312). The noise model may be selected from among multiple noise models generated for multiple geographic locations. Context data may optionally contribute to selection of a particular noise model among multiple noise models for the particular geographic location.
- Speech recognition is performed on the utterance using the selected noise model (314). Performing the speech recognition may include generating one or more candidate transcriptions of the utterance. A search query may be executed using the one or more candidate transcriptions.
-
FIG. 4 shows a swim lane diagram of an example of aprocess 400 for enhancing speech recognition accuracy using geotagged environmental audio. Theprocess 400 may be implemented by amobile device 402, anASR engine 404, and asearch engine 406. Themobile device 402 may provide audio signals, such as environmental audio signals or audio signals that correspond to an utterance, to theASR engine 404. Although only onemobile device 402 is illustrated, themobile device 402 may represent a large quantity ofmobile devices 402 contributing environmental audio signals and voice queries to theprocess 400. TheASR engine 404 may generate noise models based upon the environmental audio signals, and may apply one or more noise models to an incoming voice search query when performing speech recognition. TheASR engine 404 may provide transcriptions of utterances within a voice search query to thesearch engine 406 to complete the voice search query request. - The
process 400 begins with themobile device 402 providing 408 a geotagged audio signal to theASR engine 404. The audio signal may include environmental audio along with an indication regarding the location at which the environmental audio was recorded. Optionally, the geotagged audio signal may include context data, for example in the form of metadata. TheASR engine 404 may store the geotagged audio signal in an environmental audio data store. - The
mobile device 402 provides 410 an utterance to theASR engine 404. The utterance, for example, may include a voice search query. The recording of the utterance may optionally include a sample of environmental audio, for example recorded briefly before or after the recording of the utterance. - The
mobile device 402 provides 412 a geographic location to theASR engine 404. The mobile device, in some examples, may provide navigational coordinates detected using a GPS module, a most recent (but not necessarily concurrent with recording) GPS reading, a default location, a location derived from the utterance previously provided, or a location estimated through dead reckoning or triangulation of transmission towers. Themobile device 402 may optionally provide context data, such as sensor data, device model identification, or device settings, to theASR engine 404. - The
ASR engine 404 generates 414 a noise model. The noise model may be generated, in part, by training a GMM. The noise model may be generated based upon the geographic location provided by themobile device 402. For example, geotagged audio signals submitted from a location at or near the location of themobile device 402 may contribute to a noise model. Optionally, context data provided by themobile device 402 may be used to filter geotagged audio signals to select those most appropriate to the conditions in which the utterances were recorded. For example, the geotagged audio signals near the geographic location provided by themobile device 402 may be filtered by a day of the week or a time of day. If a sample of environmental audio was included with the utterance provided by themobile device 402, the environmental audio sample may optionally be included in the noise model. - The
ASR engine 404 performsspeech recognition 416 upon the provided utterance. Using the noise model generated by theASR engine 404, the utterance provided by themobile device 402 may be transcribed into one or more sets of query terms. - The
ASR engine 404forwards 418 the generated transcription(s) to thesearch engine 406. If theASR engine 404 generated more than one transcription, the transcriptions may optionally be ranked in order of confidence. TheASR engine 404 may optionally provide context data to thesearch engine 406, such as the geographic location, which thesearch engine 406 may use to filter or rank search results. - The
search engine 406 performs 420 a search operation using the transcription(s). Thesearch engine 406 may locate one or more URIs related to the transcription term(s). - The
search engine 406 provides 422 search query results to themobile device 402. For example, thesearch engine 406 may forward HTML code which generates a visual listing of the URI(s) located. - A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. For example, various forms of the flows shown above may be used, with steps re-ordered, added, or removed. Accordingly, other implementations are within the scope of the following claims.
- Embodiments and all of the functional operations described in this specification may be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments may be implemented as one or more computer program products, i.e., one or more modules of computer program instructions encoded on a computer readable medium for execution by, or to control the operation of, data processing apparatus. The computer readable medium may be a machine-readable storage device, a machine-readable storage substrate, a memory device, a composition of matter effecting a machine-readable propagated signal, or a combination of one or more of them. The term "data processing apparatus" encompasses all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus may include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them. A propagated signal is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.
- A computer program (also known as a program, software, software application, script, or code) may be written in any form of programming language, including compiled or interpreted languages, and it may be deployed in any form, including as a stand alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program may be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program may be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.
- The processes and logic flows described in this specification may be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows may also be performed by, and apparatus may also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer may be embedded in another device, e.g., a tablet computer, a mobile telephone, a personal digital assistant (PDA), a mobile audio player, a Global Positioning System (GPS) receiver, to name just a few. Computer readable media suitable for storing computer program instructions and data include all forms of non volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks. The processor and the memory may be supplemented by, or incorporated in, special purpose logic circuitry.
- To provide for interaction with a user, embodiments may be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user may provide input to the computer. Other kinds of devices may be used to provide for interaction with a user as well; for example, feedback provided to the user may be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input.
- Embodiments may be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user may interact with an implementation, or any combination of one or more such back end, middleware, or front end components. The components of the system may be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network ("LAN") and a wide area network ("WAN"), e.g., the Internet.
- The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- While this specification contains many specifics, these should not be construed as limitations on the scope of the disclosure or of what may be claimed, but rather as descriptions of features specific to particular embodiments. Certain features that are described in this specification in the context of separate embodiments may also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment may also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination may in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.
- Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems may generally be integrated together in a single software product or packaged into multiple software products.
- In each instance where an HTML file is mentioned, other file types or formats may be substituted. For instance, an HTML file may be replaced by an XML, JSON, plain text, or other types of files. Moreover, where a table or hash table is mentioned, other data structures (such as spreadsheets, relational databases, or structured files) may be used.
- Thus, particular embodiments have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims may be performed in a different order and still achieve desirable results.
- Although the present invention is defined in the attached claims, it is to be understood that the invention can alternatively also be defined in accordance with the following embodiments:
-
- 1. A system comprising:
- one or more computers; and
- a computer-readable medium coupled to the one or more computers having instructions stored thereon which, when executed by the one or more computers, cause the one or more computers to perform operations comprising:
- receiving geotagged audio signals that correspond to environmental audio recorded by multiple mobile devices in multiple geographic locations,
- receiving an audio signal that corresponds to an utterance recorded by a particular mobile device,
- determining a particular geographic location associated with the particular mobile device,
- generating a noise model for the particular geographic location using a subset of the geotagged audio signals, and
- performing noise compensation on the audio signal that corresponds to the utterance using the noise model that has been generated for the particular geographic location.
- 2. The system of
embodiment 1, wherein the operations further comprise performing speech recognition on the utterance using the noise-compensated audio signal. - 3. The system of
embodiment 1, wherein generating the noise model further comprises generating the noise model before receiving the audio signal that corresponds to the utterance. - 4. The system of
embodiment 1, wherein generating the noise model further comprises generating the noise model after receiving the audio signal that corresponds to the utterance. - 5. The system of
embodiment 1, wherein the operations further comprise:- determining, for each of the geotagged audio signals, a distance between the particular geographic location and a geographic location associated the geotagged audio signal; and
- selecting, as the subset of the geotagged audio signals, the geotagged audio signals that are associated with geographic locations which are within a predetermined distance of the particular geographic location, or that are associated with geographic locations which are among the N closest geographic locations to the particular geographic location.
- 6. The system of
embodiment 1, wherein the operations further comprise:- selecting, as the subset of the geotagged audio signals, the geotagged audio signals that are associated with the particular geographic location.
- 7. The system of
embodiment 1, wherein the operations further comprise selecting the subset of the geotagged audio signals based on the particular geographic location, and based on context data associated with the utterance. - 8. The system of embodiment 6, wherein the context data comprises data that references a time or a date when the utterance was recorded by the mobile device, data that references a speed or an amount of motion measured by the particular mobile device when the utterance was recorded, data that references settings of the mobile device, or data that references a type of the mobile device.
- 9. The system of
embodiment 1, wherein the utterance represents a voice search query, or an input to a digital dictation application or a dialog system. - 10. The system of
embodiment 1, wherein determining the particular geographic location further comprises receiving data referencing the particular geographic location from the mobile device. - 11. The system of
embodiment 1, wherein determining the particular geographic location further comprises determining a past geographic location or a default geographic location associated with the device. - 12. The system of
embodiment 1, wherein generating the noise model comprises training a Gaussian Mixture Model (GMM) using the subset of the geotagged audio signals as a training set. - 13. The system of
embodiment 1, wherein the operations further comprise:- generating one or more candidate transcriptions of the utterance; and
- executing a search query using the one or more candidate transcriptions.
- 14. The system of
embodiment 1, wherein the operations further comprise:- processing the received geotagged audio signals to exclude portions of the environmental audio that include voices of users of the multiple mobile devices.
- 15. The system of
embodiment 1, wherein the operations further comprise selecting the noise model generated for the particular geographic location from among multiple noise models generated for the multiple geographic locations. - 16. The system of embodiment 14, wherein:
- the operations further comprise:
- defining an area surrounding the particular geographic location,
- selecting a plurality of noise models associated with geographic locations within the area from among the multiple noise models, and
- generating a weighted combination of the selected noise models; and
- the noise compensation is performed using the weighted combination of selected noise models.
- the operations further comprise:
- 17. The system of
embodiment 1, wherein generating the noise model further comprises generating the noise model for the particular geographic location using the subset of the geotagged audio signals and using an environmental audio portion of the audio signal that corresponds to the utterance. - 18. The system of
embodiment 1, wherein the operations further comprise:- defining an area surrounding the particular geographic location; and
- 19. A computer storage medium encoded with a computer program, the program comprising instructions that when executed by one or more computers cause the one or more computers to perform operations comprising:
- receiving geotagged audio signals that correspond to environmental audio recorded by multiple mobile devices in multiple geographic locations;
- receiving an audio signal that corresponds to an utterance recorded by a particular mobile device;
- determining a particular geographic location associated with the particular mobile device;
- generating a noise model for the particular geographic location using a subset of the geotagged audio signals; and
- performing noise compensation on the audio signal that corresponds to the utterance using the noise model that has been generated for the particular geographic location.
- 20. A computer-implemented method comprising:
- receiving geotagged audio signals that correspond to environmental audio recorded by multiple mobile devices in multiple geographic locations;
- receiving an audio signal that corresponds to an utterance recorded by a particular mobile device;
- determining a particular geographic location associated with the particular mobile device;
- generating a noise model for the particular geographic location using a subset of the geotagged audio signals; and
- performing noise compensation on the audio signal that corresponds to the utterance using the noise model that has been generated for the particular geographic location.
- A number of implementations have been described. Other implementations are within the scope of the following claims:
Claims (15)
- A system comprising:one or more computers; anda computer-readable medium coupled to the one or more computers having instructions stored thereon which, when executed by the one or more computers, cause the one or more computers to perform operations comprising:receiving geotagged audio signals that correspond to environmental audio recorded by multiple mobile devices in multiple geographic locations,receiving an audio signal that corresponds to an utterance recorded by a particular mobile device,determining a particular geographic location associated with the particular mobile device,generating a noise model for the particular geographic location using a subset of the geotagged audio signals, wherein the noise model generated for the particular geographic location is based on multiple noise models generated for the multiple geographic locations, andperforming noise compensation on the audio signal that corresponds to the utterance using the noise model that has been generated for the particular geographic location.
- The system of claim 1, wherein the operations further comprise performing speech recognition on the utterance using the noise-compensated audio signal.
- The system of claim 1, wherein generating the noise model further comprises generating the noise model before receiving the audio signal that corresponds to the utterance.
- The system of claim 1, wherein generating the noise model further comprises generating the noise model after receiving the audio signal that corresponds to the utterance.
- The system of claim 1, wherein the operations further comprise selecting the subset of the geotagged audio signals based on the particular geographic location, and based on context data associated with the utterance, wherein the context data comprises data that references a time or a date when the utterance was recorded by the mobile device, data that references a speed or an amount of motion measured by the particular mobile device when the utterance was recorded, data that references settings of the mobile device, or data that references a type of the mobile device.
- The system of claim 1, wherein determining the particular geographic location further comprises receiving data referencing the particular geographic location from the mobile device.
- The system of claim 1, wherein determining the particular geographic location further comprises determining a past geographic location or a default geographic location associated with the device.
- The system of claim 1, wherein generating the noise model comprises training a Gaussian Mixture Model (GMM) using the subset of the geotagged audio signals as a training set.
- The system of claim 1, wherein the operations further comprise:generating one or more candidate transcriptions of the utterance; andexecuting a search query using the one or more candidate transcriptions.
- The system of claim 1, wherein the operations further comprise:processing the received geotagged audio signals to exclude portions of the environmental audio that include voices of users of the multiple mobile devices.
- The system of claim 1, wherein:the operations further comprise:defining an area surrounding the particular geographic location,selecting a plurality of noise models associated with geographic locations within the area from among the multiple noise models, andgenerating a weighted combination of the selected noise models; andthe noise compensation is performed using the weighted combination of selected noise models.
- The system of claim 1, wherein generating the noise model further comprises generating the noise model for the particular geographic location using the subset of the geotagged audio signals and using an environmental audio portion of the audio signal that corresponds to the utterance.
- The system of claim 1, wherein the operations further comprise:defining an area surrounding the particular geographic location; andselecting, as the subset of the geotagged audio signals, the geotagged audio signals recorded within the area.
- A computer storage medium encoded with a computer program, the program comprising instructions that when executed by one or more computers cause the one or more computers to perform operations comprising:receiving geotagged audio signals that correspond to environmental audio recorded by multiple mobile devices in multiple geographic locations,receiving an audio signal that corresponds to an utterance recorded by a particular mobile device,determining a particular geographic location associated with the particular mobile device,generating a noise model for the particular geographic location using a subset of the geotagged audio signals, wherein the noise model generated for the particular geographic location is based on multiple noise models generated for the multiple geographic locations, andperforming noise compensation on the audio signal that corresponds to the utterance using the noise model that has been generated for the particular geographic location.
- A computer-implemented method comprising:receiving geotagged audio signals that correspond to environmental audio recorded by multiple mobile devices in multiple geographic locations,receiving an audio signal that corresponds to an utterance recorded by a particular mobile device,determining a particular geographic location associated with the particular mobile device,generating a noise model for the particular geographic location using a subset of the geotagged audio signals, wherein the noise model generated for the particular geographic location is based on multiple noise models generated for the multiple geographic locations, andperforming noise compensation on the audio signal that corresponds to the utterance using the noise model that has been generated for the particular geographic location.
Priority Applications (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
EP18188692.0A EP3425634B1 (en) | 2010-04-14 | 2011-03-22 | Noise compensation using geotagged audio signals |
EP21189184.1A EP3923281B1 (en) | 2010-04-14 | 2011-03-22 | Noise compensation using geotagged audio signals |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/760,147 US8265928B2 (en) | 2010-04-14 | 2010-04-14 | Geotagged environmental audio for enhanced speech recognition accuracy |
PCT/US2011/029407 WO2011129954A1 (en) | 2010-04-14 | 2011-03-22 | Geotagged environmental audio for enhanced speech recognition accuracy |
EP11713118.5A EP2559031B1 (en) | 2010-04-14 | 2011-03-22 | Noise compensation using geotagged audio signals |
Related Parent Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP11713118.5A Division EP2559031B1 (en) | 2010-04-14 | 2011-03-22 | Noise compensation using geotagged audio signals |
EP11713118.5A Division-Into EP2559031B1 (en) | 2010-04-14 | 2011-03-22 | Noise compensation using geotagged audio signals |
Related Child Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP21189184.1A Division EP3923281B1 (en) | 2010-04-14 | 2011-03-22 | Noise compensation using geotagged audio signals |
EP18188692.0A Division EP3425634B1 (en) | 2010-04-14 | 2011-03-22 | Noise compensation using geotagged audio signals |
Publications (2)
Publication Number | Publication Date |
---|---|
EP2750133A1 true EP2750133A1 (en) | 2014-07-02 |
EP2750133B1 EP2750133B1 (en) | 2018-08-29 |
Family
ID=44041584
Family Applications (4)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP14162078.1A Active EP2750133B1 (en) | 2010-04-14 | 2011-03-22 | Noise compensation using geotagged audio signals |
EP11713118.5A Active EP2559031B1 (en) | 2010-04-14 | 2011-03-22 | Noise compensation using geotagged audio signals |
EP21189184.1A Active EP3923281B1 (en) | 2010-04-14 | 2011-03-22 | Noise compensation using geotagged audio signals |
EP18188692.0A Active EP3425634B1 (en) | 2010-04-14 | 2011-03-22 | Noise compensation using geotagged audio signals |
Family Applications After (3)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
EP11713118.5A Active EP2559031B1 (en) | 2010-04-14 | 2011-03-22 | Noise compensation using geotagged audio signals |
EP21189184.1A Active EP3923281B1 (en) | 2010-04-14 | 2011-03-22 | Noise compensation using geotagged audio signals |
EP18188692.0A Active EP3425634B1 (en) | 2010-04-14 | 2011-03-22 | Noise compensation using geotagged audio signals |
Country Status (5)
Country | Link |
---|---|
US (4) | US8265928B2 (en) |
EP (4) | EP2750133B1 (en) |
CN (2) | CN102918591B (en) |
AU (1) | AU2011241065B2 (en) |
WO (1) | WO2011129954A1 (en) |
Families Citing this family (352)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
AU6630800A (en) * | 1999-08-13 | 2001-03-13 | Pixo, Inc. | Methods and apparatuses for display and traversing of links in page character array |
US8645137B2 (en) * | 2000-03-16 | 2014-02-04 | Apple Inc. | Fast, language-independent method for user authentication by voice |
ITFI20010199A1 (en) | 2001-10-22 | 2003-04-22 | Riccardo Vieri | SYSTEM AND METHOD TO TRANSFORM TEXTUAL COMMUNICATIONS INTO VOICE AND SEND THEM WITH AN INTERNET CONNECTION TO ANY TELEPHONE SYSTEM |
US7669134B1 (en) | 2003-05-02 | 2010-02-23 | Apple Inc. | Method and apparatus for displaying information during an instant messaging session |
US8677377B2 (en) | 2005-09-08 | 2014-03-18 | Apple Inc. | Method and apparatus for building an intelligent automated assistant |
US7633076B2 (en) | 2005-09-30 | 2009-12-15 | Apple Inc. | Automated response to and sensing of user activity in portable devices |
US9318108B2 (en) | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
US20080129520A1 (en) * | 2006-12-01 | 2008-06-05 | Apple Computer, Inc. | Electronic device with enhanced audio feedback |
US7912828B2 (en) * | 2007-02-23 | 2011-03-22 | Apple Inc. | Pattern searching methods and apparatuses |
US8977255B2 (en) | 2007-04-03 | 2015-03-10 | Apple Inc. | Method and system for operating a multi-function portable electronic device using voice-activation |
ITFI20070177A1 (en) | 2007-07-26 | 2009-01-27 | Riccardo Vieri | SYSTEM FOR THE CREATION AND SETTING OF AN ADVERTISING CAMPAIGN DERIVING FROM THE INSERTION OF ADVERTISING MESSAGES WITHIN AN EXCHANGE OF MESSAGES AND METHOD FOR ITS FUNCTIONING. |
US9053089B2 (en) * | 2007-10-02 | 2015-06-09 | Apple Inc. | Part-of-speech tagging using latent analogy |
US8364694B2 (en) * | 2007-10-26 | 2013-01-29 | Apple Inc. | Search assistant for digital media assets |
US8620662B2 (en) | 2007-11-20 | 2013-12-31 | Apple Inc. | Context-aware unit selection |
US10002189B2 (en) | 2007-12-20 | 2018-06-19 | Apple Inc. | Method and apparatus for searching using an active ontology |
US9330720B2 (en) | 2008-01-03 | 2016-05-03 | Apple Inc. | Methods and apparatus for altering audio output signals |
US8327272B2 (en) | 2008-01-06 | 2012-12-04 | Apple Inc. | Portable multifunction device, method, and graphical user interface for viewing and managing electronic calendars |
US8065143B2 (en) | 2008-02-22 | 2011-11-22 | Apple Inc. | Providing text input using speech data and non-speech data |
US8289283B2 (en) * | 2008-03-04 | 2012-10-16 | Apple Inc. | Language input interface on a device |
US8996376B2 (en) | 2008-04-05 | 2015-03-31 | Apple Inc. | Intelligent text-to-speech conversion |
US10496753B2 (en) | 2010-01-18 | 2019-12-03 | Apple Inc. | Automatically adapting user interfaces for hands-free interaction |
US8464150B2 (en) | 2008-06-07 | 2013-06-11 | Apple Inc. | Automatic language identification for dynamic text processing |
US20100030549A1 (en) | 2008-07-31 | 2010-02-04 | Lee Michael M | Mobile device having human language translation capability with positional feedback |
US8768702B2 (en) | 2008-09-05 | 2014-07-01 | Apple Inc. | Multi-tiered voice feedback in an electronic device |
US8898568B2 (en) * | 2008-09-09 | 2014-11-25 | Apple Inc. | Audio user interface |
US8583418B2 (en) | 2008-09-29 | 2013-11-12 | Apple Inc. | Systems and methods of detecting language and natural language strings for text to speech synthesis |
US8352272B2 (en) * | 2008-09-29 | 2013-01-08 | Apple Inc. | Systems and methods for text to speech synthesis |
US20100082328A1 (en) * | 2008-09-29 | 2010-04-01 | Apple Inc. | Systems and methods for speech preprocessing in text to speech synthesis |
US8355919B2 (en) * | 2008-09-29 | 2013-01-15 | Apple Inc. | Systems and methods for text normalization for text to speech synthesis |
US8396714B2 (en) * | 2008-09-29 | 2013-03-12 | Apple Inc. | Systems and methods for concatenation of words in text to speech synthesis |
US8352268B2 (en) * | 2008-09-29 | 2013-01-08 | Apple Inc. | Systems and methods for selective rate of speech and speech preferences for text to speech synthesis |
US8712776B2 (en) * | 2008-09-29 | 2014-04-29 | Apple Inc. | Systems and methods for selective text to speech synthesis |
US8676904B2 (en) | 2008-10-02 | 2014-03-18 | Apple Inc. | Electronic devices with voice command and contextual data processing capabilities |
WO2010067118A1 (en) | 2008-12-11 | 2010-06-17 | Novauris Technologies Limited | Speech recognition involving a mobile device |
US8862252B2 (en) | 2009-01-30 | 2014-10-14 | Apple Inc. | Audio user interface for displayless electronic device |
US8380507B2 (en) * | 2009-03-09 | 2013-02-19 | Apple Inc. | Systems and methods for determining the language to use for speech generated by a text to speech engine |
US10241752B2 (en) | 2011-09-30 | 2019-03-26 | Apple Inc. | Interface for a virtual digital assistant |
US9858925B2 (en) | 2009-06-05 | 2018-01-02 | Apple Inc. | Using context information to facilitate processing of commands in a virtual assistant |
US10540976B2 (en) | 2009-06-05 | 2020-01-21 | Apple Inc. | Contextual voice commands |
US10241644B2 (en) | 2011-06-03 | 2019-03-26 | Apple Inc. | Actionable reminder entries |
US10706373B2 (en) | 2011-06-03 | 2020-07-07 | Apple Inc. | Performing actions associated with task items that represent tasks to perform |
US9431006B2 (en) * | 2009-07-02 | 2016-08-30 | Apple Inc. | Methods and apparatuses for automatic speech recognition |
US20110010179A1 (en) * | 2009-07-13 | 2011-01-13 | Naik Devang K | Voice synthesis and processing |
US20110066438A1 (en) * | 2009-09-15 | 2011-03-17 | Apple Inc. | Contextual voiceover |
US8682649B2 (en) * | 2009-11-12 | 2014-03-25 | Apple Inc. | Sentiment prediction from textual data |
US11416214B2 (en) | 2009-12-23 | 2022-08-16 | Google Llc | Multi-modal input on an electronic device |
EP2339576B1 (en) | 2009-12-23 | 2019-08-07 | Google LLC | Multi-modal input on an electronic device |
US20110167350A1 (en) * | 2010-01-06 | 2011-07-07 | Apple Inc. | Assist Features For Content Display Device |
US8600743B2 (en) * | 2010-01-06 | 2013-12-03 | Apple Inc. | Noise profile determination for voice-related feature |
US8381107B2 (en) | 2010-01-13 | 2013-02-19 | Apple Inc. | Adaptive audio feedback system and method |
US8311838B2 (en) | 2010-01-13 | 2012-11-13 | Apple Inc. | Devices and methods for identifying a prompt corresponding to a voice input in a sequence of prompts |
US20110177809A1 (en) * | 2010-01-15 | 2011-07-21 | Qualcomm Incorporated | Affecting a navigation function in response to a perceived transition from one environment to another |
US10705794B2 (en) | 2010-01-18 | 2020-07-07 | Apple Inc. | Automatically adapting user interfaces for hands-free interaction |
US10553209B2 (en) | 2010-01-18 | 2020-02-04 | Apple Inc. | Systems and methods for hands-free notification summaries |
US10276170B2 (en) | 2010-01-18 | 2019-04-30 | Apple Inc. | Intelligent automated assistant |
US10679605B2 (en) | 2010-01-18 | 2020-06-09 | Apple Inc. | Hands-free list-reading by intelligent automated assistant |
US8682667B2 (en) | 2010-02-25 | 2014-03-25 | Apple Inc. | User profiling for selecting user specific voice input processing information |
US8468012B2 (en) * | 2010-05-26 | 2013-06-18 | Google Inc. | Acoustic model adaptation using geographic information |
US8639516B2 (en) | 2010-06-04 | 2014-01-28 | Apple Inc. | User-specific noise suppression for voice quality improvements |
US20110316672A1 (en) * | 2010-06-29 | 2011-12-29 | International Business Machines Corporation | Mobile communication based tagging |
US8713021B2 (en) | 2010-07-07 | 2014-04-29 | Apple Inc. | Unsupervised document clustering using latent semantic density analysis |
US9104670B2 (en) | 2010-07-21 | 2015-08-11 | Apple Inc. | Customized search or acquisition of digital media assets |
US8521526B1 (en) * | 2010-07-28 | 2013-08-27 | Google Inc. | Disambiguation of a spoken query term |
US8719006B2 (en) | 2010-08-27 | 2014-05-06 | Apple Inc. | Combined statistical and rule-based part-of-speech tagging for text-to-speech synthesis |
US8812014B2 (en) * | 2010-08-30 | 2014-08-19 | Qualcomm Incorporated | Audio-based environment awareness |
US9277362B2 (en) * | 2010-09-03 | 2016-03-01 | Blackberry Limited | Method and apparatus for generating and using location information |
US8719014B2 (en) | 2010-09-27 | 2014-05-06 | Apple Inc. | Electronic device with text error correction based on voice recognition data |
US8532674B2 (en) * | 2010-12-10 | 2013-09-10 | General Motors Llc | Method of intelligent vehicle dialing |
US10515147B2 (en) | 2010-12-22 | 2019-12-24 | Apple Inc. | Using statistical language models for contextual lookup |
US10762293B2 (en) | 2010-12-22 | 2020-09-01 | Apple Inc. | Using parts-of-speech tagging and named entity recognition for spelling correction |
US8352245B1 (en) | 2010-12-30 | 2013-01-08 | Google Inc. | Adjusting language models |
US8296142B2 (en) | 2011-01-21 | 2012-10-23 | Google Inc. | Speech recognition using dock context |
WO2012107561A1 (en) * | 2011-02-10 | 2012-08-16 | Dolby International Ab | Spatial adaptation in multi-microphone sound capture |
US8781836B2 (en) | 2011-02-22 | 2014-07-15 | Apple Inc. | Hearing assistance system for providing consistent human speech |
US9262612B2 (en) | 2011-03-21 | 2016-02-16 | Apple Inc. | Device access using voice authentication |
CA2831678A1 (en) * | 2011-03-28 | 2012-10-04 | Ambientz | Methods and systems for searching utilizing acoustical context |
US9137734B2 (en) * | 2011-03-30 | 2015-09-15 | Microsoft Technology Licensing, Llc | Mobile device configuration based on status and location |
US9842168B2 (en) | 2011-03-31 | 2017-12-12 | Microsoft Technology Licensing, Llc | Task driven user intents |
US9760566B2 (en) | 2011-03-31 | 2017-09-12 | Microsoft Technology Licensing, Llc | Augmented conversational understanding agent to identify conversation context between two humans and taking an agent action thereof |
US9298287B2 (en) | 2011-03-31 | 2016-03-29 | Microsoft Technology Licensing, Llc | Combined activation for natural user interface systems |
US9858343B2 (en) | 2011-03-31 | 2018-01-02 | Microsoft Technology Licensing Llc | Personalization of queries, conversations, and searches |
US9244984B2 (en) | 2011-03-31 | 2016-01-26 | Microsoft Technology Licensing, Llc | Location based conversational understanding |
US10642934B2 (en) | 2011-03-31 | 2020-05-05 | Microsoft Technology Licensing, Llc | Augmented conversational understanding architecture |
US20120265526A1 (en) * | 2011-04-13 | 2012-10-18 | Continental Automotive Systems, Inc. | Apparatus and method for voice activity detection |
US9064006B2 (en) | 2012-08-23 | 2015-06-23 | Microsoft Technology Licensing, Llc | Translating natural language utterances to keyword search queries |
US9454962B2 (en) | 2011-05-12 | 2016-09-27 | Microsoft Technology Licensing, Llc | Sentence simplification for spoken language understanding |
US10672399B2 (en) | 2011-06-03 | 2020-06-02 | Apple Inc. | Switching between text data and audio data based on a mapping |
US10057736B2 (en) | 2011-06-03 | 2018-08-21 | Apple Inc. | Active transport based notifications |
US8812294B2 (en) | 2011-06-21 | 2014-08-19 | Apple Inc. | Translating phrases from one language into another using an order-based set of declarative rules |
US8595015B2 (en) * | 2011-08-08 | 2013-11-26 | Verizon New Jersey Inc. | Audio communication assessment |
US8706472B2 (en) | 2011-08-11 | 2014-04-22 | Apple Inc. | Method for disambiguating multiple readings in language conversion |
US8994660B2 (en) | 2011-08-29 | 2015-03-31 | Apple Inc. | Text correction processing |
US8762156B2 (en) | 2011-09-28 | 2014-06-24 | Apple Inc. | Speech recognition repair using contextual information |
US10134385B2 (en) | 2012-03-02 | 2018-11-20 | Apple Inc. | Systems and methods for name pronunciation |
US9483461B2 (en) | 2012-03-06 | 2016-11-01 | Apple Inc. | Handling speech synthesis of content for multiple languages |
US20140108448A1 (en) * | 2012-03-30 | 2014-04-17 | Intel Corporation | Multi-sensor velocity dependent context aware voice recognition and summarization |
US9280610B2 (en) | 2012-05-14 | 2016-03-08 | Apple Inc. | Crowd sourcing information to fulfill user requests |
US10417037B2 (en) | 2012-05-15 | 2019-09-17 | Apple Inc. | Systems and methods for integrating third party services with a digital assistant |
US8775442B2 (en) | 2012-05-15 | 2014-07-08 | Apple Inc. | Semantic search using a single-source semantic model |
US9123338B1 (en) | 2012-06-01 | 2015-09-01 | Google Inc. | Background audio identification for speech disambiguation |
US11023520B1 (en) | 2012-06-01 | 2021-06-01 | Google Llc | Background audio identification for query disambiguation |
JP2013254395A (en) * | 2012-06-07 | 2013-12-19 | Ricoh Co Ltd | Processing apparatus, processing system, output method and program |
US9721563B2 (en) | 2012-06-08 | 2017-08-01 | Apple Inc. | Name recognition system |
WO2013185109A2 (en) | 2012-06-08 | 2013-12-12 | Apple Inc. | Systems and methods for recognizing textual identifiers within a plurality of words |
US9953638B2 (en) * | 2012-06-28 | 2018-04-24 | Nuance Communications, Inc. | Meta-data inputs to front end processing for automatic speech recognition |
US9495129B2 (en) | 2012-06-29 | 2016-11-15 | Apple Inc. | Device, method, and user interface for voice-activated navigation and browsing of a document |
US8639577B1 (en) * | 2012-07-31 | 2014-01-28 | Wal-Mart Stores, Inc. | Use of sound to authenticate and enable a return with an electronic receipt |
US8831957B2 (en) * | 2012-08-01 | 2014-09-09 | Google Inc. | Speech recognition models based on location indicia |
US8571865B1 (en) * | 2012-08-10 | 2013-10-29 | Google Inc. | Inference-aided speaker recognition |
US20140074466A1 (en) | 2012-09-10 | 2014-03-13 | Google Inc. | Answering questions using environmental context |
US9576574B2 (en) | 2012-09-10 | 2017-02-21 | Apple Inc. | Context-sensitive handling of interruptions by intelligent digital assistant |
US9547647B2 (en) | 2012-09-19 | 2017-01-17 | Apple Inc. | Voice-based media searching |
US8935167B2 (en) | 2012-09-25 | 2015-01-13 | Apple Inc. | Exemplar-based latent perceptual modeling for automatic speech recognition |
WO2014079483A1 (en) * | 2012-11-21 | 2014-05-30 | Huawei Technologies Co., Ltd. | Method and device for reconstructing a target signal from a noisy input signal |
WO2014081429A2 (en) * | 2012-11-21 | 2014-05-30 | Empire Technology Development | Speech recognition |
US20140163768A1 (en) * | 2012-12-11 | 2014-06-12 | At&T Intellectual Property I, L.P. | Event and condition determination based on sensor data |
US9190057B2 (en) * | 2012-12-12 | 2015-11-17 | Amazon Technologies, Inc. | Speech model retrieval in distributed speech recognition systems |
US9653070B2 (en) | 2012-12-31 | 2017-05-16 | Intel Corporation | Flexible architecture for acoustic signal processing engine |
CN104969289B (en) | 2013-02-07 | 2021-05-28 | 苹果公司 | Voice trigger of digital assistant |
US9032000B2 (en) * | 2013-02-19 | 2015-05-12 | Digital Globe Inc. | System and method for geolocation of social media posts |
US9734819B2 (en) | 2013-02-21 | 2017-08-15 | Google Technology Holdings LLC | Recognizing accented speech |
US10229701B2 (en) | 2013-02-28 | 2019-03-12 | Nuance Communications, Inc. | Server-side ASR adaptation to speaker, device and noise condition via non-ASR audio transmission |
WO2014133525A1 (en) * | 2013-02-28 | 2014-09-04 | Nuance Communication, Inc. | Server-side asr adaptation to speaker, device and noise condition via non-asr audio transmission |
US9237225B2 (en) | 2013-03-12 | 2016-01-12 | Google Technology Holdings LLC | Apparatus with dynamic audio signal pre-conditioning and methods therefor |
US20140278393A1 (en) | 2013-03-12 | 2014-09-18 | Motorola Mobility Llc | Apparatus and Method for Power Efficient Signal Conditioning for a Voice Recognition System |
US20140278392A1 (en) * | 2013-03-12 | 2014-09-18 | Motorola Mobility Llc | Method and Apparatus for Pre-Processing Audio Signals |
US20140270249A1 (en) | 2013-03-12 | 2014-09-18 | Motorola Mobility Llc | Method and Apparatus for Estimating Variability of Background Noise for Noise Suppression |
US20140278415A1 (en) * | 2013-03-12 | 2014-09-18 | Motorola Mobility Llc | Voice Recognition Configuration Selector and Method of Operation Therefor |
EP2975829B1 (en) * | 2013-03-13 | 2022-02-09 | Clarion Co., Ltd. | Display apparatus |
US10652394B2 (en) | 2013-03-14 | 2020-05-12 | Apple Inc. | System and method for processing voicemail |
US10572476B2 (en) | 2013-03-14 | 2020-02-25 | Apple Inc. | Refining a search based on schedule items |
US9368114B2 (en) | 2013-03-14 | 2016-06-14 | Apple Inc. | Context-sensitive handling of interruptions |
US9977779B2 (en) | 2013-03-14 | 2018-05-22 | Apple Inc. | Automatic supplementation of word correction dictionaries |
US9733821B2 (en) | 2013-03-14 | 2017-08-15 | Apple Inc. | Voice control to diagnose inadvertent activation of accessibility features |
US10642574B2 (en) | 2013-03-14 | 2020-05-05 | Apple Inc. | Device, method, and graphical user interface for outputting captions |
KR101759009B1 (en) | 2013-03-15 | 2017-07-17 | 애플 인크. | Training an at least partial voice command system |
US10748529B1 (en) | 2013-03-15 | 2020-08-18 | Apple Inc. | Voice activated device for use with a voice-based digital assistant |
KR102057795B1 (en) | 2013-03-15 | 2019-12-19 | 애플 인크. | Context-sensitive handling of interruptions |
WO2014144579A1 (en) | 2013-03-15 | 2014-09-18 | Apple Inc. | System and method for updating an adaptive speech recognition model |
US9886160B2 (en) * | 2013-03-15 | 2018-02-06 | Google Llc | Managing audio at the tab level for user notification and control |
EP2973002B1 (en) | 2013-03-15 | 2019-06-26 | Apple Inc. | User training by intelligent digital assistant |
US9437208B2 (en) * | 2013-06-03 | 2016-09-06 | Adobe Systems Incorporated | General sound decomposition models |
WO2014197336A1 (en) | 2013-06-07 | 2014-12-11 | Apple Inc. | System and method for detecting errors in interactions with a voice-based digital assistant |
US9582608B2 (en) | 2013-06-07 | 2017-02-28 | Apple Inc. | Unified ranking with entropy-weighted information for phrase-based semantic auto-completion |
WO2014197334A2 (en) | 2013-06-07 | 2014-12-11 | Apple Inc. | System and method for user-specified pronunciation of words for speech synthesis and recognition |
WO2014197335A1 (en) | 2013-06-08 | 2014-12-11 | Apple Inc. | Interpreting and acting upon commands that involve sharing information with remote devices |
KR101959188B1 (en) | 2013-06-09 | 2019-07-02 | 애플 인크. | Device, method, and graphical user interface for enabling conversation persistence across two or more instances of a digital assistant |
US10176167B2 (en) | 2013-06-09 | 2019-01-08 | Apple Inc. | System and method for inferring user intent from speech inputs |
WO2014200731A1 (en) | 2013-06-13 | 2014-12-18 | Apple Inc. | System and method for emergency calls initiated by voice command |
US9727129B2 (en) * | 2013-06-28 | 2017-08-08 | Harman International Industries, Incorporated | System and method for audio augmented reality |
WO2015017303A1 (en) * | 2013-07-31 | 2015-02-05 | Motorola Mobility Llc | Method and apparatus for adjusting voice recognition processing based on noise characteristics |
KR101749009B1 (en) | 2013-08-06 | 2017-06-19 | 애플 인크. | Auto-activating smart responses based on activities from remote devices |
US9530416B2 (en) | 2013-10-28 | 2016-12-27 | At&T Intellectual Property I, L.P. | System and method for managing models for embedded speech and language processing |
US9666188B2 (en) | 2013-10-29 | 2017-05-30 | Nuance Communications, Inc. | System and method of performing automatic speech recognition using local private data |
US10296160B2 (en) | 2013-12-06 | 2019-05-21 | Apple Inc. | Method for extracting salient dialog usage from live data |
US8862467B1 (en) * | 2013-12-11 | 2014-10-14 | Google Inc. | Contextual speech recognition |
CN103680493A (en) * | 2013-12-19 | 2014-03-26 | 百度在线网络技术（北京）有限公司 | Voice data recognition method and device for distinguishing regional accents |
WO2015100587A1 (en) * | 2013-12-31 | 2015-07-09 | 海能达通信股份有限公司 | Voice recording method, call record playback method, and related apparatus and system |
US9842592B2 (en) | 2014-02-12 | 2017-12-12 | Google Inc. | Language models using non-linguistic context |
US9412365B2 (en) | 2014-03-24 | 2016-08-09 | Google Inc. | Enhanced maximum entropy models |
CA2887291A1 (en) * | 2014-04-02 | 2015-10-02 | Speakread A/S | Systems and methods for supporting hearing impaired users |
US10104452B2 (en) * | 2014-05-08 | 2018-10-16 | Paypal, Inc. | Gathering unique information from dispersed users |
US9620105B2 (en) | 2014-05-15 | 2017-04-11 | Apple Inc. | Analyzing audio input for efficient speech and music recognition |
US10592095B2 (en) | 2014-05-23 | 2020-03-17 | Apple Inc. | Instantaneous speaking of content on touch devices |
US9502031B2 (en) | 2014-05-27 | 2016-11-22 | Apple Inc. | Method for supporting dynamic grammars in WFST-based ASR |
US9760559B2 (en) * | 2014-05-30 | 2017-09-12 | Apple Inc. | Predictive text input |
US10078631B2 (en) | 2014-05-30 | 2018-09-18 | Apple Inc. | Entropy-guided text prediction using combined word and character n-gram language models |
US9734193B2 (en) | 2014-05-30 | 2017-08-15 | Apple Inc. | Determining domain salience ranking from ambiguous words in natural speech |
US10170123B2 (en) | 2014-05-30 | 2019-01-01 | Apple Inc. | Intelligent assistant for home automation |
US9715875B2 (en) | 2014-05-30 | 2017-07-25 | Apple Inc. | Reducing the need for manual start/end-pointing and trigger phrases |
US9842101B2 (en) | 2014-05-30 | 2017-12-12 | Apple Inc. | Predictive conversion of language input |
US9966065B2 (en) | 2014-05-30 | 2018-05-08 | Apple Inc. | Multi-command single utterance input method |
US9633004B2 (en) | 2014-05-30 | 2017-04-25 | Apple Inc. | Better resolution when referencing to concepts |
US9785630B2 (en) | 2014-05-30 | 2017-10-10 | Apple Inc. | Text prediction using combined word N-gram and unigram language models |
US10289433B2 (en) | 2014-05-30 | 2019-05-14 | Apple Inc. | Domain specific language for encoding assistant dialog |
US9430463B2 (en) | 2014-05-30 | 2016-08-30 | Apple Inc. | Exemplar-based natural language processing |
US9904851B2 (en) | 2014-06-11 | 2018-02-27 | At&T Intellectual Property I, L.P. | Exploiting visual information for enhancing audio signals via source separation and beamforming |
US9384738B2 (en) * | 2014-06-24 | 2016-07-05 | Google Inc. | Dynamic threshold for speaker verification |
US9639854B2 (en) | 2014-06-26 | 2017-05-02 | Nuance Communications, Inc. | Voice-controlled information exchange platform, such as for providing information to supplement advertising |
US9338493B2 (en) | 2014-06-30 | 2016-05-10 | Apple Inc. | Intelligent automated assistant for TV user interactions |
US10659851B2 (en) | 2014-06-30 | 2020-05-19 | Apple Inc. | Real-time digital assistant knowledge updates |
US9837102B2 (en) * | 2014-07-02 | 2017-12-05 | Microsoft Technology Licensing, Llc | User environment aware acoustic noise reduction |
US10073607B2 (en) | 2014-07-03 | 2018-09-11 | Qualcomm Incorporated | Single-channel or multi-channel audio control interface |
US10446141B2 (en) | 2014-08-28 | 2019-10-15 | Apple Inc. | Automatic speech recognition based on user feedback |
US9953646B2 (en) | 2014-09-02 | 2018-04-24 | Belleau Technologies | Method and system for dynamic speech recognition and tracking of prewritten script |
US9818400B2 (en) | 2014-09-11 | 2017-11-14 | Apple Inc. | Method and apparatus for discovering trending terms in speech requests |
US10789041B2 (en) | 2014-09-12 | 2020-09-29 | Apple Inc. | Dynamic thresholds for always listening speech trigger |
US9886432B2 (en) | 2014-09-30 | 2018-02-06 | Apple Inc. | Parsimonious handling of word inflection via categorical stem + suffix N-gram language models |
US10127911B2 (en) | 2014-09-30 | 2018-11-13 | Apple Inc. | Speaker identification and unsupervised speaker adaptation techniques |
US9668121B2 (en) | 2014-09-30 | 2017-05-30 | Apple Inc. | Social reminders |
US9646609B2 (en) | 2014-09-30 | 2017-05-09 | Apple Inc. | Caching apparatus for serving phonetic pronunciations |
US10074360B2 (en) | 2014-09-30 | 2018-09-11 | Apple Inc. | Providing an indication of the suitability of speech recognition |
US9530408B2 (en) | 2014-10-31 | 2016-12-27 | At&T Intellectual Property I, L.P. | Acoustic environment recognizer for optimal speech processing |
US10552013B2 (en) | 2014-12-02 | 2020-02-04 | Apple Inc. | Data detection |
US9711141B2 (en) | 2014-12-09 | 2017-07-18 | Apple Inc. | Disambiguating heteronyms in speech synthesis |
US11275757B2 (en) | 2015-02-13 | 2022-03-15 | Cerner Innovation, Inc. | Systems and methods for capturing data, creating billable information and outputting billable information |
US9865280B2 (en) | 2015-03-06 | 2018-01-09 | Apple Inc. | Structured dictation using intelligent automated assistants |
US10152299B2 (en) | 2015-03-06 | 2018-12-11 | Apple Inc. | Reducing response latency of intelligent automated assistants |
US10567477B2 (en) | 2015-03-08 | 2020-02-18 | Apple Inc. | Virtual assistant continuity |
US9721566B2 (en) | 2015-03-08 | 2017-08-01 | Apple Inc. | Competing devices responding to voice triggers |
US9886953B2 (en) | 2015-03-08 | 2018-02-06 | Apple Inc. | Virtual assistant activation |
CN104777998B (en) * | 2015-03-17 | 2018-12-18 | 惠州Tcl移动通信有限公司 | The method and intelligent terminal of picture rotation |
US9899019B2 (en) | 2015-03-18 | 2018-02-20 | Apple Inc. | Systems and methods for structured stem and suffix language models |
US10134394B2 (en) | 2015-03-20 | 2018-11-20 | Google Llc | Speech recognition using log-linear model |
US9842105B2 (en) | 2015-04-16 | 2017-12-12 | Apple Inc. | Parsimonious continuous-space phrase representations for natural language processing |
US10419869B2 (en) * | 2015-04-24 | 2019-09-17 | Dolby Laboratories Licensing Corporation | Augmented hearing system |
US10460227B2 (en) | 2015-05-15 | 2019-10-29 | Apple Inc. | Virtual assistant in a communication session |
US10200824B2 (en) | 2015-05-27 | 2019-02-05 | Apple Inc. | Systems and methods for proactively identifying and surfacing relevant content on a touch-sensitive device |
CN104951182B (en) * | 2015-05-27 | 2021-05-28 | 深圳市万普拉斯科技有限公司 | Method and device for replacing interface theme of application and intelligent terminal |
US10083688B2 (en) | 2015-05-27 | 2018-09-25 | Apple Inc. | Device voice control for selecting a displayed affordance |
US10127220B2 (en) | 2015-06-04 | 2018-11-13 | Apple Inc. | Language identification from short strings |
US10101822B2 (en) | 2015-06-05 | 2018-10-16 | Apple Inc. | Language input correction |
US9578173B2 (en) | 2015-06-05 | 2017-02-21 | Apple Inc. | Virtual assistant aided communication with 3rd party service in a communication session |
US10255907B2 (en) | 2015-06-07 | 2019-04-09 | Apple Inc. | Automatic accent detection using acoustic models |
US10186254B2 (en) | 2015-06-07 | 2019-01-22 | Apple Inc. | Context-based endpoint detection |
US11025565B2 (en) | 2015-06-07 | 2021-06-01 | Apple Inc. | Personalized prediction of responses for instant messaging |
WO2016200381A1 (en) * | 2015-06-10 | 2016-12-15 | Nuance Communications, Inc. | Motion adaptive speech recognition for enhanced voice destination entry |
US20160378747A1 (en) | 2015-06-29 | 2016-12-29 | Apple Inc. | Virtual assistant for media playback |
EP3317878B1 (en) | 2015-06-30 | 2020-03-25 | Fraunhofer Gesellschaft zur Förderung der Angewand | Method and device for creating a database |
CN105824550B (en) * | 2015-07-23 | 2021-11-30 | 维沃移动通信有限公司 | Screen protection interface control method and device |
US10331312B2 (en) | 2015-09-08 | 2019-06-25 | Apple Inc. | Intelligent automated assistant in a media environment |
US10740384B2 (en) | 2015-09-08 | 2020-08-11 | Apple Inc. | Intelligent automated assistant for media search and playback |
US10747498B2 (en) | 2015-09-08 | 2020-08-18 | Apple Inc. | Zero latency digital assistant |
US10671428B2 (en) | 2015-09-08 | 2020-06-02 | Apple Inc. | Distributed personal assistant |
CN105654960A (en) * | 2015-09-21 | 2016-06-08 | 宇龙计算机通信科技(深圳)有限公司 | Terminal sound denoising processing method and apparatus thereof |
US9697820B2 (en) | 2015-09-24 | 2017-07-04 | Apple Inc. | Unit-selection text-to-speech synthesis using concatenation-sensitive neural networks |
US10366158B2 (en) | 2015-09-29 | 2019-07-30 | Apple Inc. | Efficient word encoding for recurrent neural network language models |
US11010550B2 (en) | 2015-09-29 | 2021-05-18 | Apple Inc. | Unified language modeling framework for word prediction, auto-completion and auto-correction |
US11587559B2 (en) | 2015-09-30 | 2023-02-21 | Apple Inc. | Intelligent device identification |
KR102494139B1 (en) * | 2015-11-06 | 2023-01-31 | 삼성전자주식회사 | Apparatus and method for training neural network, apparatus and method for speech recognition |
US10691473B2 (en) | 2015-11-06 | 2020-06-23 | Apple Inc. | Intelligent automated assistant in a messaging environment |
US10956666B2 (en) | 2015-11-09 | 2021-03-23 | Apple Inc. | Unconventional virtual assistant interactions |
US10049668B2 (en) | 2015-12-02 | 2018-08-14 | Apple Inc. | Applying neural network language models to weighted finite state transducers for automatic speech recognition |
US10223066B2 (en) | 2015-12-23 | 2019-03-05 | Apple Inc. | Proactive assistance based on dialog communication between devices |
US10390155B2 (en) | 2016-02-08 | 2019-08-20 | K/S Himpp | Hearing augmentation systems and methods |
US10750293B2 (en) * | 2016-02-08 | 2020-08-18 | Hearing Instrument Manufacture Patent Partnership | Hearing augmentation systems and methods |
US10341791B2 (en) | 2016-02-08 | 2019-07-02 | K/S Himpp | Hearing augmentation systems and methods |
US10284998B2 (en) | 2016-02-08 | 2019-05-07 | K/S Himpp | Hearing augmentation systems and methods |
US10631108B2 (en) | 2016-02-08 | 2020-04-21 | K/S Himpp | Hearing augmentation systems and methods |
US10446143B2 (en) | 2016-03-14 | 2019-10-15 | Apple Inc. | Identification of voice inputs providing credentials |
US9978367B2 (en) | 2016-03-16 | 2018-05-22 | Google Llc | Determining dialog states for language models |
US9934775B2 (en) | 2016-05-26 | 2018-04-03 | Apple Inc. | Unit-selection text-to-speech synthesis based on predicted concatenation parameters |
US9972304B2 (en) | 2016-06-03 | 2018-05-15 | Apple Inc. | Privacy preserving distributed evaluation framework for embedded personalized systems |
US11227589B2 (en) | 2016-06-06 | 2022-01-18 | Apple Inc. | Intelligent list reading |
US10249300B2 (en) | 2016-06-06 | 2019-04-02 | Apple Inc. | Intelligent list reading |
US10049663B2 (en) | 2016-06-08 | 2018-08-14 | Apple, Inc. | Intelligent automated assistant for media exploration |
DK179309B1 (en) | 2016-06-09 | 2018-04-23 | Apple Inc | Intelligent automated assistant in a home environment |
US10509862B2 (en) | 2016-06-10 | 2019-12-17 | Apple Inc. | Dynamic phrase expansion of language input |
US10490187B2 (en) | 2016-06-10 | 2019-11-26 | Apple Inc. | Digital assistant providing automated status report |
US10067938B2 (en) | 2016-06-10 | 2018-09-04 | Apple Inc. | Multilingual word prediction |
US10586535B2 (en) | 2016-06-10 | 2020-03-10 | Apple Inc. | Intelligent digital assistant in a multi-tasking environment |
US10192552B2 (en) | 2016-06-10 | 2019-01-29 | Apple Inc. | Digital assistant providing whispered speech |
DK179415B1 (en) | 2016-06-11 | 2018-06-14 | Apple Inc | Intelligent device arbitration and control |
DK179343B1 (en) | 2016-06-11 | 2018-05-14 | Apple Inc | Intelligent task discovery |
DK179049B1 (en) | 2016-06-11 | 2017-09-18 | Apple Inc | Data driven natural language event detection and classification |
DK201670540A1 (en) | 2016-06-11 | 2018-01-08 | Apple Inc | Application integration with a digital assistant |
US10832664B2 (en) | 2016-08-19 | 2020-11-10 | Google Llc | Automated speech recognition using language models that selectively use domain-specific model components |
US10474753B2 (en) | 2016-09-07 | 2019-11-12 | Apple Inc. | Language identification using recurrent neural networks |
US10043516B2 (en) | 2016-09-23 | 2018-08-07 | Apple Inc. | Intelligent automated assistant |
US10181321B2 (en) * | 2016-09-27 | 2019-01-15 | Vocollect, Inc. | Utilization of location and environment to improve recognition |
US9959864B1 (en) * | 2016-10-27 | 2018-05-01 | Google Llc | Location-based voice query recognition |
US11281993B2 (en) | 2016-12-05 | 2022-03-22 | Apple Inc. | Model and ensemble compression for metric learning |
US10593346B2 (en) | 2016-12-22 | 2020-03-17 | Apple Inc. | Rank-reduced token representation for automatic speech recognition |
US10296586B2 (en) * | 2016-12-23 | 2019-05-21 | Soundhound, Inc. | Predicting human behavior by machine learning of natural language interpretations |
US11204787B2 (en) | 2017-01-09 | 2021-12-21 | Apple Inc. | Application integration with a digital assistant |
US10311860B2 (en) | 2017-02-14 | 2019-06-04 | Google Llc | Language model biasing system |
US10417266B2 (en) | 2017-05-09 | 2019-09-17 | Apple Inc. | Context-aware ranking of intelligent response suggestions |
DK201770383A1 (en) | 2017-05-09 | 2018-12-14 | Apple Inc. | User interface for correcting recognition errors |
US10726832B2 (en) | 2017-05-11 | 2020-07-28 | Apple Inc. | Maintaining privacy of personal information |
US10395654B2 (en) | 2017-05-11 | 2019-08-27 | Apple Inc. | Text normalization based on a data-driven learning network |
DK180048B1 (en) | 2017-05-11 | 2020-02-04 | Apple Inc. | MAINTAINING THE DATA PROTECTION OF PERSONAL INFORMATION |
DK201770439A1 (en) | 2017-05-11 | 2018-12-13 | Apple Inc. | Offline personal assistant |
DK201770427A1 (en) | 2017-05-12 | 2018-12-20 | Apple Inc. | Low-latency intelligent automated assistant |
DK179745B1 (en) | 2017-05-12 | 2019-05-01 | Apple Inc. | SYNCHRONIZATION AND TASK DELEGATION OF A DIGITAL ASSISTANT |
US11301477B2 (en) | 2017-05-12 | 2022-04-12 | Apple Inc. | Feedback analysis of a digital assistant |
DK179496B1 (en) | 2017-05-12 | 2019-01-15 | Apple Inc. | USER-SPECIFIC Acoustic Models |
DK201770432A1 (en) | 2017-05-15 | 2018-12-21 | Apple Inc. | Hierarchical belief states for digital assistants |
DK201770431A1 (en) | 2017-05-15 | 2018-12-20 | Apple Inc. | Optimizing dialogue policy decisions for digital assistants using implicit feedback |
DK179560B1 (en) | 2017-05-16 | 2019-02-18 | Apple Inc. | Far-field extension for digital assistant services |
US10403278B2 (en) | 2017-05-16 | 2019-09-03 | Apple Inc. | Methods and systems for phonetic matching in digital assistant services |
US10311144B2 (en) | 2017-05-16 | 2019-06-04 | Apple Inc. | Emoji word sense disambiguation |
US10303715B2 (en) | 2017-05-16 | 2019-05-28 | Apple Inc. | Intelligent automated assistant for media exploration |
US20180336892A1 (en) | 2017-05-16 | 2018-11-22 | Apple Inc. | Detecting a trigger of a digital assistant |
US10657328B2 (en) | 2017-06-02 | 2020-05-19 | Apple Inc. | Multi-task recurrent neural network architecture for efficient morphology handling in neural language modeling |
US11567726B2 (en) * | 2017-07-21 | 2023-01-31 | Google Llc | Methods, systems, and media for providing information relating to detected events |
CN107564546A (en) * | 2017-07-27 | 2018-01-09 | 上海师范大学 | A kind of sound end detecting method based on positional information |
US10445429B2 (en) | 2017-09-21 | 2019-10-15 | Apple Inc. | Natural language understanding using vocabularies with compressed serialized tries |
US10755051B2 (en) | 2017-09-29 | 2020-08-25 | Apple Inc. | Rule-based natural language processing |
GB2566759B8 (en) * | 2017-10-20 | 2021-12-08 | Please Hold Uk Ltd | Encoding identifiers to produce audio identifiers from a plurality of audio bitstreams |
GB2566760B (en) | 2017-10-20 | 2019-10-23 | Please Hold Uk Ltd | Audio Signal |
US10636424B2 (en) | 2017-11-30 | 2020-04-28 | Apple Inc. | Multi-turn canned dialog |
US10733982B2 (en) | 2018-01-08 | 2020-08-04 | Apple Inc. | Multi-directional dialog |
US10733375B2 (en) | 2018-01-31 | 2020-08-04 | Apple Inc. | Knowledge-based framework for improving natural language understanding |
CN108335694B (en) * | 2018-02-01 | 2021-10-15 | 北京百度网讯科技有限公司 | Far-field environment noise processing method, device, equipment and storage medium |
US10789959B2 (en) | 2018-03-02 | 2020-09-29 | Apple Inc. | Training speaker recognition models for digital assistants |
US10592604B2 (en) | 2018-03-12 | 2020-03-17 | Apple Inc. | Inverse text normalization for automatic speech recognition |
US10818288B2 (en) | 2018-03-26 | 2020-10-27 | Apple Inc. | Natural assistant interaction |
US10909331B2 (en) | 2018-03-30 | 2021-02-02 | Apple Inc. | Implicit identification of translation payload with neural machine translation |
US10928918B2 (en) | 2018-05-07 | 2021-02-23 | Apple Inc. | Raise to speak |
US11145294B2 (en) | 2018-05-07 | 2021-10-12 | Apple Inc. | Intelligent automated assistant for delivering content from user experiences |
US10984780B2 (en) | 2018-05-21 | 2021-04-20 | Apple Inc. | Global semantic word embeddings using bi-directional recurrent neural networks |
KR20190136578A (en) * | 2018-05-31 | 2019-12-10 | 삼성전자주식회사 | Method and apparatus for speech recognition |
DK201870355A1 (en) | 2018-06-01 | 2019-12-16 | Apple Inc. | Virtual assistant operation in multi-device environments |
US11386266B2 (en) | 2018-06-01 | 2022-07-12 | Apple Inc. | Text correction |
DK179822B1 (en) | 2018-06-01 | 2019-07-12 | Apple Inc. | Voice interaction at a primary device to access call functionality of a companion device |
US10892996B2 (en) | 2018-06-01 | 2021-01-12 | Apple Inc. | Variable latency device coordination |
DK180639B1 (en) | 2018-06-01 | 2021-11-04 | Apple Inc | DISABILITY OF ATTENTION-ATTENTIVE VIRTUAL ASSISTANT |
US10496705B1 (en) | 2018-06-03 | 2019-12-03 | Apple Inc. | Accelerated task performance |
CN109087659A (en) * | 2018-08-03 | 2018-12-25 | 三星电子（中国）研发中心 | Audio optimization method and apparatus |
US11010561B2 (en) | 2018-09-27 | 2021-05-18 | Apple Inc. | Sentiment prediction from textual data |
US11462215B2 (en) | 2018-09-28 | 2022-10-04 | Apple Inc. | Multi-modal inputs for voice commands |
US11170166B2 (en) | 2018-09-28 | 2021-11-09 | Apple Inc. | Neural typographical error modeling via generative adversarial networks |
US10839159B2 (en) | 2018-09-28 | 2020-11-17 | Apple Inc. | Named entity normalization in a spoken dialog system |
US11475898B2 (en) | 2018-10-26 | 2022-10-18 | Apple Inc. | Low-latency multi-speaker speech recognition |
US11062704B1 (en) | 2018-12-21 | 2021-07-13 | Cerner Innovation, Inc. | Processing multi-party conversations |
US11398232B1 (en) | 2018-12-21 | 2022-07-26 | Cerner Innovation, Inc. | Natural language understanding of conversational sources |
US11875883B1 (en) | 2018-12-21 | 2024-01-16 | Cerner Innovation, Inc. | De-duplication and contextually-intelligent recommendations based on natural language understanding of conversational sources |
US11410650B1 (en) | 2018-12-26 | 2022-08-09 | Cerner Innovation, Inc. | Semantically augmented clinical speech processing |
CN109545195B (en) * | 2018-12-29 | 2023-02-21 | 深圳市科迈爱康科技有限公司 | Accompanying robot and control method thereof |
CN109545196B (en) * | 2018-12-29 | 2022-11-29 | 深圳市科迈爱康科技有限公司 | Speech recognition method, device and computer readable storage medium |
US11638059B2 (en) | 2019-01-04 | 2023-04-25 | Apple Inc. | Content playback on multiple devices |
US11348573B2 (en) | 2019-03-18 | 2022-05-31 | Apple Inc. | Multimodality in digital assistant systems |
US20220157293A1 (en) * | 2019-04-08 | 2022-05-19 | Sony Group Corporation | Response generation device and response generation method |
US11475884B2 (en) | 2019-05-06 | 2022-10-18 | Apple Inc. | Reducing digital assistant latency when a language is incorrectly determined |
US11423908B2 (en) | 2019-05-06 | 2022-08-23 | Apple Inc. | Interpreting spoken requests |
US11307752B2 (en) | 2019-05-06 | 2022-04-19 | Apple Inc. | User configurable task triggers |
DK201970509A1 (en) | 2019-05-06 | 2021-01-15 | Apple Inc | Spoken notifications |
US11140099B2 (en) | 2019-05-21 | 2021-10-05 | Apple Inc. | Providing message response suggestions |
DK180129B1 (en) | 2019-05-31 | 2020-06-02 | Apple Inc. | User activity shortcut suggestions |
US11289073B2 (en) | 2019-05-31 | 2022-03-29 | Apple Inc. | Device text to speech |
DK201970510A1 (en) | 2019-05-31 | 2021-02-11 | Apple Inc | Voice identification in digital assistant systems |
US11496600B2 (en) | 2019-05-31 | 2022-11-08 | Apple Inc. | Remote execution of machine-learned models |
US11227599B2 (en) | 2019-06-01 | 2022-01-18 | Apple Inc. | Methods and user interfaces for voice-based control of electronic devices |
US11360641B2 (en) | 2019-06-01 | 2022-06-14 | Apple Inc. | Increasing the relevance of new available information |
CN114008607A (en) | 2019-06-04 | 2022-02-01 | 吉奥奎斯特系统公司 | Applying geotags to images for identifying exploration opportunities |
KR102260216B1 (en) * | 2019-07-29 | 2021-06-03 | 엘지전자 주식회사 | Intelligent voice recognizing method, voice recognizing apparatus, intelligent computing device and server |
US11488406B2 (en) | 2019-09-25 | 2022-11-01 | Apple Inc. | Text detection using global geometry estimators |
US20210256176A1 (en) * | 2020-02-18 | 2021-08-19 | International Business Machines Corporation | Development of geo-spatial physical models using historical lineage data |
US20210304736A1 (en) * | 2020-03-30 | 2021-09-30 | Nvidia Corporation | Media engagement through deep learning |
US11295543B2 (en) * | 2020-03-31 | 2022-04-05 | International Business Machines Corporation | Object detection in an image |
US11043220B1 (en) | 2020-05-11 | 2021-06-22 | Apple Inc. | Digital assistant hardware abstraction |
US11061543B1 (en) | 2020-05-11 | 2021-07-13 | Apple Inc. | Providing relevant data items based on context |
US11810578B2 (en) | 2020-05-11 | 2023-11-07 | Apple Inc. | Device arbitration for digital assistant-based intercom systems |
US11755276B2 (en) | 2020-05-12 | 2023-09-12 | Apple Inc. | Reducing description length based on confidence |
WO2021258240A1 (en) * | 2020-06-22 | 2021-12-30 | Qualcomm Incorporated | Voice or speech recognition in noisy environments |
US11490204B2 (en) | 2020-07-20 | 2022-11-01 | Apple Inc. | Multi-device audio adjustment coordination |
US11438683B2 (en) | 2020-07-21 | 2022-09-06 | Apple Inc. | User identification using headphones |
US20220100796A1 (en) * | 2020-09-29 | 2022-03-31 | Here Global B.V. | Method, apparatus, and system for mapping conversation and audio data to locations |
CN113496099B (en) * | 2021-04-06 | 2022-05-06 | 西南交通大学 | Slope permanent displacement prediction model training method based on deep learning |
CN113506565A (en) * | 2021-07-12 | 2021-10-15 | 北京捷通华声科技股份有限公司 | Speech recognition method, speech recognition device, computer-readable storage medium and processor |
CN114333881B (en) * | 2022-03-09 | 2022-05-24 | 深圳市迪斯声学有限公司 | Audio transmission noise reduction method, device and medium based on environment self-adaptation |
CN116962935B (en) * | 2023-09-20 | 2024-01-30 | 深圳市齐奥通信技术有限公司 | Earphone noise reduction method and system based on data analysis |
Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20040138882A1 (en) * | 2002-10-31 | 2004-07-15 | Seiko Epson Corporation | Acoustic model creating method, speech recognition apparatus, and vehicle having the speech recognition apparatus |
US20050187763A1 (en) * | 2004-02-23 | 2005-08-25 | General Motors Corporation | Dynamic tuning of hands-free algorithm for noise and driving conditions |
US20080188271A1 (en) * | 2007-02-07 | 2008-08-07 | Denso Corporation | Communicating road noise control system, in-vehicle road noise controller, and server |
US20090271188A1 (en) * | 2008-04-24 | 2009-10-29 | International Business Machines Corporation | Adjusting A Speech Engine For A Mobile Computing Device Based On Background Noise |
Family Cites Families (22)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
DE19533541C1 (en) * | 1995-09-11 | 1997-03-27 | Daimler Benz Aerospace Ag | Method for the automatic control of one or more devices by voice commands or by voice dialog in real time and device for executing the method |
US6778959B1 (en) * | 1999-10-21 | 2004-08-17 | Sony Corporation | System and method for speech verification using out-of-vocabulary models |
US7219058B1 (en) * | 2000-10-13 | 2007-05-15 | At&T Corp. | System and method for processing speech recognition results |
US7457750B2 (en) | 2000-10-13 | 2008-11-25 | At&T Corp. | Systems and methods for dynamic re-configurable speech recognition |
US6876966B1 (en) * | 2000-10-16 | 2005-04-05 | Microsoft Corporation | Pattern recognition training method and apparatus using inserted noise followed by noise reduction |
US6915262B2 (en) | 2000-11-30 | 2005-07-05 | Telesector Resources Group, Inc. | Methods and apparatus for performing speech recognition and using speech recognition results |
US6959276B2 (en) * | 2001-09-27 | 2005-10-25 | Microsoft Corporation | Including the category of environmental noise when processing speech signals |
US6950796B2 (en) * | 2001-11-05 | 2005-09-27 | Motorola, Inc. | Speech recognition by dynamical noise model adaptation |
US7224981B2 (en) | 2002-06-20 | 2007-05-29 | Intel Corporation | Speech recognition of mobile devices |
JP4109063B2 (en) * | 2002-09-18 | 2008-06-25 | パイオニア株式会社 | Speech recognition apparatus and speech recognition method |
US7457745B2 (en) | 2002-12-03 | 2008-11-25 | Hrl Laboratories, Llc | Method and apparatus for fast on-line automatic speaker/environment adaptation for speech/speaker recognition in the presence of changing environments |
US7392188B2 (en) * | 2003-07-31 | 2008-06-24 | Telefonaktiebolaget Lm Ericsson (Publ) | System and method enabling acoustic barge-in |
JP4548646B2 (en) * | 2003-09-12 | 2010-09-22 | 株式会社エヌ・ティ・ティ・ドコモ | Noise model noise adaptation system, noise adaptation method, and speech recognition noise adaptation program |
US7620546B2 (en) * | 2004-03-23 | 2009-11-17 | Qnx Software Systems (Wavemakers), Inc. | Isolating speech signals utilizing neural networks |
JP2009524273A (en) * | 2005-11-29 | 2009-06-25 | グーグル・インコーポレーテッド | Repetitive content detection in broadcast media |
CN101361301A (en) * | 2005-11-29 | 2009-02-04 | 谷歌公司 | Detecting repeating content in broadcast media |
US7890326B2 (en) | 2006-10-13 | 2011-02-15 | Google Inc. | Business listing search |
US8041568B2 (en) | 2006-10-13 | 2011-10-18 | Google Inc. | Business listing search |
US20090030687A1 (en) | 2007-03-07 | 2009-01-29 | Cerra Joseph P | Adapting an unstructured language model speech recognition system based on usage |
JP4455614B2 (en) * | 2007-06-13 | 2010-04-21 | 株式会社東芝 | Acoustic signal processing method and apparatus |
US8589163B2 (en) | 2009-12-04 | 2013-11-19 | At&T Intellectual Property I, L.P. | Adapting language models with a bit mask for a subset of related words |
US8468012B2 (en) | 2010-05-26 | 2013-06-18 | Google Inc. | Acoustic model adaptation using geographic information |
-
2010
- 2010-04-14 US US12/760,147 patent/US8265928B2/en active Active
-
2011
- 2011-03-22 EP EP14162078.1A patent/EP2750133B1/en active Active
- 2011-03-22 WO PCT/US2011/029407 patent/WO2011129954A1/en active Application Filing
- 2011-03-22 CN CN201180019038.8A patent/CN102918591B/en active Active
- 2011-03-22 AU AU2011241065A patent/AU2011241065B2/en active Active
- 2011-03-22 CN CN201610320104.XA patent/CN105741848B/en active Active
- 2011-03-22 EP EP11713118.5A patent/EP2559031B1/en active Active
- 2011-03-22 EP EP21189184.1A patent/EP3923281B1/en active Active
- 2011-03-22 EP EP18188692.0A patent/EP3425634B1/en active Active
- 2011-09-30 US US13/250,843 patent/US8175872B2/en active Active
-
2012
- 2012-08-01 US US13/564,636 patent/US8428940B2/en active Active
-
2013
- 2013-04-12 US US13/862,170 patent/US8682659B2/en active Active
Patent Citations (4)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20040138882A1 (en) * | 2002-10-31 | 2004-07-15 | Seiko Epson Corporation | Acoustic model creating method, speech recognition apparatus, and vehicle having the speech recognition apparatus |
US20050187763A1 (en) * | 2004-02-23 | 2005-08-25 | General Motors Corporation | Dynamic tuning of hands-free algorithm for noise and driving conditions |
US20080188271A1 (en) * | 2007-02-07 | 2008-08-07 | Denso Corporation | Communicating road noise control system, in-vehicle road noise controller, and server |
US20090271188A1 (en) * | 2008-04-24 | 2009-10-29 | International Business Machines Corporation | Adjusting A Speech Engine For A Mobile Computing Device Based On Background Noise |
Also Published As
Publication number | Publication date |
---|---|
EP3425634B1 (en) | 2021-09-15 |
US8265928B2 (en) | 2012-09-11 |
CN105741848B (en) | 2019-07-23 |
US8428940B2 (en) | 2013-04-23 |
EP2559031A1 (en) | 2013-02-20 |
US20110257974A1 (en) | 2011-10-20 |
US20130238325A1 (en) | 2013-09-12 |
EP3923281A1 (en) | 2021-12-15 |
US20120296643A1 (en) | 2012-11-22 |
US8175872B2 (en) | 2012-05-08 |
WO2011129954A1 (en) | 2011-10-20 |
CN102918591A (en) | 2013-02-06 |
US8682659B2 (en) | 2014-03-25 |
EP3425634A3 (en) | 2019-03-20 |
AU2011241065A1 (en) | 2012-10-04 |
EP3923281B1 (en) | 2024-01-31 |
EP2559031B1 (en) | 2014-05-14 |
AU2011241065B2 (en) | 2014-04-17 |
CN105741848A (en) | 2016-07-06 |
EP3425634A2 (en) | 2019-01-09 |
CN102918591B (en) | 2016-06-08 |
EP2750133B1 (en) | 2018-08-29 |
US20120022870A1 (en) | 2012-01-26 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
EP2559031B1 (en) | Noise compensation using geotagged audio signals | |
AU2014202785B2 (en) | Acoustic model adaptation using geographic information | |
AU2011267982B2 (en) | Speech and noise models for speech recognition | |
AU2014200999B2 (en) | Geotagged environmental audio for enhanced speech recognition accuracy |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
17P | Request for examination filed |
Effective date: 20140327 |
|
AC | Divisional application: reference to earlier application |
Ref document number: 2559031Country of ref document: EPKind code of ref document: P |
|
AK | Designated contracting states |
Kind code of ref document: A1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
PUAI | Public reference made under article 153(3) epc to a published international application that has entered the european phase |
Free format text: ORIGINAL CODE: 0009012 |
|
RIN1 | Information on inventor provided before grant (corrected) |
Inventor name: KRISTJANSSON, TRAUSTIInventor name: LLOYD, MATTHEW I. |
|
R17P | Request for examination filed (corrected) |
Effective date: 20141015 |
|
RBV | Designated contracting states (corrected) |
Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
17Q | First examination report despatched |
Effective date: 20151013 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: EXAMINATION IS IN PROGRESS |
|
RAP1 | Party data changed (applicant data changed or rights of an application transferred) |
Owner name: GOOGLE LLC |
|
GRAP | Despatch of communication of intention to grant a patent |
Free format text: ORIGINAL CODE: EPIDOSNIGR1 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: GRANT OF PATENT IS INTENDED |
|
RIC1 | Information provided on ipc code assigned before grant |
Ipc: G10L 15/20 20060101ALN20180301BHEPIpc: G10L 21/0208 20130101AFI20180301BHEP |
|
INTG | Intention to grant announced |
Effective date: 20180316 |
|
GRAS | Grant fee paid |
Free format text: ORIGINAL CODE: EPIDOSNIGR3 |
|
GRAA | (expected) grant |
Free format text: ORIGINAL CODE: 0009210 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: THE PATENT HAS BEEN GRANTED |
|
AC | Divisional application: reference to earlier application |
Ref document number: 2559031Country of ref document: EPKind code of ref document: P |
|
AK | Designated contracting states |
Kind code of ref document: B1Designated state(s): AL AT BE BG CH CY CZ DE DK EE ES FI FR GB GR HR HU IE IS IT LI LT LU LV MC MK MT NL NO PL PT RO RS SE SI SK SM TR |
|
REG | Reference to a national code |
Ref country code: GBRef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: EP |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: REFRef document number: 1036098Country of ref document: ATKind code of ref document: TEffective date: 20180915 |
|
REG | Reference to a national code |
Ref country code: IERef legal event code: FG4D |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R096Ref document number: 602011051649Country of ref document: DE |
|
REG | Reference to a national code |
Ref country code: NLRef legal event code: MPEffective date: 20180829 |
|
REG | Reference to a national code |
Ref country code: LTRef legal event code: MG4D |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180829Ref country code: RSFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180829Ref country code: ISFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20181229Ref country code: NOFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20181129Ref country code: FIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180829Ref country code: LTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180829Ref country code: GRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20181130Ref country code: NLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180829Ref country code: BGFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20181129 |
|
REG | Reference to a national code |
Ref country code: ATRef legal event code: MK05Ref document number: 1036098Country of ref document: ATKind code of ref document: TEffective date: 20180829 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LVFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180829Ref country code: ALFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180829Ref country code: HRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180829 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: ITFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180829Ref country code: EEFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180829Ref country code: PLFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180829Ref country code: ROFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180829Ref country code: CZFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180829Ref country code: ESFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180829Ref country code: ATFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180829 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180829Ref country code: DKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180829Ref country code: SMFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180829 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R097Ref document number: 602011051649Country of ref document: DE |
|
PLBE | No opposition filed within time limit |
Free format text: ORIGINAL CODE: 0009261 |
|
STAA | Information on the status of an ep patent application or granted ep patent |
Free format text: STATUS: NO OPPOSITION FILED WITHIN TIME LIMIT |
|
26N | No opposition filed |
Effective date: 20190531 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: SIFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180829 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MCFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180829 |
|
REG | Reference to a national code |
Ref country code: CHRef legal event code: PL |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: LUFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20190322 |
|
REG | Reference to a national code |
Ref country code: BERef legal event code: MMEffective date: 20190331 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: CHFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20190331Ref country code: LIFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20190331Ref country code: IEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20190322 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: BEFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20190331 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: TRFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180829 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: PTFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20181229Ref country code: MTFree format text: LAPSE BECAUSE OF NON-PAYMENT OF DUE FEESEffective date: 20190322 |
|
REG | Reference to a national code |
Ref country code: DERef legal event code: R082Ref document number: 602011051649Country of ref document: DERepresentative=s name: VENNER SHIPLEY GERMANY LLP, DERef country code: DERef legal event code: R082Ref document number: 602011051649Country of ref document: DERepresentative=s name: VENNER SHIPLEY LLP, DE |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: CYFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180829 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: HUFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMIT; INVALID AB INITIOEffective date: 20110322 |
|
PG25 | Lapsed in a contracting state [announced via postgrant information from national office to epo] |
Ref country code: MKFree format text: LAPSE BECAUSE OF FAILURE TO SUBMIT A TRANSLATION OF THE DESCRIPTION OR TO PAY THE FEE WITHIN THE PRESCRIBED TIME-LIMITEffective date: 20180829 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: FRPayment date: 20230327Year of fee payment: 13 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: GBPayment date: 20230327Year of fee payment: 13Ref country code: DEPayment date: 20230329Year of fee payment: 13 |
|
P01 | Opt-out of the competence of the unified patent court (upc) registered |
Effective date: 20230508 |
|
PGFP | Annual fee paid to national office [announced via postgrant information from national office to epo] |
Ref country code: DEPayment date: 20240327Year of fee payment: 14Ref country code: GBPayment date: 20240327Year of fee payment: 14 |