CN105683944B - Method, equipment and medium for the order training method in machine learning framework - Google Patents
Method, equipment and medium for the order training method in machine learning framework Download PDFInfo
- Publication number
- CN105683944B CN105683944B CN201480060487.0A CN201480060487A CN105683944B CN 105683944 B CN105683944 B CN 105683944B CN 201480060487 A CN201480060487 A CN 201480060487A CN 105683944 B CN105683944 B CN 105683944B
- Authority
- CN
- China
- Prior art keywords
- training
- model layer
- layer
- current check
- model
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N20/00—Machine learning
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06N—COMPUTING ARRANGEMENTS BASED ON SPECIFIC COMPUTATIONAL MODELS
- G06N5/00—Computing arrangements using knowledge-based models
- G06N5/02—Knowledge representation; Symbolic representation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q30/00—Commerce
- G06Q30/02—Marketing; Price estimation or determination; Fundraising
- G06Q30/0241—Advertisements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06Q—INFORMATION AND COMMUNICATION TECHNOLOGY [ICT] SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES; SYSTEMS OR METHODS SPECIALLY ADAPTED FOR ADMINISTRATIVE, COMMERCIAL, FINANCIAL, MANAGERIAL OR SUPERVISORY PURPOSES, NOT OTHERWISE PROVIDED FOR
- G06Q30/00—Commerce
- G06Q30/02—Marketing; Price estimation or determination; Fundraising
- G06Q30/0241—Advertisements
- G06Q30/0242—Determining effectiveness of advertisements
Abstract
A kind of computer implemented method of the order training method for machine learning framework, comprising: multiple data elements are received, wherein each data element is associated with timestamp；Determine the training window of each model layer of the model layer stack for layering；There is the data element of timestamp corresponding with each trained window by identifying, to determine multiple training data elements for each trained window；It identifies for each model layer in preceding checkpoint, wherein being generated for each model layer by father's model layer in preceding checkpoint；Using the identified training data element for each model layer and what is identified train each model layer in preceding checkpoint for each model layer；Multiple current check points are generated, each current check point in plurality of current check point is associated with model layer, and multiple current check points are stored at memory.
Description
Technical field
This specification is related to machine learning, and more particularly relates to use order training method in machine learning framework
Method and system.
Background technique
The known method of online machine learning receives streamed data and carrys out training pattern to meet streamed data.In addition, machine
Other known methods of study receive batch data and carry out training pattern to meet received batch data.At least some known systems
Streamed data is distinguished based on the time associated therewith.Therefore, when reaching in streamed data evening, the data may lacked
In the case of had trained model.Therefore, at least some known systems, the delay requirement that streamed data reaches is reconstructed, again
Calibrate or relearn model.Such delay can further result in unstability, because the data that evening reaches may cause
The Important Adjustment of model.In at least some known systems, before training, the data that system waits all evenings to reach may be inefficient
Or it is not practical.
Summary of the invention
In an aspect, a kind of computer implemented method of order training method for machine learning framework is provided.It should
Method calculates equipment by the training for being couple to memory and realizes.This method includes receiving multiple data elements, wherein each data
Element is associated with timestamp；Determine the training window of each model layer for hierarchical mode layer stack；It is each by being used in
It trains the corresponding timestamp of window to identify data element, determines the multiple training data elements for being used for each trained window；
If identified for each model layer for existing in preceding checkpoint for each model layer in preceding checkpoint, wherein by father's mould
Type layer is generated for each model layer in preceding checkpoint；Using the determining training data element for each model layer, with
And if any, using identification for each model layer in preceding checkpoint, each model layer of training；It generates multiple current
Checkpoint, each current check point in plurality of current check point is associated with model layer, and by multiple current checks
Point is stored at memory.
In another aspect, the training for providing a kind of order training method for machine learning framework calculates equipment.It should
It includes memory for storing data, and the processor communicated with memory that training, which calculates equipment,.Processor is configured as
Multiple data elements are received, wherein each data element is associated with timestamp；Determine each mould for being used for hierarchical mode layer stack
The training window of type layer；By identifying data element using timestamp corresponding with each trained window, determine for every
Multiple training data elements of a trained window；If identification is for each for existing in preceding checkpoint for each model layer
Model layer in preceding checkpoint, wherein being generated for each model layer by father's model layer in preceding checkpoint；Utilize determining use
In the training data element of each model layer, and if any, using identification for each model layer in preceding inspection
Point, each model layer of training；Multiple current check points are generated, each current check point and mould in plurality of current check point
Type layer is associated, and multiple current check points are stored at memory.
On the other hand, a kind of computer readable storage medium of order training method for machine learning framework is provided,
Realize there is processor-executable instruction on it, when being executed by calculating equipment, processor-executable instruction connects calculating equipment
Receive multiple data elements, wherein each data element is associated with timestamp；Determine each model for being used for hierarchical mode layer stack
The training window of layer；By identifying data element using timestamp corresponding with each trained window, determine for each
Multiple training data elements of training window；If identification is used for each mould for existing in preceding checkpoint for each model layer
Type layer in preceding checkpoint, wherein being generated for each model layer by father's model layer in preceding checkpoint；It is used for using determining
The training data element of each model layer, and if any, using identification for each model layer in preceding checkpoint,
The each model layer of training；Multiple current check points are generated, each current check point and model in plurality of current check point
Layer is associated, and multiple current check points are stored at memory.
In an aspect, a kind of system of order training method for machine learning framework is provided.The system includes using
In the device for receiving multiple data elements, wherein each data element is associated with timestamp；Hierarchical mode is used for for determining
The device of the training window of each model layer of layer stack；For by being known using timestamp corresponding with each trained window
Other data element determines the device of multiple training data elements for each trained window；If for being used for each model
Layer exists in preceding checkpoint, and identification is used for the device in preceding checkpoint of each model layer, wherein is generated and is used by father's model layer
In each model layer in preceding checkpoint；For utilizing the determining training data element for each model layer, and if
If having, using identification for each model layer in preceding checkpoint, the device of each model layer of training；It is multiple for generating
The device of current check point, each current check point in plurality of current check point is associated with model layer, and is used for
Multiple current check points are stored in the device at memory.
In another aspect, system described above is provided, wherein the system further comprises for by being applied to
A few machine learning algorithm, makes each model layer adapt to the device of the training data element determined to each model layer.
In another aspect, system described above is provided, wherein the system further comprises for determining each mould
The device of the layer depth of type layer；For retrieving the device of training pattern configuration, wherein training pattern configuration is specified and each layer depth phase
Associated training delay record；Identify that training associated with the layer depth of each model layer is prolonged for configuring based on training pattern
Slow device；And for based on the device for calculating trained window for the training delay of each model layer.
In another aspect, system described above is provided, wherein the system further comprises for making external service
Device and the device synchronous at least one associated current check point of at least one model layer, wherein external server is at least
It is based in part on synchronous current check point service.
In another aspect, system described above is provided, wherein the system further comprises for determining for every
The device of the layer depth of a model layer；For retrieving the device of training pattern configuration, wherein training pattern configuration is specified and each layer
Deep associated duration；Instruction associated with the layer depth of each model layer is identified for configuring based on training pattern
Practice the device of duration；Device for the training pattern layer in the duration of identification；And it is used for each mould
The processing of type layer is the device of current check point.
In another aspect, system described above is provided, wherein the system further comprises for removing for every
The device in preceding checkpoint of a model layer；And the device for each model layer of re -training.
In another aspect, system described above is provided, wherein the system further comprises for relative to multiple
Data element usually verifies the device of each checkpoint in multiple current check points；And for will be in multiple current check points
Authenticated checkpoint is stored in the device at memory.
In another aspect, system described above is provided, wherein the system further comprises for receiving multiple turns
Change the device of data, wherein conversion data indicates conversion activity associated with online advertisement is distributed.
Feature, function and advantage described herein can individually realize in each example of the disclosure, or can be
It is combined in other embodiments, with reference to described below and attached drawing, it will be appreciated that further details.
Detailed description of the invention
Fig. 1 is the figure of depicted example advertising environments；
Fig. 2 is the frame of the calculating equipment of the order training method for machine learning framework as shown in the advertising environments of Fig. 1
Figure；
Fig. 3 is the typical training system for training without using the machine learning framework of system and method described herein
Known procedure；
Fig. 4 is the machine learning framework being used for order training method in advertising environments shown in Fig. 1 in the calculating equipment of Fig. 1
Training system example data process figure；
Fig. 5 be include evening reach data processing Fig. 4 training system example data process figure；
Fig. 6 is the illustrative methods that the order training method of machine learning framework is used for using the advertising environments of Fig. 1；And
Fig. 7 is the figure of the component for the one or more exemplary computer devices that can be used in environment shown in FIG. 1.
Although might show the special characteristic of each embodiment in some of the figures and be not shown in other figs., this is only
It is for convenience.Any feature of any figure can be cited and/or be claimed in conjunction with any feature of any other figure.
Specific embodiment
The following detailed descriptions realized refer to attached drawing.Same reference numbers in different figures can identify same or similar
Element.Moreover, following detailed description does not limit claim.
Theme described herein is usually directed to machine learning framework, such as training of gradient decline machine learning framework.Such as
It is used herein, machine learning system be can any combination based on applied analysis method, numerical method or these methods from data
The system of study.Specifically, method and system described herein makes it possible to receive multiple data elements, wherein each data element
Element is associated with timestamp, to each model layer of the hierarchical stack of model layer, training window is determined, by utilizing and each training
The corresponding time-stamp Recognition data element of window, determines the multiple training data elements for being used for each trained window, and identification is used
In each model layer in preceding checkpoint, wherein generated for each model layer by father's model layer in preceding checkpoint, using pair
The training data element that each model layer determines is with identification for each model layer in preceding checkpoint, each model of training
Layer, generates multiple current check points, and each current check point in plurality of current check point is associated with model layer, and
Multiple current check points are stored at memory.
In many examples, machine learning system can learn to carry out related new data based on other data in preceding analysis
Deduction.Machine learning framework can be used to create adaptive model based on including the data of such as conversion data.Turn over number
According to the potential customers that can indicate online advertisement master (" user ") action or do not take action.More specifically, conversion data can wrap
Include the information in relation to conversion action (interacting including click, purchase and with other of online advertisement master).In many examples, conversion
Data may include with user, advertiser, the context of conversion, the position of conversion and time information.It can analyze such
Conversion data determines the possibility mode of the conversion for the following online advertisement.
Although machine learning system can be used for pure analysis purpose, at least some machine learning systems are used to make fortune
Battalion determines.For example, the machine learning system for understanding conversion data can help to distribute and manage online in the case where online advertisement
Advertising campaign.The knowledge of the raising of possibility mode in relation to the conversion for the following online advertisement, which can help to enhance, to be distributed and manages
Online advertisement activity.
Whether be used to carry out operation decision regardless of machine learning system, it is expected that machine learning system balance will be used in operation or
At least three different considerations in analysis ability.Firstly, being used to as complete as possible convenient for the data set of machine learning or " training "
It is critically important.It is not useable for the degree of machine learning system in data, machine is potentially based on by the model ratio of such system development
Study is more inaccurate with the hypothesis that more extensive data improve.Therefore, when the asynchronous arrival of data, it may be desirable to a series of handling
Before training data is with the training model, the data of evening arrival are waited.
Secondly, critically important in operating environment in order to be used in by the model stability that machine learning system generates.Any arrival
Data model will be made by re -training and change.If the asynchronous arrival of data, the data that evening reaches will lead to training pattern and change
Become.This brings problem to such training system, because model is in particular time range training then due to the data of evening arrival
And re -training.If the model for make operation determine, influence be the model may indicate that based on specific set of data one
Then a decision after receiving the data of evening arrival and training, may indicate that different decisions.
Third, model is as newest as possible and to respond environment critically important.In some cases, machine learning system learns
The feature of data can be substantially change.In the example to the machine learning of conversion data, advertisement, customer action or enabled production
Specific change with service can be significant change conversion data, therefore, cause to conversion data training model change.
By before machine learning system handles data application delay period will adapt to the stability and integrality of data
Consider.In other words, by waiting essentially all training data to reach, the stability of data and examining for integrality are adapted to
Consider.However, such delay periods are not suitable with newest consideration.This is because at least some cases, waiting substantially institute
There is training data to reach the responsiveness that can interfere the variation to environment.In other words, during such delay periods, environment
Condition can change, but model does not detect at some intervals or the variation in the duration.
Method and system described herein solves these three by using order training method model training machine learning framework
Misgivings.Order training method model includes the model stack with different delay periods.Each model and parent-child relationship in model stack
At least one of other models it is related.
In machine learning framework, it is critically important to distinguish event time.The time for causing the event for generating training data to occur
" event time " can be referred to as.In the case where some data include conversion data, event may be (such as aobvious with reference event
Show advertisement) it is related." reference time " can be referred to as with reference to the time of event.The time that machine learning system receives data can be with
Referred to as " receiving time ".The time of training data can be referred to as " training time ".The current time of system can be referred to as
" current time " or " wallclock timestamp ".
The minimum model layer of stack has the longest delay periods for receiving and handling evening arrival data.In other words, minimum mould
Therefore type layer, trains window to longest for the data training with the longest delay between event time and receiving time
The data training of mouth.In one example, minimum model layer may include the training data of the training window beyond 90 days to instruct
Practice the model for being used for bottom model layer.As long as the minimum model layer of stack is instructed on specified training window using all available training datas
Practice, submit the model for bottom model layer and creates checkpoint.
As used in this, " checkpoint " or " snapshot " refers to the shape for the model layer being trained to when creating checkpoint
State.Checkpoint can be the best fit for example for training data.In another example, checkpoint can more specifically indicate
The parameter of state for model.In at least some examples, it can be further simplified or reduce parameter to adapt to by for example same
Walk the processing of server or submodel layer.
In one exemplary embodiment, bottom model layer restarts to train after creating checkpoint.In alternative embodiment
In, in the case where training of the data without departing from bottom for guaranteeing that evening reaches delays to reach, or can safely ignore in this way
The data that reach of evening in the case where, bottom model layer not re -training after create checkpoint, and be to continue with trained and write a self-criticism
Point.Checkpoint is used as the reference of the training of the submodel layer of bottom model layer.Therefore, bottom model layer is the submodel of bottom model layer
Father's model layer of layer.
The submodel layer of bottom model layer has the training window more closer the bottom model layer of than.Submodel layer includes to have in the instruction
Practice the training data of the event time in window.Submodel layer includes the more training data of Short Training window and training is prolonged at this
Any data reached in the slow period.In one example, the submodel layer of bottom model layer may include training data, wherein
The training data has the event time before the time for initializing this layer between 90 days and 45 days, while training is used for bed die type
The model of the submodel layer of layer.Once the submodel layer of bottom model layer includes for specifying delay periods (for example, 90 days to 45
It) training data, bottom model layer submodel layer training for bottom model layer submodel layer model and create second
Checkpoint.Second checkpoint is used as the reference of the submodel layer in the submodel layer for training bottom model layer.Therefore, bed die
The submodel layer of type layer is the father of the submodel layer in the submodel layer of bottom model layer.
Model stack may include the multiple submodel layers similar with above-mentioned submodel layer.In general, each model layer training number
According to and comprising up to specific to the evening arrival data of the duration of the model layer.After period training, each model layer
Generating can be by submodel layer using starting trained checkpoint.It, can be with when each model layer has different trained windows
Checkpoint is not created simultaneously.As long as generating checkpoint on the contrary, each model layer trains window training at it.Therefore, it writes a self-criticism
The point time will depend on model layer and change.Model stack further includes top model layer.Top model layer is in model layer stack for highest mould
The submodel layer of type layer.Therefore, top model layer does not have submodel layer and has the shortest delay period.Top model layer can produce
Biopsy is made an inventory of, but is not used to train submodel layer, unless or until until adding new submodel layer.
Pay attention to identify that late arrival data are critically important by the period belonging to identifying first.For example, in specific time
Point, for example, on January 1st, 2015, system can receive multiple conversion datas.Each element of conversion data can be with difference
It is associated with reference to event.For example, data on January 1st, 2015 may include on October 1st, 2014, on November 1st, 2014 and
On December 1st, 2014 associated conversion data.In other words, the element of conversion data can be about 90 days, 60 days and 30 days.
Therefore, in this example, the conversion data as 90 days will be used to train bed die type on January 1st, 2015, but not wrapped directly
It is contained in any submodel layer.60 days conversion datas are used to train bottom model layer until close on April 1st, 2015.
30 days conversion datas are used to train the submodel layer of bottom model layer, cross the border until current time shifts to an earlier date conversion data enough
Into the training window of bottom model layer, in the time, conversion data is used to train bottom model layer.
Training calculates the hierarchical stack that equipment firstly generates model layer, wherein each model layer has layer depth and training window
And associated training delay.Bottom model layer have the training window of longest training window in minimum layer depth and model layer stack with
And top model layer has the training window of most Short Training window in highest layer depth and model layer stack.Training calculates equipment and determines mould
The training window of each model layer in the hierarchical stack of type layer.Matched by the layer depth of each model layer of determination, retrieval training pattern
It sets, the wherein specified training delay associated with each layer depth of training pattern configuration is recorded, known based on training pattern configuration
Training not associated with the layer depth of each model layer is postponed and is instructed based on the training delay for each model layer to calculate
Practice window, determines training window.Training calculates equipment and receives multiple data elements.Each data element is related to event time
Connection.Training calculates equipment by identification timestamp corresponding with each trained window to identify data element, determines for often
Multiple training data elements of a trained window.The beginning of training window is generally also generated in father's layer model of preceding checkpoint
The end of training window, or recent events exactly in the checkpoint refer to after time.The end of training window is to pass through instruction
Practice the current time of delay adjustment.In at least some examples, bottom model layer trains window to be not over the time it.
Other than trained bottom since empty model, training calculates equipment identification for each model layer in preceding inspection
Point.For being generated in preceding checkpoint by father's model layer for each model layer.If training calculates equipment without available in preceding checkpoint
It waits until father's model layer generates until preceding checkpoint.
Training calculates the training data element for each model layer that equipment utilization determines and is used for each mould with what is identified
Type layer trains each model layer at preceding checkpoint (if any).Again, it is to be noted that bottom model layer does not have in many examples
Have from its training in preceding checkpoint.Training calculates equipment by least one machine learning algorithm of application to train.Engineering
Practising algorithm can be any machine learning algorithm appropriate for being used together with the system and method.In exemplary reality
It applies in example, machine learning algorithm is gradient optimization algorithm.As an alternative, machine learning algorithm can be any machine learning and calculate
Method or training process, including but not limited to supervised learning algorithm, semi-supervised learning algorithm, unsupervised learning algorithm and reinforcing are calculated
Method.
In appropriate duration when training pattern layer (that is, until the passage of training window), it is raw that training calculates equipment
At the checkpoint of the model layer for each model layer.It as described herein, can be with because training window changes each model layer
Asynchronous write checkpoint.It is specified associated with each layer depth by the layer depth of determining each model layer, retrieval that training calculates equipment
Duration model configuration, training associated with the layer depth of each model layer is identified based on training pattern configuration
Duration and duration is determined based on layer depth and model configuration, determines duration appropriate.Training
Calculating equipment restarts the training process of model layer based on nearest father layer checkpoint.It is some at least some examples
Model layer can not restart training process.For example, bottom model layer trains in longest training window and independent of father
Model layer creates on it trained checkpoint.Therefore, restarting bottom model layer can take a long time.Similarly, the phase
The model layer of such as bottom is hoped to continue to train and write a self-criticism a little without restarting.Training calculates equipment and can also remove for mould
At least one of type layer is in preceding checkpoint, if such if preceding checkpoint exists.
Training calculates equipment and is based on checkpoint also to train submodel layer.Submodel layer has the layer depth smaller than model layer
(that is, relatively shallow in stack).Submodel layer also has the training window shorter than model layer.In other words, submodel layer includes to come
From the training data of the nearest window of event time, and compared with the model layer of bottom, the evening for being likely encountered incrementss reaches data.
Initially, all model layers are created, and wait the checkpoint of father to become available before training.Therefore, bottom model layer immediately
Start after training and after writing a self-criticism a little, the submodel layer of bottom model layer can start to train.Therefore, top model layer is to start to instruct
The last one experienced model layer.
When any generation can be used for synchronous current check point, training calculates equipment also by least one current check
Point is synchronous with external server.External server can be such as content server, Analysis server and its mixed operation clothes
Business device.External server is at least partially based on synchronous current check point service.
As proposed above, the level that each model layer is father is applied to immediately in mould above it by the hierarchical stack of model
Type layer, top model layer be not any model layer father except.It is therefore intended that father's model layer and submodel layer with parent-child close
System.As an alternative, in some instances, the hierarchical stack of model can be with application level, wherein each model layer is immediately above it
At least one model layer father.In such an example, father's model layer can have multiple submodel layers, the hierarchical stack of model
It can substantially indicate tree-shaped level.
Training calculates equipment and multiple current check points is stored at memory.As described above, because the training of variation is prolonged
Asynchronous it can occur with training window, storage late.Each of multiple current check points model layer corresponding with them and layer depth
It is associated.Training calculates equipment and in addition usually verifies each current check in multiple current check points relative to multiple data elements
Point.After such verifying, in memory by the storage of current check point.Therefore, storage is used for the current inspection of each model layer
Making an inventory of asynchronous can occur.In other words, current check point can be stored in different time for each model layer.
In the exemplary embodiment, multiple data elements indicate conversion data.Conversion data indicates and distributes online advertisement
Associated conversion activity.In other examples, multiple data elements can indicate any other type for machine learning
Data, rather than limit.
Such as from used, being attempted by "one" subsequent element of word or step with odd number record should be understood that and do not arrange
Except multiple elements or step, unless such exclusion is obviously recorded.In addition, to " a reality for theme disclosed herein
Apply example " reference be not intended to be interpreted exclude also merge documented by feature additional embodiment presence.
Computer programming or engineering technology can be used to realize in method and system described herein, and the computer is compiled
Journey or engineering technology include computer software, firmware, hardware or any combination thereof or its subset, wherein can be following by executing
At least one of step realizes these technical effects: (a) receiving multiple data elements, wherein each data element and when
Between stab it is associated；(b) the training window of each model layer of the hierarchical stack for model layer is determined；(c) by using with it is each
It trains the corresponding timestamp of window to identify data element, determines the multiple training data elements for being used for each trained window；
If (d) identified for each model layer in the presence of preceding checkpoint in preceding checkpoint, wherein by father for each model layer
Model layer is generated for each model layer in preceding checkpoint；(e) the determining training data member for each model layer is utilized
Element, and if any, using identification for each model layer in preceding checkpoint, each model layer of training；(f) it generates
Multiple current check points, wherein each current check point in multiple current check points is associated with model layer；It (g) will be multiple
Current check point is stored at memory；(h) by applying at least one machine learning algorithm, each model layer is made to adapt to determine
The training data element for each model layer；(i) layer depth of each model layer is determined；(j) retrieval training pattern configuration,
The middle specified training delay record associated with each layer depth of training pattern configuration；(k) it is configured, is identified and every based on training pattern
The associated training delay of the layer depth of a model layer；(l) based on the training delay for each model layer, training window is calculated；
(m) make external server with and at least one associated current check point of at least one model layer it is synchronous, wherein external service
Device is based at least partially on synchronous current check point service；(n) layer depth for being used for each model layer is determined；(o) retrieval training
Model configuration, wherein training pattern, which configures, specifies duration associated with each layer depth；(p) it is based on training pattern
Configuration identifies duration associated with the layer depth of each model layer；(q) it is instructed in the duration of identification
Practice model layer；(r) each model layer is handled as current check point；(s) it removes for each model layer in preceding checkpoint；
(t) each model layer of re -training；(u) relative to multiple data elements, each checkpoint in multiple current check points is verified；
(v) checkpoint by the verifying in multiple current check points is stored at memory；And multiple conversion datas (w) are received,
Middle conversion tables of data shows conversion activity associated with online advertisement is distributed.
Fig. 1 is the figure of illustrative exemplary online content environment 100.Online content environment 100, which can be used in, combines online hair
In the context that cloth distributes online advertisement to user (user including mobile computing device).With reference to Fig. 1, example context 100 can
To include one or more advertisers 102 (that is, online content supplier), one or more publishers 104, advertising management system
System (AMS) 106 and one or more user access devices 108 that network 110 can be couple to.User access device by with
Family 150,152 and 154 uses.Each of element 102,104,106,108 and 110 in Fig. 1 can use hardware component, soft
Any combination of part component or fastener components or such component is realized or is associated with it.Element 102,104,106,
108 and 110 can be realized or therewith with such as generic server, software process and engine, and/or various embedded systems
It is associated.Element 102,104,106 and 110 can be used for example as ad distribution network.Although with reference to distributing advertisement is related to,
Environment 100 may adapt to the content of the other forms for the sponsored content that distribution includes other forms.AMS 16 can also be claimed
For Content Management System 106.
Advertiser 102 may include any entity associated with advertisement (" ad ").Advertisement or " ad " refer to any form
Communication, identify and publicize wherein (or communication) one or more products, service, theory, message, people, mechanism or its
Its project.Advertisement is not limited to business promotion or other communications.Advertisement can be the logical of public service bulletin or any other type
Know, such as the common notification to print or e-newspaper or broadcast mode are issued.Advertisement can be referred to as sponsor content.
Advertisement can be communicated via a variety of media and in a variety of forms.In some instances, advertisement can pass through
The interactive media of such as internet is communicated, and may include graphical advertisement (such as banner), text advertisements, figure
As advertisement, audio advertisement, video ads, the advertisement or any form of one or more of any such component are combined
Electron transmission advertisement.Advertisement may include embedded information, and such as embedded media, link, metamessage and/or machine can
It executes instruction.Advertisement can also by RSS (really simple syndication) feeding, radio channel, television channel, print media or
Other media are communicated.
Term " advertisement " can refer to single " intention " and " advertisement group ".Intention refers to any reality for indicating an ad impression
Body.Ad impression refers to any appearance form of advertisement, so that can be checked/be received by user.In some instances, when in user
When showing advertisement in the display equipment of access equipment, ad impression can occur.Advertisement group, which can refer to, for example indicates shared total
The entity of the intention group of same characteristic (such as with same advertisement selection and proposed standard).Advertisement group can be used to create advertisement living
It is dynamic.
Advertiser 102 can provide (or associated with it in another manner) product relevant to advertisement and/or service.Extensively
Accuse main 102 may include or be associated with for example retailer, whole seller, warehouse, manufacturer, dealer, healthcare provider,
Educational institution, financial institution, technology supply person, electricity providers, infrastructure supplier or any other product or service
Supplier or dealer.
Advertiser 102 directly or indirectly can generate and/or safeguard advertisement, can with as provided by advertiser or
Person is related with the associated product of advertiser or service in another manner.Advertiser 102 may include or maintenance is coupled to network
110 one or more data processing systems 112, such as server or embedded system.Advertiser 102 may include or tie up
Shield operates in one or more processes in one or more data processing systems.
Publisher 104 may include in generating, safeguard in environment 100, providing, presenting and/or handling in other ways
Any entity held." publisher " specifically includes the author of content, wherein author can be individual, or employ work some
In the case where works out, the personal owner for being responsible for creation online content is employed.Term " content " refers to various types of
Information that is based on web, based on software application and/or presenting in other ways, including article, discussion clue, report, point
Analysis, financial statement, music, video, figure, search result, web page listings, information feeding (such as RSS feeding), TV are wide
It broadcasts, radio broadcasting, printed publication, or is presented to use using one calculating equipment in such as user access device 108
The information of any other form at family.
In some implementations, publisher 104 may include the content provider that there is internet to present, such as Online release
With newsprovider's (such as online newspaper, online magazine, TV network station etc.), online service supplier (for example, financial service mentions
Donor, health service supplier etc.) etc..Publisher 104 can include software application supplier, television broadcasting, radio broadcasting,
Satellite broadcasting and other content supplier.One or more of publisher 104 can indicate content associated with AMS 106
Network.
Publisher 104 can receive request from user access device 108 (or other elements in environment 100) and will
Content provides or is presented to request equipment.Publisher can provide via various media or in a variety of manners or presentation content, packet
Include based on web's and/or be not based on the medium and form of web.Publisher 104 can be generated and/or safeguard such content and/
Or content is retrieved from other Internet resources.
In addition to content, publisher 104 may be configured to the content that will be retrieved with and the content retrieved it is related or
Relevant other properties collection (such as advertisement) integration is combined to show to user 150,152 and 154.As further below
Discuss, these relevant advertisements can provide and can be combined with content from AMS 106 with to user 150,152 and
154 displays.In some instances, publisher 104 can retrieve content to show in specific user access device 108, so
The content is transmitted to user access device 108 together with code afterwards, the code makes one or more from AMS 106
A advertisement is displayed to user 150,152 and 154.As used in this, user access device 108 can also be referred to as visitor
Family calculates equipment 108.In other examples, publisher 104 can retrieve content, retrieve one or more relevant advertisements (for example,
From AMS 106 or advertiser 102), integrate these advertisements and article then to be formed for showing to user 150,152 or 154
Content page.
As described above, one or more of publisher 104 can indicate content network.In such an implementation, advertisement
Main 102 can be presented advertisement to user by the content network.
Publisher 104 may include or safeguard the one or more data processing systems 114 for being coupled to network 110, such as
Server or embedded system.These data processing systems 114 may include or maintenance operation on a data processing system one
A or multiple processes.In some instances, publisher 104 may include the one or more for storage content and other information
Content library 124.
AMS 106 manages advertisement and provides various clothes to advertiser 102, publisher 104 and user access device 108
Business.Advertisement can be stored in advertisement base 126 by AMS 106, and convenient for being divided by environment 100 to user access device 108
Hair or selectivity offer and recommended advertisements.In some configurations, AMS 106 may include or access with management online content and/
Or the associated function of online advertisement, specifically to distribute online content to mobile computing device and/or online advertisement is related
The function of connection.
AMS 106 may include the one or more data processing systems 116 for being coupled to network 110, such as server or
Embedded system.It can also include one or more processes, such as server process.In some instances, AMS 106 can wrap
Include ad serving system 120 and one or more back-end processing systems 118.The ad serving system 120 may include one or
Multiple data processing systems 116 and it can execute and deliver the associated function of advertisement with to publisher or user access device 108
Energy.Back-end processing system 118 may include one or more data processing systems 116, and can execute and identify and to deliver
Relevant advertisements, the various rules of processing, execute filter process, generate report, maintenance account and use information and other rear ends
The associated function of system processing.AMS 106 can be come selectively using back-end processing system 118 and ad serving system 120
Relevant advertisements are recommended from advertiser 102 by publisher 104 and are provided to user access device 108.
AMS 106 may include or access one or more is crawled, indexed and search module (not shown).These modules can
Information is identified, indexes and stored to browse accessible resource (for example, WWW, publisher's content, data feeding etc.).Module
It may browse through the copy of information and creation for the browsing information of subsequent processing.Module can also verify link, identifying code, result
Information and/or execute other maintenance or other tasks.
Search module can be from each of such as WWW, publisher's content, Intranet, news cluster, database and/or catalogue
Kind resource searching information.Search module can search for data using one or more known search or other processes.Some
In realization, search module can index the content crawled and/or feed received content from data and search to construct one or more
Rustling sound draws.Search index can be used to be convenient for quick-searching information relevant to search inquiry.
AMS 106 may include for providing one of various features to advertiser, publisher and user access device
Or multiple interfaces or front-end module.For example, AMS 106 can provide one or more publisher's front end interfaces (PFE), for permitting
Perhaps publisher interacts with AMS 106.AMS 106 can also provide one or more advertiser's front end interfaces (AFE), for permitting
Perhaps advertiser interacts with AMS 106.In some instances, which may be configured to web application, mention to user
It is accessed for the network to available feature in AMS 106.
AMS 106 provides various advertising management features to advertiser 102.The characteristic of advertisement of AMS 106 can permit user
Establish user account, setting account preference, creation advertisement, be Advertising selection keywords, be that multiple products or service creation are movable
Or it proposes, checks report associated with account, analysis cost and rate of return on investment, selectively identifies disappearing in different zones
Fei Zhe, selectively recommend and provide advertisement, analyzing financial information, analysis advertisement performance, assessment ad stream to particular delivery person
Amount, access key tool add figure and animation etc. into advertisement.
AMS 106 can permit the pass of those of advertiser 102 creates advertisement and it will occur in input advertisement
Key word or other location advertising descriptors.In some instances, when keyword associated with advertisement is included in user's request
Or when in requested content, AMS 106 can provide those advertisements to user access device or publisher.AMS 106 may be used also
It is bid with allowing advertiser 102 that advertisement is arranged.Bid can indicate that advertiser is ready for the user of each ad impression, advertisement
It clicks through or interacts the Maximum Amounts paid with the other of advertisement.Click-through may include that user is to select advertisement and take any dynamic
Make.Other movements include generating the touch feedback clicked through or gyroscope feedback.Advertiser 102 is it is also an option that currency and monthly pre-
It calculates.
AMS 106 can also allow for advertiser 102 to check the information about ad impression, and the information of the ad impression can
To be maintained by AMS 106.AMS 106 is configured for determining and safeguarding related with specific website or keyword wide
Accuse the number flashed.The ratio that AMS 106 can also determine and safeguard the number of the click-through of advertisement and click through and flash.
It is advertisement selection and/or creation conversion type that AMS 106, which can also allow for advertiser 102,.When user completes and gives
When determining the related transaction of advertisement, " conversion " can occur.Conversion can be defined as (such as to pass through tactile when user is direct or implicit
Or gyroscope feedback) click advertisement, quote the webpage of advertiser and leave when completing to buy before webpage.At another
In example, conversion can be defined as in the display of predetermined time (for example, 7 days) the interior advertisement to user and advertiser's
It is bought accordingly on webpage.Conversion data and other information can be stored in conversion data library 136 by AMS 106.
AMS 106 can permit advertiser 102 and input description information associated with advertisement.The information can be used for auxiliary
Help the advertisement to be issued of 104 determination of publisher.Advertiser 102 can in addition input it is associated with selected conversion type at
Sheet/value, all each products for example bought or service to publisher provide five dollars pay a bill.
AMS 106 can provide various features to publisher 104.When user accesses from the content of publisher 104,
AMS 106 can transmit advertisement (associated with advertiser 102) to user access device 108.AMS 106 may be configured to pass
Pass advertisement relevant to publisher website, web site contents and publisher audient.
In some instances, AMS 106 can crawl the content provided by publisher 104 and based on the content crawled
Transmit advertisement relevant to publisher website, web site contents and publisher audient.AMS 106 be also based on user information and
Behavior, all particular search queries executed on search engine web site as described herein or is specified for the wide of subsequent comments
Accuse etc., selectively recommend and/or provide advertisement.User related information can be stored in Universal Database 146 by AMS 106
In.In some instances, search service can be added to publisher website and transmit advertisement by AMS 106, the advertisement quilt
It is configured to provide the suitable and relevant interior of the request search result generated relative to the visitor from publisher website
Hold.The combination of these and other method can be used to transmit relevant advertisements.
AMS 106 can permit publisher 104 search for and select specific products & services and will with by issuing
The associated advertisement that content provided by person 104 is display together.For example, publisher 104 can search for extensively in advertisement base 126
It accuses, and selects certain advertisements for displaying together with its content.
AMS 106 may be configured to selectively recommend simultaneously directly or by publisher 104 to user access device 108
The advertisement created by advertiser 102 is provided.When user requests search result or loading content from publisher 104, AMS 106
The property of can choose to user access device particular delivery person 104 (will such as be described in further detail herein) or requested
108 recommend and provide advertisement.
In some implementations, AMS 106 can manage and handle in the element in environment 100 and between finance hand over
Easily.For example, AMS 106 can pay a bill to account associated with publisher 104, and to the account button account of advertiser 102.These
It can be based on the conversion data for being received and being safeguarded by AMS 106, information of flashing and/or click-through rate with other transaction.
Such as " the calculating equipment " of user access device 108 may include that can receive any of information from network 110 to set
It is standby.User access device 108 can including the use of the specific components for executing particular task optimize general-purpose computations component and/or
Embedded system.The example of user access device includes personal computer (such as desktop computer), mobile computing device, honeycomb
Phone, smart phone, wear-type calculate equipment, media player/recorder, music player, game console, in media
It is the heart, media player, electronic plane, personal digital assistant (PDA), television system, audio system, radio system, dismountable
Store equipment, navigation system, set-top box, other electronic equipments etc..User access device 108 can also include various other elements,
Such as operate in the process on various machines.
Network 110 may include any element or system of the communication facilitated between the neutralization of each network node, all
Such as element 108,112,114 and 116.Network 110 may include one or more telecommunication networks, such as computer network, phone
Or other communication networks, internet etc..Network 110 may include shared, public or private data network, cover wide area
(such as WAN) or local (such as LAN).In some implementations, network 110 can be by using Internet protocol (IP) packet switch
Mode convenience data exchange.Network 110 can contribute to connectivity and communication wiredly and/or wirelessly.
The purpose only explained describes some aspects of the disclosure with reference to discrete elements illustrated in Fig. 1.Environment
The number, mark and arrangement of element are not limited to shown in 100.For example, environment 100 may include it is any number of geographically
Advertiser 102, publisher 104 and/or the user access device 108 of dispersion can be discrete, integrated module or divide
Cloth system.Similarly, environment 100 is not limited to single AMS 106, but may include any number of integrated or dispersion
AMS system or element.
In addition, unshowned additional and/or different element can be contained in or be coupled to member shown in figure 1
In part, and/or certain illustrated elements may be lacked.It in some instances, can be by being less than illustrated package count
Purpose component is even executed the function that illustrated element provides by single element.Illustrated element can be implemented
To operate in the individual process on discrete machine, or operate in the single process on single machine.
Fig. 2 be as shown in advertising environments 100 (shown in Fig. 1) for dynamic online content to be automatically communicated to move
The block diagram of the dynamic calculating equipment 200 for calculating equipment.
Fig. 2 shows be intended to indicate that (such as on knee, desk-top, work station, individual digital help various forms of digital computers
Reason, server, blade server, host and other suitable computers) universal computing device 200 example.Calculate equipment 200
Also aiming to indicates various forms of mobile devices, such as personal digital assistant, cellular phone, smart phone and other similar meter
Calculate equipment.Component, their connection and relationship shown in this article and their function are only examples, and are not intended to limit this
Described in document and/or the realization of required theme.
In the exemplary embodiment, calculating equipment 200 can be user access device 108 or data processing equipment 112,114
Any one of or 116 (shown in Fig. 1).Calculate equipment 200 may include bus 202, processor 204, main memory 206,
Read-only memory (ROM) 208, storage equipment 210, input equipment 212, output equipment 214 and communication interface 216.Bus 202
It may include the access for allowing to communicate between the component for calculating equipment 200.
Processor 204 may include any kind of conventional processors, microprocessor or explanation and the processing that executes instruction
Logic.Processor 204 can be handled for calculating the instruction executed in equipment 200, be included in memory 206 or set in storage
The instruction stored on standby 210 on external input/output device (being such as couple to the output equipment 214 of high-speed interface) to show
Graphical information for GUI.It in other implementations, where appropriate, can be in conjunction with multiple memories and multiple memorizers, using more
A processor and/or multiple buses.Furthermore, it is possible to connect multiple calculating equipment 200, each equipment provides one of necessary operation
Divide (such as server group, blade server group or multicomputer system).
Main memory 206 may include random access memory (RAM) or another dynamic memory, store information
With the instruction executed by processor 204.ROM 208 may include common ROM device or another static storage device, storage
The static information used by processor 204 and instruction.Main memory 206 is stored in the information calculated in equipment 200.In a reality
In existing, main memory 206 is volatile memory cell.In a further implementation, main memory 206 is non-volatile memory cells.
Main memory 206 can also be another form of computer-readable medium, such as magnetically or optically disk.
Storage equipment 210 may include magnetic and/or optical recording medium and its corresponding driving.Storing equipment 210 can be
It calculates equipment 200 and massive store is provided.In one implementation, storage equipment 210 can be or comprising computer-readable medium,
Such as floppy device, hard disc apparatus, compact disk equipment or carrying device, flash memory or other similar solid storage device or equipment battle array
Column, including the equipment in storage area networks or other configurations.Computer program product can be realized visibly in the information carrier.Meter
Calculation machine program product can also execute when executed one or more methods comprising instruction, described instruction, all as described above
Method.Information carrier is computer or machine-readable media, such as main memory 206, ROM 208, storage equipment 210 or place
Manage the memory on device 204.
High-speed controller management is used to calculate the bandwidth-intensive operations of equipment 200, and low speed controller management is compared with low strap
Wide intensive.Such function distribution is merely to example.In one implementation, high-speed controller is couple to main memory
206, it display 214 (such as passing through graphics processor or accelerator) and is couple to and can accommodate various expansion card (not shown)
High-speed expansion ports.In the implementation, low speed controller is couple to storage equipment 210 and low-speed expansion port.Low-speed expansion end
Mouthful, it may include various communication port (for example, USB, bluetooth, Ethernet, wireless ethernet), one or more can be couple to
Input-output apparatus, such as keyboard, indicating equipment, scanner or networked devices, such as switch or router, such as pass through
Network adapter.
Input equipment 212 may include allow calculate equipment 200 from user 150,152 or 154 receive order, instruction or
Other inputs, including common mechanism such as vision, audio, touch, button press, stylus clicks.In addition, input equipment can be with
Receive location information.Therefore, input equipment 212 may include for example camera, microphone, one or more button, touch screen and/
Or GPS receiver.Output equipment 214 may include the common mechanism that information is output to user, including display (including touch
Screen) and/or loudspeaker.Communication interface 216 may include the mechanism of any similar transceiver, make to calculate equipment 200 and other
Equipment and/or system communication.For example, communication interface 216 may include for the net via such as network 110 (shown in Fig. 1)
The mechanism of network and another equipment or system communication.
As described herein, equipment 200 is calculated to be convenient for that the content from one or more publishers is presented to user, and
The set of one or more sponsored contents, such as advertisement.Calculating equipment 200 can execute in response to processor 204 included in all
As memory 206 computer-readable medium in software instruction and execute these and other operations.Computer-readable medium can
To be defined as physically or logically memory device and/or carrier wave.It can be by software instruction from the another of such as data storage device 210
One computer-readable medium is read in memory 206 via communication interface 216 from another equipment.Included in memory 206
In software instruction processor 204 can be made to execute process described herein.As an alternative, hard-wired circuitry can be used, instead of
Or it is realized and the consistent process of theme in this in conjunction with software instruction.It is consistent with the principle of theme disclosed herein as a result,
It is practiced without limitation to any specific combination of hardware circuit and software.
It can realize in many different forms and calculate equipment 200, as shown in the figure.For example, standard can be implemented these as
Server, or more the time carry out be such server group in.It can also implement these as the portion of frame server system
Point.Furthermore, it is possible to personal computer is implemented these as, such as laptop computer.Each of such equipment may include
One or more calculates equipment 200 and whole system and can be made of the multiple calculating equipment 200 to communicate with one another.
Processor 204 can execute the instruction calculated in equipment 200, including the instruction stored in main memory 206.Place
Reason device can be implemented as including independent and multiple analog- and digital- processors chips.Processor can provide such as equipment 200
Other assemblies coordination, the control of such as user interface, by equipment 200 run application and equipment 200 wireless communication.
Equipment 200 is calculated in addition to the component for including such as receiver and transceiver, further includes processor 204, main memory
206, ROM 208, input equipment 212, the output equipment of such as display 214, communication interface 216.Equipment 200 can also provide
There are storage equipment 210, such as micro-move device or other equipment, to provide additional storage.Each of component is mutual using various buses
Even and several components may be mounted in common motherboard or in another appropriate manner.
Calculating equipment 200 can wirelessly be communicated by communication interface 216, and when necessary, communication interface 216 may include number
Word signal processing circuit.Communication interface 216 can be provided in the communication under various modes or agreement, wherein such as GSM voice is exhaled
It cries, SMS, EMS, MMS message, CDMA, TDMA, PDC, WCDMA, CDMA2000 or GPRS.Such communication can for example pass through
RF transceiver occurs.In addition, short haul connection can be such as using bluetooth, WiFi or other such transceiver (not shown)
Occur.In addition, GPS (global positioning system) receiver module can provide other navigation with position in relation to nothing to equipment 200
Line number evidence, where appropriate, its can by run on the device 200 using.
Fig. 3 is the typical training system for training without using the machine learning framework of system and method described herein
300 known procedure.Training system 300 does not use order training method method described herein clearly.On the contrary, training system 300 makes
With the training of single model 306.Therefore, because above-mentioned reason, training system 300 faces the evening arrival for reaching data 312 in such as evening
The difficulty of data, because evening reaches data 312 and influences to be used to the current of the integrality of the data of training pattern, stability and data
Property.
In training system 300, data 302 are received from computer equipment 301.In the exemplary embodiment, computer is set
Standby 301 provide conversion data 302.Conversion data 302 is analyzed by back-end system 305.More specifically, back-end system 305 is trained
Calculate equipment 305.Training calculates equipment 305 and data 302 is parsed into training data 304, is applied to training pattern 306 simultaneously
And generate housebroken model 308.When (reference point relative to transformation event) provides data 302 in a manner of timely, through instructing
Experienced model 308 is very accurate.However, when that night arrival data 312 are present in training system 300, housebroken model
308 do not reflect all possible data, and are only its subset.Therefore, when housebroken model 308 is synchronous with Advertisement Server 350
When 310, since evening reaches data 312, synchronous 310 is unreliable.
Training system 300 receives evening arrival data 312 after some delay.Equipment 305A is calculated by training and receives evening arrival
Data 312.Training, which calculates equipment 305A, indicates that the training that the later time point that data 312 reach is reached in evening calculates equipment
305.Evening is reached into data 312 and resolves to late arrival training data 314 and for housebroken model 308 to be updated to again
Trained model 316.The property of data 312 is reached depending on evening related with data 302, the model 316 of re -training is therefore
It is markedly different from housebroken model 308.Therefore, when the model 316 and 350 re-synchronization of Advertisement Server for making re -training
When, re-synchronization 318 may be significantly different from synchronous 310.Therefore, after synchronous 310 and after re-synchronization 318, advertising service
Device 350 can make dramatically different operation and determine.Therefore, because shown reason, is handled when calculating equipment 305 by training
When evening reaches data 312, training system 300 encounters undesirable result.
Fig. 4 is will be by the order training method of machine learning framework based on training in advertising environments 100 (shown in Fig. 1)
Calculate the example data process figure of the training system 400 in equipment 116.Compared with training system 300, training system 400 is by mould
Type layer stack 420 is used for machine learning.In the exemplary embodiment, model layer stack 420 includes three layers, pushes up model layer 422, intermediate die
Type layer 424 and bottom model layer 426.In alternative embodiment, any number of model layer 422,424 and 426 may include in mould
In type layer stack 420.As described above, each model layer 422,424 and 426 have instruction training data 404 can be applied to it is each
The different training windows of layer.In other words, it is assumed that training data 404 have correspond to for particular model layer 422,424 or
The reference event of 426 training window, each model layer 422,424 and 426 will be trained training data 404.In addition, every
The training in the duration for corresponding to training window of a model layer 422,424 and 426.In each model layer 422,424
After reaching its corresponding duration with 426 training, corresponding checkpoint 432,434 and 436 is written.In write-in checkpoint
432, after 434 and 436, model layer 422,424 and 426 usually resets and starts the checkpoint training according to corresponding father's layer.
However, as noted previously, as bottom model layer 422 does not have father's layer and has the fact that longest training window, such as bed die type
Some model layers of layer 422 can not re -training.On the contrary, such model layer will continue training without resetting.In addition, resetting
The value of upper model layer is originated from the fact that it may include the new data of the checkpoint from father's model layer.Again, because of bed die
Type layer 422 does not have father's model layer, will not obtain the value.
In operation, data 402 are sent instruction by Advertisement Server 112 associated with advertiser 102 (shown in Fig. 1)
Practice and calculates equipment 116.In the exemplary embodiment, data 402 are conversion datas.In alternative embodiment, data 402 be can be
Any data being suitable for use in training machine study framework.Data 402 are parsed into training data by training server 116
404.Data are parsed into training data 404 and include at least identification reference associated with each data element in data 402
Event.
Training calculates equipment 116 and retrieves training pattern configuration 428.Training pattern configuration 428 indicates to define training system 400
Characteristic configuration file, more specifically, model layer stack 420.Training pattern configuration 428 include for each model layer 422,
424 and 426 training window, duration and training identifier.Training pattern configuration 428 passes through model layer 422,424
Depth with 426 identifies each model layer.In the exemplary embodiment, bottom has depth " 2 ", and middle layer has depth
" 1 ", and top layer has depth " 0 ".Therefore, training calculates the retrieval of equipment 116 corresponding to each layer depth (correspondingly, each model
Layer 422,424 and training window 426).
Training calculates equipment 116 and passes through the training data 404 that will be parsed applied to each model layer 422,424 and 426
Training data 404 is divided into the training data 406 of segmentation by determining training window.In at least some examples, the training of segmentation
Data 406 can be applied to more than one trained window and more than one model layer 422,424 and 426.In one example,
Top model layer 422 starts to the training of intermediate checkpoint 434, then, the training of training data 406 to segmentation.Meanwhile first premise
For middle inspection point 434 and be then reset to bottom checkpoint 436 mid-module layer 424 can train it is at least some with
Push up the identical event of model layer 422.Therefore, it is only handled by the model layer 422,424 and 426 with corresponding trained window appropriate
Training data 404.
The training data 406 of segmentation is used to train each model layer 422,424 and 426.Training is indicated at least one
Machine learning algorithm is applied to the processing of the training data 406 of segmentation.In the exemplary embodiment, decline engineering using gradient
Algorithm is practised to train the training data 406 of segmentation.In other embodiments, any machine learning algorithm appropriate can be used.
Trained up to after duration, each model layer 422,424 and in each model layer 422,424 and 426
The corresponding checkpoint of 426 write-ins or snapshot 432,434 and 436.When recognition training continues in training pattern 428 files of configuration
Between.Therefore, training calculates equipment 116 and determines duration based on training pattern configuration 428, when duration knot
Shu Shi, each model layer 422,424 and 426 of deconditioning, and it is respectively written into checkpoint 432,434 and 436.Checkpoint
432, the state of the 434 and 436 each model layers 422,424 and 426 of expression can be used to do the seed of other layers and make
Operation determines.After each checkpoint 432,434 and 436 is written, associated model layer 422,424 and 426 restarts to instruct
Practice.In at least some examples, training calculates equipment 116 can inspection of the removing in preceding write-in after new checkpoint is written
Point.
Non- bottom model layer 422 and 424 (not being the model layer of bottom model layer 426) is extraly generated by retrieval by father's layer
Checkpoint be trained.In other words, non-bottom model layer 422 and 424 retrieves checkpoint 434 and 436 and respectively to segmentation
Training data 406 training.Therefore, non-bottom model layer 422 and 424 is trained based on the machine learning of lower layer 424 and 426.Such as
Lower described, the method dependent on the checkpoint from lower layer arrives convenient for improving the evening that such as evening reaches data 312 (shown in Fig. 3)
Up to the processing of data.
Checkpoint 432,434 and 436 can calculate the verifying of equipment 116 additionally by training to check.More specifically, instruction
Practicing calculating equipment 116 ensures before memory 206 (shown in Fig. 2) is written in each checkpoint 432,434 and 436, checkpoint
432, the 434 and 436 prediction requirement for meeting checkpoint.
Top checkpoint 432 can be used to same with the AMS server 116 that is used as ad serving system 120 (shown in Fig. 1)
Step 440.As an alternative, any checkpoint 432,434 and 436 synchronous with AMS 116 440 can be used.As described below, different
The same problem of data is reached in the evening that training system 300,400 will not encounter such as evening arrival data 312.
Fig. 5 is the example data process figure for including the training system 400 that processing evening reaches data 510.Different from training
System 300 obtains evening arrival data by model layer 422,424 and 426 depending on the corresponding trained window of each model layer
510.It carries out at any time, evening reaches data 510 and is then included in each checkpoint and each model layer.Therefore, by by
The checkpoint of each layer of write-in, evening reach data 510 and are comprised in the training of sublayer.
In the exemplary embodiment, it includes associated with the reference event between 30 and 70 days that evening, which reaches data 510,
Big data set.Because evening, which reaches data 510, has large capacity, it is likely that the stabilization for the model being trained to it can be interfered
Property.In addition, it is not nearest data that evening, which reaches data 510,.Therefore, handling late arrival data 510 by top model layer 422 will not mention
For current data and the model for pushing up model layer 422 may be made unstable.Therefore, model layer stack 420, which is realized, reaches number comprising evening
According to 510 without will lead to unstability or deviate the benefit of current data.
Fig. 6 is the illustrative methods 600 using the order training method of the machine learning framework of advertising environments 100 (shown in Fig. 1).
Method 600 calculates equipment 116 by training and realizes.Training calculates equipment 116 and receives more than 620 a data elements, wherein each data
Element is associated with timestamp.Receiving 620 indicates that receiving such as data 402 and evening reaches the data of data 510, wherein such
Data 402 and 510 are associated with timestamp.Timestamp can be such as event time.In the example of conversion data, timestamp
It can be the time for being distributed to the conversion of advertisement of user.As an alternative, it can be the processing with data 402 and 510 with reference to event
Any other relevant timestamp.
Training calculates equipment 116 and determines the 620 training window for each model layer of hierarchical mode layer stack.Determine 620
Indicate training window of the identification for each model layer 422,424 and 426 (shown in Fig. 4) of model layer stack 420 (shown in Fig. 1).
428 (shown in Fig. 1) are configured by the layer depth of each model layer 422,424 and 426 of determination, retrieval training pattern, wherein training mould
The specified training delay record associated with each layer depth of type configuration 428, determines each trained window.Training calculates equipment 116
Training pattern configuration 428 is based further on to identify that training associated with the layer depth of each model layer 422,424 and 426 is prolonged
Late, and based on the training for each model layer 422,424 and 426 postpone to calculate trained window.
Training calculates the data element that equipment 116 has timestamp corresponding with each trained window by identification, really
Fixed 630 are used for the training data element of each trained window.Accordingly, it is determined that 630 indicate identification have with each model layer 422,
The subset of the data 402 and 510 of the 424 and 426 corresponding timestamp of training window.
If training calculates the further identification 640 of equipment 116 for every for existing in preceding checkpoint for each model layer
A model layer in preceding checkpoint, wherein being generated for each model layer by father's model layer in preceding checkpoint.Identification 640 indicates
Training calculates the checkpoint 432,434 and 436 that the identification of equipment 116 is used for each model layer 422,424 and 426.More specifically, knowing
It is other 640 indicate training calculate equipment identification by each model layer 422,424 and 426 father's model layer generate checkpoint 432,
434 and 436.In at least some examples, as described herein, bottom model layer 426 may not have the inspection generated by father's model layer
It makes an inventory of.Therefore, in such an example, because existing without checkpoint, nonrecognition checkpoint.
Training calculates equipment 116 additionally by the training data element determined to each model layer and to each model layer
Identification in preceding checkpoint, if any, training 650 each model layers.Training 650 indicates to calculate at least one machine learning
Method is applied to the place of the training data 406 to each checkpoint and segmentation of each model layer 422,424 and 426 identification 640
Reason.As described above, may be not present at least some examples by the checkpoint that father's model layer of bottom model layer generates.Therefore,
In such an example, training, which calculates equipment 116, can train 650 without using in preceding checkpoint.In such an example, it instructs
Practice and calculates equipment 116 to training data element training 650, without training since checkpoint.
Training calculates equipment 116 and also generates more than 660 a current check points, wherein each of multiple current check points are worked as
Preceding checkpoint is associated with model layer.Generating 660 indicates to create the state of each model layers 422,424 and 426, allow to by
The state is received as checkpoint 432,434 and 436 and is used to train at least one model layer 422,424 and 426.
Training calculates equipment 116 additionally by the storage 670 of multiple checkpoints at memory.Storage 670 indicates will at least
One checkpoint 432,434 and 436 is stored at memory 206 (shown in Fig. 2).
Fig. 7 is one or more of training calculating equipment 116 (shown in Fig. 4) for that can be used in environment 100 (Fig. 1)
The schematic diagram 700 of the component of a exemplary computer device.
For example, calculating, one or more of equipment 200 can form Advertising Management System (AMS) 106, client calculates and sets
Standby 108 (all showing in Fig. 1) and training calculate equipment 116.Fig. 7 further shows matching for database 126 and 146 (shown in Fig. 1)
It sets.Database 126 and 146 is couple to AMS 106, content provider's data processing system 112 and the visitor for executing particular task
Family calculates several discrete components in equipment 108.
AMS 106 includes reaching the more of data 510 (shown in Fig. 5) for receiving such as data 402 (shown in Fig. 4) and evening
The receiving element 702 of a data element, wherein each data element is associated with timestamp.AMS 106 additionally includes first
Component 703 is determined, for determining the training window of each model layer for hierarchical mode layer stack.AMS 106 further comprises
Second determines component 704, for having the data element of timestamp corresponding with each trained window by identification, determines and uses
In multiple training data elements of each trained window.AMS 106 further includes recognizer component 705, is used for each mould for identification
Type layer in preceding checkpoint, wherein being generated for each model layer by father's model layer in preceding checkpoint.AMS 106 is additionally wrapped
Training assembly 706 is included, for being used for each model with what is identified by the determining training data element for each model layer
Layer in preceding checkpoint, each model layer of training.AMS 106 further comprises formation component 707, for generating multiple current inspections
It makes an inventory of, each current check point in plurality of current check point is associated with model layer.AMS 106 further includes storage assembly
708, for multiple current check points to be stored at memory.
In the exemplary embodiment, database 126 and 146 is divided into multiple portions, including but not limited to algorithm content portion
710, training pattern configuration content portion 712 and checkpoint portion 714.These portions in database 126 and 146 are interconnected with on-demand update
With retrieval information.
These computer programs (also referred to as program, software, software application or code) include for programmable processor
Machine instruction, and can be realized with the programming language and/or compilation/machine language of level process and/or object-oriented.Such as
Used herein, term " machine readable media ", " computer-readable medium " refer to any computer program product, device
And/or equipment (such as disk, CD, memory, programmable logic device (PLD)), it is used to provide for machine instruction and/or number
According to arrive programmable processor, including receive machine instruction machine readable medium as a machine-readable signal.However, " the machine
Device readable medium " and " computer-readable medium " do not include instantaneity signal.Term " machine-readable signal " is referred to for mentioning
For any signal of machine instruction and/or data to programmable processor.
In addition, discribed logic flow does not need shown particular order or consecutive order in figure, it is desired to obtain
As a result.It is furthermore possible to also provide other steps, or step can be deleted from described stream, and can be to described
System addition or from the other components of described system-kill.Therefore, other embodiments are within the scope of the appended claims.
It should be understood that the above-described embodiment being specifically described in detail is only example or possible embodiment, and can be with
Including many other combinations, addition or alternative.
In addition, the specific name of component, the capitalization of word, attribute, data structure, any other programming or configuration aspects are not
It is compulsory or important, and the mechanism for realizing theme described herein or its feature can have different names, format
Or agreement.In addition, the system can via hardware and software combination (as described) or entirely realized in hardware element.
Moreover, the particular division of the function between each system component described herein is only example purpose, rather than it is mandatory
's；The function of being executed by triangular web component can be executed alternatively by multiple components, and the function that multiple components execute can also
Alternatively to be executed by single component.
Some parts described above present feature in terms of the algorithm and symbol expression operated to information.These algorithms
Description and expression can be by the technical staff of data processing field using most effectively to pass to others skilled in the art
Up to its work essence.Although these operations functionally or are in logic described, it is understood that for by computer program reality
It is existing.In addition, also it has been proved that the arrangement of these operations can be known as module sometimes or its function title be it is convenient, without
It loses general.
Unless specifically stated, as from the discussion above it will be evident that it is understood that throughout the specification, using such as
The discussion of the word of " processing " or " calculating " or " operation " or " determination " or " display " or " offer " etc. refer to computer system or
The movement and process of similar electronic computing device, manipulate and convert be expressed as computer system memory or register or its
The data of physics (electronics) amount in its such information storage, transmission or display equipment.
Based on the foregoing description, embodiment of the disclosure discussed above can be used including computer software, firmware, hardware
Or any combination thereof or the computer programming or engineering technology of subset realize.It is any that there is computer-readable and/or computer
Such object routine of executable instruction can be realized or be provided in one or more computer-readable medium, to be made
Computer program product, i.e. product.The computer-readable medium can be for example fixed (hard) driver, cartridge, CD, magnetic
The semiconductor memory or such as internet or other communication networks of band, read-only memory (ROM) or flash memory etc.
Any transmission/reception medium of network or link.Product comprising computer code can be by executing directly from a medium
Instruction, by the way that code is copied to another medium from a medium or is made by transmitting the code on network
At and/or use.
Although the disclosure is described according to various specific embodiments, it should be appreciated that the disclosure can use
Modification in the spirit and scope of the claims is practiced.
Claims (19)
1. a kind of computer implemented method of the order training method for machine learning framework, the method is by including being couple to deposit
The training of the processor of reservoir calculates equipment to realize, which comprises
Multiple data elements are received by the receiving unit that the training calculates equipment, wherein each data element is related to timestamp
Connection；
The first of equipment, which is calculated, by the training determines that component determines the training of each model layer of the model layer stack for layering
Window；
The second of equipment, which is calculated, by the training determines that component has timestamp corresponding with each trained window by identification
The data element, to determine multiple training data elements for each trained window；
It is identified for each model layer by the recognizer component that the training calculates equipment in preceding checkpoint, wherein by father's model layer
It generates for each model layer in preceding checkpoint；
The identified training data element for each model layer and institute are utilized by the training assembly that the training calculates equipment
Identification for each model layer in preceding checkpoint, each model layer of training；
Multiple current check points are generated by the formation component that the training calculates equipment, wherein in the multiple current check point
Each current check point is associated with model layer；
The multiple current check point is stored in the memory by the storage assembly that the training calculates equipment；And
Calculating equipment by the training makes external server and at least one current check associated at least one model layer
Point synchronizes.
2. a kind of computer implemented method of the order training method for machine learning framework, the method is by including being couple to deposit
The training of the processor of reservoir calculates equipment to realize, which comprises
Multiple data elements are received by the receiving unit that the training calculates equipment, wherein each data element is related to timestamp
Connection；
The first of equipment, which is calculated, by the training determines that component determines the training of each model layer of the model layer stack for layering
Window；
The second of equipment, which is calculated, by the training determines that component has timestamp corresponding with each trained window by identification
The data element, to determine multiple training data elements for each trained window；
It is identified for each model layer by the recognizer component that the training calculates equipment in preceding checkpoint, wherein by father's model layer
It generates for each model layer in preceding checkpoint；
The identified training data element for each model layer and institute are utilized by the training assembly that the training calculates equipment
Identification for each model layer in preceding checkpoint, each model layer of training；
Multiple current check points are generated by the formation component that the training calculates equipment, wherein in the multiple current check point
Each current check point is associated with model layer；And
The multiple current check point is stored in the memory by the storage assembly that the training calculates equipment, wherein
Storing the multiple current check point further includes:
Relative to the multiple data element, each checkpoint in the multiple current check point is verified；And
Checkpoint verified in the multiple current check point is stored at the memory.
3. method according to claim 1 or 2, wherein each model layer of training further comprises:
By applying at least one machine learning algorithm, the training data member for making each model layer adapt to determine each model layer
Element.
4. method according to claim 1 or 2, wherein determine that the training window for each model layer further comprises:
Determine the layer depth of each model layer；
Training pattern configuration is retrieved, wherein the specified training delay record associated with each layer depth of training pattern configuration；
It is configured based on the training pattern to identify training delay associated with the layer depth of each model layer；And
Postponed based on the training for each model layer to calculate trained window.
5. the method as described in claim 1, wherein the external server is based at least partially on synchronous current check point
To service.
6. method according to claim 1 or 2, wherein generating multiple current check points further includes:
Determine the layer depth for being used for each model layer；
Training pattern configuration is retrieved, wherein the training pattern, which configures, specifies duration associated with each layer depth；
It is configured based on the training pattern to identify duration associated with the layer depth of each model layer；
The training model layer in the duration identified；And
Each model layer is handled as current check point.
7. method according to claim 1 or 2, wherein generating multiple current check points further includes:
It removes for each model layer in preceding checkpoint；And
The each model layer of re -training.
8. method according to claim 1 or 2, wherein receiving multiple data elements further comprises:
Multiple conversion datas are received, wherein the conversion data indicates conversion activity associated with online advertisement is distributed.
9. a kind of training of order training method for machine learning framework calculates equipment, it includes for depositing that the training, which calculates equipment,
The processor for storing up the memory of data and communicating with the memory, the processor are programmed to:
Multiple data elements are received from the memory, wherein each data element is associated with timestamp；
Determine the training window of each model layer of the model layer stack for layering；
There is the data element of timestamp corresponding with each trained window by identifying, to determine for each training
Multiple training data elements of window；
It identifies for each model layer in preceding checkpoint, wherein being generated for each model layer by father's model layer in preceding inspection
Point；
Using the identified training data element for each model layer and identified for each model layer in preceding inspection
It makes an inventory of, each model layer of training；
Multiple current check points are generated, wherein each current check point in the multiple current check point is related to model layer
Connection；
In the memory by the storage of the multiple current check point；
It removes for each model layer in preceding checkpoint；And
The each model layer of re -training.
10. training as claimed in claim 9 calculates equipment, wherein the processor is further programmed to:
By applying at least one machine learning algorithm, the training data member for making each model layer adapt to determine each model layer
Element.
11. training as claimed in claim 9 calculates equipment, wherein the processor is further programmed to:
Determine the layer depth of each model layer；
Training pattern configuration is retrieved, wherein the specified training delay record associated with each layer depth of training pattern configuration；
It is configured based on the training pattern to identify training delay associated with the layer depth of each model layer；And
Postponed based on the training for each model layer to calculate trained window.
12. training as claimed in claim 9 calculates equipment, wherein the processor is further programmed to:
Make external server and synchronous at least one associated current check point of at least one model layer, wherein the outside
Server is based at least partially on synchronous current check point to service.
13. training as claimed in claim 9 calculates equipment, wherein the processor is further programmed to:
Determine the layer depth for being used for each model layer；
Training pattern configuration is retrieved, wherein the training pattern, which configures, specifies duration associated with each layer depth；
It is configured based on the training pattern to identify duration associated with the layer depth of each model layer；
The training model layer in the duration identified；And
Each model layer is handled as current check point.
14. training as claimed in claim 9 calculates equipment, wherein the processor is further programmed to:
Relative to the multiple data element, each checkpoint in the multiple current check point is verified；And
Checkpoint verified in the multiple current check point is stored at the memory.
15. training as claimed in claim 9 calculates equipment, wherein the processor is further programmed to:
Multiple conversion datas are received, wherein the conversion data indicates conversion activity associated with online advertisement is distributed.
16. a kind of non-instantaneous computer readable storage devices of the order training method for machine learning framework, are embodied on it
Processor-executable instruction, wherein the computer includes at least one processor and the memory for being couple to the processor,
Wherein, when being executed by the computer, the processor-executable instruction makes the computer:
Multiple data elements are received, wherein each data element is associated with timestamp；
Determine the training window of each model layer of the model layer stack for layering；
There is the data element of timestamp corresponding with each trained window by identifying, to determine for each training
Multiple training data elements of window；
It identifies for each model layer in preceding checkpoint, wherein being generated for each model layer by father's model layer in preceding inspection
Point；
Using the identified training data element for each model layer and identified for each model layer in preceding inspection
It makes an inventory of, each model layer of training；
Multiple current check points are generated, wherein each current check point in the multiple current check point is related to model layer
Connection；
The multiple current check point is stored at the memory；And
Make external server and synchronous at least one associated current check point of at least one model layer.
17. computer readable storage devices as claimed in claim 16, wherein the processor-executable instruction makes the meter
Calculation machine:
By applying at least one machine learning algorithm, the training data member for making each model layer adapt to determine each model layer
Element.
18. computer readable storage devices as claimed in claim 16, wherein the processor-executable instruction makes the meter
Calculation machine:
Determine the layer depth of each model layer；
Training pattern configuration is retrieved, wherein the specified training delay record associated with each layer depth of training pattern configuration；
It is configured based on the training pattern to identify training delay associated with the layer depth of each model layer；And
Postponed based on the training for each model layer to calculate trained window.
19. computer readable storage devices as claimed in claim 16, wherein the external server is based at least partially on
Synchronous current check point services.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/071,375 | 2013-11-04 | ||
US14/071,375 US9286574B2 (en) | 2013-11-04 | 2013-11-04 | Systems and methods for layered training in machine-learning architectures |
PCT/US2014/063159 WO2015066331A1 (en) | 2013-11-04 | 2014-10-30 | Systems and methods for layered training in machine-learning architectures |
Publications (2)
Publication Number | Publication Date |
---|---|
CN105683944A CN105683944A (en) | 2016-06-15 |
CN105683944B true CN105683944B (en) | 2019-08-09 |
Family
ID=53005123
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201480060487.0A Active CN105683944B (en) | 2013-11-04 | 2014-10-30 | Method, equipment and medium for the order training method in machine learning framework |
Country Status (4)
Country | Link |
---|---|
US (1) | US9286574B2 (en) |
EP (1) | EP3066580A4 (en) |
CN (1) | CN105683944B (en) |
WO (1) | WO2015066331A1 (en) |
Families Citing this family (23)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9537706B2 (en) | 2012-08-20 | 2017-01-03 | Plentyoffish Media Ulc | Apparatus, method and article to facilitate matching of clients in a networked environment |
US11568008B2 (en) | 2013-03-13 | 2023-01-31 | Plentyoffish Media Ulc | Apparatus, method and article to identify discrepancies between clients and in response prompt clients in a networked environment |
US9672289B1 (en) | 2013-07-23 | 2017-06-06 | Plentyoffish Media Ulc | Apparatus, method and article to facilitate matching of clients in a networked environment |
US9870465B1 (en) | 2013-12-04 | 2018-01-16 | Plentyoffish Media Ulc | Apparatus, method and article to facilitate automatic detection and removal of fraudulent user information in a network environment |
US10540607B1 (en) | 2013-12-10 | 2020-01-21 | Plentyoffish Media Ulc | Apparatus, method and article to effect electronic message reply rate matching in a network environment |
US10387795B1 (en) * | 2014-04-02 | 2019-08-20 | Plentyoffish Media Inc. | Systems and methods for training and employing a machine learning system in providing service level upgrade offers |
JP6558188B2 (en) * | 2015-09-30 | 2019-08-14 | 富士通株式会社 | Distributed processing system, learning model creation method, data processing method, learning model creation program, and data processing program |
US11113716B2 (en) * | 2016-06-27 | 2021-09-07 | Adobe Inc. | Attribution that accounts for external viewing conditions |
CN107784364B (en) * | 2016-08-25 | 2021-06-15 | 微软技术许可有限责任公司 | Asynchronous training of machine learning models |
WO2018156745A1 (en) * | 2017-02-22 | 2018-08-30 | Stackray Corporation | Computer network modeling |
CN108154237B (en) * | 2016-12-06 | 2022-04-05 | 华为技术有限公司 | Data processing system and method |
US11521045B2 (en) | 2017-06-14 | 2022-12-06 | Knowm, Inc. | Anti-Hebbian and Hebbian (AHAH) computing |
US11403540B2 (en) | 2017-08-11 | 2022-08-02 | Google Llc | On-device machine learning platform |
US11138517B2 (en) | 2017-08-11 | 2021-10-05 | Google Llc | On-device machine learning platform |
CN107563512B (en) * | 2017-08-24 | 2023-10-17 | 腾讯科技（上海）有限公司 | Data processing method, device and storage medium |
US11017039B2 (en) * | 2017-12-01 | 2021-05-25 | Facebook, Inc. | Multi-stage ranking optimization for selecting content |
WO2019125516A1 (en) * | 2017-12-23 | 2019-06-27 | Barkly Protects, Inc. | Continuous malicious software identification through responsive machine learning |
CN108062573A (en) | 2017-12-29 | 2018-05-22 | 广东欧珀移动通信有限公司 | Model training method and device |
CN110647996B (en) * | 2018-06-08 | 2021-01-26 | 上海寒武纪信息科技有限公司 | Execution method and device of universal machine learning model and storage medium |
EP3751477A4 (en) | 2018-06-08 | 2021-11-03 | Shanghai Cambricon Information Technology Co., Ltd | General machine learning model, and model file generation and parsing method |
US11348032B1 (en) * | 2018-09-02 | 2022-05-31 | Meta Platforms, Inc. | Automated generation of machine learning models |
US10740223B1 (en) * | 2019-01-31 | 2020-08-11 | Verizon Patent And Licensing, Inc. | Systems and methods for checkpoint-based machine learning model |
CN113381865A (en) * | 2020-02-25 | 2021-09-10 | 华为技术有限公司 | Network operation and maintenance method, device and system |
Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1649311A (en) * | 2005-03-23 | 2005-08-03 | 北京首信科技有限公司 | Detecting system and method for user behaviour abnormal based on machine study |
CN1838150A (en) * | 2005-03-09 | 2006-09-27 | 西门子共同研究公司 | Probabilistic boosting tree structure for learned discriminative models |
Family Cites Families (19)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6879971B1 (en) * | 1995-12-22 | 2005-04-12 | Pavilion Technologies, Inc. | Automated method for building a model |
US7660459B2 (en) * | 2001-06-12 | 2010-02-09 | International Business Machines Corporation | Method and system for predicting customer behavior based on data network geography |
US20030130899A1 (en) * | 2002-01-08 | 2003-07-10 | Bruce Ferguson | System and method for historical database training of non-linear models for use in electronic commerce |
US7203635B2 (en) * | 2002-06-27 | 2007-04-10 | Microsoft Corporation | Layered models for context awareness |
US20070271184A1 (en) * | 2003-12-16 | 2007-11-22 | Norbert Niebert | Technique for Transferring Media Data Files |
WO2007041769A1 (en) * | 2005-10-13 | 2007-04-19 | Flying Bark Interactive Pty Limited | Token trading |
US8407226B1 (en) * | 2007-02-16 | 2013-03-26 | Google Inc. | Collaborative filtering |
US20090011743A1 (en) * | 2007-07-02 | 2009-01-08 | Yahoo! Inc. | Mobile trading cards |
US20110125867A1 (en) * | 2007-12-14 | 2011-05-26 | Denk Jr William E | System and Method for the Improvement of Sharing Digital Data Between Mobile Devices |
US20110163944A1 (en) * | 2010-01-05 | 2011-07-07 | Apple Inc. | Intuitive, gesture-based communications with physics metaphors |
US8478699B1 (en) * | 2010-04-30 | 2013-07-02 | Google Inc. | Multiple correlation measures for measuring query similarity |
US20120136812A1 (en) | 2010-11-29 | 2012-05-31 | Palo Alto Research Center Incorporated | Method and system for machine-learning based optimization and customization of document similarities calculation |
US8464184B1 (en) * | 2010-11-30 | 2013-06-11 | Symantec Corporation | Systems and methods for gesture-based distribution of files |
US9367224B2 (en) * | 2011-04-29 | 2016-06-14 | Avaya Inc. | Method and apparatus for allowing drag-and-drop operations across the shared borders of adjacent touch screen-equipped devices |
US9930128B2 (en) * | 2011-09-30 | 2018-03-27 | Nokia Technologies Oy | Method and apparatus for accessing a virtual object |
WO2013078640A1 (en) * | 2011-11-30 | 2013-06-06 | Google Inc. | Estimating user demographics |
US8515746B1 (en) * | 2012-06-20 | 2013-08-20 | Google Inc. | Selecting speech data for speech recognition vocabulary |
US20140100944A1 (en) * | 2012-10-09 | 2014-04-10 | Share This Inc. | Method and system for online campaign optimization |
US9538409B2 (en) * | 2012-10-29 | 2017-01-03 | T-Mobile Usa, Inc. | Quality of user experience analysis |
-
2013
- 2013-11-04 US US14/071,375 patent/US9286574B2/en not_active Expired - Fee Related
-
2014
- 2014-10-30 WO PCT/US2014/063159 patent/WO2015066331A1/en active Application Filing
- 2014-10-30 EP EP14858582.1A patent/EP3066580A4/en not_active Ceased
- 2014-10-30 CN CN201480060487.0A patent/CN105683944B/en active Active
Patent Citations (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN1838150A (en) * | 2005-03-09 | 2006-09-27 | 西门子共同研究公司 | Probabilistic boosting tree structure for learned discriminative models |
CN1649311A (en) * | 2005-03-23 | 2005-08-03 | 北京首信科技有限公司 | Detecting system and method for user behaviour abnormal based on machine study |
Also Published As
Publication number | Publication date |
---|---|
US20150127590A1 (en) | 2015-05-07 |
EP3066580A4 (en) | 2017-06-21 |
US9286574B2 (en) | 2016-03-15 |
CN105683944A (en) | 2016-06-15 |
EP3066580A1 (en) | 2016-09-14 |
WO2015066331A1 (en) | 2015-05-07 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN105683944B (en) | Method, equipment and medium for the order training method in machine learning framework | |
US10684738B1 (en) | Social retail platform and system with graphical user interfaces for presenting multiple content types | |
JP6298220B2 (en) | System and method for suggesting creative types to advertisers for online content items | |
US11665248B2 (en) | Graphical user interface and system for viewing landing page content | |
US8689136B2 (en) | System and method for backend advertisement conversion | |
CN104919482B (en) | Dynamic content establishment of item | |
US9286342B1 (en) | Tracking changes in on-line spreadsheet | |
US9256589B2 (en) | Web-based spreadsheet interaction with large data set | |
JP5676557B2 (en) | Editing interface | |
US20140279078A1 (en) | Enriching An Advertiser's Product-Related Information | |
US20160134934A1 (en) | Estimating audience segment size changes over time | |
CN103678452B (en) | Visualization and the integration with the analysis of business object | |
US20140108139A1 (en) | Marketing Segment Rule Definition for Real-time and Back-end Processing | |
CN107430618A (en) | Realize the system and method interacted with host computer device progress user speech | |
US10402180B2 (en) | Latency reduction in feedback-based system performance determination | |
US20120143677A1 (en) | Discoverability Using Behavioral Data | |
CN107889532A (en) | The system and method classified based on response data sets to data query | |
US8074234B2 (en) | Web service platform for keyword technologies | |
US11818221B1 (en) | Transferring a state of user interaction with an online content item to a computer program | |
US20160171572A1 (en) | Methods and systems that aggregate multi-media reviews of products and services | |
KR20150106023A (en) | System and method for advertisement delivery, and apparatus applied to the same | |
US10607255B1 (en) | Product detail page advertising | |
US8918381B1 (en) | Selection criteria diversification | |
US20170124602A1 (en) | Demand matching method on network and workspace trading platform using such method | |
WO2016000631A1 (en) | Demand matching method on network and workspace trading platform using such method |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
C06 | Publication | ||
PB01 | Publication | ||
C10 | Entry into substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
CB02 | Change of applicant information | ||
CB02 | Change of applicant information |
Address after: American CaliforniaApplicant after: Google limited liability companyAddress before: American CaliforniaApplicant before: Google Inc. |
|
GR01 | Patent grant | ||
GR01 | Patent grant |