KR20150052272A - Construction zone detection using a plurality of information sources - Google Patents
Construction zone detection using a plurality of information sources Download PDFInfo
- Publication number
- KR20150052272A KR20150052272A KR1020157008687A KR20157008687A KR20150052272A KR 20150052272 A KR20150052272 A KR 20150052272A KR 1020157008687 A KR1020157008687 A KR 1020157008687A KR 20157008687 A KR20157008687 A KR 20157008687A KR 20150052272 A KR20150052272 A KR 20150052272A
- Authority
- KR
- South Korea
- Prior art keywords
- information
- vehicle
- computing device
- road
- work area
- Prior art date
Links
- 238000001514 detection method Methods 0.000 title claims abstract description 58
- 238000010276 construction Methods 0.000 title claims description 194
- 238000000034 method Methods 0.000 claims abstract description 118
- 238000011217 control strategy Methods 0.000 claims abstract description 65
- 238000004891 communication Methods 0.000 claims description 32
- 230000006870 function Effects 0.000 claims description 32
- 238000012549 training Methods 0.000 claims description 25
- 230000008859 change Effects 0.000 claims description 15
- 238000012545 processing Methods 0.000 claims description 2
- 230000003213 activating effect Effects 0.000 claims 1
- 230000003466 anti-cipated effect Effects 0.000 claims 1
- 230000006399 behavior Effects 0.000 description 40
- 230000008569 process Effects 0.000 description 15
- 238000004422 calculation algorithm Methods 0.000 description 14
- 230000001276 controlling effect Effects 0.000 description 8
- 230000033001 locomotion Effects 0.000 description 8
- 238000007635 classification algorithm Methods 0.000 description 7
- 238000010586 diagram Methods 0.000 description 7
- 239000000463 material Substances 0.000 description 7
- 238000003860 storage Methods 0.000 description 7
- 238000004590 computer program Methods 0.000 description 6
- 238000009826 distribution Methods 0.000 description 6
- 238000005516 engineering process Methods 0.000 description 6
- 230000004927 fusion Effects 0.000 description 6
- 230000002093 peripheral effect Effects 0.000 description 5
- 229910003460 diamond Inorganic materials 0.000 description 4
- 239000010432 diamond Substances 0.000 description 4
- 238000005259 measurement Methods 0.000 description 4
- 230000007246 mechanism Effects 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 4
- 230000007704 transition Effects 0.000 description 4
- 230000001133 acceleration Effects 0.000 description 3
- 230000005540 biological transmission Effects 0.000 description 3
- 230000001419 dependent effect Effects 0.000 description 3
- 239000000446 fuel Substances 0.000 description 3
- 238000004519 manufacturing process Methods 0.000 description 3
- 230000004044 response Effects 0.000 description 3
- LFQSCWFLJHTTHZ-UHFFFAOYSA-N Ethanol Chemical compound CCO LFQSCWFLJHTTHZ-UHFFFAOYSA-N 0.000 description 2
- XEEYBQQBJWHFJM-UHFFFAOYSA-N Iron Chemical compound [Fe] XEEYBQQBJWHFJM-UHFFFAOYSA-N 0.000 description 2
- ATUOYWHBWRKTHZ-UHFFFAOYSA-N Propane Chemical compound CCC ATUOYWHBWRKTHZ-UHFFFAOYSA-N 0.000 description 2
- 238000001069 Raman spectroscopy Methods 0.000 description 2
- 230000009471 action Effects 0.000 description 2
- 238000003491 array Methods 0.000 description 2
- 239000011324 bead Substances 0.000 description 2
- 239000003502 gasoline Substances 0.000 description 2
- 239000011521 glass Substances 0.000 description 2
- 238000003384 imaging method Methods 0.000 description 2
- 230000009467 reduction Effects 0.000 description 2
- 238000001228 spectrum Methods 0.000 description 2
- 101001093748 Homo sapiens Phosphatidylinositol N-acetylglucosaminyltransferase subunit P Proteins 0.000 description 1
- DGAQECJNVWCQMB-PUAWFVPOSA-M Ilexoside XXIX Chemical compound C[C@@H]1CC[C@@]2(CC[C@@]3(C(=CC[C@H]4[C@]3(CC[C@@H]5[C@@]4(CC[C@@H](C5(C)C)OS(=O)(=O)[O-])C)C)[C@@H]2[C@]1(C)O)C)C(=O)O[C@H]6[C@@H]([C@H]([C@@H]([C@H](O6)CO)O)O)O.[Na+] DGAQECJNVWCQMB-PUAWFVPOSA-M 0.000 description 1
- HBBGRARXTFLTSG-UHFFFAOYSA-N Lithium ion Chemical compound [Li+] HBBGRARXTFLTSG-UHFFFAOYSA-N 0.000 description 1
- ZLMJMSJWJFRBEC-UHFFFAOYSA-N Potassium Chemical compound [K] ZLMJMSJWJFRBEC-UHFFFAOYSA-N 0.000 description 1
- 239000002253 acid Substances 0.000 description 1
- 238000013528 artificial neural network Methods 0.000 description 1
- QVGXLLKOCUKJST-UHFFFAOYSA-N atomic oxygen Chemical compound [O] QVGXLLKOCUKJST-UHFFFAOYSA-N 0.000 description 1
- 230000004888 barrier function Effects 0.000 description 1
- -1 batteries Substances 0.000 description 1
- 239000003990 capacitor Substances 0.000 description 1
- 230000015556 catabolic process Effects 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 238000002485 combustion reaction Methods 0.000 description 1
- 238000006731 degradation reaction Methods 0.000 description 1
- 239000000835 fiber Substances 0.000 description 1
- 239000002828 fuel tank Substances 0.000 description 1
- 239000007789 gas Substances 0.000 description 1
- 230000014509 gene expression Effects 0.000 description 1
- 238000009434 installation Methods 0.000 description 1
- 230000010354 integration Effects 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 229910052742 iron Inorganic materials 0.000 description 1
- 229910001416 lithium ion Inorganic materials 0.000 description 1
- 238000007477 logistic regression Methods 0.000 description 1
- 238000013507 mapping Methods 0.000 description 1
- 238000007620 mathematical function Methods 0.000 description 1
- 229910052751 metal Inorganic materials 0.000 description 1
- 239000002184 metal Substances 0.000 description 1
- 238000003032 molecular docking Methods 0.000 description 1
- 238000012544 monitoring process Methods 0.000 description 1
- 239000010705 motor oil Substances 0.000 description 1
- 229910052755 nonmetal Inorganic materials 0.000 description 1
- 229910052760 oxygen Inorganic materials 0.000 description 1
- 239000001301 oxygen Substances 0.000 description 1
- 238000010422 painting Methods 0.000 description 1
- 239000003208 petroleum Substances 0.000 description 1
- 229910052700 potassium Inorganic materials 0.000 description 1
- 239000011591 potassium Substances 0.000 description 1
- 239000001294 propane Substances 0.000 description 1
- 238000013139 quantization Methods 0.000 description 1
- 230000005855 radiation Effects 0.000 description 1
- 238000011084 recovery Methods 0.000 description 1
- 230000001172 regenerating effect Effects 0.000 description 1
- 230000001105 regulatory effect Effects 0.000 description 1
- 230000035945 sensitivity Effects 0.000 description 1
- 229910052708 sodium Inorganic materials 0.000 description 1
- 239000011734 sodium Substances 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 238000010897 surface acoustic wave method Methods 0.000 description 1
Images
Classifications
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B60—VEHICLES IN GENERAL
- B60W—CONJOINT CONTROL OF VEHICLE SUB-UNITS OF DIFFERENT TYPE OR DIFFERENT FUNCTION; CONTROL SYSTEMS SPECIALLY ADAPTED FOR HYBRID VEHICLES; ROAD VEHICLE DRIVE CONTROL SYSTEMS FOR PURPOSES NOT RELATED TO THE CONTROL OF A PARTICULAR SUB-UNIT
- B60W30/00—Purposes of road vehicle drive control systems not related to the control of a particular sub-unit, e.g. of systems using conjoint control of vehicle sub-units, or advanced driver assistance systems for ensuring comfort, stability and safety or drive control systems for propelling or retarding the vehicle
- B60W30/10—Path keeping
- B60W30/12—Lane keeping
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B60—VEHICLES IN GENERAL
- B60W—CONJOINT CONTROL OF VEHICLE SUB-UNITS OF DIFFERENT TYPE OR DIFFERENT FUNCTION; CONTROL SYSTEMS SPECIALLY ADAPTED FOR HYBRID VEHICLES; ROAD VEHICLE DRIVE CONTROL SYSTEMS FOR PURPOSES NOT RELATED TO THE CONTROL OF A PARTICULAR SUB-UNIT
- B60W30/00—Purposes of road vehicle drive control systems not related to the control of a particular sub-unit, e.g. of systems using conjoint control of vehicle sub-units, or advanced driver assistance systems for ensuring comfort, stability and safety or drive control systems for propelling or retarding the vehicle
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B60—VEHICLES IN GENERAL
- B60W—CONJOINT CONTROL OF VEHICLE SUB-UNITS OF DIFFERENT TYPE OR DIFFERENT FUNCTION; CONTROL SYSTEMS SPECIALLY ADAPTED FOR HYBRID VEHICLES; ROAD VEHICLE DRIVE CONTROL SYSTEMS FOR PURPOSES NOT RELATED TO THE CONTROL OF A PARTICULAR SUB-UNIT
- B60W30/00—Purposes of road vehicle drive control systems not related to the control of a particular sub-unit, e.g. of systems using conjoint control of vehicle sub-units, or advanced driver assistance systems for ensuring comfort, stability and safety or drive control systems for propelling or retarding the vehicle
- B60W30/14—Adaptive cruise control
- B60W30/16—Control of distance between vehicles, e.g. keeping a distance to preceding vehicle
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B60—VEHICLES IN GENERAL
- B60W—CONJOINT CONTROL OF VEHICLE SUB-UNITS OF DIFFERENT TYPE OR DIFFERENT FUNCTION; CONTROL SYSTEMS SPECIALLY ADAPTED FOR HYBRID VEHICLES; ROAD VEHICLE DRIVE CONTROL SYSTEMS FOR PURPOSES NOT RELATED TO THE CONTROL OF A PARTICULAR SUB-UNIT
- B60W40/00—Estimation or calculation of non-directly measurable driving parameters for road vehicle drive control systems not related to the control of a particular sub unit, e.g. by using mathematical models
- B60W40/02—Estimation or calculation of non-directly measurable driving parameters for road vehicle drive control systems not related to the control of a particular sub unit, e.g. by using mathematical models related to ambient conditions
- B60W40/06—Road conditions
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B60—VEHICLES IN GENERAL
- B60W—CONJOINT CONTROL OF VEHICLE SUB-UNITS OF DIFFERENT TYPE OR DIFFERENT FUNCTION; CONTROL SYSTEMS SPECIALLY ADAPTED FOR HYBRID VEHICLES; ROAD VEHICLE DRIVE CONTROL SYSTEMS FOR PURPOSES NOT RELATED TO THE CONTROL OF A PARTICULAR SUB-UNIT
- B60W60/00—Drive control systems specially adapted for autonomous road vehicles
- B60W60/001—Planning or execution of driving tasks
- B60W60/0011—Planning or execution of driving tasks involving control alternatives for a single driving scenario, e.g. planning several paths to avoid obstacles
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B60—VEHICLES IN GENERAL
- B60W—CONJOINT CONTROL OF VEHICLE SUB-UNITS OF DIFFERENT TYPE OR DIFFERENT FUNCTION; CONTROL SYSTEMS SPECIALLY ADAPTED FOR HYBRID VEHICLES; ROAD VEHICLE DRIVE CONTROL SYSTEMS FOR PURPOSES NOT RELATED TO THE CONTROL OF A PARTICULAR SUB-UNIT
- B60W60/00—Drive control systems specially adapted for autonomous road vehicles
- B60W60/001—Planning or execution of driving tasks
- B60W60/0015—Planning or execution of driving tasks specially adapted for safety
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B60—VEHICLES IN GENERAL
- B60W—CONJOINT CONTROL OF VEHICLE SUB-UNITS OF DIFFERENT TYPE OR DIFFERENT FUNCTION; CONTROL SYSTEMS SPECIALLY ADAPTED FOR HYBRID VEHICLES; ROAD VEHICLE DRIVE CONTROL SYSTEMS FOR PURPOSES NOT RELATED TO THE CONTROL OF A PARTICULAR SUB-UNIT
- B60W50/00—Details of control systems for road vehicle drive control not related to the control of a particular sub-unit, e.g. process diagnostic or vehicle driver interfaces
- B60W2050/0062—Adapting control system settings
- B60W2050/0075—Automatic parameter input, automatic initialising or calibrating means
- B60W2050/009—Priority selection
- B60W2050/0091—Priority selection of control inputs
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B60—VEHICLES IN GENERAL
- B60W—CONJOINT CONTROL OF VEHICLE SUB-UNITS OF DIFFERENT TYPE OR DIFFERENT FUNCTION; CONTROL SYSTEMS SPECIALLY ADAPTED FOR HYBRID VEHICLES; ROAD VEHICLE DRIVE CONTROL SYSTEMS FOR PURPOSES NOT RELATED TO THE CONTROL OF A PARTICULAR SUB-UNIT
- B60W2420/00—Indexing codes relating to the type of sensors based on the principle of their operation
- B60W2420/40—Photo or light sensitive means, e.g. infrared sensors
- B60W2420/403—Image sensing, e.g. optical camera
-
- B60W2420/408—
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B60—VEHICLES IN GENERAL
- B60W—CONJOINT CONTROL OF VEHICLE SUB-UNITS OF DIFFERENT TYPE OR DIFFERENT FUNCTION; CONTROL SYSTEMS SPECIALLY ADAPTED FOR HYBRID VEHICLES; ROAD VEHICLE DRIVE CONTROL SYSTEMS FOR PURPOSES NOT RELATED TO THE CONTROL OF A PARTICULAR SUB-UNIT
- B60W2556/00—Input parameters relating to data
- B60W2556/45—External transmission of data to or from the vehicle
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B60—VEHICLES IN GENERAL
- B60W—CONJOINT CONTROL OF VEHICLE SUB-UNITS OF DIFFERENT TYPE OR DIFFERENT FUNCTION; CONTROL SYSTEMS SPECIALLY ADAPTED FOR HYBRID VEHICLES; ROAD VEHICLE DRIVE CONTROL SYSTEMS FOR PURPOSES NOT RELATED TO THE CONTROL OF A PARTICULAR SUB-UNIT
- B60W2556/00—Input parameters relating to data
- B60W2556/45—External transmission of data to or from the vehicle
- B60W2556/50—External transmission of data to or from the vehicle for navigation systems
Abstract
복수의 정보원들로부터의 정보를 이용한 공사 구역의 검출을 위한 방법들 및 시스템들이 기술된다. 일 예시에서, 차량을 제어하도록 구성된 컴퓨팅 디바이스는, 복수의 정보원들로부터, 상기 차량이 운행하고 있는 도로상의 공사 구역의 검출에 관한 정보를 수신하도록 구성된다. 또한, 상기 컴퓨팅 디바이스는, 상기 정보에 기반해서, 상기 도로상의 공사 구역의 존재의 가능성을 결정하도록 구성된다. 더욱이 상기 컴퓨팅 디바이스는 상기 가능성에 기반해서, 상기 차량의 운전 거동과 관련된 제어 전략을 수정하고, 상기 수정된 제어 전략에 기반해서 상기 차량을 제어하도록 구성된다.Methods and systems for the detection of a work area using information from a plurality of sources are described. In one example, a computing device configured to control a vehicle is configured to receive, from a plurality of sources, information regarding the detection of a work area on the road on which the vehicle is running. In addition, the computing device is configured to determine, based on the information, the likelihood of the presence of a work area on the road. Further, the computing device is configured to modify a control strategy associated with the driving behavior of the vehicle based on the likelihood, and to control the vehicle based on the modified control strategy.
Description
이 출원은 2012년 9월 5일 제출된 미국 출원 제 13/603,613호에 대해 우선권을 주장하고, 상기 미국 출원의 전체는 본 문서에 참조로 통합된다.This application claims priority to U.S. Serial No. 13 / 603,613, filed September 5, 2012, the entirety of which is incorporated herein by reference.
자율 주행차들은 한 위치로부터 다른 위치까지 승객들을 수송하는 것을 돕기 위해 다양한 컴퓨팅 시스템들을 이용한다. 몇몇의 자율 주행차들은 파일럿, 운전자 또는 승객과 같은 오퍼레이터로부터 몇몇의 초기 입력 또는 연속 입력을 필요로 할 수 있다. 다른 시스템들, 예를 들어, 오토파일럿 시스템들은, 상기 시스템이 개입된 때에만 사용될 수 있고, 이는 상기 오퍼레이터로 하여금 (오퍼레이터가 차량의 움직임에 대한 높은 정도의 제어를 행사하는)수동 모드로부터 (차량이 본질적으로 스스로 운전하는)자율 모드로, 그리고 상기 수동 모드와 자율 모드 사이의 어딘가에 놓인 모드들로 스위칭하는 것을 허용한다.Autonomous vehicles use a variety of computing systems to help transport passengers from one location to another. Some autonomous vehicles may require some initial or continuous input from an operator, such as a pilot, driver, or passenger. Other systems, e. G., Autopilot systems, can only be used when the system is engaged, which allows the operator to move from a manual mode (where the operator exercises a high degree of control over the motion of the vehicle) To operate in an autonomous mode (which is essentially self-driving), and to switch to modes lying somewhere between the passive mode and the autonomous mode.
본 출원은 다수의 정보원들을 이용한 공사 구역의 검출에 관한 실시예들을 개시한다. 일 양상에서, 본 출원은 한 방법을 기술한다. 상기 방법은, 차량을 제어하도록 구성된 컴퓨팅 디바이스에서, 복수의 정보원들로부터, 상기 차량이 운행하고 있는 도로상의 공사 구역의 검출에 관한 정보를 수신하는 것을 포함한다. 상기 복수의 정보원들 중 정보원 각각에 그 정보원으로부터 수신된 정보 각각에 기반한 상기 공사 구역의 검출의 신뢰의 레벨을 표시하는 신뢰성 메트릭이 할당된다. 상기 방법은 또한, 상기 컴퓨팅 디바이스를 이용해서, 상기 정보 및 상기 복수의 정보원들의 각각의 신뢰성 메트릭들에 기반해서, 상기 도로상의 공사 구역의 존재 가능성을 결정하는 것을 포함한다. 상기 방법은 또한, 상기 컴퓨팅 디바이스를 이용해서, 상기 가능성에 기반해서, 상기 차량의 운전 거동과 관련된 제어 전략을 수정하는 것을 포함하고, 상기 컴퓨팅 디바이스를 이용해서, 상기 수정된 제어 전략에 기반해서 상기 차량을 제어하는 것을 더 포함한다.The present application discloses embodiments relating to the detection of a work area using a plurality of information sources. In one aspect, the present application describes a method. The method includes receiving, from a plurality of sources, information about the detection of a work area on the road on which the vehicle is running, at a computing device configured to control the vehicle. A reliability metric is assigned to each of the plurality of information sources to indicate a level of confidence in detection of the work area based on each of the information received from the information source. The method also includes using the computing device to determine the likelihood of a work area on the road based on the information and the reliability metrics of each of the plurality of sources. The method also includes using the computing device to modify a control strategy associated with the driving behavior of the vehicle based on the likelihood, using the computing device to determine, based on the modified control strategy, Further comprising controlling the vehicle.
다른 양상에서, 본 출원은 컴퓨팅 디바이스로 하여금 기능들을 수행하게 하기 위해 차량의 컴퓨팅 디바이스에 의해 실행 가능한 명령들을 저장하는 비일시적인 컴퓨터 판독 가능 매체를 기술한다. 상기 기능들은 복수의 정보원들로부터, 차량이 운행하고 있는 도로상의 공사 구역의 검출에 관한 정보를 수신하는 것을 포함한다. 상기 복수의 정보원들 중 정보원 각각에 그 정보원으로부터 수신된 각각의 정보에 기반한 상기 공사 구역의 검출의 신뢰의 레벨을 표시하는 신뢰성 메트릭이 할당된다. 상기 기능들은 또한, 상기 정보 및 상기 복수의 정보원들의 각각의 신뢰성 메트릭들에 기반해서, 상기 도로상의 공사 구역의 존재 가능성을 결정하는 것을 포함한다. 상기 기능들은 상기 가능성에 기반해서, 상기 차량의 운전 거동과 관련된 제어 전략을 수정하는 것을 더 포함한다. 상기 기능들은 또한 상기 수정된 제어 전략에 기반해서 상기 차량을 제어하는 것을 포함한다.In another aspect, the present application describes a non-volatile computer readable medium storing instructions executable by a computing device of a vehicle to cause the computing device to perform functions. The functions include receiving, from a plurality of sources, information regarding the detection of a work area on the road on which the vehicle is running. A reliability metric is assigned to each of the plurality of information sources to indicate a level of confidence in detection of the work area based on each information received from the information source. The functions also include determining the likelihood of a work area on the road based on the information and the reliability metrics of each of the plurality of sources. The functions further include modifying a control strategy associated with the driving behavior of the vehicle based on the possibility. The functions also include controlling the vehicle based on the modified control strategy.
또 다른 양상에서, 본 출원은 차량용 제어 시스템을 기술한다. 상기 제어 시스템은 컴퓨팅 디바이스를 포함한다. 상기 컴퓨팅 디바이스는, 복수의 정보원들로부터, 상기 차량이 운행하고 있는 도로상의 공사 구역의 검출에 관한 정보를 수신하도록 구성된다. 상기 복수의 정보원들 중 정보원 각각에 그 정보원으로부터 수신된 각각의 정보에 기반해서 상기 공사 구역의 검출의 신뢰의 레벨을 표시하는 각각의 신뢰성 메트릭이 할당된다. 상기 컴퓨팅 디바이스는 또한, 상기 정보 및 상기 복수의 정보원들의 각각의 신뢰성 메트릭들에 기반해서, 상기 도로상의 공사 구역의 존재의 가능성을 결정하도록 구성된다. 상기 컴퓨팅 디바이스는 상기 가능성에 기반해서, 상기 차량의 운전 거동과 관련된 제어 전략을 수정하고, 상기 수정된 제어 전략에 기반해서 상기 차량을 제어하도록 더 구성된다.In another aspect, the present application describes a vehicle control system. The control system includes a computing device. The computing device is configured to receive, from a plurality of sources, information regarding the detection of a work area on the road on which the vehicle is running. Each of the plurality of information sources is assigned a respective reliability metric indicative of the level of confidence in detection of the work area based on respective information received from the information source. The computing device is also configured to determine the likelihood of the presence of a work area on the road based on the information and the reliability metrics of each of the plurality of sources. The computing device is further configured to modify a control strategy associated with the driving behavior of the vehicle based on the likelihood and to control the vehicle based on the modified control strategy.
전술한 요약은 예시적인 것일 뿐이고 어떤 방식으로든 한정하는 것으로 의도되지 않는다. 위에 기술된 예시적인 양상들, 실시예들 및 구성들에 더하여, 추가적인 양상들, 실시예들 및 구성들은 상기 도면들에 대한 참조 및 후속하는 상세한 설명에 의해 명백해질 것이다.The foregoing summary is exemplary only and is not intended to be limiting in any way. In addition to the exemplary aspects, embodiments, and configurations described above, additional aspects, embodiments, and configurations will become apparent by reference to the drawings and the following detailed description.
도 1은 일 실시예에 부합하는, 자동차 예시의 단순화된 블럭도이다.
도 2는 일 실시예에 부합하는, 자동차 예시를 기술한다.
도 3은 일 실시예에 부합하는, 정보의 복수의 정보원들을 이용한 공사 구역의 검출을 위한 방법의 순서도이다.
도 4는 일 실시예에 부합하는, 공사 구역에 접근하는 차량을 기술한다.
도 5는 일 실시예에 부합하는, 공사 구역 표지판의 검출을 위한 방법의 순서도이다.
도 6a-6b는 일 실시예에 부합하는, 차량이 운행하고 있는 도로 및 상기 도로의 부근(vicinity)의 이미지들을 기술한다.
도 6c-6d는 일 실시예에 부합하는, 미리 결정된 높이 범위에서 상기 도로의 측면들을 묘사하는 상기 도로 및 상기 도로의 부근의 이미지들의 부분들을 기술한다.
도 7은 일 실시예에 부합하는, LIDAR-기반 정보를 이용한 공사 구역 표지판의 검출을 위한 방법의 순서도이다.
도 8a는 일 실시예에 부합하는, 상기 도로의 표면으로부터의 임계 높이보다 높은 높이에 있는 영역에서 공사 구역 표지판의 LIDAR-기반 검출을 기술한다.
도 8b는 일 실시예에 부합하는, 상기 도로의 표면으로부터의 임계 높이보다 높은 높이에 있는 영역을 묘사하는 LIDAR-기반 이미지를 기술한다.
도 9는 일 실시예에 부합하는, LIDAR-기반 정보를 이용한 공사 구역 물체들(construction zone objects)의 검출을 위한 방법의 순서도이다.
도 10a는 일 실시예에 부합하는, 상기 도로의 표면으로부터 임계 거리 내에 있는 영역에서의 공사 구역 콘의 LIDAR-기반 검출을 기술한다.
도 10b는 일 실시예에 부합하는, 상기 도로의 표면으로부터 임계 거리 내에 있는 영역을 묘사하는 LIDAR-기반 이미지를 기술한다.
도 10c는 일 실시예에 부합하는, 차로 경계를 형성하는 공사 구역 콘들의 LIDAR-기반 검출을 기술한다.
도 10d는 일 실시예에 부합하는, 차로 경계를 형성하는 공사 구역 콘들을 묘사하는 LIDAR-기반 이미지를 기술한다.
도 11은 일 실시예에 부합하는, 컴퓨터 프로그램의 개념적인 부분도의 개략적인 기술이다.1 is a simplified block diagram of an automotive example consistent with one embodiment.
Figure 2 illustrates an automotive example consistent with one embodiment.
3 is a flowchart of a method for detection of a work area using a plurality of information sources of information consistent with an embodiment.
Figure 4 illustrates a vehicle approaching a construction area, consistent with an embodiment.
5 is a flow chart of a method for detection of a work area sign conforming to one embodiment.
Figures 6A-6B illustrate images of the road on which the vehicle is running and the vicinity of the road, consistent with an embodiment.
Figures 6C-6D illustrate portions of images in the vicinity of the road and the road depicting the sides of the road in a predetermined height range consistent with an embodiment.
7 is a flowchart of a method for detection of a work area sign using LIDAR-based information consistent with an embodiment.
Figure 8A describes LIDAR-based detection of a work area sign at an area that is higher than the critical height from the surface of the road, consistent with an embodiment.
Figure 8B describes a LIDAR-based image depicting an area at a height above the threshold height from the surface of the road, consistent with an embodiment.
9 is a flowchart of a method for detection of construction zone objects using LIDAR-based information, consistent with an embodiment.
10A illustrates a LIDAR-based detection of a construction zone cone in an area that is within a critical distance from the surface of the road, consistent with an embodiment.
Figure 10B describes an LIDAR-based image depicting an area within a critical distance from the surface of the road, consistent with an embodiment.
FIG. 10C illustrates LIDAR-based detection of construction zone cones forming a road boundary, consistent with an embodiment.
FIG. 10D illustrates a LIDAR-based image depicting a construction zone cone forming a road boundary, consistent with an embodiment.
Figure 11 is a schematic description of a conceptual portion of a computer program consistent with one embodiment.
다음의 상세한 설명은 첨부 도면들을 참조로 하여 상기 개시된 시스템들 및 방법들의 다양한 구성들 및 기능들을 기술한다. 도면들에서, 컨텍스트가 달리 지시하지 않는 한, 유사한 부호들은 유사한 컴포넌트들을 식별한다. 본 명세서에서 기술된 예시적인 시스템 및 방법 실시예들은 한정하는 것으로 의도되지 않았다. 개시된 시스템들 및 방법들의 특정 양상들은 다양한 서로 다른 구성들로 배열되고 조합될 수 있음은 쉽게 이해될 것이고, 이 구성들 모두는 본 명세서에서 고려된다.The following detailed description describes various configurations and functions of the systems and methods disclosed above with reference to the accompanying drawings. In the Figures, similar symbols identify similar components, unless the context indicates otherwise. The exemplary systems and method embodiments described herein are not intended to be limiting. It will be readily understood that the specific aspects of the disclosed systems and methods may be arranged and combined in various different configurations, all of which are contemplated herein.
도로상에서 동작하는 자율 주행차는 내비게이션용 맵들에 의존하도록 구성될 수 있다. 몇몇의 예시들에서, 상기 도로상의 공사 구역의 존재로 인한 변화들은 상기 맵들에 반영되어 있지 않을 수 있다. 그러므로, 상기 자율 주행차는 상기 공사 구역을 검출하고 상기 공사 구역을 통해 안전하게 운전하도록 구성될 수 있다.An autonomous vehicle running on the road can be configured to rely on maps for navigation. In some instances, changes due to the presence of a work area on the road may not be reflected in the maps. Therefore, the autonomous vehicle can be configured to detect the work area and operate safely through the work area.
한 예시에서, 상기 차량을 제어하도록 구성된 컴퓨팅 디바이스는, 복수의 정보원들로부터 상기 차량이 운행하고 있는 상기 도로상의 공사 구역의 검출에 관한 정보를 수신하도록 구성된다. 또한, 상기 컴퓨팅 디바이스는 상기 정보에 기반해서, 상기 도로상의 공사 구역의 존재의 가능성을 결정하도록 구성된다. 추가적으로, 상기 컴퓨팅 디바이스는 상기 가능성에 기반해서, 상기 차량의 운전 거동과 관련된 제어 전략을 수정하고, 상기 차량을 상기 수정된 제어 전략에 기반해서 제어하도록 구성된다. In one example, a computing device configured to control the vehicle is configured to receive information regarding the detection of a work area on the road on which the vehicle is traveling from a plurality of sources. In addition, the computing device is configured to determine, based on the information, the likelihood of the presence of a work area on the road. Additionally, the computing device is configured to modify a control strategy associated with the driving behavior of the vehicle based on the likelihood, and to control the vehicle based on the modified control strategy.
예시적인 차량 제어 시스템은 자동차에서 구현되거나 자동차의 형태를 가질 수 있다. 대안적으로, 차량 제어 시스템은 승용차들, 트럭들, 모터사이클들, 버스들, 보트들, 비행기들, 헬리콥터들, 잔디 깎는 기계들, 레저 차량들, 놀이 기구들, 농장 시설, 공사 시설, 전차들, 골프 카트들, 기차들, 및 트롤리들(trolleys)과 같은 다른 차량들에서 구현되거나 다른 차량들의 형태를 가질 수 있다. 기타 차량들 또한 가능하다.Exemplary vehicle control systems may be implemented in an automobile or in the form of an automobile. Alternatively, the vehicle control system may be a passenger car, trucks, motorcycles, buses, boats, airplanes, helicopters, lawn mowers, leisure vehicles, rides, farm facilities, Such as golf carts, trains, and trolleys, or may take the form of other vehicles. Other vehicles are also available.
추가적으로, 예시적인 시스템은 본 명세서에 기술된 기능성을 제공하도록 적어도 하나의 프로세서에 의해서 실행 가능한 프로그램 명령들을 갖는 비일시적인 컴퓨터 판독 가능 매체의 형태를 가질 수 있다. 예시적인 시스템은 또한 그러한 프로그램 명령들을 갖는 그러한 비일시적인 컴퓨터 판독 가능 매체를 포함하는 자동차 또는 자동차의 서브시스템의 형태를 가질 수 있다.Additionally, the exemplary system may take the form of a non-volatile computer readable medium having program instructions executable by at least one processor to provide functionality described herein. An exemplary system may also take the form of a subsystem of a vehicle or automobile that includes such non-transitory computer-readable media having such program instructions.
이제 도면들을 참조하면, 도 1은 일 실시예에 부합하는, 예시 자동차(100)의 단순화된 블럭도이다. 자동차(100)에 결합되거나 상기 자동차에 포함된 컴포넌트들은 추진 시스템(102), 센서 시스템(104), 제어 시스템(106), 주변 장치들(108), 전력 공급원(110), 컴퓨팅 디바이스(111) 및 사용자 인터페이스(112)를 포함한다. 상기 컴퓨팅 디바이스(111)는 프로세서(113) 및 메모리(114)를 포함한다. 상기 메모리(114)는 상기 프로세서(113)에 의해 실행 가능한 명령들(115)을 포함하고, 또한 맵 데이터(116)를 저장한다. 자동차(100)의 컴포넌트들은 서로와 그리고/또는 각각의 시스템들에 결합된 다른 컴포넌트들과 상호연결된 방식으로 동작하도록 구성될 수 있다. 예를 들어, 전력 공급원(110)은 자동차(100)의 모든 컴포넌트들에 전력을 공급할 수 있다. 상기 컴퓨팅 디바이스(111)는 추진 시스템(102), 센서 시스템(104), 제어 시스템(106) 및 주변 장치(108)로부터 정보를 수신하고 이들을 제어하도록 구성될 수 있다. 상기 컴퓨팅 디바이스(111)는 사용자 인터페이스(112)상에 이미지들의 디스플레이를 생성하고 상기 사용자 인터페이스(112)로부터 입력들을 수신하도록 구성될 수 있다.Referring now to the drawings, FIG. 1 is a simplified block diagram of an exemplary automobile 100 in accordance with one embodiment. The components coupled to or contained in the vehicle 100 may include a propulsion system 102, a sensor system 104, a control system 106, peripheral devices 108, a power supply 110, a computing device 111, And a
다른 예시들에서, 상기 자동차(100)는 더 많은, 더 적은 또는 다른 시스템들을 포함할 수 있고, 각각의 시스템은 더 많은, 더 적은 또는 다른 컴포넌트들을 포함할 수 있다. 추가적으로, 도시된 상기 시스템들 및 컴포넌트들은 어떤 수의 방식들로 조합되거나 분리될 수 있다.In other instances, the automobile 100 may include more, fewer, or different systems, and each system may include more, fewer, or other components. Additionally, the depicted systems and components may be combined or separated in any number of ways.
상기 추진 시스템(102)은 상기 자동차(100)에 대한 전동의 움직임을 제공하도록 구성될 수 있다. 보여진 바와 같이, 추진 시스템(102)은 엔진/모터(118), 에너지 소스(120), 전송기(122) 및 휠/타이어들(124)을 포함한다.The propulsion system 102 may be configured to provide movement of the motor vehicle 100 relative to the motor vehicle 100. [ As shown, the propulsion system 102 includes an engine /
상기 엔진/모터(118)는 내부 연소 엔진, 전기 모터, 증기 엔진 및 스털링 엔진(Stirling engine)이거나 이들의 어떤 조합을 포함할 수 있다. 다른 모터들 및 엔진들 또한 가능하다. 몇몇의 예시들에서, 추진 시스템(102)은 다수의 유형들의 엔진들 및/또는 모터들을 포함할 수 있다. 예를 들어, 가스-전기 하이브리드 자동차는 가솔린 엔진 및 전기 모터를 포함할 수 있다. 기타 예시들도 가능하다.The engine /
에너지 소스(120)는 엔진/모터(118)에 완전히 또는 부분적으로 전력을 공급하는 에너지의 소스이다. 즉, 엔진/모터(118)는 에너지 소스(120)를 기계적 에너지로 전환하도록 구성된다. 에너지 소스들(120)의 예시들은 가솔린, 디젤, 다른 석유기반 연료, 프로판, 다른 압축된 가스-기반 연료들, 에탄올, 태양 전지판들, 배터리들, 및 전기전력의 다른 소스들을 포함한다. 에너지 소스(들)(120)은 추가적으로 또는 대안적으로 연료 탱크들, 배터리들, 커패시터들, 및/또는 플라이휠들을 포함할 수 있다. 몇몇의 예시들에서, 에너지 소스(120)는 또한 자동차(100)의 다른 시스템들에 대한 에너지를 공급할 수 있다.The energy source 120 is a source of energy that fully or partially powers the engine /
전송기(122)는 엔진/모터(118)로부터 휠/타이어들(124)에 기계적 힘을 전송하도록 구성될 수 있다. 이를 위하여, 전송기(122)는 기어박스, 클러치, 디퍼렌셜, 드라이브 섀프트, 및/또는 다른 요소들을 포함할 수 있다. 전송기(122)가 드라이브 섀프트들을 포함하는 예시들에서, 상기 드라이브 섀프트들은 상기 휠/타이어들(124)에 결합되도록 구성되는 하나 이상의 차축들을 포함할 수 있다.The transmitter 122 may be configured to transmit mechanical forces from the engine /
상기 자동차(100)의 휠/타이어들(124)은, 유니사이클, 바이사이클/모터사이클, 트리사이클, 또는 4-휠 포맷의 차/트럭을 포함하는, 다양한 포맷들로 구성될 수 있다. 6개 이상의 휠들을 포함하는 휠/타이어 포맷과 같은, 다른 휠/타이어 포맷들 또한 가능하다. 자동차(100)의 휠/타이어들(124)은 다른 휠/타이어들(124)에 관해 서로 다르게 회전하도록 구성될 수 있다. 몇몇의 예시들에서, 상기 휠/타이어들(124)은 상기 전송기(122)에 고정되게 부착된 적어도 하나의 휠 및 압력면(driving surface)과 접촉을 이룰 수 있는 휠의 둘레에 결합된 적어도 하나의 타이어를 포함한다. 휠/타이어들(124)은 금속 및 고무의 어떤 조합, 또는 다른 물질들의 조합을 포함할 수 있다.The wheels / tires 124 of the automobile 100 may be configured in various formats, including unicycle, bicycle / motorcycle, trike, or car / truck in a four-wheel format. Other wheel / tire formats are also possible, such as a wheel / tire format that includes six or more wheels. The wheels / tires 124 of the vehicle 100 may be configured to rotate differently with respect to the other wheels / tires 124. In some examples, the wheels / tires 124 include at least one wheel fixedly attached to the transmitter 122, and at least one wheel < RTI ID = 0.0 > Of tires. The wheels / tires 124 may comprise any combination of metal and rubber, or a combination of other materials.
추진 시스템(102)은 추가적으로 또는 대안적으로, 보여진 컴포넌트들 외의 컴포넌트들을 포함한다.Propulsion system 102 additionally or alternatively includes components other than those shown.
센서 시스템(104)은 자동차(100)가 위치된 환경에 관한 정보를 센싱하도록 구성되는 다수의 센서들을 포함한다. 보여진 바와 같이, 센서 시스템의 센서들은 센서들의 위치 및/또는 배향을 수정하도록 구성되는, 글로벌 포지셔닝 시스템(GPS) 모듈(126), 관성 측정 유닛(IMU)(128), 레이더(RADAR) 유닛(130), 라이다(LIDAR) 유닛(132), 카메라(134) 및 작동기들(136)을 포함한다. 센서 시스템(104)은 예를 들어, 자동차(100)의 내부 시스템들을 감시하는 센서들을 포함하는 추가적인 센서들을 또한 포함한다(예를 들어, 산소 감시, 연료 게이지, 엔진 오일 온도 등). 기타 센서들 또한 가능하다.The sensor system 104 includes a plurality of sensors configured to sense information regarding the environment in which the vehicle 100 is located. As shown, the sensors of the sensor system include a Global Positioning System (GPS)
GPS 모듈(126)은 상기 자동차(100)의 지리적 위치를 추정하도록 구성되는 어떤 센서일 수 있다. 이를 위해서, GPS 모듈(126)은 위성-기반 포지셔닝 데이터에 기반해서, 지구에 관한 자동차(100)의 위치를 추정하도록 구성되는 트랜시버를 포함한다. 일 예시에서, 컴퓨팅 디바이스(111)는 자동차(100)가 운행하고 있는 도로상의 차로 경계의 위치를 추정하기 위해 지도 데이터(116)와 조합하여 GPS 모듈(126)을 사용하도록 구성될 수 있다. 상기 GPS 모듈(126)은 기타 형태들 또한 취할 수 있다.The
IMU(128)는 관성 가속도에 기반한 자동차(100)의 위치 및 배향 변화들을 센싱하도록 구성되는 어떤 조합의 센서들일 수 있다. 몇몇의 예시들에서, 센서들의 조합은, 예를 들어, 가속도계 및 자이로스코프들을 포함할 수 있다. 센서들의 기타 조합들 또한 가능하다.
RADAR 유닛(130)은 물체의 범위, 고도, 방향, 또는 속도와 같은 물체의 특징들을 결정하기 위해 라디오파들을 사용하도록 구성될 수 있는 물체 검출 시스템으로 고려될 수 있다. RADAR 유닛(130)은 파동들의 경로에 있는 어떤 물체의 반응을 살피는 라디오파 또는 마이크로파들을 송신하도록 구성된다. 상기 물체는 파동의 에너지의 부분을 리시버(예를 들어, 접시 안테나 또는 안테나)에 돌려보내고, 상기 리시버는 또한 RADAR 유닛(130)의 부분일 수 있다. RADAR 유닛(130)은 또한 수신된 신호들의 디지털 신호 프로세싱을 수행하도록 구성될 수 있고(물체의 반응을 살핌), 물체를 식별하도록 구성될 수 있다.The
RADAR와 유사한 다른 시스템들은 전자기 스펙트럼의 다른 부분들에서 사용되어져 왔다. 일 예시는 LIDAR(광선 검출 및 거리 측정기)이고, 상기 LIDAR는 라디오파보다는 레이저들로부터의 가시광선을 사용하도록 구성된다.Other systems similar to RADAR have been used in other parts of the electromagnetic spectrum. One example is LIDAR (light beam detection and range finder), which is configured to use visible light from lasers rather than radio waves.
LIDAR 유닛(132)은 광선을 이용해서 자동차(100)가 위치된 환경에 있는 물체들을 센싱하거나 검출하도록 구성되는 센서를 포함한다. 일반적으로, LIDAR는 광선으로 타겟을 비춤에 의해 타겟까지의 거리 또는 타겟의 다른 특성들을 측정할 수 있는 광학적 원격 센싱 기술이다. 상기 광선은 레이저와 같은 어떤 유형의 전자기파일 수 있다. 일 예시로서, LIDAR 유닛(132)은 레이저의 펄스들을 방출하도록 구성되는 레이저 소스 및/또는 레이저 스캐너 및 레이저의 반사들을 수신하도록 구성되는 검출기를 포함한다. 예를 들어, LIDAR 유닛(132)은 회전 거울에 의해 반사되는 레이저 범위 검색기를 포함할 수 있고, 상기 레이저는, 1차원 또는 2차원에서, 디지털화되는 현장 주위에서 스캔되고, 명시된 각도 구간들에서 거리 측정들을 모으게 된다. 예시들에서, LIDAR 유닛(132)은 광선(예를 들어, 레이저) 소스, 스캐너 및 광학기기들, 광검출기 및 수신기 전자기기들과 위치 및 내비게이션 시스템과 같은 컴포넌트들을 포함할 수 있다.The LIDAR unit 132 includes sensors that are configured to sense or detect objects in an environment in which the vehicle 100 is located using light rays. Generally, LIDAR is an optical remote sensing technique that can measure distances to a target or other characteristics of a target by illuminating the target with a beam of light. The light beam can be any type of electromagnetic file, such as a laser. As one example, the LIDAR unit 132 includes a laser source and / or a laser scanner configured to emit pulses of the laser and a detector configured to receive reflections of the laser. For example, the LIDAR unit 132 may include a laser range finder that is reflected by a rotating mirror, which is scanned around the field being digitized, in one or two dimensions, The measurements are collected. In the examples, the LIDAR unit 132 may include components such as a light (e.g., laser) source, a scanner and optics, a photodetector and receiver electronics and a location and navigation system.
일 예시에서, LIDAR 유닛(132)은 객체들의 상을 비추기 위해 자외선(UV), 가시광선, 또는 적외선을 사용하도록 구성되고 비-금속 물체들을 포함하는, 넓은 범위의 타겟들과 함께 사용될 수 있다. 일 예시에서, 좁은 레이저 빔은 고해상도로 물체의 물리적 특징들을 매핑하기 위해 사용될 수 있다.In one example, the LIDAR unit 132 can be used with a wide range of targets, including non-metal objects, configured to use ultraviolet (UV), visible, or infrared radiation to illuminate the images of objects. In one example, a narrow laser beam can be used to map the physical characteristics of an object at high resolution.
예시들에서, 약 10 마이크로미터(적외선)로부터 약 250 나노미터(자외선)까지의 범위에 있는 파장들이 사용될 수 있다. 전형적으로 광선은 후방 산란을 통해 반사된다. 예를 들어, 형광 발광(fluorescence)뿐만 아니라, 레일리(Rayleigh) 산란, 미(Mie) 산란 및 라만(Raman) 산란과 같은, 서로 다른 유형들의 산란은 서로 다른 LIDAR 응용들에 대해 사용된다. 후방 산란의 서로 다른 종류들에 기반해서, 예시로서, LIDAR는 레일리 LIDAR, 미 LIDAR, 라만 LIDAR 및 나트륨/철/칼륨 형광 발광 LIDAR로서 불릴 수 있다. 파장들의 적절한 조합들은 예를 들어, 반사된 신호들의 강도에 있어서의 파장-의존적 변화들을 검색함에 의해 물체들의 원격 매핑을 가능하게 한다.In the examples, wavelengths ranging from about 10 micrometers (infrared) to about 250 nanometers (ultraviolet) can be used. Typically, the light beam is reflected through backscattering. For example, different types of scattering, such as fluorescence as well as Rayleigh scattering, Mie scattering and Raman scattering, are used for different LIDAR applications. Based on different kinds of backscattering, for example, LIDAR can be referred to as Rayleigh LIDAR, LIDAR, Raman LIDAR, and sodium / iron / potassium fluorescence LIDAR. Suitable combinations of wavelengths enable remote mapping of objects, for example, by searching for wavelength-dependent changes in the intensity of the reflected signals.
3차원(3D) 이미징은 스캐닝 및 비-스캐닝 LIDAR 시스템들 둘 모두를 사용해서 달성될 수 있다. "3D 게이트로 된 뷰잉 레이저 레이더"는 펄스로 된 레이저 및 고속 게이트로 된 카메라를 적용하는 비-스캐닝 레이저 거리 측정 시스템의 일 예시이다. LIDAR를 이미징하는 것은, 전형적으로 CMOS 및 하이브리드 CMOS/CCD 제작 기법들을 사용한 단일 칩들 상에 구축될 수 있는 고속 검출기들의 어레이 및 변조 감도 검출기 어레이들을 사용해서 수행될 수도 있다. 이러한 디바이스들에서, 픽셀 각각은, 어레이가 카메라로부터의 이미지를 나타내게 프로세싱되도록 복조 또는 고속 게이팅에 의해 국부적으로 프로세싱될 수 있다. 이 기법을 사용해서, 많은 수천 개의 픽셀들은 LIDAR 유닛(132)에 의해 검출되는 물체 또는 장면을 나타내는 3D 포인트 클라우드를 생성하기 위해 동시에 획득될 수 있다.Three-dimensional (3D) imaging can be achieved using both scanning and non-scanning LIDAR systems. "3D gated viewing laser radar" is an example of a non-scanning laser distance measurement system employing pulsed lasers and cameras with high-speed gates. Imaging LIDAR may be performed using arrays of high-speed detectors and modulation sensitivity detector arrays, which may typically be built on single chips using CMOS and hybrid CMOS / CCD fabrication techniques. In such devices, each of the pixels may be locally processed by demodulation or fast gating so that the array is processed to represent an image from the camera. Using this technique, many thousands of pixels can be obtained at the same time to generate a 3D point cloud representing an object or scene detected by the LIDAR unit 132.
포인트 클라우드는 3D 좌표 시스템에서 꼭짓점들의 세트를 포함한다. 이 꼭짓점들은, 예를 들어, X, Y 및 Z 좌표들에 의해 정의될 수 있고, 이들은 물체의 외부 표면을 나타낼 수 있다. LIDAR 유닛(132)은 상기 물체의 표면상의 다수의 포인트들을 측정함에 의해 포인트 클라우드를 생성하도록 구성되고, 데이터 파일로서 상기 포인트 클라우드를 출력한다. LIDAR 유닛(132)에 의한 물체의 3D 스캐닝 프로세스의 결과로서, 포인트 클라우드는 물체를 식별하고 시각화하기 위해 사용된다.The point cloud contains a set of vertices in the 3D coordinate system. These vertices can be defined, for example, by X, Y and Z coordinates, which can represent the outer surface of the object. The LIDAR unit 132 is configured to generate a point cloud by measuring a plurality of points on the surface of the object, and outputs the point cloud as a data file. As a result of the 3D scanning process of the object by the LIDAR unit 132, the point cloud is used to identify and visualize the object.
일 예시에서, 포인트 클라우드는 물체를 시각화하기 위해 직접 렌더링된다. 다른 예시에서, 상기 포인트 클라우드는 표면 복구로 언급되는 프로세스를 통해 폴리곤 또는 삼각형의 그물망 모델들로 전환될 수 있다. 포인트 클라우드를 3D 표면으로 전환하기 위한 예시 기법들은 들로네 삼각 측량(Delaunay triangulation), 알파 모양들(alpha shapes) 및 볼 피벗(ball pivoting)을 포함한다. 이러한 기법들은 상기 포인트 클라우드의 존재하는 꼭짓점들을 통한 삼각형들의 네트워크의 구축을 포함한다. 다른 예시 기법들은 상기 포인트 클라우드를 체적 거리 필드(volumetric distance field)로 전환하는 것과 마칭 큐브 알고리즘(marching cubes algorithm)을 통해 정의되는 음함수 곡면(implicit surface)을 복구하는 것을 포함한다.In one example, a point cloud is rendered directly to visualize an object. In another example, the point cloud can be converted to mesh models of polygons or triangles through a process referred to as surface recovery. Exemplary techniques for converting a point cloud to a 3D surface include Delaunay triangulation, alpha shapes, and ball pivoting. These techniques involve building a network of triangles through existing vertices of the point cloud. Other exemplary techniques include converting the point cloud to a volumetric distance field and recovering an implicit surface defined through a marching cubes algorithm.
카메라(134)는 자동차(100)가 위치된 환경의 이미지들을 캡쳐하도록 구성되는 어떤 카메라(예를 들어, 스틸 카메라, 비디오 카메라 등)일 수 있다. 이를 위해서, 상기 카메라는 가시광선을 검출하도록 구성되거나, 적외선 또는 자외광선과 같은, 스펙트럼의 다른 부분들로부터 광선을 검출하도록 구성된다. 카메라의 다른 유형들 또한 가능하다. 상기 카메라(134)는 2차원 검출기일 수 있거나, 3차원 공간범위를 가질 수 있다. 몇몇의 예시들에서, 상기 카메라(134)는, 예를 들어, 상기 카메라(134)로부터 상기 환경의 다수의 포인트들까지의 거리를 표시하는 2차원 이미지를 발생시키도록 구성되는 범위 검출기일 수 있다. 이를 위해서, 상기 카메라(134)는 하나 이상의 범위 검출 기법들을 사용할 수 있다. 예를 들어, 상기 카메라(134)는 그리드 또는 체크보드 패턴과 같은 미리 결정된 광선 패턴을 갖는 환경에서의 물체를 비추고 상기 물체로부터 상기 미리 결정된 광선 패턴의 반사를 검출하기 위해 카메라(134)를 사용하는 자동차(100)의 구조화된 광선 기법을 사용하도록 구성될 수 있다. 상기 반사된 광선 패턴에서의 왜곡들에 기반해서, 상기 자동차(100)는 상기 물체상의 포인트들까지의 거리를 결정하도록 구성될 수 있다. 상기 미리 결정된 광선 패턴은 적외광선, 또는 기타 파장의 광선을 포함할 수 있다.The camera 134 may be any camera (e.g., a still camera, a video camera, etc.) configured to capture images of the environment in which the vehicle 100 is located. To this end, the camera is configured to detect visible light or to detect light rays from other parts of the spectrum, such as infrared or ultraviolet light. Other types of cameras are also possible. The camera 134 may be a two-dimensional detector or may have a three-dimensional spatial extent. In some instances, the camera 134 may be a range detector configured to generate a two-dimensional image representing, for example, the distance from the camera 134 to a plurality of points of the environment . To this end, the camera 134 may use one or more range detection techniques. For example, the camera 134 may use a camera 134 to illuminate an object in an environment having a predetermined pattern of light rays, such as a grid or checkered pattern, and to detect reflections of the predetermined pattern of light from the object And may be configured to use the structured light beam technique of the automobile 100. Based on the distortions in the reflected light beam pattern, the automobile 100 can be configured to determine the distance to the points on the object. The predetermined light beam pattern may comprise infrared light, or other wavelengths of light.
상기 작동기들(136)은, 예를 들어, 상기 센서들의 위치 및/또는 배향을 수정하도록 구성될 수 있다.The actuators 136 can be configured, for example, to modify the position and / or orientation of the sensors.
상기 센서 시스템(104)은 추가적으로 또는 대안적으로, 보여진 컴포넌트들 외의 컴포넌트들을 포함할 수 있다.The sensor system 104 may additionally or alternatively comprise components other than those shown.
제어 시스템(106)은 자동차(100) 및 자동차의 컴포넌트들의 동작을 제어하도록 구성될 수 있다. 이를 위해서, 상기 제어 시스템(106)은 조종 유닛(138), 드로틀(throttle)(140), 브레이크 유닛(142), 센서 퓨전 알고리즘(144), 컴퓨터 시각 시스템(146), 내비게이션 또는 경로 시스템(148) 및 장애물 회피 시스템(150)을 포함할 수 있다.The control system 106 may be configured to control the operation of the vehicle 100 and the components of the vehicle. To this end, the control system 106 includes a
조종 유닛(138)은 자동차(100)의 진로 또는 방향을 조정하도록 구성되는 메커니즘들의 어떤 조합일 수 있다.The
드로틀(140)은 엔진/모터(118)의 동작 속도 및 가속도, 또한 자동차(100)의 속도 및 가속도를 제어하도록 구성되는 메커니즘들의 어떤 조합일 수 있다.The
브레이크 유닛(142)은 자동차(100)를 감속하도록 구성되는 메커니즘들의 어떤 조합일 수 있다. 예를 들어, 브레이크 유닛(142)은 휠들/타이어들(124)을 늦추기 위해 마찰을 사용할 수 있다. 다른 예시로서, 브레이크 유닛(142)은 재생적이 되고 휠/타이어들(142)의 운동 에너지를 전류로 전환하도록 구성된다. 브레이크 유닛(142)은 기타 형태들 또한 취할 수 있다.The
센서 퓨전 알고리즘(144)은 예를 들어, 컴퓨팅 디바이스(111)에 의해 실행 가능한 알고리즘(또는 알고리즘을 저장하는 컴퓨터 프로그램 물)을 포함한다. 상기 센서 퓨전 알고리즘(144)은 입력으로서 센서 시스템(104)으로부터의 데이터를 받아들이도록 구성된다. 상기 데이터는, 예를 들어, 센서 시스템(104)의 센서들에서 센싱된 정보를 나타내는 데이터를 포함할 수 있다. 상기 센서 퓨전 알고리즘(144)은, 예를 들어, 칼만 필터(Kalman filter), 바예시안 네트워크(Bayesian network) 또는 기타 알고리즘을 포함할 수 있다. 상기 센서 퓨전 알고리즘(144)은, 예를 들어, 자동차(100)가 위치된 환경에서의 개별적인 물체들 및/또는 구성들의 평가, 특정 상황들의 평가, 그리고/또는 특정 상황들에 기반한 가능한 영향들의 평가와 같은 것을 포함하는, 센서 시스템(104)으로부터의 데이터에 기반한 다양한 평가들을 제공하도록 더 구성될 수 있다. 기타 평가들도 또한 가능하다.The sensor fusion algorithm 144 includes, for example, algorithms (or computer programs that store algorithms) that are executable by the computing device 111. The sensor fusion algorithm 144 is configured to accept data from the sensor system 104 as input. The data may include, for example, data indicative of information sensed at the sensors of the sensor system 104. The sensor fusion algorithm 144 may include, for example, a Kalman filter, a Bayesian network or other algorithms. The sensor fusion algorithm 144 may be used to evaluate individual objects and / or configurations in, for example, an environment in which the vehicle 100 is located, an assessment of specific situations, and / or an assessment of possible impacts Based on data from the sensor system 104, including, for example, < RTI ID = 0.0 > and / or < / RTI > Other assessments are also possible.
컴퓨터 시각 시스템(146)은 예를 들어, 차로 정보, 교통 신호들 및 장애물들과 같은 것을 포함하는, 자동차(100)가 위치된 환경에서의 물체들 및/또는 특징들을 식별하기 위한 카메라(134)에 의해 캡쳐된 이미지들을 프로세싱하고 분석하도록 구성되는 어떤 시스템일 수 있다. 이를 위해서, 컴퓨터 시각 시스템(146)은 물체 인식 알고리즘, 운동으로부터의 구조(SFM) 알고리즘, 비디오 추적 또는 다른 컴퓨터 시각 기법들을 사용할 수 있다. 몇몇의 예시들에서, 컴퓨터 시각 시스템(146)은 환경, 트랙 물체들을 매핑하고, 물체들의 속도를 추정하는 등을 하도록 더 구성될 수 있다.The
내비게이션 및 경로 시스템(148)은 자동차(100)를 위한 운전 경로를 결정하도록 구성되는 어떤 시스템일 수 있다. 내비게이션 및 경로 시스템(148)은 자동차(100)가 동작 중일 동안 동적으로 운전 경로를 업데이트하도록 더 구성될 수 있다. 몇몇의 예시들에서, 상기 내비게이션 및 경로 시스템(148)은 자동차(100)를 위한 운전 경로를 결정하기 위해, 센서 퓨전 알고리즘(144), GPS 모듈(126) 및 하나 이상의 미리 결정된 맵들로부터 데이터를 통합하도록 구성될 수 있다.The navigation and routing system 148 may be any system configured to determine a driving path for the automobile 100. The navigation and routing system 148 may be further configured to dynamically update the driving route while the vehicle 100 is operating. In some examples, the navigation and path system 148 may include data from sensor fusion algorithm 144,
장애물 회피 시스템(150)은 자동차(100)가 위치된 환경에서의 장애물들을 식별, 평가 및 회피하거나 그렇지 않으면 지나치도록 구성되는 어떤 시스템일 수 있다.The
제어 시스템(106)은 추가적으로 또는 대안적으로 보여지는 컴포넌트들 외의 컴포넌트들을 포함할 수 있다.The control system 106 may include components other than those shown additionally or alternatively.
주변 장치들(108)은 자동차(100)로 하여금 외부 센서들, 다른 자동차들 및/또는 사용자와 인터랙션하는 것을 가능하게 하도록 구성될 수 있다. 이를 위해서, 상기 주변 장치들(108)은 무선 통신 시스템(152), 터치스크린(154), 마이크로폰(156) 및/또는 스피커(158)를 포함할 수 있다.Peripherals 108 may be configured to enable vehicle 100 to interact with external sensors, other automobiles, and / or users. To this end, the peripheral devices 108 may include a wireless communication system 152, a touch screen 154, a
무선 통신 시스템(152)은 직접 또는 통신 네트워크를 통해, 하나 이상의 다른 자동차들, 센서들, 또는 다른 엔티티들에 무선으로 결합되도록 구성되는 어떤 시스템일 수 있다. 이를 위해서, 상기 무선 통신 시스템(152)은 직접 또는 에어 인터페이스를 통해 다른 자동차들, 센서들, 또는 다른 엔티티들과 통신하기 위한 안테나 및 칩셋을 포함할 수 있다. 상기 칩셋 또는 무선 통신 시스템(152)은 일반적으로 다른 가능성들 중에서도, 블루투스, IEEE 802.11(IEEE 802.11의 어떤 개정들이든지 포함해서)에 기술된 통신 프로토콜들, (GSM, CDMA, UMTS, EV-DO, WiMAX 또는 LTE와 같은)셀룰러 기술, 지그비(Zigbee), 단거리 전용 통신들(DSRC) 및 라디오 주파수 식별(RFID) 통신들과 같은 무선 통신의 하나 이상의 다른 유형들에 따라 통신하도록 배열될 수 있다. 무선 통신 시스템(152)은 기타 형태들 또한 취할 수 있다.Wireless communication system 152 may be any system configured to wirelessly couple to one or more other automobiles, sensors, or other entities, either directly or over a communication network. To this end, the wireless communication system 152 may include an antenna and chipset for communicating with other automobiles, sensors, or other entities, either directly or through an air interface. The chipset or wireless communication system 152 generally includes, among other possibilities, the communication protocols described in Bluetooth, IEEE 802.11 (including any revisions of IEEE 802.11), (GSM, CDMA, UMTS, EV- (E.g., WiMAX or LTE) cellular technology, Zigbee, short range dedicated communications (DSRC), and radio frequency identification (RFID) communications. The wireless communication system 152 may take other forms as well.
터치스크린(154)은 자동차(100)에 대한 명령들을 입력하기 위해 사용자에 의해 사용될 수 있다. 이를 위해서, 터치스크린(154)은 다른 가능성들 중에서도, 용량성 센싱, 저항성 센싱, 또는 표면 탄성파 프로세스를 통해 사용자의 손가락의 위치 및 움직임 중 적어도 하나를 센싱하도록 구성될 수 있다. 상기 터치스크린(154)은 터치스크린 표면에 평행하거나 평면상의 방향, 상기 터치스크린 표면에 수직인 방향, 또는 둘 모두의 방향으로의 손가락 움직임을 센싱할 수 있고, 또한 상기 터치스크린 표면에 인가되는 압력의 레벨을 센싱할 수 있다. 상기 터치스크린(154)은 하나 이상의 반투명 또는 투명 절연층 및 하나 이상의 반투명 또는 투명 전도층들로 형성될 수 있다. 상기 터치스크린(154)은 기타 형태들 또한 취할 수 있다.The touch screen 154 may be used by the user to input commands for the automobile 100. To this end, the touch screen 154 may be configured to sense at least one of the position and movement of the user's finger through capacitive sensing, resistive sensing, or surface acoustic wave processes among other possibilities. The touch screen 154 may sense finger motion in a direction parallel to or parallel to the surface of the touch screen, in a direction perpendicular to the touch screen surface, or both, Can be detected. The touch screen 154 may be formed of one or more semitransparent or transparent insulating layers and one or more semitransparent or transparent conducting layers. The touch screen 154 may take other forms as well.
마이크로폰(156)은 자동차(100)의 사용자로부터 오디오(예를 들어, 음성 커맨드 또는 다른 오디오 입력)를 수신하도록 구성될 수 있다. 유사하게, 스피커들(158)은 자동차(100)의 사용자에게 오디오를 출력하도록 구성될 수 있다.The
주변 장치들(108)은 추가적으로 또는 대안적으로 보여진 컴포넌트들 외의 컴포넌트들을 포함할 수 있다.Peripherals 108 may additionally or alternatively include components other than those shown.
전력 공급원(110)은 자동차(100)의 몇몇의 또는 전부의 컴포넌트들에 전력을 제공하도록 구성될 수 있다. 이를 위해서, 상기 전력 공급원(110)은, 예를 들어, 재충전 가능한 리튬-이온 또는 납축 배터리를 포함할 수 있다. 몇몇의 예시들에서, 배터리들의 하나 이상의 뱅크들은 전력을 제공하도록 구성될 수 있다. 기타 전력 공급 물질들 및 구성들 또한 가능하다. 몇몇의 예시들에서, 상기 전력 공급원(110) 및 에너지 소스(120)는 몇몇의 순수 전기자동차들에서와 같이, 함께 구현될 수 있다.The power supply 110 may be configured to provide power to some or all of the components of the vehicle 100. [ To this end, the power source 110 may comprise, for example, a rechargeable lithium-ion or lead-acid battery. In some examples, one or more banks of batteries may be configured to provide power. Other power supply materials and configurations are also possible. In some instances, the power source 110 and the energy source 120 may be implemented together, such as in some pure electric vehicles.
컴퓨팅 디바이스(111)에 포함된 프로세서(113)는 하나 이상의 일반목적의 프로세서들 및/또는 하나 이상의 특수목적 프로세서들(예를 들어, 이미지 프로세서, 디지털 신호 프로세서 등)을 포함할 수 있다. 상기 프로세서(113)가 2개 이상의 프로세서들을 포함하는 범위에 있어서는, 그러한 프로세서들은 개별적으로 또는 조합으로 동작할 수 있다. 상기 컴퓨팅 디바이스(111)는 예를 들어, 사용자 인터페이스(112)를 통해 수신된 입력에 기반해서 자동차(100)의 기능들을 제어하도록 구성될 수 있다.The
메모리(114)는, 또한, 광학적, 자기적, 및/또는 유기적 저장소와 같은, 하나 이상의 휘발성 및/또는 하나 이사의 비휘발성 저장 컴포넌트들을 포함할 수 있고, 상기 메모리(114)는 프로세서(113)와 전체로서 또는 일부로서 통합될 수 있다. 상기 메모리(114)는 다양한 자동차 기능들을 실행하기 위해 프로세서(113)에 의해 실행 가능한 명령들(115)(예를 들어, 프로그래밍 로직)을 포함할 수 있다.The memory 114 may also include one or more volatile and / or one non-volatile storage components, such as optical, magnetic, and / or organic storage, As a whole or as part thereof. The memory 114 may include instructions 115 (e.g., programming logic) executable by the
자동차(100)의 컴포넌트들은 각각의 시스템들 내부 그리고/또는 외부의 다른 컴포넌트들과 함께 상호 연결된 방식으로 동작하도록 구성될 수 있다. 이를 위해서, 자동차(100)의 상기 컴포넌트들 및 시스템들은 시스템 버스, 네트워크, 및/또는 기타 연결 메커니즘(미도시)에 의해 함께 통신적으로 연결될 수 있다.The components of the vehicle 100 may be configured to operate in a manner interconnected with other components within and / or within each of the systems. To that end, the components and systems of the vehicle 100 may be communicatively coupled together by a system bus, a network, and / or other connection mechanisms (not shown).
더욱이, 컴포넌트들 및 시스템들 각각이 자동차(100)에 통합된 것으로 도시된 반면에, 몇몇의 예시들에서, 하나 이상의 컴포넌트들 또는 시스템들은 유선 또는 무선 연결들을 사용해서 상기 자동차(100)에 제거 가능하게 마운팅되거나 그렇지 않으면 (기계적으로 또는 전기적으로) 상기 자동차(100)에 연결될 수 있다.Further, while each of the components and systems is shown as being integrated in the vehicle 100, in some instances, one or more components or systems may be removable (e.g., removable) to the vehicle 100 using wired or wireless connections. (Mechanically or electrically) to the vehicle 100. The vehicle 100 may be connected to the vehicle 100 via a cable.
상기 자동차(100)는 보여진 요소들에 더하여 또는 상기 보여진 요소들 대신에 하나 이상의 요소들을 포함할 수 있다. 예를 들어, 자동차(100)는 하나 이상의 추가적인 인터페이스들 및/또는 전력 공급원들을 포함할 수 있다. 기타 추가적인 컴포넌트들 또한 가능하다. 이러한 예시들에서, 상기 메모리(114)는 상기 추가적인 컴포넌트들을 제어하고 그리고/또는 상기 추가적인 컴포넌트들과 통신하기 위해 상기 프로세서(113)에 의해 실행 가능한 명령들을 더 포함할 수 있다.The vehicle 100 may include one or more elements in addition to, or instead of, the elements shown. For example, the vehicle 100 may include one or more additional interfaces and / or power supplies. Other additional components are also possible. In these instances, the memory 114 may further include instructions executable by the
도 2는 일 실시예에 따른, 예시 자동차(200)를 도해한다. 특별히, 도 2는 상기 자동차(200)의 우측면도, 정면도, 배면도 및 상면도를 도시한다. 자동차(200)가 도 2에서 자동차로서 도해되었지만, 기타 예시들이 가능하다. 예를 들어, 상기 자동차(200)는 다른 예시들 중에서도, 트럭, 밴, 세미-트레일러 트럭, 모터사이클, 골프 카트, 오프-로드 차량 또는 농장 차량을 나타낼 수 있다. 보여진 바와 같이, 상기 자동차(200)는 제1 센서 유닛(202), 제2 센서 유닛(204), 제3 센서 유닛(206), 무선 통신 시스템(208) 및 카메라(210)를 포함한다.FIG. 2 illustrates an
제1, 제2 및 제3 센서 유닛들(202-206) 각각은 글로벌 포지셔닝 시스템 센서들, 관성 측정 유닛들, RADAR 유닛들, LIDAR 유닛들, 카메라들, 차로 검출 센서들, 음향 센서들의 어떤 조합이든 포함할 수 있다. 기타 유형의 센서들 또한 가능하다.Each of the first, second and third sensor units 202-206 may comprise any combination of global positioning system sensors, inertial measurement units, RADAR units, LIDAR units, cameras, lane detection sensors, acoustic sensors . Other types of sensors are also possible.
제1, 제2 및 제3 센서 유닛들(202)이 자동차(200) 상의 특정 위치들에 마운팅되는 것으로 도시되는 반면에, 몇몇의 예시들에서, 센서 유닛(202)은 자동차(200)의 내부 또는 외부 둘 중 한 곳의, 자동차 상의 다른 곳에 마운팅될 수 있다. 더욱이, 3개의 센서 유닛들이 도시된 반면에, 몇몇의 예시들에서 더 많은 또는 더 적은 센서 유닛들이 자동차(200)에 포함될 수 있다.The
몇몇의 예시들에서, 하나 이상의 제1, 제2 및 제3 센서 유닛들(202-206)은 상기 센서들이 이동 가능하게 마운팅될 수 있는 하나 이상의 이동 가능한 마운트들을 포함할 수 있다. 상기 이동가능한 마운트들은 예를 들어, 회전 플랫폼을 포함할 수 있다. 상기 회전 플랫폼상에 마운팅된 센서들은 상기 센서들이 자동차(200) 주위의 각각의 방향으로부터 정보를 획득하도록 회전될 수 있다. 대안적으로 또는 추가적으로, 상기 이동 가능한 마운트는 기울어진 플랫폼을 포함할 수 있다. 기울어진 플랫폼상에 마운팅된 센서들은, 상기 센서들이 다양한 각도들로부터 정보를 획득할 수 있도록, 특정한 범위의 각도들 및/또는 방위각들(azimuths) 내로 기울어질 수 있다. 상기 이동 가능한 마운트는 기타 형태들 또한 취할 수 있다.In some instances, the one or more first, second and third sensor units 202-206 may comprise one or more movable mounts from which the sensors may be movably mounted. The moveable mounts may include, for example, a rotating platform. The sensors mounted on the rotating platform can be rotated so that the sensors acquire information from each direction around the
더욱이, 몇몇의 예시들에서, 하나 이상의 제1, 제2 및 제3 센서 유닛들(202-206)은 센서들 및/또는 이동 가능한 마운트들을 움직임에 의해 센서 유닛에서 센서들의 위치 및/또는 배향을 조정하도록 구성되는 하나 이상의 작동기들을 포함할 수 있다. 예시적인 작동기들은 모터들, 기압 작동기들(pneumatic actuators), 유압 피스톤들, 릴레이들, 솔레노이드들 및 압전 작동기들(piezoelectric actuators)을 포함한다. 기타 작동기들 또한 가능하다.Moreover, in some examples, the one or more first, second and third sensor units 202-206 may be positioned and / or oriented in the sensor unit by movement of the sensors and / And may include one or more actuators configured to condition the actuator. Exemplary actuators include motors, pneumatic actuators, hydraulic pistons, relays, solenoids, and piezoelectric actuators. Other actuators are also possible.
상기 무선 통신 시스템(208)은 도 1의 무선 통신 시스템(152)에 관해 위에 기술된 것과 같은, 직접 또는 통신 네트워크를 통해 하나 이상의 다른 자동차들, 센서들, 또는 다른 엔티티들과 무선으로 결합 되도록 구성되는 어떤 시스템일 수 있다. 상기 무선 통신 시스템(208)은 상기 자동차(200)의 지붕 상에 위치되는 것으로 도시된 반면에, 다른 예시들에서 상기 무선 통신 시스템(208)은 다른 곳에, 완전히 또는 부분적으로 위치될 수 있다.The
카메라(210)는 자동차(200)가 위치된 환경의 이미지들을 캡쳐하도록 구성되는 어떤 카메라(예를 들어, 스틸 카메라, 비디오 카메라 등)일 수 있다. 이를 위해서, 상기 카메라는 도 1의 카메라(134)에 관해 위에 기술된 형태들 중 어떤 것이든 취할 수 있다. 상기 카메라(210)가 자동차(200)의 앞유리 내에 마운팅되는 것으로 도시된 반면에, 다른 예시들에서 상기 카메라(210)는 자동차(200)의 내부 또는 외부 둘 중 하나의, 자동차(200)의 어느 곳에서든지 마운팅될 수 있다.The
자동차(200)는 보여진 컴포넌트들에 추가하여 또는 상기 보여진 컴포넌트들 대신에 하나 이상의 기타 컴포넌트들을 포함할 수 있다.The
자동차(200)의 제어 시스템은 다수의 가능한 제어 전략들 중으로부터의 제어 전략에 부합하게 자동차(200)를 제어하도록 구성될 수 있다. 상기 제어 시스템은 자동차(200)(자동차(200) 상에서 또는 자동차(200) 밖에서)에 결합되는 센서들로부터의 정보를 수신하고, 상기 정보에 기반해서 상기 제어 전략(및 관련된 운전 거동)을 수정하고, 상기 수정된 제어 전략에 부합하게 상기 자동차(200)를 제어하도록 구성될 수 있다. 상기 제어 시스템은 상기 센서들로부터 수신된 정보를 감시하고, 계속적으로 운전 상황들을 평가하도록 더 구성될 수 있고, 또한 상기 운전 상황들에서의 변화들에 기반해서 상기 제어 전략 및 운전 거동을 수정하도록 구성될 수 있다.The control system of the
도 3은 일 실시예에 부합하게, 복수의 정보원들을 사용한 공사 구역의 검출을 위한 방법(300)의 흐름도이다. 도 4는 상기 방법(300)을 도해하기 위해, 일 실시예에 부합하게, 공사 구역에 접근하는 차량을 도해한다. 도 3 및 도 4는 함께 기술될 것이다.3 is a flow diagram of a
상기 방법(300)은 하나 이상의 블럭들(302-308)에 의해 도해된 대로, 하나 이상의 동작들, 기능들 또는 동작들을 포함할 수 있다. 상기 블럭들이 순차적인 순서로 도해되었음에도 불구하고, 이러한 블럭들은 몇몇의 예시들에서 병렬로 그리고/또는 본 명세서에 기술된 순서와는 다른 순서로 수행될 수 있다. 또한, 다양한 블럭들이 더 적은 블럭들로 조합되고, 추가적인 블럭들로 분리되고, 그리고/또는 바람직한 구현에 기반해서 제거될 수 있다.The
이에 더하여, 상기 방법(300) 및 본 명세서에 개시된 다른 프로세스 및 방법들에 대해, 상기 흐름도는 현재 실시예들의 하나의 가능한 구현의 기능성 및 동작을 도시한다. 이러한 관점에서, 블럭 각각은 프로그래밍 코드의 모듈, 세그먼트, 또는 부분을 나타낼 수 있고, 이들은 상기 프로세스에서의 구체적인 논리적 기능들 또는 단계들을 구현하기 위한 프로세서에 의해 실행 가능한 하나 이상의 명령들을 포함한다. 상기 프로그래밍 코드는 예를 들어, 디스크 또는 하드 드라이브를 포함하는 저장 디바이스와 같은, 어떤 유형의 컴퓨터 판독 가능 매체 또는 메모리상에 저장될 수 있다. 상기 컴퓨터 판독 가능 매체는 예를 들어, 레지스터 메모리, 프로세서 캐시 및 랜덤 액세스 메모리(RAM)와 같은 짧은 기간동안 데이터를 저장하는 컴퓨터 판독 가능 매체와 같은, 비일시적 컴퓨터 판독 가능 매체를 포함할 수 있다. 컴퓨터 판독 가능 매체는 또한, 예를 들어 판독 전용 메모리(ROM), 광학적 또는 자기적 디스크들, 컴팩트 디스크 판독 전용 메모리(CD-ROM)과 같은, 2차적인 또는 지속적인 장기간 저장소와 같은, 비일시적 매체 또는 메모리를 포함할 수 있다. 컴퓨터 판독 가능 매체는 또한 어떤 다른 휘발성 또는 비휘발성 저장 시스템일 수 있다. 컴퓨터 판독 가능 매체는, 예를 들어, 컴퓨터 판독 가능 저장 매체, 유형의 저장 디바이스, 또는 제작물의 기타 물품으로 고려될 수 있다.In addition, for the
이에 더하여, 방법(300) 및 본 명세서에 개시된 다른 프로세스들 및 방법들에 대해, 도 3의 각 블럭은 프로세스에서 구체적인 논리적 기능들을 수행하기 위해 연결된 회로망을 나타낼 수 있다.In addition, for the
블럭(302)에서, 상기 방법(300)은 차량을 제어하도록 구성된 컴퓨팅 디바이스에서, 복수의 정보원들으로부터, 상기 차량이 운행하고 있는 도로상의 공사 구역의 검출에 관한 정보를 수신하는 것을 포함하고, 그리고 상기 복수의 정보원들 중 정보원 각각에 그 정보원으로부터 수신된 상기 정보에 기반해서 상기 공사 구역의 검출의 신뢰의 레벨을 표시하는 각각의 신뢰성 메트릭이 할당된다. 상기 컴퓨팅 디바이스는, 예를 들어, 상기 차량 상에 있거나 상기 차량 밖에 있지만 상기 차량과 무선 통신중일 수 있다. 또한, 컴퓨팅 디바이스는 차량을 자율 또는 반자율 동작 모드로 제어하도록 구성될 수 있다. 더욱이, 컴퓨팅 디바이스는 예를 들어, 차량의 시스템들 및 서브시스템들의 상황, 운전 상황들, 도로 상황들 등과 관련된 정보를 상기 차량에 결합된 센서들로부터 수신하도록 구성될 수 있다. At
도 4는 도로(404) 상의 공사 구역에 접근하는 차량(402)을 도해한다. 상기 차량(402)을 제어하도록 구성되는 컴퓨팅 디바이스는, 공사 구역의 검출에 관한 정보를 수신하도록 구성된다. 상기 정보는 복수의 정보원들로부터 수신된다. 예를 들어, 상기 정보는 상기 컴퓨팅 디바이스에 결합되는 이미지-캡쳐 디바이스 또는 카메라(예를 들어, 도 1의 카메라(134), 또는 도 2의 카메라(210))로부터 수신되는 이미지-기반 정보를 포함할 수 있다. 일 예시에서, 상기 이미지-캡쳐 디바이스는 상기 차량(402) 상에 있을 수 있다. 하지만, 다른 예시에서, 상기 이미지-캡쳐 디바이스는 차량 밖에 있을 수 있다(예를 들어, 교통 신호 후단에 결합된 주어진 카메라). 상기 이미지-기반 정보는, 예를 들어, 공사 구역 콘(들)(406), 공사 구역 배럴(들)(408), 공사 장비(410a-b), 공사 구역 표지판들(412a-412b) 등과 같은 도로에 관한 하나 이상의 정적인 물체들의 위치를 표시한다. 상기 공사 구역 콘(들)(406)은 본 명세서에서 이후에 단일 콘 또는 콘들의 그룹/시리즈를 언급하도록 사용된다. 상기 이미지-기반 정보는 또한 주황색 조끼들 및 급커브들(chevrons)과 같은 공사 구역들에 관련된 다른 물체들을 표시할 수 있다. 상기 이미지-기반 정보는 또한 도로의 지형(예를 들어, 커브들, 차로들 등)을 표시할 수 있다.FIG. 4 illustrates a
다른 예시에서, 공사 구역의 검출에 관한 정보는 상기 차량(402)에 결합되고 상기 컴퓨팅 디바이스와 통신하는 LIDAR 센서(예를 들어, 도 1의 LIDAR 유닛(132))로부터 수신되는 LIDAR-기반 정보를 포함할 수 있다. 상기 LIDAR 센서는 도로(404) 및 도로(404)의 부근의 3차원(3D) 포인트 클라우드를 제공하도록 구성된다. 상기 컴퓨팅 디바이스는 예를 들어, 3D 포인트 클라우드에서의 포인트들의 세트들에 의해 나타나는 물체들(예를 들어, 공사 구역 콘(들)(406), 공사 구역 배럴(들)(408), 공사 장비(410a-b), 공사 구역 표지판들(412a-412b) 등)을 식별하도록 구성된다.In another example, information regarding the detection of a work area includes LIDAR-based information received from a LIDAR sensor (e.g., LIDAR unit 132 of FIG. 1) coupled to the
또 다른 예시에서, 상기 정보는 상기 차량(402)에 결합되고 상기 컴퓨팅 디바이스와 통신하는 RADAR 센서(예를 들어, 도 1의 RADAR 유닛(130))로부터 수신되는 RADAR-기반 정보를 포함한다. 예를 들어, RADAR 센서는 라디오파들을 방출하고 상기 도로(404)상의 또는 상기 도로(404)의 부근의 물체들에 튕겨 나온 방출된 라디오파들을 다시 수신하도록 구성된다. 상기 특징들은, 예를 들어, 상기 물체의 차원 특징들, 상기 물체와 상기 차량(402) 간의 거리 및 상기 물체가 정적인지 또는 이동중인지, 이에 더하여 움직임의 속도 및 방향을 포함한다.In another example, the information includes RADAR-based information received from a RADAR sensor coupled to the
또 다른 예시에서, 상기 정보는 교통 정보를 포함한다. 상기 교통 정보는 도로(404) 상의 차량들(414a-414b)과 같은, 다른 차량들의 거동을 표시한다. 일 예시로서, 상기 교통 정보는 상기 차량들(414a-414b)에 결합되는 글로벌 포지셔닝 위성(GPS) 디바이스들로부터 수신된다. 각각의 GPS 디바이스로부터 수신되는 GPS 정보는 위성-기반 포지셔닝 데이터에 기반해서, 각각의 GPS 디바이스를 포함하는 각각의 차량의 지구에 대한 위치를 표시한다.In another example, the information includes traffic information. The traffic information indicates the behavior of other vehicles, such as vehicles 414a-414b on
또 다른 예시에서, 차량들(414a-414b)은 도로 인프라 디바이스(예를 들어, 도로(404)의 포스트(post)상의 디바이스)와 위치/포지션 및 속도 정보를 통신하도록 구성되고, 그리고 상기 인프라 디바이스는 그러한 교통 정보를 컴퓨팅 디바이스와 통신할 수 있다. 이 통신은 차량-대-인프라 통신으로 언급될 수 있다. 차량-대-인프라 통신은, 넓은 범위의 안전, 이동성, 도로 상황, 교통, 및 환경 정보를 인에이블하도록 의도되는, 차량들(예를 들어, 차량(402) 및 차량들(414a-414b))과 도로 인프라 간의 필수 안전 및 동작 데이터의 무선 교환을 포함한다. 차량-대-인프라 통신은 모든 차량 유형들 및 모든 도로들에 적용될 수 있고, 예를 들어, 차량(402)에 결합되는 컴퓨팅 디바이스에 의해, 계산들을 수행하기 위해 차량들과 인프라 요소들 간에 교환된 데이터를 이용하는 알고리즘들의 통합을 통해서 인프라 장비를 "스마트 인프라"로 변환할 수 있고, 이는 사전에 높은 위험의 상황들을 인식하고, 구체적인 대책을 통해 운전자 경보 및 경고들의 결과를 낳는다. 일 예시로서, 도로(404) 상의 교통 신호 시스템들은 차량(402) 또는 차량(402)의 운전자에게 활성 교통 정보, 안전 경보들, 및 경고들을 전달하기 위해 상기 차량(402)과 신호 위상 및 시간(SPAT) 정보를 통신하도록 구성된다.In another example, the vehicles 414a-414b are configured to communicate position / position and velocity information with a road infrastructure device (e.g., a device on the post of the road 404) May communicate such traffic information with the computing device. This communication can be referred to as vehicle-to-infrastructure communication. Vehicle-to-infrastructure communications are used to communicate with vehicles (e.g.,
또 다른 예시에서, 상기 교통 정보는 직접적인 차량-대-차량 통신으로부터 수신될 수 있다. 이 예시에서, 차량(402 및 414a-414b)의 소유자들에게 차량들간의 정보를 공유하는 것을 옵트 인 또는 옵트 아웃할 옵션이 주어질 수 있다. 도로(404) 상의 차량들(414a-414b)은 차량(402)을 제어하도록 구성되는 컴퓨팅 디바이스에 추적 가능한 신호들을 제공하도록 구성되는 디바이스들(예를 들어, 차량들(414a-414b)에 결합되는 GPS 디바이스들 또는 차량들(414a-414b)의 운전자들에 의해 사용되는 모바일 폰들)을 포함한다. 상기 컴퓨팅 디바이스는, 예를 들어, 상기 추적 가능한 신호들을 수신하고 차량들(414a-414b)의 교통 정보 및 거동 정보를 추출하도록 구성된다.In another example, the traffic information may be received from a direct vehicle-to-vehicle communication. In this example, the owners of
또 다른 예시에서, 상기 교통 정보는 교통 안내 방송(예를 들어, 라디오 교통 서비스들)으로부터 수신될 수 있다. 또 다른 예시에서, 컴퓨팅 디바이스는 차량(402)을 제어하도록 구성되는 컴퓨팅 디바이스와 통신하는 차량 내 또는 차량 밖의 센서들로부터 교통 정보를 수신하도록 구성된다. 일 예시로서, 레이저-기반 센서들은 고속도로의 차로들을 통해 통과하는 차량들의 속도 통계들을 제공하고, 상기 컴퓨팅 디바이스와 그러한 정보를 통신할 수 있다.In another example, the traffic information may be received from a traffic announcement (e.g., radio traffic services). In another example, the computing device is configured to receive traffic information from sensors in or out of the vehicle communicating with a computing device that is configured to control the
상기 교통 정보에 기반해서, 상기 컴퓨팅 디바이스는 도로(404) 상의, 차량들(414a-414b)과 같은, 다른 차량들의 정격 속도 및 교통 흐름을 추정하도록 구성된다. 일 예시에서, 컴퓨팅 디바이스는 상기 교통 정보에 기반해서 차량들(414a-414b)의 정격 속도 및 교통 흐름의 변화를 결정하고, 차량들(414a-414b)의 거동의 변화를 주어진 공사 구역에 접근하는 것과 관련된 교통 변화들의 미리 결정된 또는 전형적인 패턴과 비교하도록 구성된다.Based on the traffic information, the computing device is configured to estimate the rated speed and traffic flow of other vehicles, such as vehicles 414a-414b, on the roadway. In one example, the computing device determines a change in traffic velocity and the rated speed of the vehicles 414a-414b based on the traffic information, and changes the behavior of the vehicles 414a-414b to a given construction area To a predetermined or exemplary pattern of traffic changes associated with the traffic.
또 다른 예시에서, 공사 구역의 검출에 관련된 정보는 기존의 지도에 관련된 지도 정보를 포함한다. 예를 들어, 지도 정보는 교통 표지판(416a-416b), 도로(404) 상의 다수의 차로들, 차로 경계들의 위치들 등에 관련된 정보를 포함한다. 기존의 지도들은 수동적으로 또는 존재하는 표지판들의 전자적 검출을 통해 존재하는 표지판들로 덧붙여진다. 하지만, 상기 지도 정보는 도로 차로들에 변화를 줄 수 있는 일시적인 도로 작업으로 인한 최근의 도로 변화들에 관한 정보를 포함하지 않을 수 있다. 예를 들어, 상기 지도 정보는 공사 구역 표지판들(412a-412b)과 같은 일시적인 공사 구역 표지판들에 관한 각각의 정보를 포함하지 않을 수 있다.In another example, the information related to the detection of the work area includes map information related to the existing map. For example, the map information includes information related to traffic signs 416a-416b, multiple lanes on
추가적으로, 복수의 정보원들 중 정보원 각각에 각각의 신뢰성 메트릭이 할당된다. 상기 신뢰성 메트릭은 그 정보원으로부터 수신된 각각의 정보에 기반해서 공사 구역의 검출의 신뢰의 레벨을 표시한다. 도해의 예시로서, 상기 교통 정보는 RADAR-기반 정보보다는 공사 구역을 검출하는 데 있어서 더 신뢰할 수 있을 것이다. 다시 말해서, 상기 컴퓨팅 디바이스는, 상기 교통 정보에 기반해서, RADAR-기반 정보에 기반해서 공사 구역을 검출하는 각각의 신뢰의 레벨보다 높은 신뢰의 레벨로 공사 구역의 존재를 검출하도록 구성된다. 이 예시에서, 교통 정보원에는 RADAR-기반 정보원일 수 있는, RADAR 유닛보다 더 높은 신뢰성 메트릭이 할당된다. 예시들에서, 상기 신뢰성 메트릭은 복수의 운전 상황들로부터 이전에 수집된 데이터에 기반해서 결정될 수 있다.Additionally, each of the plurality of sources is assigned a respective reliability metric. The reliability metric indicates a level of confidence in the detection of the work area based on each piece of information received from the information source. As an illustration of the illustration, the traffic information may be more reliable in detecting a work area than RADAR-based information. In other words, the computing device is configured to, based on the traffic information, detect the presence of the work area at a level of confidence that is greater than the level of each trust that detects the work area based on the RADAR-based information. In this example, the traffic information source is assigned a higher reliability metric than the RADAR unit, which may be a RADAR-based information source. In the examples, the reliability metric may be determined based on data previously collected from a plurality of driving situations.
도 3을 다시 참조하면, 블럭(304)에서, 방법(300)은 컴퓨팅 디바이스를 이용해서, 정보 및 복수의 정보원들의 각각의 신뢰성 메트릭들에 기반해서, 도로상의 공사 구역의 존재의 가능성을 결정하는 것을 포함한다. 일 예시로서, 도 4에서, 공사 구역의 검출에 관련되고, 차량(402)을 제어하도록 구성되는 컴퓨팅 디바이스에서 복수의 정보원들로부터 수신되는 정보에 기반해서, 컴퓨팅 디바이스는 도로(404)상의 공사 구역의 존재의 가능성을 결정하도록 구성될 수 있다.Referring again to FIG. 3, at
일 예시에서, 컴퓨팅 디바이스는 이미지-캡쳐 디바이스로부터 수신되는 이미지-기반 정보로부터, 공사 구역으로 인한 도로의 지형의 변화를 결정하도록 구성되고, 그리고 컴퓨팅 디바이스는 상기 결정된 변화에 기반해서 상기 가능성을 할당한다. 예를 들어, 컴퓨팅 디바이스는 결정된 변화를 전형적인 공사 구역과 관련된 전형적인 변화와 비교하고, 상기 비교에 기반해서 상기 가능성을 결정하도록 구성된다. 다른 예시로서, 컴퓨팅 디바이스는 본 기술 분야에서 알려진 이미지 인식 기법들을 이용해서, 상기 이미지-캡쳐 디바이스에 의해 캡쳐된 이미지들에 묘사된 공사 구역 물체들(예를 들어, 공사 구역 콘(들)(406), 공사 구역 배럴(들)(408), 공사 구역 표지판들(412a-412b), 공사 장비(410a-b), 또는 어떤 다른 공사 지표들)을 식별하도록 구성된다. 일 예시에서, 컴퓨팅 디바이스는 공사 구역 물체들을 식별하는 것과 관련된 신뢰의 레벨을 표시하는 각각의 식별의 가능성을 할당하고, 상기 공사 구역 물체들의 각각의 식별의 가능성들에 기반해서 상기 공사 구역의 존재의 가능성을 결정하도록 구성된다.In one example, the computing device is configured to determine, from image-based information received from the image-capturing device, a change in terrain of the road due to the work area, and the computing device allocates the possibility based on the determined change . For example, a computing device is configured to compare a determined change to a typical change associated with a typical construction area, and to determine the likelihood based on the comparison. As another example, a computing device may utilize image recognition techniques known in the art to determine the location of objects (e.g., construction site cones (s) 406 ), Construction area barrel (s) 408, construction area signs 412a-412b, construction equipment 410a-b, or any other construction indicators. In one example, the computing device allocates the possibility of each identification indicative of the level of trust associated with identifying objects in the work area and determines the presence of the presence of the work area based on the likelihoods of each of the objects in the work area Probability.
유사하게, 상기 컴퓨팅 디바이스는 LIDAR-기반 및/또는 RADAR-기반 정보에 기반해서 공사 구역 물체들을 식별하도록 구성된다. 일 예시로서, 컴퓨팅 디바이스는 LIDAR 센서에 의해 제공되는 3D 포인트 클라우드의 포인트들의 세트에 의해 나타내진 후보 공사 구역 물체(후보 공사 구역 콘, 배럴, 또는 표지판)를 식별하도록 구성된다. 그리고 상기 컴퓨팅 디바이스는 상기 물체의 식별에 대한 각각의 신뢰의 레벨에 기반해서 상기 후보 공사 구역 물체에 대한 각각의 식별의 가능성을 할당하도록 구성된다.Similarly, the computing device is configured to identify work area objects based on LIDAR-based and / or RADAR-based information. As an example, the computing device is configured to identify a candidate construction zone object (candidate construction zone cone, barrel, or sign) represented by a set of points in the 3D point cloud provided by the LIDAR sensor. And the computing device is configured to assign a probability of each identification to the candidate construction zone object based on a level of each confidence in the identity of the object.
일 예시에서, 상기 컴퓨팅 디바이스는 후보 공사 구역 물체(이미지-기반 정보, LIDAR-기반 정보, 또는 RADAR-기반 정보에서 식별됨)의 모양을 전형적인 공사 구역 물체들의 하나 이상의 미리 결정된 모양들과 비교하도록 구성된다. 그리고, 또한 상기 컴퓨팅 디바이스는 상기 후보 공사 구역 물체가 주어진 미리 결정된 모양(예를 들어, 상기 후보 물체의 모양과 상기 주어진 미리 결정된 모양의 차원 특징들간의 일치의 퍼센트)과 얼마나 유사한지를 표시하는 매칭도를 결정하도록 구성된다. 상기 컴퓨팅 디바이스는 또한 공사 구역 물체들을 검출 및/또는 식별하기 위해 도 9에 관해 아래에 기술된 기법들 중 어떤 것이든 사용하도록 구성될 수 있다.In one example, the computing device is configured to compare the shape of a candidate construction zone object (identified in image-based information, LIDAR-based information, or RADAR-based information) with one or more predetermined shapes of typical construction zone objects do. And, the computing device is also a matching device that indicates how similar the candidate construction space object is to a given predetermined shape (e.g., a percentage of the match between the shape of the candidate object and the dimension features of the given shape) . The computing device may also be configured to use any of the techniques described below with respect to FIG. 9 to detect and / or identify workspace objects.
일 예시에서, 상기 컴퓨팅 디바이스는 상기 도로(404)와 관련된 지도 정보를 수신하도록 구성되고, 그리고 상기 지도 정보는 도로(404)상에 존재하는 표지판들(예를 들어, 표지판들(416a-416b))의 위치들 및 유형들을 포함할 수 있다. 상기 컴퓨팅 디바이스는, 지도 정보로부터 누락되어 있을 수 있는, 후보 공사 구역 표지판(예를 들어, 공사 구역 표지판들(412a-412b) 중 하나 또는 둘 모두)의 존재를 결정하도록 더 구성될 수 있다. 일 예시에서, 상기 지도 정보로부터 누락되어 있는 상기 후보 공사 구역 표지판은 상기 후보 공사 구역 표지판의 일시성을 표시할 수 있고, 그러므로 상기 후보 공사 구역 표지판은 공사 구역 표지판일 가능성이 높다는 것을 표시할 수 있다. 이에 따라서, 상기 컴퓨팅 디바이스는 상기 후보 공사 구역 표지판이 공사 구역과 관련될 각각의 가능성을 할당한다. 일 예시에서, 상기 컴퓨팅 디바이스는 상기 도로(404)상에 주어진 공사 구역의 존재의 주어진 가능성이 있음을 도로(404)상의 다른 차량들 또는 운전자들로 하여금 주의하게 하기 위해서, 상기 후보 공사 구역 표지판과 관련된 각각의 표지판 정보 및 상기 도로상의 공사 구역의 존재의 가능성을 포함하기 위해 상기 지도 정보를 업데이트 하도록 구성될 수 있다. 상기 컴퓨팅 디바이스는 또한 공사 구역 표지판들을 검출 및/또는 식별하기 위해 도 5 및 도 7에 관해 아래에 기술되는 기법들 중 어떤 것이든 사용하도록 구성될 수 있다.In one example, the computing device is configured to receive map information associated with the
또 다른 예시에서, 상기 컴퓨팅 디바이스는 도로(404)상의 다른 차량들(414a-414b)의 거동을 표시하는 교통 정보를 수신하도록 구성될 수 있다. 이 예시에서, 상기 가능성을 결정하기 위해, 상기 컴퓨팅 디바이스는 상기 교통 정보에 기반해서 정격 속도 및 다른 차량들(414a-414b)의 교통 흐름의 변화를 결정하도록 구성될 수 있다. 상기 컴퓨팅 디바이스는 다른 차량들(414a-414b)의 거동의 변화와 주어진 공사 구역에 접근하는 것과 관련된 교통 변화의 미리 결정된 또는 전형적인 패턴을 비교하도록 구성되고, 상기 컴퓨팅 디바이스는 상기 비교에 기반해서 상기 가능성을 결정하도록 구성된다. 일 예시에서, 상기 컴퓨팅 디바이스는, 상기 비교에 기반해서 상기 가능성을 결정함에 있어서, 사고 현장과 관련된 교통의 주어진 변화를 각각의 공사 구역에 접근하는 것과 관련된 교통의 각각의 변화로부터 구별하도록 구성될 수 있다. 예를 들어, 사고 현장은 혼잡 포인트에 의해 특징지어질 수 있고, 차량들은 상기 혼잡 포인트를 향해 느려지며 일단 상기 혼잡 포인트가 통과되면 가속된다. 대안적으로, 공사 구역들은 변화된 속도 및 교통 흐름의 보다 긴 도로 섹션에 의해 특징지어질 수 있다. 다른 예시에서, 상기 컴퓨팅 디바이스는 사고 방송 서비스로부터 수신된 사고 정보에 기반해서 사고 현장을 공사 구역으로부터 구별하도록 구성될 수 있다.In another example, the computing device may be configured to receive traffic information indicative of the behavior of other vehicles 414a-414b on
한 예시에서, 컴퓨팅 디바이스는 각각의 정보의 유형 또는 정보원(예를 들어, 이미지-기반 정보, LIDAR-기반 정보, RADAR-기반 정보, 지도 정보, 교통 정보)에 대해 공사 구역의 존재의 각각의 가능성을 할당하거나 결정하도록 구성될 수 있고, 상기 각각의 가능성의 조합에 기반해서 단일의 가능성을 결정(예를 들어, 상기 각각의 가능성들의 가중치가 부여된 조합)하도록 더 구성될 수 있다. 예를 들어, 복수의 정보원들 중 각각의 정보원에 할당되는 상기 각각의 가능성은 그 정보원에 할당된 신뢰성 메트릭에 기반할 수 있다. 또한, 한 예시에서, 상기 복수의 정보원들 중 한 정보원에 대해 결정된 각각의 가능성에 기반해서, 컴퓨팅 디바이스는 공사 구역의 존재를 확인하기 위해 다른 정보원으로부터 정보를 수신하기 위해서 차량(402)에 결합된 센서 또는 모듈을 인에이블하도록 구성될 수 있다.In one example, the computing device may determine the availability of each zone of the construction zone for each type of information or source (e.g., image-based information, LIDAR-based information, RADAR-based information, map information, traffic information) , And may be further configured to determine a single likelihood based on a combination of the respective probabilities (e.g., a weighted combination of the probabilities of the respective probabilities). For example, each of the possibilities assigned to each of the plurality of sources may be based on a reliability metric assigned to that source. Also, in one example, based on each probability determined for one of the plurality of sources, the computing device may be coupled to the
다른 예시에서, 공사 구역의 존재의 가능성을 결정하기 위해서, 컴퓨팅 디바이스는 복수의 정보원들로부터 수신된 공사 구역의 검출에 관한 정보 및 상기 복수의 정보원들에 할당된 각각의 신뢰성 메트릭들에 기반해서, 확률적 모델(예를 들어, 정규 분포(Gaussian distribution))을 발생시키도록 구성될 수 있다. 예를 들어, 공사 구역의 존재의 가능성은 복수의 정보원들로부터의 정보 및 각각의 신뢰성 메트릭들에 기반해서 결정되는 파라미터 값들의 세트의 함수로서 결정될 수 있다. 이 예시에서, 상기 가능성은 파라미터 값들이 주어졌을 때 관측된 결과(공사 구역의 존재)의 확률과 동일한 것으로서 정의될 수 있다. 통상의 기술자들은 상기 가능성의 함수는 이산 확률 분포, 연속 확률 분포와 혼합 연속-이산 분포 간의 구별을 수반하고, 로그 가능성(log likelihood), 상대적 가능성, 조건부 가능성, 마진 가능성(marginal likelihood), 프로파일 가능성(profile likelihood), 부분적 가능성과 같은 가능성의 몇몇의 유형들이 존재함을 이해할 것이다.In another example, to determine the likelihood of the presence of a work area, the computing device may determine, based on information about the detection of a work area received from a plurality of information sources and respective reliability metrics assigned to the plurality of information sources, May be configured to generate a stochastic model (e.g., a Gaussian distribution). For example, the likelihood of the presence of a workspace can be determined as a function of a set of parameter values that are determined based on information from a plurality of sources and respective reliability metrics. In this example, the likelihood can be defined as being equal to the probability of the observed result (presence of the work area) given the parameter values. The skilled artisan will appreciate that the function of the likelihood involves the distinction between a discrete probability distribution, a continuous probability distribution, and a mixed continuous-discrete distribution, and can be expressed as log likelihood, relative likelihood, conditional probability, marginal likelihood, and that there are several types of possibilities, such as profile likelihood, and partial likelihood.
또 다른 예시에서, 컴퓨팅 디바이스는 가능성을 결정하기 위해 분류기를 통해서 복수의 정보원들로부터의 정보 및 각각의 신뢰성 메트릭들을 프로세싱하도록 구성된다. 상기 분류기는 입력 정보(예를 들어, 공사 구역의 검출에 관한 정보 및 각각의 신뢰성 메트릭들)를 클래스(예를 들어, 공사 구역의 존재)에 매핑하는 분류 알고리즘에 의해 구현되는 알고리즘 또는 수학적 함수로서 정의될 수 있다.In another example, a computing device is configured to process information and a plurality of reliability metrics from a plurality of sources via a classifier to determine a probability. The classifier may be implemented as an algorithm or mathematical function implemented by a classification algorithm that maps input information (e.g., information about the detection of a construction zone and respective reliability metrics) to a class (e.g., presence of a construction zone) Can be defined.
분류는 알려진 클래스를 갖는 관측들(또는 인스턴스들(instances))을 포함하는 데이터의 훈련 세트에 기초해서, 새 관측이 클래스들의 어떤 세트(예를 들어, 공사 구역의 존재 또는 부존재)에 속하는지를 식별하는 것을 수반한다. 개별적인 관측들은, 다양한 예시적인 변수들 또는 특징들로 알려진, 수량화할 수 있는 특징들의 세트로 분석될 수 있다. 일 예시로서, 공사 구역의 검출에 관한 수신된 정보(예를 들어, 이미지-기반 정보, LIDAR-기반 정보, RADAR-기반 정보, 지도 정도, 교통 정보 등)에 의해 표시된 것과 같이, 분류는 각각의 가능성을 "공사 구역의 존재" 또는 "공사 구역의 부존재"에 할당하는 것을 포함할 수 있다.The classification identifies whether the new observation belongs to any set of classes (e.g., presence or absence of a construction zone), based on a training set of data comprising observations (or instances) having known classes Lt; / RTI > Individual observations can be analyzed with a set of quantifiable features known as various exemplary parameters or characteristics. As an example, as indicated by the received information (e.g., image-based information, LIDAR-based information, RADAR-based information, map accuracy, traffic information, etc.) May include assigning the possibility to "presence of a construction zone" or "absence of a construction zone ".
일 예시에서, 상기 분류는 확률적 분류를 포함할 수 있다. 확률적 분류 알고리즘은 가능한 클래스들인 "공사 구역의 존재" 또는 "공사 구역의 부존재" 각각의 멤버인 인스턴스(예를 들어, 공사 구역의 검출에 관한 수신된 정보에 의해 표시되는 운전 상황 또는 관측들의 그룹)의 확률을 출력한다. 공사 구역의 존재의 가능성을 결정하는 것은 클래스 각각에 할당된 확률에 기반할 수 있다. 또한, 상기 확률적 분류는 공사 구역의 존재와 관련된 신뢰성 메트릭값을 출력할 수 있다.In one example, the classification may comprise a stochastic classification. The probabilistic classification algorithm is based on an instance that is a member of each of the possible classes "presence of construction zone" or "absence of construction zone" (eg, a group of operating conditions or observations represented by received information about the detection of a construction zone ). ≪ / RTI > Determining the likelihood of existence of a construction zone may be based on the probabilities assigned to each class. The probabilistic classification may also output a reliability metric value associated with the presence of the work area.
예시적인 분류 알고리즘은 선형 분류기(예를 들어, 피셔(Fisher)의 선형 판별식, 로지스틱 회귀(logistic regression), 단순 베이즈(naive Bayes) 및 퍼셉트론(perceptron)), 이차 분류기, 커널 추정(예를 들어, k-근접 이웃), 부스팅, 디시젼 트리들(예를 들어, 랜덤 포레스트들), 뉴럴 네트워크들, 유전자 발현 프로그래밍, 베이즈 네트워크들, 히든 마르코프 모델들 및 벡터 양자화 학습을 포함한다.Exemplary classification algorithms include linear classifiers (e.g., Fisher's linear discriminant, logistic regression, naive Bayes and perceptron), secondary classifiers, kernel estimators Neural networks, gene expression programming, Bayes networks, Hidden Markov models, and vector quantization learning, for example.
도해를 위한 일 예시로서, 선형 분류기는 내적을 이용해서, 인스턴스(예를 들어, 운전 상황)의 특징 벡터(공사 구역의 검출과 관련되고 복수의 정보원들로부터 수신된 정보, 그리고 각각의 신뢰성 메트릭들과 관련된 파라미터들의 벡터)를 가중치들의 벡터와 조합함에 의해 각각의 가능한 클래스 k(예를 들어, "공사 구역의 존재" 또는 "공사 구역의 부존재")에 스코어 또는 가능성을 할당하는 선형 함수로서 표현될 수 있다. 더 높은 스코어 또는 가능성을 갖는 클래스는 예측 클래스로서 선택될 수 있다. 이 유형의 스코어 함수는 선형 예측기 함수로 알려져 있고 이러한 일반적 형태를 갖는다.As an example for illustration, the linear classifier uses an inner product to generate a feature vector of an instance (e.g., a driving situation) associated with the detection of a work area and received from a plurality of sources and respective reliability metrics By assigning a score or probability to each possible class k (e.g., "presence of construction zone" or "absence of construction zone") by combining it with the vector of weights . A class with a higher score or probability may be selected as a prediction class. This type of score function is known as a linear predictor function and has this general form.
여기서 Xi는 인스턴스 i에 대한 특징 벡터이고, βk는 카테고리 k에 대응하는 가중치들의 벡터이고, score(Xi,k)는 인스턴스 i를 카테고리 k에 할당하는 것과 관련된 스코어이다.Where Xi is a feature vector for instance i, beta k is a vector of weights corresponding to category k, and score (Xi, k) is a score associated with assigning instance i to category k.
하나의 예시로서, 훈련 컴퓨팅 디바이스는 주어진 차량의 복수의 운전 상황들에 대한 훈련 데이터를 수신하도록 구성될 수 있다. 예를 들어, 복수의 운전 상황들 각각에 대해서, 각각의 훈련 데이터는 각각의 이미지-기반 정보, 각각의 LIDAR-기반 정보, 각각의 RADAR-기반 정보, 각각의 교통 정보, 각각의 지도 정보를 포함할 수 있다. 또한, 상기 훈련 컴퓨팅 디바이스는 운전 상황들 각각에 대한 각각의 훈련 데이터에 대응하는 각각의 공사 구역의 존재의 긍정적인 또는 부정적인 표시를 수신하도록 구성될 수 있다. 더욱이 상기 훈련 컴퓨팅 디바이스는 각각의 운전 상황에 대해, 상기 긍정적인 또는 부정적인 표시를 각각의 훈련 데이터와 상관시키고, 상기 복수의 운전 상황들에 대한 상관도들에 기반해서 분류기의 파라미터들(예를 들어, 방정식 (1)의 가중치들의 벡터)을 결정하도록 구성될 수 있다. 더욱이, 일 예시에서, 상기 훈련 컴퓨팅 디바이스는 상기 상관도에 기반해서 각각의 정보원에 대한 각각의 신뢰성 메트릭을 결정하도록 구성될 수 있다. 복수의 정보원들의 상기 파라미터들 및 각각의 신뢰성 메트릭들은, 상기 컴퓨팅 디바이스가 복수의 정보원들로부터 공사 구역의 검출에 관한 정보를 수신함에 따라, 상기 컴퓨팅 디바이스는 상기 가능성을 결정하기 위해 상기 분류기의 결정된 파라미터들을 이용해서 상기 분류기를 통해 상기 정보를 프로세싱하도록 구성되게끔 상기 차량(402)을 제어하도록 구성되는 컴퓨팅 디바이스에 제공될 수 있다.As one example, a training computing device may be configured to receive training data for a plurality of driving situations of a given vehicle. For example, for each of a plurality of driving situations, each training data includes respective image-based information, respective LIDAR-based information, respective RADAR-based information, respective traffic information, and respective map information can do. In addition, the training computing device may be configured to receive a positive or negative indication of the presence of each construction zone corresponding to each training data for each of the driving situations. Further, the training computing device may be configured to correlate the positive or negative indication with respective training data for each driving situation, and to determine parameters of the classifier based on the correlations for the plurality of driving situations , A vector of weights of the equation (1)). Moreover, in one example, the training computing device may be configured to determine a respective reliability metric for each information source based on the correlation. Wherein the parameters of the plurality of sources and respective reliability metrics are determined such that as the computing device receives information regarding the detection of a work area from a plurality of sources, To control the vehicle (402) so as to be configured to process the information via the classifier using a computer-readable medium.
일 예시에서, 상기 가능성은 "낮은," "중간의," 또는 "높은"과 같이 질적일 수 있거나, 예를 들어, 척도상의 숫자와 같이 수량적일 수 있다. 기타 예시들도 가능하다.In one example, the likelihood may be qualitative, such as "low," " intermediate, "or" high, " Other examples are possible.
도 3을 다시 참조하면, 블럭(306)에서, 방법(300)은 상기 가능성에 기반해서, 상기 컴퓨팅 디바이스를 이용해서 상기 차량의 운전 거동과 관련된 제어 전략을 수정하는 것을 포함한다.Referring again to FIG. 3, at
차량의 제어 시스템은 다수의 제어 전략들, 그리고 미리 결정되거나 상기 차량의 운전 환경에서의 변화들에 적응적인 관련된 운전 거동들을 지원한다. 일반적으로, 제어 전략은, 공사 구역에 접근하는 것과 같은 다양한 운전 컨텍스트들에서의 교통 인터렉션과 관련된 규칙들의 세트들을 포함한다. 상기 제어 전략은 안전 및 교통 규칙들 및 관련사항들을 고려하는 한편, 상기 차량의 속도 및 상기 차량이 운행하는 차로를 결정하는 규칙들을 포함한다(예를 들어, 공사 구역의 존재로 인한 도로의 지형의 변화들, 교차로에 정지해 있는 차량들 및 양보 상황에서의 기회의 창, 차로 추적, 도로상의 다른 차량들로부터의 거리, 다른 차량들을 통과하는 것, 그리고 가다 서다를 반복하는 교통에서의 대기, 그리고 차량이 다가오는 차로들과 같은 안전하지 않은 거동을 낳을 수 있는 영역들을 피하는 것 등). 예를 들어, 공사 구역에 접근하는데 있어서, 상기 컴퓨팅 디바이스는, 공사 구역의 존재의 결정된 가능성에 기반해서, 다른 물체들과의 거리를 안전하게 유지하기 위해 차량 속도를 제어하고 공사 구역의 존재로 인한 주어진 도로 변화들에서 가장 안전하다고 고려되는 차로를 선택하는 거동들에 대한 규칙들을 포함하는 제어 전략을 수정하거나 선택하도록 구성된다.The control system of the vehicle supports a number of control strategies and associated driving behaviors that are predetermined or adaptable to changes in the driving environment of the vehicle. In general, the control strategy includes sets of rules associated with traffic interactions in various operating contexts, such as approaching a work area. The control strategy includes safety and traffic rules and related considerations, as well as rules for determining the speed of the vehicle and the lane on which the vehicle travels (e.g., Changes in vehicles at the intersection, window of opportunity in the concession situation, tracking by car, distance from other vehicles on the road, passing through other vehicles, Avoiding areas where the vehicle could lead to unsafe behavior such as coming lanes, etc.). For example, in accessing a work area, the computing device may control the vehicle speed to maintain the distance to other objects securely based on the determined probability of the presence of the work area, And to modify or select a control strategy that includes rules for behaviors that select a lane that is considered to be the safest in road changes.
일 예시로서, 도 4에서, 만약 공사 구역의 존재의 가능성이 높으면(예를 들어, 미리 결정된 임계치를 초과함), 컴퓨팅 디바이스는, 내비게이션 결정을 함에 있어서, 공사 구역에 관련된 정보 및 변화들을 포함하지 않을 수 있는 기존의 지도 정보보다는, 상기 컴퓨팅 디바이스와 통신하는 차량(402) 내 센서들 또는 차량 밖 센서들로부터 수신되는 센서 정보를 활용하도록 구성될 수 있다. 또한, 상기 컴퓨팅 디바이스는, 차로 경계들을 추정하기 위해 기존의 지도 정보보다는 센서 정보를 활용하도록 구성될 수 있다. 예를 들어, 도 4를 참조하면, 상기 컴퓨팅 디바이스는, 차로 경계들을 추정하고 따르기 위해서, 상기 도로(404)상의 차로 마커들(418)보다는, 공사 구역 마커들(예를 들어, 공사 구역 콘(들)(406))의 위치를 결정하도록 구성된다. 다른 예시로서, 컴퓨팅 디바이스는 공사장 인부들(420)의 검출을 위해 하나 이상의 센서들을 활성화하고 상기 검출에 기반해서 내비게이션 결정을 하도록 구성될 수 있다.In one example, in FIG. 4, if the likelihood of the presence of a work area is high (e.g., exceeds a predetermined threshold), the computing device may include information and changes related to the work area in making navigation decisions And may be configured to utilize sensor information received from sensors in
일 예시에서, 제1 제어 전략은 기본 운전 거동을 포함하고 제2 제어 전략은 방어적인 운전 거동을 포함한다. 방어적인 운전 거동의 특징들은, 예를 들어, 차량들(414a-414b) 중 한 차량을 따라가는 것, 기본 운전 거동에서 유지되었던 거리보다 긴 미리 결정된 안전 거리를 상기 차량들(414a-414b)과 유지하는 것, 라이트들을 켜는 것, 차량(402)의 속도를 줄이는 것, 그리고 차량(402)을 멈추는 것을 포함한다. 이 예시에서, 상기 차량(402)의 컴퓨팅 디바이스는 결정된 가능성을 임계 가능성과 비교하도록 구성되고, 상기 컴퓨팅 디바이스는 상기 비교에 기반해서, 상기 제1 또는 상기 제2 제어 전략을 선택하도록 구성된다. 예를 들어, 만약 결정된 가능성이 임계 가능성보다 높다면, 상기 컴퓨팅 디바이스는 제2 운전 거동(예를 들어, 방어적인 운전 거동)을 선택하도록 구성된다. 만약 결정된 가능성이 임계 가능성보다 낮다면, 상기 컴퓨팅 디바이스는 상기 제어 전략을 제1 제어 전략(예를 들어, 기본 운전 거동을 선택)으로 수정하도록 구성된다.In one example, the first control strategy includes a basic driving behavior and the second control strategy includes a defensive driving behavior. The characteristics of the defensive driving behavior include, for example, following one of the vehicles 414a-414b, maintaining a predetermined safety distance longer than the distance maintained in the basic driving behavior with the vehicles 414a-414b , Turning on lights, reducing the speed of the
또 다른 예시에서, 이산적인 제어 전략들(예를 들어, 제1 제어 전략 및 제2 제어 전략)간의 천이에 대안적으로 또는 상기 천이에 추가적으로, 상기 컴퓨팅 디바이스는 결정된 가능성에 기반해서 운전 모드들 또는 상태들의 연속체로부터 선택하도록 구성된다. 또 다른 예시에서, 컴퓨팅 디바이스는 이산적인 제어 전략을 선택하도록 구성되고, 또한 상기 선택된 이산적인 제어 전략 내의 운전 모드들의 연속체로부터 운전 모드를 선택하도록 구성된다. 이 예시에서, 주어진 제어 전략은 운전 규칙들의 다수의 세트들을 포함하고, 여기서 운전 규칙들의 한 세트는 차량(402)의 속도 및 방향의 제어에 대한 거동들을 기술한다. 컴퓨팅 디바이스는, 상기 결정된 가능성에 기반해서, 운전 규칙들의 다수의 세트들 중 운전 규칙들의 주어진 세트로부터 운전 규칙들의 다른 세트로 부드러운 천이를 초래하도록 구성된다. 부드러운 천이는, 예를 들어, 차량(402)의 승객에 의해 차량(402)의 속도 또는 방향에 있어서 급격한 변화로서 여겨지지 않는 규칙들의 주어진 세트로부터 다른 세트로의 천이를 표시한다.In another example, alternatively or in addition to transitions between discrete control strategies (e. G., A first control strategy and a second control strategy), the computing device may determine whether the operating modes From the continuum of states. In another example, the computing device is configured to select a discrete control strategy, and is also configured to select an operation mode from a continuum of operation modes within the selected discrete control strategy. In this example, a given control strategy includes a plurality of sets of driving rules, wherein one set of driving rules describe behaviors for control of the speed and direction of the
일 예시에서, 주어진 제어 전략은 상기 결정된 가능성에 기반해서 차량(402)(예를 들어, 드로틀, 조타기, 브레이크, 액셀러레이터, 또는 전송 시프터)을 제어하는 작동기를 특징으로 하는 프로그램 또는 컴퓨터 명령들을 포함한다. 상기 주어진 제어 전략은 우선순위에 의해 순위가 매겨진 동작 세트들을 포함하고, 상기 동작 세트들은 차량(402)이 임무(예를 들어, 한 위치로부터 다른 위치로 운전함)을 달성하기 위해 취할 수 있는 대안적인 동작들을 포함한다. 상기 대안적인 동작들은 예를 들어, 상기 결정된 가능성에 기반해서 순위가 매겨질 수 있다. 또한, 상기 컴퓨팅 디바이스는, 수행될, 그리고 선택적으로, 상기 결정된 가능성에 기반해서 수정될 동작을 선택하도록 구성된다.In one example, a given control strategy includes program or computer instructions that characterize an actuator that controls the vehicle 402 (e.g., throttle, steering, brake, accelerator, or transmission shifter) based on the determined probability . The given control strategy includes sets of operations ranked by priority, and the sets of operations include alternatives that the
다른 예시에서, 다수의 제어 전략들(예를 들어, 프로그램들)은 컴퓨팅 디바이스에 동작들을 계속적으로 제안할 수 있다. 상기 컴퓨팅 디바이스는 예를 들어, 목표들(예를 들어, 안전성, 속도 등)의 가중치가 부여된 세트에 기반해서 어떤 전략이 선택될지를 결정하도록 구성되거나 제어 전략을 수정하도록 구성될 수 있다. 목표들의 가중치가 부여된 세트의 가중치들은 결정된 가능성의 함수일 수 있다. 목표들의 가중치가 부여된 세트의 계산에 기반해서, 상기 컴퓨팅 디바이스는, 예를 들어, 다수의 제어 전략들 및 각각의 동작 세트들의 순위를 매기고 상기 순위에 기반해서 주어진 전략 및 각각의 동작 세트를 선택하거나 수정하도록 구성될 수 있다.In another example, multiple control strategies (e.g., programs) may continuously suggest operations to the computing device. The computing device may be configured to determine which strategy is to be selected based on a weighted set of goals (e.g., safety, speed, etc.) or to modify the control strategy. The weights of the set of weighted sets of goals may be a function of the determined likelihood. Based on the computation of the weighted set of goals, the computing device may, for example, rank a plurality of control strategies and respective operation sets and select a given strategy and each operation set based on the ranking Or modify the information.
이러한 예시들 및 운전 상황들은 기술을 위한 것일 뿐이다. 기타 예시들, 제어 전략들 및 운전 거동들 또한 가능하다.These examples and operating situations are for technical purposes only. Other examples, control strategies and driving behavior are also possible.
도 3을 다시 참조하면, 블럭(308)에서, 방법(300)은 컴퓨팅 디바이스를 사용해서, 수정된 제어 전략에 기반해서 차량을 제어하는 것을 포함한다. 일 예시에서, 상기 컴퓨팅 디바이스는 상기 수정된 제어 전략과 관련된 동작 세트 또는 규칙 세트를 이용해서 상기 차량의 작동기들을 제어하도록 구성된다. 예를 들어, 상기 컴퓨팅 디바이스는 상기 수정된 운전 거동에 기반해서, 상기 차량의 병진 속도, 또는 회전 속도, 또는 둘 모두를 조정하도록 구성될 수 있다.Referring again to FIG. 3, at
일 예시로서, 도 4에서, 차량(402)을 제어하는 것은 상기 가능성에 기반해서, 상기 차량의 바람직한 경로를 결정하는 것을 포함한다. 일 예시에서, 상기 컴퓨팅 디바이스는 상기 차량(402)이 운행하는 도로(404)상에 공사 구역이 존재할 높은 가능성을 결정했을 수 있다. 이 예시에서, 상기 컴퓨팅 디바이스는 상기 바람직한 경로를 결정할 때, 부드러운 제약 조건(예를 들어, 만약 더 안전한 경로가 결정된다면 차로 경게가 위반될 수 있음)으로서 도로(404)상의 차로 마커들(418)에 의해 표시되는 차로 경계를 고려하도록 구성될 수 있다. 상기 컴퓨팅 디바이스는 그러므로 수정된 차로 경계를 형성하는 공사 구역 콘(들)(406)의 개수 및 위치들을 결정하도록 구성되고, 그리고 차로 마커들(418)에 의해 표시되는 차로 경계 대신에 수정된 차로 경계를 고수하도록 구성될 수 있다.As an example, in FIG. 4, controlling the
도 4에 도시된 것과 같이, 차량(402)은 도로(404)상의 공사 구역에 접근하고 있을 수 있고, 그리고 컴퓨팅 디바이스는 상기 공사 구역을 안전하게 내비게이션 하기 위해 방어적인 운전 거동에 따라 차량(402)을 제어하도록 구성될 수 있다. 예를 들어, 상기 컴퓨팅 디바이스는 차량(402)의 속도를 감소시키고, 상기 차량(402)으로 하여금 차로를 변경하고 상기 공사 구역 콘(들)(406)에 의해 형성된 수정된 차로 경계들 을 고수하고, 상기 차량(414a) 뒤의 위치로 이동하고, 그리고 미리 결정된 안전 거리를 유지한 채로 상기 차량(414a)을 따라가도록 구성될 수 있다.4,
일 예시에서, 상기 공사 구역의 존재의 가능성을 결정하는 것에 추가로, 상기 컴퓨팅 디바이스는 공사 구역의 존재로 인한 도로(404)에 대한 변화들의 심각도(severity)를 결정하거나 추정하도록 구성된다. 상기 컴퓨팅 디바이스는 변화들의 심각도에 더 기반해서 상기 제어 전략을 수정하도록 구성된다. 일 예시로서, 도 4에서, 상기 컴퓨팅 디바이스는, 공사 장비(410a-b)에 기반해서, 상기 공사 구역 콘(들)(406) 및 배럴(들)(480)의 개수 및 위치들 및 상기 도로(404)에 대한 변화들(예를 들어, 차로 폐쇄, 이동 등)이 얼마나 심각한지를 결정하고, 방어적인 운전 거동과 부합되게 차량(402)을 제어하도록 구성된다. 다른 예시에서, 상기 공사 구역은 도로(404)의 일 측면의 커브 차로를 페인팅하고 있는 인부를 포함할 수 있다. 이 예시에서, 도로(404)에 대한 변화들은 도 4에 묘사된 변화들보다 덜 심각할 수 있고, 상기 컴퓨팅 디바이스는 예를 들어, 차량(402)을 멈추거나 차량(402)으로 하여금 차로를 변경하게 하는 것이 아니라 차량(402)의 속도를 감소시키도록 구성될 수 있다.In one example, in addition to determining the likelihood of the presence of the work area, the computing device is configured to determine or estimate the severity of changes to the
이러한 제어 거동들 및 운전 상황들은 기술을 위한 것일 뿐이다. 다른 거동들 및 상황들 또한 가능하다. 일 예시에서, 컴퓨팅 디바이스는 운전자가 차량의 제어를 맡을 수 있을 때까지 임시 제어로서 수정된 제어 전략에 기반해서 상기 차량을 제어하도록 구성된다.These control behaviors and operating conditions are only for the technology. Other behaviors and situations are also possible. In one example, the computing device is configured to control the vehicle based on a modified control strategy as a temporary control until the driver can assume control of the vehicle.
도 3 및 4에 관해 위에 기술된 것과 같이, 컴퓨팅 디바이스는 공사 구역을 표시하는 공사 구역 표지판(예를 들어, 공사 구역 표지판(412a))의 식별 또는 검출에 기반해서 공사 구역의 존재의 가능성을 판단하도록 구성된다.As described above with respect to Figures 3 and 4, the computing device determines the likelihood of the presence of a construction zone based on the identification or detection of a construction zone sign indicating the construction zone (e.g., a construction zone sign 412a) .
도 5는 일 실시예에 부합하는, 공사 구역 표지판의 검출을 위한 방법(500)의 흐름도이다. 도 6a-6b는, 일 실시예에 부합하는, 차량이 운행하는 도로 및 도로의 부근의 이미지들을 기술하고, 도 6c-6d는, 일 실시예에 부합하는, 미리 결정된 높이 범위로 도로의 측면들을 묘사하는, 도로 및 도로의 부근의 이미지들의 부분들을 기술한다. 도 5 및 도 6a-6d는 함께 기술될 것이다.5 is a flow diagram of a
방법(500)은 하나 이상의 블럭들(502-512)에 의해 기술되는 것으로서, 하나 이상의 동작들, 기능들, 또는 동작들을 포함한다. 상기 블럭들이 순차적인 순서로 기술됨에도 불구하고, 이러한 블럭들은 몇몇의 예들에서 병렬로, 그리고/또는 본 명세서에 기술된 순서와 다른 순서로 수행될 수 있다. 또한, 상기 다양한 블럭들은 바람직한 구현에 기반해서, 더 적은 블럭들로 조합되고, 추가적인 블럭들로 분리되고, 그리고/또는 제거될 수 있다.The
블럭(502)에서, 방법(500)은 차량을 제어하도록 구성되는 컴퓨팅 디바이스에서, 컴퓨팅 디바이스에 결합되는 이미지-캡쳐 디바이스로부터, 상기 차량이 운행하는 도로의 부근의 하나 이상의 이미지들을 수신하는 것을 포함한다. 상기 컴퓨팅 디바이스는, 예를 들어, 차량 내에 있거나 차량 밖에 있지만 상기 차량과 무선 통신 중일 수 있다. 또한, 상기 컴퓨팅 디바이스는 자율 또는 반자율 동작 모드로 상기 차량을 제어하도록 구성될 수 있다. 더욱이, 이미지-캡쳐 디바이스(예를 들어, 도 1에서의 카메라(134) 또는 도 2에서의 카메라(210))는 차량에 결합되고 상기 컴퓨팅 디바이스와 통신할 수 있다. 상기 이미지-캡쳐 디바이스는 상기 차량이 운행하고 있는 상기 도로 및 도로의 부근의 이미지들 또는 비디오를 캡쳐하도록 구성된다.At
도 6a-6b는, 예를 들어, 각각 예시 이미지들(602 및 604)을 기술하고, 상기 예시 이미지들은 도 4에서의 차량(402)에 결합된 이미지-캡쳐 디바이스에 의해 캡쳐된다. 일 예시에서, 상기 이미지-캡쳐 디바이스는 정지 이미지들 또는 정지 이미지들이 추출될 수 있는 비디오를 계속적으로 캡쳐하도록 구성될 수 있다. 일 예시에서, 하나 이상의 이미지-캡쳐 디바이스들은 상기 차량(402)에 결합된다. 상기 하나 이상의 이미지-캡쳐 디바이스들은 모든 방향들로부터 상기 차량(402)의 주변들 및 도로 상황을 고려하기 위해 다수의 시야들(views)로부터 이미지들을 캡쳐하도록 구성된다.Figures 6A-6B illustrate
도 5를 다시 참조하면, 블럭(504)에서, 방법(500)은 컴퓨팅 디바이스를 이용해서, 하나 이상의 이미지들 중 하나 이상의 이미지 부분들을 결정하는 것을 포함하고, 그리고 상기 하나 이상의 이미지 부분들은 미리 결정된 높이 범위로 상기 도로의 측면들을 묘사할 수 있다. 몇몇의 예시들에서, 상기 미리 결정된 높이 범위는 공사 구역 표지판들에 대해 전형적으로 사용되는 높이 범위에 대응한다. 많은 관할 구역들에서, 도로상의 공사 구역들은 표준 설명서 및 규칙들에 의해 규제되고, 이러한 표준 설명서 및 규칙들은 미리 결정된 높이 범위를 정의하는 데에 사용될 수 있다. 일 예시 규칙은 도로상의 공사 구역의 존재를 표시하는 공사 구역 표지판은 3일보다 더 오랫동안 계속적으로 주어진 위치에 놓여지고 도로의 일 측면의 기둥(post)에 설치된다고 언급할 수 있다. 더욱이, 다른 규칙은 일시적인 경고성 공사 구역 표지판은 예를 들어, 도로 지상 높이의 1 피트 위임을 특정할 수 있다. 다른 예시들에서, 최소 표지판 설치 높이에 추가적으로 또는 대안적으로, 예를 들어, 일시적인 경고성 공사 구역 표지판에 대한 높이 범위는 1 피트와 6 피트 사이일 수 있는 것과 같이, 높이 범위가 특정될 수 있다. 교통 안전 드럼 또는 임시 장벽과 같은 교통 제어 디바이스 뒤에 상기 공사 구역 표지판이 위치되는 몇몇의 위치들에서, 상기 최소 높이는, 추가적인 가시성을 제공하기 위해 5 피트로 올려질 수 있다. 추가적으로 또는 대안적으로, 높이 범위는 예를 들어, 5 피트와 11 피트 사이로 특정될 수 있다. 이 숫자들 및 규칙들은 기술을 위한 것일 뿐이다. 기타 표준들 및 규칙들 또한 가능하다. 몇몇의 예시들에서, 전형적인 공사 구역 표지판의 상기 미리 결정된 높이 범위 또는 최소 높이는 위치에 의존적일 수 있다(예를 들어, 지리적 영역, 미국에서 어떤 주, 국가 등)Referring again to Figure 5, at
상기 컴퓨팅 디바이스는 표준 명세들에 따라, 전형적인 공사 구역 표지판의 미리 결정된 높이 범위에서 도로 측면들을 묘사하는 부분들을 이미지 캡쳐 디바이스에 의해 캡쳐된 이미지들에서 결정하도록 구성된다. 일 예시로서, 도 6a에서, 상기 컴퓨팅 디바이스는 표준 명세들에 따라, 전형적인 공사 구역 표지판들에 대해 특정된 미리 결정된 높이 범위에서 도로(404)의 일 측면을 도시하는 이미지(602) 내의 부분(606)을 결정하도록 구성된다. 유사하게, 도 6b에서, 상기 컴퓨팅 디바이스는, 미리 결정된 높이 범위에서 도로(404)의 다른 측면을 도시하는 이미지(604)내의 부분(608)을 결정하도록 구성된다. 도 6c는 도 6a에서 기술된 이미지(602)의 이미지 부분(606)을 기술하고, 도 6d는 도 6b에서 기술된 이미지(604)의 이미지 부분(608)을 기술한다.The computing device is configured to determine, in accordance with standard specifications, the portions of the images captured by the image capture device that depict the road sides in a predetermined height range of a typical work area sign. In one example, in Figure 6A, the computing device is configured to display a
도 5를 다시 참조하면, 블럭(506)에서, 상기 방법(500)은, 상기 컴퓨팅 디바이스를 이용해서, 상기 하나 이상의 이미지 부분들에서 공사 구역 표지판을 검출하는 것을 포함한다. 또한, 상기 표준 명세들은 전형적인 공사 구역 표지판들의 모양, 색상, 패턴 및 역반사 특성들에 대한 규칙들을 포함한다. 기술을 위한 일 예시로서, 상기 표준 명세들은 반사 시트의 표준 유형을 갖는, 주황색 배경 상에 검은색 글씨의 부호들을 갖는 다이아몬드 모양의 48 인치 x 48 인치인 전형적인 공사 구역 표지판을 특정한다. 이러한 명세들은 기술을 위한 것일 뿐이고, 그리고 기타 명세들 또한 가능하다.Referring again to FIG. 5, at
일 예시로서, 도 6a 내지 도 6d를 참조하면, 상기 컴퓨팅 디바이스는, 각각 이미지 부분(608) 및 이미지 부분(606)에 있는 표지판(412a) 및 표지판(416b)과 같은 후보 공사 구역 표지판들을 검출하도록 구성된다. 상기 컴퓨팅 디바이스는 본 기술분야에서 알려진 이미지 인식 기법들을 이용해서, 전형적인 공사 구역 표지판들의 표준 명세들과 비교한 후보 공사 구역 표지판의 모양, 색상, 패턴 및 반사 특성들 중 하나 이상에 기반해서, 상기 후보 공사 구역 표지판이 공사 구역에 관련되는지를 결정하도록 구성된다. 예를 들어, 상기 컴퓨팅 디바이스는, 상기 비교에 기반해서, 상기 표지판(412a)이 공사 구역 표지판이고, 반면에 상기 표지판(416b)은 공사 구역 표지판이 아님을 결정하도록 구성된다.6A-6D, the computing device is configured to detect candidate construction area signs such as sign 412a and sign 416b in
이미지 인식의 이용을 기술하기 위한 일 예시에서, 상기 컴퓨팅 디바이스는 하나 이상의 이미지 부분들에서 검출된 물체를, 전형적인 공사 구역 표지판의 템플릿(template)과 비교하도록 구성된다. 예를 들어, 상기 컴퓨팅 디바이스는 하나 이상의 이미지 부분들에서, 물체의 색상, 모양, 에지들, 그리고 모서리들과 같은 물체의 특징들을 식별하도록 구성된다. 이후에, 상기 컴퓨팅 디바이스는 이러한 특징들을, 전형적인 공사 구역 표지판의 오렌지색/노란색, 예리한 에지들을 갖는 다이아몬드 모양, 그리고 모서리들(예를 들어, 모서리 특징)과 비교하도록 구성된다. 상기 컴퓨팅 디바이스는, 물체의 특징들이 전형적인 공사 구역 표지판의 전형적인 특징들과 매칭되는지를 결정하기 위해, 특징들(예를 들어, 색상, 모양 등) 또는 물체의 특징들을 대표하는 파라미터들을 분류기를 통해 프로세싱하도록 구성된다. 상기 분류기는 입력 정보(예를 들어, 물체의 특징들)를 클래스(예를 들어, 공사 구역 표지판을 나타내는 물체)에 매핑시킨다. 분류기들, 훈련 데이터, 및 분류 알고리즘들의 예시들은 도 3에 기술된 방법(300)의 블럭(304)에 관해 위에 기술되었다.In one example for describing the use of image recognition, the computing device is configured to compare an object detected in one or more image portions with a template of a typical construction area sign. For example, the computing device is configured to identify, in one or more image portions, features of an object, such as color, shape, edges, and edges of the object. Thereafter, the computing device is configured to compare these features with the orange / yellow color of a typical construction area sign, the diamond shape with sharp edges, and the corners (e.g., edge features). The computing device may be configured to process parameters representative of features (e.g., color, shape, etc.) or characteristics of an object through a classifier to determine if the features of the object match typical characteristics of a typical work area sign . The classifier maps the input information (e.g., the characteristics of the object) to a class (e.g., an object representing a construction area sign). Examples of classifiers, training data, and classification algorithms have been described above with respect to block 304 of the
일 예시에서, 상기 컴퓨팅 디바이스는, 공사 구역 표지판의 검출을 확인 또는 입증하기 위해, 상기 이미지 캡쳐 디바이스로부터 수신되는 이미지 기반 정보에 더하여, 차량(402)에 결합되는 다른 센서들 또는 유닛들로부터 수신되는 정보를 이용하도록 구성된다. 예를 들어, 상기 컴퓨팅 디바이스는, 상기 이미지 기반 정보에 기반해서, 상기 이미지 부분들 내의 후보 공사 구역 표지판이 공사 구역에 관련될 제1 가능성을, 할당하거나 결정하도록 구성된다. 더욱이, 상기 컴퓨팅 디바이스는, 차량(402)에 결합되고 상기 컴퓨팅 디바이스와 콘신하는 LIDAR 센서(예를 들어, 도 1의 LIDAR 유닛(132))로부터, 상기 후보 공사 구역 표지판(예를 들어, 표지판(412a))을 묘사하는 이미지 부분들(예를 들어, 이미지 부분(608))에 대응하는 3D 포인트 클라우드를 포함하는 LIDAR 기반 정보를 수신하도록 구성된다. 상기 3D 포인트 클라우드는 LIDAR로부터 방출되고 상기 후보 공사 구역 표지판의 표면으로부터 반사되는 광선에 기반한 포인트들의 세트를 포함한다. 상기 컴퓨팅 디바이스는, 상기 LIDAR 기반 정보에 기반해서, 상기 후보 공사 구역 표지판이 상기 공사 구역과 관련될 제2 가능성을 결정하고, 상기 제1 가능성 및 제2 가능성에 기반해서 상기 공사 구역 표지판의 존재 또는 검출을 확인하도록 구성된다.In one example, the computing device may receive, in addition to the image-based information received from the image capture device, from other sensors or units coupled to the
다른 예시에서, LIDAR 기반 정보를 수신하는 것에 대해 추가적으로 또는 대안적으로, 상기 컴퓨팅 디바이스는, 상기 컴퓨팅 디바이스에 결합된 RADAR 센서(예를 들어, 도 1의 RADAR 유닛(130))으로부터, 후보 공사 구역 표지판의 위치 및 특징들에 관련된 RADAR 기반 정보를 수신하도록 구성된다. 상기 RADAR 센서는 라디오파들을 방출하고 상기 후보 공사 구역 표지판의 표면을 튕겨 나온 방출된 라디오 파를 다시 수신하도록 구성된다. 상기 수신된 신호들 또는 RADAR 기반 정보는, 예를 들어, 상기 후보 공사 구역 표지판의 차원 특징들을 표시하고, 그리고 상기 후보 공사 구역 표지판이 정지해 있다는 것을 표시할 수 있다. 상기 컴퓨팅 디바이스는, 상기 RADAR 기반 정보에 기반해서, 예를 들어, 상기 후보 공사 구역 표지판의 특징들과 전형적인 공사 구역 표지판의 표준 특징들의 비교에 기반해서, 상기 후보 공사 구역 표지판이 상기 공사 구역에 관련될 제3 가능성을 결정하도록 구성된다. 더욱이, 상기 컴퓨팅 디바이스는 상기 제1 가능성, 제2 가능성 및 제3 가능성에 기반해서 상기 공사 구역 표지판을 검출하도록 구성된다.In another example, in addition to or alternatively to receiving LIDAR-based information, the computing device may receive, from a RADAR sensor (e.g.,
일 예시로서, 상기 컴퓨팅 디바이스는 상기 제1 가능성, 제2 가능성 및 제3 가능성의 함수인 전체 가능성(예를 들어, 상기 제1 가능성, 제2 가능성 및 제3 가능성의 가중치가 부여된 조합)을 결정하도록 구성되고, 그리고 상기 컴퓨팅 디바이스는 상기 전체 가능성에 기반해서 상기 공사 구역 표지판을 검출하도록 구성된다.As an example, the computing device may be configured to determine the total probability (e.g., a weighted combination of the first probability, the second probability, and the third probability) that is a function of the first probability, the second probability, and the third probability And the computing device is configured to detect the work area sign based on the overall probability.
일 예시에서, 상기 컴퓨팅 디바이스는 상기 이미지 캡쳐 디바이스, LIDAR 센서 및 RADAR 센서와 같은 복수의 정보원들로부터 수신된 정보에 기반해서 상기 공사 구역 표지판을 검출하도록 구성될 수 있다. 하지만, 다른 예시에서, 상기 컴퓨팅 디바이스는 상기 복수의 정보원들의 서브세트로부터 수신된 정보의 서브세트에 기반해서 상기 공사 구역 표지판을 검출하도록 구성될 수 있다. 예를 들어, 상기 이미지 캡쳐 디바이스에 의해 캡쳐된 이미지들은 이미지 캡쳐 디바이스의 기능 저하로 인해 블러링될 수 있다. 다른 예시로서, 도로(404)의 상세들은 안개 때문에 이미지들에서 흐려질 수 있다. 이러한 예시들에서, 상기 컴퓨팅 디바이스는 LIDAR 및/또는 RADAR 유닛들로부터 수신되는 정보에 기반해서 상기 공사 구역 표지판을 검출하도록 구성되고 상기 이미지 캡쳐 디바이스로부터 수신되는 정보를 무시하도록 구성될 수 있다.In one example, the computing device may be configured to detect the construction area sign based on information received from a plurality of sources, such as the image capture device, the LIDAR sensor, and the RADAR sensor. However, in another example, the computing device may be configured to detect the construction area sign based on a subset of information received from the plurality of information sources. For example, images captured by the image capture device may be blurred due to degradation of the image capture device. As another example, the details of
다른 예시에서, 상기 차량(402)은 일부 전기적 잡음 또는 재밍 신호들이 존재하는 도로(404)의 부분을 운행하고 있을 수 있고, 그러므로 LIDAR 및/또는 RADAR 신호들이 올바르게 동작하지 않을 수 있다. 이러한 경우에 있어서, 상기 컴퓨팅 디바이스는 상기 이미지 캡쳐 디바이스로부터 수신된 정보에 기반해서 상기 공사 구역 표지판을 검출하도록 구성되고, 그리고 LIDAR 및/또는 RADAR 유닛들로부터 수신되는 정보를 무시하도록 구성될 수 있다.In another example, the
일 예시에서, 상기 컴퓨팅 디바이스는 상기 도로(404)의 상황(예를 들어, 안개, 전기적 재밍 등)에 기반해서, 그리고/또는 상기 복수의 정보원들 중 각각의 정보원에 할당되는 각각의 신뢰성 메트릭에 기반해서 상기 복수의 정보원들의 순위를 매기도록 구성될 수 있다. 상기 순위는 상기 공사 구역 표지판을 검출하는 데 있어서 어떤 센서(들)에 의존할지 또는 어떤 센서(들)에 더 많은 가중치를 부여할지를 표시한다. 일 예시로서, 만약 안개가 상기 도로의 일 부분에 존재하면, 상기 LIDAR 및 RADAR 센서들에 이미지 기반 디바이스보다 높은 순위가 매겨질 것이고, 그리고 상기 LIDAR 및/또는 RADAR 센서로부터 수신되는 정보에 상기 이미지 캡쳐 디바이스로부터 수신되는 각각의 정보보다 더 많은 가중치가 부여될 것이다.In one example, the computing device is configured to determine, based on the circumstances (e.g., fog, electrical jamming, etc.) of the
도 5를 다시 참조하면, 블럭(508)에서, 상기 방법(500)은, 상기 컴퓨팅 디바이스를 이용해서, 하나 이상의 이미지 부분들에서 상기 공사 구역 표지판의 유형을 결정하는 것을 포함한다. 하나의 공사 구역 표지판 유형은 도로상의 공사 구역에 접근하고 상기 공사 구역을 콘과할 때의 속도 제한들을 규정하는 것과 관련될 수 있다. 다른 공사 구역 표지판 유형은 차로 변경들, 폐쇄, 감소, 콘합 등에 관련될 수 있다. 또 다른 공사 구역 표지판 유형은 상기 도로상에서 운행의 방향의 일시적인 변경에 관련될 수 있다. 공사 구역 표지판들의 예시 유형들은, "전방 우측 차로 폐쇄," "전방 도로 작업중," "정지할 준비를 하시오," "1500 피트간 도로 공사중," "전방 단일 차로," "감소된 속도 제한 30," "갓길 작업중" 등을 포함할 수 있다. 기타 예시 유형들이 가능하다.Referring again to FIG. 5, at
일 예시에서, 상기 차량의 컴퓨팅 디바이스는 상기 공사 구역 표지판의 모양, 색상, 단어들의 서체 등에 기반해서, 상기 검출된 공사 구역의 유형을 결정하도록 구성될 수 있다. 일 예시로서, 상기 컴퓨팅 디바이스는 상기 검출된 공사 구역 표지판의 이미지로부터 상기 유형(예를 들어, 상기 공사 구역 표지판의 모양, 또는 상기 공사 구역 표지판에 쓰여진 단어들)을 식별하기 위해, 이미지 인식 기법들을 이용하도록 구성될 수 있다. In one example, the computing device of the vehicle may be configured to determine the type of the detected work area based on the shape, color, fonts, and the like of the work area sign. As an example, the computing device may use image recognition techniques to identify the type (e.g., the shape of the work area sign, or the words written on the work area sign) from the image of the detected work area sign As shown in FIG.
블럭(506)에 관해 위에 기술된 바와 같이, 상기 컴퓨팅 디바이스는, 공사 구역 표지판을 검출하기 위해서, 물체를 전형적인 공사 구역 표지판의 템플릿과 비교하기 위한 이미지 인식 기법들을 활용하도록 구성될 수 있다. 일 예시에서, 상기 검출된 공사 구역 표지판의 유형을 결정하기 위해서, 상기 컴퓨팅 디바이스는 상기 검출된 공사 구역 표지판의 부분들을 전형적인 공사 구역 표지판들의 서브-템플릿들과 비교하도록 구성될 수 있다. 일 예시에서, 상기 컴퓨팅 디바이스는 상기 검출된 공사 구역 표지판에 타이핑된 개별적인 단어들 또는 문자들을 식별하고, 상기 식별된 단어들 또는 문자들을 전형적인 공사 구역 표지판들의 서브-템플릿들과 비교하도록 구성될 수 있다. 다른 예시에서, 상기 컴퓨팅 디바이스는 상기 문자들 또는 상기 단어들 간의 공간을 결정하고, 그리고/또는 상기 단어들과 상기 검출된 공사 구역 표지판의 에지들 간의 공간을 결정하도록 구성될 수 있다. 또 다른 예시에서, 상기 컴퓨팅 디바이스는 상기 검출된 공사 구역 표지판에 상기 단어들 또는 문자들이 프린트된 폰트를 식별하고, 상기 식별된 폰트를 전형적인 공사 구역 표지판들에 관련된 폰트 서브-템플릿들과 비교하도록 구성될 수 있다.As described above with respect to block 506, the computing device may be configured to utilize image recognition techniques to compare an object with a template of a typical construction area sign to detect a construction area sign. In one example, in order to determine the type of the detected work area sign, the computing device may be configured to compare portions of the detected work area sign with sub-templates of typical work area signs. In one example, the computing device may be configured to identify individual words or characters typed in the detected work area signs and to compare the identified words or characters with sub-templates of typical construction area signs . In another example, the computing device may be configured to determine the space between the characters or the words, and / or to determine a space between the words and the edges of the detected construction area signs. In another example, the computing device is configured to identify a font in which the words or characters are printed on the detected construction area sign, and to compare the identified font with font sub-templates associated with typical construction area signs .
일 예시로서, 상기 컴퓨팅 디바이스는 상기 검출된 공사 구역 표지판에 "전방 도로 공사중"이라는 단어들이 타이핑된 공사 구역 표지판을 검출하도록 구성된다. 상기 컴퓨팅 디바이스는 "도로," "작업," "전방"과 같은 개별적인 문자들 또는 단어들을 상기 하나 이상의 이미지 부분들에서 추출하고, 이러한 단어들 및 이러한 단어들의 특징들(예를 들어, 폰트들, 글자 사이즈들 등)을 전형적인 공사 구역 표지판들의 대응하는 서브 템플릿들과 비교하도록 구성된다. 또한, 상기 컴퓨팅 디바이스는 상기 3개의 단어들 간의 공간 및 상기 단어들을 형성하는 글자들 간의 공간을 대응하는 서브 템플릿들과 비교하도록 구성된다. 더욱이, 상기 컴퓨팅 디바이스는 "도로"라는 단어와 상기 검출된 공사 구역 표지판의 좌측 에지 간의 공간 및 "전방"이라는 단어와 상기 검출된 공사 구역 표지판의 우측 에지 간의 공간을 대응하는 서브 템플릿들과 비교하도록 구성될 수 있다. 이러한 비교들에 기반해서, 상기 컴퓨팅 디바이스는 상기 검출된 공사 구역 표지판의 유형을 결정하도록 구성될 수 있다.In one example, the computing device is configured to detect a construction area sign that is typed with the words "forward road under construction" in the detected construction area sign. The computing device may extract individual characters or words such as " road, "" task," " forward ", from the one or more image portions, Font sizes, etc.) with corresponding sub-templates of typical construction area signs. In addition, the computing device is configured to compare the space between the three words and the space between the letters forming the words with corresponding sub-templates. Further, the computing device may compare the space between the word "road" and the space between the left edge of the detected construction area sign and the space between the word "forward" and the right edge of the detected construction area sign to the corresponding sub- Lt; / RTI > Based on these comparisons, the computing device may be configured to determine the type of the detected construction area sign.
이러한 특징들(예를 들어, 문자들, 단어들, 폰트들, 공간 등)은 기술들을 위한 예시들이고, 상기 검출된 공사 구역 표지판의 유형을 결정하기 위해서, 다른 특징들이 이용될 수 있고, 상기 다른 특징들이 전형적인 공사 구역 표지판들의 전형적인 특징들(예를 들어, 서브 템플릿들)과 비교될 수 있다.These features (e.g., characters, words, fonts, space, etc.) are examples for techniques, and other features may be used to determine the type of the detected construction area sign, Features may be compared to typical features of typical construction area signs (e.g., sub-templates).
일 예시에서, 이미지 인식을 이용하는 것에 추가적으로 또는 대안적으로, RADAR 기반 정보에 기반해서, 상기 컴퓨팅 디바이스는 상기 공사 구역 표지판의 모양 및 차원들을 결정하고, 상기 결정된 모양 및 차원들로부터, 상기 유형 및 관련된 도로 변화들을 추론하도록 구성될 수 있다. 기타 예시들이 가능하다.In one example, in addition to or alternatively to utilizing image recognition, based on RADAR-based information, the computing device may determine the shape and dimensions of the construction area sign and determine from the determined shape and dimensions, Can be configured to deduce road changes. Other examples are possible.
블럭(510)에서, 상기 방법(500)은, 상기 컴퓨팅 디바이스를 이용해서, 상기 공사 구역 표지판의 유형에 기반하여, 상기 차량의 운전 거동과 관련된 제어 전략을 수정하는 것을 포함한다. 공사 구역의 존재로 인한 도로상의 상기 도로 변화들은 상기 공사 구역 앞에 존재하는 상기 공사 구역 표지판의 유형에 의해 표시될 수 있다. 상기 컴퓨팅 디바이스는 상기 공사 구역 표지판의 결정된 유형에 기반해서 상기 차량의 제어 전략을 수정하도록 구성될 수 있다.At
제어 전략을 수정하는 것의 예시들은 도 3에 기술된 상기 방법(300)의 블럭(306)에 관해서 위에 기술되었다. 예시들로서, 상기 컴퓨팅 디바이스는 상기 유형에 표시된 대로 차로 이동 및/또는 속도 변경이 요구되는지를 결정하고, 내비게이션 결정을 함에 있어서 기존의 지도 정보보다는 차량 내 또는 차량 밖 센서들로부터 수신되는 센서 정보를 활용하고, 차로 경계들을 추정하기 위해 기존의 지도 정보보다는 센서 정보를 활용하고, 차로 경계들을 추정하고 따라가기 위해 차로 마커들보다는 공사 구역 콘들 또는 콘들의 위치들을 결정하고, 그리고 공사 인부들의 검출을 위해 그리고 상기 검출에 기반한 내비게이션 결정을 내리기 위해 하나 이상의 센서들을 활성화하도록 구성된다. 이러한 예시들 및 운전 상황들은 기술을 위한 것일 뿐이다. 기타 예시들 및 제어 전략들 및 운전 거동들 또한 가능하다.Examples of modifying the control strategy have been described above with respect to block 306 of the
블럭(512)에서, 상기 방법(500)은, 컴퓨팅 디바이스를 이용해서, 상기 수정된 제어 전략에 기반해서 상기 차량을 제어하는 것을 포함한다. 상기 수정된 제어 전략에 기반해서 상기 차량을 제어하는 것의 예시들은 도 3에 기술된 방법(300)의 블럭(308)에 관해 위에 기술되었다. 예시들로서, 상기 컴퓨팅 디바이스는, 다른 차량을 따라가기 위해서, 수정된 운전 거동에 기반해서, 상기 차량의 병진 속도, 또는 회전 속도, 또는 둘 모두를 조정하고, 다른 차량들과 미리 결정된 안전 거리를 유지하고, 라이트들을 켜고, 차량의 속도를 감소시키고, 차로를 이동하고, 그리고 차량을 멈추도록 구성된다. 이러한 제어 동작들 및 운전 상황들은 기술을 위한 것일 뿐이다. 기타 동작들 및 상황들 또한 가능하다.At
도 5에 기술된 상기 방법(500)의 블럭(506)에 관해 기술된 바와 같이, 상기 컴퓨팅 디바이스는 상기 차량에 결합되고 상기 컴퓨팅 디바이스와 콘신하는 LIDAR 센서로부터 수신되는 정보에 기반해서 상기 공사 구역 표지판을 검출하거나 상기 공사 구역 표지판의 검출을 확인하도록 구성된다.As described in relation to block 506 of the
도 7은 일 실시예에 부합하는, LIDAR 기반 정보를 이용한 공사 구역 표지판의 검출을 위한 방법(700)의 흐름도이다. 도 8a는 일 실시예에 부합하는, 상기 도로의 표면으로부터 임계 높이보다 높은 높이에서의 공사 구역 표지판의 LIDAR 기반 검출을 기술한다. 도 8b는 일 실시예에 부합하는, 상기 도로의 표면으로부터 임계 높이보다 높은 높이에서의 상기 영역을 묘사하는 LIDAR 기반 이미지를 기술한다. 도 7 및 도 8a-8b는 함께 기술될 것이다.7 is a flow diagram of a
방법(700)은 하나 이상의 블럭들(702-712)에 의해 기술되는 것으로서, 하나 이상의 동작들, 기능들, 또는 거동들을 포함한다. 상기 블럭들이 순차적 순서로 기술되었음에도 불구하고, 이러한 블럭들은 몇몇의 예시들에서 병렬적으로, 및/또는 본 명세서에서 기술된 순서와 다른 순서로 수행될 수 있다. 또한, 다양한 블럭들은, 바람직한 구현에 기반해서, 더 적은 블럭들로 조합되고, 추가적인 블럭들로 분리되고, 그리고/또는 제거될 수 있다.The
블럭(702)에서, 상기 방법(700)은, 차량을 제어하도록 구성되는 컴퓨팅 디바이스에서, 상기 컴퓨팅 디바이스에 결합되는 LIDAR 센서로부터, LIDAR 기반 정보를 수신하는 것을 포함하고, 상기 LIDAR 기반 정보는 (i) 차량이 운행하고 있는 도로의 부근의 3차원(3D) 포인트 클라우드와, 상기 3D 포인트 클라우드는 상기 LIDAR로부터 방출되고 상기 도로의 부근에서의 하나 이상의 물체들로부터 반사되는 광선에 대응하는 포인트들을 포함하고, (ii) 포인트들에 대한 반사된 빛의 강도값(intensity value)을 포함한다. LIDAR 센서 또는 유닛(예를 들어, 도 1의 LIDAR 유닛(132))은 차량에 결합되고 상기 차량을 제어하도록 구성되는 컴퓨팅 디바이스와 콘신한다. LIDAR 동작은, 떨어져 있는 타겟의 범위 및/또는 다른 정보를 찾기 위해, 산란된 광선의 특징들을 측정하는 것을 인에이블하는 광학적 원격 센싱 기술을 수반한다. LIDAR 센서/유닛은, 예를 들어, 레이저 펄스들을 빔으로 방출하고, 2차원 또는 3차원 범위 매트릭스들을 발생시키기 위해 상기 빔을 스캔하도록 구성될 수 있다. 일 예시에서, 상기 범위 매트릭스들은, 펄스의 전송과 각각의 반사된 신호의 검출 사이의 시간 딜레이를 측정함에 의해 물체 또는 표면까지의 거리를 결정하기 위해 사용될 수 있다.At
다른 예시에서, LIDAR 센서는 3차원으로 상기 차량을 둘러싸고 있는 환경을 빠르게 스캔하도록 구성될 수 있다. 몇몇의 예시들에서, 2개 이상의 LIDAR 센서는 상기 차량의 완전한 수평 360°를 스캔하기 위해 상기 차량에 결합될 수 있다. 상기 LIDAR 센서는 도로상에서 그리고 상기 도로의 부근에서 레이저에 의해 부딪혔던 물체들을 나타내는 포인트 데이터의 클라우드를 상기 컴퓨팅 디바이스에 제공하도록 구성된다. 상기 포인트들은 범위에 더하여, 방위각 및 고도에 관해서 LIDAR 센서에 의해 나타내질 수 있고, 이들은 차량에 부착된 국부 좌표계에 상대적으로 (X, Y, Z) 포인트 데이터로 변환될 수 있다. 추가적으로, 상기 LIDAR 센서는 광선 또는 물체들로부터 반사되어 나온 레이저의 강도값을 상기 컴퓨팅 디바이스에 제공하도록 구성될 수 있다.In another example, the LIDAR sensor can be configured to rapidly scan the environment surrounding the vehicle in three dimensions. In some examples, two or more LIDAR sensors may be coupled to the vehicle to scan a full 360 degrees of the vehicle. The LIDAR sensor is configured to provide a cloud of point data to the computing device on the road and in the vicinity of the road that is indicative of objects impacted by the laser. In addition to the range, the points can be represented by the LIDAR sensor in terms of azimuth and elevation, and they can be converted into (X, Y, Z) point data relative to the local coordinate system attached to the vehicle. Additionally, the LIDAR sensor may be configured to provide the computing device with an intensity value of the laser reflected from the light beam or objects.
블럭(704)에서, 상기 방법(700)은, 상기 컴퓨팅 디바이스를 이용해서, 상기 도로의 표면으로부터 임계 높이보다 높은 높이에서의 영역을 나타내는 3D 포인트 클라우드에서의 포인트들의 세트를 결정하는 것을 포함한다. 상기 방법(500)에 관해 기술된 바와 같이, 도로들 상의 공사 구역들은 표준 명세들 및 규칙들에 의해 규정된다. 최소 표지판 설치 높이는, 예를 들어, 전형적인 공사 구역 표지판에 대해 특정되어 있다. 도 8a는 상기 도로(404)상을 운행하고 있고 상기 공사 구역 표지판(412a)에 의해 표시되는 공사 구역에 접근하고 있는 차량(402)을 기술한다. 상기 차량(402)에 결합되는 LIDAR 센서는 상기 수평면을 스캔하고 상기 컴퓨팅 디바이스에 상기 도로(404) 및 상기 도로(404)의 부근(예를 들어, 측면들)의 3D 포인트 클라우드를 제공할 수 있다. 더욱이, 상기 컴퓨팅 디바이스는 임계 높이(804)보다 높은 높이에서 영역(802)을 결정하도록 구성된다. 상기 임계 높이(804)는, 예를 들어, 공사 구역들의 표준 명세들에 따른 전형적인 공사 구역 표지판에 대해 특정된 상기 최소 표지판 설치 높이일 수 있다. 도 8b는 결정된 영역(802)을 나타내는 또는 상기 결정된 영역(802)에 대응하는 포인트들의 세트(예를 들어, 3D 포인트 클라우드의 서브세트)를 포함하는 LIDAR 기반 이미지(806)를 기술한다.At
도 7을 다시 참조하면, 블럭(706)에서, 상기 방법(700)은, 상기 컴퓨팅 디바이스를 이용해서, 포인트들의 세트와 관련된 모양을 추정하는 것을 포함한다. 상기 컴퓨팅 디바이스는 임계 높이보다 높은 높이에서 상기 영역을 나타내는 포인트들의 세트에 의해 묘사되는 모양을 식별 또는 추정하도록 구성된다. 예를 들어, 상기 컴퓨팅 디바이스는 상기 모양의 차원적인 특징들을 추정하도록 구성된다. 일 예시에서, 상기 컴퓨팅 디바이스는 모양의 차원적 특징들을 추정하도록 구성된다. 일 예시에서, 상기 컴퓨팅 디바이스는 상기 모양을 추정하기 위해서, 미리 결정된 모양을 포인트의 세트에 묘사된 모양과 맞추도록 구성된다. 일 예시로서, 도 8b에서, 상기 컴퓨팅 디바이스는 LIDAR 기반 이미지(806)에 포함되는 포인트의 세트에서 다이아몬드 모양(808)을 추정하도록 구성될 수 있다.Referring again to FIG. 7, at
도 7을 다시 참조하면, 블럭(708)에서, 상기 방법(700)은, 상기 컴퓨팅 디바이스를 이용해서, 상기 포인트들의 세트에 관해 추정된 모양 및 각각의 강도값들에 기반해서 포인트들의 세트가 공사 구역 표지판을 묘사할 가능성을 결정하는 것을 포함한다. 일 예시에서, 도 8b를 참조하면, 상기 컴퓨팅 디바이스는 상기 추정된 모양(808)을 전형적인 공사 구역 표지판들의 하나 이상의 모양들과 매칭하거나 비교하고, 상기 컴퓨팅 디바이스는 상기 추정된 모양(808)이 주어진 미리 결정된 모양과 얼마나 유사한지(예를 들어, 상기 추정된 모양(808)의 차원적 특징들과 전형적인 공사 구역 표지판의 다이아몬드 모양 간의 매칭의 퍼센트)를 표시하는 매칭도를 결정하도록 구성된다. 일 예시에서, 상기 컴퓨팅 디바이스는 상기 추정된 모양(808)의 에지들을 식별하고 상기 에지들에 의해 형성되는 모양을 전형적인 공사 구역 표지판들의 전형적인 다이아몬드 모양과 매칭하도록 구성된다. 상기 가능성은, 예를 들어, 상기 매칭도에 기반해서 결정될 수 있다.Referring back to Figure 7, at
더욱이, 전형적인 공사 구역 표지판들은 공사 구역들의 표준 명세들에 의해 유리 구슬들 또는 프리즘들과 같은 역반사 시트 물질들로 만들어지도록 요구될 수 있고, 그리고 상기 컴퓨팅 디바이스는 상기 추정된 모양(808)을 형성하는 포인트들의 강도값들을 역반사 시트 물질의 임계 강도값과 비교하도록 구성될 수 있다. 상기 비교에 기반해서, 상기 컴퓨팅 디바이스는 상기 추정된 모양(808)이 주어진 공사 구역 표지판을 나타냄을 확인하도록 구성될 수 있다. 예를 들어, 만약 강도값들이 임계 강도값의 미리 결정된 값에 가깝거나 상기 미리 결정된 값 내에 있는 경우, 상기 컴퓨팅 디바이스는 상기 추정된 모양(808)이 공사 구역 표지판을 나타낼 높은 가능성을 결정하도록 구성될 수 있다.Moreover, typical construction area signs may be required to be made of retroreflective sheet materials, such as glass beads or prisms, by standard specifications of construction areas, and the computing device may form the estimated
일 예시에서, 상기 컴퓨팅 디바이스는 상기 추정된 모양(808)의 전형적인 공사 구역 표지판의 미리 결정된 모양과의 비교에 기반해서 제1 가능성을 결정하도록 구성되고, 그리고 상기 강도값들의 임계 강도값과의 비교에 기반해서 제2 가능성을 결정하도록 구성된다. 상기 컴퓨팅 디바이스는, 추정된 모양(808)을 형성하는 포인트들을 포함하는, 포인트들의 세트들이 공사 구역 표지판을 묘사할 단일의 가능성을 결정하기 위해, 상기 제1 가능성과 제2 가능성을 조합하도록 구성된다.In one example, the computing device is configured to determine a first probability based on a comparison with a predetermined shape of a typical work area sign of the estimated
다른 예시에서, 상기 컴퓨팅 디바이스는, 포인트들의 세트가 공사 구역 표지판을 묘사할 가능성을 결정하기 위해서, 상기 추정된 모양(808)(예를 들어, 추정된 모양(808)의 차원적 특징들) 및 상기 강도값에 기반해서, 확률적 모델(예를 들어, 정규 분포)을 발생하도록 구성된다. 예를 들어, 상기 가능성은 추정된 모양(808) 및 각각의 강도 값의 차원들에 기반해서 결정되는 파라미터 값들의 세트의 함수로서 결정된다. 이 예시에서, 상기 가능성은 이러한 파라미터 값들이 주어졌을 때, 관측된 결과(추정된 모양(808)이 공사 구역 표지판을 나타냄)의 확률과 동일하게 정의된다.In another example, the computing device may determine the estimated shape 808 (e.g., the dimensional features of the estimated shape 808) and the estimated
또 다른 예시에서, 상기 컴퓨팅 디바이스는 서로에 대한 포인트들의 위치들 또는 상기 포인트들의 상대적 위치들에 기반해서, LIDAR 기반 이미지(806)에 묘사되는 포인트들(예를 들어, 상기 추정된 모양(808)을 형성하는 포인트들)이 함께 무리를 이루게 하도록 구성된다. 상기 컴퓨팅 디바이스는 포인트들의 무리로부터 특징들의 세트(예를 들어, 추정된 모양(808)의 차원적 특징들, 그리고 상기 추정된 모양(808)을 형성하는 포인트들의 강도값들)를 추출하도록 구성된다. 상기 컴퓨팅 디바이스는 상기 가능성을 결정하기 위해 분류기를 콘해 이러한 특징들의 세트를 프로세싱하도록 구성된다. 상기 분류기는 입력 정보(예를 들어, 포인트들의 무리로부터 추출되는 특징들의 세트)를 클래스(예를 들어, 공사 구역 표지판을 나타내는 무리)에 매핑시킬 수 있다. 분류기들, 훈련 데이터, 및 분류 알고리즘들의 예시들은 도 3에 기술된 상기 방법(300)의 블럭(304)에 관해 위에 기술되었다.In another example, the computing device may calculate the points (e.g., the estimated shape 808) depicted in the LIDAR-based
일 예시에서, 상기 가능성은, 예를 들어, "낮은," "중간의," "높은"과 같이 질적이거나 척도 상의 숫자와 같이 수량적일 수 있다. 기타 예시들이 가능하다.In one example, the likelihood may be qualitative, such as, for example, "low," "intermediate," "high," or quantitative, such as a number on a scale. Other examples are possible.
도 7을 다시 참조하면, 블럭(710)에서, 상기 방법(700)은, 상기 컴퓨팅 디바이스를 이용해서, 상기 가능성에 기반해서, 상기 차량의 운전 거동과 관련된 제어 전략을 수정하는 것을 포함한다. 상기 가능성에 기반해서(예를 들어, 상기 가능성이 미리 결정된 임계치를 초과함), 상기 컴퓨팅 디바이스는 접근하는 공사 구역을 표시하는 공사 구역 표지판의 존재를 결정하도록 구성된다. 더욱이, 상기 컴퓨팅 디바이스는 상기 도로상의 공사 구역의 존재로 인한 도로 변경들의 심각성을 결정하기 위해 상기 공사 구역 표지판의 유형을 결정하도록 구성된다. 예를 들어, 상기 컴퓨팅 디바이스는 상기 공사 구역 표지판의 결정된 유형에 기반해서 상기 차량의 제어 전략을 수정하도록 구성된다.Referring again to FIG. 7, at
도 5의 상기 방법(500)의 블럭(508)에 관해 위에 기술된 바와 같이, 예를 들어, 상기 공사 구역에 접근하고 상기 공사 구역을 콘과할 때의 속도 제한을 규정하고, 차로 변경들, 폐쇄, 감소, 콘합 등을 기술하고, 그리고 도로상의 운행의 방향의 일시적 변경들을 기술하기 위해 공사 구역 표지판들의 다양한 유형들이 존재한다. 상기 차량의 컴퓨팅 디바이스는 상기 검출된 공사 구역 표지판의 모양, 색상, 단어들의 서체 등에 기반해서, 상기 검출된 공사 구역 표지판의 유형을 결정하도록 구성된다.As described above with respect to block 508 of the
상기 제어 전략을 수정하는 예시들은 도 3에 기술된 상기 방법(300)의 블럭(306)에 관해 위에 기술된다.Examples of modifying the control strategy are described above with respect to block 306 of the
블럭(712)에서, 상기 방법은, 상기 컴퓨팅 디바이스를 이용해서, 상기 수정된 제어 전략에 기반해서 상기 차량을 제어하는 것을 포함한다. 상기 차량을 제어하는 것은, 상기 수정된 운전 거동에 기반해서, 상기 차량의 병진 속도, 또는 회전 속도, 또는 둘 모두를 조정하는 것을 포함한다. 상기 수정된 제어 전략에 기반해서 상기 차량을 제어하는 것의 예시들은 도 3에 기술된 상기 방법(300)의 블럭(308)에 관해 위에 기술되었다.At
LIDAR 기반 정보를 이용한 상기 공사 구역 표지판의 검출에 추가적으로 또는 대안적으로, 상기 컴퓨팅 디바이스는 상기 LIDAR 기반 정보를 이용한 공사 구역 물체들(예를 들어, 콘들, 콘들, 장비, 조끼들, 급커브들 등)을 검출하도록 구성된다.In addition to or alternatively to the detection of the work area signs using the LIDAR-based information, the computing device may be able to detect construction area objects (e.g., cones, cones, equipment, vests, steep curves, etc.) using the LIDAR- ).
도 9는 일 실시예에 부합하는, LIDAR 기반 정보를 이용한 공사 구역 물체들의 검출을 위한 방법의 흐름도이다. 도 10a는 일 실시예에 따른, 상기 도로의 표면으로부터 임계 거리 내의 영역에서 공사 구역 콘들의 LIDAR 기반 검출을 기술한다. 도 10b는 일 실시예에 부합하는, 상기 도로의 표면으로부터 임계 거리 내의 영역을 묘사하는 LIDAR 기반 이미지를 기술한다. 도 10c는 일 실시예에 부합하는, 차로 경계를 형성하는 공사 구역 콘들의 LIDAR 기반 검출을 기술한다. 도 10d는 일 실시예에 부합하는, 차로 경계를 형성하는 공사 구역 콘들을 묘사하는 LIDAR 기반 이미지를 기술한다. 도 9 및 도 10a-10d는 함께 기술될 것이다. 공사 구역 콘들의 검출은 방법(900)을 기술하기 위해 본 명세서에서 이용된다. 그러나, 다른 공사 구역 물체들(예를 들어, 공사 구역 콘들, 장비, 조끼들, 급커브들 등)은 상기 방법(900)을 이용해서도 검출될 수 있다.9 is a flow diagram of a method for detecting objects in a work area using LIDAR-based information consistent with an embodiment. 10A illustrates LIDAR-based detection of construction zone cones in an area within a critical distance from the surface of the road, according to one embodiment. Figure 10B describes an LIDAR-based image depicting an area within a critical distance from the surface of the road, consistent with an embodiment. FIG. 10C illustrates LIDAR-based detection of construction zone cones forming a road boundary, consistent with an embodiment. FIG. 10D illustrates an LIDAR-based image depicting a construction zone cone that forms a boundary by road, consistent with an embodiment. 9 and 10A-10D will be described together. The detection of construction zone cones is used herein to describe
상기 방법(900)은 하나 이상의 블럭들(902-914)에 의해 기술되는 것과 같이 하나 이상의 동작들, 기능들, 또는 거동들을 포함한다. 상기 블럭들이 순차적인 순서로 기술됨에도 불구하고, 이러한 블럭들은 몇몇의 예시들에서 병렬적으로, 그리고/또는 본 명세서에서 기술된 순서와 다른 순서로 수행될 수 있다. 또한, 상기 다양한 블럭들은 바람직한 구현에 기반해서, 더 적은 블럭들로 조합되고, 추가적인 블럭들로 분리되고, 그리고/또는 제거될 수 있다.The
블럭(902)에서, 상기 방법(900)은, 차량을 제어하도록 구성되는 컴퓨팅 디바이스에서, 상기 컴퓨팅 디바이스에 결합되는 LIDAR 센서로부터, 차량이 운행하고 있는 도로의 3차원(3D) 포인트 클라우드에 관한 LIDAR 기반 정보를 수신하는 것을 포함하고, 상기 3D 포인트 클라우드는 상기 LIDAR로부터 방출되고 상기 도로의 하나 이상의 물체들로부터 반사되는 광선에 대응하는 포인트들을 포함한다. LIDAR 센서 또는 유닛(예를 들어, 도 1의 LIDAR 유닛)은 상기 차량에 결합되고 상기 컴퓨팅 디바이스와 통신한다. 도 1의 LIDAR 유닛(132) 및 도 7에 기술된 방법(700)의 블럭(702)에 관해 위에 기술된 것과 같이, 상기 LIDAR 센서는 도로 및 도로의 부근 상의 물체들을 나타내는 포인트 데이터의 클라우드를 상기 컴퓨팅 디바이스에 제공하도록 구성된다. 상기 포인트들은, 범위에 추가적으로, 방위각 및 고도에 관해서 상기 LIDAR 센서에 의해 나타내지고, 이는 상기 차량에 부착되는 국부 좌표계에 상대적인 (X, Y, Z) 포인트 데이터로 변환될 수 있다.At block 902, the
블럭(904)에서, 상기 방법(900)은, 컴퓨팅 디바이스를 이용해서, 상기 도로의 표면으로부터 임계 거리 내에 있는 영역을 나타내는 3D 포인트 클라우드에서 포인트들의 하나 이상의 세트들을 결정하는 것을 포함한다. 방법들(500 및 700)에 관해 위에 기술되었듯이, 도로상의 공사 구역들은 표준 명세들 및 규칙들에 의해 규정된다. 일 예시로서, 교통 안전 콘들은 공사 구역 작업 영역을 지나는 교통을 구별하고 안내하기 위해 사용된다. 콘들은, 예를 들어, 약 18 인치만큼 크도록 특정될 수 있다. 다른 예시에서, 높은 속도 및 높은 교통량, 또는 밤시간대 동작들에 대해, 상기 콘들은 28 인치만큼 크고, 역반사체화 되거나, 또는 상기 콘들은 역반사 물질로 만들어진 밴드들을 포함하는 것으로 특정될 수 있다. 이러한 예시들은 기술만을 위한 것이고, 기타 예시들이 가능하다.At
도 10a는 도로(404)상을 운행하고 공사 구역 콘(406)에 의해 표시되는 공사 구역에 접근하는 차량(402)을 기술한다. 상기 차량(402)에 결합되는 LIDAR 센서는 수평면을 스캔하고, 상기 컴퓨팅 디바이스에 상기 도로(404) 및 상기 도로(404)의 부근의 3D 포인트 클라우드를 제공하도록 구성된다. 더욱이, 상기 컴퓨팅 디바이스는 영역(1002)이 상기 도로(404)의 표면의 임계 거리 내에 있는지를 결정하도록 구성된다. 예를 들어, 임계 거리(1004)는 표준 길이(예를 들어, 18 인치 또는 28 인치)의 콘들을 포함하기 위해 약 30 인치 이상일 수 있다. 특정 공사 구역을 규정하는 표준 명세들에 기반해서 기타 임계 거리들이 가능하다. 도 10b는 영역(1002)의 물체들을 나타내는 포인트들의 세트들을 포함하는 LIDAR 기반 이미지(1006)를 기술한다.10A depicts a
도 9를 다시 참조하면, 블럭(906)에서, 상기 블럭(900)은 포인트들의 하나 이상의 세트들에서 하나 이상의 공사 구역 물체들을 식별하는 것을 포함한다. 예를 들어, 상기 컴퓨팅 디바이스는, LIDAR 기반 3D 포인트 클라우드의 포인트들의 세트들에 의해 나타내지는 물체들의 모양들을 식별하도록 구성된다. 예를 들어, 상기 컴퓨팅 디바이스는 포인트들의 세트에 의해 묘사되는 물체의 모양의 특징들(예를 들어, 차원 특징들)을 추정하도록 구성되고, 상기 컴퓨팅 디바이스는 상기 물체를 식별하기 위해 미리 결정된 모양을 상기 모양에 맞추도록 구성된다. 일 예시로서, 도 10b에서, 상기 컴퓨팅 디바이스는 LIDAR 기반 이미지(1006)의 공사 구역 콘(1008)을 식별하도록 구성된다.Referring again to FIG. 9, at
일 예시에서, 포인트들의 세트들에서 상기 공사 구역 물체들을 식별하기 위해서, 상기 컴퓨팅 디바이스는, 각각의 식별된 공사 구역 물체에 대해, 식별의 각각의 가능성을 결정하도록 구성된다. 일 예시로서, 도 10b에서, 상기 컴퓨팅 디바이스는 상기 콘(1008)을 나타내는 포인트들의 세트의 각각의 포인트들에 의해 정의되는 상기 콘(1008)의 모양을 결정하도록 구성된다. 더욱이, 상기 컴퓨팅 디바이스는 상기 모양을 표준 공사 구역 콘들의 하나 이상의 모양들에 매칭시키도록 구성된다. 상기 컴퓨팅 디바이스는 상기 모양이 전형적인 공사 구역 콘의 주어진 표준 모양에 얼마나 유사한지(예를 들어, 상기 모양의 차원 특징들과 주어진 표준 모양 간의 매칭의 퍼센트)를 표시하는 매칭도를 결정하도록 구성된다. 상기 각각의 가능성은 상기 매칭도에 기반해서 결정될 수 있다.In one example, in order to identify the objects of the construction zone in the sets of points, the computing device is configured to determine, for each identified construction zone object, each probability of identification. As an example, in FIG. 10B, the computing device is configured to determine the shape of the
다른 예시에서, 상기 모양에 기반해서 콘(1008)을 식별하는 것에 추가적으로 또는 대안적으로, 상기 컴퓨팅 디바이스는 포인트들의 위치들 또는 서로에 대한 포인트들의 상대적인 위치들에 기반해서, LIDAR 기반 이미지(1006)에 묘사된 포인트들(예를 들어, 상기 콘(1008)을 형성하는 포인트들)이 함께 무리를 이루게 하도록 구성된다. 상기 컴퓨팅 디바이스는 포인트들의 상기 무리로부터 특징들의 세트(예를 들어, 포인트들의 최소 높이, 포인트들의 최대 높이, 포인트들의 개수, 포인트들의 무리의 폭, 다양한 높이들에서의 포인트들의 일반적인 통계들 등)를 추출하도록 더 구성될 수 있다. 상기 컴퓨팅 디바이스는 포인트들의 상기 무리가 주어진 공사 구역 콘을 나타내는지를 결정하기 위해서, 분류기를 통해 특징들의 이 세트를 프로세싱하도록 구성된다. 상기 분류기는 입력 정보(예를 들어, 포인트들의 상기 무리로부터 추출되는 특징들의 상기 세트)를 클래스(예를 들어, 공사 구역 콘을 나타내는 상기 무리)에 매핑할 수 있다. 분류기들, 훈련 데이터, 및 분류 알고리즘들은 도 3에 기술된 방법(300)의 블럭(304)에 관해 위에 기술되었다.In another example, in addition to or alternatively to identifying the
더욱이, 전형적인 공사 구역 콘들은, 공사 구역들의 표준 명세들에 의해, 유리 구슬들 또는 프리즘들과 같은 역반사 시트 물질들로 만들어질 것이 요구되고, 그리고 상기 컴퓨팅 디바이스는 상기 콘(1008)을 형성하는 포인트들의 강도값들을 상기 역반사 시트 물질의 임계 강도값과 비교하도록 구성된다. 상기 비교에 기반해서, 상기 컴퓨팅 디바이스는, 예를 들어, 상기 콘(1008)의 식별을 확인하도록 구성된다.Moreover, typical construction zone cones are required to be made of retroreflective sheet materials, such as glass beads or prisms, by standard specifications of construction zones, and the computing device forms the
몇몇의 예시들에서, 상기 컴퓨팅 디바이스는 특정한 거리만큼 도로로부터 떨어져 있는 콘들을 배제하도록 구성될 수 있고, 이는 그러한 콘들은 상기 도로로부터 떨어져 있고 교통에 영향을 주지 않는 작업 영역을 표시하는 것이기 때문이다. 또한, 일 예시에서, 상기 컴퓨팅 디바이스는, 사이즈를 전형적인 공사 구역 콘들의 전형적인 사이즈와 비교해서, 분명히 공사 구역 콘들이 될 수 없는 물체들(예를 들어, 공사 구역 콘들이 되기에는 너무 크거나 너무 작음)을 나타내는 포인트들의 세트들을 배제하도록 구성된다.In some instances, the computing device may be configured to exclude the cones away from the road by a certain distance, since such cones are indicative of a work area remote from the road and not affecting traffic. Also, in one example, the computing device may compare the size to the typical size of typical construction zone cones to determine that objects that can not be clearly construction zone cones (e.g., too large or too small to be construction zone cones ≪ / RTI >
일 예시에서, 공사 구역 콘의 신뢰할 수 있는 식별을 위해서, 상기 컴퓨팅 디바이스는, 상기 식별을 확인하고 단일의 스캔에서의 전기적 또는 신호적 잡음에 의해 초래된 잘못된 식별을 필터링 해내기 위해서, LIDAR에 의한 2개(또는 그 이상)의 연속적인 스캔들로부터 수신된 LIDAR 기반 정보에 기반해서 상기 공사 구역 콘을 식별하도록 구성된다.In one example, for reliable identification of a construction zone cone, the computing device may be able to identify the identification of the construction zone cone by LIDAR in order to verify the identification and filter out the false identification caused by electrical or signal noise in a single scan. Based on the received LIDAR-based information from two (or more) consecutive scans.
도 9를 다시 참조하면, 블럭(908)에서, 상기 방법(900)은, 상기 컴퓨팅 디바이스를 이용해서, 상기 하나 이상의 공사 구역 물체들의 개수 및 위치들을 결정하는 것을 포함한다. 일 예시로서, 전형적인 공사 구역 콘들의 차우너 특징들 및 반사 특징들을 특정하는 것에 추가적으로, 공사 구역들에 대한 표준 명세들은 또한 상기 콘들의 개수 및 상기 콘들 간의 공간에 대한 요구사항들을 특정한다. 일 예시에서, 차량들 및 운전자들의 안내를 향상시키기 위해, 몇몇의 상황들에서는, 더 조밀한 공간이 특정될 수 있다. 표 1은 속도 제한들에 기반해서 공사 구역 콘들 간의 최소 공간의 일 예시를 기술한다.Referring again to FIG. 9, at
속도 B : 시속 70 마일Speed A: 50 miles per hour
Speed B: 70 miles per hour
속도 B : 시속 45 마일Speed A: 35 miles per hour
Speed B: 45 miles per
속도 B : 시속 30 마일Speed A: 20 miles per hour
Speed B: 30 miles per hour
이러한 예시들은 기술을 위한 것일 뿐이다. 공간 요구사항들에 대한 기타 예시들 또한 가능하다.These examples are for technology only. Other examples of space requirements are also possible.
일 예시에서, 만약 상기 식별된 콘들의 식별에 대한 각각의 가능성들이 임계 가능성을 초과하면, 상기 컴퓨팅 디바이스는 상기 식별된 콘들의 개수 및 위치들을 더 결정하도록 구성된다. 도 10c에서, 상기 컴퓨팅 디바이스는, 상기 LIDAR 기반 정보에 기반해서, 상기 콘(들)(406)을 검출 또는 식별하고, 또한, 서로에 관한 상기 콘(들)(406)의 위치들 또는 상대적인 위치들뿐만 아니라 상기 콘(들)(406)의 개수를 결정하도록 구성된다. 예를 들어, 상기 콘(들)간의 거리(1010)를 결정하고, 상기 거리(1010)를 표준 명세들에 특정된 미리 결정된 거리(또는 공간)와 비교하도록 구성된다.In one example, if each of the possibilities for identification of the identified cones exceeds a threshold probability, the computing device is configured to further determine the number and locations of the identified cones. In FIG. 10C, the computing device detects or identifies the cone (s) 406 based on the LIDAR-based information and also determines the positions of the cone (s) 406 relative to each other or relative positions (S) 406 as well as the number of the cones (s) 406. For example, to determine a
도 10d는 공사 구역 콘들(1012a-1012d)을 나타내는 포인트들의 세트들을 포함하는 LIDAR 기반 이미지(1011)를 기술한다. 상기 LIDAR 기반 이미지(1011)에서 공사 구역 콘들(1012a-1012d)을 검출하거나 식별하는 것에 추가적으로, 상기 컴퓨팅 디바이스는 콘들의 쌍들 간의 각각의 거리를 추정하도록 구성된다.Figure 10d describes a LIDAR based
도 9를 다시 참조하면, 블럭(910)에서, 상기 방법(900)은, 상기 컴퓨팅 디바이스를 이용해서, 하나 이상의 공사 구역 물체들의 개수 및 위치들에 기반해서 상기 공사 구역의 존재의 가능성을 결정하는 것을 포함한다. 일 예시로서, 상기 도로의 측면 상의 단일의 콘은 활성 공사 구역을 표시하는 것이 아닐 수 있다. 그러므로, 상기 도로상의 콘들의 존재를 검출하고 상기 콘들을 식별하는 것에 추가적으로, 상기 컴퓨팅 디바이스는, 예를 들어, 상기 콘들의 개수 및 위치들(예를 들어, 상대적 거리)에 기반해서, 상기 콘들이 차로 경계를 형성하고 서로간에 미리 결정된 거리 내에 있다는 것을 결정하도록 구성될 수 있고, 이는 도로 변화들을 초래하는 활성 공사 구역의 표시일 수 있다. 그러므로, 상기 컴퓨팅 디바이스는, 상기 콘들의 결정된 개수 및 위치들에 기반해서 상기 콘들이 공사 구역을 표시할 가능성을 결정하거나 상기 콘들이 공사 구역을 표시할 가능성을 확인하도록 구성된다.9, at
일 예시에서, 상기 컴퓨팅 디바이스는 LIDAR 기반 정보에 기반해서 공사 구역 콘들의 개수 및 위치들을 결정하고, 식별된 공사 구역 콘들에 의해 형성된 패턴을 전형적인 공사 구역의 공사 구역 콘들에 의해 형성되는 전형적인 패턴(예를 들어, 차로 경계를 형성하는 콘들의 패턴)과 비교하도록 구성된다. 상기 컴퓨팅 디바이스는 상기 비교에 기반해서, 검출된 공사 구역 콘들이 공사 구역과 관련된다는 것을 결정하고, 그에 따라 상기 가능성을 결정하도록 구성된다.In one example, the computing device determines the number and locations of construction zone cones based on the LIDAR-based information, and determines the pattern formed by the identified construction zone cones in a typical pattern formed by the construction zone cones of a typical construction zone For example, a pattern of cones forming a boundary with a lane). The computing device is configured to determine, based on the comparison, that the detected construction area cones are associated with a work area, and thereby determine the likelihood.
다른 예시에서, 상기 컴퓨팅 디바이스는, 상기 공사 구역의 존재의 가능성을 결정하기 위해서, 상기 콘들의 결정된 개수 및 위치들에 기반해서, 확률적 모델(예를 들어, 정규 분포)을 발생시키도록 구성된다. 예를 들어, 상기 가능성은 상기 식별된 콘들의 개수 및 위치들에 기반해서 결정되는 파라미터 값들의 세트의 함수로서 결정될 수 있다. 이 예시에서, 상기 가능성은 이러한 파라미터 값들이 주어졌을 때, 관측된 결과(상기 콘들이 상기 도로상의 공사 구역을 표시함)의 확률과 동일하게 정의될 수 있다.In another example, the computing device is configured to generate a probabilistic model (e.g., a normal distribution) based on the determined number and locations of the cones to determine the likelihood of the presence of the construction zone . For example, the likelihood may be determined as a function of a set of parameter values determined based on the number and locations of the identified cones. In this example, the likelihood can be defined to be the same as the probability of the observed result (the cones indicate the construction area on the road) given these parameter values.
또 다른 예시에서, 상기 컴퓨팅 디바이스는, 상기 가능성을 결정하기 위해서, 분류기를 통해 상기 콘들의 개수 및 위치들에 관한 정보를 프로세싱하도록 구성된다. 상기 분류기는 입력 정보(예를 들어, 상기 콘들의 개수 및 위치)를 클래스(예를 들어, 공사 구역의 존재)에 매핑할 수 있다. 분류기들 및 분류 알고리즘들의 예시들은 도 3에서 기술된 방법(300)의 블럭(304)에 관해 위에 기술되었다.In another example, the computing device is configured to process information about the number and locations of the cones through a classifier to determine the likelihood. The classifier may map the input information (e.g., the number and position of the cones) to a class (e.g., the presence of a construction zone). Examples of classifiers and classification algorithms have been described above with respect to block 304 of the
일 예시로서, 훈련 컴퓨팅 디바이스는 주어진 차량의 복수의 운전 상황들에 대한 훈련 데이터를 수신하도록 구성될 수 있다. 예를 들어, 각각의 훈련 데이터는, 복수의 운전 상황들 각각에 대해, 각각의 도로의 각각의 3D 포인트 클라우드에 관한 각각의 LIDAR 기반 정보를 포함할 수 있다. 상기 각각의 훈련 데이터의 각각의 LIDAR 기반 정보에 기반해서, 상기 컴퓨팅 디바이스는 각각의 콘들의 각각의 개수 및 위치들을 결정할 뿐만 아니라, 상기 각각의 콘들을 식별하도록 구성된다. 또한. 상기 컴퓨팅 디바이스는 운전 상황들 각각에 대한 각각의 훈련 데이터에 대응하는 각각의 공사 구역의 각각의 존재의 긍정적인 또는 부정적인 표시를 수신하도록 구성된다. 더욱이, 상기 훈련 컴퓨팅 디바이스는, 각각의 운전 상황에 대해, 상기 긍정적인 또는 부정적인 표시를 상기 각각의 훈련 데이터와 상관시키고, 상기 복수의 운전 상황들에 대한 상관도들에 기반해서, 상기 분류기의 파라미터들(예를 들어, 방정식 1의 가중치들의 벡터)을 결정하도록 구성된다. 이러한 파라미터들은, 상기 컴퓨팅 디바이스가 LIDAR 기반 정보를 수신할 때, 상기 컴퓨팅 디바이스가 상기 가능성을 결정하기 위해서, 상기 분류기의 결정된 파라미터들을 이용해서 상기 분류기를 통해 LIDAR 기반 정보를 프로세싱하도록 구성되게끔, 상기 차량을 제어하도록 구성되는 컴퓨팅 디바이스들에 제공된다.As an example, a training computing device may be configured to receive training data for a plurality of driving situations of a given vehicle. For example, each training data may include, for each of a plurality of driving situations, respective LIDAR-based information about each 3D point cloud of each road. Based on each LIDAR-based information of each training data, the computing device is configured to not only determine the number and locations of each of the respective cones, but also to identify the respective cones. Also. The computing device is configured to receive a positive or negative indication of the presence of each of the respective work areas corresponding to respective training data for each of the operational situations. Furthermore, the training computing device may further comprise, for each driving situation, correlating the positive or negative indication with the respective training data, and based on the correlations for the plurality of driving situations, (E. G., The vector of weights of Equation 1). These parameters may be configured such that when the computing device receives LIDAR-based information, the computing device is configured to process LIDAR-based information through the classifier using determined parameters of the classifier to determine the likelihood, And is provided to computing devices configured to control the vehicle.
일 예시에서, 상기 가능성은, 예를 들어, "낮은," "중간의," "높은"과 같이 질적이거나 척도상의 숫자와 같이 수량적일 수 있다. 기타 예시들이 가능하다.In one example, the likelihood may be qualitative, such as, for example, "low," "intermediate," "high," or quantitative, such as a number on a scale. Other examples are possible.
도 9를 다시 참조하면, 블럭(912)에서, 상기 방법(900)은, 상기 도로상의 공사 구역의 존재의 가능성에 기반해서, 상기 컴퓨팅 디바이스에서, 상기 차량의 운전 거동과 관련된 제어 전략을 수정하는 것을 포함한다. 상기 컴퓨팅 디바이스는, 공사 구역의 존재로 인한 도로 변화들이 주어졌을 때, 다른 물체들과 거리를 안전하게 유지하고 가장 안전하다고 간주되는 차로를 선택하기 위해서, 상기 차량 속도를 제어하는 거동들에 대한 규칙들을 포함하는 제어 전략을, 상기 공사 구역의 존재의 결정된 가능성에 기반해서, 수정 또는 선택하도록 구성된다. 상기 가능성에 기반해서 상기 제어 전략을 수정하는 예시들은 도 3에 기술된 방법(300)의 블럭(306)에 관해 위에 기술되었다.9, at
블럭(914)에서, 상기 방법(900)은, 상기 컴퓨팅 디바이스를 이용해서, 상기 수정된 제어 전략에 기반해서, 상기 차량을 제어하는 것을 포함한다. 상기 차량을 제어하는 것은, 상기 수정된 운전 거동에 기반해서, 상기 차량의 병진 속도, 또는 회전 속도, 또는 둘 모두를 조정하는 것을 포함한다. 상기 수정된 제어 전략에 기반해서 상기 차량을 제어하는 예시들은 도 3에 기술된 방법(300)의 블럭(308)에 관해 위에 기술되었다.At
몇몇의 실시예들에서, 상기 개시된 방법들은, 기계에 의해 판독 가능한 포맷으로 컴퓨터 판독 가능 저장 매체 상에, 또는 제작물의 다른 비-일시적 매체 또는 물품들 상에 인코딩된 컴퓨터 프로그램 명령들로서 구현될 수 있다. 도 11은 본 명세서에 제시된 최소한 몇몇의 실시예들에 따라 배열된, 컴퓨팅 디바이스상에서 컴퓨터 프로세스를 실행하기 위한 컴퓨터 프로그램을 포함하는, 예시적인 컴퓨터 프로그램 물(1100)의 개념적인 부분도를 기술하는 개략도이다. 일 실시예에서, 상기 예시적인 컴퓨터 프로그램 물(1100)은 신호 베어링 매체(1101)를 이용해서 제공된다. 상기 신호 베어링 매체는, 하나 이상의 프로세서들에 의해 실행되었을 때, 도 1 내지 도 10에 관해 위에 기술된 기능성 또는 상기 기능성의 부분들을 제공하는 하나 이상의 프로그램 명령들(1102)을 포함할 수 있다. 그러므로, 예를 들어, 도 3, 도 5, 도 7 및 도 9에 보여진 실시예들을 참조하면, 블럭들(302-308, 502-512, 702-712, 902-914)의 하나 이상의 구성들은 상기 신호 베어링 매체(1101)와 관련된 하나 이상의 명령들에 의해 착수될 수 있다. 추가적으로, 도 11의 상기 프로그램 명령들(1102)은 또한 예시 명령들을 기술한다.In some embodiments, the disclosed methods may be implemented as computer program instructions encoded on a computer-readable storage medium in a machine-readable format, or on another non-transitory medium or articles of manufacture . 11 is a schematic diagram illustrating a conceptual partial view of an exemplary computer program product 1100, including a computer program for executing a computer process on a computing device, arranged in accordance with at least some of the embodiments set forth herein. to be. In one embodiment, the exemplary computer program product 1100 is provided using a signal bearing medium 1101. The signal bearing medium may include one or more program instructions 1102 that, when executed by one or more processors, provide functionality or portions of the functionality described above with respect to FIGS. 1-10. Thus, for example, referring to the embodiments shown in Figures 3, 5, 7 and 9, one or more configurations of blocks 302-308, 502-512, 702-712, May be undertaken by one or more instructions associated with the signal bearing medium 1101. Additionally, the program instructions 1102 of Figure 11 also illustrate example instructions.
몇몇의 예시들에서, 상기 신호 베어링 매체(1101)는, 하드 디스크 드라이브, 컴팩트 디스크(CD), 디지털 비디오 디스크(DVD), 디지털 테이프, 메모리 등과 같은, 컴퓨터 판독 가능 매체(1103)를 망라하지만, 이에 한정되지는 않는다. 몇몇의 구현들에서, 상기 신호 베어링 매체(1101)는 디지털 및/또는 아날로그 통신 매체(예를 들어, 광섬유 케이블, 도파관, 유선 통신 링크, 무선 통신 링크 등)와 같은, 그러나 이에 한정되지 않는 통신 매체(1105)를 망라한다. 그러므로, 예를 들어, 상기 신호 베어링 매체(1101)는 통신 매체(1105)의 무선 형태(예를 들어, IEEE 802.11 표준 또는 다른 전송 프로토콜에 따르는 무선 통신 매체)에 의해 전달될 수 있다.In some instances, the signal bearing medium 1101 includes a computer readable medium 1103, such as a hard disk drive, a compact disk (CD), a digital video disk (DVD), a digital tape, But is not limited thereto. In some implementations, the signal bearing medium 1101 may be a communication medium such as but not limited to digital and / or analog communication media (e.g., fiber optic cables, waveguides, wired communication links, (1105). Thus, for example, the signal bearing medium 1101 may be carried by a wireless form of the communication medium 1105 (e.g., a wireless communication medium conforming to the IEEE 802.11 standard or other transmission protocol).
상기 하나 이상의 프로그래밍 명령들(1102)은, 예를 들어, 컴퓨터에 의해 실행 가능하고 그리고/또는 논리로 구현되는 명령들일 수 있다. 몇몇의 예시들에서, 도 1 내지 도 10에 관해 기술된 컴퓨팅 디바이스와 같은 컴퓨팅 디바이스는, 하나 이상의 컴퓨터 판독 가능 매체(1103), 컴퓨터 기록 가능 매체(1104), 및/또는 통신 매체(1105)에 의해 상기 컴퓨팅 디바이스에 전달되는 프로그래밍 명령들(1102)에 응답하여, 다양한 동작들, 기능들, 또는 거동들을 제공하도록 구성된다. 본 명세서에서 기술되는 배열들은 단지 예시의 목적들을 위한 것이라는 점이 이해되어야 한다. 그와 같이, 바람직한 결과들에 따라, 기타 배열들 및 기타 요소들(예를 들어, 기계들, 인터페이스들, 기능들, 기능들의 순서들 및 그룹들 등)이 대신 사용될 수 있고, 그리고 몇몇의 요소들은 함께 생략될 수 있음을 통상의 기술자들은 이해할 것이다. 더욱이, 기술되는 많은 요소들은, 어떤 적절한 조합으로 또는 적절한 위치에서, 이산적인 또는 분포된 컴포넌트들 또는 다른 컴포넌트들과 함께 구현되는 기능적인 엔티티들이다.The one or more programming instructions 1102 may be, for example, instructions that are executable by a computer and / or implemented in logic. In some instances, a computing device, such as the computing device described with respect to Figs. 1-10, may be coupled to one or more computer
다양한 양상들 및 실시예들이 본 명세서에서 개시된 반면에, 다른 양상들 및 실시예들은 통상의 기술자들에게 명백할 것이다. 상기 본 명세서에서 개시된 다양한 양상들 및 실시예들은 기술의 목적들을 위한 것이고 한정하고자 하는 의도는 아니며, 진정한 범위는 다음의 청구항들에 의해 표시되고, 이와 함께, 그러한 청구항들에 대한 균등물들의 완전한 범위에 권리가 부여된다. 본 명세서에 사용되는 용어는 특정 실시예들만을 기술하기 위한 목적이고, 한정을 하는 것으로 의도되지 않았다.While various aspects and embodiments are disclosed herein, other aspects and embodiments will be apparent to those of ordinary skill in the art. The various aspects and embodiments disclosed herein are for the purpose of the description and are not intended to be limiting, the true scope being indicated by the following claims, and the full scope of equivalents to such claims . The terminology used herein is for the purpose of describing particular embodiments only, and is not intended to be limiting.
Claims (20)
차량을 제어하도록 구성되는 컴퓨팅 디바이스에서, 복수의 정보원들(information sources)로부터, 상기 차량이 운행하고 있는 도로상의 공사 구역의 검출에 관한 정보를 수신하는 단계와, 상기 복수의 정보원들 중 정보원 각각에는, 그 정보원으로부터 수신된 각각의 정보에 기반해서 상기 공사 구역의 검출의 신뢰의 레벨을 표시하는 각각의 신뢰성 메트릭(reliability metric)이 할당되고;
상기 컴퓨팅 디바이스를 이용해서, 상기 정보 및 상기 복수의 정보원들 각각의 신뢰성 메트릭에 기반해서, 상기 도로상의 공사 구역의 존재의 가능성을 결정하는 단계와;
상기 컴퓨팅 디바이스를 이용해서, 상기 가능성에 기반해서, 상기 차량의 운전 거동과 관련된 제어 전략을 수정하는 단계와; 그리고
상기 컴퓨팅 디바이스를 이용해서, 상기 수정된 제어 전략에 기반해서, 상기 차량을 제어하는 단계를 포함하는 것을 특징으로 하는 방법.In the method,
CLAIMS What is claimed is: 1. A computing device configured to control a vehicle, the method comprising: receiving information from a plurality of information sources about detection of a work area on the road on which the vehicle is running; A respective reliability metric is assigned to indicate a level of confidence in detection of the work area based on respective information received from the information source;
Using the computing device, determining the likelihood of the presence of a work area on the road based on the information and the reliability metric of each of the plurality of sources;
Using the computing device, modifying a control strategy associated with the driving behavior of the vehicle based on the likelihood; And
And using the computing device to control the vehicle based on the modified control strategy.
상기 차량은 자율 동작 모드에 있는 것을 특징으로 하는 방법.The method according to claim 1,
Characterized in that the vehicle is in an autonomous mode of operation.
상기 복수의 정보원들로부터 수신된 상기 정보는,
(i) 이미지 기반 정보;
(ii) LIDAR 기반 정보;
(iii) RADAR 기반 정보;
(iv) 교통 정보;
(v) 지도 정보 중 하나 이상을 포함하는 것을 특징으로 하는 방법. The method according to claim 1,
Wherein the information received from the plurality of information sources comprises:
(i) image-based information;
(ii) LIDAR-based information;
(iii) RADAR-based information;
(iv) traffic information;
and (v) map information.
상기 이미지 기반 정보는 상기 컴퓨팅 디바이스와 결합된 하나 이상의 이미지 캡쳐 디바이스들로부터 수신되고, 상기 이미지 기반 정보는 (i) 상기 도로에 관한 하나 이상의 정지 물체들의 위치, 그리고 (ii) 도로의 지형(geometry)을 표시하는 것을 특징으로 하는 방법.The method of claim 3,
Wherein the image-based information is received from one or more image capture devices associated with the computing device, the image-based information including (i) a location of one or more stationary objects on the road, and (ii) And a display unit for displaying the image.
상기 가능성을 결정하는 단계는 이전에 수집된 훈련 데이터에 의해 훈련된 분류기(classifier)에 의해 상기 정보를 프로세싱하는 것을 포함하고, 상기 분류기를 훈련시키는 것은,
상기 차량의 복수의 운전 상황들에 대한 훈련 데이터를 수신하는 것과, 상기 훈련 데이터는 상기 복수의 운전 상황들 각각에 대해, (i) 각각의 이미지 기반 정보, (ii) 각각의 LIDAR 기반 정보, (iii) 각각의 RADAR 기반 정보, (iv) 각각의 교통 정보 및 (v) 각각의 지도 정보 중 하나 이상의 정보를 포함하고;
상기 운전 상황들 각각에 대한 각각의 훈련 데이터에 대응하는 각각의 공사 구역의 각각의 존재의 긍정적인 또는 부정적인 표시를 수신하는 것과;
각각의 운전 상황에 대해, 상기 긍정적인 또는 부정적인 표시를 상기 각각의 훈련 데이터와 상관시키는 것과; 그리고
상기 복수의 운전 상황들에 대한 상관도들에 기반해서, 상기 분류기의 파라미터들을 결정하는 것을 포함하는 것을 특징으로 하는 방법.The method of claim 3,
Wherein determining the likelihood comprises processing the information by a classifier trained by previously collected training data, wherein training the classifier comprises:
The method comprising: (a) receiving training data for a plurality of driving situations of the vehicle; and (b) for each of the plurality of driving situations, training data includes (i) iii) each RADAR-based information, (iv) respective traffic information, and (v) respective map information;
Receiving a positive or negative indication of the presence of each of the respective work zones corresponding to respective training data for each of the driving situations;
Correlating the positive or negative indication with the respective training data for each driving situation; And
And determining parameters of the classifier based on correlations for the plurality of operating conditions.
상기 수정된 제어 전략에 기반해서 차량을 제어하는 단계는, (i) 내비게이션 결정을 함에 있어서 기존의 지도 정보보다는 차량 내 또는 차량 밖의 센서들로부터 수신된 센서 정보를 활용하는 것과, (ii) 차로 경계들(lane boundaries)을 추정하기 위해서 상기 기존의 지도 정보보다는 상기 센서 정보를 활용하는 것과, (iii) 상기 차로 경계들을 추정하고 이들을 따라가기 위해서 상기 도로상의 차로 마커들(lane markers)보다는 공사 구역 마커들의 위치를 결정하는 것과, (iv) 공사 인부들의 검출을 위해 하나 이상의 센서들을 활성화하고 상기 검출에 기반해서 상기 내비게이션 결정을 하는 것과, (v) 다른 차량을 따라가는 것과, (vi) 다른 차량들과 미리 결정된 안전 거리를 유지하는 것과, (vii) 라이트들을 켜는 것과, (viii) 상기 차량의 속도를 감소시키는 것과, 그리고 (ix) 상기 차량을 정지시키는 것 중 하나 이상을 포함하는 것을 특징으로 하는 방법.The method according to claim 1,
The step of controlling the vehicle based on the modified control strategy may include: (i) utilizing sensor information received from the in-vehicle or out-of-vehicle sensors rather than existing map information in making navigation decisions; (ii) (Iii) estimating the lane boundaries by using the sensor information rather than the existing map information, and (iii) estimating the lane boundaries and estimating lane boundaries based on the lane markers, (Iv) activating one or more sensors for detection of construction workers and making the navigation decision based on the detection, (v) following another vehicle, (vi) Maintaining a predetermined safety distance, (vii) turning on lights, (viii) reducing the speed of the vehicle, and (ix) stopping the vehicle. < Desc / Clms Page number 13 >
상기 공사 구역의 존재로 인한 도로 변화들의 심각도(severity)를 결정하는 단계를 더 포함하고, 상기 제어 전략을 수정하는 단계는 상기 도로 변화들의 심각도에 추가적으로 기반하는 것을 특징으로 하는 방법.The method according to claim 1,
Further comprising determining a severity of road changes due to the presence of the work area, wherein modifying the control strategy is additionally based on severity of the road changes.
복수의 정보원들로부터, 상기 차량이 운행하고 있는 도로상의 공사 구역의 검출에 관한 정보를 수신하는 기능과, 상기 복수의 정보원들 중 정보원 각각에 그 정보원으로부터 수신된 각각의 정보에 기반해서 상기 공사 구역의 검출의 신뢰의 레벨을 표시하는 각각의 신뢰성 메트릭이 할당되고;
상기 정보 및 상기 복수의 정보원들의 각각의 신뢰성 메트릭들에 기반해서, 상기 도로상의 공사 구역의 존재의 가능성을 결정하는 기능과;
상기 가능성에 기반해서, 상기 차량의 운전 거동과 관련된 제어 전략을 수정하는 기능과; 그리고
상기 수정된 제어 전략에 기반해서 상기 차량을 제어하는 기능을 포함하는 기능들을 수행하게 하는 것을 특징으로 하는 비일시적인 컴퓨터 판독 가능 매체.A non-transitory computer-readable medium storing instructions executable by a computing device of a vehicle, the computer-
A function of receiving, from a plurality of information sources, information relating to the detection of a construction zone on the road on which the vehicle is running; and a function of determining, based on each information received from the information source, Each reliability metric indicative of a level of confidence of detection of the reliability metric;
Determining the likelihood of the presence of a work area on the road based on the information and the reliability metrics of each of the plurality of sources;
A function of modifying a control strategy related to the driving behavior of the vehicle, based on the possibility; And
And to perform functions including the function of controlling the vehicle based on the modified control strategy.
상기 차량은 자율 동작 모드에 있는 것을 특징으로 하는 비일시적인 컴퓨터 판독 가능 매체.9. The method of claim 8,
Characterized in that the vehicle is in an autonomous mode of operation.
상기 복수의 정보원들로부터 수신된 상기 정보는,
(i) 이미지 기반 정보;
(ii) LIDAR 기반 정보;
(iii) RADAR 기반 정보;
(iv) 교통 정보;
(v) 지도 정보 중 하나 이상을 포함하는 것을 특징으로 하는 비일시적인 컴퓨터 판독 가능 매체.9. The method of claim 8,
Wherein the information received from the plurality of information sources comprises:
(i) image-based information;
(ii) LIDAR-based information;
(iii) RADAR-based information;
(iv) traffic information;
(v) map information. < RTI ID = 0.0 > 11. < / RTI >
상기 교통 정보는 상기 도로상의 다른 차량들의 거동을 표시하고, (i) 상기 도로상의 다른 차량들 내의 글로벌 포지셔닝 위성(GPS) 디바이스들, (ii) 차량 대 인프라구조 통신, (iii) 차량 대 차량 통신, (iv) 교통 안내 방송, 그리고 (v) 상기 컴퓨팅 디바이스와 통신하는 차량 내 또는 차량 밖의 센서들 중 하나 이상으로부터 수신된 각각의 정보를 포함하는 것을 특징으로 하는 비일시적인 컴퓨터 판독 가능 매체.11. The method of claim 10,
(I) global positioning satellite (GPS) devices in other vehicles on the road, (ii) vehicle to infrastructure communication, (iii) vehicle to vehicle communication (iv) a traffic announcement; and (v) each information received from one or more of the in-vehicle or out-of-vehicle sensors in communication with the computing device.
상기 정보는 교통 정보를 포함하고, 상기 가능성을 결정하는 기능은 상기 교통 정보에 기반해서 상기 도로상의 다른 차량들의 정격 속도 및 교통의 흐름의 변화를 결정하는 것을 포함하는 것을 특징으로 하는 비일시적인 컴퓨터 판독 가능 매체.11. The method of claim 10,
Wherein the information includes traffic information and the function of determining the likelihood includes determining a change in the rated speed and traffic flow of other vehicles on the road based on the traffic information. Available media.
상기 가능성을 결정하는 기능은, 상기 교통 정보에 기반해서, 상기 도로상의 사고 장소를 상기 도로상의 공사 구역으로부터 구별하는 것을 포함하는 것을 특징으로 하는 비일시적인 컴퓨터 판독 가능 매체.13. The method of claim 12,
Wherein the function of determining the likelihood comprises distinguishing, based on the traffic information, an accident site on the road from a construction zone on the road.
상기 컴퓨팅 디바이스는,
복수의 정보원들로부터, 상기 차량이 운행하고 있는 도로상의 공사 구역의 검출에 관한 정보를 수신하고, 상기 복수의 정보원들 중 정보원 각각에 그 정보원으로부터 수신된 각각의 정보에 기반한 상기 공사 구역의 검출의 신뢰의 레벨을 표시하는 각각의 신뢰성 메트릭이 할당되고;
상기 정보 및 상기 복수의 정보원들의 각각의 신뢰성 메트릭들에 기반해서, 상기 도로상의 공사 구역의 존재의 가능성을 결정하고;
상기 가능성에 기반해서, 상기 차량의 운전 거동과 관련된 제어 전략을 수정하고; 그리고
상기 수정된 제어 전략에 기반해서 상기 차량을 제어하도록 구성되는 것을 특징으로 하는 차량용 제어 시스템.1. A vehicle control system including a computing device,
The computing device includes:
The method comprising the steps of: receiving, from a plurality of information sources, information on the detection of a work area on the road on which the vehicle is running; and detecting each of the plurality of information sources from each of the information sources Each reliability metric indicating a level of trust is assigned;
Determine the likelihood of the presence of a work area on the road based on the information and the reliability metrics of each of the plurality of sources;
Based on the possibility, modifying a control strategy associated with the driving behavior of the vehicle; And
And to control the vehicle based on the modified control strategy.
상기 컴퓨팅 디바이스는 상기 차량을 자율 동작 모드에서 제어하도록 더 구성되는 것을 특징으로 하는 차량용 제어 시스템.15. The method of claim 14,
Wherein the computing device is further configured to control the vehicle in an autonomous mode of operation.
상기 복수의 정보원들로부터 수신된 정보는,
(i) 이미지 기반 정보;
(ii) LIDAR 기반 정보;
(iii) RADAR 기반 정보;
(iv) 교통 정보; 및
(v) 지도 정보 중 하나 이상을 포함하는 것을 특징으로 하는 차량용 제어 시스템.15. The method of claim 14,
Wherein the information received from the plurality of information sources comprises:
(i) image-based information;
(ii) LIDAR-based information;
(iii) RADAR-based information;
(iv) traffic information; And
and (v) map information.
상기 컴퓨팅 디바이스는 상기 지도 정보를 수신하도록 구성되고, 상기 지도 정보는 상기 도로상에 존재하는 표지판들의 위치들 및 유형들에 관한 표지판 정보를 포함하고, 그리고 상기 가능성을 결정하는 것은 상기 지도 정보로부터 빠진, 예기되는 공사 구역 표지판의 존재를 결정하는 것을 포함하는 것을 특징으로 하는 차량용 제어 시스템.17. The method of claim 16,
Wherein the computing device is configured to receive the map information, the map information including signpost information about locations and types of signposts present on the road, and determining the likelihood includes missing , ≪ / RTI > determining the presence of an expected construction area sign.
상기 컴퓨팅 디바이스는 상기 지도 정보로 하여금, 상기 예기되는 공사 구역 표지판과 관련된 각각의 표지판 정보 및 상기 도로상의 공사 구역의 존재의 가능성을 포함하게끔 업데이트하도록 더 구성되는 것을 특징으로 하는 차량용 제어 시스템.18. The method of claim 17,
Wherein the computing device is further configured to update the map information to include the respective signpost information associated with the anticipated construction area sign and the likelihood of the presence of a construction zone on the road.
상기 컴퓨팅 디바이스에 결합되고 상기 컴퓨팅 디바이스에 상기 이미지 기반
정보를 제공하도록 구성되는 하나 이상의 이미지 캡쳐 디바이스들을 더 포함하고, 상기 이미지 기반 정보는, (i) 상기 도로에 관한 하나 이상의 정지 물체들의 위치, 그리고 (ii) 도로의 지형을 표시하는 것을 특징으로 하는 차량용 제어 시스템.17. The method of claim 16,
A computing device coupled to the computing device,
Further comprising one or more image capture devices configured to provide information, wherein the image-based information is indicative of (i) the location of one or more stationary objects on the road, and (ii) the terrain of the road Vehicle control system.
상기 컴퓨팅 디바이스는 상기 공사 구역의 존재로 인한 도로 변화들의 심각도를 결정하도록 더 구성되고, 상기 컴퓨팅 디바이스는 상기 도로 변화들의 심각도에 기반해서 상기 제어 전략을 수정하도록 더 구성되는 것을 특징으로 하는 차량용 제어 시스템.15. The method of claim 14,
Wherein the computing device is further configured to determine a severity of road changes due to the presence of the work area and the computing device is further configured to modify the control strategy based on the severity of the road changes. .
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/603,613 US9221461B2 (en) | 2012-09-05 | 2012-09-05 | Construction zone detection using a plurality of information sources |
US13/603,613 | 2012-09-05 | ||
PCT/US2013/053944 WO2014039200A1 (en) | 2012-09-05 | 2013-08-07 | Construction zone detection using a plurality of information sources |
Publications (2)
Publication Number | Publication Date |
---|---|
KR20150052272A true KR20150052272A (en) | 2015-05-13 |
KR101557464B1 KR101557464B1 (en) | 2015-10-06 |
Family
ID=50188580
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
KR1020157008687A KR101557464B1 (en) | 2012-09-05 | 2013-08-07 | Construction zone detection using a plurality of information sources |
Country Status (5)
Country | Link |
---|---|
US (1) | US9221461B2 (en) |
EP (1) | EP2879929B1 (en) |
JP (4) | JP6083882B2 (en) |
KR (1) | KR101557464B1 (en) |
WO (1) | WO2014039200A1 (en) |
Cited By (3)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2019112309A1 (en) * | 2017-12-07 | 2019-06-13 | 삼성전자주식회사 | Vehicle and method for controlling same |
KR20200081516A (en) * | 2017-12-29 | 2020-07-07 | 웨이모 엘엘씨 | Autonomous vehicle system configured to respond to temporary speed limit signs |
KR20210037622A (en) * | 2020-09-04 | 2021-04-06 | 베이징 바이두 넷컴 사이언스 테크놀로지 컴퍼니 리미티드 | Map data updating method, apparatus, device, and readable storage medium |
Families Citing this family (122)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2010124339A1 (en) * | 2009-05-01 | 2010-11-04 | The University Of Sydney | Integrated automation system with picture compilation system |
US8509982B2 (en) | 2010-10-05 | 2013-08-13 | Google Inc. | Zone driving |
US9195914B2 (en) * | 2012-09-05 | 2015-11-24 | Google Inc. | Construction zone sign detection |
DE102012110219A1 (en) * | 2012-10-25 | 2014-04-30 | Continental Teves Ag & Co. Ohg | Method and device for detecting marked danger and / or construction sites in the area of roadways |
EP2914975B1 (en) * | 2012-11-05 | 2019-03-06 | The Chancellor, Masters and Scholars of The University of Oxford | Extrinsic calibration of imaging sensing devices and 2d lidars mounted on transportable apparatus |
US9043069B1 (en) * | 2012-11-07 | 2015-05-26 | Google Inc. | Methods and systems for scan matching approaches for vehicle heading estimation |
US8676431B1 (en) | 2013-03-12 | 2014-03-18 | Google Inc. | User interface for displaying object-based indications in an autonomous driving system |
USD750663S1 (en) | 2013-03-12 | 2016-03-01 | Google Inc. | Display screen or a portion thereof with graphical user interface |
USD754189S1 (en) | 2013-03-13 | 2016-04-19 | Google Inc. | Display screen or portion thereof with graphical user interface |
US9092695B1 (en) * | 2013-03-13 | 2015-07-28 | Google Inc. | High-accuracy real-time road sign detection from images |
USD754190S1 (en) * | 2013-03-13 | 2016-04-19 | Google Inc. | Display screen or portion thereof with graphical user interface |
US9141107B2 (en) * | 2013-04-10 | 2015-09-22 | Google Inc. | Mapping active and inactive construction zones for autonomous driving |
US8930124B1 (en) * | 2013-08-30 | 2015-01-06 | International Business Machines Corporation | Dynamic speed limit generation |
US9796400B2 (en) | 2013-11-27 | 2017-10-24 | Solfice Research, Inc. | Real time machine vision and point-cloud analysis for remote sensing and vehicle control |
US10086857B2 (en) | 2013-11-27 | 2018-10-02 | Shanmukha Sravan Puttagunta | Real time machine vision system for train control and protection |
WO2015100483A1 (en) | 2014-01-06 | 2015-07-09 | Geodigital International Inc. | Determining portions of a roadway model requiring updating |
US10328932B2 (en) * | 2014-06-02 | 2019-06-25 | Magna Electronics Inc. | Parking assist system with annotated map generation |
KR101632486B1 (en) * | 2014-07-09 | 2016-06-22 | 고려대학교 산학협력단 | Method for extracting curb of road using laser range finder and method for localizing of mobile robot using curb information of road |
EP3183688B1 (en) | 2014-08-18 | 2023-08-23 | Mobileye Vision Technologies Ltd. | Recognition and prediction of lane constraints |
US9779314B1 (en) * | 2014-08-21 | 2017-10-03 | Waymo Llc | Vision-based detection and classification of traffic lights |
US9321461B1 (en) | 2014-08-29 | 2016-04-26 | Google Inc. | Change detection using curve alignment |
US9424475B1 (en) * | 2014-09-17 | 2016-08-23 | Google Inc. | Construction object detection |
US9248834B1 (en) | 2014-10-02 | 2016-02-02 | Google Inc. | Predicting trajectories of objects based on contextual information |
US9586585B2 (en) * | 2014-11-20 | 2017-03-07 | Toyota Motor Engineering & Manufacturing North America, Inc. | Autonomous vehicle detection of and response to traffic officer presence |
CN104408443B (en) * | 2014-12-15 | 2017-07-18 | 长春理工大学 | The method for recognizing road surface types and device based on laser radar of multisensor auxiliary |
CN107209514B (en) * | 2014-12-31 | 2020-06-05 | 深圳市大疆创新科技有限公司 | Selective processing of sensor data |
EP3248140A4 (en) * | 2015-01-20 | 2018-12-05 | Solfice Research, Inc. | Real time machine vision and point-cloud analysis for remote sensing and vehicle control |
EP3251023A4 (en) * | 2015-01-26 | 2018-09-26 | TRW Automotive U.S. LLC | Vehicle driver assist system |
US11277721B2 (en) | 2015-02-13 | 2022-03-15 | Nokia Technologies Oy | Flexible security rating and decision mechanism for machine type communications |
US9555736B2 (en) | 2015-04-03 | 2017-01-31 | Magna Electronics Inc. | Vehicle headlamp control using sensing and communication systems |
CN105467392A (en) * | 2015-11-20 | 2016-04-06 | 江苏中利电子信息科技有限公司 | Radar early-warning protection method for road work |
US9858819B2 (en) * | 2016-02-03 | 2018-01-02 | Caterpillar Inc. | Traffic control system having deadlock avoidance functionality |
US10304335B2 (en) | 2016-04-12 | 2019-05-28 | Ford Global Technologies, Llc | Detecting available parking spaces |
FR3052130B1 (en) * | 2016-06-02 | 2018-07-06 | Peugeot Citroen Automobiles Sa | ASSISTANCE METHOD FOR A SELF-CONTAINING VEHICLE CIRCULATING ON A MODIFIED CIRCULATION PATH |
US20170357267A1 (en) * | 2016-06-10 | 2017-12-14 | Cnh Industrial America Llc | Autonomous work vehicle obstacle detection system |
US20170356748A1 (en) * | 2016-06-14 | 2017-12-14 | nuTonomy Inc. | Route Planning for an Autonomous Vehicle |
US10309792B2 (en) | 2016-06-14 | 2019-06-04 | nuTonomy Inc. | Route planning for an autonomous vehicle |
US11092446B2 (en) | 2016-06-14 | 2021-08-17 | Motional Ad Llc | Route planning for an autonomous vehicle |
DE102016214045A1 (en) * | 2016-07-29 | 2018-02-01 | Bayerische Motoren Werke Aktiengesellschaft | Method and device for determining a roadway model for a vehicle environment |
KR20180039892A (en) | 2016-10-11 | 2018-04-19 | 현대자동차주식회사 | Navigation apparatus, vehicle comprising the same and control method of the vehicle |
KR102039487B1 (en) * | 2016-11-11 | 2019-11-26 | 엘지전자 주식회사 | Vehicle driving control apparatus and method |
US11238726B2 (en) | 2016-12-02 | 2022-02-01 | International Business Machines Corporation | Control of driverless vehicles in construction zones |
US11027730B2 (en) | 2016-12-07 | 2021-06-08 | Toyota Motor Europe | Systems and methods for regulation of autonomous cruise control |
KR102406502B1 (en) * | 2016-12-14 | 2022-06-10 | 현대자동차주식회사 | Apparatus and method for controlling narrow road driving of vehicle |
JP2018135068A (en) * | 2017-02-23 | 2018-08-30 | パナソニックＩｐマネジメント株式会社 | Information processing system, information processing method, and program |
JP2018135069A (en) * | 2017-02-23 | 2018-08-30 | パナソニックＩｐマネジメント株式会社 | Information processing system, information processing method, and program |
KR20180099280A (en) * | 2017-02-28 | 2018-09-05 | 삼성전자주식회사 | Method and device to generate virtual lane |
US10282999B2 (en) * | 2017-03-17 | 2019-05-07 | GM Global Technology Operations LLC | Road construction detection systems and methods |
US11113555B2 (en) * | 2017-03-23 | 2021-09-07 | Nec Corporation | Object detection apparatus, traffic monitoring system, method of controlling an object detection apparatus and program |
WO2018232681A1 (en) * | 2017-06-22 | 2018-12-27 | Baidu.Com Times Technology (Beijing) Co., Ltd. | Traffic prediction based on map images for autonomous driving |
KR102064223B1 (en) * | 2017-07-12 | 2020-02-11 | 엘지전자 주식회사 | Driving system for vehicle and Vehicle |
DE102017215505A1 (en) * | 2017-09-05 | 2019-03-07 | Robert Bosch Gmbh | Method and device for predicting a construction site-related route change a route for a vehicle |
US10520319B2 (en) * | 2017-09-13 | 2019-12-31 | Baidu Usa Llc | Data driven map updating system for autonomous driving vehicles |
US11874126B1 (en) | 2017-09-15 | 2024-01-16 | Apple Inc. | Map with location-based observations, actions, and rules |
US10713940B2 (en) | 2017-10-31 | 2020-07-14 | Waymo Llc | Detecting and responding to traffic redirection for autonomous vehicles |
US10401862B2 (en) | 2017-10-31 | 2019-09-03 | Waymo Llc | Semantic object clustering for autonomous vehicle decision making |
CA3080739A1 (en) * | 2017-10-31 | 2019-05-09 | Waymo Llc | Detecting and responding to traffic redirection for autonomous vehicles |
US10535138B2 (en) | 2017-11-21 | 2020-01-14 | Zoox, Inc. | Sensor data segmentation |
EP3523753A4 (en) * | 2017-12-11 | 2019-10-23 | Beijing Didi Infinity Technology and Development Co., Ltd. | Systems and methods for identifying and positioning objects around a vehicle |
JP7273828B2 (en) * | 2017-12-31 | 2023-05-15 | スリーエム イノベイティブ プロパティズ カンパニー | Visually transparent, infrared retroreflective article |
KR102521656B1 (en) | 2018-01-03 | 2023-04-13 | 삼성전자주식회사 | Method and apparatus of identifying object |
JP6973100B2 (en) * | 2018-01-17 | 2021-11-24 | トヨタ自動車株式会社 | Vehicle driving support system |
US10755111B2 (en) | 2018-01-29 | 2020-08-25 | Micron Technology, Inc. | Identifying suspicious entities using autonomous vehicles |
FR3078517B1 (en) * | 2018-03-01 | 2020-02-07 | Psa Automobiles Sa | METHOD AND DEVICE FOR ASSISTING THE AUTOMATED DRIVING OF A VEHICLE NEAR A PASSED ZONE (S). |
JP6929437B2 (en) * | 2018-03-06 | 2021-09-01 | 三菱電機株式会社 | Traffic information processing device and traffic information processing method |
US11391845B2 (en) * | 2018-03-12 | 2022-07-19 | Mitsubishi Electric Corporation | Fog determination apparatus, fog determination method, and computer readable medium |
JP6723492B2 (en) | 2018-03-12 | 2020-07-15 | 三菱電機株式会社 | Fog identification device, fog identification method, and fog identification program |
US11009876B2 (en) * | 2018-03-14 | 2021-05-18 | Micron Technology, Inc. | Systems and methods for evaluating and sharing autonomous vehicle driving style information with proximate vehicles |
US11727794B2 (en) | 2018-03-14 | 2023-08-15 | Micron Technology, Inc. | Systems and methods for evaluating and sharing human driving style information with proximate vehicles |
JP2019156192A (en) * | 2018-03-14 | 2019-09-19 | 本田技研工業株式会社 | Vehicle controller |
US10540554B2 (en) | 2018-03-29 | 2020-01-21 | Toyota Jidosha Kabushiki Kaisha | Real-time detection of traffic situation |
US10915159B2 (en) * | 2018-04-03 | 2021-02-09 | GM Global Technology Operations LLC | Method of controlling a vehicle to adjust perception system energy usage |
US10997429B2 (en) * | 2018-04-11 | 2021-05-04 | Micron Technology, Inc. | Determining autonomous vehicle status based on mapping of crowdsourced object data |
US10983524B2 (en) * | 2018-04-12 | 2021-04-20 | Baidu Usa Llc | Sensor aggregation framework for autonomous driving vehicles |
US10816992B2 (en) * | 2018-04-17 | 2020-10-27 | Baidu Usa Llc | Method for transforming 2D bounding boxes of objects into 3D positions for autonomous driving vehicles (ADVs) |
EP3794313A1 (en) * | 2018-05-14 | 2021-03-24 | 3M Innovative Properties Company | Autonomous navigation systems for temporary zones |
US11161518B2 (en) | 2018-06-15 | 2021-11-02 | Micron Technology, Inc. | Detecting road conditions based on braking event data received from vehicles |
CN108898116B (en) * | 2018-07-02 | 2021-05-04 | 科大讯飞股份有限公司 | Safe driving detection method, device, equipment and storage medium |
US10970929B2 (en) * | 2018-07-16 | 2021-04-06 | Occipital, Inc. | Boundary detection using vision-based feature mapping |
EP4339905A2 (en) * | 2018-07-17 | 2024-03-20 | NVIDIA Corporation | Regression-based line detection for autonomous driving machines |
US10920401B2 (en) * | 2018-07-26 | 2021-02-16 | Caterpillar Paving Products Inc. | Managing work area reservations for autonomous vehicles |
DE102018127061B3 (en) * | 2018-10-30 | 2019-11-14 | Daimler Ag | Method for operating an assistance system of a vehicle, device for carrying out the method and vehicle |
US10823855B2 (en) | 2018-11-19 | 2020-11-03 | Fca Us Llc | Traffic recognition and adaptive ground removal based on LIDAR point cloud statistics |
US10900804B2 (en) | 2018-12-12 | 2021-01-26 | Here Global B.V. | Methods and systems for roadwork extension identification using speed funnels |
US11322025B2 (en) | 2018-12-12 | 2022-05-03 | Here Global B.V. | Method and system for validating existence of roadwork |
US11262209B2 (en) * | 2018-12-12 | 2022-03-01 | Here Global B.V. | Methods and systems for road work extension identification |
US10942516B2 (en) * | 2018-12-12 | 2021-03-09 | Valeo Schalter Und Sensoren Gmbh | Vehicle path updates via remote vehicle control |
US11681294B2 (en) * | 2018-12-12 | 2023-06-20 | Here Global B.V. | Method and system for prediction of roadwork zone |
US11592813B2 (en) * | 2018-12-17 | 2023-02-28 | Robert Bosch Gmbh | Method and controller for the situational transmission of surroundings information of a vehicle |
KR102092482B1 (en) * | 2018-12-21 | 2020-03-23 | 부산대학교 산학협력단 | Method and Apparatus for Detection of Feature Interaction in Autonomous System Using Pattern |
CN109671245B (en) * | 2018-12-29 | 2021-03-16 | 南京奥杰智能科技有限公司 | Safety early warning system for road operators |
US11385338B2 (en) * | 2019-03-06 | 2022-07-12 | Cnh Industrial America Llc | System and method for disregarding obscured sensor data during the performance of an agricultural operation |
WO2020256174A1 (en) * | 2019-06-18 | 2020-12-24 | 엘지전자 주식회사 | Method for managing resources of vehicle in automated vehicle & highway system, and apparatus therefor |
US11195064B2 (en) * | 2019-07-11 | 2021-12-07 | Waymo Llc | Cross-modal sensor data alignment |
US20210058814A1 (en) * | 2019-08-22 | 2021-02-25 | Toyota Motor Engineering & Manufacturing North America, Inc. | Methods and systems for processing traffic data from vehicles |
AU2020336151A1 (en) * | 2019-08-30 | 2022-03-03 | Carnegie Mellon University | System and method of control for autonomous or remote-controlled vehicle platform |
CN110789533B (en) * | 2019-09-25 | 2021-08-13 | 华为技术有限公司 | Data presentation method and terminal equipment |
CN113124860A (en) * | 2020-01-14 | 2021-07-16 | 上海仙豆智能机器人有限公司 | Navigation decision method, navigation decision system and computer readable storage medium |
DE102020200951A1 (en) * | 2020-01-27 | 2021-07-29 | Robert Bosch Gesellschaft mit beschränkter Haftung | Driver assistance system and method for controlling a vehicle, in particular a commercial vehicle |
US20210304596A1 (en) * | 2020-03-31 | 2021-09-30 | Arcadis U.S., Inc. | Traffic reporting and analysis |
CN113496514B (en) * | 2020-04-01 | 2022-09-20 | 阿里巴巴集团控股有限公司 | Data processing method, monitoring system, electronic equipment and display equipment |
CN111611955B (en) * | 2020-05-28 | 2023-09-26 | 百度在线网络技术（北京）有限公司 | Method, device, equipment and storage medium for identifying passable construction road |
US11521127B2 (en) * | 2020-06-05 | 2022-12-06 | Waymo Llc | Road condition deep learning model |
US11662219B2 (en) * | 2020-06-23 | 2023-05-30 | Baidu Usa Llc | Routing based lane guidance system under traffic cone situation |
DE102020118630A1 (en) * | 2020-07-15 | 2022-01-20 | Bayerische Motoren Werke Aktiengesellschaft | Method for operating a driver assistance system of a vehicle and processing device for controlling a driver assistance system |
US11697432B2 (en) | 2020-09-04 | 2023-07-11 | Here Global B.V. | Method, apparatus and computer program product for creating hazard probability boundaries with confidence bands |
US20220081003A1 (en) * | 2020-09-15 | 2022-03-17 | Tusimple, Inc. | DETECTING A CONSTRUCTION ZONE BY A LEAD AUTONOMOUS VEHICLE (AV) AND UPDATING ROUTING PLANS FOR FOLLOWING AVs |
DE102020131996A1 (en) * | 2020-12-02 | 2022-06-02 | Bayerische Motoren Werke Aktiengesellschaft | Securing a geographic position |
CN115112131A (en) * | 2021-03-23 | 2022-09-27 | 华为技术有限公司 | Navigation method, navigation device, map and navigation system |
CN113706913A (en) * | 2021-09-03 | 2021-11-26 | 张立华 | Road construction warning system and method thereof |
JP7376547B2 (en) * | 2021-10-12 | 2023-11-08 | 本田技研工業株式会社 | Vehicle control device, vehicle control method, and program |
CN113997943A (en) * | 2021-10-28 | 2022-02-01 | 山东新一代信息产业技术研究院有限公司 | Automatic driving vehicle control method, equipment and medium based on semantic clustering |
WO2023076887A1 (en) * | 2021-10-29 | 2023-05-04 | Tusimple, Inc. | Autonomous vehicle maneuver in response to construction zone hand signals |
JP2023072128A (en) * | 2021-11-12 | 2023-05-24 | 本田技研工業株式会社 | Vehicle control device |
US20230152800A1 (en) * | 2021-11-17 | 2023-05-18 | Here Global B.V. | Method, apparatus and computer program product for identifying road work within a road network |
US20230160713A1 (en) * | 2021-11-19 | 2023-05-25 | Here Global B.V. | Method, apparatus and computer program product for identifying work zones within a map |
CN113947929B (en) * | 2021-11-27 | 2024-03-08 | 北京工业大学 | Variable speed limit control method for highway reconstruction and extension continuous construction area |
US20230204372A1 (en) * | 2021-12-27 | 2023-06-29 | Here Global B.V. | Method, apparatus, and system for determining road work zone travel time reliability based on vehicle sensor data |
JP7398498B2 (en) | 2022-03-25 | 2023-12-14 | 本田技研工業株式会社 | Control device |
CN115063974A (en) * | 2022-06-08 | 2022-09-16 | 中国第一汽车股份有限公司 | Road construction detection method, device, vehicle-mounted terminal, vehicle and medium |
CN115497288A (en) * | 2022-09-06 | 2022-12-20 | 玺大建设工程有限公司 | Intelligent management early warning method, device and system for highway maintenance construction |
CN116935016B (en) * | 2023-07-28 | 2024-03-08 | 上海济目科技有限公司 | Integrated tunnel construction site risk identification and laser positioning method |
Family Cites Families (51)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5220497A (en) * | 1987-11-20 | 1993-06-15 | North American Philips Corp. | Method and apparatus for controlling high speed vehicles |
JP3324821B2 (en) * | 1993-03-12 | 2002-09-17 | 富士重工業株式会社 | Vehicle exterior monitoring device |
JP3569587B2 (en) * | 1996-02-07 | 2004-09-22 | 本田技研工業株式会社 | Vehicle steering system |
JP3566802B2 (en) * | 1996-02-16 | 2004-09-15 | 本田技研工業株式会社 | Vehicle steering system |
US7979173B2 (en) | 1997-10-22 | 2011-07-12 | Intelligent Technologies International, Inc. | Autonomous vehicle travel control systems and methods |
DE19842176A1 (en) * | 1998-09-15 | 2000-03-16 | Bosch Gmbh Robert | Method to identify traffic signs in surroundings of vehicle and for vehicle navigation, involves including satellite-based navigation system and unit to identify vehicle signs, which are used to update data from each other |
JP4380838B2 (en) * | 1999-04-08 | 2009-12-09 | アジア航測株式会社 | Video image automatic road sign recognition method, road sign automatic recognition device, and road sign automatic recognition program |
JP2003123197A (en) * | 2001-10-16 | 2003-04-25 | Alpine Electronics Inc | Recognition device for road mark or the like |
US6970779B2 (en) * | 2002-11-25 | 2005-11-29 | Denso Corporation | Vehicle speed control system and program |
JP4321142B2 (en) * | 2003-07-02 | 2009-08-26 | 日産自動車株式会社 | Sign recognition device |
JP2006113918A (en) * | 2004-10-15 | 2006-04-27 | Fujitsu Ten Ltd | Driving support device |
US7720580B2 (en) * | 2004-12-23 | 2010-05-18 | Donnelly Corporation | Object detection system for vehicle |
JP2006184106A (en) * | 2004-12-27 | 2006-07-13 | Aisin Aw Co Ltd | On-vehicle navigation device |
JP2006264465A (en) * | 2005-03-23 | 2006-10-05 | Advics:Kk | Vehicle driving support system |
DE102006001710A1 (en) * | 2006-01-13 | 2007-08-16 | Audi Ag | Method for operating a longitudinal guidance system in a motor vehicle |
US7912628B2 (en) * | 2006-03-03 | 2011-03-22 | Inrix, Inc. | Determining road traffic conditions using data from multiple data sources |
JP4901275B2 (en) | 2006-04-07 | 2012-03-21 | 富士重工業株式会社 | Travel guidance obstacle detection device and vehicle control device |
JP2007290539A (en) * | 2006-04-25 | 2007-11-08 | Denso Corp | Driving support apparatus for vehicle |
EP1906339B1 (en) * | 2006-09-01 | 2016-01-13 | Harman Becker Automotive Systems GmbH | Method for recognizing an object in an image and image recognition device |
GB2442776A (en) * | 2006-10-11 | 2008-04-16 | Autoliv Dev | Object detection arrangement and positioning system for analysing the surroundings of a vehicle |
JP4929997B2 (en) * | 2006-11-15 | 2012-05-09 | アイシン・エィ・ダブリュ株式会社 | Driving assistance device |
JP2008191781A (en) * | 2007-02-01 | 2008-08-21 | Hitachi Ltd | Collision avoidance system |
JP4345832B2 (en) * | 2007-03-12 | 2009-10-14 | トヨタ自動車株式会社 | Road condition detection system |
US8725309B2 (en) * | 2007-04-02 | 2014-05-13 | Panasonic Corporation | Safety driving support apparatus |
JP4232167B1 (en) * | 2007-08-27 | 2009-03-04 | 三菱電機株式会社 | Object identification device, object identification method, and object identification program |
DE102007051260A1 (en) | 2007-10-26 | 2009-04-30 | Volkswagen Ag | Lane-holding assistance system for motor vehicles, has optical sensor for detecting lane boundaries, where evaluation unit and position detection unit are provided for accessing road map data |
KR101372482B1 (en) * | 2007-12-11 | 2014-03-26 | 삼성전자주식회사 | Method and apparatus of path planning for a mobile robot |
JP5309778B2 (en) * | 2007-12-19 | 2013-10-09 | 日産自動車株式会社 | Inter-vehicle maintenance support device and inter-vehicle maintenance support method |
US8996294B2 (en) | 2007-12-19 | 2015-03-31 | Nissan Motor Co., Ltd. | Inter-vehicle distance maintenance supporting system and method |
JP4831433B2 (en) * | 2007-12-27 | 2011-12-07 | アイシン・エィ・ダブリュ株式会社 | Own vehicle position recognition device, own vehicle position recognition program, and navigation device |
JP4831434B2 (en) * | 2007-12-27 | 2011-12-07 | アイシン・エィ・ダブリュ株式会社 | Feature information collection device, feature information collection program, own vehicle position recognition device, and navigation device |
US8311695B2 (en) * | 2008-03-19 | 2012-11-13 | Honeywell International Inc. | Construction of evidence grid from multiple sensor measurements |
US8332134B2 (en) * | 2008-04-24 | 2012-12-11 | GM Global Technology Operations LLC | Three-dimensional LIDAR-based clear path detection |
US8605947B2 (en) * | 2008-04-24 | 2013-12-10 | GM Global Technology Operations LLC | Method for detecting a clear path of travel for a vehicle enhanced by object detection |
US8060271B2 (en) | 2008-06-06 | 2011-11-15 | Toyota Motor Engineering & Manufacturing North America, Inc. | Detecting principal directions of unknown environments |
JP2010003157A (en) * | 2008-06-20 | 2010-01-07 | Toyota Motor Corp | Travel support device |
US8188887B2 (en) * | 2009-02-13 | 2012-05-29 | Inthinc Technology Solutions, Inc. | System and method for alerting drivers to road conditions |
US8179393B2 (en) * | 2009-02-13 | 2012-05-15 | Harris Corporation | Fusion of a 2D electro-optical image and 3D point cloud data for scene interpretation and registration performance assessment |
JP5075152B2 (en) | 2009-03-24 | 2012-11-14 | 日立オートモティブシステムズ株式会社 | Vehicle control device |
US8384776B2 (en) | 2009-04-22 | 2013-02-26 | Toyota Motor Engineering And Manufacturing North America, Inc. | Detection of topological structure from sensor data with application to autonomous driving in semi-structured environments |
US8376595B2 (en) * | 2009-05-15 | 2013-02-19 | Magna Electronics, Inc. | Automatic headlamp control |
DE102009033058A1 (en) * | 2009-07-03 | 2010-04-08 | Daimler Ag | Device for displaying driving introduction to driver of motor vehicle, has navigation system for providing driving introduction, where navigation system is coupled with transverse regulating unit |
JP2011145166A (en) * | 2010-01-14 | 2011-07-28 | Toyota Motor Corp | Vehicle detector |
US8031085B1 (en) | 2010-04-15 | 2011-10-04 | Deere & Company | Context-based sound generation |
DE112011101635A5 (en) * | 2010-05-11 | 2013-03-28 | Continental Teves Ag & Co. Ohg | Method for determining a driving tube |
CN103347757B (en) | 2010-07-21 | 2016-11-09 | 伊顿公司 | Optimize the system and method for fuel economy by using prediction environment and driving behavior information |
JP5423631B2 (en) * | 2010-09-24 | 2014-02-19 | 株式会社デンソー | Image recognition device |
US8509982B2 (en) * | 2010-10-05 | 2013-08-13 | Google Inc. | Zone driving |
JP5896505B2 (en) * | 2010-12-17 | 2016-03-30 | アルパイン株式会社 | Vehicle driving support device |
US8972147B2 (en) * | 2011-01-10 | 2015-03-03 | Bendix Commercial Vehicle Systems Llc | ACC and AM braking range variable based on internal and external factors |
US9582006B2 (en) * | 2011-07-06 | 2017-02-28 | Peloton Technology, Inc. | Systems and methods for semi-autonomous convoying of vehicles |
-
2012
- 2012-09-05 US US13/603,613 patent/US9221461B2/en active Active
-
2013
- 2013-08-07 KR KR1020157008687A patent/KR101557464B1/en active IP Right Grant
- 2013-08-07 WO PCT/US2013/053944 patent/WO2014039200A1/en active Application Filing
- 2013-08-07 JP JP2015531089A patent/JP6083882B2/en active Active
- 2013-08-07 EP EP13835010.3A patent/EP2879929B1/en active Active
-
2017
- 2017-01-20 JP JP2017008154A patent/JP6257057B2/en active Active
- 2017-12-01 JP JP2017231388A patent/JP6550117B2/en active Active
-
2019
- 2019-06-28 JP JP2019120623A patent/JP6987095B2/en active Active
Cited By (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
WO2019112309A1 (en) * | 2017-12-07 | 2019-06-13 | 삼성전자주식회사 | Vehicle and method for controlling same |
KR20190067366A (en) * | 2017-12-07 | 2019-06-17 | 삼성전자주식회사 | Vehicle and control method thereof |
US11256934B2 (en) | 2017-12-07 | 2022-02-22 | Samsung Electronics Co., Ltd. | Vehicle and method for controlling same |
KR20200081516A (en) * | 2017-12-29 | 2020-07-07 | 웨이모 엘엘씨 | Autonomous vehicle system configured to respond to temporary speed limit signs |
US11594044B2 (en) | 2017-12-29 | 2023-02-28 | Waymo Llc | Autonomous vehicle system configured to respond to temporary speed limit signs |
KR20210037622A (en) * | 2020-09-04 | 2021-04-06 | 베이징 바이두 넷컴 사이언스 테크놀로지 컴퍼니 리미티드 | Map data updating method, apparatus, device, and readable storage medium |
US11859998B2 (en) | 2020-09-04 | 2024-01-02 | Beijing Baidu Netcom Science Technology Co., Ltd. | Map data updating method, apparatus, device, and readable storage medium |
Also Published As
Publication number | Publication date |
---|---|
JP2018060572A (en) | 2018-04-12 |
JP2015537268A (en) | 2015-12-24 |
US9221461B2 (en) | 2015-12-29 |
JP6987095B2 (en) | 2021-12-22 |
KR101557464B1 (en) | 2015-10-06 |
EP2879929A1 (en) | 2015-06-10 |
WO2014039200A1 (en) | 2014-03-13 |
JP6083882B2 (en) | 2017-02-22 |
EP2879929B1 (en) | 2016-10-12 |
EP2879929A4 (en) | 2015-11-18 |
JP2019194900A (en) | 2019-11-07 |
JP2017097906A (en) | 2017-06-01 |
US20140067187A1 (en) | 2014-03-06 |
JP6550117B2 (en) | 2019-07-24 |
JP6257057B2 (en) | 2018-01-10 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
KR101557464B1 (en) | Construction zone detection using a plurality of information sources | |
USRE48322E1 (en) | Construction zone sign detection | |
US9193355B2 (en) | Construction zone sign detection using light detection and ranging | |
US9199641B2 (en) | Construction zone object detection using light detection and ranging | |
US9561797B2 (en) | Predictive reasoning for controlling speed of a vehicle | |
US9090259B2 (en) | Controlling vehicle lateral lane positioning | |
US9081385B1 (en) | Lane boundary detection using images | |
US9014905B1 (en) | Cyclist hand signal detection by an autonomous vehicle | |
EP3851351B1 (en) | A robust method for detecting traffic signals and their associated states | |
US10347127B2 (en) | Driving mode adjustment | |
US9195236B1 (en) | Road flare detection |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A201 | Request for examination | ||
A302 | Request for accelerated examination | ||
E902 | Notification of reason for refusal | ||
E701 | Decision to grant or registration of patent right | ||
GRNT | Written decision to grant | ||
FPAY | Annual fee payment |
Payment date: 20190917Year of fee payment: 5 |