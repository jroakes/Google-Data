US10381022B1 - Audio classifier - Google Patents
Audio classifier Download PDFInfo
- Publication number
- US10381022B1 US10381022B1 US15/041,379 US201615041379A US10381022B1 US 10381022 B1 US10381022 B1 US 10381022B1 US 201615041379 A US201615041379 A US 201615041379A US 10381022 B1 US10381022 B1 US 10381022B1
- Authority
- US
- United States
- Prior art keywords
- audio
- video frames
- classifier
- labels
- image
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/48—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use
- G10L25/51—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use for comparison or discrimination
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/48—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use
- G10L25/51—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use for comparison or discrimination
- G10L25/57—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use for comparison or discrimination for processing of video signals
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/60—Information retrieval; Database structures therefor; File system structures therefor of audio data
- G06F16/63—Querying
- G06F16/638—Presentation of query results
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/60—Information retrieval; Database structures therefor; File system structures therefor of audio data
- G06F16/68—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually
- G06F16/683—Retrieval characterised by using metadata, e.g. metadata not derived from the content or metadata generated manually using metadata automatically derived from the content
-
- G06K9/00744—
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/46—Extracting features or characteristics from the video content, e.g. video fingerprints, representative shots or key frames
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06V—IMAGE OR VIDEO RECOGNITION OR UNDERSTANDING
- G06V20/00—Scenes; Scene-specific elements
- G06V20/40—Scenes; Scene-specific elements in video content
- G06V20/49—Segmenting video sequences, i.e. computational techniques such as parsing or cutting the sequence, low-level clustering or determining units such as shots or scenes
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/01—Assessment or evaluation of speech recognition systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/04—Segmentation; Word boundary detection
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/06—Creation of reference templates; Training of speech recognition systems, e.g. adaptation to the characteristics of the speaker's voice
- G10L15/063—Training
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L17/00—Speaker identification or verification
-
- G10L17/005—
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L17/00—Speaker identification or verification
- G10L17/04—Training, enrolment or model building
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L17/00—Speaker identification or verification
- G10L17/26—Recognition of special voice characteristics, e.g. for use in lie detectors; Recognition of animal voices
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/48—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use
- G10L25/51—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use for comparison or discrimination
- G10L25/54—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use for comparison or discrimination for retrieval
Definitions
- This specification generally relates to audio classifiers.
- Digital audio data can be stored in one or more audio files.
- the audio files can include files with only audio content (e.g., music files) as well as audio files that are associated with, or part of, other files containing other content (e.g., video files with one or more audio tracks).
- the audio data can include speech and music as well as other categories of sound including natural sounds (e.g., rain, wind), human emotions (e.g., screams, laughter), animal vocalization (e.g., lion roar, purring cats), or other sounds (e.g., explosions, racing cars, ringing telephone).
- a typical audio classifier seeks to classify a portion of input audio data as having a particular label.
- Conventional audio classifiers are typically trained based on a collection of human annotated training data.
- one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of obtaining a plurality of video frames from a plurality of videos, wherein each of the plurality of video frames is associated with one or more image labels of a plurality of image labels determined based on image recognition; obtaining a plurality of audio segments corresponding to the plurality of video frames, wherein each audio segment has a specified duration relative to the corresponding video frame; and generating an audio classifier trained using the plurality of audio segment and the associated image labels as input, wherein the audio classifier is trained such that the one or more groups of audio segments are determined to be associated with respective one or more audio labels.
- inventions of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- a system of one or more computers to be configured to perform particular operations or actions means that the system has installed on it software, firmware, hardware, or a combination of them that in operation cause the system to perform the operations or actions.
- one or more computer programs to be configured to perform particular operations or actions means that the one or more programs include instructions that, when executed by data processing apparatus, cause the apparatus to perform the operations or actions.
- the foregoing and other embodiments can each optionally include one or more of the following features, alone or in combination.
- one embodiment includes all the following features in combination.
- the method includes scoring each of a collection of video frames from the plurality of videos, wherein the each video frame is scored for one or more of the plurality of image labels; determining a score of the each video frame satisfies a threshold; and selecting the plurality of video frames in response to determining a score of the each video frame satisfies the threshold.
- the method includes selecting video frames periodically from the plurality of video; and scoring the selected video frames for the image label. The corresponding video frame occurs during the specified duration.
- the one or more audio labels are determined by using an image classifier.
- the method includes identifying an object on video frames from the plurality of videos; and determining the image label associated with the object.
- the method includes evaluating the audio classifier using sample videos having known an audio label for the sample videos.
- one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving a query, the query including one or more query terms; using the one or more query terms to identify one or more audio resources responsive to the query, wherein each audio resource is associated with one or more audio labels generated by an audio classifier trained using audio segments extracted from video content and corresponding image labels identified from the video content; and providing search results identifying the one or more audio resources.
- Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.
- a system of one or more computers to be configured to perform particular operations or actions means that the system has installed on it software, firmware, hardware, or a combination of them that in operation cause the system to perform the operations or actions.
- one or more computer programs to be configured to perform particular operations or actions means that the one or more programs include instructions that, when executed by data processing apparatus, cause the apparatus to perform the operations or actions.
- a conventional audio classifier obtains annotated audio data by manual annotation of the data. Since the conventional audio classifier requires manual annotation, it is cost-ineffective and time-consuming. Compared to the conventional audio classifier, an audio classifier can obtain unannotated audio data from video content. Thus, the audio classifier can be trained using any suitable videos without need for manual annotation of the audio data. In particular, labels generated by an image classifier can be used as supervision for training the audio classifier to exploit a correlation between audio and video modalities. The audio classifier is trained using image labels identified from image detection systems that are more sophisticated than conventional automated audio annotation systems, consequently, the audio classifier can be quickly trained using a large amount of audio data.
- FIG. 1 is a diagram of an example system for generating an audio classifier
- FIGS. 2A-2C are diagrams illustrating examples of obtaining audio segments corresponding to video frames.
- FIG. 3 is a diagram of an example search system.
- FIG. 4 is a flowchart of an example method for generating an audio classifier.
- FIG. 5 is a flowchart of an example method for providing search results.
- Audio classifiers assign one or more audio labels to input audio data.
- Different types of audio data may be classified including speech, music, natural sounds such as rain and wind, human emotions such as crying and laughter, animal vocalization such as roaring and purring, or other artifact sound such as explosions, racing cars and telephone ringers.
- Machine learning can be used to train an audio classifier designed to perform a particular classification.
- Image content in videos can be highly correlated with associated audio content. Using this correlation, an audio classifier can be trained using audio data extracted from videos and associated image labels such that the trained audio classifier can assign one or more audio labels to input audio data.
- the trained audio classifier can be used for various applications. For example, to provide search results, responsive to a search query, that specifically identify audio files either alone or in addition to general search results.
- the audio files can be identified based on the audio labels associated with the audio files, which allows audio content to be returned accurately in response to a search query even.
- FIG. 1 is a diagram of an example system for generating an audio classifier.
- the system 100 includes an audio classifier generator 120 that receives a collection of training videos 110 and uses the training videos 110 to generate an audio classifier 130 .
- the system 100 also includes an evaluator 140 to validate the accuracy of the audio classifier 130 .
- Each of the training videos include multiple video frames. For example, a video file, when played, presents a consecutive frames at a specified rate, e.g., 30 video frames per second.
- the audio classifier generator 120 includes a video analyzing engine 122 , an audio engine 124 , and an audio classifier engine 126 .
- the video analyzing engine 122 analyzes the training videos 110 to extract a collection of video frames. Each of the extracted video frames can be associated with an image label determined based, for example, on an image recognition process, which may broadly include image content analysis to identify particular image content corresponding to one or more labels.
- the video analyzing engine 122 annotates the video frames of the training videos 110 with one or more image labels according to a suitable image recognition technique. In some other implementations, the video analyzing engine 122 receives annotated videos that have been annotated with image labels by a suitable image recognition system.
- the video analyzing engine 122 may associate the video frames with one or more image labels.
- the video analyzing engine 122 can associate the video frames having the specific object or objects with one or more image labels describing the specific object or objects. For example, if the annotated video frames include a birthday cake or a candle, the video analyzing engine 122 can associate the video frames with one or more image labels such as “birthday cake,” “birthday,” “cake,” or “candle.”
- the video analyzing engine 122 can obtain, from the training videos 110 , a collection of video frames associated with an image label. Examples of obtaining video frames from videos are described in greater detail below.
- the audio engine 124 obtains a group of audio segments corresponding to the video frames. After the video analyzing engine 122 obtains a group of video frames from the training videos 110 , the audio engine 124 obtains audio segments corresponding to the obtained video frames. Since the obtained video frames are associated with the image label, the audio segments corresponding to the obtained video frames can also be associated with the image label. In some implementations, each audio segment has a specified duration relative to the corresponding video frame. Examples of obtaining audio segments corresponding to video frames are described in greater detail below with FIG. 2 .
- the audio classifier engine 126 generates and trains the audio classifier 130 using the obtained audio segments and the associated image labels as input.
- the audio classifier 130 is trained to associate one or more groups of audio segments with respective one or more audio labels. After the audio classifier is trained, the audio classifier 130 can associate one or more groups of audio segments with respective audio labels.
- the system can process the image labels to associate the audio segments with the audio labels.
- the system may derive audio labels from the image labels assigned by the image recognition technique. For example, if the system obtains the image label “birthday,” the system may generate audio labels such as “birthday song” and “applause,” which can be relevant to the image label “birthday.” The system then associates the image label “birthday” with the audio labels “birthday song” and “applause” by mapping the image label into the audio labels. Thus, the audio segments associated with the image label “birthday” can be associated with the audio labels “birthday song” and “applause.” Examples of training the audio classifier are described in greater detail below.
- the evaluator 140 can evaluate the audio classifier using a group of sample audio segments 150 having known associations with respective audio labels, for example, from manual annotation by human evaluators. Examples of evaluating the audio classifier are described in greater detail below.
- the video analyzing engine 122 scores the recognized video frames.
- the video analyzing engine 122 can score each of the recognized video frames with respect to one or more identified image labels. In some implementations, the score indicates a confidence that a video frame is associated with a particular image label.
- the video analyzing engine 122 can score a video frame based on various criteria including color, shape, or texture of the object in the video frame.
- the video analyzing engine 122 determines whether a score of each video frame satisfies a threshold.
- the video analyzing engine 122 can compare the scores of the recognized video frames to the threshold. As noted above, the score indicates the confidence that a video frame is associated with a particular image label. If a score of a video frame satisfies the threshold, e.g., the score exceeds a threshold score value, the video analyzing engine 122 can determine that the video frame is associated with a particular image label. If a score of a video frame does not satisfy the threshold, the video analyzing engine 122 can determine that the video frame is not associated with a particular image label.
- the video analyzing engine 122 selects a group of video frames in response to determining the scores of the recognized video frames satisfy the threshold score. If the video analyzing engine 122 determines that the score of the recognized video frame satisfies the threshold, the video analyzing engine 122 selects the video frame. Then, the video analyzing engine 122 continues to obtain an audio segment for the video frame. For example, if the score of a video frame associated with the image label “birthday” satisfies the threshold, the video analyzing engine 122 selects the video frame. If the video analyzing engine 122 determines that the score of a video frame does not satisfy the threshold, the video analyzing engine 122 does not select the video frame.
- FIGS. 2A-2C show diagrams illustrating examples of obtaining audio segments corresponding to the selected video frames.
- the audio segments are audio data included in the videos.
- the audio data may include speech, music, natural sounds such as rain and wind, human emotions such as crying and laughter, animal vocalization such as roaring and purring, or other audio concepts such as explosions, racing cars and telephone ringers.
- the video analyzing engine 122 may select the video frames VF 1 ⁇ 9 associated with the image label “birthday” from a multiple sets of videos V 1 ⁇ 3 . From the multiple sets of videos V 1 ⁇ 3 , the video analyzing engine 122 obtains audio segments AS 1 ⁇ 9 corresponding to the video frames VF 1 ⁇ 9 .
- the audio segments AS 1 ⁇ 9 are audio data included in the multiple sets of videos V 1 ⁇ V 3 .
- the audio data may include sounds corresponding to, for example, human emotions such as laughter, sounds of applause to celebrate, or sounds of singing a birthday song.
- the duration of the audio segments can be specified according to one or more criteria, for example, a duration likely to provide audio correlated to the image recognized in the video frame. In some implementations, the duration of each audio segment is one second. In some other implementations, the duration of each audio segment can be more or less than one second.
- the audio segments AS 1 ⁇ 9 may have a specified duration D relative to the corresponding video frames VF 1 ⁇ 9 .
- the video analyzing engine 122 can obtain the audio segments AS 1 ⁇ 9 relative to the corresponding video frames VF 1 ⁇ 9 in various ways.
- the video analyzing engine 122 may obtain the audio segments AS 1 ⁇ 3 from a video of a first set of videos V 1 .
- the video analyzing engine 122 may obtain the audio segments AS 1 ⁇ 3 such that each of the video frames VF 1 ⁇ 3 occurs during each specified duration of the audio segments AS 1 ⁇ 3 .
- the video frame VF 1 occurs at the time T 1
- the time T 1 occurs during the specified duration D of the audio segment AS 1 .
- the video analyzing engine 122 obtains the audio segment AS 1 having the specified duration relative to the video frame VF 1 .
- the audio segment AS 1 can have a midpoint corresponding to the time T 1 .
- the video analyzing engine 122 may obtain additional audio segments from other videos of the first set of videos V 1 such that each of video frames occurs during each specified duration of the additional audio segments.
- the video analyzing engine 122 may obtain the audio segments AS 4 ⁇ 6 from a video of a second set of videos V 2 .
- the video analyzing engine 122 may obtain the audio segments AS 4 ⁇ 6 such that each of the video frames VF 4 ⁇ 6 occurs during each specified duration of the audio segments AS 4 ⁇ 6 .
- the video frame VF 4 occurs at the time T 4
- the time T 4 occurs before the specified duration D of the audio segment AS 4 .
- the video analyzing engine 122 obtains the audio segment AS 4 having the specified duration relative to the video frame VF 4 .
- the audio segment AS 4 can have a beginning point just after the time T 4 .
- the video analyzing engine 122 may obtain additional audio segments from other videos of the second set of videos V 2 such that each of video frames occurs during each specified duration of the additional audio segments.
- the video analyzing engine 122 may obtain the audio segments AS 4 ⁇ 6 from a video of a second set of videos V 2 .
- the video analyzing engine 122 may obtain the audio segments AS 7 ⁇ 9 such that each of the video frames VF 7 ⁇ 9 occurs during each specified duration of the audio segments AS 7 ⁇ 9 .
- the video frame VF 7 occurs at the time T 7
- the time T 7 occurs after the specified duration D of the audio segment AS 7 .
- the video analyzing engine 122 obtains the audio segment AS 7 having the specified duration relative to the video frame VF 7 .
- the audio segment AS 7 can have an end point just before the time T 7 .
- the video analyzing engine 122 may obtain additional audio segments from other videos of the second set of videos V 2 such that each of video frames occurs during each specified duration of the additional audio segments.
- the audio classifier engine 126 trains the audio classifier 130 .
- the trained audio classifier 130 is a classifier that assigns a score to input audio data for one or more audio labels. If a particular audio label score exceeds a specified threshold, then the input audio data is classified as associated with that label. Any suitable audio classifier model can be used as the audio classifier 130 .
- the audio classifier 130 is trained using audio segments, e.g., obtained from the audio engine 124 and associated image labels, e.g., obtained from the video analyzing engine 122 .
- the audio segments form a training set of audio data.
- the corresponding image labels are used to provide supervision for the training process.
- the image labels can be associated with one or more audio labels.
- the training process is performed such that for a given audio segment of the training set of audio data the classifier identifies an audio label associated with the image label associated with the audio segment.
- the audio classifier is trained based on a number of features extracted from the audio segments.
- the features can be acoustic features identified, for example, from an auditory image.
- the auditory image can be generated, for example, from a particular acoustic model e.g., a cochlear model.
- the audio classifier is trained using the extracted features as variables. The training process determines optimum weights for the respective features that maximize the likelihood of correctly classifying an input audio segment as having an audio label associated with a corresponding image label for the input audio segment.
- the audio classifier 130 assigns a score to each audio segment for a particular audio label.
- the score indicates the likelihood that an audio segment is associated with a particular audio label. If an audio classifier 130 assigns a high score to the audio segment for a particular audio label, the audio segment is strongly associated with the particular audio label. If an audio classifier 130 assigns a low score to the audio segment for a particular audio label, the audio segment is weakly associated with a particular audio label. For example, as noted above, where the image label is “birthday,” the video analyzing engine 122 may obtain the audio labels “birthday song” and “applause.” If an audio segment includes sound of singing a birthday song or sound of applause, the audio classifier 130 can assign a high score to the audio segment. In contrast, if an audio segment includes sound of a lecture about birthday cake baking, the audio classifier 130 can assign a low score to the audio segment. In some implementations, different techniques or combinations of techniques can be used to assign scores to the group of audio segments for the audio label.
- the classifier generator 120 can perform additional iterations to train the audio classifier 130 .
- the audio classifier 130 can repeat the process of assigning scores to each audio segment.
- the classifier generator 120 After additional iterations, the classifier generator 120 generates the trained audio classifier 130 .
- the system 100 may store the scores to the one or more groups of audio segments for the respective one or more audio labels.
- the evaluator 140 can evaluate the trained audio classifier 130 using a group of sample audio segments 150 having known associations with respective sample audio labels.
- the sample audio segments 150 are distinct from the audio segments used for training the audio classifier 130 .
- the trained audio classifier 130 can classify each sample audio segment with particular audio labels.
- the classification of the sample audio segments with the respective audio labels can be evaluated based on the known association between the sample audio segments and the respective sample audio labels. As a result, the evaluator can evaluate the accuracy of the audio classifier 130 . If the output association of the audio classifier 130 does not substantially match the known association, the audio classifier 130 can be adjusted or trained on a new set of training audio segments and associated image labels. If the output association of the audio classifier 130 substantially matches the known association, the audio classifier 130 can be applied to a larger group of audio segments to associate audio segments with respective audio labels. In some implementations, the trained audio classifier 130 can be evaluated by using suitable metrics of audio similarity.
- the trained audio classifier 130 can be used to process audio data so as to associate the audio data audio labels.
- the audio data can be obtained as stand-alone audio data or audio data as part of other content, e.g., extracted from video data.
- the audio data classified with the audio labels can be provided to a search system. For example, a user can search a particular sound based on one or more audio labels.
- FIG. 3 is a diagram of an example search system 300 .
- the search system 300 is configured to provide search results 370 in response to a received search query 330 .
- the search system 300 includes one or more computers programmed to receive the search query 330 from a user device 320 , retrieve a mapping 360 that relates keywords and audio labels, identify one or more keywords from one or more query terms, identify one or more audio segments responsive to the search query 330 , and provide the search results 370 identifying the audio segments.
- the search system 300 can be implemented, for example, on one or more computers including a server or on a user device.
- the search engine 340 receives the search query 330 for audio content.
- the search engine 340 receives a query from the user device 320 .
- a user 30 can input 330 search query to a search interface provided by the user device 320 .
- the received query includes one or more query terms.
- the received query is “what is the most famous birthday song.”
- the search engine 340 identifies resources that are responsive to the search query 330 .
- the search engine 340 can use an index engine that indexes resource, e.g., web pages, images, and audio data, and a ranking engine to rank the resources that match the search query 340 .
- the indexing and ranking of the resources can be performed using conventional techniques.
- the index can include index information for audio resources according to the associated audio labels.
- the search engine 340 uses the terms of the search query 330 to identify audio resources responsive to the search query. For example, the search engine 340 can match the query terms to the audio labels of the index. Matching audio labels can be ranked according to the ranking engine. One or more search results identifying audio data based on the ranking can be determined. For example, the search engine 340 can match the query terms “birthday” and “song” to the audio label “birthday song.” Then, the search engine 340 identifies the audio label “birthday song” responsive to the search query 330 . Based on the identified audio label “birthday song,” the search engine 340 identifies the audio data resources associated with the audio label “birthday song.”
- the search engine 340 provides the search results 370 , which can include search results for one or more audio resources.
- the search results 370 can be presented as one or more ranked hyperlinks directed to resources corresponding to the audio segments having the audio content responsive to the search query 330 .
- the hyperlinks are directed to specific portions of video resources that include the audio content. For example, in response to the query “what is the most famous birthday song,” the search engine 340 can provide the hyperlinks directed to specific portions of videos and a birthday song is played during the specific portions of videos.
- FIG. 4 is a flowchart of an example method 400 for generating an audio classifier.
- the method 400 is described with respect to a system that performs the method 400 .
- the system may include one or more computers.
- the system obtains 420 a group of video frames from a group of videos. Each of the video frames is associated with an image label determined based on image recognition.
- the system may receive a collection of training video files. Each video file can include multiple video frames.
- the system annotates the video frames from the video files according to a suitable image recognition technique.
- the system may associate the video frames with an image label.
- the annotated video frames include a specific object or objects
- the system can associate the video frames having the specific object or objects with the image label describing the specific object or objects. For example, if the annotated video frames include a birthday cake or a candle, the system can associate the video frames with the image label “birthday,” “cake,” or “candle.”
- the system obtains 440 a group of audio segments corresponding to the video frames. After the system obtains a group of video frames from the videos, the system obtains audio segments corresponding to the obtained video frames. Since the obtained video frames are associated with the image label, the audio segments corresponding to the obtained video frames can also be associated with the image label. In some implementations, each audio segment has a specified duration relative to the corresponding video frame.
- the system generates 460 an audio classifier trained using the audio segments and the associated image label as input. Once the system obtains audio segments and associated image labels, the system generates and trains the audio classifier using the obtained audio segments and the associated image labels as input. The audio classifier is trained to associate one or more groups of audio segments with respective one or more audio labels. After the training, the audio classifier can associate a group of audio segments with an audio label.
- FIG. 5 is a flowchart of an example method 500 for providing search results.
- the method 500 is described as being performed by a system of one or more computers.
- the system can be a search system configured to provide search results in response to a received query.
- the system receives 510 a search query for audio content.
- the search system receives a query from a user device. For example, a user can input a search query to a search interface provided by the search system.
- the received search query includes one or more query terms. For example, the received query is “what is the most famous birthday song.”
- the system identifies 530 resources that are responsive to the search query.
- the system can index resources, e.g., web pages, images, and audio data, and rank the resources that match the search query.
- the indexing and ranking of the resources can be performed using conventional techniques.
- the index can include index information for audio resources according to the associated audio labels.
- the system uses the terms of the search query to identify audio resources responsive to the search query. For example, the system can match the query terms to the audio labels of the index. Matching audio labels can be ranked according to the ranking engine. One or more search results identifying audio data based on the ranking can be determined. For example, the system can match the query terms “birthday” and “song” to the audio label “birthday song.” Then, the system identifies the audio label “birthday song” responsive to the search query. Based on the identified audio label “birthday song,” the system identifies the audio data resources associated with the audio label “birthday song.”
- the system provides 550 search results, which can include search results for one or more audio resources.
- the search results can be presented as one or more hyperlinks directed to resources corresponding to the audio segments having the audio content responsive to the search query.
- the hyperlinks can be directed to specific portions of video resources that include the audio content. For example, in response to the query “what is the most famous birthday song,” the system can provide the hyperlinks directed to specific portions of videos and a birthday song is played during the specific portions of videos.
- Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them.
- Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non transitory storage medium for execution by, or to control the operation of, data processing apparatus.
- the computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them.
- the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
- data processing apparatus refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers.
- the apparatus can also be, or further include, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).
- the apparatus can optionally include, in addition to hardware, code that creates an execution environment for computer programs, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
- a computer program which may also be referred to or described as a program, software, a software application, an app, a module, a software module, a script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- a program may, but need not, correspond to a file in a file system.
- a program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub programs, or portions of code.
- a computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
- the processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output.
- the processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA or an ASIC, or by a combination of special purpose logic circuitry and one or more programmed computers.
- Computers suitable for the execution of a computer program can be based on general or special purpose microprocessors or both, or any other kind of central processing unit.
- a central processing unit will receive instructions and data from a read only memory or a random access memory or both.
- the essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data.
- the central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.
- a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices.
- a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.
- PDA personal digital assistant
- GPS Global Positioning System
- USB universal serial bus
- Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
- semiconductor memory devices e.g., EPROM, EEPROM, and flash memory devices
- magnetic disks e.g., internal hard disks or removable disks
- magneto optical disks e.g., CD ROM and DVD-ROM disks.
- a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer.
- a display device e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor
- a keyboard and a pointing device e.g., a mouse or a trackball
- Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.
- a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to
- Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components.
- the components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (LAN) and a wide area network (WAN), e.g., the Internet.
- LAN local area network
- WAN wide area network
- the computing system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- a server transmits data, e.g., an HTML page, to a user device, e.g., for purposes of displaying data to and receiving user input from a user interacting with the device, which acts as a client.
- Data generated at the user device e.g., a result of the user interaction, can be received at the server from the device.
Abstract
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for audio classifiers. In one aspect, a method includes obtaining a plurality of video frames from a plurality of videos, wherein each of the plurality of video frames is associated with one or more image labels of a plurality of image labels determined based on image recognition; obtaining a plurality of audio segments corresponding to the plurality of video frames, wherein each audio segment has a specified duration relative to the corresponding video frame; and generating an audio classifier trained using the plurality of audio segment and the associated image labels as input, wherein the audio classifier is trained such that the one or more groups of audio segments are determined to be associated with respective one or more audio labels.
Description
This application claims priority to U.S. Provisional Application No. 62/387,297, filed on Dec. 23, 2015. The disclosure of the prior application is considered part of and is incorporated by reference in the disclosure of this application.
This specification generally relates to audio classifiers.
Digital audio data (e.g., representing speech, music, or other sounds) can be stored in one or more audio files. The audio files can include files with only audio content (e.g., music files) as well as audio files that are associated with, or part of, other files containing other content (e.g., video files with one or more audio tracks). The audio data can include speech and music as well as other categories of sound including natural sounds (e.g., rain, wind), human emotions (e.g., screams, laughter), animal vocalization (e.g., lion roar, purring cats), or other sounds (e.g., explosions, racing cars, ringing telephone).
It can be useful to classify audio data as being associated with a particular label. One conventional technique for classifying audio data is to use an audio classifier. A typical audio classifier seeks to classify a portion of input audio data as having a particular label. Conventional audio classifiers are typically trained based on a collection of human annotated training data.
In general, one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of obtaining a plurality of video frames from a plurality of videos, wherein each of the plurality of video frames is associated with one or more image labels of a plurality of image labels determined based on image recognition; obtaining a plurality of audio segments corresponding to the plurality of video frames, wherein each audio segment has a specified duration relative to the corresponding video frame; and generating an audio classifier trained using the plurality of audio segment and the associated image labels as input, wherein the audio classifier is trained such that the one or more groups of audio segments are determined to be associated with respective one or more audio labels. Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods. For a system of one or more computers to be configured to perform particular operations or actions means that the system has installed on it software, firmware, hardware, or a combination of them that in operation cause the system to perform the operations or actions. For one or more computer programs to be configured to perform particular operations or actions means that the one or more programs include instructions that, when executed by data processing apparatus, cause the apparatus to perform the operations or actions.
The foregoing and other embodiments can each optionally include one or more of the following features, alone or in combination. In particular, one embodiment includes all the following features in combination. The method includes scoring each of a collection of video frames from the plurality of videos, wherein the each video frame is scored for one or more of the plurality of image labels; determining a score of the each video frame satisfies a threshold; and selecting the plurality of video frames in response to determining a score of the each video frame satisfies the threshold. The method includes selecting video frames periodically from the plurality of video; and scoring the selected video frames for the image label. The corresponding video frame occurs during the specified duration. The one or more audio labels are determined by using an image classifier. The method includes identifying an object on video frames from the plurality of videos; and determining the image label associated with the object. The method includes evaluating the audio classifier using sample videos having known an audio label for the sample videos.
In general, one innovative aspect of the subject matter described in this specification can be embodied in methods that include the actions of receiving a query, the query including one or more query terms; using the one or more query terms to identify one or more audio resources responsive to the query, wherein each audio resource is associated with one or more audio labels generated by an audio classifier trained using audio segments extracted from video content and corresponding image labels identified from the video content; and providing search results identifying the one or more audio resources. Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods. For a system of one or more computers to be configured to perform particular operations or actions means that the system has installed on it software, firmware, hardware, or a combination of them that in operation cause the system to perform the operations or actions. For one or more computer programs to be configured to perform particular operations or actions means that the one or more programs include instructions that, when executed by data processing apparatus, cause the apparatus to perform the operations or actions.
Particular embodiments of the subject matter described in this specification can be implemented so as to realize one or more of the following advantages. A conventional audio classifier obtains annotated audio data by manual annotation of the data. Since the conventional audio classifier requires manual annotation, it is cost-ineffective and time-consuming. Compared to the conventional audio classifier, an audio classifier can obtain unannotated audio data from video content. Thus, the audio classifier can be trained using any suitable videos without need for manual annotation of the audio data. In particular, labels generated by an image classifier can be used as supervision for training the audio classifier to exploit a correlation between audio and video modalities. The audio classifier is trained using image labels identified from image detection systems that are more sophisticated than conventional automated audio annotation systems, consequently, the audio classifier can be quickly trained using a large amount of audio data.
The details of one or more embodiments of the subject matter of this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.
Like reference numbers and designations in the various drawings indicate like elements.
Audio classifiers assign one or more audio labels to input audio data. Different types of audio data may be classified including speech, music, natural sounds such as rain and wind, human emotions such as crying and laughter, animal vocalization such as roaring and purring, or other artifact sound such as explosions, racing cars and telephone ringers. Machine learning can be used to train an audio classifier designed to perform a particular classification. Image content in videos can be highly correlated with associated audio content. Using this correlation, an audio classifier can be trained using audio data extracted from videos and associated image labels such that the trained audio classifier can assign one or more audio labels to input audio data.
The trained audio classifier can be used for various applications. For example, to provide search results, responsive to a search query, that specifically identify audio files either alone or in addition to general search results. In particular, the audio files can be identified based on the audio labels associated with the audio files, which allows audio content to be returned accurately in response to a search query even.
The audio classifier generator 120 includes a video analyzing engine 122, an audio engine 124, and an audio classifier engine 126. The video analyzing engine 122 analyzes the training videos 110 to extract a collection of video frames. Each of the extracted video frames can be associated with an image label determined based, for example, on an image recognition process, which may broadly include image content analysis to identify particular image content corresponding to one or more labels.
In some implementations, the video analyzing engine 122 annotates the video frames of the training videos 110 with one or more image labels according to a suitable image recognition technique. In some other implementations, the video analyzing engine 122 receives annotated videos that have been annotated with image labels by a suitable image recognition system.
To annotate the video frames of the training videos 110, the video analyzing engine 122 may associate the video frames with one or more image labels. In some implementations, if the annotated video frames include a specific object or objects, the video analyzing engine 122 can associate the video frames having the specific object or objects with one or more image labels describing the specific object or objects. For example, if the annotated video frames include a birthday cake or a candle, the video analyzing engine 122 can associate the video frames with one or more image labels such as “birthday cake,” “birthday,” “cake,” or “candle.” Thus, the video analyzing engine 122 can obtain, from the training videos 110, a collection of video frames associated with an image label. Examples of obtaining video frames from videos are described in greater detail below.
The audio engine 124 obtains a group of audio segments corresponding to the video frames. After the video analyzing engine 122 obtains a group of video frames from the training videos 110, the audio engine 124 obtains audio segments corresponding to the obtained video frames. Since the obtained video frames are associated with the image label, the audio segments corresponding to the obtained video frames can also be associated with the image label. In some implementations, each audio segment has a specified duration relative to the corresponding video frame. Examples of obtaining audio segments corresponding to video frames are described in greater detail below with FIG. 2 .
Once the audio engine 124 obtains the audio segments and the associated image labels, the audio classifier engine 126 generates and trains the audio classifier 130 using the obtained audio segments and the associated image labels as input. The audio classifier 130 is trained to associate one or more groups of audio segments with respective one or more audio labels. After the audio classifier is trained, the audio classifier 130 can associate one or more groups of audio segments with respective audio labels.
In some implementations, the system can process the image labels to associate the audio segments with the audio labels. The system may derive audio labels from the image labels assigned by the image recognition technique. For example, if the system obtains the image label “birthday,” the system may generate audio labels such as “birthday song” and “applause,” which can be relevant to the image label “birthday.” The system then associates the image label “birthday” with the audio labels “birthday song” and “applause” by mapping the image label into the audio labels. Thus, the audio segments associated with the image label “birthday” can be associated with the audio labels “birthday song” and “applause.” Examples of training the audio classifier are described in greater detail below.
Once the audio classifier 130 is generated, the evaluator 140 can evaluate the audio classifier using a group of sample audio segments 150 having known associations with respective audio labels, for example, from manual annotation by human evaluators. Examples of evaluating the audio classifier are described in greater detail below.
Examples of Selecting Annotated Video Frames
After the image recognition associates video frames with image labels, the video analyzing engine 122 scores the recognized video frames. The video analyzing engine 122 can score each of the recognized video frames with respect to one or more identified image labels. In some implementations, the score indicates a confidence that a video frame is associated with a particular image label. The video analyzing engine 122 can score a video frame based on various criteria including color, shape, or texture of the object in the video frame.
The video analyzing engine 122 determines whether a score of each video frame satisfies a threshold. The video analyzing engine 122 can compare the scores of the recognized video frames to the threshold. As noted above, the score indicates the confidence that a video frame is associated with a particular image label. If a score of a video frame satisfies the threshold, e.g., the score exceeds a threshold score value, the video analyzing engine 122 can determine that the video frame is associated with a particular image label. If a score of a video frame does not satisfy the threshold, the video analyzing engine 122 can determine that the video frame is not associated with a particular image label.
The video analyzing engine 122 selects a group of video frames in response to determining the scores of the recognized video frames satisfy the threshold score. If the video analyzing engine 122 determines that the score of the recognized video frame satisfies the threshold, the video analyzing engine 122 selects the video frame. Then, the video analyzing engine 122 continues to obtain an audio segment for the video frame. For example, if the score of a video frame associated with the image label “birthday” satisfies the threshold, the video analyzing engine 122 selects the video frame. If the video analyzing engine 122 determines that the score of a video frame does not satisfy the threshold, the video analyzing engine 122 does not select the video frame.
Examples of Obtaining Audio Segments Corresponding to Selected Video Frames
For example, the video analyzing engine 122 may select the video frames VF1˜9 associated with the image label “birthday” from a multiple sets of videos V1˜3. From the multiple sets of videos V1˜3, the video analyzing engine 122 obtains audio segments AS1˜9 corresponding to the video frames VF1˜9. The audio segments AS1˜9 are audio data included in the multiple sets of videos V1˜V3. The audio data may include sounds corresponding to, for example, human emotions such as laughter, sounds of applause to celebrate, or sounds of singing a birthday song. The duration of the audio segments can be specified according to one or more criteria, for example, a duration likely to provide audio correlated to the image recognized in the video frame. In some implementations, the duration of each audio segment is one second. In some other implementations, the duration of each audio segment can be more or less than one second.
The audio segments AS1˜9 may have a specified duration D relative to the corresponding video frames VF1˜9. The video analyzing engine 122 can obtain the audio segments AS1˜9 relative to the corresponding video frames VF1˜9 in various ways.
In some implementations, as shown in FIG. 2A , the video analyzing engine 122 may obtain the audio segments AS1˜3 from a video of a first set of videos V1. The video analyzing engine 122 may obtain the audio segments AS1˜3 such that each of the video frames VF1˜3 occurs during each specified duration of the audio segments AS1˜3. For example, where the video frame VF1 occurs at the time T1, the time T1 occurs during the specified duration D of the audio segment AS1. The video analyzing engine 122 obtains the audio segment AS1 having the specified duration relative to the video frame VF1. For example, the audio segment AS1 can have a midpoint corresponding to the time T1. The video analyzing engine 122 may obtain additional audio segments from other videos of the first set of videos V1 such that each of video frames occurs during each specified duration of the additional audio segments.
In some other implementations, as shown in FIG. 2B , the video analyzing engine 122 may obtain the audio segments AS4˜6 from a video of a second set of videos V2. The video analyzing engine 122 may obtain the audio segments AS4˜6 such that each of the video frames VF4˜6 occurs during each specified duration of the audio segments AS4˜6. For example, where the video frame VF4 occurs at the time T4, the time T4 occurs before the specified duration D of the audio segment AS4. The video analyzing engine 122 obtains the audio segment AS4 having the specified duration relative to the video frame VF4. For example, the audio segment AS4 can have a beginning point just after the time T4. The video analyzing engine 122 may obtain additional audio segments from other videos of the second set of videos V2 such that each of video frames occurs during each specified duration of the additional audio segments.
In some other implementations, as shown in FIG. 2C , the video analyzing engine 122 may obtain the audio segments AS4˜6 from a video of a second set of videos V2. The video analyzing engine 122 may obtain the audio segments AS7˜9 such that each of the video frames VF7˜9 occurs during each specified duration of the audio segments AS7˜9. For example, where the video frame VF7 occurs at the time T7, the time T7 occurs after the specified duration D of the audio segment AS7. The video analyzing engine 122 obtains the audio segment AS7 having the specified duration relative to the video frame VF7. For example, the audio segment AS7 can have an end point just before the time T7. The video analyzing engine 122 may obtain additional audio segments from other videos of the second set of videos V2 such that each of video frames occurs during each specified duration of the additional audio segments.
Training the Audio Classifier
The audio classifier engine 126 trains the audio classifier 130. The trained audio classifier 130 is a classifier that assigns a score to input audio data for one or more audio labels. If a particular audio label score exceeds a specified threshold, then the input audio data is classified as associated with that label. Any suitable audio classifier model can be used as the audio classifier 130.
The audio classifier 130 is trained using audio segments, e.g., obtained from the audio engine 124 and associated image labels, e.g., obtained from the video analyzing engine 122. In particular, the audio segments form a training set of audio data. The corresponding image labels are used to provide supervision for the training process. The image labels can be associated with one or more audio labels. The training process is performed such that for a given audio segment of the training set of audio data the classifier identifies an audio label associated with the image label associated with the audio segment.
More specifically, the audio classifier is trained based on a number of features extracted from the audio segments. The features can be acoustic features identified, for example, from an auditory image. The auditory image can be generated, for example, from a particular acoustic model e.g., a cochlear model. The audio classifier is trained using the extracted features as variables. The training process determines optimum weights for the respective features that maximize the likelihood of correctly classifying an input audio segment as having an audio label associated with a corresponding image label for the input audio segment.
The audio classifier 130 assigns a score to each audio segment for a particular audio label. The score indicates the likelihood that an audio segment is associated with a particular audio label. If an audio classifier 130 assigns a high score to the audio segment for a particular audio label, the audio segment is strongly associated with the particular audio label. If an audio classifier 130 assigns a low score to the audio segment for a particular audio label, the audio segment is weakly associated with a particular audio label. For example, as noted above, where the image label is “birthday,” the video analyzing engine 122 may obtain the audio labels “birthday song” and “applause.” If an audio segment includes sound of singing a birthday song or sound of applause, the audio classifier 130 can assign a high score to the audio segment. In contrast, if an audio segment includes sound of a lecture about birthday cake baking, the audio classifier 130 can assign a low score to the audio segment. In some implementations, different techniques or combinations of techniques can be used to assign scores to the group of audio segments for the audio label.
If necessary, the classifier generator 120 can perform additional iterations to train the audio classifier 130. The audio classifier 130 can repeat the process of assigning scores to each audio segment.
After additional iterations, the classifier generator 120 generates the trained audio classifier 130. The system 100 may store the scores to the one or more groups of audio segments for the respective one or more audio labels.
Examples of Evaluating an Audio Classifier
In some implementations, once the audio classifier 130 is trained, the evaluator 140 can evaluate the trained audio classifier 130 using a group of sample audio segments 150 having known associations with respective sample audio labels. The sample audio segments 150 are distinct from the audio segments used for training the audio classifier 130.
The trained audio classifier 130 can classify each sample audio segment with particular audio labels. The classification of the sample audio segments with the respective audio labels can be evaluated based on the known association between the sample audio segments and the respective sample audio labels. As a result, the evaluator can evaluate the accuracy of the audio classifier 130. If the output association of the audio classifier 130 does not substantially match the known association, the audio classifier 130 can be adjusted or trained on a new set of training audio segments and associated image labels. If the output association of the audio classifier 130 substantially matches the known association, the audio classifier 130 can be applied to a larger group of audio segments to associate audio segments with respective audio labels. In some implementations, the trained audio classifier 130 can be evaluated by using suitable metrics of audio similarity.
Once the trained audio classifier 130 is evaluated, the trained audio classifier 130 can be used to process audio data so as to associate the audio data audio labels. The audio data can be obtained as stand-alone audio data or audio data as part of other content, e.g., extracted from video data. In some implementations, the audio data classified with the audio labels can be provided to a search system. For example, a user can search a particular sound based on one or more audio labels.
The search engine 340 receives the search query 330 for audio content. In some implementations, the search engine 340 receives a query from the user device 320. For example, a user 30 can input 330 search query to a search interface provided by the user device 320. The received query includes one or more query terms. For example, the received query is “what is the most famous birthday song.”
The search engine 340 identifies resources that are responsive to the search query 330. For example, the search engine 340 can use an index engine that indexes resource, e.g., web pages, images, and audio data, and a ranking engine to rank the resources that match the search query 340. The indexing and ranking of the resources can be performed using conventional techniques. The index can include index information for audio resources according to the associated audio labels.
In some implementations, the search engine 340 uses the terms of the search query 330 to identify audio resources responsive to the search query. For example, the search engine 340 can match the query terms to the audio labels of the index. Matching audio labels can be ranked according to the ranking engine. One or more search results identifying audio data based on the ranking can be determined. For example, the search engine 340 can match the query terms “birthday” and “song” to the audio label “birthday song.” Then, the search engine 340 identifies the audio label “birthday song” responsive to the search query 330. Based on the identified audio label “birthday song,” the search engine 340 identifies the audio data resources associated with the audio label “birthday song.”
The search engine 340 provides the search results 370, which can include search results for one or more audio resources. The search results 370 can be presented as one or more ranked hyperlinks directed to resources corresponding to the audio segments having the audio content responsive to the search query 330. In some implementations, the hyperlinks are directed to specific portions of video resources that include the audio content. For example, in response to the query “what is the most famous birthday song,” the search engine 340 can provide the hyperlinks directed to specific portions of videos and a birthday song is played during the specific portions of videos.
The system obtains 420 a group of video frames from a group of videos. Each of the video frames is associated with an image label determined based on image recognition. The system may receive a collection of training video files. Each video file can include multiple video frames. The system annotates the video frames from the video files according to a suitable image recognition technique. The system may associate the video frames with an image label. In some implementations, if the annotated video frames include a specific object or objects, the system can associate the video frames having the specific object or objects with the image label describing the specific object or objects. For example, if the annotated video frames include a birthday cake or a candle, the system can associate the video frames with the image label “birthday,” “cake,” or “candle.”
The system obtains 440 a group of audio segments corresponding to the video frames. After the system obtains a group of video frames from the videos, the system obtains audio segments corresponding to the obtained video frames. Since the obtained video frames are associated with the image label, the audio segments corresponding to the obtained video frames can also be associated with the image label. In some implementations, each audio segment has a specified duration relative to the corresponding video frame.
The system generates 460 an audio classifier trained using the audio segments and the associated image label as input. Once the system obtains audio segments and associated image labels, the system generates and trains the audio classifier using the obtained audio segments and the associated image labels as input. The audio classifier is trained to associate one or more groups of audio segments with respective one or more audio labels. After the training, the audio classifier can associate a group of audio segments with an audio label.
The system receives 510 a search query for audio content. In some implementations, the search system receives a query from a user device. For example, a user can input a search query to a search interface provided by the search system. The received search query includes one or more query terms. For example, the received query is “what is the most famous birthday song.”
The system identifies 530 resources that are responsive to the search query. For example, the system can index resources, e.g., web pages, images, and audio data, and rank the resources that match the search query. The indexing and ranking of the resources can be performed using conventional techniques. The index can include index information for audio resources according to the associated audio labels.
In some implementations, the system uses the terms of the search query to identify audio resources responsive to the search query. For example, the system can match the query terms to the audio labels of the index. Matching audio labels can be ranked according to the ranking engine. One or more search results identifying audio data based on the ranking can be determined. For example, the system can match the query terms “birthday” and “song” to the audio label “birthday song.” Then, the system identifies the audio label “birthday song” responsive to the search query. Based on the identified audio label “birthday song,” the system identifies the audio data resources associated with the audio label “birthday song.”
The system provides 550 search results, which can include search results for one or more audio resources. The search results can be presented as one or more hyperlinks directed to resources corresponding to the audio segments having the audio content responsive to the search query. In some implementations, the hyperlinks can be directed to specific portions of video resources that include the audio content. For example, in response to the query “what is the most famous birthday song,” the system can provide the hyperlinks directed to specific portions of videos and a birthday song is played during the specific portions of videos.
Embodiments of the subject matter and the functional operations described in this specification can be implemented in digital electronic circuitry, in tangibly-embodied computer software or firmware, in computer hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the subject matter described in this specification can be implemented as one or more computer programs, i.e., one or more modules of computer program instructions encoded on a tangible non transitory storage medium for execution by, or to control the operation of, data processing apparatus. The computer storage medium can be a machine-readable storage device, a machine-readable storage substrate, a random or serial access memory device, or a combination of one or more of them. Alternatively or in addition, the program instructions can be encoded on an artificially generated propagated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal, that is generated to encode information for transmission to suitable receiver apparatus for execution by a data processing apparatus.
The term “data processing apparatus” refers to data processing hardware and encompasses all kinds of apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can also be, or further include, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit). The apparatus can optionally include, in addition to hardware, code that creates an execution environment for computer programs, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them.
A computer program, which may also be referred to or described as a program, software, a software application, an app, a module, a software module, a script, or code, can be written in any form of programming language, including compiled or interpreted languages, or declarative or procedural languages; and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A program may, but need not, correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data, e.g., one or more scripts stored in a markup language document, in a single file dedicated to the program in question, or in multiple coordinated files, e.g., files that store one or more modules, sub programs, or portions of code. A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a data communication network.
The processes and logic flows described in this specification can be performed by one or more programmable computers executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by special purpose logic circuitry, e.g., an FPGA or an ASIC, or by a combination of special purpose logic circuitry and one or more programmed computers.
Computers suitable for the execution of a computer program can be based on general or special purpose microprocessors or both, or any other kind of central processing unit. Generally, a central processing unit will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a central processing unit for performing or executing instructions and one or more memory devices for storing instructions and data. The central processing unit and the memory can be supplemented by, or incorporated in, special purpose logic circuitry. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer can be embedded in another device, e.g., a mobile telephone, a personal digital assistant (PDA), a mobile audio or video player, a game console, a Global Positioning System (GPS) receiver, or a portable storage device, e.g., a universal serial bus (USB) flash drive, to name just a few.
Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks.
To provide for interaction with a user, embodiments of the subject matter described in this specification can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input. In addition, a computer can interact with a user by sending documents to and receiving documents from a device that is used by the user; for example, by sending web pages to a web browser on a user's device in response to requests received from the web browser.
Embodiments of the subject matter described in this specification can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface, a web browser, or an app through which a user can interact with an implementation of the subject matter described in this specification, or any combination of one or more such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (LAN) and a wide area network (WAN), e.g., the Internet.
The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other. In some embodiments, a server transmits data, e.g., an HTML page, to a user device, e.g., for purposes of displaying data to and receiving user input from a user interacting with the device, which acts as a client. Data generated at the user device, e.g., a result of the user interaction, can be received at the server from the device.
While this specification contains many specific implementation details, these should not be construed as limitations on the scope of any invention or on the scope of what may be claimed, but rather as descriptions of features that may be specific to particular embodiments of particular inventions. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable sub-combination. Moreover, although features may be described above as acting in certain combinations and even initially be claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a sub-combination or variation of a sub-combination.
Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system modules and components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.
Particular embodiments of the subject matter have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims can be performed in a different order and still achieve desirable results. As one example, the processes depicted in the accompanying figures do not necessarily require the particular order shown, or sequential order, to achieve desirable results. In some cases, multitasking and parallel processing may be advantageous.
Claims (21)
1. A computer-implemented method for generating an audio classifier that assigns audio labels to input audio segments, the method comprising:
obtaining a plurality of video frames from a plurality of videos, wherein each of the plurality of video frames is unlabeled;
generating, for each of the plurality of video frames, one or more image labels of a plurality of image labels based on image recognition and associating the one or more image labels with each of the plurality of video frames;
obtaining a plurality of unlabeled audio segments corresponding to the plurality of video frames, wherein each unlabeled audio segment has a specified duration relative to the corresponding video frame;
generating the audio classifier, wherein the audio classifier is trained using the plurality of unlabeled audio segments and the associated image labels generated from the plurality of video frames as input training data, and wherein the audio classifier is trained to assign one or more audio labels to unlabeled audio segments input to the audio classifier;
receiving a query, the query including one or more query terms;
using the one or more query terms to identify one or more audio resources responsive to the query using the audio classifier, wherein each audio resource is associated with one or more audio labels generated by the audio classifier based on the query terms; and
providing search results identifying the one or more audio resources.
2. The method of claim 1 , wherein obtaining the plurality of video frames from a plurality of videos comprising:
scoring each of a collection of video frames from the plurality of videos, wherein the each video frame is scored for one or more of the plurality of image labels;
determining a score of the each video frame satisfies a threshold; and
selecting the plurality of video frames in response to determining a score of the each video frame satisfies the threshold.
3. The method of claim 2 , wherein scoring the video frames, from the plurality of videos, for the image label comprising:
selecting video frames periodically from the plurality of video; and
scoring the selected video frames for the image label.
4. The method of claim 1 , wherein the corresponding video frame occurs during the specified duration.
5. The method of claim 1 , wherein the one or more audio labels are determined by using an image classifier.
6. The method of claim 1 , wherein obtaining a plurality of video frames from a plurality of videos further comprising:
identifying an object on video frames from the plurality of videos; and
determining the image label associated with the object.
7. The method of claim 1 , further comprising:
evaluating the audio classifier using sample videos having known an audio label for the sample videos.
8. A computer-implemented method for providing search results identifying audio resources, the method comprising:
receiving a query, the query including one or more query terms;
using the one or more query terms to identify one or more audio resources responsive to the query, wherein each audio resource is associated with one or more audio labels generated by an audio classifier, wherein the audio classifier is trained using a plurality of unlabeled audio segments extracted from a plurality of video frames that are obtained from a plurality of videos and corresponding image labels respectively generated for the plurality of video frames as input training data, and wherein the audio classifier is trained to assign one or more audio labels to unlabeled audio segments input to the audio classifier; and
providing search results identifying the one or more audio resources.
9. A system comprising:
one or more computers and one or more storage devices storing instructions that are operable, when executed by the one or more computers, to cause the one or more computers to perform operations for generating an audio classifier that assigns audio labels to input audio segments, the operations comprising:
obtaining a plurality of video frames from a plurality of videos, wherein each of the plurality of video frames is unlabeled;
generating, for each of the plurality of video frames, one or more image labels of a plurality of image labels based on image recognition and associating the one or more image labels with each of the plurality of video frames;
obtaining a plurality of unlabeled audio segments corresponding to the plurality of video frames, wherein each unlabeled audio segment has a specified duration relative to the corresponding video frame;
generating the audio classifier, wherein the audio classifier is trained using the plurality of unlabeled audio segments and the associated image labels generated from the plurality of video frames as input training data, and wherein the audio classifier is trained to assign one or more audio labels to unlabeled audio segments input to the audio classifier;
receiving a query, the query including one or more query terms;
using the one or more query terms to identify one or more audio resources responsive to the query using the audio classifier, wherein each audio resource is associated with one or more audio labels generated by the audio classifier based on the query terms; and
providing search results identifying the one or more audio resources.
10. The system of claim 9 , wherein obtaining the plurality of video frames from a plurality of videos comprising:
scoring each of a collection of video frames from the plurality of videos, wherein the each video frame is scored for one or more of the plurality of image labels;
determining a score of the each video frame satisfies a threshold; and
selecting the plurality of video frames in response to determining a score of the each video frame satisfies the threshold.
11. The system of claim 10 , wherein scoring the video frames, from the plurality of videos, for the image label comprising:
selecting video frames periodically from the plurality of video; and
scoring the selected video frames for the image label.
12. The system of claim 9 , wherein the corresponding video frame occurs during the specified duration.
13. The system of claim 9 , wherein the one or more audio labels are determined by using an image classifier.
14. The system of claim 9 , wherein obtaining a plurality of video frames from a plurality of videos further comprising:
identifying an object on video frames from the plurality of videos; and
determining the image label associated with the object.
15. The system of claim 9 , further comprising:
evaluating the audio classifier using sample videos having known an audio label for the sample videos.
16. A system comprising:
one or more computers and one or more storage devices storing instructions that are operable, when executed by the one or more computers, to cause the one or more computers to perform operations for providing search results identifying audio resources, the operations comprising:
receiving a query, the query including one or more query terms;
using the one or more query terms to identify one or more audio resources responsive to the query, wherein each audio resource is associated with one or more audio labels generated by an audio classifier, wherein the audio classifier is trained using a plurality of unlabeled audio segments extracted from a plurality of video frames that are obtained from a plurality of videos and corresponding image labels respectively generated for the plurality of video frames as input training data, and wherein the audio classifier is trained to assign one or more audio labels to unlabeled audio segments input to the audio classifier; and
providing search results identifying the one or more audio resources.
17. A non-transitory computer-readable medium storing software comprising instructions executable by one or more computers which, upon such execution, cause the one or more computers to perform operations for generating an audio classifier that assigns audio labels to input audio segments, the operations comprising:
obtaining a plurality of video frames from a plurality of videos, wherein each of the plurality of video frames is unlabeled;
generating, for each of the plurality of video frames, one or more image labels of a plurality of image labels based on image recognition and associating the one or more image labels with each of the plurality of video frames;
obtaining a plurality of unlabeled audio segments corresponding to the plurality of video frames, wherein each audio segment has a specified duration relative to the corresponding video frame;
generating the audio classifier, wherein the audio classifier is trained using the plurality of unlabeled audio segments and the associated image labels generated from the plurality of video frames as input training data, and wherein the audio classifier is trained to assign one or more audio labels to unlabeled audio segments input to the audio classifier;
receiving a query, the query including one or more query terms;
using the one or more query terms to identify one or more audio resources responsive to the query using the audio classifier, wherein each audio resource is associated with one or more audio labels generated by the audio classifier based on the query terms; and
providing search results identifying the one or more audio resources.
18. The non-transitory computer-readable medium of claim 17 , wherein the corresponding video frame occurs during the specified duration.
19. The non-transitory computer-readable medium of claim 17 , wherein the image label is determined by using an image classifier.
20. The non-transitory computer-readable medium of claim 17 , further comprising:
evaluating the audio classifier using sample videos having known an audio label for the sample videos.
21. A non-transitory computer-readable medium storing software comprising instructions executable by one or more computers which, upon such execution, cause the one or more computers to perform operations for providing search results identifying audio resources, the operations comprising:
receiving a query, the query including one or more query terms;
using the one or more query terms to identify one or more audio resources responsive to the query, wherein each audio resource is associated with one or more audio labels generated by an audio classifier, wherein the audio classifier is trained using a plurality of unlabeled audio segments extracted from a plurality of video frames that are obtained from a plurality of videos and corresponding image labels respectively generated for the plurality of video frames as input training data, and wherein the audio classifier is trained to assign one or more audio labels to unlabeled audio segments input to the audio classifier; and
providing search results identifying the one or more audio resources.
Priority Applications (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US15/041,379 US10381022B1 (en) | 2015-12-23 | 2016-02-11 | Audio classifier |
US16/520,633 US10566009B1 (en) | 2015-12-23 | 2019-07-24 | Audio classifier |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201562387297P | 2015-12-23 | 2015-12-23 | |
US15/041,379 US10381022B1 (en) | 2015-12-23 | 2016-02-11 | Audio classifier |
Related Child Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/520,633 Continuation US10566009B1 (en) | 2015-12-23 | 2019-07-24 | Audio classifier |
Publications (1)
Publication Number | Publication Date |
---|---|
US10381022B1 true US10381022B1 (en) | 2019-08-13 |
Family
ID=67543663
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US15/041,379 Active US10381022B1 (en) | 2015-12-23 | 2016-02-11 | Audio classifier |
US16/520,633 Active US10566009B1 (en) | 2015-12-23 | 2019-07-24 | Audio classifier |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US16/520,633 Active US10566009B1 (en) | 2015-12-23 | 2019-07-24 | Audio classifier |
Country Status (1)
Country | Link |
---|---|
US (2) | US10381022B1 (en) |
Cited By (16)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20190037270A1 (en) * | 2017-07-31 | 2019-01-31 | Zhilabs S.L. | Determination of qoe in encrypted video streams using supervised learning |
CN110992984A (en) * | 2019-12-02 | 2020-04-10 | 新华智云科技有限公司 | Audio processing method and device and storage medium |
CN111400511A (en) * | 2020-03-12 | 2020-07-10 | 北京奇艺世纪科技有限公司 | Multimedia resource interception method and device |
CN111816170A (en) * | 2020-07-29 | 2020-10-23 | 网易（杭州）网络有限公司 | Training of audio classification model and junk audio recognition method and device |
CN112188306A (en) * | 2020-09-23 | 2021-01-05 | 腾讯科技（深圳）有限公司 | Label generation method, device, equipment and storage medium |
CN112599152A (en) * | 2021-03-05 | 2021-04-02 | 北京智慧星光信息技术有限公司 | Voice data labeling method, system, electronic equipment and storage medium |
US10984246B2 (en) * | 2019-03-13 | 2021-04-20 | Google Llc | Gating model for video analysis |
CN113163272A (en) * | 2020-01-07 | 2021-07-23 | 海信集团有限公司 | Video editing method, computer device and storage medium |
WO2021190433A1 (en) * | 2020-03-24 | 2021-09-30 | 华为技术有限公司 | Method and device for updating object recognition model |
CN113823323A (en) * | 2021-09-30 | 2021-12-21 | 深圳万兴软件有限公司 | Audio processing method and device based on convolutional neural network and related equipment |
US11232794B2 (en) * | 2020-05-08 | 2022-01-25 | Nuance Communications, Inc. | System and method for multi-microphone automated clinical documentation |
US20220028372A1 (en) * | 2018-09-20 | 2022-01-27 | Nec Corporation | Learning device and pattern recognition device |
US20220262406A1 (en) * | 2020-08-16 | 2022-08-18 | Evolve Media AI Corporation | System and methods for autonomous synchronous rendering of media objects obtained from a plurality of media sources |
CN115662447A (en) * | 2022-09-22 | 2023-01-31 | 北京邮电大学 | Lie detection analysis method and device based on multi-feature fusion |
WO2023082830A1 (en) * | 2021-11-12 | 2023-05-19 | 腾讯科技（深圳）有限公司 | Video editing method and apparatus, computer device, and storage medium |
US11682415B2 (en) * | 2021-03-19 | 2023-06-20 | International Business Machines Corporation | Automatic video tagging |
Families Citing this family (2)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
CN111783892B (en) * | 2020-07-06 | 2021-10-01 | 广东工业大学 | Robot instruction identification method and device, electronic equipment and storage medium |
CN112256871B (en) * | 2020-10-16 | 2021-05-07 | 国网江苏省电力有限公司连云港供电分公司 | Material fulfillment system and method |
Citations (36)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6038527A (en) | 1995-07-19 | 2000-03-14 | Daimler Benz Ag | Method for generating descriptors for the classification of texts |
US6490556B2 (en) | 1999-05-28 | 2002-12-03 | Intel Corporation | Audio classifier for half duplex communication |
US20030154084A1 (en) * | 2002-02-14 | 2003-08-14 | Koninklijke Philips Electronics N.V. | Method and system for person identification using video-speech matching |
US20050125223A1 (en) * | 2003-12-05 | 2005-06-09 | Ajay Divakaran | Audio-visual highlights detection using coupled hidden markov models |
US6996268B2 (en) | 2001-12-28 | 2006-02-07 | International Business Machines Corporation | System and method for gathering, indexing, and supplying publicly available data charts |
US20060149693A1 (en) * | 2005-01-04 | 2006-07-06 | Isao Otsuka | Enhanced classification using training data refinement and classifier updating |
US7245767B2 (en) | 2003-08-21 | 2007-07-17 | Hewlett-Packard Development Company, L.P. | Method and apparatus for object identification, classification or verification |
US7295977B2 (en) | 2001-08-27 | 2007-11-13 | Nec Laboratories America, Inc. | Extracting classifying data in music from an audio bitstream |
US20080040362A1 (en) * | 2006-03-30 | 2008-02-14 | Sony France S.A. | Hybrid audio-visual categorization system and method |
US7336890B2 (en) * | 2003-02-19 | 2008-02-26 | Microsoft Corporation | Automatic detection and segmentation of music videos in an audio/video stream |
US7337115B2 (en) | 2002-07-03 | 2008-02-26 | Verizon Corporate Services Group Inc. | Systems and methods for providing acoustic classification |
US20080228749A1 (en) * | 2007-03-13 | 2008-09-18 | Microsoft Corporation | Automatic tagging of content based on a corpus of previously tagged and untagged content |
US7472063B2 (en) | 2002-12-19 | 2008-12-30 | Intel Corporation | Audio-visual feature fusion and support vector machine useful for continuous speech recognition |
US20090112864A1 (en) * | 2005-10-26 | 2009-04-30 | Cortica, Ltd. | Methods for Identifying Relevant Metadata for Multimedia Data of a Large-Scale Matching System |
US7558809B2 (en) * | 2006-01-06 | 2009-07-07 | Mitsubishi Electric Research Laboratories, Inc. | Task specific audio classification for identifying video highlights |
US7590259B2 (en) * | 1995-07-27 | 2009-09-15 | Digimarc Corporation | Deriving attributes from images, audio or video to obtain metadata |
US20100182501A1 (en) * | 2009-01-20 | 2010-07-22 | Koji Sato | Information processing apparatus, information processing method, and program |
US20110047163A1 (en) * | 2009-08-24 | 2011-02-24 | Google Inc. | Relevance-Based Image Selection |
US20110075851A1 (en) * | 2009-09-28 | 2011-03-31 | Leboeuf Jay | Automatic labeling and control of audio algorithms by audio recognition |
US20120039539A1 (en) * | 2010-03-08 | 2012-02-16 | Oren Boiman | Method and system for classifying one or more images |
WO2012129255A2 (en) | 2011-03-21 | 2012-09-27 | The Intellisis Corporation | Systems and methods for segmenting and/or classifying an audio signal from transformed audio information |
US20120323925A1 (en) * | 2011-06-17 | 2012-12-20 | Fitzsimmons Jeffrey E | System and Method for Synchronously Generating An Index to a Media Stream |
US20130089304A1 (en) * | 2011-10-10 | 2013-04-11 | Wei Jiang | Video concept classification using video similarity scores |
US8452778B1 (en) * | 2009-11-19 | 2013-05-28 | Google Inc. | Training of adapted classifiers for video categorization |
US8463719B2 (en) | 2009-03-11 | 2013-06-11 | Google Inc. | Audio classification for information retrieval using sparse features |
US8566329B1 (en) * | 2011-06-27 | 2013-10-22 | Amazon Technologies, Inc. | Automated tag suggestions |
US20130339021A1 (en) * | 2012-06-19 | 2013-12-19 | International Business Machines Corporation | Intent Discovery in Audio or Text-Based Conversation |
US8682132B2 (en) * | 2006-05-11 | 2014-03-25 | Mitsubishi Electric Corporation | Method and device for detecting music segment, and method and device for recording data |
US8682654B2 (en) * | 2006-04-25 | 2014-03-25 | Cyberlink Corp. | Systems and methods for classifying sports video |
US8856051B1 (en) * | 2011-04-08 | 2014-10-07 | Google Inc. | Augmenting metadata of digital objects |
US8855796B2 (en) * | 2005-12-27 | 2014-10-07 | Mitsubishi Electric Corporation | Method and device for detecting music segment, and method and device for recording data |
US8938393B2 (en) * | 2011-06-28 | 2015-01-20 | Sony Corporation | Extended videolens media engine for audio recognition |
US9075825B2 (en) * | 2011-09-26 | 2015-07-07 | The University Of Kansas | System and methods of integrating visual features with textual features for image searching |
US9098533B2 (en) * | 2011-10-03 | 2015-08-04 | Microsoft Technology Licensing, Llc | Voice directed context sensitive visual search |
US9292552B2 (en) * | 2012-07-26 | 2016-03-22 | Telefonaktiebolaget L M Ericsson (Publ) | Apparatus, methods, and computer program products for adaptive multimedia content indexing |
EP3096243A1 (en) * | 2015-05-22 | 2016-11-23 | Thomson Licensing | Methods, systems and apparatus for automatic video query expansion |
Family Cites Families (1)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8205237B2 (en) * | 2000-09-14 | 2012-06-19 | Cox Ingemar J | Identifying works, using a sub-linear time search, such as an approximate nearest neighbor search, for initiating a work-based action, such as an action on the internet |
-
2016
- 2016-02-11 US US15/041,379 patent/US10381022B1/en active Active
-
2019
- 2019-07-24 US US16/520,633 patent/US10566009B1/en active Active
Patent Citations (38)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6038527A (en) | 1995-07-19 | 2000-03-14 | Daimler Benz Ag | Method for generating descriptors for the classification of texts |
US7590259B2 (en) * | 1995-07-27 | 2009-09-15 | Digimarc Corporation | Deriving attributes from images, audio or video to obtain metadata |
US6490556B2 (en) | 1999-05-28 | 2002-12-03 | Intel Corporation | Audio classifier for half duplex communication |
US7295977B2 (en) | 2001-08-27 | 2007-11-13 | Nec Laboratories America, Inc. | Extracting classifying data in music from an audio bitstream |
US6996268B2 (en) | 2001-12-28 | 2006-02-07 | International Business Machines Corporation | System and method for gathering, indexing, and supplying publicly available data charts |
US20030154084A1 (en) * | 2002-02-14 | 2003-08-14 | Koninklijke Philips Electronics N.V. | Method and system for person identification using video-speech matching |
US7337115B2 (en) | 2002-07-03 | 2008-02-26 | Verizon Corporate Services Group Inc. | Systems and methods for providing acoustic classification |
US7472063B2 (en) | 2002-12-19 | 2008-12-30 | Intel Corporation | Audio-visual feature fusion and support vector machine useful for continuous speech recognition |
US7336890B2 (en) * | 2003-02-19 | 2008-02-26 | Microsoft Corporation | Automatic detection and segmentation of music videos in an audio/video stream |
US7245767B2 (en) | 2003-08-21 | 2007-07-17 | Hewlett-Packard Development Company, L.P. | Method and apparatus for object identification, classification or verification |
US20050125223A1 (en) * | 2003-12-05 | 2005-06-09 | Ajay Divakaran | Audio-visual highlights detection using coupled hidden markov models |
US20060149693A1 (en) * | 2005-01-04 | 2006-07-06 | Isao Otsuka | Enhanced classification using training data refinement and classifier updating |
US20090112864A1 (en) * | 2005-10-26 | 2009-04-30 | Cortica, Ltd. | Methods for Identifying Relevant Metadata for Multimedia Data of a Large-Scale Matching System |
US8855796B2 (en) * | 2005-12-27 | 2014-10-07 | Mitsubishi Electric Corporation | Method and device for detecting music segment, and method and device for recording data |
US7558809B2 (en) * | 2006-01-06 | 2009-07-07 | Mitsubishi Electric Research Laboratories, Inc. | Task specific audio classification for identifying video highlights |
US8392414B2 (en) * | 2006-03-30 | 2013-03-05 | Sony France S.A. | Hybrid audio-visual categorization system and method |
US20080040362A1 (en) * | 2006-03-30 | 2008-02-14 | Sony France S.A. | Hybrid audio-visual categorization system and method |
US8682654B2 (en) * | 2006-04-25 | 2014-03-25 | Cyberlink Corp. | Systems and methods for classifying sports video |
US8682132B2 (en) * | 2006-05-11 | 2014-03-25 | Mitsubishi Electric Corporation | Method and device for detecting music segment, and method and device for recording data |
US8103646B2 (en) * | 2007-03-13 | 2012-01-24 | Microsoft Corporation | Automatic tagging of content based on a corpus of previously tagged and untagged content |
US20080228749A1 (en) * | 2007-03-13 | 2008-09-18 | Microsoft Corporation | Automatic tagging of content based on a corpus of previously tagged and untagged content |
US20100182501A1 (en) * | 2009-01-20 | 2010-07-22 | Koji Sato | Information processing apparatus, information processing method, and program |
US8463719B2 (en) | 2009-03-11 | 2013-06-11 | Google Inc. | Audio classification for information retrieval using sparse features |
US20110047163A1 (en) * | 2009-08-24 | 2011-02-24 | Google Inc. | Relevance-Based Image Selection |
US20110075851A1 (en) * | 2009-09-28 | 2011-03-31 | Leboeuf Jay | Automatic labeling and control of audio algorithms by audio recognition |
US8452778B1 (en) * | 2009-11-19 | 2013-05-28 | Google Inc. | Training of adapted classifiers for video categorization |
US20120039539A1 (en) * | 2010-03-08 | 2012-02-16 | Oren Boiman | Method and system for classifying one or more images |
WO2012129255A2 (en) | 2011-03-21 | 2012-09-27 | The Intellisis Corporation | Systems and methods for segmenting and/or classifying an audio signal from transformed audio information |
US8856051B1 (en) * | 2011-04-08 | 2014-10-07 | Google Inc. | Augmenting metadata of digital objects |
US20120323925A1 (en) * | 2011-06-17 | 2012-12-20 | Fitzsimmons Jeffrey E | System and Method for Synchronously Generating An Index to a Media Stream |
US8566329B1 (en) * | 2011-06-27 | 2013-10-22 | Amazon Technologies, Inc. | Automated tag suggestions |
US8938393B2 (en) * | 2011-06-28 | 2015-01-20 | Sony Corporation | Extended videolens media engine for audio recognition |
US9075825B2 (en) * | 2011-09-26 | 2015-07-07 | The University Of Kansas | System and methods of integrating visual features with textual features for image searching |
US9098533B2 (en) * | 2011-10-03 | 2015-08-04 | Microsoft Technology Licensing, Llc | Voice directed context sensitive visual search |
US20130089304A1 (en) * | 2011-10-10 | 2013-04-11 | Wei Jiang | Video concept classification using video similarity scores |
US20130339021A1 (en) * | 2012-06-19 | 2013-12-19 | International Business Machines Corporation | Intent Discovery in Audio or Text-Based Conversation |
US9292552B2 (en) * | 2012-07-26 | 2016-03-22 | Telefonaktiebolaget L M Ericsson (Publ) | Apparatus, methods, and computer program products for adaptive multimedia content indexing |
EP3096243A1 (en) * | 2015-05-22 | 2016-11-23 | Thomson Licensing | Methods, systems and apparatus for automatic video query expansion |
Non-Patent Citations (1)
Title |
---|
Glasberg, Ronald, et al. "New real-time approaches for video-genre-classification using high-level descriptors and a set of classifiers." Semantic Computing, 2008 IEEE International Conference on. IEEE, 2008. * |
Cited By (30)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US11234048B2 (en) * | 2017-07-31 | 2022-01-25 | Zhilabs S.L. | Determination of QOE in encrypted video streams using supervised learning |
US20190037270A1 (en) * | 2017-07-31 | 2019-01-31 | Zhilabs S.L. | Determination of qoe in encrypted video streams using supervised learning |
US20220028372A1 (en) * | 2018-09-20 | 2022-01-27 | Nec Corporation | Learning device and pattern recognition device |
US11948554B2 (en) * | 2018-09-20 | 2024-04-02 | Nec Corporation | Learning device and pattern recognition device |
US11587319B2 (en) | 2019-03-13 | 2023-02-21 | Google Llc | Gating model for video analysis |
US10984246B2 (en) * | 2019-03-13 | 2021-04-20 | Google Llc | Gating model for video analysis |
CN110992984A (en) * | 2019-12-02 | 2020-04-10 | 新华智云科技有限公司 | Audio processing method and device and storage medium |
CN113163272A (en) * | 2020-01-07 | 2021-07-23 | 海信集团有限公司 | Video editing method, computer device and storage medium |
CN111400511A (en) * | 2020-03-12 | 2020-07-10 | 北京奇艺世纪科技有限公司 | Multimedia resource interception method and device |
WO2021190433A1 (en) * | 2020-03-24 | 2021-09-30 | 华为技术有限公司 | Method and device for updating object recognition model |
US11670298B2 (en) | 2020-05-08 | 2023-06-06 | Nuance Communications, Inc. | System and method for data augmentation for multi-microphone signal processing |
US11837228B2 (en) | 2020-05-08 | 2023-12-05 | Nuance Communications, Inc. | System and method for data augmentation for multi-microphone signal processing |
US11232794B2 (en) * | 2020-05-08 | 2022-01-25 | Nuance Communications, Inc. | System and method for multi-microphone automated clinical documentation |
US11631411B2 (en) | 2020-05-08 | 2023-04-18 | Nuance Communications, Inc. | System and method for multi-microphone automated clinical documentation |
US11335344B2 (en) | 2020-05-08 | 2022-05-17 | Nuance Communications, Inc. | System and method for multi-microphone automated clinical documentation |
US11676598B2 (en) | 2020-05-08 | 2023-06-13 | Nuance Communications, Inc. | System and method for data augmentation for multi-microphone signal processing |
US11699440B2 (en) | 2020-05-08 | 2023-07-11 | Nuance Communications, Inc. | System and method for data augmentation for multi-microphone signal processing |
CN111816170B (en) * | 2020-07-29 | 2024-01-19 | 杭州网易智企科技有限公司 | Training of audio classification model and garbage audio recognition method and device |
CN111816170A (en) * | 2020-07-29 | 2020-10-23 | 网易（杭州）网络有限公司 | Training of audio classification model and junk audio recognition method and device |
US11600302B2 (en) * | 2020-08-16 | 2023-03-07 | Evolve Media AI Corporation | System and methods for autonomous synchronous rendering of media objects obtained from a plurality of media sources |
US20220262406A1 (en) * | 2020-08-16 | 2022-08-18 | Evolve Media AI Corporation | System and methods for autonomous synchronous rendering of media objects obtained from a plurality of media sources |
CN112188306B (en) * | 2020-09-23 | 2022-06-21 | 腾讯科技（深圳）有限公司 | Label generation method, device, equipment and storage medium |
CN112188306A (en) * | 2020-09-23 | 2021-01-05 | 腾讯科技（深圳）有限公司 | Label generation method, device, equipment and storage medium |
CN112599152A (en) * | 2021-03-05 | 2021-04-02 | 北京智慧星光信息技术有限公司 | Voice data labeling method, system, electronic equipment and storage medium |
US11682415B2 (en) * | 2021-03-19 | 2023-06-20 | International Business Machines Corporation | Automatic video tagging |
CN113823323A (en) * | 2021-09-30 | 2021-12-21 | 深圳万兴软件有限公司 | Audio processing method and device based on convolutional neural network and related equipment |
CN113823323B (en) * | 2021-09-30 | 2023-12-12 | 深圳万兴软件有限公司 | Audio processing method and device based on convolutional neural network and related equipment |
WO2023082830A1 (en) * | 2021-11-12 | 2023-05-19 | 腾讯科技（深圳）有限公司 | Video editing method and apparatus, computer device, and storage medium |
CN115662447B (en) * | 2022-09-22 | 2023-04-07 | 北京邮电大学 | Lie detection analysis method and device based on multi-feature fusion |
CN115662447A (en) * | 2022-09-22 | 2023-01-31 | 北京邮电大学 | Lie detection analysis method and device based on multi-feature fusion |
Also Published As
Publication number | Publication date |
---|---|
US10566009B1 (en) | 2020-02-18 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US10566009B1 (en) | Audio classifier | |
US11853879B2 (en) | Generating vector representations of documents | |
US20230306057A1 (en) | Relevance-Based Image Selection | |
US11960519B2 (en) | Classifying data objects | |
US20220342926A1 (en) | User interface for context labeling of multimedia items | |
US10430689B2 (en) | Training a classifier algorithm used for automatically generating tags to be applied to images | |
US9727584B2 (en) | Refining image annotations | |
US9514405B2 (en) | Scoring concept terms using a deep network | |
US9818409B2 (en) | Context-dependent modeling of phonemes | |
US9852363B1 (en) | Generating labeled images | |
US9177046B2 (en) | Refining image relevance models | |
CN106462640B (en) | Contextual search of multimedia content | |
CN110737783B (en) | Method and device for recommending multimedia content and computing equipment | |
US20180336193A1 (en) | Artificial Intelligence Based Method and Apparatus for Generating Article | |
JP2018517959A (en) | Selecting a representative video frame for the video | |
US11531928B2 (en) | Machine learning for associating skills with content | |
US11080560B2 (en) | Low-shot learning from imaginary 3D model | |
CN111125422A (en) | Image classification method and device, electronic equipment and storage medium | |
US10108707B1 (en) | Data ingestion pipeline | |
Mazloom et al. | Tagbook: A semantic video representation without supervision for event detection | |
US9218366B1 (en) | Query image model | |
US20210117736A1 (en) | Automatic generation of content using multimedia | |
CN108959531A (en) | Information search method, device, equipment and storage medium | |
Blanchard et al. | Getting the subtext without the text: Scalable multimodal sentiment classification from visual and acoustic modalities | |
KR20210062522A (en) | Control method, device and program of user participation keyword selection system |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |