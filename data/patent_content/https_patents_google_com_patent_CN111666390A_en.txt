CN111666390A - Saving and retrieving locations of objects - Google Patents
Saving and retrieving locations of objects Download PDFInfo
- Publication number
- CN111666390A CN111666390A CN202010348783.8A CN202010348783A CN111666390A CN 111666390 A CN111666390 A CN 111666390A CN 202010348783 A CN202010348783 A CN 202010348783A CN 111666390 A CN111666390 A CN 111666390A
- Authority
- CN
- China
- Prior art keywords
- location
- physical object
- voice query
- query
- user
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Granted
Links
- 238000000034 method Methods 0.000 claims abstract description 89
- 230000004044 response Effects 0.000 claims description 47
- 238000013507 mapping Methods 0.000 claims description 3
- 230000008569 process Effects 0.000 description 54
- 230000015654 memory Effects 0.000 description 34
- 230000009471 action Effects 0.000 description 26
- 238000004891 communication Methods 0.000 description 21
- 241000867614 Mimus polyglottos Species 0.000 description 18
- 239000011435 rock Substances 0.000 description 13
- 238000004590 computer program Methods 0.000 description 9
- 238000012549 training Methods 0.000 description 8
- 238000012545 processing Methods 0.000 description 6
- 230000008520 organization Effects 0.000 description 5
- 230000001960 triggered effect Effects 0.000 description 5
- 238000010586 diagram Methods 0.000 description 4
- 230000003993 interaction Effects 0.000 description 4
- 150000003839 salts Chemical class 0.000 description 4
- 230000003287 optical effect Effects 0.000 description 3
- 230000001413 cellular effect Effects 0.000 description 2
- 239000002131 composite material Substances 0.000 description 2
- 238000005516 engineering process Methods 0.000 description 2
- 230000006870 function Effects 0.000 description 2
- 239000004973 liquid crystal related substance Substances 0.000 description 2
- CXKWCBBOMKCUKX-UHFFFAOYSA-M methylene blue Chemical compound [Cl-].C1=CC(N(C)C)=CC2=[S+]C3=CC(N(C)C)=CC=C3N=C21 CXKWCBBOMKCUKX-UHFFFAOYSA-M 0.000 description 2
- 230000000644 propagated effect Effects 0.000 description 2
- 239000004575 stone Substances 0.000 description 2
- 238000013518 transcription Methods 0.000 description 2
- 230000035897 transcription Effects 0.000 description 2
- XLYOFNOQVPJJNP-UHFFFAOYSA-N water Substances O XLYOFNOQVPJJNP-UHFFFAOYSA-N 0.000 description 2
- 241000287227 Fringillidae Species 0.000 description 1
- 241000289619 Macropodidae Species 0.000 description 1
- 230000000386 athletic effect Effects 0.000 description 1
- 230000004888 barrier function Effects 0.000 description 1
- 230000008901 benefit Effects 0.000 description 1
- 230000015572 biosynthetic process Effects 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 230000009193 crawling Effects 0.000 description 1
- 238000000605 extraction Methods 0.000 description 1
- 238000010295 mobile communication Methods 0.000 description 1
- 238000012986 modification Methods 0.000 description 1
- 230000004048 modification Effects 0.000 description 1
- 239000007787 solid Substances 0.000 description 1
- 239000013589 supplement Substances 0.000 description 1
- 238000003786 synthesis reaction Methods 0.000 description 1
- 239000010409 thin film Substances 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
- 230000001755 vocal effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/3331—Query processing
- G06F16/334—Query execution
- G06F16/3344—Query execution using natural language analysis
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/30—Information retrieval; Database structures therefor; File system structures therefor of unstructured textual data
- G06F16/33—Querying
- G06F16/332—Query formulation
- G06F16/3329—Natural language query formulation or dialogue systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/903—Querying
- G06F16/9032—Query formulation
- G06F16/90332—Natural language query formulation or dialogue systems
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F16/00—Information retrieval; Database structures therefor; File system structures therefor
- G06F16/90—Details of database functions independent of the retrieved data types
- G06F16/903—Querying
- G06F16/90335—Query processing
Abstract
The invention relates to saving and retrieving a location of an object. Described herein, among other things, is a computer-implemented method for storing and retrieving information related to a location of an object. The method may include receiving a first query including one or more terms identifying an object. The first query can be determined to include a command to store location information of the object. The first query can be parsed to determine identification information for the object and a location for the object can be determined. The method further includes identifying one or more attributes of the object that are not specified in the first query, and causing a first set of data characterizing the identification information of the object, the location of the object, and the one or more attributes of the object to be stored.
Description
Description of the cases
The present application belongs to the divisional application of Chinese patent application No.201580035581.5, which has an application date of 2015, 6-month and 17-day.
Technical Field
This document relates generally to information storage and retrieval.
Background
In daily life, people encounter many different objects. Objects must be kept track of routinely so that they can be found after a period of non-use. Some objects are frequently and routinely used so that a user is generally able to recall where the object is conventionally located. For example, a user may charge her phone on a bedside table every night, so that the phone can be expected to be on the bedside table the next morning. Similarly, a user may place a car key in a tray near the garage each day when going home from work so that the key can be easily located in the same place the next morning. However, sometimes the convention is broken and the object is left in an unusual location or lent to others for a period of time. Other objects are not frequently used but must be easily found (and some even deliberately hidden) when needed, such as spare house keys, birth certificates or vehicle certificates that are hidden.
Disclosure of Invention
Technologies are generally described herein for saving and obtaining information about a location of an object. In some implementations, a user can instruct an application or service on a computing device to store information about the location of an object so that the location can be easily invoked upon subsequent requests. For example, a user can provide a query indicating that George is browsing her rock collection, in response to which an entry can be created in a data structure for the user that captures the location of the rock collection as being with George. After a few months, the user may find a new stone for her collection, but cannot remember who she loaned the collection or when it was loaned. To help her recall, she may submit another query requesting the location of the rock collection, to which the application or service responds based on an entry in the data structure. The user can also indicate when the object position has changed, thereby causing the position information of the object to be updated in the data structure. Thus, when George returns the rock collection to the user, the return can be recorded in the data structure such that the current location of the object is no longer represented as being with George. By maintaining the location of various objects in a data structure, complex actions can be performed in response to user queries. For example, a notification can be triggered when an object moves from a particular location. In some embodiments, object location entries are stored in a data structure in a manner that allows relationships to be defined between one or more objects and locations. For example, common objects or locations that are mentioned in a series of queries can be identified as the same objects or locations that were referenced previously, so that information about the objects or locations can be reliably accessed or updated. Further, a representation of an object or location in the data structure can be automatically associated with one or more attributes. The object or location can thus be identified by the user based on the attribute even if the object or location is not mentioned directly in the query.
The techniques described herein may provide one or more of the following advantages. Using a simple natural language query, a user can save, update, and obtain information about the location of an object, such as information indicating where the object was last placed, or information identifying a person, organization, or other entity possessing the object. In some implementations, a user can input an object location query using voice commands, which are then processed based on a particular set of grammars that have been trained for the sample object location query. The grammar can be used in a model that probabilistically determines the particular type of language command to which a query corresponds, without requiring the user to speak the command using a particular term or a particular language construct. Further, by associating the stored objects and locations with one or more attributes (i.e., concepts determined to be sufficiently related to the objects and locations), the user can then refer to the objects and locations indirectly based on their attributes rather than directly based on the names of the objects or locations. Thus, information about an object or location can be requested or updated with a greater degree of flexibility than would be the case if strict reference to the language originally used to store the object information were required. For example, object location information may first be stored in response to a user query of "The rock collection with George". Subsequently, the user may want to know where the rock collections are, but only use a short reference to the rock collections, such as "rocks" or "stores". A short reference to "rocks" may be sufficient to identify the object "rock collections" by string comparison, and "stocks" is a synonym associated with a rock collection that is also identified as sufficiently closely related to the rock collection. Furthermore, attributes related to the rock collection and/or related to George, such as his full name and address, can be determined. For example, using such attributes, a user can query any object located at George's address without mentioning George's own name. Also, for example, books may be mentioned by their authors, DVD movies may be mentioned by a protagonist or a protagonist, and music albums may be mentioned by their artists or genres. Each of these examples refers to the object indirectly rather than directly to the name of the object itself. In addition, responses may be generated for more complex queries by maintaining relationships between objects and locations in a data structure. For example, if object a is left at location X at a first time and object B is left at location X at a second time, location X can be associated with both objects so that the user can obtain a response to a query that simultaneously asks all objects that have been left at a particular location.
In some implementations, a computer-implemented method includes receiving a first query that includes one or more terms that identify an object. The first query can be determined to include a command to store location information of the object. The first query can be parsed to determine identification information for the object and a location for the object can be determined. The method further includes identifying one or more attributes of the object that are not specified in the first query, and causing a first set of data characterizing the identification information of the object, the location of the object, and the one or more attributes of the object to be stored.
These and other embodiments may include one or more of the following features. The method may further include determining that the object corresponds to one or more entities in a data structure of a plurality of entities, wherein the data structure includes representations of the plurality of entities and maps relationships between particular ones of the plurality of entities. Identifying one or more attributes of the object that are not specified in the first query may include selecting one or more facts or categories associated with one or more entities in the data structure to which the object is determined to correspond.
The method may further include causing the first set of data for the object to be stored in a personal object data structure associated with the particular user account, wherein the personal object data structure: (i) includes representations of one or more objects and one or more locations, and (ii) maps relationships between the one or more objects and particular ones of the one or more locations.
The data structure is capable of mapping the relationship to indicate at least a last known location of a particular plurality of the one or more objects.
The first query may further include one or more terms that identify a location of the object. The one or more terms identifying the location of the object may include one or more terms specifying a geographic location of the object.
The first query may further include one or more terms that identify a person or organization to which the object is left. The method may further include associating the person or organization to which the object has been left with the contact from the user account; detecting user interaction with contacts associated with a person or organization to which the object has been left; and causing a notification about the object to be presented to the user in response to detecting the user's interaction with the contact.
The method may further include determining that the user device is located within a threshold distance of the location of the object, and in response, causing a notification about the object to be presented to the user.
The first query may not include information identifying a location of an object in the content of the first query, and the location of the object may be determined based on a current location of the computing device at which the first query was originally entered or spoken.
The method may further include receiving a second query that includes one or more terms that identify at least one of the one or more attributes of the object. In response to receiving the second query, the method may further include determining that the second query references the object identified in the first query based on the one or more terms identifying at least one of the one or more attributes of the object; determining that the second query includes a command to perform an action associated with the object; and performing an action associated with the object.
The second query may include a command to obtain a location of the object. Performing the action associated with the object may include identifying a location of the object from the first set of data and outputting the location of the object.
The second query can include a command to update information about the object. Performing the action associated with the object may include causing the updated information to be stored in the first set of data or other data.
The command to update information about the object may include an indication that the object is located at a new location. Causing the updated information to be stored in the first set of data or other data may include updating the location information stored for the object to specify a new location for the object.
The first query can specify that the user has left the object in the location of the object. The second query can specify that the object has been returned to the user. Performing the action associated with the object in response to receiving the second query may include causing information to be stored that indicates that the object is no longer located at the location of the object identified in the first query.
The second query can indicate that the object is no longer located at the location of the object identified in the first query. Performing the action associated with the object in response to receiving the second query may include causing information to be stored that indicates that the object is no longer located at the location of the object identified in the first query. The method may further include causing a notification to be presented to a user that includes a reminder regarding one or more events associated with the object in response to determining that the object is no longer located at the location of the object identified in the first query.
The one or more terms in the first query that identify the location of the object can indicate that the object has been left to one or more first persons. The second query can indicate that the object has been returned from the one or more first persons, and the reminder regarding the one or more events associated with the object can include a reminder to make the object available to the one or more persons.
The method may further include identifying one or more attributes of the location of the object that are not specified in the first query. The second query may include a determination to execute a command associated with the object further based on one or more attributes of the location of the object.
The second query may not include any one or more terms in the first query that identify the object.
In some implementations, one or more computer-readable devices having instructions stored thereon are provided. When executed by one or more processors, the instructions are capable of causing operations to be performed comprising: receiving a query including one or more terms identifying an object; determining that the query includes a command to store location information for the object; parsing the query to determine identifying information for the object and to determine a location of the object; one or more attributes of the object that are not specified in the query are identified, and a first set of data characterizing the identification information of the object, the location of the object, and the one or more attributes of the object is caused to be stored.
In some implementations, a computer-implemented method can include: receiving, by a computing device, a first query comprising a command to store information specifying a location of an object; causing data identifying the object and the location of the object to be stored; receiving, by the computing device, a second query comprising a command to obtain a location of the object, wherein the second query identifies the object by one or more terms indicating one or more properties of the object that are not specified in the first query; in response to receiving the second query: identifying the object from a plurality of objects using one or more attributes of the object and obtaining a location of the object from the data set; and outputting information identifying the location of the object for presentation to a user.
Drawings
FIG. 1 is a schematic diagram of an example process for saving and obtaining information about the location of an object;
FIG. 2A is an example graphical representation of a data structure of an entity in which a particular plurality of entities are interconnected;
FIG. 2B depicts an example portion of a data structure of interconnected entities;
FIG. 3 shows a flow diagram of an example process for saving and obtaining information about the location of an object;
FIG. 4 illustrates an example of a system for managing and reporting information about the location of objects;
FIG. 5 illustrates an example of a computing device 500 and a mobile computing device that can be used to implement the techniques described herein.
Like reference symbols in the various drawings indicate like elements.
Detailed Description
FIG. 1 is a schematic diagram of an example process 100 for saving and obtaining information about the location of an object. In some implementations, the process 100 can be implemented by the system 400 depicted in fig. 4 below. In general, the process 100 allows a user to store information indicating the location of one or more objects using an unstructured natural language query. Process 100 can build a personal object data structure that maps relationships between objects and their locations. The personal object data structure can be used to respond to user queries that relate objects and locations indirectly or otherwise in a different manner in subsequent queries. For example, based on a particular path through the personal object data structure, one object may be identified in a query based on its relationship to another object; objects can be associated with a hierarchy of locations (e.g., shoes can be located directly in a closet, which can be located in a guest room of a Sue house); objects can be associated by location; and an object location history can be determined. Further, objects and locations in the personal object data structure can be associated with attributes representing concepts related to the respective objects and locations. As described further below, the process 100 allows a user to identify an object or location along one or more dimensions, such as by directly using the name of the object or location, a partial or short reference to the name of the object or location, and by indirectly referencing one or more determined attributes of the object or location. Thus, the user is not required to remember the particular terms originally used to describe the object and its location, but is able to subsequently identify previously stored objects or locations using a flexible vocabulary having a broader term basis describing the object or location.
In response to receiving query 106, process 100 adds an entry to the user's personal object data structure 102 to indicate that the object "keys" has been placed in the front pocket of the backpack. An example representation of a personal object data structure 102 is shown in FIG. 1. The personal object data structure 102 is represented graphically herein as a graph that includes nodes for respective objects and locations and a graph that links the nodes and establishes relationships between the objects and the locations. In some implementations, the location mentioned in the query 106 can be recorded in the personal object data structure in such a way that the location is identified in the query 106. For example, the string "front pocket of backpack (the backpack)" may be stored as the location of the key. In some implementations, the location of the name mentioned may be divided into its component parts and split, but a relative location is created for each part in the data structure 102. Here, the location in query 106 specifies a location hierarchy. The "backpack" is the first position that can be generally identified. However, the backpack also has subassemblies including various barriers such as a "front pocket". The "front pocket" is thus an individually identifiable element of the data structure that is at the same level as the "backpack". Thus, the backpack node 102a is connected to the front pocket node 102b, which in turn is connected to the key node 102c to indicate that the key 102c is currently in the front pocket of the backpack. If the backpack was previously represented in data structure 102, an additional or otherwise updated connection is made to the representation of the backpack to reflect that the key was added to the backpack. Thus, the new query can cause the O & L engine to make changes and build to the data structure 102 if it existed in advance. If the data structure did not previously exist, it may be created.
At stage B (112), the user 104 provides a second query 110. The second query indicates that the user's "Peyton Manning gym in the backpack" is in the backpack. Additional entries can be made for the personal object data structure to reflect that additional objects are located in the backpack. Thus, a jersey node 102d is added and connected to the backpack node 102 a.
At stage C (114), the process 100 identifies one or more attributes determined to be relevant to the sportswear object 102d mentioned in the query 110. The attributes are associated with the object so that they can be subsequently used to identify the object in response to future user queries that describe the jersey in a manner different from the manner in which the jersey was described in query 110. In some implementations, the process 100 can tag the object or location with additional keywords for the determined attributes. For example, the sports apparel referred to in query 110 is labeled with keywords 118 "quaterback", "NFL", "Colts", "Broncos", "Indianapolis", and "Denver". Keywords 118 can be determined from one or more knowledge sources 116. In some implementations, the knowledge sources 116 can include or point to electronic resources, such as web pages and other electronic documents whose content discusses the object or location to which the query pertains. In some implementations, knowledge sources 116 may include auxiliary (cured) data structures that include information about real entities including various people, places, things, and ideas. Attributes (e.g., facts, topics, categories) of the entity can be organized in a data structure, and attributes in an external entity data structure of the identified entity can be associated with an object or location in the query. For example, professional quarterback Peyton Manning can be identified as a highly relevant entity for the "Peyton Manning sportswear" object referred to in query 110. Keywords 118 are attributes associated with the Peyton Manning entity and therefore can also be associated with the sportswear object in the personal object data structure 102.
At stage D (120), the user 104 submits an object-location retrieval command in the query 120 asking "where did i place the Colts jersey? (Where did I leave my Colts jersey). While the user 104 wants to know the location of his Peyton Manning jersey because the object was previously described in the query 110, the subsequently submitted query 120 uses a different description of the same object, "Colts jersey. However, at stage E (124), process 100 correctly determines that query 120 is most likely to involve "Peyton Manning gym" previously mentioned in query 110 and represented as node 102d in the personal object data structure. The association between the corresponding references for "Colts sports wear" and "Peyton Manning sports wear" is determined based on the keywords of the attributes of the sports wear identified at stage C (114). The process 100 identifies that Peyton management has a strong association with "Colts" and, therefore, the term is tagged to the representation of the athletic garment 102 d. A match can be determined between the search query 120 and the attributes of the sports garment representation 102 d.
At stage F (130), the user 104 submits a fourth query 128 "I have retrieved my key". The query 128 constitutes a command indicating the object-completion type to which the key has been returned. Process 100 can recognize from the context (e.g., structure and terms) of query 120 that query 128 is an object-complete type of command, and in response update personal object data structure 102 at stage G (132) to reflect that the key is no longer located in the front side pocket of the backpack. For example, although not shown in FIG. 1, the representation of the key in the data structure 102 may be eliminated. In some implementations, the current location of the object represented in the data structure 102 may expire in response to a completion type command rather than the object being deleted. In this way, the location history of the object can be analyzed.
The changes made in response to the query 128 at stage G (132) are illustrated at stages H (136) and I (138). At stage H (136), the user 104 asks "what is in my backpack? (What do I have in the backing pack). Since the personal object data structure 102 has been updated to reflect that the key is no longer in the backpack, at stage I (138), the process 100 responds to the fifth user query 134 with the answer "Peyton Manning gym," which is the only object that is still in the backpack now.
As mentioned above, there is an entity data structure between one or more knowledge sources 116 that can be accessed to identify attributes associated with an object or location mentioned in a query. Examples of such data structures are depicted in fig. 2A and 2B.
FIG. 2A is an example graph 200 of an entity data structure in accordance with an example embodiment of the techniques described herein. Data graph 200 may represent the storage and structure of information in the data structures of interconnected entities. Such a data graph 200 stores information about nodes (entities) and edges (attributes or relationships) from which a graph such as the graph illustrated in fig. 2A can be generated. The nodes 202 may be referred to as entities and the edges 204 may be referred to as attributes, which form connections between entities.
Fig. 2B depicts an example portion of a data structure 220 of interconnected entities. In some implementations, the data structure 220 may store information about entities in the data structure in the form of triples. For example, a triple mark shows the body, properties, and values in the triple. Tom Hanks is an entity in the data structure 220 and is an entity of the triple. The first triple 222 identifies the property "occupational (has professional)" and the second triple 222 identifies the property "spouse (has spouse)". The value of the property is the third component in the triplet. Tom Hanks has an occupational "Actor" (Actor) and a spouse "Rita Wilson". In the first triple 222, the property "attribute" has a value that is a fact in the data structure. For example, an actor may or may not be an entity in the data structure. In some implementations, the numeric component of the triplet may reference a classification of the entity (e.g., actor) from which the named entity recognition engine can determine the category of the named entity in the text sample. In the second triplet 222, the numeric component is another entity in the data structure 220, specifically the entity of Rita Wilson. Thus, the triplets 222 specify that entities of Tom Hanks are connected by spouse relationships or relate to entities of Rita Wilson. Additional triples and their inversely related triples are shown in triples 224, 226 and 226'.
Referring to FIG. 3, a flow diagram of an example process 300 for saving and retrieving information about the location of an object is shown. In some implementations, the process 300 can be implemented by any system described herein, such as the system 400 depicted in fig. 4. Process 300 can be performed on one or more devices. For example, process 300 may be performed locally on a user device (e.g., smartphone, tablet, notebook computer), on one or more servers, or may be performed partially locally on the user device and partially remotely on one or more servers.
Among the one or more commands that the voice assistant can be configured to respond to is a command for object location storage and retrieval. At stage 304, the process 300 determines whether the received query corresponds to one or more object location commands. If the query is a voice query, the utterance can be transcribed into text using an automatic speech recognizer. The speech recognizer may include multiple grammars corresponding to different object position commands. In some implementations, the carrier phrase in the query can specifically identify a particular command. For example, in the query "Remember that Jane holds my chemistry book", the carrier phrase "Remember (Rember that)" may indicate that the query is a command to save the location of an object.
In some implementations, the speech recognizer can include multiple grammars that each correspond to a respective type of object position command. For example, the speech recognizer may include a declaration grammar, a completion grammar, a retrieval grammar, and a trigger action grammar. Each grammar may be used to determine the particular type of object location command to which the query corresponds. The grammar can train text samples (e.g., natural language queries) that correspond to the natural way in which a user speaks an object position command in one language. In this way, for example, a carrier phrase is not necessary for a voice assistant or other application to determine that a query is a particular type of object location command. Conversely, even if the required carrier phrase or other keyword is not included in the query, the words and query structure used naturally may be sufficient to determine what type of object location command the query corresponds to. For example, The query "My chemistry book is with Jane" and "Jane holds My chemistry book" and "chemistry book is with Jane's" may all be recognized as a command to store information about The location of an object using declarative syntax.
The declaration syntax can be used to determine that the query is a command to store information about the location of the object. Some query patterns that may be identified as commands to store object location information using declarative syntax include [ $ X is at $ Y ], [ I lent $ Xto $ Y ], [ I bounded $ X from $ Y ], [ $ X is in $ Y ], [ $ Y has $ X ], [ I left $ X with $ Y ], [ Rember that X is in the $ Y ]. Additional or other query patterns may also be understood by the declarative grammar as object location store commands, which may be determined, for example, through statistical training of object location store command samples.
The completion syntax can be used to determine that the query is a command that records that the object has returned or is otherwise no longer in a previously stored location. For example, if the user has made the statement "I lend 'James and the Giant Peach' to Fred", thereby causing the system to store information about the book 'James and the Giant Peach' together with Fred ", then an object complete command may be received from the user entering" Fred returns 'James and the Giant Peach' ". The completion command indicates that the book has been returned or otherwise no longer in a previously stored location, and the completion grammar can be used to identify the query as a completion command. For example, the completion grammar can identify query patterns for completion commands such as: [ $ Xreturn $ Y ], [ I go $ X back ], [ Clear location of $ X ], [ $ X is no location at $ Y ], and so on. Additional or other query patterns may also be understood by the completion grammar, which may be determined, for example, by performing statistical training of command samples for the object.
The search grammar can be used to determine that the query is a command identifying respective locations of one or more objects. Once the location of the object has been stored in response to the object location store command, the user may then submit an object retrieve command to determine where to recall the object. For example, the location of an album that has been previously recorded as being located in the attic can be responsive to a query "is the album there? (Where is the photo album). The process 300 can then identify that the album is located in the attic and present an indication to the user that the album is located in the attic. The response may be immediately presented to the user in a text message, such as email, SMS, or application text. In some embodiments, the response may be a response such as that spoken by speech synthesis. The search grammar can identify query patterns for search commands such as: [ Where is $ X? [ Where are $ X, $ Y, and $ Z? [ Where did I leave $ X? [ Who has $ X? [ Did I leave $ X somewhere? Is $ Y ringing $ X? And other query patterns that may be determined, for example, by statistical training of object location retrieval command samples. In some implementations, the search grammar can identify a search command that the request for one or more objects identifies is stored in one or more locations. For example, the user may ask "what is there in the kitchen drawer? (What are all of the items in the kitchen drawer.
The trigger action grammar can be used to determine that a query is a command to perform a specified action when an event occurs. Both the action and the event that is a condition of the action may be specified by the user, or may be determined by using a context or default settings if not specified by the user. In some implementations, the event may be that the object has been left in a given location or returned from there. In some implementations, the action can be to remind the user what to do with the object. For example, on day 5 of month 1, the user may record that he borrowed his 'Harry Potter' corpus to Sam. One week later, Jill tells the user that she wants to borrow the corpus as soon as possible after the user's ' Harry Potter ' corpus comes back. The user can speak a voice query that instructs him to be reminded to lend to Jill after the 'Harry Potter' corpus returns. The command can be recognized using a trigger action grammar. For example, the user may say "Remind me Jill to see the 'Harry Potter' corpus after Sam has seen it (reminded me that Jill walks the 'Harry Potter' collection where Sam is donewith)". Alternatively, the command may not include a condition, but may otherwise imply "remind me Jill to see 'Harry Potter' corpus". In accordance with such a command, the process 300 can generate a reminder to provide the 'HarryPotter' corpus to Jill. The reminders may be presented to the user periodically, or the user may ask him which reminders to set. In one embodiment, the reminder is provided when a completion event occurs. For example, when Sam returns an object to the user, the user may speak the done command "I go back to the 'Harry Potter' corpus (I go the 'Harry Potter' collection)". In response to recording the completion command, the process 300 may present the user with any relevant Reminder about the object, such as "remind me Jill to want to borrow your 'Harry Potter' corpus (Reminder that Jill will leave like to borrow your 'Harry Potter' collection)". In some implementations, the reminder can be provided when a statement (e.g., object location store command) occurs. For example, the user may set a first reminder in may that he must be ready to practice using a football pad 8 months and 1 day ago. In june of the same year, the user may submit a subject location storage command indicating that bob borrowed the football pad. Upon issuing such a command, process 300 may remind the user that the football pad must be returned 8 months and 1 day ago in order to practice the football pad in a timely manner. The process 300 may also alert the user at a particular time before the 8/1/expiration date that Bobby needs to return the football pad before 8/1/day. In addition, if the user makes a retrieval command, for example, "where is my football pad? (Where are my football pads),. In some implementations, the user can specify that reminders regarding the objects and/or their locations are provided to the user at particular times or in a recurring manner.
In one embodiment, the trigger action grammar can be used to set a notification event that alerts a user that an object has been left at a particular location when the user interacts with a representation of the location. If the user indicates that the object has been left for a particular person (or a particular organization or other generally particular entity), a contact corresponding to the person for which the object was loaned can be determined such that when the user interacts with the contact (e.g., sends an email, SMS message, video call, social media communication, phone call for the contact), a reminder can be presented that the object is there for the person corresponding to the determined contact. For example, if a user loans her snow shovel to Paula, the next time the user calls Paula or when Paula calls the user, a message can be automatically displayed to the user reminding the user that Paula holds the snow shovel. In some implementations, a geographic location can be determined for the location of the mentioned object from the query, and a notification can be automatically generated when the user (or the user's device) determines that their current location is within a threshold distance of the determined location. For example, a reminder about the snow shovel can be automatically presented when the user drives through Paula neighborhood home so that the user can conveniently retrieve the object, for example, when the user is in the area of Paula.
In one embodiment, a query can include different combinations of one or more objects, one or more locations, and one or more commands. For example, the composite query may be [ $ A is at $ X, and $ B is at $ Y ] or [ where $ A and $ B located? In the form of (c) ]. In such a case, the process 300 can parse the query and perform similar actions for each component of the query in a similar manner as is done in the case of a single component query, discussed generally herein for simplicity. For example, a respective grammar can be used to identify each component in the composite query, where the grammars can be the same or different depending on whether the individual components of the query point to different commands. In some implementations, a single command may be issued, but the process 300 may identify multiple objects and/or multiple locations associated with the command. For example, the object location storage command "My hat and scarf are in the glove compartment" can be divided into separate claims of hat in glove compartment and scarf in glove compartment, and such information can be stored as if separate claims had been formed for hat and scarf objects.
At stage 306, upon determining that the query includes an object location command of a particular type, the process 300 parses the query to identify the objects mentioned in the query. One or more objects may be identified from the query. In some embodiments, the name of an object may be extracted from a query and stored substantially in the manner in which the object was provided in the query. For example, given an object location store command "Colts sportswear at The laundry" (The Colts jersey is inside wash room), a "Colts sportswear" string can be extracted and stored in The personal object data structure at The "laundry" location. In some implementations, one or more related terms that can be stored or used can be identified for an object (e.g., depending on whether the query is a store, complete, or retrieve command) in addition to or instead of the original object string extracted from the query. For example, synonyms or pronunciation-corrected terms may be identified for the object. Thus, an "Indianapolis gums jersey" with a full team name can be identified based on the partial name "gums jersey" as used in the natural language query. In some implementations, the extracted object string can be compared to other objects that have been previously stored and associated with multiple locations in the personal object data structure. If the extracted object string or related term is determined to be similar or related to an existing (previously stored) object, it may be determined that the two objects are the same. If the two objects are the same, an additional entry may be created in the personal object data structure for the new instance of the object, which may be linked to the previously stored object. In some implementations, if two objects are determined to be likely to be the same object, additional or updated parameters may be generated for the existing representation of the object stored in the personal object data structure. For example, if the wrench set was previously located in the tool rack, but the new query indicates that the wrench set is now located in the garage, the original wrench set object representation may be accessed and its current location updated from the tool rack to the garage. In some implementations, an object string can be extracted from a query and an entry stored for the object without performing a comparison of the extracted object string to other objects at the time of extraction and storage. At a later time, when a retrieve or complete command is received from the user, for example, the process 300 may identify all entities matching the retrieve or complete command, including identifying the same object. For example, a user may state his "cards" in the office for the first time, and then state his "deck of cards" in the basement. May be at the time of making a statement of "deck" or at the user's subsequent inquiry of "where are two places i last put my cards? (the round are the last two places I left my cards). If the process 300 determines that the partial object string "cards" identifies the same object as "deck of cards," the process 300 may respond to the final query with an indication that the last two locations of the cards are the basement and office.
At stage 308, the process 300 identifies the location of the object from the query. The location may be identified using techniques similar to those used to identify objects from the query. For example, the query may be parsed and one or more words corresponding to the location of the object extracted. For an object location store command, one or more words corresponding to the location may be saved in a personal object data structure and associated with the object. In some implementations, the object location specified in the new query can be determined to be the same as other locations previously recorded in the personal object data structure. In some implementations, relationships between multiple locations identified in a particular query and/or between locations in a query and locations that have been previously recorded in the personal object data structure may be determined. For example, a location previously saved in the data structure may be an "office". The new query may indicate that the object is located in an "office closet". A hierarchical relationship may be established between the offices and the office closet such that objects in the office closet may also be identified as being in the office, although not all objects of the office are in the office closet. In some implementations, associated with the object may be geographic location information rather than a conceptual location. For example, a geographic location may be implied from some queries that do not explicitly reference a location or from queries that reference a location for which a geographic location can be identified. Thus, the query "Remember I stop me car here (Remember that I left my car here)" may cause process 300 to use a location tool such as GPS to determine the geographic coordinates of the device that received the query, and such coordinates can be recorded as the current location of the car. In another example, the user may say "I left mycoat Jerry's World Famous Deli (I left mycoat Jerry's World Famous Deli)". The geographic location of Jerry's World Famous Deli may be determined and stored in the personal object data structure in addition to or instead of the name of the Deli.
At stage 310, process 300 determines that the object identified from the query corresponds to one or more entities. An entity may include, for example, a person, thing, or idea. Examples of entities are extensive and may include anyone and anything from headquarter bark Obama and american congress to Jon Bon Jovi, reality shows and kangaroos, movies, businesses, etc. One or more known entities may have been previously identified or may be identified after receiving the query. In some embodiments, an entity may be indexed in a data structure of the entity, in the event that the entity has been previously identified. In some implementations, the data structure may include one or more sub-data structures and may be represented as a graph (e.g., see FIG. 2A). For example, each entity may be represented by a node in a graph, a particular plurality of entities may be interconnected in the graph with one or more relationships (e.g., an entity representing j.k.rowling may be connected to an entity representing the book by indicating that j.k.rowling authored the bi-directional relationship of the book ' HarryPotter and the phylospher's Stone '), each entity may be associated with a respective set of attributes, and an entity may be associated with one or more topics (categories). For example, an entity representing j.k.rowling may be connected to other entities, such as an entity representing british yard as the origin of the author. The rowling entity may also have attributes associated with her such as birth date, interests, etc. Information about an entity can be manually entered by a user and/or can be pieced together from publicly available information (e.g., web pages, blogs, and other electronic documents) and considered accurate data about the entity if at least one threshold confidence level is determined based on the source of the information (e.g., based on the reliability of the data source, the frequency with which data is repeated across multiple sources, and the staleness of the data). In some embodiments, the entity data structures may be represented in a manner similar to the entity graph 202 depicted in fig. 2A and 2B. In some implementations, information about entities corresponding to objects or locations in a query can be identified after receiving the query, such as by performing an internet search based on the query to identify publicly available information about the query.
At stage 312, one or more attributes are determined for the entity that has been determined to correspond to the object specified in the query. Attributes of an entity can generally be used to identify additional information related to the object but not spoken in the query. The stored object can then be tagged or otherwise associated with the attributes of its corresponding entity, enabling the user to then identify the object by referencing the attributes. In some implementations, additional keywords are associated with the object based on the attributes. For example, in response To a query indicating that the user has left her copy of "toll Kill a Mockingbird" To Ryan, data representing that is stored in a location representing that the object "toll Kill a Mockingbird" is currently "Ryan" is caused To be stored in the personal object data structure. The object "To Kill a Mockingbird" may be determined at stage 310 To correspond To the classic novel of Harper Lee (i.e., the novel is the entity To which the query object corresponds in this example). Since the object referenced in the query is determined to likely correspond to a novel, additional attributes related to the novel can be associated with the object. Thus, author name (Harper Lee), year of first publication (1960), subject matter (ethnic discrimination, law), genre (southern goth, fictional fictitious), and primary role (Atticus Finch) are all attributes that describe this book. Keywords can be identified from attributes and associated with stored objects, such as "book", "Harper Lee", "1960", "ethnic discrimination", "law", "southern goth", and "attentus Finch". Thus, even if the user initially indicated "ToKill a Mockingbird" at Ryan, the object can be subsequently referenced using a different term. For example, the user may issue a retrieve command "who has my number about Atticus Finch" or a complete command "Harper Lee's book has been returned," and the process 300 can then recognize that the object in these subsequent commands is the book "To Kill a Mockingbird" that Ryan has borrowed. Similarly, object can be tagged with synonyms, short object names, full length object names, categories, and topic or category information with the object enabling the object to be referenced using alternative terms that were previously used to hold terms for information about the object.
In some implementations, entity attributes can be identified by accessing an entity data structure that includes information about entities and their relationships. Sometimes, multiple entities may be determined to correspond to objects in a query. For example, "ToKill a Mockingbird" may not refer to that book, but rather to a movie changed from this book in 1962, which was sponsored by Gregory Peck. In the event that an object specified in the query is ambiguous and may correspond to one of a plurality of different candidate entities, attributes from one or more of the candidate entities may be associated with the object.
To facilitate selection of candidate entities, a confidence score can be determined for the candidate entities that reflects a respective likelihood that each candidate entity matches an object indicated in the user query. The confidence score can be used to select which entity's attributes to associate with the object. In some implementations, the attributes of the n entities with the n highest confidence scores can be associated with one object. In some implementations, only entities whose confidence scores meet (e.g., exceed) a threshold confidence score may have their attributes associated with the object. The confidence scores for an entity may be based on one or more factors, including the context of the query that mentions the object, and the frequency with which the corresponding entity is found in one or more corpuses of electronic resources. For example, a book "toll Kill a Mockingbird" may be ten times more frequent than the discussion of a movie on an Internet page or other resource that includes the phrase "toll Kill a Mockingbird". Thus, the entity data structure may include information indicating the relative popularity of the book compared to the movie, and such information may cause the confidence score for the book to be higher than the confidence score for the movie. Similarly, if the query itself includes a context that disambiguates the candidate entity, such context can be used to affect the confidence score of the candidate entity. For example, it can be clear from this query that The "DVD of" ToKill a Mockingbird ' includes a reference To a movie instead of a book at Ryan (The DVDs for "To Kill a Mockingbird ' are at Ryan's place). Thus, the confidence score for the movie entity may be made higher than the confidence score for the book in this example, and it is the attributes of the movie that can be associated with the object specified in the query rather than the attributes of the book.
At stage 314, one or more additional attributes that are not associated with the corresponding entity can be associated with the query object. The additional attributes can include terms, phrases, and other information that relate to the object but are not necessarily identified from the entity for the object. Even in the case where a corresponding entity cannot be identified from the entity data structure for an object, other sources of information may simply be the relevant attributes of the object. For example, a personal object having a particular and possibly private meaning may not be an entity represented in an entity data structure (which may be generated based largely on publicly available information). Thus, a "Grandma's hand-knit sweater" may be an object that has no entities in the entity data structure. Also, keyword attributes such as "grandma," "family," "wielder," "sew," and "shirt" may be associated with the object to allow for expanded retrieval capabilities when the object is subsequently referenced by the user in a query. The additional attributes may be identified in some embodiments by accessing a thesaurus to determine synonyms for the keywords by identifying related (but not corresponding) entities from the entity data structure and/or by allowing a web-based search for the object and identifying keywords from related search results.
Additional attributes of the object can also be determined based on the user account data. For example, an object may have a description of data stored in an account associated with the user that can be parsed to identify keyword attributes to be associated with the object. This is advantageous, for example, for determining attribute information for an object, regardless of whether the entity corresponding to the object is represented in an existing entity data structure. Thus, keywords or other data derived from a description of an object from a user's email, application, social media account, profile information, or other source can be stored in a personal object data structure in association with the object. The user may opt-in or out so that the user account data is analyzed for such purposes, and may limit what data is analyzed.
At stage 316, information identifying the object specified in the query, the location of the object, and any attributes associated with the object are stored in a personal object data structure. The personal object data structure may include a collection of data structures at one or more locations such that the data, while associated, may or may not be stored together. Information may be stored at stage 316 of process 300 when the query is a command to store information about the location of an object. Queries against other commands, such as commands requesting information about the location of an object, may not cause new information about the object or its location to be stored. In the case of storing object location information, additional information may be stored in addition to the object identifier, location, and object attributes. For example, keywords or other attributes of a location may be identified using techniques similar to those used for attributes of an object (e.g., by determining attributes of entities corresponding to the location mentioned in the command). Location attribute information can thus be stored with object-location entries. Timestamps that mark the date and time that an object was left in a particular location or that a query was received can also be stored with the object-location entry. The time stamp can be used to perform complex object-location lookup operations. For example, the timestamp can be used to respond to user queries that ask when an object was left in a location, how long the object has been in a location, or all locations where the object was in within a particular time period.
In some embodiments, the information stored in the personal object library can be represented as a graph. The graph may include nodes (vertices) representing various objects and locations stored in the corpus and edges connecting the nodes, the latter representing relationships between the nodes. The graph may be represented in a manner similar to the graph shown in fig. 2. For example, a particular location where one or more objects have been located or have been located may be represented by a node that is connected to other nodes of each of the one or more objects. Objects already located at one or more locations may have multiple nodes respectively connected to other nodes at their respective locations. In some implementations, attributes of an object or location may also be represented by nodes in the graph. For example, the object "To Kill a Mockingbird" may correspond To a book of Harper Lee, which may be determined by accessing an entity data structure and matching the object To a book represented as an entity in the data structure. The author Harper Lee of this book may therefore be an attribute associated with the object and may be represented in the personal object graph as a node connected To the "To Kill a Mockingbird" object node.
At stage 318, a second query is received. The second query may relate to an object that has been previously stored in the personal object repository. For example, the first query received at stage 302 may already be an object-location storage command, and the second query may be a retrieval command requesting that process 300 respond with the location of the object from the first query. The second query may refer to the same object in the same, similar, or even very different language than the first query. For example, the first query may relate To a novel by its title (e.g., "toll Kill a Mockingbird"), while the second query may relate To the novel by only a portion of its title (e.g., "Mockingbird"), or even by a completely different parameter, such as the author of the novel (e.g., "Harper Lee") or another attribute. Similarly, a particular location specified in a query may be referred to in the same, similar, or very different manner that the same location was specified in a previous query. For example, one query may identify that the object is located at "mom's home," while a second query may identify the same location as a street address of mom's home, or at the corresponding geographic coordinates of mom's home.
At stage 320, the second query is parsed. The process 300 determines whether the query is of the type of object-location command and, if so, what command it is (e.g., object location store command (declaration), done command, retrieve command, trigger action command). For example, various declarations, completions, retrievals, trigger actions, and other grammars can be used to parse the spoken query. Object and/or location strings in the query are identified by parsing the query. For example, the query "where did i put Harper's book? (Where did I leave the Harper Lee book.
At stage 322, the process 300 determines that the object and/or location specified in the second query is the same as the object and/or location specified in the previously received first query. The object and/or location string identified from the second query may be determined to be the same between queries if it matches a previously received object and/or location string from the first query. If the same object or location is identified in the second query in a different manner than it was identified in the first query, one or more of the most closely matching objects or locations can be identified, respectively. In some implementations, string comparisons are performed between portions of the second query and portions of the first query, such as comparisons between object strings in the same query and object strings extracted from the first query. In some implementations, the object referred to in the second query indirectly references the same object referred to in the first query through one or more attributes of the object. For example, "Harper Lee book" is an indirect reference To "To Kill a Mockingbird". If the second query includes the object string "Harper Lee book", such string may be matched To "To Kill a Mockingbird" by comparing the object string "Harper Leebook" To the properties of the book entitled "To Kill a Mockingbird", and a match can be determined from the author of the book. In some embodiments, the attributes are keywords that have been tagged against a "toll Killa Mockingbird" object previously stored in the personal object data structure, and the match is determined by performing a comparison of the indirect object specified in the second query with the keywords of the previously stored object. In this manner, the process 300 can identify one or more objects in the personal object corpus that may correspond to objects indirectly referenced in the query. In some implementations, the entities of objects or locations in the personal object data structure can be updated when the corresponding object or location is referenced in a series of queries, whether the references to the object or location are the same or different. For example, an entry for a location named "Bill's Apartment" can be created in the data structure in response to receiving a first query indicating that a first object is located at Bill's Apartment. The second query may indicate that the second object is located at Bill's Apartment, but may identify Bill's Apartment by street address 555Candy Cane Lane, Apartment No. 3. Process 300 can determine that the address referred to in the second query corresponds to Bill's apartment, and thus the location entry for Bill's apartment is updated to indicate that the second object is also located at Bill's apartment. For example, in the graphical representation of the personal data structure, the node of the second object may be a branch from the node representing the "apartment in Bill" location.
At stage 324, process 300 performs an action associated with the object in response to the second query. For example, identification information of the object and its current location can be saved for subsequent retrieval; information about the object or location can be obtained and provided to the user; a completion action can be performed; and a trigger action can be performed in response to the second query.
FIG. 4 depicts an example system 400 for managing and reporting location information for objects. The system 400 can be configured to receive a user query for storing information about where the user has left a personal object so that such information can be recalled at a later user request. The system 400 allows for the use of flexible query structures and natural language in queries to instruct the system 400 to store information about objects and object locations, perform operations on representations of stored objects and their locations, and respond to queries with information that can be used to present stored information about objects and locations to users. The system 400 can include one or more user devices 402, a network 410, an object and location engine 404, an entity data structure 408, and a speech recognizer 406. While the various components of the system 400 are illustrated as separate from one another, in some embodiments, certain ones of the components can be integrated with other components in various combinations, and some of the components can be divided into additional sub-components. For example, speech recognizer 406 may be implemented on user device 402 or on one or more server systems remote from user device 402. Similarly, the object and location engine 404 can be provided locally on the user 402, on one or more server systems remote from the user device 402, or certain subcomponents can be provided locally while others are provided on remote servers. In one embodiment, the system 400 can be configured to perform certain processes described herein, such as the process 100 depicted in fig. 1 or the process 300 depicted in fig. 3.
The user device 402 is configured to receive a query from a user and present information to the user. The user device 402 may be a smartphone, a tablet computing device, a desktop computer, a notebook computer, or in some implementations, a smart watch. The user device 402 may be configured to receive queries based on verbal input from the user or based on textual input entered by the user. The user device 402 may also include or be operatively coupled to an electronic display, such as a touch-sensitive display, capable of presenting information to a user. The user device 402 may include an I/O interface 412, a speech processing engine 414, a display manager 416, a time and location engine 418, and an account manager 420. The I/O interface 412 is configured to transmit and receive data from the user device 402. For example, the I/O interface 412 may include one or more wireless communication chips capable of sending and receiving data over a network 410, such as the internet or a private network. The speech processing engine 414 is capable of performing speech recognition on spoken input from a user. In some implementations, the speech processing engine 414 can have a limited grammar that recognizes only a limited set of speech commands. The audio of the spoken input of the complex natural language query can, in some implementations, be communicated to the speech recognizer 406 for remote processing on a server that has more complex speech recognition capabilities and larger grammars than those on the user device 402. In some implementations, all speech processing is performed on the user device 402. The display manager 416 manages what is to be displayed on one or more electronic displays associated with the user device 402. The query may be presented to the user when the query is received by the device 402, and a query response may be shown on the electronic display to present the response to the user.
The time and location engine 418 is configured to track the current time and location of the user device 402. Using information from the time and location engine 418, the system 400 can associate the user query with a time and/or a location. A timestamp can be associated with the query to indicate, for example, a date and time that the object was left in a particular location. Thus, when the user makes an object-location storage command such as "I borrow basketball to Paige (I lens the basketball to Paige threshaft) in the afternoon"), a timestamp is associated with the command (query) to mark the date and time that the basketball was borrowed to Paige. In some implementations, the user can specify explicitly when an object is left in a location, and such an explicit timestamp overrides the default current timestamp from the time and location engine 418. For example, a query received on day 5 of 6 months indicating that "I'm given basketball 4days ago to Paige 4 days" could be tagged with the date of 6 months day 1 instead of 6 months day 5 because the contents of the query explicitly indicated the date. By recording the date and time that the object was left in a particular location, the system 400 can then use such information to respond to time-based queries about the object and location. For example, the system 400 may use timestamps associated with entries in the personal object data structure to respond to queries regarding when an object was left in one or more locations, how long the object had been left in one or more locations, and where all locations the object was placed in within a particular time period.
The account manager 420 of the user device 402 manages one or more user accounts on the device 402. For example, the account manager 420 manages a respective account for each of a plurality of users. Each user account may include user profile information as well as information about the user's preferences with respect to object-location type commands. The user account information can be stored in a user profile repository 422. One or more personal object data structures for respective user accounts can be stored in object data corpus 424. Thus, different users can have their own personal data structures that are independent of other users. When a first user logs into device 402 with a first account, he may save and retrieve information about the location of the object in his corresponding personal object data structure. When another user logs into device 402 with a second account, the user may save and retrieve information about the location of the object in their respective personal object data structure. The account may be associated with the user device 402 (e.g., maintained by an operating system of the user device 402), or maintained by one or more services or applications that are on the device 402 or that are otherwise accessible to the device 402. For example, an object-location application installed on a device may allow multiple different users to log into the application and may maintain a respective account for each user. In some implementations, at least one of the user profile corpus 422 and the object corpus 424 can be maintained at least partially remotely from the user device 402, such as at an object-location server. In some embodiments, user data, including personal object data structures, may be maintained both locally and remotely. Changes to user data, such as updates to personal object data structures, can be automatically propagated between one or more local and remote devices when a change is made at one of the devices.
The object and location engine 404(O & L engine) is configured to receive, process, and respond to queries that include one or more types of object-location commands. The O & L engine 404 can receive queries, parse the queries, and respond based on the content of the queries. For example, when a user issues a voice query at the user device 402, the user device 402 may send the query or a characterized form of the query to the O & L engine 404 over the network 410, which may be hosted on one or more remote servers. The O & L engine 404 can perform one or more actions in response to the query, such as storing information about where the object is located, identifying where the object is located, updating the information about where the object is located, determining information that triggers the action, and generating instructions to send to the user device 402 to cause the action to be performed in response to the triggering event.
O & L engine 404 may include I/O interface 426, query parser 428, named entity identification engine 430, O & L data manager 432, and account manager 434. In addition, the O & L engine 404 can communicate with a speech recognizer 406 and one or more external data sources, such as an entity data structure 408. The I/O interface 426 is configured to communicate over one or more networks 410, such as a private network or the internet, and is capable of communicating with the speech recognizer 406, the entity data structures 408, and other external sources, and with the user device 402.
The query parser 428 is configured to parse a query received from the user device 402. The query parser 428 can process the user query to determine if it is an object-location command and, if so, the particular type of object-location command (e.g., object location store command, retrieve command, complete command, trigger action command, or another type). The query parser can identify each object referred to by the query, each location referred to in the query, and a correspondence between the object and the location referred to in the query. For example, The query may indicate "towels in The dryer and sheets in The closet" (The towels are in The dryer and The sheets are in The closet) ". Query parser 428 can recognize that the query is an object-location storage command that includes references to two objects (towel and sheet), two locations (dryer and closet), and that the towel and dryer correspond as an object-location pair and that the sheet and closet correspond as another object-location pair.
In some implementations, the query parser 428 processes and parses the query using one or more grammars. For example, the voice query may be transcribed by the speech recognizer 406. Speech recognizer 406 includes a grammar configured to recognize object-position commands in various types of natural language. The declaration grammar 440 is configured to allow identification of object-location storage commands (e.g., "puck is in The backseat," "Remember collection of art in Bill —"). The retrieval grammar 442 is configured to allow identification of object retrieval commands (e.g., "is the hockey puck. The completion grammar 444 is configured to allow recognition of object completion commands (e.g., "I've returned the hockey puck", "Bill no longer has the art collection)". The event-triggered grammar 446 is configured to allow identification of event-triggered commands ("Remind me salt want puck (reminded me salt water places the hockey puck)", "Remind me salt want art collections after Bill (reminded me salt water places the artcollection Bill)"). The query parser 428 can send queries for transcription by the speech recognizer 406 using the various grammars 440 and 446 of the speech recognizer 406. The query parser 428 is capable of extracting one or more strings from the transcription of the voice query that correspond to objects and locations in the string. In some implementations, the grammar 440 and 446 can be used to identify which terms in a text query correspond to objects and locations, and also determine what specific type of object-location command the query is. For example, each grammar can be included in one or more language models that have been trained on a large number of training queries of a particular type. The declaration grammar 440 can be based on training samples of a declaration type query (e.g., object-location storage commands), the retrieval grammar 442 can be based on training samples of a retrieval type query, the completion grammar 444 can be based on training samples of a completion type query, and the event-triggered grammar 446 can be based on training samples of an event-triggered type query. When a new query is received, a model that has been statistically trained for various types of queries can make probabilistic determinations of the particular types of queries that have been made (e.g., statement, search, completion, event trigger, others).
The named entity recognition engine 430 is configured to determine one or more attributes of the object or location referenced in the query. The named entity identification engine 430 can determine attributes for an object or a location in a query, or for both an object and a location. If more than one object or location is provided in the query, the engine 430 can determine attributes for all or only some of the objects or locations. The named entity recognition engine 430 can access one or more data sources to identify attributes of an object or location. In some implementations, the data source can be external to the system 400. For example, the named entity recognition engine 430 can perform a search of one or more corpora based on the query or based on the object or location of interest. The content of the search results can then be used to determine one or more keywords associated with the object or location. For example, the named entity recognition engine 430 can identify keyword attributes of the object "camera" by performing a search for the word camera, and determine the most relevant keywords associated with the camera from one or more top ranked search results. These keyword attributes can then be associated with the camera object such that the camera can then be identified based on its attributes (e.g., photograph, lens, viewfinder, D-SLR), etc., in addition to or instead of the original way in which the camera was identified in the query.
In some implementations, the named entity recognition engine 430 determines attributes of an object or location by identifying one or more entities in the real world to which the object or location corresponds and associating the attributes of those entities with the object or location mentioned in the query. In one example, the entities and their attributes are identified with reference to an entity data structure 408. The entity data structures 408 typically include structured representations of entities (e.g., people, places, things, ideas), relationships between entities, and attributes of such entities. The entity data structure 408 can be represented as a graph having nodes representing entities and edges connecting the nodes representing relationships between the entities and various attributes (e.g., facts) about the entities. One example of such a graph is shown in fig. 2A and 2B. The entity repository 448 includes data for the entities represented in the data structure 408, and the attribute repository 450 includes data about attributes of the entities. The entity data structure 448 can be obtained manually, automatically, or a combination of both. For example, an entity may be determined by crawling publicly available resources on the internet, analyzing the content of the publicly available resources, and making determinations about the entity's presence and attributes based on the analysis of the resources. For example, the topic of a large number of resources may be a particular object (or series of objects), such as the Sherlock Holmes series book by Arthur Conan Doyle. It can be determined that each book in the series is a high confidence of an entity, just as its author Arthur Conan Doyle Jazz. In addition, various attributes may be determined for the book, such as genre (inferences), and the time and place they were written and published for the first time. Such information may be stored in an entity data structure 448.
In some implementations, the named entity recognition engine 430 can associate an object or location in the query with an entity in the entity data structure 448, such as by referencing a pointer to the associated entity. In this manner, if information about an entity is added or otherwise updated, such information can also be updated for the corresponding object and location from the query.
The O & L data manager 432 manages information about objects and locations. For example, working with the account manager 434 and in response to a user query, the O & L data manager 432 can store and access information about objects and object storage locations for the user. Once the query has been parsed and the object or location attributes determined, the O & L data manager 432 can cause such information to be stored by the query parser 428 and named entity recognition engine 430, respectively, and information about the object, its location, and attributes of the object and location can be stored in a personal object data structure 438, which is maintained by an account manager 434. For example, as indicated by the data in user account repository 436, a respective personal object data structure 438 can be maintained for each of a plurality of user accounts. Thus, for example, Fred can save and obtain information about where its own personal objects are located, whereas Emma cannot. For example, the O & L data manager 432 can also pull data from the personal object data structure 438 to respond to queries such as object retrieval lookups.
Fig. 5 illustrates an example of a computing device 500 and a mobile computing device that may be used to implement the techniques described herein. Computing device 500 is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers, and computing device 550 is intended to represent various forms of mobile devices, such as personal digital assistants, mobile telephones, smartphones, and other similar computing devices. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed herein.
The memory 504 stores information within the computing device 500. In one implementation, the memory 504 is a volatile memory unit or units. In another implementation, the memory 504 is a non-volatile memory unit or units. The memory 504 may also be another form of computer-readable medium, such as a magnetic or optical disk.
The storage device 506 is capable of providing large amounts of storage for the computing device 500. In one implementation, the storage device 506 may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. The computer program product may be tangibly embodied in an information carrier. The computer program product may also contain instructions that, when executed, perform one or more methods such as those described above. The information carrier is a computer-or machine-readable medium, such as the memory 504, the storage device 506, or memory on processor 502.
The high-speed controller 508 manages bandwidth-intensive operations for the computing device 500, while the low-speed controller 512 manages lower bandwidth-intensive operations. Such allocation of functions is exemplary only. In one implementation, the high-speed controller 508 is coupled to memory 504, display 516 (e.g., through a graphics processor or accelerator), and to high-speed expansion ports 510, which may accept various expansion cards (not shown). In the depicted embodiment, low-speed controller 512 is coupled to storage device 506 and low-speed expansion port 514. The low-speed control port 514, which may include various communication ports (e.g., USB, bluetooth, ethernet, wireless ethernet), may be coupled to one or more input/output devices such as a keyboard, a pointing device, a scanner, or networked devices such as switches and routers, for example, through network adapters.
As shown, the computing device 500 can be implemented in a variety of different forms. For example, it may be implemented as a standard server 520, or as a plurality of servers in such a server group. It may also be implemented as part of a rack server system 524. It may also be implemented in a personal computer such as a laptop computer 522. Alternatively, components from computing device 500 may be combined with other components in a mobile device (not shown), such as device 550. Each such device may contain one or more computing devices 500 and mobile computing device 550, and an entire system may be made up of multiple computing devices in communication with each other.
The processor 552 can execute instructions within the mobile computing device 550, including instructions stored in the memory 564. The processor may be implemented as a chipset of chips that include separate and multiple analog and digital processors. For example, the processor may provide coordination of the other components of the mobile computing device 550, such as control of user interfaces, applications run by the mobile computing device 550, and wireless communication by the mobile computing device 550.
The processor 552 may communicate with a user through a control interface 558 and a display interface coupled to a display 554. The display 554 may be, for example, a TFT LCD (thin film transistor liquid Crystal display) display or an OLED (organic light emitting diode) display, or other suitable display technology. The display interface 556 may comprise appropriate circuitry for driving the display 554 to display graphical and other information to a user. The control interface 558 may receive commands from a user and convert them for submission to the processor 552. In addition, an external interface 562 may be provided in communication with the processor 552, so as to enable near area communication of the mobile computing device 550 with other devices. For example, external interface 562 may provide, in some implementations, for wired communication, or in other implementations, for wireless communication, and multiple interfaces may also be used.
The memory 564 stores information within the mobile computing device 550. The memory 564 can be implemented as one or more computer-readable media or media, one or more volatile memory units, or one or more non-volatile memory units. Expansion memory 574 is also provided and is connected to device 550 through expansion interface 572, which may include, for example, a SIMM (Single in line memory Module) card interface. Such expansion memory 574 may provide additional storage space for device 550, or may also store applications or other information for device 550. In particular, expansion memory 574 may include instructions to perform or supplement the processes described above, and may include secure information also. Expansion memory 574 may thus be provided, for example, as a security module for device 550, and may be programmed with instructions that permit secure use of device 550. In addition, secure applications may be provided via the SIMM card, as well as additional information, such as setting identification information on the SIMM card in a non-destructible manner.
For example, as described below, the memory may include flash memory and/or NVRAM memory (non-volatile random access memory). The computer program product may also contain instructions that, when executed, perform one or more methods such as those described above. The computer program product is a computer-or machine-readable medium, such as the memory 564, expansion memory 574, memory on processor 552. In some implementations, the computer program product can be received in a propagated signal, for example, over the transceiver 568 or the external interface 562.
The mobile computing device 550 also communicates audibly using the audio codec 560, which audio codec 560 receives voice information from the user and converts it to usable digital information. The audio codec 560 may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of the mobile computing device 550. Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.), and may also include sound generated by applications running on the mobile computing device 550.
As shown, the mobile computing device 550 may be implemented in a number of different ways. For example, it may be implemented as a cellular telephone 580. It may also be implemented as part of a smartphone 582, personal digital assistant, or other similar mobile device.
Thus, various implementations of the systems and techniques described here can be realized in digital electronic circuitry, integrated circuitry, specially designed ASICs (application specific integrated circuits), computer hardware, firmware, software, and/or combinations thereof. These various implementations can include implementation in one or more computer programs that are executable and/or interpretable on a programmable system including at least one programmable processor, which may be special or general purpose, coupled to receive data and instructions from, and to transmit data and instructions to, a storage device, at least one input device, and at least one output device.
These computer programs (also known as programs, software applications or code) include machine instructions for a programmable processor, and can be implemented in a high-level procedural and/or object-oriented programming language, and/or in assembly/machine language. As used herein, the terms machine-readable medium, computer-readable medium, refer to any computer program product, apparatus and/or device (e.g., magnetic discs, optical disks, memory, Programmable Logic Devices (PLDs)) used to provide machine instructions and/or data to a programmable processor, including a machine-readable medium that receives machine instructions as a machine-readable signal. The term machine-readable signal refers to any signal used to provide machine instructions and/or data to a programmable processor.
To provide for interaction with a user, the systems and techniques described here can be implemented on a computer having a display device (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor) for displaying information to the user and a keyboard and a pointing device (e.g., a mouse or a trackball) by which the user can provide input to the computer. Other types of devices may also be used to provide for interaction with a user; for example, feedback provided to the user can be any form of sensor feedback (e.g., visual feedback, auditory feedback, or tactile feedback); and input from the user can be received in any form, including acoustic, speech, or tactile input.
The systems and techniques described here can be implemented in a computing system that includes a back-end component (e.g., as a data server), or that includes a middleware component (e.g., an application server), or that includes a front-end component (e.g., a client computer having a graphical user interface or a web browser through which a user can interact with an implementation of the systems and techniques described here), or any combination of such back-end, middleware, or front-end components. The components of the system can be interconnected by any form of media or digital data communication (e.g., a communication network). Examples of communication networks include a Local Area Network (LAN), a Wide Area Network (WAN), and the internet.
The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
Although some embodiments have been described in detail above, other modifications are possible. Further, the logic flows depicted in the figures do not require the particular order shown, or sequential order, to achieve desirable results. In addition, other acts may be provided, or acts may be eliminated, from the described flows, and components may be added to, or removed from, the described systems. Accordingly, other implementations are within the scope of the following claims.
Claims (20)
1. A computer-implemented method, comprising:
receiving, by a computing device at a first time, a first voice query identifying a physical object and a location where a user left the physical object for storage, wherein the physical object is capable of being carried or moved by the user;
transmitting the first voice query from the computing device to a remote computing system to cause the remote computing system to store an indication that the physical object was left for storage at a location identified in the first voice query;
receiving, by the computing device, at a second time after causing the remote computing system to store an indication that the physical object was left for storage at a location identified in the first voice query, a second voice query that identifies a property of the physical object, wherein the property of the physical object was not identified in the first voice query;
transmitting the second voice query to the remote computing system;
receiving, by the computing device, information indicating that the physical object is left for storage at a location identified in the first voice query as a response to the second voice query; and
providing information indicating that the physical object is left for storage at the location identified in the first voice query for presentation to a user.
2. The computer-implemented method of claim 1, wherein the second voice query includes one or more terms that identify the physical object that are not included in the first voice query.
3. The computer-implemented method of claim 1, further comprising determining that the computing device is located within a threshold distance of a location where the user left the physical object for storage, and in response, causing the computing device to present information indicating that the physical object is located in proximity to the computing device.
4. The computer-implemented method of claim 1, further comprising, in response to transmitting the first voice query from the computing device to the remote computing system to cause the remote computing system to store an indication that the physical object was left for storage at a location identified in the first voice query, causing the remote computing system to:
generating a first node representing the physical object in a data structure;
generating a second node in the data structure representing a location where the user left the physical object for storage; and
linking the first node and the second node in the data structure to represent an indication that the physical object is left in the identified location in the first voice query for storage.
5. The computer-implemented method of claim 1, wherein receiving, by the computing device and as a response to the second voice query, information indicating that the physical object was left for storage at a location identified in the first voice query further comprises:
in response to the remote computing system matching attributes of the physical object in the second voice query to one or more attributes of the physical object that are recognized by the remote computing system but not recognized in the first voice query, receiving information indicating a location where the physical object is left recognized in the first voice query for storage.
6. The computer-implemented method of claim 1, further comprising:
causing the remote computing system to store data for the physical object in a data structure corresponding to a particular account of the user, wherein the data:
(i) a representation comprising one or more physical objects and one or more corresponding locations where the user left the one or more physical objects for storage; and
(ii) mapping a relationship between the one or more physical objects and a particular location in one or more corresponding locations where the one or more physical objects were left by the user.
7. The computer-implemented method of claim 1, wherein the first voice query does not include information identifying a location at which the physical object was left by the user, and wherein the location at which the physical object was left by the user is determined based on a current location of the computing device that originally spoken the first voice query.
8. The computer-implemented method of claim 1, wherein the second voice query that identifies attributes of the physical object further comprises a retrieve command requesting a location of the physical object identified in the first voice query.
9. The computer-implemented method of claim 1, wherein the property of the physical object comprises a relationship of the physical object to another object.
10. A system, comprising:
one or more computers and one or more storage devices storing instructions operable, when executed by the one or more computers, to cause the one or more computers to perform operations comprising:
receiving, by a computing device at a first time, a first voice query identifying a physical object and a location where a user left the physical object for storage, wherein the physical object is capable of being carried or moved by the user;
transmitting the first voice query from the computing device to a remote computing system to cause the remote computing system to store an indication that the physical object was left for storage at a location identified in the first voice query;
receiving, by the computing device, at a second time after causing the remote computing system to store an indication that the physical object was left for storage at a location identified in the first voice query, a second voice query that identifies a property of the physical object, wherein the property of the physical object was not identified in the first voice query;
transmitting the second voice query to the remote computing system;
receiving, by the computing device, information indicating that the physical object is left for storage at a location identified in the first voice query as a response to the second voice query; and
providing information indicating that the physical object is left for storage at the location identified in the first voice query for presentation to a user.
11. The system of claim 10, wherein the operations further comprise determining that the computing device is located within a threshold distance of a location where the user left the physical object for storage, and in response, causing the computing device to present information indicating that the physical object is located in proximity to the computing device.
12. The system of claim 10, wherein the operations further comprise, in response to transmitting the first voice query from the computing device to a remote computing system to cause the remote computing system to store an indication that the physical object was left for storage at a location identified in the first voice query, causing the remote computing system to:
generating a first node representing the physical object in a data structure;
generating a second node in the data structure representing a location where the user left the physical object for storage; and
linking the first node and the second node in the data structure to represent an indication that the physical object is left in the identified location in the first voice query for storage.
13. The system of claim 10, wherein receiving, by the computing device and in response to the second voice query, information indicating that the physical object was left for storage at a location identified in the first voice query further comprises:
in response to the remote computing system matching attributes of the physical object in the second voice query to one or more attributes of the physical object identified by the remote computing system but not identified in the first voice query, receiving information indicating a location where the physical object is left identified in the first voice query for storage.
14. The system of claim 10, wherein the operations further comprise:
causing the remote computing system to store data for the physical object in a data structure corresponding to a particular account of the user, wherein the data:
(i) a representation comprising one or more physical objects and one or more corresponding locations where the user left the one or more physical objects for storage; and
(ii) mapping a relationship between the one or more physical objects and a particular location in one or more corresponding locations where the one or more physical objects were left by the user.
15. The system of claim 10, wherein the first voice query does not include information identifying a location where the user left the physical object, and wherein the location where the user left the physical object is determined based on a current location of the computing device that originally spoken the first voice query.
16. The system of claim 10, wherein the second voice query identifying attributes of the physical object further comprises a retrieve command requesting a location of the physical object identified in the first voice query.
17. A non-transitory computer-readable medium storing software comprising instructions executable by one or more computers and upon such execution cause the one or more computers to perform operations comprising:
receiving, by a computing device at a first time, a first voice query identifying a physical object and a location where a user left the physical object for storage, wherein the physical object is capable of being carried or moved by the user;
transmitting the first voice query from the computing device to a remote computing system to cause the remote computing system to store an indication that the physical object was left for storage at a location identified in the first voice query;
receiving, by the computing device, at a second time after causing the remote computing system to store an indication that the physical object was left for storage at a location identified in the first voice query, a second voice query that identifies a property of the physical object, wherein the property of the physical object was not identified in the first voice query;
transmitting the second voice query to the remote computing system;
receiving, by the computing device, information indicating that the physical object is left for storage at a location identified in the first voice query as a response to the second voice query; and
providing information indicating that the physical object is left for storage at the location identified in the first voice query for presentation to a user.
18. The computer-readable medium of claim 17, wherein the second voice query includes one or more terms that identify the physical object that are not included in the first voice query.
19. The computer-readable medium of claim 17, wherein the operations further comprise determining that the computing device is within a threshold distance of a location where the physical object was left by the user for storage, and in response, causing the computing device to present information indicating that the physical object is located in proximity to the computing device.
20. The computer-readable medium of claim 17, wherein the operations further comprise, in response to transmitting the first voice query from the computing device to a remote computing system to cause the remote computing system to store an indication that the physical object was left for storage at a location identified in the first voice query, causing the remote computing system to:
generating a first node representing the physical object in a data structure;
generating a second node in the data structure representing a location where the user left the physical object for storage; and
linking the first node and the second node in the data structure to represent an indication that the physical object is left in the identified location in the first voice query for storage.
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
CN202010348783.8A CN111666390B (en) | 2014-07-31 | 2015-06-17 | Saving and retrieving the location of an object |
Applications Claiming Priority (7)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201462031186P | 2014-07-31 | 2014-07-31 | |
US62/031,186 | 2014-07-31 | ||
US14/620,246 US9875322B2 (en) | 2014-07-31 | 2015-02-12 | Saving and retrieving locations of objects |
US14/620,246 | 2015-02-12 | ||
PCT/US2015/036165 WO2016018521A1 (en) | 2014-07-31 | 2015-06-17 | Saving and retrieving locations of objects |
CN202010348783.8A CN111666390B (en) | 2014-07-31 | 2015-06-17 | Saving and retrieving the location of an object |
CN201580035581.5A CN106663113B (en) | 2014-07-31 | 2015-06-17 | Saving and retrieving locations of objects |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201580035581.5A Division CN106663113B (en) | 2014-07-31 | 2015-06-17 | Saving and retrieving locations of objects |
Publications (2)
Publication Number | Publication Date |
---|---|
CN111666390A true CN111666390A (en) | 2020-09-15 |
CN111666390B CN111666390B (en) | 2024-04-23 |
Family
ID=55180282
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202010348783.8A Active CN111666390B (en) | 2014-07-31 | 2015-06-17 | Saving and retrieving the location of an object |
CN201580035581.5A Active CN106663113B (en) | 2014-07-31 | 2015-06-17 | Saving and retrieving locations of objects |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201580035581.5A Active CN106663113B (en) | 2014-07-31 | 2015-06-17 | Saving and retrieving locations of objects |
Country Status (5)
Country | Link |
---|---|
US (2) | US9875322B2 (en) |
CN (2) | CN111666390B (en) |
DE (1) | DE112015003523T5 (en) |
GB (2) | GB2545108B (en) |
WO (1) | WO2016018521A1 (en) |
Families Citing this family (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10304444B2 (en) * | 2016-03-23 | 2019-05-28 | Amazon Technologies, Inc. | Fine-grained natural language understanding |
US11562035B2 (en) * | 2017-03-16 | 2023-01-24 | Driftwood Capital, Llc | Enhancement of electronic communications and documents with links to contextually relevant information |
EP3879297A1 (en) * | 2017-04-14 | 2021-09-15 | Signify Holding B.V. | A positioning system for determining a location of an object |
DE102018214552A1 (en) * | 2018-08-28 | 2020-03-05 | Bayerische Motoren Werke Aktiengesellschaft | Acoustic feedback when approaching plug / deposit points |
US11914561B2 (en) | 2020-03-03 | 2024-02-27 | Rovi Guides, Inc. | Systems and methods for interpreting natural language search queries using training data |
US11594213B2 (en) | 2020-03-03 | 2023-02-28 | Rovi Guides, Inc. | Systems and methods for interpreting natural language search queries |
US11507572B2 (en) * | 2020-09-30 | 2022-11-22 | Rovi Guides, Inc. | Systems and methods for interpreting natural language search queries |
Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6505191B1 (en) * | 1998-07-24 | 2003-01-07 | Jarg Corporation | Distributed computer database system and method employing hypertext linkage analysis |
US20050162523A1 (en) * | 2004-01-22 | 2005-07-28 | Darrell Trevor J. | Photo-based mobile deixis system and related techniques |
US20090292716A1 (en) * | 2008-05-23 | 2009-11-26 | Nokia Corporation | Apparatus, method and computer program product for processing resource description framework statements |
CN102253945A (en) * | 2010-05-20 | 2011-11-23 | 盛乐信息技术（上海）有限公司 | Handheld article manager |
US20120116559A1 (en) * | 2010-11-04 | 2012-05-10 | Davis Bruce L | Smartphone-Based Methods and Systems |
US20130318582A1 (en) * | 2012-05-23 | 2013-11-28 | William Jon McCann | Inboxes for documents, music, videos, and photos |
CN103425658A (en) * | 2012-05-15 | 2013-12-04 | 阿里巴巴集团控股有限公司 | Commodity information storing and searching method, mobile terminal and server |
Family Cites Families (24)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6377296B1 (en) * | 1999-01-28 | 2002-04-23 | International Business Machines Corporation | Virtual map system and method for tracking objects |
US6317718B1 (en) * | 1999-02-26 | 2001-11-13 | Accenture Properties (2) B.V. | System, method and article of manufacture for location-based filtering for shopping agent in the physical world |
IL174107A0 (en) * | 2006-02-01 | 2006-08-01 | Grois Dan | Method and system for advertising by means of a search engine over a data network |
CN101136028B (en) * | 2006-07-10 | 2012-07-04 | 日电（中国）有限公司 | Position enquiring system based on free-running speech and position enquiring system based on key words |
US9318108B2 (en) * | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
US8255225B2 (en) | 2008-08-07 | 2012-08-28 | Vocollect Healthcare Systems, Inc. | Voice assistant system |
KR101545582B1 (en) * | 2008-10-29 | 2015-08-19 | 엘지전자 주식회사 | Terminal and method for controlling the same |
WO2010105218A2 (en) * | 2009-03-13 | 2010-09-16 | Invention Machine Corporation | System and method for knowledge research |
CA2814615A1 (en) * | 2009-10-13 | 2011-04-21 | Ezsav Inc. | Apparatuses, methods, and computer program products enabling association of related product data and execution of transaction |
US8396888B2 (en) * | 2009-12-04 | 2013-03-12 | Google Inc. | Location-based searching using a search area that corresponds to a geographical location of a computing device |
JP5781353B2 (en) * | 2011-03-31 | 2015-09-24 | 株式会社ソニー・コンピュータエンタテインメント | Information processing apparatus, information processing method, and data structure of position information |
US9286182B2 (en) * | 2011-06-17 | 2016-03-15 | Microsoft Technology Licensing, Llc | Virtual machine snapshotting and analysis |
US9059802B2 (en) * | 2011-11-09 | 2015-06-16 | At&T Mobility Ii Llc | Received signal strength indicator snapshot analysis |
US8687104B2 (en) * | 2012-03-27 | 2014-04-01 | Amazon Technologies, Inc. | User-guided object identification |
US8914393B2 (en) * | 2012-11-26 | 2014-12-16 | Facebook, Inc. | Search results using density-based map tiles |
US9367625B2 (en) * | 2013-05-03 | 2016-06-14 | Facebook, Inc. | Search query interactions on online social networks |
US9367880B2 (en) * | 2013-05-03 | 2016-06-14 | Facebook, Inc. | Search intent for queries on online social networks |
US9513778B1 (en) * | 2013-06-27 | 2016-12-06 | Ca, Inc. | Defining objects using an object-relationship map |
US9317614B2 (en) * | 2013-07-30 | 2016-04-19 | Facebook, Inc. | Static rankings for search queries on online social networks |
US9514230B2 (en) * | 2013-07-30 | 2016-12-06 | Facebook, Inc. | Rewriting search queries on online social networks |
US9269012B2 (en) * | 2013-08-22 | 2016-02-23 | Amazon Technologies, Inc. | Multi-tracker object tracking |
US9465811B2 (en) * | 2014-03-20 | 2016-10-11 | Facebook, Inc. | Polygon-based indexing of places |
US9646055B2 (en) * | 2014-04-03 | 2017-05-09 | Facebook, Inc. | Blending search results on online social networks |
US9740895B1 (en) * | 2014-05-30 | 2017-08-22 | Google Inc. | Method and system for identifying and tracking tagged, physical objects |
-
2015
- 2015-02-12 US US14/620,246 patent/US9875322B2/en not_active Expired - Fee Related
- 2015-06-17 DE DE112015003523.0T patent/DE112015003523T5/en active Pending
- 2015-06-17 CN CN202010348783.8A patent/CN111666390B/en active Active
- 2015-06-17 WO PCT/US2015/036165 patent/WO2016018521A1/en active Application Filing
- 2015-06-17 GB GB1621651.7A patent/GB2545108B/en active Active
- 2015-06-17 CN CN201580035581.5A patent/CN106663113B/en active Active
- 2015-06-17 GB GB2112426.8A patent/GB2595799B/en active Active
-
2017
- 2017-12-20 US US15/848,098 patent/US10783189B2/en active Active
Patent Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6505191B1 (en) * | 1998-07-24 | 2003-01-07 | Jarg Corporation | Distributed computer database system and method employing hypertext linkage analysis |
US20050162523A1 (en) * | 2004-01-22 | 2005-07-28 | Darrell Trevor J. | Photo-based mobile deixis system and related techniques |
US20090292716A1 (en) * | 2008-05-23 | 2009-11-26 | Nokia Corporation | Apparatus, method and computer program product for processing resource description framework statements |
CN102253945A (en) * | 2010-05-20 | 2011-11-23 | 盛乐信息技术（上海）有限公司 | Handheld article manager |
US20120116559A1 (en) * | 2010-11-04 | 2012-05-10 | Davis Bruce L | Smartphone-Based Methods and Systems |
CN103425658A (en) * | 2012-05-15 | 2013-12-04 | 阿里巴巴集团控股有限公司 | Commodity information storing and searching method, mobile terminal and server |
US20130318582A1 (en) * | 2012-05-23 | 2013-11-28 | William Jon McCann | Inboxes for documents, music, videos, and photos |
Also Published As
Publication number | Publication date |
---|---|
US20160034600A1 (en) | 2016-02-04 |
CN106663113A (en) | 2017-05-10 |
GB2595799B (en) | 2022-03-30 |
GB2545108A (en) | 2017-06-07 |
CN111666390B (en) | 2024-04-23 |
GB201621651D0 (en) | 2017-02-01 |
US9875322B2 (en) | 2018-01-23 |
GB202112426D0 (en) | 2021-10-13 |
US10783189B2 (en) | 2020-09-22 |
WO2016018521A1 (en) | 2016-02-04 |
CN106663113B (en) | 2020-05-08 |
US20180113953A1 (en) | 2018-04-26 |
GB2595799A (en) | 2021-12-08 |
GB2545108B (en) | 2021-10-13 |
DE112015003523T5 (en) | 2017-06-22 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN106663113B (en) | Saving and retrieving locations of objects | |
KR101712296B1 (en) | Voice-based media searching | |
US20190034040A1 (en) | Method for extracting salient dialog usage from live data | |
US11630955B2 (en) | Contextual document recall | |
EP2570974B1 (en) | Automatic crowd sourcing for machine learning in information extraction | |
US9229974B1 (en) | Classifying queries | |
WO2018222776A1 (en) | Methods and systems for customizing suggestions using user-specific information | |
US9928296B2 (en) | Search lexicon expansion | |
US9015043B2 (en) | Choosing recognized text from a background environment | |
US20150279360A1 (en) | Language modeling in speech recognition | |
US20150019216A1 (en) | Performing an operation relative to tabular data based upon voice input | |
US20060271520A1 (en) | Content-based implicit search query | |
CN112840335A (en) | User-centric contextual information for browsers | |
US20140201229A1 (en) | Providing display suggestions | |
US11366966B1 (en) | Named entity recognition and disambiguation engine | |
CN110325987B (en) | Context voice driven deep bookmarks | |
Crestani et al. | Mobile information retrieval | |
US20230169134A1 (en) | Annotation and retrieval of personal bookmarks | |
KR101441219B1 (en) | Automatic association of informational entities | |
US8538946B1 (en) | Creating model or list to identify queries | |
WO2023014454A1 (en) | Context-aware observational entity recognition and capture | |
US10691702B1 (en) | Generating ranked lists of entities | |
CN110399468A (en) | A kind of data processing method, device and the device for data processing | |
TW202316291A (en) | Patent search system and method thereof | |
TW202240461A (en) | Text editing using voice and gesture inputs for assistant systems |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination | ||
GR01 | Patent grant | ||
GR01 | Patent grant |