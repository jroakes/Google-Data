US9035878B1 - Input system - Google Patents
Input system Download PDFInfo
- Publication number
- US9035878B1 US9035878B1 US13/408,905 US201213408905A US9035878B1 US 9035878 B1 US9035878 B1 US 9035878B1 US 201213408905 A US201213408905 A US 201213408905A US 9035878 B1 US9035878 B1 US 9035878B1
- Authority
- US
- United States
- Prior art keywords
- graphic object
- pointer
- active region
- graphic
- movement
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Expired - Fee Related, expires
Links
- 230000033001 locomotion Effects 0.000 claims abstract description 143
- 238000000034 method Methods 0.000 claims abstract description 32
- 230000004044 response Effects 0.000 claims abstract description 17
- 230000009471 action Effects 0.000 claims description 26
- 230000004424 eye movement Effects 0.000 claims description 14
- 230000006870 function Effects 0.000 claims description 13
- 230000001133 acceleration Effects 0.000 claims description 12
- 230000000007 visual effect Effects 0.000 claims description 10
- 230000000977 initiatory effect Effects 0.000 claims description 5
- 238000010586 diagram Methods 0.000 description 27
- 230000004886 head movement Effects 0.000 description 21
- 210000003128 head Anatomy 0.000 description 19
- 238000003860 storage Methods 0.000 description 11
- 230000003213 activating effect Effects 0.000 description 7
- 238000004891 communication Methods 0.000 description 7
- 230000003190 augmentative effect Effects 0.000 description 6
- 238000005516 engineering process Methods 0.000 description 6
- 230000003287 optical effect Effects 0.000 description 6
- 230000001953 sensory effect Effects 0.000 description 6
- 230000003993 interaction Effects 0.000 description 4
- 239000000463 material Substances 0.000 description 4
- 206010044565 Tremor Diseases 0.000 description 3
- 238000013500 data storage Methods 0.000 description 3
- 230000009849 deactivation Effects 0.000 description 3
- 238000012545 processing Methods 0.000 description 3
- 230000001413 cellular effect Effects 0.000 description 2
- 239000011248 coating agent Substances 0.000 description 2
- 238000000576 coating method Methods 0.000 description 2
- 210000005069 ears Anatomy 0.000 description 2
- 239000011159 matrix material Substances 0.000 description 2
- 238000012986 modification Methods 0.000 description 2
- 230000004048 modification Effects 0.000 description 2
- 239000007787 solid Substances 0.000 description 2
- 230000001131 transforming effect Effects 0.000 description 2
- 206010028347 Muscle twitching Diseases 0.000 description 1
- 230000003466 anti-cipated effect Effects 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- 230000008859 change Effects 0.000 description 1
- 238000006243 chemical reaction Methods 0.000 description 1
- 238000005520 cutting process Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 230000026058 directional locomotion Effects 0.000 description 1
- 230000005057 finger movement Effects 0.000 description 1
- 239000011521 glass Substances 0.000 description 1
- 230000009191 jumping Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 238000004519 manufacturing process Methods 0.000 description 1
- 239000002184 metal Substances 0.000 description 1
- 230000004118 muscle contraction Effects 0.000 description 1
- 238000004091 panning Methods 0.000 description 1
- 230000002093 peripheral effect Effects 0.000 description 1
- 238000007639 printing Methods 0.000 description 1
- 230000008569 process Effects 0.000 description 1
- 210000001525 retina Anatomy 0.000 description 1
- 230000035945 sensitivity Effects 0.000 description 1
- 230000035939 shock Effects 0.000 description 1
- 230000003068 static effect Effects 0.000 description 1
- 238000010897 surface acoustic wave method Methods 0.000 description 1
- 230000001755 vocal effect Effects 0.000 description 1
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/011—Arrangements for interaction with the human body, e.g. for user immersion in virtual reality
- G06F3/012—Head tracking input arrangements
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/0093—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00 with means for monitoring data relating to the user, e.g. head-tracking, eye-tracking
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/017—Head mounted
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/03—Arrangements for converting the position or the displacement of a member into a coded form
- G06F3/033—Pointing devices displaced or positioned by the user, e.g. mice, trackballs, pens or joysticks; Accessories therefor
- G06F3/0354—Pointing devices displaced or positioned by the user, e.g. mice, trackballs, pens or joysticks; Accessories therefor with detection of 2D relative movements between the device, or an operating part thereof, and a plane or surface, e.g. 2D mice, trackballs, pens or pucks
- G06F3/03547—Touch pads, in which fingers can move on a surface
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/04815—Interaction with a metaphor-based environment or interaction object displayed as three-dimensional, e.g. changing the user viewpoint with respect to the environment or object
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0481—Interaction techniques based on graphical user interfaces [GUI] based on specific properties of the displayed interaction object or a metaphor-based environment, e.g. interaction with desktop elements like windows or icons, or assisted by a cursor's changing behaviour or appearance
- G06F3/0482—Interaction with lists of selectable items, e.g. menus
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/0485—Scrolling or panning
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/0101—Head-up displays characterised by optical features
- G02B2027/014—Head-up displays characterised by optical features comprising information/image processing systems
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/017—Head mounted
- G02B2027/0178—Eyeglass type
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/01—Head-up displays
- G02B27/0179—Display position adjusting means not related to the information to be displayed
- G02B2027/0187—Display position adjusting means not related to the information to be displayed slaved to motion of at least a part of the body of the user, e.g. head, eye
Definitions
- Computing devices such as personal computers, laptop computers, tablet computers, cellular phones, and countless types of Internet-capable devices are prevalent in numerous aspects of modern life. Over time, the manner in which these devices are providing information to users is becoming more intelligent, more efficient, more intuitive, and/or less obtrusive.
- wearable computing The trend toward miniaturization of computing hardware, peripherals, as well as of sensors, detectors, and image and audio processors, among other technologies, has helped open up a field sometimes referred to as “wearable computing.”
- wearable displays In the area of image and visual processing and production, in particular, it has become possible to consider wearable displays that place a very small image display element close enough to a wearer's (or user's) eye(s) such that the displayed image fills or nearly fills the field of view, and appears as a normal sized image, such as might be displayed on a traditional image display device.
- the relevant technology may be referred to as “near-eye displays.”
- Near-eye displays are fundamental components of wearable displays, also sometimes called “head-mounted displays” (HMDs).
- a head-mounted display places a graphic display or displays close to one or both eyes of a wearer.
- a computer processing system may be used to generate the images on a display.
- Such displays may occupy a wearer's entire field of view, or only occupy part of wearer's field of view.
- head-mounted displays may be as small as a pair of glasses or as large as a helmet.
- Augmented reality generally refers to a real-time view of a real-world environment that is augmented with additional content.
- a user experiences augmented reality through the use of a computing device.
- the computing device is typically configured to generate the real-time view of the environment, either by allowing a user to directly view the environment or by allowing the user to indirectly view the environment by generating and displaying a real-time representation of the environment to be viewed by the user.
- the computing device is typically configured to generate the additional content.
- the additional content may include, for example, a user-interface through which the user may interact with the computing device.
- the computing device overlays the view of the environment with the user-interface, such that the user sees the view of the environment and the user-interface at the same time.
- a system may include a non-transitory computer-readable medium, as well as program instructions stored on the non-transitory computer-readable medium that are executable by at least one processor to: (1) display a pointer in a graphic display, where at least one graphic object is also displayed in the graphic display; (2) receive body-movement data that is indicative of body movement; (3) use the body-movement data as a basis to move the pointer in the graphic display; (4) define an active region in an area of the graphic display that corresponds to the graphic object; (5) define an expanded active region in an area of the graphic display that encompasses and is larger than the active region, and (6) make the graphic object active in response to the pointer being moved into the active region and then keep the graphic object active until the pointer is moved outside of the expanded active region.
- a computer-implemented method includes: (1) displaying a pointer and at least one graphic object in a graphic display of a head-mounted display (HMD), where movement of the pointer in the graphic display is based on the body-movement data; (2) determining that the pointer is located within an active region, where the active region includes an area of the graphic display that corresponds to a graphic object; and (3) in response to determining that the pointer is located within the active region: (a) making the graphic object active; and (b) keeping the graphic object active so long as the pointer is within an expanded active region, where the expanded active region comprises an area of the graphic display that encompasses and is larger than the active region.
- HMD head-mounted display
- a non-transitory computer readable medium may include program instructions stored on the non-transitory computer-readable medium and executable by at least one processor to perform functions that include: (1) displaying a pointer and at least one graphic object in a graphic display of a head-mounted display (HMD), where movement of the pointer in the graphic display is based on the body-movement data; (2) determining that the pointer is located within an active region, where the active region includes an area of the graphic display that corresponds to a graphic object; and (3) in response to determining that the pointer is located within the active region: (a) making the graphic object active; and (b) keeping the graphic object active so long as the pointer is within an expanded active region, where the expanded active region comprises an area of the graphic display that encompasses and is larger than the active region.
- HMD head-mounted display
- FIG. 1 is a flow chart illustrating a method for interacting with a graphic object, according to an exemplary embodiment.
- FIGS. 2A , 2 B, and 2 C are diagrams illustrating body-movement and corresponding pointer movement, according to an exemplary embodiment.
- FIG. 3A is a diagram illustrating a pointer in a graphic display, according to an exemplary embodiment.
- FIG. 3B is a diagram illustrating an interaction with a graphic object, according to an exemplary embodiment.
- FIG. 4 is a diagram illustrating a substantially transparent display, according to an exemplary embodiment.
- FIG. 5A is a diagram illustrating a first example system for receiving, transmitting, and displaying data, according to an exemplary embodiment.
- FIG. 5B is a diagram illustrating an alternate view of the system illustrated in FIG. 5A , according to an exemplary embodiment.
- FIG. 6A is a diagram illustrating a second example system for receiving, transmitting, and displaying data, according to an exemplary embodiment.
- FIG. 6B is a diagram illustrating a third example system for receiving, transmitting, and displaying data, according to an exemplary embodiment.
- FIG. 7 is a simplified block diagram illustrating an example computer network infrastructure, according to an exemplary embodiment.
- FIG. 8 is a simplified block diagram illustrating example components of an example computing system, according to an exemplary embodiment.
- FIG. 9A is a diagram illustrating aspects of an example user-interface, according to an exemplary embodiment.
- FIG. 9B is a diagram illustrating aspects of an example user-interface after receiving movement data corresponding to an upward movement, according to an exemplary embodiment.
- FIG. 9C is a diagram illustrating aspects of an example user-interface after selection of a selected content object, according to an exemplary embodiment.
- FIG. 9D is a diagram illustrating aspects of an example user-interface after receiving input data corresponding to a user input, according to an exemplary embodiment.
- a wearable computer may include a graphical display (e.g., such as a head-mounted display (HMD) or heads-up display (HUD)).
- a wearable-computer may also be configured for eye tracking functionality, such that it can generally track a wearer's eye movements.
- a wearable computer may include sensors, such as a gyroscope, an accelerometer, and/or a magnetometer, which may indicate a wearer's head movements. Configured as such, a wearable computer may allow a wearer to provide input data via eye movement and/or via head-movement.
- a wearable computer may display a moveable pointer (e.g., a selection icon, cursor, arrow, indicator, reticle, or other graphic icon) in the display of its HMD.
- the pointer may include a tip, such as a cursor, to provide an indication of its computational point.
- the pointer may be a reticle providing a given position (e.g., the center) of a display as its computational point.
- the wearable computer may allow a user to control the movement and/or positioning of the pointer based on eye- and/or head-movements.
- the pointer may operate much like a mouse pointer and the graphic object may function similar to a desktop icon on a personal computer.
- such a comparison of aspects of the disclosure herein to other known computing systems is for purposes of example only, and should not be taken to be limiting, as the pointer may take other forms without departing from the scope of the invention.
- moving the pointer over a graphic object may put a graphic object in an “active” state (herein, making a graphic object active may interchangeably be referred to as activating the graphic object).
- a graphic object is active, the user may initiate an action that is associated with the graphic object (e.g., opening a file or hyperlink that is associated with the graphic object).
- a wearer might use eye-movements to move the pointer over a thumbnail image representing a document.
- the wearer may blink or voice a command (e.g., saying “open”) in order to initiate an action associated with the thumbnail image (e.g., opening a document represented by the thumbnail image).
- allowing eye- and/or head-movement control of a pointer may allow for hands-free use of the wearable computer.
- using human interface devices to interact with an activated graphic object might also lead to unintended eye- and/or head-movement.
- the act of moving the arm to tap a touchpad, in an attempt to select an activated object might cause head movement that results in the pointer moving away from the graphic object.
- the graphic object may be deactivated before the user taps the touchpad, such that the desired interaction with the graphic object does not occur.
- exemplary embodiments may help a wearer of an HMD use a pointer to interact with graphic objects on an HMD.
- a graphic object may be initially activated when the pointer is moved into an active region, which is typically the area of the display that is occupied by the object. Responsive to making the graphic object active, an expanded active region may encompass the pointer. Therefore, once activated, the graphic object may be kept active so long as the pointer remains within an expanded active region, which includes and extends beyond the area of the display that is occupied by the active region.
- FIG. 1 is a flow chart illustrating a method for interacting with a graphic object, according to an exemplary embodiment.
- method 100 is described by way of example as being carried out by a wearable computer, and in particular, by a wearable computer that includes a head-mounted display (HMD).
- HMD head-mounted display
- exemplary methods such as method 100
- devices other than a wearable computer may be carried out by sub-systems in a wearable computer or in other devices.
- an exemplary method may alternatively be carried out by a device such as a mobile phone, which is programmed to simultaneously display a graphic object in a graphic display and also provide a point-of-view video feed in a physical-world window.
- a device such as a mobile phone, which is programmed to simultaneously display a graphic object in a graphic display and also provide a point-of-view video feed in a physical-world window.
- Other examples are also possible.
- method 100 involves a wearable computer displaying a pointer and at least one graphic object in a graphic display of an HMD. Further, the wearable computer receives body-movement data that is indicative of body movement, as shown by block 104 . The pointer may be moved in the graphic display according to the body movements indicated by the body-movement data, as shown by block 105 . At some point, the wearable computer may determine that the pointer is located within an active region that corresponds to the graphic object, as shown by block 106 .
- the wearable computer makes the graphic object active, as shown by block 108 .
- the wearable computer then keeps the graphic object active so long as the pointer is within an expanded active region that is associated with the graphic object, as shown by block 110 .
- the expanded active region encompasses and is larger than the active region.
- the wearable computer may receive input data to initiate an action associated with the graphic object, as shown by block 112 .
- Method 100 further involves the wearable computer displaying a pointer and at least one graphic object in a graphic display of an HMD, as shown by block 102 .
- displaying a pointer in the graphic display is based on body-movement data.
- FIGS. 2A , 2 B, and 2 C are diagrams illustrating body-movement and corresponding pointer movement, according to an exemplary embodiment.
- FIGS. 2A , 2 B, and 2 C illustrate displaying a pointer and a graphic object in a graphic display.
- FIG. 2A illustrates displaying pointer 210 with graphic object 214 on graphic display 212 .
- FIG. 2B pointer 210 is displayed with graphic object 214 on graphic display 212 .
- FIG. 2C pointer 210 is displayed with graphic object 214 on graphic display 212 .
- exemplary method 100 involves a wearable computer receiving body-movement data indicative of body movement, as shown by block 104 .
- sensory configurations may be used with a wearable computer to receive body-movement data that is indicative of body movement (e.g., movements of the head or eyes of a wearer of the wearable computer, hand gestures, arm movements, etc.).
- sensors may be mounted on the wearable computer or provided on other parts of the wearable to include more than one type of sensor device or element.
- example sensors could be any one or more of a motion detector (e.g., a gyroscope, an accelerometer, a camera, and/or a shock sensor), an impact sensor, a contact sensor (e.g., capacitive sensing device), a location determination device (e.g., a GPS device), a magnetometer, and an orientation sensor (e.g., a theodolite).
- a motion detector e.g., a gyroscope, an accelerometer, a camera, and/or a shock sensor
- an impact sensor e.g., a contact sensor (e.g., capacitive sensing device)
- a location determination device e.g., a GPS device
- magnetometer e.g., a magnetometer
- orientation sensor e.g., a theodolite
- a wearable computer may include one or more sensors to receive body-movement data corresponding to a wearer's body movement. In some instances, sensors in a wearable computer may receive body-movement data corresponding to movements of a wearer's head while wearing an HMD
- wearable computer 200 may include a variety of sensory configurations to receive body-movement data from vertical rotation of the head 202 .
- wearable computer 200 may utilize a variety of sensory configurations to receive body-movement data from horizontal rotation of the head 204 .
- wearable computer 200 may utilize a variety of sensory configurations to receive body-movement data from diagonal rotation of the head 206 .
- Other movements of the wearable computer are possible and the wearable computer may utilize a variety of sensory configurations to receive such data accordingly.
- exemplary method 100 involves a wearable computer using body-movement data to move a pointer, as shown by block 105 .
- the body-movement data received from eye- and/or head movement may correspond to pointer movement in a graphic display.
- body-movement data may be based on spherical coordinates in a three-dimensional space corresponding to the wearable computer.
- such body-movement data may be mathematically converted to Cartesian coordinates in a two-dimensional space, perhaps shown on a graphical display.
- software algorithms including 3D computational language and/or numerical computing environments, may be utilized to make such conversions.
- one or more software algorithms may be used to create a speed parameter to map a given amount of body movement to an amount of pointer movement.
- the body-movement data received from eye- and/or head movement while wearing an HMD may control the pointer movement in the graphic display.
- wearable computer 200 may receive data associated with vertical rotation of the head 202 and responsively cause pointer 210 to move on a vertical path 208 along the y-axis shown in graphic display 212 .
- wearable computer 200 may receive data associated with horizontal rotation of the head 204 and responsively cause pointer 210 to move on a horizontal path 216 along the x-axis shown in graphic display 212 .
- FIG. 2A wearable computer 200 may receive data associated with vertical rotation of the head 202 and responsively cause pointer 210 to move on a vertical path 208 along the y-axis shown in graphic display 212 .
- wearable computer 200 may receive data associated with horizontal rotation of the head 204 and responsively cause pointer 210 to move on a horizontal path 216 along the x-axis shown in graphic display 212 .
- wearable computer 200 may receive data associated with diagonal rotation of the head 206 and responsively cause pointer 210 to move on a diagonal path 218 with respect to both the x-axis and y-axis in graphic display 212 . It should be understood that many other body movements and corresponding movements of the pointer are also possible.
- the pointer may not provide an indication of its computational point (e.g., may not be visible) and instead, may simply be defined by a position in the graphic display, such as the center of the graphic display.
- the pointer may be fixed in one position of the graphic display (e.g., the center of the graphic display) and the body-movement data may move the entire graphic display throughout a larger navigable area (e.g., content in the graphic display).
- the pointer may not be fixed to any position in the graphic display and may move relative to the bounds of the graphic display.
- the graphic display may adjust its view to show additional portions of the navigable area as indicated and/or directed by the pointer's movement. Other possibilities may also exist.
- Method 100 further involves determining that the pointer is located within an active region that corresponds to the graphic object, as shown by block 106 .
- the active region is the area of the graphic display in which the graphic object is displayed.
- the boundary of the active region may accordingly be defined by the edge or outline of the graphic object.
- FIG. 3A is a diagram illustrating a pointer in a graphic display, according to an exemplary embodiment.
- FIG. 3A illustrates pointer 304 in graphic display 302 , which can be visibly provided to a wearer of an HMD.
- graphic object 306 which includes active region 308 defined by the area inside of the outlining border for graphic object 306 .
- body-movement data received from eye- and/or head movement while wearing the HMD may cause the pointer to position itself within the active region.
- a wearer of an HMD may make head movements to move pointer 304 into active region 308 .
- the active region may be larger than the defined edge or outline of the graphic object. Further in some instances, a larger active region may be implemented in practice such that it is programmatically distinct from an active region that is defined by the edge or outline of the graphic object. However, in practice, a pointer may enter a larger active region in a similar way as with an active region that is defined by the edge or outline of the graphic object.
- an active region may be virtually defined but may not be visible to a wearer of the HMD.
- an active region may be bigger than the defined edge or outline of the graphic object and not visible to a wearer of the HMD.
- an active region may be smaller than the defined edge or outline of the graphic object and not visible to a wearer of the HMD.
- active region 308 may be visible to the wearer to further facilitate using eye- and/or head-movements to move pointer 304 into active region 308 .
- an active region may adjust its size, shape, and/or color depending on its surroundings. In particular of these embodiments, the active region may make such adjustments to make it easier to position the pointer into the active region. For example, active regions may shrink, expand, or take the form of various shapes based on the amount of free space available on the graphic display. In some instances, an active region may adjust to the amount of free space surrounding the graphic object such that it does not overlap with other active regions respective to other graphic objects.
- an active region may be temporarily visible on the graphic display.
- a visual representation of the active region may only appear as the pointer approaches the vicinity of an active region respective to the graphic object. Then, after the pointer moves away from the active region without activating the graphic object, the visual representation of the active region may gradually fade away.
- a representation of the active region may only appear when the pointer crosses the boundary of the active region. Thereafter, the pointer may move away from the active region and the visual representation of the active region may fade away.
- method 100 further involves making the graphic object active, as shown by block 108 .
- a graphic object becomes active when the pointer enters an active region respective to the graphic object.
- a graphic object may become active (i.e., be activated) when the pointer enters the active region respective to the graphic object and is completely encompassed within the active region.
- a pointer may be a collection of points where one or more may have to enter the active region to activate the graphic object or alternatively, exit an expanded active region to deactivate it.
- graphic object 306 may become active when pointer 304 fully enters active region 308 such that it is encompassed by the edge or outline of graphic object 306 .
- active region 308 may be defined by a set of (x1 . . . xn, y1 . . . yn) points in a coordinate system embedded within graphic display 302 .
- pointer 304 may be defined a location (x, y) within graphic display 302 such that its location (x, y) changes corresponding to the body-movement data received.
- the location (x, y) of pointer 304 matches any of the (x1 . . . xn, y1 . . . yn) points and/or coordinates that define active region 308 , it may be determined when pointer 308 enters active region 308 , resulting in making graphic object 306 active.
- a graphic object may become active when the pointer is partially encompassed within the active region.
- a pointer 304 may also be defined by a set of (x1 . . . xn, y1 . . . yn) points in a coordinate system embedded within graphic display 302 . Further, pointer 304 may be partially encompassed within active region 308 , making graphic object 306 active. Other possibilities may also exist.
- method 100 further involves keeping the graphic object active so long as the pointer is within the expanded active region, as shown by block 110 .
- the graphic object may be larger than the outlining border of the graphic object, making it easier to activate the graphic object.
- the expanded active region is typically defined to be larger than the active region and/or encompasses the active region.
- the expanded active region may be a similar shape as the active region.
- expanded active region 310 is a large circle encompassing and including active region 308 , which also takes the form of a circle.
- expanded active regions can take the form of other shapes as well.
- graphic object 306 may continue to be active so long as pointer 304 is within expanded active region 310 . Once pointer 304 moves outside of expanded active region 310 , graphic object 306 may be deactivated and pointer 304 may activate other graphic objects. Although, in some embodiments, pointer 304 may activate more than one graphic object. Other possibilities may also exist.
- a speed parameter may be used to map a given amount of body movement to an amount of pointer movement (such as a first amount of pointer movement.) Further, in some embodiments, the speed parameter may be adjusted so as to map the amount of body movement to a second amount of pointer movement.
- the second amount of pointer movement is less than the first amount of pointer movement originally mapped to the given amount of body movement.
- the sensitivity of the pointer may be configured in terms of counts per inch (CPI), which includes the number of counts for the pointer to move one inch on the graphic display.
- CPI counts per inch
- the CPI may be increased when pointer is in the expanded active region.
- pointer 304 may move slower with corresponding body movements within expanded active region 310 as opposed to areas outside of expanded active region 310 .
- pointer 304 may move slower to account for any natural jitter that could cause graphic object 306 to be inadvertently deactivated.
- a predetermined exit velocity may be required to move the pointer outside of the expanded active region.
- a predetermined exit velocity may include a specific movement of the wearable computer and/or the HMD, which may result from body movement of the wearer.
- data received from horizontal rotation of the head 204 may correspond to pointer 210 moving along the x-axis, away from graphic object 214 .
- horizontal rotation of the head 204 while wearing wearable computer 200 may be a specific movement that causes a deactivation of graphic object 214 .
- Other possibilities may also exist.
- a determined velocity exceeding a predetermined exit velocity may cause the pointer to move outside of the expanded active region. For example, referring back to FIG. 3A , a head movement with a specific velocity may exceed a predetermined exit velocity such that pointer 304 moves outside of expanded active region 310 , causing a deactivation of graphic object 306 .
- the velocity of the head movement may be continuously sampled and compared with a threshold or predetermined exit velocity. Once the threshold or predetermined exit velocity is exceeded, pointer 304 may move outside of the expanded active region 310 .
- a predetermined exit acceleration may be required to move pointer 304 outside of the expanded active region 310 , resulting in deactivating graphic object 306 . Similar to the predetermined exit velocity described above, the acceleration of the head movement may be compared with a predetermined exit acceleration and once predetermined exit acceleration is exceeded, pointer 304 may move outside of expanded active region 310 . Other possibilities may also exist.
- expanded active regions may vary in shape.
- expanded active regions may be custom-designed for a particular pattern of head movements unique to a wearer of the HMD and/or wearable computer. For example, a particular wearer may make a regular tremor such that the HMD moves continuously in a certain diagonal pattern. In such a case, the expanded active region may be designed and shaped to offset such movements corresponding to the regular tremor.
- the expanded active region may be virtually defined, but not visible to a wearer of the HMD.
- the expanded active region may also be visible to the wearer once the respective graphic object is activated.
- a visual representation of the expanded active region 310 may be provided on graphic display 302 to further facilitate in keeping pointer 304 within expanded active region 310 .
- an expanded active region may be modified.
- an expanded active region may change based on interacting with the graphic object.
- FIG. 3B illustrates activating a graphic object, according to an exemplary embodiment.
- FIG. 3B illustrates pointer 304 which activates graphic object 306 .
- FIG. 3B may include the same or similar graphic display 302 , pointer 304 , graphic object 306 , active region 308 , and expanded active region 310 as shown in FIG. 3B .
- An exemplary method may further involve receiving input data while a graphic object is active, which corresponds to an action that is associated with the graphic object, as shown by block 112 .
- Receiving input data may involve, but is not limited to, receiving eye- and/or head-movement data from a head-mounted display (HMD) associated with a wearable computer, receiving vocal signals provided by a wearer of the HMD, and/or receiving input data from touchpad and/or keypad associated with the wearable computer.
- HMD head-mounted display
- Other types of input data are also possible.
- body movements may include a first head movement that may be used to position the pointer in the active region of a graphic object, thus activating the graphic object.
- body movements including a second head movement, which is typically different than the first head movement, may be used to initiate an action.
- actions may be initiated while a graphic object is active. For example, such actions may include selecting the graphic object, opening a file associated with the graphic object, displaying information associated with the graphic object, zooming in on the graphic object, viewing information regarding relationships between the graphic object and other graphic objects, and/or providing an option menu to specify an action associated with the graphic object. Other actions are also possible.
- an action may be initiated when the pointer enters the active region and remains in the active region for a period of time exceeding a predetermined period of time.
- determining that the pointer has been active for a threshold period of time may responsively trigger selecting the graphic object.
- a counter may be used to compute the number of units in time (e.g., milliseconds, seconds, etc.) pointer 304 is within active region 308 . Once the number of units meet or exceed a threshold period of time, graphic object 306 may be selected.
- option menu 312 may appear upon selecting graphic object 306 .
- Pointer 304 may then move onto option menu 312 and position on arrow 314 to view other option menus for opening graphic object 306 .
- further actions may be initiated such as printing graphic object 306 , sending graphic object 306 to another location, cutting graphic object 306 , copying graphic object 306 , and/or deleting graphic object 306 .
- Other actions are also possible.
- an action may be initiated once the pointer enters the active region and moves at a rate of movement less than a predetermined rate of movement. For example, in FIG. 3B , a rate of movement for pointer 304 may be determined after pointer 304 enters active region 308 .
- the rate of movement may be determined by measuring the distance traveled by pointer 304 (e.g., pixels, inches, etc.) over a given period of time (e.g., milliseconds, seconds, etc.) Further, this rate of movement may be monitored and/or compared with a predetermined threshold rate of movement for selecting graphic object 306 . Once this rate of movement drops below the predetermined rate of movement, graphic object 306 may be selected, perhaps causing option menu 312 to appear.
- FIG. 4 is a diagram illustrating a substantially transparent display, according to an exemplary embodiment.
- an example system 400 is shown in the form of a wearable computing device.
- Graphic display 402 is provided through the left frame of the wearable computing device and is substantially transparent to provide a view of the physical world 416 .
- graphic display 402 includes graphic objects 404 , 410 , and 414 .
- Pointer 410 is shown within active region 412 and expanded active region 406 , possibly selecting graphic object 410 .
- active region 412 and expanded active region 406 are shown with a semi-transparent background, providing a visual representation of the regions.
- active region 412 and expanded active region 406 may be provided virtually but without a visual representation to the wearer of the wearable computing device.
- graphic objects 404 , 410 , and 414 are shown in graphic display 402 by adapting to the background of physical world 416 .
- graphic object 404 is shown with a dark background to illustrate its contrast with a lighter area of physical world 416 .
- graphic object 414 is shown with a light background to also illustrate its contrast with a darker area of physical world 416 .
- Both graphic object 414 and 416 are shown to adapt to physical world 416 and to improve the wearer's ability to see the graphic objects within the graphic display 402 .
- FIG. 5A is a diagram illustrating a first example system for receiving, transmitting, and displaying data, according to an exemplary embodiment.
- the system 500 is shown in the form of a wearable computing device. While FIG. 5A illustrates a head-mounted device 502 as an example of a wearable computing device, other types of wearable computing devices could additionally or alternatively be used.
- the head-mounted device 502 has frame elements including lens-frames 504 , 506 and a center frame support 508 , lens elements 510 , 512 , and extending side-arms 514 , 516 .
- the center frame support 508 and the extending side-arms 514 , 516 are configured to secure the head-mounted device 502 to a user's face via a user's nose and ears, respectively.
- Each of the frame elements 504 , 506 , and 508 and the extending side-arms 514 , 516 may be formed of a solid structure of plastic and/or metal, or may be formed of a hollow structure of similar material so as to allow wiring and component interconnects to be internally routed through the head-mounted device 502 . Other materials may be possible as well.
- each of the lens elements 510 , 512 may be formed of any material that can suitably display a projected image or graphic.
- Each of the lens elements 510 , 512 may also be sufficiently transparent to allow a user to see through the lens element. Combining these two features of the lens elements may facilitate an augmented reality or heads-up display where the projected image or graphic is superimposed over a real-world view as perceived by the user through the lens elements 510 , 512 .
- the extending side-arms 514 , 516 may each be projections that extend away from the lens-frames 504 , 506 , respectively, and may be positioned behind a user's ears to secure the head-mounted device 502 to the user.
- the extending side-arms 514 , 516 may further secure the head-mounted device 502 to the user by extending around a rear portion of the user's head.
- the system 500 may connect to or be affixed within a head-mounted helmet structure. Other possibilities exist as well.
- the system 500 may also include an on-board computing system 518 , a video camera 520 , a sensor 522 , and a finger-operable touch pad 524 .
- the on-board computing system 518 is shown to be positioned on the extending side-arm 514 of the head-mounted device 502 ; however, the on-board computing system 518 may be provided on other parts of the head-mounted device 502 or may be positioned remote from the head-mounted device 502 (e.g., the on-board computing system 518 could be connected by wires or wirelessly connected to the head-mounted device 502 ).
- the on-board computing system 518 may include a processor and memory, for example.
- the on-board computing system 518 may be configured to receive and analyze data from the video camera 520 , the sensor 522 , and the finger-operable touch pad 524 (and possibly from other sensory devices, user-interfaces, or both) and generate images for output by the lens elements 510 and 512 .
- the on-board computing system 518 may additionally include a speaker or a microphone for user input (not shown).
- An example computing system is further described below in connection with FIG. 8 .
- the video camera 520 is shown positioned on the extending side-arm 514 of the head-mounted device 502 ; however, the video camera 520 may be provided on other parts of the head-mounted device 502 .
- the video camera 520 may be configured to capture images at various resolutions or at different frame rates. Video cameras with a small form-factor, such as those used in cell phones or webcams, for example, may be incorporated into an example embodiment of the system 500 .
- FIG. 5A illustrates one video camera 520
- more video cameras may be used, and each may be configured to capture the same view, or to capture different views.
- the video camera 520 may be forward facing to capture at least a portion of the real-world view perceived by the user. This forward facing image captured by the video camera 520 may then be used to generate an augmented reality where computer generated images appear to interact with the real-world view perceived by the user.
- the sensor 522 is shown on the extending side-arm 516 of the head-mounted device 502 ; however, the sensor 522 may be positioned on other parts of the head-mounted device 502 .
- the sensor 522 may include one or more of a gyroscope or an accelerometer, for example. Other sensing devices may be included within, or in addition to, the sensor 522 or other sensing functions may be performed by the sensor 522 .
- the finger-operable touch pad 524 is shown on the extending side-arm 514 of the head-mounted device 502 . However, the finger-operable touch pad 524 may be positioned on other parts of the head-mounted device 502 . Also, more than one finger-operable touch pad may be present on the head-mounted device 502 .
- the finger-operable touch pad 524 may be used by a user to input commands.
- the finger-operable touch pad 524 may sense at least one of a position and a movement of a finger via capacitive sensing, resistance sensing, or a surface acoustic wave process, among other possibilities.
- the finger-operable touch pad 524 may be capable of sensing finger movement in a direction parallel or planar to the pad surface, in a direction normal to the pad surface, or both, and may also be capable of sensing a level of pressure applied to the pad surface.
- the finger-operable touch pad 524 may be formed of one or more translucent or transparent insulating layers and one or more translucent or transparent conducting layers. Edges of the finger-operable touch pad 524 may be formed to have a raised, indented, or roughened surface, so as to provide tactile feedback to a user when the user's finger reaches the edge, or other area, of the finger-operable touch pad 524 . If more than one finger-operable touch pad is present, each finger-operable touch pad may be operated independently, and may provide a different function.
- FIG. 5B is a diagram illustrating an alternate view of the system illustrated in FIG. 5A , according to an exemplary embodiment.
- the lens elements 510 , 512 may act as display elements.
- the head-mounted device 502 may include a first projector 528 coupled to an inside surface of the extending side-arm 516 and configured to project a display 530 onto an inside surface of the lens element 512 .
- a second projector 532 may be coupled to an inside surface of the extending side-arm 514 and configured to project a display 534 onto an inside surface of the lens element 510 .
- the lens elements 510 , 512 may act as a combiner in a light projection system and may include a coating that reflects the light projected onto them from the projectors 528 , 532 .
- a reflective coating may be omitted (e.g., when the projectors 528 , 532 are scanning laser devices).
- the lens elements 510 , 512 themselves may include: a transparent or semi-transparent matrix display, such as an electroluminescent display or a liquid crystal display, one or more waveguides for delivering an image to the user's eyes, or other optical elements capable of delivering an in focus near-to-eye image to the user.
- a corresponding display driver may be disposed within the frame elements 504 , 506 for driving such a matrix display.
- a laser or light emitting diode (LED) source and scanning system could be used to draw a raster display directly onto the retina of one or more of the user's eyes. Other possibilities exist as well.
- FIG. 6A is a diagram illustrating a second example system for receiving, transmitting, and displaying data, according to an exemplary embodiment.
- the system 600 is shown in the form of a wearable computing device 602 .
- the wearable computing device 602 may include frame elements and side-arms such as those described with respect to FIGS. 5A and 5B .
- the wearable computing device 602 may additionally include an on-board computing system 604 and a video camera 606 , such as those described with respect to FIGS. 5A and 5B .
- the video camera 606 is shown mounted on a frame of the wearable computing device 602 ; however, the video camera 606 may be mounted at other positions as well.
- the wearable computing device 602 may include a single display 608 which may be coupled to the device.
- the display 608 may be formed on one of the lens elements of the wearable computing device 602 , such as a lens element described with respect to FIGS. 5A and 5B , and may be configured to overlay computer-generated graphics in the user's view of the physical world.
- the display 608 is shown to be provided in a center of a lens of the wearable computing device 602 , however, the display 608 may be provided in other positions.
- the display 608 is controllable via the computing system 604 that is coupled to the display 608 via an optical waveguide 610 .
- FIG. 6B is a diagram illustrating a third example system for receiving, transmitting, and displaying data, according to an exemplary embodiment.
- the system 620 is shown in the form of a wearable computing device 622 .
- the wearable computing device 622 may include side-arms 623 , a center frame support 624 , and a bridge portion with nosepiece 625 .
- the center frame support 624 connects the side-arms 623 .
- the wearable computing device 622 does not include lens-frames containing lens elements.
- the wearable computing device 622 may additionally include an on-board computing system 626 and a video camera 628 , such as those described with respect to FIGS. 5A and 5B .
- the wearable computing device 622 may include a single lens element 630 that may be coupled to one of the side-arms 623 or the center frame support 624 .
- the lens element 630 may include a display such as the display described with reference to FIGS. 5A and 5B , and may be configured to overlay computer-generated graphics upon the user's view of the physical world.
- the single lens element 630 may be coupled to a side of the extending side-arm 623 .
- the single lens element 630 may be positioned in front of or proximate to a user's eye when the wearable computing device 622 is worn by a user.
- the single lens element 630 may be positioned below the center frame support 624 , as shown in FIG. 6B .
- FIG. 7 is a simplified block diagram illustrating an example computer network infrastructure, according to an exemplary embodiment.
- a device 710 communicates using a communication link 720 (e.g., a wired or wireless connection) to a remote device 730 .
- the device 710 may be any type of device that can receive data and display information corresponding to or associated with the data.
- the device 710 may be a heads-up display system, such as the head-mounted device 502 , 600 , or 620 described with reference to FIGS. 5A-6B .
- the device 710 may include a display system 712 comprising a processor 714 and a display 716 .
- the display 716 may be, for example, an optical see-through display, an optical see-around display, or a video see-through display.
- the processor 714 may receive data from the remote device 730 , and configure the data for display on the display 716 .
- the processor 714 may be any type of processor, such as a micro-processor or a digital signal processor, for example.
- the device 710 may further include on-board data storage, such as memory 718 coupled to the processor 714 .
- the memory 718 may store software that can be accessed and executed by the processor 714 , for example.
- the remote device 730 may be any type of computing device or transmitter including a laptop computer, a mobile telephone, or tablet computing device, etc., that is configured to transmit data to the device 710 .
- the remote device 730 and the device 710 may contain hardware to enable the communication link 720 , such as processors, transmitters, receivers, antennas, etc.
- the communication link 720 is illustrated as a wireless connection; however, wired connections may also be used.
- the communication link 720 may be a wired serial bus such as a universal serial bus or a parallel bus, among other connections.
- the communication link 720 may also be a wireless connection using, e.g., Bluetooth® radio technology, communication protocols described in IEEE 802.11 (including any IEEE 802.11 revisions), Cellular technology (such as GSM, CDMA, UMTS, EV-DO, WiMAX, or LTE), and/or Zigbee, among other possibilities. Either of such a wired and/or wireless connection may be a proprietary connection as well.
- the remote device 730 may be accessible via the Internet and may include a computing cluster associated with a particular web service (e.g., social-networking, photo sharing, address book, etc.).
- an example wearable computing device may include, or may otherwise be communicatively coupled to, a computing system, such as computing system 518 or computing system 604 .
- FIG. 8 is a simplified block diagram illustrating example components of an example computing system, according to an exemplary embodiment.
- One or both of the device 710 and the remote device 730 may take the form of computing system 800 .
- Computing system 800 may include at least one processor 802 and system memory 804 .
- computing system 800 may include a system bus 806 that communicatively connects processor 802 and system memory 804 , as well as other components of computing system 800 .
- processor 802 can be any type of processor including, but not limited to, a microprocessor ( ⁇ P), a microcontroller ( ⁇ C), a digital signal processor (DSP), or any combination thereof.
- system memory 804 can be of any type of memory now known or later developed including but not limited to volatile memory (such as RAM), non-volatile memory (such as ROM, flash memory, etc.) or any combination thereof.
- An example computing system 800 may include various other components as well.
- computing system 800 includes an A/V processing unit 808 for controlling graphical display 810 and speaker 812 (via A/V port 814 ), one or more communication interfaces 816 for connecting to other computing devices 818 , and a power supply 820 .
- Graphical display 810 may be arranged to provide a visual depiction of various input regions provided by user-interface module 822 .
- user-interface module 822 may be configured to provide a user-interface, such as the example user-interface described below in connection with other FIGS. 9A-D below and graphical display 810 may be configured to provide a visual depiction of the user-interface.
- FIG. 9A is a diagram illustrating aspects of an example user-interface, according to an exemplary embodiment.
- FIG. 9B is a diagram illustrating aspects of an example user-interface after receiving movement data corresponding to an upward movement, according to an exemplary embodiment.
- FIG. 9C is a diagram illustrating aspects of an example user-interface after selection of a selected content object, according to an exemplary embodiment.
- FIG. 9D is a diagram illustrating aspects of an example user-interface after receiving input data corresponding to a user input, according to an exemplary embodiment.
- User-interface module 822 may be further configured to receive data from and transmit data to (or be otherwise compatible with) one or more user-interface devices 828 .
- computing system 800 may also include one or more data storage devices 824 , which can be removable storage devices, non-removable storage devices, or a combination thereof.
- removable storage devices and non-removable storage devices include magnetic disk devices such as flexible disk drives and hard-disk drives (HDD), optical disk drives such as compact disk (CD) drives or digital versatile disk (DVD) drives, solid state drives (SSD), and/or any other storage device now known or later developed.
- Computer storage media can include volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information, such as computer readable instructions, data structures, program modules, or other data.
- computer storage media may take the form of RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium now known or later developed that can be used to store the desired information and which can be accessed by computing system 800 .
- computing system 800 may include program instructions 826 that are stored in system memory 804 (and/or possibly in another data-storage medium) and executable by processor 802 to facilitate the various functions described herein including, but not limited to, those functions described with respect to FIG. 1 .
- program instructions 826 are stored in system memory 804 (and/or possibly in another data-storage medium) and executable by processor 802 to facilitate the various functions described herein including, but not limited to, those functions described with respect to FIG. 1 .
- system memory 804 and/or possibly in another data-storage medium
- processor 802 may include program instructions 826 that are stored in system memory 804 (and/or possibly in another data-storage medium) and executable by processor 802 to facilitate the various functions described herein including, but not limited to, those functions described with respect to FIG. 1 .
- various components of computing system 800 are shown as distributed components, it should be understood that any of such components may be physically integrated and/or distributed according to the desired configuration of the computing system.
- FIGS. 9A-D show aspects of an example user-interface 900 .
- the user-interface 900 may be displayed by, for example, a wearable computing device as described above for FIGS. 5A-6B .
- FIG. 9A An example state of the user-interface 900 is shown in FIG. 9A .
- the example state shown in FIG. 9A may correspond to a first position of the wearable computing device. That is, the user-interface 900 may be displayed as shown in FIG. 9A when the wearable computing device is in the first position.
- the first position of the wearable computing device may correspond to a position of the wearable computing device when a wearer of the wearable computing device is looking in a direction that is generally parallel to the ground (e.g., a position that does not correspond to the wearer looking up or looking down). Other examples are possible as well.
- the user-interface 900 includes a view region 902 .
- An example boundary of the view region 902 is shown by a dotted frame. While the view region 902 is shown to have a landscape shape (in which the view region 902 is wider than it is tall), in other embodiments the view region 902 may have a portrait or square shape, or may have a non-rectangular shape, such as a circular or elliptical shape. The view region 902 may have other shapes as well.
- the view region 902 may be, for example, the viewable area between (or encompassing) the upper, lower, left, and right boundaries of a display on the wearable computing device. As shown, when the wearable computing device is in the first position, the view region 902 is substantially empty (e.g., completely empty) of user-interface elements, such that the user's view of their real-world environment is generally uncluttered, and objects in the user's environment are not obscured.
- the view region 902 may correspond to a field of view of a wearer of the wearable computing device, and an area outside the view region 902 may correspond to an area outside the field of view of the wearer. In other embodiments, the view region 902 may correspond to a non-diagonal portion of a field of view of a wearer of the wearable computing device, and an area outside the view region 902 may correspond to a diagonal portion of the field of view of the wearer. In still other embodiments, the user-interface 900 may be larger than or substantially the same as a field of view of a wearer of the wearable computing device, and the field of view of the wearer may be larger than or substantially the same size as the view region 902 . The view region 902 may take other forms as well.
- the portions of the user-interface 900 outside of the view region 902 may be outside of or in a diagonal portion of a field of view of a wearer of the wearable computing device.
- a menu 904 may be outside of or in a diagonal portion of the field of view of the user in the user-interface 900 . While the menu 904 is shown to be not visible in the view region 902 , in some embodiments the menu 904 may be partially visible in the view region 902 .
- the wearable computing device may be configured to receive movement data corresponding to, for example, an upward movement of the wearable computing device to a position above the first position.
- the wearable computing device may, in response to receiving the movement data corresponding to the upward movement, cause one or both of the view region 902 and the menu 904 to move such that the menu 904 becomes more visible in the view region 902 .
- the wearable computing device may cause the view region 902 to move upward and may cause the menu 904 to move downward.
- the view region 902 and the menu 904 may move the same amount, or may move different amounts.
- the menu 904 may move further than the view region 902 .
- the wearable computing device may cause only the menu 904 to move. Other examples are possible as well.
- the upward movement may encompass any movement having any combination of moving, tilting, rotating, shifting, sliding, or other movement that results in a generally upward movement. Further, in some embodiments “upward” may refer to an upward movement in the reference frame of a wearer of the wearable computing device. Other reference frames are possible as well. In embodiments where the wearable computing device is a head-mounted device, the upward movement of the wearable computing device may also be an upward movement of a wearer's head such as, for example, the user looking upward.
- the movement data corresponding to the upward movement may take several forms.
- the movement data may be (or may be derived from) data received from one or more movement sensors, accelerometers, and/or gyroscopes configured to detect the upward movement, such as the sensor 922 described above in connection with FIG. 9A .
- the movement data may comprise a binary indication corresponding to the upward movement.
- the movement data may comprise an indication corresponding to the upward movement as well as an extent of the upward movement.
- the movement data may take other forms as well.
- FIG. 9B shows aspects of an example user-interface after receiving movement data corresponding to an upward movement.
- the user-interface 900 includes the view region 902 and the menu 904 .
- the view region 902 may be moved only when the upward movement exceeds a threshold speed, acceleration, and/or magnitude. In response to receiving data corresponding to an upward movement that exceeds such a threshold or thresholds, the view region 902 may pan, scroll, slide, or jump to a new field of view. The view region 902 may be moved in other manners as well.
- the wearable computing device could be configured to receive data corresponding to other directional movement (e.g., downward, leftward, rightward, etc.) as well, and that the view region 902 may be moved in response to receiving such data in a manner similar to that described above in connection with upward movement.
- other directional movement e.g., downward, leftward, rightward, etc.
- the menu 904 includes a number of content objects 906 .
- the content objects 906 may be arranged in a ring (or partial ring) around and above the head of a wearer of the wearable computing device.
- the content objects 906 may be arranged in a dome-shape above the wearer's head. The ring or dome may be centered above the wearable computing device and/or the wearer's head.
- the content objects 906 may be arranged in other ways as well.
- the number of content objects 906 in the menu 904 may be fixed or may be variable. In embodiments where the number is variable, the content objects 906 may vary in size according to the number of content objects 906 in the menu 904 . In embodiments where the content objects 906 extend circularly around a wearer's head, like a ring (or partial ring), only some of the content objects 906 may be visible at a particular moment. In order to view other content objects 904 , a wearer of the wearable computing device may interact with the wearable computing device to, for example, rotate the content objects 906 along a path (e.g., clockwise or counterclockwise) around the wearer's head.
- a path e.g., clockwise or counterclockwise
- the wearable computing device may be configured to receive data indicating such an interaction through, for example, a touch pad, such as finger-operable touch pad 924 .
- the wearable computing device may be configured to receive such data through other input devices as well.
- the content objects 906 may take several forms.
- the content objects 906 may include one or more of people, contacts, groups of people and/or contacts, calendar items, lists, notifications, alarms, reminders, status updates, incoming messages, recorded media, audio recordings, video recordings, photographs, digital collages, previously-saved states, webpages, and applications, as well as tools, such as a still camera, a video camera, and an audio recorder.
- Content objects 906 may take other forms as well.
- the tools may be located in a particular region of the menu 904 , such as the center. In some embodiments, the tools may remain in the center of the menu 904 , even if the other content objects 906 rotate, as described above. Tool content objects may be located in other regions of the menu 904 as well.
- the particular content objects 906 that are included in menu 904 may be fixed or variable.
- the content objects 906 may be preselected by a wearer of the wearable computing device.
- the content objects 906 for each content region may be automatically assembled by the wearable computing device from one or more physical or digital contexts including, for example, people, places, and/or objects surrounding the wearable computing device, address books, calendars, social-networking web services or applications, photo sharing web services or applications, search histories, and/or other contexts.
- some content objects 906 may fixed, while the content objects 906 may be variable.
- the content objects 906 may be selected in other manners as well.
- an order or configuration in which the content objects 906 are displayed may be fixed or variable.
- the content objects 906 may be pre-ordered by a wearer of the wearable computing device.
- the content objects 906 may be automatically ordered based on, for example, how often each content object 906 is used (on the wearable computing device only or in other contexts as well), how recently each content object 906 was used (on the wearable computing device only or in other contexts as well), an explicit or implicit importance or priority ranking of the content objects 906 , and/or other criteria.
- the wearable computing device may be further configured to receive from the wearer a selection of a content object 906 from the menu 904 .
- the user-interface 900 may include a cursor 908 , shown in FIG. 9B as a reticle, which may be used to navigate to and select content objects 906 from the menu 904 .
- the cursor 908 may be controlled by a wearer of the wearable computing device through one or more predetermined movements. Accordingly, the wearable computing device may be further configured to receive selection data corresponding to the one or more predetermined movements.
- the selection data may take several forms.
- the selection data may be (or may be derived from) data received from one or more movement sensors, accelerometers, gyroscopes, and/or detectors configured to detect the one or more predetermined movements.
- the one or more movement sensors may be included in the wearable computing device, like the sensor 922 , or may be included in a diagonal device communicatively coupled to the wearable computing device.
- the selection data may be (or may be derived from) data received from a touch pad, such as the finger-operable touch pad 924 described above in connection with FIG. 9A , or other input device included in or coupled to the wearable computing device and configured to detect one or more predetermined movements.
- the selection data may take the form of a binary indication corresponding to the predetermined movement. In other embodiments, the selection data may indicate the extent, the direction, the velocity, and/or the acceleration associated with the predetermined movement. The selection data may take other forms as well.
- the predetermined movements may take several forms.
- the predetermined movements may be certain movements or sequence of movements of the wearable computing device or diagonal device.
- the predetermined movements may include one or more predetermined movements defined as no or substantially no movement, such as no or substantially no movement for a predetermined period of time.
- one or more predetermined movements may involve a predetermined movement of the wearer's head (which is assumed to move the wearable computing device in a corresponding manner).
- the predetermined movements may involve a predetermined movement of a diagonal device communicatively coupled to the wearable computing device.
- the diagonal device may similarly be wearable by a wearer of the wearable computing device, such that the movement of the diagonal device may follow a movement of the wearer, such as, for example, a movement of the wearer's hand.
- one or more predetermined movements may be, for example, a movement across a finger-operable touch pad or other input device. Other predetermined movements are possible as well.
- a wearer of the wearable computing device has navigated the cursor 908 to the content object 906 using one or more predetermined movements.
- the wearer may perform an additional predetermined movement, such as holding the cursor 908 over the content object 906 for a predetermined period of time.
- the wearer may select the content object 906 in other manners as well.
- the wearable computing device may cause the content object 906 to be displayed in the view region 902 as a selected content object.
- FIG. 9C shows aspects of an example user-interface after selection of a selected content object, in accordance with an embodiment.
- the content object 906 is displayed in the view region 902 as a selected content object 910 .
- the selected content object 910 is displayed larger and in more detail in the view region 902 than in the menu 904 .
- the selected content object 910 could be displayed in the view region 902 smaller than or the same size as, and in less detail than or the same detail as, the menu 904 .
- additional content e.g., actions to be applied to, with, or based on the selected content object 910 , information related to the selected content object 910 , and/or modifiable options, preferences, or parameters for the selected content object 910 , etc.
- additional content may be showed adjacent to or nearby the selected content object 910 in the view region 902 .
- a wearer of the wearable computing device may interact with the selected content object 910 .
- the selected content object 910 may wish to read one of the emails in the email inbox.
- the wearer may interact with the selected content object in other ways as well (e.g., the wearer may locate additional information related to the selected content object 910 , modify, augment, and/or delete the selected content object 910 , etc.).
- the wearable computing device may be further configured to receive input data corresponding to one or more predetermined movements indicating interactions with the user-interface 900 .
- the input data may take any of the forms described above in connection with the selection data.
- FIG. 9D shows aspects of an example user-interface after receiving input data corresponding to a user input, in accordance with an embodiment.
- a wearer of the wearable computing device has navigated the cursor 908 to a particular subject line in the email inbox and selected the subject line.
- the email 912 is displayed in the view region, so that the wearer may read the email 912 .
- the wearer may interact with the user-interface 900 in other manners as well, depending on, for example, the selected content object.
Abstract
Description
Claims (21)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/408,905 US9035878B1 (en) | 2012-02-29 | 2012-02-29 | Input system |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/408,905 US9035878B1 (en) | 2012-02-29 | 2012-02-29 | Input system |
Publications (1)
Publication Number | Publication Date |
---|---|
US9035878B1 true US9035878B1 (en) | 2015-05-19 |
Family
ID=53054642
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/408,905 Expired - Fee Related US9035878B1 (en) | 2012-02-29 | 2012-02-29 | Input system |
Country Status (1)
Country | Link |
---|---|
US (1) | US9035878B1 (en) |
Cited By (28)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20130293488A1 (en) * | 2012-05-02 | 2013-11-07 | Lg Electronics Inc. | Mobile terminal and control method thereof |
US20140145925A1 (en) * | 2012-07-13 | 2014-05-29 | Symbol Technologies, Inc. | Device and method for performing a functionality |
US20140225814A1 (en) * | 2013-02-14 | 2014-08-14 | Apx Labs, Llc | Method and system for representing and interacting with geo-located markers |
US20140372944A1 (en) * | 2013-06-12 | 2014-12-18 | Kathleen Mulcahy | User focus controlled directional user input |
US20150215581A1 (en) * | 2014-01-24 | 2015-07-30 | Avaya Inc. | Enhanced communication between remote participants using augmented and virtual reality |
US20150241998A1 (en) * | 2014-02-26 | 2015-08-27 | Lenovo (Singapore) Pte, Ltd. | Wearable device authentication and operation |
US20160011420A1 (en) * | 2014-07-08 | 2016-01-14 | Lg Electronics Inc. | Glasses-type terminal and method for controlling the same |
US20160103511A1 (en) * | 2012-06-15 | 2016-04-14 | Muzik LLC | Interactive input device |
US20160124606A1 (en) * | 2014-10-31 | 2016-05-05 | Samsung Electronics Co., Ltd. | Display apparatus, system, and controlling method thereof |
US20160162020A1 (en) * | 2014-12-03 | 2016-06-09 | Taylor Lehman | Gaze target application launcher |
US20160349838A1 (en) * | 2015-05-31 | 2016-12-01 | Fieldbit Ltd. | Controlling a head mounted device |
US20170076503A1 (en) * | 2015-09-16 | 2017-03-16 | Bandai Namco Entertainment Inc. | Method for generating image to be displayed on head tracking type virtual reality head mounted display and image generation device |
US20170092002A1 (en) * | 2015-09-30 | 2017-03-30 | Daqri, Llc | User interface for augmented reality system |
DE102016003073A1 (en) * | 2016-03-12 | 2017-09-14 | Audi Ag | Method for operating a virtual reality system and virtual reality system |
US9933864B1 (en) | 2013-08-29 | 2018-04-03 | Amazon Technologies, Inc. | Steady content display |
US20180121083A1 (en) * | 2016-10-27 | 2018-05-03 | Alibaba Group Holding Limited | User interface for informational input in virtual reality environment |
WO2018101995A1 (en) * | 2016-11-30 | 2018-06-07 | Google Llc | Switching of active objects in an augmented and/or virtual reality environment |
US10088921B2 (en) | 2014-10-10 | 2018-10-02 | Muzik Inc. | Devices for sharing user interactions |
US10152851B2 (en) | 2016-11-29 | 2018-12-11 | Microsoft Technology Licensing, Llc | Notification artifact display |
EP3396511A4 (en) * | 2015-12-21 | 2019-07-24 | Sony Interactive Entertainment Inc. | Information processing device and operation reception method |
US20190378334A1 (en) * | 2018-06-08 | 2019-12-12 | Vulcan Inc. | Augmented reality portal-based applications |
US20200026413A1 (en) * | 2018-06-29 | 2020-01-23 | Vulcan Inc. | Augmented reality cursors |
US10586514B2 (en) | 2017-05-01 | 2020-03-10 | Elbit Systems Ltd | Head mounted display device, system and method |
US10620710B2 (en) | 2017-06-15 | 2020-04-14 | Microsoft Technology Licensing, Llc | Displacement oriented interaction in computer-mediated reality |
US20220035448A1 (en) * | 2020-07-30 | 2022-02-03 | Jins Holdings Inc. | Information Processing Method, Non-Transitory Recording Medium, and Information Processing Apparatus |
US11340758B1 (en) * | 2018-12-27 | 2022-05-24 | Meta Platforms, Inc. | Systems and methods for distributing content |
US11455032B2 (en) * | 2014-09-19 | 2022-09-27 | Utherverse Digital Inc. | Immersive displays |
US20220337693A1 (en) * | 2012-06-15 | 2022-10-20 | Muzik Inc. | Audio/Video Wearable Computer System with Integrated Projector |
Citations (42)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
JPH08111878A (en) | 1994-10-13 | 1996-04-30 | Minolta Co Ltd | Hmd |
US5742263A (en) | 1995-12-18 | 1998-04-21 | Telxon Corporation | Head tracking system for a head mounted display system |
US6061064A (en) | 1993-08-31 | 2000-05-09 | Sun Microsystems, Inc. | System and method for providing and using a computer user interface with a view space having discrete portions |
US6084556A (en) * | 1995-11-28 | 2000-07-04 | Vega Vista, Inc. | Virtual computer monitor |
US6198462B1 (en) | 1994-10-14 | 2001-03-06 | Hughes Electronics Corporation | Virtual display screen system |
US6288704B1 (en) | 1999-06-08 | 2001-09-11 | Vega, Vista, Inc. | Motion detection and tracking system to control navigation and display of object viewers |
US20030020707A1 (en) | 2001-06-27 | 2003-01-30 | Kangas Kari J. | User interface |
KR20030024021A (en) | 2001-09-15 | 2003-03-26 | 김도균 | Head Mount Display |
US6577329B1 (en) * | 1999-02-25 | 2003-06-10 | International Business Machines Corporation | Method and system for relevance feedback through gaze tracking and ticker interfaces |
US6847336B1 (en) | 1996-10-02 | 2005-01-25 | Jerome H. Lemelson | Selectively controllable heads-up display system |
US20050076303A1 (en) | 2002-04-23 | 2005-04-07 | Myorigo Oy | Graphical user interface and method and electronic device for navigating in the graphical user interface |
US20060012884A1 (en) | 2004-07-13 | 2006-01-19 | Snap-On Incorporated | Portable diagnostic system with heads-up display |
US20070011609A1 (en) | 2005-07-07 | 2007-01-11 | Florida International University Board Of Trustees | Configurable, multimodal human-computer interface system and method |
US7190378B2 (en) | 2001-08-16 | 2007-03-13 | Siemens Corporate Research, Inc. | User interface for augmented and virtual reality systems |
US7199934B2 (en) | 2004-05-06 | 2007-04-03 | Olympus Corporation | Head-mounted display apparatus |
KR100751290B1 (en) | 2006-03-31 | 2007-08-23 | 한국과학기술연구원 | Image system for head mounted display |
US7365734B2 (en) | 2002-08-06 | 2008-04-29 | Rembrandt Ip Management, Llc | Control of display content by movement on a fixed spherical space |
US20080276196A1 (en) | 2007-05-04 | 2008-11-06 | Apple Inc. | Automatically adjusting media display in a personal display system |
US20090231687A1 (en) | 2008-03-11 | 2009-09-17 | Kakuya Yamamoto | Display apparatus, display method, goggle-type head-mounted display, and vehicle |
US7647175B2 (en) | 2005-09-09 | 2010-01-12 | Rembrandt Technologies, Lp | Discrete inertial display navigation |
WO2010057304A1 (en) | 2008-11-21 | 2010-05-27 | London Health Sciences Centre Research Inc. | Hands-free pointer system |
EP2211224A1 (en) | 2009-01-27 | 2010-07-28 | Thomson Licensing SA | Head-mounted display and operating method thereof |
US20100259471A1 (en) | 2007-11-16 | 2010-10-14 | Nikon Corporation | Control device, head-mount display device, program, and control method |
WO2010118292A1 (en) | 2009-04-09 | 2010-10-14 | Dynavox Systems, Llc | Calibration free, motion tolerant eye-gaze direction detector with contextually aware computer interaction and communication methods |
US20110052009A1 (en) | 2009-08-27 | 2011-03-03 | Rafael Advanced Defense Systems Ltd. | Unconstrained spatially aligned head-up display |
USRE42336E1 (en) | 1995-11-28 | 2011-05-10 | Rembrandt Portable Display Technologies, Lp | Intuitive control of portable data displays |
US20110115883A1 (en) | 2009-11-16 | 2011-05-19 | Marcus Kellerman | Method And System For Adaptive Viewport For A Mobile Device Based On Viewing Angle |
US7948451B2 (en) | 2004-06-18 | 2011-05-24 | Totalförsvarets Forskningsinstitut | Interactive method of presenting information in an image |
WO2011097226A1 (en) | 2010-02-02 | 2011-08-11 | Kopin Corporation | Wireless hands-free computing headset with detachable accessories controllable by motion, body gesture and/or vocal commands |
US20110213664A1 (en) | 2010-02-28 | 2011-09-01 | Osterhout Group, Inc. | Local advertising content on an interactive head-mounted eyepiece |
US20110227820A1 (en) | 2010-02-28 | 2011-09-22 | Osterhout Group, Inc. | Lock virtual keyboard position in an augmented reality eyepiece |
US20110241982A1 (en) | 2010-04-06 | 2011-10-06 | Hon Hai Precision Industry Co., Ltd. | Electronic device capable of automatically adjusting file displayed on display and method thereof |
US20120005623A1 (en) | 2007-08-22 | 2012-01-05 | Ishak Edward W | Methods, Systems, and Media for Providing Content-Aware Scrolling |
US20120054603A1 (en) | 2010-08-30 | 2012-03-01 | Sap Ag | View model aspects of component objects |
US20120151397A1 (en) * | 2010-12-08 | 2012-06-14 | Tavendo Gmbh | Access to an electronic object collection via a plurality of views |
US20120223884A1 (en) | 2011-03-01 | 2012-09-06 | Qualcomm Incorporated | System and method to display content |
US20120236037A1 (en) | 2011-01-06 | 2012-09-20 | Research In Motion Limited | Electronic device and method of displaying information in response to a gesture |
US20120272179A1 (en) * | 2011-04-21 | 2012-10-25 | Sony Computer Entertainment Inc. | Gaze-Assisted Computer Interface |
US20130091470A1 (en) | 2000-04-21 | 2013-04-11 | Sony Corporation | System for managing data objects |
US20130097539A1 (en) | 2011-10-18 | 2013-04-18 | Research In Motion Limited | Method of modifying rendered attributes of list elements in a user interface |
US20130117689A1 (en) | 2011-01-06 | 2013-05-09 | Research In Motion Limited | Electronic device and method of displaying information in response to a gesture |
US20130139082A1 (en) | 2011-11-30 | 2013-05-30 | Google Inc. | Graphical Interface Having Adjustable Borders |
-
2012
- 2012-02-29 US US13/408,905 patent/US9035878B1/en not_active Expired - Fee Related
Patent Citations (48)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6061064A (en) | 1993-08-31 | 2000-05-09 | Sun Microsystems, Inc. | System and method for providing and using a computer user interface with a view space having discrete portions |
US6396497B1 (en) | 1993-08-31 | 2002-05-28 | Sun Microsystems, Inc. | Computer user interface with head motion input |
JPH08111878A (en) | 1994-10-13 | 1996-04-30 | Minolta Co Ltd | Hmd |
US6198462B1 (en) | 1994-10-14 | 2001-03-06 | Hughes Electronics Corporation | Virtual display screen system |
US6127990A (en) | 1995-11-28 | 2000-10-03 | Vega Vista, Inc. | Wearable display and methods for controlling same |
USRE42336E1 (en) | 1995-11-28 | 2011-05-10 | Rembrandt Portable Display Technologies, Lp | Intuitive control of portable data displays |
US6445364B2 (en) | 1995-11-28 | 2002-09-03 | Vega Vista, Inc. | Portable game display and method for controlling same |
US6359603B1 (en) | 1995-11-28 | 2002-03-19 | Vega Vista, Inc. | Portable display and methods of controlling same |
US6084556A (en) * | 1995-11-28 | 2000-07-04 | Vega Vista, Inc. | Virtual computer monitor |
US5742263A (en) | 1995-12-18 | 1998-04-21 | Telxon Corporation | Head tracking system for a head mounted display system |
US6847336B1 (en) | 1996-10-02 | 2005-01-25 | Jerome H. Lemelson | Selectively controllable heads-up display system |
US20050206583A1 (en) | 1996-10-02 | 2005-09-22 | Lemelson Jerome H | Selectively controllable heads-up display system |
US6577329B1 (en) * | 1999-02-25 | 2003-06-10 | International Business Machines Corporation | Method and system for relevance feedback through gaze tracking and ticker interfaces |
US6288704B1 (en) | 1999-06-08 | 2001-09-11 | Vega, Vista, Inc. | Motion detection and tracking system to control navigation and display of object viewers |
US20130091470A1 (en) | 2000-04-21 | 2013-04-11 | Sony Corporation | System for managing data objects |
US20030020707A1 (en) | 2001-06-27 | 2003-01-30 | Kangas Kari J. | User interface |
US7190378B2 (en) | 2001-08-16 | 2007-03-13 | Siemens Corporate Research, Inc. | User interface for augmented and virtual reality systems |
KR20030024021A (en) | 2001-09-15 | 2003-03-26 | 김도균 | Head Mount Display |
US20050076303A1 (en) | 2002-04-23 | 2005-04-07 | Myorigo Oy | Graphical user interface and method and electronic device for navigating in the graphical user interface |
US7406661B2 (en) | 2002-04-23 | 2008-07-29 | Myorigo L.L.C. | Graphical user interface and method and electronic device for navigating in the graphical user interface |
US7365734B2 (en) | 2002-08-06 | 2008-04-29 | Rembrandt Ip Management, Llc | Control of display content by movement on a fixed spherical space |
US7199934B2 (en) | 2004-05-06 | 2007-04-03 | Olympus Corporation | Head-mounted display apparatus |
US7948451B2 (en) | 2004-06-18 | 2011-05-24 | Totalförsvarets Forskningsinstitut | Interactive method of presenting information in an image |
US20060012884A1 (en) | 2004-07-13 | 2006-01-19 | Snap-On Incorporated | Portable diagnostic system with heads-up display |
US20070011609A1 (en) | 2005-07-07 | 2007-01-11 | Florida International University Board Of Trustees | Configurable, multimodal human-computer interface system and method |
US7647175B2 (en) | 2005-09-09 | 2010-01-12 | Rembrandt Technologies, Lp | Discrete inertial display navigation |
KR100751290B1 (en) | 2006-03-31 | 2007-08-23 | 한국과학기술연구원 | Image system for head mounted display |
US20080276196A1 (en) | 2007-05-04 | 2008-11-06 | Apple Inc. | Automatically adjusting media display in a personal display system |
US20120005623A1 (en) | 2007-08-22 | 2012-01-05 | Ishak Edward W | Methods, Systems, and Media for Providing Content-Aware Scrolling |
US20100259471A1 (en) | 2007-11-16 | 2010-10-14 | Nikon Corporation | Control device, head-mount display device, program, and control method |
US20090231687A1 (en) | 2008-03-11 | 2009-09-17 | Kakuya Yamamoto | Display apparatus, display method, goggle-type head-mounted display, and vehicle |
WO2010057304A1 (en) | 2008-11-21 | 2010-05-27 | London Health Sciences Centre Research Inc. | Hands-free pointer system |
EP2211224A1 (en) | 2009-01-27 | 2010-07-28 | Thomson Licensing SA | Head-mounted display and operating method thereof |
WO2010118292A1 (en) | 2009-04-09 | 2010-10-14 | Dynavox Systems, Llc | Calibration free, motion tolerant eye-gaze direction detector with contextually aware computer interaction and communication methods |
US20110052009A1 (en) | 2009-08-27 | 2011-03-03 | Rafael Advanced Defense Systems Ltd. | Unconstrained spatially aligned head-up display |
US20110115883A1 (en) | 2009-11-16 | 2011-05-19 | Marcus Kellerman | Method And System For Adaptive Viewport For A Mobile Device Based On Viewing Angle |
WO2011097226A1 (en) | 2010-02-02 | 2011-08-11 | Kopin Corporation | Wireless hands-free computing headset with detachable accessories controllable by motion, body gesture and/or vocal commands |
US20110213664A1 (en) | 2010-02-28 | 2011-09-01 | Osterhout Group, Inc. | Local advertising content on an interactive head-mounted eyepiece |
US20110227820A1 (en) | 2010-02-28 | 2011-09-22 | Osterhout Group, Inc. | Lock virtual keyboard position in an augmented reality eyepiece |
US20110241982A1 (en) | 2010-04-06 | 2011-10-06 | Hon Hai Precision Industry Co., Ltd. | Electronic device capable of automatically adjusting file displayed on display and method thereof |
US20120054603A1 (en) | 2010-08-30 | 2012-03-01 | Sap Ag | View model aspects of component objects |
US20120151397A1 (en) * | 2010-12-08 | 2012-06-14 | Tavendo Gmbh | Access to an electronic object collection via a plurality of views |
US20120236037A1 (en) | 2011-01-06 | 2012-09-20 | Research In Motion Limited | Electronic device and method of displaying information in response to a gesture |
US20130117689A1 (en) | 2011-01-06 | 2013-05-09 | Research In Motion Limited | Electronic device and method of displaying information in response to a gesture |
US20120223884A1 (en) | 2011-03-01 | 2012-09-06 | Qualcomm Incorporated | System and method to display content |
US20120272179A1 (en) * | 2011-04-21 | 2012-10-25 | Sony Computer Entertainment Inc. | Gaze-Assisted Computer Interface |
US20130097539A1 (en) | 2011-10-18 | 2013-04-18 | Research In Motion Limited | Method of modifying rendered attributes of list elements in a user interface |
US20130139082A1 (en) | 2011-11-30 | 2013-05-30 | Google Inc. | Graphical Interface Having Adjustable Borders |
Non-Patent Citations (3)
Title |
---|
Hine et al., "VEVI: A Virtual Environment Teleoperations Interface for Planetary Exploration," University of California, Berkely, SAE 25th International Conference on Environmental Systems, Jul. 1995, p. 8-10, San Diego, CA, USA. |
Sanders-Reed et al., "Enhanced and Synthetic Vision System (ESVS) Flight Demonstration," SPIE Digital Library, Mar. 19, 2008, pp. 1, 5, 6, and 8, Proc. SPIE 6957, 69570I (2008), doi:10.1117/12.775910, Orlando, FL, USA. |
Sowizral and Nadeau, "Introduction to Programming with Java 3D," Java 3D Tutorial Notes, 1999, pp. 332, 344, and 352, Palo Alto, CA, USA. |
Cited By (47)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20130293488A1 (en) * | 2012-05-02 | 2013-11-07 | Lg Electronics Inc. | Mobile terminal and control method thereof |
US11924364B2 (en) | 2012-06-15 | 2024-03-05 | Muzik Inc. | Interactive networked apparatus |
US9992316B2 (en) | 2012-06-15 | 2018-06-05 | Muzik Inc. | Interactive networked headphones |
US20160103511A1 (en) * | 2012-06-15 | 2016-04-14 | Muzik LLC | Interactive input device |
US10567564B2 (en) | 2012-06-15 | 2020-02-18 | Muzik, Inc. | Interactive networked apparatus |
US20220337693A1 (en) * | 2012-06-15 | 2022-10-20 | Muzik Inc. | Audio/Video Wearable Computer System with Integrated Projector |
US20140145925A1 (en) * | 2012-07-13 | 2014-05-29 | Symbol Technologies, Inc. | Device and method for performing a functionality |
US9791896B2 (en) * | 2012-07-13 | 2017-10-17 | Symbol Technologies, Llc | Device and method for performing a functionality |
US20140225814A1 (en) * | 2013-02-14 | 2014-08-14 | Apx Labs, Llc | Method and system for representing and interacting with geo-located markers |
US20140372944A1 (en) * | 2013-06-12 | 2014-12-18 | Kathleen Mulcahy | User focus controlled directional user input |
US9710130B2 (en) * | 2013-06-12 | 2017-07-18 | Microsoft Technology Licensing, Llc | User focus controlled directional user input |
US9933864B1 (en) | 2013-08-29 | 2018-04-03 | Amazon Technologies, Inc. | Steady content display |
US20150215581A1 (en) * | 2014-01-24 | 2015-07-30 | Avaya Inc. | Enhanced communication between remote participants using augmented and virtual reality |
US9524588B2 (en) * | 2014-01-24 | 2016-12-20 | Avaya Inc. | Enhanced communication between remote participants using augmented and virtual reality |
US9594443B2 (en) * | 2014-02-26 | 2017-03-14 | Lenovo (Singapore) Pte. Ltd. | Wearable device authentication and operation |
US20150241998A1 (en) * | 2014-02-26 | 2015-08-27 | Lenovo (Singapore) Pte, Ltd. | Wearable device authentication and operation |
US10031337B2 (en) * | 2014-07-08 | 2018-07-24 | Lg Electronics Inc. | Glasses-type terminal and method for controlling the same |
US20160011420A1 (en) * | 2014-07-08 | 2016-01-14 | Lg Electronics Inc. | Glasses-type terminal and method for controlling the same |
US11455032B2 (en) * | 2014-09-19 | 2022-09-27 | Utherverse Digital Inc. | Immersive displays |
US10088921B2 (en) | 2014-10-10 | 2018-10-02 | Muzik Inc. | Devices for sharing user interactions |
US10824251B2 (en) | 2014-10-10 | 2020-11-03 | Muzik Inc. | Devices and methods for sharing user interaction |
US20160124606A1 (en) * | 2014-10-31 | 2016-05-05 | Samsung Electronics Co., Ltd. | Display apparatus, system, and controlling method thereof |
US10248192B2 (en) * | 2014-12-03 | 2019-04-02 | Microsoft Technology Licensing, Llc | Gaze target application launcher |
US20160162020A1 (en) * | 2014-12-03 | 2016-06-09 | Taylor Lehman | Gaze target application launcher |
US10437323B2 (en) * | 2015-05-31 | 2019-10-08 | Fieldbit Ltd. | Controlling a head mounted device |
US20160349838A1 (en) * | 2015-05-31 | 2016-12-01 | Fieldbit Ltd. | Controlling a head mounted device |
US10636212B2 (en) * | 2015-09-16 | 2020-04-28 | Bandai Namco Entertainment Inc. | Method for generating image to be displayed on head tracking type virtual reality head mounted display and image generation device |
US20170076503A1 (en) * | 2015-09-16 | 2017-03-16 | Bandai Namco Entertainment Inc. | Method for generating image to be displayed on head tracking type virtual reality head mounted display and image generation device |
JP2017058971A (en) * | 2015-09-16 | 2017-03-23 | 株式会社バンダイナムコエンターテインメント | Program and image formation device |
US20170092002A1 (en) * | 2015-09-30 | 2017-03-30 | Daqri, Llc | User interface for augmented reality system |
EP3396511A4 (en) * | 2015-12-21 | 2019-07-24 | Sony Interactive Entertainment Inc. | Information processing device and operation reception method |
DE102016003073A1 (en) * | 2016-03-12 | 2017-09-14 | Audi Ag | Method for operating a virtual reality system and virtual reality system |
US20180121083A1 (en) * | 2016-10-27 | 2018-05-03 | Alibaba Group Holding Limited | User interface for informational input in virtual reality environment |
US10152851B2 (en) | 2016-11-29 | 2018-12-11 | Microsoft Technology Licensing, Llc | Notification artifact display |
WO2018101995A1 (en) * | 2016-11-30 | 2018-06-07 | Google Llc | Switching of active objects in an augmented and/or virtual reality environment |
CN109891368B (en) * | 2016-11-30 | 2022-08-19 | 谷歌有限责任公司 | Switching of moving objects in augmented and/or virtual reality environments |
CN109891368A (en) * | 2016-11-30 | 2019-06-14 | 谷歌有限责任公司 | Switching of the moving object in enhancing and/or reality environment |
US10586514B2 (en) | 2017-05-01 | 2020-03-10 | Elbit Systems Ltd | Head mounted display device, system and method |
US11004425B2 (en) | 2017-05-01 | 2021-05-11 | Elbit Systems Ltd. | Head mounted display device, system and method |
US10620710B2 (en) | 2017-06-15 | 2020-04-14 | Microsoft Technology Licensing, Llc | Displacement oriented interaction in computer-mediated reality |
US11195336B2 (en) | 2018-06-08 | 2021-12-07 | Vulcan Inc. | Framework for augmented reality applications |
US20190378334A1 (en) * | 2018-06-08 | 2019-12-12 | Vulcan Inc. | Augmented reality portal-based applications |
US10996831B2 (en) * | 2018-06-29 | 2021-05-04 | Vulcan Inc. | Augmented reality cursors |
US20200026413A1 (en) * | 2018-06-29 | 2020-01-23 | Vulcan Inc. | Augmented reality cursors |
US11340758B1 (en) * | 2018-12-27 | 2022-05-24 | Meta Platforms, Inc. | Systems and methods for distributing content |
US20220035448A1 (en) * | 2020-07-30 | 2022-02-03 | Jins Holdings Inc. | Information Processing Method, Non-Transitory Recording Medium, and Information Processing Apparatus |
US11604507B2 (en) * | 2020-07-30 | 2023-03-14 | Jins Holdings Inc. | Information processing method, non-transitory recording medium, and information processing apparatus |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9035878B1 (en) | Input system | |
US8643951B1 (en) | Graphical menu and interaction therewith through a viewing window | |
US20190011982A1 (en) | Graphical Interface Having Adjustable Borders | |
US9552676B2 (en) | Wearable computer with nearby object response | |
US10330940B1 (en) | Content display methods | |
US20130246967A1 (en) | Head-Tracked User Interaction with Graphical Interface | |
US10379346B2 (en) | Methods and devices for rendering interactions between virtual and physical objects on a substantially transparent display | |
US9058054B2 (en) | Image capture apparatus | |
US8866852B2 (en) | Method and system for input detection | |
US20150143297A1 (en) | Input detection for a head mounted device | |
US10114466B2 (en) | Methods and systems for hands-free browsing in a wearable computing device | |
US8799810B1 (en) | Stability region for a user interface | |
US20160011724A1 (en) | Hands-Free Selection Using a Ring-Based User-Interface | |
US9454288B2 (en) | One-dimensional to two-dimensional list navigation | |
US9448687B1 (en) | Zoomable/translatable browser interface for a head mounted device | |
US20130117707A1 (en) | Velocity-Based Triggering | |
US9213403B1 (en) | Methods to pan, zoom, crop, and proportionally move on a head mountable display | |
US20150199081A1 (en) | Re-centering a user interface | |
US20150193098A1 (en) | Yes or No User-Interface | |
US20150185971A1 (en) | Ring-Based User-Interface | |
US9316830B1 (en) | User interface | |
US9153043B1 (en) | Systems and methods for providing a user interface in a field of view of a media item | |
US9547406B1 (en) | Velocity-based triggering | |
WO2022103741A1 (en) | Method and device for processing user input for multiple devices |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:WHEELER, AARON J.;REEL/FRAME:027790/0384Effective date: 20120228 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044334/0466Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |
|
FEPP | Fee payment procedure |
Free format text: MAINTENANCE FEE REMINDER MAILED (ORIGINAL EVENT CODE: REM.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
LAPS | Lapse for failure to pay maintenance fees |
Free format text: PATENT EXPIRED FOR FAILURE TO PAY MAINTENANCE FEES (ORIGINAL EVENT CODE: EXP.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
STCH | Information on status: patent discontinuation |
Free format text: PATENT EXPIRED DUE TO NONPAYMENT OF MAINTENANCE FEES UNDER 37 CFR 1.362 |
|
FP | Lapsed due to failure to pay maintenance fee |
Effective date: 20230519 |