JP5592039B2 - Merge 3D models based on confidence scores - Google Patents
Merge 3D models based on confidence scores Download PDFInfo
- Publication number
- JP5592039B2 JP5592039B2 JP2014513809A JP2014513809A JP5592039B2 JP 5592039 B2 JP5592039 B2 JP 5592039B2 JP 2014513809 A JP2014513809 A JP 2014513809A JP 2014513809 A JP2014513809 A JP 2014513809A JP 5592039 B2 JP5592039 B2 JP 5592039B2
- Authority
- JP
- Japan
- Prior art keywords
- distance
- model
- pixel
- dimensional model
- dimensional
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T15/00—3D [Three Dimensional] image rendering
- G06T15/10—Geometric effects
- G06T15/20—Perspective computation
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T17/00—Three dimensional [3D] modelling, e.g. data description of 3D objects
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06T—IMAGE DATA PROCESSING OR GENERATION, IN GENERAL
- G06T2200/00—Indexing scheme for image data processing or generation, in general
- G06T2200/08—Indexing scheme for image data processing or generation, in general involving all processing steps from image acquisition to 3D model generation
Description
（背景）
（分野）
実施形態は、一般に、３次元モデリングに関連する。
(background)
(Field)
Embodiments generally relate to three-dimensional modeling.
（関連技術）
写真家は、しばしば、高所（例えば、航空機）から地球の画像を撮る。そのような航空機からの写真は、異なる視点（ｐｅｒｓｐｅｃｔｉｖｅ）から撮られ得る。航空機からの写真から、３次元モデルは、例えば、ステレオマッチングを使用して構築され得る。ステレオマッチングを使用して構築されたモデルは、完全に正確ではないことがある。例えば、不正確は、基礎をなす画像における変動、カメラの視点における不正確、およびステレオマッチングアルゴリズムの限界によって、導入され得る。
(Related technology)
Photographers often take images of the earth from high altitudes (eg, aircraft). Pictures from such aircraft can be taken from different perspectives. From photographs from the aircraft, a three-dimensional model can be constructed using, for example, stereo matching. Models built using stereo matching may not be completely accurate. For example, inaccuracies can be introduced by variations in the underlying image, inaccuracies in the camera perspective, and limitations of stereo matching algorithms.
構築された３次元モデルは、関連した視点を有し得る。例えば、３次元モデルは、関連した視点からの深度マップとして表現され得る。各深度マップは、想像上の「カメラ」とシーンの表面との間のピクセルによる距離を提供し得る。深度マップは、深度マップの関連した視点から明白である表面の３次元的特徴を説明することが可能であり得るが、深度マップは、深度マップの視点から見えなくされた３次元的特徴を説明することが不可能であり得る。この理由で、異なる視点を有する複数の深度マップは、３次元シーンを完全に説明するために必要とされ得る。 The constructed 3D model may have an associated viewpoint. For example, a three-dimensional model can be represented as a depth map from an associated viewpoint. Each depth map may provide the distance in pixels between the imaginary “camera” and the surface of the scene. While the depth map may be able to explain the three-dimensional features of the surface that are apparent from the associated viewpoint of the depth map, the depth map explains the three-dimensional features that are hidden from the viewpoint of the depth map. It may be impossible to do. For this reason, multiple depth maps with different viewpoints may be needed to fully describe the 3D scene.
（簡単な概要）
実施形態は、異なるビューポイントを使用して生成された複数の深度マップを併合する。実施形態において、方法は、複数の３次元モデルを併合し、その複数の３次元モデルの各々は、異なるビューポイントを有するカメラの画像から生成された。方法は、３次元空間における複数の３Ｄ画素を決定することを含む。複数の３Ｄ画素におけるそれぞれの３Ｄ画素に対して、複数の距離値が、決定される。各決定された距離値は、３Ｄ画素から、カメラモデルの視点方向の３次元モデルまでの距離であり、そのカメラモデルは、３次元モデルを生成するために使用される。決定された距離値のうちの少なくとも１つが、３Ｄ画素と対応する３次元モデルとの間の距離がしきい値の範囲内にあること、または３Ｄ画素が対応する３次元モデルの上方に位置していることを示す場合、信頼度スコアは、決定され、信頼度スコアは、対応する３次元モデルのカメラモデルの視点が、３Ｄ画素に正対するように配向されている度合を示し、３Ｄ画素に対応する点を併合された３次元モデルに含むべきかどうかが、決定された信頼度スコアに少なくとも一部基づいて、決定される。
(Simple overview)
Embodiments merge multiple depth maps generated using different viewpoints. In an embodiment, the method merged a plurality of 3D models, each of the plurality of 3D models being generated from an image of a camera having a different viewpoint. The method includes determining a plurality of 3D pixels in a three-dimensional space. A plurality of distance values are determined for each 3D pixel in the plurality of 3D pixels. Each determined distance value is the distance from the 3D pixel to the 3D model in the viewing direction of the camera model, and the camera model is used to generate the 3D model. At least one of the determined distance values is such that the distance between the 3D pixel and the corresponding 3D model is within a threshold range, or the 3D pixel is located above the corresponding 3D model. The confidence score is determined, and the confidence score indicates the degree to which the viewpoint of the camera model of the corresponding three-dimensional model is oriented to face the 3D pixel. Whether a corresponding point should be included in the merged three-dimensional model is determined based at least in part on the determined confidence score.
システムおよびコンピュータープログラム製品の実施形態も、開示される。 Embodiments of system and computer program products are also disclosed.
本発明の種々の実施形態の構造および動作と同様に、本発明のさらなる実施形態、特徴、および利点は、添付の図面を参照して下記に詳細に説明される。
本発明はさらに、たとえば、以下を提供する。
（項目１）
複数の３次元モデルを併合する方法であって、該複数の３次元モデルの各々は、異なるビューポイントを有するカメラの画像から生成され、該方法は、
（ａ）３次元空間における複数の３Ｄ画素を決定することと、
該複数の３Ｄ画素におけるそれぞれの３Ｄ画素に対して、
（ｂ）複数の距離値を決定することであって、各距離値は、該３Ｄ画素から、カメラモデルの視点方向の該複数の３次元モデルからの対応する３次元モデルまでの距離であり、該カメラモデルは、該３次元モデルを生成するために使用されている、ことと、
（ｉ）該複数の距離値からの第一の距離値が、該３Ｄ画素と該第一の距離値に対応する該３次元モデルとの間の距離がしきい値の範囲内にあることを示し、かつ、（ｉｉ）該複数の距離値からの第二の距離値が、該３Ｄ画素が該しきい値の外側かつ該第二の距離値に対応する該３次元モデルの上方にあることを示す場合に、
（ｃ）該第一の距離値に対応する該３次元モデルの該カメラモデルの該視点が、該３Ｄ画素に正対するように配向されている度合を示す第一の信頼度スコアを決定することと、
（ｄ）該第二の距離値に対応する該３次元モデルの該カメラモデルの該視点が、該３Ｄ画素に正対するように配向されている度合を示す第二の信頼度スコアを決定することと、
（ｅ）該決定された第一および第二の信頼度スコアに少なくとも一部基づいて、該３Ｄ画素に対応する点を併合された３次元モデルに含むべきかどうかを決定することであって、該第二の信頼度スコアが、該第一の信頼度スコアよりも事前決定された度合だけ大きい場合に、該３Ｄ画素は、該併合された３次元モデルに含まれない、ことと
を含む方法。
（項目２）
前記決定すること（ｃ）は、前記３次元モデルと前記カメラモデルの前記視点から前記３Ｄ画素に向かって延びている光線との間の角度に基づいて、該角度がより鋭角になるにつれて、前記点が前記併合された３次元モデルに含まれる可能性が減少するように、前記信頼度スコアを決定することを含む、項目１に記載の方法。
（項目３）
前記決定すること（ｃ）は、前記３Ｄ画素を基準として決定される近接域における前記対応する３次元モデルのサンプリング間の距離に基づいて、該サンプリング間の距離が増加するにつれて、前記点が前記併合された３次元モデルに含まれる可能性が減少するように、前記信頼度スコアを決定することを含む、項目１に記載の方法。
（項目４）
（ｆ）（ｉ）前記点が、（ｅ）において、前記併合された３次元モデルに含まれるように決定され、かつ、（ｉｉ）前記複数の決定された距離値からの第三および第四の距離値の両方が、前記３Ｄ画素とそれぞれの３次元モデルとの間の距離が前記しきい値を超過しないことを示す場合に、該第三および第四の距離値の両方に従って、該３Ｄ画素に対する重みを決定することと、
（ｇ）２つの隣接した３Ｄ画素に対して決定された重みに基づいて、該２つの隣接した３Ｄ画素の間で、該併合された３次元モデルの縁をどこに構築すべきかを決定することと
をさらに含む、項目１に記載の方法。
（項目５）
前記複数の３次元モデルにおける各３次元モデルは、深度マップとして表現されている、項目１に記載の方法。
（項目６）
前記複数の３次元モデルにおける各３次元モデルは、高度フィールドとして表現されている、項目１に記載の方法。
（項目７）
前記複数の３次元モデルにおける各３次元モデルは、ステレオ再構築である、項目１に記載の方法。
（項目８）
複数の３次元モデルを併合するシステムであって、該複数の３次元モデルの各々は、異なるビューポイントを有するカメラの画像から生成され、該システムは、
１つ以上のプロセッサーと、
該１つ以上のプロセッサーに連結されているメモリーと、
３次元空間における複数の３Ｄ画素を決定するように構成されている３Ｄ画素決定モジュールと、
該複数の３Ｄ画素におけるそれぞれの３Ｄ画素に対して、複数の距離値を決定するように構成されている距離テストモジュールであって、各距離値は、該３Ｄ画素から、カメラモデルの視点方向の該複数の３次元モデルからの３次元モデルまでの距離であり、該カメラモデルは、該３次元モデルを生成するために使用され、該距離テストモジュールは、該１つ以上のプロセッサーに実装されている、距離テストモジュールと、
信頼度スコアモジュールであって、該信頼度スコアモジュールは、（ｉ）該複数の距離値からの第一の距離値が、該３Ｄ画素と該第一の距離値に対応する該３次元モデルとの間の距離がしきい値の範囲内あることを示し、かつ、（ｉｉ）該複数の距離値からの第二の距離値が、該３Ｄ画素が該しきい値の外側かつ該第二の距離値に対応する該３次元モデルの上方にあることを示す場合に、
該第一の距離値に対応する該３次元モデルの該カメラモデルの該視点が、該３Ｄ画素に正対するように配向されている度合を示す第一の信頼度スコアを決定し、
該第二の距離値に対応する該３次元モデルの該カメラモデルの該視点が、該３Ｄ画素に正対するように配向されている度合を示す第二の信頼度スコアを決定する
ように構成されている、信頼度スコアモジュールと、
点セレクターモジュールであって、該点セレクターモジュールは、該決定された第一および第二の信頼度スコアに少なくとも一部基づいて、該３Ｄ画素に対応する点を併合された３次元モデルに含むべきかどうかを決定するように構成され、該第二の信頼度スコアが、該第一の信頼度スコアよりも事前決定された度合だけ大きい場合に、該３Ｄ画素は、該併合された３次元モデルに含まれない、点セレクターモジュールと
を含む、システム。
（項目９）
前記信頼度スコアモジュールは、前記３次元モデルと前記カメラモデルの前記視点から前記３Ｄ画素に向かって延びている光線との間の角度に基づいて、該角度がより鋭角になるにつれて、前記点が前記併合された３次元モデルに含まれる可能性が減少するように、前記信頼度スコアを決定するように構成されている、項目８に記載のシステム。
（項目１０）
前記信頼度スコアモジュールは、前記３Ｄ画素を基準として決定された近接域における前記対応する３次元モデルのサンプリング間の距離に基づいて、該サンプリング間の距離が増加するにつれて、前記点が前記併合された３次元モデルに含まれる可能性が減少するように、前記信頼度スコアを決定するように構成されている、項目８に記載のシステム。
（項目１１）
モデル構築モジュールをさらに含み、該モデル構築モジュールは、（ｉ）前記点が、前記併合された３次元モデルに含まれるように前記点セレクターモジュールによって決定され、かつ、前記複数の決定された距離値からの第三および第四の距離値の両方が、前記３Ｄ画素とそれぞれの３次元モデルとの間の距離が前記しきい値を超過しないことを示す場合に、該第三および第四の距離値の両方に従って、該３Ｄ画素に対する重みを決定し、（ｉｉ）２つの隣接した３Ｄ画素に対して決定された重みに基づいて、該２つの隣接した３Ｄ画素の間で、該併合された３次元モデルの縁をどこに構築すべきかを決定するように構成されている、項目８に記載のシステム。
（項目１２）
前記複数の３次元モデルにおける各３次元モデルは、深度マップとして表現されている、項目８に記載のシステム。
（項目１３）
前記複数の３次元モデルにおける各３次元モデルは、高度フィールドとして表現されている、項目８に記載のシステム。
（項目１４）
前記複数の３次元モデルにおける各３次元モデルは、ステレオ再構築である、項目８に記載のシステム。
（項目１５）
非一時的なコンピューター読み込み可能な媒体を含むコンピュータープログラム製品であって、該媒体は、命令を記憶し、該命令は、コンピューティングデバイスによって実行されると、該コンピューティングデバイスに、複数の３次元モデルを併合する動作を実施させ、該複数の３次元モデルの各々は、異なるビューポイントを有するカメラの画像から生成され、該動作は、
（ａ）３次元空間における複数の３Ｄ画素を決定することと
該複数の３Ｄ画素におけるそれぞれの３Ｄ画素に対して、
（ｂ）複数の距離値を決定することであって、各距離値は、該３Ｄ画素から、カメラモデルの視点方向の該複数の３次元モデルからの対応する３次元モデルまでの距離であり、該カメラモデルは、該３次元モデルを生成するために使用されている、ことと、
（ｉ）該複数の距離値からの第一の距離値が、該３Ｄ画素と該第一の距離値に対応する該３次元モデルとの間の距離がしきい値の範囲内にあることを示し、かつ、（ｉｉ）該複数の距離値からの第二の距離値が、該３Ｄ画素が該しきい値の外側かつ該第二の距離値に対応する該３次元モデルの上方にあることを示す場合に、
（ｃ）該第一の距離値に対応する該３次元モデルの該カメラモデルの該視点が、該３Ｄ画素に正対するように配向されている度合を示す第一の信頼度スコアを決定することと、
（ｄ）該第二の距離値に対応する該３次元モデルの該カメラモデルの該視点が、該３Ｄ画素に正対するように配向されている度合を示す第二の信頼度スコアを決定することと、
（ｅ）該決定された第一および第二の信頼度スコアに少なくとも一部基づいて、該３Ｄ画素に対応する点を併合された３次元モデルに含むべきかどうかを決定することであって、該第二の信頼度スコアが、該第一の信頼度スコアよりも事前決定された度合だけ大きい場合に、該３Ｄ画素は、該併合された３次元モデルに含まれない、ことと
を含む、コンピュータープログラム製品。
（項目１６）
前記決定すること（ｃ）は、前記３次元モデルと前記カメラモデルの前記視点から前記３Ｄ画素に向かって延びている光線との間の角度に基づいて、該角度がより鋭角になるにつれて、前記点が前記併合された３次元モデルに含まれる可能性が減少するように、前記信頼度スコアを決定することを含む、項目１５に記載のコンピュータープログラム製品。
（項目１７）
前記決定すること（ｃ）は、前記３Ｄ画素を基準として決定された近接域における前記対応する３次元モデルのサンプリング間の距離にも基づいて、該サンプリング間の距離が増加するにつれて、前記点が前記併合された３次元モデルに含まれる可能性が減少するように、前記信頼度スコアを決定することをさらに含む、項目１６に記載のコンピュータープログラム製品。
（項目１８）
前記動作は、
（ｆ）（ｉ）前記点が、（ｄ）において、前記併合された３次元モデルに含まれるように決定され、かつ、（ｉｉ）前記複数の決定された距離値からの第三および第四の距離値の両方が、前記３Ｄ画素とそれぞれの３次元モデルとの間の距離が前記しきい値を超過しないことを示す場合に、該第三および第四の距離値の両方に従って、該３Ｄ画素に対する重みを決定することと、
（ｇ）２つの隣接した３Ｄ画素に対して決定された重みに基づいて、該２つの隣接した３Ｄ画素の間で、該併合された３次元モデルの縁をどこに構築すべきかを決定することと
をさらに含む、項目１７に記載のコンピュータープログラム製品。
（項目１９）
前記複数の３次元モデルにおける各３次元モデルは、深度マップとして表現されている、項目１５に記載のコンピュータープログラム製品。
（項目２０）
前記複数の３次元モデルにおける各３次元モデルは、高度フィールドとして表現されている、項目１５に記載のコンピュータープログラム製品。
（項目２１）
前記複数の３次元モデルにおける各３次元モデルは、ステレオ再構築である、項目１５に記載のコンピュータープログラム製品。
（項目２２）
複数の３次元モデルを併合するシステムであって、該複数の３次元モデルの各々は、異なるビューポイントを有するカメラの画像から生成され、該システムは、
１つ以上のプロセッサーと、
該１つ以上のプロセッサーに連結されているメモリーと、
３次元空間における複数の３Ｄ画素を決定する手段と、
該複数の３Ｄ画素におけるそれぞれの３Ｄ画素に対して、複数の距離値を決定する手段であって、各距離値は、該３Ｄ画素から、カメラモデルの視点方向の該複数の３次元モデルからの３次元モデルまでの距離であり、該カメラモデルは、該３次元モデルを生成するために使用され、該距離テストモジュールは、該１つ以上のプロセッサーに実装されている、手段と、
（ｉ）該複数の距離値からの第一の距離値が、該３Ｄ画素と該第一の距離値に対応する該３次元モデルとの間の距離がしきい値の範囲内にあることを示し、かつ、（ｉｉ）該複数の距離値からの第二の距離値が、該３Ｄ画素が該しきい値の外側かつ該第二の距離値に対応する該３次元モデルの上方にあることを示す場合に、
該第一の距離値に対応する該３次元モデルの該カメラモデルの該視点が、該３Ｄ画素に正対するように配向されている度合を示す第一の信頼度スコアを決定し、
該第二の距離値に対応する該３次元モデルの該カメラモデルの該視点が、該３Ｄ画素に正対するように配向されている度合を示す第二の信頼度スコアを決定する
手段と、
該決定された第一および第二の信頼度スコアに少なくとも一部基づいて、該３Ｄ画素に対応する点を併合された３次元モデルに含むべきかどうかを決定する手段であって、該第二の信頼度スコアが、該第一の信頼度スコアよりも事前決定された度合だけ大きい場合に、該３Ｄ画素は、該併合された３次元モデルに含まれない、手段と
を含む、システム。
Further embodiments, features, and advantages of the present invention, as well as the structure and operation of the various embodiments of the present invention, are described in detail below with reference to the accompanying drawings.
The present invention further provides, for example:
(Item 1)
A method of merging a plurality of 3D models, each of the plurality of 3D models being generated from images of cameras having different viewpoints, the method comprising:
(A) determining a plurality of 3D pixels in a three-dimensional space;
For each 3D pixel in the plurality of 3D pixels,
(B) determining a plurality of distance values, each distance value being a distance from the 3D pixel to a corresponding three-dimensional model from the plurality of three-dimensional models in the viewpoint direction of the camera model; The camera model is used to generate the three-dimensional model;
(I) The first distance value from the plurality of distance values indicates that the distance between the 3D pixel and the three-dimensional model corresponding to the first distance value is within a threshold range. And (ii) a second distance value from the plurality of distance values is such that the 3D pixel is outside the threshold and above the three-dimensional model corresponding to the second distance value. When indicating
(C) determining a first confidence score indicating the degree to which the viewpoint of the camera model of the three-dimensional model corresponding to the first distance value is oriented to face the 3D pixel; When,
(D) determining a second confidence score indicating the degree to which the viewpoint of the camera model of the three-dimensional model corresponding to the second distance value is oriented to face the 3D pixel; When,
(E) determining, based at least in part on the determined first and second confidence scores, whether the points corresponding to the 3D pixels should be included in the merged 3D model; The 3D pixel is not included in the merged 3D model if the second confidence score is greater than the first confidence score by a predetermined amount;
Including methods.
(Item 2)
The determining (c) is based on an angle between the three-dimensional model and a ray extending from the viewpoint of the camera model toward the 3D pixel, and as the angle becomes more acute, The method of item 1, comprising determining the confidence score such that the likelihood that a point is included in the merged three-dimensional model is reduced.
(Item 3)
The determining (c) is based on the distance between the samplings of the corresponding three-dimensional model in the close range determined with respect to the 3D pixel, and as the distance between the samplings increases, the points become The method of item 1, comprising determining the confidence score such that the likelihood of being included in a merged three-dimensional model is reduced.
(Item 4)
(F) (i) the point is determined in (e) to be included in the merged three-dimensional model, and (ii) third and fourth from the plurality of determined distance values According to both the third and fourth distance values, if both distance values indicate that the distance between the 3D pixel and the respective 3D model does not exceed the threshold. Determining weights for the pixels;
(G) determining where to build the edges of the merged 3D model between the two adjacent 3D pixels based on the weights determined for the two adjacent 3D pixels;
The method according to Item 1, further comprising:
(Item 5)
The method according to item 1, wherein each three-dimensional model in the plurality of three-dimensional models is expressed as a depth map.
(Item 6)
The method according to item 1, wherein each three-dimensional model in the plurality of three-dimensional models is expressed as an altitude field.
(Item 7)
The method according to item 1, wherein each three-dimensional model in the plurality of three-dimensional models is a stereo reconstruction.
(Item 8)
A system for merging a plurality of 3D models, wherein each of the plurality of 3D models is generated from images of cameras having different viewpoints, the system comprising:
One or more processors;
A memory coupled to the one or more processors;
A 3D pixel determination module configured to determine a plurality of 3D pixels in a three-dimensional space;
A distance test module configured to determine a plurality of distance values for each 3D pixel in the plurality of 3D pixels, wherein each distance value is derived from the 3D pixel in the viewpoint direction of the camera model. A distance from the plurality of 3D models to the 3D model, the camera model is used to generate the 3D model, and the distance test module is implemented in the one or more processors. The distance test module,
A confidence score module comprising: (i) a first distance value from the plurality of distance values, the 3D model corresponding to the 3D pixel and the first distance value; And (ii) a second distance value from the plurality of distance values indicates that the 3D pixel is outside the threshold value and the second distance value is within the threshold value range. If we indicate that we are above the 3D model corresponding to the distance value,
Determining a first confidence score indicating the degree to which the viewpoint of the camera model of the three-dimensional model corresponding to the first distance value is oriented to face the 3D pixel;
Determining a second confidence score indicating the degree to which the viewpoint of the camera model of the three-dimensional model corresponding to the second distance value is oriented to face the 3D pixel;
A confidence score module, configured as follows:
A point selector module, which should include a point corresponding to the 3D pixel in the merged three-dimensional model based at least in part on the determined first and second confidence scores If the second confidence score is greater than the first confidence score by a predetermined degree, then the 3D pixel is the merged three-dimensional model. Not included in the point selector module
Including the system.
(Item 9)
Based on the angle between the three-dimensional model and a ray extending from the viewpoint of the camera model toward the 3D pixel, the confidence score module determines that the point becomes more acute as the angle becomes more acute. 9. The system of item 8, wherein the system is configured to determine the confidence score such that the likelihood of being included in the merged three-dimensional model is reduced.
(Item 10)
The confidence score module is configured to merge the points as the distance between the samplings increases based on the distance between the samplings of the corresponding three-dimensional model in the proximity determined with reference to the 3D pixels. 9. The system of item 8, wherein the system is configured to determine the confidence score such that the likelihood of being included in a three-dimensional model is reduced.
(Item 11)
The model building module further includes: (i) the points determined by the point selector module such that the points are included in the merged three-dimensional model, and the plurality of determined distance values. The third and fourth distance values if both the third and fourth distance values from indicate that the distance between the 3D pixel and the respective three-dimensional model does not exceed the threshold. Determine a weight for the 3D pixel according to both of the values, and (ii) the merged 3 between the two adjacent 3D pixels based on the weight determined for the two adjacent 3D pixels. 9. A system according to item 8, configured to determine where the edges of the dimensional model should be built.
(Item 12)
The system according to item 8, wherein each three-dimensional model in the plurality of three-dimensional models is expressed as a depth map.
(Item 13)
9. The system according to item 8, wherein each three-dimensional model in the plurality of three-dimensional models is expressed as an altitude field.
(Item 14)
9. The system according to item 8, wherein each three-dimensional model in the plurality of three-dimensional models is a stereo reconstruction.
(Item 15)
A computer program product comprising a non-transitory computer readable medium, wherein the medium stores instructions that, when executed by the computing device, cause the computing device to receive a plurality of three-dimensional Performing an operation of merging models, each of the plurality of three-dimensional models being generated from images of cameras having different viewpoints, the operations comprising:
(A) determining a plurality of 3D pixels in a three-dimensional space;
For each 3D pixel in the plurality of 3D pixels,
(B) determining a plurality of distance values, each distance value being a distance from the 3D pixel to a corresponding three-dimensional model from the plurality of three-dimensional models in the viewpoint direction of the camera model; The camera model is used to generate the three-dimensional model;
(I) The first distance value from the plurality of distance values indicates that the distance between the 3D pixel and the three-dimensional model corresponding to the first distance value is within a threshold range. And (ii) a second distance value from the plurality of distance values is such that the 3D pixel is outside the threshold and above the three-dimensional model corresponding to the second distance value. When indicating
(C) determining a first confidence score indicating the degree to which the viewpoint of the camera model of the three-dimensional model corresponding to the first distance value is oriented to face the 3D pixel; When,
(D) determining a second confidence score indicating the degree to which the viewpoint of the camera model of the three-dimensional model corresponding to the second distance value is oriented to face the 3D pixel; When,
(E) determining, based at least in part on the determined first and second confidence scores, whether the points corresponding to the 3D pixels should be included in the merged 3D model; The 3D pixel is not included in the merged 3D model if the second confidence score is greater than the first confidence score by a predetermined amount;
Including computer program products.
(Item 16)
The determining (c) is based on an angle between the three-dimensional model and a ray extending from the viewpoint of the camera model toward the 3D pixel, and as the angle becomes more acute, 16. The computer program product of item 15, comprising determining the confidence score such that the likelihood that a point will be included in the merged three-dimensional model is reduced.
(Item 17)
The determining (c) is based on the distance between the samplings of the corresponding three-dimensional model in the close range determined with reference to the 3D pixel, and as the distance between the samplings increases, The computer program product of item 16, further comprising determining the confidence score such that the likelihood of being included in the merged three-dimensional model is reduced.
(Item 18)
The operation is
(F) (i) the point is determined in (d) to be included in the merged three-dimensional model, and (ii) third and fourth from the plurality of determined distance values. According to both the third and fourth distance values, if both distance values indicate that the distance between the 3D pixel and the respective 3D model does not exceed the threshold. Determining weights for the pixels;
(G) determining where to build the edges of the merged 3D model between the two adjacent 3D pixels based on the weights determined for the two adjacent 3D pixels;
The computer program product of item 17, further comprising:
(Item 19)
Item 16. The computer program product according to Item 15, wherein each three-dimensional model in the plurality of three-dimensional models is expressed as a depth map.
(Item 20)
Item 16. The computer program product according to Item 15, wherein each three-dimensional model in the plurality of three-dimensional models is expressed as an altitude field.
(Item 21)
Item 16. The computer program product according to Item 15, wherein each three-dimensional model in the plurality of three-dimensional models is a stereo reconstruction.
(Item 22)
A system for merging a plurality of 3D models, wherein each of the plurality of 3D models is generated from images of cameras having different viewpoints, the system comprising:
One or more processors;
A memory coupled to the one or more processors;
Means for determining a plurality of 3D pixels in a three-dimensional space;
Means for determining a plurality of distance values for each 3D pixel in the plurality of 3D pixels, wherein each distance value is derived from the plurality of three-dimensional models in the viewpoint direction of the camera model from the 3D pixels; A distance to a three-dimensional model, wherein the camera model is used to generate the three-dimensional model, and the distance test module is implemented in the one or more processors;
(I) The first distance value from the plurality of distance values indicates that the distance between the 3D pixel and the three-dimensional model corresponding to the first distance value is within a threshold range. And (ii) a second distance value from the plurality of distance values is such that the 3D pixel is outside the threshold and above the three-dimensional model corresponding to the second distance value. When indicating
Determining a first confidence score indicating the degree to which the viewpoint of the camera model of the three-dimensional model corresponding to the first distance value is oriented to face the 3D pixel;
Determining a second confidence score indicating the degree to which the viewpoint of the camera model of the three-dimensional model corresponding to the second distance value is oriented to face the 3D pixel;
Means,
Means for determining, based at least in part on the determined first and second confidence scores, whether a point corresponding to the 3D pixel should be included in a merged three-dimensional model comprising: Means that the 3D pixel is not included in the merged 3D model if the confidence score of is greater than the first confidence score by a predetermined degree;
Including the system.
本明細書中に組み込まれ、本明細書の一部を形成する添付の図面は、本発明を図示し、添付の図面は、説明と共に、本発明の原理を明白にすること、および当業者が本発明を実施することを可能にすることにさらに役立つ。 The accompanying drawings, which are incorporated in and form a part of this specification, illustrate the present invention, and together with the description, clarify the principles of the invention and enable those skilled in the art to understand the principles of the invention. It further helps to make it possible to implement the invention.
要素が初めて現れる図は、典型的に、対応する参照数字における最も左の桁（単数または複数）によって、示される。図面において、同様の参照数字は、同一または機能的に類似した要素を示し得る。 The figure in which an element first appears is typically indicated by the leftmost digit (s) in the corresponding reference numeral. In the drawings, like reference numbers can indicate identical or functionally similar elements.
（実施形態の詳細な説明）
上記で言及したように、異なる視点を有する複数の深度マップは、３次元シーンを完全に説明するために必要とされ得る。全体の３次元シーンの表現を生成するために、複数の深度マップは、１つの３次元的表現に併合される必要があり得る。複数の深度マップをさらに併合するための１つの方法は、共通部分をとることであり得る。共通部分をとるために、任意の測定された深度値より大きい任意の３Ｄ画素は、取り除かれ得る。しかし、本技術は、欠点をこうむり得る。特に、深度マップが、３次元的特徴が実際よりも奥行きの深いことを示す場合の任意の誤った測定値は、併合された深度マップ内へ伝播させられ得る。
(Detailed description of embodiment)
As mentioned above, multiple depth maps with different viewpoints may be needed to fully describe the 3D scene. Multiple depth maps may need to be merged into a single three-dimensional representation in order to generate a representation of the entire three-dimensional scene. One way to further merge multiple depth maps may be to take a common part. To take the intersection, any 3D pixel that is larger than any measured depth value can be removed. However, this technique can suffer from disadvantages. In particular, any erroneous measurements where the depth map indicates that the three-dimensional feature is deeper than it can be propagated into the merged depth map.
深度マップにおけるノイズを補償するために、他の技術は、測定値を平均することにより、深度マップを併合する。しかし、上記で言及されたように、深度マップは、深度マップの関連した視点から明白である３次元的特徴のみを説明し得る。見えなくされた３次元的特徴は、深度マップにおいて全く表現されないこともある。同様に、深度マップの視点に正対していない特徴は、深度マップにおいて低い解像度でのみ表現され得る。これらの深度マップの測定値を、３次元的特徴のよりまっすぐな眺めを有する深度マップからの測定値と共に平均することは、併合された深度マップの正確さを低下させ得る。 To compensate for noise in the depth map, other techniques merge the depth map by averaging the measurements. However, as mentioned above, the depth map may only describe three-dimensional features that are apparent from the associated viewpoint of the depth map. Three-dimensional features that are made invisible may not be represented at all in the depth map. Similarly, features that do not face the depth map viewpoint can only be represented in the depth map at a lower resolution. Averaging these depth map measurements along with measurements from depth maps that have a more straightforward view of the three-dimensional features may reduce the accuracy of the merged depth map.
少なくとも一部分欠点に対処するために、実施形態は、深度マップの測定値に関連した信頼度水準に基づいて、どの深度マップの測定値を併合して最終的な３次元モデルにするかを選択する。 To address at least in part the shortcomings, the embodiment selects which depth map measurements are merged into a final three-dimensional model based on a confidence level associated with the depth map measurements. .
一実施形態において、種々の深度マップは、複数の３Ｄ画素を含む符号付距離フィールドを決定するために使用され得る。符号付距離フィールドにおける各３Ｄ画素は、評価され得、どのような評価であるかに基づいて、３つの操作のうちの１つが、画素に対して行われることにより、併合された３次元モデルを決定することを助け得る。第一に、しきい値距離「ｔ」が与えられると、符号付距離値のうちの少なくとも１つが、−ｔよりも小さい場合、３Ｄ画素は、空に保たれ得る（例えば、３Ｄ画素に大きな負の値を割り当てる）。３Ｄ画素を空に保つことは、３Ｄ画素を併合された３次元モデルから切り取り得る。第二に、符号付距離値のうちの任意のものが、−ｔとｔとの間にある場合、３Ｄ画素の符号付距離は、それらの値の平均に設定され得る。２つの値の平均をとることによって、２つの併合深度マップは、その３Ｄ画素において併合され得る。第三に、符号付距離のうちの任意のものが、ｔよりも大きい場合、３Ｄ画素は、充填され得る（つまり、３Ｄ画素に大きな正の値を割り当てる）。このように、各３Ｄ画素は、切り取られ、併合され、または充填されることにより、併合された３次元モデルにおける対応する３Ｄ画素を決定し得る。
In one embodiment, various depth maps can be used to determine a signed distance field that includes multiple 3D pixels. Contact Keru each 3D pixel to the signed distance field can be assessed, based on what rating a either, one of the three operations, by being made to the pixel, the merged 3D Can help determine the model. First, given a threshold distance “t”, a 3D pixel can be kept empty (eg, larger for a 3D pixel) if at least one of the signed distance values is less than −t. Assign negative values). Keeping the 3D pixels empty may cut out the 3D pixels from the merged 3D model. Second, if any of the signed distance values are between -t and t, the signed distance of the 3D pixels can be set to the average of those values. By taking the average of the two values, the two merged depth maps can be merged at the 3D pixel. Third, if any of the signed distances is greater than t, the 3D pixel can be filled (ie, assigning a large positive value to the 3D pixel). Thus, each 3D pixel can be cut, merged, or filled to determine the corresponding 3D pixel in the merged 3D model.
以下の実施形態の詳細な説明において、「一実施形態」、「実施形態」、「例示の実施形態」等への言及は、説明された実施形態が、特定の特徴、構造、または特性を含み得るが、あらゆる実施形態が、必ずしもその特定の特徴、構造、または特性を含まなくてもよいことを示す。そのうえ、そのような句は、必ずしも同じ実施形態を言及しているわけではない。さらに、特定の特徴、構造、または特性が、実施形態と関連して説明される場合、明示的に説明されていようとなかろうと、他の実施形態に関連したそのような特徴、構造、または特性をもたらすことは、当業者の知識の範囲内であることが思料される。 In the detailed description of the embodiments below, references to “one embodiment”, “embodiments”, “exemplary embodiments”, etc. include a specific feature, structure, or characteristic of the described embodiment. It should be understood that every embodiment may not necessarily include that particular feature, structure, or characteristic. Moreover, such phrases are not necessarily referring to the same embodiment. Further, where a particular feature, structure, or characteristic is described in connection with an embodiment, such feature, structure, or characteristic in relation to other embodiments, whether explicitly described or not Is conceivable to be within the knowledge of one of ordinary skill in the art.
図は、例証の目的のために２次元断面図を図示しているが、当業者は、断面が、３次元要素を表現し得ることを認識する。 Although the figure illustrates a two-dimensional cross-sectional view for purposes of illustration, those skilled in the art will recognize that a cross-section can represent a three-dimensional element.
図１は、異なるビューポイントから構築されたシーンの深度マップを図示している図１００を示している。図１００は、仮想カメラ１０２から構築された３次元モデル１１２と、仮想カメラ１０４から構築された３次元モデル１１４とを図示する。両方の３次元モデル１１２および１１４は、建造物１３０を表現し得る。
FIG. 1 shows a diagram 100 illustrating a depth map of a scene constructed from different viewpoints. FIG. 100 illustrates a three-
３次元モデル１１２および１１４は、航空機または衛星の画像から生成されたステレオ再構築であり得る。画像は、種々の斜めまたは天底の視点における頭上カメラによって撮られ得る。画像において、特徴は検出され、互いに関連づけられる。既知の視点情報を使用して、３次元空間における点は、対応する特徴から三角測量される。これらの点は、２つの画像からステレオメッシュを決定するために使用され得る。このように、３次元モデルの情報は、２次元画像から決定され得る。
The three-
しかし、３次元モデルの情報は、関連づけられた視点情報も有し得る。例えば、３次元モデルの情報は、特定の仮想カメラから再構築され得る。実施形態において、少なくとも２つの画像のステレオ再構築を使用して決定されたステレオメッシュは、特定のカメラビューポイントに投影し返され得る。３次元モデルが、深度マップとして表現される場合の実施形態において、投影における各画素は、想像上のカメラ視点からその画素におけるステレオメッシュまでの距離を示す値を有し得る。３次元モデルが、高度フィールドとして表現される場合の実施形態において、投影における各画素は、その画素におけるステレオメッシュの高度を示す値を有し得る。どちらの実施形態においても、各ステレオメッシュ自身に対する３次元モデルは、視点情報を関連づけている。３次元モデルは、他の任意のタイプの表面であり得、その表面のために、符号付距離は、計算（例えば、閉じたメッシュ）または別の符号付距離フィールドであり得る。 However, the information of the three-dimensional model can also have associated viewpoint information. For example, the information of the three-dimensional model can be reconstructed from a specific virtual camera. In an embodiment, a stereo mesh determined using stereo reconstruction of at least two images may be projected back to a particular camera viewpoint. In embodiments where the 3D model is represented as a depth map, each pixel in the projection may have a value that indicates the distance from the imaginary camera viewpoint to the stereo mesh at that pixel. In embodiments where the 3D model is represented as an altitude field, each pixel in the projection may have a value that indicates the altitude of the stereo mesh at that pixel. In both embodiments, the three-dimensional model for each stereo mesh itself associates viewpoint information. The three-dimensional model can be any other type of surface, for which the signed distance can be a calculation (eg, a closed mesh) or another signed distance field.
図１００において、モデル１１２および１１４についての視点情報は、仮想カメラ１０２および１０４によって例証される。仮想カメラ１０２および１０４の各々は、対応する深度マップまたは高度フィールドについての視点またはビューポイントを指定するために必要とされるすべての情報を含み得る。例えば、各仮想カメラモデルは、対応する位置、配向、および視野を有し得る。そのうえ、各仮想カメラモデルは、直交投影または平行投影であり得る。
In FIG. 100, viewpoint information about
１つの統合された３次元モデルを生成するために、実施形態は、３次元モデル１１２および３次元モデル１１４を併合する。上記で言及されたように、モデル１１２および１１４を併合するための１つの方法は、２つのモデルの共通部分をとることであり得る。しかし、本技術が適用される場合、建造物１３０が、実際よりも低いことを示しているモデル１１２および１１４における任意の誤差は、最終的な併合されたモデルまで伝播する。例えば、図１００において、モデル１１４は、建造物１３０が、領域１２４において実際よりも低いと示している。モデルが、交わる場合、その誤差は、最終的な併合されたモデルまで伝播する。少なくとも一部分この誤差を避けるために、実施形態は、深度マップの測定値に関連した信頼度水準に基づいて、どの深度マップの測定値を併合して最終的な３次元モデルにするかを選択する。どの深度マップの測定値を併合して最終的な３次元モデルにするかを決定する方法は、図２に図示される。
The embodiment merges the
図２は、実施形態に従った、深度マップを併合する方法２００を図示しているフローチャートである。方法２００は、図３〜６に図示されている例に関連して説明される。
FIG. 2 is a flowchart illustrating a
方法２００は、３Ｄ画素格子の構築に関するステップ２０２において始まる。３Ｄ画素格子は、符号付距離フィールドであり得る。３Ｄ画素格子は、入力３次元モデルデータの解像度、または併合された３次元モデルの所望の解像度に従って、構築され得る。入力または所望の出力３次元モデルの解像度が増加する場合、３Ｄ画素格子は、より細かくあり得る。例示の３Ｄ画素格子は、図３に関連して例証される。
The
図３は、図１におけるシーンのための３Ｄ画素格子を図示している図３００を示す。３Ｄ画素格子における各３Ｄ画素は、３次元空間における関連した位置を有し得る。３Ｄ画素は、３次元環境の全域で規則的に間隔をあけられ得る。３Ｄ画素格子における各３Ｄ画素は、各３Ｄ画素が、併合された３次元モデルの一部であるかどうかについて、独立して評価され得る。 FIG. 3 shows a diagram 300 illustrating a 3D pixel grid for the scene in FIG. Each 3D pixel in the 3D pixel grid may have an associated position in 3D space. The 3D pixels can be regularly spaced throughout the 3D environment. Each 3D pixel in the 3D pixel grid can be independently evaluated as to whether each 3D pixel is part of a merged 3D model.
図３００の３Ｄ画素格子における３Ｄ画素は、連続的な解像度を有する規則的な空間であると示されている。しかし、当業者は、他の実施形態は、適応性のある解像度を含み得ることを認識する。規則的に間隔をあけられた３Ｄ画素におけるサンプリングの代わりに、適応性のある解像度を用いる場合、サンプリング点は、表面の近くではより密集し得るが、表面から遠くではむしろ密集しなくてもよい。本実施形態は、より少ないメモリー使用について、より多くの詳細を提供し得る。図２を再び参照すると、各３Ｄ画素またはサンプリング点の評価は、方法２００におけるステップ２０４において始まる。
The 3D pixels in the 3D pixel grid of FIG. 300 are shown to be regular spaces with continuous resolution. However, those skilled in the art will recognize that other embodiments may include adaptive resolution. When using adaptive resolution instead of sampling at regularly spaced 3D pixels, the sampling points may be more dense near the surface, but rather dense away from the surface. . This embodiment may provide more details for less memory usage. Referring back to FIG. 2, the evaluation of each 3D pixel or sampling point begins at
ステップ２０４において、距離値は、各入力された３次元モデルに対して決定される。特に、距離値は、３Ｄ画素格子における３Ｄ画素と３次元モデルとの間の距離を表現し得る。各入力された３次元モデルに対して、距離は、３次元モデルに関連した視点方向に測定される。より具体的には、距離は、３Ｄ画素からまっすぐに仮想カメラに向かう光線または３Ｄ画素からまっすぐに仮想カメラから離れる光線に沿って測定され得る。３次元モデルが、高度フィールドまたは深度マップとして表現される場合、距離決定は、速い実行に役立つ単純なルックアップを含み得る。
In
距離は、符号付距離値であり得、符号付距離値の大きさ（絶対値）は、２点間の距離であり得ることを意味しているが、符号付距離値は、正または負にもなり得る。例えば、決定された距離値は、正であることにより、３Ｄ画素が、仮想カメラの視点からの３次元モデルの下方（または内側）に位置していることを示し、負の決定された距離値は、３Ｄ画素が、仮想カメラの視点からの３次元モデルの上方（または外側）に位置していることを示し得る。 The distance can be a signed distance value, meaning that the magnitude (absolute value) of the signed distance value can be the distance between two points, but the signed distance value can be positive or negative. Can also be. For example, the determined distance value is positive, indicating that the 3D pixel is located below (or inside) the 3D model from the viewpoint of the virtual camera, and the negative determined distance value May indicate that the 3D pixel is located above (or outside) the 3D model from the viewpoint of the virtual camera.
ひとたび距離値が、各深度マップに対して決定されると、３Ｄ画素がモデルのはるか下方にあることを示している任意の距離値は、ステップ２０６において放棄される。実施形態において、３Ｄ画素が、モデルのはるか下方にあるかどうかを決定するために、距離値は、距離値がしきい値を超過するかどうかを調べるために評価され得る。しきい値を超過する距離値は、３Ｄ画素が３次元モデルの視点の所与の眺めから見えなくされていることを示し得るので、しきい値を超過する距離値は、放棄される。対象が、３次元モデルの１つの視点からの眺めから見えなくされている一方、他のモデルは、３Ｄ画素のより良い眺めを有する角度から構築され得る。この理由で、３Ｄ画素がモデルの内側深くに位置していることを示している大きな距離値は、有用な情報を提供しなくてもよい。本ステップは、例えば、図４Ａに関連して例証される。
Once a distance value is determined for each depth map, any distance value indicating that the 3D pixel is far below the model is discarded in
図４Ａは、３Ｄ画素に対する種々の距離測定値を図示している図４００を示す。図１００および図１と同様に、図４００は、対応する仮想カメラ１０２および１０４を有する３次元モデル１１２および１１４を示す。そのうえ、図４００は、仮想カメラ４０６の視点から構築された３次元モデル４１６を示す。図４００は、３Ｄ画素４０２に対して生成された３次元モデルの各々についての距離値を図示する。
FIG. 4A shows a diagram 400 illustrating various distance measurements for 3D pixels. Similar to FIGS. 100 and 1, diagram 400 shows three-
特に、図４００は、モデル１１２についての距離４３２と、モデル１１４についての距離４３４と、モデル４１６についての距離４３６とを示す。例えば、仮定のしきい値距離が、５ｍであったと仮定する。図４００に図示されているように、距離４３４は、＋７ｍと測定され、３Ｄ画素４０２が、表面１１４の７ｍ下方にあることを意味している。その例において、距離４３４は、方法２００のステップ２０６において無視される。距離４３６および４３２は、より詳細に下記に説明される。
In particular, diagram 400 shows
図２を再び参照すると、ステップ２０６において、モデルの内側深くにある距離測定値を取り除いた後、残りの距離は、評価されることにより、決定ボックス２０８において、距離が、３Ｄ画素がモデルのはるか上方にあることを示すかどうかを決定する。距離が、３Ｄ画素がモデルのはるか上方にあることを示すかどうかを決定するために、距離値は、負のしきい値と比較され得る。距離が、負のしきい値の下方にある（大きさが、しきい値の大きさを超過する）場合、距離は、３Ｄ画素が対応するモデルのはるか上方にあることを示す。そうでなければ、距離は、３Ｄ画素が対応するモデルのはるか上方にあることを示さない。距離測定値が、３Ｄ画素が対応するモデルのはるか上方にあることを示さない場合、方法は、ステップ２１４へ進み、表面のはるか下方（例えば、しきい値のはるか下方）にあると決定された任意の測定値は、放棄され得る。測定値は、単にその測定値に対する信頼度をゼロに設定することによって、放棄され得る。そうでなければ、方法は、ステップ２１０へ進む。決定された距離が、３Ｄ画素がモデルのはるか上方にあることを示す例は、図４Ｂに図示されている。
Referring back to FIG. 2, after removing distance measurements deep inside the model in
図４Ｂは、３Ｄ画素に対する種々の距離測定値を図示している図４５０を示す。図１００および図１、ならびに図４Ｂの図４００と同様に、図４５０は、対応する仮想カメラ１０２および１０４を有する３次元モデル１１２および１１４を示す。そのうえ、図４５０は、仮想カメラ４５８の視点から構築された３次元モデル４６８を示す。図４５０は、３Ｄ画素４５２のために生成された３次元モデルの各々についての距離値を図示する。
FIG. 4B shows a diagram 450 illustrating various distance measurements for 3D pixels. Similar to FIGS. 100 and 1 and 400 of FIG. 4B, FIG. 450 shows three-
特に、図４５０は、モデル１１２についての距離４８６と、モデル１１４についての距離４３４と、モデル４１６についての距離４３８とを示す。例えば、仮定のしきい値距離が、−４ｍであったと仮定する。図４００に図示されているように、距離４８６は、−７ｍと測定され、３Ｄ画素４０２が、表面１１４の７ｍ下方にあることを意味している。その例において、距離４８６は、方法２００のステップ２０８において、３Ｄ画素が、３次元モデルのはるか上方にあることを示す。結果として、方法２００は、ステップ２１０へ進む。距離４３６および４３２は、より詳細に下記に説明される。
In particular, FIG. 450 shows
ステップ２１０において、信頼度スコアは、その距離に対して決定される。信頼度スコアは、深度マップにおいて読み込む距離の質を推定し得る。一般に、特徴のよりまっすぐな通常の眺めは、特徴のより接線に沿った眺めよりも高い解像度、およびことによると良い質を有する。その理由で、信頼度スコアは、対応する３次元モデルのカメラモデルの視点が、３Ｄ画素領域に正対するように配向されている度合を示し得る。信頼度スコアを決定する方法の例は、図５に図示されている。
In
図５は、距離測定値の信頼度スコアを決定する方法を図示している図５００を示す。図５００は、３Ｄ画素４５２と仮想カメラ４５８から構築された３次元モデル４６８とについて測定された距離４８６における信頼度を決定する方法を図示する。光線５７０は、仮想カメラ４５８から３Ｄ画素４５２を通って延びることにより、点５６０において３次元モデル４６８と交わる。３次元モデル４６８が、深度マップまたは高度フィールドとして表現される場合の実施形態において、位置５６０を決定することは、単純なルックアップを含み得る。光線２７０と３次元モデル４６８の間に、角度５５２が、決定され得る。決定された角度５５２は、信頼度スコアを決定するために使用され得る。角度５５２が、より鈍角になるにつれて、併合された３次元モデルに３Ｄ画素４５２を含む可能性は、増加し得る。同様に、角度５５２が、より鋭角になるにつれて、併合された３次元モデルに３Ｄ画素４５２を含む可能性は、減少し得る。このように、３次元モデルと仮想カメラの視点との間の角度は、併合された３次元モデルを決定することを助けるために使用され得る。
FIG. 5 shows a diagram 500 illustrating a method for determining a confidence score for a distance measurement. The diagram 500 illustrates a method for determining the reliability at a
角度に加えて、近くのサンプリング点の頻度も、併合された３次元モデルを決定するために使用され得る。図５００において、３次元モデル４６８は、点５６０においてサンプリングされた。点５６０に隣接したサンプリング点が、決定され得る。サンプリング点間の距離が、決定され得、信頼度スコアが、その距離に基づいて決定され得る。図５００において、点５６０は、サンプリング点５６２に隣接し得る。点５６０と点５６２との間で、距離５５４が、測定され得る。距離値４８６における信頼度の水準を示している信頼度スコアは、測定された距離５５４に基づいて決定され得る。距離５５４が、より長くなるにつれて、併合された３次元モデルに３Ｄ画素４５２を含む可能性は、減少し得る。同様に、距離５５４が、より短くなるにつれて、併合された３次元モデルに３Ｄ画素４５２を含む可能性は、増加し得る。このように、距離５５４と角度５５２との両方が、測定された距離４８６における信頼度の水準を決定するために使用され得る。
In addition to the angle, the frequency of nearby sampling points can also be used to determine the merged 3D model. In FIG. 500, the three-
他の実施形態において、ステレオマッチングの質は、信頼度スコアを決定するために使用され得る。そのうえ、信頼度スコアは、上記で説明された方法のうちの任意のものの組み合わせを使用して決定され得る。 In other embodiments, the quality of stereo matching can be used to determine a confidence score. Moreover, the confidence score can be determined using a combination of any of the methods described above.
さらなる例において、重みつき平均は、複数の信頼度値がある場合に使用され得る。例えば、図４を再び参照すると、距離４８２が、信頼度値０．２を有し、距離４８４が、０．５の信頼度値を有し、距離４８６が、信頼度値０．１を有する場合、３Ｄ画素４５２の３Ｄ画素格子に記憶され得る総計の符号付距離値は、（２＊０．２＋３＊０．５−７＊０．１）／（０．２＋０．５＋０．１）＝１．５である。
In a further example, a weighted average may be used when there are multiple confidence values. For example, referring again to FIG. 4,
図２を再び参照すると、ひとたび信頼度スコアが、ステップ２１０において決定されると、信頼度スコアは、ステップ２１２において、併合された３次元モデルに３Ｄ画素を含むべきかどうかを決定するために使用される。実施形態において、ステップ２１２は、近距離と遠距離との両方に対して決定された信頼度スコアを使用し得る。図５に関連して論じられた技術を使用して、値Ｃ近は、近距離に対して決定され得、値Ｃ遠は、遠距離に対して決定され得る。３Ｄ画素は、ステップ２１２において、以下の式が満足される場合に含まれ得る：Ｃ近＜αＣ遠、ここで、αは、事前定義されたバイアス値、または他の事前決定された度合である。このように、方法２００は、ステップ２１２において、３Ｄ画素が、併合された３次元モデルに含まれるべきかどうかを決定する。
Referring again to FIG. 2, once the confidence score is determined in
３Ｄ画素が含まれない場合、大きな負の距離が、３Ｄ画素格子に記憶され得る。その値は、３Ｄ画素の後のアルゴリズムが、併合された３次元モデルに含まれるべきではないことを示し得る。３Ｄ画素が、含まれる場合（決定ブロック２１６）、符号付距離は、ステップ２１４において３Ｄ画素に対して決定される。符号付距離に加えて、当業者は、重みが使用され得ることを認識する。
If 3D pixels are not included, large negative distances can be stored in the 3D pixel grid. That value may indicate that the algorithm after the 3D pixel should not be included in the merged 3D model. If 3D pixels are included (decision block 216), a signed distance is determined for the 3D pixels in
ステップ２１４において、符号付距離は、３Ｄ画素に対して決定される。符号付距離は、例えば、ステップ２０６および２０８において論じられた２つのしきい値間で測定された距離測定値に基づいて、決定され得る。これら２つのしきい値間の距離値は、対応する３次元モデルが、３Ｄ画素に近いことを示す。符号付距離は、例えば、３Ｄ画素に十分に近い残りの距離値を平均すること、または重みつき平均をとることによって、決定され得る。ステップ２１４の例は、図４Ａにおける距離４３２および４３６、ならびに図４Ｂにおける距離４８２および４８４に関連して例証される。
In
図４Ａにおいて、距離４３２および４３６は、３Ｄ画素に十分に近い。特に、それらの距離は、ステップ２０６および２０８に関連して前に説明されたしきい値の範囲内にあり得る。３Ｄ画素４０２に対する符号付距離を決定するために、距離値は、平均され得る。図４００に図示されているように、距離４３６は、３Ｄ画素４０２が、モデル４１６の２メートル下方にあることを示し、＋２ｍの距離値を生み出す。同様に、距離４３２は、３Ｄ画素４０２が、モデル４１６の３メートル上方にあることを示し、−３ｍの距離値を生み出す。それから、それぞれの符号付距離値は、平均され、−０．５の３Ｄ画素４０２に対する重みをもたらし得る。この符号付距離４０２は、３Ｄ画素格子または符号付距離フィールドに記憶され得る。
In FIG. 4A, distances 432 and 436 are sufficiently close to a 3D pixel. In particular, those distances may be within the threshold range previously described in connection with
同様に、図４Ｂにおいて、距離４８２および４８４は、３Ｄ画素に十分に近い。特に、それらの距離は、ステップ２０６および２０８に関連して前に説明されたしきい値の範囲内にあり得る。３Ｄ画素４５２に対する符号付距離を決定するために、距離値は、平均され得る。図４５０に図示されているように、距離４８２は、３Ｄ画素４５２が、モデル４１６の２メートル下方にあることを示し、＋２ｍの距離値を生み出す。同様に、距離４８４は、３Ｄ画素４５２が、モデル４１６の３ｍ下方にあることを示し、＋３ｍの距離値を生み出す。それから、それぞれの符号付距離値は、平均され、＋２．５の３Ｄ画素４５２に対する符号付距離をもたらし得る。この符号付距離４０２はまた、３Ｄ画素格子に記憶され得る。
Similarly, in FIG. 4B, distances 482 and 484 are sufficiently close to a 3D pixel. In particular, those distances may be within the threshold range previously described in connection with
決定ブロック２１８において、ステップ２０４〜２１６は、ステップ２０２において構築された３Ｄ画素空間における各３Ｄ画素に対して反復される。この反復の結果として、３Ｄ画素格子は、各３Ｄ画素に対する符号付距離値を含み得る。符号付距離のこの３次元行列は、ステップ２２０において、併合された３次元モデルを構築するために使用され得る。併合された３次元モデルは、３Ｄ画素格子における値に基づいて、併合された３次元モデルの縁を画定することによって構築され得る。一例において、縁は、正の値を有する３Ｄ画素と負の値を有する３Ｄ画素との間に画定され得る。正の３Ｄ画素と負の３Ｄ画素との間の位置の間で、縁の位置は、正および負の値の相対的な大きさに基づいて画定され得る。
In
３Ｄ画素格子から、メッシュ表面を導き出すいくつかの方法があり得る。一例は、マーチングキューブアルゴリズムの使用である。ステップ２２０の別の例は、図６に図示されている。
There can be several ways to derive the mesh surface from the 3D pixel grid. One example is the use of a marching cube algorithm. Another example of
図６は、実施形態に従って、符号付の符号付距離の行列から併合された３次元モデル６０２を構築することを図示している図６００を示す。特に図６００は、隣接した３Ｄ画素６１２および６１４を有する３Ｄ画素格子を図示する。３Ｄ画素６１２が、関連づけられた正の符号付距離（＋３）を有し、３Ｄ画素６１４が、関連づけられた負の符号付距離（−１）を有するので、３次元モデル６０２は、３Ｄ画素６１２と３Ｄ画素６１４との間のあたりに構築される。さらに、３Ｄ画素６１４の符号付距離の大きさ（１）が、３Ｄ画素６１２の符号付距離の大きさ（３）よりも小さいので、３次元モデル６０２は、３Ｄ画素６１２よりも３Ｄ画素６１４の近くを通過するように構築され得る。３Ｄ画素６１４と３Ｄ画素６１２との間の３次元モデル６０２の位置は、それぞれの符号付距離に線形に比例し得る。このように、併合された３次元モデル６０２は、３Ｄ画素格子を使用して決定され得る。
FIG. 6 shows a diagram 600 illustrating the construction of a merged three-
図７は、実施形態に従った、深度マップを併合するシステム７００を図示している図である。実施形態において、システム７００は、図２における方法に従って動作し得る。システム７００は、複数の入力深度マップ７０２を入力として受信する処理パイプラインサーバー７１０を含む。入力３次元モデル７０２が、深度マップと呼ばれているが、当業者は、３次元モデルデータを表現する他の方法があることを認識する。例えば、３次元モデル７０２は、高度フィールドとしても表現され得る。３次元モデル７０２の各々は、特定の視点からのステレオ再構築であり得る。入力３次元モデル７０２から、処理パイプラインサーバー７１０は、併合されたモデル７３０を生成する。
FIG. 7 is a diagram illustrating a
処理パイプラインサーバー７１０は、３Ｄ画素空間モジュール７１２と、距離テストモジュール７１４と、信頼度スコアモジュール７１６と、点セレクターモジュール７１８と、モデル構築モジュール７２０とを含む。これらのモジュールの各々は、下記に説明される。
The processing pipeline server 710 includes a 3D
３Ｄ画素空間モジュール７１２は、３次元空間における複数の３Ｄ画素を決定するように構成されている。３Ｄ画素は、３次元環境の全域で規則的に間隔をあけられ得る。３Ｄ画素格子における各３Ｄ画素は、各３Ｄ画素が、併合された３次元モデルの一部であるかどうかについて、独立して評価され得る。３Ｄ画素空間モジュール７１２によって生成されうる例示の３Ｄ画素空間は、図３に図示されている。
The 3D
距離テストモジュール７１４は、複数の３Ｄ画素においてそれぞれの３Ｄ画素に対する複数の距離値を決定するように構成されている。各距離値は、３Ｄ画素から、カメラモデルの視点方向の複数の深度マップ７０２からの３次元モデルまでの距離であり得、そのカメラモデルは、３次元モデルを生成するために使用される。種々の測定された距離の例は、図４Ａ〜Ｂに図示されている。
The
信頼度スコアモジュール７１６は、距離テストモジュール７１４によって決定されたそれぞれの距離に対する信頼度スコアを決定するように構成されている。信頼度スコアは、対応する３次元モデルのカメラモデルの視点が、３Ｄ画素に正対するように配向されている度合を示し得る。信頼度スコアモジュール７１６は、３次元モデルと、入力３次元モデルの視点から３Ｄ画素を通って延びている光線との間の角度に基づいて、信頼度スコアを決定するように構成され得る。同様に、信頼度スコアモジュール７１６は、３Ｄ画素を基準として決定された近接領域における対応する３次元モデルのサンプリング間の距離に基づいて、信頼度スコアを決定することを決定するように構成され得る。近接領域は、仮想カメラ情報に従ってレンダリングされる場合の３Ｄ画素を示す画素に隣接した画素を含み得る。信頼度スコアモジュール７１６は、図５に関連して説明されたような信頼度値を決定し得る。
The confidence score module 716 is configured to determine a confidence score for each distance determined by the
点セレクターモジュール７１８は、決定された信頼度スコアに少なくとも一部基づいて、３Ｄ画素に対応する点を併合された３次元モデルに含むべきかどうかを決定するように構成されている。
The
モデル構築モジュール７２０は、第一および第二の距離値の両方に従って、３Ｄ画素に対する符号付距離を決定するように構成されている。さらに、モデル構築モジュール７２０は、２つの隣接した３Ｄ画素に対して決定された符号付距離に基づいて、２つの隣接した３Ｄ画素の間で、併合された３次元モデルの縁をどこに構築すべきかを決定するように構成されている。モデル構築モジュールの動作は、例えば、図６に関連して説明される。
The
処理パイプラインサーバー７１０は、任意のコンピューティングデバイスに実装され得る。そのようなコンピューティングデバイスは、パーソナルコンピューター、携帯電話のようなモバイルデバイス、ワークステーション、埋め込みシステム、ゲームコンソール、テレビジョン、セットトップボックス、または他の任意のコンピューティングデバイスを含み得るが、それらに制限されない。さらに、コンピューティングデバイスは、命令の実行および記憶のためのプロセッサーおよびメモリーを有するデバイスを含み得るが、そのデバイスに制限されない。ソフトウェアは、１つ以上のアプリケーションと、オペレーティングシステムとを含み得る。ハードウェアは、プロセッサー、メモリー、およびグラフィカルユーザーインターフェースディスプレイを含み得るが、それらに制限されない。コンピューティングデバイスはまた、複数のプロセッサーと、複数の共有または分離メモリー構成要素とを有し得る。例えば、コンピューティングデバイスは、クラスタコンピューティング環境またはサーバーファームであり得る。 Processing pipeline server 710 may be implemented on any computing device. Such computing devices may include, but are not limited to, personal computers, mobile devices such as cell phones, workstations, embedded systems, game consoles, televisions, set-top boxes, or any other computing device. Not limited. Further, a computing device may include, but is not limited to, a device having a processor and memory for instruction execution and storage. The software may include one or more applications and an operating system. The hardware can include, but is not limited to, a processor, memory, and a graphical user interface display. The computing device may also have multiple processors and multiple shared or separate memory components. For example, the computing device can be a cluster computing environment or a server farm.
３Ｄ画素空間モジュール７１２、距離テストモジュール７１４、信頼度スコアモジュール７１６、点セレクターモジュール７１８、およびモデル構築モジュール７２０の各々は、ハードウェア、ソフトウェア、ファームウェア、またはそれらの任意の組み合わせに実装され得る。
Each of 3D
概要および要約のセクションは、発明者（単数または複数）によって想定されるような本発明の１つ以上であるがすべてではない例示的実施形態を述べ得、したがって、概要および要約のセクションは、本発明および添付の特許請求の範囲を制限することは決して意図されない。 The summary and summary sections may describe one or more but not all exemplary embodiments of the invention as contemplated by the inventor (s), and thus the summary and summary sections It is in no way intended to limit the invention and the appended claims.
本発明は、指定された機能およびその関係の実装を例証している機能的な基本構成要素の助けを借りて上記で説明されてきた。これらの機能的な基本成要素の境界は、説明の便利のために、本明細書中に独断的に定義されてきた。指定された機能およびその関係が、適切に実施される限り、代替の境界は、定義され得る。 The present invention has been described above with the help of functional building blocks illustrating the implementation of specified functions and relationships. The boundaries of these functional basic components have been arbitrarily defined herein for convenience of explanation. Alternative boundaries can be defined as long as the specified function and its relationships are properly implemented.
特定の実施形態の前述の説明は、本発明の一般的な性質を完全に明らかにするので、人々は、技術分野の範囲内の知識を適用することによって、過度な実験なしに、本発明の一般的な概念から逸脱することなしに、そのような特定の実施形態を種々の用途に容易に改変し、かつ／または適合させ得る。したがって、そのような適合および改変は、本明細書中に提示された教示および指導に基づいて、開示された実施形態の均等物の趣意および範囲の範囲内にあることが意図される。本明細書の用語または語法は、教示および指導の見地から当業者によって解釈されるように、本明細書中の語法または用語は、説明の目的のためのものであり、制限の目的のためのものではないことが理解される。 The foregoing description of specific embodiments fully reveals the general nature of the invention, so that people can apply the knowledge within the technical field without undue experimentation. Such particular embodiments may be readily modified and / or adapted to various applications without departing from the general concept. Accordingly, such adaptations and modifications are intended to be within the spirit and scope of equivalents of the disclosed embodiments based on the teachings and guidance presented herein. The terminology or terminology herein is for the purpose of explanation and is intended for the purpose of limitation, as the terminology or terminology herein is interpreted by those skilled in the art from the standpoint of teaching and teaching. It is understood that it is not a thing.
本発明の広さおよび範囲は、上記で説明された例示的実施形態のうちのいずれによっても制限されるべきではないが、本発明の広さおよび範囲は、下記の特許請求の範囲およびその均等物に従ってのみ定義されるべきである。 The breadth and scope of the present invention should not be limited by any of the above-described exemplary embodiments, but the breadth and scope of the present invention is not limited by the following claims and their equivalents. Should be defined only according to things.
Claims (21)
（ａ）３次元空間における複数の３Ｄ画素を決定することと、 (A) determining a plurality of 3D pixels in a three-dimensional space;
該複数の３Ｄ画素におけるそれぞれの３Ｄ画素に対して、 For each 3D pixel in the plurality of 3D pixels,
（ｂ）複数の距離値を決定することであって、各距離値は、該３Ｄ画素から、カメラモデルの視点方向の該複数の３次元モデルからの対応する３次元モデルまでの距離であり、該カメラモデルは、該３次元モデルを生成するために使用されている、ことと、 (B) determining a plurality of distance values, each distance value being a distance from the 3D pixel to a corresponding three-dimensional model from the plurality of three-dimensional models in the viewpoint direction of the camera model; The camera model is used to generate the three-dimensional model;
（ｉ）該複数の距離値からの第一の距離値が、該３Ｄ画素と該第一の距離値に対応する該３次元モデルとの間の距離がしきい値の範囲内にあることを示し、かつ、（ｉｉ）該複数の距離値からの第二の距離値が、該３Ｄ画素が該しきい値の外側かつ該第二の距離値に対応する該３次元モデルの上方にあることを示す場合に、 (I) The first distance value from the plurality of distance values indicates that the distance between the 3D pixel and the three-dimensional model corresponding to the first distance value is within a threshold range. And (ii) a second distance value from the plurality of distance values is such that the 3D pixel is outside the threshold and above the three-dimensional model corresponding to the second distance value. When indicating
（ｃ）該第一の距離値に対応する該３次元モデルの該カメラモデルの該視点が、該３Ｄ画素に正対するように配向されている度合を示す第一の信頼度スコアを決定することと、 (C) determining a first confidence score indicating the degree to which the viewpoint of the camera model of the three-dimensional model corresponding to the first distance value is oriented to face the 3D pixel; When,
（ｄ）該第二の距離値に対応する該３次元モデルの該カメラモデルの該視点が、該３Ｄ画素に正対するように配向されている度合を示す第二の信頼度スコアを決定することと、 (D) determining a second confidence score indicating the degree to which the viewpoint of the camera model of the three-dimensional model corresponding to the second distance value is oriented to face the 3D pixel; When,
（ｅ）該決定された第一および第二の信頼度スコアに少なくとも一部基づいて、該３Ｄ画素に対応する点を併合された３次元モデルに含むべきかどうかを決定することであって、該第二の信頼度スコアが、該第一の信頼度スコアよりも事前決定された度合だけ大きい場合に、該３Ｄ画素は、該併合された３次元モデルに含まれない、ことと (E) determining, based at least in part on the determined first and second confidence scores, whether the points corresponding to the 3D pixels should be included in the merged 3D model; The 3D pixel is not included in the merged 3D model if the second confidence score is greater than the first confidence score by a predetermined amount;
を含む方法。 Including methods.
（ｇ）２つの隣接した３Ｄ画素に対して決定された重みに基づいて、該２つの隣接した３Ｄ画素の間で、該併合された３次元モデルの縁をどこに構築すべきかを決定することと (G) determining where to build the edges of the merged 3D model between the two adjacent 3D pixels based on the weights determined for the two adjacent 3D pixels;
をさらに含む、請求項１に記載の方法。 The method of claim 1, further comprising:
１つ以上のプロセッサーと、 One or more processors;
該１つ以上のプロセッサーに連結されているメモリーと、 A memory coupled to the one or more processors;
３次元空間における複数の３Ｄ画素を決定するように構成されている３Ｄ画素決定モジュールと、 A 3D pixel determination module configured to determine a plurality of 3D pixels in a three-dimensional space;
該複数の３Ｄ画素におけるそれぞれの３Ｄ画素に対して、複数の距離値を決定するように構成されている距離テストモジュールであって、各距離値は、該３Ｄ画素から、カメラモデルの視点方向の該複数の３次元モデルからの３次元モデルまでの距離であり、該カメラモデルは、該３次元モデルを生成するために使用され、該距離テストモジュールは、該１つ以上のプロセッサーに実装されている、距離テストモジュールと、 A distance test module configured to determine a plurality of distance values for each 3D pixel in the plurality of 3D pixels, wherein each distance value is derived from the 3D pixel in the viewpoint direction of the camera model. A distance from the plurality of 3D models to the 3D model, the camera model is used to generate the 3D model, and the distance test module is implemented in the one or more processors. The distance test module,
信頼度スコアモジュールであって、該信頼度スコアモジュールは、（ｉ）該複数の距離値からの第一の距離値が、該３Ｄ画素と該第一の距離値に対応する該３次元モデルとの間の距離がしきい値の範囲内あることを示し、かつ、（ｉｉ）該複数の距離値からの第二の距離値が、該３Ｄ画素が該しきい値の外側かつ該第二の距離値に対応する該３次元モデルの上方にあることを示す場合に、 A confidence score module comprising: (i) a first distance value from the plurality of distance values, the 3D model corresponding to the 3D pixel and the first distance value; And (ii) a second distance value from the plurality of distance values indicates that the 3D pixel is outside the threshold value and the second distance value is within the threshold value range. If we indicate that we are above the 3D model corresponding to the distance value,
該第一の距離値に対応する該３次元モデルの該カメラモデルの該視点が、該３Ｄ画素に正対するように配向されている度合を示す第一の信頼度スコアを決定し、 Determining a first confidence score indicating the degree to which the viewpoint of the camera model of the three-dimensional model corresponding to the first distance value is oriented to face the 3D pixel;
該第二の距離値に対応する該３次元モデルの該カメラモデルの該視点が、該３Ｄ画素に正対するように配向されている度合を示す第二の信頼度スコアを決定する Determining a second confidence score indicating the degree to which the viewpoint of the camera model of the three-dimensional model corresponding to the second distance value is oriented to face the 3D pixel;
ように構成されている、信頼度スコアモジュールと、 A confidence score module, configured as follows:
点セレクターモジュールであって、該点セレクターモジュールは、該決定された第一および第二の信頼度スコアに少なくとも一部基づいて、該３Ｄ画素に対応する点を併合された３次元モデルに含むべきかどうかを決定するように構成され、該第二の信頼度スコアが、該第一の信頼度スコアよりも事前決定された度合だけ大きい場合に、該３Ｄ画素は、該併合された３次元モデルに含まれない、点セレクターモジュールと A point selector module, which should include a point corresponding to the 3D pixel in the merged three-dimensional model based at least in part on the determined first and second confidence scores If the second confidence score is greater than the first confidence score by a predetermined degree, then the 3D pixel is the merged three-dimensional model. Not included in the point selector module
を含む、システム。 Including the system.
（ａ）３次元空間における複数の３Ｄ画素を決定することと (A) determining a plurality of 3D pixels in a three-dimensional space;
該複数の３Ｄ画素におけるそれぞれの３Ｄ画素に対して、 For each 3D pixel in the plurality of 3D pixels,
（ｂ）複数の距離値を決定することであって、各距離値は、該３Ｄ画素から、カメラモデルの視点方向の該複数の３次元モデルからの対応する３次元モデルまでの距離であり、該カメラモデルは、該３次元モデルを生成するために使用されている、ことと、 (B) determining a plurality of distance values, each distance value being a distance from the 3D pixel to a corresponding three-dimensional model from the plurality of three-dimensional models in the viewpoint direction of the camera model; The camera model is used to generate the three-dimensional model;
（ｉ）該複数の距離値からの第一の距離値が、該３Ｄ画素と該第一の距離値に対応する該３次元モデルとの間の距離がしきい値の範囲内にあることを示し、かつ、（ｉｉ）該複数の距離値からの第二の距離値が、該３Ｄ画素が該しきい値の外側かつ該第二の距離値に対応する該３次元モデルの上方にあることを示す場合に、 (I) The first distance value from the plurality of distance values indicates that the distance between the 3D pixel and the three-dimensional model corresponding to the first distance value is within a threshold range. And (ii) a second distance value from the plurality of distance values is such that the 3D pixel is outside the threshold and above the three-dimensional model corresponding to the second distance value. When indicating
（ｃ）該第一の距離値に対応する該３次元モデルの該カメラモデルの該視点が、該３Ｄ画素に正対するように配向されている度合を示す第一の信頼度スコアを決定することと、 (C) determining a first confidence score indicating the degree to which the viewpoint of the camera model of the three-dimensional model corresponding to the first distance value is oriented to face the 3D pixel; When,
（ｄ）該第二の距離値に対応する該３次元モデルの該カメラモデルの該視点が、該３Ｄ画素に正対するように配向されている度合を示す第二の信頼度スコアを決定することと、 (D) determining a second confidence score indicating the degree to which the viewpoint of the camera model of the three-dimensional model corresponding to the second distance value is oriented to face the 3D pixel; When,
（ｅ）該決定された第一および第二の信頼度スコアに少なくとも一部基づいて、該３Ｄ画素に対応する点を併合された３次元モデルに含むべきかどうかを決定することであって、該第二の信頼度スコアが、該第一の信頼度スコアよりも事前決定された度合だけ大きい場合に、該３Ｄ画素は、該併合された３次元モデルに含まれない、ことと (E) determining, based at least in part on the determined first and second confidence scores, whether the points corresponding to the 3D pixels should be included in the merged 3D model; The 3D pixel is not included in the merged 3D model if the second confidence score is greater than the first confidence score by a predetermined amount;
を含む、コンピュータープログラム製品。 Including computer program products.
（ｆ）（ｉ）前記点が、（ｄ）において、前記併合された３次元モデルに含まれるように決定され、かつ、（ｉｉ）前記複数の決定された距離値からの第三および第四の距離値の両方が、前記３Ｄ画素とそれぞれの３次元モデルとの間の距離が前記しきい値を超過しないことを示す場合に、該第三および第四の距離値の両方に従って、該３Ｄ画素に対する重みを決定することと、 (F) (i) the point is determined in (d) to be included in the merged three-dimensional model, and (ii) third and fourth from the plurality of determined distance values. According to both the third and fourth distance values, if both distance values indicate that the distance between the 3D pixel and the respective 3D model does not exceed the threshold. Determining weights for the pixels;
（ｇ）２つの隣接した３Ｄ画素に対して決定された重みに基づいて、該２つの隣接した３Ｄ画素の間で、該併合された３次元モデルの縁をどこに構築すべきかを決定することと (G) determining where to build the edges of the merged 3D model between the two adjacent 3D pixels based on the weights determined for the two adjacent 3D pixels;
をさらに含む、請求項１７に記載のコンピュータープログラム製品。 The computer program product of claim 17, further comprising:
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/461,403 | 2012-05-01 | ||
US13/461,403 US8462155B1 (en) | 2012-05-01 | 2012-05-01 | Merging three-dimensional models based on confidence scores |
PCT/US2013/038881 WO2013166023A1 (en) | 2012-05-01 | 2013-04-30 | Merging three-dimensional models based on confidence scores |
Publications (2)
Publication Number | Publication Date |
---|---|
JP2014514682A JP2014514682A (en) | 2014-06-19 |
JP5592039B2 true JP5592039B2 (en) | 2014-09-17 |
Family
ID=48538396
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
JP2014513809A Active JP5592039B2 (en) | 2012-05-01 | 2013-04-30 | Merge 3D models based on confidence scores |
Country Status (7)
Country | Link |
---|---|
US (1) | US8462155B1 (en) |
EP (1) | EP2705500B1 (en) |
JP (1) | JP5592039B2 (en) |
KR (1) | KR101399236B1 (en) |
CN (1) | CN103503033B (en) |
DE (1) | DE202013012443U1 (en) |
WO (1) | WO2013166023A1 (en) |
Families Citing this family (6)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
TWI441042B (en) * | 2011-07-01 | 2014-06-11 | Pixart Imaging Inc | Interactive image system, interactive control device and operation method thereof |
US20130141433A1 (en) * | 2011-12-02 | 2013-06-06 | Per Astrand | Methods, Systems and Computer Program Products for Creating Three Dimensional Meshes from Two Dimensional Images |
US9083960B2 (en) * | 2013-01-30 | 2015-07-14 | Qualcomm Incorporated | Real-time 3D reconstruction with power efficient depth sensor usage |
US9767378B2 (en) * | 2015-08-31 | 2017-09-19 | Sony Corporation | Method and system to adaptively track objects |
US11830140B2 (en) * | 2021-09-29 | 2023-11-28 | Verizon Patent And Licensing Inc. | Methods and systems for 3D modeling of an object by merging voxelized representations of the object |
CN116310149B (en) * | 2023-05-18 | 2023-07-21 | 深圳优立全息科技有限公司 | Voxel merging method, device, equipment and storage medium |
Family Cites Families (23)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6912293B1 (en) | 1998-06-26 | 2005-06-28 | Carl P. Korobkin | Photogrammetry engine for model construction |
JP2001143057A (en) * | 1999-11-18 | 2001-05-25 | Asia Air Survey Co Ltd | Method for creating three-dimensional model of city |
US7193633B1 (en) | 2000-04-27 | 2007-03-20 | Adobe Systems Incorporated | Method and apparatus for image assisted modeling of three-dimensional scenes |
US6868191B2 (en) | 2000-06-28 | 2005-03-15 | Telefonaktiebolaget Lm Ericsson (Publ) | System and method for median fusion of depth maps |
JP2002074323A (en) * | 2000-09-01 | 2002-03-15 | Kokusai Kogyo Co Ltd | Method and system for generating three-dimensional urban area space model |
JP2002157576A (en) | 2000-11-22 | 2002-05-31 | Nec Corp | Device and method for processing stereo image and recording medium for recording stereo image processing program |
JP2003317081A (en) * | 2002-04-25 | 2003-11-07 | Sony Corp | System and method for three-dimensional model generation, and computer program |
JP4206449B2 (en) * | 2002-10-09 | 2009-01-14 | 株式会社ジオ技術研究所 | Method for generating 3D electronic map data |
US20050128212A1 (en) | 2003-03-06 | 2005-06-16 | Edecker Ada M. | System and method for minimizing the amount of data necessary to create a virtual three-dimensional environment |
US20050212794A1 (en) | 2004-03-29 | 2005-09-29 | Communications Research Laboratory, Independent Administrative Institution | Method and apparatus for removing of shadows and shadings from texture images |
KR100748719B1 (en) * | 2005-07-14 | 2007-08-13 | 연세대학교 산학협력단 | Apparatus and method for 3-dimensional modeling using multiple stereo cameras |
US7856125B2 (en) * | 2006-01-31 | 2010-12-21 | University Of Southern California | 3D face reconstruction from 2D images |
KR100891549B1 (en) * | 2007-05-22 | 2009-04-03 | 광주과학기술원 | Method and apparatus for generating depth information supplemented using depth-range camera, and recording medium storing program for performing the method thereof |
WO2009008864A1 (en) * | 2007-07-12 | 2009-01-15 | Thomson Licensing | System and method for three-dimensional object reconstruction from two-dimensional images |
US8531472B2 (en) | 2007-12-03 | 2013-09-10 | Pictometry International Corp. | Systems and methods for rapid three-dimensional modeling with real façade texture |
US8275194B2 (en) * | 2008-02-15 | 2012-09-25 | Microsoft Corporation | Site modeling using image data fusion |
US8350850B2 (en) | 2008-03-31 | 2013-01-08 | Microsoft Corporation | Using photo collections for three dimensional modeling |
JP5243612B2 (en) * | 2008-10-02 | 2013-07-24 | フラウンホッファー−ゲゼルシャフト ツァ フェルダールング デァ アンゲヴァンテン フォアシュンク エー．ファオ | Intermediate image synthesis and multi-view data signal extraction |
US9330494B2 (en) | 2009-10-26 | 2016-05-03 | Pictometry International Corp. | Method for the automatic material classification and texture simulation for 3D models |
US8619122B2 (en) * | 2010-02-02 | 2013-12-31 | Microsoft Corporation | Depth camera compatibility |
US8773424B2 (en) | 2010-02-04 | 2014-07-08 | Microsoft Corporation | User interfaces for interacting with top-down maps of reconstructed 3-D scences |
US8885890B2 (en) * | 2010-05-07 | 2014-11-11 | Microsoft Corporation | Depth map confidence filtering |
US20120056982A1 (en) * | 2010-09-08 | 2012-03-08 | Microsoft Corporation | Depth camera based on structured light and stereo vision |
-
2012
- 2012-05-01 US US13/461,403 patent/US8462155B1/en not_active Expired - Fee Related
-
2013
- 2013-04-30 WO PCT/US2013/038881 patent/WO2013166023A1/en active Application Filing
- 2013-04-30 EP EP13747946.5A patent/EP2705500B1/en active Active
- 2013-04-30 CN CN201380001180.9A patent/CN103503033B/en active Active
- 2013-04-30 DE DE202013012443.1U patent/DE202013012443U1/en not_active Expired - Lifetime
- 2013-04-30 JP JP2014513809A patent/JP5592039B2/en active Active
- 2013-04-30 KR KR1020137024779A patent/KR101399236B1/en active IP Right Grant
Also Published As
Publication number | Publication date |
---|---|
US8462155B1 (en) | 2013-06-11 |
EP2705500A1 (en) | 2014-03-12 |
CN103503033A (en) | 2014-01-08 |
WO2013166023A1 (en) | 2013-11-07 |
EP2705500A4 (en) | 2015-09-23 |
EP2705500B1 (en) | 2017-06-07 |
DE202013012443U1 (en) | 2016-11-08 |
KR20130131444A (en) | 2013-12-03 |
CN103503033B (en) | 2015-07-08 |
JP2014514682A (en) | 2014-06-19 |
KR101399236B1 (en) | 2014-05-27 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US11688138B2 (en) | Methods and systems for detecting and combining structural features in 3D reconstruction | |
JP5592039B2 (en) | Merge 3D models based on confidence scores | |
US9350969B2 (en) | Target region filling involving source regions, depth information, or occlusions | |
US8463024B1 (en) | Combining narrow-baseline and wide-baseline stereo for three-dimensional modeling | |
US10249052B2 (en) | Stereo correspondence model fitting | |
KR20170068462A (en) | 3-Dimensional Model Generation Using Edges | |
WO2020039166A1 (en) | Method and system for reconstructing colour and depth information of a scene | |
US20140313187A1 (en) | Stereoscopic Target Region Filling | |
US20210241435A1 (en) | Point cloud fusion method, electronic device, and computer storage medium | |
WO2014159483A2 (en) | Translated view navigation for visualizations | |
US9691175B2 (en) | 3-D models as a navigable container for 2-D raster images | |
EP3304500B1 (en) | Smoothing 3d models of objects to mitigate artifacts | |
GB2553363B (en) | Method and system for recording spatial information | |
US10354444B2 (en) | Resolution adaptive mesh that is generated using an intermediate implicit representation of a point cloud | |
US9171393B2 (en) | Three-dimensional texture reprojection | |
US9224368B2 (en) | Merging three-dimensional models of varying resolution | |
CN113920275B (en) | Triangular mesh construction method and device, electronic equipment and readable storage medium | |
AU2013219167B1 (en) | Merging three-dimensional models based on confidence scores | |
TW201528774A (en) | Apparatus and method for creating 3D scene | |
US20180025479A1 (en) | Systems and methods for aligning measurement data to reference data | |
Yoo | Rapid three-dimensional urban model production using bilayered displacement mapping | |
Chia et al. | Novel view image synthesis based on photo-consistent 3D model deformation | |
JP2020017109A (en) | Break line extraction program, and break line extraction system |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20130909 |
|
A621 | Written request for application examination |
Free format text: JAPANESE INTERMEDIATE CODE: A621Effective date: 20130909 |
|
A871 | Explanation of circumstances concerning accelerated examination |
Free format text: JAPANESE INTERMEDIATE CODE: A871Effective date: 20130909 |
|
A975 | Report on accelerated examination |
Free format text: JAPANESE INTERMEDIATE CODE: A971005Effective date: 20140415 |
|
A131 | Notification of reasons for refusal |
Free format text: JAPANESE INTERMEDIATE CODE: A131Effective date: 20140508 |
|
A521 | Request for written amendment filed |
Free format text: JAPANESE INTERMEDIATE CODE: A523Effective date: 20140703 |
|
TRDD | Decision of grant or rejection written | ||
A01 | Written decision to grant a patent or to grant a registration (utility model) |
Free format text: JAPANESE INTERMEDIATE CODE: A01Effective date: 20140718 |
|
A61 | First payment of annual fees (during grant procedure) |
Free format text: JAPANESE INTERMEDIATE CODE: A61Effective date: 20140730 |
|
R150 | Certificate of patent or registration of utility model |
Ref document number: 5592039Country of ref document: JPFree format text: JAPANESE INTERMEDIATE CODE: R150 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
S533 | Written request for registration of change of name |
Free format text: JAPANESE INTERMEDIATE CODE: R313533 |
|
R350 | Written notification of registration of transfer |
Free format text: JAPANESE INTERMEDIATE CODE: R350 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |
|
R250 | Receipt of annual fees |
Free format text: JAPANESE INTERMEDIATE CODE: R250 |