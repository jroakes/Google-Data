CN117634495A - Suggested response based on message decal - Google Patents
Suggested response based on message decal Download PDFInfo
- Publication number
- CN117634495A CN117634495A CN202311382957.2A CN202311382957A CN117634495A CN 117634495 A CN117634495 A CN 117634495A CN 202311382957 A CN202311382957 A CN 202311382957A CN 117634495 A CN117634495 A CN 117634495A
- Authority
- CN
- China
- Prior art keywords
- message
- user
- decal
- response
- suggested
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 230000004044 response Effects 0.000 title claims abstract description 521
- 238000000034 method Methods 0.000 claims abstract description 125
- 238000010801 machine learning Methods 0.000 claims description 100
- 238000013519 translation Methods 0.000 claims description 5
- 230000008485 antagonism Effects 0.000 claims description 2
- 239000000758 substrate Substances 0.000 claims 1
- 238000004891 communication Methods 0.000 abstract description 60
- 238000012549 training Methods 0.000 description 56
- 230000015654 memory Effects 0.000 description 52
- 238000012545 processing Methods 0.000 description 25
- 230000000694 effects Effects 0.000 description 21
- 230000003993 interaction Effects 0.000 description 21
- 238000013507 mapping Methods 0.000 description 17
- 230000009471 action Effects 0.000 description 15
- 230000006870 function Effects 0.000 description 12
- 230000008569 process Effects 0.000 description 12
- 238000004458 analytical method Methods 0.000 description 10
- 238000010586 diagram Methods 0.000 description 9
- 230000009118 appropriate response Effects 0.000 description 8
- 230000008901 benefit Effects 0.000 description 8
- 238000013528 artificial neural network Methods 0.000 description 7
- 230000000007 visual effect Effects 0.000 description 7
- 238000013475 authorization Methods 0.000 description 5
- 238000004590 computer program Methods 0.000 description 5
- 230000008859 change Effects 0.000 description 4
- 239000002131 composite material Substances 0.000 description 4
- 230000007246 mechanism Effects 0.000 description 4
- 230000006855 networking Effects 0.000 description 4
- 230000005540 biological transmission Effects 0.000 description 3
- 239000003795 chemical substances by application Substances 0.000 description 3
- 238000005516 engineering process Methods 0.000 description 3
- 235000013305 food Nutrition 0.000 description 3
- 230000003287 optical effect Effects 0.000 description 3
- 238000012360 testing method Methods 0.000 description 3
- 230000004913 activation Effects 0.000 description 2
- 238000002059 diagnostic imaging Methods 0.000 description 2
- 239000000835 fiber Substances 0.000 description 2
- 239000011521 glass Substances 0.000 description 2
- 238000010191 image analysis Methods 0.000 description 2
- 238000003384 imaging method Methods 0.000 description 2
- 230000002452 interceptive effect Effects 0.000 description 2
- 230000033001 locomotion Effects 0.000 description 2
- 230000036651 mood Effects 0.000 description 2
- 230000008520 organization Effects 0.000 description 2
- 239000004065 semiconductor Substances 0.000 description 2
- 239000007787 solid Substances 0.000 description 2
- 238000012546 transfer Methods 0.000 description 2
- 244000248349 Citrus limon Species 0.000 description 1
- 235000005979 Citrus limon Nutrition 0.000 description 1
- 206010027940 Mood altered Diseases 0.000 description 1
- 206010047700 Vomiting Diseases 0.000 description 1
- 230000003213 activating effect Effects 0.000 description 1
- 238000013459 approach Methods 0.000 description 1
- 230000003190 augmentative effect Effects 0.000 description 1
- 239000012752 auxiliary agent Substances 0.000 description 1
- 235000013361 beverage Nutrition 0.000 description 1
- 230000010267 cellular communication Effects 0.000 description 1
- 230000001413 cellular effect Effects 0.000 description 1
- 238000004883 computer application Methods 0.000 description 1
- 238000013527 convolutional neural network Methods 0.000 description 1
- 238000013136 deep learning model Methods 0.000 description 1
- 238000001514 detection method Methods 0.000 description 1
- 230000008451 emotion Effects 0.000 description 1
- 230000002996 emotional effect Effects 0.000 description 1
- 239000000284 extract Substances 0.000 description 1
- 238000010348 incorporation Methods 0.000 description 1
- 239000011159 matrix material Substances 0.000 description 1
- 235000012054 meals Nutrition 0.000 description 1
- 239000000203 mixture Substances 0.000 description 1
- 230000007510 mood change Effects 0.000 description 1
- 238000003058 natural language processing Methods 0.000 description 1
- 230000001537 neural effect Effects 0.000 description 1
- 230000007935 neutral effect Effects 0.000 description 1
- 238000005192 partition Methods 0.000 description 1
- 230000009467 reduction Effects 0.000 description 1
- 230000006403 short-term memory Effects 0.000 description 1
- 238000012706 support-vector machine Methods 0.000 description 1
- 230000002459 sustained effect Effects 0.000 description 1
- 230000029305 taxis Effects 0.000 description 1
- 230000001960 triggered effect Effects 0.000 description 1
- 239000013598 vector Substances 0.000 description 1
- 238000012795 verification Methods 0.000 description 1
- 230000008673 vomiting Effects 0.000 description 1
- XLYOFNOQVPJJNP-UHFFFAOYSA-N water Substances O XLYOFNOQVPJJNP-UHFFFAOYSA-N 0.000 description 1
Classifications
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L51/00—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail
- H04L51/04—Real-time or near real-time messaging, e.g. instant messaging [IM]
- H04L51/046—Interoperability with other network applications or services
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F3/00—Input arrangements for transferring data to be processed into a form capable of being handled by the computer; Output arrangements for transferring data from processing unit to output unit, e.g. interface arrangements
- G06F3/01—Input arrangements or combined input and output arrangements for interaction between user and computer
- G06F3/048—Interaction techniques based on graphical user interfaces [GUI]
- G06F3/0484—Interaction techniques based on graphical user interfaces [GUI] for the control of specific functions or operations, e.g. selecting or manipulating an object, an image or a displayed text element, setting a parameter value or selecting a range
- G06F3/04842—Selection of displayed objects or displayed text elements
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/20—Natural language analysis
- G06F40/274—Converting codes to words; Guess-ahead of partial word inputs
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/40—Processing or translation of natural language
- G06F40/55—Rule-based translation
- G06F40/56—Natural language generation
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L51/00—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail
- H04L51/02—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail using automatic reactions or user delegation, e.g. automatic replies or chatbot-generated messages
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04L—TRANSMISSION OF DIGITAL INFORMATION, e.g. TELEGRAPHIC COMMUNICATION
- H04L51/00—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail
- H04L51/07—User-to-user messaging in packet-switching networks, transmitted according to store-and-forward or real-time protocols, e.g. e-mail characterised by the inclusion of specific contents
- H04L51/08—Annexed information, e.g. attachments
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04W—WIRELESS COMMUNICATION NETWORKS
- H04W4/00—Services specially adapted for wireless communication networks; Facilities therefor
- H04W4/12—Messaging; Mailboxes; Announcements
- H04W4/14—Short messaging services, e.g. short message services [SMS] or unstructured supplementary service data [USSD]
-
- G—PHYSICS
- G06—COMPUTING; CALCULATING OR COUNTING
- G06F—ELECTRIC DIGITAL DATA PROCESSING
- G06F40/00—Handling natural language data
- G06F40/30—Semantic analysis
Abstract
Embodiments relate to an auto-suggest response based on a message decal provided in a messaging application. In some implementations, a computer-implemented method for providing message suggestions in a messaging application includes detecting a first message sent by a first user device over a communication network to a second user device, programmatically analyzing the first message to determine semantic concepts associated with the first message, identifying one or more message stickers based at least in part on the semantic concepts, and sending instructions to cause the one or more message stickers to be displayed in a user interface that is displayed on the second user device.
Description
This application is a divisional application of the following application,
international application number of the original application: PCT/US2017/052349
International date of original application: 2017, 09, 19
National application number of the original application: 201780058037.1
Title of the original application: suggested response based on message decal
Cross Reference to Related Applications
The present application claims priority to U.S. provisional patent application No.62/397,316, entitled "message decal based advice response," filed on even date 9 and 20, herein incorporated by reference in its entirety.
Background
The popularity and convenience of digital devices and the widespread use of internet communications have made communication between user devices ubiquitous. Users may use their user devices to send various forms of media to each other, including text, emoticons, images, video, and animations, that are displayed or otherwise output on the devices. For example, a user may enter text and select an image or other media form that is sent over a communication network to another user's device.
The background description provided herein is for the purpose of generally presenting the context of the disclosure. Work of the presently named inventors, to the extent it is described in this background section, as well as aspects of the description that may not otherwise qualify as prior art at the time of filing, are neither expressly nor impliedly admitted as prior art against the present disclosure.
Disclosure of Invention
Embodiments of the present application relate to an auto-suggest response based on message stickers (message stickers) provided in a message application. In some implementations, a computer-implemented method for providing message suggestions in a messaging application includes detecting a first message sent by a first user device over a communication network to a second user device, programmatically analyzing the first message to determine semantic concepts associated with the first message, identifying one or more message stickers based at least in part on the semantic concepts, and sending instructions to cause the one or more message stickers to be displayed in a user interface that is displayed on the second user device.
Various embodiments and examples of the method are described herein. For example, in some embodiments, the first message is part of a communication in the messaging application between a first user of the first user device and a second user of the second user device, the method further comprising: the communication is programmatically analyzed to determine one or more additional semantic concepts associated with the communication, wherein identifying the one or more message stickers is further based on the one or more additional semantic concepts. In some implementations, identifying the one or more message stickers includes determining one or more suggested responses based on the semantic concepts; comparing one or more descriptors associated with the plurality of message decals to the one or more suggested responses; and selecting the one or more message stickers from the plurality of message stickers based on the comparison.
In some examples, selecting the one or more message stickers from the plurality of message stickers based on the comparison comprises: checking correspondence between the one or more descriptors and the one or more suggested responses, wherein the correspondence includes alphabetical matches between words of the one or more descriptors and words of the one or more suggested responses, and/or semantic similarities between the one or more descriptors and the one or more suggested responses; and determining that the one or more message stickers have the correspondence between the one or more descriptors and the one or more suggested responses. In a further example, the one or more message stickers include a plurality of message stickers, and further comprising determining respective ranks of the plurality of message stickers based on one or more correspondences between descriptions of each of the plurality of message stickers and the one or more suggested responses, wherein sending instructions to cause the plurality of message stickers to be displayed comprises sending instructions to indicate the ranks of the plurality of message stickers.
In some implementations, selecting the one or more message stickers from the plurality of message stickers based on the comparison includes: determining a similarity score between the one or more descriptors and the one or more suggested responses, and selecting the one or more message stickers based on the similarity score of the one or more message stickers. In some embodiments, at least one of the one or more message stickers includes image data to be displayed and a sticker Identification (ID) that effectively identifies the at least one message sticker.
The method further includes, in some embodiments, receiving a selection through user input of a particular message decal of the one or more message decals, and in response to receiving the selection, providing the particular message decal to the first user device in the message application, wherein providing the particular message decal includes one or more of: transmitting a sticker ID of the message sticker to the first user device through the communication network, and transmitting image data of the message sticker to the first user device through the communication network. In some implementations, identifying the one or more message stickers includes determining that the first message is part of a conversation in the message application between the first user device and the second user device, the method further including identifying the one or more message stickers based at least in part on one or more semantic concepts in one or more messages previously received in the conversation.
In some embodiments, for providing a message suggestion in a message application, the method includes detecting a first message sent by a first user device to a second user device over a communication network, wherein the first message includes a message decal. The method includes programmatically analyzing the message decal to determine semantic concepts associated with the first message, determining one or more suggested responses based at least in part on the semantic concepts, and sending instructions that cause the one or more suggested responses to be displayed by the second user device.
Various embodiments and examples of the method are described herein. For example, in some implementations, the suggested response includes at least one suggested message decal response that includes a message decal. In some implementations, determining the one or more suggested responses further includes comparing one or more descriptors associated with a plurality of message stickers to the one or more suggested responses, and selecting the at least one suggested message sticker response from the plurality of message stickers based on the comparison. In some examples, the method further comprises determining that the message decal is stored on the second user device.
In some embodiments, the method further comprises receiving a selection of at least one suggestion response of the one or more suggestion responses based on the received user input to the second user device, and in response to receiving the selection, transmitting the at least one suggestion response to the first user device over the communication network. In some examples, the message decal is associated with image data and a decal Identification (ID).
In some embodiments, a system for providing message suggestions in a messaging application includes a memory and at least one processor for accessing the memory and for performing operations. The operations include receiving, at a second user device, a first message sent by a first user device over a communication network, obtaining a suggested response associated with the first message, wherein the suggested response is based on semantic concepts determined by programmatically analyzing the first message. The operations include identifying one or more message stickers based at least in part on the suggested response, wherein the one or more message stickers are stored on the second user device, and causing the one or more message stickers to be displayed in a user interface that is displayed on the second user device.
Various embodiments and examples of the system are described herein. For example, in some implementations, the operation of obtaining the suggested response includes receiving the suggested response from a server device, wherein the server device programmatically analyzes the first message to determine the semantic concepts, and determining the suggested response based on a mapping from the semantic concepts to a library of stored suggested responses. In some implementations, the operations further include obtaining a plurality of semantic concepts associated with a plurality of message stickers stored on the second user device and comparing the plurality of semantic concepts to the suggested response, wherein identifying the one or more message stickers based at least in part on the suggested response includes selecting the one or more message stickers from the plurality of semantic concepts based on the comparison.
In some system embodiments, further operations include receiving a selection of at least one suggestion response of the one or more suggestion responses based on the received user input to the second user device, and transmitting the at least one suggestion response to the first user device over the communication network in response to receiving the selection. In some examples, at least one message decal of the one or more message decals includes image data to be displayed and a decal Identification (ID) identifying the at least one message decal.
Drawings
FIG. 1 is a block diagram of an example system and network environment that may be used with one or more embodiments described herein;
FIG. 2 is a block diagram illustrating an example system implementing one or more features described herein, according to some embodiments;
FIG. 3 is a flow chart illustrating an example method for providing suggested responses to a received message decal, according to some embodiments;
FIG. 4 is a flow chart illustrating an example method of providing a message decal suggestion in response to a received message, according to some embodiments;
FIG. 5 is a flow diagram illustrating another example method of providing a message decal suggestion in response to a received message, according to some embodiments;
6-7 are graphical representations showing examples of user interfaces in which suggested responses are provided in response to receiving a message decal, according to some embodiments.
8-9 are graphical representations showing examples of user interfaces providing suggestion responses including location suggestions, according to some embodiments;
FIGS. 10-11 are graphical representations of examples of user interfaces displaying suggested responses to received images, according to some embodiments; and
FIG. 12 is a block diagram of an example device that may be used in one or more embodiments described herein.
Detailed Description
One or more embodiments described herein relate to an auto-suggest response based on a message decal provided in a message application. In some implementations, a device detects a message sent from a first device of a first user to a second device of a second user over a communication network. The message is programmatically analyzed by the device to determine semantic concepts associated with the message. One or more message stickers are identified based at least in part on the semantic concept and displayed in a user interface on the second device as a second user-selectable suggestion response. In response to selecting the suggested message decal, for example, by user input, the selected message decal is transmitted as a response to the message in the conversation, for example, to the first device, and any other devices in the conversation to which the message was sent. For example, in a message application executing on a first device, the sent message decal is displayed as a response message.
In various embodiments, a suggested response is determined for a received message based on semantic concepts determined to exist in the message. Descriptors, such as descriptions and keywords, associated with the plurality of message stickers are obtained, the descriptors are compared to the suggested responses, and based on the comparison, a suggested message sticker is selected from the plurality of message stickers. In various embodiments, a message decal is associated with image data to be displayed and with a decal Identification (ID) for identifying the message decal and/or a decal set ID for identifying a decal set or group within which the message decal is included.
In some implementations, the server device may store a message decal, may determine a message response to the received message, may determine a message decal corresponding to a suggested response to the received message, and may send the message decal to the second device for display as the suggested response. In some implementations, the suggested response of the received message is determined by the server device and sent to the second device, and the second device may determine a locally stored message decal corresponding to the suggested response of the received message and display such message decal as the suggested response.
In further embodiments, the received message includes one or more message stickers. For example, semantic concepts may be identified for a message decal by obtaining descriptors (e.g., descriptions and/or keywords) associated with the standardized message decal. One or more suggested responses are determined based on the semantic concepts of the received message decal, wherein the suggested responses may be text, images, and/or may include the message decal. The suggestion response may be displayed by the second user device for selection by the second user, and the selected suggestion response is sent to the first user device and/or other user devices in the conversation.
In some implementations, the described computer-implemented methods, systems, and/or computer-readable media are capable of receiving, at a device, a message from a second device operated by a user, for example, over a communication network, and automatically generating a suggestion response, which the user may select to send in response to the received message. The received message may include one or more message stickers and/or the generated suggested response may include one or more message stickers that may be sent by the second device.
In some implementations, at many times during a conversation between user devices, the user may not be able to provide enough attention or focus to respond to received messages with relevant responses, and/or may not be able to provide detailed user input to the devices to create such relevant responses. In some examples, the user may be in progress or in an activity or environment where the user cannot or hardly provide an appropriate response. For example, a user may not have an opportunity to browse through a list of available stickers and select a sticker to send to other users in the conversation.
One or more features described herein facilitate providing an automated message suggestion to a user in response to a message, the message suggestion including a message decal suggestion and a message suggestion for a received message decal. For example, based on received messages in a message conversation between user devices, one or more suggested message responses are automatically suggested, and the user may simply select a response from the suggestions for publication in the conversation. Such suggestions allow the user to respond simply and quickly to received messages, reducing user input and reducing the time to compose a response on the device, thereby reducing consumption of device resources that would otherwise be used to enable and process added input from the user to compose a response and/or participate in a conversation held by the electronic device.
One or more features described herein are useful in providing automated message suggestions based on a message decal in response to obtaining a message on a device. One or more features described herein allow a user to easily and quickly use a message decal in conversational and other device communications, regardless of the language used in the message, device preferences, and the like. Sending a message decal in an electronic message allows a user to express themselves to other users in chat and other communications provided through the user device. For example, if a user receives a message decal received in a message conversation between user devices, one or more automated message responses (e.g., text responses, message decal responses, or other types of content responses) are suggested based on the message decal, and the user may simply select a desired response from the suggestions. In another example, if a user receives a message (e.g., a text message or other type of content message) in a device conversation, one or more automatic message decal responses are suggested based on the message, and the user may simply select a desired message decal response from the suggestion to send to other user devices.
Furthermore, the described message response suggestions are related to the received message. For example, using predefined associations of semantic concepts and suggestion responses, mapping models, machine learning models, etc., enable relevant message suggestions to be determined for concepts detected in a message. The use of descriptions and keywords associated with standardized message stickers allows related message stickers to be determined and presented as suggested responses to messages, and allows related response suggestions to be determined for message stickers received in a message. Such related suggestions allow a user to simply and quickly respond to received messages, reduce user input and reduce the time to compose a response on the device, thereby reducing consumption of device resources that would otherwise be used to enable and process added input from the user to compose a response instead of selecting less related suggestions, or reduce the resources that would be required to display a large number of possible responses (e.g., including less related suggestions). Further, these features can reduce consumption of device resources that would otherwise be used to enable and process added input from a user to compose responses, search, edit or complete suggested responses, and/or participate in conversations maintained by the electronic device.
A technical effect of one or more of the described embodiments is thus that the creation and transmission of responses in a device-implemented dialog is achieved with less computation time and resources for obtaining results. For example, a technical effect of the described features is a reduction in consumption of system processing resources to create and send message responses as compared to a system that does not provide one or more of the described features as described above.
Certain embodiments discussed herein may provide a user with one or more machine controls on whether to collect personal information, store personal information, use personal information, and how to collect information about the user, where personal information about the user (e.g., user data, information about the user's social network, the user's location and time at the location, user's biometric information, user's activities and demographic information) may be collected or used. That is, the systems and methods discussed herein collect, store, and/or use user personal information upon receipt of explicit authorization of the relevant user. For example, a user is provided with control over whether a program or feature gathers user information about that particular user or other users associated with the program or feature. Each user of the collected personal information is presented with one or more options that allow control of the collection of information associated with that user to provide permissions or authorizations as to whether to collect the information and as to which portions of the information are collected. For example, one or more such control options may be provided to the user over a communications network. In addition, certain data may be processed in one or more ways prior to storage or use to remove personally identifiable information. As one example, the identity of the user may be processed so that personally identifiable information cannot be determined. As another example, the geographic location of a user may be generalized over a larger area such that the user's particular location cannot be determined.
"decals" or "message decals" provide users of a messaging application with an intuitive visual mechanism of interaction with other users. The decal includes visual content displayed by the device, such as image pixel content, animation, video, and may include other types of content, such as text, audio data, and the like. For example, the decal may be based on one or more characters, and the characters may be used to express various emotions. The decals may also be based on themes, such as movie themes (e.g., "sky and star" movie decals), cartoon characters, genres (e.g., food, beverages, dance, etc.), messages ("call me"), and so forth. In some embodiments, the decal may be larger than the emoticon. In some embodiments, the decal may include motion and/or audio in addition to or in lieu of the still image. In some implementations, when the decal is received in the messaging application, the decal may be displayed in a larger size (e.g., full screen, or a majority of the user interface of the messaging application), and may be folded into a smaller size, for example, after a particular time has elapsed since the decal was received. In some implementations, the decal can be displayed as an overlay over a conversation in a messaging application, for example. The decal may enable a user to reduce the time spent entering text and may make communication through a messaging application easier and/or faster.
In some implementations, the message decal includes image data (e.g., pixel data) that indicates a visual display appearance of the message decal. The message decal may also be associated with decal information such as metadata. For example, the message decals may be included in a group of related message decals, such as a "decal set". The message decal may be associated with a decal Identification (ID), such as a decal Identification (ID), that identifies the decal, and metadata for a decal set ID that identifies the decal set to which the decal belongs. The message decal may be associated with metadata including one or more thumbnail versions of the decal, e.g., a lower resolution or smaller version of the image data. The message decal may be associated with metadata that includes descriptors, such as a decal-related description (e.g., a textual description) and/or one or more keywords related to the decal, as described in examples below. In some implementations, the message decals and decal sets are defined, created, and/or provided by one or more providers, who may maintain the decals in a standardized format, e.g., maintaining decal image data and metadata. In some implementations, the user of the user device cannot directly modify the message decal data, and the user of the user device may obtain the modified message decal data from the provider.
An image as referred to herein is a digital image having pixels with one or more pixel values (e.g., color values, luminance values, etc.). The image may be a still image or a single image, or may be an image comprised in a series of images, for example, a frame in a video sequence of video frames, or an image in a different type of image sequence or image animation. For example, the embodiments described herein may be used with a single image, a video sequence of images, or animated images (e.g., movie drawings, animated graphic interchange format images (GIFs), or other animations).
Fig. 1 illustrates a block diagram of an example environment 100 for providing a message service that enables and, in some embodiments, provides an automatic assistance agent, such as a robot. The exemplary environment 100 includes a message server 101, one or more client devices 115a,115n, a server 135, and a network 140. The users 125a-125n can be associated with respective client devices 115a,115 n. The server 135 may be a third party server, for example, a server controlled by another party different from the party providing the message service. In various embodiments, server 135 may implement robotic services as described in further detail below. In some implementations, environment 100 may not include one or more servers or devices shown in fig. 1, or may include other servers or devices not shown in fig. 1. In fig. 1 and the remaining figures, letters following a reference number, such as "115a", represent references to elements having that particular reference number. Reference numbers in the text that have no subsequent letters, such as "115", represent an overall reference to an embodiment of the element with that reference number.
In the illustrated embodiment, message server 101, client device 115, and server 135 are communicatively coupled via network 140. In various embodiments, network 140 may be of a conventional type, wired or wireless, and may have many different configurations including a star configuration, a token ring configuration, or other configurations. Further, network 140 may include a Local Area Network (LAN), a Wide Area Network (WAN) (e.g., the Internet), and/or other interconnecting data paths through which multiple devices may communicate. In some implementations, the network 140 may be a peer-to-peer network. The network 140 may also be coupled to or include portions of a telecommunications network for transmitting data in a variety of different communication protocols. In some embodiments, network 140 includes a network for transmitting and receiving dataCommunication network->Or a cellular communication network, transmitting and receiving data includes via Short Message Service (SMS), multimedia Message Service (MMS), hypertext transfer protocol (HTTP), direct data connection, email, etc. Although fig. 1 shows one network 140 coupled to client devices 115, message server 101 and server 135, in practice one or more networks 140 may be coupled to these entities.
Message server 101 may include a processor, memory (memory) and network communication capabilities. In some implementations, the message server 101 is a hardware server. In some implementations, the message server 101 may be embedded in a virtualized environment, for example, the message server 101 may be a virtual machine executing on a hardware server that may include one or more other virtual machines. Message server 101 is communicatively coupled to network 140 via signal line 102. The signal line 102 may be a wired connection such as Ethernet, coaxial cable, fiber optic cable, etc., or a wireless connection such as Wi-Fi, bluetooth, or other wireless technology. In some implementations, message server 101 sends data to and receives data from one or more of client devices 115a-115n, server 135, and robot 113 via network 140. In some implementations, the message server 101 includes a message application 103a, the message application 103a providing client functionality to enable a user (e.g., any user 125) to exchange messages with other users and/or with the bot. The message application 103a may be a server application, a server module of a client-server application, or a distributed application (e.g., with corresponding client message application 103b on one or more client devices 115).
The message server 101 may also include a database 199, and the database 199 may store messages exchanged via the message server 101, data and/or configuration of one or more robots, and user data associated with one or more users 125, all stored under explicit permissions of the respective users. In some embodiments, message server 101 may include one or more auxiliary agents, such as robots 107a and 111. In other embodiments, the secondary agent may be implemented on the client devices 115a-n, rather than on the message server 101.
The messaging application 103a may be code and routines operable by a processor to enable exchange of messages between the user 125 and one or more of the robots 105,107a,107b,109a,109b,111, and 113. In some implementations, the messaging application 103a can be implemented using hardware including a Field Programmable Gate Array (FPGA) or an Application Specific Integrated Circuit (ASIC). In some implementations, the messaging application 103a can be implemented using a combination of hardware and software.
In various embodiments, database 199 may store messages exchanged between one or more client devices 115 when the respective users associated with client devices 115 provide consent for storing the messages. In some implementations, database 199 may store messages exchanged between one or more client devices 115 and one or more robots implemented on different devices, e.g., another client device, message server 101 and server 135, etc., when the respective users associated with client devices 115 provide consent for storing the messages. In embodiments where one or more users do not provide consent, messages received and sent by those users are not stored.
In some implementations, the message may be encrypted, for example, so that only the sender and recipient of the message may view the encrypted message. In some implementations, the message is stored. In some implementations, database 199 may further store data and/or configuration for one or more robots, e.g., robot 107a, robot 111, etc. In some implementations, when a user 125 provides consent for storing user data (such as social network data, contact information, images, etc.), database 199 may also store user data associated with each user 125 that provided such consent.
In some implementations, the messaging application 103a/103b can provide a user interface that enables the user 125 to create a new robot. In these embodiments, the messaging application 103a/103b may include functionality that enables a user-created bot to be included in a conversation between users of the messaging application 103a/103 b.
The client device 115 may be a computing device including memory and a hardware processor, such as a camera, laptop computer, tablet computer, mobile phone, wearable device, mobile email device, portable gaming device, portable music player, reader device, head mounted display, or other electronic device capable of wireless access to the network 140.
In the illustrated embodiment, client device 115a is coupled to network 140 via signal line 108 and client device 115n is coupled to network 140 via signal line 110. The signal lines 108 and 110 may be wired connections, such as Ethernet, or wireless connections, such as Wi-Fi, bluetooth, or other wireless technologies. Client devices 115a,115n (e.g., user devices) are accessed by users 125a,125n, respectively. The client devices 115a,115n in fig. 1 are used as examples. Although fig. 1 shows two client devices 115a and 115n, the present disclosure is applicable to a system architecture having one or more client devices 115.
In some implementations, the client device 115 may be a wearable device worn by the user 125. For example, the client device 115 may be included as part of a buckle (e.g., a bracelet), part of jewelry, or part of a pair of eyeglasses. In another example, the client device 115 is a smartwatch. In various implementations, the user 125 may view the message from the message application 103a/103b on a display of the device, may access the message via a speaker of the device or other output device, or the like. For example, the user 125 may view the message on a display of a smart watch or smart bracelet. In another example, the user 125 may access the message via headphones (not shown) coupled to the client device 115 or to portions of the client device 115, speakers of the client device 115, haptic feedback elements of the client device 115, and the like.
In some implementations, the message application 103b is stored on the client device 115 a. In some implementations, the message application 103b (e.g., a thin client application, a client module, etc.) can be a client application stored on the client device 115a with a corresponding message application 103a (e.g., a server application, a server module, etc.) stored on the message server 101. For example, the message application 103b may send a message created by the user 125a on the client device 115a to the message application 103a stored on the message server 101.
In some implementations, the message application 103a can be a stand-alone application stored on the message server 101. The user 125a may access the messaging application 103a via a web page using a browser or other software on the client device 115 a. In some implementations, the messaging application 103b implemented on the client device 115a can include the same or similar modules as those included in the messaging server 101. In some implementations, the messaging application 103b may be implemented as a stand-alone client application, for example, in a peer-to-peer configuration or other configuration in which one or more of the client devices 115 include functionality capable of exchanging messages with other client devices 115. In these embodiments, message server 101 may include limited or no message functions (e.g., client authentication, backup, etc.). In some implementations, message server 101 may implement one or more robots, such as robot 107a and robot 111.
A response suggestion generator (not shown) is a module for generating a relevant response based on a specific semantic concept input to the response suggestion generator. Some examples of using a response suggestion generator are described herein. In various embodiments, the response proposal generator is a standalone device or incorporated into one or more other devices of the environment 100, such as the message server 101 or other servers.
Server 135 may include a processor, memory, and network communication capabilities. In some implementations, the server 135 is a hardware server. Server 135 is communicatively coupled to network 140 via signal line 128. The signal line 128 may be a wired connection such as Ethernet, coaxial cable, fiber optic cable, etc., or a wireless connection such as Wi-Fi, bluetooth, or other wireless technology. In some implementations, the server 135 sends data to and receives data from one or more of the message server 101 and the client device 115 via the network 140. Although server 135 is shown as one server, various embodiments may include one or more servers 135. Server 135 may implement one or more robots as server applications or server modules, such as robot 109a and robot 113.
In various embodiments, server 135 may be part of the same entity that manages message server 101, e.g., a provider of a message service. In some implementations, the server 135 can be a third party server, e.g., a server controlled by an entity different from the entity providing the messaging application 103a/103 b.
In some implementations, one or more components of environment 100 provide or host a robot. The bot is an automated service implemented on one or more computers with which a user interacts through user input, e.g., text, such as through messaging applications 103a/103b or other applications, etc. The robot is described in more detail below.
In some implementations, the message application 103a/103b can also provide one or more suggestions, e.g., suggestion responses, to the user 125 via the user interface. In some implementations, the suggestion response is provided in response to user input, e.g., via a button or other user interface element. Suggesting a response may make the interaction faster, for example, by reducing or eliminating the need for the user to type in the response. For example, when the client device lacks text input functionality (e.g., a smart watch that does not include a keyboard or microphone), the suggested response may enable the user to quickly and easily respond to the message. For example, when a user selects a suggestion response (e.g., by selecting a corresponding user interface element on a touch screen), the suggestion response may also enable the user to quickly respond to the message. The suggested responses may be generated using a predictive model, e.g., a machine learning model, trained to generate the responses.
For example, the message application 103a/103b may implement machine learning, such as a deep learning model, which may enhance user interaction with the message application 103. In some implementations, machine learning may be implemented on one or more components of environment 100. The machine learning model may be trained using synthetic data, such as data automatically generated by a computer, without using user information. In some implementations, the machine learning model may be trained, for example, based on sample data for which permissions to train with user data have been obtained explicitly from the user. For example, the sample data may include a received message and a response sent to the received message. Based on the sample data, the machine learning model may predict a response to the received message, which may then be provided as a suggested response.
For example, by reducing the burden on the user to compose a response to a received message, user interaction is enhanced by providing a selection of a response tailored based on the received message and the user's context. For example, after the user agrees, the suggestion response may be customized based on previous activities of the user, e.g., earlier messages in the conversation, messages in different conversations, etc. For example, such activity may be used to determine an appropriate suggested response for the user, e.g., humor response based on the user's interaction style, formal response, etc. In another example, when a user specifies one or more preferred languages and/or regions, the message application 103a/103b may generate a suggestion response in the user's preferred language. In various examples, the suggested response may be a text response, an image, multimedia, or the like.
In some implementations, machine learning can be implemented on the message server 101, on the client device 115, or on both the message server 101 and the client device 115. In some implementations, a simple machine learning model (e.g., allowing model operations within memory, storage, and processing constraints of the client device) can be implemented on the client device 115, and a complex machine learning model can be implemented on the message server 101. If the user does not agree to use the machine learning techniques, then these techniques are not implemented. In some implementations, the user may selectively agree to machine learning implemented only on the client device 115. In these implementations, machine learning may be implemented on the client device 115 such that user information for updates to the machine learning model or use of the machine learning model is stored or used locally and is not shared to other devices such as the message server 101, the server 135, or other client devices 115. Some other examples of machine learning implementations are described below.
For users who agree to receive suggestions, e.g., based on machine learning techniques, the suggestions may be provided by the messaging application 103. For example, suggestions may include suggestions of content (e.g., movies, books, etc.), calendars (e.g., available time on a user's calendar), events/places (e.g., restaurants, concerts, etc.), and so forth. In some implementations, if a user participating in a conversation agrees to use the conversation data, the suggestion can include a suggestion response based on the conversation content of the incoming message.
For example, if the first user of two users who have agreed to advice based on conversational content, a message "what do you want to eat? How does italian? The "response may be suggested to the second user, for example" @ assistant, lunch, italian, 2-position meal. "in this example, the suggested response includes the robot (identified by the symbol @ and the robot handle assistant). If the second user selects this response, the assistant robot is added to the conversation and a message is sent to the robot. The response from the robot may then be displayed in a conversation and either of the two users may send further messages to the robot. In this example, the assistant robot is not provided access to the conversation content and a suggested response is generated by the message application 103.
In some implementations, the content of the suggested response may be customized based on whether the robot is already present in the conversation or can be incorporated into the conversation. For example, if it is determined that the travel robot may be incorporated into a messaging application, then the suggested response to the ticket cost question to France may be "let us ask the travel robot-! "
In various embodiments, for example, the suggestion of the suggestion response may include one or more of the following: text (e.g., "too-rod |"), emoticons (e.g., smiley face, drowsy face, etc.), images (e.g., photos from a photo library of the user), text generated based on the template with user data inserted into fields of the template (e.g., "her number is < telephone number >" if the user provides access to the user data, with the field "telephone number" being filled in based on the user data), links (e.g., uniform resource locators), messaging stickers, etc. In some implementations, the suggestion response may be formatted and/or stylized, e.g., using color, font, layout, etc. For example, a suggestion response that includes a movie recommendation may include descriptive text about the movie, images from the movie, and links to purchase tickets. In different embodiments, the suggestion response may be presented in different types of user interface elements, such as text boxes, information cards, and the like.
In various embodiments, users are given control of whether they receive suggestions (e.g., suggestion responses), what types of suggestions they receive, the frequency of the suggestions, and so forth. For example, the user may refuse to receive the suggestion entirely, or may select a particular type of suggestion, or may receive the suggestion only at certain times of the day. In another example, the user may choose to receive personalized suggestions. In this example, machine learning may be used to provide suggestions based on user preferences regarding the use of their data and the use of machine learning techniques.
FIG. 2 is a block diagram illustrating one example system in which one or more features described herein may be implemented. In some implementations, one or more of the modules shown in fig. 2 are components of the message application 103a/103b, and/or implemented in other components of the system. In some implementations, one or more of the modules shown in fig. 2 are components of a server system, e.g., one or more separate servers or components of a single server.
The first user 202 may operate the first user device 204. In various embodiments, the first user device 204 is, for example, the client device 115 of fig. 1, or a server device. The second user 206 may operate the second user device 208, and the second user device 208 may be the client device 115 or may be a server device.
The server interface 210 may be provided on a server device or a client device and may receive messages sent by the first user device 204 and/or the second user device 208 and may forward such messages to one or more recipient devices. For example, the server interface 210 may be a component of the message server 101 and the message applications 103a/103b, and may be provided to control message functions.
The decal suggestion module 212 may be used in some implementations. The decal suggestion module 212 may receive messages and message decal information (e.g., decal metadata), communicate with other modules to obtain additional information, and send suggestion information to the server interface 210 such that the suggestion information is obtained by user devices such as the first user device 204 and the second user device 208 via the server interface 210 (e.g., the first and second user devices retrieve the suggestion information and/or the server interface 210 sends the suggestion information to the first and second user devices). For example, the suggested information may include a suggested reply, which is a response to the message decal received by the user device, and/or a suggested message decal for replying to the message received by the user device.
In some implementations, the decal data module 214 can store decal data (e.g., metadata) related to a standardized message decal that has been published to a number of devices and is accessible by devices including the first user device 204 and the second user device 208. The decal data module 214 may be included in one or more modules of the environment 100. For example, the decal data may include descriptors (e.g., descriptions and keywords) for the available message decal set. In some examples, the message stickers may each be associated with an identification of the message sticker, e.g., a sticker Identification (ID) that indicates the identity of the individual message sticker. Some embodiments may organize message decals into decal sets, and each decal set may have an identification, such as a decal set ID. Further, some embodiments may provide a version number of the message decal (and/or decal set), where the version number may be compared to a stored reference version number that indicates the most recent version of the decal that has been released.
In some implementations, a particular message decal can be associated with a decal ID, one or more decal set IDs (indicating a decal set that includes the particular message decal), and a version number (indicating a version of the particular message decal, e.g., indicating a time or order of publication and provision to a user). In some embodiments, the decal ID and/or decal set ID may be referenced on one or more standard lists of message decals, which may be obtained from various providers, e.g., the decal ID, other parameters and related image data may be downloaded over the internet, with the respective decal ID referencing specific image data (and/or other data) that is displayed on the device using the decal ID for the associated decal ID.
The decal data module 214 may store descriptors, such as descriptions and keywords, for the associated individual decals and/or for the decal set. For example, the decal data module may store decal IDs and associated descriptions of those decal IDs. The description may be considered to provide one or more semantic concepts that are present in (or associated with) the associated decal. For example, the description may be text related to the associated decal. For example, the description may be text (e.g., one or more words or phrases) indicating the visual appearance of the message decal, an action associated with the message decal, including a category, emotional state or connotation, command, statement, comment or exclamation, colloquial or adage, symbol, or other concept associated with the message decal. In some examples, the description may include "cook", "traffic", "get up", "call me on the road", "boring", "coffee", "bye", "love", "cool lemon water", "vomiting", and the like. For example, in some implementations, the message decal description is output as audio (e.g., spoken by a recorded voice or speech synthesizer) to indicate receipt of the message decal.
One or more keywords may also be associated with the message decal. Similar to the description, keywords may be considered semantic concepts describing one or more aspects of the visual appearance of the message decal, e.g., the type or category of the message decal's subject matter, actions, moods, etc., related to the message decal's subject matter, and so forth. In some embodiments, for example, keywords are used to group, categorize, or otherwise organize message stickers. In some examples, the keywords are searched to locate an associated particular message decal that is related to a particular search term or query, or to a particular generated suggestion response or concept as described below.
In some implementations, the user device may receive the decal data directly from the decal data module 214, as shown by the connection between the second user device 208 and the decal data module 214 shown in fig. 2.
The response suggestion generator 216 may be a module provided in some system implementations to generate relevant responses based on specific semantic concepts input to the response suggestion generator 216. For example, suggestion generator 216 may access database 218 and determine suggestion responses using one or more models stored in the database and/or access data stored in the database. For example, database 218 may store a knowledge base or knowledge graph that may include taxonomies of different semantic concepts and provide hierarchical relationships and connections between the semantic concepts.
Database 218 may include stored, structured graphs indicating appropriate responses to particular concepts. Database 218 may also include defined grammars that may provide rules that indicate particular responses for particular semantic concepts. For example, text keywords and descriptions entered into response suggestion generator 216 may be used as labels to indicate concepts, and these labels may be entered into these graphs and grammars of database 218 by suggestion generator 216 to determine corresponding response suggestions for those keywords and descriptions. The generated response advice may be returned to the decal advice module 212.
In some implementations, the generated response suggestions are ranked, for example by the decal suggestion module 212, and a particular number of top ranked response suggestions are sent to one or more user devices 204 and/or 208 via the server interface 210 for display on the user device or output on the user device in contrast. In some implementations, the received suggested responses from response suggestion generator 216 are matched to particular decals (e.g., matched to descriptors of the decals, such as keywords and/or descriptions), where the matched decals are considered suggested decals and may be ranked, for example, based on the rank of their associated suggested responses and/or based on the strength of the correspondence of the matched decals to their associated suggested responses. A particular number of highest ranked message decal suggestions may be sent to one or more user devices 204 and/or 208 via server interface 210 for display on the user device or conversely for output on the user device. One or more of these response suggestions and/or decal suggestions may be selected by user input at the user device to send one or more messages to the other device, where the one or more messages include content indicated by the selected suggestion.
FIG. 3 is a flowchart illustrating an example method 300 for providing one or more response suggestions to a received message decal, according to some embodiments. In some implementations, the method 300 is implemented, for example, on a server system, such as the message server 101 shown in fig. 1. In some implementations, some or all of the method 300 is implemented on a system such as one or more client devices 115 shown in fig. 1 and/or on both a server system and one or more client systems. In the described examples, the implemented system includes one or more processors or processing circuits and one or more storage devices, such as a database or other accessible storage. In some implementations, different components of one or more servers and/or clients perform different blocks or other portions of method 300.
In block 302, it is checked whether user consent (e.g., user permissions) has been obtained to use the user data in the implementation of method 300. For example, the user data may include messages sent or received by the user, e.g., using the messaging application 103, user preferences, user biometric information, user characteristics (identity, name, age, gender, occupation, etc.), information about the user's social network and contacts, social and other types of actions and activities, content created or submitted by the user, ratings, and opinion, the user's current location, historical user data, images generated, received, and/or accessed by the user, videos viewed or shared by the user, and so forth. In some implementations, one or more blocks of the methods described herein may use such user data.
If user consent has been obtained from the relevant user in method 300 where the user data is available, then at block 304, the blocks of determining the method herein may be implemented through the rational use of the user data as described for those blocks, and the method continues to block 308.
If user consent has not been obtained, then in block 306 it is determined that the block will be implemented without using user data, and the method continues to block 308. In some implementations, if user consent has not been obtained, the blocks will not be implemented using user data, but rather using generic, synthesized, and/or publicly accessible data.
Some embodiments described herein, for example in connection with fig. 3, may provide a suggested message response based on obtaining a standardized message decal, such as a message decal having specific associated image data and decal IDs, because the message decal is from a specific official source. The suggestion response may be provided in various contexts. For example, in response to receiving a message decal at a server and/or from a client device 115a of a particular user (e.g., user 125 a) of any client device 115, such as by the message application 103, a suggested response may be provided. For example, the messaging application 103a/103b may be an instant messaging application, a social networking application, an email application, a multimedia messaging application, and the like.
If consent has been obtained from the particular user and one or more other users that sent and/or received the image, a suggested response may be automatically generated and automatically provided to the particular user without user input or intervention. For example, if the message application is an instant message application, the message decal may be received as part of an instant message communication between a particular user 125a of a user (e.g., client) device and one or more other users 125 of the user device, e.g., in a message session (e.g., chat) with two participants, in a group message session including more than two participants, and so forth.
In various embodiments, one or more users may be robots. In some implementations, the robot may be an automated agent implemented in software and/or hardware. In some implementations, the robot can represent, be associated with, or send one or more images through the messaging application 103, a camera (e.g., security camera, webcam, surveillance camera, etc.), an appliance (e.g., smart refrigerator, alarm device, industrial appliance, etc.), an imaging device (e.g., microscope, medical imaging device, etc.), or a camera (e.g., security camera, webcam, surveillance camera, etc.), an appliance (e.g., smart refrigerator, alarm device, industrial appliance, etc.), an imaging device (e.g., microscope, medical imaging device, etc.). In embodiments that include one or more users being robots, consent is obtained from the owner or operator of the robot to use the robot-generated message. In some implementations, the consent can be designated as a configuration parameter for the robot, camera, appliance, etc., and provided to the messaging application 103a/103b when the robot interacts with the messaging application 103.
In some implementations, the suggested responses may be provided exclusively to those users who have agreed to access the user data. In some implementations, the suggestion response may be provided automatically or based on user input, e.g., a user interface may be provided to the user for requesting suggestions.
In block 308, a message decal is received, for example, that has been sent from a first user of a first device to a second user of a second device over a communication network. In various embodiments, the message decal may be an image, such as a still image (e.g., a photograph, emoticon, or other image), a moving picture or animated image (e.g., an image including motion, a decal including animation and audio, etc.), video, audio data of an audio message, and so forth. In some implementations, the received message decal is included in the message that also includes one or more other types of content (e.g., text, emoticons, video, audio, etc.). The message decal may be a standardized message image, such as an image that has been published from a particular source (e.g., web service, company, or other organization) and has a particular decal ID and/or decal set ID, as described above.
In some implementations, the received message decal includes image data (e.g., pixel data) indicating a visual display appearance of the message decal, and metadata associated with the message decal, such as a decal ID, a decal set ID, or the like. In some implementations, the received message decal includes some or all of the associated metadata that is devoid of image data and/or some metadata, e.g., where the image data and the non-received associated metadata may be obtained from a storage device. In some implementations, the received message decal includes image data that is devoid of some or all associated metadata, e.g., where the metadata associated with the image data may be obtained from a storage device.
In some examples, the message decal is sent by a first device, which is one or more client devices 115 (or other devices), or by a robot that sends the message, and the message decal is received by a second device, which is a client device 115 (or other device), via network 140. In some examples, a message decal is sent in communication between a first device and a second device. In another example, the communication is a one-to-one (1:1) message session of two participants, and the message decal is received by a client device of a particular user. In another example, the communication is a group message session and the message decal is received by two or more client devices of participants in the group message session.
In some implementations, the message decal is obtained from memory, e.g., local memory (e.g., memory, storage drive, etc.) of the client device 115 and/or remote storage (e.g., remote client device, server device, or storage drive of a storage device) accessible to the client device over the network 140. For example, in some implementations, a first device stores a message decal on a storage device, and a second device obtains the message decal from the storage device.
In an example involving the system 200 of fig. 2, a message decal may be sent by the first user 202 using the first user device 204. In some implementations, the message decal is included in the message, which may include, for example, text or other data in addition to the message decal.
In some example embodiments, in block 308, the message decal sent by the first user device 204 is received by a server, e.g., the server interface 210 as shown in fig. 2. The server interface 210 then sends the received message decal to the intended recipient (e.g., to the second user device 208), and one or more server modules may also process copies of the received message decal as in one or more blocks of the method 300. In some implementations, in block 308, the message decal is received by a client device, e.g., second user device 208, and the client device may process the received message decal as in one or more blocks of method 300. Some implementations may use both a server and a client device to process one or more blocks of method 300.
In block 310, decal ID information and decal version information are determined from the received decal message. For example, a decal ID identifying a particular message decal may be determined, as well as a decal set ID identifying the message decal set to which the message decal belongs. The decal version information may include a version number of the message decal as described above. In some implementations, for example, referring to fig. 2, the server interface 210 forwards a copy of the message decal to the decal suggestion module 212. In some examples of such implementations, the decal suggestion module 212 extracts a decal ID, a decal set ID, and version information from the message decal after detecting that the received message is or includes a message decal. In some embodiments, these three parameters uniquely define a message decal.
In some embodiments, the decal ID information, version information, and/or other information may be obtained from the received data in the message decal by the receiving second device.
In block 312, one or more descriptors of the message decal are determined. For example, the descriptor may include a description and one or more keywords associated with the message decal. For example, the descriptor may be embedded or included in the message decal, or may be stored in a separate memory and accessed based on a decal parameter, such as a decal ID, a decal set ID, a version ID, and/or other decal parameter of the message decal. In some implementations, for example, referring to fig. 2, the descriptors of the message decals are stored in the decal data module 214. For example, the decal suggestion module 212 may access the decal data module 214 using the decal parameters of the message decal as search criteria, and find and retrieve descriptors associated with the message decal from the decal data module 214. In some implementations, other parameters of the message decal can be used to uniquely identify a descriptor in the decal data module 214, such as a location area of the message decal, which can be determined, for example, from the message decal and/or other data sources.
In block 314, one or more suggested responses are determined or generated based on the descriptors of the message decal. The suggested response may be a text response or, in some implementations, may be (or include) other forms of media content (e.g., audio, images, video, etc.). In some implementations, suggested responses are generated based on semantic concepts, e.g., by using stored associations between concepts and predefined responses, by using a model that provides responses based on concepts and relationships of concepts to responses as defined in one or more graphs, by using a machine learning model, and so forth. In some implementations, one or more suggested responses are determined to be a message decal, similar to that described below with respect to fig. 4 and 5. For example, the suggested response may be generated based on a mapping of semantic concepts to a particular suggested response. For example, the suggested response may be obtained from a collection or library of stored suggested responses, e.g., the stored suggested response is a composite response, or if user consent has been obtained, the stored suggested response is a response obtained from an actual user in a previous device communication (e.g., using a messaging application).
Descriptors (e.g., descriptions and keywords) of the received message decals may be used as semantic concepts and/or concepts may be obtained from the descriptors to determine such responses. For example, words in the description may be provided as separate concepts, and each keyword may be considered a separate concept for determining suggested responses using defined relationships. In some examples, a list of predefined messages associated with the predefined responses may be searched for a matching string that includes words describing and/or keywords, and a message that best matches the string (e.g., with the largest number of matching letters, with the highest similarity score during the matching process, etc.) is selected to determine the associated suggested response.
In some implementations of block 314, for example, referring to fig. 2, the decal suggestion module 212 may send the descriptor to the response suggestion generator 216. Suggestion generator 216 may consult database 218 to determine a suggestion response (e.g., a text response) based on the descriptors and send the determined response to decal suggestion module 212, which database 218 includes knowledge graphs, grammars, and/or other defined relationships. In some examples, suggestion generator 216 may determine suggested responses from concepts using predefined associations between concepts and suggested responses, one or more mapping models, and/or one or more machine learning models, similar to that described below with respect to block 416 of fig. 4. For example, suggestion generator 216 may determine the response using one or more mapping models and/or machine learning models based on descriptors used as concepts.
In some examples, the message decal may depict an image requesting or indicating that the receiving user should call the person who is sending the message decal. The message decal may have a description of "call me". The description may be input to suggestion generator 216 as a tag, and suggestion generator 216 may determine one or more suggestion responses based on the accessed relationships, knowledge graphs, databases, models, and other data. For example, suggestion generator 216 may output a response to the decal suggestion module that includes "i would like" and "later," the response being a response defined in a database associated with or based on the input label, and/or a response obtained from a mapping model or machine learning model similar to that described below.
In further examples, the message decal of the smiley image may have a description of "happy" and two keywords of "smiley" and "face". These descriptors (tags) may be input to suggestion generator 216 as separate semantic concepts, and suggestion generator 216 may determine one or more suggestion responses based on these separate semantic concepts and/or based on a combination of two or more of these semantic concepts. For example, suggestion generator 216 may output responses to the decal suggestion module that include "thank you", "same as you" and "beautiful smiles," which are responses associated with or defined in the database based on a combination of input concepts, and/or responses that are similar to those output by the mapping model or the machine learning model as described below.
In some implementations, one or more of the generated suggested responses are or include one or more message stickers. For example, the suggested message decal response may be determined by decal suggestion module 212 or the second user device using a descriptor associated with the available message decal and finding a correspondence of the message decal to the suggested response generated as described above, e.g., the suggested message decal response may be determined similarly as described below with reference to fig. 4 (e.g., the message tag response stored on the server) and fig. 5 (e.g., the message tag response stored on the second user device). In some implementations, the semantic concepts determined from the message are directly compared to stored semantic concepts associated with one or more particular message decal responses such that if the message semantic concepts correspond (e.g., match or are similar) to the stored semantic concepts, the associated message decal response may be selected for block 314, e.g., similar to that described below with respect to fig. 4. For example, in some implementations, the stored semantic concepts may include descriptors of available message stickers.
In some implementations, the determined suggestion response may include a suggestion response in response to one or more previous messages received by the second user device. For example, one or more previous messages (e.g., including a message decal) have been received in the same message conversation in which the messages of block 308 were received, e.g., a conversation in which messages have been exchanged between the first user device and the second user device. In response to the second user device receiving those previous messages, one or more suggested responses may have been previously generated for the second user device, and the one or more suggested responses may have been stored in the accessible memory. One or more such previously generated suggested responses may be retrieved and included in the suggested responses determined for the currently received message. In some implementations, one or more suggested responses are generated for one or more previously received message stickers in block 314 and included in the set of suggested responses determined for the currently received message. Similarly, descriptors of previously received message stickers may have been stored (e.g., received in the same conversation), and these semantic concepts may be used to determine message suggestions in block 416.
At block 316, at least one of the suggested responses is provided to a second user who is the recipient of the message decal. In some embodiments, these provided suggestion responses are sent to the second user device. For example, referring to fig. 2, a suggestion response may be sent from the decal suggestion module 212 to the second user device 208 via the server interface 210. In some examples, the instructions are sent to the second user device 208 with the suggestion response, causing the second user device to display the suggestion response. Some embodiments or situations may have multiple recipients of a message decal and suggested response.
The provided suggested response may be displayed by the second user device and/or otherwise output (e.g., audio output) by the second user device. In some implementations, the suggested responses determined in block 314 are ranked and a particular number of top ranked suggested responses are provided to the second user. For example, in some implementations, the suggested responses are ordered based on how closely the descriptors match the data entity (e.g., a predefined message) used to determine the suggested responses.
In block 318, a selection of one or more displayed suggested responses is received. For example, the selection may be based on user input provided by a user at the user device, e.g., through an input device such as a touch screen or touchpad that receives user touches, a microphone that receives voice commands, buttons, a mouse, or a joystick that receives user manipulations, etc. In some implementations, for example, referring to fig. 2, the second user 206 provides user input at the second user device 208. In some implementations, one or more suggestion responses are automatically selected (without user input), e.g., based on user preferences, user context (e.g., geographic location, time or date, etc., if user consent has been obtained), or other predetermined conditions.
In block 320, the selected suggestion response is output as one or more messages to one or more recipient devices. For example, a message may be sent via the message server 101 to one or more other client devices 115 and/or directly to another client device 115 over the network 140. In some embodiments, the message is sent to the user device that sent the image obtained in block 308. For example, referring to fig. 2, the second user device 208 sends the selected response to the first user device 204. In some implementations, the message is sent to a plurality of user devices participating in the chat conversation or session, including the first user device 204. The method may then return to block 308 in some embodiments, for example, if another image has been obtained.
Fig. 4 is a flow diagram illustrating an example method 400 of providing one or more message decal suggestions in response to a received message, according to some embodiments. In some embodiments, the method 400 is implemented, for example, on a server system, such as the message server 101 shown in fig. 1. In some implementations, some or all of the method 400 is implemented on a system, such as one or more client devices 115 shown in fig. 1, and/or on both a server system and one or more client systems. In the described examples, the implementation system includes one or more processors or processing circuits, and one or more storage devices, such as a database or other accessible storage. In some implementations, different components of one or more servers and/or clients may perform different blocks or other portions of method 400.
In block 402, it is checked whether user consent (e.g., user permissions) has been obtained to use the user data in the implementation of method 400. For example, the user data may include messages sent or received by the user, e.g., using the messaging application 103, user preferences, user biometric information, user characteristics (identity, name, age, gender, occupation, etc.), information about the user's social network and contacts, social and other types of actions and activities, content created or submitted by the user, ratings, and opinion, the user's current location, historical user data, images generated, received, and/or accessed by the user, videos viewed or shared by the user, and so forth. In some implementations, one or more blocks of the methods described herein may use such user data.
If user consent has been obtained from the relevant user that may use the user data in method 400, then at block 404, the blocks of determining the method herein may be implemented through the rational use of the user data as described for those blocks, and the method continues to block 408.
If user consent has not been obtained, then in block 406 it is determined that the block will be implemented without using user data, and the method continues to block 408. In some implementations, if user consent has not been obtained, the blocks are not implemented using user data, but rather are implemented using generic, synthesized, and/or publicly accessible data.
Some embodiments described herein, for example, in connection with fig. 4, may provide a suggested standardized message decal to be used as a response to receiving a message. Suggested message stickers may be provided in a variety of contexts. For example, in response to receiving a message at a server and/or from a client device 115a of a particular user (e.g., user 125 a) of any client device 115, such as by the message application 103, a suggested message decal may be provided. For example, the messaging application 103a/103b may be an instant messaging application, a social networking application, an email application, a multimedia messaging application, and the like.
In the event that one or more other users who send and/or receive images agree with a particular user, a suggested message decal may be generated and automatically provided to the particular user. For example, if the message application is an instant message application, the message may be received as part of an instant message communication between a particular user 125a and one or more other users 125 of the user device, e.g., in a message session (e.g., chat) with two participants, in a group message session including more than two participants, and so on. In various embodiments, one or more users may be robots, similar to that described in fig. 3.
In some embodiments, the suggested message decals may be provided exclusively to those users who have provided consent to access the user data. In some implementations, the suggested message decals may be provided automatically or based on user input, e.g., a user interface may be provided to the user for requesting the suggestion.
At block 408, a message is received, for example, via a communication network, that has been sent from a first user to a second user. In various implementations, the message may be a text message, an image (e.g., a still image, a moving picture or animated image, etc.), video, audio data of an audio message, and so forth. In some implementations, for example, the message can include a message decal, similar to that described in fig. 3. In some implementations, a combination of two or more of these types of content can be included in the message.
In some examples, the message may have been sent by a first device, which is one or more client devices 115 (or other devices), or by a robot that sent the message, and the message may be received by a second device, which is a client device 115 (or other device), via network 140. In a further example, in a one-to-one (1:1) message session with two participants, a message is received by a client device of a particular user. In a further example, in a group message session, a message is received by two or more client devices of participants in the group message session. In some implementations, the message is obtained from a memory, e.g., local memory (e.g., memory, storage drive, etc.) of the client device 115 and/or remote memory (e.g., a remote client device, server device, or storage drive of a storage device) accessible to the client device over the network 140. For example, in some implementations, a first device stores a message on a storage device and a second device obtains the message from the storage device. In some cases or embodiments, the message is included in a communication that also includes one or more other messages or data provided to the second user device, for example, from the first user device.
In an example involving the system 200 of fig. 2, a received message is sent by the first user 202 using the first user device 204. In some implementations, in block 408, the message sent by the first user device 204 is received by a client device, such as the second user device 208 shown in fig. 2, and/or by a server device, such as the server interface 210 shown in fig. 2. For example, similar to that described above for fig. 3, the server interface 210 may receive a message from a first user device and forward the message to a second user device 208, and provide a copy of the message to the decal suggestion module 212.
At block 410, user input from the second user requests display of one or more decal suggestions that the second user is able to select. For example, in some implementations, block 410 is implemented on the second user device 208, where user input is provided to the second user device 208 to command a display of a user interface (e.g., window or other display area) that displays a list of message decals. As described below, in response to a received message (or a newly designated receiving user), the user can select one or more message stickers from the list to send. In some implementations, the system commands automatically display some or all of the one or more decal suggestions in response to, for example, receiving the message in block 408 and/or based on stored user preferences, rather than in response to user input received in block 410.
At block 412, in response to the user input of block 410, a request for a decal suggestion is provided, wherein the request includes decal information (e.g., metadata) identifying or indicating message decal data currently stored on the second user device 208. For example, referring to fig. 2, a request may be sent from the second user device 208 to the server interface 210. In some implementations, the request can also include the received message, for example, if the received message was not previously provided to the decal suggestion module 212.
In some implementations, the decal information includes a list of decal IDs, decal set IDs, and version information for all message decals or specified message decal sets that are installed on the second user device 208 and available for display and transmission on the second user device 208. In some implementations, in response to the second user device installing an additional message decal or the like, a request for a decal suggestion is provided at other times or under other conditions that are not responsive to the user input of block 410, e.g., upon activation of the second user device.
At block 414, one or more descriptors, e.g., descriptions and/or keywords, of the message decals stored on the second user device are determined. For example, the descriptor may be embedded or included in the message decal, or may be stored in a separate store and accessed based on decal parameters, including a decal ID, a decal set ID, a version ID, and/or other decal parameters of the message decal. In some implementations, for example, referring to fig. 2, the descriptors of the message decals are stored in the decal data module 214. For example, the decal suggestion module 212 may access the decal data module 214 using the decal parameters of the message decal as search criteria and find and retrieve descriptors associated with the message decal of the second user device 208.
In some implementations, other parameters of the message decal can be used to uniquely identify the descriptor in the decal data module 214, similar to that described for fig. 3. In some implementations, the determination of the descriptor of the message decal on the second user device is made at the time of startup of the second user device or at other times or in response to a predefined condition (e.g., receiving a message in block 408, receiving user input in block 410, or other condition).
In block 416, one or more suggested responses are determined or generated based on the received message. In some implementations, the suggestion response is generated based on semantic concepts determined to be associated with the received message, such as semantic concepts determined to be included in text data, image data, or other data of the message. For example, the suggested response may be generated based on a mapping of semantic concepts to a particular suggested response. For example, the suggested response may be obtained from a collection or library of stored suggested responses, e.g., the stored suggested response is a composite response, or if user consent has been obtained, the stored suggested response is a response obtained from an actual user in a previous device communication (e.g., using a messaging application).
The message may be used to provide one or more concepts that are used to determine the suggested response. For example, words in a message may be parsed or extracted and provided as concepts for determining a response using defined relationships. In some implementations, the tags detected in the image content of the image or video included in the message are provided as concepts (e.g., tags added to the image by a user to describe features of the image content, and/or tags automatically detected using one or more image detection or image recognition techniques to determine features of the image content). In some implementations, semantic concepts may also be determined for one or more additional messages included in communication with the second user device, where these semantic concepts may also be used to determine the suggested response.
A build response may be generated based on the determined semantic concepts. For example, similar to that described above for FIG. 3, the stored associations between concepts and predefined responses may be used to generate suggested responses based on models of concepts providing responses, their relationships to responses defined in one or more graphs, and so forth. In one example, the received "happy birthday-! The message of "is associated with a proposed response of" thank you ". In some implementations of block 416, for example, referring to fig. 2, the decal suggestion module 212 may send the received message and/or descriptor to the response suggestion generator 216 to determine semantic concepts and suggested responses. In some examples, suggestion generator 216 may consult database 218 to determine a response (e.g., a text response) based on the message, database 218 including a mapping model, knowledge graph, grammar, and/or other defined relationships.
For example, a mapping model may be used to map concepts to message suggestions. In some examples, the mapping model may include a machine learning model. For example, the mapping model may include a graph-based machine learning model, which may include a model that is trained based on training data. Before receiving concepts for which message suggestions are to be generated, a graph-based model may be generated based on training data during a training phase. After receiving the concepts, the message suggestions may be generated using the model, for example, at block 416, at an inference stage after the model has been trained. In some examples, the training data may be message data that includes concepts and responses to those concepts. For example, the message data may be synthesized data, e.g., based on a simulated conversation in which no human user is a participant. In further examples, the training data may include images and messages exchanged between users agreeing to provide data for training purposes. The training data is processed prior to use to remove the user identifier and other information.
In a further example, the mapping model may include one or more grammars that provide message suggestions in response to particular concepts (e.g., each grammar is a set of particular rules that govern the composition of the message suggestions). The grammar may use hierarchical concept classifications of the stores accessible to suggestion generator 216. In some examples, several grammars may be predefined and stored in an accessible store to allow suggestion generator 216 to determine message suggestions based on message concepts that match particular related concepts stored in the taxonomies.
If a particular concept is associated with a grammar-specified concept, e.g., higher or lower in the hierarchy than the specified concept, then some grammar may be used for that particular concept. In an example, the stored grammar can specify that if concept c is a sub-concept of a specified concept "dog" in the hierarchy of referenced classifications, then the message suggestion provided by that grammar can be used for concept c. For example, the grammar may specify a suggested response that outputs "lovely [ c ]" where [ c ] is the concept c.
In some implementations, suggestion generator 216 uses other types of machine learning models to determine responses based on received messages and/or based on descriptors. For example, before receiving concepts for which message suggestions are to be generated, a machine learning model may be trained based on training data during a training phase. After receiving the concepts, the message suggestions may be generated using the machine learning model, for example, at block 416, at an inference stage after the machine learning model has been trained. In some examples, similar to that described above, if user consent to the usage data has been obtained, the training data may be message data that includes concepts and responses to those concepts. Further examples of machine learning models are described below with reference to fig. 12.
In some implementations, the suggestion response determined at block 416 includes a suggestion response in response to one or more previous messages received by the second user device. For example, one or more previous messages (e.g., including a message decal) may have been received in the same message conversation in which the messages of block 408 were received, e.g., a conversation in which messages have been exchanged between the first user device and the second user device. In response to the second user device receiving those previous messages, one or more suggested responses may have been previously generated for the second user device and the one or more suggested responses have been stored in the accessible store. One or more such previously generated suggested responses may be retrieved and included in the suggested responses determined for the currently received message.
In some implementations, one or more suggested responses are generated for one or more previously received messages in block 416 and included in the set of suggested responses determined for the currently received message. Similarly, in previously received messages, semantic concepts may have been previously detected (e.g., received in the same conversation) and stored, and these semantic concepts may be used to determine message suggestions in block 416.
In some implementations, the message suggestions are ordered relative to each other based on the strength of correspondence of the message suggestions to semantic concepts of the message. In some examples, the strength of the correspondence may be based on the proximity of matches between message concepts and stored concepts used to determine associated suggested responses, and/or the frequency of occurrence of message suggestions as responses in training data indicating previous responses to previous messages.
In block 418, one or more suggested message decals are determined and ordered based on a comparison and matching of the decal information (e.g., descriptors) to the suggested responses determined in block 416. In some implementations referring to fig. 2, response advice generator 216 and/or decal advice module 212 may determine and order the suggested message decals.
In some implementations, descriptors of the message decals on the second user device are compared to the generated suggested response, and if any of the descriptors corresponds to the suggested response, a message decal corresponding to the matching description or keyword is selected as the suggested message decal. In some implementations, matches are checked for correspondence, e.g., matches between letters of words in the same sequence, and/or matches of multiple words in the same sequence. In some implementations, exact and imprecise matches are used or allowed for correspondence, and similarities between descriptors and suggested responses may be checked (e.g., differences caused by past times and complex forms of words may be ignored, etc.). In some examples, a similarity score for the descriptor may be determined for the suggested response (e.g., for each descriptor of each suggested response), the similarity score indicating a degree of similarity between the descriptor and the suggested response. For example, the similarity score may be based on the number or percentage of letters of a particular order of matching.
In some embodiments, the system examines semantic similarity as a correspondence, e.g., using an accessible dictionary, synonym dictionary, knowledge graph, and/or other reference information, to determine if the meaning of the descriptor and the suggested response are the same. In some implementations, a decal similarity score for a message decal and a suggested response can be determined based on a separate similarity score determined for each descriptor of the message decal. For example, the decal similarity score may be a sum of individual similarity scores determined for the descriptors.
In some implementations, a message decal having the highest similarity to the suggested response is selected for the suggested response, e.g., based on the similarity score. In some examples, a message decal with the highest similarity score for the suggested response is selected for the response. In further examples, message stickers may be selected based on a degree of similarity, where a higher similarity may be determined based on, for example, more words in a descriptor of the message sticker that correspond to words in the suggested response (e.g., matching or similar words as described herein), and/or a highest score in a determination of semantic similarity between the descriptor and the suggested response. In some implementations, the plurality of message stickers is determined to correspond to a particular message suggestion, e.g., a particular number of message stickers having a highest similarity to the message suggestion.
In some implementations, semantic concepts determined in the received message (message concept) are compared to stored semantic concepts that have been directly associated with a particular suggested message decal response. These suggested decals have been stored in association with stored concepts as suggested responses to the stored concepts. For example, similar to that described herein, correspondence of message concepts to stored concepts may be checked (e.g., check matches, semantic similarity, partial matches, etc.), and if the message concepts match or correspond to the stored concepts, a suggested decal response associated with the stored concepts may be selected for block 418. For example, in some implementations, the stored semantic concepts may include descriptors of available message stickers.
The determined suggested message decals may be ordered relative to each other. For example, the placement or ranking of a message decal in a ranking may be based on the ranking of the message suggestion associated with the message decal relative to other message suggestions. In a further example, the ranking of the suggested message decals is based on the strength of correspondence between the suggested message decals and their associated suggested responses. For example, the placement or ranking of a message decal in a ranking may be based on the number of matches (correspondences) between the associated suggested response and the descriptor of the message decal.
In some examples, the number of matches may be the number of matches of words or phrases in the descriptor with words of the suggestion response. In some examples, if there are 5 matches between the words of the suggested response and the keywords of the first message decal, the first message decal has a stronger (e.g., higher) ranking than the second message decal, which has 3 keywords that match the words of the suggested response.
In some implementations, confidence scores are used to determine correspondence between suggested responses and message stickers (e.g., to determine correspondence of semantic meaning), and these scores are used to rank message stickers based on the confidence of the correspondence between message stickers and suggested responses. In some implementations, a particular number of top ranked message stickers is determined to be suggested message stickers. For example, in some implementations referring to fig. 2, response suggestion generator 216 may send the particular number of top ranked message decals to decal suggestion module 212.
In block 420, at least one suggested message decal is provided to the second user. In some implementations, the suggested message decal is sent to the second user device. In some examples, a certain number of top-ranked suggested message decals are sent. For example, referring to fig. 2, the suggested message decal may be sent from the decal suggestion module 212 to the second user device 208 via the server interface 210. In some examples, the instructions are sent with the message decal to the second user device 208, causing the second user device to display the message decal. In some implementations or situations, there may be multiple recipient devices receiving the message and suggested message decals. The suggested message decal provided may be displayed by the second user device, for example, in a user interface, as requested by the second user in block 410. For example, the top ranked message decal may be displayed in an order or configuration based on its ranking.
In block 422, a selection of one or more of the displayed suggested message stickers is received. For example, the selection may be based on user input provided by a user at the user device, e.g., through an input device such as a touch screen or touchpad that receives user touches, a microphone that receives voice commands, buttons, a mouse, or a joystick that receives user manipulations, etc. In some implementations, referring to fig. 2, the second user 206 provides user input at the second user device 208. In some implementations, one or more suggested message stickers are automatically selected (without user input), e.g., based on user preferences, user context (e.g., geographic location, time or date, etc., if user consent has been obtained), or other predetermined conditions.
In block 424, the selected suggested message decal is output as one or more messages to one or more recipient devices. For example, a message including the selected message decal may be sent to one or more other client devices 115 and/or directly to another client device 115 via the message server 101 over the network 140. In some embodiments, the message is sent to the user device that sent the image obtained in block 408. For example, referring to fig. 2, the second user device 208 sends the selected message decal to the first user device 204. The method may then return to block 408 in some embodiments, for example, if another image has been obtained.
Fig. 5 is a flow diagram illustrating another example method 500 of providing one or more message decal suggestions in response to a received message, according to some embodiments. In some implementations, the method 500 is implemented, for example, on a server system, such as the message server 101 shown in fig. 1. In some implementations, some or all of the method 500 is implemented on a system, such as one or more client devices 115 shown in fig. 1, and/or on both a server system and one or more client systems. In the described examples, the implementation system includes one or more processors or processing circuits, and one or more storage devices, such as a database or other accessible storage. In some implementations, different components of one or more servers and/or clients may perform different blocks or other portions of method 500.
In block 502, it is checked whether user consent (e.g., user permissions) to use the user data has been obtained in the implementation of method 500. For example, the user data may include messages sent or received by the user, e.g., using the messaging application 103, user preferences, user biometric information, user characteristics (identity, name, age, gender, occupation, etc.), information about the user's social network and contacts, social and other types of actions and activities, content created or submitted by the user, ratings, and opinion, the user's current location, historical user data, images generated, received, and/or accessed by the user, videos viewed or shared by the user, and so forth. In some implementations, one or more blocks of the methods described herein may use such user data.
If user consent has been obtained from the relevant users who may use the user data in method 500, then at block 504, the blocks of determining the method herein may be implemented through rational use of the user data as described for those blocks, and the method continues to block 508.
If user consent has not been obtained, then it is determined in block 506 that the block will be implemented without using user data, and the method continues to block 508. In some implementations, if user consent has not been obtained, the blocks are not implemented using user data, but rather are implemented using general or publicly accessible data.
Some embodiments described herein, for example, in connection with fig. 5, may provide a suggested standardized message decal to be used as a response to receiving a message. Suggested message stickers may be provided in a variety of contexts. For example, in response to receiving a message at a server and/or from a client device 115a of a particular user (e.g., user 125 a) of any client device 115, such as by the message application 103, a suggested message decal may be provided. For example, the messaging application 103a/103b may be an instant messaging application, a social networking application, an email application, a multimedia messaging application, and the like.
If consent has been obtained from the particular user and one or more other users transmitting and/or receiving images, a suggested message decal may be generated and automatically provided to the particular user. For example, if the message application is an instant message application, the message may be received as part of an instant message communication between a particular user 125a and one or more other users 125 of the user device, e.g., in a message session (e.g., chat) with two participants, in a group message session including more than two participants, and so on. In various embodiments, one or more users may be robots, similar to that described in fig. 3.
In some embodiments, suggested message stickers may be provided exclusively to those users who have agreed to access the user data. In some implementations, the suggested message decals may be provided automatically or based on user input, e.g., a user interface may be provided to the user for requesting the suggestion.
At block 508, a message is received that has been sent from a first user, e.g., to a second user over a communication network. In various embodiments, the message may be a text message, an image (e.g., a still image, a moving picture or animated image, etc.), video, audio data of an audio message, etc. In some implementations, for example, the message can include a message decal, similar to that described in fig. 3. In some implementations, a combination of two or more of these types of content is included in the message.
In some examples, the message may have been sent by a first device, which is one or more client devices 115 (or other devices), or by a robot that sent the message, and the message may be received by a second device, which is a client device 115 (or other device), via network 140. In a further example, in a one-to-one (1:1) message session with two participants, a message is received by a client device of a particular user.
In a further example, in a group message session, a message is received by two or more client devices of participants in the group message session. In some implementations, the message is obtained from storage, e.g., local memory (e.g., memory, storage drive, etc.) of the client device 115 and/or remote memory (e.g., a remote client device, server device, or storage drive of a storage device) accessible to the client device over the network 140. For example, in some implementations, a first device stores a message on a storage device and a second device obtains the message from the storage device.
In an example involving the system 200 of fig. 2, a received message is sent by the first user 202 using the first user device 204. In some implementations, in block 408, the message sent by the first user device 204 is received by a client device, such as the second user device 208 shown in fig. 2, and/or by a server device, such as the server interface 210 shown in fig. 2. For example, similar to that described above for fig. 3, the server interface 210 may receive a message from a first user device and forward the message to a second user device 208, and provide a copy of the message to the decal suggestion module 212.
In block 510, a list of descriptors (e.g., descriptions and/or keywords) is determined for the available message decals. For example, in some implementations, the list may include descriptions and keywords of all available message stickers (or a subset thereof) that may be used by the first user device 204 and the second user device 208. In some embodiments, the message decal used to provide this information is a message decal having the latest version number. For example, the descriptors may be stored in memory, such as in the decal data module 214 described above. In some implementations, a server (e.g., server interface 210 or decal suggestion module 212) can access decal data module 214 to obtain a list of descriptors. In some implementations, descriptors (e.g., descriptors can include words determined from the description) are grouped into a single list of descriptors. For example, if the version number has not been changed for the cached message decal, some embodiments may cache the descriptor list in the memory of the executing device (e.g., a server in some embodiments).
In block 512, one or more suggested responses are determined or generated based on the received message. In some implementations, the suggested responses are generated based on semantic concepts detected in the received message, e.g., a mapping of semantic concepts to specific suggested responses, similar to that described above. Suggestion generator 216 may consult database 218 to determine a suggestion response (e.g., a text response) based on the message, e.g., similar to that described for block 416 of fig. 4, using stored associations, mapping models, and/or machine learning models. In some examples, the suggested response may be obtained from a collection or library of stored suggested responses, e.g., the stored suggested response is a composite response, or if user consent has been obtained, the stored suggested response is a response obtained from an actual user in a previous device communication (e.g., using a messaging application).
In some implementations, the one or more suggested responses may also be a list of descriptors based on the available message decals determined in block 510. For example, one or more suggestion responses may be generated in the form of one or more descriptors (e.g., keywords or descriptions) present in the descriptor list. In some implementations, for example, referring to fig. 2, the decal suggestion module 212 can send the list of descriptors to the response suggestion generator 216. The decal descriptors in the list may be used as or to provide one or more concepts that are matched to concepts in the suggested response determined as described above. For example, if the suggested response is determined to be "lovely dog-! The suggestion module 212 or suggestion generator 216 may examine the list of descriptors to find keywords that match or partially match words in the suggestion response. For example, the keyword "dog" and the keyword "lovely" and the description "lovely dog" may be found in the list (as separate descriptors) that match the suggested response. Additional suggested responses may then be generated in the form of these matched descriptors, e.g., a suggested response may be a "dog" and another suggested response may be "lovely". In some embodiments, the "lovely dog-! The "suggested response" may be modified to be a "lovely dog" that matches the description in the descriptor list.
In some implementations, semantic concepts detected in the received message can be directly compared to the list of decal descriptors and descriptors that match the detected concepts (e.g., partially or precisely match, and/or semantically match, as described herein) are generated as suggestion responses.
In block 514, at least one suggested response is provided to the second user. In some embodiments, the provided suggestion response is sent from the server device to the second user device. For example, referring to fig. 2, a suggestion response may be sent from the decal suggestion module 212 to the second user device 208 via the server interface 210. In some implementations or situations, there may be multiple recipient devices receiving the suggested response. Some embodiments may alternatively or additionally send semantic concepts determined in the received message to the second user device, e.g., as described below, to allow the second user device to compare such semantic concepts with stored associations.
In some implementations, the suggested response is displayed by the second device as a response option (e.g., a text response), which may be selected by user input at the second user device to cause the second user device to send a message corresponding to the selected response option. For example, such suggested responses may be displayed and selected in addition to the suggested message decal responses being determined and displayed as described below.
At block 516, a user input request from the second user provides one or more decal suggestions that the second user is able to select. For example, in some implementations, block 516 is implemented by the second user device 208, where user input is provided to the second user device 208 to command display of a user interface (e.g., window or other display area) that displays a list of message stickers.
In block 518, in response to the user input of block 516, a descriptor (e.g., a keyword and description) is determined for the message decal (e.g., "device descriptor") available on the second user device. For example, the second user device may scan message stickers stored on the second user device and determine descriptions and keywords for those message stickers. In some implementations, similar to that described above, the second user device can retrieve the descriptor from a data source, such as the decal data module 214, for example, by directly accessing the decal data module 214, or by the server interface 210 and/or the decal suggestion module 212. In some implementations, block 518 occurs at other times or under other conditions, e.g., upon startup of the second user device, in response to the second user device installing an additional message decal or the like, rather than in response to the user input of block 516.
In block 520, the device descriptor of the message decal on the second user device is compared to the suggested response determined in block 512, for example, by the second user device, and the one or more device descriptors are matched to the one or more suggested responses. For example, if the suggestion response is in the form of suggestion descriptors obtained from a list of descriptors of available message stickers as described above, these received descriptors may be compared with device descriptors of message stickers stored on the second user device. For example, a descriptor of a message decal may be considered a semantic concept associated with the message decal.
In some implementations, the match is a correspondence, which includes an approximate or imprecise match. For example, if there is a threshold percentage of letters that match, if there is a similarity score or confidence score that exceeds the threshold during the matching process, the past times and complex forms of words, etc. may be ignored. In some implementations, the correspondence is checked for semantic similarity, e.g., using accessible dictionaries, synonym dictionaries, knowledge maps, and/or other reference information, to determine whether the meaning of the descriptor and the suggested response are the same, similar to those described in other blocks and methods of determining the correspondence herein.
In block 522, one or more suggested message stickers, for example, displayed on the second user device, are determined and provided based on the comparison of block 520. In some implementations referring to fig. 2, the second user device 208 may determine and order one or more suggested message stickers. For example, a message decal with a descriptor that has been determined to correspond to a suggested response (e.g., suggested text response) for a received message may be included in a set of suggested message decals.
In some implementations, the device descriptors of the message decals on the second user device are compared to the generated suggested responses, and if any device descriptors correspond to (e.g., match) the suggested responses, the message decal corresponding to the matching device descriptor is selected as the suggested message decal. For correspondence check matches, e.g. matches between letters of words in the same sequence, and/or matches of words in the same sequence. In some implementations, exact and imprecise matches are used or allowed for correspondence, and similarities between device descriptors and suggested responses may be checked (e.g., differences caused by past times and complex forms of words may be ignored, etc.). In some examples, a similarity score for a device descriptor may be determined for a suggested response (e.g., for each device descriptor for each suggested response), the similarity score indicating a degree of similarity between the descriptor and the suggested response. For example, the similarity score may be based on the number or percentage of letters of a particular order of matching.
In some implementations, the system examines semantic similarity as a correspondence, e.g., using an accessible dictionary, synonym dictionary, knowledge graph, and/or other reference information, to determine whether the meaning of the device descriptor and the suggested response are the same. In some implementations, a decal similarity score for a message decal and a suggested response can be determined based on a separate similarity score determined for each descriptor of the message decal. For example, the decal similarity score may be a sum of individual similarity scores determined for the descriptors.
In some implementations, a message decal having the highest similarity to the suggested response is selected for the suggested response, e.g., based on the similarity score. In some examples, a message decal with the highest similarity score for the suggested response is selected for the response. In further examples, message stickers may be selected based on a degree of similarity, where higher similarity may be determined based on, for example, more words in a device descriptor of the message sticker corresponding to words in the suggested response, and/or a higher score in the determination of semantic similarity between the device descriptor and the suggested response. In some implementations, the plurality of message stickers is determined to correspond to a particular message suggestion, e.g., a particular number of message stickers having a highest similarity to the message suggestion.
In some implementations, semantic concepts determined in the received message (message concept) are provided to the second user device, and the second user device can compare these message concepts to stored semantic concepts that have been directly associated with a particular suggested message decal response. These suggested decals have been stored in association with stored concepts as suggested responses to the stored concepts. For example, similar to that described herein, correspondence of message concepts to stored concepts may be checked (e.g., check matches, semantic similarity, partial matches, etc.), and if the message concepts match or correspond to the stored concepts, a suggested decal response associated with the stored concepts may be selected for block 418. In some examples, the device descriptor of the message decal on the second device may be used as a stored semantic concept.
The determined suggested message decals may be ordered relative to each other. For example, the placement or ranking of a message decal in the ordering may be based on the ranking of the message suggestion associated with the message decal relative to other message suggestions. In a further example, the ranking of the suggested message decals may be based on the strength of correspondence between the suggested message decals and their associated suggested responses. For example, the placement or ranking of a message decal in the ordering may be based on the number of matches (correspondences) between the associated suggested response and the device descriptor of the message decal.
In some examples, the number of matches may be the number of matches of words or phrases in the device descriptor with words of the suggestion response. In some examples, if there are multiple matches between the word of the suggested response and the device descriptor of the first message decal, the first message decal has a stronger (e.g., higher) rank than the second message decal, which has a single device descriptor that matches the word of the suggested response.
In some implementations, confidence scores are used to determine correspondence between suggested responses and message stickers (e.g., to determine correspondence of semantic meaning), and these scores are used to rank message stickers based on the confidence of the correspondence between message stickers and suggested responses. In some implementations, a particular number of top ranked message stickers is determined to be suggested message stickers and provided by the second device. For example, in some implementations referring to fig. 2, the second client device is caused to display a particular number of top ranked message stickers, e.g., in a user interface, as requested by the second user in block 516. For example, the top ranked message decal may be displayed in an order or configuration based on its ranking.
In block 524, a selection of one or more of the displayed suggested message stickers is received, e.g., sent in response to the received message (or in response to a newly designated receiving user). For example, the selection may be based on user input provided by a user at the user device, e.g., through an input device such as a touch screen or touchpad that receives user touches, a microphone that receives voice commands, buttons, a mouse, or a joystick that receives user manipulations, etc. In some implementations, referring to fig. 2, the second user 206 provides user input at the second user device 208. In some implementations, one or more suggested message stickers are automatically selected (without user input), e.g., based on user preferences, user context (e.g., geographic location, time or date, etc., if user consent has been obtained), or other predetermined conditions.
In block 526, the selected suggested message decal is output as one or more messages to one or more recipient devices. For example, a message including the selected message decal may be sent to one or more other client devices 115 and/or directly to another client device 115 via the message server 101 over the network 140. In some embodiments, the message is sent to the user device that sent the image obtained in block 408. For example, referring to fig. 2, the second user device 208 sends the selected message decal to the first user device 204. The method may then return to block 508 in some embodiments, for example, if another message has been received.
In some embodiments, personalized or customized decals may be used to suggest message decal responses as described for embodiments herein. In some examples, as with corresponding standardized decals, custom decals may have similar decal IDs and decal set IDs, and may also be associated with image data that is custom-made by a particular user. For example, such customized message stickers, when displayed on a user's device, may be displayed with customized image data and may be displayed with standard image data on other devices (or customized for those user devices, if applicable). For example, the customized image data may be stored in association with standardized decal information stored on the device used by the user. In some implementations, the customized message decal may be stored and displayed as a new decal for the user's device.
In any of the embodiments described herein, a metadata file containing information about the decal set and individual decals may be maintained by the system, for example, to allow reference to consistent data related to the message decal. For example, metadata about a decal set may be stored in a global metadata file (or other data set). In some examples, the global metadata file may be stored in the decal data module 214. The metadata may include a sticker ID, a sticker set ID, and a version number as described above. In some implementations, the decal metadata is specified to be unchangeable once referenced by the global metadata file. In some implementations, the client device is notified to update a message decal having a newer version than the version stored at the client device.
Some embodiments may allow a user to download message decals and decal sets that are related to the capabilities of the user's device, e.g., a low resolution (e.g., DPI) device may download a low resolution decal set and not need to download a decal set that is not a higher resolution.
Current version data may be maintained for the message decal. In some implementations, version polling (polling) may be used. The latest version of the message decal may reference the latest version of the global metadata file, where the global metadata file may reference the version of each decal set used by the user. In some implementations, the client device can determine when and/or how frequently to poll the latest version of the message decal. The client device may request new metadata or message stickers for the set of changed versions. In some implementations, if a message decal is added, removed, or updated in the decal data module 214, the user who has downloaded the decal may need to re-download the entire decal set including the message decal.
In some implementations, one or more client devices are responsible for managing the device's own versioned decals. In some cases, for example, during a period of updating a decal set, a client device may send and receive messages referencing a message decal of a previous version or a version that the device does not currently have. In the former case, the client device may retain versions of previously stored message stickers so that the device may render the sticker messages referencing the old versions without having to re-download the message stickers. If the device does not store a newer version of the message decal, in some embodiments, when the device receives a decal message referencing a newer decal set version, the device may first download the message decal data (e.g., image data) for display, and may then trigger a decal version sync to retrieve updated metadata and/or other data (e.g., a thumbnail version of the message decal).
In some embodiments, the message decal is sent by reference. For example, instead of transmitting actual message sticker data (e.g., pixel data or frames for a sticker image or animation), sticker ID information such as parameters of a sticker ID, a sticker set ID, and a version number may be transmitted. This may reduce the delay in sending the decal message and reduce the message payload size. Using these parameters, the client device may determine whether the device has a particular message decal stored in a device local cache or other local memory, or whether the device needs to retrieve (e.g., download) the message decal and metadata from a different device that stores the message decal. In some examples, if a message decal is to be retrieved, a URL or other network address for retrieving the message decal may be generated using the value of the parameter.
Some embodiments may perform verification and testing of message stickers, for example, to verify the fidelity of the sticker messages and the fidelity of references to data stored in the sticker data module. The test may be performed, for example, to test for changes to a message decal catalog or other data set. For example, it may be checked whether the current version data references an existing decal set metadata file, whether the global metadata file references an existing decal set version, whether the downloadable decal set includes a thumbnail image for each message decal in the set, whether the message decal image data is properly resized (e.g., low resolution or high resolution according to a predefined size), and so forth.
Embodiments relate generally to messaging applications. Some embodiments may automatically analyze the image content of one or more message conversations and/or user information to automatically provide message suggestions to users within a message application. In some examples, the auto-suggestion may provide one or more appropriate responses that are to be selected by the user to respond in the messaging application, and/or may automatically send one or more appropriate responses on behalf of the user. In some other examples, the suggestion may automatically incorporate specific non-message functions into the message application.
While the foregoing description includes techniques for providing message suggestions in response to receiving a message or message decal, message suggestions may be provided in response to any type of media content received in a conversation. For example, such content may include stickers (e.g., in chat applications), animated images (e.g., moving pictures, GIF images, etc.), and videos. Furthermore, while the foregoing description describes the message suggestion as a text response or a decal response, other types of responses may be suggested based on, for example, analysis of the received message or decal. For example, other responses may include one or more of a suggested image, a suggested animated image (e.g., a moving picture, a GIF image, etc.), a suggested video, and a suggested audio clip or clip.
To provide these suggestions, suggestion generator 216 may proceed, for example, to compare identified concepts in received items (e.g., text, images, videos, stickers, animated images, etc.) to concepts in different types of responses and select an appropriate response, as described herein with reference to message suggestions. In various embodiments where the user provides consent, the type of response may be selected or prioritized based on context, e.g., a decal may be selected as a suggested message in response to an incoming decal, a video may be selected as a suggested response in response to an input image, etc.
Certain embodiments enable messaging with human users and/or chat robots. In some implementations, automatic message suggestions can be customized based on whether the chat bot is engaged in a message conversation. In some examples, a first set of automated message suggestions may be provided if a chat bot is not present in the message conversation, and a second set of automated message suggestions may be provided if a chat bot is present in the message conversation, wherein the first and second sets of responses are at least partially different. For example, these embodiments may employ conversation rules that chat robots follow and suggest messages to users based on the rules. This may alleviate challenges that users may encounter when communicating with chat robots in languages and forms that are easily understood by chat robots.
Some implementations may include determining one or more trend responses (e.g., message responses that include popular message content sent by many different users) based on other messages in at least one of the region, market, and country associated with the user's location. The one or more determined message suggestions may include one or more trend responses. In some implementations, a user context, e.g., geographic location, holiday or event, etc., is used to generate and determine one or more message suggestions for presentation.
Determining the suggested response may be further based on developing a personalized model for the user using machine learning. Determining the suggestion response may be based on the user's preferences and/or the user's previous actions in the communication (if the user's consent to use such actions and data has been obtained). For example, the user preferences may include a white list indicating particular words (or other media content items) that may be included and/or a black list indicating particular words that cannot be included in the message suggestion. If user consent has been obtained, a message suggestion may be generated or modified based on one or more of punctuation usage, emoji usage, or other content provided by the user at a previous occasion.
The model for providing message suggestions may be implemented by the client device 115 and/or the message server 101. In some implementations, the conversation may be encrypted such that only client devices of participants in the conversation may access conversation content. In these embodiments, the model implemented by the respective client device may be used to provide message suggestions, and the model implemented by the server is not used. The model implemented by the client device may also be used, for example, when the user does not provide consent to use the server-implemented model. In some implementations, the client-implemented model may be a server-based model or a model derived from a server implementation. In some implementations, a server model may be used and a client model not used, for example, when a client device lacks the ability to implement the client model. In some implementations, a combination of client and server models may be used.
Although the examples described herein utilize concepts shown in English, suggestions may be provided in any language, such as a language configured for client device 115, a region or other geography, a language selected based on user preferences, and so forth. In some implementations, where the user agrees to analyze the context of the conversation, the language used in the various conversations in which the user is engaged (e.g., in the most recent message) can be detected and message suggestions provided in that language.
FIG. 6 illustrates an example user interface 600 with message suggestions in response to receiving a message decal. The user interface 600 may be displayed on a user device, for example, on a client device in some implementations. In the example shown in fig. 6, a first user using a user interface 600 displayed on the first user device sends a message "what is? "602 to the second user equipment. The second user 604 responds with a message decal 606, which message decal 606 is received by the first user device and displayed in the user interface 600. In the example shown in fig. 6, the message decal is an image that includes the stylized text "call me" and a description of the call.
One or more suggested responses are provided in the first user interface in response to receiving the message decal. For example, the suggested response may include a text response "call" 610 and another text response "i am not able" (612). The user interface 600 allows the first user to select from the suggested responses 610 and 612, or to compose their own responses using the message box 620, where the first user can enter text or other information, or select other content items to be sent. In the example shown in fig. 6, the one or more suggested responses may be based on receipt of the message decal 606. In some implementations, if the first user agrees, the suggested response can be personalized for the first user. For example, the suggested response may be determined based on the frequency of use of the different responses by the first user, the conversational style of the first user (e.g., humorous, formal, etc.). In some examples, the first user 702 is a human user. In some implementations, the first user 702 can be a robot.
Fig. 7 illustrates an example user interface 700 resulting from fig. 6, and the example user interface 700 may be displayed when a first user selects a suggestion response, such as element 610 of fig. 6. The user interface of FIG. 7 includes elements 602-606 and 620 described above in connection with FIG. 6. For ease of illustration, some reference numerals are omitted from fig. 7. Upon user selection of element 610, a response "call" 702 is sent to the second user device.
FIG. 8 illustrates an example user interface 800 with a suggested response that includes one or more message stickers. In the example shown in fig. 8, the first user sends a message "hi? "802 to the second user equipment. The second user 804 lets us meet-! "806 in response to the first user device, the message" let us see-! "806 is displayed in the user interface 800.
In response to receiving the message, one or more suggested responses including a message decal are provided in the first user interface 800. For example, the suggested responses include message decal responses 810,812, and 814, each message decal response 810,812, and 814 indicating a display appearance (e.g., image data) of a message decal to be sent if the message decal response is selected. The user interface allows the user to select a particular message decal from the suggested responses or compose their own response using a message box 820 in which the user can enter text and/or other information, e.g., from one or more decals available in the messaging application. In the example shown in fig. 8, message decal responses 810,812, and 814 can be selected by the system and provided as suggested responses based on the dialog context. For example, based on the received message 806, it may be determined that the likelihood of the first user selecting the message decal responses 810,812, and 814 is high.
In some implementations, if the user has agreed, the message decal included as the suggestion response can be personalized for the first user. For example, the suggested message decals may be determined based on a set of decals available locally on the user's device (e.g., client device 115), a set of decals that the user has purchased, a set of decals that are popular among users when receiving message 806, the first user's location, the first user's language setting, the language used by the user when communicating through the message application, popularity of various decal sets, and so forth.
FIG. 9 illustrates an example user interface 900 derived from FIG. 8, and the user interface 900 may be displayed when a user selects a suggested response, such as suggested message decal response 814 of FIG. 8. The user interface of FIG. 9 includes elements 802-806 and 820 described above in connection with FIG. 8. For ease of illustration, some reference numerals are omitted from fig. 9. Upon user selection of element 814, a response 902 including a message decal 814 is sent to the second user device.
FIG. 10 illustrates an example user interface 1000 with message suggestions that include one or more location suggestions. The location suggestion may be based on one or more messages sent in the communication. In the example shown in fig. 10, a first user using the user interface 1000 on a first user device sends a message "hi" to a second user device (1002). The second user 1004 uses the message "where is you? "(1006) in response to the message, message" where does you? "(1006) has been sent from the second user device to the first user device and displayed in the user interface 1000.
Based on the analysis of the context of the message, if the user has agreed to such analysis, a determination is made as to whether the location response is appropriate. In some implementations, for example, based on determining that the conversation is location-related, e.g., that the message in the conversation includes a particular phrase (stored in and compared to accessible memory), such as "where you are," "see for us," etc., it is appropriate to determine the location response. In some implementations, the dialog context may be analyzed to determine that the dialog is about a particular location. For example, if the message in the conversation is "where is the game tonight? The suggested response may be a stadium associated with the user's city. In another example, if the message in the conversation is "where is the house of Jim? The suggested response may be an address associated with the contact "Jim" in the address book associated with the user. In some implementations, location advice may be triggered, for example, based on one or more entities in a conversation, e.g., "let us meet at XYZ coffee shops.
When it is determined that the location response is appropriate and the user has agreed to acquire and share the user location, the messaging application may provide a suggested response to share the user location. For example, in FIG. 10, user interface elements 1010,1012, and 1014, each corresponding to a suggested response, are caused to be displayed in user interface 1000. The first user may select any of the elements 1010,1012, and 1014 to respond to the message 1006. Alternatively, the user may compose a message using the message box 1020, where the user may enter text and/or other information.
In some implementations, if the user selects element 1010, the user's location is obtained, for example, using GPS, cellular triangulation, or other mechanisms involving the first user device. In some implementations, in response to a user selection of element 1010, a link (e.g., a URL) to a map can be determined based on the user's location and sent in a message. In some implementations, in response to a user selection of element 1010, an image or schematic is sent as a message, the image or schematic showing a map indicating a location of the first user device.
In some implementations, the messaging application can utilize the current geographic coordinates of the first user device to find maps and other location information sources (e.g., accessible over the network and the internet) to obtain a place name (e.g., "XYZ coffee shops," "football stadium," etc.), which can then be provided as a suggested reply. For example, the first user device has determined element 1012 in this example by referring to a map and location information having the current location of the first user device, and finding the name of the institution in which the device is currently located. If the user selects element 1012, a response may be sent that includes the text "XYZ coffee shop".
In some implementations, upon selection of element 1012, a formatted response may be sent, e.g., the response may be formatted as a card that includes information about XYZ coffee shops (e.g., images, phone numbers, business hours, etc.). In some implementations, upon selection of element 1012, a response may be sent that includes a link to "XYZ coffee shop" (e.g., to cause receipt of data of and display of a website or social media account of "XYZ coffee shop" over a network), where the link is displayed and may be selected by user input on the device receiving the response.
In various embodiments, the location suggestion may be provided in a language, such as a language of a dialog, or a language determined based on the location of the user or other factors. In various embodiments, the dialog context may be determined by using a machine learning model that may analyze strings of multiple languages. In some implementations, dialogs in languages other than English may be translated into English for processing by the machine learning model. In some implementations, a machine learning model can be trained on a native corpus of detected language.
FIG. 11 illustrates an example user interface 1100 resulting from FIG. 10, and the example user interface 1100 is displayed when a user selects a suggestion response, such as element 1010 of FIG. 10. The user interface of FIG. 11 includes elements 1002-1006 and 1020 described above in connection with FIG. 10. For ease of illustration, some reference numerals are omitted from fig. 11. Upon user selection of element 1010, a map 1102 (e.g., thumbnail, interactive map, etc.) is sent in response, where the map may graphically indicate the user's location, e.g., with a pointer as shown. If user consent has been obtained to use such data, some embodiments may provide suggestions of geographic locations for future meeting locations, events or other locations indicated in the conversation of the message application, and these locations may be displayed in the message interface in response to selection of the location suggestion.
Robot embodiment
One or more robots may be implemented using one or more features described herein, e.g., a robot may be implemented or accessed by one or more components of environment 100 of fig. 1. The bot is an automated service implemented on one or more computers with which the user interacts through, for example, text, such as through messaging applications 103a/103b or other applications, etc. The robot may be implemented by a robot provider so that the robot may interact with users of various messaging applications. In some implementations, the provider of the messaging application 103a/103b can also provide one or more robots. In some implementations, robots provided by the providers of message applications 103a/103b may be configured such that the robots may be included in other message applications provided by other providers, for example.
Robots can provide several advantages over other modes. For example, the bot may allow a user to try new services (e.g., taxi booking services, restaurant booking services, etc.) without installing an application or accessing a website on the client device. In addition, the user may interact with the bot through text, requiring minimal or no learning compared to telephone calls through a website, software application, such as an Interactive Voice Response (IVR) service, or other means of interacting with the service. Incorporation of robots in a messaging service or application may also allow users to cooperate with other users in the messaging service to accomplish various tasks such as travel planning, shopping, scheduling events, retrieving information, etc., and eliminate cumbersome operations such as switching between various applications (e.g., taxi booking applications, restaurant booking applications, calendar applications, etc.) or websites to accomplish tasks.
The bot may be implemented as a computer program or application (e.g., a software application) configured to interact with one or more users (e.g., any of the users 125 a-n) via the messaging application 103a/103b to provide information or to perform specific actions in the messaging application 103. As one example, the information retrieval robot may search for information on the internet and present the most relevant search results within a messaging application. As another example, the travel robot may have the ability to conduct travel arrangements via the messaging application 103, such as by implementing purchasing travel and hotel tickets within the messaging application, conducting hotel reservations within the messaging application, and conducting rental car reservations within the messaging application. As another example, the taxi robot may have the ability to call a taxi to a user location (obtained by the taxi robot from the client device 115 when the user 125 allows access to location information), for example, without invoking or calling a separate taxi booking application. As another example, the trainer/tutor robot may tutor the user on certain topics within the messaging application to guide the user, for example, by asking questions that may be present in the examination and providing feedback as to whether the user's response is correct. As another example, the game robot may play a game with an opponent or teammate of the user within the messaging application. As another example, the commerce robot may provide services from a particular merchant, for example, by retrieving product information from a merchant's catalog and effecting a purchase through a messaging application. As another example, the interface robot may engage a remote device or vehicle such that a user of the messaging application may chat with the remote device or vehicle, retrieve information from the remote device or vehicle, and/or provide instructions to the remote device or vehicle.
The capabilities of the robot may include understanding the user's intent and executing the intent. The user's intent may be understood by analyzing and understanding the user's dialog and the context of the dialog. The robot may also understand the context change of the dialog or the mood change and/or the intent change of the user based on the dialog evolving over time. For example, if user A suggests that the meeting is to drink coffee, but if user B states that he does not like coffee, the robot may assign a negative mood score for coffee to user B and may not suggest a coffee shop to meet.
Implementing a robot that can communicate with a user of the messaging application 103a/103b can provide a number of advantages. Traditionally, users may utilize software applications or websites to conduct activities such as paying bills, ordering food, booking tickets, and the like. A problem with this embodiment is that the user is required to install or use a plurality of software applications and websites in order to perform a variety of activities. For example, the user may have to install a different software application to pay a utility bill (e.g., from a utility company), purchase movie tickets (e.g., from a ticketing application of a ticketing service provider), make restaurant reservations (e.g., from various restaurants), or may need to access various websites for each event. Another problem with such an embodiment is that the user may need to learn a complex user interface, e.g., a user interface implemented using multiple user interface elements, such as windows, buttons, check boxes, dialog boxes, and the like.
Thus, an advantage of one or more of the described embodiments is that a single application enables a user to conduct activities involving interactions with any number of parties without having to access a separate website or install and run a software application, with the technical effect of reducing memory, storage and processing resource consumption on the client device. An advantage of the described embodiments is that, for example, without learning a complex user interface, a dialog interface makes it easier and faster for a user to accomplish such activities, with the technical effect of reducing computing resource consumption. Another advantage of the described embodiments is that implementing robots may enable various participating entities to provide user interactions at a lower cost, which has the technical effect of reducing the need to deploy computing resources for achieving user interactions, such as toll free numbers implemented using one or more communication servers, websites hosted on one or more Web servers, customer support emails hosted on email servers, and so forth. Another technical effect of the described features is to reduce the consumption of system processing and transmission resources required to accomplish user tasks across a communication network.
Although some examples herein describe interactions between a robot and one or more users, various types of interactions, such as one-to-one interactions between a robot and a user 125, one-to-many interactions between a robot and two or more users (e.g., in a group message conversation), many-to-one interactions between multiple robots and users, and many-to-many interactions between multiple robots and multiple users are possible. Further, in some implementations, the robots may also be configured to interact with another robot (e.g., robots 107a/107b,109a/109b,111,113, etc.) via the messaging application 103, via direct communication between the robots, or a combination thereof. For example, a restaurant reservation robot may interact with a particular restaurant's robot to reserve a table.
In some embodiments, the bot may use a dialog interface to interact with the user using natural language. In some embodiments, the bot may use a template-based format to create sentences that interact with the user, for example, in response to a request for a restaurant address, using a template such as "restaurant R is located at L". In some cases, the user is enabled to select a robotic interaction format, e.g., whether the robot interacts with the user using natural language, whether the robot uses template-based interactions, etc.
In the case of a robot interacting in a dialogue using natural language, the content and/or style of the robot interaction may dynamically change based on one or more of: the content of the dialog determined using natural language processing, the identity of the user in the dialog, and the context of one or more dialogs (e.g., historical information regarding interactions of the user, relationships between users in the social graph-based dialog), external conditions (e.g., weather, traffic), schedules of the user, related contexts associated with the user, etc. In these cases, the content and style of the robotic interaction varies based on these factors that only the users participating in the conversation have agreed to.
As one example, if the user of the conversation is determined to be using a formal language (e.g., no or minimal slang or emoticons), the robot may also interact within the conversation using the formal language, and vice versa. As another example, if a user in a conversation is determined to be a heavy user of emoticons (based on current and/or past conversations), the robot may also interact with the user using one or more emoticons. As another example, if two users in a conversation are determined to be far apart in the social graph (e.g., having two or more intermediate nodes between them, representing friends, such as friends that they are friends of friends), then the bot may use a more formal language in the conversation. In the event that the user engaged in the conversation does not agree that the robot utilizes factors such as the user's social graph, calendar, location, or other context associated with the user, the content and style of the robot's interaction may be a default style, e.g., a neutral style that does not require the use of these factors.
Further, in some implementations, one or more robots may include functionality to engage in back and forth conversations with a user. For example, if the user provides, for example, the input "@ moviebot you can recommend a movie? ", to request information about a movie, the robot" moviebot "may respond: "do you like comedy? The user may then respond, e.g., "no", and the robot may respond "well. The science fiction movie 'space and star' has been well evaluated. Do i order you? "then the user may indicate" yes, i may go after 6 pm. Please check if still he can join. When the user agrees to the bot to access information about their contacts and the friend steve agrees to receive messages from the bot, the bot may send a message to the user's friend steve and take further actions to subscribe to the movie ticket for the appropriate time.
In some embodiments, a user engaged in a conversation can invoke a particular robot or robot performing a particular task, for example, by typing a robot name or robot handle (e.g., taxi, @ move, etc.), by using voice commands (e.g., "invoke a bank bot," etc.), by activating a user interface element (e.g., a button or other element labeled with a robot name or handle), and so forth. Once the bot is invoked, the user 125 may send a message to the bot via the messaging application 103a/103b in a manner similar to sending messages to other users 125. For example, to order a taxi, the user may type "@ taxi" to find me taxi "; to book a hotel, the user can type in a chinese restaurant reservation 4 people in the vicinity of me "@ hotelbot. "
In some embodiments, the robot may automatically suggest information or actions within a message conversation without being specifically invoked. That is, the user may not need to specifically invoke the robot. In these embodiments, the robot may rely on analysis and understanding of the dialog at sustained or discrete points in time if user consent has been obtained. Analysis of the dialog may be used to understand specific user needs and to identify when assistance should be suggested by the robot. As one example, if it is determined that a user needs information (e.g., based on a user asking a question to another user, based on multiple users indicating that they do not have some information), the bot may search for some information and suggest an answer. As another example, if it is determined that multiple users have expressed an interest in eating chinese food, the robot may automatically suggest a set of chinese restaurants in the vicinity of the user and include selectable information such as location, ratings, and links to restaurant websites.
In some embodiments, instead of automatically invoking a robot or waiting for the user to explicitly invoke a robot, one or more users in a message conversation may make an automatic suggestion to invoke one or more robots. In these embodiments, if user consent has been obtained, the dialog may be analyzed continuously or at discrete points in time, and the analysis of the dialog may be used to understand the specific user needs and to identify when the robot should be suggested in the dialog. For example, a particular keyword, phrase, or exchange of words or phrases in a message conversation may be detected by a program executing on a device participating in the message conversation, where such a keyword or phrase may indicate that a robot is invoked and added to the message conversation.
In embodiments where the robot may automatically suggest information or actions within a message conversation without being explicitly invoked, for example, such functionality may be disabled if one or more users participating in the message conversation do not agree to the robot to conduct analysis of the user conversation. Further, such functionality may also be disabled temporarily based on user input. For example, when the user indicates that the conversation is private, analysis of the conversation context is suspended until the user provides input for the robot to be activated. Furthermore, an indication may be provided to the participants in the conversation that the analysis function is disabled, for example, using a user interface element.
In various embodiments, the robot may be implemented in various configurations. For example, as shown in fig. 1, the robot 105 is implemented on a client device 115 a. In this example, the robot may be a module in a software application local to the client device 115 a. For example, if the user has installed a taxi calling application on client device 115a, the robotic functionality may be incorporated as a module into the taxi calling application. In this example, the user may invoke the taxi robot, for example, by sending a message "@ taxi to me find a taxi. Message application 103b may automatically launch a robotic module in the taxi calling application. In this way, the bot may be implemented locally on the client device so that the user may talk to the bot via the messaging application 103.
In another example shown in fig. 1, robot 107a is shown implemented on client device 115a and robot 107b is shown implemented on message server 101. In this example, the robot may be implemented as, for example, a client-server computer program having therein a part of the robot functions provided by each of the robot 107a (server module) and the robot 107b (client module). For example, if the robot is a scheduling robot with a handle @ calendar, the user 125a may schedule the reminder by typing "@ calendar to remind me to go to clothes at night", which may be handled by the robot 107b (client module). Continuing with this example, if the user 125a tells the robot "check if Jim is available at 4 points," the robot 107a (server module) can contact the user Jim (or Jim's scheduling robot) to exchange messages and provide a response to the user 125.
In another example, robot 109a (server module) is implemented on server 135 and robot 109b (client module) is implemented on client device 115. In this example, the robotic functionality is provided by modules implemented on the client device 115 and the server 135, the server 135 being different from the message server 101. In some implementations, the robot may be implemented as a distributed application, for example, having modules distributed across multiple client devices and servers (e.g., client device 115, server 135, message server 101, etc.). In some implementations, robots may be implemented as server applications, such as robot 111 implemented on message server 101 and robot 113 implemented on server 135.
Different implementations, such as client only, server only, client-server, distributed, etc., may provide different advantages. For example, client-only implementations allow for providing robotic functionality locally, e.g., without network access, which may be advantageous in certain contexts, e.g., when a user is outside of a network coverage area or in any area with low or limited network bandwidth. Embodiments such as server-only, client-server, or distributed configurations that include one or more servers may allow certain functions, e.g., financial transactions, ticket reservations, etc., that may not be provided locally on the client device.
Although fig. 1 shows a different robot than the messaging application 103, in some embodiments, one or more robots may be implemented as part of the messaging application 103. In embodiments where the bot is implemented as part of the messaging application 103, the user permissions are obtained prior to implementing the bot. For example, where the robots are implemented as part of the message application 103a/103b, the message application 103a/103b may provide robots that can perform certain activities, e.g., a translation robot that translates incoming and outgoing messages, a scheduling robot that schedules events on a user's calendar, etc. In this example, the translation robot is activated only under certain rights of the user. If the user does not agree, the robots within the messaging applications 103a/103b are not implemented (e.g., disabled, removed, etc.). If the user agrees, the bot or messaging application 103a/103b may have limited access to messages exchanged between users via the messaging application 103a/103b to provide specific functionality, such as translation, etc.
In some implementations, a third party, different from the provider of the message application 103a/103b and the user 125, may provide a robot for a particular purpose that is capable of communicating with the user 125 via the message application 103a/103 b. For example, a taxi service provider may provide a taxi robot, a ticketing service may provide a robot that may subscribe to event tickets, a banking robot may provide the ability to conduct financial transactions, and so on.
When the robot is implemented via the messaging application 103, the robot is allowed to communicate with the user only when a particular user is authorized. For example, if the user invokes the robot, the robot may reply, e.g., based on the user invoking an action of the robot. In another example, the user may indicate a particular robot or a particular type of robot that may contact the user. For example, the user may allow the travel robot to communicate with her, but not provide authorization for the shopping robot. In this example, the message application 103a/103b may allow the travel robot to exchange messages with the user, but filter or reject messages from the shopping robot.
Further, to provide some functionality (e.g., order out a rental car, make a flight reservation, contact friends, etc.), the bot may request that the user allow the bot to access user data, such as location, payment information, contact listings, etc. In this case, the user is presented with the option of allowing or denying access to the robot. If the user refuses access, the robot may respond via a message, such as "sorry, i cannot book you for taxis". Furthermore, the user may provide access to the information on a limited basis, e.g., the user may only allow the taxi robot to access the current location when a particular call is made by the robot, but not otherwise. In various embodiments, the user may control the type, amount, and granularity of information that the robot may access, as well as provide the user with the ability to change these permissions at any time (e.g., via a user interface). In some implementations, the user data may be processed, for example, to remove personally identifiable information, limit the information to specific data elements, etc., before the robot can access the data. In addition, the user may control the use of user data through the messaging application 103a/103b and one or more bots. For example, a user may specify that a robot providing the capability of a financial transaction requires user authorization before the transaction is completed, e.g., the robot may send the message "movie space and star tickets are $ 12 each. Is i to continue to subscribe? The optimal price for the "or" shirt is $ 125, including freight. Do i pay a fee from a credit card with a tail number of 1235? "etc.
Embodiments described herein relate generally to messaging applications. Some embodiments may automatically analyze image content and/or user information of one or more message conversations to automatically provide message suggestions to a user within a message application. In some examples, the auto-suggestion may provide one or more appropriate responses to be selected by the user to respond in the messaging application, and/or may automatically send one or more appropriate responses on behalf of the user. In some other examples, the suggestion may automatically incorporate specific non-message functions into the message application.
The foregoing description includes techniques for providing message suggestions in response to receiving a message. The message suggestions may be provided in response to any type of media content received in the conversation. For example, such content may include decals (e.g., in chat applications), animated images (e.g., moving pictures, GIF images, etc.), video, and audio data (e.g., clips or clips). For example, various types of message suggestions may be suggested based on an analysis of the received message. For example, other responses may include suggested images, suggested decals, suggested animated images (e.g., moving pictures, GIF images, etc.), and suggested videos. To provide these suggestions, suggestion module 212 may, for example, compare the identified concepts in the received item (e.g., text, image, video, decal, animated image, etc.) to concepts in different types of responses and select an appropriate response, as described above with reference to message suggestions including text and/or message decals. In various embodiments where the user provides consent, the type of response may be selected or prioritized based on context, e.g., the same type of response as the received message type may be selected; for example, a message decal may be selected as a suggested message in response to an incoming message decal, a video may be selected as a suggested response in response to an incoming image, and so on.
Certain embodiments enable messaging with human users and/or chat robots. In some implementations, automatic message suggestions can be customized based on whether the chat bot is engaged in a message conversation. In some examples, a first set of automated message suggestions may be provided if a chat bot is not present in the message conversation, and a second set of automated message suggestions may be provided if a chat bot is present in the message conversation, wherein the first and second sets of responses are at least partially different. For example, these embodiments may employ conversation rules that chat robots follow and suggest messages to users based on the rules. This may alleviate challenges that users may encounter when communicating with chat robots in languages and forms that are easily understood by chat robots.
Some implementations may include determining one or more trend responses (e.g., message responses that include popular message content sent by many different users) based on other messages in at least one of the region, market, and country associated with the user's location. The one or more determined message suggestions may include one or more trend responses. In some implementations, a user context, e.g., geographic location, holiday or event, etc., is used to generate and determine one or more message suggestions for presentation.
Determining the suggested response may be further based on developing a personalized model for the user using machine learning. Determining the suggestion response may be based on the user's preferences and/or the user's previous actions in the communication (if the user's consent to use such actions and data has been obtained). For example, the user preferences may include a white list indicating particular words that may be included and/or a black list indicating particular words that cannot be included in the message suggestion. If user consent has been obtained, a message suggestion may be generated or modified based on one or more of punctuation usage, emoji usage, or other content provided by the user at a previous occasion.
The model for providing message suggestions, e.g., a mapping model (machine learning model, grammar, association, etc.), message suggestion model, etc., may be implemented by the client device 115 and/or the message server 101. In some implementations, the conversation may be encrypted such that only client devices of participants in the conversation may access conversation content. In these embodiments, the model implemented by the respective client device may be used to provide message suggestions, and the model implemented by the server is not used. A model implemented by the client device may also be used, for example, when the user does not agree to use the server-implemented model. In some implementations, the client-implemented model may be a server-based model or a model derived from a server implementation. In some implementations, for example, when a client device lacks the ability to implement a client model, a server model may be used and the client model is not used. In some implementations, a combination of client and server models may be used.
Although the examples described herein utilize concepts shown in English, suggestions may be provided in any language, such as a language configured for client device 115, a region or other geography, a language selected based on user preferences, and so forth. In some implementations, where the user agrees to analyze the context of the conversation, the language used in the various conversations in which the user is engaged (e.g., in the most recent message) can be detected and message suggestions provided in that language.
Fig. 12 is a block diagram of an example device 1200 that can be used to implement one or more features described herein. In one example, device 1200 may be used to implement a client device, such as any of client devices 115 shown in fig. 1. Alternatively, device 1200 may implement server devices, such as message server 101, concept identifier 120, content classifier 130, and query compositor 132 of fig. 1. Device 1200 may be any suitable computer system, server, or other electronic or hardware device as described above.
One or more of the methods described herein may be run in a stand-alone program that may run on any type of computing device, a method may be run in a program that runs on a web browser, a mobile application ("app") that runs on a mobile computing device (e.g., a cell phone, a smart phone, a tablet, a wearable device (watch, arm band, jewelry, headwear, virtual reality goggles or glasses, augmented reality goggles or glasses, etc.), a notebook, etc.). In one example, a client/server architecture may be used, for example, a mobile computing device (as a client device) to send user input data to a server device and to receive final output data from the server for output (e.g., for display). In another example, all of the computations may be performed within a mobile application (and/or other applications) on the mobile computing device. In another example, the computation may be distributed between the mobile computing device and one or more server devices.
In some implementations, the device 1200 includes a processor 1202, memory 1204, and an input/output (I/O) interface 1206. The processor 1202 may be one or more processors and/or processing circuits to execute program code and to control the basic operations of the device 1200. A "processor" includes any suitable hardware and/or software system, mechanism or component that processes data, signals or other information. A processor includes a system with a general purpose Central Processing Unit (CPU), multiple processing units, dedicated circuitry for achieving functionality, or other systems. The processing need not be limited to a particular geographic location, or have time constraints. For example, a processor may perform its functions "in real time", "offline", "batch mode", etc. Portions of the processing may be performed by different (or the same) processing systems at different times and at different locations. The computer may be any processor in communication with the memory.
Memory 1204 is typically provided in device 1200 for access by processor 1202, and memory 1204 may be any suitable processor-readable storage medium, such as Random Access Memory (RAM), read Only Memory (ROM), electrically Erasable Programmable Read Only Memory (EEPROM), flash memory, etc., suitable for storing instructions for execution by processor 1202, and provided separately from processor 1202 and/or integrated with processor 1202. The memory 1204 may store software running by the processor 1202 on the server device 1200, including an operating system 1208, messaging applications 1216, and other applications 1214 such as data display engines, network hosting engines, image display engines, notification engines, social networking engines, and the like. Application data 1210 may be stored in memory 1204 and input to and/or output from message applications 1216 and/or 1214. For example, the application data may include data described herein, such as exchanged messages, images, database data, configuration data, user preferences, and the like.
In some implementations, the messaging application 1216 can include instructions that enable the processor 1202 to perform the functions described herein, for example, some or all of the method of fig. 2. For example, message application 1216 may provide message suggestions as described herein. In some implementations, the messaging application 1216 includes one or more modules, such as a decal suggestion module 1216A, a response suggestion module 1216B, a decal data module 1216C, and a user interaction module 1216D, and/or these modules are implemented in other applications or devices in communication with the device 1200. Other applications 1214 (or engines) may also or alternatively be included, such as an image editing application, a media display application, a communication application, a network hosting engine or application, and the like. One or more of the methods disclosed herein may operate in several environments and platforms, e.g., as a stand-alone computer program that may be run on any type of computing device, as a web application with web pages, as a mobile application ("app") that is run on a mobile computing device, etc. One or more of the operating system 1208 and applications 1216 and 1214 may provide a displayed user interface, for example, in response to user input, to display selectable options or controls, and display data based on the selected options.
In some implementations, the machine learning application 1230 is stored in the memory 1204. In various implementations, the machine learning application 1230 may utilize a bayesian classifier, support vector machine, neural network, or other learning technique. In some implementations, the machine learning application 1230 can include a training model 1234, an inference engine 1236, and data 1232. In some implementations, the data 1232 can include training data, e.g., data used to generate the training model 1234. For example, the training data may include any type of data, such as text, images, audio, video, and the like. The training data may be obtained from any source, e.g., data specifically labeled as a data store for training, data provided permission to use as training data for machine learning, etc. In embodiments where one or more users are allowed to train a machine learning model using their respective user data, for example, trained model 1234, the training data may include such user data. In embodiments where users allow access to their respective user data, data 1232 may include permission data such as images (e.g., photographs or other user-generated images), communications (e.g., emails; chat data such as text messages, voice, video, etc.), files (e.g., spreadsheets, text files, presentations, etc.).
In some implementations, the data 1232 can include collected data, such as map data, image data (e.g., satellite images, overhead images, etc.), game data, and the like. In some implementations, the training data may include synthetic data generated for training, such as data that is not based on user input or activity in the context being trained, e.g., data generated from a simulated dialog, computer-generated images, and so forth. In some implementations, the machine learning application 1230 excludes the data 1232. For example, in these embodiments, training model 1234 may be generated, for example, on a different device and provided as part of machine learning application 1230. In various embodiments, training model 1234 may be provided as a data file that includes model structures or forms and associated weights. Inference engine 1236 can read the data files for training model 834 and implement a neural network with node connections, layers and weights based on the model structure or form specified in training model 1234.
The machine learning application 1230 also includes a training model 1234. In some implementations, the training model may include one or more model forms or structures. For example, the model form or structure may include any type of neural network, such as a linear network, a deep neural network implementing multiple layers (e.g., a "hidden layer" between an input layer and an output layer, each layer being a linear network), a convolutional neural network (e.g., a network that partitions or blocks input data into multiple portions or blocks, each block being processed separately using one or more neural network layers, and the results from the processing of each block being aggregated), a sequence to sequence neural network (e.g., a network such as words in a sentence, frames in a video, etc., as input sequential data, and producing a sequence of results as output), and the like.
The model form or structure may specify the connections between the various nodes and the organization of the nodes into layers. For example, a node of a first layer (e.g., an input layer) may receive data as input data 1232 or application data 1210. Such data may include, for example, one or more pixels per node, such as when training a model for image analysis. The subsequent middle tier may receive as input the output of the nodes of the previous tier in terms of connectivity specified in the model form or structure. These layers may also be referred to as hidden layers. The last layer (e.g., output layer) produces the output of the machine learning application. For example, depending on the particular training model, the output may be a set of labels for the image, a representation of the image that allows the image to be compared to other images (e.g., feature vectors of the image), one or more categories of input data in response to the output sentence of the input sentence, and so forth. In some implementations, the model form or structure also specifies the number and/or type of nodes in each layer.
In various embodiments, training model 1234 may include a plurality of nodes arranged in layers according to a model structure or form. In some embodiments, the node may be a computational node without memory, e.g., configured to process an input unit to produce an output unit. The computation performed by the node may include, for example, multiplying each of the plurality of node inputs by a weight, obtaining a weighted sum, and adjusting the weighted sum with a bias or intercept value (intercept value) to produce a node output. In some implementations, the computation by the node may further include applying a ladder/activation function to the adjusted weighted sum. In some implementations, the step/activate function may be a nonlinear function. In various embodiments, such computation may include operations such as matrix multiplication. In some implementations, the computation of the plurality of nodes may be performed in parallel, e.g., using multiple processor cores of a multi-core processor, using separate processing units of a GPU, or dedicated neural circuits. In some implementations, a node may include memory, for example, may be able to store and use one or more earlier inputs when processing subsequent inputs. For example, the node with memory may include a Long Short Term Memory (LSTM) node. LSTM nodes may use memory to maintain a "state" that allows the node to behave as a Finite State Machine (FSM). Models with such nodes may be used to process sequential data, such as words in sentences or paragraphs, frames in video, speech or other audio, and so forth.
In some implementations, training model 1234 may include embeddings or weights for the various nodes. For example, a model may be initiated as a plurality of nodes organized into layers as specified by the model form or structure. At initialization, various weights may be applied to the connections between each pair of nodes connected in a model form, e.g., nodes in successive layers of a neural network. For example, the weights may be randomly assigned or initialized to default values. The model may then be trained, for example, using data 1232, to produce results.
For example, training may include employing supervised learning techniques. In supervised learning, training data may include a plurality of inputs (e.g., a set of images) and a corresponding expected output for each input (e.g., one or more labels for each image). The value of the weight is automatically adjusted based on a comparison of the output of the model to the expected output, e.g., in a manner that increases the probability that the model will produce the expected output when providing similar inputs.
In some implementations, training may include employing unsupervised learning techniques. In unsupervised learning, only input data may be provided and the model trained to distinguish the data, e.g., cluster the input data into groups, where each group includes input data that are similar in some way. For example, the model may be trained to distinguish images such that the model distinguishes abstract images (e.g., composite images, human drawn images, etc.) from natural images (e.g., photographs).
In another example, a model trained using unsupervised learning may cluster words based on the use of words in the input sentence. In some implementations, the knowledge representation may be generated using unsupervised learning, e.g., a knowledge representation that may be used by the machine learning application 1230. In various embodiments, the training model includes a set of weights or embeddings corresponding to the model structure. In embodiments where the data 1232 is omitted, the machine learning application 1230 may include a training model 1234 that is based on, for example, previous training by a developer of the machine learning application 1230, a third party, or the like. In some implementations, training model 1234 may include, for example, a fixed set of weights downloaded from a server that provides the weights.
The machine learning application 1230 also includes an inference engine 1236. The inference engine 1236 is configured to apply a training model 1234 to data, such as application data 1210, to provide inferences. In some implementations, the inference engine 1236 can include software code to be executed by the processor 1202. In some implementations, the inference engine 1236 can specify a circuit configuration (e.g., for a programmable processor, for a Field Programmable Gate Array (FPGA), etc.) so that the processor 1202 can apply the training model. In various embodiments, inference engine 1236 comprises software instructions, hardware instructions, or a combination thereof. In some implementations, the inference engine 1236 provides an Application Programming Interface (API) to, for example, apply the training model 1234 to the application data 1210 to generate inferences that can be used by the operating system 1208 and/or other applications 1214 and/or 1216 to invoke the inference engine 1236.
The machine learning application 1230 may provide several technical advantages. For example, when training model 1234 is generated based on unsupervised learning, training model 1234 may be applied by inference engine 1236 to generate a knowledge representation (e.g., a digital representation) from input data, such as application data 1210. For example, a model trained for image analysis may produce a representation of an image having a smaller data size (e.g., 1 KB) than the input image (e.g., 10 MB). In some implementations, such representations may help reduce processing costs (e.g., computational costs, memory usage, etc.) of generating output (e.g., tags, classifications, sentences describing images, etc.). In some implementations, such representations can be provided as inputs to different machine learning applications that produce output from the output of the inference engine 1236. In some implementations, the knowledge representation generated by the machine learning application 1230 can be provided to a different device for further processing, e.g., over a network. In such embodiments, providing a knowledge representation rather than an image may provide technical benefits, e.g., faster data transfer at lower cost. In another example, a model trained to cluster documents may generate document clusters from input documents. Document clustering (clusters) may be suitable for further processing (e.g., determining whether a document is related to a topic, determining a classification category of a document, etc.) without accessing the original document, thus saving computational costs.
In some implementations, the machine learning application 1230 can be implemented in an offline manner. In these embodiments, training model 1234 may be generated in a first stage and provided as part of machine learning application 1230. In some implementations, the machine learning application 1230 can be implemented in an online manner. For example, in such embodiments, an application (e.g., operating system 1208, one or more other applications 1214, etc.) invoking the machine learning application 1230 may utilize the inferences made by the machine learning application 1230, e.g., to provide the inferences to the user, and may generate a system log (e.g., actions taken by the user based on the inferences if allowed by the user; or results of further processing if used as input for further processing). The system log may be generated periodically, e.g., hourly, monthly, quarterly, etc., and may be used to update training model 1234, e.g., update the embedding of training model 1234, if permitted by the user.
In some implementations, the machine learning application 1230 may be implemented in a manner that can accommodate the particular configuration of the device 1200 on which the machine learning application 1230 is to be executed. For example, the machine learning application 1230 may determine a computational graph that utilizes available computational resources, such as the processor 1202. For example, if the machine learning application 1230 is implemented as a distributed application across multiple devices, the machine learning application 1230 may determine the computations to be performed on the respective devices in a manner that optimizes the computations. In another example, the machine learning application 1230 may determine that the processor 1202 includes a GPU having a particular number of GPU cores (e.g., 1,000) and implement the inference engine accordingly (e.g., as 1,000 separate processes or threads).
In some implementations, the machine learning application 1230 can implement a combination of training models. For example, training model 1234 may include multiple training models, each adapted to the same input data. In these embodiments, the machine learning application 1230 may select a particular training model, for example, based on available computing resources, previously inferred success rates, and the like. In some implementations, the machine learning application 1230 can execute the inference engine 1236 such that a plurality of training models are applied. In these embodiments, the machine learning application 1230 may combine the outputs from the individual models being applied, for example, using voting techniques that score the individual outputs from each training model being applied, or by selecting one or more particular outputs. Further, in these embodiments, the machine learning application may apply a time threshold (e.g., 0.5 ms) to the individual training models being applied, and utilize only those individual outputs that are available within the time threshold. The output that is not received within the time threshold may not be utilized, e.g., discarded. Such an approach may be appropriate when time constraints are specified, for example, while invoking a machine learning application, for example, via the operating system 1208 or one or more applications 1214 and/or 1216.
In different implementations, the machine learning application 1230 may generate different types of output. For example, the machine learning application 1230 may provide representations or clusters (e.g., digital representations of input data), tags (e.g., for input data including images, documents, etc.), phrases or sentences (e.g., describing images or videos, suitable for use as responses to input sentences, etc.), images (e.g., generated by the machine learning application in response to input), audio or video (e.g., in response to input video, the machine learning application 1230 may generate output video with particular effects, e.g., when training the training model 1234 using training data from a comic book or particular artist, etc., then output video rendered in the style of the comic book or particular artist). In some implementations, the machine learning application 1230 can generate output based on a format specified by, for example, the operating system 1208 or one or more applications 1214 and/or 1216 invoking the application. In some implementations, the calling application may be another machine learning application. For example, such a configuration may be used in generating an antagonism network (generative adversarial networks) in which output from the machine learning application 1230 is used to train a calling machine learning application and vice versa.
Any software in the memory 1204 may alternatively be stored on any other suitable storage location or computer readable medium. In addition, the memory 1204 (and/or other connected storage devices) may store one or more messages, message decals and decal information (e.g., metadata), message decal descriptions and keywords, one or more taxonomies, electronic encyclopedias, dictionaries, synonym dictionaries, message data, grammars, user preferences, and/or other instructions and data used in the features described herein (e.g., such instructions and/or data may be included in the application data 1210 in some embodiments). Memory 1204 and any other type of storage (magnetic disk, optical disk, magnetic tape, or other tangible medium) may be considered a "memory" or "storage device.
The I/O interface 1206 may provide functionality to interface the server device 1200 with other systems and devices. The engaged devices may be included as part of device 1200 or may be separate from and in communication with device 1200. For example, network communication devices, storage devices (e.g., memory and/or database 106), and input/output devices may communicate via I/O interface 1206. In some implementations, the I/O interface may be connected to interface devices such as input devices (keyboard, pointing device, touch screen, microphone, camera, scanner, sensor, etc.) and/or output devices (display device, speaker device, printer, motor, etc.).
Some examples of interface devices that may be connected to I/O interface 1206 may include a display device 1220, which display device 1220 may be used to display, for example, images, content of video, and/or a user interface of an output application as described herein. The display device 1220 may be connected to the device 1200 via a local connection (e.g., a display bus) and/or via a network connection, and may be any suitable display device. Display device 1220 may include any suitable display device, such as an LCD, LED or plasma display screen, CRT, television, monitor, touch screen, 3D display screen, or other visual display device. For example, the display device 1220 may be a flat display screen provided on a mobile device, a plurality of display screens provided in a goggle device, or a monitor screen for a computer device.
The I/O interface 1206 may be engaged with other input and output devices. Some examples include one or more cameras that may capture images. Some embodiments may provide a microphone for capturing sound (e.g., as part of capturing an image, voice commands, etc.), an audio speaker device for outputting sound, or other input and output devices.
For ease of illustration, FIG. 12 shows one block for each of the processor 1202, memory 1204, I/O interface 1206, and software blocks 1208,1214 and 1216. These blocks may represent one or more processors or processing circuits, operating systems, memory, I/O interfaces, applications, and/or software modules. In other embodiments, device 1200 may not have all of the components shown and/or may have other elements including other types of elements, which may be substituted for or include other elements in addition to those shown herein. Although some components are described as performing the blocks and operations as described in some embodiments herein, the environment 100, the device 1200, any suitable component or combination of components of a similar system, or any suitable processor or processors associated with such a system, may perform the described blocks and operations.
The methods described herein may be implemented by computer program instructions or code that may be executed on a computer. For example, the code may be implemented by one or more digital processors (e.g., microprocessors or other processing circuits), and may be stored on a computer program product comprising a non-transitory computer readable medium (e.g., a storage medium) including, for example, magnetic, optical, electromagnetic, or semiconductor storage media, including semiconductor or solid state memory, magnetic tape, removable computer diskette, random Access Memory (RAM), read-only memory (ROM), flash memory, rigid magnetic disk, optical disk, solid state memory drive, and the like. The program instructions may also be embodied in and provided as electronic signals, for example, in the form of software as a service (SaaS) delivered from a server (e.g., a distributed system and/or cloud computing system). Alternatively, one or more of the methods may be implemented in hardware (logic gates, etc.) or a combination of hardware and software. Example hardware may be a programmable processor (e.g., a Field Programmable Gate Array (FPGA), a complex programmable logic device), a general purpose processor, a graphics processor, an Application Specific Integrated Circuit (ASIC), etc. One or more methods may be performed as part of or a component of an application running on a system, or as an application or software running with other applications and operating systems.
Other embodiments are summarized in the examples below.
Example 1: a computer-implemented method for providing message suggestions in a message application, the method comprising detecting a first message sent by a first user device to a second user device over a communication network; programmatically analyzing the first message to determine semantic concepts associated with the first message; identifying one or more message stickers based at least in part on the semantic concepts; and sending instructions to cause the one or more message stickers to be displayed in a user interface, the user interface being displayed on the second user device.
Example 2: the computer-implemented method of example 1, wherein the first message is part of a communication in the messaging application between a first user of the first user device and a second user of the second user device, the method further comprising: the communication is programmatically analyzed to determine one or more additional semantic concepts associated with the communication, wherein identifying the one or more message stickers is further based on the one or more additional semantic concepts.
Example 3: the computer-implemented method of any of examples 1 and 2, wherein identifying the one or more message stickers includes determining one or more suggested responses based on the semantic concepts; comparing one or more descriptors associated with the plurality of message decals to the one or more suggested responses; and selecting the one or more message stickers from the plurality of message stickers based on the comparison.
Example 4: the computer-implemented method of example 3, wherein the one or more message stickers comprise a plurality of message stickers, and further comprising determining respective ranks of the plurality of message stickers based on one or more correspondences between descriptions of each of the plurality of message stickers and the one or more suggested responses, wherein sending instructions to cause the plurality of message stickers to be displayed comprises sending instructions to indicate the respective ranks of the plurality of message stickers.
Example 5: the computer-implemented method of any of examples 3 and 4, wherein selecting the one or more message stickers from the plurality of message stickers based on the comparison comprises: checking correspondence between the one or more descriptors and the one or more suggested responses, wherein the correspondence includes at least one of: letter matches between the words of the one or more descriptors and the words of the one or more suggested responses, and semantic similarities between the one or more descriptors and the one or more suggested responses; and determining that the one or more message stickers have the correspondence between the one or more descriptors and the one or more suggested responses.
Example 6: the computer-implemented method of any of examples 1-5, wherein selecting the one or more message stickers from the plurality of message stickers based on the comparison comprises: determining a similarity score between the one or more descriptors and the one or more suggested responses; and selecting the one or more message stickers based on the similarity scores of the one or more message stickers.
Example 7: the computer-implemented method of any of examples 1-6, wherein at least one of the one or more message stickers includes image data to be displayed and a sticker Identification (ID) that effectively identifies the at least one message sticker.
Example 8: the computer-implemented method of any of examples 1-7, further comprising: receiving a selection by user input of a particular message decal of the one or more message decals; and in response to receiving the selection, providing the particular message decal to the first user device in the message application, wherein providing the particular message decal comprises one or more of: transmitting the sticker ID of the message sticker to the first user equipment through the communication network; and transmitting the image data of the message decal to the first user device via the communication network.
Example 9: the computer-implemented method of any of examples 1-8, wherein identifying the one or more message stickers includes determining that the first message is part of a conversation in the message application between the first user device and the second user device, the method further comprising: the one or more message stickers are identified based at least in part on one or more semantic concepts in one or more messages previously received in the conversation.
Example 10: a computer-implemented method for providing message suggestions in a messaging application, the method comprising: detecting a first message sent by first user equipment to second user equipment through a communication network, wherein the first message comprises a message sticker; programmatically analyzing the message decal to determine semantic concepts associated with the first message; determining one or more suggested responses based at least in part on the semantic concepts; and transmitting instructions to cause the one or more suggested responses to be displayed by the second user device.
Example 11: the computer-implemented method of example 10, wherein the one or more suggested responses comprise at least one suggested message decal response comprising a message decal.
Example 12: the computer-implemented method of example 11, wherein determining the one or more suggested responses further comprises: comparing one or more descriptors associated with the plurality of message decals to the one or more suggested responses; and selecting the at least one suggested message decal response from the plurality of message decals based on the comparison.
Example 13: the computer-implemented method of any of examples 11-12, further comprising: determining that the message decal is stored on the second user device.
Example 14: the computer-implemented method of any of examples 10-13, wherein the method further comprises: receiving a selection of at least one suggestion response of the one or more suggestion responses based on the received user input to the second user device; and in response to receiving the selection, transmitting the at least one suggestion response to the first user device over the communication network.
Example 15: the computer-implemented method of any of examples 10-14, wherein the message decal is associated with image data and a decal Identification (ID).
Example 16: a system for providing message suggestions in a messaging application, comprising:
A memory; and at least one processor for accessing the memory, and for performing operations comprising: receiving, at a second user equipment, a first message sent by a first user equipment through a communication network; obtaining a suggested response associated with the first message, wherein the suggested response is based on semantic concepts determined by programmatically analyzing the first message; based at least in part on the suggested response, identifying one or more message stickers, wherein the one or more message stickers are stored on the second user device; and causing the one or more message stickers to be displayed in a user interface, the user interface being displayed on the second user device.
Example 17: the system of example 16, wherein obtaining the suggested response comprises: the suggested responses are received from a server device, wherein the server device programmatically analyzes the first message to determine the semantic concepts, and determines the suggested responses based on a mapping from the semantic concepts to a library of stored suggested responses.
Example 18: the system of any of examples 16-17, wherein the at least one processor is further configured to: obtaining a plurality of semantic concepts associated with a plurality of message stickers stored on the second user device; and comparing the plurality of semantic concepts to the suggested response, wherein identifying the one or more message stickers based at least in part on the suggested response comprises selecting the one or more message stickers from the plurality of semantic concepts based on the comparison.
Example 19: the system of any of examples 16-18, wherein the at least one processor further comprises: receiving a selection of at least one suggestion response of the one or more suggestion responses based on the received user input to the second user device; and in response to receiving the selection, transmitting the at least one suggestion response to the first user device over the communication network.
Example 20: the system of any of examples 16-19, wherein at least one message decal of the one or more message decals includes image data to be displayed and a decal Identification (ID) identifying the at least one message decal.
Although the specification has described specific embodiments with respect to the specification, these specific embodiments are illustrative only and not limiting. The concepts shown in the examples may be applied to other examples and implementations.
In the case where certain embodiments discussed herein may collect or use personal information about a user (e.g., user data, information about a user's social network, user's location and time, user's biometric information, user's activities and demographic information), the user is provided with one or more mechanisms to control whether personal information is collected, whether personal information is stored, whether personal information is used, and how information about the user, storage, and use is collected. That is, the systems and methods discussed herein collect, store, and/or use user personal information upon receipt of explicit authorization of the relevant user. In addition, certain data may be processed in one or more ways prior to storage or use to remove personally identifiable information. As one example, the identity of the user may be processed so that personally identifiable information cannot be determined. As another example, the geographic location of a user may be generalized over a larger area such that the user's specificity cannot be determined.
Note that the functional blocks, operations, features, methods, devices, and systems described in this disclosure may be integrated or partitioned into different combinations of systems, devices, and functional blocks as known to those of skill in the art. The routines of the particular embodiments may be implemented using any suitable programming language and programming technique. Different programming techniques may be employed, such as procedural or object oriented. The routines may execute on a single processing device or multiple processors. Although steps, operations or computations may be presented in a specific order, the order may be changed in different specific implementations. In some embodiments, a plurality of steps or operations shown in sequence in the present specification may be performed simultaneously.
Claims (14)
1. A method of providing a suggestion response in a messaging application, the method comprising:
receiving, by the first user device and using the message application, a message;
determining one or more suggested responses using a machine learning model;
determining a plurality of message stickers based on at least one of the one or more suggested responses; and
at least one message decal of the plurality of message decals is displayed by the first user device.
2. The method of claim 1, wherein determining one or more suggested responses using a machine learning model comprises,
the machine learning model is applied to the message by the first user device, wherein the machine learning model generates the one or more suggested responses.
3. The method of claim 1, wherein determining one or more suggested responses using a machine learning model comprises,
the one or more suggested responses are received from a server, wherein the machine learning model is applied to the message to generate the one or more suggested responses.
4. The method of claim 1, wherein the message comprises a message decal, the method further comprising programmatically analyzing the message to determine semantic concepts associated with the message,
wherein determining one or more suggested responses using the machine learning model includes,
providing the semantic concepts as input to the machine learning model; and
the one or more suggestion responses are received in response to providing the semantic concepts as the input.
5. The method of claim 1, wherein the step of determining the position of the substrate comprises,
Comparing one or more descriptors associated with the message decal to the one or more suggested responses;
selecting the plurality of message decals from the message decals based on the comparison; and
determining a ranking of the plurality of message stickers based on the ranking of the one or more suggested responses,
wherein displaying at least one message decal of the plurality of message decals comprises displaying two or more message decals of the plurality of message decals based on the ranking of the message decals.
6. The method of claim 1, wherein the one or more suggested responses comprise one or more of a text response, an audio response, a picture response, and a video response.
7. The method of claim 1, wherein the machine learning model is part of generating an antagonism network.
8. The method of claim 1, wherein the machine learning model is a graph-based machine learning model.
9. The method of claim 1, wherein the message comprises text in a language other than english, and determining one or more suggested responses comprises:
Translating the text from the language other than english to english; and
providing an english translation of the text to the machine learning model, wherein the machine learning model generates the one or more suggestion responses based on the english translation of the text.
10. The method of claim 1, wherein the machine learning model is customized for the user of the first user device based on content provided by the user on a previous occasion.
11. The method of claim 1, wherein the machine learning application executing at the first user device comprises the machine learning model, the method further comprising:
a computational graph is determined by the first user device and using the machine learning application, the computational graph utilizing available computational resources of the first user device.
12. The method of claim 1, wherein the machine learning model comprises a plurality of machine learning models, the method further comprising:
applying the plurality of machine learning models to the message; and
outputs from each of the plurality of machine learning models are combined to generate the one or more suggested responses.
13. A system for providing a suggested response based on a message, characterized in that the system comprises means for performing the method of any of claims 1-12.
14. A non-transitory computer-readable storage medium storing instructions that, when executed, cause at least one processor of a computing device to perform the method of any of claims 1-12.
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201662397316P | 2016-09-20 | 2016-09-20 | |
US62/397,316 | 2016-09-20 | ||
PCT/US2017/052349 WO2018057541A1 (en) | 2016-09-20 | 2017-09-19 | Suggested responses based on message stickers |
CN201780058037.1A CN109952572B (en) | 2016-09-20 | 2017-09-19 | Suggested response based on message decal |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201780058037.1A Division CN109952572B (en) | 2016-09-20 | 2017-09-19 | Suggested response based on message decal |
Publications (1)
Publication Number | Publication Date |
---|---|
CN117634495A true CN117634495A (en) | 2024-03-01 |
Family
ID=60022182
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202311382957.2A Pending CN117634495A (en) | 2016-09-20 | 2017-09-19 | Suggested response based on message decal |
CN201780058037.1A Active CN109952572B (en) | 2016-09-20 | 2017-09-19 | Suggested response based on message decal |
Family Applications After (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN201780058037.1A Active CN109952572B (en) | 2016-09-20 | 2017-09-19 | Suggested response based on message decal |
Country Status (3)
Country | Link |
---|---|
US (3) | US10547574B2 (en) |
CN (2) | CN117634495A (en) |
WO (1) | WO2018057541A1 (en) |
Families Citing this family (101)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8977255B2 (en) | 2007-04-03 | 2015-03-10 | Apple Inc. | Method and system for operating a multi-function portable electronic device using voice-activation |
EP3395019B1 (en) | 2015-12-21 | 2022-03-30 | Google LLC | Automatic suggestions and other content for messaging applications |
CN108781175B (en) | 2015-12-21 | 2021-09-21 | 谷歌有限责任公司 | Method, medium, and system for automatic suggestion of message exchange contexts |
WO2018017628A1 (en) * | 2016-07-19 | 2018-01-25 | American Innovative Application Corporation | Multi-functional integrated communications system application |
US10387461B2 (en) | 2016-08-16 | 2019-08-20 | Google Llc | Techniques for suggesting electronic messages based on user activity and other context |
CN117634495A (en) | 2016-09-20 | 2024-03-01 | 谷歌有限责任公司 | Suggested response based on message decal |
US10015124B2 (en) | 2016-09-20 | 2018-07-03 | Google Llc | Automatic response suggestions based on images received in messaging applications |
US10511450B2 (en) | 2016-09-20 | 2019-12-17 | Google Llc | Bot permissions |
WO2018057627A1 (en) * | 2016-09-20 | 2018-03-29 | Google Llc | System and method for transmitting a response in a messaging application |
US10867241B1 (en) * | 2016-09-26 | 2020-12-15 | Clarifai, Inc. | Systems and methods for cooperative machine learning across multiple client computing platforms and the cloud enabling off-line deep neural network operations on client computing platforms |
US10361975B2 (en) * | 2016-10-10 | 2019-07-23 | Microsoft Technology Licensing, Llc | Messaging bot selection in multi-bot chat sessions |
US10416846B2 (en) | 2016-11-12 | 2019-09-17 | Google Llc | Determining graphical element(s) for inclusion in an electronic communication |
CN108075959B (en) * | 2016-11-14 | 2021-03-12 | 腾讯科技（深圳）有限公司 | Session message processing method and device |
US20180143973A1 (en) * | 2016-11-23 | 2018-05-24 | Mh Sub I, Llc | Semi-automated form-based chat |
US10831932B2 (en) * | 2017-01-31 | 2020-11-10 | Telenav, Inc. | Computing system with a presentation mechanism and method of operation thereof |
US11494440B1 (en) | 2017-04-12 | 2022-11-08 | Meta Platforms, Inc. | Proactive and reactive suggestions for a messaging system |
US10846615B2 (en) | 2017-04-12 | 2020-11-24 | Facebook, Inc. | Techniques for reinforcement for bots using capability catalogs |
US11341173B2 (en) | 2017-04-12 | 2022-05-24 | Meta Platforms, Inc. | Techniques for personalized search for bots |
US11025566B2 (en) | 2017-04-12 | 2021-06-01 | Facebook, Inc. | Techniques for intent-based search for bots |
US20180302345A1 (en) * | 2017-04-12 | 2018-10-18 | Facebook, Inc. | Techniques for event-based recommendations for bots |
US10333868B2 (en) * | 2017-04-14 | 2019-06-25 | Facebook, Inc. | Techniques to automate bot creation for web pages |
DK179496B1 (en) | 2017-05-12 | 2019-01-15 | Apple Inc. | USER-SPECIFIC Acoustic Models |
US10860854B2 (en) | 2017-05-16 | 2020-12-08 | Google Llc | Suggested actions for images |
US10402647B2 (en) * | 2017-05-17 | 2019-09-03 | Microsoft Technology Licensing, Llc | Adapted user interface for surfacing contextual analysis of content |
WO2018212599A1 (en) * | 2017-05-17 | 2018-11-22 | Samsung Electronics Co., Ltd. | Super-resolution processing method for moving image and image processing apparatus therefor |
US10404636B2 (en) | 2017-06-15 | 2019-09-03 | Google Llc | Embedded programs and interfaces for chat conversations |
US10348658B2 (en) * | 2017-06-15 | 2019-07-09 | Google Llc | Suggested items for use with embedded applications in chat conversations |
CN109388297B (en) * | 2017-08-10 | 2021-10-22 | 腾讯科技（深圳）有限公司 | Expression display method and device, computer readable storage medium and terminal |
US20190056841A1 (en) * | 2017-08-18 | 2019-02-21 | Ante Diem, Inc. d/b/a SportsManias | Apparatus, method, and computer-readable storage medium for mobile keyboard content delivery |
US10951558B2 (en) * | 2017-09-27 | 2021-03-16 | Slack Technologies, Inc. | Validating application dialog associated with a triggering event identification within user interaction data received via a group-based communication interface |
US10593087B2 (en) | 2017-10-23 | 2020-03-17 | Paypal, Inc. | System and method for generating emoji mashups with machine learning |
US11145103B2 (en) * | 2017-10-23 | 2021-10-12 | Paypal, Inc. | System and method for generating animated emoji mashups |
US20190140990A1 (en) * | 2017-11-03 | 2019-05-09 | Gfycat, Inc. | Generating a selectable response to an electronic message |
US11050700B2 (en) * | 2017-11-03 | 2021-06-29 | Salesforce.Com, Inc. | Action response selection based on communication message analysis |
EP3486850A1 (en) * | 2017-11-17 | 2019-05-22 | Orange | Method for generating an sms message and equipment for implementing the method |
CN110019722B (en) * | 2017-12-21 | 2023-11-24 | 株式会社理光 | Method and device for ordering replies of dialogue model and computer readable storage medium |
EP4239498A3 (en) | 2017-12-22 | 2023-10-04 | Google LLC | Image selection suggestions |
US11209442B2 (en) | 2017-12-22 | 2021-12-28 | Google Llc | Image selection suggestions |
US10891526B2 (en) | 2017-12-22 | 2021-01-12 | Google Llc | Functional image archiving |
US10664512B1 (en) * | 2018-02-13 | 2020-05-26 | Snap Inc. | Query matching to media collections in a messaging system |
US11240180B2 (en) * | 2018-03-20 | 2022-02-01 | Fujifilm Business Innovation Corp. | Message providing device and non-transitory computer readable medium |
US11310176B2 (en) * | 2018-04-13 | 2022-04-19 | Snap Inc. | Content suggestion system |
US11288299B2 (en) * | 2018-04-24 | 2022-03-29 | International Business Machines Corporation | Enhanced action fulfillment using classification valency |
WO2019230439A1 (en) * | 2018-05-31 | 2019-12-05 | ソニーセミコンダクタソリューションズ株式会社 | Information processing device, information processing method, transmitter, transmission method, receiver, and reception method |
US11556897B2 (en) | 2018-05-31 | 2023-01-17 | Microsoft Technology Licensing, Llc | Job-post budget recommendation based on performance |
US11188194B2 (en) | 2018-06-27 | 2021-11-30 | Microsoft Technology Licensing, Llc | Personalization and synonym hierarchy for smart replies |
US11062084B2 (en) * | 2018-06-27 | 2021-07-13 | Microsoft Technology Licensing, Llc | Generating diverse smart replies using synonym hierarchy |
US11658926B2 (en) * | 2018-06-27 | 2023-05-23 | Microsoft Technology Licensing, Llc | Generating smart replies involving image files |
US11089147B2 (en) * | 2018-06-29 | 2021-08-10 | Google Llc | Systems, devices, and methods for generating messages |
US11887014B2 (en) * | 2018-07-27 | 2024-01-30 | Sap Se | Dynamic question recommendation |
US10768953B2 (en) * | 2018-08-07 | 2020-09-08 | Citrix Systems, Inc. | Computing system providing suggested actions within a shared application platform and related methods |
US11157694B2 (en) * | 2018-08-14 | 2021-10-26 | Snap Inc. | Content suggestion system |
US11386304B2 (en) | 2018-08-20 | 2022-07-12 | Samsung Electronics Co., Ltd. | Electronic device and method of controlling the same |
KR102481910B1 (en) * | 2018-08-31 | 2022-12-27 | 구글 엘엘씨 | Animation image positioning method and system in dynamic keyboard interface |
US20200162341A1 (en) * | 2018-11-20 | 2020-05-21 | Cisco Technology, Inc. | Peer comparison by a network assurance service using network entity clusters |
US11163843B1 (en) * | 2018-12-28 | 2021-11-02 | Facebook, Inc. | Systems and methods for recommending content |
WO2020148689A1 (en) * | 2019-01-16 | 2020-07-23 | Hike Private Limited | System and method for recommending stickers in a social networking application |
JP7104277B2 (en) * | 2019-03-29 | 2022-07-21 | 株式会社Aill | Communication support server, communication support system, communication support method, and communication support program |
US11113475B2 (en) * | 2019-04-15 | 2021-09-07 | Accenture Global Solutions Limited | Chatbot generator platform |
US11140099B2 (en) * | 2019-05-21 | 2021-10-05 | Apple Inc. | Providing message response suggestions |
US11238221B2 (en) * | 2019-06-19 | 2022-02-01 | Microsoft Technology Licensing, Llc | Language profiling service |
US20210390553A1 (en) * | 2019-06-23 | 2021-12-16 | Litlingo Technologies Inc. | Method for recommending and implementing communication optimizations |
US20210004832A1 (en) * | 2019-07-05 | 2021-01-07 | Talkdesk, Inc. | System and method for escalation using agent assist within a cloud-based contact center |
WO2021006906A1 (en) * | 2019-07-11 | 2021-01-14 | Google Llc | System and method for providing an artificial intelligence control surface for a user of a computing device |
WO2021021012A1 (en) * | 2019-07-29 | 2021-02-04 | Ai Robotics Limited | Stickering method and system for linking contextual text elements to actions |
US11308110B2 (en) * | 2019-08-15 | 2022-04-19 | Rovi Guides, Inc. | Systems and methods for pushing content |
US11544333B2 (en) * | 2019-08-26 | 2023-01-03 | Adobe Inc. | Analytics system onboarding of web content |
JP7410613B2 (en) * | 2019-08-27 | 2024-01-10 | キヤノン株式会社 | Information processing device and its control method and program |
US11252274B2 (en) * | 2019-09-30 | 2022-02-15 | Snap Inc. | Messaging application sticker extensions |
US11082375B2 (en) * | 2019-10-02 | 2021-08-03 | Sap Se | Object replication inside collaboration systems |
US20210117882A1 (en) | 2019-10-16 | 2021-04-22 | Talkdesk, Inc | Systems and methods for workforce management system deployment |
US11880484B2 (en) * | 2019-11-12 | 2024-01-23 | Salesforce, Inc. | Enforcing data isolation in jobs executed by a multi-tenant system on a secondary platform |
US11063891B2 (en) * | 2019-12-03 | 2021-07-13 | Snap Inc. | Personalized avatar notification |
CN113032592A (en) * | 2019-12-24 | 2021-06-25 | 徐大祥 | Electronic dynamic calendar system, operating method and computer storage medium |
US10841251B1 (en) * | 2020-02-11 | 2020-11-17 | Moveworks, Inc. | Multi-domain chatbot |
US11675494B2 (en) * | 2020-03-26 | 2023-06-13 | Snap Inc. | Combining first user interface content into second user interface |
US11818286B2 (en) | 2020-03-30 | 2023-11-14 | Snap Inc. | Avatar recommendation and reply |
US11625873B2 (en) | 2020-03-30 | 2023-04-11 | Snap Inc. | Personalized media overlay recommendation |
US11151195B1 (en) * | 2020-05-30 | 2021-10-19 | CareerAmerica, LLC | Method and system for predicative QandA and resource suggestions |
US11757999B1 (en) | 2020-06-02 | 2023-09-12 | State Farm Mutual Automobile Insurance Company | Thick client and common queuing framework for contact center environment |
CN113760416A (en) * | 2020-06-04 | 2021-12-07 | 支付宝实验室(新加坡)有限公司 | Information display method, device and equipment |
US11209964B1 (en) * | 2020-06-05 | 2021-12-28 | SlackTechnologies, LLC | System and method for reacting to messages |
US11502983B2 (en) * | 2020-06-08 | 2022-11-15 | Snap Inc. | Reply interface with selectable stickers for messaging system |
US20220269354A1 (en) * | 2020-06-19 | 2022-08-25 | Talent Unlimited Online Services Private Limited | Artificial intelligence-based system and method for dynamically predicting and suggesting emojis for messages |
US11671388B1 (en) * | 2020-07-16 | 2023-06-06 | State Farm Mutual Automobile Insurance Company | Contact center messaging |
US11895271B1 (en) | 2020-07-31 | 2024-02-06 | State Farm Mutual Automobile Insurance Company | Representative client devices in a contact center environment |
CN111898752A (en) * | 2020-08-03 | 2020-11-06 | 乐鑫信息科技（上海）股份有限公司 | Apparatus and method for performing LSTM neural network operations |
US20220350825A1 (en) * | 2020-11-06 | 2022-11-03 | Khoros, Llc | Automated response engine to implement internal communication interaction data via a secured omnichannel electronic data channel and external communication interaction data |
CN112416203A (en) * | 2020-11-26 | 2021-02-26 | 维沃移动通信有限公司 | Message reply method, device and storage medium |
US11706344B2 (en) | 2020-12-08 | 2023-07-18 | State Farm Mutual Automobile Insurance Company | Monitoring representatives in a contact center environment |
CN114816599B (en) * | 2021-01-22 | 2024-02-27 | 北京字跳网络技术有限公司 | Image display method, device, equipment and medium |
CN112929255B (en) * | 2021-01-22 | 2023-04-07 | 维沃移动通信有限公司 | Message sending method and device |
US11610434B2 (en) * | 2021-02-02 | 2023-03-21 | T-Mobile Usa, Inc. | Emotional response capturing with automatic reply |
US20220292261A1 (en) * | 2021-03-15 | 2022-09-15 | Google Llc | Methods for Emotion Classification in Text |
US11516163B2 (en) * | 2021-03-31 | 2022-11-29 | T-Mobile Usa, Inc. | Image-based communication and response suggestions |
US11778279B2 (en) | 2021-04-02 | 2023-10-03 | Sony Interactive Entertainment Inc. | Social media crowd-sourced discussions |
CN112948412B (en) * | 2021-04-21 | 2024-03-12 | 携程旅游网络技术（上海）有限公司 | Flight inventory updating method, system, electronic device and storage medium |
US11973734B2 (en) | 2021-06-23 | 2024-04-30 | Microsoft Technology Licensing, Llc | Processing electronic communications according to recipient points of view |
US11409800B1 (en) | 2021-07-23 | 2022-08-09 | Bank Of America Corporation | Generating search queries for database searching |
US11971908B2 (en) | 2022-06-17 | 2024-04-30 | Talkdesk, Inc. | Method and apparatus for detecting anomalies in communication data |
US20240062008A1 (en) * | 2022-08-17 | 2024-02-22 | Snap Inc. | Text-guided sticker generation |
Family Cites Families (316)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US5963649A (en) | 1995-12-19 | 1999-10-05 | Nec Corporation | Message authorization system for authorizing message for electronic document |
US6092102A (en) | 1997-10-24 | 2000-07-18 | University Of Pittsburgh Of The Commonwealth System Of Higher Education | System and method for notifying users about information or events of an enterprise |
US7904187B2 (en) | 1999-02-01 | 2011-03-08 | Hoffberg Steven M | Internet appliance system and method |
JP2000298676A (en) | 1999-04-14 | 2000-10-24 | Bandai Co Ltd | Information supply device |
US6883140B1 (en) | 2000-02-24 | 2005-04-19 | Microsoft Corporation | System and method for editing digitally represented still images |
US7216080B2 (en) | 2000-09-29 | 2007-05-08 | Mindfabric Holdings Llc | Natural-language voice-activated personal assistant |
JP2002132804A (en) | 2000-10-24 | 2002-05-10 | Sanyo Electric Co Ltd | User support system |
US20020103837A1 (en) | 2001-01-31 | 2002-08-01 | International Business Machines Corporation | Method for handling requests for information in a natural language understanding system |
WO2003024094A1 (en) | 2001-09-10 | 2003-03-20 | Nikon Corporation | Digital camera system, image storage apparatus, and digital camera |
US7636750B2 (en) | 2001-10-24 | 2009-12-22 | Sprint Spectrum L.P. | Method and system for controlling scope of user participation in a communication session |
US7283992B2 (en) | 2001-11-30 | 2007-10-16 | Microsoft Corporation | Media agent to suggest contextually related media content |
US20040001099A1 (en) | 2002-06-27 | 2004-01-01 | Microsoft Corporation | Method and system for associating actions with semantic labels in electronic documents |
US7234117B2 (en) | 2002-08-28 | 2007-06-19 | Microsoft Corporation | System and method for shared integrated online social interaction |
US20110107223A1 (en) | 2003-01-06 | 2011-05-05 | Eric Tilton | User Interface For Presenting Presentations |
JP3669702B2 (en) | 2003-02-25 | 2005-07-13 | 松下電器産業株式会社 | Application program prediction method and mobile terminal |
BRPI0410362B1 (en) | 2003-05-16 | 2017-06-20 | Google Inc. | SYSTEMS AND METHODS OF SHARING NETWORK AND NETWORK MEDIA |
US8645697B1 (en) | 2003-08-08 | 2014-02-04 | Radix Holdings, Llc | Message authorization |
US7363029B2 (en) | 2003-09-12 | 2008-04-22 | Core Mobility, Inc. | Unified interface for voice, text or picture message authoring |
US20120322428A1 (en) | 2004-09-30 | 2012-12-20 | Motedata Inc. | Network of tags |
US10635723B2 (en) | 2004-02-15 | 2020-04-28 | Google Llc | Search engines and systems with handheld document data capture devices |
US7363295B2 (en) | 2004-04-19 | 2008-04-22 | Yahoo! Inc. | Techniques for inline searching in an instant messenger environment |
US20060029106A1 (en) | 2004-06-14 | 2006-02-09 | Semandex Networks, Inc. | System and method for providing content-based instant messaging |
US7464110B2 (en) | 2004-06-30 | 2008-12-09 | Nokia Corporation | Automated grouping of image and other user data |
US7734927B2 (en) | 2004-07-21 | 2010-06-08 | International Business Machines Corporation | Real-time voting based authorization in an autonomic workflow process using an electronic messaging system |
US20060150119A1 (en) | 2004-12-31 | 2006-07-06 | France Telecom | Method for interacting with automated information agents using conversational queries |
US7720436B2 (en) | 2006-01-09 | 2010-05-18 | Nokia Corporation | Displaying network objects in mobile devices based on geolocation |
US7197324B2 (en) | 2005-01-31 | 2007-03-27 | Sweeney Robert J | Permission based text messaging |
US7603413B1 (en) | 2005-04-07 | 2009-10-13 | Aol Llc | Using automated agents to facilitate chat communications |
US7860319B2 (en) | 2005-05-11 | 2010-12-28 | Hewlett-Packard Development Company, L.P. | Image management |
US7512580B2 (en) | 2005-08-04 | 2009-03-31 | Sap Ag | Confidence indicators for automated suggestions |
CN1988461A (en) | 2005-12-23 | 2007-06-27 | 腾讯科技（深圳）有限公司 | Chat scence music playing method and system for instant communication tool |
US7747785B2 (en) | 2006-04-14 | 2010-06-29 | Microsoft Corporation | Instant messaging plug-ins |
US8719342B2 (en) * | 2006-04-25 | 2014-05-06 | Core Wireless Licensing, S.a.r.l. | Third-party session modification |
US9318108B2 (en) | 2010-01-18 | 2016-04-19 | Apple Inc. | Intelligent automated assistant |
US20080120371A1 (en) | 2006-11-16 | 2008-05-22 | Rajat Gopal | Relational framework for non-real-time audio/video collaboration |
JP2008191748A (en) | 2007-02-01 | 2008-08-21 | Oki Electric Ind Co Ltd | Inter-user communication method, inter-user communication program and inter-user communication device |
WO2008153098A1 (en) | 2007-06-14 | 2008-12-18 | Sharp Kabushiki Kaisha | Image data reception device, operation device, operation system, image data structure, control method, operation method, program, and recording medium |
JP2009009334A (en) | 2007-06-27 | 2009-01-15 | Ricoh Co Ltd | Image processor, image processing method, and image processing program |
CN101159576B (en) | 2007-08-30 | 2013-07-17 | 腾讯科技（深圳）有限公司 | Chatting method, chatting room client terminal, system management background and server |
US8638363B2 (en) | 2009-02-18 | 2014-01-28 | Google Inc. | Automatically capturing information, such as capturing information using a document-aware device |
US20110145068A1 (en) | 2007-09-17 | 2011-06-16 | King Martin T | Associating rendered advertisements with digital content |
US8082151B2 (en) | 2007-09-18 | 2011-12-20 | At&T Intellectual Property I, Lp | System and method of generating responses to text-based messages |
US20090119584A1 (en) | 2007-11-02 | 2009-05-07 | Steve Herbst | Software Tool for Creating Outlines and Mind Maps that Generates Subtopics Automatically |
CA2708757A1 (en) | 2007-12-17 | 2009-06-25 | Play Megaphone | System and method for managing interaction between a user and an interactive system |
US8379914B2 (en) | 2008-01-18 | 2013-02-19 | Mitek Systems, Inc. | Systems and methods for mobile image capture and remittance processing |
US8577118B2 (en) | 2008-01-18 | 2013-11-05 | Mitek Systems | Systems for mobile image capture and remittance processing |
US20110022992A1 (en) | 2008-03-31 | 2011-01-27 | Koninklijke Philips Electronics N.V. | Method for modifying a representation based upon a user instruction |
US20090282114A1 (en) | 2008-05-08 | 2009-11-12 | Junlan Feng | System and method for generating suggested responses to an email |
US8671112B2 (en) | 2008-06-12 | 2014-03-11 | Athenahealth, Inc. | Methods and apparatus for automated image classification |
US20090327436A1 (en) | 2008-06-30 | 2009-12-31 | Chen Shihn-Cheng | Instant messaging network control module |
US8166019B1 (en) | 2008-07-21 | 2012-04-24 | Sprint Communications Company L.P. | Providing suggested actions in response to textual communications |
JP5273712B2 (en) | 2008-08-11 | 2013-08-28 | シャープ株式会社 | Information processing apparatus, information processing method, and information processing program |
US8805110B2 (en) | 2008-08-19 | 2014-08-12 | Digimarc Corporation | Methods and systems for content processing |
EP2321948B1 (en) | 2008-08-25 | 2017-11-15 | Orange | Method to identify and transfer to a wireless device actionable items based on user selected content |
US8391618B1 (en) | 2008-09-19 | 2013-03-05 | Adobe Systems Incorporated | Semantic image classification and search |
US8055710B2 (en) | 2008-09-24 | 2011-11-08 | International Business Machines Corporation | System, method and computer program product for intelligent multi-person chat history injection |
USD611053S1 (en) | 2008-11-24 | 2010-03-02 | Microsoft Corporation | Transitional user interface for a portion of a display screen |
USD599363S1 (en) | 2008-11-24 | 2009-09-01 | Microsoft Corporation | Transitional cursor user interface for a portion of a display screen |
US20130036162A1 (en) | 2009-02-10 | 2013-02-07 | Mikekoenigs.Com, Inc. | Automated Communication Techniques |
US20100228590A1 (en) | 2009-03-03 | 2010-09-09 | International Business Machines Corporation | Context-aware electronic social networking |
US8335754B2 (en) * | 2009-03-06 | 2012-12-18 | Tagged, Inc. | Representing a document using a semantic structure |
JP4739438B2 (en) | 2009-03-26 | 2011-08-03 | 株式会社エヌ・ティ・ティ・ドコモ | Communication terminal and mail reply method |
US9195898B2 (en) | 2009-04-14 | 2015-11-24 | Qualcomm Incorporated | Systems and methods for image recognition using mobile devices |
US20120131520A1 (en) | 2009-05-14 | 2012-05-24 | Tang ding-yuan | Gesture-based Text Identification and Selection in Images |
US9782527B2 (en) | 2009-05-27 | 2017-10-10 | Tc1 Llc | Monitoring of redundant conductors |
US9043407B1 (en) | 2009-06-12 | 2015-05-26 | Avaya Inc. | Interactive user interface to communication-enabled business process platforms method and apparatus |
USD651609S1 (en) | 2009-06-26 | 2012-01-03 | Microsoft Corporation | Display screen with an animated image |
CA2767033A1 (en) | 2009-07-02 | 2011-01-06 | Livechime, Inc. | System and method for enhancing digital content |
US8572084B2 (en) | 2009-07-28 | 2013-10-29 | Fti Consulting, Inc. | System and method for displaying relationships between electronically stored information to provide classification suggestions via nearest neighbor |
US9128610B2 (en) | 2009-09-30 | 2015-09-08 | At&T Mobility Ii Llc | Virtual predictive keypad |
US8121618B2 (en) | 2009-10-28 | 2012-02-21 | Digimarc Corporation | Intuitive computing methods and systems |
US8831279B2 (en) | 2011-03-04 | 2014-09-09 | Digimarc Corporation | Smartphone-based methods and systems |
US8400548B2 (en) | 2010-01-05 | 2013-03-19 | Apple Inc. | Synchronized, interactive augmented reality displays for multifunction devices |
USD624927S1 (en) | 2010-01-19 | 2010-10-05 | Microsoft Corporation | User interface for a portion of a display screen |
US8650210B1 (en) | 2010-02-09 | 2014-02-11 | Google Inc. | Identifying non-search actions based on a search query |
US8782556B2 (en) | 2010-02-12 | 2014-07-15 | Microsoft Corporation | User-centric soft keyboard predictive technologies |
US8655965B2 (en) | 2010-03-05 | 2014-02-18 | Qualcomm Incorporated | Automated messaging response in wireless communication systems |
US8266109B1 (en) | 2010-03-09 | 2012-09-11 | Symantec Corporation | Performance of scanning containers for archiving |
JP2011211696A (en) | 2010-03-10 | 2011-10-20 | Nikon Corp | Image data processing system, image data processing program, and image data processing apparatus |
JP5733907B2 (en) | 2010-04-07 | 2015-06-10 | キヤノン株式会社 | Image processing apparatus, image processing method, and computer program |
US9929982B2 (en) | 2010-04-08 | 2018-03-27 | Microsoft Technology Licensing, Llc | Designating automated agents as friends in a social network service |
US20110252207A1 (en) | 2010-04-08 | 2011-10-13 | Oracle International Corporation | Dynamic content archiving |
US20170098122A1 (en) | 2010-06-07 | 2017-04-06 | Affectiva, Inc. | Analysis of image content with associated manipulation of expression presentation |
USD648343S1 (en) | 2010-06-24 | 2011-11-08 | Microsoft Corporation | Display screen with user interface |
USD648735S1 (en) | 2010-06-25 | 2011-11-15 | Microsoft Corporation | Display screen with animated user interface |
US20120030289A1 (en) | 2010-07-30 | 2012-02-02 | Avaya Inc. | System and method for multi-model, context-sensitive, real-time collaboration |
US8781152B2 (en) | 2010-08-05 | 2014-07-15 | Brian Momeyer | Identifying visual media content captured by camera-enabled mobile device |
KR101722687B1 (en) | 2010-08-10 | 2017-04-04 | 삼성전자주식회사 | Method for providing information between objects or object and user, user device, and storage medium thereof |
US9936333B2 (en) | 2010-08-10 | 2018-04-03 | Microsoft Technology Licensing, Llc | Location and contextual-based mobile application promotion and delivery |
US8966590B2 (en) | 2010-08-17 | 2015-02-24 | Facebook, Inc. | Managing social network accessibility based on age |
US9262517B2 (en) | 2010-08-18 | 2016-02-16 | At&T Intellectual Property I, L.P. | Systems and methods for social media data mining |
US8566911B2 (en) | 2010-10-06 | 2013-10-22 | Blackberry Limited | Method of obtaining authorization for accessing a service |
KR101753031B1 (en) | 2010-11-15 | 2017-06-30 | 엘지전자 주식회사 | Mobile terminal and Method for setting metadata thereof |
KR101060753B1 (en) | 2011-01-04 | 2011-08-31 | (주)올라웍스 | Method, terminal, and computer-readable recording medium for supporting collection of object included in inputted image |
US20120179717A1 (en) | 2011-01-11 | 2012-07-12 | Sony Corporation | System and method for effectively providing entertainment recommendations to device users |
US8688698B1 (en) | 2011-02-11 | 2014-04-01 | Google Inc. | Automatic text suggestion |
GB2502736A (en) | 2011-02-23 | 2013-12-04 | Bottlenose Inc | System and method for analyzing messages in a network or across networks |
US20130262574A1 (en) | 2011-03-15 | 2013-10-03 | Gabriel Cohen | Inline User Addressing in Chat Sessions |
US8938669B1 (en) | 2011-03-15 | 2015-01-20 | Google Inc. | Inline user addressing in chat and document editing sessions |
US8849931B2 (en) | 2011-03-15 | 2014-09-30 | Idt Messaging, Llc | Linking context-based information to text messages |
US8554701B1 (en) | 2011-03-18 | 2013-10-08 | Amazon Technologies, Inc. | Determining sentiment of sentences from customer reviews |
JP2012221480A (en) | 2011-04-06 | 2012-11-12 | L Is B Corp | Message processing system |
WO2012150602A1 (en) | 2011-05-03 | 2012-11-08 | Yogesh Chunilal Rathod | A system and method for dynamically monitoring, recording, processing, attaching dynamic, contextual & accessible active links & presenting of physical or digital activities, actions, locations, logs, life stream, behavior & status |
EP2523436A1 (en) | 2011-05-11 | 2012-11-14 | Alcatel Lucent | Mobile device and method of managing applications for a mobile device |
USD658201S1 (en) | 2011-05-27 | 2012-04-24 | Microsoft Corporation | Display screen with animated user interface |
USD658677S1 (en) | 2011-05-27 | 2012-05-01 | Microsoft Corporation | Display screen with animated user interface |
USD658678S1 (en) | 2011-05-27 | 2012-05-01 | Microsoft Corporation | Display screen with animated user interface |
US8832284B1 (en) | 2011-06-16 | 2014-09-09 | Google Inc. | Virtual socializing |
US8589407B2 (en) * | 2011-06-17 | 2013-11-19 | Google Inc. | Automated generation of suggestions for personalized reactions in a social network |
US8700480B1 (en) | 2011-06-20 | 2014-04-15 | Amazon Technologies, Inc. | Extracting quotes from customer reviews regarding collections of items |
US9588668B2 (en) | 2011-07-21 | 2017-03-07 | Imerj, Llc | Methods of displaying a second view |
US9245253B2 (en) | 2011-08-19 | 2016-01-26 | Disney Enterprises, Inc. | Soft-sending chat messages |
US8659667B2 (en) | 2011-08-29 | 2014-02-25 | Panasonic Corporation | Recipe based real-time assistance for digital image capture and other consumer electronics devices |
US9179278B2 (en) | 2011-09-01 | 2015-11-03 | Qualcomm Incorporated | Systems and methods involving augmented menu using mobile device |
US10102546B2 (en) | 2011-09-15 | 2018-10-16 | Stephan HEATH | System and method for tracking, utilizing predicting, and implementing online consumer browsing behavior, buying patterns, social networking communications, advertisements and communications, for online coupons, products, goods and services, auctions, and service providers using geospatial mapping technology, and social networking |
GB2495222B (en) | 2011-09-30 | 2016-10-26 | Apple Inc | Using context information to facilitate processing of commands in a virtual assistant |
US20180032997A1 (en) * | 2012-10-09 | 2018-02-01 | George A. Gordon | System, method, and computer program product for determining whether to prompt an action by a platform in connection with a mobile device |
KR101521332B1 (en) | 2011-11-08 | 2015-05-20 | 주식회사 다음카카오 | Method of provicing a lot of services extended from a instant messaging service and the instant messaging service |
US9697016B2 (en) * | 2011-11-15 | 2017-07-04 | Microsoft Technology Licensing, Llc | Search augmented menu and configuration for computer applications |
USD673172S1 (en) | 2011-11-21 | 2012-12-25 | Microsoft Corporation | Display screen with animated graphical user interface |
KR101402506B1 (en) | 2011-12-01 | 2014-06-03 | 라인 가부시키가이샤 | System and method for providing information interactively by instant messaging application |
USD701228S1 (en) | 2012-01-06 | 2014-03-18 | Samsung Electronics Co., Ltd. | Display screen or portion thereof with transitional graphical user interface |
USD699744S1 (en) | 2012-01-06 | 2014-02-18 | Microsoft Corporation | Display screen with an animated graphical user interface |
AU344579S (en) | 2012-01-09 | 2012-09-27 | Samsung Electronics Co Ltd | Display screen for an electronic device |
USD705802S1 (en) | 2012-02-07 | 2014-05-27 | Microsoft Corporation | Display screen with animated graphical user interface |
USD705251S1 (en) | 2012-02-09 | 2014-05-20 | Microsoft Corporation | Display screen with animated graphical user interface |
US9306878B2 (en) | 2012-02-14 | 2016-04-05 | Salesforce.Com, Inc. | Intelligent automated messaging for computer-implemented devices |
GB2499395A (en) | 2012-02-14 | 2013-08-21 | British Sky Broadcasting Ltd | Search method |
USD699739S1 (en) | 2012-02-22 | 2014-02-18 | Microsoft Corporation | Display screen with animated graphical user interface |
US20130218885A1 (en) | 2012-02-22 | 2013-08-22 | Salesforce.Com, Inc. | Systems and methods for context-aware message tagging |
USD701527S1 (en) | 2012-02-23 | 2014-03-25 | Htc Corporation | Display screen with transitional graphical user interface |
USD701528S1 (en) | 2012-02-24 | 2014-03-25 | Htc Corporation | Display screen with transitional graphical user interface |
US8620021B2 (en) | 2012-03-29 | 2013-12-31 | Digimarc Corporation | Image-related methods and arrangements |
US8855430B1 (en) * | 2012-05-30 | 2014-10-07 | Google Inc. | Refining image annotations |
US20130346235A1 (en) | 2012-06-20 | 2013-12-26 | Ebay, Inc. | Systems, Methods, and Computer Program Products for Caching of Shopping Items |
USD705244S1 (en) | 2012-06-20 | 2014-05-20 | Microsoft Corporation | Display screen with animated graphical user interface |
US9191786B2 (en) | 2012-06-27 | 2015-11-17 | At&T Intellectual Property I, L.P. | Method and apparatus for generating a suggested message to be sent over a network |
US8903929B2 (en) * | 2012-07-05 | 2014-12-02 | Microsoft Corporation | Forgotten attachment detection |
US9412136B2 (en) | 2012-07-09 | 2016-08-09 | Facebook, Inc. | Creation of real-time conversations based on social location information |
KR20140011073A (en) | 2012-07-17 | 2014-01-28 | 삼성전자주식회사 | Method and apparatus for recommending text |
US9019415B2 (en) | 2012-07-26 | 2015-04-28 | Qualcomm Incorporated | Method and apparatus for dual camera shutter |
US9195645B2 (en) | 2012-07-30 | 2015-11-24 | Microsoft Technology Licensing, Llc | Generating string predictions using contexts |
KR101899817B1 (en) | 2012-08-01 | 2018-09-19 | 엘지전자 주식회사 | Mobile terminal and controlling method thereof |
USD695755S1 (en) | 2012-08-06 | 2013-12-17 | Samsung Electronics Co., Ltd. | TV monitor with graphical user interface |
US20140047413A1 (en) | 2012-08-09 | 2014-02-13 | Modit, Inc. | Developing, Modifying, and Using Applications |
US20140052540A1 (en) | 2012-08-20 | 2014-02-20 | Giridhar Rajaram | Providing content using inferred topics extracted from communications in a social networking system |
KR102068604B1 (en) | 2012-08-28 | 2020-01-22 | 삼성전자 주식회사 | Apparatus and method for recognizing a character in terminal equipment |
USD706802S1 (en) | 2012-08-28 | 2014-06-10 | Samsung Electronics Co., Ltd. | Portable electronic device displaying transitional graphical user interface |
US9218333B2 (en) | 2012-08-31 | 2015-12-22 | Microsoft Technology Licensing, Llc | Context sensitive auto-correction |
JP6160996B2 (en) | 2012-09-12 | 2017-07-12 | パナソニックＩｐマネジメント株式会社 | Imaging device |
US20140088954A1 (en) | 2012-09-27 | 2014-03-27 | Research In Motion Limited | Apparatus and method pertaining to automatically-suggested emoticons |
US10691743B2 (en) | 2014-08-05 | 2020-06-23 | Sri International | Multi-dimensional realization of visual content of an image collection |
US9299060B2 (en) | 2012-10-12 | 2016-03-29 | Google Inc. | Automatically suggesting groups based on past user interaction |
KR20140052155A (en) | 2012-10-19 | 2014-05-07 | 삼성전자주식회사 | Display apparatus, method for controlling the display apparatus and processor for controlling the display apparatus |
USD714821S1 (en) | 2012-10-24 | 2014-10-07 | Microsoft Corporation | Display screen with animated graphical user interface |
US20150286371A1 (en) | 2012-10-31 | 2015-10-08 | Aniways Advertising Solutions Ltd. | Custom emoticon generation |
US20140156801A1 (en) | 2012-12-04 | 2014-06-05 | Mobitv, Inc. | Cowatching and connected platforms using a push architecture |
US9244905B2 (en) | 2012-12-06 | 2016-01-26 | Microsoft Technology Licensing, Llc | Communication context based predictive-text suggestion |
US20140164506A1 (en) | 2012-12-10 | 2014-06-12 | Rawllin International Inc. | Multimedia message having portions of networked media content |
US20140171133A1 (en) | 2012-12-18 | 2014-06-19 | Google Inc. | Query response |
CN103067490B (en) | 2012-12-26 | 2015-11-25 | 腾讯科技（深圳）有限公司 | The Notification Method of mobile terminal communication session, terminal, server and system |
GB201322037D0 (en) | 2013-12-12 | 2014-01-29 | Touchtype Ltd | System and method for inputting images/labels into electronic devices |
US20140189538A1 (en) | 2012-12-31 | 2014-07-03 | Motorola Mobility Llc | Recommendations for Applications Based on Device Context |
US8930481B2 (en) | 2012-12-31 | 2015-01-06 | Huawei Technologies Co., Ltd. | Message processing method, terminal and system |
US9020956B1 (en) | 2012-12-31 | 2015-04-28 | Google Inc. | Sentiment and topic based content determination methods and systems |
US9374327B2 (en) | 2013-01-08 | 2016-06-21 | Vmware, Inc. | Intelligent chat system |
KR20140091633A (en) | 2013-01-11 | 2014-07-22 | 삼성전자주식회사 | Method for providing recommended items based on conext awareness and the mobile terminal therefor |
KR101821358B1 (en) | 2013-01-22 | 2018-01-25 | 네이버 주식회사 | Method and system for providing multi-user messenger service |
US20140237057A1 (en) | 2013-02-21 | 2014-08-21 | Genesys Telecommunications Laboratories, Inc. | System and method for processing private messages in a contact center |
USD704726S1 (en) | 2013-03-04 | 2014-05-13 | Roger Leslie Maxwell | Display screen or portion thereof with animated graphical user interface |
JP6255646B2 (en) | 2013-03-04 | 2018-01-10 | 株式会社Ｌ ｉｓ Ｂ | Message system |
US20140344058A1 (en) | 2013-03-15 | 2014-11-20 | Fision Holdings, Inc | Systems and methods for distributed marketing automation |
US8825474B1 (en) | 2013-04-16 | 2014-09-02 | Google Inc. | Text suggestion output using past interaction data |
US9177318B2 (en) | 2013-04-22 | 2015-11-03 | Palo Alto Research Center Incorporated | Method and apparatus for customizing conversation agents based on user characteristics using a relevance score for automatic statements, and a response prediction function |
US9923849B2 (en) | 2013-05-09 | 2018-03-20 | Ebay Inc. | System and method for suggesting a phrase based on a context |
US9760803B2 (en) | 2013-05-15 | 2017-09-12 | Google Inc. | Associating classifications with images |
US10523454B2 (en) | 2013-06-13 | 2019-12-31 | Evernote Corporation | Initializing chat sessions by pointing to content |
US10599765B2 (en) | 2013-06-27 | 2020-03-24 | Avaya Inc. | Semantic translation model training |
CA2918459C (en) | 2013-07-16 | 2019-06-04 | Pinterest, Inc. | Object based contextual menu controls |
US9330110B2 (en) | 2013-07-17 | 2016-05-03 | Xerox Corporation | Image search system and method for personalized photo applications using semantic networks |
US9794198B2 (en) | 2013-07-19 | 2017-10-17 | Tencent Technology (Shenzhen) Company Limited | Methods and systems for creating auto-reply messages |
US10162884B2 (en) | 2013-07-23 | 2018-12-25 | Conduent Business Services, Llc | System and method for auto-suggesting responses based on social conversational contents in customer care services |
EP2838060A1 (en) | 2013-08-14 | 2015-02-18 | Facebook, Inc. | Methods and systems for facilitating e-commerce payments |
US9161188B2 (en) | 2013-08-22 | 2015-10-13 | Yahoo! Inc. | System and method for automatically suggesting diverse and personalized message completions |
CN104035947B (en) | 2013-09-16 | 2016-04-13 | 腾讯科技（深圳）有限公司 | Point of interest recommend method and device, acquisition recommend method and the device of point of interest |
US9401881B2 (en) | 2013-09-26 | 2016-07-26 | International Business Machines Corporation | Automatic question generation and answering based on monitored messaging sessions |
US9329692B2 (en) | 2013-09-27 | 2016-05-03 | Microsoft Technology Licensing, Llc | Actionable content displayed on a touch screen |
US20150100537A1 (en) * | 2013-10-03 | 2015-04-09 | Microsoft Corporation | Emoji for Text Predictions |
US8996639B1 (en) | 2013-10-15 | 2015-03-31 | Google Inc. | Predictive responses to incoming communications |
US20150127753A1 (en) | 2013-11-04 | 2015-05-07 | Meemo, Llc | Word Recognition and Ideograph or In-App Advertising System |
WO2015089483A1 (en) | 2013-12-12 | 2015-06-18 | Mobile Iron, Inc. | Application synchornization |
KR20150071768A (en) | 2013-12-18 | 2015-06-29 | 에스케이하이닉스 주식회사 | Image sensor and method for fabricating the same |
US10565268B2 (en) | 2013-12-19 | 2020-02-18 | Adobe Inc. | Interactive communication augmented with contextual information |
WO2015100362A1 (en) | 2013-12-23 | 2015-07-02 | 24/7 Customer, Inc. | Systems and methods for facilitating dialogue mining |
US9519408B2 (en) | 2013-12-31 | 2016-12-13 | Google Inc. | Systems and methods for guided user actions |
US9817813B2 (en) | 2014-01-08 | 2017-11-14 | Genesys Telecommunications Laboratories, Inc. | Generalized phrases in automatic speech recognition systems |
US20150207765A1 (en) | 2014-01-17 | 2015-07-23 | Nathaniel Brantingham | Messaging Service with Conversation Suggestions |
US9721183B2 (en) | 2014-01-31 | 2017-08-01 | Hulu, LLC | Intelligent determination of aesthetic preferences based on user history and properties |
US9515968B2 (en) | 2014-02-05 | 2016-12-06 | Facebook, Inc. | Controlling access to ideograms |
WO2015120019A1 (en) | 2014-02-10 | 2015-08-13 | Google Inc. | Smart camera user interface |
CN104836720B (en) | 2014-02-12 | 2022-02-25 | 北京三星通信技术研究有限公司 | Method and device for information recommendation in interactive communication |
US10095748B2 (en) | 2014-03-03 | 2018-10-09 | Microsoft Technology Licensing, Llc | Personalized information query suggestions |
KR102106787B1 (en) | 2014-03-17 | 2020-05-06 | 에스케이텔레콤 주식회사 | Method for coupling application with instant messenger, apparatus and system for the same |
CN104951428B (en) | 2014-03-26 | 2019-04-16 | 阿里巴巴集团控股有限公司 | User's intension recognizing method and device |
US9544257B2 (en) | 2014-04-04 | 2017-01-10 | Blackberry Limited | System and method for conducting private messaging |
US9213941B2 (en) | 2014-04-22 | 2015-12-15 | Google Inc. | Automatic actions based on contextual replies |
US10482163B2 (en) | 2014-04-23 | 2019-11-19 | Klickafy, Llc | Clickable emoji |
CN103995872B (en) | 2014-05-21 | 2017-04-05 | 王青 | It is a kind of in the application based on developing scenes discussion with chat method and system |
US10255449B2 (en) | 2014-05-30 | 2019-04-09 | Apple Inc. | Permission request |
US10445396B2 (en) | 2014-05-31 | 2019-10-15 | Apple Inc. | Device, method, and graphical user interface for extending functionality of a host application to another application |
US9380010B2 (en) | 2014-06-03 | 2016-06-28 | International Business Machines Corporation | Conversation branching for more efficient resolution |
CN104063427A (en) * | 2014-06-06 | 2014-09-24 | 北京搜狗科技发展有限公司 | Expression input method and device based on semantic understanding |
CN104063683B (en) * | 2014-06-06 | 2017-05-17 | 北京搜狗科技发展有限公司 | Expression input method and device based on face identification |
WO2015200350A1 (en) | 2014-06-24 | 2015-12-30 | Google Inc. | Ranking and selecting images for display from a set of images |
US10785173B2 (en) | 2014-07-03 | 2020-09-22 | Nuance Communications, Inc. | System and method for suggesting actions based upon incoming messages |
US9420331B2 (en) | 2014-07-07 | 2016-08-16 | Google Inc. | Method and system for categorizing detected motion events |
US9043196B1 (en) | 2014-07-07 | 2015-05-26 | Machine Zone, Inc. | Systems and methods for identifying and suggesting emoticons |
US9990105B2 (en) | 2014-07-08 | 2018-06-05 | Verizon Patent And Licensing Inc. | Accessible contextual controls within a graphical user interface |
US20160043817A1 (en) | 2014-07-18 | 2016-02-11 | RSS Technologies, LLC | Methods and apparatus for locality based broadcasting |
WO2016018111A1 (en) * | 2014-07-31 | 2016-02-04 | Samsung Electronics Co., Ltd. | Message service providing device and method of providing content via the same |
CN104202718A (en) | 2014-08-05 | 2014-12-10 | 百度在线网络技术（北京）有限公司 | Method and device for providing information for user |
US10218652B2 (en) | 2014-08-08 | 2019-02-26 | Mastercard International Incorporated | Systems and methods for integrating a chat function into an e-reader application |
US9965559B2 (en) | 2014-08-21 | 2018-05-08 | Google Llc | Providing automatic actions for mobile onscreen content |
US9705832B2 (en) | 2014-08-27 | 2017-07-11 | Lenovo (Singapore) Pte. Ltd. | Context-aware aggregation of text-based messages |
US10447621B2 (en) | 2014-09-04 | 2019-10-15 | Microsoft Technology Licensing, Llc | App powered extensibility of messages on an existing messaging service |
US10146748B1 (en) | 2014-09-10 | 2018-12-04 | Google Llc | Embedding location information in a media collaboration using natural language processing |
US11010726B2 (en) | 2014-11-07 | 2021-05-18 | Sony Corporation | Information processing apparatus, control method, and storage medium |
US20160140477A1 (en) | 2014-11-13 | 2016-05-19 | Xerox Corporation | Methods and systems for assigning tasks to workers |
US9569728B2 (en) | 2014-11-14 | 2017-02-14 | Bublup Technologies, Inc. | Deriving semantic relationships based on empirical organization of content by users |
CN105786455B (en) | 2014-12-17 | 2020-02-18 | 深圳市腾讯计算机系统有限公司 | Data processing method and device and terminal |
US20160179816A1 (en) | 2014-12-22 | 2016-06-23 | Quixey, Inc. | Near Real Time Auto-Suggest Search Results |
US9727218B2 (en) | 2015-01-02 | 2017-08-08 | Microsoft Technology Licensing, Llc | Contextual browser frame and entry box placement |
KR20160089152A (en) | 2015-01-19 | 2016-07-27 | 주식회사 엔씨소프트 | Method and computer system of analyzing communication situation based on dialogue act information |
KR101634086B1 (en) * | 2015-01-19 | 2016-07-08 | 주식회사 엔씨소프트 | Method and computer system of analyzing communication situation based on emotion information |
US20160224524A1 (en) | 2015-02-03 | 2016-08-04 | Nuance Communications, Inc. | User generated short phrases for auto-filling, automatically collected during normal text use |
US20160226804A1 (en) | 2015-02-03 | 2016-08-04 | Google Inc. | Methods, systems, and media for suggesting a link to media content |
US9661386B2 (en) | 2015-02-11 | 2017-05-23 | Google Inc. | Methods, systems, and media for presenting a suggestion to watch videos |
US10079785B2 (en) | 2015-02-12 | 2018-09-18 | Google Llc | Determining reply content for a reply to an electronic communication |
US20160284011A1 (en) | 2015-03-25 | 2016-09-29 | Facebook, Inc. | Techniques for social messaging authorization and customization |
US10353542B2 (en) | 2015-04-02 | 2019-07-16 | Facebook, Inc. | Techniques for context sensitive illustrated graphical user interface elements |
US10965622B2 (en) | 2015-04-16 | 2021-03-30 | Samsung Electronics Co., Ltd. | Method and apparatus for recommending reply message |
US9703541B2 (en) | 2015-04-28 | 2017-07-11 | Google Inc. | Entity action suggestion on a mobile device |
US9883358B2 (en) | 2015-05-08 | 2018-01-30 | Blackberry Limited | Electronic device and method of determining suggested responses to text-based communications |
US10909329B2 (en) | 2015-05-21 | 2021-02-02 | Baidu Usa Llc | Multilingual image question answering |
US10504509B2 (en) | 2015-05-27 | 2019-12-10 | Google Llc | Providing suggested voice-based action queries |
US10091140B2 (en) | 2015-05-31 | 2018-10-02 | Microsoft Technology Licensing, Llc | Context-sensitive generation of conversational responses |
KR20160148260A (en) * | 2015-06-16 | 2016-12-26 | 삼성전자주식회사 | Electronic device and Method for controlling the electronic device thereof |
US10274911B2 (en) | 2015-06-25 | 2019-04-30 | Intel Corporation | Conversational interface for matching text of spoken input based on context model |
US10042866B2 (en) * | 2015-06-30 | 2018-08-07 | Adobe Systems Incorporated | Searching untagged images with text-based queries |
US9712466B2 (en) | 2015-11-10 | 2017-07-18 | Wrinkl, Inc. | Integrating actionable objects into an on-line chat communications platform |
CN108604234A (en) | 2015-07-15 | 2018-09-28 | 查比公司 | System and method for screenshot capture link |
US20170031575A1 (en) | 2015-07-28 | 2017-02-02 | Microsoft Technology Licensing, Llc | Tailored computing experience based on contextual signals |
CN105141503A (en) | 2015-08-13 | 2015-12-09 | 北京北信源软件股份有限公司 | Novel instant messaging intelligent robot |
CN105183276A (en) | 2015-08-19 | 2015-12-23 | 小米科技有限责任公司 | Method and apparatus for realizing game in chat interface, and and terminal device |
CN105068661B (en) | 2015-09-07 | 2018-09-07 | 百度在线网络技术（北京）有限公司 | Man-machine interaction method based on artificial intelligence and system |
US10445425B2 (en) | 2015-09-15 | 2019-10-15 | Apple Inc. | Emoji and canned responses |
US9467435B1 (en) | 2015-09-15 | 2016-10-11 | Mimecast North America, Inc. | Electronic message threat protection system for authorized users |
US11025569B2 (en) | 2015-09-30 | 2021-06-01 | Apple Inc. | Shared content presentation with integrated messaging |
US10789525B2 (en) * | 2015-10-02 | 2020-09-29 | Adobe Inc. | Modifying at least one attribute of an image with at least one attribute extracted from another image |
KR101632436B1 (en) * | 2015-10-23 | 2016-06-21 | 이요훈 | IP network based Social Network Service and chat application software system |
KR20170048964A (en) | 2015-10-27 | 2017-05-10 | 라인 가부시키가이샤 | Method and apparatus of providing message, Method and apparatus of controlling display and computer program for executing one of the method |
CN105262675A (en) | 2015-10-29 | 2016-01-20 | 北京奇虎科技有限公司 | Method and apparatus for controlling chat based on electronic book |
KR102393928B1 (en) | 2015-11-10 | 2022-05-04 | 삼성전자주식회사 | User terminal apparatus for recommanding a reply message and method thereof |
US9633048B1 (en) * | 2015-11-16 | 2017-04-25 | Adobe Systems Incorporated | Converting a text sentence to a series of images |
US10129193B2 (en) * | 2015-11-17 | 2018-11-13 | International Business Machines Corporation | Identifying relevant content contained in message streams that appear to be irrelevant |
US20170147202A1 (en) * | 2015-11-24 | 2017-05-25 | Facebook, Inc. | Augmenting text messages with emotion information |
KR102427833B1 (en) | 2015-11-30 | 2022-08-02 | 삼성전자주식회사 | User terminal device and method for display thereof |
CN105306281B (en) | 2015-12-03 | 2019-05-14 | 腾讯科技（深圳）有限公司 | Information processing method and client |
US20170171117A1 (en) | 2015-12-10 | 2017-06-15 | International Business Machines Corporation | Message Suggestion Using Dynamic Information |
CN108781175B (en) | 2015-12-21 | 2021-09-21 | 谷歌有限责任公司 | Method, medium, and system for automatic suggestion of message exchange contexts |
EP3395019B1 (en) | 2015-12-21 | 2022-03-30 | Google LLC | Automatic suggestions and other content for messaging applications |
US10732783B2 (en) * | 2015-12-28 | 2020-08-04 | Microsoft Technology Licensing, Llc | Identifying image comments from similar images |
KR101712180B1 (en) | 2015-12-29 | 2017-03-06 | 라인 가부시키가이샤 | Computer Readable Recording Medium with Program, method and apparatus for Transmitting/Receiving Message |
US9560152B1 (en) | 2016-01-27 | 2017-01-31 | International Business Machines Corporation | Personalized summary of online communications |
US11477139B2 (en) | 2016-02-25 | 2022-10-18 | Meta Platforms, Inc. | Techniques for messaging bot rich communication |
US20170250935A1 (en) | 2016-02-25 | 2017-08-31 | Facebook, Inc. | Techniques for messaging bot app interactions |
US20170250930A1 (en) | 2016-02-29 | 2017-08-31 | Outbrain Inc. | Interactive content recommendation personalization assistant |
US20170288942A1 (en) | 2016-03-30 | 2017-10-05 | Microsoft Technology Licensing, Llc | Portal for Provisioning Autonomous Software Agents |
US10831802B2 (en) | 2016-04-11 | 2020-11-10 | Facebook, Inc. | Techniques to respond to user requests using natural-language machine learning based on example conversations |
US10452671B2 (en) * | 2016-04-26 | 2019-10-22 | Facebook, Inc. | Recommendations from comments on online social networks |
US9866693B2 (en) | 2016-05-06 | 2018-01-09 | Genesys Telecommunications Laboratories, Inc. | System and method for monitoring progress of automated chat conversations |
US20170344224A1 (en) | 2016-05-27 | 2017-11-30 | Nuance Communications, Inc. | Suggesting emojis to users for insertion into text-based messages |
CN105898627B (en) | 2016-05-31 | 2019-04-12 | 北京奇艺世纪科技有限公司 | A kind of video broadcasting method and device |
US10785175B2 (en) | 2016-06-12 | 2020-09-22 | Apple Inc. | Polling extension application for interacting with a messaging application |
US10368208B2 (en) | 2016-06-12 | 2019-07-30 | Apple Inc. | Layers in messaging applications |
US10554599B2 (en) | 2016-06-12 | 2020-02-04 | Apple Inc. | Conversion of detected URL in text message |
US10852912B2 (en) | 2016-06-12 | 2020-12-01 | Apple Inc. | Image creation app in messaging app |
US10194288B2 (en) | 2016-06-12 | 2019-01-29 | Apple Inc. | Sticker distribution system for messaging apps |
US9990128B2 (en) | 2016-06-12 | 2018-06-05 | Apple Inc. | Messaging application interacting with one or more extension applications |
US10505872B2 (en) | 2016-06-12 | 2019-12-10 | Apple Inc. | Messaging application interacting with one or more extension applications |
US20170359283A1 (en) | 2016-06-12 | 2017-12-14 | Apple Inc. | Music creation app in messaging app |
US11088973B2 (en) | 2016-06-12 | 2021-08-10 | Apple Inc. | Conversion of text relating to media content and media extension apps |
US10595169B2 (en) | 2016-06-12 | 2020-03-17 | Apple Inc. | Message extension app store |
US20170366479A1 (en) | 2016-06-20 | 2017-12-21 | Microsoft Technology Licensing, Llc | Communication System |
US10254935B2 (en) | 2016-06-29 | 2019-04-09 | Google Llc | Systems and methods of providing content selection |
US10515393B2 (en) | 2016-06-30 | 2019-12-24 | Paypal, Inc. | Image data detection for micro-expression analysis and targeted data services |
US10387888B2 (en) | 2016-07-08 | 2019-08-20 | Asapp, Inc. | Assisting entities in responding to a request of a user |
US20180032499A1 (en) | 2016-07-28 | 2018-02-01 | Google Inc. | Automatically Generating Spelling Suggestions and Corrections Based on User Context |
US10049310B2 (en) * | 2016-08-30 | 2018-08-14 | International Business Machines Corporation | Image text analysis for identifying hidden text |
KR20180026983A (en) * | 2016-09-05 | 2018-03-14 | 삼성전자주식회사 | Electronic device and control method thereof |
CN117634495A (en) | 2016-09-20 | 2024-03-01 | 谷歌有限责任公司 | Suggested response based on message decal |
US10511450B2 (en) | 2016-09-20 | 2019-12-17 | Google Llc | Bot permissions |
US10015124B2 (en) | 2016-09-20 | 2018-07-03 | Google Llc | Automatic response suggestions based on images received in messaging applications |
WO2018057537A1 (en) | 2016-09-20 | 2018-03-29 | Google Llc | Bot interaction |
US11176931B2 (en) | 2016-09-23 | 2021-11-16 | Microsoft Technology Licensing, Llc | Conversational bookmarks |
CN106484831A (en) * | 2016-09-29 | 2017-03-08 | 百度在线网络技术（北京）有限公司 | Search system, method and apparatus |
US10416846B2 (en) | 2016-11-12 | 2019-09-17 | Google Llc | Determining graphical element(s) for inclusion in an electronic communication |
US20180196854A1 (en) | 2017-01-11 | 2018-07-12 | Google Inc. | Application extension for generating automatic search queries |
US10146768B2 (en) | 2017-01-25 | 2018-12-04 | Google Llc | Automatic suggested responses to images received in messages using language model |
US10229427B2 (en) | 2017-04-10 | 2019-03-12 | Wildfire Systems, Inc. | Virtual keyboard trackable referral system |
US20180316637A1 (en) | 2017-05-01 | 2018-11-01 | Microsoft Technology Licensing, Llc | Conversation lens for context |
AU2018261870B2 (en) | 2017-05-05 | 2020-11-05 | Seetvun AMIR | Dynamic response prediction for improved bot task processing |
US10860854B2 (en) | 2017-05-16 | 2020-12-08 | Google Llc | Suggested actions for images |
US10827319B2 (en) | 2017-06-02 | 2020-11-03 | Apple Inc. | Messaging system interacting with dynamic extension app |
US10348658B2 (en) | 2017-06-15 | 2019-07-09 | Google Llc | Suggested items for use with embedded applications in chat conversations |
US10404636B2 (en) | 2017-06-15 | 2019-09-03 | Google Llc | Embedded programs and interfaces for chat conversations |
-
2017
- 2017-09-19 CN CN202311382957.2A patent/CN117634495A/en active Pending
- 2017-09-19 US US15/709,423 patent/US10547574B2/en active Active
- 2017-09-19 WO PCT/US2017/052349 patent/WO2018057541A1/en active Application Filing
- 2017-09-19 CN CN201780058037.1A patent/CN109952572B/en active Active
-
2019
- 2019-12-04 US US16/703,699 patent/US10979373B2/en active Active
-
2021
- 2021-04-07 US US17/224,949 patent/US11303590B2/en active Active
Also Published As
Publication number | Publication date |
---|---|
CN109952572B (en) | 2023-11-24 |
US10979373B2 (en) | 2021-04-13 |
US10547574B2 (en) | 2020-01-28 |
US20180083898A1 (en) | 2018-03-22 |
US20210243143A1 (en) | 2021-08-05 |
US20200106726A1 (en) | 2020-04-02 |
WO2018057541A1 (en) | 2018-03-29 |
US11303590B2 (en) | 2022-04-12 |
CN109952572A (en) | 2019-06-28 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
CN109952572B (en) | Suggested response based on message decal | |
US10862836B2 (en) | Automatic response suggestions based on images received in messaging applications | |
KR102050334B1 (en) | Automatic suggestion responses to images received in messages, using the language model | |
US11721093B2 (en) | Content summarization for assistant systems | |
US10452783B2 (en) | Conversational agent | |
US10896295B1 (en) | Providing additional information for identified named-entities for assistant systems | |
US11159767B1 (en) | Proactive in-call content recommendations for assistant systems | |
US11829404B2 (en) | Functional image archiving | |
CN114556333A (en) | Smart camera enabled by assistant system | |
US20230222605A1 (en) | Processing Multimodal User Input for Assistant Systems |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |