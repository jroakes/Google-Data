CN116762333A - Superimposing images of conference call participants and shared documents - Google Patents
Superimposing images of conference call participants and shared documents Download PDFInfo
- Publication number
- CN116762333A CN116762333A CN202280008982.1A CN202280008982A CN116762333A CN 116762333 A CN116762333 A CN 116762333A CN 202280008982 A CN202280008982 A CN 202280008982A CN 116762333 A CN116762333 A CN 116762333A
- Authority
- CN
- China
- Prior art keywords
- document
- participant
- image
- teleconference
- client device
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000034 method Methods 0.000 claims abstract description 28
- 230000004044 response Effects 0.000 claims description 54
- 238000012545 processing Methods 0.000 claims description 28
- 238000012986 modification Methods 0.000 claims 2
- 230000004048 modification Effects 0.000 claims 2
- 238000007726 management method Methods 0.000 description 76
- 238000012549 training Methods 0.000 description 55
- 238000000605 extraction Methods 0.000 description 52
- 238000009877 rendering Methods 0.000 description 16
- 238000001514 detection method Methods 0.000 description 12
- 230000015654 memory Effects 0.000 description 12
- 238000010801 machine learning Methods 0.000 description 10
- 238000010586 diagram Methods 0.000 description 6
- 230000000694 effects Effects 0.000 description 4
- 238000013461 design Methods 0.000 description 3
- 230000006870 function Effects 0.000 description 3
- 230000008569 process Effects 0.000 description 3
- 238000013528 artificial neural network Methods 0.000 description 2
- 230000001413 cellular effect Effects 0.000 description 2
- 238000004891 communication Methods 0.000 description 2
- 238000013480 data collection Methods 0.000 description 2
- 238000013500 data storage Methods 0.000 description 2
- 230000005291 magnetic effect Effects 0.000 description 2
- 230000007246 mechanism Effects 0.000 description 2
- 230000003287 optical effect Effects 0.000 description 2
- 230000002085 persistent effect Effects 0.000 description 2
- 230000005236 sound signal Effects 0.000 description 2
- 230000003068 static effect Effects 0.000 description 2
- 238000004458 analytical method Methods 0.000 description 1
- 230000003247 decreasing effect Effects 0.000 description 1
- 230000007812 deficiency Effects 0.000 description 1
- 238000005516 engineering process Methods 0.000 description 1
- 230000037406 food intake Effects 0.000 description 1
- 230000007274 generation of a signal involved in cell-cell signaling Effects 0.000 description 1
- 230000003993 interaction Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 230000007774 longterm Effects 0.000 description 1
- 230000001360 synchronised effect Effects 0.000 description 1
- 238000012360 testing method Methods 0.000 description 1
- 230000007704 transition Effects 0.000 description 1
- 230000000007 visual effect Effects 0.000 description 1
Abstract
Systems and methods for overlaying images of teleconference participants with shared documents are provided. A request to initiate a document sharing operation is received to share a document displayed via a first Graphical User Interface (GUI) on a first client device associated with a first participant of the teleconference with a second participant of the teleconference via a GUI on a second client device. Image data corresponding to a view of the first participant in the surrounding environment is also received. An image depicting the first participant is obtained based on the received image data. One or more regions of a document that satisfy one or more image placement criteria are identified. The document and the image depicting the first participant are provided for presentation via a second GUI on a second client device. An image depicting the first participant is presented at an area of the identified one or more areas of the document.
Description
Technical Field
Aspects and embodiments of the present disclosure relate to overlaying images of teleconferencing participants with a shared document.
Background
Video or audio based teleconferencing discussions can be conducted between multiple participants via a conference platform. The conferencing platform includes tools that allow multiple client devices to connect over a network and share each other's audio data (e.g., the user's voice recorded via a microphone of the client device) and/or video data (e.g., video captured by a camera of the client device, or video captured from a screen image of the client device) for efficient communication. The conference platform can also include tools that allow a participant of the teleconference to share documents displayed via a User Interface (UI) on a client device associated with the participant with other participants of the teleconference.
Disclosure of Invention
The following summary is a simplified summary of the disclosure in order to provide a basic understanding of some aspects of the disclosure. This summary is not an extensive overview of the disclosure. It is intended to neither identify key or critical elements of the disclosure nor delineate any scope of the particular embodiments of the disclosure or any scope of the claims. Its sole purpose is to present some concepts of the disclosure in a simplified form as a prelude to the more detailed description that is presented later.
In some implementations, a system and method for overlaying images of teleconference participants with a shared document is disclosed. In one embodiment, a request to initiate a document sharing operation is received to share a document displayed via a first Graphical User Interface (GUI) on a first client device associated with a first participant of a teleconference with a second participant of the teleconference via a GUI on the second client device. Image data corresponding to a view of the first participant in the surrounding environment is also received. An image depicting the first participant is obtained based on the received image data. One or more regions of a document that satisfy one or more image placement criteria are identified. A document and image depicting the first participant is provided for presentation via a second GUI on a second client device. An image depicting the first participant is presented at an area of the identified one or more areas of the document.
In some implementations, another system and method for overlaying images of teleconference participants with a shared document is disclosed. In one implementation, a document displayed via a first Graphical User Interface (GUI) on a first client device associated with a first participant of a teleconference is shared with a second participant of the teleconference via a GUI on the second client device. A request is received to display, via a second GUI, an image depicting a first participant of the teleconference and a document shared with a second participant. Image data corresponding to a view of a first participant in a surrounding environment is received. An image depicting the first participant is obtained based on the received image data. At least one of a format or a direction of one or more content items of the shared document is modified in view of an image depicting the first participant. An image depicting the first participant and the modified document are provided for presentation via a second GUI on a second client device.
Drawings
The various aspects and embodiments of the present disclosure will be understood more fully from the detailed description given below and from the accompanying drawings of the various aspects and embodiments, which, however, should not be taken to limit the disclosure to the specific aspects or embodiments, but are for explanation and understanding only.
Fig. 1 illustrates an example system architecture according to an embodiment of this disclosure.
Fig. 2 is a block diagram illustrating an example conference platform and an example context extraction engine according to an embodiment of the disclosure.
Fig. 3 is a block diagram illustrating an example conference platform and an example image overlay engine according to an embodiment of the disclosure.
Fig. 4 depicts a flowchart of an example method of overlaying an image of a conference call participant with a shared document, in accordance with an embodiment of the present disclosure.
Fig. 5A-5C illustrate examples of overlaying images of teleconference participants with shared documents for presentation via a GUI according to embodiments of the present disclosure.
Fig. 6A-6C illustrate another example of overlaying images of teleconference participants with a shared document for presentation via a GUI in accordance with embodiments of the present disclosure.
Fig. 7 depicts a flowchart of another example method of overlaying an image of a conference call participant with a shared document in accordance with an embodiment of the present disclosure.
Fig. 8A-8B illustrate another example of overlaying images of teleconference participants with a shared document for presentation via a GUI in accordance with embodiments of the present disclosure.
Fig. 9A-9B illustrate examples of overlaying images of multiple teleconferencing participants with a shared document for presentation via a GUI in accordance with embodiments of the present disclosure.
Fig. 10 is a block diagram illustrating an exemplary computer system according to an embodiment of the present disclosure.
Detailed Description
Aspects of the present disclosure relate to overlaying images of teleconferencing participants with a shared document. The conference platform is capable of implementing video or audio-based teleconferencing discussions among a plurality of participants via respective client devices that are connected over a network and that share each other's audio data (e.g., the user's voice recorded via a microphone of the client device) and/or video data (e.g., video captured by a camera of the client device) during the teleconference. In some cases, the conferencing platform can enable a large number of client devices (e.g., up to one hundred or more client devices) to connect via a teleconference.
It may be common for participants of a live teleconference (e.g., video teleconference) to use shared documents (e.g., slide presentation documents, word processing documents, web page documents, etc.) to interface with other participants of the teleconference. For example, a presenter of a teleconference can prepare a document that includes content that the presenter plans to discuss during the teleconference. Existing conference platforms enable a presenter to share documents displayed via the GUI of the client device associated with the presenter with other participants of the phone via the conference platform GUI on the respective client device as the presenter discusses the content included in the shared document. However, such a conference platform cannot effectively display the content of the shared document while simultaneously displaying images depicting the presenter via the conference platform GUI on the client devices associated with the other participants. For example, some existing conference platforms may not provide images depicting the conference call presenter and documents shared via the conference platform GUI, which prevents the presenter from effectively interfacing with the participants via the video features of the conference platform. As a result, the attention of the conference call participants is not captured for a long time (or not at all), and presenting the shared document during the conference call may give the impression of non-personal or mechanical. Other existing conferencing platforms may display the content of the shared document via a first portion of the conferencing platform GUI and display an image depicting the presenter via a second portion of the conferencing platform GUI. However, assuming that the presenter's image is displayed in a separate portion of the conference platform GUI from the content of the shared document, the participant may not be able to focus on or view the visual cues or gestures provided by the presenter while consuming the content provided by the shared document.
Hardware limitations associated with different client devices connected to the teleconference platform may prevent the conference platform GUI from simultaneously or effectively displaying both the content of the shared document and the presenter's image. Existing conferencing platforms do not provide a mechanism that can modify the display of a conferencing platform GUI on a client device associated with a participant of a conference call in view of one or more hardware limitations associated with the client device. In an illustrative example, a client device associated with a presenter of a teleconference can include a large display screen. Client devices associated with some participants of a teleconference may include a large display screen, while client devices associated with other participants of the telephone may include a small display screen. Existing conferencing platforms can provide the same document for presentation via the conferencing platform GUI at each client device, regardless of the size of the display screen at the respective client device. Thus, a participant accessing a teleconference via a client device that includes a small display screen may not easily consume all of the content of a document shared by the presenter. As a result, the presenter may not be able to effectively engage these participants during the teleconference.
Aspects of the present disclosure address the above and other deficiencies by providing techniques for layering images of teleconference presenter with documents shared via a conference platform GUI on a client device associated with a participant of a teleconference. A client device associated with a presenter of the teleconference can send a request to initiate a document sharing operation to the conference platform to share a document displayed via a GUI of the client device with participants of the teleconference via a GUI on the client device associated with the participants of the teleconference. Additionally or in response to receiving a request to initiate a document sharing operation, the conference platform can receive image data (e.g., pixel data, etc.) from a client device associated with the teleconference presenter. The image data can correspond to a view of the first participant in a surrounding environment (e.g., a background environment). The conference platform is capable of obtaining an image depicting the presenter based on the received image data. For example, the received image data can include a first set of pixels associated with the presenter and a second set of pixels associated with the surrounding environment. The conference platform can extract the identified first set of pixels from the received image data and generate an image depicting the first participant based on the extracted first set of pixels.
In some embodiments, the conferencing platform can identify one or more regions of the document that satisfy one or more image placement criteria. In one example, an area of a document can meet image placement criteria if the area does not include any content or does not include content related to a presentation (e.g., the area includes a company logo, etc.). In other or similar embodiments, the conferencing platform can modify the format or orientation of one or more content items of the shared document to accommodate images depicting the presenter. For example, if the title of the slide presentation document is large in size and occupies a large amount of space in the conference platform GUI, the conference platform can reduce the size of the title or can move a portion of the title to another area of the slide to accommodate an image depicting the presenter. The conference platform can provide documents and images depicting the presenter for presentation via a conference GUI on a client device associated with the conference participant. The image depicting the presenter can be displayed at an area that was previously identified (or modified) to meet one or more image placement criteria.
A technical solution to the above technical problem of the conventional technology may include overlaying an image of a conference call presenter with a document shared via a conference platform GUI on a client device associated with a participant of the conference call. In some embodiments, the conference platform may identify one or more regions of the document (e.g., one or more regions that do not include content, etc.) that satisfy one or more placement criteria for presenting images depicting the teleconference presenter. In other or similar embodiments, the conferencing platform may modify one or more content items of the document to accommodate images depicting teleconference presenter. Thus, an image depicting the teleconference presenter is presented in an area of the shared document that does not interfere (or minimally interferes) with the existing content of the document.
Another technical solution to the above technical problem is to modify the presentation of documents and images depicting a presenter via a conference platform GUI on a particular client device in view of one or more hardware limitations associated with the client device. The conference platform can obtain data indicative of one or more hardware limitations (e.g., image resolution limitations, screen sizes, etc.) associated with client devices associated with conference call participants. If the one or more hardware limitations meet hardware limitation criteria (e.g., below a threshold image resolution, a threshold screen size, etc.), the conferencing platform can modify the presentation of the document and the image depicting the presenter in view of the one or more hardware limitations. For example, the conference platform can present the first portion of content included in the document and an image depicting the presenter via the conference platform GUI. In response to detecting that the presenter has shifted focus of the presentation to the second portion of content, the conference platform can update a conference platform GUI at the client device to display the second portion of content included in the document with an image depicting the platform.
Thus, technical effects may include improving the presentation of images of teleconference presenter and documents shared with participants of a teleconference. By providing a mechanism to present images of a conference call presenter in areas that do not interfere (or minimally interfere) with the existing content of the shared document, all important information is presented to participants of the conference call in an unobstructed and convenient manner while mimicking a face-to-face conference experience, which enables the presenter to effectively interface with participants of the conference call. Further, by modifying the conference platform GUI in view of hardware limitations (e.g., image resolution limitations, display screen size, etc.) of client devices associated with the conference call participants, both the conference call presenter image and the shared document can be presented to the participants in a format compatible with the hardware limitations (e.g., such that all content is displayed on a limited screen of the participant devices). Thus, the participant associated with the client device can consume all content included in the document and depict the image of the presenter, and the presenter of the teleconference can effectively interface with the participant via the modified conference platform GUI.
Fig. 1 illustrates an example system architecture 100 according to an embodiment of the disclosure. System architecture 100 (also referred to herein as a "system") includes client devices 102A-N, a data store 110, and a conference platform 120, each connected to a network 108. In some embodiments, the system 100 can additionally include a prediction system 112. The prediction system 112 can include one or more server machines 130-150, each server connected to the network 108.
In an embodiment, the network 108 can include a public network (e.g., the internet), a private network (e.g., a Local Area Network (LAN) or Wide Area Network (WAN)), a wired network (e.g., ethernet), a wireless network (e.g., an 802.11 network or Wi-Fi network), a cellular network (e.g., a Long Term Evolution (LTE) network), a router, a hub, a switch, a server computer, and/or a combination thereof.
In some implementations, the data store 110 is a persistent store that is capable of storing data as well as data structures for marking, organizing, and indexing data. According to embodiments described herein, the data items can include audio data and/or image data. In accordance with embodiments described herein, in other or similar embodiments, the data items can correspond to documents displayed via a Graphical User Interface (GUI) on the client device 102. The data store 110 can be hosted by one or more storage devices, such as main memory, magnetic or optical storage-based disks, tape or hard drives, NAS, SAN, and the like. In some implementations, data store 110 can be a file server attached to a network, while in other embodiments data store 110 can be some other type of persistent storage, such as an object-oriented database, a relational database, etc., that can be hosted by conferencing platform 120 or one or more different machines coupled to conferencing platform 120 via network 108.
Conference platform 120 may enable users of client devices 102A-N to connect to each other via a teleconference, such as a video teleconference or an audio teleconference. Teleconferencing refers to audio-based telephones and/or video-based telephones in which the participants of the telephone are able to connect with a plurality of additional participants. Conference platform 120 can allow users to join and participate in video teleconferencing and/or audio teleconferencing with other users of the platform. Although embodiments of the present disclosure relate to multiple participants (e.g., 3 or more) connected via a teleconference, it should be noted that embodiments of the present disclosure can be implemented with any number of participants (e.g., 2 or more) connected via a teleconference.
Client devices 102A-N can each include a computing device, such as a Personal Computer (PC), notebook computer, mobile phone, smart phone, tablet computer, netbook computer, networked television, and the like. In some implementations, the client devices 102A-N may also be referred to as "user devices". Each client device 102A-N can include a web browser and/or a client application (e.g., a mobile application or desktop application). In some implementations, the web browser and/or client application can display a User Interface (UI) provided by conference platform 120 for a user to access conference platform 120. For example, a user can join and participate in a video teleconference or an audio teleconference via a UI provided by conference platform 120 and presented by a web browser or client application.
Each client device 102A-N can include one or more audiovisual components that can generate audio and/or image data to be streamed to conference platform 120. In some implementations, the audiovisual component can include a device (e.g., a camera) configured to capture an image and generate image data associated with the captured image. For example, a camera of the client device 102 can capture images of teleconference participants in the surrounding environment (e.g., background) during a teleconference. In additional or alternative implementations, the audiovisual assembly can include a device (e.g., a microphone) to capture an audio signal representative of user speech and generate audio data (e.g., an audio file) based on the captured audio signal. The audiovisual component can include another device (e.g., a speaker) that outputs audio data to a user associated with a particular client device 102A-N.
In some implementations, conference platform 120 can include conference management component 122. Conference management component 122 is configured to manage teleconferences between multiple users of conference platform 120. In some implementations, conference management component 122 can provide a GUI (referred to herein as a conference platform GUI) to each client device to enable users to view and listen to each other during a teleconference. In some embodiments, meeting management component 122 can also enable users to share documents (e.g., slide presentation documents, word processing documents, web page documents, etc.) displayed via GUIs on client devices associated with other users. For example, during a teleconference, conference management component 122 can receive a request to share a document displayed via a GUI on a first client device associated with a first participant of the teleconference with other participants of the teleconference. In some embodiments, conference management platform 122 can modify a conference platform GUI at client device 102 associated with other teleconference participants to display at least a portion of the shared document.
In some embodiments, conference management component 122 can overlay a document that depicts an image of a participant of a teleconference shared with the participant and present the shared document with the overlaid image to other participants via a conference platform GUI on a client device associated with the other participants. For example, a participant in a teleconference can prepare a document (e.g., a slide presentation document) for presentation to other participants in the teleconference. In some embodiments, such participants are referred to as presenter. Conference management component 122 can receive requests from client devices 102 associated with presenter to share documents with other teleconference participants via conference platform GUIs on respective client devices 102 associated with the other teleconference participants. In some embodiments, conference management component 122 is also capable of receiving additional requests to overlay images depicting the presenter with the shared document.
In response to receiving one or more requests from client devices 102 associated with the presenter, conference management component 122 can obtain an image depicting the presenter. As previously described, the audiovisual components of each client device 102A-N are capable of capturing images and generating image data associated with the captured images. A camera of the client device 102 associated with the presenter is capable of capturing an image of the presenter in the surrounding environment and generating image data associated with the captured image. In some embodiments, conference management component 122 can receive image data generated by client device 102 associated with the presenter and can obtain an image depicting the presenter from the received image data. In some embodiments, conference management component 122 can provide image data received from client device 102 associated with a presenter to background extraction engine 124. In some embodiments, the background extraction engine 124 can be configured to parse the image data and identify portions of the image data corresponding to participants of the teleconference and portions of the image data corresponding to the environment surrounding the participants. For example, in some embodiments, image data received from a client device 102 associated with a presenter can include a first set of pixels associated with the presenter and a second set of pixels associated with the surrounding environment. The background extraction engine 124 can parse the received image data to identify a first set of pixels associated with the presenter and can extract the first set of pixels from the image data. In accordance with embodiments described below, in other or similar embodiments, the background extraction engine 124 can be configured to identify portions of image data corresponding to conference call participants based on one or more outputs of a machine learning model. Conference management component 122 and/or background extraction engine 124 can generate an image depicting the presenter based on the extracted first set of pixels. Further details regarding the background extraction engine 124 are provided below with reference to fig. 2.
Conference platform 120 can also include an image overlay engine 126 configured to overlay images depicting the presenter with documents shared with participants of the teleconference. In some embodiments, the image overlay engine 126 can identify one or more regions of the document that meet one or more image placement criteria and can cause an image depicting the presenter to be presented at one of the identified regions. For example, if an area of a document does not include any content (e.g., is empty space), the area can meet image placement criteria. The image overlay engine 126 can identify one or more regions that do not include any content and can select one of the identified one or more regions to include an image depicting the presenter. In another example, the image overlay engine 126 may not identify any regions in the document that meet the image placement criteria. In such embodiments, the image overlay engine 126 can modify the size, shape, and/or transparency of the image depicting the presenter and can overlay the modified image depicting the presenter with the document in accordance with embodiments described herein. Further details regarding the image overlay engine 126 are provided with reference to FIG. 3.
In response to image overlay engine 126 identifying an area that includes an image (or modified image) depicting the presenter, conference management component 122 can provide the document and the image depicting the presenter for presentation via a conference platform GUI on a client device associated with other participants of the conference call. An image depicting the presenter can be included at an area of the document identified by the image overlay engine 126. In some embodiments, conference management component 122 can receive a request from a client device 102 associated with a presenter to move an image depicting the presenter from the identified region to another region of the document. In such embodiments, conference management component 122 can move the image depicting the presenter to another area of the document upon request. In some embodiments, the requested area of the document can include one or more content items. In some embodiments, conference management platform 122 can modify a format or orientation of an image and/or one or more content items depicting a presenter in view of the image. Further details regarding meeting management component 122 modifying content items depicting images and/or documents of a presenter are provided herein.
As described above, in some embodiments, the system architecture can include a prediction system 112 that includes one or more server machines 130-150. In some embodiments, the context extraction engine 124 described above can be part of the prediction system 112. In such embodiments, the prediction system 112 can be configured to train an image extraction model that the background extraction engine 124 can use to identify image portions corresponding to teleconference participants and image portions corresponding to the environment surrounding the teleconference participants. In additional or alternative embodiments, the prediction system 112 can include a gesture detection engine 151. In such embodiments, the prediction system 112 can be configured to train a gesture detection model that the gesture detection engine 151 can use to detect gestures made by teleconference participants during a teleconference and generate GUI elements corresponding to the detected gestures for presentation at the conference platform GUI at the client device 102 associated with other participants of the teleconference. Further details regarding the image extraction model and the gesture detection model are provided herein.
Prediction system 112 can include at least training set generator 131, training engine 141, and one or more machine learning models 160A-N. In some embodiments, the prediction system 112 can also include a background extraction engine 124 and/or a gesture detection engine 151, as described above. The server machine 130 can include a training set generator 131 that can generate training data (e.g., a training input set and a target output set) to train the ML models 160A-N. For the image extraction model, training data can be generated based on images that have been previously captured by the audiovisual component of the client device associated with the participant of the previous teleconference hosted by the conferencing platform 120. For example, during a previous teleconference, an audiovisual component (e.g., a camera) of a client device associated with a teleconference participant can generate images depicting the teleconference participant and the environment surrounding the teleconference participant. In some embodiments, the teleconference participant is able to provide an indication depicting an image portion of the teleconference participant (e.g., via a conference platform GUI at the client device) and/or an indication depicting an image portion of the environment surrounding the teleconference participant. The client device can send the generated image and one or more indications provided by the conference call participants to conference platform 120 (e.g., via network 108). In response to receiving the generated image and one or more indications, conference management component 122 (or another component of conference platform 120) can store the received image and indications as training data in data store 110.
In other or similar embodiments, the teleconferencing participant may not provide an indication depicting an image portion of the teleconferencing participant and/or an indication depicting an image portion of the environment surrounding the teleconferencing participant. In such embodiments, client device 102 associated with the conference call participant can send the generated image to the conference platform and conference management component 122 (or another component of conference platform 120) can store the generated image at data store 110. In some embodiments, a client device 102 associated with another user (e.g., programmer, developer, operator, etc.) of the conference platform 120 (or another platform connected to the platform 120 via the network 108 or another network) is able to obtain the generated image from the data store 110. In such embodiments, other users can provide an indication depicting an image portion of the teleconference participant and/or an indication depicting an image portion of the environment surrounding the teleconference participant. In accordance with the previously described embodiments, client device 102 associated with another user can send one or more indications to conferencing platform 120. As described above, conference management component 122 can store one or more provided indications with the image as training data in data store 110.
As described above, in some embodiments, images generated by client devices 102 associated with conference call participants can depict images of participants during a previous conference call hosted by conference platform 120. In other or similar embodiments, the image generated by client device 102 can depict an image of a participant just prior to a teleconference to be hosted by conference platform 120. For example, according to embodiments described herein, a conference call participant can be a presenter of a conference call and can prepare one or more documents to be shared during the conference call. Prior to the teleconference, the teleconference presenter can cause an audiovisual component (e.g., a camera) of a client device associated with the presenter to generate one or more images depicting the presenter prior to the teleconference. In some embodiments, one or more of the generated images can depict conditions associated with the presenter and/or an environment surrounding the presenter that are expected to be captured by an audiovisual component of the client device during the teleconference. For example, the generated image can depict an intended location or direction of the presenter during the teleconference, an intended garment of the presenter during the teleconference, an intended location of one or more objects included in the presenter's surroundings during the teleconference, an intended lighting condition associated with the presenter and/or the presenter's surroundings during the teleconference, and so forth. In some embodiments, as previously described, client device 102 associated with the presenter can send the generated image to conference platform 120. In other or similar embodiments, as previously described, the presenter can provide, via the GUI of the client device, an indication of a portion of each of the one or more generated images depicting the presenter and/or an indication of a portion of the one or more generated images depicting the environment surrounding the presenter. In such embodiments, client device 102 associated with the presenter can send one or more generated images and one or more provided indications to conference platform 120, as previously described. As described above, the one or more generated images and the one or more indications can be stored as training data to the data store 110.
The training set generator 131 of the server machine 130 can obtain training data from the data store 110 and can generate a training set based on the obtained training data. The training set can include a subset of training inputs based on the retrieved training data and a target output. As described above, the subset of training inputs can include image data associated with images depicting teleconference participants (i.e., generated during or prior to a previous teleconference). The training set generator 131 is capable of generating one or more target outputs for each of the training input subsets. In some embodiments, training set generator 131 can determine a set of pixels corresponding to the conference call participant and a set of pixels corresponding to the environment surrounding the conference call participant based on one or more indications associated with each image of the training data. The target outputs of the respective training inputs of the training set can correspond to at least an indication of the set of pixels associated with the conference call participant.
The server 140 can include a training engine 141. Training engine 141 can use training data from training set generator 131 to train machine learning models 160A-N. The machine learning models 160A-N can refer to model artifacts created by the training engine 141 using training data including training inputs and corresponding target outputs (correct answers to the respective training inputs). The training engine 141 can find patterns in the training data that map training inputs to target outputs (answers to be predicted) and provide machine learning models 160A-N that capture these patterns. For convenience, the remainder of this disclosure will refer to this embodiment as a neural network, even though some embodiments may employ SVM or other types of learning machines in place of or in addition to the neural network.
The background extraction engine 124 of the server 150 can provide image data associated with one or more images generated by an audiovisual component (e.g., camera) of the client device 102 associated with a participant (e.g., presenter) of a current teleconference as input to the trained machine learning model 160 to obtain one or more outputs. In some embodiments, the provided image data can be associated with an image depicting a teleconference presenter under the same or similar conditions as described above in association with one or more images used to train the machine learning model 160. The model 160 can be used to determine the likelihood that each pixel of the provided image data corresponds to a participant of the current teleconference or the environment surrounding the teleconference participant. In some embodiments, one or more outputs of model 160 can include data indicating a confidence level that one or more pixels of image data correspond to a teleconference participant (or an environment surrounding the teleconference participant). In accordance with embodiments provided herein (e.g., with respect to fig. 2), in response to determining that a confidence level associated with one or more pixels of image data meets a confidence criterion (e.g., meets or exceeds a threshold level of confidence), the background extraction engine 124 can determine that the one or more pixels correspond to a view of a teleconference participant and can extract an image depicting the teleconference participant from the provided image data.
As described above, in some embodiments, prediction system 112 can be configured to train a gesture detection model that gesture detection engine 151 uses to detect gestures made by teleconference participants during a teleconference hosted by conference platform 120. In some embodiments, training set generator 131 can generate training data to train the gesture detection model based on image and/or video data that an audiovisual component of a client device associated with a participant of a previous teleconference hosted by conferencing platform 120 has previously captured. For example, during a previous teleconference, an audiovisual component (e.g., a camera) of the client device 102 associated with the teleconference participant can generate a video depicting the teleconference participant providing the gesture (e.g., with his or her hand, with an object such as a pen or laser pointer, etc.). In some embodiments, the teleconference participant can provide (e.g., during or after the teleconference) an indication of whether the gesture is directed to one or more content items displayed in a document presented via the conference platform GUI of the client device 102. In an additional or alternative embodiment, the teleconference participant is able to provide another indication that one or more content items of the presented document are the focus of the gesture provided. In some embodiments, the teleconference participant can provide one or more indications associated with gestures and/or content items of the presented document via the conference platform GUI at the client device 102. In accordance with the previously described embodiments, in response to receiving one or more indications via conference platform GUI, client device 102 is capable of sending video data associated with the generated video and the one or more indications to conference platform 120. In some embodiments, the client device 102 is also capable of sending one or more portions of a document presented via the conference platform GUI at the time of capturing video depicting the gesture. As described above, conference management component 122 (or another component of conference platform 120) can store received video data, one or more indications, and/or documents as training data at data store 110.
As described above, the training set generator 131 of the server machine 130 can obtain training data from the data store 110 and can generate a training set based on the obtained training data. The training set can include a subset of training inputs based on the obtained training data and a target output. The subset of training inputs can include video data associated with video depicting gestures provided by teleconferencing participants. In some embodiments, the subset of training inputs can also include documents presented via the conference platform GUI at the time of capturing the video depicting the gesture. The training set generator 131 is capable of generating one or more target outputs for each of a subset of training inputs. In some embodiments, training set generator 131 can determine, based on one or more indications associated with respective video data of the training data, whether a gesture depicted in the video captured by client device 102 is made with respect to one or more content items of a document presented via a conference platform GUI of client device 102, and can generate a target output based on the determination. In other or similar embodiments, training set generator 131 can determine one or more content items of a document that are subject of a gesture based on one or more indications associated with a respective video. The training set generator 131 can generate additional target outputs indicative of the determined one or more content items.
According to the previously described embodiment, training engine 141 can use training data from training set generator 131 to train machine learning models 160A-N. Gesture detection engine 151 can provide video data associated with one or more videos generated by an audiovisual component (e.g., camera) of a client device associated with a participant (e.g., presenter) of a current teleconference as input to trained machine learning model 160 to obtain one or more outputs. Model 160 can determine a likelihood that a gesture depicted in a video associated with the video data is directed to one or more content items of a document currently displayed via a conference platform GUI of a client device associated with one or more participants of a current teleconference. For example, one or more outputs of the model 160 can provide a level of confidence that the gesture depicted in the video is directed to a corresponding content item included in the document. In response to determining that the confidence level exceeds the threshold confidence level, gesture detection engine 151 can determine that a participant in the teleconference may be gesturing with the corresponding content item. Gesture detection engine 151 can generate GUI elements that highlight respective content items that are gestured by teleconference participants (or send instructions to client devices associated with one or more participants of the teleconference to generate GUI elements). Gesture detection engine 151 can update the conference platform GUI at each client device associated with the conference platform participant to include the generated GUI elements.
In some implementations, conference platform 120 and/or server machines 130-150 can run on one or more computing devices (e.g., rack-mounted servers, router computers, server computers, personal computers, mainframe computers, notebook computers, tablet computers, desktop computers, etc.), data stores (e.g., hard disk, memory, database), networks, software components, and/or hardware components that can be used to enable users to connect with other users via teleconferencing. In some implementations, the functionality of conference platform 120 may be provided by more than one machine. For example, in some implementations, the functionality of conference management component 122, background extraction engine 124, and image overlay engine 126 may be provided by two or more separate server machines. Conference platform 120 may also include web sites (e.g., web pages) or application backend software that may be used to enable users to connect with other users via teleconferences.
It should be noted that in some other embodiments, the functionality of server machines 130, 140, and 150 or conferencing platform 120 can be provided by a fewer number of machines. For example, in some embodiments, server machines 130 and 140 can be integrated into a single machine, while in other embodiments, server machines 130, 140, and 150 can be integrated into multiple machines. Further, in some implementations, one or more of server machines 130, 140, and 150 can be integrated into conference platform 120.
In general, in other embodiments, the functions described in the embodiments as being performed by conference platform 120 can also be performed on client devices 102A-N, if appropriate. Furthermore, the functionality attributed to a particular component can be performed by different or multiple components operating together. Conference platform 120 may also be accessible as a service provided to other systems or devices through an appropriate application programming interface and is therefore not limited to use in a website.
Although embodiments of the present disclosure are discussed in terms of conference platform 120 and users of conference platform 120 participating in video and/or audio teleconferencing, these embodiments are generally applicable to any type of telephone call or teleconference between users. Embodiments of the present disclosure are not limited to content sharing platforms that provide teleconferencing tools to users.
In embodiments of the present disclosure, a "user" can be represented as a single individual. However, other embodiments of the present disclosure contemplate a "user" as an entity controlled by a set of users and/or automation sources. For example, a group of individual users that are joined as a community in a social network may be considered a "user. In another example, the automated consumer can be an automated ingestion pipeline of the conference platform 120, such as a theme tunnel.
In addition to the above description, controls may be provided to the user that allow the user to select whether and when the systems, programs, or features described herein are capable of collecting user information (e.g., information about the user's social network, social actions or activities, profession, user preferences, or the user's current location), and whether to send content or communications from the server to the user. In addition, certain data can be processed in one or more ways to remove personal identity information before it is stored or used. For example, the identity of the user can be processed such that personal identity information of the user cannot be determined, or the geographic location of the user (such as to a city, zip code, or state level) can be summarized in the case of obtaining location information such that a specific location of the user cannot be determined. Thus, the user can control what information is collected about the user, how that information is used, and what information is provided to the user.
Fig. 2 is a block diagram illustrating a conferencing platform 120 and a background extraction engine 124 for conferencing platform 120 in accordance with an embodiment of the present disclosure. As described with respect to fig. 1, conferencing platform 120 can provide a user of client device 102 with tools to join and participate in video and/or audio teleconferences. Conference platform 120 can include conference management component 122. As also described with respect to fig. 1, the background extraction engine 124 can be configured to extract images of participants (e.g., presenter) depicting a conference call from images corresponding to the participants' views in the surrounding environment. In some embodiments, background extraction engine 124 can be included as a component of conference platform 120. In other or similar embodiments, the background extraction engine can be separate from the conference platform 120, as shown in fig. 2. For example, the context extraction engine 124 can reside on one or more server machines separate from one or more server machines associated with the conference platform 120. In another example, the context extraction engine 124 can be communicatively coupled to a plurality of platforms (e.g., conferencing platform 120, content sharing platform, document sharing platform, etc.) via one or more networks. In such examples, the background extraction engine 124 can be configured to extract images of users of such platforms from the image data in accordance with embodiments described herein.
In some embodiments, the background extraction engine 124 can include at least an extraction component 220 and an image generation component 222. As described with respect to fig. 1, the audiovisual component of the client device 102 is capable of capturing an image and generating image data 210 associated with the captured image. In some embodiments, the generated image data 210 can include two or more sets of pixels, each set of pixels corresponding to a different portion of the view depicted in the captured image. For example, the first set of pixels can correspond to a portion of a view depicted in a captured image associated with a participant of a teleconference hosted by the conference platform 120. The second set of pixels can correspond to a portion of the view associated with the environment surrounding the participant, also referred to as the participant's background. The client device 102 can send the generated image data 210 associated with the captured image to the conferencing platform 120 (e.g., during a teleconference with one or more additional users of the conferencing platform 120). In response to receiving image data from client device 102, conference platform 120 can provide received image data 210 to background extraction engine 124. In some embodiments, the background extraction engine 124 can store the received image data 210 in a memory (e.g., the data store 110) associated with the background extraction engine 124 and/or the conference platform 120.
The extraction component 220 of the background extraction engine 124 can be configured to obtain an image depicting a participant of the teleconference (referred to herein as a participant image 212) from the image data 210 generated by the client device 102. As described above, the image data can include a first set of pixels corresponding to a view of a participant of the teleconference and a second set of pixels corresponding to a view of an environment surrounding the participant. In some embodiments, the extraction component 220 can parse the image data 210 to identify a first set of pixels and a second set of pixels. For example, in some embodiments, a participant of a conference call can provide an indication of a first portion of the generated image corresponding to the participant and a second portion of the generated image corresponding to the surrounding environment (e.g., by drawing an outline of the participant using elements of the conference platform GUI). The extraction component 220 can identify a first set of pixels associated with a first portion of the generated image and a second set of pixels associated with a second portion of the generated image in view of the indication provided by the teleconferencing participant. In another example, pixels of image data 210 corresponding to the environment surrounding the teleconference participant can be associated with a unique color that is different from any color associated with pixels of image data 210 corresponding to the teleconference participant (e.g., if the teleconference participant is sitting or standing in front of a green screen). The extraction component 220 can determine that each pixel of the image data 210 associated with the unique color is included in a second set of pixels corresponding to the ambient environment and that each pixel of the image data 210 not associated with the unique color is included in a first set of pixels corresponding to the conference call participant.
In other or similar embodiments, the extraction component 220 can identify a first set of pixels and a second set of pixels of the image data 210 based on the output of the trained image extraction model 234. In some embodiments, the trained image extraction model 234 can be a machine learning model trained to determine the likelihood that each pixel of the image data 210 corresponds to a teleconference participant or the environment surrounding the teleconference participant. In some embodiments, the trained image extraction model 234 can be trained by the prediction system 112 according to the embodiment described with respect to fig. 1. The extraction component 220 can provide the image data 210 generated by the client device 102 as input to the trained image extraction model 234 and obtain one or more outputs of the trained image extraction model 234. In some embodiments, the one or more obtained outputs can include data indicating a confidence level that one or more pixels of the image data 210 correspond to a teleconference participant (or an environment surrounding the teleconference participant). In response to determining that the confidence level associated with one or more pixels of image data 210 meets a confidence criterion (e.g., meets or exceeds a threshold confidence level), extraction component 220 can determine that the one or more pixels are included in a first set of pixels corresponding to a view of a conference call participant. In response to determining that the confidence level associated with the one or more pixels does not meet the confidence criteria (e.g., is below a threshold confidence level), the extraction component 220 can determine that the one or more pixels are included in a second set of pixels corresponding to a view of the environment surrounding the conference call participant.
In response to identifying the first set of pixels from the image data 210, the extraction component 220 can extract the first set of pixels and store the extracted pixels 232 in the data store 110. In some embodiments, the image generation component 222 of the background extraction engine 124 is capable of generating the participant image 212 based on the extracted pixels 232. In accordance with embodiments described herein, in response to image generation component 222 generating participant image 212, background extraction engine 124 can send generated participant image 212 to meeting management component 122 to overlay with the shared document.
Fig. 3 is a block diagram illustrating an example conference platform 120 and an example image overlay engine 126 for conference platform 120 according to an embodiment of the disclosure. As described with respect to fig. 1, conference management component 122 can enable teleconference participants to share documents displayed via GUIs on client devices associated with teleconference participants with other participants of the teleconference. For example, a presenter of a teleconference can prepare a slide presentation document for sharing with participants during the teleconference. The conference platform GUI provided to the client device 102 associated with the presenter can include one or more GUI elements that enable the presenter to initiate a document sharing operation to share documents with conference platform participants. Client device 102 can send a request to conference platform 120 to initiate a document sharing operation in response to detecting that the presenter has accessed (e.g., clicked on) one or more GUI elements.
Conference management component 122 of conference platform 120 can receive a request from client device 102 to initiate a document sharing operation. In some embodiments, conference management component 122 is also capable of receiving images depicting document 310 (or a portion of a document) to be shared with other participants of a teleconference. In other or similar embodiments, meeting management component 122 can receive an identifier of document 310 stored in a data store associated with a document sharing platform communicatively coupled to meeting platform 120. In such embodiments, conference management component 122 can retrieve document 310 from the data store (e.g., in response to determining that the presenter is permitted to access document 310 from the data store). In some embodiments, conference management component 122 is also capable of receiving image data 210 generated by client device 102. According to the embodiment described with respect to fig. 2, conference management 122 can obtain an image depicting the presenter based on received image data 210 (e.g., by providing image data 210 to background extraction engine 124).
In response to receiving the participant image 212 and the shared document 310, the conference management component 122 can provide the participant image 212 and the shared document 310 to the image overlay engine 126. As described with respect to fig. 1, the image overlay engine 126 can be configured to overlay the participant image 212 with the shared document 310. In some embodiments, image overlay engine 126 can be included as a component of conference platform 120. In other or similar embodiments, the image overlay engine 126 can be separate from the conference platform 120, as shown in fig. 2. For example, image overlay engine 126 can reside on one or more server machines separate from one or more server machines associated with conference platform 120.
The image overlay engine 126 can include at least a document region identifier component 320, a GUI layout component 322, and an overlay component 324. In response to the image overlay engine 126 receiving the shared document 310 from the meeting management component 122, the document region identifier component 320 can identify one or more regions of the shared document 310 that satisfy one or more image placement criteria associated with the shared document 310. The one or more image placement criteria correspond to a set of features associated with a target region of the shared document 310 for image placement. For example, if the region of the shared document 310 does not include any content (e.g., white space), the region can meet the image placement criteria. In some embodiments, such an area is referred to as a blank area of document 310. In another example, if an area of shared document 310 includes one or more content items that can be modified to accommodate presenter image 212, the area can satisfy another image placement criteria. In some embodiments, the set of features associated with the target region of the shared document 310 can be defined by the participants who have requested to share the document 310 with other participants of the teleconference (i.e., teleconference presenter). In other or similar embodiments, the set of features can be determined by conference platform 120 in view of test and/or runtime data collected for one or more teleconferences at one or more client devices 102 connected to conference platform 120. Further details associated with one or more image placement criteria are provided herein.
In some embodiments, the document region identifier component 320 can identify one or more regions that meet one or more image placement criteria based on metadata 332 associated with the document and/or metadata 334 associated with the participant image 212. The document metadata 332 can include data associated with features of one or more regions of the shared document 310. For example, in some embodiments, the client device 102 can send a request to render an image of the shared document 310 and to share the document 310 with other participants of the teleconference. The client device 102 can also include document metadata 332 that includes pixel data associated with one or more regions of the shared document 310. In some embodiments, the pixel data can indicate a color associated with one or more pixels depicting an image of the shared document 310. In some embodiments, the image metadata 334 can include data associated with features of one or more portions of the participant image 212. For example, the image metadata 334 can include data associated with the size of the participant image 212, the shape of the participant image 212, and/or pixel data associated with one or more portions of the participant image 212.
The document region identifier component 320 can identify regions that satisfy one or more image placement criteria in view of the document metadata 332 and/or the image metadata 334. In some embodiments, the document area identifier component 320 can determine the size of the participant image 212 and/or the shape of the participant image 212 based on the image metadata 334. The document area identifier component 320 can also determine an image boundary associated with the participant 212 in view of the determined size and/or shape of the participant image 212. In some embodiments, the determined image boundaries can correspond to a maximum and/or minimum size associated with the participant image 212 at the region of the shared document 310. The determined image boundaries can also correspond to target shapes associated with the participant image 212 at the region of the shared document 310. For example, the document area identifier component 320 can determine that a target shape associated with the participant image 212 corresponds to a square in view of the determined size and/or shape of the participant image 212.
In some embodiments, the document region identifier component 320 can parse pixel data included in the document metadata 332 to identify regions in the shared document 310 that do not include any content. For example, the document area identifier 320 can determine that pixels corresponding to text content items of the shared document 310 are associated with black and pixels corresponding to the background of the shared document 310 are associated with white based on the document metadata 332. The document region identifier 320 is capable of parsing pixel data included in the document metadata 332 to determine a region of the shared document 310 that includes pixels associated with a white color (i.e., a region that does not include any text content items). In response to determining the regions of the shared document 310 that include pixels associated with a white color, the document region identifier component 320 can determine whether the size and/or shape of each respective region corresponds to the size and/or shape associated with the participant image 212. For example, the document region identifier component 320 can determine whether the size of the respective region is equal to or greater than the size associated with the participant image 212. In response to determining that the size of the respective region of the shared document 310 corresponds to the size and/or shape associated with the participant image 212, the document region identifier can determine that the respective region meets one or more image placement criteria.
In some embodiments, the document region identifier component 320 can determine whether the region of the shared document 310 meets one or more image placement criteria in view of pixel data associated with the participant image 212. For example, in some embodiments, pixel data associated with the participant image 212 can include an indication of a color associated with one or more pixels of the participant image 212. In response to identifying that the region of the shared document 310 corresponds to the size and/or shape associated with the participant image 212, the document region identifier component 320 can determine whether the color associated with the pixels of the identified region corresponds to the color associated with the pixels of the participant image 212. In response to determining that the color associated with the pixel of the identified region does not correspond to the color associated with the pixel of the participant image 212, the document region identifier component 320 can determine that one or more image placement criteria are met. In response to determining that the color associated with the pixel of the participant image 212 corresponds to the color associated with the pixel of the participant image 212, the document region identifier component 320 can determine that one or more image placement criteria are not met.
In some embodiments, the document region identifier component 320 can identify that multiple regions of the shared document 310 meet one or more image placement criteria. In such embodiments, the document region identifier component 320 can determine a region for presenting the participant image 212 image placement conditions associated with the shared document 310. The image placement conditions can be a predefined set of conditions associated with presenting the participant image 212 and sharing the document 310. In some embodiments, the image placement conditions can be defined by the participants requesting sharing of the document 310 with other participants of the teleconference. For example, before or during a teleconference, a participant can provide (i.e., via a GUI of a client device associated with the participant) an indication of a target image area for each document shared with other teleconference participants. In response to determining that the target image region corresponds to a region determined to meet one or more image placement criteria, the document region identifier component 320 can select the target image region for placement of the participant image 212.
In some embodiments, the document region identifier component 320 can determine that no region in the shared document 310 meets one or more image placement criteria. For example, the document area identifier component 320 can determine that no blank area in the shared document 310 corresponds to a size and/or shape associated with the participant image 212. In such examples, the document area identifier component 320 can determine whether the size and/or shape of the participant image 212 can be modified for presentation with the shared document 310. For example, in response to determining that the shared document 310 does not have a blank region corresponding to an image boundary associated with the participant image 212, the document region identifier component 320 can determine whether the size and/or shape of the participant image 212 can be modified to fit within the blank region of the shared document 310 in view of the maximum and/or minimum size associated with the participant image 212. In response to determining that the size and/or shape can be modified (e.g., the size of the participant image 212 can be made smaller) to fit within the blank area of the shared document 310, the document area identifier component 320 can modify the size and/or shape of the shared document 310, selecting an area of the shared document 310 for placement of the modified participant image 212.
In another example, the document region identifier component 320 can determine that the size of the participant image 212 cannot be modified to fit within the blank region of the shared document 310. In such embodiments, the document region identifier component 320 can determine whether the participant image 212 can be placed over the content of any region of the shared document 310. For example, in some embodiments, the respective regions of shared document 310 can include logos of companies or entities associated with one or more participants of the teleconference. In response to determining that the respective region of the shared document 310 corresponds to an image boundary associated with the participant image 212, the document region identifier component 320 can select the respective region to place the participant image 212.
In another example, the document region identifier component 320 can determine that no pixels of any blank region of the shared document 310 are associated with a different color than the pixels associated with the participant image 212. In such examples, the document region identifier component 320 can determine whether one or more pixels of the participant image 212 can be modified to be associated with a color that is different from pixels of the blank region of the shared document 310. For example, the document region identifier component 320 can determine that a color temperature associated with one or more pixels of the participant image 212 can be modified (e.g., increased or decreased) to associate the pixels of the participant image 212 with a different color. In some embodiments, by modifying the color temperature associated with one or more pixels of the participant image 212, the color associated with one or more pixels of the participant image 212 can be different than the color associated with pixels of the blank region of the shared document 310. In response to modifying the color temperature associated with one or more pixels of the participant image 212, the document region identifier component 320 can select a blank region of the shared document 310 to place the modified participant image 212.
In yet another example, the document region identifier component 320 can determine that a size, shape, and/or color associated with a pixel of the participant image 212 cannot be modified to fit within a blank region of the shared document 310. In such examples, the document region identifier component 320 can identify that a region of the shared document 310 corresponds to an image boundary of the participant image 212 and includes a fewer number of content items than other regions of the shared document 310. In some embodiments, the document region identifier component 320 can additionally modify the transparency of the participant image 212 such that other participants of the teleconference can detect the content item at the identified region while the participant image 212 is presented at the identified region.
As described above, in some embodiments, the request to initiate a document sharing operation can include an identifier of the document 310 stored in a data store associated with a document sharing platform that is communicatively coupled to the conference platform 120. In such embodiments, as described above, the document region identifier component 320 can identify regions based on metadata 332 associated with stored documents and/or image metadata 334. For example, in such embodiments, the document metadata 332 can include metadata associated with one or more content items included in the document 310. Metadata associated with one or more content items can include an indication of a style associated with the one or more content items (e.g., bold style, italic style, underline style, etc.), a format associated with the one or more content items (e.g., a size of the content item), and/or a direction of the one or more content items within the document 310 (i.e., a positioning of the content item relative to one or more other content items of the document 310). In accordance with the previously described embodiments, the document region identifier component 320 can determine whether any region of the document 310 corresponds to a size and/or shape associated with the participant image 212. In response to determining that no region in the document 310 corresponds to the size and/or shape associated with the participant image 212, the document region identifier component 320 can determine whether any region of the document 310 includes one or more content items that can be modified to accommodate the participant image 212. For example, the content items of the document 310 can correspond to titles associated with slides of the slide presentation document. The document area identifier component 320 can obtain a style, format, and/or direction associated with the title based on the document metadata 332. In response to obtaining the style, format, and/or orientation associated with the title, the document area identifier component 320 can determine whether the size, format, and/or orientation associated with the title can be modified to accommodate the participant image 212. In response to determining that, for example, the format of the title can be modified to accommodate the participant image 212, the document region identifier can modify the title to accommodate the participant image 212, and a region associated with the modified title can be selected to present the participant image 212.
In response to the document region identifier component 320 identifying a region of the shared document 310 for presentation of the participant image 212, the overlay component 322 can overlay the participant image 212 for presentation at the identified region. In some embodiments, the overlay component 322 can generate a rendering of the participant image 212 at the identified region of the shared document 310 and can send the rendering to the conference platform 120. In accordance with embodiments described herein, in response to receiving the rendering from the overlay component 322, the conference management component 122 can send the rendering to each client device 102 associated with a participant of the teleconference. In other or similar embodiments, overlay component 322 can generate one or more instructions for rendering participant image 212 at the identified region of document 310 and can send the generated instructions to conference platform 120. In some embodiments, conference management component 122 can execute the received instructions to generate a rendering of participant image 212 at the identified region of document 310. In other or similar embodiments, the conference management component 122 can send received instructions (with or without the participant image 212 and/or the shared document 310) to each client device 102 associated with a participant of the teleconference, and the client devices 102 can execute the instructions to generate a rendering of the participant image 212 at the identified region of the document 310.
As described above, the image overlay engine 126 can also include a GUI layout component 322. The GUI layout component 324 can be configured to modify the presentation of the shared document 310 at the respective client device 102 in view of one or more hardware limitations associated with the client device 102. In the illustrative example, a presenter of the teleconference can be associated with client device 102A and participants of the teleconference can be associated with client device 102B. The client device 102A can include a larger display screen than the display screen of the client device 102B. For example, client device 102A can be a desktop computing device and client device 102B can be a mobile computing device. In this case, the one or more hardware limitations associated with displaying the shared document 310 and the participant image 212 at the client device 102B can be different from the hardware limitations of the client device associated with the client device 102A. In some embodiments, GUI layout component 324 can obtain one or more hardware limitations associated with displaying shared document 310 and participant image 212 at client device 102B (e.g., by requesting the hardware limitations from client device 102B, in a request from client device 102B to join a conference call hosted by conference platform 120, etc.), and can store the obtained hardware limitations as hardware limitation data 336 at data store 110. In response to determining that the one or more hardware limitations meet the hardware limitation criteria, the GUI layout component 324 can determine to modify the presentation of the shared document 310 at the client device 102B. In some embodiments, the GUI layout component 324 can determine that the hardware limitations of the client device 102B meet the hardware limitation criteria in response to determining that the display screen size associated with the client device 102B is below the threshold screen size. In other or similar embodiments, the GUI layout component 324 can determine that the hardware limitations of the client device 102B meet the hardware limitation criteria in response to determining that the display resolution associated with the client device 102B is below the threshold display resolution.
In some embodiments, the GUI layout component 324 is capable of modifying the presentation of the shared document 310 at the client device 102B by identifying two or more unique portions of content at the shared document 310. For example, the GUI layout component 324 can determine that the shared document 310 includes a first portion of content comprising one or more text content items and a second portion of content comprising one or more image content items. In some embodiments, in response to identifying the first and second portions of content at the shared document 310, the GUI layout component 324 can send instructions to the overlay component 322 to display the participant image 212 over the second portion of content while also displaying the first portion of content at another region of the document 310. During the teleconference, the GUI layout component 324 can detect that the presenter of the teleconference has shifted focus from the first portion of content to the second portion of content (i.e., obscured by the participant image 212). For example, the GUI layout component 324 can detect that the presenter has moved a GUI element (e.g., mouse, cursor, etc.) of the conference platform GUI to highlight one or more content items at the first portion of content of the display document 310. In response to detecting that the presenter has moved focus to the second portion of content, the GUI layout component 324 can update the conference platform GUI to display the participant image 212 at the area of the document 310 that includes the first portion of content while the second portion of content of the document 310 is displayed. In accordance with embodiments described herein, in some embodiments, the GUI layout component 324 can update the conference platform GUI by generating instructions that cause the overlay component 324 to display the participant image 212 on the first portion of content.
In other or similar embodiments, the GUI layout component 324 can generate a new document 338 that includes one or more identified unique portions of content at the shared document 310. For example, the GUI layout component 324 can select an area including the first portion of content for display with the participant image 212. The GUI layout component 324 can also generate a document 338, the document 338 including one or more design features (e.g., style, format, orientation, background, etc.) similar to the shared document 310. The document 338 can also include a second portion of content contained in the shared document 310. In some embodiments, document 338 can also include blank space (e.g., an area corresponding to the first portion of content included at shared document 310). During the teleconference, the overlay component 324 can present the participant image 212 at an area of the shared document 310 corresponding to the second portion of content. In response to the GUI layout component 324 detecting that the presenter has moved focus to the second portion of content, the GUI layout component 324 can update the conference platform GUI to display the generated document 338 that includes the second portion of content. The overlay component 324 can also present the participant image 212 at an area of the generated document 338 that includes the white space (e.g., corresponding to an area that includes the first portion of content at the shared document 310). Further details and examples regarding the generation of document 338 are provided with respect to fig. 6A-6C.
Fig. 4 depicts a flowchart of an example method 400 for providing images of shared documents and teleconference participants for presentation via a GUI, in accordance with an embodiment of the present disclosure. The method 400 can be performed by processing logic that comprises hardware (circuitry, dedicated logic, etc.), software (such as is run on a processing device), or a combination thereof. In one embodiment, some or all of the operations of method 400 can be performed by one or more components of system 100 of fig. 1.
At block 410, processing logic can receive a request to share a document associated with a first participant of a teleconference with one or more second participants. In some embodiments, processing logic is capable of receiving a request to receive a shared document from a client device associated with a first participant of a teleconference. Fig. 5A depicts an example GUI 500 on a client device associated with a first participant (e.g., presenter) of a teleconference in accordance with embodiments of the present disclosure. In some embodiments, GUI 500 can include at least a first portion 510 and a second portion 530. The first portion 510 of GUI 500 can include one or more GUI elements that enable one or more users of conference platform 120 (e.g., demonstrators, participants a-N, etc.) to join and participate in the conference call. The second portion 530 of the GUI 500 can display a document 532 (e.g., a slide presentation document, a word document, a web page document, etc.) to be shared by a presenter of the teleconference with one or more participants (e.g., participants a-N) of the teleconference. In some embodiments, one or more elements of the first portion 510 of the GUI 500 correspond to GUI elements of a conference platform GUI provided by the conference management component 122, as described above. In other or similar embodiments, elements of both the first portion 510 and the second portion 530 of the GUI 500 correspond to elements of a conference platform GUI.
In some embodiments, the first portion 510 of the GUI 500 can include a first section 512 and a second section 518, both of which are configured to output video data captured at the client device 102 associated with each participant of the teleconference. For example, the first section 512 can display image data captured by a client device associated with a presenter of a video teleconference. The second section 518 can display image data captured by client devices associated with participants of the teleconference. In other or similar embodiments, first portion 510 can include one or more sections configured to display image data associated with a user of conference platform 120 in other directions than shown in fig. 5A. For example, portion 510 can include a single section that displays image data captured by a client device of a presenter of the teleconference without displaying video data captured by client devices of other participants of the teleconference. In some embodiments, the image data displayed at the first section 512 and/or the second section 518 of the first portion 510 can correspond to a view of a user (e.g., presenter, participants a-N) in the surrounding environment. As shown in fig. 5A, a first section 512 of the portion 510 is capable of displaying image data corresponding to a presenter's 514 view in a surrounding environment 516. In some embodiments, the second section 512 of the portion 510 is also capable of displaying image data corresponding to the views of the participants a-N in the respective surroundings (not shown).
The first portion 510 of the GUI 500 can also include one or more GUI elements that enable a presenter of the teleconference to share the document 522 displayed at the second portion 530 with participants of the teleconference. For example, the first portion 510 can include a button 520 that enables the presenter to share the document 522 displayed at the second portion 530 with the participants A-N. The presenter can initiate an operation to share the document 522 with participants a-N by engaging (e.g., clicking) button 520. In response to detecting that the presenter has engaged button 520, a client device associated with the presenter is able to detect an operation to initiate sharing of document 532 with participants a-N. According to the previously described embodiments, the client device can send a request to the meeting management component 122 to initiate a document sharing operation. It should be noted that the presenter can initiate the operation of sharing the document 522 with participants A-N in accordance with other techniques. For example, in response to detecting that the document 522 has been retrieved from the client device's local memory and displayed at the second portion 530 of the GUI 500, the settings of the client device associated with the presenter can cause an operation to be initiated to share the document 522.
Referring back to fig. 4, at block 412, processing logic can receive image data corresponding to a view of a first participant (e.g., presenter) in the surrounding environment. As described above, the audiovisual component of the client device associated with the first participant can be configured to capture an image of the first participant and generate image data associated with the captured image. According to some embodiments, the generated image data can be displayed at a first section 512 of the first portion 510 of the GUI 500. In response to detecting that the presenter has engaged button 520 of GUI 500, a client device associated with the first participant is able to send the generated image data displayed at first section 512 of first portion 510 of GUI 500 to conference management component 122. The client device associated with the first participant can send the generated image data along with or separately from the request to initiate the document sharing operation.
At block 414, processing logic can obtain an image depicting the first participant based on the received image data. In some embodiments, in response to meeting management component 122 receiving the generated image data, meeting management component 122 can provide the received image data to context extraction engine 124. As previously described, the background extraction engine 124 can obtain an image depicting a teleconference presenter by extracting a set of pixels corresponding to the teleconference presenter and generating an image depicting the teleconference presenter based on the extracted set of pixels. In some embodiments, the background extraction engine 124 can identify the set of pixels corresponding to the teleconference presenter based on the output of the trained image extraction model in accordance with the previously described embodiments.
At block 416, processing logic can identify one or more regions of a document (e.g., document 522) that satisfy one or more image placement criteria. As previously described, in some embodiments, the conference management component 122 can receive images of the document 522 to be shared with the participants a-N of the teleconference from client devices associated with the teleconference presenter. In other or similar embodiments, the document 522 can be stored in a data store associated with a document sharing platform communicatively coupled to the conference platform 120. In such embodiments, meeting management component 122 can receive an identifier of document 522 at a data store associated with the document sharing platform. In accordance with the previously described embodiments, conference management component 122 can retrieve document 522 (or a portion of document 522) from a data store associated with the document sharing platform. As previously described, in response to obtaining at least a portion of the document 522 (or an image of the document 522) to be shared with participants A-N, the meeting management component 122 can provide the document 522 to the image overlay engine 126. As described above, the image overlay engine 126 can identify one or more regions of the document 522 that satisfy one or more image placement criteria. For example, the image overlay engine 126 can identify one or more blank regions of the document 522 corresponding to image boundaries associated with images of the teleconference presenter. In another example, the image overlay engine 126 can identify one or more regions of the document 522 that include content items that can be modified to accommodate images of teleconference presenter.
At block 418, processing logic can provide the document and an image depicting the first participant for presentation via a GUI on a client device associated with the second participant. As described above, in response to identifying one or more regions of document 522 that meet one or more image placement criteria, image overlay engine 126 can overlay an image of a teleconference presenter at one of the identified regions, as described above. For example, the image overlay engine 126 can generate a rendering of the image and document 522 depicting the teleconference presenter and provide the generated rendering to the conference management component 122. In accordance with the previously described embodiments, conference management component 122 can provide the generated rendering to client devices associated with one or more users (e.g., demonstrators, participants a-N) of conference platform 120. In response to receiving the generated rendering, client devices associated with respective participants (e.g., participant a) of the teleconference can update the GUI to display a rendering of the document 522 and images depicting the teleconference presenter. In other or similar embodiments, the image overlay engine 126 can generate instructions to render images and documents 522 depicting teleconference presenter. In accordance with the previously described embodiments, the conference management component 122 and/or client devices associated with respective participants of the teleconference can execute instructions to generate the rendering.
Fig. 5B depicts an example GUI 550 displaying a rendering depicting an image of a teleconference presenter and a document 522, in accordance with an embodiment of the present disclosure. In some embodiments, GUI 550 can be displayed via a client device associated with a participant (e.g., participant a) of the teleconference. In other or similar embodiments, GUI 550 can be displayed via a client device associated with a presenter of the teleconference. As shown in FIG. 5B, GUI 550 depicts document 532 and image 552 of a teleconference presenter at area 554 of document 532. In some embodiments, in response to determining that region 554 meets one or more image placement criteria (e.g., includes a white space corresponding to an image boundary associated with image 552), image overlay engine 126 can select region 554 to include image 552. The presenter of the teleconference can engage participants a-N by emphasizing one or more content items of document 532 (e.g., by physically pointing to the one or more content items) while presenting image 552 at area 554. In some embodiments, GUI 550 can additionally display image data captured by one or more client devices associated with participants (e.g., participants a-N) of the teleconference, as described above.
As described above, in some embodiments, GUI 550 can be displayed via a client device associated with a presenter of a teleconference. In such embodiments, in some embodiments, the teleconference presenter is able to interface with one or more elements of GUI 550 to modify the presentation of image 552 and document 532. In an illustrative example, a teleconference presenter can request that image 552 be moved from region 554 of document 532 to another region of document 532 (e.g., by clicking on image 552 and dragging image 552 to another region of document 532 by pressing one or more buttons on a keyboard connected to a client device). In response to detecting that the conference call presenter has requested that the image 552 be moved to another area of the document 532, the conference management component 122 can update the GUI 550 at each client device associated with the conference call presenter and each participant in accordance with the received request. Fig. 5C depicts an example updated GUI 550 according to an embodiment of the disclosure. As shown in fig. 5C, the image 552 depicting the teleconference presenter has moved from the area 554 of the GUI 550 to the area 556 of the GUI 550. In some embodiments, region 556 can be associated with a different feature than region 554. For example, the size of region 556 can be smaller than the size of region 554. In such embodiments, conference management component 122 (or image overlay engine 126) can modify the size and/or shape of image 552 to fit within region 556 (e.g., to account for image boundaries associated with image 552).
In some embodiments, meeting management component 122 cannot modify the size and/or shape of image 552 to fit within region 556 in view of the image boundaries associated with image 552. In such embodiments, conference management component 122 can move image 552 to region 556 upon request from a teleconference presenter. However, in some embodiments, at least a portion of the image 552 can overlap with one or more content items of the document 532. In this case, conference management component 122 can modify the transparency of image 552 such that participants A-N of the teleconference can detect content items of document 532 that overlap image 552.
Fig. 6A-6C illustrate another example of overlaying images of teleconference participants with a shared document for presentation via a GUI in accordance with embodiments of the present disclosure. Fig. 6A depicts another example GUI 600 on a client device associated with a first participant (e.g., presenter) of a teleconference. In some embodiments, GUI 600 can correspond to GUI 500 described with respect to fig. 5A, except that document 632 displayed at second portion 630 of GUI 600 can include one or more additional content items than are included in document 532 displayed at second portion 530 of GUI 500. As previously described, the teleconference presenter is able to initiate an operation to share a document 632 with participants a-N of the teleconference (e.g., via the interface button 620).
In response to receiving the request to initiate a document sharing operation, conference management component 122 can send image data and/or document 632 (or a portion of document 632) generated by a client device associated with the teleconference presenter to image overlay engine 126. In some embodiments, the client devices associated with the participants of the teleconference (e.g., participant a) may be subject to hardware limitations (e.g., display size, display resolution, etc.) that are different than the hardware limitations of the client devices associated with the teleconference presenter. In such embodiments, for example, in response to determining that the hardware limitations of the client device associated with participant a satisfy the hardware limitations, the image overlay engine 126 can determine to modify the presentation of the document 632 and the image depicting the teleconference presenter. For example, the image overlay engine 126 can determine to display a first portion of content of the document 632 (e.g., one or more text content items associated with data points 1-5 of the document 632) and to display an image depicting a teleconference presenter at an area that includes a second portion of content of the document 632 (e.g., one image content item of the document 632). In another example, as previously described, the image overlay engine 126 can generate an additional document that includes the second portion of the content of the document 632. In such examples, the image overlay engine 126 can determine to display the first portion of the content of the document 632 and the image depicting the teleconference presenter at the area associated with the second portion of the content of the document 632.
Fig. 6B depicts an example GUI 650 displaying a rendering of an image depicting a teleconference presenter and a document 632 in accordance with an embodiment of the present disclosure. In some embodiments, GUI 650 can be displayed via a client device associated with a participant (e.g., participant a) of the teleconference. In other or similar embodiments, GUI 650 can be displayed via a client device associated with a presenter of a teleconference. As shown in fig. 6B, GUI 650 displays content included in a first portion of document 632 in a first region 654 of document 632 and displays an image 652 depicting a presenter of the teleconference at a second region 656 of document 632. In some embodiments, the region 654 is associated with a second portion of content included in the document 632 (e.g., graphics included in the document 632 shown in fig. 6A). According to the previously described embodiment, the GUI 650 is capable of displaying the image 652 at the region 656 associated with the graphical content item. In some embodiments, conference management component 122 (or image overlay engine 126) can detect that the teleconference presenter has shifted focus to the second portion of content included in document 632. For example, conference management component 122 can detect that a teleconference presenter has moved a GUI element (e.g., a mouse) at a GUI on a client device associated with the presenter from an area 654 of document 632 that includes a first portion of content to an area 656 that includes a second portion of content. In such embodiments, conference management component 122 (or image overlay engine 126) can update GUI 650 to display the second portion of the content of document 632.
Fig. 6C depicts an example updated GUI 650 according to an embodiment of the present disclosure. In some embodiments, updated GUI 650 displays document 632 with image 622 at region 656. In other or similar embodiments, the updated GUI 650 displays a document generated by the image overlay engine 126 that includes the second portion of content included in the document 632. In such an embodiment, the updated GUI 650 can display the image 622 at the area 654 of the document 632 in accordance with the previously described embodiments.
Fig. 7 depicts a flowchart of another example method 700 for providing images of shared documents and teleconference participants for presentation via a GUI in accordance with an embodiment of the present disclosure. The method 400 can be performed by processing logic that comprises hardware (circuitry, dedicated logic, etc.), software (such as is run on a processing device), or a combination thereof. In one embodiment, some or all of the operations of method 700 can be performed by one or more components of system 100 of fig. 1.
At block 710, processing logic can share, with a second participant of the teleconference via a second GUI on a second client device, documents displayed via a first GUI on a first client device associated with a first participant (e.g., presenter) of the teleconference. Fig. 8A depicts an example GUI 800 displaying a document 812 shared with one or more participants (e.g., participants a-N) of a teleconference, in accordance with an embodiment of the present disclosure. In some embodiments, GUI 800 can be displayed via a client device associated with a participant (e.g., participant a) of the teleconference. In other or similar embodiments, GUI 800 can be displayed via a client device associated with a presenter of a teleconference. In accordance with the previously described embodiments, in some embodiments, a presenter of a teleconference is able to share a document 812 with participants of the teleconference by engaging a GUI element (e.g., button 820). In some embodiments, the document 812 can be stored in a data store associated with a document sharing platform communicatively coupled to the conference platform 120.
Referring back to fig. 7, at block 712, processing logic can receive a request to display an image depicting the first participant and a document shared with the second participant. In some embodiments, processing logic is capable of receiving the request in response to the teleconference presenter interfacing with a particular GUI element (not shown) of GUI element 800. At block 714, processing logic can receive image data corresponding to a view of the first participant in the surrounding environment. According to the previously described embodiments, a client device associated with a conference call presenter is capable of generating image data corresponding to a presenter's view in the surrounding environment. In accordance with the previously described embodiments, conference management component 122 can receive generated image data from a client device associated with a teleconference presenter.
At block 716, processing logic can obtain an image depicting the first participant based on the received image data. As previously described, conference management component 122 can provide received image data to background extraction engine 124. In accordance with the previously described embodiments, the background extraction engine 124 is capable of generating an image depicting the first participant. At block 718, the processing logic is capable of modifying a format and/or orientation of one or more content items of the shared document in view of an image depicting the first participant. As described above, document 812 can be stored in a data store associated with a document sharing platform communicatively coupled to conference platform 120. As previously described, the content management component 122 can retrieve the document 812 from the data store. In some embodiments, the image overlay engine 126 can identify an area of the document 812 that includes one or more content items that can be modified in view of an image depicting a teleconference presenter. For example, the image overlay engine 126 can identify an area 814 of the document 812 that includes the title content item. The image overlay engine 126 can determine that the format and/or orientation of the title content item of the region 814 can be modified (e.g., in view of metadata associated with the document 812) to accommodate the image depicting the teleconference presenter. In other examples, the image overlay engine 126 can determine that the format and/or orientation of one or more text content items can additionally or alternatively be modified to accommodate images depicting teleconference presenter.
At block 720, processing logic can provide an image depicting the first participant and the modified document for presentation via a second GUI on the second client device. Fig. 8B depicts the updated GUI 800 according to an embodiment of the present disclosure. As described above, in accordance with the previously described embodiments, the image overlay engine 126 is capable of modifying the format and/or orientation of one or more content items at the region 814 and/or the region 816. In accordance with the previously described embodiments, conference management component 122 can present modified document 812 via updated GUI 800. As shown in FIG. 8B, the format of the title content item in region 814 of document 812 is modified from the alignment of the center portion of document 812 to the alignment of the left-hand portion of document 812. As also shown in fig. 8B, the size of one or more text items in region 816 of document 812 has been reduced, and the orientation of one or more text items in region 814 has been modified to accommodate image 822 depicting the conference presenter.
Fig. 9A-9B illustrate examples of overlaying images of multiple teleconference participants with a shared document for presentation via GUI 900, in accordance with embodiments of the present disclosure. As described above, according to the previously described embodiments, the image 910 depicting the teleconference presenter can be displayed at a particular region 912 of the document via the conference platform GUI. In some embodiments, the teleconference presenter can invite additional participants of the teleconference to demonstrate a shared document (or a portion of a document) with the teleconference presenter. For example, a client device associated with a teleconference presenter can send a request to a client device associated with an additional participant to present a shared document with the teleconference presenter. In response to detecting that the additional teleconference presenter has accessed the GUI element indicating acceptance of the request, conference management component 122 can initiate a process of displaying an image depicting additional participants of the teleconference and a rendering of the image depicting the presenter of the teleconference.
In one example, conference management component 122 can receive image data generated by an audiovisual component (e.g., a camera) of a client device associated with an additional participant. In accordance with the previously described embodiments, conference management component 122 is able to obtain images depicting additional participants. In some embodiments, conference management component 122 can identify regions of the shared document that satisfy one or more image placement criteria. In some embodiments, conference management component 122 can identify areas that satisfy one or more image placement criteria relative to images depicting additional participants. In other or similar embodiments, conference management component 122 can identify areas that satisfy one or more image placement criteria relative to images depicting both teleconference presenter and additional participants. In response to identifying an area meeting one or more image placement criteria, conference management component 122 can update GUI 900 on the client device for each participant of the teleconference to display additional participants (and/or teleconference presenter) at the identified area. As shown in fig. 9A, the region 916 can be identified as a region that meets one or more image placement criteria. In this way, an image 914 depicting the additional participant can be displayed at region 916. In some embodiments, conference management component 122 may not identify areas of the shared document that satisfy image placement criteria relative to image 910 depicting a teleconference presenter and/or image 914 depicting additional participants. In such embodiments, the conference management component 122 can modify the format and/or orientation of one or more content items of the shared document to accommodate the image depicting the teleconference presenter and the image depicting the additional participants in accordance with the embodiments described above.
In an additional or alternative embodiment, the teleconference presenter and/or additional participants can invite another teleconference participant to present the shared document (or a portion of the shared document) in place of the teleconference presenter. In such embodiments, conference management component 122 can obtain images depicting other teleconference participants, as described above. In some embodiments, conference management component 122 can remove image 910 depicting the teleconference presenter from GUI 900 and replace the removed image 910 with an image depicting the other teleconference participants, as shown in fig. 9B. In accordance with the previously described embodiments, in other or similar embodiments, conference management component 122 can identify another region of the shared document that meets one or more image placement criteria and display an image 918 depicting other teleconference participants at the identified region. In some embodiments, as previously described, conference management component 122 can modify the format and/or orientation of one or more content items of the shared document to accommodate image 914 and/or image 918.
Fig. 10 is a block diagram illustrating an exemplary computer system 1000 according to an embodiment of the present disclosure. Computer system 1000 can correspond to conferencing platform 120 and/or client devices 102A-N described with respect to fig. 1. Computer system 1000 can operate as a server or an endpoint machine in an endpoint server network environment, or as a peer machine in a peer-to-peer (or distributed) network environment. The machine can be a television, a Personal Computer (PC), a tablet PC, a set-top box (STB), a Personal Digital Assistant (PDA), a cellular telephone, a network appliance, a server, a network router, switch or bridge, or any machine capable of executing a set of instructions (sequential or otherwise) that specify actions to be taken by that machine. Furthermore, while only a single machine is illustrated, the term "machine" shall also be taken to include any collection of machines that individually or jointly execute a set (or multiple sets) of instructions to perform any one or more of the methodologies discussed herein.
The example computer system 1000 includes a processing device (processor) 1002, a main memory 1004 (e.g., read Only Memory (ROM), flash memory, dynamic Random Access Memory (DRAM), such as Synchronous DRAM (SDRAM), double data rate (DDR SDRAM), DRAM (RDRAM), etc.), a static memory 1006 (e.g., flash memory, static Random Access Memory (SRAM), etc.), and a data storage device 1018, which communicate with each other via a bus 1040.
The processor (processing device) 1002 represents one or more general-purpose processing devices, such as a microprocessor, central processing unit, or the like. More specifically, the processor 1002 can be a Complex Instruction Set Computing (CISC) microprocessor, a Reduced Instruction Set Computing (RISC) microprocessor, a Very Long Instruction Word (VLIW) microprocessor, or a processor implementing other instruction sets or processors implementing a combination of instruction sets. The processor 1002 can also be one or more special-purpose processing devices, such as an Application Specific Integrated Circuit (ASIC), a Field Programmable Gate Array (FPGA), a Digital Signal Processor (DSP), a network processor, or the like. The processor 1002 is configured to execute instructions 1005 (e.g., for predicting channel lineup ratings) to perform the operations discussed herein.
The computer system 1000 can also include a network interface device 1008. The computer system 1000 can also include a video display unit 1010 (e.g., a Liquid Crystal Display (LCD) or Cathode Ray Tube (CRT)), an input device 1012 (e.g., a keyboard and an alphanumeric keyboard, a motion-sensing input device, a touch screen), a cursor control device 1014 (e.g., a mouse), and a signal generation device 1020 (e.g., a speaker).
The data storage device 1018 can include a non-transitory machine-readable storage medium 1024 (also referred to as a computer-readable storage medium) having stored thereon one or more sets of instructions 1005 (e.g., for overlaying an image depicting a teleconference presenter with a shared document) embodying any one or more of the methods or functions described herein. The instructions may also reside, completely or at least partially, within the main memory 1004 and/or within the processor 1002 during execution thereof by the computer system 1000, the main memory 1004 and the processor 1002 also constituting machine-readable storage media. These instructions can also be transmitted or received over the network 1030 via the network interface device 1008.
In one implementation, the instructions 1005 include instructions for overlaying an image depicting a conference call participant with a shared document. While the computer-readable storage medium 1024 (machine-readable storage medium) is shown in an example embodiment to be a single medium, the terms "computer-readable storage medium" and "machine-readable storage medium" should be taken to include a single medium or multiple media (e.g., a centralized or distributed database, and/or associated caches and servers) that store the one or more sets of instructions. The terms "computer-readable storage medium" and "machine-readable storage medium" should also be taken to include any medium that is capable of storing, encoding or carrying a set of instructions for execution by a machine and that cause the machine to perform any one or more of the methodologies of the present disclosure. The terms "computer-readable storage medium" and "machine-readable storage medium" shall accordingly include, but not be limited to, solid-state memories, optical media, and magnetic media.
Reference throughout this specification to "one embodiment," "an example," "an embodiment," or "an example" means that a particular feature, structure, or characteristic described in connection with the embodiment and/or example is included in at least one embodiment and/or example. Thus, the appearances of the phrase "in one embodiment" or "in an embodiment" in various places throughout this specification are capable of, but not necessarily, referring to the same embodiment, depending on the environment. Furthermore, the particular features, structures, or characteristics may be combined in any suitable manner in one or more embodiments.
To the extent that the terms "includes," "including," "has," "contains," variants thereof, and other similar words are used in either the detailed description or the claims, these terms are intended to be inclusive in a manner similar to the term "comprising" as an open transition word without precluding any additional or other elements.
As used in this disclosure, the terms "component," "module," "system," and the like are generally intended to refer to a computer-related entity, either hardware (e.g., a circuit), software, a combination of hardware and software, or an entity related to an operating machine having one or more particular functionalities. For example, a component can be, but is not limited to being, a process running on a processor (e.g., a digital signal processor), a processor, an object, an executable, a thread of execution, a program, and/or a computer. For example, both an application running on a controller and the controller can be a component. One or more components can reside within a process and/or thread of execution and a component can be localized on one computer and/or distributed between two or more computers. Furthermore, the "device" can be in the form of specially designed hardware; general purpose hardware specialized by executing software thereon that enables the hardware to perform specific functions (e.g., generate points of interest and/or descriptors); software on a computer readable medium; or a combination thereof.
The foregoing systems, circuits, modules, etc. have been described in terms of interactions between several components and/or blocks. It is to be understood that such systems, circuits, components, blocks, etc. can include those components or specified sub-components, some of the specified components or sub-components, and/or additional components, and in accordance with various permutations and combinations of the foregoing. Sub-components can also be implemented as components communicatively coupled to other components rather than included within parent components (hierarchical). Additionally, it should be noted that one or more components can be combined into a single component providing aggregate functionality, or divided into several separate sub-components, and any one or more intermediate layers, such as a management layer, can be provided to communicatively couple to these sub-components in order to provide integrated functionality. Any component described herein is also capable of interacting with one or more other components not specifically described herein but known to those of skill in the art.
Furthermore, the words "example" or "exemplary" are used herein to mean serving as an example, instance, or illustration. Any aspect or design described herein as "exemplary" is not necessarily to be construed as preferred or advantageous over other aspects or designs. Rather, use of the word "example" or "exemplary" is intended to present concepts in a concrete fashion. As used herein, the term "or" is intended to mean an inclusive "or" rather than an exclusive "or". That is, unless otherwise indicated or clear from the context, "X employs A or B" is intended to mean any of the natural inclusive permutations. That is, if X employs A; x is B; or X employs both A and B, then "X employs A or B" is satisfied in any of the foregoing cases. Furthermore, the articles "a" and "an" as used in this disclosure and the appended claims should generally be construed to mean "one or more" unless specified otherwise or clear from context to be directed to a singular form.
Finally, embodiments described herein include collecting data describing a user and/or user activity. In one embodiment, such data is collected only when the user agrees to collect such data. In some embodiments, the user is prompted to explicitly allow data collection. In addition, the user can opt-in or opt-out of such data collection activities. In one embodiment, the collected data is anonymized prior to performing any analysis to obtain any statistical patterns, such that the identity of the user cannot be determined from the collected data.
Claims (20)
1. A method, comprising:
receiving a request to initiate a document sharing operation to share a document displayed via a first GUI on a first client device associated with a first participant of a teleconference with a second participant of the teleconference via a second graphical user interface GUI on a second client device;
receiving image data corresponding to a view of the first participant in a surrounding environment;
obtaining an image depicting the first participant based on the received image data;
identifying one or more regions of the document that meet one or more image placement criteria; and
The document and the image depicting the first participant are provided for presentation via the second GUI on the second client device, wherein the image depicting the first participant is presented at one of the identified one or more regions of the document.
2. The method of claim 1, wherein the received image data includes a first set of pixels associated with the first participant of the conference call and a second set of pixels associated with the surrounding environment, and wherein obtaining an image depicting the first participant based on the received image data includes:
extracting the identified first set of pixels from the received image data; and
the image depicting the first participant is generated based on the extracted first set of pixels.
3. The method of claim 1, wherein identifying the one or more regions of the document that meet the image placement criteria comprises:
detecting an area of the document that does not include any content; and
it is determined that the detected region meets the image placement criteria.
4. The method of claim 1, wherein identifying the one or more regions of the document that meet the image placement criteria comprises:
Obtaining pixel data associated with a plurality of regions of the document; and
based on the obtained pixel data, an area associated with one or more pixels of a color different from a color associated with one or more pixels of the image depicting the first participant is selected.
5. The method of claim 1, further comprising:
obtaining data indicative of one or more hardware limitations associated with the second client device;
in response to determining that the one or more hardware limitations meet a hardware limitation criterion, identifying first content included in a first portion of the document and second content included in a second portion of the document; and
an additional document is generated that includes the second content included in the second portion of the document.
6. The method of claim 5, wherein presenting the document and the image depicting the first participant via the second GUI of the second client device comprises:
the identified first content included in the first portion of the document is presented with the image depicting the first participant, while the identified second content included in the second portion of the document is not presented.
7. The method of claim 6, further comprising:
receiving a notification from the first client device indicating that the first participant is transferring to the second content included in the second portion of the document; and
the second GUI of the second client device is updated to present the additional document including the second content and the image depicting the first participant.
8. The method of claim 1, further comprising:
receiving an additional request to move the image depicting the first participant from the one or more regions of the document to an additional region of the document; and
the image depicting the first participant is presented at the additional area of the document.
9. The method of claim 8, further comprising:
determining that the additional region of the document includes content; and
modifying at least one of a size, shape or transparency of the image depicting the first participant,
wherein a modified image depicting the first participant is presented at the additional area of the document.
10. A system, comprising:
a memory device; and
A processing device coupled to the memory device, the processing device performing operations comprising:
sharing, with a second participant of a teleconference via a second graphical user interface GUI on a second client device, documents displayed via a first GUI on a first client device associated with a first participant of the teleconference;
receiving a request to display, via the second GUI, an image depicting the first participant of the conference call and the document shared with the second participant;
receiving image data corresponding to a view of the first participant in a surrounding environment;
obtaining an image depicting the first participant based on the received image data;
modifying at least one of a format or a direction of one or more content items of the shared document in view of the image depicting the first participant; and
an image depicting the first participant and a modified document are provided for presentation via the second GUI on the second client device.
11. The system of claim 10, wherein the received image data includes a first set of pixels associated with the first participant of the conference call and a second set of pixels associated with the surrounding environment, and wherein obtaining an image depicting the first participant based on the received image data includes:
Extracting the identified first set of pixels from the received image data; and
the image depicting the first participant is generated based on the extracted first set of pixels.
12. The system of claim 10, wherein modifying at least one of the format or the orientation of the one or more content items of the shared document in view of the image depicting the first participant comprises:
identifying regions of the document that include a lesser amount of content than other regions of the document, wherein the identified regions include the one or more content items; and
one or more modifications that can be applied to the one or more content items are determined in view of one or more modification rules associated with the shared document.
13. The system of claim 10, wherein the shared document corresponds to a document stored in a data store associated with a document sharing platform communicatively coupled to a conference platform hosting the teleconference.
14. The system of claim 10, wherein the operations further comprise:
obtaining data indicative of one or more hardware limitations associated with the second client device;
In response to determining that the one or more hardware limitations meet a hardware limitation criterion, identifying first content included in a first portion of the modified document and second content included in a second portion of the modified document; and
generating an additional document comprising the second content included in the second portion of the modified document.
15. The system of claim 14, wherein presenting the modified document and the image depicting the first participant via the second GUI of the second client device comprises:
the identified first content included in the first portion of the modified document is presented with the image depicting the first participant, while the identified second content included in the second portion of the modified document is not presented.
16. The system of claim 15, wherein the operations further comprise:
receiving a notification from the first client device indicating that the first participant is transferring to the second content included in the second portion of the document; and
updating the second GUI of the second device to present the additional document including the second content and the image depicting the first participant.
17. A non-transitory computer-readable storage medium comprising instructions for a server, which when executed by a processing device, cause the processing device to perform operations comprising:
receiving a request to initiate a document sharing operation to share a document displayed via a first GUI on a first client device associated with a first participant of a conference call with a second participant of the conference call via a second graphical user interface GUI on a second client device;
receiving image data corresponding to a view of the first participant in a surrounding environment;
obtaining an image depicting the first participant based on the received image data;
identifying one or more regions of the document that meet one or more image placement criteria; and
providing the document and the image depicting the first participant for presentation via the second GUI on the second client device, wherein the image depicting the first participant is presented at one of the identified one or more regions of the document.
18. The non-transitory computer-readable storage medium of claim 17, wherein the received image data includes a first set of pixels associated with the first participant of the conference call and a second set of pixels associated with the surrounding environment, and wherein obtaining an image depicting the first participant based on the received image data includes:
Extracting the identified first set of pixels from the received image data; and
the image depicting the first participant is generated based on the extracted first set of pixels.
19. The non-transitory computer-readable storage medium of claim 17, wherein identifying the one or more regions of the document that satisfy the image placement criteria comprises:
detecting an area of the document that does not include any content; and
determining that the detected region meets the image placement criteria.
20. The non-transitory computer-readable storage medium of claim 17, wherein identifying the one or more regions of the document that satisfy the image placement criteria comprises:
obtaining pixel data associated with a plurality of regions of the document; and
based on the obtained pixel data, an area associated with one or more pixels of a color different from a color associated with one or more pixels of the image depicting the first participant is selected.
Applications Claiming Priority (4)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US63/192,509 | 2021-05-24 | ||
US17/549,708 | 2021-12-13 | ||
US17/549,708 US20220374190A1 (en) | 2021-05-24 | 2021-12-13 | Overlaying an image of a conference call participant with a shared document |
PCT/US2022/030779 WO2022251257A1 (en) | 2021-05-24 | 2022-05-24 | Overlaying an image of a conference call participant with a shared document |
Publications (1)
Publication Number | Publication Date |
---|---|
CN116762333A true CN116762333A (en) | 2023-09-15 |
Family
ID=87953845
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202280008982.1A Pending CN116762333A (en) | 2021-05-24 | 2022-05-24 | Superimposing images of conference call participants and shared documents |
Country Status (1)
Country | Link |
---|---|
CN (1) | CN116762333A (en) |
-
2022
- 2022-05-24 CN CN202280008982.1A patent/CN116762333A/en active Pending
Similar Documents
Publication | Publication Date | Title |
---|---|---|
KR102469295B1 (en) | Remove video background using depth | |
US9179096B2 (en) | Systems and methods for real-time efficient navigation of video streams | |
US11087068B2 (en) | Systems and methods for bringing document interactions into the online conversation stream | |
US9606695B2 (en) | Event notification | |
EP3195601B1 (en) | Method of providing visual sound image and electronic device implementing the same | |
US10139917B1 (en) | Gesture-initiated actions in videoconferences | |
US20160042249A1 (en) | Event-based image classification and scoring | |
KR101686830B1 (en) | Tag suggestions for images on online social networks | |
US10996839B2 (en) | Providing consistent interaction models in communication sessions | |
CN112584086A (en) | Real-time video transformation in video conferencing | |
KR20190058701A (en) | Gallery of messages with a shared interest | |
US9715751B2 (en) | Zooming to faces depicted in images | |
US20230208894A1 (en) | Integrating a video feed with shared documents during a conference call discussion | |
US20170109339A1 (en) | Application program activation method, user terminal, and server | |
WO2019020061A1 (en) | Video dialogue processing method, video client, video server, and computer readable storage medium | |
US11164418B2 (en) | Impromptu community streamer | |
US10732806B2 (en) | Incorporating user content within a communication session interface | |
US20180268049A1 (en) | Providing a heat map overlay representative of user preferences relating to rendered content | |
US20220374190A1 (en) | Overlaying an image of a conference call participant with a shared document | |
US11481933B1 (en) | Determining a change in position of displayed digital content in subsequent frames via graphics processing circuitry | |
US20180300301A1 (en) | Enhanced inking capabilities for content creation applications | |
CN116762333A (en) | Superimposing images of conference call participants and shared documents | |
US11303464B2 (en) | Associating content items with images captured of meeting content | |
WO2022251257A1 (en) | Overlaying an image of a conference call participant with a shared document | |
US20220147739A1 (en) | Video annotating method, client, server, and system |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |