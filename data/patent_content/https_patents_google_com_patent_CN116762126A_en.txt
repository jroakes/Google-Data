CN116762126A - Locally performed device arbitration for automatic speech recognition - Google Patents
Locally performed device arbitration for automatic speech recognition Download PDFInfo
- Publication number
- CN116762126A CN116762126A CN202180088457.0A CN202180088457A CN116762126A CN 116762126 A CN116762126 A CN 116762126A CN 202180088457 A CN202180088457 A CN 202180088457A CN 116762126 A CN116762126 A CN 116762126A
- Authority
- CN
- China
- Prior art keywords
- additional
- client device
- spoken utterance
- text representation
- audio data
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Pending
Links
- 238000000034 method Methods 0.000 claims description 102
- 238000012545 processing Methods 0.000 claims description 49
- 230000004044 response Effects 0.000 claims description 14
- 230000015654 memory Effects 0.000 claims description 10
- 238000004891 communication Methods 0.000 claims description 9
- 230000008569 process Effects 0.000 description 45
- 230000002452 interceptive effect Effects 0.000 description 12
- 241000282472 Canis lupus familiaris Species 0.000 description 10
- 230000009471 action Effects 0.000 description 9
- 230000003993 interaction Effects 0.000 description 8
- 230000000694 effects Effects 0.000 description 4
- 230000007246 mechanism Effects 0.000 description 4
- 238000010586 diagram Methods 0.000 description 3
- 238000005516 engineering process Methods 0.000 description 3
- 230000002093 peripheral effect Effects 0.000 description 3
- 241000282412 Homo Species 0.000 description 2
- 230000003190 augmentative effect Effects 0.000 description 2
- 239000003795 chemical substances by application Substances 0.000 description 2
- 239000011521 glass Substances 0.000 description 2
- 238000010801 machine learning Methods 0.000 description 2
- 238000013442 quality metrics Methods 0.000 description 2
- 230000000007 visual effect Effects 0.000 description 2
- 206010011469 Crying Diseases 0.000 description 1
- 238000006243 chemical reaction Methods 0.000 description 1
- 230000006835 compression Effects 0.000 description 1
- 238000007906 compression Methods 0.000 description 1
- 238000004590 computer program Methods 0.000 description 1
- 230000001934 delay Effects 0.000 description 1
- 230000001419 dependent effect Effects 0.000 description 1
- 238000011143 downstream manufacturing Methods 0.000 description 1
- 230000006870 function Effects 0.000 description 1
- 239000004973 liquid crystal related substance Substances 0.000 description 1
- 239000003550 marker Substances 0.000 description 1
- 235000012054 meals Nutrition 0.000 description 1
- 238000012544 monitoring process Methods 0.000 description 1
- 230000003287 optical effect Effects 0.000 description 1
- 230000002085 persistent effect Effects 0.000 description 1
- 230000011273 social behavior Effects 0.000 description 1
- 230000026676 system process Effects 0.000 description 1
- 238000012549 training Methods 0.000 description 1
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/32—Multiple recognisers used in sequence or in parallel; Score combination systems therefor, e.g. voting systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/02—Feature extraction for speech recognition; Selection of recognition unit
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/48—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use
- G10L25/51—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use for comparison or discrimination
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/48—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use
- G10L25/69—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00 specially adapted for particular use for evaluating synthetic or decoded voice signals
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/26—Speech to text systems
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/28—Constructional details of speech recognition systems
- G10L15/30—Distributed recognition, e.g. in client-server systems, for mobile phones or network applications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/08—Speech classification or search
- G10L2015/088—Word spotting
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L15/00—Speech recognition
- G10L15/22—Procedures used during a speech recognition process, e.g. man-machine dialogue
- G10L2015/223—Execution procedure of a spoken command
Abstract
The text representation of the spoken utterance can be generated based on candidate text representations of the spoken utterance generated using the given client device and/or based on one or more additional candidate text representations of the spoken utterances each generated using a corresponding additional client device. Various implementations include determining additional client devices from a set of additional client devices in an environment having a given client device. Various implementations additionally or alternatively include determining whether the additional client device is to generate additional candidate text representations of the spoken utterance based on audio data captured by a microphone of the given client device and/or based on additional audio data captured by a microphone of the additional client device.
Description
Background
Automatic Speech Recognition (ASR) technology converts spoken natural language input into text. For example, audio data captured using a microphone can be converted to text. The ASR system can include an ASR model for generating a set of candidate identifications. The ASR system can select the generated text from the candidate recognition set.
Humans are able to participate in human-machine conversations using interactive software applications referred to herein as "automated assistants" (also referred to as "digital agents," "chat robots," "interactive personal assistants," "intelligent personal assistants," "assistant applications," "conversation agents," etc.). For example, humans (which may be referred to as "users" when they interact with an automated assistant) can provide commands and/or requests to the automated assistant using spoken natural language input (i.e., utterances) that can in some cases be converted to text (e.g., converted to text using ASR techniques) and then processed.
Disclosure of Invention
Embodiments described herein relate to generating a text representation of a spoken utterance based on candidate text representations of the spoken utterance generated at a given client device and/or based on one or more additional candidate text representations of the spoken utterance. Each of the additional candidate text representations of the spoken utterance is generated locally at a corresponding one of one or more additional client devices that are in a local environment with the given client device and that communicate with the given client device using one or more local networks, the one or more additional client devices being in the same room as the given client device, the one or more additional client devices being within a defined range of the given client device, the one or more additional client devices corresponding to the same user account, the one or more additional client devices being in an environment with the given client device in an additional or alternative manner, and/or a combination thereof. Candidate text representations of the spoken utterance can be generated by processing audio data captured of the spoken utterance and captured at a given client device. Candidate text representations are generated using an Automatic Speech Recognition (ASR) model stored locally at a given client device. The additional candidate text representations can be generated by the additional client device by processing the audio data at the additional client device and using an ASR model stored locally at the additional client device. The audio data processed at the additional client device can be audio data captured at the given client device (e.g., it can be sent from the given client device to the additional client device), or it can be additional audio data captured via a microphone of the additional client device.
As one example, audio data of the captured spoken utterance "set the thermostat to degeres (set thermostat to 70 degrees)" can be captured at the user's mobile phone, and candidate text representations of the spoken utterance can be generated by processing the audio data using an ASR model stored locally at the user's mobile phone. In some implementations, the audio data that captures the spoken utterance can also be sent to additional client devices in an environment with a mobile phone, such as a laptop computer, an automated assistant smart speaker, and/or an automated assistant smart display. In those implementations, each of the additional client devices is capable of generating a corresponding additional candidate text representation by processing the audio data using the corresponding locally stored additional ASR model. The additional candidate text representations can then be sent to the user's mobile phone, and the mobile phone can generate a text representation based on the candidate text representations (generated at the mobile phone) and the received additional candidate text representations (each generated at a corresponding one of the additional client devices). For example, two additional candidate text representations can be received at the mobile phone, each candidate text representation generated by a corresponding additional client device.
The mobile phone can then determine a final text representation based on the two additional candidate text representations and the candidate text representation. The final text representation can be determined using various techniques. For example, candidate text representations can be generated with confidence metrics (e.g., corresponding metrics for each word or other segment), and additional candidate representations can each be received with corresponding confidence metrics, and the mobile phone can use the confidence metrics to determine the final text representation. For example, a given additional candidate representation can be used as the final text representation based on its confidence measure with the highest indicated confidence. As another example, a final text representation can be generated to include the most common word segments in the candidate text representation and the additional candidate text representations. For example, assume that the candidate text representation is "get the thermostat to70 degeres (thermostat is set to70 degrees)", the first additional candidate text representation is "set the thermostat to7 degeres (thermostat is set to7 degrees)", and the second additional candidate text representation is "set the thermometer to70 degeres (thermometer is set to70 degrees)". In this case, "set the thermostat to70 settings (thermostat set to70 degrees)" can be generated as the final text representation, where "set" appears twice is selected instead of "get" appearing once, "thermostat" appears twice is selected instead of "thermo" appearing once, and "70" appears twice is selected instead of "7" appearing once.
The foregoing examples describe the mobile phone sending locally captured audio data to the additional client device for use by the additional client device in performing local ASR. However, as described above, in some implementations, one or more of the additional client devices can additionally or alternatively utilize audio data captured locally via the microphone of the additional client device in generating the corresponding candidate text representations. In some of these embodiments, the given client device can optionally not send audio data captured at the given client device to any additional client devices. As an example, and continuing with the previous example, additional candidate text representations of the spoken utterance of "Hey Assistant, set the thermostat to, 70 degeres (Hey, assistant, set thermostat to 70 degrees)" can be generated by the additional client device by processing the additional audio data using an ASR model stored locally at the additional client device. The additional audio data can capture the spoken utterance and can be captured via a microphone of the additional client device.
In some implementations, and optionally, for each of the additional client devices, a determination is made as to whether to send audio data from the given client device to the additional client device for use by the additional client device in performing local ASR. For example, a given client device (or other component of the system) can determine whether to send audio data captured using the given client device to the additional client device based on the hardware and/or software capabilities of the additional client device. The hardware and/or software capabilities of the additional client device can be determined from a homepage chart or other data stored locally at the given client device and/or based on data sent by the additional client device to the given client device. For example, when it is determined that the additional client device has a low quality microphone, the system can send audio data captured at the given client device to the additional client device. For example, the system may send audio data captured at the mobile phone to the smart watch based on knowledge that the smart watch has a low quality microphone. Additionally or alternatively, the system can determine characteristics (e.g., signal-to-noise ratio) of audio data captured using a given device, and can determine whether to send audio data to an additional client device based on the characteristics and optionally based on characteristics (e.g., signal-to-noise ratio) of the additional audio data captured at the additional client device. For example, when the signal-to-noise ratio indicates that the captured audio data is of poor quality, the system can determine not to transmit the audio data captured at the given client device. As another example, when the characteristic of the additional audio data indicates that it has a high quality and/or indicates that it has a better quality than the audio data captured at the given client device, the system can determine not to transmit the audio data captured at the given client device. Additionally or alternatively, the system can determine not to send audio data based on a communication link between a given client device and an additional client device (e.g., a wired connection between devices, a wireless connection between devices, etc.). For example, the system can determine not to send audio data when there is a low bandwidth connection between the given client device and the additional client device, and/or when there is a high delay in the connection between the given client device and the additional client device.
As yet another example, the system can determine whether to send audio data captured at the given client device to the additional client device based on historical instances of audio data at the given client device and/or the additional client device. For example, when instances of audio data captured at a given client device historically have low quality and/or instances of audio data captured at additional client devices historically have high quality, the system can determine not to send the audio data. Similarly, the system can determine to transmit audio data when instances of audio data captured at a given client device historically have high quality and/or instances of audio data captured at additional client devices historically have low quality. As yet another example, the system can determine whether to send audio data captured at a given client device to an additional client device based on whether the additional client device is physically proximate to the given client device (e.g., as determined using a stored home page chart and/or active techniques to determine current proximity). For example, the system can determine to transmit audio data only if the additional client device is not in the same room as the given client device (e.g., as determined based on the home page diagram) and/or is greater than a threshold distance from the given client device (e.g., as determined based on the active technique determining the distance between the given client device and the additional client device). As yet another example, the system can determine whether to send audio data captured at a given client device to an additional client device based on whether the additional client device locally detects voice activity (e.g., using a local voice activity detector). For example, the system can determine to send audio data only when the additional client device does not locally detect voice activity.
In some additional or alternative implementations in which the additional client device receives audio data from a given client device, the additional client device is able to determine whether to utilize the audio data or alternatively to utilize locally captured additional audio data when performing local ASR. In some of these embodiments, the additional client device can utilize one or more of the above-described considerations regarding determining whether to transmit audio data in determining whether to utilize audio data or additional audio data. For example, the additional client device can compare the signal-to-noise ratio of the audio data and the additional audio data and utilize audio data having a higher signal-to-noise ratio.
As described above, in some implementations, a given client device can be in an environment with one or more additional client devices. For example, a given client device as a mobile phone can be in an environment with a user's smart watch, a separate interactive speaker, and a smart camera. In some of these implementations, the system can select one or more of the one or more additional client devices for use in generating one or more additional candidate text representations of the spoken utterance. For example, the system can select one or more of the additional client devices based on historical interactions with the one or more client devices, based on hardware and/or software capabilities of the one or more additional client devices, and the like. For example, the system can select the additional client device based on data indicating that the additional client device includes a locally stored ASR model used by the additional client device in ASR that is more robust, accurate, and/or updated than the native ASR model of the given client device. Additionally or alternatively, the system can select the additional client device based on previous interactions between the user and the additional client device. For example, the system can select the additional client device based on the additional client device having received more queries from the user (and thus the user has more opportunities to provide feedback to the ASR model). In some of these embodiments, the ASR model at the additional client device that is more frequently used by the user can be better tailored to the user's speech and can generate more accurate candidate text representations of the spoken utterance.
As also described above, in some implementations, a textual representation of the spoken utterance can be generated based on a candidate textual representation of the spoken utterance generated at a given client device and based on one or more additional candidate textual representations of the spoken utterance generated at one or more corresponding additional client devices. For example, the system can randomly (or pseudo-randomly) select one or more of the candidate text representations of the spoken utterance as the text representation of the spoken utterance, the system can select the text representation of the spoken utterance based on historical interactions between the given client device and one or more additional client devices, the system can select the text representation of the spoken utterance based on hardware and/or software configuration of the given client device and/or one or more additional client devices, the system can select the text representation based on whether additional or alternative conditions are met, the system can select the text representation of the spoken utterance based on those word segments of the candidate text representations that are most frequent and/or of the highest confidence, the system can select the text representation of the spoken utterance based on the highest confidence candidate text representations, and/or a combination thereof.
For example, the system can select a first additional candidate text representation generated using the first additional client device as a text representation of the spoken utterance based on historical interactions between the given client device and the first additional client device that indicate that the first additional client device more frequently generates accurate candidate text representations. Additionally or alternatively, the system can select a second additional candidate text representation generated using the second additional client device as the text representation of the spoken utterance based on a quality metric and/or other metrics associated with an ASR model that is local to the second additional client device and that is used in generating the second additional candidate text representation.
Accordingly, various embodiments set forth techniques for generating a textual representation of a spoken utterance based on instances of local speech recognition, each instance being performed by a corresponding one of a plurality of client devices in an environment. Using device arbitration techniques, a single client device in an environment with a user can be selected to generate a textual representation of a spoken utterance spoken by the user. However, one or more additional client devices in the environment can generate a more accurate textual representation of the spoken utterance. For example, the first additional client device can have an updated and/or more robust and/or accurate version of the ASR model than the selected client device, and the second additional client device can capture spoken utterances, etc., in instances of the audio data that contain less noise than instances of the audio data captured by the selected client device. Thus, embodiments disclosed herein are capable of selectively utilizing at least additional client devices in performing local speech recognition, and at least a portion of additional candidate text representations generated by local speech recognition in generating final text representations of spoken utterances. These and other implementations can result in the occurrence of more accurate and/or more robust speech recognition. This enables more efficient human/machine interaction because speech recognition is more likely to be accurate and, in view of the more accurate speech recognition, recognition-dependent downstream processes (e.g., natural language understanding) can be performed more accurately. Thus, the occurrence of users who need to repeat spoken utterances due to speech recognition failure is reduced. This reduces the overall duration of the human/machine interaction and thus reduces the network and/or computing resources required to extend the interaction.
Various embodiments disclosed herein relate to selectively selecting one or more additional client devices in an environment having a given client device for generating one or more corresponding additional candidate text representations of a spoken utterance, wherein the text representations of the spoken utterance can be generated based on the candidate text representations generated using the given client device and the one or more candidate text representations generated using the corresponding one or more additional client devices. In other words, some embodiments disclosed herein do not always utilize additional client devices to generate additional candidate text representations and/or do not always utilize all available additional client devices to generate additional candidate text representations. Rather, some implementations may selectively utilize only any additional client devices to generate additional candidate text representations and/or may selectively utilize only some additional client devices to generate additional candidate text representations. Those embodiments can alternatively determine whether and/or which additional client devices to use based on one or more criteria. Considering such criteria can strive to balance the desire for more accurate speech recognition (and resulting computational resource savings, network resource savings, and/or system delays) with the use of computational and/or network resources required for more accurate speech recognition. In these and other ways, computing resources (e.g., battery power, power supply, processor cycles, memory, etc.) can be conserved by selectively determining only one or more additional candidate text representations that generated the spoken utterance.
As one example, a given client device can determine a hotword confidence score that indicates a probability that a spoken utterance includes a hotword, and utilize the hotword confidence score to determine whether to utilize additional client devices for speech recognition and/or how many additional client devices are utilized. For example, a given client device can determine that the hotword confidence score meets a threshold required to invoke an automated assistant, but that the hotword confidence score fails to meet a second threshold (e.g., less than 5% higher than the threshold). This can potentially indicate a low quality audio data stream capturing the spoken utterance. In some of these embodiments, the system can determine to use one or more corresponding additional client devices to generate one or more additional candidate text representations of the spoken utterance based on the identified weaker confidence level for the hotword. Utilizing additional candidate text representations of the spoken utterance can improve the accuracy of the text representation of the spoken utterance. In some cases, this can prevent the system from generating an incorrect text representation of the spoken utterance, which in turn can prevent the user from having to repeat the spoken utterance.
As another example, in addition to determining that the hotword confidence score meets a threshold, the system can also determine that a given client device is able to determine that the hotword confidence score indicates a very strong confidence for the hotword (e.g., it is 10% or more above the threshold). For example, a given client device may determine that the hotword confidence score satisfactorily meets a threshold, which may indicate a good quality audio data stream capturing a spoken utterance. In some of these implementations, the system may not utilize any additional client devices to generate one or more corresponding additional candidate text representations of the spoken utterance. Where the system is confident of capturing the quality of the audio data stream of the spoken utterance, selectively using one or more additional client devices to generate one or more additional corresponding candidate text representations of the spoken utterance can additionally or alternatively save computational resources required to generate the one or more additional candidate text representations of the spoken utterance.
Techniques are described herein with respect to generating text representations of spoken utterances using an ASR model. However, this is not meant to be limiting. In some implementations, the techniques described herein can additionally or alternatively be used to determine intent of a spoken utterance and/or to determine parameters of intent based on processing a textual representation of the spoken utterance using a local Natural Language Understanding (NLU) model.
The foregoing description is provided merely as an overview of some of the embodiments disclosed herein. These and other embodiments of the present technology are disclosed in additional detail below.
It should be appreciated that all combinations of the foregoing concepts and additional concepts described in more detail herein are considered to be part of the subject matter disclosed herein. For example, all combinations of claimed subject matter appearing at the end of this disclosure are considered part of the subject matter disclosed herein.
Drawings
Fig. 1 illustrates an example of a user in an environment having multiple client devices in accordance with various embodiments disclosed herein.
Fig. 2 illustrates an example of generating a textual representation of a spoken utterance using a client device, a first additional client device, and a second additional client device, in accordance with various embodiments disclosed herein.
FIG. 3 illustrates an exemplary environment in which various embodiments disclosed herein may be implemented.
Fig. 4 is a flow chart illustrating an exemplary process of generating a text representation of a spoken utterance in accordance with various embodiments disclosed herein.
Fig. 5 is a flowchart illustrating an exemplary process of selecting a subset of one or more additional client devices according to various embodiments disclosed herein.
Fig. 6 is a flow chart illustrating an exemplary process of generating additional candidate text representations of a spoken utterance, according to various embodiments disclosed herein.
Fig. 7 is a flow chart illustrating another exemplary process of generating a textual representation of a spoken utterance in accordance with various embodiments disclosed herein.
FIG. 8 illustrates another exemplary environment in which various embodiments disclosed herein may be implemented.
FIG. 9 illustrates an exemplary architecture of a computing device.
Detailed Description
FIG. 1 illustrates a user in an exemplary environment 100 having a plurality of client devices. In the example shown, the user 102 is in an environment 100 with a mobile phone 104, a smart watch 106, an automated assistant with a display 108, a Wi-Fi access point 110, a smart camera 112, and a laptop 114. The client devices in environment 100 are merely illustrative, and a user can be in an environment with one or more additional and/or alternative client devices. For example, the environment can include one or more of a desktop computer, a laptop computer, a tablet computing device, a mobile phone, a smart watch, one or more additional or alternative wearable computing devices, a stand-alone interactive speaker, an automated assistant with integrated display, a Wi-Fi access point, a smart thermostat, a smart oven, a smart camera, one or more additional or alternative smart computing devices, one or more additional or alternative computing devices, and/or combinations thereof.
In some implementations, a client device in an environment with a user is capable of executing an instance of an automated assistant client. For example, the smart watch 106 can execute an instance of an automated assistant client, the mobile phone 104 can execute an instance of an automated assistant client, the automated assistant with the display 108 can execute an instance of an automated assistant client, the Wi-Fi access point 110 can execute an instance of an automated assistant client, the smart camera 112 can execute an instance of an automated assistant client, and/or the laptop computer 114 can execute an instance of an automated assistant client.
In some implementations, different client devices can each include different hardware and/or software configurations. For example, the microphone of the mobile phone 104 may be better than the microphone of the smart watch 106. This can result in the mobile phone 104 capturing a higher quality audio data stream than the additional audio data stream captured using the smart watch 106. Additionally or alternatively, the ASR model of the laptop computer 114 may generate more accurate candidate text predictions than the ASR model of the smart camera 112.
As an illustrative example, the user 102 can speak a spoken utterance of "Hey assstat, turn on all the lights (Hey, assistant, all lights on)". One or more client devices in environment 100 can capture audio data that captures spoken utterances. Different factors can affect the quality of audio data captured at each of one or more client devices. In some implementations, a gesture of a user in an environment (e.g., a user's position and/or orientation) relative to a client device can affect the quality of audio data captured at one or more client devices. For example, a client device in front of a user may capture a higher quality audio data stream of spoken utterances than a client device in front of the user.
Additionally or alternatively, noise sources in the environment (e.g., a barking dog, a white noise machine, audio data from a television, one or more additional user utterances, one or more additional or alternative noise sources, and/or combinations thereof) can affect the quality of the audio data stream captured at the client device. For example, a dog may bark in the environment when the user is speaking an spoken utterance. The pose of a dog in an environment (e.g., the position and/or orientation of the dog) relative to client devices can affect the quality of audio data captured at one or more client devices. For example, a client device closest to the dog may capture a lower quality audio data stream than a client device further away from the dog. In other words, the audio data stream captured by the device closest to the dog may capture a higher percentage of bark dogs and a lower percentage of spoken utterances than one or more other client devices in the environment. Additional and/or alternative factors can affect the quality of an audio data stream captured at a client device in an environment.
In some implementations, the system can determine a given client device from client devices in the environment. For example, the system can select the mobile phone 104 as a given client device, and can generate candidate text representations of the spoken utterance by processing audio data that captures the spoken utterance using an ASR model local to the mobile phone 104. Additionally or alternatively, the system can select a subset of additional client devices in the environment to generate corresponding additional candidate text representations of the spoken utterance. In some implementations, the system can select one or more additional client devices according to process 404 of fig. 5 as described herein. For example, the system can select a subset of the automated assistant having the display 108, the smart camera 112, and the laptop computer 114.
In some implementations, the system can determine whether to send audio data that captures a spoken utterance captured at a given client device to a selected subset of additional client devices. In some implementations, the system can determine whether to send audio data captured at a given client device to one or more of a subset of additional client devices. Additionally or alternatively, the system can transmit audio data that captures spoken utterances captured at a given client device to one or more additional client devices in various ways. For example, the system can transmit a compressed version of audio data (e.g., generated by processing the audio data using lossy and/or lossless audio compression), can transmit an encrypted version of the audio data, can transmit the audio data in a streaming manner (e.g., in real-time or near real-time with spoken words to minimize delay), can transmit an unprocessed version of the audio data, and/or combinations thereof.
In some implementations, the system can generate one or more additional candidate text representations of the spoken utterance. For each additional client device in the subset of client devices, the client device is capable of determining whether to generate a corresponding additional candidate text representation based on audio data captured at the given client device and/or audio data captured at the corresponding additional client device. In some implementations, the additional client devices can generate corresponding additional candidate text representations of the utterance by processing the selected audio data using an ASR model local at the corresponding additional client device. In some implementations, the system can generate one or more additional candidate text representations of the spoken utterance according to the process 408 of fig. 6 described herein. For example, the system can generate a first additional candidate text representation of the spoken utterance by processing the audio data at a local ASR model of the automated assistant having the display 108, a second additional candidate text representation of the spoken utterance by processing the audio data at an ASR model local to the smart camera 112, and a third candidate text representation of the spoken utterance by processing the audio data at an ASR model local to the laptop computer 114.
In some implementations, a given client device can generate a text representation of a spoken utterance based on candidate text representations of the spoken utterance. In some implementations, the system can generate a textual representation of the spoken utterance according to process 412 of fig. 7 as described herein. For example, the system can generate a text representation of the spoken utterance based on the candidate text representation generated using the mobile telephone 104, the first additional candidate text representation generated using the automated assistant with the display 108, the second additional candidate text representation generated using the smart camera 112, and/or the third additional candidate text representation generated using the laptop computer 114.
Fig. 2 illustrates an example 200 of generating candidate text representations of a spoken utterance, in accordance with various embodiments. The illustrated example 200 includes a client device 202, a first additional client device 204, and a second additional client device 206 in an environment with a user. For example, the client device can be a mobile phone of a user, the first additional client device can be an automated assistant with a display, and the second additional client device can be a smart camera. In some implementations, the client device 202, the first additional client device 204, and/or the second additional client device 206 can each execute an instance of an automated assistant client.
At point 208, the client device 202 can capture audio data that captures the spoken utterance. For example, the client device 202 can capture a spoken utterance of "set the temperature to degeres (set temperature to 72 degrees)". In some implementations, at point 210, the first additional client device 204 can capture a first additional instance of the audio data of the captured spoken utterance. For example, the first additional client device can capture a first additional instance of the spoken utterance of "set the temperature to 72 grooves (set temperature to 72 degrees)". Additionally or alternatively, at point 212, the second additional client device 206 can capture a second additional instance of the audio data of the captured spoken utterance. For example, the second additional client device can capture a second additional instance of the spoken utterance of "set the temperature to 72 grooves (set temperature to 72 degrees)".
In some implementations, different quality audio data is captured at the client device, the first additional client device, and/or the second additional client device. For example, one of the client devices may have a better quality microphone, thereby enabling the corresponding client device to capture a higher quality audio data stream. Additionally or alternatively, background noise (e.g., additional user speech, barking, noise generated by an electronic device, baby crying, audio from a television, additional or alternative noise sources, and/or combinations thereof) may be captured in one or more audio data streams. In some implementations, more background noise can be captured at one client device than another client device. For example, the dog may be closer to the first additional client device than the second additional client device, and the first additional instance of capturing audio data of the spoken utterance may be able to capture more dogs barking than the second additional instance of capturing audio data of the spoken utterance. In some implementations, one or more of the client devices may not have the user interface input capabilities required to capture audio data (e.g., the client device does not have a microphone), and thus the client device may not capture corresponding audio data at points 208, 210, and/or 212.
In some implementations, at point 214, the client device 202 can send the audio data that captured the spoken utterance (i.e., the audio data captured using the client device 202 at point 208) to the first additional client device 204 and/or the second additional client device 206. In some other implementations, the client device 202 may not send audio data to the first additional client device 204 and/or the second additional client device 206 (not depicted). For example, the client device 202 may not transmit audio data that captures the spoken utterance based on an indication that the audio data is of poor quality.
At point 216, the first additional client device 204 can determine whether to process the audio data captured at the client device 202 and/or the first additional instance of the audio data captured at point 212. In some implementations, the first additional client device 204 can determine whether to process the audio data and/or the first additional instance of the audio data according to the process 408 of fig. 6 described herein. Similarly, at point 218, the second additional client device 206 can determine whether to process the audio data captured at the client device 202 and/or the second additional instance of audio captured at point 212. In some implementations, the second additional client device 206 can determine whether to process the audio data and/or the second additional instance of the audio data according to the process 408 of fig. 6 described herein.
At point 220, the client device 202 can generate candidate text representations of the spoken utterance. In some implementations, the client device 202 can generate candidate text representations of the spoken utterance by processing the captured audio data that captures the spoken utterance using an ASR model stored locally at the client device 202. In some implementations, the first additional client device 204 can generate a first additional candidate text representation of the spoken utterance at point 222. In some implementations, the first additional candidate text representation of the spoken utterance can be generated by processing the audio data and/or the first additional instance of the audio data using an ASR model stored locally at the first additional client device. In some implementations, a first additional candidate text representation of the spoken utterance can be generated according to the process 408 of fig. 6 described herein. Similarly, at point 224, a second additional candidate text representation of the spoken utterance can be generated using a second additional client device 206. In some implementations, the second additional candidate text representation of the spoken utterance can be generated by processing the audio data and/or the second additional instance of the audio data using an ASR model stored locally at the second additional client device.
At point 226, the first additional client device 204 can send a first additional candidate text representation of the spoken utterance to the client device 202. Similarly, at point 228, the second additional client device 206 can send a second additional candidate text representation of the spoken utterance to the client device 202.
At point 230, the client device 202 can generate a textual representation of the spoken utterance. In some implementations, the client device 202 can generate a text representation of the spoken utterance based on the candidate text representation of the spoken utterance, the first additional candidate text representation of the spoken utterance, and/or the second additional candidate text representation of the spoken utterance. In some implementations, the client device 202 can generate a textual representation of the spoken utterance according to the process 412 of fig. 7 described herein.
Fig. 3 illustrates a block diagram of an exemplary environment 300 in which embodiments disclosed herein may be implemented. The exemplary environment 300 includes a client device 302 and an additional client device 314. The client device 302 can include a user interface input/output device 304, a candidate text representation engine 306, a text representation engine 308, an add-on device engine 310, an add-on or replacement engine (not shown), an ASR model 312, and/or an add-on or replacement model (not shown). The additional client devices 314 can include additional user interface input/output devices 316, an audio source engine 318, additional candidate text representation engines 320, additional or alternative engines (not shown), additional ASR models 322, and/or additional or alternative models (not shown).
In some implementations, the client device 302 and/or the additional client device 314 may include a user interface input/output device, which may include, for example, a physical keyboard, a touch screen (e.g., implementing a virtual keyboard or other text input mechanism), a microphone, a camera, a display screen, and/or a speaker. For example, a user's mobile telephone may include a user interface input output device; the stand-alone digital assistant hardware device may include a user interface input/output device; the first computing device may include a user interface input device, while the separate computing device may include a user interface output device, or the like. In some implementations, all or aspects of the client device 302 and/or the additional client device 314 may be implemented on a computing system that also includes a user interface input/output device.
Some non-limiting examples of client devices 302 and/or additional client devices 314 include one or more of the following: a desktop computing device, a laptop computing device, a standalone hardware device dedicated at least in part to an automated assistant, a tablet computing device, a mobile phone computing device, a computing device of a vehicle (e.g., an in-vehicle communication system and an in-vehicle entertainment system, an in-vehicle navigation system), or a wearable apparatus that includes a user of the computing device (e.g., a watch of a user with the computing device, glasses of a user with the computing device, a virtual or augmented reality computing device). Additional and/or alternative computing systems may be provided. The client device 302 and/or additional client devices 314 can include one or more memories for storing data and software applications, one or more processors for accessing data and executing applications, and other components that facilitate communication over a network. Operations performed by client device 302 and/or additional client devices 314 may be distributed across multiple computing devices. For example, computer programs running on one or more computers in one or more locations can be coupled to each other through a network.
In some implementations, the client device 302 may include a user interface input/output device 304, while the additional client device 314 can include an additional user interface input/output device 316, the additional user interface input/output device 316 may include, for example, a physical keyboard, a touch screen (e.g., implementing a virtual keyboard or other text input mechanism), a microphone, a camera, a display screen, and/or a speaker. In some implementations, the client device 302 and/or the additional client device 314 may include an automation assistant (not depicted), and all or aspects of the automation assistant may be implemented on a computing device separate and apart from the client device containing the user interface input/output device (e.g., all or aspects may be implemented "in the cloud"). In some of those embodiments, those aspects of the automated assistant may communicate with the computing device via one or more networks, such as a Local Area Network (LAN) and/or a Wide Area Network (WAN) (e.g., the internet).
In some implementations, the user interface input/output device 304 is capable of capturing audio data that captures spoken utterances spoken by the user. For example, one or more microphones of the client device 304 can capture audio data that captures the spoken utterance "Hey Assistant, set an alarm for 8am (Hey, assistant, set alarm at 8 am)". In some implementations, the candidate text representation engine 306 can process the audio data of the captured spoken utterance using the ASR model 312 to generate a candidate text representation of the spoken utterance.
Additionally or alternatively, the additional device engine 310 can be used to select a subset of one or more additional client devices in the environment 300, and can be used to determine whether to send audio data captured at the client device 302 to the one or more selected additional client devices, and/or can be used to send audio data capturing spoken utterances to the one or more selected additional client devices. In some implementations, the add-on engine 310 can select a subset of one or more additional client devices according to the process 404 of fig. 5 as described herein. For example, the add-on engine 310 can be used to select the add-on client device 314. Additionally or alternatively, the add-on device engine 310 can determine whether to send audio data captured at the client device 302 to one or more additional client devices.
In some implementations, the text representation engine 308 can be used to generate a text representation of the spoken utterance based on a candidate text representation of the spoken utterance generated using the client device 302 and/or one or more additional candidate text representations of the spoken utterance generated using one or more corresponding additional client devices. For example, the text representation engine 308 can generate a text representation of the spoken utterance based on candidate text representations of the spoken utterance generated using the client device 302 and/or additional candidate text representations of the spoken utterance generated using the additional client devices 314. In some implementations, the text representation engine 308 can generate a text representation of the spoken utterance according to the process 412 of fig. 7 described herein.
In some implementations, the additional client device 314 can use the additional user interface input/output device 316 to capture additional instances of the audio data that captured the spoken utterance. For example, the additional client device 314 can use one or more additional microphones of the additional client device to capture additional instances of the spoken utterance "Hey Assistant, set an alarm for 8am (Hey, assistant, alarm clock set at 8 am)". In some implementations, the additional client device 314 can use the audio source engine 318 to determine whether to process audio data that captures the spoken utterance generated using the client device 302 and/or to capture additional audio data of the spoken utterance generated using the additional client device 314 to generate additional candidate text representations of the spoken utterance. In some implementations, the additional client device 314 can use the additional candidate text representation engine 320 to generate additional candidate text representations of the spoken utterance by processing the audio data selected using the audio source engine 318 using the additional ASR model 322. In some implementations, the additional candidate text representation engine 320 can generate additional candidate text representations of the spoken utterance according to the process 408 of fig. 6 described herein.
Fig. 4 is a flow chart illustrating an exemplary process 400 of generating candidate text representations of a spoken utterance, according to various embodiments disclosed herein. For convenience, the operations of the flowcharts are described with reference to systems performing the operations. The system may include various components of various computer systems, such as one or more components of client device 302, additional client devices 314, client device 802, and/or computing system 810. Furthermore, although the operations of process 400 are illustrated in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 402, the system captures audio data of a spoken utterance at a client device, wherein the client device is in an environment with one or more additional client devices. In some implementations, the client device and/or the additional client device are capable of executing corresponding instances of the automated assistant client. In some implementations, a user can be in a room with several client devices (such as a mobile phone, a laptop, a standalone automated assistant, etc.). In some implementations, when two or more client devices are able to capture a spoken utterance spoken by a user, classical device arbitration techniques can be used to determine a given client device for processing the spoken utterance. For example, audio data capturing the spoken utterance can be captured at a given client device of a separate interactive speaker, and the separate interactive speaker can be in an environment with a first additional client device of the mobile phone and a second additional client device of the smart camera.
At block 404, the system selects a subset of one or more additional client devices. In some implementations, the system can select a subset of one or more additional client devices according to process 404 as shown in fig. 5. For example, the system can select a first additional client device of the mobile phone, a second additional client device of the smart camera, or a first additional client device of the mobile phone and a second additional client device of the smart camera.
At block 406, the system generates candidate text representations of the spoken utterance by processing the captured audio data using the local ASR model. In some implementations, the candidate text representations of the spoken utterance can be the highest ranked hypotheses generated using the ASR model. Additionally or alternatively, the candidate text representations of the spoken utterance can include a plurality of hypotheses generated using the ASR model.
At block 408, the system generates, at one or more additional client devices, one or more additional candidate text representations of the spoken utterance(s). In some implementations, the system can generate one or more additional candidate text representations at one or more additional client devices according to process 408 as shown in fig. 6. For example, the system can generate a first additional candidate text representation of the spoken utterance using a first additional ASR model locally stored at the first additional client device, and/or the system can generate a second additional candidate text representation of the spoken utterance using a second additional ASR model locally stored at the second additional client device.
At block 410, the system receives one or more additional candidate text representations of the spoken utterance from the selected subset of one or more additional client devices. For example, if the system selects a first additional client device and a second additional client device at block 404, the system can receive a first additional candidate text representation generated at the first additional client device (e.g., generated according to process 408 of fig. 6) and a second additional candidate text representation generated at the second additional client device (e.g., generated according to process 408 of fig. 6).
At block 412, the system generates a text representation of the spoken utterance based on the candidate text representation of the spoken utterance and one or more additional candidate text representations of the spoken utterance. In some implementations, the system can generate a text representation of the spoken utterance based on the candidate text representation and the one or more additional candidate text representations according to process 412 of fig. 7.
Fig. 5 is a flow chart illustrating an exemplary process 404 of selecting a subset of one or more additional client devices according to various embodiments disclosed herein. For convenience, the operations of the flowcharts are described with reference to systems performing the operations. The system may include various components of various computer systems, such as one or more components of client device 302, additional client devices 314, client device 802, and/or computing system 810. Further, although the operations of process 404 are illustrated in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 502, the system selects an additional client device of the one or more additional client devices, wherein the one or more additional client devices are in an environment with the given client device. For example, a given client device can be in an environment having a first additional client device, a second additional client device, and a third additional client device.
At block 504, the system determines whether to select an additional client device based on one or more client device parameters. In some implementations, the one or more client device parameters can include a power source of the client device, hardware of the client device (e.g., whether the client device has a microphone, a processor, available memory, etc.), software of the client device (e.g., ASR model version, ASR model size, ASR model capacity, one or more additional or alternative model versions, etc.), one or more additional or alternative device parameters, and/or combinations thereof. For example, in some implementations, the system can include each of the one or more additional client devices in the subset.
In some implementations, the system can select each of one or more additional client devices (e.g., each client device running alternating current) that are powered by plugging in an electrical outlet. In other words, the system is able to select additional client devices when the power cost is negligible. In some implementations, the system can select additional client devices if the battery power of the client devices satisfies one or more conditions. For example, if the remaining battery power exceeds a threshold (e.g., the battery has more than 25% of the remaining power), if the battery's capacity exceeds a threshold (e.g., the battery's capacity exceeds 1000 mAh), if the battery is currently charging, if additional or alternative conditions are met, and/or combinations thereof, the system can select additional client devices. In some implementations, the system can select the additional client device based on hardware of the additional client device. For example, the system can process hardware of each of the one or more additional client devices using a machine learning model to select a subset of the one or more client devices.
In some implementations, the system can select the additional client device based on whether the additional client device was previously selected in a previous iteration of the process. For example, if the system determines that the first additional client device was selected and the second additional client device was not selected when processing the previous spoken utterance, the system can select the first additional client device and not the second additional client device.
The system can determine a confidence value that indicates a confidence of a candidate text representation generated at the client device using the ASR model. In some implementations, the system can determine whether the confidence value satisfies one or more conditions, such as whether the confidence value satisfies a threshold. When the confidence value indicates a low confidence in the candidate text representation, the system can select one or more additional client devices. For example, when the confidence value is below a threshold, the system can select one or more additional client devices.
At block 506, the system determines whether a further additional client device is selected. In some implementations, the system can determine whether to select further additional client devices based on whether there are any remaining unused additional client devices, whether a threshold number of additional client devices have been selected, whether one or more additional or alternative conditions are met, and/or combinations thereof. If so, the system returns to block 502, selects a further additional client device, and proceeds to block 504 based on the further additional client device. If not, the process ends.
Fig. 6 is a flow chart illustrating an exemplary process of generating 408 additional candidate text representations of a spoken utterance in accordance with various embodiments disclosed herein. For convenience, the operations of the flowcharts are described with reference to systems performing the operations. The system may include various components of various computer systems, such as one or more components of client device 302, additional client devices 314, client device 802, and/or computing system 810. Further, although the operations of process 408 are illustrated in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 602, at an additional client device, the system captures additional instances of audio data that captured the spoken utterance. For example, the additional client device can capture the spoken word "Hey assstat, what is the temperature on Tuesday (Hey, assistant, temperature how on tuesday)".
At block 604, at the additional client device, the system receives an instance of capturing audio data of a spoken utterance captured at a given client device, wherein the given client device is in an environment with the additional client device. For example, the additional client device can receive audio data captured at a given client device that captures the spoken utterance "Hey assstat, what is the temperature on Tuesday (Hey, assistant, temperature of tuesday)".
At block 606, the system compares the additional instance of audio data with the received instance of audio data.
At block 608, the system determines whether to process additional instances of audio data and/or instances of received audio data based on the comparison. In some implementations, the system can randomly (or pseudo-randomly) select instances of audio data or additional instances of audio data for processing. In some implementations, the system can select both instances of audio data and additional instances of audio data. In some implementations, the system can select audio data for processing based on the quality of the audio data. For example, the system can select additional instances of audio data or instances of audio data based on microphones of additional client devices and/or microphones of a given client device. For example, when the microphone of an additional client device captures better quality audio data than the microphone of a given client device, the system can select additional instances of the audio data.
Additionally or alternatively, the system can determine a signal-to-noise ratio for an instance of audio data and an additional signal-to-noise ratio for an additional instance of audio data. The system is able to select instances of audio data having a signal-to-noise ratio that indicates a better quality audio data stream. Additional or alternative perceived quality metrics can be utilized to determine a better quality audio data stream. For example, a machine learning model that has been trained to predict the quality level of an audio data stream can be utilized in selecting the audio data stream.
At block 610, the system processes the determined audio data using an additional ASR model stored locally at the additional client device to generate additional candidate text representations of the spoken utterance. For example, if additional instances of audio data are selected for processing, the system can generate additional candidate text representations of the spoken utterance by processing the additional instances of audio data using additional ASR models stored locally at the additional client devices. As another example, if an instance of audio data is selected for processing, the system can generate additional candidate text representations of the spoken utterance by processing the instance of audio data using an additional ASR model stored locally at the additional client device.
At block 612, the system sends additional candidate text representations of the spoken utterance to the given client device.
Fig. 7 is a flow chart illustrating an exemplary process of generating 412 a text representation of a spoken utterance in accordance with various embodiments disclosed herein. For convenience, the operations of the flowcharts are described with reference to systems performing the operations. The system may include various components of various computer systems, such as one or more components of client device 302, additional client devices 314, client device 802, and/or computing system 810. Furthermore, although the operations of process 412 are illustrated in a particular order, this is not meant to be limiting. One or more operations may be reordered, omitted, and/or added.
At block 702, the system captures audio data of a spoken utterance at a client device, wherein the client device is in an environment with one or more additional client devices. For example, a separate interactive speaker can capture audio data that captures a spoken utterance of "Hey Assistant, turn off the living room lights (Hey, assistant, off living room lights)", where the separate interactive speaker is in an environment with a mobile phone and a smart television.
At block 704, the system generates candidate text representations of the spoken utterance by processing the audio data at the client device using the local ASR model. For example, the system can process audio data of the captured spoken utterance "Hey Assistant, turn off the living room lights (Hey, assistant, turn off the living room light)" using the local ASR model of the independent interactive speaker to generate candidate text representations of the spoken utterance. In some implementations, the system can generate candidate text representations of the spoken utterance using the local ASR model. Additionally or alternatively, the system can generate a plurality of hypotheses for a text representation of the spoken utterance using the local ASR model.
At block 706, the system receives one or more candidate text representations of the spoken utterance from one or more additional client devices. For example, the system can receive a first additional candidate text representation of the spoken utterance "Hey Assistant, turn off the living room lights (Hey, assistant, off living room light)" from the mobile phone and a second additional candidate text representation of the spoken utterance "Hey Assistant, turn off the living room lights (Hey, assistant, off living room light)" from the smart television. In some implementations, one or more additional candidate text representations can be generated using one or more additional client devices according to process 408 of fig. 6 as described herein. In some implementations, the system can receive additional candidate text representations of the spoken utterance from each of the one or more additional client devices using corresponding local ASR models local to the additional client devices. In some other implementations, the system can receive, from each of the one or more additional client devices, a plurality of candidate text representations of the spoken utterance generated using a corresponding local ASR model local to the additional client device.
At block 708, the system compares the candidate text representation of the spoken utterance with one or more additional candidate text representations of the spoken utterance.
At block 710, the system generates a textual representation of the spoken utterance based on the comparison. In some implementations, the system can randomly (or pseudo-randomly) select one of the candidate text representations of the spoken utterance as the text representation of the spoken utterance. For example, the system can randomly (or pseudo-randomly) select candidate text representations of the spoken utterance generated using the first additional client device as the text representations of the spoken utterance. Additionally or alternatively, the system can randomly (or pseudo-randomly) select candidate text representations of the spoken utterance generated using the given client device as the text representations of the spoken utterance.
In some implementations, the system can rank candidate text representations of the spoken utterance, where the candidate text representation of the spoken utterance with the most "votes" can be selected as the text representation of the spoken utterance. For example, the system can compare candidate text representations of the spoken utterance "Hey Assistant, turn off the living room lights (Hey, assistant, turn off living room light)" generated using a given client device, a first additional candidate text representation of the spoken utterance "Hey Assistant, turn on the living room lights (Hey, assistant, turn on living room light)" generated using a first additional client device, and a second additional candidate text representation of the spoken utterance "Hey Assistant, turn off the living room lights (Hey, assistant, turn off living room light)" generated using a second additional client device. In other words, two of the client devices (e.g., a given client device and a second additional client device) generate candidate text representations of the spoken utterance of "Hey Assistant, turn off the living room lights (Hey, assistant, living room light off)", while only one of the client devices (e.g., the first additional client device) generates candidate text representations of the spoken utterance of "Hey Assistant, turn on the living room lights (Hey, assistant, living room light on)". In some implementations, candidate text representations of the spoken utterance can be uniformly weighted. For example, the system can generate "Hey Assistant, turn off the living room lights (Hey Assistant, turn off living room light)" as a candidate text representation of the spoken utterance based on two of the three client devices to select "Hey Assistant, turn off the living room lights (Hey Assistant, turn off living room light)" as a text representation of the spoken utterance.
In some other implementations, candidate text representations of the spoken utterance can be weighted based on a client device used in generating the spoken utterance. For example, the candidate text representations of the spoken utterance can be weighted based on a version of the ASR model used in generating the candidate text representations (e.g., the system can weight the candidate text representations of the spoken utterance more heavily when generating the candidate text representations of the spoken utterance using a higher quality ASR model), hardware of the corresponding client device (e.g., the system can weight the candidate text representations of the spoken utterance more heavily when the corresponding client device captures a higher quality audio data stream), based on one or more additional or alternative conditions, and/or a combination thereof. For example, a mobile phone may have better hardware (such as a better microphone) to capture higher quality audio data and may have a higher quality version of the ASR model. In some implementations, the system can weight the first additional candidate text representation of the spoken utterance generated using the mobile phone (with a higher quality microphone and a higher quality ASR model) more heavily than other candidate text representations of the spoken utterance. In some implementations, the system can select a candidate text representation of "Hey Assistant, turn on the living room lights (Hey, assistant, turn on living room lights)" generated using the mobile phone as the text representation of the spoken utterance, although the other two candidate representations of the spoken utterance indicate turn off of the living room lights.
In some implementations, the system can selectively combine portions of candidate text representations of the spoken utterance. In some implementations, the system can cooperatively generate the top N hypothesis lists using one or more candidate text representations generated using a given client device and one or more candidate text representations generated using one or more additional client devices. For example, the system can incorporate a list of hypotheses from various devices.
In some implementations, the system can determine a confidence score that indicates a probability that the candidate text representation captures the spoken utterance. For example, the system can generate a confidence score indicating a probability of a candidate text representation of the spoken utterance, a first additional confidence score indicating a probability of a first additional candidate text representation capturing the spoken utterance, and a second additional candidate text representation indicating a probability of a second additional candidate text representation capturing the spoken utterance. In some implementations, the system can determine a text representation of the spoken utterance based on the candidate text representation of the spoken utterance with the highest confidence score.
Additionally or alternatively, the system can generate a confidence score based on one or more portions of the candidate text representation of the spoken utterance. In some implementations, the system can generate a hotword confidence score based on the probability that the spoken utterance captures the hotword. For example, the system can generate a hotword confidence score that indicates a probability that the candidate text representation of the spoken utterance includes the hotword "Hey Assistant".
In some implementations, the system can generate a plurality of candidate text representations using a given client device, a plurality of first additional candidate text representations of the spoken utterance using a first additional client device, and/or a plurality of second additional candidate text representations of the spoken utterance using a second additional client device. In some implementations, the system can determine the text representation of the spoken utterance based on the plurality of candidate text representations of the spoken utterance, the plurality of first additional candidate text representations of the spoken utterance, and/or the plurality of second additional candidate text representations of the spoken utterance, in accordance with the techniques described herein.
In some implementations, the system can bias one or more of the plurality of candidate textual representations of the spoken utterance. For example, a mobile phone may have a better ASR model, but the contact list for biasing may be accessible (or only accessible) via a separate interactive speaker. In some implementations, a contact list stored at a separate interactive speaker can be used to bias a plurality of first additional candidate text representations generated using a mobile phone (i.e., a device with a "better" ASR model). In some of these implementations, the system can determine a textual representation of the spoken utterance based on the bias.
Turning now to fig. 8, an exemplary environment in which various embodiments can be implemented is illustrated. Fig. 8 is initially described and includes a client computing device 802 that executes an instance of an automated assistant client 804. The one or more cloud-based automation assistant components 810 can be implemented on one or more computing systems (collectively, "cloud" computing systems) communicatively coupled to the client device 802 via one or more local and/or wide area networks (e.g., the internet) indicated generally at 808.
Through its interaction with one or more cloud-based automation assistant components 810, an instance of an automation assistant client 804 may form content that appears to a user to be a logical instance of the automation assistant 800 with which the user may engage in a human-machine conversation. An example of such an automated assistant 800 is depicted in fig. 8. Thus, it should be appreciated that in some implementations, a user participating in an automated assistant client 804 executing on a client device 802 may actually participate in his or her own logical instance of the automated assistant 800. For brevity and simplicity, the term "automated assistant" used herein as "serving" a particular user will generally refer to a combination of an automated assistant client 804 executing on a client device 802 operated by the user and one or more cloud-based automated assistant components 810 (which may be shared among multiple automated assistant clients of multiple client computing devices). It should also be appreciated that in some embodiments, the automated assistant 800 may respond to requests from any user, regardless of whether the user is actually "served" by that particular instance of the automated assistant 800.
The client computing device 802 may be, for example: a desktop computing device, a laptop computing device, a tablet computing device, a mobile phone computing device, a computing device of a user's vehicle (e.g., an in-vehicle communication system, an in-vehicle entertainment system, an in-vehicle navigation system), a stand-alone interactive speaker, a smart appliance such as a smart television, and/or a wearable device of a user including a computing device (e.g., a watch of a user with a computing device, glasses of a user with a computing device, a virtual or augmented reality computing device). Additional and/or alternative client computing devices may be provided. In various implementations, the client computing device 802 can also optionally operate one or more other applications in addition to the automated assistant client 804, such as a message exchange client (e.g., SMS, MMS, online chat), browser, and the like. In some of those various embodiments, one or more of the other applications can optionally interface with the automated assistant 800 (e.g., via an application programming interface) or include their own instance of the automated assistant application (which can also interface with the cloud-based automated assistant component 810).
The automated assistant 800 engages in a human-machine conversation session with a user via user interface input and output devices of the client device 802. In order to preserve user privacy and/or save resources, in many cases, the user must often explicitly invoke the automated assistant 800 before the automated assistant will fully process the spoken utterance. Explicit invocation of the automated assistant 800 can occur in response to some user interface input received at the client device 802. For example, user interface inputs that can invoke the automated assistant 800 via the client device 802 can optionally include actuation of hardware and/or virtual buttons of the client device 802. In addition, the automated assistant client can include one or more local engines 806, such as a call engine operable to detect the presence of one or more spoken call phrases. The invocation engine can invoke the automated assistant 800 in response to detecting one of the spoken invocation phrases. For example, the invocation engine can invoke the automated Assistant 800 in response to detecting a spoken invocation phrase such as "Hey Assistant", "OK Assistant", and/or "Assistant". The invocation engine can continuously process (e.g., if not in "inactive" mode) a stream of audio data frames based on output from one or more microphones of the client device 802 to monitor for the occurrence of spoken invocation phrases. While monitoring for the occurrence of a spoken call phrase, the call engine discards (e.g., after temporary storage in a buffer) any audio data frames that do not include the spoken call phrase. However, when the call engine detects the presence of a spoken call phrase in the processed audio data frame, the call engine can call the automated assistant 800. As used herein, "invoking" the automated assistant 800 can include causing one or more previously inactive functions of the automated assistant 800 to be activated. For example, invoking the automated assistant 800 can include having the one or more local engines 806 and/or the cloud-based automated assistant component 810 further process the audio data frames based on which the invocation phrase was detected, and/or one or more subsequent audio data frames (without further processing of the audio data frames occurring prior to invocation).
The one or more local engines 806 of the automated assistant 800 are optional and can include, for example, the call engine described above, a local speech-to-text ("STT") engine (that converts captured audio to text), a local text-to-speech ("TTS") engine (that converts text to speech), a local natural language processor (that determines semantic meaning of audio and/or text converted from audio), and/or other local components. Because the client device 802 is relatively limited in terms of computing resources (e.g., processor cycles, memory, battery, etc.), the local engine 806 may have limited functionality relative to any counterpart included in the cloud-based automation assistant component 810.
The cloud-based automation assistant component 810 utilizes virtually unlimited resources of the cloud to perform more robust and/or accurate processing of audio data and/or other user interface inputs relative to any counterpart of the local engine 806. Again, in various implementations, the client device 802 can provide audio data and/or other data to the cloud-based automation assistant component 810 in response to the invocation engine detecting a spoken invocation phrase or detecting some other explicit invocation of the automation assistant 800.
The illustrated cloud-based automation assistant component 810 includes a cloud-based TTS module 812, a cloud-based STT module 814, a natural language processor 816, a dialog state tracker 818, and a dialog manager 820. In some implementations, one or more of the engines and/or modules of the automated assistant 800 can be omitted, combined, and/or implemented in a separate component from the automated assistant 800. Furthermore, in some implementations, the automated assistant 800 can include additional and/or alternative engines and/or modules. The cloud-based STT module 814 can convert the audio data to text, which can then be provided to the natural language processor 816.
The cloud-based TTS module 812 can convert text data (e.g., natural language responses formulated by the automated assistant 800) into computer-generated speech output. In some implementations, TTS module 812 can provide computer-generated speech output to client device 802 for direct output, e.g., using one or more speakers. In other implementations, text data (e.g., natural language responses) generated by the automated assistant 800 can be provided to one of the local engines 806, and the local engines 806 can then convert the text data to locally output computer-generated speech.
The natural language processor 816 of the automated assistant 800 processes the free-form natural language input and generates annotation output for use by one or more other components of the automated assistant 800 based on the natural language input. For example, the natural language processor 816 can process natural language freeform input, which is text input that is a conversion by the STT module 814 of audio data provided by a user via the client device 802. The generated annotation output may include one or more annotations of the natural language input, and optionally one or more (e.g., all) terms of the natural language input.
In some implementations, the natural language processor 816 is configured to identify and annotate various types of grammar information in natural language inputs. In some implementations, the natural language processor 816 can additionally and/or alternatively include an entity marker (not depicted) configured to annotate entity references in one or more segments, such as references to persons (including, for example, literature personas, celebrities, public personas, etc.), organizations, locations (real and imaginary), and the like. In some implementations, the natural language processor 816 may additionally and/or alternatively include a co-fingering parser (coreference resolver, not depicted) configured to group or "cluster" references to the same entity based on one or more contextual cues. For example, the term "skin" in natural language input "I liked Hypothetical Caf e last time we ate there" (where I would like me last time we had a meal at the hypo-logical cafe) may be parsed into "hypo-logical cafe" using a co-fingering parser. In some implementations, one or more components of the natural language processor 816 may rely on annotations from one or more other components of the natural language processor 816. In some implementations, one or more components of the natural language processor 816 can use related prior inputs and/or other related data other than the specific natural language input to determine one or more annotations when processing the specific natural language input.
In some implementations, the dialog state tracker 818 may be configured to track "dialog states" including, for example, belief states of targets (or "intents") of one or more users during a human-machine dialog session and/or across multiple dialog sessions. In determining dialog states, some dialog state trackers may seek to determine the most likely value of a slot instantiated in a dialog based on user and system utterances in the dialog session. Some techniques utilize a fixed ontology that defines a set of slots and a set of values associated with those slots. Additionally or alternatively, some techniques may be customized for individual slots and/or domains. For example, some techniques may require training a model for each slot type in each domain.
Dialog manager 820 can be configured to map a current dialog state, for example, provided by dialog state tracker 818, to one or more "response actions" of a plurality of candidate response actions that are then performed by automated assistant 800. The responsive action may occur in various forms depending on the current dialog state. For example, initial and midstream dialog states corresponding to the rounds of dialog sessions that occurred prior to the last round (e.g., when performing the end user desired task) may be mapped to various responsive actions of the automated assistant 800 that include outputting additional natural language dialogs. The response dialog may include, for example, a request by the user to provide parameters for some action (i.e., filling the slot) that the dialog state tracker 818 believes the user intends to perform. In some implementations, the responsive action may include actions such as "request" (e.g., find parameters for slot fill), "propose" (e.g., suggest actions or course of actions for the user), "select", "notify" (e.g., provide requested information to the user), "not match" (e.g., notify the user of the last input by the user), command to the peripheral device (e.g., turn off the light bulb), etc.
FIG. 9 is a block diagram of an exemplary computing device 910 that may optionally be used to perform one or more aspects of the techniques described herein. In some implementations, one or more of the client computing devices and/or other components may include one or more components of the example computing device 910.
Computing device 910 typically includes at least one processor 914 that communicates with a number of peripheral devices via a bus subsystem 912. These peripheral devices may include a storage subsystem 924 (including, for example, a memory subsystem 925 and a file storage subsystem 926), a user interface output device 920, a user interface input device 922, and a network interface subsystem 916. Input and output devices allow users to interact with computing device 910. Network interface subsystem 916 provides an interface to external networks and couples to corresponding interface devices among other computing devices.
User interface input devices 922 may include, a keyboard, a pointing device such as a mouse, trackball, touch pad, or tablet, a scanner, a touch screen incorporated into a display, an audio input device such as a voice recognition system, a microphone, and/or other types of input devices. In general, use of the term "input device" is intended to include all possible types of devices and ways of inputting information into computing device 910 or onto a communication network.
The user interface output device 920 may include a display subsystem, a printer, a facsimile machine, or a non-visual display such as an audio output device. The display subsystem may include a cathode ray tube ("CRT"), a flat panel device such as a liquid crystal display ("LCD"), a projection device, or some other mechanism for creating a viewable image. The display subsystem may also provide for non-visual display, such as via an audio output device. In general, use of the term "output device" is intended to include all possible types of devices and ways to output information from computing device 910 to a user or to another machine or computing device.
Storage subsystem 924 stores programming and data structures that provide the functionality of some or all of the modules described herein. For example, storage subsystem 924 may include logic to perform selected aspects of one or more of the processes of fig. 4, 5, 6, and/or 7, as well as to implement the various components depicted in fig. 3 and/or 9.
These software modules are typically executed by processor 914 alone or in combination with other processors. The memory 925 used in the storage subsystem 924 can include a number of memories including a main random access memory ("RAM") 930 for storing instructions and data during program execution and a read only memory ("ROM") 932 in which fixed instructions are stored. File storage subsystem 926 is capable of providing persistent storage for program and data files, and may include a hard disk drive, a floppy disk drive, and associated removable media, CD-ROM drive, optical disk drive, or removable media cartridge. Modules implementing the functionality of certain embodiments may be stored by file storage subsystem 926 in storage subsystem 924 or in another machine accessible to processor 914.
Bus subsystem 912 provides a mechanism for letting the various components and subsystems of computing device 910 communicate with each other as intended. Although bus subsystem 912 is shown schematically as a single bus, alternative embodiments of bus subsystem 912 may use multiple buses.
Computing device 910 can be of various types including a workstation, a server, a computing cluster, a blade server, a server farm, or any other data processing system or computing device. Due to the ever-changing nature of computers and networks, the description of computing device 910 depicted in FIG. 9 is intended only as a specific example for purposes of illustrating some embodiments. Many other configurations of computing device 910 are possible with more or fewer components than the computing device depicted in fig. 9.
Where the system described herein gathers personal information about a user (or sometimes referred to herein as a "participant") or may utilize personal information, the user may be provided with an opportunity to control whether programs or features gather user information (e.g., information about the user's social network, social behavior or activity, profession, user's preferences, or the user's current geographic location) or whether and/or how content that may be more relevant to the user is received from a content server. Moreover, certain data may be processed in one or more ways before it is stored or used so that personally identifiable information is removed. For example, the identity of the user may be processed such that personally identifiable information of the user cannot be determined, or the geographic location of the user may be generalized (such as to a city, zip code, or state level) where geographic location information is obtained such that a specific geographic location of the user cannot be determined. Thus, the user may control how information about the user is collected and/or used.
In some embodiments, there is provided a method implemented by one or more processors, the method comprising: audio data capturing a spoken utterance of a user is detected at a client device, wherein the client device is in an environment with one or more additional client devices and in local communication with the one or more additional client devices via a local network, the one or more additional client devices including at least a first additional client device. The method further includes processing, at the client device, the audio data using an automatic speech recognition ("ASR") model stored locally at the client device to generate candidate text representations of the spoken utterance. The method further includes, at the client device, receiving a first additional candidate text representation of the spoken utterance from the first additional client device and via the local network, the first additional candidate text representation of the spoken utterance generated locally at the first additional client device being generated based on (a) the audio data and/or (b) locally detected audio data that captures the spoken utterance detected at the first additional client device, wherein the first additional candidate text representation of the spoken utterance is generated by processing the audio data and/or the locally generated audio data using a first additional ASR model stored locally at the first additional client device. The method further includes determining a text representation of the spoken utterance based on the candidate text representation of the spoken utterance and the first additional candidate text representation of the spoken utterance generated by the first additional client device.
These and other implementations of the technology can include one or more of the following features.
In some implementations, the one or more additional client devices include at least the first additional client device and a second additional client device. In some implementations, receiving, at the client device, the first additional candidate text representation from the first additional client device and via the local network further includes: at the client device, receiving, from the second additional client device and via the local network, a second additional candidate text representation of the spoken utterance, the second additional candidate text representation of the spoken utterance locally generated at the second additional client device being generated based on (a) the audio data and/or (b) additional locally detected audio data of the spoken utterance detected at the second additional client device, wherein the second additional candidate text representation of the spoken utterance is generated by processing the audio data and/or the additional locally generated audio data using a second additional ASR model stored locally at the second additional client device. In some implementations, determining the text representation of the spoken utterance based on the candidate text representation of the spoken utterance and the first additional candidate text representation of the spoken utterance generated by the first additional client device further includes: the text representation of the spoken utterance is determined based on the candidate text representation of the spoken utterance, the first additional candidate text representation of the spoken utterance generated by the first additional client device, and the second additional candidate text representation of the spoken utterance generated by the second additional client device.
In some implementations, determining the text representation of the spoken utterance based on the candidate text representation of the spoken utterance and the first additional candidate text representation of the spoken utterance generated by the first additional client device includes: the candidate text representation of the spoken utterance or the first additional candidate text representation of the spoken utterance is randomly selected. In some implementations, the method further includes determining the text representation of the spoken utterance based on the random selection.
In some implementations, determining the text representation of the spoken utterance based on the candidate text representation of the spoken utterance and the first additional candidate text representation of the spoken utterance generated by the first additional client device includes: a confidence score for the candidate text representation is determined, the confidence score indicating a probability that the candidate text representation is the text representation, wherein the confidence score is based on one or more device parameters of the client device. In some implementations, the method further includes determining an additional confidence score for the additional candidate text representation, the additional confidence score indicating an additional probability that the additional candidate text representation is the text representation, wherein the additional confidence score is based on one or more additional device parameters of the additional client device. In some embodiments, the method further comprises comparing the confidence score to the additional confidence score. In some implementations, the method further includes determining the text representation of the spoken utterance based on the comparison.
In some implementations, determining the text representation of the spoken utterance based on the candidate text representation of the spoken utterance and the first additional candidate text representation of the spoken utterance generated by the first additional client device includes: an audio quality value is determined that indicates a quality of the audio data capturing the spoken utterance detected at the client device. In some implementations, the method further includes determining an additional audio quality value that indicates a quality of the additional audio data capturing the spoken utterance detected at the first additional client device. In some implementations, the method further includes comparing the audio quality value with the additional audio quality value. In some implementations, the method further includes determining the text representation of the spoken utterance based on the comparison.
In some implementations, determining the text representation of the spoken utterance based on the candidate text representation of the spoken utterance and the first additional candidate text representation of the spoken utterance generated by the first additional client device includes: an ASR quality value is determined, the ASR quality value being indicative of a quality of the ASR model stored locally at the client device. In some implementations, the method further includes determining an additional ASR quality value that indicates a quality of the additional ASR model stored locally at the additional client device. In some implementations, the method further includes comparing the ASR quality value to the additional ASR quality value. In some implementations, the method further includes determining the text representation of the spoken utterance based on the comparison.
In some implementations, the first additional candidate text representation of the spoken utterance includes a plurality of hypotheses, and wherein determining the text representation of the spoken utterance based on the candidate text representation of the spoken utterance and the first additional candidate text representation of the spoken utterance generated by the first additional client device includes: the plurality of hypotheses are reordered using the client device. In some implementations, the method further includes determining the text representation of the spoken utterance based on the candidate text representation of the spoken utterance and the reordered plurality of hypotheses.
In some implementations, prior to receiving, at the client device, the first additional candidate text representation of the spoken utterance from the first additional client device via the local network, and further comprising (a) the audio data and/or (b) capturing the locally detected audio data of the spoken utterance detected at the first additional client device, determining whether to locally generate the first additional candidate representation of the spoken utterance at the first additional client device, wherein determining whether to locally generate the first additional candidate representation of the spoken utterance at the first additional client device based on (a) the audio data and/or (b) capturing the locally detected audio data of the spoken utterance detected at the first additional client device comprises: an audio quality value is determined that indicates a quality of the audio data capturing the spoken utterance detected at the client device. In some implementations, the method further includes determining an additional audio quality value that indicates a quality of the locally detected audio data capturing the spoken utterance detected at the first additional client device. In some implementations, the method further includes comparing the audio quality value with the additional audio quality value. In some implementations, the method further includes determining whether to locally generate the first additional candidate representation of the spoken utterance at the first additional client device based on (a) the audio data and/or (b) the locally detected audio data that captures the spoken utterance detected at the first additional client device based on the comparison. In some versions of those implementations, determining the audio quality value indicative of a quality of the audio data capturing the spoken utterance detected at the client device includes identifying one or more microphones of the client device. In some versions of those implementations, the method further includes determining the audio quality value based on the one or more microphones of the client device. In some versions of those implementations, determining the additional audio quality value indicative of a quality of the locally detected audio data that captures the spoken utterance detected at the first additional client device includes identifying one or more first additional microphones of the first additional client device. In some versions of those implementations, the method further includes determining the additional audio quality value based on the one or more first additional microphones of the first additional client device. In some versions of those implementations, determining the audio quality value indicative of a quality of capturing the audio data of the spoken utterance detected at the client device includes generating a signal-to-noise value based on processing the audio data capturing the spoken utterance. In some versions of those implementations, the method further includes determining the audio quality value based on the signal-to-noise ratio value. In some versions of those implementations, determining the additional audio quality value indicative of a quality of the locally detected audio data that captured the spoken utterance detected at the first additional client device includes generating an additional signal-to-noise value based on processing the audio data that captured the spoken utterance. In some versions of those implementations, the method further includes determining the additional audio quality value based on the additional signal-to-noise value.
In some implementations, before receiving, at the client device, a first additional candidate text representation of the spoken utterance from the first additional client device via the local network, the method further includes determining whether to send a request to the first additional client device for the first additional candidate text representation of the spoken utterance. In some implementations, in response to determining to send the request for the first additional candidate text representation of the spoken utterance to the first additional client device, the method further includes sending the request for the first additional candidate text representation of the spoken utterance to the first additional client device. In some versions of those implementations, determining whether to send the request for the first additional candidate text representation of the spoken utterance to the first additional client device includes determining a hotword confidence score based on processing at least a portion of the audio data capturing the spoken utterance of the user using a hotword model, wherein the hotword confidence score indicates a probability of whether at least the portion of the audio data includes a hotword. In some versions of those embodiments, the method further comprises determining whether the hotword confidence score meets one or more conditions, wherein determining whether the hotword confidence score meets the one or more conditions comprises determining whether the hotword confidence score meets a threshold. In some versions of those implementations, in response to determining that the hotword confidence score meets a threshold, the method further includes determining whether the hotword confidence score indicates a weak probability that at least the portion of the audio data includes the hotword. In some versions of those implementations, in response to determining that the hotword confidence score indicates the weak probability that the at least part of the audio data includes the hotword, the method further includes determining to send the request for the first additional candidate text representation of the spoken utterance to the first additional client device.
Additionally, some implementations include one or more processors (e.g., a Central Processing Unit (CPU), a Graphics Processing Unit (GPU), and/or a Tensor Processing Unit (TPU)) of one or more computing devices, wherein the one or more processors are operable to execute instructions stored in an associated memory, and wherein the instructions are configured to cause performance of any of the methods described herein. Some implementations also include one or more transitory or non-transitory computer-readable storage media storing computer instructions executable by the one or more processors to perform any of the methods described herein.
Claims (14)
1. A method implemented by one or more processors, the method comprising:
detecting, at a client device, audio data for capturing a spoken utterance of a user, wherein the client device is in an environment with one or more additional client devices and in local communication with the one or more additional client devices via a local network, the one or more additional client devices including at least a first additional client device;
processing, at the client device, the audio data using an automatic speech recognition "ASR" model stored locally at the client device to generate candidate text representations of the spoken utterance;
Receiving, at the client device, a first additional candidate text representation of the spoken utterance from the first additional client device and via the local network, the first additional candidate text representation of the spoken utterance generated locally at the first additional client device being generated based on (a) the audio data and/or (b) locally detected audio data for capturing the spoken utterance detected at the first additional client device, wherein the first additional candidate text representation of the spoken utterance is generated by processing the audio data and/or the locally generated audio data using a first additional ASR model locally stored at the first additional client device; and
a text representation of the spoken utterance is determined based on the candidate text representation of the spoken utterance and the first additional candidate text representation of the spoken utterance generated by the first additional client device.
2. The method according to claim 1,
wherein the one or more additional client devices include at least the first additional client device and a second additional client device;
Wherein receiving, at the client device, the first additional candidate text representation from the first additional client device and via the local network further comprises:
receiving, at the client device, a second additional candidate text representation of the spoken utterance from the second additional client device and via the local network, the second additional candidate text representation of the spoken utterance generated locally at the second additional client device being based on (a) the audio data and/or (b) additional locally detected audio data for capturing the spoken utterance detected at the second additional client device, wherein the second additional candidate text representation of the spoken utterance is generated by processing the audio data and/or the additional locally generated audio data using a second additional ASR model stored locally at the second additional client device; and
wherein determining the text representation of the spoken utterance based on the candidate text representation of the spoken utterance and the first additional candidate text representation of the spoken utterance generated by the first additional client device further comprises:
The text representation of the spoken utterance is determined based on the candidate text representation of the spoken utterance, the first additional candidate text representation of the spoken utterance generated by the first additional client device, and the second additional candidate text representation of the spoken utterance generated by the second additional client device.
3. The method of claim 1, wherein determining the text representation of the spoken utterance based on the candidate text representation of the spoken utterance and the first additional candidate text representation of the spoken utterance generated by the first additional client device comprises:
randomly selecting the candidate text representation of the spoken utterance or the first additional candidate text representation of the spoken utterance; and
the text representation of the spoken utterance is determined based on the random selection.
4. The method of claim 1, wherein determining the text representation of the spoken utterance based on the candidate text representation of the spoken utterance and the first additional candidate text representation of the spoken utterance generated by the first additional client device comprises:
Determining a confidence score for the candidate text representation, the confidence score indicating a probability that the candidate text representation is the text representation, wherein the confidence score is based on one or more device parameters of the client device;
determining an additional confidence score for the additional candidate text representation, the additional confidence score indicating an additional probability that the additional candidate text representation is the text representation, wherein the additional confidence score is based on one or more additional device parameters of the additional client device;
comparing the confidence score to the additional confidence score; and
the text representation of the spoken utterance is determined based on the comparison.
5. The method of claim 1 or claim 4, wherein determining the text representation of the spoken utterance based on the candidate text representation of the spoken utterance and the first additional candidate text representation of the spoken utterance generated by the first additional client device comprises:
determining an audio quality value indicative of a quality of the audio data used to capture the spoken utterance detected at the client device;
Determining an additional audio quality value indicative of a quality of the additional audio data used to capture the spoken utterance detected at the first additional client device;
comparing the audio quality value with the additional audio quality value; and
the text representation of the spoken utterance is determined based on the comparison.
6. The method of claim 1, claim 4, or claim 5, wherein determining the text representation of the spoken utterance based on the candidate text representation of the spoken utterance and the first additional candidate text representation of the spoken utterance generated by the first additional client device comprises:
determining an ASR quality value indicative of a quality of the ASR model stored locally at the client device;
determining an additional ASR quality value indicative of a quality of an additional ASR model stored locally at the additional client device;
comparing the ASR quality value to the additional ASR quality value; and
the text representation of the spoken utterance is determined based on the comparison.
7. The method of any of the preceding claims, wherein the first additional candidate text representation of the spoken utterance comprises a plurality of hypotheses, and wherein determining the text representation of the spoken utterance based on the candidate text representation of the spoken utterance and the first additional candidate text representation of the spoken utterance generated by the first additional client device comprises:
Reordering the plurality of hypotheses using the client device; and
the text representation of the spoken utterance is determined based on the candidate text representation of the spoken utterance and a plurality of reordered hypotheses.
8. The method of any of the preceding claims, further comprising: prior to receiving, at the client device, the first additional candidate text representation of the spoken utterance from the first additional client device via the local network:
determining whether to locally generate the first additional candidate representation of the spoken utterance at the first additional client device based on (a) the audio data and/or (b) the locally detected audio data for capturing the spoken utterance detected at the first additional client device, wherein determining whether to locally generate the first additional candidate representation of the spoken utterance at the first additional client device based on (a) the audio data and/or (b) the locally detected audio data for capturing the spoken utterance detected at the first additional client device comprises:
determining an audio quality value indicative of a quality of the audio data used to capture the spoken utterance detected at the client device;
Determining an additional audio quality value for indicating a quality of the locally detected audio data capturing the spoken utterance detected at the first additional client device;
comparing the audio quality value with the additional audio quality value; and
based on the comparison, determining whether to locally generate the first additional candidate representation of the spoken utterance at the first additional client device based on (a) the audio data and/or (b) the locally detected audio data for capturing the spoken utterance detected at the first additional client device.
9. The method of claim 8, wherein determining the audio quality value indicative of a quality of the audio data used to capture the spoken utterance detected at the client device comprises:
one or more microphones identifying the client device; and
determining the audio quality value based on the one or more microphones of the client device; and
wherein determining the additional audio quality value indicative of a quality of the locally detected audio data for capturing the spoken utterance detected at the first additional client device comprises:
Identifying one or more first additional microphones of the first additional client device; and
the additional audio quality value is determined based on the one or more first additional microphones of the first additional client device.
10. The method of claim 8 or claim 9, wherein determining the audio quality value indicative of a quality of the audio data used to capture the spoken utterance detected at the client device comprises:
generating a signal-to-noise value based on processing the audio data for capturing the spoken utterance; and
determining the audio quality value based on the signal-to-noise ratio value; and
wherein determining the additional audio quality value indicative of a quality of the locally detected audio data for capturing the spoken utterance detected at the first additional client device comprises:
generating additional signal-to-noise values based on processing the audio data for capturing the spoken utterance; and
the additional audio quality value is determined based on the additional signal-to-noise value.
11. The method of any of the preceding claims, further comprising:
determining, prior to receiving, at the client device, a first additional candidate text representation of the spoken utterance from the first additional client device via the local network, whether to send a request to the first additional client device for the first additional candidate text representation of the spoken utterance;
In response to determining to send the request for the first additional candidate text representation of the spoken utterance to the first additional client device, the request for the first additional candidate text representation of the spoken utterance is sent to the first additional client device.
12. The method of claim 11, wherein determining whether to send the request for the first additional candidate text representation of the spoken utterance to the first additional client device comprises:
determining a hotword confidence score based on processing at least a portion of the audio data for capturing the spoken utterance of the user using a hotword model, wherein the hotword confidence score indicates a probability of whether at least a portion of the audio data includes a hotword;
determining whether the hotword confidence score meets one or more conditions, wherein determining whether the hotword confidence score meets the one or more conditions comprises determining whether the hotword confidence score meets a threshold;
in response to determining that the hotword confidence score meets a threshold, determining whether the hotword confidence score indicates a weak probability that the at least part of the audio data includes the hotword; and
Responsive to determining that the hotword confidence score indicates the weak probability that the at least part of the audio data includes the hotword, determining to send the request for the first additional candidate text representation of the spoken utterance to the first additional client device.
13. A non-transitory computer-readable medium configured to store instructions that, when executed by one or more processors, cause the one or more processors to perform operations comprising:
detecting, at a client device, audio data for capturing a spoken utterance of a user, wherein the client device is in an environment with one or more additional client devices and in local communication with the one or more additional client devices via a local network, the one or more additional client devices including at least a first additional client device;
processing, at the client device, the audio data using an automatic speech recognition "ASR" model stored locally at the client device to generate candidate text representations of the spoken utterance;
receiving, at the client device, a first additional candidate text representation of the spoken utterance from the first additional client device and via the local network, the first additional candidate text representation of the spoken utterance generated locally at the first additional client device being generated based on (a) the audio data and/or (b) locally detected audio data for capturing the spoken utterance detected at the first additional client device, wherein the first additional candidate text representation of the spoken utterance is generated by processing the audio data and/or the locally generated audio data using a first additional ASR model locally stored at the first additional client device; and
A text representation of the spoken utterance is determined based on the candidate text representation of the spoken utterance and the first additional candidate text representation of the spoken utterance generated by the first additional client device.
14. A system, comprising:
one or more processors; and
a memory configured to store instructions that, when executed by one or more processors, cause the one or more processors to perform operations comprising:
detecting, at a client device, audio data for capturing a spoken utterance of a user, wherein the client device is in an environment with one or more additional client devices and in local communication with the one or more additional client devices via a local network, the one or more additional client devices including at least a first additional client device;
processing, at the client device, the audio data using an automatic speech recognition "ASR" model stored locally at the client device to generate candidate text representations of the spoken utterance;
receiving, at the client device, a first additional candidate text representation of the spoken utterance from the first additional client device and via the local network, the first additional candidate text representation of the spoken utterance generated locally at the first additional client device being generated based on (a) the audio data and/or (b) locally detected audio data for capturing the spoken utterance detected at the first additional client device, wherein the first additional candidate text representation of the spoken utterance is generated by processing the audio data and/or the locally generated audio data using a first additional ASR model locally stored at the first additional client device; and
A text representation of the spoken utterance is determined based on the candidate text representation of the spoken utterance and the first additional candidate text representation of the spoken utterance generated by the first additional client device.
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US17/198,679 | 2021-03-11 | ||
US17/198,679 US20220293109A1 (en) | 2021-03-11 | 2021-03-11 | Device arbitration for local execution of automatic speech recognition |
PCT/US2021/063370 WO2022191892A1 (en) | 2021-03-11 | 2021-12-14 | Device arbitration for local execution of automatic speech recognition |
Publications (1)
Publication Number | Publication Date |
---|---|
CN116762126A true CN116762126A (en) | 2023-09-15 |
Family
ID=79927589
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
CN202180088457.0A Pending CN116762126A (en) | 2021-03-11 | 2021-12-14 | Locally performed device arbitration for automatic speech recognition |
Country Status (6)
Country | Link |
---|---|
US (1) | US20220293109A1 (en) |
EP (1) | EP4139918A1 (en) |
JP (1) | JP2024505788A (en) |
KR (1) | KR20230153450A (en) |
CN (1) | CN116762126A (en) |
WO (1) | WO2022191892A1 (en) |
Family Cites Families (18)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US8010369B2 (en) * | 2007-10-30 | 2011-08-30 | At&T Intellectual Property I, L.P. | System and method for controlling devices that are connected to a network |
US20110184740A1 (en) * | 2010-01-26 | 2011-07-28 | Google Inc. | Integration of Embedded and Network Speech Recognizers |
US9674328B2 (en) * | 2011-02-22 | 2017-06-06 | Speak With Me, Inc. | Hybridized client-server speech recognition |
JP6198432B2 (en) * | 2013-04-09 | 2017-09-20 | 小島プレス工業株式会社 | Voice recognition control device |
US20150031416A1 (en) * | 2013-07-23 | 2015-01-29 | Motorola Mobility Llc | Method and Device For Command Phrase Validation |
US9443527B1 (en) * | 2013-09-27 | 2016-09-13 | Amazon Technologies, Inc. | Speech recognition capability generation and control |
EP3958255A1 (en) * | 2015-01-16 | 2022-02-23 | Samsung Electronics Co., Ltd. | Method and device for performing voice recognition |
KR102387567B1 (en) * | 2015-01-19 | 2022-04-18 | 삼성전자주식회사 | Method and apparatus for speech recognition |
US10559309B2 (en) * | 2016-12-22 | 2020-02-11 | Google Llc | Collaborative voice controlled devices |
CN109523991B (en) * | 2017-09-15 | 2023-08-18 | 阿里巴巴集团控股有限公司 | Voice recognition method, device and equipment |
KR102471493B1 (en) * | 2017-10-17 | 2022-11-29 | 삼성전자주식회사 | Electronic apparatus and method for voice recognition |
US10616726B1 (en) * | 2018-02-22 | 2020-04-07 | Amazon Technologies, Inc. | Outputing notifications using device groups |
US11514917B2 (en) * | 2018-08-27 | 2022-11-29 | Samsung Electronics Co., Ltd. | Method, device, and system of selectively using multiple voice data receiving devices for intelligent service |
US11017778B1 (en) * | 2018-12-04 | 2021-05-25 | Sorenson Ip Holdings, Llc | Switching between speech recognition systems |
US11580969B2 (en) * | 2019-03-27 | 2023-02-14 | Lg Electronics Inc. | Artificial intelligence device and method of operating artificial intelligence device |
US20190318742A1 (en) * | 2019-06-26 | 2019-10-17 | Intel Corporation | Collaborative automatic speech recognition |
US11138969B2 (en) * | 2019-07-31 | 2021-10-05 | Sonos, Inc. | Locally distributed keyword detection |
CN111768783B (en) * | 2020-06-30 | 2024-04-02 | 北京百度网讯科技有限公司 | Voice interaction control method, device, electronic equipment, storage medium and system |
-
2021
- 2021-03-11 US US17/198,679 patent/US20220293109A1/en active Pending
- 2021-12-14 JP JP2023536526A patent/JP2024505788A/en active Pending
- 2021-12-14 KR KR1020237033835A patent/KR20230153450A/en unknown
- 2021-12-14 CN CN202180088457.0A patent/CN116762126A/en active Pending
- 2021-12-14 EP EP21847810.5A patent/EP4139918A1/en active Pending
- 2021-12-14 WO PCT/US2021/063370 patent/WO2022191892A1/en active Application Filing
Also Published As
Publication number | Publication date |
---|---|
WO2022191892A1 (en) | 2022-09-15 |
JP2024505788A (en) | 2024-02-08 |
US20220293109A1 (en) | 2022-09-15 |
EP4139918A1 (en) | 2023-03-01 |
KR20230153450A (en) | 2023-11-06 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
JP7386878B2 (en) | Dynamically adapting assistant responses | |
CN112262430A (en) | Automatically determining language for speech recognition of a spoken utterance received via an automated assistant interface | |
US11545157B2 (en) | Speaker diartzation using an end-to-end model | |
US11922951B2 (en) | Targeted voice separation by speaker conditioned on spectrogram masking | |
US11854533B2 (en) | Speaker awareness using speaker dependent speech model(s) | |
JP2024019405A (en) | 2-pass end-to-end speech recognition | |
US20220284049A1 (en) | Natural language understanding clarifications | |
US11817106B2 (en) | Selectively storing, with multiple user accounts and/or to a shared assistant device: speech recognition biasing, NLU biasing, and/or other data | |
EP3939033B1 (en) | Automated assistant control of external applications lacking automated assistant application programming interface functionality | |
US20230237312A1 (en) | Reinforcement learning techniques for selecting a software policy network and autonomously controlling a corresponding software client based on selected policy network | |
US20220293109A1 (en) | Device arbitration for local execution of automatic speech recognition | |
CN113767379B (en) | Rendering content using content agents and/or stored content parameters | |
US20230317082A1 (en) | Generating and/or utilizing unintentional memorization measure(s) for automatic speech recognition model(s) |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
PB01 | Publication | ||
PB01 | Publication | ||
SE01 | Entry into force of request for substantive examination | ||
SE01 | Entry into force of request for substantive examination |