US8131543B1 - Speech detection - Google Patents
Speech detection Download PDFInfo
- Publication number
- US8131543B1 US8131543B1 US12/102,611 US10261108A US8131543B1 US 8131543 B1 US8131543 B1 US 8131543B1 US 10261108 A US10261108 A US 10261108A US 8131543 B1 US8131543 B1 US 8131543B1
- Authority
- US
- United States
- Prior art keywords
- energy
- speech
- noise
- dependent
- model
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- G—PHYSICS
- G10—MUSICAL INSTRUMENTS; ACOUSTICS
- G10L—SPEECH ANALYSIS OR SYNTHESIS; SPEECH RECOGNITION; SPEECH OR VOICE PROCESSING; SPEECH OR AUDIO CODING OR DECODING
- G10L25/00—Speech or voice analysis techniques not restricted to a single one of groups G10L15/00 - G10L21/00
- G10L25/78—Detection of presence or absence of voice signals
Definitions
- This instant specification relates to speech or noise detection.
- Some speech recognition systems attempt to classify portions of an audio signal as speech. These systems may then selective transmit the portions that appear to be speech to a speech decoder for further processing. Speech recognition systems may attempt to classify the portions of an audio signal based on an amplitude of the signal. For example, the systems may classify a portion of an audio signal as speech if the portion has a high amplitude.
- Classification schemes may operate on an assumption that speech is more likely to have a higher amplitude than that of noise. However, loud background noises or significant interference of the audio signal caused by a device in the transmission chain may generate noise with a high amplitude. In these cases, a classification scheme that relies upon signal amplitude may misclassify frames that contain noise as containing speech.
- this document describes systems and methods for determining whether a portion of a signal represents speech or noise using both gain-dependent and gain-independent features of the portion to make the determination.
- a computer-implemented method includes receiving an audio signal, determining an energy-invariant component of a portion of the audio signal associated with a spectral shape of the portion, and determining an energy-variant component of the portion associated with a gain level of the portion.
- the method also comprises comparing the energy-invariant and energy-variant components to a speech model, comparing the energy-invariant and energy-variant components to a noise model, and outputting an indication whether the portion of the audio signal more closely corresponds to the speech model or to the noise model based on the comparisons.
- a system in another general aspect, includes a signal feature calculator to determine energy-variant and energy-invariant Mel-frequency cepstral coefficients (MFCC) components associated with a portion of a received audio signal, means for classifying the portion of the audio signal as speech or noise based on a comparison of the determined energy-variant and energy-invariant MFCC components to a speech model and a noise model, and an interface to output an indication of whether the portion of the audio signal is classified as speech or noise.
- MFCC Mel-frequency cepstral coefficients
- a system can provide speech detection that uses both gain-invariant and gain-dependent features of an audio signal in classifying the signal as noise or speech.
- the system may rely almost exclusively on the gain-invariant features before estimates for the background noise and speech levels are determined with a specified confidence.
- use of a bi-variate dynamic distribution may result in more accurate classification of a signal portion as including speech or noise by enforcing restrictions on individual levels (i.e., the speech and noise levels) as well as simultaneously restricting relative levels between the two (i.e., a signal-to-noise ratio (SNR)).
- SNR signal-to-noise ratio
- FIG. 1 is a diagram of an example system for determining whether a received audio signal should be classified as noise or speech.
- FIG. 2 is a diagram of an example system for identifying portions of an audio signal that include speech (or noise) using Mel-frequency cepstral coefficients (MFCC) components of the audio signal.
- MFCC Mel-frequency cepstral coefficients
- FIG. 3 is a flowchart showing an example method of determining whether a frame includes speech or noise.
- FIGS. 4A , 4 B, and 4 C show an example model used to classify a signal portion as speech or noise and example Gaussian components of the model, respectively.
- FIGS. 5A and 5B show graphs of two example Gaussian distributions and an example of how gain components of a model are estimated, respectively.
- FIG. 6 shows a diagram of an example of gain parameter propagation in a speech/noise model.
- FIG. 7 is a graph of an example SNR prior distribution.
- FIGS. 8A-G are examples of speech endpointing using a switching dynamic noise adaptation (DySANA) model.
- DySANA switching dynamic noise adaptation
- FIG. 9 is an example of a general computing system.
- FIG. 10 is a diagram of an exemplary full dynamic distribution composed of a random walk component and a signal-to-noise ratio prior component.
- a system for classifying the audio includes statistical models of speech and noise.
- the models can incorporate, for example, Mel-frequency cepstral coefficients (MFCCs), which represent both signal level, or gain dependent, features of an audio signal and features that are independent of gain, such as features that are relevant to a signal's spectral shape.
- MFCCs Mel-frequency cepstral coefficients
- an audio signal is sampled so that it is represented as a sequence of digital audio frames, and the system can processes each frame sequentially. For example, the system can calculate MFCCs for each frame and classify the frame as including speech or noise based on probabilities generated by comparing the frame to the speech and noise models.
- the models include a current estimate of the speech and/or noise gain level:
- P ⁇ ( frame ⁇ ⁇ contains ⁇ ⁇ speech ) P ⁇ ( speech ⁇ current ⁇ ⁇ frame , speech ⁇ ⁇ level ⁇ ⁇ estimate ) ( P ⁇ ( speech ⁇ current ⁇ ⁇ frame , speech ⁇ ⁇ level ⁇ ⁇ estimate ) + P ⁇ ( speech ⁇ current ⁇ ⁇ frame , noise ⁇ ⁇ level ⁇ ⁇ estimate ) ) . ( 1 )
- the system classifies the frame as containing speech.
- the system uses the probability to predict speech and noise levels (e.g., gain levels) for a subsequent audio frame.
- the system may predict estimated gain levels using an extended Kalman filter, where the system's confidence in speech and noise level estimates may be expressed as a probability distribution over the estimated gain levels.
- large variances over the distribution may imply that a confidence in the estimates is low.
- gain level estimates may be constrained so that the estimates are consistent with prior knowledge of an expected signal-to-noise ratio (SNR) of a received signal.
- SNR signal-to-noise ratio
- the system can include constraints that specify that a signal including speech will have a gain level higher than noise and that the SNR will fall within a range such as 5 dB ⁇ SNR ⁇ 25 dB.
- some implementations may also include restraints on the individual gain levels associated with the noise and speech signals.
- the system may constrain the speech model so that it restricts speech levels to a range within “soft” boundaries based on a dynamic distribution (e.g., Gaussian distribution).
- the statistical speech and noise models may include Gaussian mixture models having a diagonal covariance.
- each feature dimension e.g., each MFCC component
- each MFCC component may be modeled separately as a Gaussian random variable.
- the majority of the signal's MFCC components are independent of gain level estimates. This may imply that the previously described confidence in predicted gain levels used to determine whether the signal is speech or noise primarily affects the system's ability to distinguish speech from noise using gain-dependent features of the signal.
- the system's ability to make this determination using gain-independent MFCC components may be substantially unaffected.
- the system may rely less on the gain-dependent features when the previously described confidence level is low (e.g., before the system has received enough frames to accurately classify subsequent frames as noise or speech based on a frame's gain level). Instead, the system may rely more heavily on the gain-independent features to categorize the signal as noise or speech.
- This may permit the system to use the gain-independent, or level-invariant, features to make an accurate speech or noise classification even when there is a severe mismatch between a gain level of a prior model and a current observed gain level for a signal.
- the above described example implementation may avoid incorrectly classifying a frame of a signal as speech or noise due to a gain level of a previously received frame of the signal (e.g., a very loud noise may be incorrectly classified as speech solely due to the fact that the observed signal level is closer to that of the speech model than that of the noise model).
- gain and energy level are used interchangeable. Additionally, the terms independent and invariant (and dependent and variant) are used interchangeable.
- FIG. 1 is a diagram of an example system 100 for determining whether a received audio signal 102 should be classified as noise or speech.
- the system 100 may include a speech recognition system 104 and an audio device 106 such as a cell phone for transmitting the audio signal 102 .
- the speech recognition system 104 may include a speech detector 108 that detects whether portions of the received audio include speech or background noise.
- the speech recognition system 104 can forward portions that include speech to a speech decoder 110 , which may translate the audio into a textual representation of the audio.
- the speech decoder 110 is illustrated as part of the speech recognition system 104 ; however, in other implementations, the speech decoder 110 can be implemented on another server or multiple servers.
- an arrow labeled “ 1 ” indicates a transmission of the audio signal 102 from the audio device 106 to the speech recognition system 104 .
- the speech detector 108 'associated with the speech recognition system 104 can include a signal feature calculator 112 that extracts, or derives, characteristic features of the audio signal 102 .
- the speech detector 108 can break the received audio signal 102 into digital frames or signal portions.
- the signal feature calculator 112 can determine both gain-dependent 114 and gain-independent 116 characteristics for the frames. For example, some features of the signal with the frames may vary in gain depending an energy level (e.g., loudness) of audio used to generate the audio signal. Other features may be independent of energy level such as spectral shape features of the audio signal.
- the signal feature calculator 112 can determine gain dependent features such as autocorrelation is based features and gain independent features such as normalized autocorrelation based features. In yet another implementation, the signal feature calculator 112 can determine gain independent perceptual linear prediction (PLP) features in addition to deriving an energy-based component that is gain dependent.
- PLP perceptual linear prediction
- the signal feature calculator 112 can transmit the gain-variant 114 and gain-invariant 116 features of a frame to a classifier 118 that compares the features 114 , 116 with gain-invariant and gain-variant components of a speech model 120 and a noise model 122 .
- the classifier 118 can classify the frame as speech or noise based on which model has features that best match the gain-invariant 116 and gain-variant 114 features of the signal frame.
- the classifier 118 can send an indication 124 of whether the frame includes speech (or noise) to the speech decoder 110 as indicated by an arrow labeled “ 3 .”
- the speech decoder 110 can access and decode digital frames that are associated with speech (as specified by indications receive a classifier 118 ). Digital frames associated with noise can be ignored or discarded.
- the decoded symbolic interpretation 126 of the speech portions of the signal can be transmitted to another system for processing.
- the speech recognition system 104 can transmit the symbolic interpretation 126 to a search engine 128 (as indicated by an arrow labeled “ 4 ”) for use in initiating a search of Internet web pages.
- the search engine 128 can transmit the search results 132 the audio device 106 as indicated by an arrow “ 5 .”
- a user can access a web page using a browser installed on a cell phone 106 .
- the search web page prompts the user to speak a search term or search phrase.
- the cell phone 106 transmits the spoken search as the audio signal 102 to the speech recognition system 104 .
- the speech recognition system 106 determines which portions of the audio signal are speech and decode these portions.
- the speech recognition system 104 transmits the decoded speech portions to the search engine 128 , which initiate a search using the decoded search term and returns the search results 130 for display on the cell phone 106 .
- numbered arrows illustrate an example sequence of steps involved in speech/noise detection, however, the sequence is primarily for use in explanation and is not intended to limit the number or order of steps used to detect speech or noise. For example, so steps shown in FIG. 1 may occur in a different order, such as in parallel. In other implementations, additional steps may be added, replaced, or some steps can be removed. For example, a step illustrated by the arrow labeled “ 4 ” may be modified so that the symbolic representation of the speech within the signal is transmitted directly to the cell phone 106 for display to a user for confirmation.
- FIG. 2 is a diagram of an example system 200 for identifying portions of an audio signal that include speech (or noise) using MFCC components of the audio signal.
- the example system 200 includes a speech detector 202 that receives an audio signal 204 and outputs an indication 206 of whether a frame of the signal includes speech or noise.
- the speech detector 202 includes a digitizing module 208 that can digitize the audio signal 204 .
- the audio signal 204 may be an analog signal.
- the digitizing module 208 can include a signal sampler 210 that samples of the analog signal to generate a digital representation.
- the digitized signal can be divided into digital frames, or portions, of the audio signal that are sent to a signal feature calculator 212 .
- the audio signal 204 is received as a digital signal.
- the digitizing module may be replaced with a module that merely portions the digital signal into discrete frames formatted so that the speech detector can process the frames as subsequently described.
- the signal feature calculator 212 can generate MFCC components based on a received frame.
- the signal feature calculator 212 can include a FFT (Fast Fourier Transform) module that performs a Fourier transform on the received frame.
- the FFT-processed frame can be rectified and squared by a rectifying/squaring module 216 .
- the signal feature calculator 212 can include a Mel scale filter module 218 that may map the amplitudes of a spectrum obtained from previous processing onto a mel scale (i.e., a perceptual scale of pitches that were determined by listeners to be substantially equal in distance from one another) using, for example, triangular overlapping windows. Additionally, the Mel scale filter module 218 may compute the log of the magnitude spectrum or Mel-scale magnitude spectrum.
- a mel scale i.e., a perceptual scale of pitches that were determined by listeners to be substantially equal in distance from one another
- the Mel scale filter module 218 may compute the log of the magnitude spectrum or Mel-scale magnitude spectrum.
- a discrete cosine transform (DCT) module can take the DCT of the resulting mel log-amplitudes as if they represented a signal according to some implementations.
- the resulting output can include, for example, MFCC components C 0 -C 22 .
- the indices 0 - 22 are used to label the components; however, the actual component indices can vary.
- the gain invariant component, C 0 is a (scaled) sum of the component magnitudes or magnitude spectrum.
- a portion of the MFCC components can be discarded.
- the signal feature calculator 212 can transit C 0 -C 12 and discard the components C 13 -C 22 .
- the number of component (e.g., in the previous implementation) used in the analysis may vary depending on a number of Mel filters. Consequently, other implementations may use varying numbers of components.
- the use of 13 components in the following description is for illustrative purposes only and is not meant to be limiting in any way.
- the maximum index (e.g., 22 in the previous implementation) may vary depending on the FFT length and the number of filters in a Mel filterbank.
- Whether a component is gain invariant or not may depend on the components of the linear transform.
- the linear transform is a DCT which separates the components into completely gain invariant and gain variant components. If a different Linear transform is used, the system may generate components having various degrees of gain variance.
- signal feature calculator 212 transmits the MFCC components to a classifier 222 for use in determining whether the frame associated with the components should be classified as noise or speech.
- the classifier 222 can include comparing the MFCC components to models that include distributions of MFCC component values that are typically associated with speech or noise.
- the classifier can include or access a Gaussian mixture model for speech 224 and a Gaussian mixture model for noise 226 .
- each Gaussian mixture model includes one or more distributions associated with each MFCC component.
- the speech and noise models can each include thirteen Gaussian distributions—one for each of the C 0 through C 12 components.
- the classifier 222 can use a speech/noise probability (SNP) calculator to determine the probabilities that a frame is associated with noise, speech, or both.
- SNP speech/noise probability
- the SNP calculator 228 can compare the MFCC components to the corresponding Gaussian distributions.
- the SNP calculator 228 can classify the frame associated with the MFCCs as speech.
- the SNP calculator executes more complicated determinations of probability (e.g., different Gaussian distributions can be weighted more heavily than others, correspondence of a MFCC component to the mean of a distribution is weighted more heavily for some distributions, etc.).
- the models 224 , 226 are generated using a hybrid of an extended Kalman filter and a hidden marker model (HMM) that operates as a dynamic Bayesian network as illustrated in FIG. 4A .
- HMM hidden marker model
- These nodes may be considered the hidden variables in the HMM.
- s can specify a particular distribution.
- y t represents an observation at time t.
- o t may represent a selection of which model best explains the observation, e.g.,
- o t ⁇ 1 , if ⁇ ⁇ speech ⁇ ⁇ dominates ⁇ ⁇ ( i . e . , speech ⁇ ⁇ occludes ⁇ ⁇ background ⁇ ⁇ noise ) 0 , if ⁇ ⁇ noise ⁇ ⁇ dominates .
- values can be selected for the o, g, and s values to generate observation y (i.e., a vector of MFCC values).
- observation y can be derived from the received MFCC components and a particular s can be selected.
- the SNP calculator 228 can use previously generated model to derive g and o.
- the occlusion variable o indicates whether the received observation fits better with the speech or the noise model. The previous values of g and o also influence the new derived values for g and o as will be subsequently described.
- a gain estimator 230 can estimate a global gain across all mixture components for each model.
- the gain estimator 230 can enforce temporal constraints.
- the dynamic distribution of the gain estimator can 1) prevent the gain estimates from changing too rapidly, 2) enforce that the gain values separately lie in reasonable predetermined ranges and 3) to enforce that the relative values of the speech and noise gains lie in a reasonable pre-determined range.
- the gain estimator 230 may enforce a speech-to-noise ratio (SNR) 232 and a speech/noise range 234 .
- SNR speech-to-noise ratio
- the occlusion dynamic distribution, or occlusion transition matrix prevents switching between speech and noise too rapidly.
- the gain estimator 230 can estimate gain estimates 236 used update the models 224 , 226 based on weights derived from gain invariant components 238 of a signal portion within a frame as indicated by arrows in FIG. 2 and as more fully described below.
- the joint distribution of parameters for a single time step can be factored as: P ( g t ,s t ,y t ,o t
- y 0:t ⁇ 1 ) P ( y t
- P(s t ) is the gaussian mixture prior for the component of the speech or noise model
- y 0:t ⁇ 1 ) is the conditional prior for the occlusion variable described below, and the observation likelihood
- an additive observation model may be used instead of the occlusion observation model in equation 4.
- y 0:t ⁇ 1 ) may be multinomial distributions.
- y 0:t )>T, where T is a predetermined threshold, the observation may be labeled as containing speech. If P(o t 1
- conditional prior of o t may be calculated as
- y 0:t ⁇ 1 ) and P(o t 0
- z x ⁇ ( y t ) N ⁇ ( y t ; ⁇ x , s x t + [ 0 1 ] ⁇ ⁇ g x t , [ ⁇ x , s x t , 1 : D ⁇ 0 0 ⁇ g x t + ⁇ x , s x t , 0 ] ) .
- parameters of the covariance matrix of Gaussian mixture components are comprised of gain dependent components and gain independent components.
- the variance parameter ⁇ g x t + ⁇ x,s x t ,0 is the gain dependent component and ⁇ x,s x t ,1:D is the gain independent component.
- the relative influence of the gain dependent and gain independent components of the to model may be determined by the conditional prior gain variance ⁇ g x t . If the prior gain variance is large in relation to the other covariance components, then the influence of the gain dependent component of the model may be small in determining the fit of the model to the observation y t .
- the covariance matrix may be diagonal.
- Equation 11 can be rewritten as
- the gain estimator 230 updates the conditional prior distributions on the dynamic parameters between frames.
- the gain estimator 230 can determine the occlusion condition prior for frame t+1 by multiplying the posterior distribution by the occlusion transitional matrix:
- the gain estimator 230 can determine the conditional gain priors using:
- g t ), (which may describe how the gain for each model evolves) is parameterized as: P(g t+1
- This parameterization may compactly specify the dynamic behavior of the gain estimates, for example, generated by the gain estimator 230 .
- it is a product of two factors, i.e. the random walk factor N(g t+1 ; g t ; ⁇ RW ) which constrains how much the gains can change between time steps, and the SNR prior factor N(g t+1 ; ⁇ SNR , ⁇ SNR ) which is shown in FIG. 7 .
- FIG. 10 is a diagram of an exemplary dynamic distribution that can be composed of a random walk component and an SNR prior component.
- ⁇ SNR is a full covariance matrix that has the dual role of constraining the range of both the speech and noise gain, and constraining the relative values that speech and noise gains.
- This factor can be referred to as the Signal to Noise Ratio (SNR) prior.
- SNR Signal to Noise Ratio
- An affect of the SNR prior is that the model will adjust the speech gain, even if only noise is observed, e.g. the speech gain will be increased if the noise gain is increased. This may improve performance since it captures the Lombard effect which is the tendency of a human speaker to increase his or her vocal intensity in the presence of noise.
- the Minimum Mean Squared Error (MMSE) estimate may be used in computing the conditional prior P(g t+1
- a likelihood term in equation 17 is a mixture of Gaussians, so the full conditional prior of g t+1 has a distribution with
- the mixture of Gaussians may be approximated with a single Gaussian on the most probable mode of full distribution. For example, equation 5 given above may be used to approximate the mixture of Gaussians.
- the speech component ⁇ lp,x of ⁇ lp is a weighted sum of the conditional gain prior ⁇ g t and an error term based on the observation (y 0 t ⁇ x, ⁇ x t ,0 ).
- the observation y t may not be present in the update of the noise component ⁇ lp,n of ⁇ lp .
- the influence of a speech observation on the noise gain will come through the SNR prior when the SNR dynamic distribution is taken into account.
- the an error term based on the observation (y 0 t ⁇ x, ⁇ x t ,0 ) may be present in update of all parameters.
- the relative weight given to the error term may depend on likelihood covariance matrix.
- the update of ⁇ lp and ⁇ lp will be a weighted sum of components, where the weights are proportional to the model fit of each component.
- the influence of the SNR dynamic distribution may be taken into account next
- the propagated mean ⁇ g t+1 is a weighted sum of the conditional prior gain from the last observation (i.e., ⁇ g t ), the SNR prior gain (i.e., ⁇ SNR ), and the gain estimate based on the observation (i.e., y 0 t ⁇ x, ⁇ x t ,0 ). Because in some implementations observing speech may give no new information about the instantaneous noise gain, ⁇ lp and ⁇ lp reduce the prior values for the noise gain. This may cause the speech gain to drift towards the prior ⁇ g x during a long sequence of noise observations.
- the variance ratio, W can control how strongly the prior mean attracts the prior (see 22).
- ⁇ SNR is a full matrix
- W is a full matrix
- ⁇ lp,x will contain the term (y 0 t ⁇ x, ⁇ x t ,0 ) from the observation, but ⁇ lp,n may not. This allows the observation to influence the gain for the noise model ⁇ g t+1 ,n , even when the noise is not observed.
- FIG. 4A and FIGS. 8A-G show examples of the adaptation in action.
- output by the speech model is compared to speech posteriors without adaptation, as well as the output when the gain adaptations or transition constraints of the model are not used.
- FIG. 3 is a flowchart showing an example method of determining whether a frame includes speech or noise.
- the example method may be performed, for example, by the systems 100 or 200 and for clarity of presentation, the description that follows uses these systems as the basis for an example. However, another system, or combination of systems, may be used to perform the method 300 .
- a signal is received.
- a cell phone can transmit the audio signal 106 to a speech recognition system 104 , which receives the audio signal.
- the signal 106 is digitized (e.g., using an analog-to-digital converter) if it is received as an analog signal.
- the digital signal may be divided into multiple frames for processing by the each detector 108 within the speech recognition system 104 .
- a determination may be made whether unprocessed frames exist. For example, the speech recognition system 104 can determine whether the audio signal is still being received. If speech recognition system 104 no longer detects the audio signal, the method 300 can end. Otherwise, the method 300 can proceed to box 330 .
- MFCC's may be calculated for a next portion of the received signal.
- the speech detector 202 can access a digitized frame of the audio signal 204 .
- the signal feature calculator (SFC) 212 can calculate the FFT of the frame as shown in box 332 .
- the SFC can square the magnitude of coefficients resulting from the FFT, compute the log of the amplitudes and map a log of the amplitudes onto the mel scale as shown in boxes 334 and 336 , respectively.
- the SFC may take the discrete cosine transform (DCT) as previously described and illustrated in box 338 .
- DCT discrete cosine transform
- the method 300 can proceed to execute a parallel sequence indicated by two branches shown in FIG. 3 .
- the method 300 describes the updating of models used to determine whether future examined signal portions are speech or noise.
- an instant signal portion may be examined to determine whether the portion includes speech or noise.
- a gain condition prior is estimated.
- these sources can include the gain conditional prior from a previous time step, an observation likelihood, and an SNR dynamic distribution. In some implementations, these correspond to the arrows in FIG. 6 .
- the variance of the gain component of conditional prior may be large for the speech gain, and that component may have a small weight when compared to the gain-independent component when computing the observation likelihood.
- the weight of the observation likelihood may be large for updating the speech model if the observation is determined to be speech (e.g., if the observation closely matches the speech model).
- the weight of the SNR prior is reflected by the covariance matrix.
- the weighting between the gain variant components and gain dependent components can be determined by the variance of the gain dependent component i.e., ⁇ g x t . If this variance is large, then the model can disregard the gain variant components.
- the weight given to SNR prior versus the observational evidence up to time T is given by W.
- posterior probability weights may be generated from the MFCC components.
- the weight can be based on the components that are independent of gain, or the weights that are dependent of gain, or a combination of both, as indicated by the box 340 .
- MFCC components C 1 -C 12 may be invariant to the gain of the signal included in a frame analyzed by the speech detector, and MFCC component C 0 or an explicit energy dependent component can be gain dependent.
- the relative influence of the gain invariant and gain dependent components depends on the variance of the respective components.
- Posterior probability weights based upon these gain dependent and gain invariant components can be transmitted to the gain estimator for use in predicting updated gain estimates for the models as indicated by the transmission of information 238 in FIG. 2 .
- the speech/noise models are updated with the new gain estimates.
- the new gain estimates can be transmitted from the gain estimator 230 to the classifier 222 for integration into the Gaussian mixture models 224 and 226 .
- the classifier 222 may use the updated models for future analysis of received frames.
- a selected frame also may be analyzed according to some implementations.
- a probability that a frame contains speech or noise may be calculated using the speech/noise models as indicate by box 346 .
- the classifier 222 can calculate the probability that a frame includes speech a probability that a frame includes noise using the equations described in association with FIG. 2 .
- the classifier 222 can classify the frame as speech or noise based on the determined probabilities resulting from the calculations of box 346 . For example, if the probability that the frame is noise is higher than the probability that the frame is speech, the frame is classified as including noise.
- an indication whether the frame is speech or noise is output.
- the speech detector can output the indication to the speech decoder 110 .
- the speech decoder may only attempt to decode frames that are associated with a speech indicator and may ignore frames associated with a noise indicator. This may decrease computational requirements of the speech recognition system 104 and increase accuracy of speech decoding because frames that are likely noise are not sent to the decoder 110 .
- the method 300 can return to box 320 where a determination is made whether more frames are available for analysis. If more frames are available, the method may repeat as previously described, else the method 300 can end.
- FIGS. 4A-4C are diagrams of examples illustrating the updating and use of the noise/speech models.
- FIG. 4A is an example implementation of a speech (and/or noise) model, where the model is implemented as a hybrid extended Kalman filter and hidden marker model (HMM) that operates as a Bayesian network and as previously described in association with the models 224 , 226 of FIG. 2 .
- HMM hybrid extended Kalman filter and hidden marker model
- FIGS. 4B and 4C illustrate that in some implementations the speech and noise models can include multiple Gaussian distributions each having a weight that indicates the how much influence the associated distribution has in the calculation of whether a selected portion of a signal is noise and/or speech.
- each Gaussian is a Multivariate Gaussian, i.e., it has a vector of means, and a Covariance matrix.
- FIG. 4B shows an example table that includes components of a speech model.
- the table has a column of n (i.e., some number) Gaussian distributions and a column or vector of weights where the weights of each component vector also may be referred to as a mixture-priors P(s), each of which is associated with a particular Gaussian distribution.
- each of the Gaussian distributions is associated with a particular feature extracted from a portion of the signal.
- a Gaussian distribution 430 may be associated with the MFCC component C 0
- a Gaussian distribution 432 may be associate with the MFCC component C 1
- a Gaussian distribution 434 may be associated with MFCC component C 2 , etc.
- the speech model may rely on certain Gaussian distributions more heavily in a determination of whether a signal portion is speech. For example, a weight of 0.3 is associated with the Gaussian distribution 432 and a weight of 0.1 is is associated with the Gaussian distribution 430 . This may indicate that a similarity of a first signal feature to the Gaussian 432 is more important in the characterization of whether a signal is classified as speech than whether a second signal feature is similar to the Gaussian distribution 430 .
- FIG. 4C shows an example table that includes Gaussian and associated weights used to calculate the probability a signal portion is noise according to one implementation.
- the example table of FIG. 4C may be substantially similar to the previously described example table of FIG. 4B .
- FIG. 5A shows graphs of two example Gaussian distributions.
- An example Gaussian distribution 502 may be included in a speech model and an example Gaussian distribution 504 may be included in a noise model.
- the Gaussian distribution 502 may be expressed using a function 506 .
- one or more features can be extracted from a portion of a received audio signal and input into the function 506 .
- the output of the function may indicate a probability that the input feature should be classified as speech.
- the input may be a MFCC component extracted from a signal frame.
- the classifier 222 can input the MFCC value into the function 506 .
- s t ) 508 of the function 506 is close to the mean of the Gaussian 502 , which indicates that the MFCC component as a high probability that it is associated with speech according to this implementation.
- the classifier 222 can input the same MFCC value into a function 510 associated with a Gaussian distribution 504 for a noise model.
- s t ) 512 of the function 510 is not close to the mean, but is instead a few standard deviations from the mean indicating that the MFCC component has a low probability that it is associated with noise.
- FIG. 5B shows an example of how a gain-adapted model 550 is generated.
- an original model 552 includes several Gaussians, where each Gaussian is indicated by a column of a matrix for the model 552 .
- Each row in the matrix may correspond to a MFCC component.
- a bottom row 554 may include values that correspond to the gain-dependent MFCC component C 0 .
- the classifier 222 can combine the C 0 components for each of the Gaussians in the original model with gain values observed in a current signal frame 556 to generate new gain estimates 558 that are incorporated into the gain-adapted model 550 .
- the probability that an observed frame is speech can be calculated using:
- FIG. 6 shows a diagram of an example of gain parameter propagation in a speech/noise model 600 .
- the model 600 is used to calculate an occlusion prior as described earlier in association with equation 14.
- the model 600 can also calculate new gain parameters P(g t+1
- the estimation of the gain condition prior may be implemented using equation 15 above.
- the model 600 can approximate the gain conditional prior using P(g t+1
- the implementation is a random walk model and is constrained by a prior SNR (signal-to-noise ratio) distribution 604 .
- FIG. 7 is a graph 700 of an example SNR prior distribution as expressed in equation 16.
- the SNR prior distribution may constrain the gain estimates used to update the speech and/or noise models.
- the SNR prior distribution may couple the speech and noise gain to enforce a signal-to-noise ratio (e.g., the SNR prior may facilitate an inference of a speech gain from a noise gain even when speech is not observed).
- the SNR prior distribution and limit maximum/minimum speech and noise levels.
- FIGS. 8A-G are examples of speech endpointing using dynamic speech and noise adaptation (DySANA) model previously described.
- DySANA dynamic speech and noise adaptation
- tracking the instantaneous SNR of a signal can improve speech endpoint performance. For instance, prior levels built into speech and noise models may be a poor match for outliers in the data set (e.g., signals with high noise where the noise level is comparable to the prior speech level causing a misclassification of frames as speech). Accounting for an instantaneous SNR levels may alleviate this misclassification.
- a graph 800 shows a signal of 1 an audio recording that includes several frames of noise (some of which have a high gain) and a few frames of speech, which occur approximately between 200-300 ms on the graph 800 .
- a graph 802 (depicted across FIGS. 8B-D ) shows a posterior speech probability under an unadapted model and the DySANA model, and under both models when utilizing transition constraints that prevent the system from switching between noise and speech states too quickly.
- a graph 804 shows the observed signal level (C 0 ), the signal and noise levels for each frame under their respective models, and the switching DNA gain estimates.
- the noise level varies through the signal and at some points becomes almost speech-like (e.g., at 0.5 seconds, 1 second, and 1.75 seconds as indicated in the graph 802 ).
- the noise gain level may cause the unadapted model to misclassify the noise frames as speech. Applying transition constraints may alleviate the misclassifications of the unadapted model, but the unadapted model may still generate false positives (e.g., at 0.5 and 1 second). Application of the DySANA adaptation may further reduce these errors.
- FIG. 9 is a schematic diagram of a computer system 900 .
- the system 900 can be used for the operations described in association with any of the computer-implement methods described previously, according to one implementation.
- the system 900 is intended to include various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers.
- the system 900 can also include mobile devices, such as personal digital assistants, cellular telephones, smartphones, and other similar computing devices. Additionally the system can include portable storage media, such as, Universal Serial Bus (USB) flash drives. For example, the USB flash drives may store operating systems and other applications. The USB flash drives can include input/output components, such as a wireless transmitter or USB connector that may be inserted into a USB port of another computing device.
- mobile devices such as personal digital assistants, cellular telephones, smartphones, and other similar computing devices.
- portable storage media such as, Universal Serial Bus (USB) flash drives.
- USB flash drives may store operating systems and other applications.
- the USB flash drives can include input/output components, such as a wireless transmitter or USB connector that may be inserted into a USB port of another computing device.
- the system 900 includes a processor 910 , a memory 920 , a storage device 930 , and an input/output device 940 .
- Each of the components 910 , 920 , 930 , and 940 are interconnected using a system bus 950 .
- the processor 910 is capable of processing instructions for execution within the system 900 .
- the processor may be designed using any of a number of architectures.
- the processor 910 may be a CISC (Complex Instruction Set Computers) processor, a RISC (Reduced Instruction Set Computer) processor, or a MISC (Minimal Instruction Set Computer) processor.
- the processor 910 is a single-threaded processor. In another implementation, the processor 910 is a multi-threaded processor.
- the processor 910 is capable of processing instructions stored in the memory 920 or on the storage device 930 to display graphical information for a user interface on the input/output device 940 .
- the memory 920 stores information within the system 900 .
- the memory 920 is a computer-readable medium.
- the memory 920 is a volatile memory unit.
- the memory 920 is a non-volatile memory unit.
- the storage device 930 is capable of providing mass storage for the system 900 .
- the storage device 930 is a computer-readable medium.
- the storage device 930 may be a floppy disk device, a hard disk device, an optical disk device, or a tape device.
- the input/output device 940 provides input/output operations for the system 900 .
- the input/output device 940 includes a keyboard and/or pointing device.
- the input/output device 940 includes a display unit for displaying graphical user interfaces.
- the features described can be implemented in digital electronic circuitry, or in computer hardware, firmware, software, or in combinations of them.
- the apparatus can be implemented in a computer program product tangibly embodied in an information carrier, e.g., in a machine-readable storage device, for execution by a programmable processor; and method steps can be performed by a programmable processor executing a program of instructions to perform functions of the described implementations by operating on input data and generating output.
- the described features can be implemented advantageously in one or more computer programs that are executable on a programmable system including at least one programmable processor coupled to receive data and instructions from, and to transmit data and instructions to, a data storage system, at least one input device, and at least one output device.
- a computer program is a set of instructions that can be used, directly or indirectly, in a computer to perform a certain activity or bring about a certain result.
- a computer program can be written in any form of programming language, including compiled or interpreted languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
- Suitable processors for the execution of a program of instructions include, by way of example, both general and special purpose microprocessors, and the sole processor or one of multiple processors of any kind of computer.
- a processor will receive instructions and data from a read-only memory or a random access memory or both.
- the essential elements of a computer are a processor for executing instructions and one or more memories for storing instructions and data.
- a computer will also include, or be operatively coupled to communicate with, one or more mass storage devices for storing data files; such devices include magnetic disks, such as internal hard disks and removable disks; magneto-optical disks; and optical disks.
- Storage devices suitable for tangibly embodying computer program instructions and data include all forms of non-volatile memory, including by way of example semiconductor memory devices, such as EPROM, EEPROM, and flash memory devices; magnetic disks such as internal hard disks and removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.
- semiconductor memory devices such as EPROM, EEPROM, and flash memory devices
- magnetic disks such as internal hard disks and removable disks
- magneto-optical disks and CD-ROM and DVD-ROM disks.
- the processor and the memory can be supplemented by, or incorporated in, ASICs (application-specific integrated circuits).
- ASICs application-specific integrated circuits
- the features can be implemented on a computer having a display device such as a CRT (cathode ray tube) or LCD (liquid crystal display) monitor for displaying information to the user and a keyboard and a pointing device such as a mouse or a trackball by which the user can provide input to the computer.
- a display device such as a CRT (cathode ray tube) or LCD (liquid crystal display) monitor for displaying information to the user and a keyboard and a pointing device such as a mouse or a trackball by which the user can provide input to the computer.
- the features can be implemented in a computer system that includes a back-end component, such as a data server, or that includes a middleware component, such as an application server or an Internet server, or that includes a front-end component, such as a client computer having a graphical user interface or an Internet browser, or any combination of them.
- the components of the system can be connected by any form or medium of digital data communication such as a communication network. Examples of communication networks include a local area network (“LAN”), a wide area network (“WAN”), peer-to-peer networks (having ad-hoc or static members), grid computing infrastructures, and the Internet.
- LAN local area network
- WAN wide area network
- peer-to-peer networks having ad-hoc or static members
- grid computing infrastructures and the Internet.
- the computer system can include clients and servers.
- a client and server are generally remote from each other and typically interact through a network, such as the described one.
- the relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.
- gain estimates for the speech and noise models can be determined based on multiple sources, each of which can have more or less influence in a calculation result depending on a state or condition of the source (e.g., whether an analyzed signal appears to fit a noise model could be considered a condition for an observation likelihood component of a calculation to determine an estimated gain for the noise model).
- the gain estimator 230 can calculate gain estimates for use in the speech and noise models.
- the estimates can be based on, for example, three sources the previous gain condition prior, a current observation likelihood (e.g., how well the current observation fits the speech/noise model, and the SNR dynamic distribution. If, for example, the current observation is likely noise based on a close fit of the current observation to the noise model (e.g., low variance), the influence of the current observation likelihood for the noise model is increased, the influence of the previous condition prior for the noise model is decreased, and the gain for the noise is not influence (or is influence to a lower extend) by the SNR dynamic distribution in accordance with the previously described equations. Based on the relative influence of each of these sources, a gain estimate can be calculated and used to update the noise model.
- the current observation likelihood for the speech model may be low (e.g. the current observation has a high variance when compared to the speech model).
- the influence of the previous gain conditional prior for the speech model will be greater, and the speech gain will be pushed higher based on the SNR dynamic distribution for the speech model. Based on the relative influence of each of these sources, a gain estimate can be calculated and used to update the speech model.
Abstract
Description
P(g t ,s t ,y t ,o t |y 0:t−1)=P(y t |s t ,g t ,o t)P(s t)P(g t |y 0:t−1)P(o t |y 0:t−1), (3)
where P(st) is the gaussian mixture prior for the component of the speech or noise model, P(ot|y0:t−1) is the conditional prior for the occlusion variable described below, and the observation likelihood
is a component of the Gaussian mixture model.
is the conditional prior for the gain. P(st) and P(ot|y0:t−1) may be multinomial distributions.
where P(ot=1|y0:t−1) and P(ot=0|y0:t−1) are the conditional occlusion priors, and observation likelihood is
where a gain-adapted gaussian mixture component is
where Z is a normalizing factor. The weights of the energy independent components are
and the weight of the energy dependent component is
This illustrates that if σg
P(gt+1|gt)αN(gt+1;gt;ΣRW)N(gt+1;μSNR,ΣSNR). (16)
are the mean and variance of the gain component of product of the conditional gain prior and the observation likelihood. Under the MAP approximation, only a single Gaussian Mixture component is considered when updating the gains. Hence if the occlusion variable {circumflex over (σ)}t=1 the speech component μlp,x of μlp is a weighted sum of the conditional gain prior μg
μg
P(gt+1|gt)αN(gt+1;gt,ΣRW)N(gt+1;μSNR,ΣSNR) (31);
to define the gain dynamics as described earlier in association with equations 18 to 24. In this approximation, the implementation is a random walk model and is constrained by a prior SNR (signal-to-noise ratio)
- yt observation vector at time t which may be a vector of MFCC values.
- gt Gain under each model for a particular time t
- st State variable representing the Gaussian component within the Gaussian Mixture Model at time t.
- sx t State variable representing the Gaussian component within the speech Gaussian Mixture Model at time t.
- sn t State variable representing the Gaussian component within the noise Gaussian Mixture Model at time t.
- ot Voice activity state variable representing the presence of speech or noise. Also called occlusion state variable.
- P(gt|y0:t−1) Conditional prior for gain gt. Takes into account all observations up until time t−1.
- P(ot|y0:t−1) Conditional prior for occlusion variable ot. Takes into account all observations up until time t−1.
- P(gt+1|gt) Gain dynamic distribution. Also called SNR dynamic distribution.
- P(ot+1|ot) Transition matrix for occlusion state variable.
- P(s) Prior for state s within the noise or speech Gaussian Mixture Model.
- μn,s
n t Mean of Gaussian mixture component sn at time t for the noise model. - μn,s
n t ,0 Gain dependent mean of Gaussian mixture component sn at time t for the noise model. - μn,s
n t ,1:D Gain invariant vector of means of Gaussian mixture component sn at time t for the noise model. - Σnms
n t Covariance matrix of Gaussian mixture component sn at time t for the noise model. - Σn,s
n t Gain dependent component of the covariance matrix of Gaussian mixture component sn at time t for the noise model. - Σn,s
n t ,1:D Gain invariant components of the covariance matrix of Gaussian mixture component sn at time t for the noise model. - μx,s
x t Mean of Gaussian mixture component sx at time t for the speech model. - μx,s
x t ,0 Gain dependent mean of Gaussian mixture component sx at time t for the speech model. - μx,s
x t ,1:D Gain invariant vector of means of Gaussian mixture component sx at time t for the speech model. - Σx,s
x t Covariance matrix of Gaussian mixture component sx at time t for the speech model. - σx,s
x t ,0 Gain dependent component of the covariance matrix of Gaussian mixture component sx at time t for the speech model. - Σx,s
x t ,1:D Gain invariant components of the covariance matrix of Gaussian mixture component sx at time t for the speech model. - μg
x t Gain of speech model. - μg
n t Gain of noise model. - ΣRW Covariance of the random walk factor of the gain dynamic distribution.
- μSNR mean of the SNR factor of the gain dynamic distribution. Also called the mean of the SNR prior.
- ΣSNR Covariance of the SNR factor of the gain dynamic distribution. Also called the covariance of the SNR prior.
- μlp Intermediate result in the gain update. Represents the mean of the product of the conditional prior covariance and the likelihood due to the current observation.
- Σlp Intermediate result in the gain update. Represents the covariance of the product of the conditional prior covariance and the likelihood due to the current observation.
- W Weight which modifies influence of the SNR prior in the update of the mean and variance of the gains.
Claims (30)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/102,611 US8131543B1 (en) | 2008-04-14 | 2008-04-14 | Speech detection |
Applications Claiming Priority (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US12/102,611 US8131543B1 (en) | 2008-04-14 | 2008-04-14 | Speech detection |
Publications (1)
Publication Number | Publication Date |
---|---|
US8131543B1 true US8131543B1 (en) | 2012-03-06 |
Family
ID=45757990
Family Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US12/102,611 Active 2030-07-12 US8131543B1 (en) | 2008-04-14 | 2008-04-14 | Speech detection |
Country Status (1)
Country | Link |
---|---|
US (1) | US8131543B1 (en) |
Cited By (25)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20100268533A1 (en) * | 2009-04-17 | 2010-10-21 | Samsung Electronics Co., Ltd. | Apparatus and method for detecting speech |
US20110029309A1 (en) * | 2008-03-11 | 2011-02-03 | Toyota Jidosha Kabushiki Kaisha | Signal separating apparatus and signal separating method |
US20110077939A1 (en) * | 2009-09-30 | 2011-03-31 | Electronics And Telecommunications Research Institute | Model-based distortion compensating noise reduction apparatus and method for speech recognition |
US20110196820A1 (en) * | 2008-10-20 | 2011-08-11 | Siemens Corporation | Robust Filtering And Prediction Using Switching Models For Machine Condition Monitoring |
US20120239385A1 (en) * | 2011-03-14 | 2012-09-20 | Hersbach Adam A | Sound processing based on a confidence measure |
US20120245927A1 (en) * | 2011-03-21 | 2012-09-27 | On Semiconductor Trading Ltd. | System and method for monaural audio processing based preserving speech information |
US20120330656A1 (en) * | 2006-11-16 | 2012-12-27 | International Business Machines Corporation | Voice activity detection |
US20130054236A1 (en) * | 2009-10-08 | 2013-02-28 | Telefonica, S.A. | Method for the detection of speech segments |
US20130090926A1 (en) * | 2011-09-16 | 2013-04-11 | Qualcomm Incorporated | Mobile device context information using speech detection |
US20140095156A1 (en) * | 2011-07-07 | 2014-04-03 | Tobias Wolff | Single Channel Suppression Of Impulsive Interferences In Noisy Speech Signals |
GB2510036A (en) * | 2012-11-21 | 2014-07-23 | Secr Defence | Determining whether a measured signal matches a model signal |
US8965763B1 (en) * | 2012-02-02 | 2015-02-24 | Google Inc. | Discriminative language modeling for automatic speech recognition with a weak acoustic model and distributed training |
US20150088509A1 (en) * | 2013-09-24 | 2015-03-26 | Agnitio, S.L. | Anti-spoofing |
US20150269954A1 (en) * | 2014-03-21 | 2015-09-24 | Joseph F. Ryan | Adaptive microphone sampling rate techniques |
US9258653B2 (en) | 2012-03-21 | 2016-02-09 | Semiconductor Components Industries, Llc | Method and system for parameter based adaptation of clock speeds to listening devices and audio applications |
US20160275964A1 (en) * | 2015-03-20 | 2016-09-22 | Electronics And Telecommunications Research Institute | Feature compensation apparatus and method for speech recogntion in noisy environment |
US9842608B2 (en) | 2014-10-03 | 2017-12-12 | Google Inc. | Automatic selective gain control of audio data for speech recognition |
US20190156854A1 (en) * | 2010-12-24 | 2019-05-23 | Huawei Technologies Co., Ltd. | Method and apparatus for detecting a voice activity in an input audio signal |
CN110610696A (en) * | 2018-06-14 | 2019-12-24 | 清华大学 | MFCC feature extraction method and device based on mixed signal domain |
US10582313B2 (en) * | 2015-06-19 | 2020-03-03 | Widex A/S | Method of operating a hearing aid system and a hearing aid system |
CN111667838A (en) * | 2020-06-22 | 2020-09-15 | 清华大学 | Low-power-consumption analog domain feature vector extraction method for voiceprint recognition |
US20200365141A1 (en) * | 2019-05-16 | 2020-11-19 | Samsung Electronics Co., Ltd. | Electronic device and method of controlling thereof |
US20210056961A1 (en) * | 2019-08-23 | 2021-02-25 | Kabushiki Kaisha Toshiba | Information processing apparatus and information processing method |
US11227580B2 (en) * | 2018-02-08 | 2022-01-18 | Nippon Telegraph And Telephone Corporation | Speech recognition accuracy deterioration factor estimation device, speech recognition accuracy deterioration factor estimation method, and program |
US11460927B2 (en) * | 2020-03-19 | 2022-10-04 | DTEN, Inc. | Auto-framing through speech and video localizations |
Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20020013697A1 (en) * | 2000-06-08 | 2002-01-31 | Yifan Gong | Log-spectral compensation of gaussian mean vectors for noisy speech recognition |
US6408269B1 (en) * | 1999-03-03 | 2002-06-18 | Industrial Technology Research Institute | Frame-based subband Kalman filtering method and apparatus for speech enhancement |
US6615170B1 (en) * | 2000-03-07 | 2003-09-02 | International Business Machines Corporation | Model-based voice activity detection system and method using a log-likelihood ratio and pitch |
US20050182624A1 (en) * | 2004-02-16 | 2005-08-18 | Microsoft Corporation | Method and apparatus for constructing a speech filter using estimates of clean speech and noise |
US20060155537A1 (en) * | 2005-01-12 | 2006-07-13 | Samsung Electronics Co., Ltd. | Method and apparatus for discriminating between voice and non-voice using sound model |
US20060195317A1 (en) * | 2001-08-15 | 2006-08-31 | Martin Graciarena | Method and apparatus for recognizing speech in a noisy environment |
US20100057453A1 (en) * | 2006-11-16 | 2010-03-04 | International Business Machines Corporation | Voice activity detection system and method |
-
2008
- 2008-04-14 US US12/102,611 patent/US8131543B1/en active Active
Patent Citations (7)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6408269B1 (en) * | 1999-03-03 | 2002-06-18 | Industrial Technology Research Institute | Frame-based subband Kalman filtering method and apparatus for speech enhancement |
US6615170B1 (en) * | 2000-03-07 | 2003-09-02 | International Business Machines Corporation | Model-based voice activity detection system and method using a log-likelihood ratio and pitch |
US20020013697A1 (en) * | 2000-06-08 | 2002-01-31 | Yifan Gong | Log-spectral compensation of gaussian mean vectors for noisy speech recognition |
US20060195317A1 (en) * | 2001-08-15 | 2006-08-31 | Martin Graciarena | Method and apparatus for recognizing speech in a noisy environment |
US20050182624A1 (en) * | 2004-02-16 | 2005-08-18 | Microsoft Corporation | Method and apparatus for constructing a speech filter using estimates of clean speech and noise |
US20060155537A1 (en) * | 2005-01-12 | 2006-07-13 | Samsung Electronics Co., Ltd. | Method and apparatus for discriminating between voice and non-voice using sound model |
US20100057453A1 (en) * | 2006-11-16 | 2010-03-04 | International Business Machines Corporation | Voice activity detection system and method |
Non-Patent Citations (3)
Title |
---|
Bahl et al. "Performance of the IBM Large Vocabulary Continuous Speech Recognition System on the ARPA Wall Street Journal Task", IEEE, 1995. * |
Fujimoro, et al. "Noise Robust Voice Activity Detection Based on Switching Kalman Filter" IEICE Trans. Inf. & Syst., vol. E91-D, No. 3 (Mar. 2008) pp. 467-477. |
Rennie, et al. "Dynamic Noise Adaptation" IBM T.J. Watson Research Center Yorktown Heights, NY 10598, USA Acoustics, Speech and Signal Processing, 2006. ICASSP 2006 Proceedings. 2006 IEEE International Conference, vol. 1 (May 14-19, 2006) 4 pages. |
Cited By (44)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20120330656A1 (en) * | 2006-11-16 | 2012-12-27 | International Business Machines Corporation | Voice activity detection |
US8554560B2 (en) * | 2006-11-16 | 2013-10-08 | International Business Machines Corporation | Voice activity detection |
US20110029309A1 (en) * | 2008-03-11 | 2011-02-03 | Toyota Jidosha Kabushiki Kaisha | Signal separating apparatus and signal separating method |
US8452592B2 (en) * | 2008-03-11 | 2013-05-28 | Toyota Jidosha Kabushiki Kaisha | Signal separating apparatus and signal separating method |
US8560492B2 (en) * | 2008-10-20 | 2013-10-15 | Siemens Corporation | Robust filtering and prediction using switching models for machine condition monitoring |
US20110196820A1 (en) * | 2008-10-20 | 2011-08-11 | Siemens Corporation | Robust Filtering And Prediction Using Switching Models For Machine Condition Monitoring |
US8874440B2 (en) * | 2009-04-17 | 2014-10-28 | Samsung Electronics Co., Ltd. | Apparatus and method for detecting speech |
US20100268533A1 (en) * | 2009-04-17 | 2010-10-21 | Samsung Electronics Co., Ltd. | Apparatus and method for detecting speech |
US20110077939A1 (en) * | 2009-09-30 | 2011-03-31 | Electronics And Telecommunications Research Institute | Model-based distortion compensating noise reduction apparatus and method for speech recognition |
US8346545B2 (en) * | 2009-09-30 | 2013-01-01 | Electronics And Telecommunications Research Institute | Model-based distortion compensating noise reduction apparatus and method for speech recognition |
US20130054236A1 (en) * | 2009-10-08 | 2013-02-28 | Telefonica, S.A. | Method for the detection of speech segments |
US11430461B2 (en) | 2010-12-24 | 2022-08-30 | Huawei Technologies Co., Ltd. | Method and apparatus for detecting a voice activity in an input audio signal |
US10796712B2 (en) * | 2010-12-24 | 2020-10-06 | Huawei Technologies Co., Ltd. | Method and apparatus for detecting a voice activity in an input audio signal |
US20190156854A1 (en) * | 2010-12-24 | 2019-05-23 | Huawei Technologies Co., Ltd. | Method and apparatus for detecting a voice activity in an input audio signal |
US9589580B2 (en) * | 2011-03-14 | 2017-03-07 | Cochlear Limited | Sound processing based on a confidence measure |
US20120239385A1 (en) * | 2011-03-14 | 2012-09-20 | Hersbach Adam A | Sound processing based on a confidence measure |
US10249324B2 (en) | 2011-03-14 | 2019-04-02 | Cochlear Limited | Sound processing based on a confidence measure |
US20120245927A1 (en) * | 2011-03-21 | 2012-09-27 | On Semiconductor Trading Ltd. | System and method for monaural audio processing based preserving speech information |
US20140095156A1 (en) * | 2011-07-07 | 2014-04-03 | Tobias Wolff | Single Channel Suppression Of Impulsive Interferences In Noisy Speech Signals |
US9858942B2 (en) * | 2011-07-07 | 2018-01-02 | Nuance Communications, Inc. | Single channel suppression of impulsive interferences in noisy speech signals |
US20130090926A1 (en) * | 2011-09-16 | 2013-04-11 | Qualcomm Incorporated | Mobile device context information using speech detection |
US8965763B1 (en) * | 2012-02-02 | 2015-02-24 | Google Inc. | Discriminative language modeling for automatic speech recognition with a weak acoustic model and distributed training |
US9258653B2 (en) | 2012-03-21 | 2016-02-09 | Semiconductor Components Industries, Llc | Method and system for parameter based adaptation of clock speeds to listening devices and audio applications |
US20150255058A1 (en) * | 2012-11-21 | 2015-09-10 | The Secretary Of State For Defence | Method for determining whether a measured signal matches a model signal |
GB2510036B (en) * | 2012-11-21 | 2015-06-24 | Secr Defence | Method for determining whether a measured signal matches a model signal |
GB2510036A (en) * | 2012-11-21 | 2014-07-23 | Secr Defence | Determining whether a measured signal matches a model signal |
US9767806B2 (en) * | 2013-09-24 | 2017-09-19 | Cirrus Logic International Semiconductor Ltd. | Anti-spoofing |
US20150088509A1 (en) * | 2013-09-24 | 2015-03-26 | Agnitio, S.L. | Anti-spoofing |
US9406313B2 (en) * | 2014-03-21 | 2016-08-02 | Intel Corporation | Adaptive microphone sampling rate techniques |
US20150269954A1 (en) * | 2014-03-21 | 2015-09-24 | Joseph F. Ryan | Adaptive microphone sampling rate techniques |
US9842608B2 (en) | 2014-10-03 | 2017-12-12 | Google Inc. | Automatic selective gain control of audio data for speech recognition |
US20160275964A1 (en) * | 2015-03-20 | 2016-09-22 | Electronics And Telecommunications Research Institute | Feature compensation apparatus and method for speech recogntion in noisy environment |
US9799331B2 (en) * | 2015-03-20 | 2017-10-24 | Electronics And Telecommunications Research Institute | Feature compensation apparatus and method for speech recognition in noisy environment |
US10582313B2 (en) * | 2015-06-19 | 2020-03-03 | Widex A/S | Method of operating a hearing aid system and a hearing aid system |
US11227580B2 (en) * | 2018-02-08 | 2022-01-18 | Nippon Telegraph And Telephone Corporation | Speech recognition accuracy deterioration factor estimation device, speech recognition accuracy deterioration factor estimation method, and program |
CN110610696B (en) * | 2018-06-14 | 2021-11-09 | 清华大学 | MFCC feature extraction method and device based on mixed signal domain |
CN110610696A (en) * | 2018-06-14 | 2019-12-24 | 清华大学 | MFCC feature extraction method and device based on mixed signal domain |
US20200365141A1 (en) * | 2019-05-16 | 2020-11-19 | Samsung Electronics Co., Ltd. | Electronic device and method of controlling thereof |
US11551671B2 (en) * | 2019-05-16 | 2023-01-10 | Samsung Electronics Co., Ltd. | Electronic device and method of controlling thereof |
US20210056961A1 (en) * | 2019-08-23 | 2021-02-25 | Kabushiki Kaisha Toshiba | Information processing apparatus and information processing method |
US11823669B2 (en) * | 2019-08-23 | 2023-11-21 | Kabushiki Kaisha Toshiba | Information processing apparatus and information processing method |
US11460927B2 (en) * | 2020-03-19 | 2022-10-04 | DTEN, Inc. | Auto-framing through speech and video localizations |
CN111667838A (en) * | 2020-06-22 | 2020-09-15 | 清华大学 | Low-power-consumption analog domain feature vector extraction method for voiceprint recognition |
CN111667838B (en) * | 2020-06-22 | 2022-10-14 | 清华大学 | Low-power-consumption analog domain feature vector extraction method for voiceprint recognition |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8131543B1 (en) | Speech detection | |
US7756707B2 (en) | Signal processing apparatus and method | |
US8650029B2 (en) | Leveraging speech recognizer feedback for voice activity detection | |
US7660713B2 (en) | Systems and methods that detect a desired signal via a linear discriminative classifier that utilizes an estimated posterior signal-to-noise ratio (SNR) | |
EP2216775B1 (en) | Speaker recognition | |
US9536525B2 (en) | Speaker indexing device and speaker indexing method | |
US9754584B2 (en) | User specified keyword spotting using neural network feature extractor | |
US8532991B2 (en) | Speech models generated using competitive training, asymmetric training, and data boosting | |
US20200117995A1 (en) | Training multiple neural networks with different accuracy | |
Rangachari et al. | A noise-estimation algorithm for highly non-stationary environments | |
US8412525B2 (en) | Noise robust speech classifier ensemble | |
US7139703B2 (en) | Method of iterative noise estimation in a recursive framework | |
US20170372725A1 (en) | System and method for cluster-based audio event detection | |
US20090119103A1 (en) | Speaker recognition system | |
US9741341B2 (en) | System and method for dynamic noise adaptation for robust automatic speech recognition | |
US20080243503A1 (en) | Minimum divergence based discriminative training for pattern recognition | |
US7565284B2 (en) | Acoustic models with structured hidden dynamics with integration over many possible hidden trajectories | |
JP2005078077A (en) | Method and device to pursue vocal tract resonance using temporal restriction guided by nonlinear predictor and target | |
US11250860B2 (en) | Speaker recognition based on signal segments weighted by quality | |
US20080140399A1 (en) | Method and system for high-speed speech recognition | |
US20080189109A1 (en) | Segmentation posterior based boundary point determination | |
Esmaili et al. | An automatic prolongation detection approach in continuous speech with robustness against speaking rate variations | |
Joho et al. | Pitch estimation using models of voiced speech on three levels | |
Arslan et al. | Noise robust voice activity detection based on multi-layer feed-forward neural network | |
Droppo et al. | Efficient on-line acoustic environment estimation for FCDCN in a continuous speech recognition system |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:WEISS, RON J.;KRISTJANSSON, TRAUSTI;SIGNING DATES FROM 20080416 TO 20080630;REEL/FRAME:021273/0743 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
CC | Certificate of correction | ||
FPAY | Fee payment |
Year of fee payment: 4 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044101/0405Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 12TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1553); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 12 |