US9628684B2 - Light-field aberration correction - Google Patents
Light-field aberration correction Download PDFInfo
- Publication number
- US9628684B2 US9628684B2 US14/573,319 US201414573319A US9628684B2 US 9628684 B2 US9628684 B2 US 9628684B2 US 201414573319 A US201414573319 A US 201414573319A US 9628684 B2 US9628684 B2 US 9628684B2
- Authority
- US
- United States
- Prior art keywords
- light
- field
- calibration data
- data
- sensor
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active, expires
Links
Images
Classifications
-
- H04N5/2254—
-
- G—PHYSICS
- G02—OPTICS
- G02B—OPTICAL ELEMENTS, SYSTEMS OR APPARATUS
- G02B27/00—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00
- G02B27/0075—Optical systems or apparatus not provided for by any of the groups G02B1/00 - G02B26/00, G02B30/00 with means for altering, e.g. increasing, the depth of field or depth of focus
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N23/00—Cameras or camera modules comprising electronic image sensors; Control thereof
- H04N23/95—Computational photography systems, e.g. light-field imaging systems
- H04N23/957—Light-field or plenoptic cameras or camera modules
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N17/00—Diagnosis, testing or measuring for television systems or their details
- H04N17/002—Diagnosis, testing or measuring for television systems or their details for television cameras
-
- H—ELECTRICITY
- H04—ELECTRIC COMMUNICATION TECHNIQUE
- H04N—PICTORIAL COMMUNICATION, e.g. TELEVISION
- H04N25/00—Circuitry of solid-state image sensors [SSIS]; Control thereof
- H04N25/60—Noise processing, e.g. detecting, correcting, reducing or removing noise
- H04N25/61—Noise processing, e.g. detecting, correcting, reducing or removing noise the noise originating only from the lens unit, e.g. flare, shading, vignetting or "cos4"
-
- H04N5/3572—
Definitions
- the present disclosure relates to light-field image capture, and more specifically, to systems and methods for correcting for aberrations in the design and/or manufacture of lenses of light-field image capture devices.
- a light-field camera can digitally correct for aberrations of the main lens.
- aberration in the lens causes the lens to focus to a large spot size, resulting in reduced resolving power.
- the system includes, for example, four main processing stages, although one skilled in the art will recognize that such stages can be implemented singly or in any suitable combination with one another:
- the system uses an aberration data modeling algorithm to convert the high-resolution aberration correction data into a compact representation. Any of a number of candidate algorithms can be used. For example, in at least one embodiment the system uses 4D table re-sampling and filtering. The output compact data faithfully represents the aberration correction information, and allows retargeting, which is a useful feature for the additional processing as described below.
- any particular manufactured unit can deviate from the design due to any of a number of reasons, including, for example, lens axis tilting, optical center shifting, microlens array geometry mismatch, and the like.
- one or more calibration processing steps are performed during the manufacturing process to estimate the parameters for describing these variations.
- the calibration may require special charts to image, and multiple calibrations can be required for different system configurations (lens focus/zoom, exposure, and the like).
- the system obtains the raw image/light-field data and the corresponding system configuration information.
- a retargeting algorithm is performed to convert the compact aberration correction data to match the characteristics of the incoming image, as follows:
- the system can correct the incoming light-field. This can be done, for example, by warping the light-field and/or by correcting the 4D coordinates during each light-field processing stage (for example, at projection, depth estimation, and the like).
- the system applies an aberration correction assessment algorithm to check if the aberration correction has been applied correctly.
- the algorithm may be based, for example, on an epipolar image (EPI) analysis technique as follows:
- the system checks the correction result at multiple regions. Since the EPI analysis is more reliable in areas of the image having strong edges at fixed depths, a lookup table or chart may be used for this stage.
- FIG. 1 depicts a portion of a light-field image.
- FIG. 2 depicts an example of an architecture for a light-field capture device, according to one embodiment.
- FIG. 3 depicts an example of an architecture for implementing post-processing in a post-processing system communicatively coupled to a light-field capture device, according to one embodiment.
- FIG. 4 depicts an example of an architecture for a light-field camera according to one embodiment.
- FIG. 5 is a block diagram depicting a hardware architecture for practicing the described method, according to one embodiment.
- FIG. 6 is a schematic block diagram illustrating the database of FIG. 5 , according to one embodiment.
- FIG. 7 is an isometric view of a portion of a light-field camera.
- FIG. 8 is a diagram illustrating an example of mapping coordinates between real and idealized lens systems.
- FIG. 9 is a schematic block diagram illustrating a system for generating the product calibration data according to one embodiment.
- FIG. 10 is a flow diagram illustrating a method of using the system of FIG. 9 to generate the product calibration data.
- FIG. 11 is a schematic block diagram illustrating a system for generating the unit calibration data according to one embodiment.
- FIG. 12 is a flow diagram illustrating a method of using the system of FIG. 11 to generate the unit calibration data.
- FIG. 13 is a schematic block diagram illustrating a system for correcting light-field data and/or conducting generalized subsequent processing, according to one embodiment.
- FIG. 14 is a flow diagram illustrating a method of using the system of FIG. 13 to generate one or more light-field images.
- FIG. 15 is a schematic block diagram illustrating a system for assessing the quality of aberration correction.
- FIG. 16 is a flow diagram illustrating a method of using the system of FIG. 15 to generate the correction score.
- FIGS. 17A and 17B are exemplary epipolar images generated from corrected and uncorrected light-field data, respectively, according to one embodiment.
- a data acquisition device can be any device or system for acquiring, recording, measuring, estimating, determining and/or computing data representative of a scene, including but not limited to two-dimensional image data, three-dimensional image data, and/or light-field data.
- a data acquisition device may include optics, sensors, and image processing electronics for acquiring data representative of a scene, using techniques that are well known in the art.
- One skilled in the art will recognize that many types of data acquisition devices can be used in connection with the present disclosure, and that the disclosure is not limited to cameras.
- any use of such term herein should be considered to refer to any suitable device for acquiring image data.
- the system and method described herein can be implemented in connection with light-field images captured by light-field capture devices including but not limited to those described in Ng et al., Light-field photography with a hand-held plenoptic capture device, Technical Report CSTR 2005-02, Stanford Computer Science.
- FIG. 2 there is shown a block diagram depicting a light-field capture device such as a camera 800 .
- FIG. 3 there is shown a block diagram depicting an architecture for implementing post-processing in a post-processing system communicatively coupled to a light-field capture device such as a camera 800 , according to one embodiment.
- FIGS. 2 and 3 are merely exemplary, and that other architectures are possible for camera 800 .
- FIGS. 2 and 3 are optional, and may be omitted or reconfigured.
- camera 800 may be a light-field camera that includes light-field image data acquisition device 809 having optics 801 , image sensor 803 (including a plurality of individual sensors for capturing pixels), and microlens array 802 .
- Optics 801 may include, for example, aperture 812 for allowing a selectable amount of light into camera 800 , and main lens 813 for focusing light toward microlens array 802 .
- microlens array 802 may be disposed and/or incorporated in the optical path of camera 800 (between main lens 813 and sensor 803 ) so as to facilitate acquisition, capture, sampling of, recording, and/or obtaining light-field image data via sensor 803 . Referring now also to FIG.
- FIG. 4 shows, in conceptual form, the relationship between aperture 812 , main lens 813 , microlens array 802 , and sensor 803 , as such components interact to capture light-field data for subject 901 .
- light-field camera 800 may also include a user interface 805 for allowing a user to provide input for controlling the operation of camera 800 for capturing, acquiring, storing, and/or processing image data.
- light-field camera 800 may also include control circuitry 810 for facilitating acquisition, sampling, recording, and/or obtaining light-field image data.
- control circuitry 810 may manage and/or control (automatically or in response to user input) the acquisition timing, rate of acquisition, sampling, capturing, recording, and/or obtaining of light-field image data.
- camera 800 may include memory 811 for storing image data, such as output by image sensor 803 .
- memory 811 can include external and/or internal memory.
- memory 811 can be provided at a separate device and/or location from camera 800 .
- camera 800 may store raw light-field image data, as output by sensor 803 , and/or a representation thereof, such as a compressed image data file.
- memory 811 can also store data representing the characteristics, parameters, and/or configurations (collectively “configuration data”) of device 809 .
- captured image data is provided to post-processing circuitry 804 .
- Such circuitry 804 may be disposed in or integrated into light-field image data acquisition device 809 , as shown in FIG. 2 , or it may be in a separate component external to light-field image data acquisition device 809 , as shown in FIG. 3 . Such separate component may be local or remote with respect to light-field image data acquisition device 809 .
- Any suitable wired or wireless protocol can be used for transmitting image data 821 to circuitry 804 ; for example, camera 800 can transmit image data 821 and/or other data via the Internet, a cellular data network, a Wi-Fi network, a Bluetooth communication protocol, and/or any other suitable means.
- Such a separate component may include any of a wide variety of computing devices, including but not limited to computers, smartphones, tablets, cameras, and/or any other device that processes digital information.
- Such a separate component may include additional features such as a user input 815 and/or a display screen 816 . If desired, light-field image data may be displayed for the user on the display screen 816 .
- aspects of the system and method of the present disclosure may be implemented on the camera 800 of FIG. 2 and/or on the post-processing system of FIG. 3 .
- various aspects of the system and method can be implemented on any electronic device equipped to receive, store, and present information.
- Such an electronic device may be, for example, a desktop computer, laptop computer, smartphone, tablet computer, or the like.
- FIG. 5 there is shown a block diagram depicting a hardware architecture for practicing the described method, according to one embodiment.
- Such an architecture can be used, for example, for implementing the techniques of the system in a computer or other device 501 .
- Device 501 may be any electronic device equipped to receive, store, and/or present information, and to receive user input in connect with such information.
- device 501 has a number of hardware components well known to those skilled in the art.
- Input device 502 can be any element that receives input from user 500 , including, for example, a keyboard, mouse, stylus, touch-sensitive screen (touchscreen), touchpad, trackball, accelerometer, five-way switch, microphone, or the like.
- Input can be provided via any suitable mode, including for example, one or more of: pointing, tapping, typing, dragging, and/or speech.
- Data store 506 can be any magnetic, optical, or electronic storage device for data in digital form; examples include flash memory, magnetic hard drive, CD-ROM, DVD-ROM, or the like.
- data store 506 stores information which may include one or more databases, referred to collectively as a database 511 , that can be utilized and/or displayed according to the techniques described below.
- database 511 can be stored elsewhere, and retrieved by device 501 when needed for presentation to user 500 .
- Database 511 may include one or more data sets, which may be used for a variety of purposes and may include a wide variety of files, metadata, and/or other data.
- Display screen 503 can be any element that graphically displays information such as items from database 511 , and/or the results of steps performed on such items to provide information useful to a user.
- Such output may include, for example, raw data, data visualizations, illustrations of light-field camera components, light-field images, aberration correction metrics, and/or the like.
- Such information may be displayed by the display screen 503 in a wide variety of formats, including but not limited to lists, images, charts, graphs, and the like.
- a dynamic control such as a scrolling mechanism, may be available via input device 502 to change which information is currently displayed, and/or to alter the manner in which the information is displayed.
- Processor 504 can be a conventional microprocessor for performing operations on data under the direction of software, according to well-known techniques.
- Memory 505 can be random-access memory, having a structure and architecture as are known in the art, for use by processor 504 in the course of running software.
- Data store 506 can be local or remote with respect to the other components of device 501 .
- device 501 is configured to retrieve data from a remote data storage device when needed.
- Such communication between device 501 and other components can take place wirelessly, by Ethernet connection, via a computing network such as the Internet, via a cellular network, or by any other appropriate means. This communication with other electronic devices is provided as an example and is not necessary.
- data store 506 is detachable in the form of a CD-ROM, DVD, flash drive, USB hard drive, or the like.
- Database 511 can be entered from a source outside of device 501 into a data store 506 that is detachable, and later displayed after the data store 506 is connected to device 501 .
- data store 506 is fixed within device 501 .
- system of the present disclosure may be implemented as software written in any suitable computer programming language, whether in a standalone or client/server architecture. Alternatively, it may be implemented and/or embedded in hardware.
- FIG. 6 a schematic block diagram illustrates the database 511 of FIG. 5 , according to one embodiment.
- the database 511 may include various data structures, examples of which are illustrated in FIG. 6 .
- the main lens design 910 may include the details of the main lens 813 and/or related components (such as a microlens array 802 , phase mask, and/or other optics) to be used in a series of light-field cameras 800 .
- the main lens design 910 may include all of the information required to model the manner in which light travels through the light-field camera 800 to reach the image sensor 803 . Accordingly, the main lens design 910 may include lens shapes, sizes, materials, and/or the like.
- the sensor design 912 may include the details of the image sensor 803 and/or related components to be used in a series of light-field cameras 800 .
- the sensor design 912 may include all of the information needed to model light-field data capture with the sensor design 912 , when used in combination with the main lens design 910 . Accordingly, the sensor design 912 may include the shape, size, light-receiving properties, and/or other information pertinent to the configuration and operation of the image sensor 803 .
- Each of the samples 914 may include one or more exemplary settings of the light-field camera 800 , which may be user-adjustable settings.
- the samples 914 may include a zoom setting 610 of the light-field camera 800 , and a focus setting 612 of the light-field camera 800 .
- each of the samples 914 may be a particular combination of settings, such as a particular zoom setting 610 and a particular focus setting 612 .
- Each raw mapping table 922 may be particular to one of the samples 914 . Thus, there may be a raw mapping table 922 for each sample 914 .
- Each of the raw mapping tables 922 may list multiple sensor locations on the sensor 803 , and indicate the ideal light ray coordinates that correspond to each one of the sensor locations, for the sample 914 pertaining to the raw mapping table 922 .
- Each model 932 may be particular to one of the raw mapping tables 922 , and thus to one of the samples 914 . Thus, there may also be a model 932 for each sample 914 .
- Each of the models 932 may provide a more compact way (such as a mathematical function or a more compact table that can be interpolated) to obtain the ideal light ray coordinates that correspond to each of the sensor locations, for the sample 914 to which the model 932 pertains.
- the product calibration data 940 may be data that incorporates the raw mapping tables 922 and/or the models 932 .
- the product calibration data 940 may be applied to light-field data to at least partially compensate for departure of the main lens design 910 and/or the sensor design 912 from ideal lenses and/or sensors.
- the product calibration data 940 may be product-specific, and may thus be applied to and/or utilized by all cameras 800 of a given design.
- Each of the mapping functions 1122 may be particular to one of the samples 914 . Thus, there may be a mapping function 1122 for each sample 914 . Each of the mapping functions 1122 may list multiple sensor locations on the sensor 803 , and indicate the non-ideal light ray coordinates that correspond to each one of the sensor locations, for the sample 914 pertaining to the raw mapping table 922 .
- the unit calibration data 1140 may be data that incorporates the mapping functions 1122 .
- the unit calibration data 1140 may be applied to light-field data to at least partially compensate for departure of the actual main lens 813 and/or actual image sensor 803 in the field camera 800 to which it applies, from the main lens design 910 and/or the sensor design 912 .
- the unit calibration data 1140 may be unit-specific, and may thus be applied to and/or utilized by only one individual camera 800 . Each camera 800 of a given design may have its own unique unit calibration data 1140 .
- the light-field data 1310 may be data captured by the field camera 800 .
- the light-field data 1310 may be raw data, or may have been subjected to one or more preliminary processing steps, such as demodulation, demosaicing, auto-white balancing, saturation recovery, and/or the like.
- the corrected light-field data 1322 may be the data that results from application of the product calibration data 940 and/or the unit calibration data 1140 .
- the corrected light-field data 1322 may at least partially compensate for departure of the main lens design 910 and/or the sensor design 912 from ideal lenses and/or sensors. Additionally or alternatively, the corrected light-field data 1322 may at least partially compensate for departure of the actual main lens 813 and/or actual image sensor 803 in the field camera 800 to which it applies, from the main lens design 910 and/or the sensor design 912 .
- the light-field images 1332 may be images created from light-field data, such as the light-field data 1310 and/or the corrected light-field data 1322 .
- the light-field image 1332 may be two-dimensional representations of the corresponding light-field data.
- the epipolar images 1512 may be unique light-field images generated for testing purposes.
- an epipolar image may be a two-dimensional slice of a four-dimensional light-field given one fixed image and one set of fixed aperture coordinates. Exemplary epipolar images 1512 will be shown and described subsequently.
- the correction scores 1522 may be scores indicative of how well the corrected light-field data 1322 has successfully corrected the light-field data 1310 .
- the correction score 1522 may indicate how well the corrected light-field data 1322 compensate for departure of the main lens design 910 and/or the sensor design 912 from ideal lenses and/or sensors. Additionally or alternatively, the correction score 1522 may indicate how well the corrected light-field data 1322 compensate for departure of the actual main lens 813 and/or actual image sensor 803 in the field camera 800 to which it applies, from the main lens design 910 and/or the sensor design 912 .
- Light-field images often include a plurality of projections (which may be circular or of other shapes) of aperture 812 of camera 800 , each projection taken from a different vantage point on the camera's focal plane.
- the light-field image may be captured on sensor 803 .
- the interposition of microlens array 802 between main lens 813 and sensor 803 causes images of aperture 812 to be formed on sensor 803 , each microlens in array 802 projecting a small image of main-lens aperture 812 onto sensor 803 .
- These aperture-shaped projections are referred to herein as disks, although they need not be circular in shape.
- the term “disk” is not intended to be limited to a circular region, but can refer to a region of any shape.
- Light-field images include four dimensions of information describing light rays impinging on the focal plane of camera 800 (or other capture device).
- Two spatial dimensions (herein referred to as x and y) are represented by the disks themselves.
- the spatial resolution of a light-field image with 120,000 disks, arranged in a Cartesian pattern 400 wide and 300 high, is 400 ⁇ 300.
- Two angular dimensions (herein referred to as u and v) are represented as the pixels within an individual disk.
- the angular resolution of a light-field image with 100 pixels within each disk, arranged as a 10 ⁇ 10 Cartesian pattern is 10 ⁇ 10.
- This light-field image has a 4-D (x, y, u, v) resolution of (400,300,10,10).
- FIG. 1 there is shown an example of a 2-disk by 2-disk portion of such a light-field image, including depictions of disks 102 and individual pixels 203 ; for illustrative purposes, each disk 102 is ten pixels 203 across.
- the 4-D light-field representation may be reduced to a 2-D image through a process of projection and reconstruction.
- a virtual surface of projection may be introduced, and the intersections of representative rays with the virtual surface can be computed. The color of each representative ray may be taken to be equal to the color of its corresponding pixel.
- Any number of image processing techniques can be used to reduce color artifacts, reduce projection artifacts, increase dynamic range, and/or otherwise improve image quality. Examples of such techniques, including for example modulation, demodulation, and demosaicing, are described in related U.S. application Ser. No. 13/774,925 for “Compensating for Sensor Saturation and Microlens Modulation During Light-Field Image Processing” filed Feb. 22, 2013, the disclosure of which is incorporated herein by reference.
- a light-field is a 4D representation of light rays traveling in free space.
- each light ray can be identified by its intersection with two 2D coordinate planes.
- one plane is typically aligned with the aperture plane, and the other plane is aligned with the microlens array (this is referred to as the image plane).
- FIG. 7 is an isometric view of a portion of a light-field camera 700 with an aperture plane 710 and an image plane 720 .
- a light ray 730 is shown passing from the aperture plane 710 to the image plane 720 .
- the light ray 730 passes through (u, v) coordinates 740
- the image plane 720 the light ray 730 passes through (x, y) coordinates 750 .
- a light-field camera records the information of individual light rays (or ray bundles), each representing a sample of a four-dimensional dataset (color, brightness, polarization, and the like).
- Such a technique enables many novel applications, such as digital refocusing, range sensing, and perspective shift.
- the light-field includes light-field data captured by the light-field camera and associated metadata (exposure, gain, time, etc.).
- the light-field data is a 2D array, which is read out from the CCD or CMOS sensor array. Therefore, each sample in the data can be indexed by 2D coordinates (s, t).
- aberration causes distortions to the resulting image. Different types of distortion may take place, including for example defocus, field curvature, spherical distortion, astigmatism, coma, and/or the like. In practice, the magnitude and nature of the aberration depends on, for example, the optical design, lens configuration, wavelength, and manufacture process.
- Aberration means that after a light ray passes through the lens, it does not travel along the path predicted by the ideal thin lens model. Therefore, the 4D coordinates of that light ray are different from the ideal coordinates.
- regular light-field processing can be performed on the image data by replacing the original, non-ideal coordinates with the corrected, ideal, coordinates. This can be done in post-processing, either by software or hardware or a combination of the two.
- What the camera physically records is termed the aberrated ray space, denoted by coordinates (x′, y′, u′, v′); these coordinates can be unambiguously remapped into an ideal ray space (x, y, u, v).
- the mapping between these two spaces can be computed by knowing the design of the lens and tracing rays outward from the center of each pixel on the sensor.
- mapping coordinates between real and idealized lens systems is illustrated in diagram 850 of FIG. 8 , with reference to the center pixel.
- the rays are launched at an origin 860 .
- the rays are traced through all of the elements of the real lens 870 into the real world.
- the rays are then traced through a corresponding ideal model of the ideal lens 880 .
- the rays terminate at a terminus 890 on a sensor plane, and the rays' 4D coordinates are recorded.
- the difference between the 4D coordinate the ray was launched with (i.e., at the origin 860 ) and the 4D coordinate it terminates at (i.e., at the terminus 890 ) is the correction vector for that specific pixel. This process defines what is called the ray correction function.
- aberration correction may include multiple stages. For example, product calibration data, such as the product calibration data 940 , may be obtained for all cameras 800 with a given design. Unit calibration data, such as the unit calibration data 1140 , may be obtained for each individual camera 800 . Then, the product calibration data 940 and the unit calibration data 1140 may be used to process the light-field data 1310 to correct the effects of aberration, thereby generating the corrected light-field data 1322 .
- product calibration data such as the product calibration data 940
- Unit calibration data such as the unit calibration data 1140
- Light-field processing benefits from per-product calibration information for correctly interpreting the incoming data captured by the product, wherein a product is a particular model or design of a light-field capture device.
- the reverse mapping function b can be used for aberration correction.
- the mapping function is determined by the optical design and lens configuration during operation.
- the mapping function may be seven-dimensional, or may contain more or fewer dimensions. At any rate, given all the different permutations, generating and storing all possible mapping function can be highly impractical.
- the system addresses this problem by using a two-stage process. In at least one embodiment, this process is only performed once for each product (or each lens design).
- FIG. 9 is a schematic block diagram illustrating a system 900 for generating the product calibration data 940 according to one embodiment.
- the system 900 may include any computing device; in some embodiments, the system 900 may reside on a computing device such as the device 501 of FIG. 5 .
- the system 900 may have a raytracing engine 920 and a modeling engine 930 .
- the main lens design 910 , the sensor design 912 , and the samples 914 may be used by the raytracing engine 290 as inputs for an optical simulation process such as raytracing.
- the raytracing engine 920 may generate the raw mapping tables 922 .
- the raw mapping tables 922 may be specific to individual samples 914 .
- a raw mapping table 922 may be generated for each sample 914 .
- the raw mapping tables 922 may, collectively, be too large to be used in raw form. Thus, in at least one embodiment, they may advantageously be compressed and/or otherwise approximated to facilitate application to the light-field data 1310 .
- the raw mapping tables 922 may be received by the modeling engine 930 .
- the modeling engine 930 may perform one or more modeling procedures on the raw mapping tables 922 to mathematically approximate and/or compress the raw mapping tables 922 .
- the modeling engine 930 may provide the models 932 .
- the product calibration data 940 may be obtained using the models 932 . If desired, the product calibration data 940 may include the models 932 , indexed by the sample 914 to which they pertain. The manner in which the raytracing engine 920 and the modeling engine 930 operate will be disclosed in greater detail in connection with FIG. 10 .
- FIG. 10 is a flow diagram illustrating a method 1000 of using the system 900 of FIG. 9 to generate the product calibration data 940 .
- the method 1000 may start 1010 with a step 1020 in which the samples 914 are selected.
- the samples 914 may be selected at random, by a user, and/or in a manner that conforms to a matrix of evenly-spaced or otherwise distributed settings for the camera 800 .
- the system 900 may, in a step 1030 , receive the main lens design 910 and the sensor design 912 .
- Algorithm I may be performed, as follows:
- Algorithm I contains two main stages: raytracing (the step 1040 ) and modeling (the step 1050 ). Each of these is described in more detail below.
- step 1040 is to find accurate ideal 4D coordinates for each of the sensor locations on the image sensor 803 .
- the system 900 and method 1000 achieve this goal using an improved raytracing software engine, which may be part of the raytracing engine 920 .
- the raytracing engine 920 first constructs the optical system to be analyzed given the main lens design 910 , the sensor design 912 , and the current ⁇ zoom, focus ⁇ setting (the current sample 914 ).
- the main lens design 910 may include various lens surface properties (such as, for example, reflective indices for each wavelength, surface profiles, inter-surface distances, and the like).
- the sensor design 912 may include properties such as wavelength sensitivity, angular sensitivity, sensor area, inter-sensor spacing, and the like.
- a virtual target chart can be specified to be used as a reference object for backward tracing, as described below.
- the raytracing engine 920 estimates the (approximately) equivalent ideal, thin-lens, optical system from the design prescriptions and settings.
- the ideal system includes, for example, at least two parameters: the focal length and the lens-sensor distance.
- the first parameter may be estimated by tracing many parallel rays from infinity (in the world) toward the given lens system (into the camera), and finding the distance with minimal spatial coverage of the refracted rays.
- the second parameter may be estimated by first tracing many rays from a sensor point through the given lens system, finding the distance with minimal spatial coverage of the refracted rays, and then calculating the ideal image distance using a simple thin lens equation.
- the raytracing engine 920 may then perform a regular raytracing operation for each sensor.
- the raytracing engine 920 may shoot many rays from the sensor toward the world through the optical system and calculate the intersection with the target chart. As indicated in step 1.i d.i of Algorithm I, in one embodiment, the raytracing engine 920 may record the spatial intersection coordinates and incident angle for each ray.
- the raytracing engine 920 may then trace each ray backward from the chart through the ideal optical system to reach the image plane, and record the 4D light-field coordinates (as indicated in step 1.d.ii of Algorithm I). In at least one embodiment, in order to reduce the noise in numerical simulation, the system averages all coordinates together (step 1.d.iii in Algorithm I).
- the output of stage one is a set of high-resolution, raw, mapping tables (the raw mapping tables 922 ) that map each sensor (s, t) location to the ideal 4D coordinates (x′, y′, u′, v′) for each ⁇ zoom, focus ⁇ sample 914 .
- the mapping from (s, t) to (x, y, u, v) is straightforward in the ideal simulation environment and is therefore omitted in this description.
- the amount of data can be easily up to tens of gigabytes, making it different to transfer, load, or maintain. For example, for a 20-megapixel camera with 100 zoom and 100 focus settings, if each set of coordinates are stored using 4 floating point numbers, the amount of data for the correction table would be approximately 2.9 terabytes.
- the system 900 in order to reduce the amount of data, converts the raw mapping tables 922 to a more compact representation based on modeling. This may be done by the modeling engine 930 . As long as the error introduced by the modeling is small enough, the correction result is perceptually identical.
- the first approach uses the polynomial fitting:
- each raw mapping table 922 is smooth and can represented by a low-resolution 4D table T, and the ideal 4D coordinates can be approximated by:
- T[i, j, k, l] refers one four-value entry in the table indexed by discrete indices [i, j, k, l], and W is the weighting coefficient representing the importance of each entry in interpolating the output values.
- More complex interpolation functions such as cubic or Lanczos, can be used here in addition to or in the alternative to the foregoing.
- model parameters are saved in binary form for storage and transfer. These model parameters may be stored in the product calibration data 940 in a step 1060 . Besides the modeling technique described above, further data size reductions can be achieved by the use of well-known lossy or lossless compression techniques, such as quantization, LZW, or entropy coding. The method 1000 may then end 1090 .
- FIG. 11 is a schematic block diagram that illustrates a system 1100 for generating the unit calibration data 1140 according to one embodiment.
- the system 1100 may include any computing device; in some embodiments, the system 1100 may reside on a computing device such as the device 501 of FIG. 5 .
- the system 1100 may include a processing engine 1120 .
- the processing engine 1120 may receive the main lens design 910 , the sensor design 912 , and/or the samples 914 , and may process them to generate the mapping functions 1122 .
- the mapping functions 1122 may be specific to individual samples 914 .
- a mapping function 1122 may be generated for each sample 914 .
- the unit calibration data 1140 may be obtained using the mapping functions 1122 . If desired, the unit calibration data 1140 may include the mapping functions 1122 , indexed by the sample 914 to which they pertain. The manner in which the processing engine 1120 operates will be disclosed in greater detail in connection with FIG. 12 .
- FIG. 12 is a flow diagram illustrating a method 1200 of using the system 1100 of FIG. 11 to generate the unit calibration data 1140 .
- the method 1200 may start 1210 with a step 1220 in which the samples 914 are selected.
- the samples 914 may be selected at random, by a user, and/or in a manner that conforms to a matrix of evenly-spaced or otherwise distributed settings for the camera 800 . Additionally or alternatively, the samples 914 may be the same as those used in conjunction with the method 1000 for obtaining the product calibration data 940 .
- the method 1000 and the method 1200 need not be performed in any particular order relative to each other; rather, either may be performed before the other.
- the system 1100 may, in a step 1230 , receive the main lens design 910 and the sensor design 912 . Then, the processing engine 1120 may, in a step 1240 , generate the mapping function 1122 for each sample 914 , as follows.
- Each mapping function 1122 can includes any of a number of parameters, including the global translation and rotation between the microlens array 802 and the image sensor 803 , the distortion of the microlens array 802 , the local distortion individual microlenses of the microlens array 802 , and/or other variations due to manufacture.
- Mechanisms for representing and estimating g are described in related U.S. Utility application Ser. No. 13/774,971, for “Compensating for Variation in Microlens Position During Light-Field Image Processing”, filed Feb. 22, 2013, the disclosure of which is incorporated herein by reference in its entirety.
- the unit calibration data 1140 also uses the main lens design 910 ; thus, it may use one mapping function 1122 for each ⁇ zoom, focus ⁇ , i.e., each sample 914 .
- mapping functions 1122 may be stored in the unit calibration data 1140 in a step 1260 .
- the method 1200 may then end 1290 .
- FIG. 13 is a schematic block diagram that illustrates a system 1300 for correcting light-field data, such as the light-field data 1310 and/or conducting generalized subsequent processing, according to one embodiment.
- the system 1300 may include any computing device; in some embodiments, the system 1100 may reside on a computing device such as the device 501 of FIG. 5 . Alternatively or additionally, the system 1300 may reside on a camera 800 such as that of FIG. 2 or a post-processing device connected to a camera 800 such as that of FIG. 3 .
- the system 1300 may include an aberration correction engine 1320 and a further light-field processing engine 1330 .
- the aberration correction engine 1320 may receive the light-field data 1310 , the product calibration data 940 , and/or the unit calibration data 1140 , and may process them to generate the corrected light-field data 1322 , which has been corrected to remove and/or reduce the effects of aberrations.
- generation of the corrected light-field data 1322 by the aberration correction engine 1320 may include applying the product calibration data 940 to correct the light-field data 1310 to remove and/or reduce the effects of design departures, which are departures of the main lens design 910 and/or the sensor design 912 of a camera design from their ideal counterparts. Additionally or alternatively, generation of the corrected light-field data 1322 by the aberration correction engine 1320 may include applying the unit calibration data 1140 to correct the light-field data 1310 to remove and/or reduce the effects of manufacturing departures, which are departures of the actual main lens 813 and/or image sensor 803 of a particular camera 800 from the main lens design 910 and/or the sensor design 912 intended to be produced in that camera 800 .
- the further light-field processing engine 1330 may conduct one or more additional processing steps to create one or more light-field images 1332 from the light-field data 1322 .
- the further light-field processing engine 1330 may conduct additional processing steps to enhance and/or otherwise modify the resulting light-field images 1332 .
- the further light-field processing engine 1330 may perform functions that include, but are not limited to, modulation, demodulation, demosaicing, auto-white balancing, saturation recovery, and the like.
- the operation of the aberration correction engine 1320 and the further light-field processing engine 1330 will be described in greater detail in connection with FIG. 14 .
- FIG. 14 is a flow diagram illustrating a method 1400 of using the system 1300 of FIG. 13 to generate one or more light-field images 1332 .
- the method 1400 may optionally be performed after performance of the method 1000 of FIG. 10 and the method 1200 of FIG. 12 so that the method 1400 can utilize the product calibration data 940 and the unit calibration data 1140 .
- the method 1400 may start 1410 with a step 1420 in which the light-field data 1310 is received.
- the light-field data 1310 may, for example, be received from the image sensor 803 of the camera 800 .
- the product calibration data 940 may be received
- the unit calibration data 1140 may be received.
- the aberration correction engine 1320 may correct the light-field data 1310 to produce the corrected light-field data 1322 .
- a generalized version of the step 1450 can be described as follows:
- the most straightforward is to check the ⁇ zoom, focus ⁇ in the metadata for the light-field data 1310 , and find the portion of the product calibration data 940 with most similar ⁇ zoom, focus ⁇ value. This may entail locating, within the product calibration data 940 , the model 932 that corresponds to the ⁇ zoom, focus ⁇ value. Additionally or alternatively, one or more advanced techniques can be applied, such as fusing a few tables with similar ⁇ zoom, focus ⁇ values by weighted interpolation.
- the corrected light-field data 1322 may be used to generate one or more light-field images 1332 in the step 1460 .
- This may entail performing regular light-field processing.
- the further light-field processing engine 1330 may utilize a conventional light-field processing algorithm, except that the non-ideal four-dimensional coordinates from the light-field data 1310 may be replaced with ideal coordinates from the corrected light-field data 1322 , (obtained in step 3.b. in Algorithm II).
- Algorithm II may include any light-field processing stages or applications, including but not limited to digital refocusing, EDOF projection, and general projection, as described in related U.S. Utility application Ser. No. 13/688,026, for “Extended Depth of Field and Variable Center of Perspective in Light-Field Processing”, filed Nov. 28, 2012, the disclosure of which is incorporated herein by reference in its entirety. Additionally or alternatively, Algorithm II may include additional light-field processing operations, such as light-field filtering, denoising, light-field resampling, light-field compression, the aberration correction quality assessment (as described below), and the like.
- the one or more light-field images 1332 may be stored, for example in the data store 506 , and the method may end 1490 .
- FIG. 15 is a schematic block diagram that illustrates a system 1500 for assessing the quality of aberration correction.
- the system 1500 may be used to evaluate the corrected light-field data 1322 .
- the system 1500 may include any computing device; in some embodiments, the system 1500 may reside on a computing device such as the device 501 of FIG. 5 . Alternatively or additionally, the system 1500 may reside on a camera 800 such as that of FIG. 2 or a post-processing device connected to a camera 800 such as that of FIG. 3 .
- the system 1500 may utilize the further light-field processing engine 1330 , and may further have an epipolar image processing engine 1520 .
- the further light-field processing engine 1330 may be used to generate an epipolar image 1512 from the corrected light-field data 1322 .
- the product calibration data 940 and/or the unit calibration data 1140 may optionally be used.
- the epipolar image 1512 may be created, for example, by taking a vertical or horizontal “slice” of the light-field data.
- the light-field data may be expressed in terms of u, v, x, and y coordinates, as described previously in connection with FIG. 7 .
- a horizontal “slice” may be taken by keeping v and y (the vertical coordinates at the aperture plane and the image plane, respectively), constant.
- the resulting epipolar image 1512 may then have variation only in the x and u coordinates defining the horizontal slice.
- Vertical epipolar images 1512 may similarly be created.
- the epipolar image processing engine 1520 may receive the epipolar image 1512 , and may process the epipolar image 1512 to generate a correction score 1522 that indicates the quality of the aberration correction applied to the corrected light-field data 1322 .
- the operation of the epipolar image processing engine 1520 will be described in greater detail in connection with FIG. 16 .
- FIG. 16 is a flow diagram illustrating a method 1600 of using the system 1500 of FIG. 15 to generate the correction score 1522 .
- the method 1600 may optionally be performed after performance of the method 1400 of FIG. 14 so that the method 1600 can utilize the product calibration data 940 and the unit calibration data 1140 .
- the method may start 1610 and proceed as follows:
- the epipolar image 1512 may be a two-dimension slice of the four-dimensional light-field given one fixed image and one set of fixed aperture coordinates.
- (y′, v′) may be fixed to (0,0) (2.a.iii in Algorithm III).
- FIG. 17A illustrates an exemplary epipolar image 1700 generated from corrected light-field data 1322 .
- the key property of an epipolar image may be that, for a specific target with known distance and texture, its epipolar image with aberration correction may only contain straight edges with known slopes (the edges 1710 of FIG. 17 ). Any deviation from this may indicate that the aberration correction has not applied perfectly. Such a deviation may be, for example, a deviation in the straightness and/or slope of one or more of the edges in the epipolar image.
- the correction score 1522 may be based, at least in part, on the slopes and/or straightness levels of the edges in the epipolar image.
- the epipolar image may be analyzed by first running one or more common edge detection algorithms, and then measuring the slope and straightness of each edge (3 in Algorithm III). Finally, all slope error and straightness measurements may be combined together to compute the correction score 1522 .
- the edges 1710 of the epipolar image 1700 are relatively straight and are relatively consistently-sloped; accordingly, the epipolar image 1700 may receive a correction score 1522 that indicates that aberration correction has been successfully applied.
- FIG. 17B illustrates an exemplary epipolar image 1750 generated from light-field data 1310 that has not been corrected for aberrations.
- the epipolar image 1750 has edges 1760 that do not have the same level of straightness and/or consistency of slope as the edges 1710 of the epipolar image 1700 of FIG. 17A . Accordingly, the epipolar image 1750 may receive a correction score 1522 that indicates that aberration correction has not been successfully applied.
- Some embodiments may include a system or a method for performing the above-described techniques, either singly or in any combination.
- Other embodiments may include a computer program product comprising a non-transitory computer-readable storage medium and computer program code, encoded on the medium, for causing a processor in a computing device or other electronic device to perform the above-described techniques.
- Certain aspects include process steps and instructions described herein in the form of an algorithm. It should be noted that the process steps and instructions of described herein can be embodied in software, firmware and/or hardware, and when embodied in software, can be downloaded to reside on and be operated from different platforms used by a variety of operating systems.
- This apparatus may be specially constructed for the required purposes, or it may comprise a general-purpose computing device selectively activated or reconfigured by a computer program stored in the computing device.
- a computer program may be stored in a computer readable storage medium, such as, but is not limited to, any type of disk including floppy disks, optical disks, CD-ROMs, magnetic-optical disks, read-only memories (ROMs), random access memories (RAMs), EPROMs, EEPROMs, flash memory, solid state drives, magnetic or optical cards, application specific integrated circuits (ASICs), and/or any type of media suitable for storing electronic instructions, and each coupled to a computer system bus.
- the computing devices referred to herein may include a single processor or may be architectures employing multiple processor designs for increased computing capability.
- the techniques described herein can be implemented as software, hardware, and/or other elements for controlling a computer system, computing device, or other electronic device, or any combination or plurality thereof.
- Such an electronic device can include, for example, a processor, an input device (such as a keyboard, mouse, touchpad, trackpad, joystick, trackball, microphone, and/or any combination thereof), an output device (such as a screen, speaker, and/or the like), memory, long-term storage (such as magnetic storage, optical storage, and/or the like), and/or network connectivity, according to techniques that are well known in the art.
- Such an electronic device may be portable or nonportable.
- Examples of electronic devices that may be used for implementing the techniques described herein include: a mobile phone, personal digital assistant, smartphone, kiosk, server computer, enterprise computing device, desktop computer, laptop computer, tablet computer, consumer electronic device, television, set-top box, or the like.
- An electronic device for implementing the techniques described herein may use any operating system such as, for example: Linux; Microsoft Windows, available from Microsoft Corporation of Redmond, Wash.; Mac OS X, available from Apple Inc. of Cupertino, Calif.; iOS, available from Apple Inc. of Cupertino, Calif.; Android, available from Google, Inc. of Mountain View, Calif.; and/or any other operating system that is adapted for use on the device.
- the techniques described herein can be implemented in a distributed processing environment, networked computing environment, or web-based computing environment. Elements can be implemented on client computing devices, servers, routers, and/or other network or non-network components. In some embodiments, the techniques described herein are implemented using a client/server architecture, wherein some components are implemented on one or more client computing devices and other components are implemented on one or more servers. In one embodiment, in the course of implementing the techniques of the present disclosure, client(s) request content from server(s), and server(s) return content in response to the requests.
- a browser may be installed at the client computing device for enabling such requests and responses, and for providing a user interface by which the user can initiate and control such interactions and view the presented content.
- Any or all of the network components for implementing the described technology may, in some embodiments, be communicatively coupled with one another using any suitable electronic network, whether wired or wireless or any combination thereof, and using any suitable protocols for enabling such communication.
- a network is the Internet, although the techniques described herein can be implemented using other networks as well.
Abstract
Description
-
- Given the target lens configuration, the system finds the compact aberration correction data with similar or identical configurations. In at least one embodiment, the system can interpolate among two or more candidates.
- Given the target camera configuration, the system converts the compact aberration correction data to the high-resolution aberration correction data that matches the camera configuration. In at least one embodiment, this step includes 4D table resampling and back-projection.
-
- Generate the EPI from the aberration corrected light-field.
- Identify the strong edges in the EPI.
- Fit each edge with a line equation and measure the error. When the error is large, it means the edge is “curved” in the x-u space, and the aberration was not perfectly corrected.
-
- The per-product and per-unit one-time offline processing can significantly reduce the computation required for each captured light-field.
- Modeling and retargeting the compact aberration correction data significantly reduce the amount of data to transfer. The system described herein can avoid the need to estimate the full high-resolution aberration correction data for each assembled camera at each configuration.
-
- aberration: an accidentally or deliberately produced feature of a main lens that results in departure of the main lens from the characteristics of an ideal lens.
- correction: a step taken to at least partially compensate for the effects of an aberration.
- disk: a region in a light-field image that is illuminated by light passing through a single microlens; may be circular or any other suitable shape.
- extended depth of field (EDOF) image: an image that has been processed to have objects in focus along a greater depth range.
- image: a two-dimensional array of pixel values, or pixels, each specifying a color.
- image processing algorithm: any computer-implemented procedure for modifying an image.
- light-field data: data that describes the properties of a light-field, typically captured by a light-field capture device.
- light-field image: an image that contains a representation of light-field data captured at the sensor.
- main lens: the optical structure or structures through which light enters a camera, prior to impinging on a microlens array or sensor.
- mapping: a table, function, or other data set or mathematical structure that provides one or more output values for each of a plurality of possible input values or combinations of input values.
- microlens: a small lens, typically one in an array of similar microlenses.
- modeling: a process by which a table, function, or other data set or mathematical structure is created to approximate the operation of a different system, data set, or mathematical structure.
- phase mask: a camera component used in conjunction with a main lens to impart a phase shift on the wavefront of light.
- ray correction function: a function that converts between four-dimensional light ray coordinates in actual image space corresponding to the use of an actual main lens, and four-dimensional light ray coordinates in ideal space corresponding to the use of an ideal main lens.
- raytracing: a method of modeling light by which individual bundles of light are projected through a virtual space.
- sensor location: a specific position on a sensor such as an image sensor, typically defined in Cartesian coordinates.
- weight: a numerical representation of the importance accorded to a specific component in the construction of a combination of components.
(x, y, u, v)=ƒ(x′, y′, u′, v′, λ, P)
where λ is the wavelength, and P includes all other configurable parameters of the system.
Aberration Correction
(x′, y′, u′, v′)=b(x, y, u, v, λ, P)
-
- Input:
Main lens design 910,sensor design 912, all {zoom, focus} samples (i.e., the samples 914). - Output: Compact reverse mapping model (the models 932) for each {zoom, focus} sample
- 1. In a
step 1040, Raytracing: For each {zoom, focus} sample:- a. Construct the full light-field camera optical system
- b. Set a flat virtual target chart at a fixed distance
- c. Estimate the equivalent ideal lens model, including the focal length and lens-sensor distance.
- d. For each sensor (s, t) (i.e., each sensor location):
- i. Trace a number of rays {i} through the camera to hit the target chart, record the intersection location (xt,i, yt,i) and incident angle (ut,i, vt,i).
- ii. Trace each ray back the target through the ideal thin lens model to hit the lens and the sensor. Record the aperture intersection coordinates (u′i, v′i) and the sensor intersection coordinates (x′i, y′i).
- iii. Average all (x′i, y′i, u′i, v′i) into (x′, y′, u′, v′)
- iv. Save raw mapping table r(s, t)=(x′, y′, u′, v′)
- 2. In a
step 1050, Modeling: For each raw mapping table r{zoom, focus}:- a. Estimate the model parameters from r{zoom, focus}.
- b. Save the model parameters
- Input:
where {αi,j,k,l, βi,j,k,l, γi,j,k,l, δi,j,k,l} are the polynomial coefficients. The value of those coefficients can be estimated by minimizing the following energy function:
which can be easily done by using most optimization software packages. This fitting reduces the data size from full sensor resolution to 4KxKyKuKv for each {zoom, focus} sample.
W(x, y, u, v, i, j, k, l)=W(x, i)W(y, j)W(u, k)W(v, l),
where each one can be a simple bi-linear kernel. For example,
W(x, i)=max(1−|x−S x(i)|,0),
where Sx(i) maps the table index i to the spatial coordinates. The formulas for the other three functions are similar. More complex interpolation functions, such as cubic or Lanczos, can be used here in addition to or in the alternative to the foregoing.
-
- Input: Light-
field data 1310 and associated metadata,unit calibration data 1140, andproduct calibration data 940 - Output: Corrected light-
field data 1322 and associated metadata - 1. Use the lens configuration in the input metadata to construct the reverse mapping function b.
- 2. Use the input metadata to construct the per-unit mapping function g.
- 3. For each query of sample (s, t):
- a. Compute (x, y, u, v)=g(s, t).
- b. Compute (x′, y′, u′, v′)=b(x, y, u, v). This may complete generation of the corrected light-
field data 1322. - c. Use (x′, y′, u′, v′) as the 4D coordinates in the following processing. The previous steps may complete the
step 1450; accordingly, this step 1.c. may constitute performance of thestep 1460.
- Input: Light-
-
- Input: Corrected light-
field data 1322 and metadata of a specific test target,unit calibration data 1140,product calibration data 940. - Output:
correction score 1522 - 1. Follow step 1 and 2 in Algorithm II to generate b and g.
- 2. In a
step 1620, generate the EPI E with aberration correction- a. For each sample (s, t)
- i. Compute (x, y, u, v)=g(s, t)
- ii. Compute (x′, y′, u′, v′)=b(x, y, u, v)
- iii. If (y′, v′)=(0,0), set E(x′, u′)=I(s, t)
- iv. In a
step 1630, identify the location of strong edges in E, for each edge
- b. In a
step 1640, compute the slope 1 and the error to the expected slope as e(l) - c. In a
step 1650, compute straightness of each edge h.
- a. For each sample (s, t)
- 3. In a
step 1660, compute the correction score as sum of −e(l) and h for all edges
- Input: Corrected light-
Claims (27)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/573,319 US9628684B2 (en) | 2013-12-24 | 2014-12-17 | Light-field aberration correction |
Applications Claiming Priority (3)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US201361920709P | 2013-12-24 | 2013-12-24 | |
US201361920710P | 2013-12-24 | 2013-12-24 | |
US14/573,319 US9628684B2 (en) | 2013-12-24 | 2014-12-17 | Light-field aberration correction |
Publications (2)
Publication Number | Publication Date |
---|---|
US20150178923A1 US20150178923A1 (en) | 2015-06-25 |
US9628684B2 true US9628684B2 (en) | 2017-04-18 |
Family
ID=53400567
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/573,651 Active 2035-03-13 US9392153B2 (en) | 2013-12-24 | 2014-12-17 | Plenoptic camera resolution |
US14/573,319 Active 2035-05-25 US9628684B2 (en) | 2013-12-24 | 2014-12-17 | Light-field aberration correction |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US14/573,651 Active 2035-03-13 US9392153B2 (en) | 2013-12-24 | 2014-12-17 | Plenoptic camera resolution |
Country Status (4)
Country | Link |
---|---|
US (2) | US9392153B2 (en) |
JP (1) | JP6228300B2 (en) |
DE (1) | DE112014005866B4 (en) |
WO (1) | WO2015100105A1 (en) |
Cited By (24)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20160062100A1 (en) * | 2014-08-26 | 2016-03-03 | The Board Of Trustees Of The Leland Stanford Junior University | Light-field microscopy with phase masking |
US10205896B2 (en) | 2015-07-24 | 2019-02-12 | Google Llc | Automatic lens flare detection and correction for light-field images |
US10275892B2 (en) | 2016-06-09 | 2019-04-30 | Google Llc | Multi-view scene segmentation and propagation |
US10275898B1 (en) | 2015-04-15 | 2019-04-30 | Google Llc | Wedge-based light-field video capture |
US10298834B2 (en) | 2006-12-01 | 2019-05-21 | Google Llc | Video refocusing |
US10334151B2 (en) | 2013-04-22 | 2019-06-25 | Google Llc | Phase detection autofocus using subaperture images |
US10341632B2 (en) | 2015-04-15 | 2019-07-02 | Google Llc. | Spatial random access enabled video system with a three-dimensional viewing volume |
US10354399B2 (en) | 2017-05-25 | 2019-07-16 | Google Llc | Multi-view back-projection to a light-field |
US10412373B2 (en) | 2015-04-15 | 2019-09-10 | Google Llc | Image capture for virtual reality displays |
US10419737B2 (en) | 2015-04-15 | 2019-09-17 | Google Llc | Data structures and delivery methods for expediting virtual reality playback |
US10440407B2 (en) | 2017-05-09 | 2019-10-08 | Google Llc | Adaptive control for immersive experience delivery |
US10444931B2 (en) | 2017-05-09 | 2019-10-15 | Google Llc | Vantage generation and interactive playback |
US10469873B2 (en) | 2015-04-15 | 2019-11-05 | Google Llc | Encoding and decoding virtual reality video |
US10474227B2 (en) | 2017-05-09 | 2019-11-12 | Google Llc | Generation of virtual reality with 6 degrees of freedom from limited viewer data |
US10540818B2 (en) | 2015-04-15 | 2020-01-21 | Google Llc | Stereo image generation and interactive playback |
US10545215B2 (en) | 2017-09-13 | 2020-01-28 | Google Llc | 4D camera tracking and optical stabilization |
US10546424B2 (en) | 2015-04-15 | 2020-01-28 | Google Llc | Layered content delivery for virtual and augmented reality experiences |
US10552947B2 (en) | 2012-06-26 | 2020-02-04 | Google Llc | Depth-based image blurring |
US10567464B2 (en) | 2015-04-15 | 2020-02-18 | Google Llc | Video compression with adaptive view-dependent lighting removal |
US10565734B2 (en) | 2015-04-15 | 2020-02-18 | Google Llc | Video capture, processing, calibration, computational fiber artifact removal, and light-field pipeline |
US10594945B2 (en) | 2017-04-03 | 2020-03-17 | Google Llc | Generating dolly zoom effect using light field image data |
US10679361B2 (en) | 2016-12-05 | 2020-06-09 | Google Llc | Multi-view rotoscope contour propagation |
US10965862B2 (en) | 2018-01-18 | 2021-03-30 | Google Llc | Multi-camera navigation interface |
US11328446B2 (en) | 2015-04-15 | 2022-05-10 | Google Llc | Combining light-field data with active depth data for depth map generation |
Families Citing this family (27)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9542742B2 (en) * | 2014-01-30 | 2017-01-10 | Ricoh Company, Ltd. | Estimation of the system transfer function for certain linear systems |
US9305375B2 (en) * | 2014-03-25 | 2016-04-05 | Lytro, Inc. | High-quality post-rendering depth blur |
US9544583B2 (en) * | 2015-01-09 | 2017-01-10 | Ricoh Company, Ltd. | Object space calibration of plenoptic imaging systems |
NZ773822A (en) | 2015-03-16 | 2022-07-29 | Magic Leap Inc | Methods and systems for diagnosing and treating health ailments |
EP3094076A1 (en) * | 2015-05-13 | 2016-11-16 | Thomson Licensing | Method for obtaining a refocused image from a 4D raw light field data using a shift correction parameter |
FR3038193B1 (en) * | 2015-06-26 | 2018-07-13 | Commissariat A L'energie Atomique Et Aux Energies Alternatives | METHOD FOR DESIGNING AN IMAGING SYSTEM, SPATIAL FILTER AND IMAGING SYSTEM COMPRISING SUCH A SPATIAL FILTER |
CN106254750B (en) * | 2015-08-31 | 2019-04-26 | 北京智谷睿拓技术服务有限公司 | Image Acquisition control method and device |
DE102015011427B4 (en) | 2015-09-01 | 2019-01-17 | Thomas Engel | Image acquisition system and image evaluation system |
EP3440497B1 (en) | 2016-04-08 | 2023-08-16 | Magic Leap, Inc. | Augmented reality systems and methods with variable focus lens elements |
JP7120929B2 (en) * | 2016-06-07 | 2022-08-17 | エアリー３ディー インコーポレイティド | Light field imaging device and method for depth acquisition and 3D imaging |
NL2018914A (en) | 2016-06-09 | 2017-12-13 | Asml Netherlands Bv | Projection System Modelling Method |
WO2018022521A1 (en) | 2016-07-25 | 2018-02-01 | Magic Leap, Inc. | Light field processor system |
IL301881B1 (en) | 2017-02-23 | 2024-04-01 | Magic Leap Inc | Display system with variable power reflector |
CN106803892B (en) * | 2017-03-13 | 2019-12-03 | 中国科学院光电技术研究所 | A kind of light field high-resolution imaging method based on Optical field measurement |
DE112018002670T5 (en) | 2017-05-24 | 2020-03-05 | The Trustees Of Columbia University In The City Of New York | Broadband achromatic flat optical components due to dispersion-technical dielectric meta-surfaces |
US10795168B2 (en) | 2017-08-31 | 2020-10-06 | Metalenz, Inc. | Transmissive metasurface lens integration |
US11089265B2 (en) | 2018-04-17 | 2021-08-10 | Microsoft Technology Licensing, Llc | Telepresence devices operation methods |
JP7281901B2 (en) * | 2018-12-27 | 2023-05-26 | 東京エレクトロン株式会社 | SUBSTRATE PROCESSING APPARATUS AND SUBSTRATE PROCESSING METHOD |
CN109801273B (en) * | 2019-01-08 | 2022-11-01 | 华侨大学 | Light field image quality evaluation method based on polar plane linear similarity |
US11270464B2 (en) | 2019-07-18 | 2022-03-08 | Microsoft Technology Licensing, Llc | Dynamic detection and correction of light field camera array miscalibration |
US11064154B2 (en) | 2019-07-18 | 2021-07-13 | Microsoft Technology Licensing, Llc | Device pose detection and pose-related image capture and processing for light field based telepresence communications |
US11553123B2 (en) | 2019-07-18 | 2023-01-10 | Microsoft Technology Licensing, Llc | Dynamic detection and correction of light field camera array miscalibration |
US11082659B2 (en) * | 2019-07-18 | 2021-08-03 | Microsoft Technology Licensing, Llc | Light field camera modules and light field camera module arrays |
KR20220035971A (en) | 2019-07-26 | 2022-03-22 | 메탈렌츠 인코포레이티드 | Aperture-Metasurface and Hybrid Refractive-Metasurface Imaging Systems |
US20220283431A1 (en) * | 2019-08-12 | 2022-09-08 | Arizona Board Of Regents On Behalf Of The University Of Arizona | Optical design and optimization techniques for 3d light field displays |
US11927769B2 (en) | 2022-03-31 | 2024-03-12 | Metalenz, Inc. | Polarization sorting metasurface microlens array device |
CN114494258B (en) * | 2022-04-15 | 2022-08-30 | 清华大学 | Lens aberration prediction and image reconstruction method and device |
Citations (26)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6466207B1 (en) | 1998-03-18 | 2002-10-15 | Microsoft Corporation | Real-time image rendering with layered depth images |
US20050031203A1 (en) | 2003-08-08 | 2005-02-10 | Hiroaki Fukuda | Image processing apparatus, an image forming apparatus and an image processing method |
US20070071316A1 (en) | 2005-09-27 | 2007-03-29 | Fuji Photo Film Co., Ltd. | Image correcting method and image correcting system |
US20070230944A1 (en) | 2006-04-04 | 2007-10-04 | Georgiev Todor G | Plenoptic camera |
US20070252074A1 (en) | 2004-10-01 | 2007-11-01 | The Board Of Trustees Of The Leland Stanford Junio | Imaging Arrangements and Methods Therefor |
US7336430B2 (en) | 2004-09-03 | 2008-02-26 | Micron Technology, Inc. | Extended depth of field using a multi-focal length lens with a controlled range of spherical aberration and a centrally obscured aperture |
US20080131019A1 (en) | 2006-12-01 | 2008-06-05 | Yi-Ren Ng | Interactive Refocusing of Electronic Images |
US20090128669A1 (en) | 2006-02-07 | 2009-05-21 | Yi-Ren Ng | Correction of optical aberrations |
US7623726B1 (en) | 2005-11-30 | 2009-11-24 | Adobe Systems, Incorporated | Method and apparatus for using a virtual camera to dynamically refocus a digital image |
US20100141802A1 (en) | 2008-12-08 | 2010-06-10 | Timothy Knight | Light Field Data Acquisition Devices, and Methods of Using and Manufacturing Same |
US7949252B1 (en) | 2008-12-11 | 2011-05-24 | Adobe Systems Incorporated | Plenoptic camera with large depth of field |
US8189089B1 (en) | 2009-01-20 | 2012-05-29 | Adobe Systems Incorporated | Methods and apparatus for reducing plenoptic camera artifacts |
US8264546B2 (en) | 2008-11-28 | 2012-09-11 | Sony Corporation | Image processing system for estimating camera parameters |
US8290358B1 (en) | 2007-06-25 | 2012-10-16 | Adobe Systems Incorporated | Methods and apparatus for light-field imaging |
US20120287296A1 (en) | 2011-05-10 | 2012-11-15 | Canon Kabushiki Kaisha | Imaging apparatus, method of controlling the same, and program |
US8427548B2 (en) | 2008-06-18 | 2013-04-23 | Samsung Electronics Co., Ltd. | Apparatus and method for capturing digital images |
US20130113981A1 (en) | 2006-12-01 | 2013-05-09 | Lytro, Inc. | Light field camera image, file and configuration data, and methods of using, storing and communicating same |
US8442397B2 (en) | 2009-09-22 | 2013-05-14 | Samsung Electronics Co., Ltd. | Modulator, apparatus for obtaining light field data using modulator, and apparatus and method for processing light field data using modulator |
US20130222606A1 (en) | 2012-02-28 | 2013-08-29 | Lytro, Inc. | Compensating for variation in microlens position during light-field image processing |
US20130222652A1 (en) | 2012-02-28 | 2013-08-29 | Lytro, Inc. | Compensating for sensor saturation and microlens modulation during light-field image processing |
US8570426B2 (en) | 2008-11-25 | 2013-10-29 | Lytro, Inc. | System of and method for video refocusing |
US20130286236A1 (en) | 2012-04-27 | 2013-10-31 | Research In Motion Limited | System and method of adjusting camera image data |
US20140146201A1 (en) | 2012-05-09 | 2014-05-29 | Lytro, Inc. | Optimization of optical systems for improved light field capture and manipulation |
US8749620B1 (en) | 2010-02-20 | 2014-06-10 | Lytro, Inc. | 3D light field cameras, images and files, and methods of using, operating, processing and viewing same |
US20140176592A1 (en) | 2011-02-15 | 2014-06-26 | Lytro, Inc. | Configuring two-dimensional image processing based on light-field parameters |
US8811769B1 (en) | 2012-02-28 | 2014-08-19 | Lytro, Inc. | Extended depth of field and variable center of perspective in light-field processing |
Family Cites Families (8)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
EP1584067A2 (en) * | 2003-01-16 | 2005-10-12 | D-blur Technologies LTD. C/o Yossi Haimov CPA | Camera with image enhancement functions |
US20100265385A1 (en) | 2009-04-18 | 2010-10-21 | Knight Timothy J | Light Field Camera Image, File and Configuration Data, and Methods of Using, Storing and Communicating Same |
US20120249550A1 (en) | 2009-04-18 | 2012-10-04 | Lytro, Inc. | Selective Transmission of Image Data Based on Device Attributes |
ATE551841T1 (en) | 2009-04-22 | 2012-04-15 | Raytrix Gmbh | DIGITAL IMAGING METHOD FOR SYNTHESIZING AN IMAGE USING DATA RECORDED BY A PLENOPTIC CAMERA |
KR101608970B1 (en) | 2009-11-27 | 2016-04-05 | 삼성전자주식회사 | Apparatus and method for processing image using light field data |
US8649094B2 (en) * | 2010-05-21 | 2014-02-11 | Eastman Kodak Company | Low thermal stress birefringence imaging lens |
US8531581B2 (en) * | 2011-05-23 | 2013-09-10 | Ricoh Co., Ltd. | Focusing and focus metrics for a plenoptic imaging system |
US8978981B2 (en) * | 2012-06-27 | 2015-03-17 | Honeywell International Inc. | Imaging apparatus having imaging lens |
-
2014
- 2014-12-17 US US14/573,651 patent/US9392153B2/en active Active
- 2014-12-17 US US14/573,319 patent/US9628684B2/en active Active
- 2014-12-17 JP JP2016525068A patent/JP6228300B2/en active Active
- 2014-12-17 WO PCT/US2014/070880 patent/WO2015100105A1/en active Application Filing
- 2014-12-17 DE DE112014005866.1T patent/DE112014005866B4/en active Active
Patent Citations (35)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6466207B1 (en) | 1998-03-18 | 2002-10-15 | Microsoft Corporation | Real-time image rendering with layered depth images |
US20050031203A1 (en) | 2003-08-08 | 2005-02-10 | Hiroaki Fukuda | Image processing apparatus, an image forming apparatus and an image processing method |
US7336430B2 (en) | 2004-09-03 | 2008-02-26 | Micron Technology, Inc. | Extended depth of field using a multi-focal length lens with a controlled range of spherical aberration and a centrally obscured aperture |
US7936392B2 (en) | 2004-10-01 | 2011-05-03 | The Board Of Trustees Of The Leland Stanford Junior University | Imaging arrangements and methods therefor |
US20070252074A1 (en) | 2004-10-01 | 2007-11-01 | The Board Of Trustees Of The Leland Stanford Junio | Imaging Arrangements and Methods Therefor |
US20070071316A1 (en) | 2005-09-27 | 2007-03-29 | Fuji Photo Film Co., Ltd. | Image correcting method and image correcting system |
US7623726B1 (en) | 2005-11-30 | 2009-11-24 | Adobe Systems, Incorporated | Method and apparatus for using a virtual camera to dynamically refocus a digital image |
US20100026852A1 (en) | 2006-02-07 | 2010-02-04 | Yi-Ren Ng | Variable imaging arrangements and methods therefor |
US20090128669A1 (en) | 2006-02-07 | 2009-05-21 | Yi-Ren Ng | Correction of optical aberrations |
US7620309B2 (en) | 2006-04-04 | 2009-11-17 | Adobe Systems, Incorporated | Plenoptic camera |
US20070230944A1 (en) | 2006-04-04 | 2007-10-04 | Georgiev Todor G | Plenoptic camera |
US20080131019A1 (en) | 2006-12-01 | 2008-06-05 | Yi-Ren Ng | Interactive Refocusing of Electronic Images |
US8559705B2 (en) | 2006-12-01 | 2013-10-15 | Lytro, Inc. | Interactive refocusing of electronic images |
US20130113981A1 (en) | 2006-12-01 | 2013-05-09 | Lytro, Inc. | Light field camera image, file and configuration data, and methods of using, storing and communicating same |
US8290358B1 (en) | 2007-06-25 | 2012-10-16 | Adobe Systems Incorporated | Methods and apparatus for light-field imaging |
US8427548B2 (en) | 2008-06-18 | 2013-04-23 | Samsung Electronics Co., Ltd. | Apparatus and method for capturing digital images |
US8570426B2 (en) | 2008-11-25 | 2013-10-29 | Lytro, Inc. | System of and method for video refocusing |
US8264546B2 (en) | 2008-11-28 | 2012-09-11 | Sony Corporation | Image processing system for estimating camera parameters |
US8289440B2 (en) | 2008-12-08 | 2012-10-16 | Lytro, Inc. | Light field data acquisition devices, and methods of using and manufacturing same |
US20120327222A1 (en) | 2008-12-08 | 2012-12-27 | Lytro, Inc. | Light Field Data Acquisition |
US8724014B2 (en) | 2008-12-08 | 2014-05-13 | Lytro, Inc. | Light field data acquisition |
US20100141802A1 (en) | 2008-12-08 | 2010-06-10 | Timothy Knight | Light Field Data Acquisition Devices, and Methods of Using and Manufacturing Same |
US7949252B1 (en) | 2008-12-11 | 2011-05-24 | Adobe Systems Incorporated | Plenoptic camera with large depth of field |
US20130128081A1 (en) | 2009-01-20 | 2013-05-23 | Todor G. Georgiev | Methods and Apparatus for Reducing Plenoptic Camera Artifacts |
US8189089B1 (en) | 2009-01-20 | 2012-05-29 | Adobe Systems Incorporated | Methods and apparatus for reducing plenoptic camera artifacts |
US8442397B2 (en) | 2009-09-22 | 2013-05-14 | Samsung Electronics Co., Ltd. | Modulator, apparatus for obtaining light field data using modulator, and apparatus and method for processing light field data using modulator |
US8749620B1 (en) | 2010-02-20 | 2014-06-10 | Lytro, Inc. | 3D light field cameras, images and files, and methods of using, operating, processing and viewing same |
US20140176592A1 (en) | 2011-02-15 | 2014-06-26 | Lytro, Inc. | Configuring two-dimensional image processing based on light-field parameters |
US20120287296A1 (en) | 2011-05-10 | 2012-11-15 | Canon Kabushiki Kaisha | Imaging apparatus, method of controlling the same, and program |
US20130222606A1 (en) | 2012-02-28 | 2013-08-29 | Lytro, Inc. | Compensating for variation in microlens position during light-field image processing |
US20130222652A1 (en) | 2012-02-28 | 2013-08-29 | Lytro, Inc. | Compensating for sensor saturation and microlens modulation during light-field image processing |
US8811769B1 (en) | 2012-02-28 | 2014-08-19 | Lytro, Inc. | Extended depth of field and variable center of perspective in light-field processing |
US8831377B2 (en) | 2012-02-28 | 2014-09-09 | Lytro, Inc. | Compensating for variation in microlens position during light-field image processing |
US20130286236A1 (en) | 2012-04-27 | 2013-10-31 | Research In Motion Limited | System and method of adjusting camera image data |
US20140146201A1 (en) | 2012-05-09 | 2014-05-29 | Lytro, Inc. | Optimization of optical systems for improved light field capture and manipulation |
Non-Patent Citations (7)
Title |
---|
"Raytrix Lightfield Camera", Raytrix GmbH, Germany 2012, pp. 1-35. |
Adaptive optics: http://http://en.wikipedia.org/wiki/Adaptive-optics. Retrieved Feb. 2014. |
Adaptive optics: http://http://en.wikipedia.org/wiki/Adaptive—optics. Retrieved Feb. 2014. |
Dowski, Jr., Edward R., "Extended depth of field through wave-front coding", Applied Optics, vol. 34, No. 11, Apr. 10, 1995, pp. 1859-1866. |
Georgiev, Todor, et al., "Superresolution with Plenoptic 2.0 Cameras", Optical Society of America 2009, pp. 1-3. |
Heide, Felix, et al., "High-Quality Computational Imaging Through Simple Lenses", ACM Transactions on Graphics, SIGGRAPH 2013, pp. 1-7. |
Levoy, Marc, "Light Field Photography and Videography", Oct. 18, 2005. |
Cited By (25)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10298834B2 (en) | 2006-12-01 | 2019-05-21 | Google Llc | Video refocusing |
US10552947B2 (en) | 2012-06-26 | 2020-02-04 | Google Llc | Depth-based image blurring |
US10334151B2 (en) | 2013-04-22 | 2019-06-25 | Google Llc | Phase detection autofocus using subaperture images |
US10317597B2 (en) * | 2014-08-26 | 2019-06-11 | The Board Of Trustees Of The Leland Stanford Junior University | Light-field microscopy with phase masking |
US20160062100A1 (en) * | 2014-08-26 | 2016-03-03 | The Board Of Trustees Of The Leland Stanford Junior University | Light-field microscopy with phase masking |
US10540818B2 (en) | 2015-04-15 | 2020-01-21 | Google Llc | Stereo image generation and interactive playback |
US10469873B2 (en) | 2015-04-15 | 2019-11-05 | Google Llc | Encoding and decoding virtual reality video |
US10341632B2 (en) | 2015-04-15 | 2019-07-02 | Google Llc. | Spatial random access enabled video system with a three-dimensional viewing volume |
US10565734B2 (en) | 2015-04-15 | 2020-02-18 | Google Llc | Video capture, processing, calibration, computational fiber artifact removal, and light-field pipeline |
US10412373B2 (en) | 2015-04-15 | 2019-09-10 | Google Llc | Image capture for virtual reality displays |
US10419737B2 (en) | 2015-04-15 | 2019-09-17 | Google Llc | Data structures and delivery methods for expediting virtual reality playback |
US10567464B2 (en) | 2015-04-15 | 2020-02-18 | Google Llc | Video compression with adaptive view-dependent lighting removal |
US10275898B1 (en) | 2015-04-15 | 2019-04-30 | Google Llc | Wedge-based light-field video capture |
US11328446B2 (en) | 2015-04-15 | 2022-05-10 | Google Llc | Combining light-field data with active depth data for depth map generation |
US10546424B2 (en) | 2015-04-15 | 2020-01-28 | Google Llc | Layered content delivery for virtual and augmented reality experiences |
US10205896B2 (en) | 2015-07-24 | 2019-02-12 | Google Llc | Automatic lens flare detection and correction for light-field images |
US10275892B2 (en) | 2016-06-09 | 2019-04-30 | Google Llc | Multi-view scene segmentation and propagation |
US10679361B2 (en) | 2016-12-05 | 2020-06-09 | Google Llc | Multi-view rotoscope contour propagation |
US10594945B2 (en) | 2017-04-03 | 2020-03-17 | Google Llc | Generating dolly zoom effect using light field image data |
US10440407B2 (en) | 2017-05-09 | 2019-10-08 | Google Llc | Adaptive control for immersive experience delivery |
US10474227B2 (en) | 2017-05-09 | 2019-11-12 | Google Llc | Generation of virtual reality with 6 degrees of freedom from limited viewer data |
US10444931B2 (en) | 2017-05-09 | 2019-10-15 | Google Llc | Vantage generation and interactive playback |
US10354399B2 (en) | 2017-05-25 | 2019-07-16 | Google Llc | Multi-view back-projection to a light-field |
US10545215B2 (en) | 2017-09-13 | 2020-01-28 | Google Llc | 4D camera tracking and optical stabilization |
US10965862B2 (en) | 2018-01-18 | 2021-03-30 | Google Llc | Multi-camera navigation interface |
Also Published As
Publication number | Publication date |
---|---|
US9392153B2 (en) | 2016-07-12 |
DE112014005866T5 (en) | 2016-11-10 |
DE112014005866B4 (en) | 2018-08-02 |
WO2015100105A1 (en) | 2015-07-02 |
JP2017504082A (en) | 2017-02-02 |
US20150181091A1 (en) | 2015-06-25 |
JP6228300B2 (en) | 2017-11-08 |
US20150178923A1 (en) | 2015-06-25 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US9628684B2 (en) | Light-field aberration correction | |
US9420276B2 (en) | Calibration of light-field camera geometry via robust fitting | |
US10205896B2 (en) | Automatic lens flare detection and correction for light-field images | |
US9172853B2 (en) | Microlens array architecture for avoiding ghosting in projected images | |
US10346997B2 (en) | Depth estimation method based on light-field data distribution | |
US10334151B2 (en) | Phase detection autofocus using subaperture images | |
US9900510B1 (en) | Motion blur for light-field images | |
CN109477710B (en) | Reflectance map estimation for point-based structured light systems | |
US10529060B2 (en) | Time-of-flight measuring apparatus and image processing method for reducing blur of depth image therein | |
US8405742B2 (en) | Processing images having different focus | |
US9235063B2 (en) | Lens modeling | |
CN101341733B (en) | Single-image vignetting correction | |
JP2013531268A (en) | Measuring distance using coded aperture | |
CN111047650B (en) | Parameter calibration method for time-of-flight camera | |
JPWO2019176349A1 (en) | Image processing device, imaging device, and image processing method | |
FR2996925A1 (en) | METHOD FOR DESIGNING A PASSIVE MONOVOIE IMAGER CAPABLE OF ESTIMATING DEPTH | |
JP2019175283A (en) | Recognition apparatus, recognition system, program, and position coordinate detecting method | |
CN113272855A (en) | Response normalization for overlapping multi-image applications | |
JP6882266B2 (en) | Devices and methods for generating data representing pixel beams | |
US9582887B2 (en) | Methods and apparatus for determining field of view dependent depth map correction values | |
CN105824027B (en) | Optical ranging method and optical ranging system | |
WO2024101429A1 (en) | Camera parameter calculation device, camera parameter calculation method, and camera parameter calculation program | |
US20240126952A1 (en) | Arithmetic operation system, training method, and non-transitory computer readable medium storing training program | |
US20240126953A1 (en) | Arithmetic operation system, training method, and non-transitory computer readable medium storing training program | |
US20220408013A1 (en) | DNN Assisted Object Detection and Image Optimization |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: LYTRO, INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:LIANG, CHIA-KAI;PITTS, COLVIN;CRADDOCK, CARL (WARREN);AND OTHERS;REEL/FRAME:034528/0570Effective date: 20141215 |
|
AS | Assignment |
Owner name: TRIPLEPOINT CAPITAL LLC (GRANTEE), CALIFORNIAFree format text: SECURITY INTEREST;ASSIGNOR:LYTRO, INC. (GRANTOR);REEL/FRAME:036167/0081Effective date: 20150407 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
CC | Certificate of correction | ||
FEPP | Fee payment procedure |
Free format text: ENTITY STATUS SET TO UNDISCOUNTED (ORIGINAL EVENT CODE: BIG.); ENTITY STATUS OF PATENT OWNER: LARGE ENTITY |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:LYTRO, INC.;REEL/FRAME:050009/0829Effective date: 20180325 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 4 |