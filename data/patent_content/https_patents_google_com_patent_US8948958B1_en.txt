US8948958B1 - Estimating road lane geometry using lane marker observations - Google Patents
Estimating road lane geometry using lane marker observations Download PDFInfo
- Publication number
- US8948958B1 US8948958B1 US14/270,653 US201414270653A US8948958B1 US 8948958 B1 US8948958 B1 US 8948958B1 US 201414270653 A US201414270653 A US 201414270653A US 8948958 B1 US8948958 B1 US 8948958B1
- Authority
- US
- United States
- Prior art keywords
- location
- edge
- lane
- estimate
- vehicle
- Prior art date
- Legal status (The legal status is an assumption and is not a legal conclusion. Google has not performed a legal analysis and makes no representation as to the accuracy of the status listed.)
- Active
Links
Images
Classifications
-
- G—PHYSICS
- G05—CONTROLLING; REGULATING
- G05D—SYSTEMS FOR CONTROLLING OR REGULATING NON-ELECTRIC VARIABLES
- G05D1/00—Control of position, course or altitude of land, water, air, or space vehicles, e.g. automatic pilot
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B60—VEHICLES IN GENERAL
- B60W—CONJOINT CONTROL OF VEHICLE SUB-UNITS OF DIFFERENT TYPE OR DIFFERENT FUNCTION; CONTROL SYSTEMS SPECIALLY ADAPTED FOR HYBRID VEHICLES; ROAD VEHICLE DRIVE CONTROL SYSTEMS FOR PURPOSES NOT RELATED TO THE CONTROL OF A PARTICULAR SUB-UNIT
- B60W10/00—Conjoint control of vehicle sub-units of different type or different function
- B60W10/20—Conjoint control of vehicle sub-units of different type or different function including control of steering systems
-
- G—PHYSICS
- G01—MEASURING; TESTING
- G01S—RADIO DIRECTION-FINDING; RADIO NAVIGATION; DETERMINING DISTANCE OR VELOCITY BY USE OF RADIO WAVES; LOCATING OR PRESENCE-DETECTING BY USE OF THE REFLECTION OR RERADIATION OF RADIO WAVES; ANALOGOUS ARRANGEMENTS USING OTHER WAVES
- G01S17/00—Systems using the reflection or reradiation of electromagnetic waves other than radio waves, e.g. lidar systems
- G01S17/88—Lidar systems specially adapted for specific applications
- G01S17/89—Lidar systems specially adapted for specific applications for mapping or imaging
-
- G—PHYSICS
- G05—CONTROLLING; REGULATING
- G05D—SYSTEMS FOR CONTROLLING OR REGULATING NON-ELECTRIC VARIABLES
- G05D1/00—Control of position, course or altitude of land, water, air, or space vehicles, e.g. automatic pilot
- G05D1/02—Control of position or course in two dimensions
- G05D1/021—Control of position or course in two dimensions specially adapted to land vehicles
-
- B—PERFORMING OPERATIONS; TRANSPORTING
- B60—VEHICLES IN GENERAL
- B60W—CONJOINT CONTROL OF VEHICLE SUB-UNITS OF DIFFERENT TYPE OR DIFFERENT FUNCTION; CONTROL SYSTEMS SPECIALLY ADAPTED FOR HYBRID VEHICLES; ROAD VEHICLE DRIVE CONTROL SYSTEMS FOR PURPOSES NOT RELATED TO THE CONTROL OF A PARTICULAR SUB-UNIT
- B60W2710/00—Output or target parameters relating to a particular sub-units
- B60W2710/20—Steering systems
Definitions
- the processor is configured to determine an initial estimate for a location of a second edge of a second lane marker bounding the lane; generate a second set of possible estimates for the location of the second edge based on the offset distance; for each estimate of the second set of possible estimates, determine a second likelihood value indicative of how likely the estimate is to be the location of the second edge; select a second most likely estimate of the second set of possible estimates based on the likelihood values; filter the sensor data to identify second data points within the pre-determined distance of the selected second most likely estimate; fit the identified second data points to a second model describing a second location and shape of the second edge; and determine a second representation of the second edge based on the second model.
- the processor may also be configured to determine a centerline of the lane based on the representation and the second representation and to maneuver the vehicle along the centerline. The processor may also be configured to maneuver the vehicle autonomously based on the representation and the second representation.
- FIG. 1 is a functional diagram of a system in accordance with aspects of the disclosure.
- FIG. 5 is a diagram of laser data in accordance with an implementation.
- FIG. 6 is another diagram of laser data in accordance with an implementation.
- the processor 120 may be any conventional processor, such as commercially available CPUs. Alternatively, the processor may be a dedicated device such as an ASIC.
- FIG. 1 functionally illustrates the processor, memory, and other elements of computer 110 as being within the same block, it will be understood that the processor and memory may actually comprise multiple processors and memories that may or may not be stored within the same physical housing.
- memory may be a hard drive or other storage media located in a housing different from that of computer 110 .
- references to a processor or computer will be understood to include references to a collection of processors or computers or memories that may or may not operate in parallel. Rather than using a single processor to perform the steps described herein some of the components, such as steering components and deceleration components, may each have their own processor that only performs calculations related to the component's specific function.
- computer 110 may be an autonomous driving computing system incorporated into vehicle 101 .
- FIG. 2 depicts an exemplary design of the interior of an autonomous vehicle.
- the autonomous vehicle may include all of the features of a non-autonomous vehicle, for example: a steering apparatus, such as steering wheel 210 ; a navigation display apparatus, such as navigation display 215 ; and a gear selector apparatus, such as gear shifter 220 .
- the vehicle may also have various user input devices, such as gear shifter 220 , touch screen 217 , or button inputs 219 , for activating or deactivating one or more autonomous driving modes and for enabling a driver or passenger 290 to provide information, such as a navigation destination, to the autonomous driving computer 110 .
- data 134 may include detailed map information 136 , e.g., highly detailed maps identifying the shape and elevation of roadways, intersections, crosswalks, speed limits, traffic signals, buildings, signs, real time traffic information, or other such objects and information.
- computer 320 may comprise a server having a plurality of computers, e.g., a load balanced server farm, that exchange information with different nodes of a network for the purpose of receiving, processing and transmitting the data from computer 110 .
- the server may be configured similarly to the computer 110 , with a processor 330 , memory 350 , instructions 360 , and data 370 .
- the laser scan data may be processed by computer 110 (or computer 320 ) to generate geographic location coordinates.
- These geographic location coordinates may include GPS latitude and longitude coordinates (x,y) with an elevation component (z), or may be associated with other coordinate systems.
- the result of this processing is a set of data point.
- Each data point of this set may include an intensity value indicative of the reflectivity of the object from which the light was received by the laser as well as location information: (x,y,z).
- FIG. 6 depicts an exemplary image 600 of vehicle 101 approaching an intersection.
- the image was generated from laser scan data collected by the vehicle's lasers for a single 360 degree scan of the vehicle's surroundings, for example, using the data points of all of the beams of the collecting laser(s).
- the white lines represent how the laser “sees” its surroundings.
- the data points may indicate the shape and three-dimensional (3D) location (x,y,z) of other items in the vehicle's surroundings.
- the laser scan data may indicate the outline, shape and distance from vehicle 101 of various objects such as people 610 , vehicles 620 , and curb 630 .
- FIG. 7 depicts another example 700 of laser scan data collected for a single scan while a vehicle is driven along roadway 500 of FIG. 5 (and also that depicted in map information 400 of FIG. 4 ).
- vehicle 101 is depicted surrounded by laser lines 730 indicating the area around the vehicle scanned by the laser.
- Each laser line may represent a series of discrete data points from a single beam. Data points from more highly reflective features such as lane lines, white paint, reflectors, or those with retroreflective properties may have a greater intensity than less reflective features.
- reference line 720 connects the data points 710 associated with a solid lane line and is not part of the laser data.
- the computer may first cluster the lane marker data points, extract a general shape and use this as an initial estimate.
- An initial estimate for the lane edges may also be made by observing where vehicles (including the vehicle with the laser or lasers and/or other vehicles) have driven or where they currently are. In this example, the heading of other vehicles may also be used as an initial estimate for the lane direction.
- An initial estimate may also be randomly generated, for example using a random sample consensus (RANSAC) algorithm.
- RANSAC random sample consensus
- the criteria may also include the total number of data points (all data points, and not specifically lane marker data points) within the distance of the estimate, and, if available, the deviation of the estimate from the most recent estimate.
- the computer may select the estimate most likely to be the lane line edge. For example, all or some of the aforementioned criteria may be used to determine a likelihood value for each estimate of the set of estimates. The estimate with the highest likelihood value may be selected as the most likely estimate. The computer may then identify a set of data points by filtering all of the data points or the lane marker data points which are within a distance of the selected estimate. Again the filter may be on the order of 25 to 40 centimeters.
- the identified set of data points may be fit to a model describing the location and shape of the edge.
- the fit may be based on parametric models such linear, cubic, higher-order polynomials, splines, or clothoids, or non-parametric models.
- the fit may also involve using a least squares method and a quadratic model.
- an independent variable used for the model may first be aligned with a heading or principle direction for the estimate. For example, for edge 474 , the direction of rail 465 (or some small deviation from the direction) for lane 460 may be the direction of the estimate.
- the model may be post-processed to remove any noisy sections. This may be useful at end points (between broken sections of a lane line or simply the end of a lane line) where there are only few data points have a significant bearing on the solution.
- the computer may analyze the number points used to calculate a particular part of the estimate (the support) over various sections of the model. This may include comparing the absolute number of data points of the identified set of data points in a given section to the number of data points of the identified set of data points of other second or over the entire model. The analysis may also include evaluating the curvature or smoothness of sections and filtering or smoothing out those sections that appear spurious.
- the post-processing may also include comparing the model to the previous estimate. For example, the computer may determine how much the previous estimate and the current estimate for a lane edge differ. The computer may allow for some maximum deviation or difference (e.g., it may be unlikely that the lane has completely changed direction). If the current estimate differs more than this maximum, the entire estimate, or any section of the estimate that differs for than the maximum, may be thrown out or ignored.
- Flow diagram 1100 of FIG. 11 depicts some of the aspects described above. Each of the following steps may be performed by computer 110 , computer 320 , or a combination of both.
- laser scan data including data points is collected along a roadway using one or more lasers or other devices at block 1102 .
- the data points may describe intensity and location information for the objects from which the laser light was reflected.
- an initial estimate for the location of an edge of a lane line bounding a lane is determined at block 1104 . The initial estimate may be determined based on detailed map information, previous estimates, and/or any of the examples described above.
- the aspects described above may provide additional benefits. For example, by filtering the data points to examine only those relevant to the most likely estimate of an edge before the fitting, the amount of data to be processed and fit to a model may be dramatically reduced. This, in turn, may reduce the processing power cost required to fit the model. If the laser scan data is being processed in real time to identify the location of lane line edges to maneuver an autonomous vehicle, the modeling may be performed every tenth of a second. This is important because the vehicle's computer must be able to make decisions quickly. Thus, the value of the savings in terms of time and processing power cost may be enormous.
Abstract
Description
Claims (20)
Priority Applications (1)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US14/270,653 US8948958B1 (en) | 2012-03-23 | 2014-05-06 | Estimating road lane geometry using lane marker observations |
Applications Claiming Priority (2)
Application Number | Priority Date | Filing Date | Title |
---|---|---|---|
US13/427,959 US8755967B1 (en) | 2012-03-23 | 2012-03-23 | Estimating road lane geometry using lane marker observations |
US14/270,653 US8948958B1 (en) | 2012-03-23 | 2014-05-06 | Estimating road lane geometry using lane marker observations |
Related Parent Applications (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/427,959 Continuation US8755967B1 (en) | 2012-03-23 | 2012-03-23 | Estimating road lane geometry using lane marker observations |
Publications (1)
Publication Number | Publication Date |
---|---|
US8948958B1 true US8948958B1 (en) | 2015-02-03 |
Family
ID=50896873
Family Applications (2)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/427,959 Active 2032-09-05 US8755967B1 (en) | 2012-03-23 | 2012-03-23 | Estimating road lane geometry using lane marker observations |
US14/270,653 Active US8948958B1 (en) | 2012-03-23 | 2014-05-06 | Estimating road lane geometry using lane marker observations |
Family Applications Before (1)
Application Number | Title | Priority Date | Filing Date |
---|---|---|---|
US13/427,959 Active 2032-09-05 US8755967B1 (en) | 2012-03-23 | 2012-03-23 | Estimating road lane geometry using lane marker observations |
Country Status (1)
Country | Link |
---|---|
US (2) | US8755967B1 (en) |
Cited By (9)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20160026184A1 (en) * | 2014-07-24 | 2016-01-28 | GM Global Technology Operations LLC | Curb detection using lidar with sparse measurements |
US20170010616A1 (en) * | 2015-02-10 | 2017-01-12 | Mobileye Vision Technologies Ltd. | Sparse map for autonomous vehicle navigation |
US9968025B2 (en) | 2016-01-14 | 2018-05-15 | CNH Industrial American LLC | System and method for generating and implementing an end-of-row turn path |
US9974225B2 (en) | 2016-01-14 | 2018-05-22 | Cnh Industrial America Llc | System and method for generating and implementing an end-of-row turn path |
CN109506672A (en) * | 2017-09-15 | 2019-03-22 | 高德软件有限公司 | A kind of acquisition methods and device of pavement markers laser point cloud |
US10365658B2 (en) | 2016-07-21 | 2019-07-30 | Mobileye Vision Technologies Ltd. | Systems and methods for aligning crowdsourced sparse map data |
US11009881B2 (en) | 2019-04-05 | 2021-05-18 | Caterpillar Paving Products Inc. | Roadway center detection for autonomous vehicle control |
US20220011123A1 (en) * | 2020-07-08 | 2022-01-13 | IFP Energies Nouvelles | Method of characterizing a route travelled by a user |
CN114291086A (en) * | 2021-12-31 | 2022-04-08 | 高德软件有限公司 | Lane center line generation method and device and computer storage medium |
Families Citing this family (11)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US10422649B2 (en) * | 2014-02-24 | 2019-09-24 | Ford Global Technologies, Llc | Autonomous driving sensing system and method |
US9562779B2 (en) * | 2014-12-23 | 2017-02-07 | Here Global B.V. | Method and apparatus for providing a steering reliability map based on driven curvatures and geometry curvature |
US9530062B2 (en) | 2014-12-23 | 2016-12-27 | Volkswagen Ag | Fused raised pavement marker detection for autonomous driving using lidar and camera |
US10093315B2 (en) * | 2016-09-19 | 2018-10-09 | Ford Global Technologies, Llc | Target vehicle deselection |
CN106462757B (en) * | 2016-09-26 | 2019-09-06 | 深圳市锐明技术股份有限公司 | A kind of rapid detection method and device of pairs of lane line |
JP6986557B2 (en) * | 2017-05-19 | 2021-12-22 | パイオニア株式会社 | Measuring equipment, measuring methods and programs |
CN108931786A (en) * | 2018-05-17 | 2018-12-04 | 北京智行者科技有限公司 | Curb detection device and method |
US10909866B2 (en) | 2018-07-20 | 2021-02-02 | Cybernet Systems Corp. | Autonomous transportation system and methods |
CN109297499A (en) * | 2018-08-20 | 2019-02-01 | 武汉中海庭数据技术有限公司 | Lane model building method, device and computer can storage mediums |
CN112347210B (en) * | 2020-11-03 | 2022-05-31 | 武汉光庭信息技术股份有限公司 | Lane center line bending error correction method and system for medium-precision map |
CN113566817B (en) * | 2021-07-23 | 2024-03-08 | 北京经纬恒润科技股份有限公司 | Vehicle positioning method and device |
Citations (14)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US20020131620A1 (en) | 2000-12-27 | 2002-09-19 | Nissan Motor Co., Ltd. | Lane recognition apparatus for vehicle |
US6489887B2 (en) | 2000-09-06 | 2002-12-03 | Nissan Motor Co., Ltd. | Lane-keep assisting system for vehicle |
EP1320072A2 (en) | 2001-11-30 | 2003-06-18 | Hitachi Ltd. | Lane marker recognition method |
JP2003255047A (en) | 2001-12-27 | 2003-09-10 | Nissan Motor Co Ltd | Apparatus and method for detecting object in front |
JP2004139338A (en) | 2002-10-17 | 2004-05-13 | Nissan Motor Co Ltd | Lane marker recognition device |
US6868168B2 (en) | 2000-12-26 | 2005-03-15 | Nissan Motor Co., Ltd. | Lane recognition system for vehicle |
US20070183666A1 (en) | 2006-02-08 | 2007-08-09 | Yuhua Ding | Method utilizing intensity interpolation for measuring edge locations in a high precision machine vision inspection system |
US20100014713A1 (en) | 2008-07-18 | 2010-01-21 | Gm Global Technology Operations, Inc. | Road-lane marker detection |
US20100121577A1 (en) | 2008-04-24 | 2010-05-13 | Gm Global Technology Operations, Inc. | Three-dimensional lidar-based clear path detection |
US20100217529A1 (en) | 2009-02-20 | 2010-08-26 | Matei Nicolai Stroila | Determining Travel Path Features Based on Retroreflectivity |
US20100253542A1 (en) | 2009-04-02 | 2010-10-07 | Gm Global Technology Operations, Inc. | Point of interest location marking on full windshield head-up display |
US20100299109A1 (en) | 2009-05-22 | 2010-11-25 | Fuji Jukogyo Kabushiki Kaisha | Road shape recognition device |
US20110200258A1 (en) | 2010-02-12 | 2011-08-18 | Denso Corporation | Lane-marker recognition system with improved recognition-performance |
US8520071B2 (en) | 2010-02-24 | 2013-08-27 | Denso Corporation | Boundary line detection system with improved detection-performance |
-
2012
- 2012-03-23 US US13/427,959 patent/US8755967B1/en active Active
-
2014
- 2014-05-06 US US14/270,653 patent/US8948958B1/en active Active
Patent Citations (15)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US6489887B2 (en) | 2000-09-06 | 2002-12-03 | Nissan Motor Co., Ltd. | Lane-keep assisting system for vehicle |
US6868168B2 (en) | 2000-12-26 | 2005-03-15 | Nissan Motor Co., Ltd. | Lane recognition system for vehicle |
US20020131620A1 (en) | 2000-12-27 | 2002-09-19 | Nissan Motor Co., Ltd. | Lane recognition apparatus for vehicle |
EP1320072A2 (en) | 2001-11-30 | 2003-06-18 | Hitachi Ltd. | Lane marker recognition method |
JP2003255047A (en) | 2001-12-27 | 2003-09-10 | Nissan Motor Co Ltd | Apparatus and method for detecting object in front |
JP2004139338A (en) | 2002-10-17 | 2004-05-13 | Nissan Motor Co Ltd | Lane marker recognition device |
US20070183666A1 (en) | 2006-02-08 | 2007-08-09 | Yuhua Ding | Method utilizing intensity interpolation for measuring edge locations in a high precision machine vision inspection system |
US20100121577A1 (en) | 2008-04-24 | 2010-05-13 | Gm Global Technology Operations, Inc. | Three-dimensional lidar-based clear path detection |
US20100014713A1 (en) | 2008-07-18 | 2010-01-21 | Gm Global Technology Operations, Inc. | Road-lane marker detection |
US20100217529A1 (en) | 2009-02-20 | 2010-08-26 | Matei Nicolai Stroila | Determining Travel Path Features Based on Retroreflectivity |
US20100253542A1 (en) | 2009-04-02 | 2010-10-07 | Gm Global Technology Operations, Inc. | Point of interest location marking on full windshield head-up display |
US20100299109A1 (en) | 2009-05-22 | 2010-11-25 | Fuji Jukogyo Kabushiki Kaisha | Road shape recognition device |
US8340896B2 (en) | 2009-05-22 | 2012-12-25 | Fuji Jukogyo Kabushiki Kaisha | Road shape recognition device |
US20110200258A1 (en) | 2010-02-12 | 2011-08-18 | Denso Corporation | Lane-marker recognition system with improved recognition-performance |
US8520071B2 (en) | 2010-02-24 | 2013-08-27 | Denso Corporation | Boundary line detection system with improved detection-performance |
Non-Patent Citations (1)
Title |
---|
International Search Report and Written Opinion for Application No. PCT/US2013/033315 dated Jan. 9, 2014. |
Cited By (18)
Publication number | Priority date | Publication date | Assignee | Title |
---|---|---|---|---|
US9529087B2 (en) * | 2014-07-24 | 2016-12-27 | GM Global Technology Operations LLC | Curb detection using lidar with sparse measurements |
US20160026184A1 (en) * | 2014-07-24 | 2016-01-28 | GM Global Technology Operations LLC | Curb detection using lidar with sparse measurements |
US10317903B2 (en) | 2015-02-10 | 2019-06-11 | Mobileye Vision Technologies Ltd. | Sparse map for autonomous vehicle navigation |
US20170010616A1 (en) * | 2015-02-10 | 2017-01-12 | Mobileye Vision Technologies Ltd. | Sparse map for autonomous vehicle navigation |
US9665100B2 (en) * | 2015-02-10 | 2017-05-30 | Mobileye Vision Technologies Ltd. | Sparse map for autonomous vehicle navigation |
US9968025B2 (en) | 2016-01-14 | 2018-05-15 | CNH Industrial American LLC | System and method for generating and implementing an end-of-row turn path |
US9974225B2 (en) | 2016-01-14 | 2018-05-22 | Cnh Industrial America Llc | System and method for generating and implementing an end-of-row turn path |
US10365658B2 (en) | 2016-07-21 | 2019-07-30 | Mobileye Vision Technologies Ltd. | Systems and methods for aligning crowdsourced sparse map data |
US10558222B2 (en) | 2016-07-21 | 2020-02-11 | Mobileye Vision Technologies Ltd. | Navigating a vehicle using a crowdsourced sparse map |
US10838426B2 (en) | 2016-07-21 | 2020-11-17 | Mobileye Vision Technologies Ltd. | Distributing a crowdsourced sparse map for autonomous vehicle navigation |
US10962982B2 (en) | 2016-07-21 | 2021-03-30 | Mobileye Vision Technologies Ltd. | Crowdsourcing the collection of road surface information |
US11086334B2 (en) | 2016-07-21 | 2021-08-10 | Mobileye Vision Technologies Ltd. | Crowdsourcing a sparse map for autonomous vehicle navigation |
CN109506672A (en) * | 2017-09-15 | 2019-03-22 | 高德软件有限公司 | A kind of acquisition methods and device of pavement markers laser point cloud |
CN109506672B (en) * | 2017-09-15 | 2023-05-12 | 阿里巴巴（中国）有限公司 | Method and device for acquiring laser point cloud of pavement marker |
US11009881B2 (en) | 2019-04-05 | 2021-05-18 | Caterpillar Paving Products Inc. | Roadway center detection for autonomous vehicle control |
US20220011123A1 (en) * | 2020-07-08 | 2022-01-13 | IFP Energies Nouvelles | Method of characterizing a route travelled by a user |
CN114291086A (en) * | 2021-12-31 | 2022-04-08 | 高德软件有限公司 | Lane center line generation method and device and computer storage medium |
CN114291086B (en) * | 2021-12-31 | 2023-05-23 | 高德软件有限公司 | Lane center line generation method and device and computer storage medium |
Also Published As
Publication number | Publication date |
---|---|
US8755967B1 (en) | 2014-06-17 |
Similar Documents
Publication | Publication Date | Title |
---|---|---|
US8948958B1 (en) | Estimating road lane geometry using lane marker observations | |
US11726493B2 (en) | Modifying behavior of autonomous vehicles based on sensor blind spots and limitations | |
US11807235B1 (en) | Modifying speed of an autonomous vehicle based on traffic conditions | |
US10037039B1 (en) | Object bounding box estimation | |
US11868133B1 (en) | Avoiding blind spots of other vehicles | |
CN107798305B (en) | Detecting lane markings | |
US10185324B1 (en) | Building elevation maps from laser data | |
US8949016B1 (en) | Systems and methods for determining whether a driving environment has changed | |
US8565958B1 (en) | Removing extraneous objects from maps | |
US9255805B1 (en) | Pose estimation using long range features | |
US8874372B1 (en) | Object detection and classification for autonomous vehicles | |
US20130197736A1 (en) | Vehicle control based on perception uncertainty | |
US10094670B1 (en) | Condensing sensor data for transmission and processing |
Legal Events
Date | Code | Title | Description |
---|---|---|---|
AS | Assignment |
Owner name: GOOGLE INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNORS:FERGUSON, DAVID IAN FRANKLIN;ZHU, JIAJUN;REEL/FRAME:032840/0230Effective date: 20120321 |
|
STCF | Information on status: patent grant |
Free format text: PATENTED CASE |
|
AS | Assignment |
Owner name: WAYMO HOLDING INC., CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:GOOGLE INC.;REEL/FRAME:042099/0935Effective date: 20170321 |
|
AS | Assignment |
Owner name: WAYMO LLC, CALIFORNIAFree format text: ASSIGNMENT OF ASSIGNORS INTEREST;ASSIGNOR:WAYMO HOLDING INC.;REEL/FRAME:042108/0021Effective date: 20170322 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:044144/0001Effective date: 20170929 |
|
AS | Assignment |
Owner name: GOOGLE LLC, CALIFORNIAFree format text: CORRECTIVE ASSIGNMENT TO CORRECT THE CORRECTIVE BY NULLIFICATION TO CORRECT INCORRECTLY RECORDED APPLICATION NUMBERS PREVIOUSLY RECORDED ON REEL 044144 FRAME 0001. ASSIGNOR(S) HEREBY CONFIRMS THE CHANGE OF NAME;ASSIGNOR:GOOGLE INC.;REEL/FRAME:047894/0508Effective date: 20170929 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 4TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1551)Year of fee payment: 4 |
|
AS | Assignment |
Owner name: WAYMO LLC, CALIFORNIAFree format text: SUBMISSION TO CORRECT AN ERROR MADE IN A PREVIOUSLY RECORDED DOCUMENT THAT ERRONEOUSLY AFFECTS THE IDENTIFIED APPLICATIONS;ASSIGNOR:WAYMO LLC;REEL/FRAME:050978/0359Effective date: 20191001 |
|
MAFP | Maintenance fee payment |
Free format text: PAYMENT OF MAINTENANCE FEE, 8TH YEAR, LARGE ENTITY (ORIGINAL EVENT CODE: M1552); ENTITY STATUS OF PATENT OWNER: LARGE ENTITYYear of fee payment: 8 |